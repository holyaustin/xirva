[{"id": "2104.00008", "submitter": "Dan Roberts", "authors": "Daniel A. Roberts", "title": "Why is AI hard and Physics simple?", "comments": "written for a special issue of Machine Learning: Science and\n  Technology as an invited perspective piece", "journal-ref": null, "doi": null, "report-no": "MIT-CTP/5269", "categories": "hep-th cs.AI cs.LG physics.hist-ph stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We discuss why AI is hard and why physics is simple. We discuss how physical\nintuition and the approach of theoretical physics can be brought to bear on the\nfield of artificial intelligence and specifically machine learning. We suggest\nthat the underlying project of machine learning and the underlying project of\nphysics are strongly coupled through the principle of sparsity, and we call\nupon theoretical physicists to work on AI as physicists. As a first step in\nthat direction, we discuss an upcoming book on the principles of deep learning\ntheory that attempts to realize this approach.\n", "versions": [{"version": "v1", "created": "Wed, 31 Mar 2021 18:00:01 GMT"}], "update_date": "2021-04-09", "authors_parsed": [["Roberts", "Daniel A.", ""]]}, {"id": "2104.00170", "submitter": "Robik Shrestha", "authors": "Robik Shrestha, Kushal Kafle and Christopher Kanan", "title": "An Investigation of Critical Issues in Bias Mitigation Techniques", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI cs.CV stat.ML", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  A critical problem in deep learning is that systems learn inappropriate\nbiases, resulting in their inability to perform well on minority groups. This\nhas led to the creation of multiple algorithms that endeavor to mitigate bias.\nHowever, it is not clear how effective these methods are. This is because study\nprotocols differ among papers, systems are tested on datasets that fail to test\nmany forms of bias, and systems have access to hidden knowledge or are tuned\nspecifically to the test set. To address this, we introduce an improved\nevaluation protocol, sensible metrics, and a new dataset, which enables us to\nask and answer critical questions about bias mitigation algorithms. We evaluate\nseven state-of-the-art algorithms using the same network architecture and\nhyperparameter selection policy across three benchmark datasets. We introduce a\nnew dataset called Biased MNIST that enables assessment of robustness to\nmultiple bias sources. We use Biased MNIST and a visual question answering\n(VQA) benchmark to assess robustness to hidden biases. Rather than only tuning\nto the test set distribution, we study robustness across different tuning\ndistributions, which is critical because for many applications the test\ndistribution may not be known during development. We find that algorithms\nexploit hidden biases, are unable to scale to multiple forms of bias, and are\nhighly sensitive to the choice of tuning set. Based on our findings, we implore\nthe community to adopt more rigorous assessment of future bias mitigation\nmethods. All data, code, and results are publicly available at:\nhttps://github.com/erobic/bias-mitigators.\n", "versions": [{"version": "v1", "created": "Thu, 1 Apr 2021 00:14:45 GMT"}], "update_date": "2021-04-02", "authors_parsed": [["Shrestha", "Robik", ""], ["Kafle", "Kushal", ""], ["Kanan", "Christopher", ""]]}, {"id": "2104.00245", "submitter": "Zhe Zhang", "authors": "Zhe Zhang and Linjun Zhang", "title": "High-Dimensional Differentially-Private EM Algorithm: Methods and\n  Near-Optimal Statistical Guarantees", "comments": "45 pages, 3 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.CR cs.LG math.ST stat.ME stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we develop a general framework to design differentially\nprivate expectation-maximization (EM) algorithms in high-dimensional latent\nvariable models, based on the noisy iterative hard-thresholding. We derive the\nstatistical guarantees of the proposed framework and apply it to three specific\nmodels: Gaussian mixture, mixture of regression, and regression with missing\ncovariates. In each model, we establish the near-optimal rate of convergence\nwith differential privacy constraints, and show the proposed algorithm is\nminimax rate optimal up to logarithm factors. The technical tools developed for\nthe high-dimensional setting are then extended to the classic low-dimensional\nlatent variable models, and we propose a near rate-optimal EM algorithm with\ndifferential privacy guarantees in this setting. Simulation studies and real\ndata analysis are conducted to support our results.\n", "versions": [{"version": "v1", "created": "Thu, 1 Apr 2021 04:08:34 GMT"}], "update_date": "2021-04-02", "authors_parsed": [["Zhang", "Zhe", ""], ["Zhang", "Linjun", ""]]}, {"id": "2104.00428", "submitter": "Emile van Krieken", "authors": "Emile van Krieken, Jakub M. Tomczak, Annette ten Teije", "title": "Storchastic: A Framework for General Stochastic Automatic\n  Differentiation", "comments": "28 pages, 1 figure", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.AI cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Modelers use automatic differentiation (AD) of computation graphs to\nimplement complex Deep Learning models without defining gradient computations.\nStochastic AD extends AD to stochastic computation graphs with sampling steps,\nwhich arise when modelers handle the intractable expectations common in\nReinforcement Learning and Variational Inference. However, current methods for\nstochastic AD are limited: They are either only applicable to continuous random\nvariables and differentiable functions, or can only use simple but high\nvariance score-function estimators. To overcome these limitations, we introduce\nStorchastic, a new framework for AD of stochastic computation graphs.\nStorchastic allows the modeler to choose from a wide variety of gradient\nestimation methods at each sampling step, to optimally reduce the variance of\nthe gradient estimates. Furthermore, Storchastic is provably unbiased for\nestimation of any-order gradients, and generalizes variance reduction\ntechniques to higher-order gradient estimates. Finally, we implement\nStorchastic as a PyTorch library.\n", "versions": [{"version": "v1", "created": "Thu, 1 Apr 2021 12:19:54 GMT"}, {"version": "v2", "created": "Fri, 28 May 2021 14:13:16 GMT"}], "update_date": "2021-05-31", "authors_parsed": [["van Krieken", "Emile", ""], ["Tomczak", "Jakub M.", ""], ["Teije", "Annette ten", ""]]}, {"id": "2104.00507", "submitter": "Przemyslaw Biecek", "authors": "Jakub Wi\\'sniewski, Przemys{\\l}aw Biecek", "title": "fairmodels: A Flexible Tool For Bias Detection, Visualization, And\n  Mitigation", "comments": "15 pages, 9 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG cs.MS stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Machine learning decision systems are getting omnipresent in our lives. From\ndating apps to rating loan seekers, algorithms affect both our well-being and\nfuture. Typically, however, these systems are not infallible. Moreover, complex\npredictive models are really eager to learn social biases present in historical\ndata that can lead to increasing discrimination. If we want to create models\nresponsibly then we need tools for in-depth validation of models also from the\nperspective of potential discrimination. This article introduces an R package\nfairmodels that helps to validate fairness and eliminate bias in classification\nmodels in an easy and flexible fashion. The fairmodels package offers a\nmodel-agnostic approach to bias detection, visualization and mitigation. The\nimplemented set of functions and fairness metrics enables model fairness\nvalidation from different perspectives. The package includes a series of\nmethods for bias mitigation that aim to diminish the discrimination in the\nmodel. The package is designed not only to examine a single model, but also to\nfacilitate comparisons between multiple models.\n", "versions": [{"version": "v1", "created": "Thu, 1 Apr 2021 15:06:13 GMT"}], "update_date": "2021-04-02", "authors_parsed": [["Wi\u015bniewski", "Jakub", ""], ["Biecek", "Przemys\u0142aw", ""]]}, {"id": "2104.00530", "submitter": "Andrew Song", "authors": "Andrew H. Song, Bahareh Tolooshams, Demba Ba", "title": "Gaussian Process Convolutional Dictionary Learning", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.AP stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Convolutional dictionary learning (CDL), the problem of estimating\nshift-invariant templates from data, is typically conducted in the absence of a\nprior/structure on the templates. In data-scarce or low signal-to-noise ratio\n(SNR) regimes, which have received little attention from the community, learned\ntemplates overfit the data and lack smoothness, which can affect the predictive\nperformance of downstream tasks. To address this limitation, we propose GPCDL,\na convolutional dictionary learning framework that enforces priors on templates\nusing Gaussian Processes (GPs). With the focus on smoothness, we show\ntheoretically that imposing a GP prior is equivalent to Wiener filtering the\nlearned templates, thereby suppressing high-frequency components and promoting\nsmoothness. We show that the algorithm is a simple extension of the classical\niteratively reweighted least squares, which allows the flexibility to\nexperiment with different smoothness assumptions. Through simulation, we show\nthat GPCDL learns smooth dictionaries with better accuracy than the\nunregularized alternative across a range of SNRs. Through an application to\nneural spiking data from rats, we show that learning templates by GPCDL results\nin a more accurate and visually-interpretable smooth dictionary, leading to\nsuperior predictive performance compared to non-regularized CDL, as well as\nparametric alternatives.\n", "versions": [{"version": "v1", "created": "Sun, 28 Mar 2021 21:40:03 GMT"}], "update_date": "2021-04-02", "authors_parsed": [["Song", "Andrew H.", ""], ["Tolooshams", "Bahareh", ""], ["Ba", "Demba", ""]]}, {"id": "2104.00584", "submitter": "Vitor Cerqueira", "authors": "Vitor Cerqueira, Luis Torgo, Carlos Soares", "title": "Model Selection for Time Series Forecasting: Empirical Analysis of\n  Different Estimators", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Evaluating predictive models is a crucial task in predictive analytics. This\nprocess is especially challenging with time series data where the observations\nshow temporal dependencies. Several studies have analysed how different\nperformance estimation methods compare with each other for approximating the\ntrue loss incurred by a given forecasting model. However, these studies do not\naddress how the estimators behave for model selection: the ability to select\nthe best solution among a set of alternatives. We address this issue and\ncompare a set of estimation methods for model selection in time series\nforecasting tasks. We attempt to answer two main questions: (i) how often is\nthe best possible model selected by the estimators; and (ii) what is the\nperformance loss when it does not. We empirically found that the accuracy of\nthe estimators for selecting the best solution is low, and the overall\nforecasting performance loss associated with the model selection process ranges\nfrom 1.2% to 2.3%. We also discovered that some factors, such as the sample\nsize, are important in the relative performance of the estimators.\n", "versions": [{"version": "v1", "created": "Thu, 1 Apr 2021 16:08:25 GMT"}], "update_date": "2021-04-02", "authors_parsed": [["Cerqueira", "Vitor", ""], ["Torgo", "Luis", ""], ["Soares", "Carlos", ""]]}, {"id": "2104.00587", "submitter": "Heiko Strathmann", "authors": "Adam R. Kosiorek, Heiko Strathmann, Daniel Zoran, Pol Moreno, Rosalia\n  Schneider, So\\v{n}a Mokr\\'a, Danilo J. Rezende", "title": "NeRF-VAE: A Geometry Aware 3D Scene Generative Model", "comments": "17 pages, 15 figures, under review", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose NeRF-VAE, a 3D scene generative model that incorporates geometric\nstructure via NeRF and differentiable volume rendering. In contrast to NeRF,\nour model takes into account shared structure across scenes, and is able to\ninfer the structure of a novel scene -- without the need to re-train -- using\namortized inference. NeRF-VAE's explicit 3D rendering process further contrasts\nprevious generative models with convolution-based rendering which lacks\ngeometric structure. Our model is a VAE that learns a distribution over\nradiance fields by conditioning them on a latent scene representation. We show\nthat, once trained, NeRF-VAE is able to infer and render\ngeometrically-consistent scenes from previously unseen 3D environments using\nvery few input images. We further demonstrate that NeRF-VAE generalizes well to\nout-of-distribution cameras, while convolutional models do not. Finally, we\nintroduce and study an attention-based conditioning mechanism of NeRF-VAE's\ndecoder, which improves model performance.\n", "versions": [{"version": "v1", "created": "Thu, 1 Apr 2021 16:16:31 GMT"}], "update_date": "2021-04-02", "authors_parsed": [["Kosiorek", "Adam R.", ""], ["Strathmann", "Heiko", ""], ["Zoran", "Daniel", ""], ["Moreno", "Pol", ""], ["Schneider", "Rosalia", ""], ["Mokr\u00e1", "So\u0148a", ""], ["Rezende", "Danilo J.", ""]]}, {"id": "2104.00629", "submitter": "Florian Pargent", "authors": "Florian Pargent, Florian Pfisterer, Janek Thomas, Bernd Bischl", "title": "Regularized target encoding outperforms traditional methods in\n  supervised machine learning with high cardinality features", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Because most machine learning (ML) algorithms are designed for numerical\ninputs, efficiently encoding categorical variables is a crucial aspect during\ndata analysis. An often encountered problem are high cardinality features, i.e.\nunordered categorical predictor variables with a high number of levels. We\nstudy techniques that yield numeric representations of categorical variables\nwhich can then be used in subsequent ML applications. We focus on the impact of\nthose techniques on a subsequent algorithm's predictive performance, and -- if\npossible -- derive best practices on when to use which technique. We conducted\na large-scale benchmark experiment, where we compared different encoding\nstrategies together with five ML algorithms (lasso, random forest, gradient\nboosting, k-nearest neighbours, support vector machine) using datasets from\nregression, binary- and multiclass- classification settings. Throughout our\nstudy, regularized versions of target encoding (i.e. using target predictions\nbased on the feature levels in the training set as a new numerical feature)\nconsistently provided the best results. Traditional encodings that make\nunreasonable assumptions to map levels to integers (e.g. integer encoding) or\nto reduce the number of levels (possibly based on target information, e.g. leaf\nencoding) before creating binary indicator variables (one-hot or dummy\nencoding) were not as effective.\n", "versions": [{"version": "v1", "created": "Thu, 1 Apr 2021 17:21:42 GMT"}], "update_date": "2021-04-02", "authors_parsed": [["Pargent", "Florian", ""], ["Pfisterer", "Florian", ""], ["Thomas", "Janek", ""], ["Bischl", "Bernd", ""]]}, {"id": "2104.00635", "submitter": "Michael Platzer", "authors": "Michael Platzer and Thomas Reutterer", "title": "Holdout-Based Fidelity and Privacy Assessment of Mixed-Type Synthetic\n  Data", "comments": "12 pages, 9 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  AI-based data synthesis has seen rapid progress over the last several years,\nand is increasingly recognized for its promise to enable privacy-respecting\nhigh-fidelity data sharing. However, adequately evaluating the quality of\ngenerated synthetic datasets is still an open challenge. We introduce and\ndemonstrate a holdout-based empirical assessment framework for quantifying the\nfidelity as well as the privacy risk of synthetic data solutions for mixed-type\ntabular data. Measuring fidelity is based on statistical distances of\nlower-dimensional marginal distributions, which provide a model-free and\neasy-to-communicate empirical metric for the representativeness of a synthetic\ndataset. Privacy risk is assessed by calculating the individual-level distances\nto closest record with respect to the training data. By showing that the\nsynthetic samples are just as close to the training as to the holdout data, we\nyield strong evidence that the synthesizer indeed learned to generalize\npatterns and is independent of individual training records. We demonstrate the\npresented framework for seven distinct synthetic data solutions across four\nmixed-type datasets and compare these to more traditional statistical\ndisclosure techniques. The results highlight the need to systematically assess\nthe fidelity just as well as the privacy of these emerging class of synthetic\ndata generators.\n", "versions": [{"version": "v1", "created": "Thu, 1 Apr 2021 17:30:23 GMT"}], "update_date": "2021-04-02", "authors_parsed": [["Platzer", "Michael", ""], ["Reutterer", "Thomas", ""]]}, {"id": "2104.00641", "submitter": "Youngser Park", "authors": "Jonathan Larson, Tiona Zuzul, Emily Cox Pahnke, Neha Parikh Shah,\n  Patrick Bourke, Nicholas Caurvina, Fereshteh Amini, Youngser Park, Joshua\n  Vogelstein, Jeffrey Weston, Christopher White, and Carey E. Priebe", "title": "Dynamic Silos: Modularity in intra-organizational communication networks\n  during the Covid-19 pandemic", "comments": "19 pages, 15 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Workplace communications around the world were drastically altered by\nCovid-19, work-from-home orders, and the rise of remote work. We analyze\naggregated, anonymized metadata from over 360 billion emails within over 4000\norganizations worldwide to examine changes in network community structures from\n2019 through 2020. We find that, during 2020, organizations around the world\nbecame more siloed, evidenced by increased modularity. This shift was\nconcurrent with decreased stability, indicating that organizational siloes had\nless stable membership. We provide initial insights into the implications of\nthese network changes -- which we term dynamic silos -- for organizational\nperformance and innovation.\n", "versions": [{"version": "v1", "created": "Thu, 1 Apr 2021 17:40:21 GMT"}, {"version": "v2", "created": "Wed, 7 Apr 2021 17:41:35 GMT"}, {"version": "v3", "created": "Sat, 1 May 2021 19:54:47 GMT"}], "update_date": "2021-05-04", "authors_parsed": [["Larson", "Jonathan", ""], ["Zuzul", "Tiona", ""], ["Pahnke", "Emily Cox", ""], ["Shah", "Neha Parikh", ""], ["Bourke", "Patrick", ""], ["Caurvina", "Nicholas", ""], ["Amini", "Fereshteh", ""], ["Park", "Youngser", ""], ["Vogelstein", "Joshua", ""], ["Weston", "Jeffrey", ""], ["White", "Christopher", ""], ["Priebe", "Carey E.", ""]]}, {"id": "2104.00673", "submitter": "Stephen Bates", "authors": "Stephen Bates and Trevor Hastie and Robert Tibshirani", "title": "Cross-validation: what does it estimate and how well does it do it?", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME math.ST stat.CO stat.ML stat.TH", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Cross-validation is a widely-used technique to estimate prediction error, but\nits behavior is complex and not fully understood. Ideally, one would like to\nthink that cross-validation estimates the prediction error for the model at\nhand, fit to the training data. We prove that this is not the case for the\nlinear model fit by ordinary least squares; rather it estimates the average\nprediction error of models fit on other unseen training sets drawn from the\nsame population. We further show that this phenomenon occurs for most popular\nestimates of prediction error, including data splitting, bootstrapping, and\nMallow's Cp. Next, the standard confidence intervals for prediction error\nderived from cross-validation may have coverage far below the desired level.\nBecause each data point is used for both training and testing, there are\ncorrelations among the measured accuracies for each fold, and so the usual\nestimate of variance is too small. We introduce a nested cross-validation\nscheme to estimate this variance more accurately, and show empirically that\nthis modification leads to intervals with approximately correct coverage in\nmany examples where traditional cross-validation intervals fail. Lastly, our\nanalysis also shows that when producing confidence intervals for prediction\naccuracy with simple data splitting, one should not re-fit the model on the\ncombined data, since this invalidates the confidence intervals.\n", "versions": [{"version": "v1", "created": "Thu, 1 Apr 2021 17:58:54 GMT"}, {"version": "v2", "created": "Wed, 14 Apr 2021 16:51:23 GMT"}], "update_date": "2021-04-15", "authors_parsed": [["Bates", "Stephen", ""], ["Hastie", "Trevor", ""], ["Tibshirani", "Robert", ""]]}, {"id": "2104.00732", "submitter": "Niko Br\\\"ummer", "authors": "Niko Br\\\"ummer and Luciana Ferrer and Albert Swart", "title": "Out of a hundred trials, how many errors does your speaker verifier\n  make?", "comments": "Submitted to Interspeech 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.SD cs.LG eess.AS stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Out of a hundred trials, how many errors does your speaker verifier make? For\nthe user this is an important, practical question, but researchers and vendors\ntypically sidestep it and supply instead the conditional error-rates that are\ngiven by the ROC/DET curve. We posit that the user's question is answered by\nthe Bayes error-rate. We present a tutorial to show how to compute the\nerror-rate that results when making Bayes decisions with calibrated likelihood\nratios, supplied by the verifier, and an hypothesis prior, supplied by the\nuser. For perfect calibration, the Bayes error-rate is upper bounded by\nmin(EER,P,1-P), where EER is the equal-error-rate and P, 1-P are the prior\nprobabilities of the competing hypotheses. The EER represents the accuracy of\nthe verifier, while min(P,1-P) represents the hardness of the classification\nproblem. We further show how the Bayes error-rate can be computed also for\nnon-perfect calibration and how to generalize from error-rate to expected cost.\nWe offer some criticism of decisions made by direct score thresholding.\nFinally, we demonstrate by analyzing error-rates of the recently published\nDCA-PLDA speaker verifier.\n", "versions": [{"version": "v1", "created": "Thu, 1 Apr 2021 19:10:48 GMT"}], "update_date": "2021-04-05", "authors_parsed": [["Br\u00fcmmer", "Niko", ""], ["Ferrer", "Luciana", ""], ["Swart", "Albert", ""]]}, {"id": "2104.00995", "submitter": "Arkopal Dutt", "authors": "Arkopal Dutt, Andrey Y. Lokhov, Marc Vuffray, Sidhant Misra", "title": "Exponential Reduction in Sample Complexity with Learning of Ising Model\n  Dynamics", "comments": "Accepted to ICML 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cond-mat.stat-mech physics.data-an stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The usual setting for learning the structure and parameters of a graphical\nmodel assumes the availability of independent samples produced from the\ncorresponding multivariate probability distribution. However, for many models\nthe mixing time of the respective Markov chain can be very large and i.i.d.\nsamples may not be obtained. We study the problem of reconstructing binary\ngraphical models from correlated samples produced by a dynamical process, which\nis natural in many applications. We analyze the sample complexity of two\nestimators that are based on the interaction screening objective and the\nconditional likelihood loss. We observe that for samples coming from a\ndynamical process far from equilibrium, the sample complexity reduces\nexponentially compared to a dynamical process that mixes quickly.\n", "versions": [{"version": "v1", "created": "Fri, 2 Apr 2021 11:44:13 GMT"}, {"version": "v2", "created": "Mon, 14 Jun 2021 20:22:33 GMT"}], "update_date": "2021-06-16", "authors_parsed": [["Dutt", "Arkopal", ""], ["Lokhov", "Andrey Y.", ""], ["Vuffray", "Marc", ""], ["Misra", "Sidhant", ""]]}, {"id": "2104.01061", "submitter": "Kumar Vijay Mishra", "authors": "Kumar Vijay Mishra and M. Ashok Kumar", "title": "Information Geometry and Classical Cram\\'{e}r-Rao Type Inequalities", "comments": "34 pages, 2 figures, 1 table. arXiv admin note: text overlap with\n  arXiv:2001.04769", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IT cs.LG eess.SP math.DG math.IT stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We examine the role of information geometry in the context of classical\nCram\\'er-Rao (CR) type inequalities. In particular, we focus on Eguchi's theory\nof obtaining dualistic geometric structures from a divergence function and then\napplying Amari-Nagoaka's theory to obtain a CR type inequality. The classical\ndeterministic CR inequality is derived from Kullback-Leibler (KL)-divergence.\nWe show that this framework could be generalized to other CR type inequalities\nthrough four examples: $\\alpha$-version of CR inequality, generalized CR\ninequality, Bayesian CR inequality, and Bayesian $\\alpha$-CR inequality. These\nare obtained from, respectively, $I_\\alpha$-divergence (or relative\n$\\alpha$-entropy), generalized Csisz\\'ar divergence, Bayesian KL divergence,\nand Bayesian $I_\\alpha$-divergence.\n", "versions": [{"version": "v1", "created": "Fri, 2 Apr 2021 14:21:49 GMT"}, {"version": "v2", "created": "Thu, 1 Jul 2021 11:58:09 GMT"}], "update_date": "2021-07-02", "authors_parsed": [["Mishra", "Kumar Vijay", ""], ["Kumar", "M. Ashok", ""]]}, {"id": "2104.01120", "submitter": "Anastasios Tsiamis", "authors": "Anastasios Tsiamis and George J. Pappas", "title": "Linear Systems can be Hard to Learn", "comments": "Under review", "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.SY cs.LG cs.SY math.OC stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we investigate when system identification is statistically\neasy or hard, in the finite sample regime. Statistically easy to learn linear\nsystem classes have sample complexity that is polynomial with the system\ndimension. Most prior research in the finite sample regime falls in this\ncategory, focusing on systems that are directly excited by process noise.\nStatistically hard to learn linear system classes have worst-case sample\ncomplexity that is at least exponential with the system dimension, regardless\nof the identification algorithm. Using tools from minimax theory, we show that\nclasses of linear systems can be hard to learn. Such classes include, for\nexample, under-actuated or under-excited systems with weak coupling among the\nstates. Having classified some systems as easy or hard to learn, a natural\nquestion arises as to what system properties fundamentally affect the hardness\nof system identifiability. Towards this direction, we characterize how the\ncontrollability index of linear systems affects the sample complexity of\nidentification. More specifically, we show that the sample complexity of\nrobustly controllable linear systems is upper bounded by an exponential\nfunction of the controllability index. This implies that identification is easy\nfor classes of linear systems with small controllability index and potentially\nhard if the controllability index is large. Our analysis is based on recent\nstatistical tools for finite sample analysis of system identification as well\nas a novel lower bound that relates controllability index with the least\nsingular value of the controllability Gramian.\n", "versions": [{"version": "v1", "created": "Fri, 2 Apr 2021 15:58:30 GMT"}], "update_date": "2021-04-05", "authors_parsed": [["Tsiamis", "Anastasios", ""], ["Pappas", "George J.", ""]]}, {"id": "2104.01148", "submitter": "Karl Stelzner", "authors": "Karl Stelzner, Kristian Kersting, Adam R. Kosiorek", "title": "Decomposing 3D Scenes into Objects via Unsupervised Volume Segmentation", "comments": "15 pages, 3 figures. For project page with videos, see\n  http://stelzner.github.io/obsurf/", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present ObSuRF, a method which turns a single image of a scene into a 3D\nmodel represented as a set of Neural Radiance Fields (NeRFs), with each NeRF\ncorresponding to a different object. A single forward pass of an encoder\nnetwork outputs a set of latent vectors describing the objects in the scene.\nThese vectors are used independently to condition a NeRF decoder, defining the\ngeometry and appearance of each object. We make learning more computationally\nefficient by deriving a novel loss, which allows training NeRFs on RGB-D inputs\nwithout explicit ray marching. After confirming that the model performs equal\nor better than state of the art on three 2D image segmentation benchmarks, we\napply it to two multi-object 3D datasets: A multiview version of CLEVR, and a\nnovel dataset in which scenes are populated by ShapeNet models. We find that\nafter training ObSuRF on RGB-D views of training scenes, it is capable of not\nonly recovering the 3D geometry of a scene depicted in a single input image,\nbut also to segment it into objects, despite receiving no supervision in that\nregard.\n", "versions": [{"version": "v1", "created": "Fri, 2 Apr 2021 16:59:29 GMT"}], "update_date": "2021-04-05", "authors_parsed": [["Stelzner", "Karl", ""], ["Kersting", "Kristian", ""], ["Kosiorek", "Adam R.", ""]]}, {"id": "2104.01177", "submitter": "Colin White", "authors": "Colin White, Arber Zela, Binxin Ru, Yang Liu, Frank Hutter", "title": "How Powerful are Performance Predictors in Neural Architecture Search?", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.NE stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Early methods in the rapidly developing field of neural architecture search\n(NAS) required fully training thousands of neural networks. To reduce this\nextreme computational cost, dozens of techniques have since been proposed to\npredict the final performance of neural architectures. Despite the success of\nsuch performance prediction methods, it is not well-understood how different\nfamilies of techniques compare to one another, due to the lack of an\nagreed-upon evaluation metric and optimization for different constraints on the\ninitialization time and query time. In this work, we give the first large-scale\nstudy of performance predictors by analyzing 31 techniques ranging from\nlearning curve extrapolation, to weight-sharing, to supervised learning, to\n\"zero-cost\" proxies. We test a number of correlation- and rank-based\nperformance measures in a variety of settings, as well as the ability of each\ntechnique to speed up predictor-based NAS frameworks. Our results act as\nrecommendations for the best predictors to use in different settings, and we\nshow that certain families of predictors can be combined to achieve even better\npredictive power, opening up promising research directions. Our code, featuring\na library of 31 performance predictors, is available at\nhttps://github.com/automl/naslib.\n", "versions": [{"version": "v1", "created": "Fri, 2 Apr 2021 17:57:16 GMT"}], "update_date": "2021-04-05", "authors_parsed": [["White", "Colin", ""], ["Zela", "Arber", ""], ["Ru", "Binxin", ""], ["Liu", "Yang", ""], ["Hutter", "Frank", ""]]}, {"id": "2104.01351", "submitter": "Insu Han", "authors": "Insu Han, Haim Avron, Neta Shoham, Chaewon Kim, Jinwoo Shin", "title": "Random Features for the Neural Tangent Kernel", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  The Neural Tangent Kernel (NTK) has discovered connections between deep\nneural networks and kernel methods with insights of optimization and\ngeneralization. Motivated by this, recent works report that NTK can achieve\nbetter performances compared to training neural networks on small-scale\ndatasets. However, results under large-scale settings are hardly studied due to\nthe computational limitation of kernel methods. In this work, we propose an\nefficient feature map construction of the NTK of fully-connected ReLU network\nwhich enables us to apply it to large-scale datasets. We combine random\nfeatures of the arc-cosine kernels with a sketching-based algorithm which can\nrun in linear with respect to both the number of data points and input\ndimension. We show that dimension of the resulting features is much smaller\nthan other baseline feature map constructions to achieve comparable error\nbounds both in theory and practice. We additionally utilize the leverage score\nbased sampling for improved bounds of arc-cosine random features and prove a\nspectral approximation guarantee of the proposed feature map to the NTK matrix\nof two-layer neural network. We benchmark a variety of machine learning tasks\nto demonstrate the superiority of the proposed scheme. In particular, our\nalgorithm can run tens of magnitude faster than the exact kernel methods for\nlarge-scale settings without performance loss.\n", "versions": [{"version": "v1", "created": "Sat, 3 Apr 2021 09:08:12 GMT"}], "update_date": "2021-04-06", "authors_parsed": [["Han", "Insu", ""], ["Avron", "Haim", ""], ["Shoham", "Neta", ""], ["Kim", "Chaewon", ""], ["Shin", "Jinwoo", ""]]}, {"id": "2104.01395", "submitter": "Omer Bobrowski", "authors": "Lior Aloni, Omer Bobrowski, Ronen Talmon", "title": "Joint Geometric and Topological Analysis of Hierarchical Datasets", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In a world abundant with diverse data arising from complex acquisition\ntechniques, there is a growing need for new data analysis methods. In this\npaper we focus on high-dimensional data that are organized into several\nhierarchical datasets. We assume that each dataset consists of complex samples,\nand every sample has a distinct irregular structure modeled by a graph. The\nmain novelty in this work lies in the combination of two complementing powerful\ndata-analytic approaches: topological data analysis (TDA) and geometric\nmanifold learning. Geometry primarily contains local information, while\ntopology inherently provides global descriptors. Based on this combination, we\npresent a method for building an informative representation of hierarchical\ndatasets. At the finer (sample) level, we devise a new metric between samples\nbased on manifold learning that facilitates quantitative structural analysis.\nAt the coarser (dataset) level, we employ TDA to extract qualitative structural\ninformation from the datasets. We showcase the applicability and advantages of\nour method on simulated data and on a corpus of hyper-spectral images. We show\nthat an ensemble of hyper-spectral images exhibits a hierarchical structure\nthat fits well the considered setting. In addition, we show that our new method\ngives rise to superior classification results compared to state-of-the-art\nmethods.\n", "versions": [{"version": "v1", "created": "Sat, 3 Apr 2021 13:02:00 GMT"}], "update_date": "2021-04-06", "authors_parsed": [["Aloni", "Lior", ""], ["Bobrowski", "Omer", ""], ["Talmon", "Ronen", ""]]}, {"id": "2104.01440", "submitter": "Rodrigo Rivera-Castro", "authors": "Vladislav Zhuzhel, Rodrigo Rivera-Castro, Nina Kaploukhaya, Liliya\n  Mironova, Alexey Zaytsev, Evgeny Burnaev", "title": "COHORTNEY: Non-Parametric Clustering of Event Sequences", "comments": "18 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Cohort analysis is a pervasive activity in web analytics. One divides users\ninto groups according to specific criteria and tracks their behavior over time.\nDespite its extensive use, academic circles do not discuss cohort analysis to\nevaluate user behavior online. This work introduces an unsupervised\nnon-parametric approach to group Internet users based on their activities. In\ncomparison, canonical methods in marketing and engineering-based techniques\nunderperform. COHORTNEY is the first machine learning-based cohort analysis\nalgorithm with a robust theoretical explanation.\n", "versions": [{"version": "v1", "created": "Sat, 3 Apr 2021 16:12:21 GMT"}, {"version": "v2", "created": "Sat, 12 Jun 2021 10:14:46 GMT"}], "update_date": "2021-06-15", "authors_parsed": [["Zhuzhel", "Vladislav", ""], ["Rivera-Castro", "Rodrigo", ""], ["Kaploukhaya", "Nina", ""], ["Mironova", "Liliya", ""], ["Zaytsev", "Alexey", ""], ["Burnaev", "Evgeny", ""]]}, {"id": "2104.01459", "submitter": "Namgil Lee", "authors": "Namgil Lee, Heejung Yang, Hojin Yoo", "title": "A surrogate loss function for optimization of $F_\\beta$ score in binary\n  classification with imbalanced data", "comments": "17 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  The $F_\\beta$ score is a commonly used measure of classification performance,\nwhich plays crucial roles in classification tasks with imbalanced data sets.\nHowever, the $F_\\beta$ score cannot be used as a loss function by\ngradient-based learning algorithms for optimizing neural network parameters due\nto its non-differentiability. On the other hand, commonly used loss functions\nsuch as the binary cross-entropy (BCE) loss are not directly related to\nperformance measures such as the $F_\\beta$ score, so that neural networks\noptimized by using the loss functions may not yield optimal performance\nmeasures. In this study, we investigate a relationship between classification\nperformance measures and loss functions in terms of the gradients with respect\nto the model parameters. Then, we propose a differentiable surrogate loss\nfunction for the optimization of the $F_\\beta$ score. We show that the gradient\npaths of the proposed surrogate $F_\\beta$ loss function approximate the\ngradient paths of the large sample limit of the $F_\\beta$ score. Through\nnumerical experiments using ResNets and benchmark image data sets, it is\ndemonstrated that the proposed surrogate $F_\\beta$ loss function is effective\nfor optimizing $F_\\beta$ scores under class imbalances in binary classification\ntasks compared with other loss functions.\n", "versions": [{"version": "v1", "created": "Sat, 3 Apr 2021 18:36:23 GMT"}], "update_date": "2021-04-06", "authors_parsed": [["Lee", "Namgil", ""], ["Yang", "Heejung", ""], ["Yoo", "Hojin", ""]]}, {"id": "2104.01493", "submitter": "Negin Majidi", "authors": "Negin Majidi, Ehsan Amid, Hossein Talebi, and Manfred K. Warmuth", "title": "Exponentiated Gradient Reweighting for Robust Training Under Label Noise\n  and Beyond", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Many learning tasks in machine learning can be viewed as taking a gradient\nstep towards minimizing the average loss of a batch of examples in each\ntraining iteration. When noise is prevalent in the data, this uniform treatment\nof examples can lead to overfitting to noisy examples with larger loss values\nand result in poor generalization. Inspired by the expert setting in on-line\nlearning, we present a flexible approach to learning from noisy examples.\nSpecifically, we treat each training example as an expert and maintain a\ndistribution over all examples. We alternate between updating the parameters of\nthe model using gradient descent and updating the example weights using the\nexponentiated gradient update. Unlike other related methods, our approach\nhandles a general class of loss functions and can be applied to a wide range of\nnoise types and applications. We show the efficacy of our approach for multiple\nlearning settings, namely noisy principal component analysis and a variety of\nnoisy classification problems.\n", "versions": [{"version": "v1", "created": "Sat, 3 Apr 2021 22:54:49 GMT"}], "update_date": "2021-04-06", "authors_parsed": [["Majidi", "Negin", ""], ["Amid", "Ehsan", ""], ["Talebi", "Hossein", ""], ["Warmuth", "Manfred K.", ""]]}, {"id": "2104.01512", "submitter": "Seyednami Niyakan", "authors": "Seyednami Niyakan, Ehsan Hajiramezanali, Shahin Boluki, Siamak Zamani\n  Dadaneh, Xiaoning Qian", "title": "SimCD: Simultaneous Clustering and Differential expression analysis for\n  single-cell transcriptomic data", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "q-bio.GN stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Single-Cell RNA sequencing (scRNA-seq) measurements have facilitated\ngenome-scale transcriptomic profiling of individual cells, with the hope of\ndeconvolving cellular dynamic changes in corresponding cell sub-populations to\nbetter understand molecular mechanisms of different development processes.\nSeveral scRNA-seq analysis methods have been proposed to first identify cell\nsub-populations by clustering and then separately perform differential\nexpression analysis to understand gene expression changes. Their corresponding\nstatistical models and inference algorithms are often designed disjointly. We\ndevelop a new method -- SimCD -- that explicitly models cell heterogeneity and\ndynamic differential changes in one unified hierarchical gamma-negative\nbinomial (hGNB) model, allowing simultaneous cell clustering and differential\nexpression analysis for scRNA-seq data. Our method naturally defines cell\nheterogeneity by dynamic expression changes, which is expected to help achieve\nbetter performances on the two tasks compared to the existing methods that\nperform them separately. In addition, SimCD better models dropout (zero\ninflation) in scRNA-seq data by both cell- and gene-level factors and obviates\nthe need for sophisticated pre-processing steps such as normalization, thanks\nto the direct modeling of scRNA-seq count data by the rigorous hGNB model with\nan efficient Gibbs sampling inference algorithm. Extensive comparisons with the\nstate-of-the-art methods on both simulated and real-world scRNA-seq count data\ndemonstrate the capability of SimCD to discover cell clusters and capture\ndynamic expression changes. Furthermore, SimCD helps identify several known\ngenes affected by food deprivation in hypothalamic neuron cell subtypes as well\nas some new potential markers, suggesting the capability of SimCD for\nbio-marker discovery.\n", "versions": [{"version": "v1", "created": "Sun, 4 Apr 2021 01:06:18 GMT"}], "update_date": "2021-04-06", "authors_parsed": [["Niyakan", "Seyednami", ""], ["Hajiramezanali", "Ehsan", ""], ["Boluki", "Shahin", ""], ["Dadaneh", "Siamak Zamani", ""], ["Qian", "Xiaoning", ""]]}, {"id": "2104.01525", "submitter": "Benyamin Ghojogh", "authors": "Benyamin Ghojogh, Ali Ghodsi, Fakhri Karray, Mark Crowley", "title": "Generative Locally Linear Embedding", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Locally Linear Embedding (LLE) is a nonlinear spectral dimensionality\nreduction and manifold learning method. It has two main steps which are linear\nreconstruction and linear embedding of points in the input space and embedding\nspace, respectively. In this work, we propose two novel generative versions of\nLLE, named Generative LLE (GLLE), whose linear reconstruction steps are\nstochastic rather than deterministic. GLLE assumes that every data point is\ncaused by its linear reconstruction weights as latent factors. The proposed\nGLLE algorithms can generate various LLE embeddings stochastically while all\nthe generated embeddings relate to the original LLE embedding. We propose two\nversions for stochastic linear reconstruction, one using expectation\nmaximization and another with direct sampling from a derived distribution by\noptimization. The proposed GLLE methods are closely related to and inspired by\nvariational inference, factor analysis, and probabilistic principal component\nanalysis. Our simulations show that the proposed GLLE methods work effectively\nin unfolding and generating submanifolds of data.\n", "versions": [{"version": "v1", "created": "Sun, 4 Apr 2021 02:59:39 GMT"}], "update_date": "2021-04-06", "authors_parsed": [["Ghojogh", "Benyamin", ""], ["Ghodsi", "Ali", ""], ["Karray", "Fakhri", ""], ["Crowley", "Mark", ""]]}, {"id": "2104.01634", "submitter": "Mohammad Mahdi Kamani", "authors": "Mohammad Mahdi Kamani, Rana Forsati, James Z. Wang, Mehrdad Mahdavi", "title": "Pareto Efficient Fairness in Supervised Learning: From Extraction to\n  Tracing", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI stat.ML", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  As algorithmic decision-making systems are becoming more pervasive, it is\ncrucial to ensure such systems do not become mechanisms of unfair\ndiscrimination on the basis of gender, race, ethnicity, religion, etc.\nMoreover, due to the inherent trade-off between fairness measures and accuracy,\nit is desirable to learn fairness-enhanced models without significantly\ncompromising the accuracy. In this paper, we propose Pareto efficient Fairness\n(PEF) as a suitable fairness notion for supervised learning, that can ensure\nthe optimal trade-off between overall loss and other fairness criteria. The\nproposed PEF notion is definition-agnostic, meaning that any well-defined\nnotion of fairness can be reduced to the PEF notion. To efficiently find a PEF\nclassifier, we cast the fairness-enhanced classification as a bilevel\noptimization problem and propose a gradient-based method that can guarantee the\nsolution belongs to the Pareto frontier with provable guarantees for convex and\nnon-convex objectives. We also generalize the proposed algorithmic solution to\nextract and trace arbitrary solutions from the Pareto frontier for a given\npreference over accuracy and fairness measures. This approach is generic and\ncan be generalized to any multicriteria optimization problem to trace points on\nthe Pareto frontier curve, which is interesting by its own right. We\nempirically demonstrate the effectiveness of the PEF solution and the extracted\nPareto frontier on real-world datasets compared to state-of-the-art methods.\n", "versions": [{"version": "v1", "created": "Sun, 4 Apr 2021 15:49:35 GMT"}], "update_date": "2021-04-06", "authors_parsed": [["Kamani", "Mohammad Mahdi", ""], ["Forsati", "Rana", ""], ["Wang", "James Z.", ""], ["Mahdavi", "Mehrdad", ""]]}, {"id": "2104.01648", "submitter": "Martin Molina-Fructuoso", "authors": "Martin Molina-Fructuoso and Ryan Murray", "title": "Tukey Depths and Hamilton-Jacobi Differential Equations", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST cs.LG math.AP stat.ML stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The widespread application of modern machine learning has increased the need\nfor robust statistical algorithms. This work studies one such fundamental\nstatistical measure known as the Tukey depth. We study the problem in the\ncontinuum (population) limit. In particular, we derive the associated necessary\nconditions, which take the form of a first-order partial differential equation.\nWe discuss the classical interpretation of this necessary condition as the\nviscosity solution of a Hamilton-Jacobi equation, but with a non-classical\nHamiltonian with discontinuous dependence on the gradient at zero. We prove\nthat this equation possesses a unique viscosity solution and that this solution\nalways bounds the Tukey depth from below. In certain cases, we prove that the\nTukey depth is equal to the viscosity solution, and we give some illustrations\nof standard numerical methods from the optimal control community which deal\ndirectly with the partial differential equation. We conclude by outlining\nseveral promising research directions both in terms of new numerical algorithms\nand theoretical challenges.\n", "versions": [{"version": "v1", "created": "Sun, 4 Apr 2021 17:13:50 GMT"}], "update_date": "2021-04-06", "authors_parsed": [["Molina-Fructuoso", "Martin", ""], ["Murray", "Ryan", ""]]}, {"id": "2104.01672", "submitter": "Anthea Monod", "authors": "Athanasios Vlontzos, Yueqi Cao, Luca Schmidtke, Bernhard Kainz, and\n  Anthea Monod", "title": "Topological Data Analysis of Database Representations for Information\n  Retrieval", "comments": "15 pages, 7 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG math.AT", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Appropriately representing elements in a database so that queries may be\naccurately matched is a central task in information retrieval. This recently\nhas been achieved by embedding the graphical structure of the database into a\nmanifold so that the hierarchy is preserved. Persistent homology provides a\nrigorous characterization for the database topology in terms of both its\nhierarchy and connectivity structure. We compute persistent homology on a\nvariety of datasets and show that some commonly used embeddings fail to\npreserve the connectivity. Moreover, we show that embeddings which successfully\nretain the database topology coincide in persistent homology. We introduce the\ndilation-invariant bottleneck distance to capture this effect, which addresses\nmetric distortion on manifolds. We use it to show that distances between\ntopology-preserving embeddings of databases are small.\n", "versions": [{"version": "v1", "created": "Sun, 4 Apr 2021 19:29:47 GMT"}], "update_date": "2021-04-06", "authors_parsed": [["Vlontzos", "Athanasios", ""], ["Cao", "Yueqi", ""], ["Schmidtke", "Luca", ""], ["Kainz", "Bernhard", ""], ["Monod", "Anthea", ""]]}, {"id": "2104.01708", "submitter": "Stephen Zhang", "authors": "Stephen Y. Zhang", "title": "A unified framework for non-negative matrix and tensor factorisations\n  with a smoothed Wasserstein loss", "comments": "14 pages, 6 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG math.OC", "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/", "abstract": "  Non-negative matrix and tensor factorisations are a classical tool for\nfinding low-dimensional representations of high-dimensional datasets. In\napplications such as imaging, datasets can be regarded as distributions\nsupported on a space with metric structure. In such a setting, a loss function\nbased on the Wasserstein distance of optimal transportation theory is a natural\nchoice since it incorporates the underlying geometry of the data. We introduce\na general mathematical framework for computing non-negative factorisations of\nboth matrices and tensors with respect to an optimal transport loss. We derive\nan efficient computational method for its solution using a convex dual\nformulation, and demonstrate the applicability of this approach with several\nnumerical illustrations with both matrix and tensor-valued data.\n", "versions": [{"version": "v1", "created": "Sun, 4 Apr 2021 22:42:21 GMT"}, {"version": "v2", "created": "Thu, 15 Jul 2021 03:32:32 GMT"}], "update_date": "2021-07-16", "authors_parsed": [["Zhang", "Stephen Y.", ""]]}, {"id": "2104.01750", "submitter": "Shaojie Tang", "authors": "Shaojie Tang, Jing Yuan", "title": "Optimal Sampling Gaps for Adaptive Submodular Maximization", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG math.OC stat.ML", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Running machine learning algorithms on large and rapidly growing volumes of\ndata are often computationally expensive, one common trick to reduce the size\nof a data set, and thus reduce the computational cost of machine learning\nalgorithms, is \\emph{probability sampling}. It creates a sampled data set by\nincluding each data point from the original data set with a known probability.\nAlthough the benefit of running machine learning algorithms on the reduced data\nset is obvious, one major concern is that the performance of the solution\nobtained from samples might be much worse than that of the optimal solution\nwhen using the full data set. In this paper, we examine the performance loss\ncaused by probability sampling in the context of adaptive submodular\nmaximization. We consider a easiest probability sampling method which selects\neach data point independently with probability $r\\in[0,1]$. We define sampling\ngap as the largest ratio of the optimal solution obtained from the full data\nset and the optimal solution obtained from the samples, over independence\nsystems. Our main contribution is to show that if the utility function is\npolicywise submodular, then for a given sampling rate $r$, the sampling gap is\nboth upper bounded and lower bounded by $1/r$. One immediate implication of our\nresult is that if we can find an $\\alpha$-approximation solution based on a\nsampled data set (which is sampled at sampling rate $r$), then this solution\nachieves an $\\alpha r$ approximation ratio for the original problem when using\nthe full data set. We also show that the property of policywise submodular can\nbe found in a wide range of real-world applications, including pool-based\nactive learning and adaptive viral marketing.\n", "versions": [{"version": "v1", "created": "Mon, 5 Apr 2021 03:21:32 GMT"}, {"version": "v2", "created": "Tue, 13 Apr 2021 02:34:04 GMT"}], "update_date": "2021-04-14", "authors_parsed": [["Tang", "Shaojie", ""], ["Yuan", "Jing", ""]]}, {"id": "2104.01830", "submitter": "Vitor Cerqueira", "authors": "Vitor Cerqueira, Luis Torgo, Carlos Soares, Albert Bifet", "title": "Model Compression for Dynamic Forecast Combination", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  The predictive advantage of combining several different predictive models is\nwidely accepted. Particularly in time series forecasting problems, this\ncombination is often dynamic to cope with potential non-stationary sources of\nvariation present in the data. Despite their superior predictive performance,\nensemble methods entail two main limitations: high computational costs and lack\nof transparency. These issues often preclude the deployment of such approaches,\nin favour of simpler yet more efficient and reliable ones. In this paper, we\nleverage the idea of model compression to address this problem in time series\nforecasting tasks. Model compression approaches have been mostly unexplored for\nforecasting. Their application in time series is challenging due to the\nevolving nature of the data. Further, while the literature focuses on neural\nnetworks, we apply model compression to distinct types of methods. In an\nextensive set of experiments, we show that compressing dynamic forecasting\nensembles into an individual model leads to a comparable predictive performance\nand a drastic reduction in computational costs. Further, the compressed\nindividual model with best average rank is a rule-based regression model. Thus,\nmodel compression also leads to benefits in terms of model interpretability.\nThe experiments carried in this paper are fully reproducible.\n", "versions": [{"version": "v1", "created": "Mon, 5 Apr 2021 09:55:35 GMT"}], "update_date": "2021-04-06", "authors_parsed": [["Cerqueira", "Vitor", ""], ["Torgo", "Luis", ""], ["Soares", "Carlos", ""], ["Bifet", "Albert", ""]]}, {"id": "2104.01836", "submitter": "Hideaki Ishibashi Ph.D", "authors": "Hideaki Ishibashi and Hideitsu Hino", "title": "Stopping Criterion for Active Learning Based on Error Stability", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Active learning is a framework for supervised learning to improve the\npredictive performance by adaptively annotating a small number of samples. To\nrealize efficient active learning, both an acquisition function that determines\nthe next datum and a stopping criterion that determines when to stop learning\nshould be considered. In this study, we propose a stopping criterion based on\nerror stability, which guarantees that the change in generalization error upon\nadding a new sample is bounded by the annotation cost and can be applied to any\nBayesian active learning. We demonstrate that the proposed criterion stops\nactive learning at the appropriate timing for various learning models and real\ndatasets.\n", "versions": [{"version": "v1", "created": "Mon, 5 Apr 2021 10:15:50 GMT"}, {"version": "v2", "created": "Fri, 9 Apr 2021 01:20:21 GMT"}], "update_date": "2021-04-12", "authors_parsed": [["Ishibashi", "Hideaki", ""], ["Hino", "Hideitsu", ""]]}, {"id": "2104.01883", "submitter": "Alex Dytso", "authors": "Alex Dytso, H. Vincent Poor, Shlomo Shamai (Shitz)", "title": "A General Derivative Identity for the Conditional Mean Estimator in\n  Gaussian Noise and Some Applications", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IT math.IT math.ST stat.ML stat.TH", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Consider a channel ${\\bf Y}={\\bf X}+ {\\bf N}$ where ${\\bf X}$ is an\n$n$-dimensional random vector, and ${\\bf N}$ is a Gaussian vector with a\ncovariance matrix ${\\bf \\mathsf{K}}_{\\bf N}$. The object under consideration in\nthis paper is the conditional mean of ${\\bf X}$ given ${\\bf Y}={\\bf y}$, that\nis ${\\bf y} \\to E[{\\bf X}|{\\bf Y}={\\bf y}]$. Several identities in the\nliterature connect $E[{\\bf X}|{\\bf Y}={\\bf y}]$ to other quantities such as the\nconditional variance, score functions, and higher-order conditional moments.\nThe objective of this paper is to provide a unifying view of these identities.\n  In the first part of the paper, a general derivative identity for the\nconditional mean is derived. Specifically, for the Markov chain ${\\bf U}\n\\leftrightarrow {\\bf X} \\leftrightarrow {\\bf Y}$, it is shown that the Jacobian\nof $E[{\\bf U}|{\\bf Y}={\\bf y}]$ is given by ${\\bf \\mathsf{K}}_{{\\bf N}}^{-1}\n{\\bf Cov} ( {\\bf X}, {\\bf U} | {\\bf Y}={\\bf y})$.\n  In the second part of the paper, via various choices of ${\\bf U}$, the new\nidentity is used to generalize many of the known identities and derive some new\nones. First, a simple proof of the Hatsel and Nolte identity for the\nconditional variance is shown. Second, a simple proof of the recursive identity\ndue to Jaffer is provided. Third, a new connection between the conditional\ncumulants and the conditional expectation is shown. In particular, it is shown\nthat the $k$-th derivative of $E[X|Y=y]$ is the $(k+1)$-th conditional\ncumulant.\n  The third part of the paper considers some applications. In a first\napplication, the power series and the compositional inverse of $E[X|Y=y]$ are\nderived. In a second application, the distribution of the estimator error\n$(X-E[X|Y])$ is derived. In a third application, we construct consistent\nestimators (empirical Bayes estimators) of the conditional cumulants from an\ni.i.d. sequence $Y_1,...,Y_n$.\n", "versions": [{"version": "v1", "created": "Mon, 5 Apr 2021 12:48:28 GMT"}], "update_date": "2021-04-06", "authors_parsed": [["Dytso", "Alex", "", "Shitz"], ["Poor", "H. Vincent", "", "Shitz"], ["Shamai", "Shlomo", "", "Shitz"]]}, {"id": "2104.01980", "submitter": "Fahad Alhasoun", "authors": "Fahad Alhasoun, Sarah Alnegheimish, Joshua Tenenbaum", "title": "Probabilistic Programming Bots in Intuitive Physics Game Play", "comments": "Shorter version to appear in AAAI-21", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI stat.ML", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Recent findings suggest that humans deploy cognitive mechanism of physics\nsimulation engines to simulate the physics of objects. We propose a framework\nfor bots to deploy probabilistic programming tools for interacting with\nintuitive physics environments. The framework employs a physics simulation in a\nprobabilistic way to infer about moves performed by an agent in a setting\ngoverned by Newtonian laws of motion. However, methods of probabilistic\nprograms can be slow in such setting due to their need to generate many\nsamples. We complement the model with a model-free approach to aid the sampling\nprocedures in becoming more efficient through learning from experience during\ngame playing. We present an approach where combining model-free approaches (a\nconvolutional neural network in our model) and model-based approaches\n(probabilistic physics simulation) is able to achieve what neither could alone.\nThis way the model outperforms an all model-free or all model-based approach.\nWe discuss a case study showing empirical results of the performance of the\nmodel on the game of Flappy Bird.\n", "versions": [{"version": "v1", "created": "Mon, 5 Apr 2021 16:14:41 GMT"}], "update_date": "2021-04-06", "authors_parsed": [["Alhasoun", "Fahad", ""], ["Alnegheimish", "Sarah", ""], ["Tenenbaum", "Joshua", ""]]}, {"id": "2104.01987", "submitter": "Weijie J. Su", "authors": "Jinshuo Dong, Aaron Roth, Weijie J. Su", "title": "Rejoinder: Gaussian Differential Privacy", "comments": "Updated the references. Rejoinder to discussions on Gaussian\n  Differential Privacy, read to the Royal Statistical Society in December 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CR cs.LG math.ST stat.ML stat.TH", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  In this rejoinder, we aim to address two broad issues that cover most\ncomments made in the discussion. First, we discuss some theoretical aspects of\nour work and comment on how this work might impact the theoretical foundation\nof privacy-preserving data analysis. Taking a practical viewpoint, we next\ndiscuss how f-differential privacy (f-DP) and Gaussian differential privacy\n(GDP) can make a difference in a range of applications.\n", "versions": [{"version": "v1", "created": "Mon, 5 Apr 2021 16:27:56 GMT"}, {"version": "v2", "created": "Sat, 26 Jun 2021 02:40:17 GMT"}], "update_date": "2021-06-29", "authors_parsed": [["Dong", "Jinshuo", ""], ["Roth", "Aaron", ""], ["Su", "Weijie J.", ""]]}, {"id": "2104.02056", "submitter": "Yizheng Liao", "authors": "Yizheng Liao, Yang Weng, Chin-woo Tan, Ram Rajagopal", "title": "Quick Line Outage Identification in Urban Distribution Grids via Smart\n  Meters", "comments": "12 pages, 12 figures. arXiv admin note: substantial text overlap with\n  arXiv:1811.05646", "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.SP cs.LG cs.SY eess.SY stat.AP stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The growing integration of distributed energy resources (DERs) in\ndistribution grids raises various reliability issues due to DER's uncertain and\ncomplex behaviors. With a large-scale DER penetration in distribution grids,\ntraditional outage detection methods, which rely on customers report and smart\nmeters' last gasp signals, will have poor performance, because the renewable\ngenerators and storages and the mesh structure in urban distribution grids can\ncontinue supplying power after line outages. To address these challenges, we\npropose a data-driven outage monitoring approach based on the stochastic time\nseries analysis with a theoretical guarantee. Specifically, we prove via power\nflow analysis that the dependency of time-series voltage measurements exhibits\nsignificant statistical changes after line outages. This makes the theory on\noptimal change-point detection suitable to identify line outages. However,\nexisting change point detection methods require post-outage voltage\ndistribution, which is unknown in distribution systems. Therefore, we design a\nmaximum likelihood estimator to directly learn the distribution parameters from\nvoltage data. We prove that the estimated parameters-based detection also\nachieves the optimal performance, making it extremely useful for fast\ndistribution grid outage identifications. Furthermore, since smart meters have\nbeen widely installed in distribution grids and advanced infrastructure (e.g.,\nPMU) has not widely been available, our approach only requires voltage\nmagnitude for quick outage identification. Simulation results show highly\naccurate outage identification in eight distribution grids with 14\nconfigurations with and without DERs using smart meter data.\n", "versions": [{"version": "v1", "created": "Thu, 1 Apr 2021 07:10:34 GMT"}], "update_date": "2021-04-06", "authors_parsed": [["Liao", "Yizheng", ""], ["Weng", "Yang", ""], ["Tan", "Chin-woo", ""], ["Rajagopal", "Ram", ""]]}, {"id": "2104.02092", "submitter": "Pablo Mart\\'in-Ramiro", "authors": "Jack H. Collins, Pablo Mart\\'in-Ramiro, Benjamin Nachman, David Shih", "title": "Comparing Weak- and Unsupervised Methods for Resonant Anomaly Detection", "comments": "39 pages, 17 figures", "journal-ref": null, "doi": "10.1140/epjc/s10052-021-09389-x", "report-no": null, "categories": "hep-ph hep-ex physics.data-an stat.ML", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Anomaly detection techniques are growing in importance at the Large Hadron\nCollider (LHC), motivated by the increasing need to search for new physics in a\nmodel-agnostic way. In this work, we provide a detailed comparative study\nbetween a well-studied unsupervised method called the autoencoder (AE) and a\nweakly-supervised approach based on the Classification Without Labels (CWoLa)\ntechnique. We examine the ability of the two methods to identify a new physics\nsignal at different cross sections in a fully hadronic resonance search. By\nconstruction, the AE classification performance is independent of the amount of\ninjected signal. In contrast, the CWoLa performance improves with increasing\nsignal abundance. When integrating these approaches with a complete background\nestimate, we find that the two methods have complementary sensitivity. In\nparticular, CWoLa is effective at finding diverse and moderately rare signals\nwhile the AE can provide sensitivity to very rare signals, but only with\ncertain topologies. We therefore demonstrate that both techniques are\ncomplementary and can be used together for anomaly detection at the LHC.\n", "versions": [{"version": "v1", "created": "Mon, 5 Apr 2021 18:00:46 GMT"}], "update_date": "2021-07-28", "authors_parsed": [["Collins", "Jack H.", ""], ["Mart\u00edn-Ramiro", "Pablo", ""], ["Nachman", "Benjamin", ""], ["Shih", "David", ""]]}, {"id": "2104.02095", "submitter": "Aleksandr Beknazaryan", "authors": "Aleksandr Beknazaryan", "title": "Analytic function approximation by path norm regularized deep networks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  We provide an entropy bound for the spaces of path norm regularized neural\nnetworks with piecewise linear activation functions, such as the ReLU and the\nabsolute value functions. This bound generalizes the known entropy bound for\nthe spaces of linear functions on $\\mathbb{R}^d$. Keeping the path norm\ntogether with the depth, width and the weights of networks to have logarithmic\ndependence on $1/\\varepsilon$, we $\\varepsilon$-approximate functions that are\nanalytic on certain regions of $\\mathbb{C}^d$.\n", "versions": [{"version": "v1", "created": "Mon, 5 Apr 2021 18:02:04 GMT"}, {"version": "v2", "created": "Thu, 3 Jun 2021 13:01:53 GMT"}, {"version": "v3", "created": "Tue, 15 Jun 2021 10:54:53 GMT"}], "update_date": "2021-06-16", "authors_parsed": [["Beknazaryan", "Aleksandr", ""]]}, {"id": "2104.02120", "submitter": "Felix Xiaofeng Ye", "authors": "Felix X.-F. Ye, Sichen Yang, Mauro Maggioni", "title": "Nonlinear model reduction for slow-fast stochastic systems near\n  manifolds", "comments": "45 pages, 9 figures, 5 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG math.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We introduce a nonlinear stochastic model reduction technique for\nhigh-dimensional stochastic dynamical systems that have a low-dimensional\ninvariant effective manifold with slow dynamics, and high-dimensional, large\nfast modes. Given only access to a black box simulator from which short bursts\nof simulation can be obtained, we estimate the invariant manifold, a process of\nthe effective (stochastic) dynamics on it, and construct an efficient simulator\nthereof. These estimation steps can be performed on-the-fly, leading to\nefficient exploration of the effective state space, without losing consistency\nwith the underlying dynamics. This construction enables fast and efficient\nsimulation of paths of the effective dynamics, together with estimation of\ncrucial features and observables of such dynamics, including the stationary\ndistribution, identification of metastable states, and residence times and\ntransition rates between them.\n", "versions": [{"version": "v1", "created": "Mon, 5 Apr 2021 19:29:46 GMT"}], "update_date": "2021-04-07", "authors_parsed": [["Ye", "Felix X. -F.", ""], ["Yang", "Sichen", ""], ["Maggioni", "Mauro", ""]]}, {"id": "2104.02125", "submitter": "Quan Wang", "authors": "Roza Chojnacka, Jason Pelecanos, Quan Wang, Ignacio Lopez Moreno", "title": "SpeakerStew: Scaling to Many Languages with a Triaged Multilingual\n  Text-Dependent and Text-Independent Speaker Verification System", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.AS cs.LG cs.SD stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we describe SpeakerStew - a hybrid system to perform speaker\nverification on 46 languages. Two core ideas were explored in this system: (1)\nPooling training data of different languages together for multilingual\ngeneralization and reducing development cycles; (2) A novel triage mechanism\nbetween text-dependent and text-independent models to reduce runtime cost and\nexpected latency. To the best of our knowledge, this is the first study of\nspeaker verification systems at the scale of 46 languages. The problem is\nframed from the perspective of using a smart speaker device with interactions\nconsisting of a wake-up keyword (text-dependent) followed by a speech query\n(text-independent). Experimental evidence suggests that training on multiple\nlanguages can generalize to unseen varieties while maintaining performance on\nseen varieties. We also found that it can reduce computational requirements for\ntraining models by an order of magnitude. Furthermore, during model inference\non English data, we observe that leveraging a triage framework can reduce the\nnumber of calls to the more computationally expensive text-independent system\nby 73% (and reduce latency by 59%) while maintaining an EER no worse than the\ntext-independent setup.\n", "versions": [{"version": "v1", "created": "Mon, 5 Apr 2021 19:48:16 GMT"}, {"version": "v2", "created": "Mon, 26 Apr 2021 17:56:24 GMT"}, {"version": "v3", "created": "Tue, 15 Jun 2021 19:39:03 GMT"}], "update_date": "2021-06-17", "authors_parsed": [["Chojnacka", "Roza", ""], ["Pelecanos", "Jason", ""], ["Wang", "Quan", ""], ["Moreno", "Ignacio Lopez", ""]]}, {"id": "2104.02143", "submitter": "Chenchen Ma", "authors": "Chenchen Ma and Gongjun Xu", "title": "Learning Latent and Hierarchical Structures in Cognitive Diagnosis\n  Models", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Cognitive Diagnosis Models (CDMs) are a special family of discrete latent\nvariable models that are widely used in modern educational, psychological,\nsocial and biological sciences. A key component of CDMs is a binary $Q$-matrix\ncharacterizing the dependence structure between the items and the latent\nattributes. Additionally, researchers also assume in many applications certain\nhierarchical structures among the latent attributes to characterize their\ndependence. In most CDM applications, the attribute-attribute hierarchical\nstructures, the item-attribute $Q$-matrix, the item-level diagnostic model, as\nwell as the number of latent attributes, need to be fully or partially\npre-specified, which however may be subjective and misspecified as noted by\nmany recent studies. This paper considers the problem of jointly learning these\nlatent and hierarchical structures in CDMs from observed data with minimal\nmodel assumptions. Specifically, a penalized likelihood approach is proposed to\nselect the number of attributes and estimate the latent and hierarchical\nstructures simultaneously. An efficient expectation-maximization (EM) algorithm\nand a latent structure recovery algorithm are developed, and statistical\nconsistency theory is also established under mild conditions. The good\nperformance of the proposed method is illustrated by simulation studies and a\nreal data application in educational assessment.\n", "versions": [{"version": "v1", "created": "Mon, 5 Apr 2021 20:33:02 GMT"}], "update_date": "2021-04-07", "authors_parsed": [["Ma", "Chenchen", ""], ["Xu", "Gongjun", ""]]}, {"id": "2104.02150", "submitter": "Alexander D'Amour", "authors": "Alexander D'Amour", "title": "Revisiting Rashomon: A Comment on \"The Two Cultures\"", "comments": "Commentary to appear in a special issue of Observational Studies,\n  discussing Leo Breiman's paper \"Statistical Modeling: The Two Cultures\"\n  (https://doi.org/10.1214/ss/1009213726) and accompanying commentary", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Here, I provide some reflections on Prof. Leo Breiman's \"The Two Cultures\"\npaper. I focus specifically on the phenomenon that Breiman dubbed the \"Rashomon\nEffect\", describing the situation in which there are many models that satisfy\npredictive accuracy criteria equally well, but process information in the data\nin substantially different ways. This phenomenon can make it difficult to draw\nconclusions or automate decisions based on a model fit to data. I make\nconnections to recent work in the Machine Learning literature that explore the\nimplications of this issue, and note that grappling with it can be a fruitful\narea of collaboration between the algorithmic and data modeling cultures.\n", "versions": [{"version": "v1", "created": "Mon, 5 Apr 2021 20:51:58 GMT"}], "update_date": "2021-04-07", "authors_parsed": [["D'Amour", "Alexander", ""]]}, {"id": "2104.02189", "submitter": "Payam Delgosha", "authors": "Payam Delgosha, Hamed Hassani, Ramtin Pedarsani", "title": "Robust Classification Under $\\ell_0$ Attack for the Gaussian Mixture\n  Model", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  It is well-known that machine learning models are vulnerable to small but\ncleverly-designed adversarial perturbations that can cause misclassification.\nWhile there has been major progress in designing attacks and defenses for\nvarious adversarial settings, many fundamental and theoretical problems are yet\nto be resolved. In this paper, we consider classification in the presence of\n$\\ell_0$-bounded adversarial perturbations, a.k.a. sparse attacks. This setting\nis significantly different from other $\\ell_p$-adversarial settings, with\n$p\\geq 1$, as the $\\ell_0$-ball is non-convex and highly non-smooth. Under the\nassumption that data is distributed according to the Gaussian mixture model,\nour goal is to characterize the optimal robust classifier and the corresponding\nrobust classification error as well as a variety of trade-offs between\nrobustness, accuracy, and the adversary's budget. To this end, we develop a\nnovel classification algorithm called FilTrun that has two main modules:\nFiltration and Truncation. The key idea of our method is to first filter out\nthe non-robust coordinates of the input and then apply a carefully-designed\ntruncated inner product for classification. By analyzing the performance of\nFilTrun, we derive an upper bound on the optimal robust classification error.\nWe also find a lower bound by designing a specific adversarial strategy that\nenables us to derive the corresponding robust classifier and its achieved\nerror. For the case that the covariance matrix of the Gaussian mixtures is\ndiagonal, we show that as the input's dimension gets large, the upper and lower\nbounds converge; i.e. we characterize the asymptotically-optimal robust\nclassifier. Throughout, we discuss several examples that illustrate interesting\nbehaviors such as the existence of a phase transition for adversary's budget\ndetermining whether the effect of adversarial perturbation can be fully\nneutralized.\n", "versions": [{"version": "v1", "created": "Mon, 5 Apr 2021 23:31:25 GMT"}], "update_date": "2021-04-07", "authors_parsed": [["Delgosha", "Payam", ""], ["Hassani", "Hamed", ""], ["Pedarsani", "Ramtin", ""]]}, {"id": "2104.02240", "submitter": "Nengfeng Zhou", "authors": "Lian Yu, Nengfeng Zhou", "title": "Survey of Imbalanced Data Methodologies", "comments": "7 pages, 4 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Imbalanced data set is a problem often found and well-studied in financial\nindustry. In this paper, we reviewed and compared some popular methodologies\nhandling data imbalance. We then applied the under-sampling/over-sampling\nmethodologies to several modeling algorithms on UCI and Keel data sets. The\nperformance was analyzed for class-imbalance methods, modeling algorithms and\ngrid search criteria comparison.\n", "versions": [{"version": "v1", "created": "Tue, 6 Apr 2021 02:10:22 GMT"}], "update_date": "2021-04-07", "authors_parsed": [["Yu", "Lian", ""], ["Zhou", "Nengfeng", ""]]}, {"id": "2104.02261", "submitter": "Sanjay Kariyappa", "authors": "Sanjay Kariyappa, Ousmane Dia and Moinuddin K Qureshi", "title": "Enabling Inference Privacy with Adaptive Noise Injection", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CR cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  User-facing software services are becoming increasingly reliant on remote\nservers to host Deep Neural Network (DNN) models, which perform inference tasks\nfor the clients. Such services require the client to send input data to the\nservice provider, who processes it using a DNN and returns the output\npredictions to the client. Due to the rich nature of the inputs such as images\nand speech, the input often contains more information than what is necessary to\nperform the primary inference task. Consequently, in addition to the primary\ninference task, a malicious service provider could infer secondary (sensitive)\nattributes from the input, compromising the client's privacy. The goal of our\nwork is to improve inference privacy by injecting noise to the input to hide\nthe irrelevant features that are not conducive to the primary classification\ntask. To this end, we propose Adaptive Noise Injection (ANI), which uses a\nlight-weight DNN on the client-side to inject noise to each input, before\ntransmitting it to the service provider to perform inference. Our key insight\nis that by customizing the noise to each input, we can achieve state-of-the-art\ntrade-off between utility and privacy (up to 48.5% degradation in\nsensitive-task accuracy with <1% degradation in primary accuracy),\nsignificantly outperforming existing noise injection schemes. Our method does\nnot require prior knowledge of the sensitive attributes and incurs minimal\ncomputational overheads.\n", "versions": [{"version": "v1", "created": "Tue, 6 Apr 2021 03:06:21 GMT"}], "update_date": "2021-04-07", "authors_parsed": [["Kariyappa", "Sanjay", ""], ["Dia", "Ousmane", ""], ["Qureshi", "Moinuddin K", ""]]}, {"id": "2104.02334", "submitter": "Abed AlRahman Al Makdah", "authors": "Abed AlRahman Al Makdah and Vaibhav Katewa and Fabio Pasqualetti", "title": "Taming Adversarial Robustness via Abstaining", "comments": "Submitted to CDC 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.SY eess.SY stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this work, we consider a binary classification problem and cast it into a\nbinary hypothesis testing framework, where the observations can be perturbed by\nan adversary. To improve the adversarial robustness of a classifier, we include\nan abstaining option, where the classifier abstains from taking a decision when\nit has low confidence about the prediction. We propose metrics to quantify the\nnominal performance of a classifier with abstaining option and its robustness\nagainst adversarial perturbations. We show that there exist a tradeoff between\nthe two metrics regardless of what method is used to choose the abstaining\nregion. Our results imply that the robustness of a classifier with abstaining\ncan only be improved at the expense of its nominal performance. Further, we\nprovide necessary conditions to design the abstaining region for a\n1-dimensional binary classification problem. We validate our theoretical\nresults on the MNIST dataset, where we numerically show that the tradeoff\nbetween performance and robustness also exist for the general multi-class\nclassification problems.\n", "versions": [{"version": "v1", "created": "Tue, 6 Apr 2021 07:36:48 GMT"}], "update_date": "2021-04-07", "authors_parsed": [["Makdah", "Abed AlRahman Al", ""], ["Katewa", "Vaibhav", ""], ["Pasqualetti", "Fabio", ""]]}, {"id": "2104.02373", "submitter": "Joachim Schreurs", "authors": "Joachim Schreurs, Hannes De Meulemeester, Micha\\\"el Fanuel, Bart De\n  Moor and Johan A.K. Suykens", "title": "Leverage Score Sampling for Complete Mode Coverage in Generative\n  Adversarial Networks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Commonly, machine learning models minimize an empirical expectation. As a\nresult, the trained models typically perform well for the majority of the data\nbut the performance may deteriorate in less dense regions of the dataset. This\nissue also arises in generative modeling. A generative model may overlook\nunderrepresented modes that are less frequent in the empirical data\ndistribution. This problem is known as complete mode coverage. We propose a\nsampling procedure based on ridge leverage scores which significantly improves\nmode coverage when compared to standard methods and can easily be combined with\nany GAN. Ridge leverage scores are computed by using an explicit feature map,\nassociated with the next-to-last layer of a GAN discriminator or of a\npre-trained network, or by using an implicit feature map corresponding to a\nGaussian kernel. Multiple evaluations against recent approaches of complete\nmode coverage show a clear improvement when using the proposed sampling\nstrategy.\n", "versions": [{"version": "v1", "created": "Tue, 6 Apr 2021 09:00:38 GMT"}, {"version": "v2", "created": "Tue, 27 Apr 2021 14:38:50 GMT"}, {"version": "v3", "created": "Wed, 21 Jul 2021 07:51:18 GMT"}], "update_date": "2021-07-22", "authors_parsed": [["Schreurs", "Joachim", ""], ["De Meulemeester", "Hannes", ""], ["Fanuel", "Micha\u00ebl", ""], ["De Moor", "Bart", ""], ["Suykens", "Johan A. K.", ""]]}, {"id": "2104.02468", "submitter": "Sanghoon Myung", "authors": "Sanghoon Myung, Hyunjae Jang, Byungseon Choi, Jisu Ryu, Hyuk Kim, Sang\n  Wuk Park, Changwook Jeong and Dae Sin Kim", "title": "A Novel Approach for Semiconductor Etching Process with Inductive Biases", "comments": "5 pages; accepted to NeurIPS 2020 Workshop on Interpretable Inductive\n  Biases and Physically Structured Learning", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.AI cs.LG physics.comp-ph physics.plasm-ph", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The etching process is one of the most important processes in semiconductor\nmanufacturing. We have introduced the state-of-the-art deep learning model to\npredict the etching profiles. However, the significant problems violating\nphysics have been found through various techniques such as explainable\nartificial intelligence and representation of prediction uncertainty. To\naddress this problem, this paper presents a novel approach to apply the\ninductive biases for etching process. We demonstrate that our approach fits the\nmeasurement faster than physical simulator while following the physical\nbehavior. Our approach would bring a new opportunity for better etching process\nwith higher accuracy and lower cost.\n", "versions": [{"version": "v1", "created": "Tue, 6 Apr 2021 12:51:52 GMT"}], "update_date": "2021-04-07", "authors_parsed": [["Myung", "Sanghoon", ""], ["Jang", "Hyunjae", ""], ["Choi", "Byungseon", ""], ["Ryu", "Jisu", ""], ["Kim", "Hyuk", ""], ["Park", "Sang Wuk", ""], ["Jeong", "Changwook", ""], ["Kim", "Dae Sin", ""]]}, {"id": "2104.02496", "submitter": "Matthias A{\\ss}enmacher", "authors": "P. Schulze, S. Wiegrebe, P. W. Thurner, C. Heumann, M. A{\\ss}enmacher,\n  S. Wankm\\\"uller", "title": "Exploring Topic-Metadata Relationships with the STM: A Bayesian Approach", "comments": "8 pages, 4 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Topic models such as the Structural Topic Model (STM) estimate latent topical\nclusters within text. An important step in many topic modeling applications is\nto explore relationships between the discovered topical structure and metadata\nassociated with the text documents. Methods used to estimate such relationships\nmust take into account that the topical structure is not directly observed, but\ninstead being estimated itself. The authors of the STM, for instance, perform\nrepeated OLS regressions of sampled topic proportions on metadata covariates by\nusing a Monte Carlo sampling technique known as the method of composition. In\nthis paper, we propose two improvements: first, we replace OLS with more\nappropriate Beta regression. Second, we suggest a fully Bayesian approach\ninstead of the current blending of frequentist and Bayesian methods. We\ndemonstrate our improved methodology by exploring relationships between Twitter\nposts by German members of parliament (MPs) and different metadata covariates.\n", "versions": [{"version": "v1", "created": "Tue, 6 Apr 2021 13:28:04 GMT"}], "update_date": "2021-04-07", "authors_parsed": [["Schulze", "P.", ""], ["Wiegrebe", "S.", ""], ["Thurner", "P. W.", ""], ["Heumann", "C.", ""], ["A\u00dfenmacher", "M.", ""], ["Wankm\u00fcller", "S.", ""]]}, {"id": "2104.02550", "submitter": "Sergey Alyaev", "authors": "Kristian Fossum, Sergey Alyaev, Jan Tveranger, Ahmed Elsheikh", "title": "Deep learning for prediction of complex geology ahead of drilling", "comments": "Accepted to ICCS 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  During a geosteering operation the well path is intentionally adjusted in\nresponse to the new data acquired while drilling. To achieve consistent\nhigh-quality decisions, especially when drilling in complex environments,\ndecision support systems can help cope with high volumes of data and\ninterpretation complexities. They can assimilate the real-time measurements\ninto a probabilistic earth model and use the updated model for decision\nrecommendations.\n  Recently, machine learning (ML) techniques have enabled a wide range of\nmethods that redistribute computational cost from on-line to off-line\ncalculations. In this paper, we introduce two ML techniques into the\ngeosteering decision support framework. Firstly, a complex earth model\nrepresentation is generated using a Generative Adversarial Network (GAN).\nSecondly, a commercial extra-deep electromagnetic simulator is represented\nusing a Forward Deep Neural Network (FDNN).\n  The numerical experiments demonstrate that the combination of the GAN and the\nFDNN in an ensemble randomized maximum likelihood data assimilation scheme\nprovides real-time estimates of complex geological uncertainty. This yields\nreduction of geological uncertainty ahead of the drill-bit from the\nmeasurements gathered behind and around the well bore.\n", "versions": [{"version": "v1", "created": "Tue, 6 Apr 2021 14:42:33 GMT"}], "update_date": "2021-04-07", "authors_parsed": [["Fossum", "Kristian", ""], ["Alyaev", "Sergey", ""], ["Tveranger", "Jan", ""], ["Elsheikh", "Ahmed", ""]]}, {"id": "2104.02640", "submitter": "TrungTin Nguyen", "authors": "TrungTin Nguyen, Hien Duy Nguyen, Faicel Chamroukhi and Florence\n  Forbes", "title": "A non-asymptotic penalization criterion for model selection in mixture\n  of experts models", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST cs.AI cs.LG stat.ME stat.ML stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Mixture of experts (MoE) is a popular class of models in statistics and\nmachine learning that has sustained attention over the years, due to its\nflexibility and effectiveness. We consider the Gaussian-gated localized MoE\n(GLoME) regression model for modeling heterogeneous data. This model poses\nchallenging questions with respect to the statistical estimation and model\nselection problems, including feature selection, both from the computational\nand theoretical points of view. We study the problem of estimating the number\nof components of the GLoME model, in a penalized maximum likelihood estimation\nframework. We provide a lower bound on the penalty that ensures a weak oracle\ninequality is satisfied by our estimator. To support our theoretical result, we\nperform numerical experiments on simulated and real data, which illustrate the\nperformance of our finite-sample oracle inequality.\n", "versions": [{"version": "v1", "created": "Tue, 6 Apr 2021 16:24:55 GMT"}], "update_date": "2021-04-07", "authors_parsed": [["Nguyen", "TrungTin", ""], ["Nguyen", "Hien Duy", ""], ["Chamroukhi", "Faicel", ""], ["Forbes", "Florence", ""]]}, {"id": "2104.02650", "submitter": "Jan N. Fuhg", "authors": "Jan Niklas Fuhg, Christoph Boehm, Nikolaos Bouklas, Amelie Fau, Peter\n  Wriggers, Michele Marino", "title": "Model-data-driven constitutive responses: application to a multiscale\n  computational framework", "comments": "43 pages, 28 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG math.AP", "license": "http://creativecommons.org/licenses/by-sa/4.0/", "abstract": "  Computational multiscale methods for analyzing and deriving constitutive\nresponses have been used as a tool in engineering problems because of their\nability to combine information at different length scales. However, their\napplication in a nonlinear framework can be limited by high computational\ncosts, numerical difficulties, and/or inaccuracies. In this paper, a hybrid\nmethodology is presented which combines classical constitutive laws\n(model-based), a data-driven correction component, and computational multiscale\napproaches. A model-based material representation is locally improved with data\nfrom lower scales obtained by means of a nonlinear numerical homogenization\nprocedure leading to a model-data-driven approach. Therefore, macroscale\nsimulations explicitly incorporate the true microscale response, maintaining\nthe same level of accuracy that would be obtained with online micro-macro\nsimulations but with a computational cost comparable to classical model-driven\napproaches. In the proposed approach, both model and data play a fundamental\nrole allowing for the synergistic integration between a physics-based response\nand a machine learning black-box. Numerical applications are implemented in two\ndimensions for different tests investigating both material and structural\nresponses in large deformation.\n", "versions": [{"version": "v1", "created": "Tue, 6 Apr 2021 16:34:46 GMT"}], "update_date": "2021-04-07", "authors_parsed": [["Fuhg", "Jan Niklas", ""], ["Boehm", "Christoph", ""], ["Bouklas", "Nikolaos", ""], ["Fau", "Amelie", ""], ["Wriggers", "Peter", ""], ["Marino", "Michele", ""]]}, {"id": "2104.02705", "submitter": "David R\\\"ugamer", "authors": "David R\\\"ugamer, Ruolin Shen, Christina Bukas, Lisa Barros de Andrade\n  e Sousa, Dominik Thalmeier, Nadja Klein, Chris Kolb, Florian Pfisterer,\n  Philipp Kopper, Bernd Bischl, Christian L. M\\\"uller", "title": "deepregression: a Flexible Neural Network Framework for Semi-Structured\n  Deep Distributional Regression", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper describes the implementation of semi-structured deep\ndistributional regression, a flexible framework to learn distributions based on\na combination of additive regression models and deep neural networks.\ndeepregression is implemented in both R and Python, using the deep learning\nlibraries TensorFlow and PyTorch, respectively. The implementation consists of\n(1) a modular neural network building system for the combination of various\nstatistical and deep learning approaches, (2) an orthogonalization cell to\nallow for an interpretable combination of different subnetworks as well as (3)\npre-processing steps necessary to initialize such models. The software package\nallows to define models in a user-friendly manner using distribution\ndefinitions via a formula environment that is inspired by classical statistical\nmodel frameworks such as mgcv. The packages' modular design and functionality\nprovides a unique resource for rapid and reproducible prototyping of complex\nstatistical and deep learning models while simultaneously retaining the\nindispensable interpretability of classical statistical models.\n", "versions": [{"version": "v1", "created": "Tue, 6 Apr 2021 17:56:31 GMT"}], "update_date": "2021-04-07", "authors_parsed": [["R\u00fcgamer", "David", ""], ["Shen", "Ruolin", ""], ["Bukas", "Christina", ""], ["Sousa", "Lisa Barros de Andrade e", ""], ["Thalmeier", "Dominik", ""], ["Klein", "Nadja", ""], ["Kolb", "Chris", ""], ["Pfisterer", "Florian", ""], ["Kopper", "Philipp", ""], ["Bischl", "Bernd", ""], ["M\u00fcller", "Christian L.", ""]]}, {"id": "2104.02768", "submitter": "Jacob Pfau", "authors": "Jacob Pfau, Albert T. Young, Jerome Wei, Maria L. Wei, Michael J.\n  Keiser", "title": "Robust Semantic Interpretability: Revisiting Concept Activation Vectors", "comments": "ICML WHI 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.AI cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Interpretability methods for image classification assess model\ntrustworthiness by attempting to expose whether the model is systematically\nbiased or attending to the same cues as a human would. Saliency methods for\nfeature attribution dominate the interpretability literature, but these methods\ndo not address semantic concepts such as the textures, colors, or genders of\nobjects within an image. Our proposed Robust Concept Activation Vectors (RCAV)\nquantifies the effects of semantic concepts on individual model predictions and\non model behavior as a whole. RCAV calculates a concept gradient and takes a\ngradient ascent step to assess model sensitivity to the given concept. By\ngeneralizing previous work on concept activation vectors to account for model\nnon-linearity, and by introducing stricter hypothesis testing, we show that\nRCAV yields interpretations which are both more accurate at the image level and\nrobust at the dataset level. RCAV, like saliency methods, supports the\ninterpretation of individual predictions. To evaluate the practical use of\ninterpretability methods as debugging tools, and the scientific use of\ninterpretability methods for identifying inductive biases (e.g. texture over\nshape), we construct two datasets and accompanying metrics for realistic\nbenchmarking of semantic interpretability methods. Our benchmarks expose the\nimportance of counterfactual augmentation and negative controls for quantifying\nthe practical usability of interpretability methods.\n", "versions": [{"version": "v1", "created": "Tue, 6 Apr 2021 20:14:59 GMT"}], "update_date": "2021-04-08", "authors_parsed": [["Pfau", "Jacob", ""], ["Young", "Albert T.", ""], ["Wei", "Jerome", ""], ["Wei", "Maria L.", ""], ["Keiser", "Michael J.", ""]]}, {"id": "2104.02769", "submitter": "Liangyuan Hu", "authors": "Liangyuan Hu and Jung-Yi Joyce Lin and Jiayi Ji", "title": "Variable selection with missing data in both covariates and outcomes:\n  Imputation and machine learning", "comments": "29 pages, 17 figures, 4 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.AP stat.ML", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  The missing data issue is ubiquitous in health studies. Variable selection in\nthe presence of both missing covariates and outcomes is an important\nstatistical research topic but has been less studied. Existing literature\nfocuses on parametric regression techniques that provide direct parameter\nestimates of the regression model. Flexible nonparametric machine learning\nmethods considerably mitigate the reliance on the parametric assumptions, but\ndo not provide as naturally defined variable importance measure as the\ncovariate effect native to parametric models. We investigate a general variable\nselection approach when both the covariates and outcomes can be missing at\nrandom and have general missing data patterns. This approach exploits the\nflexibility of machine learning modeling techniques and bootstrap imputation,\nwhich is amenable to nonparametric methods in which the covariate effects are\nnot directly available. We conduct expansive simulations investigating the\npractical operating characteristics of the proposed variable selection\napproach, when combined with four tree-based machine learning methods, XGBoost,\nRandom Forests, Bayesian Additive Regression Trees (BART) and Conditional\nRandom Forests, and two commonly used parametric methods, lasso and backward\nstepwise selection. Numeric results suggest that when combined with bootstrap\nimputation, XGBoost and BART have the overall best variable selection\nperformance with respect to the $F_1$ score and Type I error across various\nsettings. In general, there is no significant difference in the variable\nselection performance due to imputation methods. We further demonstrate the\nmethods via a case study of risk factors for 3-year incidence of metabolic\nsyndrome with data from the Study of Women's Health Across the Nation.\n", "versions": [{"version": "v1", "created": "Tue, 6 Apr 2021 20:18:29 GMT"}, {"version": "v2", "created": "Wed, 7 Jul 2021 21:02:21 GMT"}], "update_date": "2021-07-09", "authors_parsed": [["Hu", "Liangyuan", ""], ["Lin", "Jung-Yi Joyce", ""], ["Ji", "Jiayi", ""]]}, {"id": "2104.02810", "submitter": "Michael Weylandt", "authors": "Michael Weylandt and George Michailidis and T. Mitchell Roddenberry", "title": "Sparse Partial Least Squares for Coarse Noisy Graph Alignment", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG cs.SI eess.SP stat.ME", "license": "http://creativecommons.org/licenses/by-sa/4.0/", "abstract": "  Graph signal processing (GSP) provides a powerful framework for analyzing\nsignals arising in a variety of domains. In many applications of GSP, multiple\nnetwork structures are available, each of which captures different aspects of\nthe same underlying phenomenon. To integrate these different data sources,\ngraph alignment techniques attempt to find the best correspondence between\nvertices of two graphs. We consider a generalization of this problem, where\nthere is no natural one-to-one mapping between vertices, but where there is\ncorrespondence between the community structures of each graph. Because we seek\nto learn structure at this higher community level, we refer to this problem as\n\"coarse\" graph alignment. To this end, we propose a novel regularized partial\nleast squares method which both incorporates the observed graph structures and\nimposes sparsity in order to reflect the underlying block community structure.\nWe provide efficient algorithms for our method and demonstrate its\neffectiveness in simulations.\n", "versions": [{"version": "v1", "created": "Tue, 6 Apr 2021 21:52:15 GMT"}], "update_date": "2021-04-08", "authors_parsed": [["Weylandt", "Michael", ""], ["Michailidis", "George", ""], ["Roddenberry", "T. Mitchell", ""]]}, {"id": "2104.02865", "submitter": "Art Owen", "authors": "Sifan Liu and Art B. Owen", "title": "Quasi-Newton Quasi-Monte Carlo for variational Bayes", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.NA math.NA stat.ML", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Many machine learning problems optimize an objective that must be measured\nwith noise. The primary method is a first order stochastic gradient descent\nusing one or more Monte Carlo (MC) samples at each step. There are settings\nwhere ill-conditioning makes second order methods such as L-BFGS more\neffective. We study the use of randomized quasi-Monte Carlo (RQMC) sampling for\nsuch problems. When MC sampling has a root mean squared error (RMSE) of\n$O(n^{-1/2})$ then RQMC has an RMSE of $o(n^{-1/2})$ that can be close to\n$O(n^{-3/2})$ in favorable settings. We prove that improved sampling accuracy\ntranslates directly to improved optimization. In our empirical investigations\nfor variational Bayes, using RQMC with stochastic L-BFGS greatly speeds up the\noptimization, and sometimes finds a better parameter value than MC does.\n", "versions": [{"version": "v1", "created": "Wed, 7 Apr 2021 02:34:03 GMT"}, {"version": "v2", "created": "Wed, 21 Apr 2021 00:58:02 GMT"}], "update_date": "2021-04-22", "authors_parsed": [["Liu", "Sifan", ""], ["Owen", "Art B.", ""]]}, {"id": "2104.02872", "submitter": "Daniel Ahfock", "authors": "Daniel Ahfock and Geoffrey J. McLachlan", "title": "Harmless label noise and informative soft-labels in supervised\n  classification", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Manual labelling of training examples is common practice in supervised\nlearning. When the labelling task is of non-trivial difficulty, the supplied\nlabels may not be equal to the ground-truth labels, and label noise is\nintroduced into the training dataset. If the manual annotation is carried out\nby multiple experts, the same training example can be given different class\nassignments by different experts, which is indicative of label noise. In the\nframework of model-based classification, a simple, but key observation is that\nwhen the manual labels are sampled using the posterior probabilities of class\nmembership, the noisy labels are as valuable as the ground-truth labels in\nterms of statistical information. A relaxation of this process is a random\neffects model for imperfect labelling by a group that uses approximate\nposterior probabilities of class membership. The relative efficiency of\nlogistic regression using the noisy labels compared to logistic regression\nusing the ground-truth labels can then be derived. The main finding is that\nlogistic regression can be robust to label noise when label noise and\nclassification difficulty are positively correlated. In particular, when\nclassification difficulty is the only source of label errors, multiple sets of\nnoisy labels can supply more information for the estimation of a classification\nrule compared to the single set of ground-truth labels.\n", "versions": [{"version": "v1", "created": "Wed, 7 Apr 2021 02:56:11 GMT"}], "update_date": "2021-04-08", "authors_parsed": [["Ahfock", "Daniel", ""], ["McLachlan", "Geoffrey J.", ""]]}, {"id": "2104.02883", "submitter": "Mingyuan Wang", "authors": "Mingyuan Wang, Adrian Barbu", "title": "Online Feature Screening for Data Streams with Concept Drift", "comments": "8 figures, 30 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Screening feature selection methods are often used as a preprocessing step\nfor reducing the number of variables before training step. Traditional\nscreening methods only focus on dealing with complete high dimensional\ndatasets. Modern datasets not only have higher dimension and larger sample\nsize, but also have properties such as streaming input, sparsity and concept\ndrift. Therefore a considerable number of online feature selection methods were\nintroduced to handle these kind of problems in recent years. Online screening\nmethods are one of the categories of online feature selection methods. The\nmethods that we proposed in this research are capable of handling all three\nsituations mentioned above. Our research study focuses on classification\ndatasets. Our experiments show proposed methods can generate the same feature\nimportance as their offline version with faster speed and less storage\nconsumption. Furthermore, the results show that online screening methods with\nintegrated model adaptation have a higher true feature detection rate than\nwithout model adaptation on data streams with the concept drift property. Among\nthe two large real datasets that potentially have the concept drift property,\nonline screening methods with model adaptation show advantages in either saving\ncomputing time and space, reducing model complexity, or improving prediction\naccuracy.\n", "versions": [{"version": "v1", "created": "Wed, 7 Apr 2021 03:16:15 GMT"}], "update_date": "2021-04-08", "authors_parsed": [["Wang", "Mingyuan", ""], ["Barbu", "Adrian", ""]]}, {"id": "2104.02929", "submitter": "AmirEmad Ghassami", "authors": "AmirEmad Ghassami, Andrew Ying, Ilya Shpitser, Eric Tchetgen Tchetgen", "title": "Minimax Kernel Machine Learning for a Class of Doubly Robust Functionals", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG econ.EM math.ST stat.TH", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  A moment function is called doubly robust if it is comprised of two nuisance\nfunctions and the estimator based on it is a consistent estimator of the target\nparameter even if one of the nuisance functions is misspecified. In this paper,\nwe consider a class of doubly robust moment functions originally introduced in\n(Robins et al., 2008). We demonstrate that this moment function can be used to\nconstruct estimating equations for the nuisance functions. The main idea is to\nchoose each nuisance function such that it minimizes the dependency of the\nexpected value of the moment function to the other nuisance function. We\nimplement this idea as a minimax optimization problem. We then provide\nconditions required for asymptotic linearity of the estimator of the parameter\nof interest, which are based on the convergence rate of the product of the\nerrors of the nuisance functions, as well as the local ill-posedness of a\nconditional expectation operator. The convergence rates of the nuisance\nfunctions are analyzed using the modern techniques in statistical learning\ntheory based on the Rademacher complexity of the function spaces. We\nspecifically focus on the case that the function spaces are reproducing kernel\nHilbert spaces, which enables us to use its spectral properties to analyze the\nconvergence rates. As an application of the proposed methodology, we consider\nthe parameter of average causal effect both in presence and absence of latent\nconfounders. For the case of presence of latent confounders, we use the\nrecently proposed proximal causal inference framework of (Miao et al., 2018;\nTchetgen Tchetgen et al., 2020), and hence our results lead to a robust\nnon-parametric estimator for average causal effect in this framework.\n", "versions": [{"version": "v1", "created": "Wed, 7 Apr 2021 05:52:15 GMT"}], "update_date": "2021-04-08", "authors_parsed": [["Ghassami", "AmirEmad", ""], ["Ying", "Andrew", ""], ["Shpitser", "Ilya", ""], ["Tchetgen", "Eric Tchetgen", ""]]}, {"id": "2104.02943", "submitter": "Myrto Limnios", "authors": "St\\'ephan Cl\\'emen\\c{c}on (LTCI), Myrto Limnios (CB), Nicolas Vayatis\n  (CB)", "title": "Concentration Inequalities for Two-Sample Rank Processes with\n  Application to Bipartite Ranking", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.ML stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The ROC curve is the gold standard for measuring the performance of a\ntest/scoring statistic regarding its capacity to discriminate between two\nstatistical populations in a wide variety of applications, ranging from anomaly\ndetection in signal processing to information retrieval, through medical\ndiagnosis. Most practical performance measures used in scoring/ranking\napplications such as the AUC, the local AUC, the p-norm push, the DCG and\nothers, can be viewed as summaries of the ROC curve. In this paper, the fact\nthat most of these empirical criteria can be expressed as two-sample linear\nrank statistics is highlighted and concentration inequalities for collections\nof such random variables, referred to as two-sample rank processes here, are\nproved, when indexed by VC classes of scoring functions. Based on these\nnonasymptotic bounds, the generalization capacity of empirical maximizers of a\nwide class of ranking performance criteria is next investigated from a\ntheoretical perspective. It is also supported by empirical evidence through\nconvincing numerical experiments.\n", "versions": [{"version": "v1", "created": "Wed, 7 Apr 2021 06:31:06 GMT"}], "update_date": "2021-04-08", "authors_parsed": [["Cl\u00e9men\u00e7on", "St\u00e9phan", "", "LTCI"], ["Limnios", "Myrto", "", "CB"], ["Vayatis", "Nicolas", "", "CB"]]}, {"id": "2104.02973", "submitter": "Pierre Gutierrez", "authors": "Antoine Cordier, Deepan Das, and Pierre Gutierrez", "title": "Active learning using weakly supervised signals for quality inspection", "comments": "8 pages, 3 Figures, QCAV 2021 conference (proceedings published in\n  SPIE)", "journal-ref": null, "doi": "10.1117/12.2586595", "report-no": null, "categories": "cs.CV cs.AI cs.LO stat.ML", "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/", "abstract": "  Because manufacturing processes evolve fast, and since production visual\naspect can vary significantly on a daily basis, the ability to rapidly update\nmachine vision based inspection systems is paramount. Unfortunately, supervised\nlearning of convolutional neural networks requires a significant amount of\nannotated images for being able to learn effectively from new data.\nAcknowledging the abundance of continuously generated images coming from the\nproduction line and the cost of their annotation, we demonstrate it is possible\nto prioritize and accelerate the annotation process. In this work, we develop a\nmethodology for learning actively, from rapidly mined, weakly (i.e. partially)\nannotated data, enabling a fast, direct feedback from the operators on the\nproduction line and tackling a big machine vision weakness: false positives. We\nalso consider the problem of covariate shift, which arises inevitably due to\nchanging conditions during data acquisition. In that regard, we show\ndomain-adversarial training to be an efficient way to address this issue.\n", "versions": [{"version": "v1", "created": "Wed, 7 Apr 2021 07:49:07 GMT"}], "update_date": "2021-07-23", "authors_parsed": [["Cordier", "Antoine", ""], ["Das", "Deepan", ""], ["Gutierrez", "Pierre", ""]]}, {"id": "2104.02988", "submitter": "Crist\\'obal Guzm\\'an", "authors": "Digvijay Boob and Crist\\'obal Guzm\\'an", "title": "Optimal Algorithms for Differentially Private Stochastic Monotone\n  Variational Inequalities and Saddle-Point Problems", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.OC cs.LG stat.ML", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  In this work, we conduct the first systematic study of stochastic variational\ninequality (SVI) and stochastic saddle point (SSP) problems under the\nconstraint of differential privacy-(DP). We propose two algorithms: Noisy\nStochastic Extragradient (NSEG) and Noisy Inexact Stochastic Proximal Point\n(NISPP). We show that sampling with replacement variants of these algorithms\nattain the optimal risk for DP-SVI and DP-SSP. Key to our analysis is the\ninvestigation of algorithmic stability bounds, both of which are new even in\nthe nonprivate case, together with a novel \"stability implies generalization\"\nresult for the gap functions for SVI and SSP problems. The dependence of the\nrunning time of these algorithms, with respect to the dataset size $n$, is\n$n^2$ for NSEG and $\\widetilde{O}(n^{3/2})$ for NISPP.\n", "versions": [{"version": "v1", "created": "Wed, 7 Apr 2021 08:37:07 GMT"}], "update_date": "2021-04-08", "authors_parsed": [["Boob", "Digvijay", ""], ["Guzm\u00e1n", "Crist\u00f3bal", ""]]}, {"id": "2104.03006", "submitter": "Albert Zeyer", "authors": "Albert Zeyer, Andr\\'e Merboldt, Wilfried Michel, Ralf Schl\\\"uter,\n  Hermann Ney", "title": "Librispeech Transducer Model with Internal Language Model Prior\n  Correction", "comments": "accepted at Interspeech 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present our transducer model on Librispeech. We study variants to include\nan external language model (LM) with shallow fusion and subtract an estimated\ninternal LM. This is justified by a Bayesian interpretation where the\ntransducer model prior is given by the estimated internal LM. The subtraction\nof the internal LM gives us over 14% relative improvement over normal shallow\nfusion. Our transducer has a separate probability distribution for the\nnon-blank labels which allows for easier combination with the external LM, and\neasier estimation of the internal LM. We additionally take care of including\nthe end-of-sentence (EOS) probability of the external LM in the last blank\nprobability which further improves the performance. All our code and setups are\npublished.\n", "versions": [{"version": "v1", "created": "Wed, 7 Apr 2021 09:18:56 GMT"}, {"version": "v2", "created": "Sun, 13 Jun 2021 00:09:16 GMT"}], "update_date": "2021-06-15", "authors_parsed": [["Zeyer", "Albert", ""], ["Merboldt", "Andr\u00e9", ""], ["Michel", "Wilfried", ""], ["Schl\u00fcter", "Ralf", ""], ["Ney", "Hermann", ""]]}, {"id": "2104.03007", "submitter": "Paul Tiwald", "authors": "Paul Tiwald, Alexandra Ebert, Daniel T. Soukup", "title": "Representative & Fair Synthetic Data", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Algorithms learn rules and associations based on the training data that they\nare exposed to. Yet, the very same data that teaches machines to understand and\npredict the world, contains societal and historic biases, resulting in biased\nalgorithms with the risk of further amplifying these once put into use for\ndecision support. Synthetic data, on the other hand, emerges with the promise\nto provide an unlimited amount of representative, realistic training samples,\nthat can be shared further without disclosing the privacy of individual\nsubjects. We present a framework to incorporate fairness constraints into the\nself-supervised learning process, that allows to then simulate an unlimited\namount of representative as well as fair synthetic data. This framework\nprovides a handle to govern and control for privacy as well as for bias within\nAI at its very source: the training data. We demonstrate the proposed approach\nby amending an existing generative model architecture and generating a\nrepresentative as well as fair version of the UCI Adult census data set. While\nthe relationships between attributes are faithfully retained, the gender and\nracial biases inherent in the original data are controlled for. This is further\nvalidated by comparing propensity scores of downstream predictive models that\nare trained on the original data versus the fair synthetic data. We consider\nrepresentative & fair synthetic data a promising future building block to teach\nalgorithms not on historic worlds, but rather on the worlds that we strive to\nlive in.\n", "versions": [{"version": "v1", "created": "Wed, 7 Apr 2021 09:19:46 GMT"}], "update_date": "2021-04-08", "authors_parsed": [["Tiwald", "Paul", ""], ["Ebert", "Alexandra", ""], ["Soukup", "Daniel T.", ""]]}, {"id": "2104.03059", "submitter": "Thomas Unterthiner", "authors": "Jean-Baptiste Cordonnier, Aravindh Mahendran, Alexey Dosovitskiy, Dirk\n  Weissenborn, Jakob Uszkoreit, Thomas Unterthiner", "title": "Differentiable Patch Selection for Image Recognition", "comments": "Accepted to IEEE/CVF Conference on Computer Vision and Pattern\n  Recognition (CVPR) 2021. Code available at\n  https://github.com/google-research/google-research/tree/master/ptopk_patch_selection/", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Neural Networks require large amounts of memory and compute to process high\nresolution images, even when only a small part of the image is actually\ninformative for the task at hand. We propose a method based on a differentiable\nTop-K operator to select the most relevant parts of the input to efficiently\nprocess high resolution images. Our method may be interfaced with any\ndownstream neural network, is able to aggregate information from different\npatches in a flexible way, and allows the whole model to be trained end-to-end\nusing backpropagation. We show results for traffic sign recognition,\ninter-patch relationship reasoning, and fine-grained recognition without using\nobject/part bounding box annotations during training.\n", "versions": [{"version": "v1", "created": "Wed, 7 Apr 2021 11:15:51 GMT"}], "update_date": "2021-04-08", "authors_parsed": [["Cordonnier", "Jean-Baptiste", ""], ["Mahendran", "Aravindh", ""], ["Dosovitskiy", "Alexey", ""], ["Weissenborn", "Dirk", ""], ["Uszkoreit", "Jakob", ""], ["Unterthiner", "Thomas", ""]]}, {"id": "2104.03065", "submitter": "Marcelo Medeiros", "authors": "Marcelo C. Medeiros, Henrique F. Pires", "title": "The Proper Use of Google Trends in Forecasting Models", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "econ.EM cs.LG stat.AP stat.ML", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  It is widely known that Google Trends have become one of the most popular\nfree tools used by forecasters both in academics and in the private and public\nsectors. There are many papers, from several different fields, concluding that\nGoogle Trends improve forecasts' accuracy. However, what seems to be widely\nunknown, is that each sample of Google search data is different from the other,\neven if you set the same search term, data and location. This means that it is\npossible to find arbitrary conclusions merely by chance. This paper aims to\nshow why and when it can become a problem and how to overcome this obstacle.\n", "versions": [{"version": "v1", "created": "Wed, 7 Apr 2021 11:33:51 GMT"}, {"version": "v2", "created": "Thu, 8 Apr 2021 14:15:57 GMT"}, {"version": "v3", "created": "Sat, 10 Apr 2021 13:09:44 GMT"}], "update_date": "2021-04-13", "authors_parsed": [["Medeiros", "Marcelo C.", ""], ["Pires", "Henrique F.", ""]]}, {"id": "2104.03088", "submitter": "Lewis Crawford Mr", "authors": "Stephane Doyen, Hugh Taylor, Peter Nicholas, Lewis Crawford, Isabella\n  Young, Michael Sughrue", "title": "Hollow-tree Super: a directional and scalable approach for feature\n  importance in boosted tree models", "comments": "28 pages, 1 table, 7 figures, PDF format - Submitted to PLOSONE\n  pending review", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG stat.ME", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Current limitations in boosted tree modelling prevent the effective scaling\nto datasets with a large feature number, particularly when investigating the\nmagnitude and directionality of various features on classification. We present\na novel methodology, Hollow-tree Super (HOTS), to resolve and visualize feature\nimportance in boosted tree models involving a large number of features.\nFurther, HOTS allows for investigation of the directionality and magnitude\nvarious features have on classification. Using the Iris dataset, we first\ncompare HOTS to Gini Importance, Partial Dependence Plots, and Permutation\nImportance, and demonstrate how HOTS resolves the weaknesses present in these\nmethods. We then show how HOTS can be utilized in high dimensional\nneuroscientific data, by taking 60 Schizophrenic subjects and applying the\nmethod to determine which brain regions were most important for classification\nof schizophrenia as determined by the PANSS. HOTS effectively replicated and\nsupported the findings of Gini importance, Partial Dependence Plots and\nPermutation importance within the Iris dataset. When applied to the\nschizophrenic brain dataset, HOTS was able to resolve the top 10 most important\nfeatures for classification, as well as their directionality for classification\nand magnitude compared to other features. Cross-validation supported that these\nsame 10 features were consistently used in the decision-making process across\nmultiple trees, and these features were localised primarily to the occipital\nand parietal cortices, commonly disturbed brain regions in those with\nSchizophrenia. It is imperative that a methodology is developed that is able to\nhandle the demands of working with large datasets that contain a large number\nof features. HOTS represents a unique way to investigate both the\ndirectionality and magnitude of feature importance when working at scale with\nboosted-tree modelling.\n", "versions": [{"version": "v1", "created": "Wed, 7 Apr 2021 12:24:56 GMT"}], "update_date": "2021-04-08", "authors_parsed": [["Doyen", "Stephane", ""], ["Taylor", "Hugh", ""], ["Nicholas", "Peter", ""], ["Crawford", "Lewis", ""], ["Young", "Isabella", ""], ["Sughrue", "Michael", ""]]}, {"id": "2104.03115", "submitter": "Michael Chertkov", "authors": "Andrei Afonin and Michael Chertkov", "title": "Which Neural Network to Choose for Post-Fault Localization, Dynamic\n  State Estimation and Optimal Measurement Placement in Power Systems?", "comments": "12 pages, 8 figures, 10 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG physics.data-an physics.soc-ph", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  We consider a power transmission system monitored with Phasor Measurement\nUnits (PMUs) placed at significant, but not all, nodes of the system. Assuming\nthat a sufficient number of distinct single-line faults, specifically pre-fault\nstate and (not cleared) post-fault state, are recorded by the PMUs and are\navailable for training, we, first, design a comprehensive sequence of Neural\nNetworks (NNs) locating the faulty line. Performance of different NNs in the\nsequence, including Linear Regression, Feed-Forward NN, AlexNet, Graphical\nConvolutional NN, Neural Linear ODE and Neural Graph-based ODE, ordered\naccording to the type and amount of the power flow physics involved, are\ncompared for different levels of observability. Second, we build a sequence of\nadvanced Power-System-Dynamics-Informed and Neural-ODE based Machine Learning\nschemes trained, given pre-fault state, to predict the post-fault state and\nalso, in parallel, to estimate system parameters. Finally, third, and\ncontinuing to work with the first (fault localization) setting we design a\n(NN-based) algorithm which discovers optimal PMU placement.\n", "versions": [{"version": "v1", "created": "Wed, 7 Apr 2021 13:35:55 GMT"}], "update_date": "2021-04-08", "authors_parsed": [["Afonin", "Andrei", ""], ["Chertkov", "Michael", ""]]}, {"id": "2104.03158", "submitter": "Jean Pauphilet", "authors": "Dimitris Bertsimas, Arthur Delarue, Jean Pauphilet", "title": "Prediction with Missing Data", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Missing information is inevitable in real-world data sets. While imputation\nis well-suited and theoretically sound for statistical inference, its relevance\nand practical implementation for out-of-sample prediction remains unsettled. We\nprovide a theoretical analysis of widely used data imputation methods and\nhighlight their key deficiencies in making accurate predictions. Alternatively,\nwe propose adaptive linear regression, a new class of models that can be\ndirectly trained and evaluated on partially observed data, adapting to the set\nof available features. In particular, we show that certain adaptive regression\nmodels are equivalent to impute-then-regress methods where the imputation and\nthe regression models are learned simultaneously instead of sequentially. We\nvalidate our theoretical findings and adaptive regression approach with\nnumerical results with real-world data sets.\n", "versions": [{"version": "v1", "created": "Wed, 7 Apr 2021 14:45:14 GMT"}], "update_date": "2021-04-08", "authors_parsed": [["Bertsimas", "Dimitris", ""], ["Delarue", "Arthur", ""], ["Pauphilet", "Jean", ""]]}, {"id": "2104.03164", "submitter": "Xin Ding", "authors": "Xin Ding and Yongwei Wang and Zuheng Xu and Z. Jane Wang and William\n  J. Welch", "title": "Distilling and Transferring Knowledge via cGAN-generated Samples for\n  Image Classification and Regression", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV stat.ML", "license": "http://creativecommons.org/publicdomain/zero/1.0/", "abstract": "  Knowledge distillation (KD) has been actively studied for image\nclassification tasks in deep learning, aiming to improve the performance of a\nstudent model based on the knowledge from a teacher model. However, there have\nbeen very few efforts for applying KD in image regression with a scalar\nresponse, and there is no KD method applicable to both tasks. Moreover,\nexisting KD methods often require a practitioner to carefully choose or adjust\nthe teacher and student architectures, making these methods less scalable in\npractice. Furthermore, although KD is usually conducted in scenarios with\nlimited labeled data, very few techniques are developed to alleviate such data\ninsufficiency. To solve the above problems in an all-in-one manner, we propose\nin this paper a unified KD framework based on conditional generative\nadversarial networks (cGANs), termed cGAN-KD. Fundamentally different from\nexisting KD methods, cGAN-KD distills and transfers knowledge from a teacher\nmodel to a student model via cGAN-generated samples. This unique mechanism\nmakes cGAN-KD suitable for both classification and regression tasks, compatible\nwith other KD methods, and insensitive to the teacher and student\narchitectures. Also, benefiting from the recent advances in cGAN methodology\nand our specially designed subsampling and filtering procedures, cGAN-KD also\nperforms well when labeled data are scarce. An error bound of a student model\ntrained in the cGAN-KD framework is derived in this work, which theoretically\nexplains why cGAN-KD takes effect and guides the implementation of cGAN-KD in\npractice. Extensive experiments on CIFAR-10 and Tiny-ImageNet show that we can\nincorporate state-of-the-art KD methods into the cGAN-KD framework to reach a\nnew state of the art. Also, experiments on RC-49 and UTKFace demonstrate the\neffectiveness of cGAN-KD in image regression tasks, where existing KD methods\nare inapplicable.\n", "versions": [{"version": "v1", "created": "Wed, 7 Apr 2021 14:52:49 GMT"}, {"version": "v2", "created": "Sat, 1 May 2021 04:50:06 GMT"}], "update_date": "2021-05-04", "authors_parsed": [["Ding", "Xin", ""], ["Wang", "Yongwei", ""], ["Xu", "Zuheng", ""], ["Wang", "Z. Jane", ""], ["Welch", "William J.", ""]]}, {"id": "2104.03180", "submitter": "Andrea Patane", "authors": "Andrea Patane, Arno Blaas, Luca Laurenti, Luca Cardelli, Stephen\n  Roberts, Marta Kwiatkowska", "title": "Adversarial Robustness Guarantees for Gaussian Processes", "comments": "Submitted for publication", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Gaussian processes (GPs) enable principled computation of model uncertainty,\nmaking them attractive for safety-critical applications. Such scenarios demand\nthat GP decisions are not only accurate, but also robust to perturbations. In\nthis paper we present a framework to analyse adversarial robustness of GPs,\ndefined as invariance of the model's decision to bounded perturbations. Given a\ncompact subset of the input space $T\\subseteq \\mathbb{R}^d$, a point $x^*$ and\na GP, we provide provable guarantees of adversarial robustness of the GP by\ncomputing lower and upper bounds on its prediction range in $T$. We develop a\nbranch-and-bound scheme to refine the bounds and show, for any $\\epsilon > 0$,\nthat our algorithm is guaranteed to converge to values $\\epsilon$-close to the\nactual values in finitely many iterations. The algorithm is anytime and can\nhandle both regression and classification tasks, with analytical formulation\nfor most kernels used in practice. We evaluate our methods on a collection of\nsynthetic and standard benchmark datasets, including SPAM, MNIST and\nFashionMNIST. We study the effect of approximate inference techniques on\nrobustness and demonstrate how our method can be used for interpretability. Our\nempirical results suggest that the adversarial robustness of GPs increases with\naccurate posterior estimation.\n", "versions": [{"version": "v1", "created": "Wed, 7 Apr 2021 15:14:56 GMT"}], "update_date": "2021-04-08", "authors_parsed": [["Patane", "Andrea", ""], ["Blaas", "Arno", ""], ["Laurenti", "Luca", ""], ["Cardelli", "Luca", ""], ["Roberts", "Stephen", ""], ["Kwiatkowska", "Marta", ""]]}, {"id": "2104.03220", "submitter": "Malte S. Kurz", "authors": "Philipp Bach, Victor Chernozhukov, Malte S. Kurz, Martin Spindler", "title": "DoubleML -- An Object-Oriented Implementation of Double Machine Learning\n  in Python", "comments": "6 pages, 2 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG econ.EM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  DoubleML is an open-source Python library implementing the double machine\nlearning framework of Chernozhukov et al. (2018) for a variety of causal\nmodels. It contains functionalities for valid statistical inference on causal\nparameters when the estimation of nuisance parameters is based on machine\nlearning methods. The object-oriented implementation of DoubleML provides a\nhigh flexibility in terms of model specifications and makes it easily\nextendable. The package is distributed under the MIT license and relies on core\nlibraries from the scientific Python ecosystem: scikit-learn, numpy, pandas,\nscipy, statsmodels and joblib. Source code, documentation and an extensive user\nguide can be found at https://github.com/DoubleML/doubleml-for-py and\nhttps://docs.doubleml.org.\n", "versions": [{"version": "v1", "created": "Wed, 7 Apr 2021 16:16:39 GMT"}], "update_date": "2021-04-08", "authors_parsed": [["Bach", "Philipp", ""], ["Chernozhukov", "Victor", ""], ["Kurz", "Malte S.", ""], ["Spindler", "Martin", ""]]}, {"id": "2104.03279", "submitter": "Philipp Seidl", "authors": "Philipp Seidl, Philipp Renz, Natalia Dyubankova, Paulo Neves, Jonas\n  Verhoeven, Marwin Segler, J\\\"org K. Wegner, Sepp Hochreiter, G\\\"unter\n  Klambauer", "title": "Modern Hopfield Networks for Few- and Zero-Shot Reaction Template\n  Prediction", "comments": "14 pages + 12 pages appendix", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI q-bio.BM stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Finding synthesis routes for molecules of interest is an essential step in\nthe discovery of new drugs and materials. To find such routes,\ncomputer-assisted synthesis planning (CASP) methods are employed which rely on\na model of chemical reactivity. In this study, we model single-step\nretrosynthesis in a template-based approach using modern Hopfield networks\n(MHNs). We adapt MHNs to associate different modalities, reaction templates and\nmolecules, which allows the model to leverage structural information about\nreaction templates. This approach significantly improves the performance of\ntemplate relevance prediction, especially for templates with few or zero\ntraining examples. With inference speed several times faster than that of\nbaseline methods, we improve predictive performance for top-k exact match\naccuracy for $\\mathrm{k}\\geq5$ in the retrosynthesis benchmark USPTO-50k.\n", "versions": [{"version": "v1", "created": "Wed, 7 Apr 2021 17:35:00 GMT"}, {"version": "v2", "created": "Mon, 7 Jun 2021 15:57:01 GMT"}, {"version": "v3", "created": "Tue, 15 Jun 2021 13:24:02 GMT"}], "update_date": "2021-06-16", "authors_parsed": [["Seidl", "Philipp", ""], ["Renz", "Philipp", ""], ["Dyubankova", "Natalia", ""], ["Neves", "Paulo", ""], ["Verhoeven", "Jonas", ""], ["Segler", "Marwin", ""], ["Wegner", "J\u00f6rg K.", ""], ["Hochreiter", "Sepp", ""], ["Klambauer", "G\u00fcnter", ""]]}, {"id": "2104.03298", "submitter": "Changxiao Cai", "authors": "Gen Li, Changxiao Cai, Yuantao Gu, H. Vincent Poor, Yuxin Chen", "title": "Minimax Estimation of Linear Functions of Eigenvectors in the Face of\n  Small Eigen-Gaps", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST cs.IT cs.LG math.IT stat.ML stat.TH", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Eigenvector perturbation analysis plays a vital role in various statistical\ndata science applications. A large body of prior works, however, focused on\nestablishing $\\ell_{2}$ eigenvector perturbation bounds, which are often highly\ninadequate in addressing tasks that rely on fine-grained behavior of an\neigenvector. This paper makes progress on this by studying the perturbation of\nlinear functions of an unknown eigenvector. Focusing on two fundamental\nproblems -- matrix denoising and principal component analysis -- in the\npresence of Gaussian noise, we develop a suite of statistical theory that\ncharacterizes the perturbation of arbitrary linear functions of an unknown\neigenvector. In order to mitigate a non-negligible bias issue inherent to the\nnatural \"plug-in\" estimator, we develop de-biased estimators that (1) achieve\nminimax lower bounds for a family of scenarios (modulo some logarithmic\nfactor), and (2) can be computed in a data-driven manner without sample\nsplitting. Noteworthily, the proposed estimators are nearly minimax optimal\neven when the associated eigen-gap is substantially smaller than what is\nrequired in prior theory.\n", "versions": [{"version": "v1", "created": "Wed, 7 Apr 2021 17:55:10 GMT"}], "update_date": "2021-04-08", "authors_parsed": [["Li", "Gen", ""], ["Cai", "Changxiao", ""], ["Gu", "Yuantao", ""], ["Poor", "H. Vincent", ""], ["Chen", "Yuxin", ""]]}, {"id": "2104.03384", "submitter": "Andrew Duncan", "authors": "Andrew B. Duncan, Andrew M. Stuart, Marie-Therese Wolfram", "title": "Ensemble Inference Methods for Models With Noisy and Expensive\n  Likelihoods", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.NA cs.NA stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The increasing availability of data presents an opportunity to calibrate\nunknown parameters which appear in complex models of phenomena in the\nbiomedical, physical and social sciences. However, model complexity often leads\nto parameter-to-data maps which are expensive to evaluate and are only\navailable through noisy approximations. This paper is concerned with the use of\ninteracting particle systems for the solution of the resulting inverse problems\nfor parameters. Of particular interest is the case where the available forward\nmodel evaluations are subject to rapid fluctuations, in parameter space,\nsuperimposed on the smoothly varying large scale parametric structure of\ninterest. Multiscale analysis is used to study the behaviour of interacting\nparticle system algorithms when such rapid fluctuations, which we refer to as\nnoise, pollute the large scale parametric dependence of the parameter-to-data\nmap. Ensemble Kalman methods (which are derivative-free) and Langevin-based\nmethods (which use the derivative of the parameter-to-data map) are compared in\nthis light. The ensemble Kalman methods are shown to behave favourably in the\npresence of noise in the parameter-to-data map, whereas Langevin methods are\nadversely affected. On the other hand, Langevin methods have the correct\nequilibrium distribution in the setting of noise-free forward models, whilst\nensemble Kalman methods only provide an uncontrolled approximation, except in\nthe linear case. Therefore a new class of algorithms, ensemble Gaussian process\nsamplers, which combine the benefits of both ensemble Kalman and Langevin\nmethods, are introduced and shown to perform favourably.\n", "versions": [{"version": "v1", "created": "Wed, 7 Apr 2021 20:29:18 GMT"}], "update_date": "2021-04-09", "authors_parsed": [["Duncan", "Andrew B.", ""], ["Stuart", "Andrew M.", ""], ["Wolfram", "Marie-Therese", ""]]}, {"id": "2104.03527", "submitter": "Kayhan Behdin", "authors": "Kayhan Behdin and Rahul Mazumder", "title": "Archetypal Analysis for Sparse Nonnegative Matrix Factorization:\n  Robustness Under Misspecification", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider the problem of sparse nonnegative matrix factorization (NMF) with\narchetypal regularization. The goal is to represent a collection of data points\nas nonnegative linear combinations of a few nonnegative sparse factors with\nappealing geometric properties, arising from the use of archetypal\nregularization. We generalize the notion of robustness studied in Javadi and\nMontanari (2019) (without sparsity) to the notions of (a) strong robustness\nthat implies each estimated archetype is close to the underlying archetypes and\n(b) weak robustness that implies there exists at least one recovered archetype\nthat is close to the underlying archetypes. Our theoretical results on\nrobustness guarantees hold under minimal assumptions on the underlying data,\nand applies to settings where the underlying archetypes need not be sparse. We\npropose new algorithms for our optimization problem; and present numerical\nexperiments on synthetic and real datasets that shed further insights into our\nproposed framework and theoretical developments.\n", "versions": [{"version": "v1", "created": "Thu, 8 Apr 2021 06:06:48 GMT"}], "update_date": "2021-04-09", "authors_parsed": [["Behdin", "Kayhan", ""], ["Mazumder", "Rahul", ""]]}, {"id": "2104.03739", "submitter": "Mostafa Mehdipour Ghazi", "authors": "Mostafa Mehdipour Ghazi, Lauge S{\\o}rensen, S\\'ebastien Ourselin, Mads\n  Nielsen", "title": "CARRNN: A Continuous Autoregressive Recurrent Neural Network for Deep\n  Representation Learning from Sporadic Temporal Data", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI cs.CV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Learning temporal patterns from multivariate longitudinal data is challenging\nespecially in cases when data is sporadic, as often seen in, e.g., healthcare\napplications where the data can suffer from irregularity and asynchronicity as\nthe time between consecutive data points can vary across features and samples,\nhindering the application of existing deep learning models that are constructed\nfor complete, evenly spaced data with fixed sequence lengths. In this paper, a\nnovel deep learning-based model is developed for modeling multiple temporal\nfeatures in sporadic data using an integrated deep learning architecture based\non a recurrent neural network (RNN) unit and a continuous-time autoregressive\n(CAR) model. The proposed model, called CARRNN, uses a generalized\ndiscrete-time autoregressive model that is trainable end-to-end using neural\nnetworks modulated by time lags to describe the changes caused by the\nirregularity and asynchronicity. It is applied to multivariate time-series\nregression tasks using data provided for Alzheimer's disease progression\nmodeling and intensive care unit (ICU) mortality rate prediction, where the\nproposed model based on a gated recurrent unit (GRU) achieves the lowest\nprediction errors among the proposed RNN-based models and state-of-the-art\nmethods using GRUs and long short-term memory (LSTM) networks in their\narchitecture.\n", "versions": [{"version": "v1", "created": "Thu, 8 Apr 2021 12:43:44 GMT"}], "update_date": "2021-04-09", "authors_parsed": [["Ghazi", "Mostafa Mehdipour", ""], ["S\u00f8rensen", "Lauge", ""], ["Ourselin", "S\u00e9bastien", ""], ["Nielsen", "Mads", ""]]}, {"id": "2104.03757", "submitter": "Livia Paranhos", "authors": "Livia Paranhos", "title": "Predicting Inflation with Neural Networks", "comments": "47 pages, 9 figures. References to other arXiv articles:\n  arXiv:2008.12477, arXiv:1502.03167", "journal-ref": null, "doi": null, "report-no": null, "categories": "econ.EM stat.AP stat.ML", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  This paper applies neural network models to forecast inflation. The use of a\nparticular recurrent neural network, the long-short term memory model, or LSTM,\nthat summarizes macroeconomic information into common components is a major\ncontribution of the paper. Results from an exercise with US data indicate that\nthe estimated neural nets usually present better forecasting performance than\nstandard benchmarks, especially at long horizons. The LSTM in particular is\nfound to outperform the traditional feed-forward network at long horizons,\nsuggesting an advantage of the recurrent model in capturing the long-term trend\nof inflation. This finding can be rationalized by the so called long memory of\nthe LSTM that incorporates relatively old information in the forecast as long\nas accuracy is improved, while economizing in the number of estimated\nparameters. Interestingly, the neural nets containing macroeconomic information\ncapture well the features of inflation during and after the Great Recession,\npossibly indicating a role for nonlinearities and macro information in this\nepisode. The estimated common components used in the forecast seem able to\ncapture the business cycle dynamics, as well as information on prices.\n", "versions": [{"version": "v1", "created": "Thu, 8 Apr 2021 13:19:26 GMT"}], "update_date": "2021-04-09", "authors_parsed": [["Paranhos", "Livia", ""]]}, {"id": "2104.03804", "submitter": "Fares Mehouachi", "authors": "Fares B. Mehouachi, Chaouki Kasmi", "title": "Exact Stochastic Second Order Deep Learning", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Optimization in Deep Learning is mainly dominated by first-order methods\nwhich are built around the central concept of backpropagation. Second-order\noptimization methods, which take into account the second-order derivatives are\nfar less used despite superior theoretical properties. This inadequacy of\nsecond-order methods stems from its exorbitant computational cost, poor\nperformance, and the ineluctable non-convex nature of Deep Learning. Several\nattempts were made to resolve the inadequacy of second-order optimization\nwithout reaching a cost-effective solution, much less an exact solution. In\nthis work, we show that this long-standing problem in Deep Learning could be\nsolved in the stochastic case, given a suitable regularization of the neural\nnetwork. Interestingly, we provide an expression of the stochastic Hessian and\nits exact eigenvalues. We provide a closed-form formula for the exact\nstochastic second-order Newton direction, we solve the non-convexity issue and\nadjust our exact solution to favor flat minima through regularization and\nspectral adjustment. We test our exact stochastic second-order method on\npopular datasets and reveal its adequacy for Deep Learning.\n", "versions": [{"version": "v1", "created": "Thu, 8 Apr 2021 14:29:31 GMT"}], "update_date": "2021-04-09", "authors_parsed": [["Mehouachi", "Fares B.", ""], ["Kasmi", "Chaouki", ""]]}, {"id": "2104.03834", "submitter": "Jinu Gong", "authors": "Jinu Gong, Osvaldo Simeone, Joonhyuk Kang", "title": "Bayesian Variational Federated Learning and Unlearning in Decentralized\n  Networks", "comments": "Submitted for conference publication", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.DC stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Federated Bayesian learning offers a principled framework for the definition\nof collaborative training algorithms that are able to quantify epistemic\nuncertainty and to produce trustworthy decisions. Upon the completion of\ncollaborative training, an agent may decide to exercise her legal \"right to be\nforgotten\", which calls for her contribution to the jointly trained model to be\ndeleted and discarded. This paper studies federated learning and unlearning in\na decentralized network within a Bayesian framework. It specifically develops\nfederated variational inference (VI) solutions based on the decentralized\nsolution of local free energy minimization problems within exponential-family\nmodels and on local gossip-driven communication. The proposed protocols are\ndemonstrated to yield efficient unlearning mechanisms.\n", "versions": [{"version": "v1", "created": "Thu, 8 Apr 2021 15:18:35 GMT"}], "update_date": "2021-04-09", "authors_parsed": [["Gong", "Jinu", ""], ["Simeone", "Osvaldo", ""], ["Kang", "Joonhyuk", ""]]}, {"id": "2104.03863", "submitter": "Gauthier Gidel", "authors": "S\\'ebastien Bubeck, Yeshwanth Cherapanamjeri, Gauthier Gidel and\n  R\\'emi Tachet des Combes", "title": "A single gradient step finds adversarial examples on random two-layers\n  neural networks", "comments": "Added a comment about universal adversarial perturbations. 18 pages,\n  7 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CR stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Daniely and Schacham recently showed that gradient descent finds adversarial\nexamples on random undercomplete two-layers ReLU neural networks. The term\n\"undercomplete\" refers to the fact that their proof only holds when the number\nof neurons is a vanishing fraction of the ambient dimension. We extend their\nresult to the overcomplete case, where the number of neurons is larger than the\ndimension (yet also subexponential in the dimension). In fact we prove that a\nsingle step of gradient descent suffices. We also show this result for any\nsubexponential width random neural network with smooth activation function.\n", "versions": [{"version": "v1", "created": "Thu, 8 Apr 2021 16:06:54 GMT"}, {"version": "v2", "created": "Fri, 9 Apr 2021 22:13:17 GMT"}], "update_date": "2021-04-13", "authors_parsed": [["Bubeck", "S\u00e9bastien", ""], ["Cherapanamjeri", "Yeshwanth", ""], ["Gidel", "Gauthier", ""], ["Combes", "R\u00e9mi Tachet des", ""]]}, {"id": "2104.03942", "submitter": "Marko J\\\"arvenp\\\"a\\\"a", "authors": "Marko J\\\"arvenp\\\"a\\\"a, Jukka Corander", "title": "Approximate Bayesian inference from noisy likelihoods with Gaussian\n  process emulated MCMC", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.CO stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present an efficient approach for doing approximate Bayesian inference\nwhen only a limited number of noisy likelihood evaluations can be obtained due\nto computational constraints, which is becoming increasingly common for\napplications of complex models. Our main methodological innovation is to model\nthe log-likelihood function using a Gaussian process (GP) in a local fashion\nand apply this model to emulate the progression that an exact\nMetropolis-Hastings (MH) algorithm would take if it was applicable. New\nlog-likelihood evaluation locations are selected using sequential experimental\ndesign strategies such that each MH accept/reject decision is done within a\npre-specified error tolerance. The resulting approach is conceptually simple\nand sample-efficient as it takes full advantage of the GP model. It is also\nmore robust to violations of GP modelling assumptions and better suited for the\ntypical situation where the posterior is substantially more concentrated than\nthe prior, compared with various existing inference methods based on global GP\nsurrogate modelling. We discuss the probabilistic interpretations and central\ntheoretical aspects of our approach, and we then demonstrate the benefits of\nthe resulting algorithm in the context of likelihood-free inference for\nsimulator-based statistical models.\n", "versions": [{"version": "v1", "created": "Thu, 8 Apr 2021 17:38:02 GMT"}], "update_date": "2021-04-09", "authors_parsed": [["J\u00e4rvenp\u00e4\u00e4", "Marko", ""], ["Corander", "Jukka", ""]]}, {"id": "2104.03946", "submitter": "David Lindner", "authors": "David Lindner, Rohin Shah, Pieter Abbeel, Anca Dragan", "title": "Learning What To Do by Simulating the Past", "comments": "Presented at ICLR 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Since reward functions are hard to specify, recent work has focused on\nlearning policies from human feedback. However, such approaches are impeded by\nthe expense of acquiring such feedback. Recent work proposed that agents have\naccess to a source of information that is effectively free: in any environment\nthat humans have acted in, the state will already be optimized for human\npreferences, and thus an agent can extract information about what humans want\nfrom the state. Such learning is possible in principle, but requires simulating\nall possible past trajectories that could have led to the observed state. This\nis feasible in gridworlds, but how do we scale it to complex tasks? In this\nwork, we show that by combining a learned feature encoder with learned inverse\nmodels, we can enable agents to simulate human actions backwards in time to\ninfer what they must have done. The resulting algorithm is able to reproduce a\nspecific skill in MuJoCo environments given a single state sampled from the\noptimal policy for that skill.\n", "versions": [{"version": "v1", "created": "Thu, 8 Apr 2021 17:43:29 GMT"}, {"version": "v2", "created": "Mon, 3 May 2021 10:51:40 GMT"}], "update_date": "2021-05-04", "authors_parsed": [["Lindner", "David", ""], ["Shah", "Rohin", ""], ["Abbeel", "Pieter", ""], ["Dragan", "Anca", ""]]}, {"id": "2104.03966", "submitter": "Anne Sabourin", "authors": "St\\'ephan Cl\\'emen\\c{c}on and Hamid Jalalzai and Anne Sabourin and\n  Johan Segers", "title": "Concentration bounds for the empirical angular measure with statistical\n  learning applications", "comments": "30 pages (main paper), 15 pages (supplement), 3 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.ML stat.TH", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  The angular measure on the unit sphere characterizes the first-order\ndependence structure of the components of a random vector in extreme regions\nand is defined in terms of standardized margins. Its statistical recovery is an\nimportant step in learning problems involving observations far away from the\ncenter. In the common situation when the components of the vector have\ndifferent distributions, the rank transformation offers a convenient and robust\nway of standardizing data in order to build an empirical version of the angular\nmeasure based on the most extreme observations. However, the study of the\nsampling distribution of the resulting empirical angular measure is\nchallenging. It is the purpose of the paper to establish finite-sample bounds\nfor the maximal deviations between the empirical and true angular measures,\nuniformly over classes of Borel sets of controlled combinatorial complexity.\nThe bounds are valid with high probability and scale essentially as the square\nroot of the effective sample size, up to a logarithmic factor. Discarding the\nmost extreme observations yields a truncated version of the empirical angular\nmeasure for which the logarithmic factor in the concentration bound is replaced\nby a factor depending on the truncation level. The bounds are applied to\nprovide performance guarantees for two statistical learning procedures tailored\nto extreme regions of the input space and built upon the empirical angular\nmeasure: binary classification in extreme regions through empirical risk\nminimization and unsupervised anomaly detection through minimum-volume sets of\nthe sphere.\n", "versions": [{"version": "v1", "created": "Wed, 7 Apr 2021 18:41:29 GMT"}], "update_date": "2021-04-12", "authors_parsed": [["Cl\u00e9men\u00e7on", "St\u00e9phan", ""], ["Jalalzai", "Hamid", ""], ["Sabourin", "Anne", ""], ["Segers", "Johan", ""]]}, {"id": "2104.03986", "submitter": "Arjit Jain", "authors": "Arjit Jain, Sunita Sarawagi, Prithviraj Sen", "title": "Deep Indexed Active Learning for Matching Heterogeneous Entity\n  Representations", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB cs.AI cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Given two large lists of records, the task in entity resolution (ER) is to\nfind the pairs from the Cartesian product of the lists that correspond to the\nsame real world entity. Typically, passive learning methods on tasks like ER\nrequire large amounts of labeled data to yield useful models. Active Learning\nis a promising approach for ER in low resource settings. However, the search\nspace, to find informative samples for the user to label, grows quadratically\nfor instance-pair tasks making active learning hard to scale. Previous works,\nin this setting, rely on hand-crafted predicates, pre-trained language model\nembeddings, or rule learning to prune away unlikely pairs from the Cartesian\nproduct. This blocking step can miss out on important regions in the product\nspace leading to low recall. We propose DIAL, a scalable active learning\napproach that jointly learns embeddings to maximize recall for blocking and\naccuracy for matching blocked pairs. DIAL uses an Index-By-Committee framework,\nwhere each committee member learns representations based on powerful\ntransformer models. We highlight surprising differences between the matcher and\nthe blocker in the creation of the training data and the objective used to\ntrain their parameters. Experiments on five benchmark datasets and a\nmultilingual record matching dataset show the effectiveness of our approach in\nterms of precision, recall and running time. Code is available at\nhttps://github.com/ArjitJ/DIAL\n", "versions": [{"version": "v1", "created": "Thu, 8 Apr 2021 18:00:19 GMT"}], "update_date": "2021-04-12", "authors_parsed": [["Jain", "Arjit", ""], ["Sarawagi", "Sunita", ""], ["Sen", "Prithviraj", ""]]}, {"id": "2104.04046", "submitter": "Geoffrey McLachlan", "authors": "Daniel Ahfock, Geoffrey J. McLachlan", "title": "Semi-Supervised Learning of Classifiers from a Statistical Perspective:\n  A Brief Review", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  There has been increasing attention to semi-supervised learning (SSL)\napproaches in machine learning to forming a classifier in situations where the\ntraining data for a classifier consists of a limited number of classified\nobservations but a much larger number of unclassified observations. This is\nbecause the procurement of classified data can be quite costly due to high\nacquisition costs and subsequent financial, time, and ethical issues that can\narise in attempts to provide the true class labels for the unclassified data\nthat have been acquired. We provide here a review of statistical SSL approaches\nto this problem, focussing on the recent result that a classifier formed from a\npartially classified sample can actually have smaller expected error rate than\nthat if the sample were completely classified.\n", "versions": [{"version": "v1", "created": "Thu, 8 Apr 2021 20:41:57 GMT"}, {"version": "v2", "created": "Tue, 13 Apr 2021 03:36:15 GMT"}, {"version": "v3", "created": "Mon, 19 Jul 2021 03:08:08 GMT"}], "update_date": "2021-07-20", "authors_parsed": [["Ahfock", "Daniel", ""], ["McLachlan", "Geoffrey J.", ""]]}, {"id": "2104.04047", "submitter": "Mingao Yuan", "authors": "Mingao Yuan and Zuofeng Shang", "title": "Heterogeneous Dense Subhypergraph Detection", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG math.ST stat.TH", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  We study the problem of testing the existence of a heterogeneous dense\nsubhypergraph. The null hypothesis corresponds to a heterogeneous\nErd\\\"{o}s-R\\'{e}nyi uniform random hypergraph and the alternative hypothesis\ncorresponds to a heterogeneous uniform random hypergraph that contains a dense\nsubhypergraph. We establish detection boundaries when the edge probabilities\nare known and construct an asymptotically powerful test for distinguishing the\nhypotheses. We also construct an adaptive test which does not involve edge\nprobabilities, and hence, is more practically useful.\n", "versions": [{"version": "v1", "created": "Thu, 8 Apr 2021 20:44:22 GMT"}], "update_date": "2021-04-12", "authors_parsed": [["Yuan", "Mingao", ""], ["Shang", "Zuofeng", ""]]}, {"id": "2104.04103", "submitter": "Carlos Fern\\'andez-Lor\\'ia", "authors": "Carlos Fern\\'andez-Lor\\'ia and Foster Provost", "title": "Causal Decision Making and Causal Effect Estimation Are Not the Same...\n  and Why It Matters", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Causal decision making (CDM) at scale has become a routine part of business,\nand increasingly CDM is based on machine learning algorithms. For example,\nbusinesses often target offers, incentives, and recommendations with the goal\nof affecting consumer behavior. Recently, we have seen an acceleration of\nresearch related to CDM and to causal effect estimation (CEE) using machine\nlearned models. This article highlights an important perspective: CDM is not\nthe same as CEE, and counterintuitively, accurate CEE is not necessary for\naccurate CDM. Our experience is that this is not well understood by\npractitioners nor by most researchers. Technically, the estimand of interest is\ndifferent, and this has important implications both for modeling and for the\nuse of statistical models for CDM. We draw on recent research to highlight\nthree of these implications. (1) We should carefully consider the objective\nfunction of the causal machine learning, and if possible, we should optimize\nfor accurate \"treatment assignment\" rather than for accurate effect-size\nestimation. (2) Confounding does not have the same effect on CDM as it does on\nCEE. The upshot here is that for supporting CDM it may be just as good to learn\nwith confounded data as with unconfounded data. Finally, (3) causal statistical\nmodeling may not be necessary at all to support CDM, because there may be (and\nperhaps often is) a proxy target for statistical modeling that can do as well\nor better. This observation helps to explain at least one broad common CDM\npractice that seems \"wrong\" at first blush: the widespread use of non-causal\nmodels for targeting interventions. Our perspective is that these observations\nopen up substantial fertile ground for future research. Whether or not you\nshare our perspective completely, we hope we facilitate future research in this\narea by pointing to related articles from multiple contributing fields.\n", "versions": [{"version": "v1", "created": "Thu, 8 Apr 2021 22:50:54 GMT"}], "update_date": "2021-04-12", "authors_parsed": [["Fern\u00e1ndez-Lor\u00eda", "Carlos", ""], ["Provost", "Foster", ""]]}, {"id": "2104.04244", "submitter": "Konstantin Donhauser", "authors": "Konstantin Donhauser, Mingqi Wu and Fanny Yang", "title": "How rotational invariance of common kernels prevents generalization in\n  high dimensions", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST cs.LG stat.ML stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Kernel ridge regression is well-known to achieve minimax optimal rates in\nlow-dimensional settings. However, its behavior in high dimensions is much less\nunderstood. Recent work establishes consistency for kernel regression under\ncertain assumptions on the ground truth function and the distribution of the\ninput data. In this paper, we show that the rotational invariance property of\ncommonly studied kernels (such as RBF, inner product kernels and\nfully-connected NTK of any depth) induces a bias towards low-degree polynomials\nin high dimensions. Our result implies a lower bound on the generalization\nerror for a wide range of distributions and various choices of the scaling for\nkernels with different eigenvalue decays. This lower bound suggests that\ngeneral consistency results for kernel ridge regression in high dimensions\nrequire a more refined analysis that depends on the structure of the kernel\nbeyond its eigenvalue decay.\n", "versions": [{"version": "v1", "created": "Fri, 9 Apr 2021 08:27:37 GMT"}], "update_date": "2021-04-12", "authors_parsed": [["Donhauser", "Konstantin", ""], ["Wu", "Mingqi", ""], ["Yang", "Fanny", ""]]}, {"id": "2104.04258", "submitter": "Tim Pearce", "authors": "Tim Pearce, Jun Zhu", "title": "Counter-Strike Deathmatch with Large-Scale Behavioural Cloning", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper describes an AI agent that plays the popular first-person-shooter\n(FPS) video game `Counter-Strike; Global Offensive' (CSGO) from pixel input.\nThe agent, a deep neural network, matches the performance of the medium\ndifficulty built-in AI on the deathmatch game mode, whilst adopting a humanlike\nplay style. Unlike much prior work in games, no API is available for CSGO, so\nalgorithms must train and run in real-time. This limits the quantity of\non-policy data that can be generated, precluding many reinforcement learning\nalgorithms. Our solution uses behavioural cloning - training on a large noisy\ndataset scraped from human play on online servers (4 million frames, comparable\nin size to ImageNet), and a smaller dataset of high-quality expert\ndemonstrations. This scale is an order of magnitude larger than prior work on\nimitation learning in FPS games.\n", "versions": [{"version": "v1", "created": "Fri, 9 Apr 2021 09:12:12 GMT"}], "update_date": "2021-04-12", "authors_parsed": [["Pearce", "Tim", ""], ["Zhu", "Jun", ""]]}, {"id": "2104.04295", "submitter": "Alexander Brenning", "authors": "Alexander Brenning", "title": "Transforming Feature Space to Interpret Machine Learning Models", "comments": "13 pages, 7 figures, 1 table", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://creativecommons.org/licenses/by-sa/4.0/", "abstract": "  Model-agnostic tools for interpreting machine-learning models struggle to\nsummarize the joint effects of strongly dependent features in high-dimensional\nfeature spaces, which play an important role in pattern recognition, for\nexample in remote sensing of landcover. This contribution proposes a novel\napproach that interprets machine-learning models through the lens of feature\nspace transformations. It can be used to enhance unconditional as well as\nconditional post-hoc diagnostic tools including partial dependence plots,\naccumulated local effects plots, or permutation feature importance assessments.\nWhile the approach can also be applied to nonlinear transformations, we focus\non linear ones, including principal component analysis (PCA) and a partial\northogonalization technique. Structured PCA and diagnostics along paths offer\nopportunities for representing domain knowledge. The new approach is\nimplemented in the R package `wiml`, which can be combined with existing\nexplainable machine-learning packages. A case study on remote-sensing landcover\nclassification with 46 features is used to demonstrate the potential of the\nproposed approach for model interpretation by domain experts.\n", "versions": [{"version": "v1", "created": "Fri, 9 Apr 2021 10:48:11 GMT"}], "update_date": "2021-04-12", "authors_parsed": [["Brenning", "Alexander", ""]]}, {"id": "2104.04448", "submitter": "David Stutz", "authors": "David Stutz, Matthias Hein, Bernt Schiele", "title": "Relating Adversarially Robust Generalization to Flat Minima", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Adversarial training (AT) has become the de-facto standard to obtain models\nrobust against adversarial examples. However, AT exhibits severe robust\noverfitting: cross-entropy loss on adversarial examples, so-called robust loss,\ndecreases continuously on training examples, while eventually increasing on\ntest examples. In practice, this leads to poor robust generalization, i.e.,\nadversarial robustness does not generalize well to new examples. In this paper,\nwe study the relationship between robust generalization and flatness of the\nrobust loss landscape in weight space, i.e., whether robust loss changes\nsignificantly when perturbing weights. To this end, we propose average- and\nworst-case metrics to measure flatness in the robust loss landscape and show a\ncorrelation between good robust generalization and flatness. For example,\nthroughout training, flatness reduces significantly during overfitting such\nthat early stopping effectively finds flatter minima in the robust loss\nlandscape. Similarly, AT variants achieving higher adversarial robustness also\ncorrespond to flatter minima. This holds for many popular choices, e.g.,\nAT-AWP, TRADES, MART, AT with self-supervision or additional unlabeled\nexamples, as well as simple regularization techniques, e.g., AutoAugment,\nweight decay or label noise. For fair comparison across these approaches, our\nflatness measures are specifically designed to be scale-invariant and we\nconduct extensive experiments to validate our findings.\n", "versions": [{"version": "v1", "created": "Fri, 9 Apr 2021 15:55:01 GMT"}], "update_date": "2021-04-12", "authors_parsed": [["Stutz", "David", ""], ["Hein", "Matthias", ""], ["Schiele", "Bernt", ""]]}, {"id": "2104.04457", "submitter": "Kevin Yang", "authors": "Zachary Wu, Kadina E. Johnston, Frances H. Arnold, Kevin K. Yang", "title": "Protein sequence design with deep generative models", "comments": "11 pages, 2 figures", "journal-ref": null, "doi": "10.1016/j.cbpa.2021.04.004", "report-no": null, "categories": "q-bio.QM cs.LG q-bio.BM stat.ML", "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/", "abstract": "  Protein engineering seeks to identify protein sequences with optimized\nproperties. When guided by machine learning, protein sequence generation\nmethods can draw on prior knowledge and experimental efforts to improve this\nprocess. In this review, we highlight recent applications of machine learning\nto generate protein sequences, focusing on the emerging field of deep\ngenerative methods.\n", "versions": [{"version": "v1", "created": "Fri, 9 Apr 2021 16:08:15 GMT"}], "update_date": "2021-05-28", "authors_parsed": [["Wu", "Zachary", ""], ["Johnston", "Kadina E.", ""], ["Arnold", "Frances H.", ""], ["Yang", "Kevin K.", ""]]}, {"id": "2104.04610", "submitter": "Vincent Le-Guen", "authors": "Vincent Le Guen, Nicolas Thome", "title": "Deep Time Series Forecasting with Shape and Temporal Criteria", "comments": "arXiv admin note: text overlap with arXiv:2010.07349", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.AI cs.LG", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  This paper addresses the problem of multi-step time series forecasting for\nnon-stationary signals that can present sudden changes. Current\nstate-of-the-art deep learning forecasting methods, often trained with variants\nof the MSE, lack the ability to provide sharp predictions in deterministic and\nprobabilistic contexts. To handle these challenges, we propose to incorporate\nshape and temporal criteria in the training objective of deep models. We define\nshape and temporal similarities and dissimilarities, based on a smooth\nrelaxation of Dynamic Time Warping (DTW) and Temporal Distortion Index (TDI),\nthat enable to build differentiable loss functions and positive semi-definite\n(PSD) kernels. With these tools, we introduce DILATE (DIstortion Loss including\nshApe and TimE), a new objective for deterministic forecasting, that explicitly\nincorporates two terms supporting precise shape and temporal change detection.\nFor probabilistic forecasting, we introduce STRIPE++ (Shape and Time diverRsIty\nin Probabilistic forEcasting), a framework for providing a set of sharp and\ndiverse forecasts, where the structured shape and time diversity is enforced\nwith a determinantal point process (DPP) diversity loss. Extensive experiments\nand ablations studies on synthetic and real-world datasets confirm the benefits\nof leveraging shape and time features in time series forecasting.\n", "versions": [{"version": "v1", "created": "Fri, 9 Apr 2021 21:24:33 GMT"}], "update_date": "2021-04-13", "authors_parsed": [["Guen", "Vincent Le", ""], ["Thome", "Nicolas", ""]]}, {"id": "2104.04676", "submitter": "Xutan Peng", "authors": "Xutan Peng, Guanyi Chen, Chenghua Lin, Mark Stevenson", "title": "Highly Efficient Knowledge Graph Embedding Learning with Orthogonal\n  Procrustes Analysis", "comments": "To appear at NAACL 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI cs.CL stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Knowledge Graph Embeddings (KGEs) have been intensively explored in recent\nyears due to their promise for a wide range of applications. However, existing\nstudies focus on improving the final model performance without acknowledging\nthe computational cost of the proposed approaches, in terms of execution time\nand environmental impact. This paper proposes a simple yet effective KGE\nframework which can reduce the training time and carbon footprint by orders of\nmagnitudes compared with state-of-the-art approaches, while producing\ncompetitive performance. We highlight three technical innovations: full batch\nlearning via relational matrices, closed-form Orthogonal Procrustes Analysis\nfor KGEs, and non-negative-sampling training. In addition, as the first KGE\nmethod whose entity embeddings also store full relation information, our\ntrained models encode rich semantics and are highly interpretable.\nComprehensive experiments and ablation studies involving 13 strong baselines\nand two standard datasets verify the effectiveness and efficiency of our\nalgorithm.\n", "versions": [{"version": "v1", "created": "Sat, 10 Apr 2021 03:55:45 GMT"}, {"version": "v2", "created": "Sat, 17 Apr 2021 12:17:05 GMT"}], "update_date": "2021-04-20", "authors_parsed": [["Peng", "Xutan", ""], ["Chen", "Guanyi", ""], ["Lin", "Chenghua", ""], ["Stevenson", "Mark", ""]]}, {"id": "2104.04679", "submitter": "Ken Kobayashi", "authors": "Akinori Tanaka, Akiyoshi Sannai, Ken Kobayashi, and Naoki Hamada", "title": "Approximate Bayesian Computation of B\\'ezier Simplices", "comments": null, "journal-ref": null, "doi": null, "report-no": "RIKEN-iTHEMS-Report-21", "categories": "cs.LG stat.ML", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  B\\'ezier simplex fitting algorithms have been recently proposed to\napproximate the Pareto set/front of multi-objective continuous optimization\nproblems. These new methods have shown to be successful at approximating\nvarious shapes of Pareto sets/fronts when sample points exactly lie on the\nPareto set/front. However, if the sample points scatter away from the Pareto\nset/front, those methods often likely suffer from over-fitting. To overcome\nthis issue, in this paper, we extend the B\\'ezier simplex model to a\nprobabilistic one and propose a new learning algorithm of it, which falls into\nthe framework of approximate Bayesian computation (ABC) based on the\nWasserstein distance. We also study the convergence property of the Wasserstein\nABC algorithm. An extensive experimental evaluation on publicly available\nproblem instances shows that the new algorithm converges on a finite sample.\nMoreover, it outperforms the deterministic fitting methods on noisy instances.\n", "versions": [{"version": "v1", "created": "Sat, 10 Apr 2021 04:20:19 GMT"}, {"version": "v2", "created": "Tue, 13 Apr 2021 01:44:44 GMT"}], "update_date": "2021-04-14", "authors_parsed": [["Tanaka", "Akinori", ""], ["Sannai", "Akiyoshi", ""], ["Kobayashi", "Ken", ""], ["Hamada", "Naoki", ""]]}, {"id": "2104.04710", "submitter": "Claudio Gallicchio", "authors": "Filippo Maria Bianchi, Claudio Gallicchio, Alessio Micheli", "title": "Pyramidal Reservoir Graph Neural Network", "comments": "this is a pre-print version of a paper submitted for journal\n  publication", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.NE stat.ML", "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/", "abstract": "  We propose a deep Graph Neural Network (GNN) model that alternates two types\nof layers. The first type is inspired by Reservoir Computing (RC) and generates\nnew vertex features by iterating a non-linear map until it converges to a fixed\npoint. The second type of layer implements graph pooling operations, that\ngradually reduce the support graph and the vertex features, and further improve\nthe computational efficiency of the RC-based GNN. The architecture is,\ntherefore, pyramidal. In the last layer, the features of the remaining vertices\nare combined into a single vector, which represents the graph embedding.\nThrough a mathematical derivation introduced in this paper, we show formally\nhow graph pooling can reduce the computational complexity of the model and\nspeed-up the convergence of the dynamical updates of the vertex features. Our\nproposed approach to the design of RC-based GNNs offers an advantageous and\nprincipled trade-off between accuracy and complexity, which we extensively\ndemonstrate in experiments on a large set of graph datasets.\n", "versions": [{"version": "v1", "created": "Sat, 10 Apr 2021 08:34:09 GMT"}], "update_date": "2021-04-13", "authors_parsed": [["Bianchi", "Filippo Maria", ""], ["Gallicchio", "Claudio", ""], ["Micheli", "Alessio", ""]]}, {"id": "2104.04714", "submitter": "Chuanhou Gao", "authors": "Qiuqiang Lin, Chuanhou Gao", "title": "Random Intersection Chains", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Interactions between several features sometimes play an important role in\nprediction tasks. But taking all the interactions into consideration will lead\nto an extremely heavy computational burden. For categorical features, the\nsituation is more complicated since the input will be extremely\nhigh-dimensional and sparse if one-hot encoding is applied. Inspired by\nassociation rule mining, we propose a method that selects interactions of\ncategorical features, called Random Intersection Chains. It uses random\nintersections to detect frequent patterns, then selects the most meaningful\nones among them. At first a number of chains are generated, in which each node\nis the intersection of the previous node and a random chosen observation. The\nfrequency of patterns in the tail nodes is estimated by maximum likelihood\nestimation, then the patterns with largest estimated frequency are selected.\nAfter that, their confidence is calculated by Bayes' theorem. The most\nconfident patterns are finally returned by Random Intersection Chains. We show\nthat if the number and length of chains are appropriately chosen, the patterns\nin the tail nodes are indeed the most frequent ones in the data set. We analyze\nthe computation complexity of the proposed algorithm and prove the convergence\nof the estimators. The results of a series of experiments verify the efficiency\nand effectiveness of the algorithm.\n", "versions": [{"version": "v1", "created": "Sat, 10 Apr 2021 08:41:15 GMT"}], "update_date": "2021-04-13", "authors_parsed": [["Lin", "Qiuqiang", ""], ["Gao", "Chuanhou", ""]]}, {"id": "2104.04781", "submitter": "Sankeerth Rao Karingula", "authors": "Sankeerth Rao Karingula and Nandini Ramanan and Rasool Tahmasbi and\n  Mehrnaz Amjadi and Deokwoo Jung and Ricky Si and Charanraj Thimmisetty and\n  Luisa Polania Cabrera and Marjorie Sayer and Claudionor Nunes Coelho Jr", "title": "Boosted Embeddings for Time Series Forecasting", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI stat.ML", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Time series forecasting is a fundamental task emerging from diverse\ndata-driven applications. Many advanced autoregressive methods such as ARIMA\nwere used to develop forecasting models. Recently, deep learning based methods\nsuch as DeepAr, NeuralProphet, Seq2Seq have been explored for time series\nforecasting problem. In this paper, we propose a novel time series forecast\nmodel, DeepGB. We formulate and implement a variant of Gradient boosting\nwherein the weak learners are DNNs whose weights are incrementally found in a\ngreedy manner over iterations. In particular, we develop a new embedding\narchitecture that improves the performance of many deep learning models on time\nseries using Gradient boosting variant. We demonstrate that our model\noutperforms existing comparable state-of-the-art models using real-world sensor\ndata and public dataset.\n", "versions": [{"version": "v1", "created": "Sat, 10 Apr 2021 14:38:11 GMT"}, {"version": "v2", "created": "Sun, 11 Jul 2021 17:45:20 GMT"}], "update_date": "2021-07-13", "authors_parsed": [["Karingula", "Sankeerth Rao", ""], ["Ramanan", "Nandini", ""], ["Tahmasbi", "Rasool", ""], ["Amjadi", "Mehrnaz", ""], ["Jung", "Deokwoo", ""], ["Si", "Ricky", ""], ["Thimmisetty", "Charanraj", ""], ["Cabrera", "Luisa Polania", ""], ["Sayer", "Marjorie", ""], ["Coelho", "Claudionor Nunes", "Jr"]]}, {"id": "2104.04790", "submitter": "Tinkle Chugh", "authors": "Clym Stock-Williams, Tinkle Chugh, Alma Rahat, Wei Yu", "title": "What Makes an Effective Scalarising Function for Multi-Objective\n  Bayesian Optimisation?", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG math.OC stat.ML", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Performing multi-objective Bayesian optimisation by scalarising the\nobjectives avoids the computation of expensive multi-dimensional integral-based\nacquisition functions, instead of allowing one-dimensional standard acquisition\nfunctions\\textemdash such as Expected Improvement\\textemdash to be applied.\nHere, two infill criteria based on hypervolume improvement\\textemdash one\nrecently introduced and one novel\\textemdash are compared with the\nmulti-surrogate Expected Hypervolume Improvement. The reasons for the\ndisparities in these methods' effectiveness in maximising the hypervolume of\nthe acquired Pareto Front are investigated. In addition, the effect of the\nsurrogate model mean function on exploration and exploitation is examined:\ncareful choice of data normalisation is shown to be preferable to the\nexploration parameter commonly used with the Expected Improvement acquisition\nfunction. Finally, the effectiveness of all the methodological improvements\ndefined here is demonstrated on a real-world problem: the optimisation of a\nwind turbine blade aerofoil for both aerodynamic performance and structural\nstiffness. With effective scalarisation, Bayesian optimisation finds a large\nnumber of new aerofoil shapes that strongly dominate standard designs.\n", "versions": [{"version": "v1", "created": "Sat, 10 Apr 2021 15:18:58 GMT"}], "update_date": "2021-04-13", "authors_parsed": [["Stock-Williams", "Clym", ""], ["Chugh", "Tinkle", ""], ["Rahat", "Alma", ""], ["Yu", "Wei", ""]]}, {"id": "2104.04874", "submitter": "Dan Roberts", "authors": "Daniel A. Roberts", "title": "SGD Implicitly Regularizes Generalization Error", "comments": "First appeared at the \"Workshop on Integration of Deep Learning\n  Theories\" at NeurIPS in 2018 and has been available since then at\n  https://research.fb.com/publications/sgd-implicitly-regularizes-generalization-error/", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We derive a simple and model-independent formula for the change in the\ngeneralization gap due to a gradient descent update. We then compare the change\nin the test error for stochastic gradient descent to the change in test error\nfrom an equivalent number of gradient descent updates and show explicitly that\nstochastic gradient descent acts to regularize generalization error by\ndecorrelating nearby updates. These calculations depends on the details of the\nmodel only through the mean and covariance of the gradient distribution, which\nmay be readily measured for particular models of interest. We discuss further\nimprovements to these calculations and comment on possible implications for\nstochastic optimization.\n", "versions": [{"version": "v1", "created": "Sat, 10 Apr 2021 23:10:14 GMT"}], "update_date": "2021-04-13", "authors_parsed": [["Roberts", "Daniel A.", ""]]}, {"id": "2104.04916", "submitter": "Xutan Peng", "authors": "Xutan Peng, Chenghua Lin, Mark Stevenson", "title": "Cross-Lingual Word Embedding Refinement by $\\ell_{1}$ Norm Optimisation", "comments": "To appear at NAACL 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Cross-Lingual Word Embeddings (CLWEs) encode words from two or more languages\nin a shared high-dimensional space in which vectors representing words with\nsimilar meaning (regardless of language) are closely located. Existing methods\nfor building high-quality CLWEs learn mappings that minimise the $\\ell_{2}$\nnorm loss function. However, this optimisation objective has been demonstrated\nto be sensitive to outliers. Based on the more robust Manhattan norm (aka.\n$\\ell_{1}$ norm) goodness-of-fit criterion, this paper proposes a simple\npost-processing step to improve CLWEs. An advantage of this approach is that it\nis fully agnostic to the training process of the original CLWEs and can\ntherefore be applied widely. Extensive experiments are performed involving ten\ndiverse languages and embeddings trained on different corpora. Evaluation\nresults based on bilingual lexicon induction and cross-lingual transfer for\nnatural language inference tasks show that the $\\ell_{1}$ refinement\nsubstantially outperforms four state-of-the-art baselines in both supervised\nand unsupervised settings. It is therefore recommended that this strategy be\nadopted as a standard for CLWE methods.\n", "versions": [{"version": "v1", "created": "Sun, 11 Apr 2021 04:37:54 GMT"}], "update_date": "2021-04-27", "authors_parsed": [["Peng", "Xutan", ""], ["Lin", "Chenghua", ""], ["Stevenson", "Mark", ""]]}, {"id": "2104.04975", "submitter": "Alexander Immer", "authors": "Alexander Immer, Matthias Bauer, Vincent Fortuin, Gunnar R\\\"atsch,\n  Mohammad Emtiyaz Khan", "title": "Scalable Marginal Likelihood Estimation for Model Selection in Deep\n  Learning", "comments": "ICML 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Marginal-likelihood based model-selection, even though promising, is rarely\nused in deep learning due to estimation difficulties. Instead, most approaches\nrely on validation data, which may not be readily available. In this work, we\npresent a scalable marginal-likelihood estimation method to select both\nhyperparameters and network architectures, based on the training data alone.\nSome hyperparameters can be estimated online during training, simplifying the\nprocedure. Our marginal-likelihood estimate is based on Laplace's method and\nGauss-Newton approximations to the Hessian, and it outperforms cross-validation\nand manual-tuning on standard regression and image classification datasets,\nespecially in terms of calibration and out-of-distribution detection. Our work\nshows that marginal likelihoods can improve generalization and be useful when\nvalidation data is unavailable (e.g., in nonstationary settings).\n", "versions": [{"version": "v1", "created": "Sun, 11 Apr 2021 09:50:24 GMT"}, {"version": "v2", "created": "Tue, 11 May 2021 13:17:14 GMT"}, {"version": "v3", "created": "Tue, 15 Jun 2021 10:50:22 GMT"}], "update_date": "2021-06-16", "authors_parsed": [["Immer", "Alexander", ""], ["Bauer", "Matthias", ""], ["Fortuin", "Vincent", ""], ["R\u00e4tsch", "Gunnar", ""], ["Khan", "Mohammad Emtiyaz", ""]]}, {"id": "2104.04999", "submitter": "Huong Ha", "authors": "Huong Ha, Sunil Gupta, Santu Rana, Svetha Venkatesh", "title": "ALT-MAS: A Data-Efficient Framework for Active Testing of Machine\n  Learning Algorithms", "comments": "Accepted to the RobustML workshop at ICLR 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI cs.SE stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Machine learning models are being used extensively in many important areas,\nbut there is no guarantee a model will always perform well or as its developers\nintended. Understanding the correctness of a model is crucial to prevent\npotential failures that may have significant detrimental impact in critical\napplication areas. In this paper, we propose a novel framework to efficiently\ntest a machine learning model using only a small amount of labeled test data.\nThe idea is to estimate the metrics of interest for a model-under-test using\nBayesian neural network (BNN). We develop a novel data augmentation method\nhelping to train the BNN to achieve high accuracy. We also devise a theoretic\ninformation based sampling strategy to sample data points so as to achieve\naccurate estimations for the metrics of interest. Finally, we conduct an\nextensive set of experiments to test various machine learning models for\ndifferent types of metrics. Our experiments show that the metrics estimations\nby our method are significantly better than existing baselines.\n", "versions": [{"version": "v1", "created": "Sun, 11 Apr 2021 12:14:04 GMT"}], "update_date": "2021-04-13", "authors_parsed": [["Ha", "Huong", ""], ["Gupta", "Sunil", ""], ["Rana", "Santu", ""], ["Venkatesh", "Svetha", ""]]}, {"id": "2104.05000", "submitter": "Samuel Gerber", "authors": "Samuel Gerber", "title": "Saddlepoints in Unsupervised Least Squares", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  This paper sheds light on the risk landscape of unsupervised least squares in\nthe context of deep auto-encoding neural nets. We formally establish an\nequivalence between unsupervised least squares and principal manifolds. This\nlink provides insight into the risk landscape of auto--encoding under the mean\nsquared error, in particular all non-trivial critical points are saddlepoints.\nFinding saddlepoints is in itself difficult, overcomplete auto-encoding poses\nthe additional challenge that the saddlepoints are degenerate. Within this\ncontext we discuss regularization of auto-encoders, in particular bottleneck,\ndenoising and contraction auto-encoding and propose a new optimization strategy\nthat can be framed as particular form of contractive regularization.\n", "versions": [{"version": "v1", "created": "Sun, 11 Apr 2021 12:15:36 GMT"}], "update_date": "2021-04-13", "authors_parsed": [["Gerber", "Samuel", ""]]}, {"id": "2104.05021", "submitter": "Soham Sarkar", "authors": "Soham Sarkar and Victor M. Panaretos", "title": "CovNet: Covariance Networks for Functional Data on Multidimensional\n  Domains", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME math.ST stat.ML stat.TH", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Covariance estimation is ubiquitous in functional data analysis. Yet, the\ncase of functional observations over multidimensional domains introduces\ncomputational and statistical challenges, rendering the standard methods\neffectively inapplicable. To address this problem, we introduce Covariance\nNetworks (CovNet) as a modeling and estimation tool. The CovNet model is\nuniversal -- it can be used to approximate any covariance up to desired\nprecision. Moreover, the model can be fitted efficiently to the data and its\nneural network architecture allows us to employ modern computational tools in\nthe implementation. The CovNet model also admits a closed-form\neigen-decomposition, which can be computed efficiently, without constructing\nthe covariance itself. This facilitates easy storage and subsequent\nmanipulation in the context of the CovNet. Moreover, we establish consistency\nof the proposed estimator and derive its rate of convergence. The usefulness of\nthe proposed method is demonstrated by means of an extensive simulation study.\n", "versions": [{"version": "v1", "created": "Sun, 11 Apr 2021 14:40:41 GMT"}], "update_date": "2021-04-13", "authors_parsed": [["Sarkar", "Soham", ""], ["Panaretos", "Victor M.", ""]]}, {"id": "2104.05048", "submitter": "Konstantinos Makantasis", "authors": "Konstantinos Makantasis, Alexandros Georgogiannis, Athanasios\n  Voulodimos, Ioannis Georgoulas, Anastasios Doulamis, Nikolaos Doulamis", "title": "Rank-R FNN: A Tensor-Based Learning Model for High-Order Data\n  Classification", "comments": "12 pages, 5 figures, 4 tables, Accepted for publication to IEEE\n  Access", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.NE stat.ML", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  An increasing number of emerging applications in data science and engineering\nare based on multidimensional and structurally rich data. The irregularities,\nhowever, of high-dimensional data often compromise the effectiveness of\nstandard machine learning algorithms. We hereby propose the Rank-R Feedforward\nNeural Network (FNN), a tensor-based nonlinear learning model that imposes\nCanonical/Polyadic decomposition on its parameters, thereby offering two core\nadvantages compared to typical machine learning methods. First, it handles\ninputs as multilinear arrays, bypassing the need for vectorization, and can\nthus fully exploit the structural information along every data dimension.\nMoreover, the number of the model's trainable parameters is substantially\nreduced, making it very efficient for small sample setting problems. We\nestablish the universal approximation and learnability properties of Rank-R\nFNN, and we validate its performance on real-world hyperspectral datasets.\nExperimental evaluations show that Rank-R FNN is a computationally inexpensive\nalternative of ordinary FNN that achieves state-of-the-art performance on\nhigher-order tensor data.\n", "versions": [{"version": "v1", "created": "Sun, 11 Apr 2021 16:37:32 GMT"}], "update_date": "2021-04-13", "authors_parsed": [["Makantasis", "Konstantinos", ""], ["Georgogiannis", "Alexandros", ""], ["Voulodimos", "Athanasios", ""], ["Georgoulas", "Ioannis", ""], ["Doulamis", "Anastasios", ""], ["Doulamis", "Nikolaos", ""]]}, {"id": "2104.05076", "submitter": "Daoji Li", "authors": "Ruipeng Dong, Daoji Li, Zemin Zheng", "title": "Parallel integrative learning for large-scale multi-response regression\n  with incomplete outcomes", "comments": "32 pages", "journal-ref": "Computational Statistics and Data Analysis, 2021", "doi": "10.1016/j.csda.2021.107243", "report-no": null, "categories": "stat.ME stat.AP stat.CO stat.ML", "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/", "abstract": "  Multi-task learning is increasingly used to investigate the association\nstructure between multiple responses and a single set of predictor variables in\nmany applications. In the era of big data, the coexistence of incomplete\noutcomes, large number of responses, and high dimensionality in predictors\nposes unprecedented challenges in estimation, prediction, and computation. In\nthis paper, we propose a scalable and computationally efficient procedure,\ncalled PEER, for large-scale multi-response regression with incomplete\noutcomes, where both the numbers of responses and predictors can be\nhigh-dimensional. Motivated by sparse factor regression, we convert the\nmulti-response regression into a set of univariate-response regressions, which\ncan be efficiently implemented in parallel. Under some mild regularity\nconditions, we show that PEER enjoys nice sampling properties including\nconsistency in estimation, prediction, and variable selection. Extensive\nsimulation studies show that our proposal compares favorably with several\nexisting methods in estimation accuracy, variable selection, and computation\nefficiency.\n", "versions": [{"version": "v1", "created": "Sun, 11 Apr 2021 19:01:24 GMT"}], "update_date": "2021-04-13", "authors_parsed": [["Dong", "Ruipeng", ""], ["Li", "Daoji", ""], ["Zheng", "Zemin", ""]]}, {"id": "2104.05089", "submitter": "Bjorn Lutjens", "authors": "Salva R\\\"uhling Cachay, Emma Erickson, Arthur Fender C. Bucker, Ernest\n  Pokropek, Willa Potosnak, Suyash Bire, Salomey Osei, Bj\\\"orn L\\\"utjens", "title": "The World as a Graph: Improving El Ni\\~no Forecasts with Graph Neural\n  Networks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.NE physics.ao-ph stat.ML", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Deep learning-based models have recently outperformed state-of-the-art\nseasonal forecasting models, such as for predicting El Ni\\~no-Southern\nOscillation (ENSO). However, current deep learning models are based on\nconvolutional neural networks which are difficult to interpret and can fail to\nmodel large-scale atmospheric patterns. In comparison, graph neural networks\n(GNNs) are capable of modeling large-scale spatial dependencies and are more\ninterpretable due to the explicit modeling of information flow through edge\nconnections. We propose the first application of graph neural networks to\nseasonal forecasting. We design a novel graph connectivity learning module that\nenables our GNN model to learn large-scale spatial interactions jointly with\nthe actual ENSO forecasting task. Our model, \\graphino, outperforms\nstate-of-the-art deep learning-based models for forecasts up to six months\nahead. Additionally, we show that our model is more interpretable as it learns\nsensible connectivity structures that correlate with the ENSO anomaly pattern.\n", "versions": [{"version": "v1", "created": "Sun, 11 Apr 2021 19:55:55 GMT"}, {"version": "v2", "created": "Wed, 19 May 2021 20:57:30 GMT"}], "update_date": "2021-05-21", "authors_parsed": [["Cachay", "Salva R\u00fchling", ""], ["Erickson", "Emma", ""], ["Bucker", "Arthur Fender C.", ""], ["Pokropek", "Ernest", ""], ["Potosnak", "Willa", ""], ["Bire", "Suyash", ""], ["Osei", "Salomey", ""], ["L\u00fctjens", "Bj\u00f6rn", ""]]}, {"id": "2104.05097", "submitter": "Louis B\\'ethune", "authors": "Louis B\\'ethune, Alberto Gonz\\'alez-Sanz, Franck Mamalet, Mathieu\n  Serrurier", "title": "The Many Faces of 1-Lipschitz Neural Networks", "comments": "21 pages, 6 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Lipschitz constrained models have been used to solve specifics deep learning\nproblems such as the estimation of Wasserstein distance for GAN, or the\ntraining of neural networks robust to adversarial attacks. Regardless the novel\nand effective algorithms to build such 1-Lipschitz networks, their usage\nremains marginal, and they are commonly considered as less expressive and less\nable to fit properly the data than their unconstrained counterpart.\n  The goal of the paper is to demonstrate that, despite being empirically\nharder to train, 1-Lipschitz neural networks are theoretically better grounded\nthan unconstrained ones when it comes to classification. To achieve that we\nrecall some results about 1-Lipschitz function in the scope of deep learning\nand we extend and illustrate them to derive general properties for\nclassification.\n  First, we show that 1-Lipschitz neural network can fit arbitrarily difficult\nfrontier making them as expressive as classical ones. When minimizing the log\nloss, we prove that the optimization problem under Lipschitz constraint is well\nposed and have a minimum, whereas regular neural networks can diverge even on\nremarkably simple situations. Then, we study the link between classification\nwith 1-Lipschitz network and optimal transport thanks to regularized versions\nof Kantorovich-Rubinstein duality theory. Last, we derive preliminary bounds on\ntheir VC dimension.\n", "versions": [{"version": "v1", "created": "Sun, 11 Apr 2021 20:31:32 GMT"}, {"version": "v2", "created": "Tue, 13 Apr 2021 10:15:02 GMT"}, {"version": "v3", "created": "Tue, 18 May 2021 20:27:55 GMT"}, {"version": "v4", "created": "Thu, 27 May 2021 10:01:49 GMT"}], "update_date": "2021-05-28", "authors_parsed": [["B\u00e9thune", "Louis", ""], ["Gonz\u00e1lez-Sanz", "Alberto", ""], ["Mamalet", "Franck", ""], ["Serrurier", "Mathieu", ""]]}, {"id": "2104.05353", "submitter": "Can Bakiskan", "authors": "Can Bakiskan, Metehan Cekic, Ahmet Dundar Sezer, Upamanyu Madhow", "title": "Sparse Coding Frontend for Robust Neural Networks", "comments": "International Conference on Learning Representations (ICLR) 2021\n  Workshop on Security and Safety in Machine Learning Systems", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://creativecommons.org/licenses/by-sa/4.0/", "abstract": "  Deep Neural Networks are known to be vulnerable to small, adversarially\ncrafted, perturbations. The current most effective defense methods against\nthese adversarial attacks are variants of adversarial training. In this paper,\nwe introduce a radically different defense trained only on clean images: a\nsparse coding based frontend which significantly attenuates adversarial attacks\nbefore they reach the classifier. We evaluate our defense on CIFAR-10 dataset\nunder a wide range of attack types (including Linf , L2, and L1 bounded\nattacks), demonstrating its promise as a general-purpose approach for defense.\n", "versions": [{"version": "v1", "created": "Mon, 12 Apr 2021 11:14:32 GMT"}], "update_date": "2021-04-13", "authors_parsed": [["Bakiskan", "Can", ""], ["Cekic", "Metehan", ""], ["Sezer", "Ahmet Dundar", ""], ["Madhow", "Upamanyu", ""]]}, {"id": "2104.05441", "submitter": "Marcus Kaiser", "authors": "Marcus Kaiser, Maksim Sipos", "title": "Unsuitability of NOTEARS for Causal Graph Discovery", "comments": "6 pages, 4 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG math.ST stat.TH", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Causal Discovery methods aim to identify a DAG structure that represents\ncausal relationships from observational data. In this article, we stress that\nit is important to test such methods for robustness in practical settings. As\nour main example, we analyze the NOTEARS method, for which we demonstrate a\nlack of scale-invariance. We show that NOTEARS is a method that aims to\nidentify a parsimonious DAG from the data that explains the residual variance.\nWe conclude that NOTEARS is not suitable for identifying truly causal\nrelationships from the data.\n", "versions": [{"version": "v1", "created": "Mon, 12 Apr 2021 13:09:10 GMT"}, {"version": "v2", "created": "Tue, 15 Jun 2021 10:38:53 GMT"}], "update_date": "2021-06-16", "authors_parsed": [["Kaiser", "Marcus", ""], ["Sipos", "Maksim", ""]]}, {"id": "2104.05508", "submitter": "Grzegorz G{\\l}uch", "authors": "Grzegorz G{\\l}uch, R\\\"udiger Urbanke", "title": "Noether: The More Things Change, the More Stay the Same", "comments": "35 pages, 6 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Symmetries have proven to be important ingredients in the analysis of neural\nnetworks. So far their use has mostly been implicit or seemingly coincidental.\n  We undertake a systematic study of the role that symmetry plays. In\nparticular, we clarify how symmetry interacts with the learning algorithm. The\nkey ingredient in our study is played by Noether's celebrated theorem which,\ninformally speaking, states that symmetry leads to conserved quantities (e.g.,\nconservation of energy or conservation of momentum). In the realm of neural\nnetworks under gradient descent, model symmetries imply restrictions on the\ngradient path. E.g., we show that symmetry of activation functions leads to\nboundedness of weight matrices, for the specific case of linear activations it\nleads to balance equations of consecutive layers, data augmentation leads to\ngradient paths that have \"momentum\"-type restrictions, and time symmetry leads\nto a version of the Neural Tangent Kernel.\n  Symmetry alone does not specify the optimization path, but the more\nsymmetries are contained in the model the more restrictions are imposed on the\npath. Since symmetry also implies over-parametrization, this in effect implies\nthat some part of this over-parametrization is cancelled out by the existence\nof the conserved quantities.\n  Symmetry can therefore be thought of as one further important tool in\nunderstanding the performance of neural networks under gradient descent.\n", "versions": [{"version": "v1", "created": "Mon, 12 Apr 2021 14:41:05 GMT"}], "update_date": "2021-04-13", "authors_parsed": [["G\u0142uch", "Grzegorz", ""], ["Urbanke", "R\u00fcdiger", ""]]}, {"id": "2104.05514", "submitter": "Giannis Karamanolakis", "authors": "Giannis Karamanolakis, Subhabrata Mukherjee, Guoqing Zheng and Ahmed\n  Hassan Awadallah", "title": "Self-Training with Weak Supervision", "comments": "Accepted to NAACL 2021 (Long Paper)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL stat.ML", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  State-of-the-art deep neural networks require large-scale labeled training\ndata that is often expensive to obtain or not available for many tasks. Weak\nsupervision in the form of domain-specific rules has been shown to be useful in\nsuch settings to automatically generate weakly labeled training data. However,\nlearning with weak rules is challenging due to their inherent heuristic and\nnoisy nature. An additional challenge is rule coverage and overlap, where prior\nwork on weak supervision only considers instances that are covered by weak\nrules, thus leaving valuable unlabeled data behind.\n  In this work, we develop a weak supervision framework (ASTRA) that leverages\nall the available data for a given task. To this end, we leverage task-specific\nunlabeled data through self-training with a model (student) that considers\ncontextualized representations and predicts pseudo-labels for instances that\nmay not be covered by weak rules. We further develop a rule attention network\n(teacher) that learns how to aggregate student pseudo-labels with weak rule\nlabels, conditioned on their fidelity and the underlying context of an\ninstance. Finally, we construct a semi-supervised learning objective for\nend-to-end training with unlabeled data, domain-specific rules, and a small\namount of labeled data. Extensive experiments on six benchmark datasets for\ntext classification demonstrate the effectiveness of our approach with\nsignificant improvements over state-of-the-art baselines.\n", "versions": [{"version": "v1", "created": "Mon, 12 Apr 2021 14:45:04 GMT"}], "update_date": "2021-04-13", "authors_parsed": [["Karamanolakis", "Giannis", ""], ["Mukherjee", "Subhabrata", ""], ["Zheng", "Guoqing", ""], ["Awadallah", "Ahmed Hassan", ""]]}, {"id": "2104.05533", "submitter": "Maria A. Zuluaga", "authors": "Francesco Galati and Maria A. Zuluaga", "title": "Efficient Model Monitoring for Quality Control in Cardiac Image\n  Segmentation", "comments": "Accepted to the 11th Biennial Meeting on Functional Imaging and\n  Modeling of the Heart (FIMH-2021)", "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Deep learning methods have reached state-of-the-art performance in cardiac\nimage segmentation. Currently, the main bottleneck towards their effective\ntranslation into clinics requires assuring continuous high model performance\nand segmentation results. In this work, we present a novel learning framework\nto monitor the performance of heart segmentation models in the absence of\nground truth. Formulated as an anomaly detection problem, the monitoring\nframework allows deriving surrogate quality measures for a segmentation and\nallows flagging suspicious results. We propose two different types of quality\nmeasures, a global score and a pixel-wise map. We demonstrate their use by\nreproducing the final rankings of a cardiac segmentation challenge in the\nabsence of ground truth. Results show that our framework is accurate, fast, and\nscalable, confirming it is a viable option for quality control monitoring in\nclinical practice and large population studies.\n", "versions": [{"version": "v1", "created": "Mon, 12 Apr 2021 14:58:58 GMT"}], "update_date": "2021-06-15", "authors_parsed": [["Galati", "Francesco", ""], ["Zuluaga", "Maria A.", ""]]}, {"id": "2104.05544", "submitter": "Mohammad Zeineldeen", "authors": "Mohammad Zeineldeen, Aleksandr Glushko, Wilfried Michel, Albert Zeyer,\n  Ralf Schl\\\"uter, Hermann Ney", "title": "Investigating Methods to Improve Language Model Integration for\n  Attention-based Encoder-Decoder ASR Models", "comments": "accepted to Interspeech 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.SD eess.AS stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Attention-based encoder-decoder (AED) models learn an implicit internal\nlanguage model (ILM) from the training transcriptions. The integration with an\nexternal LM trained on much more unpaired text usually leads to better\nperformance. A Bayesian interpretation as in the hybrid autoregressive\ntransducer (HAT) suggests dividing by the prior of the discriminative acoustic\nmodel, which corresponds to this implicit LM, similarly as in the hybrid hidden\nMarkov model approach. The implicit LM cannot be calculated efficiently in\ngeneral and it is yet unclear what are the best methods to estimate it. In this\nwork, we compare different approaches from the literature and propose several\nnovel methods to estimate the ILM directly from the AED model. Our proposed\nmethods outperform all previous approaches. We also investigate other methods\nto suppress the ILM mainly by decreasing the capacity of the AED model,\nlimiting the label context, and also by training the AED model together with a\npre-existing LM.\n", "versions": [{"version": "v1", "created": "Mon, 12 Apr 2021 15:16:03 GMT"}, {"version": "v2", "created": "Thu, 17 Jun 2021 07:27:53 GMT"}], "update_date": "2021-06-18", "authors_parsed": [["Zeineldeen", "Mohammad", ""], ["Glushko", "Aleksandr", ""], ["Michel", "Wilfried", ""], ["Zeyer", "Albert", ""], ["Schl\u00fcter", "Ralf", ""], ["Ney", "Hermann", ""]]}, {"id": "2104.05600", "submitter": "Seong Jae Hwang", "authors": "Anthony Sicilia, Xingchen Zhao, Anastasia Sosnovskikh, Seong Jae Hwang", "title": "PAC Bayesian Performance Guarantees for Deep (Stochastic) Networks in\n  Medical Imaging", "comments": "MICCAI 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV stat.ML", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Application of deep neural networks to medical imaging tasks has in some\nsense become commonplace. Still, a \"thorn in the side\" of the deep learning\nmovement is the argument that deep networks are prone to overfitting and are\nthus unable to generalize well when datasets are small (as is common in medical\nimaging tasks). One way to bolster confidence is to provide mathematical\nguarantees, or bounds, on network performance after training which explicitly\nquantify the possibility of overfitting. In this work, we explore recent\nadvances using the PAC-Bayesian framework to provide bounds on generalization\nerror for large (stochastic) networks. While previous efforts focus on\nclassification in larger natural image datasets (e.g., MNIST and CIFAR-10), we\napply these techniques to both classification and segmentation in a smaller\nmedical imagining dataset: the ISIC 2018 challenge set. We observe the\nresultant bounds are competitive compared to a simpler baseline, while also\nbeing more explainable and alleviating the need for holdout sets.\n", "versions": [{"version": "v1", "created": "Mon, 12 Apr 2021 16:21:07 GMT"}, {"version": "v2", "created": "Thu, 8 Jul 2021 21:26:29 GMT"}], "update_date": "2021-07-12", "authors_parsed": [["Sicilia", "Anthony", ""], ["Zhao", "Xingchen", ""], ["Sosnovskikh", "Anastasia", ""], ["Hwang", "Seong Jae", ""]]}, {"id": "2104.05605", "submitter": "Yogesh Balaji", "authors": "Yogesh Balaji, Mohammadmahdi Sajedi, Neha Mukund Kalibhat, Mucong\n  Ding, Dominik St\\\"oger, Mahdi Soltanolkotabi, Soheil Feizi", "title": "Understanding Overparameterization in Generative Adversarial Networks", "comments": "Accepted in ICLR 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  A broad class of unsupervised deep learning methods such as Generative\nAdversarial Networks (GANs) involve training of overparameterized models where\nthe number of parameters of the model exceeds a certain threshold. A large body\nof work in supervised learning have shown the importance of model\noverparameterization in the convergence of the gradient descent (GD) to\nglobally optimal solutions. In contrast, the unsupervised setting and GANs in\nparticular involve non-convex concave mini-max optimization problems that are\noften trained using Gradient Descent/Ascent (GDA). The role and benefits of\nmodel overparameterization in the convergence of GDA to a global saddle point\nin non-convex concave problems is far less understood. In this work, we present\na comprehensive analysis of the importance of model overparameterization in\nGANs both theoretically and empirically. We theoretically show that in an\noverparameterized GAN model with a $1$-layer neural network generator and a\nlinear discriminator, GDA converges to a global saddle point of the underlying\nnon-convex concave min-max problem. To the best of our knowledge, this is the\nfirst result for global convergence of GDA in such settings. Our theory is\nbased on a more general result that holds for a broader class of nonlinear\ngenerators and discriminators that obey certain assumptions (including deeper\ngenerators and random feature discriminators). We also empirically study the\nrole of model overparameterization in GANs using several large-scale\nexperiments on CIFAR-10 and Celeb-A datasets. Our experiments show that\noverparameterization improves the quality of generated samples across various\nmodel architectures and datasets. Remarkably, we observe that\noverparameterization leads to faster and more stable convergence behavior of\nGDA across the board.\n", "versions": [{"version": "v1", "created": "Mon, 12 Apr 2021 16:23:37 GMT"}], "update_date": "2021-04-13", "authors_parsed": [["Balaji", "Yogesh", ""], ["Sajedi", "Mohammadmahdi", ""], ["Kalibhat", "Neha Mukund", ""], ["Ding", "Mucong", ""], ["St\u00f6ger", "Dominik", ""], ["Soltanolkotabi", "Mahdi", ""], ["Feizi", "Soheil", ""]]}, {"id": "2104.05674", "submitter": "Vincent Dutordoir", "authors": "Vincent Dutordoir, Hugh Salimbeni, Eric Hambro, John McLeod, Felix\n  Leibfried, Artem Artemev, Mark van der Wilk, James Hensman, Marc P.\n  Deisenroth, ST John", "title": "GPflux: A Library for Deep Gaussian Processes", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We introduce GPflux, a Python library for Bayesian deep learning with a\nstrong emphasis on deep Gaussian processes (DGPs). Implementing DGPs is a\nchallenging endeavour due to the various mathematical subtleties that arise\nwhen dealing with multivariate Gaussian distributions and the complex\nbookkeeping of indices. To date, there are no actively maintained, open-sourced\nand extendable libraries available that support research activities in this\narea. GPflux aims to fill this gap by providing a library with state-of-the-art\nDGP algorithms, as well as building blocks for implementing novel Bayesian and\nGP-based hierarchical models and inference schemes. GPflux is compatible with\nand built on top of the Keras deep learning eco-system. This enables\npractitioners to leverage tools from the deep learning community for building\nand training customised Bayesian models, and create hierarchical models that\nconsist of Bayesian and standard neural network layers in a single coherent\nframework. GPflux relies on GPflow for most of its GP objects and operations,\nwhich makes it an efficient, modular and extensible library, while having a\nlean codebase.\n", "versions": [{"version": "v1", "created": "Mon, 12 Apr 2021 17:41:18 GMT"}], "update_date": "2021-04-13", "authors_parsed": [["Dutordoir", "Vincent", ""], ["Salimbeni", "Hugh", ""], ["Hambro", "Eric", ""], ["McLeod", "John", ""], ["Leibfried", "Felix", ""], ["Artemev", "Artem", ""], ["van der Wilk", "Mark", ""], ["Hensman", "James", ""], ["Deisenroth", "Marc P.", ""], ["John", "ST", ""]]}, {"id": "2104.05762", "submitter": "Alexander D'Amour", "authors": "Alexander D'Amour and Alexander Franks", "title": "Deconfounding Scores: Feature Representations for Causal Effect\n  Estimation with Weak Overlap", "comments": "A previous version of this paper was presented at the NeurIPS 2019\n  Causal ML workshop (https://tripods.cis.cornell.edu/neurips19_causalml/)", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A key condition for obtaining reliable estimates of the causal effect of a\ntreatment is overlap (a.k.a. positivity): the distributions of the features\nused to perform causal adjustment cannot be too different in the treated and\ncontrol groups. In cases where overlap is poor, causal effect estimators can\nbecome brittle, especially when they incorporate weighting. To address this\nproblem, a number of proposals (including confounder selection or dimension\nreduction methods) incorporate feature representations to induce better overlap\nbetween the treated and control groups. A key concern in these proposals is\nthat the representation may introduce confounding bias into the effect\nestimator. In this paper, we introduce deconfounding scores, which are feature\nrepresentations that induce better overlap without biasing the target of\nestimation. We show that deconfounding scores satisfy a zero-covariance\ncondition that is identifiable in observed data. As a proof of concept, we\ncharacterize a family of deconfounding scores in a simplified setting with\nGaussian covariates, and show that in some simple simulations, these scores can\nbe used to construct estimators with good finite-sample properties. In\nparticular, we show that this technique could be an attractive alternative to\nstandard regularizations that are often applied to IPW and balancing weights.\n", "versions": [{"version": "v1", "created": "Mon, 12 Apr 2021 18:50:11 GMT"}], "update_date": "2021-04-14", "authors_parsed": [["D'Amour", "Alexander", ""], ["Franks", "Alexander", ""]]}, {"id": "2104.05781", "submitter": "Arun Verma", "authors": "Arun Verma, Manjesh K. Hanawal, Arun Rajkumar, Raman Sankaran", "title": "Censored Semi-Bandits for Resource Allocation", "comments": "Extended version of the NeurIPS 2019 paper (Censored Semi-Bandits: A\n  Framework for Resource Allocation with Censored Feedback)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI stat.ML", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  We consider the problem of sequentially allocating resources in a censored\nsemi-bandits setup, where the learner allocates resources at each step to the\narms and observes loss. The loss depends on two hidden parameters, one specific\nto the arm but independent of the resource allocation, and the other depends on\nthe allocated resource. More specifically, the loss equals zero for an arm if\nthe resource allocated to it exceeds a constant (but unknown) arm dependent\nthreshold. The goal is to learn a resource allocation that minimizes the\nexpected loss. The problem is challenging because the loss distribution and\nthreshold value of each arm are unknown. We study this setting by establishing\nits `equivalence' to Multiple-Play Multi-Armed Bandits (MP-MAB) and\nCombinatorial Semi-Bandits. Exploiting these equivalences, we derive optimal\nalgorithms for our problem setting using known algorithms for MP-MAB and\nCombinatorial Semi-Bandits. The experiments on synthetically generated data\nvalidate the performance guarantees of the proposed algorithms.\n", "versions": [{"version": "v1", "created": "Mon, 12 Apr 2021 19:15:32 GMT"}], "update_date": "2021-04-14", "authors_parsed": [["Verma", "Arun", ""], ["Hanawal", "Manjesh K.", ""], ["Rajkumar", "Arun", ""], ["Sankaran", "Raman", ""]]}, {"id": "2104.05785", "submitter": "Kenji Kawaguchi", "authors": "Kenji Kawaguchi, Qingyun Sun", "title": "A Recipe for Global Convergence Guarantee in Deep Neural Networks", "comments": "Published in AAAI 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI cs.CV math.OC stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Existing global convergence guarantees of (stochastic) gradient descent do\nnot apply to practical deep networks in the practical regime of deep learning\nbeyond the neural tangent kernel (NTK) regime. This paper proposes an\nalgorithm, which is ensured to have global convergence guarantees in the\npractical regime beyond the NTK regime, under a verifiable condition called the\nexpressivity condition. The expressivity condition is defined to be both\ndata-dependent and architecture-dependent, which is the key property that makes\nour results applicable for practical settings beyond the NTK regime. On the one\nhand, the expressivity condition is theoretically proven to hold\ndata-independently for fully-connected deep neural networks with narrow hidden\nlayers and a single wide layer. On the other hand, the expressivity condition\nis numerically shown to hold data-dependently for deep (convolutional) ResNet\nwith batch normalization with various standard image datasets. We also show\nthat the proposed algorithm has generalization performances comparable with\nthose of the heuristic algorithm, with the same hyper-parameters and total\nnumber of iterations. Therefore, the proposed algorithm can be viewed as a step\ntowards providing theoretical guarantees for deep learning in the practical\nregime.\n", "versions": [{"version": "v1", "created": "Mon, 12 Apr 2021 19:25:30 GMT"}, {"version": "v2", "created": "Thu, 15 Apr 2021 20:35:03 GMT"}], "update_date": "2021-04-19", "authors_parsed": [["Kawaguchi", "Kenji", ""], ["Sun", "Qingyun", ""]]}, {"id": "2104.05886", "submitter": "Zuheng Xu", "authors": "Zuheng Xu, Trevor Campbell", "title": "The computational asymptotics of Gaussian variational inference", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.CO math.ST stat.ML stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Variational inference is a popular alternative to Markov chain Monte Carlo\nmethods that constructs a Bayesian posterior approximation by minimizing a\ndiscrepancy to the true posterior within a pre-specified family. This converts\nBayesian inference into an optimization problem, enabling the use of simple and\nscalable stochastic optimization algorithms. However, a key limitation of\nvariational inference is that the optimal approximation is typically not\ntractable to compute; even in simple settings the problem is nonconvex. Thus,\nrecently developed statistical guarantees -- which all involve the (data)\nasymptotic properties of the optimal variational distribution -- are not\nreliably obtained in practice. In this work, we provide two major\ncontributions: a theoretical analysis of the asymptotic convexity properties of\nvariational inference in the popular setting with a Gaussian family; and\nconsistent stochastic variational inference (CSVI), an algorithm that exploits\nthese properties to find the optimal approximation in the asymptotic regime.\nCSVI consists of a tractable initialization procedure that finds the local\nbasin of the optimal solution, and a scaled gradient descent algorithm that\nstays locally confined to that basin. Experiments on nonconvex synthetic and\nreal-data examples show that compared with standard stochastic gradient\ndescent, CSVI improves the likelihood of obtaining the globally optimal\nposterior approximation.\n", "versions": [{"version": "v1", "created": "Tue, 13 Apr 2021 01:23:34 GMT"}], "update_date": "2021-04-14", "authors_parsed": [["Xu", "Zuheng", ""], ["Campbell", "Trevor", ""]]}, {"id": "2104.06135", "submitter": "Nis Meinert", "authors": "Nis Meinert and Alexander Lavin", "title": "Multivariate Deep Evidential Regression", "comments": "20 pages, 13 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.NE stat.ML", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  There is significant need for principled uncertainty reasoning in machine\nlearning systems as they are increasingly deployed in safety-critical domains.\nA new approach with uncertainty-aware neural networks (NNs), based on learning\nevidential distributions for aleatoric and epistemic uncertainties, shows\npromise over traditional deterministic methods and typical Bayesian NNs, yet\nseveral important gaps in the theory and implementation of these networks\nremain. We discuss three issues with a proposed solution to extract aleatoric\nand epistemic uncertainties from regression-based neural networks. The approach\nderives a technique by placing evidential priors over the original Gaussian\nlikelihood function and training the NN to infer the hyperparameters of the\nevidential distribution. Doing so allows for the simultaneous extraction of\nboth uncertainties without sampling or utilization of out-of-distribution data\nfor univariate regression tasks. We describe the outstanding issues in detail,\nprovide a possible solution, and generalize the deep evidential regression\ntechnique for multivariate cases.\n", "versions": [{"version": "v1", "created": "Tue, 13 Apr 2021 12:20:18 GMT"}, {"version": "v2", "created": "Thu, 15 Apr 2021 12:47:38 GMT"}, {"version": "v3", "created": "Mon, 31 May 2021 07:41:18 GMT"}], "update_date": "2021-06-01", "authors_parsed": [["Meinert", "Nis", ""], ["Lavin", "Alexander", ""]]}, {"id": "2104.06199", "submitter": "Giuseppe Calafiore", "authors": "Giuseppe Calafiore, Giulia Fracastoro", "title": "COVID-19 case data for Italy stratified by age class", "comments": "six pages, one table, three figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The dataset described in this paper contains daily data about COVID-19 cases\nthat occurred in Italy over the period from Jan. 28, 2020 to March 20, 2021,\ndivided into ten age classes of the population, the first class being 0-9\nyears, the tenth class being 90 years and over. The dataset contains eight\ncolumns, namely: date (day), age class, number of new cases, number of newly\nhospitalized patients, number of patients entering intensive care, number of\ndeceased patients, number of recovered patients, number of active infected\npatients. This data has been officially released for research purposes by the\nItalian authority for COVID-19 epidemiologic surveillance (Istituto Superiore\ndi Sanit\\`a - ISS), upon formal request by the authors, in accordance with the\nOrdonnance of the Chief of the Civil Protection Department n. 691 dated Aug. 4\n2020. A separate file contains the numerosity of the population in each age\nclass, according to the National Institute of Statistics (ISTAT) data of the\nresident population of Italy as of Jan. 2020. This data has potential use, for\ninstance, in epidemiologic studies of the effects of the COVID-19 contagion in\nItaly, in mortality analysis by age class, and in the development and testing\nof dynamical models of the contagion.\n", "versions": [{"version": "v1", "created": "Tue, 13 Apr 2021 13:51:20 GMT"}], "update_date": "2021-04-14", "authors_parsed": [["Calafiore", "Giuseppe", ""], ["Fracastoro", "Giulia", ""]]}, {"id": "2104.06204", "submitter": "Qin Luo", "authors": "Qin Luo, Kun Fang, Jie Yang, Xiaolin Huang", "title": "Towards Unbiased Random Features with Lower Variance For Stationary\n  Indefinite Kernels", "comments": "Accepted by IJCNN2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Random Fourier Features (RFF) demonstrate wellappreciated performance in\nkernel approximation for largescale situations but restrict kernels to be\nstationary and positive definite. And for non-stationary kernels, the\ncorresponding RFF could be converted to that for stationary indefinite kernels\nwhen the inputs are restricted to the unit sphere. Numerous methods provide\naccessible ways to approximate stationary but indefinite kernels. However, they\nare either biased or possess large variance. In this article, we propose the\ngeneralized orthogonal random features, an unbiased estimation with lower\nvariance.Experimental results on various datasets and kernels verify that our\nalgorithm achieves lower variance and approximation error compared with the\nexisting kernel approximation methods. With better approximation to the\noriginally selected kernels, improved classification accuracy and regression\nability is obtained with our approximation algorithm in the framework of\nsupport vector machine and regression.\n", "versions": [{"version": "v1", "created": "Tue, 13 Apr 2021 13:56:50 GMT"}, {"version": "v2", "created": "Wed, 14 Apr 2021 00:54:53 GMT"}], "update_date": "2021-04-15", "authors_parsed": [["Luo", "Qin", ""], ["Fang", "Kun", ""], ["Yang", "Jie", ""], ["Huang", "Xiaolin", ""]]}, {"id": "2104.06237", "submitter": "Micha\\\"el Defferrard", "authors": "Jelena Banjac, Laur\\`ene Donati, Micha\\\"el Defferrard", "title": "Learning to recover orientations from projections in single-particle\n  cryo-EM", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV q-bio.QM stat.ML", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  A major challenge in single-particle cryo-electron microscopy (cryo-EM) is\nthat the orientations adopted by the 3D particles prior to imaging are unknown;\nyet, this knowledge is essential for high-resolution reconstruction. We present\na method to recover these orientations directly from the acquired set of 2D\nprojections. Our approach consists of two steps: (i) the estimation of\ndistances between pairs of projections, and (ii) the recovery of the\norientation of each projection from these distances. In step (i), pairwise\ndistances are estimated by a Siamese neural network trained on synthetic\ncryo-EM projections from resolved bio-structures. In step (ii), orientations\nare recovered by minimizing the difference between the distances estimated from\nthe projections and the distances induced by the recovered orientations. We\nevaluated the method on synthetic cryo-EM datasets. Current results demonstrate\nthat orientations can be accurately recovered from projections that are shifted\nand corrupted with a high level of noise. The accuracy of the recovery depends\non the accuracy of the distance estimator. While not yet deployed in a real\nexperimental setup, the proposed method offers a novel learning-based take on\norientation recovery in SPA. Our code is available at\nhttps://github.com/JelenaBanjac/protein-reconstruction\n", "versions": [{"version": "v1", "created": "Tue, 13 Apr 2021 14:31:37 GMT"}], "update_date": "2021-04-14", "authors_parsed": [["Banjac", "Jelena", ""], ["Donati", "Laur\u00e8ne", ""], ["Defferrard", "Micha\u00ebl", ""]]}, {"id": "2104.06323", "submitter": "Dan Ley", "authors": "Dan Ley, Umang Bhatt, Adrian Weller", "title": "{\\delta}-CLUE: Diverse Sets of Explanations for Uncertainty Estimates", "comments": "Appeared as a workshop paper at ICLR 2021 (Responsible AI | Secure ML\n  | Robust ML)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  To interpret uncertainty estimates from differentiable probabilistic models,\nrecent work has proposed generating Counterfactual Latent Uncertainty\nExplanations (CLUEs). However, for a single input, such approaches could output\na variety of explanations due to the lack of constraints placed on the\nexplanation. Here we augment the original CLUE approach, to provide what we\ncall $\\delta$-CLUE. CLUE indicates $\\it{one}$ way to change an input, while\nremaining on the data manifold, such that the model becomes more confident\nabout its prediction. We instead return a $\\it{set}$ of plausible CLUEs:\nmultiple, diverse inputs that are within a $\\delta$ ball of the original input\nin latent space, all yielding confident predictions.\n", "versions": [{"version": "v1", "created": "Tue, 13 Apr 2021 16:03:27 GMT"}, {"version": "v2", "created": "Wed, 14 Apr 2021 08:10:33 GMT"}, {"version": "v3", "created": "Sat, 24 Apr 2021 14:08:57 GMT"}, {"version": "v4", "created": "Tue, 27 Apr 2021 15:23:09 GMT"}, {"version": "v5", "created": "Sat, 8 May 2021 09:29:40 GMT"}], "update_date": "2021-05-11", "authors_parsed": [["Ley", "Dan", ""], ["Bhatt", "Umang", ""], ["Weller", "Adrian", ""]]}, {"id": "2104.06339", "submitter": "Chiara Mastrogiuseppe", "authors": "Ruben Moreno-Bote and Chiara Mastrogiuseppe", "title": "Deep imagination is a close to optimal policy for planning in large\n  decision trees under limited resources", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG q-bio.NC", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Many decisions involve choosing an uncertain course of actions in deep and\nwide decision trees, as when we plan to visit an exotic country for vacation.\nIn these cases, exhaustive search for the best sequence of actions is not\ntractable due to the large number of possibilities and limited time or\ncomputational resources available to make the decision. Therefore, planning\nagents need to balance breadth (exploring many actions at each level of the\ntree) and depth (exploring many levels in the tree) to allocate optimally their\nfinite search capacity. We provide efficient analytical solutions and numerical\nanalysis to the problem of allocating finite sampling capacity in one shot to\nlarge decision trees. We find that in general the optimal policy is to allocate\nfew samples per level so that deep levels can be reached, thus favoring depth\nover breadth search. In contrast, in poor environments and at low capacity, it\nis best to broadly sample branches at the cost of not sampling deeply, although\nthis policy is marginally better than deep allocations. Our results provide a\ntheoretical foundation for the optimality of deep imagination for planning and\nshow that it is a generally valid heuristic that could have evolved from the\nfinite constraints of cognitive systems.\n", "versions": [{"version": "v1", "created": "Tue, 13 Apr 2021 16:31:24 GMT"}], "update_date": "2021-04-14", "authors_parsed": [["Moreno-Bote", "Ruben", ""], ["Mastrogiuseppe", "Chiara", ""]]}, {"id": "2104.06384", "submitter": "Philippe Gagnon", "authors": "Sebastian M Schmon and Philippe Gagnon", "title": "Optimal scaling of random walk Metropolis algorithms using Bayesian\n  large-sample asymptotics", "comments": "Both authors contributed equally", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.CO stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  High-dimensional limit theorems have been shown to be useful to derive tuning\nrules for finding the optimal scaling in random walk Metropolis algorithms. The\nassumptions under which weak convergence results are proved are however\nrestrictive; the target density is typically assumed to be of a product form.\nUsers may thus doubt the validity of such tuning rules in practical\napplications. In this paper, we shed some light on optimal scaling problems\nfrom a different perspective, namely a large-sample one. This allows to prove\nweak convergence results under realistic assumptions and to propose novel\nparameter-dimension-dependent tuning guidelines. The proposed guidelines are\nconsistent with previous ones when the target density is close to having a\nproduct form, but significantly different otherwise.\n", "versions": [{"version": "v1", "created": "Tue, 13 Apr 2021 17:39:50 GMT"}, {"version": "v2", "created": "Tue, 27 Apr 2021 23:27:22 GMT"}], "update_date": "2021-04-29", "authors_parsed": [["Schmon", "Sebastian M", ""], ["Gagnon", "Philippe", ""]]}, {"id": "2104.06389", "submitter": "Minjie Wang", "authors": "Minjie Wang, Genevera I. Allen", "title": "Thresholded Graphical Lasso Adjusts for Latent Variables: Application to\n  Functional Neural Connectivity", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In neuroscience, researchers seek to uncover the connectivity of neurons from\nlarge-scale neural recordings or imaging; often people employ graphical model\nselection and estimation techniques for this purpose. But, existing\ntechnologies can only record from a small subset of neurons leading to a\nchallenging problem of graph selection in the presence of extensive latent\nvariables. Chandrasekaran et al. (2012) proposed a convex program to address\nthis problem that poses challenges from both a computational and statistical\nperspective. To solve this problem, we propose an incredibly simple solution:\napply a hard thresholding operator to existing graph selection methods.\nConceptually simple and computationally attractive, we demonstrate that\nthresholding the graphical Lasso, neighborhood selection, or CLIME estimators\nhave superior theoretical properties in terms of graph selection consistency as\nwell as stronger empirical results than existing approaches for the latent\nvariable graphical model problem. We also demonstrate the applicability of our\napproach through a neuroscience case study on calcium-imaging data to estimate\nfunctional neural connections.\n", "versions": [{"version": "v1", "created": "Tue, 13 Apr 2021 17:50:26 GMT"}], "update_date": "2021-04-14", "authors_parsed": [["Wang", "Minjie", ""], ["Allen", "Genevera I.", ""]]}, {"id": "2104.06487", "submitter": "Chiwoo Park", "authors": "Chiwoo Park", "title": "Gaussian Process Model for Estimating Piecewise Continuous Regression\n  Functions", "comments": "11 pages; 5 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper presents a Gaussian process (GP) model for estimating piecewise\ncontinuous regression functions. In scientific and engineering applications of\nregression analysis, the underlying regression functions are piecewise\ncontinuous in that data follow different continuous regression models for\ndifferent regions of the data with possible discontinuities between the\nregions. However, many conventional GP regression approaches are not designed\nfor piecewise regression analysis. We propose a new GP modeling approach for\nestimating an unknown piecewise continuous regression function. The new GP\nmodel seeks for a local GP estimate of an unknown regression function at each\ntest location, using local data neighboring to the test location. To\naccommodate the possibilities of the local data from different regions, the\nlocal data is partitioned into two sides by a local linear boundary, and only\nthe local data belonging to the same side as the test location is used for the\nregression estimate. This local split works very well when the input regions\nare bounded by smooth boundaries, so the local linear approximation of the\nsmooth boundaries works well. We estimate the local linear boundary jointly\nwith the other hyperparameters of the GP model, using the maximum likelihood\napproach. Its computation time is as low as the local GP's time. The superior\nnumerical performance of the proposed approach over the conventional GP\nmodeling approaches is shown using various simulated piecewise regression\nfunctions.\n", "versions": [{"version": "v1", "created": "Tue, 13 Apr 2021 20:01:43 GMT"}], "update_date": "2021-04-15", "authors_parsed": [["Park", "Chiwoo", ""]]}, {"id": "2104.06548", "submitter": "Alexander Litvinenko", "authors": "Vladimir Berikov and Alexander Litvinenko", "title": "Solving weakly supervised regression problem using low-rank manifold\n  regularization", "comments": "14 pages, 5 Tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.NA math.NA stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We solve a weakly supervised regression problem. Under \"weakly\" we understand\nthat for some training points the labels are known, for some unknown, and for\nothers uncertain due to the presence of random noise or other reasons such as\nlack of resources. The solution process requires to optimize a certain\nobjective function (the loss function), which combines manifold regularization\nand low-rank matrix decomposition techniques. These low-rank approximations\nallow us to speed up all matrix calculations and reduce storage requirements.\nThis is especially crucial for large datasets. Ensemble clustering is used for\nobtaining the co-association matrix, which we consider as the similarity\nmatrix. The utilization of these techniques allows us to increase the quality\nand stability of the solution. In the numerical section, we applied the\nsuggested method to artificial and real datasets using Monte-Carlo modeling.\n", "versions": [{"version": "v1", "created": "Tue, 13 Apr 2021 23:21:01 GMT"}], "update_date": "2021-04-15", "authors_parsed": [["Berikov", "Vladimir", ""], ["Litvinenko", "Alexander", ""]]}, {"id": "2104.06571", "submitter": "Sherri Rose", "authors": "Ani Eloyan and Sherri Rose", "title": "Considerations Across Three Cultures: Parametric Regressions,\n  Interpretable Algorithms, and Complex Algorithms", "comments": "7 pages, forthcoming in Observational Studies", "journal-ref": "Observational Studies (2021); 7(1):191-196.\n  https://muse.jhu.edu/article/799734", "doi": null, "report-no": null, "categories": "stat.ML stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider an extension of Leo Breiman's thesis from \"Statistical Modeling:\nThe Two Cultures\" to include a bifurcation of algorithmic modeling, focusing on\nparametric regressions, interpretable algorithms, and complex (possibly\nexplainable) algorithms.\n", "versions": [{"version": "v1", "created": "Wed, 14 Apr 2021 01:07:54 GMT"}], "update_date": "2021-07-29", "authors_parsed": [["Eloyan", "Ani", ""], ["Rose", "Sherri", ""]]}, {"id": "2104.06574", "submitter": "Youngdong Kim", "authors": "Youngdong Kim, Juseung Yun, Hyounguk Shon, Junmo Kim", "title": "Joint Negative and Positive Learning for Noisy Labels", "comments": "CVPR 2021, Accepted", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Training of Convolutional Neural Networks (CNNs) with data with noisy labels\nis known to be a challenge. Based on the fact that directly providing the label\nto the data (Positive Learning; PL) has a risk of allowing CNNs to memorize the\ncontaminated labels for the case of noisy data, the indirect learning approach\nthat uses complementary labels (Negative Learning for Noisy Labels; NLNL) has\nproven to be highly effective in preventing overfitting to noisy data as it\nreduces the risk of providing faulty target. NLNL further employs a three-stage\npipeline to improve convergence. As a result, filtering noisy data through the\nNLNL pipeline is cumbersome, increasing the training cost. In this study, we\npropose a novel improvement of NLNL, named Joint Negative and Positive Learning\n(JNPL), that unifies the filtering pipeline into a single stage. JNPL trains\nCNN via two losses, NL+ and PL+, which are improved upon NL and PL loss\nfunctions, respectively. We analyze the fundamental issue of NL loss function\nand develop new NL+ loss function producing gradient that enhances the\nconvergence of noisy data. Furthermore, PL+ loss function is designed to enable\nfaster convergence to expected-to-be-clean data. We show that the NL+ and PL+\ntrain CNN simultaneously, significantly simplifying the pipeline, allowing\ngreater ease of practical use compared to NLNL. With a simple semi-supervised\ntraining technique, our method achieves state-of-the-art accuracy for noisy\ndata classification based on the superior filtering ability.\n", "versions": [{"version": "v1", "created": "Wed, 14 Apr 2021 01:32:25 GMT"}], "update_date": "2021-04-15", "authors_parsed": [["Kim", "Youngdong", ""], ["Yun", "Juseung", ""], ["Shon", "Hyounguk", ""], ["Kim", "Junmo", ""]]}, {"id": "2104.06648", "submitter": "Eugene Ndiaye", "authors": "Eugene Ndiaye and Ichiro Takeuchi", "title": "Root-finding Approaches for Computing Conformal Prediction Set", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG stat.CO", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Conformal prediction constructs a confidence set for an unobserved response\nof a feature vector based on previous identically distributed and exchangeable\nobservations of responses and features. It has a coverage guarantee at any\nnominal level without additional assumptions on their distribution. Its\ncomputation deplorably requires a refitting procedure for all replacement\ncandidates of the target response. In regression settings, this corresponds to\nan infinite number of model fit. Apart from relatively simple estimators that\ncan be written as pieces of linear function of the response, efficiently\ncomputing such sets is difficult and is still considered as an open problem. We\nexploit the fact that, \\emph{often}, conformal prediction sets are intervals\nwhose boundaries can be efficiently approximated by classical root-finding\nalgorithm. We investigate how this approach can overcome many limitations of\nformerly used strategies and we discuss its complexity and drawbacks.\n", "versions": [{"version": "v1", "created": "Wed, 14 Apr 2021 06:41:12 GMT"}, {"version": "v2", "created": "Mon, 14 Jun 2021 05:05:26 GMT"}], "update_date": "2021-06-15", "authors_parsed": [["Ndiaye", "Eugene", ""], ["Takeuchi", "Ichiro", ""]]}, {"id": "2104.06655", "submitter": "Yuan Pu", "authors": "Yuan Pu, Shaochen Wang, Rui Yang, Xin Yao, Bin Li", "title": "Decomposed Soft Actor-Critic Method for Cooperative Multi-Agent\n  Reinforcement Learning", "comments": "11 pages, 5 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.MA stat.ML", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Deep reinforcement learning methods have shown great performance on many\nchallenging cooperative multi-agent tasks. Two main promising research\ndirections are multi-agent value function decomposition and multi-agent policy\ngradients. In this paper, we propose a new decomposed multi-agent soft\nactor-critic (mSAC) method, which effectively combines the advantages of the\naforementioned two methods. The main modules include decomposed Q network\narchitecture, discrete probabilistic policy and counterfactual advantage\nfunction (optinal). Theoretically, mSAC supports efficient off-policy learning\nand addresses credit assignment problem partially in both discrete and\ncontinuous action spaces. Tested on StarCraft II micromanagement cooperative\nmultiagent benchmark, we empirically investigate the performance of mSAC\nagainst its variants and analyze the effects of the different components.\nExperimental results demonstrate that mSAC significantly outperforms\npolicy-based approach COMA, and achieves competitive results with SOTA\nvalue-based approach Qmix on most tasks in terms of asymptotic perfomance\nmetric. In addition, mSAC achieves pretty good results on large action space\ntasks, such as 2c_vs_64zg and MMM2.\n", "versions": [{"version": "v1", "created": "Wed, 14 Apr 2021 07:02:40 GMT"}, {"version": "v2", "created": "Sat, 8 May 2021 03:34:07 GMT"}], "update_date": "2021-05-11", "authors_parsed": [["Pu", "Yuan", ""], ["Wang", "Shaochen", ""], ["Yang", "Rui", ""], ["Yao", "Xin", ""], ["Li", "Bin", ""]]}, {"id": "2104.06666", "submitter": "David Peter", "authors": "David Peter, Wolfgang Roth, Franz Pernkopf", "title": "End-to-end Keyword Spotting using Neural Architecture Search and\n  Quantization", "comments": "arXiv admin note: text overlap with arXiv:2012.10138", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.SD cs.LG eess.AS stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper introduces neural architecture search (NAS) for the automatic\ndiscovery of end-to-end keyword spotting (KWS) models in limited resource\nenvironments. We employ a differentiable NAS approach to optimize the structure\nof convolutional neural networks (CNNs) operating on raw audio waveforms. After\na suitable KWS model is found with NAS, we conduct quantization of weights and\nactivations to reduce the memory footprint. We conduct extensive experiments on\nthe Google speech commands dataset. In particular, we compare our end-to-end\napproach to mel-frequency cepstral coefficient (MFCC) based systems. For\nquantization, we compare fixed bit-width quantization and trained bit-width\nquantization. Using NAS only, we were able to obtain a highly efficient model\nwith an accuracy of 95.55% using 75.7k parameters and 13.6M operations. Using\ntrained bit-width quantization, the same model achieves a test accuracy of\n93.76% while using on average only 2.91 bits per activation and 2.51 bits per\nweight.\n", "versions": [{"version": "v1", "created": "Wed, 14 Apr 2021 07:22:22 GMT"}], "update_date": "2021-04-15", "authors_parsed": [["Peter", "David", ""], ["Roth", "Wolfgang", ""], ["Pernkopf", "Franz", ""]]}, {"id": "2104.06667", "submitter": "Abhishek Chakrabortty", "authors": "Yuqian Zhang, Abhishek Chakrabortty and Jelena Bradic", "title": "Double Robust Semi-Supervised Inference for the Mean: Selection Bias\n  under MAR Labeling with Decaying Overlap", "comments": "37 pages; (Supplement: 43 pages)", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME math.ST stat.ML stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Semi-supervised (SS) inference has received much attention in recent years.\nApart from a moderate-sized labeled data, L, the SS setting is characterized by\nan additional, much larger sized, unlabeled data, U. The setting of |U| >> |L|,\nmakes SS inference unique and different from the standard missing data\nproblems, owing to natural violation of the so-called 'positivity' or 'overlap'\nassumption. However, most of the SS literature implicitly assumes L and U to be\nequally distributed, i.e., no selection bias in the labeling. Inferential\nchallenges in missing at random (MAR) type labeling allowing for selection\nbias, are inevitably exacerbated by the decaying nature of the propensity score\n(PS). We address this gap for a prototype problem, the estimation of the\nresponse's mean. We propose a double robust SS (DRSS) mean estimator and give a\ncomplete characterization of its asymptotic properties. The proposed estimator\nis consistent as long as either the outcome or the PS model is correctly\nspecified. When both models are correctly specified, we provide inference\nresults with a non-standard consistency rate that depends on the smaller size\n|L|. The results are also extended to causal inference with imbalanced\ntreatment groups. Further, we provide several novel choices of models and\nestimators of the decaying PS, including a novel offset logistic model and a\nstratified labeling model. We present their properties under both high and low\ndimensional settings. These may be of independent interest. Lastly, we present\nextensive simulations and also a real data application.\n", "versions": [{"version": "v1", "created": "Wed, 14 Apr 2021 07:27:27 GMT"}], "update_date": "2021-04-15", "authors_parsed": [["Zhang", "Yuqian", ""], ["Chakrabortty", "Abhishek", ""], ["Bradic", "Jelena", ""]]}, {"id": "2104.06685", "submitter": "Heng Zhu", "authors": "Heng Zhu, Qing Ling", "title": "BROADCAST: Reducing Both Stochastic and Compression Noise to Robustify\n  Communication-Efficient Federated Learning", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.DC math.OC stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Communication between workers and the master node to collect local stochastic\ngradients is a key bottleneck in a large-scale federated learning system.\nVarious recent works have proposed to compress the local stochastic gradients\nto mitigate the communication overhead. However, robustness to malicious\nattacks is rarely considered in such a setting. In this work, we investigate\nthe problem of Byzantine-robust federated learning with compression, where the\nattacks from Byzantine workers can be arbitrarily malicious. We point out that\na vanilla combination of compressed stochastic gradient descent (SGD) and\ngeometric median-based robust aggregation suffers from both stochastic and\ncompression noise in the presence of Byzantine attacks. In light of this\nobservation, we propose to jointly reduce the stochastic and compression noise\nso as to improve the Byzantine-robustness. For the stochastic noise, we adopt\nthe stochastic average gradient algorithm (SAGA) to gradually eliminate the\ninner variations of regular workers. For the compression noise, we apply the\ngradient difference compression and achieve compression for free. We\ntheoretically prove that the proposed algorithm reaches a neighborhood of the\noptimal solution at a linear convergence rate, and the asymptotic learning\nerror is in the same order as that of the state-of-the-art uncompressed method.\nFinally, numerical experiments demonstrate effectiveness of the proposed\nmethod.\n", "versions": [{"version": "v1", "created": "Wed, 14 Apr 2021 08:16:03 GMT"}], "update_date": "2021-04-15", "authors_parsed": [["Zhu", "Heng", ""], ["Ling", "Qing", ""]]}, {"id": "2104.06718", "submitter": "Alessandro De Palma", "authors": "Alessandro De Palma, Rudy Bunel, Alban Desmaison, Krishnamurthy\n  Dvijotham, Pushmeet Kohli, Philip H.S. Torr, M. Pawan Kumar", "title": "Improved Branch and Bound for Neural Network Verification via Lagrangian\n  Decomposition", "comments": "Submitted for review to JMLR. This is an extended version of our\n  paper in the UAI-20 conference (arXiv:2002.10410)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.LO stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We improve the scalability of Branch and Bound (BaB) algorithms for formally\nproving input-output properties of neural networks. First, we propose novel\nbounding algorithms based on Lagrangian Decomposition. Previous works have used\noff-the-shelf solvers to solve relaxations at each node of the BaB tree, or\nconstructed weaker relaxations that can be solved efficiently, but lead to\nunnecessarily weak bounds. Our formulation restricts the optimization to a\nsubspace of the dual domain that is guaranteed to contain the optimum,\nresulting in accelerated convergence. Furthermore, it allows for a massively\nparallel implementation, which is amenable to GPU acceleration via modern deep\nlearning frameworks. Second, we present a novel activation-based branching\nstrategy. By coupling an inexpensive heuristic with fast dual bounding, our\nbranching scheme greatly reduces the size of the BaB tree compared to previous\nheuristic methods. Moreover, it performs competitively with a recent strategy\nbased on learning algorithms, without its large offline training cost. Finally,\nwe design a BaB framework, named Branch and Dual Network Bound (BaDNB), based\non our novel bounding and branching algorithms. We show that BaDNB outperforms\nprevious complete verification systems by a large margin, cutting average\nverification times by factors up to 50 on adversarial robustness properties.\n", "versions": [{"version": "v1", "created": "Wed, 14 Apr 2021 09:22:42 GMT"}], "update_date": "2021-04-15", "authors_parsed": [["De Palma", "Alessandro", ""], ["Bunel", "Rudy", ""], ["Desmaison", "Alban", ""], ["Dvijotham", "Krishnamurthy", ""], ["Kohli", "Pushmeet", ""], ["Torr", "Philip H. S.", ""], ["Kumar", "M. Pawan", ""]]}, {"id": "2104.06819", "submitter": "Niklas Christoffer Petersen", "authors": "Niklas Christoffer Petersen, Anders Parslov, Filipe Rodrigues", "title": "Short-term bus travel time prediction for transfer synchronization with\n  intelligent uncertainty handling", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  This paper presents two novel approaches for uncertainty estimation adapted\nand extended for the multi-link bus travel time problem. The uncertainty is\nmodeled directly as part of recurrent artificial neural networks, but using two\nfundamentally different approaches: one based on Deep Quantile Regression (DQR)\nand the other on Bayesian Recurrent Neural Networks (BRNN). Both models predict\nmultiple time steps into the future, but handle the time-dependent uncertainty\nestimation differently. We present a sampling technique in order to aggregate\nquantile estimates for link level travel time to yield the multi-link travel\ntime distribution needed for a vehicle to travel from its current position to a\nspecific downstream stop point or transfer site.\n  To motivate the relevance of uncertainty-aware models in the domain, we focus\non the connection assurance application as a case study: An expert system to\ndetermine whether a bus driver should hold and wait for a connecting service,\nor break the connection and reduce its own delay. Our results show that the\nDQR-model performs overall best for the 80%, 90% and 95% prediction intervals,\nboth for a 15 minute time horizon into the future (t + 1), but also for the 30\nand 45 minutes time horizon (t + 2 and t + 3), with a constant, but very small\nunderestimation of the uncertainty interval (1-4 pp.). However, we also show,\nthat the BRNN model still can outperform the DQR for specific cases. Lastly, we\ndemonstrate how a simple decision support system can take advantage of our\nuncertainty-aware travel time models to prioritize the difference in travel\ntime uncertainty for bus holding at strategic points, thus reducing the\nintroduced delay for the connection assurance application.\n", "versions": [{"version": "v1", "created": "Wed, 14 Apr 2021 12:38:27 GMT"}], "update_date": "2021-04-15", "authors_parsed": [["Petersen", "Niklas Christoffer", ""], ["Parslov", "Anders", ""], ["Rodrigues", "Filipe", ""]]}, {"id": "2104.06970", "submitter": "Gene Li", "authors": "Gene Li, Pritish Kamath, Dylan J. Foster, Nathan Srebro", "title": "Eluder Dimension and Generalized Rank", "comments": "Technical Note", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study the relationship between the eluder dimension for a function class\nand a generalized notion of rank, defined for any monotone \"activation\" $\\sigma\n: \\mathbb{R} \\to \\mathbb{R}$, which corresponds to the minimal dimension\nrequired to represent the class as a generalized linear model. When $\\sigma$\nhas derivatives bounded away from $0$, it is known that $\\sigma$-rank gives\nrise to an upper bound on eluder dimension for any function class; we show\nhowever that eluder dimension can be exponentially smaller than $\\sigma$-rank.\nWe also show that the condition on the derivative is necessary; namely, when\n$\\sigma$ is the $\\mathrm{relu}$ activation, we show that eluder dimension can\nbe exponentially larger than $\\sigma$-rank.\n", "versions": [{"version": "v1", "created": "Wed, 14 Apr 2021 16:53:13 GMT"}], "update_date": "2021-04-15", "authors_parsed": [["Li", "Gene", ""], ["Kamath", "Pritish", ""], ["Foster", "Dylan J.", ""], ["Srebro", "Nathan", ""]]}, {"id": "2104.07006", "submitter": "Anastasios Kyrillidis", "authors": "Junhyung Lyle Kim, George Kollias, Amir Kalev, Ken X. Wei, Anastasios\n  Kyrillidis", "title": "Fast quantum state reconstruction via accelerated non-convex programming", "comments": "46 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "quant-ph cs.IT cs.LG math.IT math.OC stat.ML", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  We propose a new quantum state reconstruction method that combines ideas from\ncompressed sensing, non-convex optimization, and acceleration methods. The\nalgorithm, called Momentum-Inspired Factored Gradient Descent (\\texttt{MiFGD}),\nextends the applicability of quantum tomography for larger systems. Despite\nbeing a non-convex method, \\texttt{MiFGD} converges \\emph{provably} to the true\ndensity matrix at a linear rate, in the absence of experimental and statistical\nnoise, and under common assumptions. With this manuscript, we present the\nmethod, prove its convergence property and provide Frobenius norm bound\nguarantees with respect to the true density matrix. From a practical point of\nview, we benchmark the algorithm performance with respect to other existing\nmethods, in both synthetic and real experiments performed on an IBM's quantum\nprocessing unit. We find that the proposed algorithm performs orders of\nmagnitude faster than state of the art approaches, with the same or better\naccuracy. In both synthetic and real experiments, we observed accurate and\nrobust reconstruction, despite experimental and statistical noise in the\ntomographic data. Finally, we provide a ready-to-use code for state tomography\nof multi-qubit systems.\n", "versions": [{"version": "v1", "created": "Wed, 14 Apr 2021 17:38:40 GMT"}, {"version": "v2", "created": "Fri, 30 Apr 2021 15:51:00 GMT"}, {"version": "v3", "created": "Fri, 25 Jun 2021 08:38:20 GMT"}], "update_date": "2021-06-28", "authors_parsed": [["Kim", "Junhyung Lyle", ""], ["Kollias", "George", ""], ["Kalev", "Amir", ""], ["Wei", "Ken X.", ""], ["Kyrillidis", "Anastasios", ""]]}, {"id": "2104.07029", "submitter": "Maciej Skorski", "authors": "Maciej Skorski", "title": "Mean-Squared Accuracy of Good-Turing Estimator", "comments": null, "journal-ref": null, "doi": "10.13140/RG.2.2.31351.44960/1", "report-no": null, "categories": "stat.ML cs.CR cs.IT cs.LG math.IT stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The brilliant method due to Good and Turing allows for estimating objects not\noccurring in a sample. The problem, known under names \"sample coverage\" or\n\"missing mass\" goes back to their cryptographic work during WWII, but over\nyears has found has many applications, including language modeling, inference\nin ecology and estimation of distribution properties. This work characterizes\nthe maximal mean-squared error of the Good-Turing estimator, for any sample\n\\emph{and} alphabet size.\n", "versions": [{"version": "v1", "created": "Wed, 14 Apr 2021 17:23:46 GMT"}], "update_date": "2021-04-16", "authors_parsed": [["Skorski", "Maciej", ""]]}, {"id": "2104.07061", "submitter": "Sebastian Macaluso", "authors": "Craig S. Greenberg, Sebastian Macaluso, Nicholas Monath, Avinava\n  Dubey, Patrick Flaherty, Manzil Zaheer, Amr Ahmed, Kyle Cranmer, Andrew\n  McCallum", "title": "Exact and Approximate Hierarchical Clustering Using A*", "comments": "30 pages, 9 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.DS physics.data-an stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Hierarchical clustering is a critical task in numerous domains. Many\napproaches are based on heuristics and the properties of the resulting\nclusterings are studied post hoc. However, in several applications, there is a\nnatural cost function that can be used to characterize the quality of the\nclustering. In those cases, hierarchical clustering can be seen as a\ncombinatorial optimization problem. To that end, we introduce a new approach\nbased on A* search. We overcome the prohibitively large search space by\ncombining A* with a novel \\emph{trellis} data structure. This combination\nresults in an exact algorithm that scales beyond previous state of the art,\nfrom a search space with $10^{12}$ trees to $10^{15}$ trees, and an approximate\nalgorithm that improves over baselines, even in enormous search spaces that\ncontain more than $10^{1000}$ trees. We empirically demonstrate that our method\nachieves substantially higher quality results than baselines for a particle\nphysics use case and other clustering benchmarks. We describe how our method\nprovides significantly improved theoretical bounds on the time and space\ncomplexity of A* for clustering.\n", "versions": [{"version": "v1", "created": "Wed, 14 Apr 2021 18:15:27 GMT"}], "update_date": "2021-04-16", "authors_parsed": [["Greenberg", "Craig S.", ""], ["Macaluso", "Sebastian", ""], ["Monath", "Nicholas", ""], ["Dubey", "Avinava", ""], ["Flaherty", "Patrick", ""], ["Zaheer", "Manzil", ""], ["Ahmed", "Amr", ""], ["Cranmer", "Kyle", ""], ["McCallum", "Andrew", ""]]}, {"id": "2104.07084", "submitter": "Hussein Hazimeh", "authors": "Hussein Hazimeh, Rahul Mazumder, Peter Radchenko", "title": "Grouped Variable Selection with Discrete Optimization: Computational and\n  Statistical Perspectives", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME cs.LG math.OC stat.CO stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a new algorithmic framework for grouped variable selection that is\nbased on discrete mathematical optimization. While there exist several\nappealing approaches based on convex relaxations and nonconvex heuristics, we\nfocus on optimal solutions for the $\\ell_0$-regularized formulation, a problem\nthat is relatively unexplored due to computational challenges. Our methodology\ncovers both high-dimensional linear regression and nonparametric sparse\nadditive modeling with smooth components. Our algorithmic framework consists of\napproximate and exact algorithms. The approximate algorithms are based on\ncoordinate descent and local search, with runtimes comparable to popular sparse\nlearning algorithms. Our exact algorithm is based on a standalone\nbranch-and-bound (BnB) framework, which can solve the associated mixed integer\nprogramming (MIP) problem to certified optimality. By exploiting the problem\nstructure, our custom BnB algorithm can solve to optimality problem instances\nwith $5 \\times 10^6$ features in minutes to hours -- over $1000$ times larger\nthan what is currently possible using state-of-the-art commercial MIP solvers.\nWe also explore statistical properties of the $\\ell_0$-based estimators. We\ndemonstrate, theoretically and empirically, that our proposed estimators have\nan edge over popular group-sparse estimators in terms of statistical\nperformance in various regimes.\n", "versions": [{"version": "v1", "created": "Wed, 14 Apr 2021 19:21:59 GMT"}], "update_date": "2021-04-16", "authors_parsed": [["Hazimeh", "Hussein", ""], ["Mazumder", "Rahul", ""], ["Radchenko", "Peter", ""]]}, {"id": "2104.07136", "submitter": "Pedro Kaufmann", "authors": "Alirio G\\'omez G\\'omez, Pedro L. Kaufmann", "title": "On the Vapnik-Chervonenkis dimension of products of intervals in\n  $\\mathbb{R}^d$", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.MG cs.LG math.CO stat.ML", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  We study combinatorial complexity of certain classes of products of intervals\nin $\\mathbb{R}^d$, from the point of view of Vapnik-Chervonenkis geometry. As a\nconsequence of the obtained results, we conclude that the Vapnik-Chervonenkis\ndimension of the set of balls in $\\ell_\\infty^d$ -- which denotes $\\R^d$\nequipped with the sup norm -- equals $\\lfloor (3d+1)/2\\rfloor$.\n", "versions": [{"version": "v1", "created": "Wed, 14 Apr 2021 21:40:15 GMT"}], "update_date": "2021-04-16", "authors_parsed": [["G\u00f3mez", "Alirio G\u00f3mez", ""], ["Kaufmann", "Pedro L.", ""]]}, {"id": "2104.07167", "submitter": "Asher Trockman", "authors": "Asher Trockman, J. Zico Kolter", "title": "Orthogonalizing Convolutional Layers with the Cayley Transform", "comments": "To appear in ICLR 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recent work has highlighted several advantages of enforcing orthogonality in\nthe weight layers of deep networks, such as maintaining the stability of\nactivations, preserving gradient norms, and enhancing adversarial robustness by\nenforcing low Lipschitz constants. Although numerous methods exist for\nenforcing the orthogonality of fully-connected layers, those for convolutional\nlayers are more heuristic in nature, often focusing on penalty methods or\nlimited classes of convolutions. In this work, we propose and evaluate an\nalternative approach to directly parameterize convolutional layers that are\nconstrained to be orthogonal. Specifically, we propose to apply the Cayley\ntransform to a skew-symmetric convolution in the Fourier domain, so that the\ninverse convolution needed by the Cayley transform can be computed efficiently.\nWe compare our method to previous Lipschitz-constrained and orthogonal\nconvolutional layers and show that it indeed preserves orthogonality to a high\ndegree even for large convolutions. Applied to the problem of certified\nadversarial robustness, we show that networks incorporating the layer\noutperform existing deterministic methods for certified defense against\n$\\ell_2$-norm-bounded adversaries, while scaling to larger architectures than\npreviously investigated. Code is available at\nhttps://github.com/locuslab/orthogonal-convolutions.\n", "versions": [{"version": "v1", "created": "Wed, 14 Apr 2021 23:54:55 GMT"}], "update_date": "2021-04-19", "authors_parsed": [["Trockman", "Asher", ""], ["Kolter", "J. Zico", ""]]}, {"id": "2104.07191", "submitter": "Xiaodong Wang", "authors": "Fushing Hsieh and Xiaodong Wang", "title": "Coarse- and fine-scale geometric information content of Multiclass\n  Classification and implied Data-driven Intelligence", "comments": "15 pages, 5 figures", "journal-ref": "In Proceedings of the 16th International Conference on Machine\n  Learning and Data Mining, MLDM 2020, July 20-21, 2020, Amsterdam, The\n  Netherlands, pp 171-184", "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Under any Multiclass Classification (MCC) setting defined by a collection of\nlabeled point-cloud specified by a feature-set, we extract only stochastic\npartial orderings from all possible triplets of point-cloud without explicitly\nmeasuring the three cloud-to-cloud distances. We demonstrate that such a\ncollective of partial ordering can efficiently compute a label embedding tree\ngeometry on the Label-space. This tree in turn gives rise to a predictive\ngraph, or a network with precisely weighted linkages. Such two multiscale\ngeometries are taken as the coarse scale information content of MCC. They\nindeed jointly shed lights on explainable knowledge on why and how labeling\ncomes about and facilitates error-free prediction with potential multiple\ncandidate labels supported by data. For revealing within-label heterogeneity,\nwe further undergo labeling naturally found clusters within each point-cloud,\nand likewise derive multiscale geometry as its fine-scale information content\ncontained in data. This fine-scale endeavor shows that our computational\nproposal is indeed scalable to a MCC setting having a large label-space.\nOverall the computed multiscale collective of data-driven patterns and\nknowledge will serve as a basis for constructing visible and explainable\nsubject matter intelligence regarding the system of interest.\n", "versions": [{"version": "v1", "created": "Thu, 15 Apr 2021 01:24:17 GMT"}], "update_date": "2021-04-16", "authors_parsed": [["Hsieh", "Fushing", ""], ["Wang", "Xiaodong", ""]]}, {"id": "2104.07232", "submitter": "Zeyu Zhou", "authors": "David I. Inouye, Zeyu Zhou, Ziyu Gong, Pradeep Ravikumar", "title": "Iterative Barycenter Flows", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The task of mapping two or more distributions to a shared representation has\nmany applications including fair representations, batch effect mitigation, and\nunsupervised domain adaptation. However, most existing formulations only\nconsider the setting of two distributions, and moreover, do not have an\nidentifiable, unique shared latent representation. We use optimal transport\ntheory to consider a natural multiple distribution extension of the Monge\nassignment problem we call the symmetric Monge map problem and show that it is\nequivalent to the Wasserstein barycenter problem. Yet, the maps to the\nbarycenter are challenging to estimate. Prior methods often ignore\ntransportation cost, rely on adversarial methods, or only work for discrete\ndistributions. Therefore, our goal is to estimate invertible maps between two\nor more distributions and their corresponding barycenter via a simple iterative\nflow method. Our method decouples each iteration into two subproblems: 1)\nestimate simple distributions and 2) estimate the invertible maps to the\nbarycenter via known closed-form OT results. Our empirical results give\nevidence that this iterative algorithm approximates the maps to the barycenter.\n", "versions": [{"version": "v1", "created": "Thu, 15 Apr 2021 04:28:56 GMT"}], "update_date": "2021-04-16", "authors_parsed": [["Inouye", "David I.", ""], ["Zhou", "Zeyu", ""], ["Gong", "Ziyu", ""], ["Ravikumar", "Pradeep", ""]]}, {"id": "2104.07294", "submitter": "Christopher Bamford", "authors": "Christopher Bamford, Alvaro Ovalle", "title": "Generalising Discrete Action Spaces with Conditional Action Trees", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI stat.ML", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  There are relatively few conventions followed in reinforcement learning (RL)\nenvironments to structure the action spaces. As a consequence the application\nof RL algorithms to tasks with large action spaces with multiple components\nrequire additional effort to adjust to different formats. In this paper we\nintroduce {\\em Conditional Action Trees} with two main objectives: (1) as a\nmethod of structuring action spaces in RL to generalise across several action\nspace specifications, and (2) to formalise a process to significantly reduce\nthe action space by decomposing it into multiple sub-spaces, favoring a\nmulti-staged decision making approach. We show several proof-of-concept\nexperiments validating our scheme, ranging from environments with basic\ndiscrete action spaces to those with large combinatorial action spaces commonly\nfound in RTS-style games.\n", "versions": [{"version": "v1", "created": "Thu, 15 Apr 2021 08:10:18 GMT"}], "update_date": "2021-04-16", "authors_parsed": [["Bamford", "Christopher", ""], ["Ovalle", "Alvaro", ""]]}, {"id": "2104.07295", "submitter": "Shuiqiao Yang", "authors": "Shuiqiao Yang, Sunny Verma, Borui Cai, Jiaojiao Jiang, Kun Yu, Fang\n  Chen, Shui Yu", "title": "Variational Co-embedding Learning for Attributed Network Clustering", "comments": "This manuscript is under review", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recent works for attributed network clustering utilize graph convolution to\nobtain node embeddings and simultaneously perform clustering assignments on the\nembedding space. It is effective since graph convolution combines the\nstructural and attributive information for node embedding learning. However, a\nmajor limitation of such works is that the graph convolution only incorporates\nthe attribute information from the local neighborhood of nodes but fails to\nexploit the mutual affinities between nodes and attributes. In this regard, we\npropose a variational co-embedding learning model for attributed network\nclustering (VCLANC). VCLANC is composed of dual variational auto-encoders to\nsimultaneously embed nodes and attributes. Relying on this, the mutual affinity\ninformation between nodes and attributes could be reconstructed from the\nembedding space and served as extra self-supervised knowledge for\nrepresentation learning. At the same time, trainable Gaussian mixture model is\nused as priors to infer the node clustering assignments. To strengthen the\nperformance of the inferred clusters, we use a mutual distance loss on the\ncenters of the Gaussian priors and a clustering assignment hardening loss on\nthe node embeddings. Experimental results on four real-world attributed network\ndatasets demonstrate the effectiveness of the proposed VCLANC for attributed\nnetwork clustering.\n", "versions": [{"version": "v1", "created": "Thu, 15 Apr 2021 08:11:47 GMT"}], "update_date": "2021-04-16", "authors_parsed": [["Yang", "Shuiqiao", ""], ["Verma", "Sunny", ""], ["Cai", "Borui", ""], ["Jiang", "Jiaojiao", ""], ["Yu", "Kun", ""], ["Chen", "Fang", ""], ["Yu", "Shui", ""]]}, {"id": "2104.07323", "submitter": "Wanyang Dai", "authors": "Wanyang Dai", "title": "Internet of quantum blockchains: security modeling and dynamic resource\n  pricing for stable digital currency", "comments": "40 pages, 11 figures. This paper initially appeared in Preprints\n  (called Proceeding) of 22th Annual Conference of Jiangsu Association of\n  Applied Statistics, pages 7-45, November 13-15, 2020, Suzhou, China", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.OC cs.GT cs.IT math.IT math.PR stat.ML", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Internet of quantum blockchains (IoB) will be the future Internet. In this\npaper, we make two new contributions to IoB: developing a block based quantum\nchannel networking technology to handle its security modeling in face of the\nquantum supremacy and establishing IoB based FinTech platform model with\ndynamic pricing for stable digital currency. The interaction between our new\ncontributions is also addressed. In doing so, we establish a generalized IoB\nsecurity model by quantum channel networking in terms of both time and space\nquantum entanglements with quantum key distribution (QKD). Our IoB can interact\nwith general structured things (e.g., supply chain systems) having online\ntrading and payment capability via stable digital currency and can handle\nvector-valued data streams requiring synchronized services. Thus, within our\ndesigned QKD, a generalized random number generator for private and public keys\nis proposed by a mixed zero-sum and non-zero-sum resource-competition pricing\npolicy. The effectiveness of this policy is justified by diffusion modeling\nwith approximation theory and numerical implementations.\n", "versions": [{"version": "v1", "created": "Thu, 15 Apr 2021 09:23:10 GMT"}], "update_date": "2021-04-16", "authors_parsed": [["Dai", "Wanyang", ""]]}, {"id": "2104.07359", "submitter": "Takuo Matsubara", "authors": "Takuo Matsubara, Jeremias Knoblauch, Fran\\c{c}ois-Xavier Briol, Chris.\n  J. Oates", "title": "Robust Generalised Bayesian Inference for Intractable Likelihoods", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME math.ST stat.CO stat.ML stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Generalised Bayesian inference updates prior beliefs using a loss function,\nrather than a likelihood, and can therefore be used to confer robustness\nagainst possible misspecification of the likelihood. Here we consider\ngeneralised Bayesian inference with a Stein discrepancy as a loss function,\nmotivated by applications in which the likelihood contains an intractable\nnormalisation constant. In this context, the Stein discrepancy circumvents\nevaluation of the normalisation constant and produces generalised posteriors\nthat are either closed form or accessible using standard Markov chain Monte\nCarlo. On a theoretical level, we show consistency, asymptotic normality, and\nbias-robustness of the generalised posterior, highlighting how these properties\nare impacted by the choice of Stein discrepancy. Then, we provide numerical\nexperiments on a range of intractable distributions, including applications to\nkernel-based exponential family models and non-Gaussian graphical models.\n", "versions": [{"version": "v1", "created": "Thu, 15 Apr 2021 10:31:22 GMT"}], "update_date": "2021-04-16", "authors_parsed": [["Matsubara", "Takuo", ""], ["Knoblauch", "Jeremias", ""], ["Briol", "Fran\u00e7ois-Xavier", ""], ["Oates", "Chris. J.", ""]]}, {"id": "2104.07495", "submitter": "Pietro Mazzaglia", "authors": "Pietro Mazzaglia, Ozan Catal, Tim Verbelen, Bart Dhoedt", "title": "Self-Supervised Exploration via Latent Bayesian Surprise", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI stat.ML", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Training with Reinforcement Learning requires a reward function that is used\nto guide the agent towards achieving its objective. However, designing smooth\nand well-behaved rewards is in general not trivial and requires significant\nhuman engineering efforts. Generating rewards in self-supervised way, by\ninspiring the agent with an intrinsic desire to learn and explore the\nenvironment, might induce more general behaviours. In this work, we propose a\ncuriosity-based bonus as intrinsic reward for Reinforcement Learning, computed\nas the Bayesian surprise with respect to a latent state variable, learnt by\nreconstructing fixed random features. We extensively evaluate our model by\nmeasuring the agent's performance in terms of environment exploration, for\ncontinuous tasks, and looking at the game scores achieved, for video games. Our\nmodel is computationally cheap and empirically shows state-of-the-art\nperformance on several problems. Furthermore, experimenting on an environment\nwith stochastic actions, our approach emerged to be the most resilient to\nsimple stochasticity. Further visualization is available on the project\nwebpage.(https://lbsexploration.github.io/)\n", "versions": [{"version": "v1", "created": "Thu, 15 Apr 2021 14:40:16 GMT"}], "update_date": "2021-04-16", "authors_parsed": [["Mazzaglia", "Pietro", ""], ["Catal", "Ozan", ""], ["Verbelen", "Tim", ""], ["Dhoedt", "Bart", ""]]}, {"id": "2104.07505", "submitter": "Karolina Stanczak", "authors": "Karolina Sta\\'nczak, Sagnik Ray Choudhury, Tiago Pimentel, Ryan\n  Cotterell, Isabelle Augenstein", "title": "Quantifying Gender Bias Towards Politicians in Cross-Lingual Language\n  Models", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  While the prevalence of large pre-trained language models has led to\nsignificant improvements in the performance of NLP systems, recent research has\ndemonstrated that these models inherit societal biases extant in natural\nlanguage. In this paper, we explore a simple method to probe pre-trained\nlanguage models for gender bias, which we use to effect a multi-lingual study\nof gender bias towards politicians. We construct a dataset of 250k politicians\nfrom most countries in the world and quantify adjective and verb usage around\nthose politicians' names as a function of their gender. We conduct our study in\n7 languages across 6 different language modeling architectures. Our results\ndemonstrate that stance towards politicians in pre-trained language models is\nhighly dependent on the language used. Finally, contrary to previous findings,\nour study suggests that larger language models do not tend to be significantly\nmore gender-biased than smaller ones.\n", "versions": [{"version": "v1", "created": "Thu, 15 Apr 2021 15:03:26 GMT"}], "update_date": "2021-04-16", "authors_parsed": [["Sta\u0144czak", "Karolina", ""], ["Choudhury", "Sagnik Ray", ""], ["Pimentel", "Tiago", ""], ["Cotterell", "Ryan", ""], ["Augenstein", "Isabelle", ""]]}, {"id": "2104.07531", "submitter": "Carles Domingo-Enrich", "authors": "Carles Domingo-Enrich, Alberto Bietti, Eric Vanden-Eijnden, Joan Bruna", "title": "On Energy-Based Models with Overparametrized Shallow Neural Networks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Energy-based models (EBMs) are a simple yet powerful framework for generative\nmodeling. They are based on a trainable energy function which defines an\nassociated Gibbs measure, and they can be trained and sampled from via\nwell-established statistical tools, such as MCMC. Neural networks may be used\nas energy function approximators, providing both a rich class of expressive\nmodels as well as a flexible device to incorporate data structure. In this work\nwe focus on shallow neural networks. Building from the incipient theory of\noverparametrized neural networks, we show that models trained in the so-called\n\"active\" regime provide a statistical advantage over their associated \"lazy\" or\nkernel regime, leading to improved adaptivity to hidden low-dimensional\nstructure in the data distribution, as already observed in supervised learning.\nOur study covers both maximum likelihood and Stein Discrepancy estimators, and\nwe validate our theoretical results with numerical experiments on synthetic\ndata.\n", "versions": [{"version": "v1", "created": "Thu, 15 Apr 2021 15:34:58 GMT"}, {"version": "v2", "created": "Wed, 5 May 2021 14:44:39 GMT"}], "update_date": "2021-05-06", "authors_parsed": [["Domingo-Enrich", "Carles", ""], ["Bietti", "Alberto", ""], ["Vanden-Eijnden", "Eric", ""], ["Bruna", "Joan", ""]]}, {"id": "2104.07651", "submitter": "Lukas Heumos", "authors": "Lukas Heumos, Philipp Ehmele, Kevin Menden, Luis Kuhn Cuellar, Edmund\n  Miller, Steffen Lemke, Gisela Gabernet and Sven Nahnsen", "title": "mlf-core: a framework for deterministic machine learning", "comments": "https://mlf-core.com and https://github.com/mlf-core/mlf-core", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.MS cs.LG q-bio.QM stat.ML", "license": "http://creativecommons.org/licenses/by-sa/4.0/", "abstract": "  Machine learning has shown extensive growth in recent years. However,\npreviously existing studies highlighted a reproducibility crisis in machine\nlearning. The reasons for irreproducibility are manifold. Major machine\nlearning libraries default to the usage of non-deterministic algorithms based\non atomic operations. Solely fixing all random seeds is not sufficient for\ndeterministic machine learning. To overcome this shortcoming, various machine\nlearning libraries released deterministic counterparts to the non-deterministic\nalgorithms. We evaluated the effect of these algorithms on determinism and\nruntime. Based on these results, we formulated a set of requirements for\nreproducible machine learning and developed a new software solution, the\nmlf-core ecosystem, which aids machine learning projects to meet and keep these\nrequirements. We applied mlf-core to develop fully reproducible models in\nvarious biomedical fields including a single cell autoencoder with TensorFlow,\na PyTorch-based U-Net model for liver-tumor segmentation in CT scans, and a\nliver cancer classifier based on gene expression profiles with XGBoost.\n", "versions": [{"version": "v1", "created": "Thu, 15 Apr 2021 17:58:03 GMT"}], "update_date": "2021-04-16", "authors_parsed": [["Heumos", "Lukas", ""], ["Ehmele", "Philipp", ""], ["Menden", "Kevin", ""], ["Cuellar", "Luis Kuhn", ""], ["Miller", "Edmund", ""], ["Lemke", "Steffen", ""], ["Gabernet", "Gisela", ""], ["Nahnsen", "Sven", ""]]}, {"id": "2104.07737", "submitter": "Farzana Nasrin", "authors": "Farzana Nasrin, Theodore Papamarkou, and Vasileios Maroulas", "title": "Random Persistence Diagram Generation", "comments": "27 pages, 10 figures and 5 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG math.AT", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Topological data analysis (TDA) studies the shape patterns of data.\nPersistent homology (PH) is a widely used method in TDA that summarizes\nhomological features of data at multiple scales and stores this in persistence\ndiagrams (PDs). As TDA is commonly used in the analysis of high dimensional\ndata sets, a sufficiently large amount of PDs that allow performing statistical\nanalysis is typically unavailable or requires inordinate computational\nresources. In this paper, we propose random persistence diagram generation\n(RPDG), a method that generates a sequence of random PDs from the ones produced\nby the data. RPDG is underpinned (i) by a parametric model based on pairwise\ninteracting point processes for inference of persistence diagrams and (ii) by a\nreversible jump Markov chain Monte Carlo (RJ-MCMC) algorithm for generating\nsamples of PDs. The parametric model combines a Dirichlet partition to capture\nspatial homogeneity of the location of points in a PD and a step function to\ncapture the pairwise interaction between them. The RJ-MCMC algorithm\nincorporates trans-dimensional addition and removal of points and\nsame-dimensional relocation of points across samples of PDs. The efficacy of\nRPDG is demonstrated via an example and a detailed comparison with other\nexisting methods is presented.\n", "versions": [{"version": "v1", "created": "Thu, 15 Apr 2021 19:33:01 GMT"}], "update_date": "2021-04-19", "authors_parsed": [["Nasrin", "Farzana", ""], ["Papamarkou", "Theodore", ""], ["Maroulas", "Vasileios", ""]]}, {"id": "2104.07773", "submitter": "Emma Jingfei Zhang", "authors": "Biao Cai, Jingfei Zhang and Will Wei Sun", "title": "Heterogeneous Tensor Mixture Models in High Dimensions", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME math.ST stat.ML stat.TH", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  We consider the problem of jointly modeling and clustering populations of\ntensors by introducing a flexible high-dimensional tensor mixture model with\nheterogeneous covariances. The proposed mixture model exploits the intrinsic\nstructures of tensor data, and is assumed to have means that are low-rank and\ninternally sparse as well as heterogeneous covariances that are separable and\nconditionally sparse. We develop an efficient high-dimensional\nexpectation-conditional-maximization (HECM) algorithm that breaks the\nchallenging optimization in the M-step into several simpler conditional\noptimization problems, each of which is convex, admits regularization and has\nclosed-form updating formulas. We show that the proposed HECM algorithm, with\nan appropriate initialization, converges geometrically to a neighborhood that\nis within statistical precision of the true parameter. Such a theoretical\nanalysis is highly nontrivial due to the dual non-convexity arising from both\nthe EM-type estimation and the non-convex objective function in the M-step. The\nefficacy of our proposed method is demonstrated through simulation studies and\nan application to an autism spectrum disorder study, where our analysis\nidentifies important brain regions for diagnosis.\n", "versions": [{"version": "v1", "created": "Thu, 15 Apr 2021 21:06:16 GMT"}], "update_date": "2021-04-19", "authors_parsed": [["Cai", "Biao", ""], ["Zhang", "Jingfei", ""], ["Sun", "Will Wei", ""]]}, {"id": "2104.07820", "submitter": "Muhammad Aurangzeb Ahmad", "authors": "Aloysius Lim, Ashish Singh, Jody Chiam, Carly Eckert, Vikas Kumar,\n  Muhammad Aurangzeb Ahmad, Ankur Teredesai", "title": "Machine Learning Approaches for Type 2 Diabetes Prediction and Care\n  Management", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Prediction of diabetes and its various complications has been studied in a\nnumber of settings, but a comprehensive overview of problem setting for\ndiabetes prediction and care management has not been addressed in the\nliterature. In this document we seek to remedy this omission in literature with\nan encompassing overview of diabetes complication prediction as well as\nsituating this problem in the context of real world healthcare management. We\nillustrate various problems encountered in real world clinical scenarios via\nour own experience with building and deploying such models. In this manuscript\nwe illustrate a Machine Learning (ML) framework for addressing the problem of\npredicting Type 2 Diabetes Mellitus (T2DM) together with a solution for risk\nstratification, intervention and management. These ML models align with how\nphysicians think about disease management and mitigation, which comprises these\nfour steps: Identify, Stratify, Engage, Measure.\n", "versions": [{"version": "v1", "created": "Thu, 15 Apr 2021 23:38:39 GMT"}, {"version": "v2", "created": "Thu, 29 Apr 2021 00:11:58 GMT"}], "update_date": "2021-04-30", "authors_parsed": [["Lim", "Aloysius", ""], ["Singh", "Ashish", ""], ["Chiam", "Jody", ""], ["Eckert", "Carly", ""], ["Kumar", "Vikas", ""], ["Ahmad", "Muhammad Aurangzeb", ""], ["Teredesai", "Ankur", ""]]}, {"id": "2104.07822", "submitter": "Bo Zhang", "authors": "Shuxiao Chen, Bo Zhang", "title": "Estimating and Improving Dynamic Treatment Regimes With a Time-Varying\n  Instrumental Variable", "comments": "67 pages, 9 figures, 6 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME cs.LG econ.EM stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Estimating dynamic treatment regimes (DTRs) from retrospective observational\ndata is challenging as some degree of unmeasured confounding is often expected.\nIn this work, we develop a framework of estimating properly defined \"optimal\"\nDTRs with a time-varying instrumental variable (IV) when unmeasured covariates\nconfound the treatment and outcome, rendering the potential outcome\ndistributions only partially identified. We derive a novel Bellman equation\nunder partial identification, use it to define a generic class of estimands\n(termed IV-optimal DTRs), and study the associated estimation problem. We then\nextend the IV-optimality framework to tackle the policy improvement problem,\ndelivering IV-improved DTRs that are guaranteed to perform no worse and\npotentially better than a pre-specified baseline DTR. Importantly, our\nIV-improvement framework opens up the possibility of strictly improving upon\nDTRs that are optimal under the no unmeasured confounding assumption (NUCA). We\ndemonstrate via extensive simulations the superior performance of IV-optimal\nand IV-improved DTRs over the DTRs that are optimal only under the NUCA. In a\nreal data example, we embed retrospective observational registry data into a\nnatural, two-stage experiment with noncompliance using a time-varying IV and\nestimate useful IV-optimal DTRs that assign mothers to high-level or low-level\nneonatal intensive care units based on their prognostic variables.\n", "versions": [{"version": "v1", "created": "Thu, 15 Apr 2021 23:44:39 GMT"}], "update_date": "2021-04-19", "authors_parsed": [["Chen", "Shuxiao", ""], ["Zhang", "Bo", ""]]}, {"id": "2104.07824", "submitter": "Shashank Sonkar", "authors": "Shashank Sonkar, Arzoo Katiyar and Richard G. Baraniuk", "title": "NePTuNe: Neural Powered Tucker Network for Knowledge Graph Completion", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Knowledge graphs link entities through relations to provide a structured\nrepresentation of real world facts. However, they are often incomplete, because\nthey are based on only a small fraction of all plausible facts. The task of\nknowledge graph completion via link prediction aims to overcome this challenge\nby inferring missing facts represented as links between entities. Current\napproaches to link prediction leverage tensor factorization and/or deep\nlearning. Factorization methods train and deploy rapidly thanks to their small\nnumber of parameters but have limited expressiveness due to their underlying\nlinear methodology. Deep learning methods are more expressive but also\ncomputationally expensive and prone to overfitting due to their large number of\ntrainable parameters. We propose Neural Powered Tucker Network (NePTuNe), a new\nhybrid link prediction model that couples the expressiveness of deep models\nwith the speed and size of linear models. We demonstrate that NePTuNe provides\nstate-of-the-art performance on the FB15K-237 dataset and near state-of-the-art\nperformance on the WN18RR dataset.\n", "versions": [{"version": "v1", "created": "Thu, 15 Apr 2021 23:48:26 GMT"}], "update_date": "2021-04-19", "authors_parsed": [["Sonkar", "Shashank", ""], ["Katiyar", "Arzoo", ""], ["Baraniuk", "Richard G.", ""]]}, {"id": "2104.07932", "submitter": "Marian-Andrei Rizoiu", "authors": "Marian-Andrei Rizoiu, Alexander Soen, Shidi Li, Pio Calderon, Leanne\n  Dong, Aditya Krishna Menon and Lexing Xie", "title": "Interval-censored Hawkes processes", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CE stat.ML", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  This work builds a novel point process and tools to use the Hawkes process\nwith interval-censored data. Such data records the aggregated counts of events\nsolely during specific time intervals -- such as the number of patients\nadmitted to the hospital or the volume of vehicles passing traffic loop\ndetectors -- and not the exact occurrence time of the events. First, we\nestablish the Mean Behavior Poisson (MBP) process, a novel Poisson process with\na direct parameter correspondence to the popular self-exciting Hawkes process.\nThe event intensity function of the MBP is the expected intensity over all\npossible Hawkes realizations with the same parameter set. We fit MBP in the\ninterval-censored setting using an interval-censored Poisson log-likelihood\n(IC-LL). We use the parameter equivalence to uncover the parameters of the\nassociated Hawkes process. Second, we introduce two novel exogenous functions\nto distinguish the exogenous from the endogenous events. We propose the\nmulti-impulse exogenous function when the exogenous events are observed as\nevent time and the latent homogeneous Poisson process exogenous function when\nthe exogenous events are presented as interval-censored volumes. Third, we\nprovide several approximation methods to estimate the intensity and compensator\nfunction of MBP when no analytical solution exists. Fourth and finally, we\nconnect the interval-censored loss of MBP to a broader class of Bregman\ndivergence-based functions. Using the connection, we show that the current\nstate of the art in popularity estimation (Hawkes Intensity Process (HIP)\n(Rizoiu et al.,2017b)) is a particular case of the MBP process. We verify our\nmodels through empirical testing on synthetic data and real-world data. We find\nthat on real-world datasets that our MBP process outperforms HIP for the task\nof popularity prediction.\n", "versions": [{"version": "v1", "created": "Fri, 16 Apr 2021 07:29:04 GMT"}, {"version": "v2", "created": "Wed, 16 Jun 2021 01:58:51 GMT"}, {"version": "v3", "created": "Fri, 23 Jul 2021 00:07:50 GMT"}], "update_date": "2021-07-26", "authors_parsed": [["Rizoiu", "Marian-Andrei", ""], ["Soen", "Alexander", ""], ["Li", "Shidi", ""], ["Calderon", "Pio", ""], ["Dong", "Leanne", ""], ["Menon", "Aditya Krishna", ""], ["Xie", "Lexing", ""]]}, {"id": "2104.07985", "submitter": "Georgia Papacharalampous", "authors": "Georgia Papacharalampous, Andreas Langousis", "title": "Probabilistic water demand forecasting using quantile regression\n  algorithms", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG stat.ME", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Machine and statistical learning algorithms can be reliably automated and\napplied at scale. Therefore, they can constitute a considerable asset for\ndesigning practical forecasting systems, such as those related to urban water\ndemand. Quantile regression algorithms are statistical and machine learning\nalgorithms that can provide probabilistic forecasts in a straightforward way,\nand have not been applied so far for urban water demand forecasting. In this\nwork, we aim to fill this gap by automating and extensively comparing several\nquantile-regression-based practical systems for probabilistic one-day ahead\nurban water demand forecasting. For designing the practical systems, we use\nfive individual algorithms (i.e., the quantile regression, linear boosting,\ngeneralized random forest, gradient boosting machine and quantile regression\nneural network algorithms), their mean combiner and their median combiner. The\ncomparison is conducted by exploiting a large urban water flow dataset, as well\nas several types of hydrometeorological time series (which are considered as\nexogenous predictor variables in the forecasting setting). The results mostly\nfavour the practical systems designed using the linear boosting algorithm,\nprobably due to the presence of trends in the urban water flow time series. The\nforecasts of the mean and median combiners are also found to be skilful in\ngeneral terms.\n", "versions": [{"version": "v1", "created": "Fri, 16 Apr 2021 09:17:00 GMT"}], "update_date": "2021-04-19", "authors_parsed": [["Papacharalampous", "Georgia", ""], ["Langousis", "Andreas", ""]]}, {"id": "2104.08043", "submitter": "Andrew Lawrence", "authors": "Andrew R. Lawrence, Marcus Kaiser, Rui Sampaio, Maksim Sipos", "title": "Data Generating Process to Evaluate Causal Discovery Techniques for Time\n  Series Data", "comments": "17 pages, 9 figures, for associated code and data sets, see\n  https://github.com/causalens/cdml-neurips2020", "journal-ref": "Causal Discovery & Causality-Inspired Machine Learning Workshop at\n  Neural Information Processing Systems, 2020", "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Going beyond correlations, the understanding and identification of causal\nrelationships in observational time series, an important subfield of Causal\nDiscovery, poses a major challenge. The lack of access to a well-defined ground\ntruth for real-world data creates the need to rely on synthetic data for the\nevaluation of these methods. Existing benchmarks are limited in their scope, as\nthey either are restricted to a \"static\" selection of data sets, or do not\nallow for a granular assessment of the methods' performance when commonly made\nassumptions are violated. We propose a flexible and simple to use framework for\ngenerating time series data, which is aimed at developing, evaluating, and\nbenchmarking time series causal discovery methods. In particular, the framework\ncan be used to fine tune novel methods on vast amounts of data, without\n\"overfitting\" them to a benchmark, but rather so they perform well in\nreal-world use cases. Using our framework, we evaluate prominent time series\ncausal discovery methods and demonstrate a notable degradation in performance\nwhen their assumptions are invalidated and their sensitivity to choice of\nhyperparameters. Finally, we propose future research directions and how our\nframework can support both researchers and practitioners.\n", "versions": [{"version": "v1", "created": "Fri, 16 Apr 2021 11:38:29 GMT"}], "update_date": "2021-04-19", "authors_parsed": [["Lawrence", "Andrew R.", ""], ["Kaiser", "Marcus", ""], ["Sampaio", "Rui", ""], ["Sipos", "Maksim", ""]]}, {"id": "2104.08134", "submitter": "Daniel Heestermans Svendsen", "authors": "Daniel Heestermans Svendsen, Maria Piles, Jordi Mu\\~noz-Mar\\'i, David\n  Luengo, Luca Martino and Gustau Camps-Valls", "title": "Integrating Domain Knowledge in Data-driven Earth Observation with\n  Process Convolutions", "comments": null, "journal-ref": null, "doi": "10.1109/TGRS.2021.3059550", "report-no": null, "categories": "stat.ML cs.LG", "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/", "abstract": "  The modelling of Earth observation data is a challenging problem, typically\napproached by either purely mechanistic or purely data-driven methods.\nMechanistic models encode the domain knowledge and physical rules governing the\nsystem. Such models, however, need the correct specification of all\ninteractions between variables in the problem and the appropriate\nparameterization is a challenge in itself. On the other hand, machine learning\napproaches are flexible data-driven tools, able to approximate arbitrarily\ncomplex functions, but lack interpretability and struggle when data is scarce\nor in extrapolation regimes. In this paper, we argue that hybrid learning\nschemes that combine both approaches can address all these issues efficiently.\nWe introduce Gaussian process (GP) convolution models for hybrid modelling in\nEarth observation (EO) problems. We specifically propose the use of a class of\nGP convolution models called latent force models (LFMs) for EO time series\nmodelling, analysis and understanding. LFMs are hybrid models that incorporate\nphysical knowledge encoded in differential equations into a multioutput GP\nmodel. LFMs can transfer information across time-series, cope with missing\nobservations, infer explicit latent functions forcing the system, and learn\nparameterizations which are very helpful for system analysis and\ninterpretability. We consider time series of soil moisture from active (ASCAT)\nand passive (SMOS, AMSR2) microwave satellites. We show how assuming a first\norder differential equation as governing equation, the model automatically\nestimates the e-folding time or decay rate related to soil moisture persistence\nand discovers latent forces related to precipitation. The proposed hybrid\nmethodology reconciles the two main approaches in remote sensing parameter\nestimation by blending statistical learning and mechanistic modeling.\n", "versions": [{"version": "v1", "created": "Fri, 16 Apr 2021 14:30:40 GMT"}], "update_date": "2021-04-19", "authors_parsed": [["Svendsen", "Daniel Heestermans", ""], ["Piles", "Maria", ""], ["Mu\u00f1oz-Mar\u00ed", "Jordi", ""], ["Luengo", "David", ""], ["Martino", "Luca", ""], ["Camps-Valls", "Gustau", ""]]}, {"id": "2104.08156", "submitter": "Eliane Maalouf", "authors": "Eliane Maalouf, David Ginsbourger and Niklas Linde", "title": "Fast ABC with joint generative modelling and subset simulation", "comments": "13 pages, 6 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME cs.AI cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a novel approach for solving inverse-problems with\nhigh-dimensional inputs and an expensive forward mapping. It leverages joint\ndeep generative modelling to transfer the original problem spaces to a lower\ndimensional latent space. By jointly modelling input and output variables and\nendowing the latent with a prior distribution, the fitted probabilistic model\nindirectly gives access to the approximate conditional distributions of\ninterest. Since model error and observational noise with unknown distributions\nare common in practice, we resort to likelihood-free inference with Approximate\nBayesian Computation (ABC). Our method calls on ABC by Subset Simulation to\nexplore the regions of the latent space with dissimilarities between generated\nand observed outputs below prescribed thresholds. We diagnose the diversity of\napproximate posterior solutions by monitoring the probability content of these\nregions as a function of the threshold. We further analyze the curvature of the\nresulting diagnostic curve to propose an adequate ABC threshold. When applied\nto a cross-borehole tomography example from geophysics, our approach delivers\npromising performance without using prior knowledge of the forward nor of the\nnoise distribution.\n", "versions": [{"version": "v1", "created": "Fri, 16 Apr 2021 15:03:23 GMT"}], "update_date": "2021-04-19", "authors_parsed": [["Maalouf", "Eliane", ""], ["Ginsbourger", "David", ""], ["Linde", "Niklas", ""]]}, {"id": "2104.08163", "submitter": "Peter Bloem", "authors": "Peter Bloem", "title": "Finding Motifs in Knowledge Graphs using Compression", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.DS cs.LG cs.SI", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  We introduce a method to find network motifs in knowledge graphs. Network\nmotifs are useful patterns or meaningful subunits of the graph that recur\nfrequently. We extend the common definition of a network motif to coincide with\na basic graph pattern. We introduce an approach, inspired by recent work for\nsimple graphs, to induce these from a given knowledge graph, and show that the\nmotifs found reflect the basic structure of the graph. Specifically, we show\nthat in random graphs, no motifs are found, and that when we insert a motif\nartificially, it can be detected. Finally, we show the results of motif\ninduction on three real-world knowledge graphs.\n", "versions": [{"version": "v1", "created": "Fri, 16 Apr 2021 15:20:44 GMT"}], "update_date": "2021-04-19", "authors_parsed": [["Bloem", "Peter", ""]]}, {"id": "2104.08166", "submitter": "Huibin Shen", "authors": "Anastasia Makarova, Huibin Shen, Valerio Perrone, Aaron Klein, Jean\n  Baptiste Faddoul, Andreas Krause, Matthias Seeger, Cedric Archambeau", "title": "Overfitting in Bayesian Optimization: an empirical study and\n  early-stopping solution", "comments": "Under review", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI stat.ML", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Tuning machine learning models with Bayesian optimization (BO) is a\nsuccessful strategy to find good hyperparameters. BO defines an iterative\nprocedure where a cross-validated metric is evaluated on promising\nhyperparameters. In practice, however, an improvement of the validation metric\nmay not translate in better predictive performance on a test set, especially\nwhen tuning models trained on small datasets. In other words, unlike\nconventional wisdom dictates, BO can overfit. In this paper, we carry out the\nfirst systematic investigation of overfitting in BO and demonstrate that this\nissue is serious, yet often overlooked in practice. We propose a novel\ncriterion to early stop BO, which aims to maintain the solution quality while\nsaving the unnecessary iterations that can lead to overfitting. Experiments on\nreal-world hyperparameter optimization problems show that our approach\neffectively meets these goals and is more adaptive comparing to baselines.\n", "versions": [{"version": "v1", "created": "Fri, 16 Apr 2021 15:26:23 GMT"}, {"version": "v2", "created": "Fri, 11 Jun 2021 14:25:25 GMT"}], "update_date": "2021-06-14", "authors_parsed": [["Makarova", "Anastasia", ""], ["Shen", "Huibin", ""], ["Perrone", "Valerio", ""], ["Klein", "Aaron", ""], ["Faddoul", "Jean Baptiste", ""], ["Krause", "Andreas", ""], ["Seeger", "Matthias", ""], ["Archambeau", "Cedric", ""]]}, {"id": "2104.08183", "submitter": "Matthew Vowels", "authors": "Matthew J. Vowels, Necati Cihan Camgoz and Richard Bowden", "title": "Shadow-Mapping for Unsupervised Neural Causal Discovery", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV stat.AP stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  An important goal across most scientific fields is the discovery of causal\nstructures underling a set of observations. Unfortunately, causal discovery\nmethods which are based on correlation or mutual information can often fail to\nidentify causal links in systems which exhibit dynamic relationships. Such\ndynamic systems (including the famous coupled logistic map) exhibit `mirage'\ncorrelations which appear and disappear depending on the observation window.\nThis means not only that correlation is not causation but, perhaps\ncounter-intuitively, that causation may occur without correlation. In this\npaper we describe Neural Shadow-Mapping, a neural network based method which\nembeds high-dimensional video data into a low-dimensional shadow\nrepresentation, for subsequent estimation of causal links. We demonstrate its\nperformance at discovering causal links from video-representations of dynamic\nsystems.\n", "versions": [{"version": "v1", "created": "Fri, 16 Apr 2021 15:50:03 GMT"}, {"version": "v2", "created": "Wed, 28 Apr 2021 12:58:44 GMT"}], "update_date": "2021-04-29", "authors_parsed": [["Vowels", "Matthew J.", ""], ["Camgoz", "Necati Cihan", ""], ["Bowden", "Richard", ""]]}, {"id": "2104.08191", "submitter": "The Tien Mai", "authors": "The Tien Mai", "title": "Bayesian matrix completion with a spectral scaled Student prior:\n  theoretical guarantee and efficient sampling", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  We study the problem of matrix completion in this paper. A spectral scaled\nStudent prior is exploited to favour the underlying low-rank structure of the\ndata matrix. Importantly, we provide a thorough theoretical investigation for\nour approach, while such an analysis is hard to obtain and limited in\ntheoretical understanding of Bayesian matrix completion. More precisely, we\nshow that our Bayesian approach enjoys a minimax-optimal oracle inequality\nwhich guarantees that our method works well under model misspecification and\nunder general sampling distribution. Interestingly, we also provide efficient\ngradient-based sampling implementations for our approach by using Langevin\nMonte Carlo which is novel in Bayesian matrix completion. More specifically, we\nshow that our algorithms are significantly faster than Gibbs sampler in this\nproblem. To illustrate the attractive features of our inference strategy, some\nnumerical simulations are conducted and an application to image inpainting is\ndemonstrated.\n", "versions": [{"version": "v1", "created": "Fri, 16 Apr 2021 16:03:43 GMT"}], "update_date": "2021-04-19", "authors_parsed": [["Mai", "The Tien", ""]]}, {"id": "2104.08279", "submitter": "Stephen Bates", "authors": "Stephen Bates, Emmanuel Cand\\`es, Lihua Lei, Yaniv Romano, Matteo\n  Sesia", "title": "Testing for Outliers with Conformal p-values", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME math.ST stat.ML stat.TH", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  This paper studies the construction of p-values for nonparametric outlier\ndetection, taking a multiple-testing perspective. The goal is to test whether\nnew independent samples belong to the same distribution as a reference data set\nor are outliers. We propose a solution based on conformal inference, a broadly\napplicable framework which yields p-values that are marginally valid but\nmutually dependent for different test points. We prove these p-values are\npositively dependent and enable exact false discovery rate control, although in\na relatively weak marginal sense. We then introduce a new method to compute\np-values that are both valid conditionally on the training data and independent\nof each other for different test points; this paves the way to stronger type-I\nerror guarantees. Our results depart from classical conformal inference as we\nleverage concentration inequalities rather than combinatorial arguments to\nestablish our finite-sample guarantees. Furthermore, our techniques also yield\na uniform confidence bound for the false positive rate of any outlier detection\nalgorithm, as a function of the threshold applied to its raw statistics.\nFinally, the relevance of our results is demonstrated by numerical experiments\non real and simulated data.\n", "versions": [{"version": "v1", "created": "Fri, 16 Apr 2021 17:59:21 GMT"}, {"version": "v2", "created": "Mon, 19 Apr 2021 16:31:16 GMT"}], "update_date": "2021-04-20", "authors_parsed": [["Bates", "Stephen", ""], ["Cand\u00e8s", "Emmanuel", ""], ["Lei", "Lihua", ""], ["Romano", "Yaniv", ""], ["Sesia", "Matteo", ""]]}, {"id": "2104.08312", "submitter": "Amirata Ghorbani", "authors": "Amirata Ghorbani, James Zou, Andre Esteva", "title": "Data Shapley Valuation for Efficient Batch Active Learning", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Annotating the right set of data amongst all available data points is a key\nchallenge in many machine learning applications. Batch active learning is a\npopular approach to address this, in which batches of unlabeled data points are\nselected for annotation, while an underlying learning algorithm gets\nsubsequently updated. Increasingly larger batches are particularly appealing in\nsettings where data can be annotated in parallel, and model training is\ncomputationally expensive. A key challenge here is scale - typical active\nlearning methods rely on diversity techniques, which select a diverse set of\ndata points to annotate, from an unlabeled pool. In this work, we introduce\nActive Data Shapley (ADS) -- a filtering layer for batch active learning that\nsignificantly increases the efficiency of active learning by pre-selecting,\nusing a linear time computation, the highest-value points from an unlabeled\ndataset. Using the notion of the Shapley value of data, our method estimates\nthe value of unlabeled data points with regards to the prediction task at hand.\nWe show that ADS is particularly effective when the pool of unlabeled data\nexhibits real-world caveats: noise, heterogeneity, and domain shift. We run\nexperiments demonstrating that when ADS is used to pre-select the\nhighest-ranking portion of an unlabeled dataset, the efficiency of\nstate-of-the-art batch active learning methods increases by an average factor\nof 6x, while preserving performance effectiveness.\n", "versions": [{"version": "v1", "created": "Fri, 16 Apr 2021 18:53:42 GMT"}], "update_date": "2021-04-20", "authors_parsed": [["Ghorbani", "Amirata", ""], ["Zou", "James", ""], ["Esteva", "Andre", ""]]}, {"id": "2104.08324", "submitter": "Cynthia Rush", "authors": "Marco Avella Medina and Jos\\'e Luis Montiel Olea and Cynthia Rush and\n  Amilcar Velez", "title": "On the Robustness to Misspecification of $\\alpha$-Posteriors and Their\n  Variational Approximations", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  $\\alpha$-posteriors and their variational approximations distort standard\nposterior inference by downweighting the likelihood and introducing variational\napproximation errors. We show that such distortions, if tuned appropriately,\nreduce the Kullback-Leibler (KL) divergence from the true, but perhaps\ninfeasible, posterior distribution when there is potential parametric model\nmisspecification. To make this point, we derive a Bernstein-von Mises theorem\nshowing convergence in total variation distance of $\\alpha$-posteriors and\ntheir variational approximations to limiting Gaussian distributions. We use\nthese distributions to evaluate the KL divergence between true and reported\nposteriors. We show this divergence is minimized by choosing $\\alpha$ strictly\nsmaller than one, assuming there is a vanishingly small probability of model\nmisspecification. The optimized value becomes smaller as the the\nmisspecification becomes more severe. The optimized KL divergence increases\nlogarithmically in the degree of misspecification and not linearly as with the\nusual posterior.\n", "versions": [{"version": "v1", "created": "Fri, 16 Apr 2021 19:11:53 GMT"}], "update_date": "2021-04-20", "authors_parsed": [["Medina", "Marco Avella", ""], ["Olea", "Jos\u00e9 Luis Montiel", ""], ["Rush", "Cynthia", ""], ["Velez", "Amilcar", ""]]}, {"id": "2104.08482", "submitter": "Kush Bhatia", "authors": "Kush Bhatia, Peter L. Bartlett, Anca D. Dragan, Jacob Steinhardt", "title": "Agnostic learning with unknown utilities", "comments": "30 pages; published as a conference paper at ITCS 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Traditional learning approaches for classification implicitly assume that\neach mistake has the same cost. In many real-world problems though, the utility\nof a decision depends on the underlying context $x$ and decision $y$. However,\ndirectly incorporating these utilities into the learning objective is often\ninfeasible since these can be quite complex and difficult for humans to\nspecify.\n  We formally study this as agnostic learning with unknown utilities: given a\ndataset $S = \\{x_1, \\ldots, x_n\\}$ where each data point $x_i \\sim\n\\mathcal{D}$, the objective of the learner is to output a function $f$ in some\nclass of decision functions $\\mathcal{F}$ with small excess risk. This risk\nmeasures the performance of the output predictor $f$ with respect to the best\npredictor in the class $\\mathcal{F}$ on the unknown underlying utility $u^*$.\nThis utility $u^*$ is not assumed to have any specific structure. This raises\nan interesting question whether learning is even possible in our setup, given\nthat obtaining a generalizable estimate of utility $u^*$ might not be possible\nfrom finitely many samples. Surprisingly, we show that estimating the utilities\nof only the sampled points~$S$ suffices to learn a decision function which\ngeneralizes well.\n  We study mechanisms for eliciting information which allow a learner to\nestimate the utilities $u^*$ on the set $S$. We introduce a family of\nelicitation mechanisms by generalizing comparisons, called the $k$-comparison\noracle, which enables the learner to ask for comparisons across $k$ different\ninputs $x$ at once. We show that the excess risk in our agnostic learning\nframework decreases at a rate of $O\\left(\\frac{1}{k} \\right)$. This result\nbrings out an interesting accuracy-elicitation trade-off -- as the order $k$ of\nthe oracle increases, the comparative queries become harder to elicit from\nhumans but allow for more accurate learning.\n", "versions": [{"version": "v1", "created": "Sat, 17 Apr 2021 08:22:04 GMT"}], "update_date": "2021-04-20", "authors_parsed": [["Bhatia", "Kush", ""], ["Bartlett", "Peter L.", ""], ["Dragan", "Anca D.", ""], ["Steinhardt", "Jacob", ""]]}, {"id": "2104.08538", "submitter": "Jong Chul Ye", "authors": "Taesung Kwon, Jong Chul Ye", "title": "Cycle-free CycleGAN using Invertible Generator for Unsupervised Low-Dose\n  CT Denoising", "comments": "12 pages, 12 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV cs.LG stat.ML", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Recently, CycleGAN was shown to provide high-performance, ultra-fast\ndenoising for low-dose X-ray computed tomography (CT) without the need for a\npaired training dataset. Although this was possible thanks to cycle\nconsistency, CycleGAN requires two generators and two discriminators to enforce\ncycle consistency, demanding significant GPU resources and technical skills for\ntraining. A recent proposal of tunable CycleGAN with Adaptive Instance\nNormalization (AdaIN) alleviates the problem in part by using a single\ngenerator. However, two discriminators and an additional AdaIN code generator\nare still required for training. To solve this problem, here we present a novel\ncycle-free Cycle-GAN architecture, which consists of a single generator and a\ndiscriminator but still guarantees cycle consistency. The main innovation comes\nfrom the observation that the use of an invertible generator automatically\nfulfills the cycle consistency condition and eliminates the additional\ndiscriminator in the CycleGAN formulation. To make the invertible generator\nmore effective, our network is implemented in the wavelet residual domain.\nExtensive experiments using various levels of low-dose CT images confirm that\nour method can significantly improve denoising performance using only 10% of\nlearnable parameters and faster training time compared to the conventional\nCycleGAN.\n", "versions": [{"version": "v1", "created": "Sat, 17 Apr 2021 13:23:36 GMT"}], "update_date": "2021-04-20", "authors_parsed": [["Kwon", "Taesung", ""], ["Ye", "Jong Chul", ""]]}, {"id": "2104.08548", "submitter": "Micha{\\l} Koziarski", "authors": "Micha{\\l} Koziarski", "title": "Potential Anchoring for imbalanced data classification", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Data imbalance remains one of the factors negatively affecting the\nperformance of contemporary machine learning algorithms. One of the most common\napproaches to reducing the negative impact of data imbalance is preprocessing\nthe original dataset with data-level strategies. In this paper we propose a\nunified framework for imbalanced data over- and undersampling. The proposed\napproach utilizes radial basis functions to preserve the original shape of the\nunderlying class distributions during the resampling process. This is done by\noptimizing the positions of generated synthetic observations with respect to\nthe potential resemblance loss. The final Potential Anchoring algorithm\ncombines over- and undersampling within the proposed framework. The results of\nthe experiments conducted on 60 imbalanced datasets show outperformance of\nPotential Anchoring over state-of-the-art resampling algorithms, including\npreviously proposed methods that utilize radial basis functions to model class\npotential. Furthermore, the results of the analysis based on the proposed data\ncomplexity index show that Potential Anchoring is particularly well suited for\nhandling naturally complex (i.e. not affected by the presence of noise)\ndatasets.\n", "versions": [{"version": "v1", "created": "Sat, 17 Apr 2021 14:00:16 GMT"}], "update_date": "2021-04-20", "authors_parsed": [["Koziarski", "Micha\u0142", ""]]}, {"id": "2104.08556", "submitter": "Alberto Garcia-Duran", "authors": "Alberto Garc\\'ia-Dur\\'an, Robert West", "title": "Recursive input and state estimation: A general framework for learning\n  from time series with missing data", "comments": "Published at ICASSP 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Time series with missing data are signals encountered in important settings\nfor machine learning. Some of the most successful prior approaches for modeling\nsuch time series are based on recurrent neural networks that transform the\ninput and previous state to account for the missing observations, and then\ntreat the transformed signal in a standard manner.\n  In this paper, we introduce a single unifying framework, Recursive Input and\nState Estimation (RISE), for this general approach and reformulate existing\nmodels as specific instances of this framework. We then explore additional\nnovel variations within the RISE framework to improve the performance of any\ninstance. We exploit representation learning techniques to learn latent\nrepresentations of the signals used by RISE instances. We discuss and develop\nvarious encoding techniques to learn latent signal representations. We\nbenchmark instances of the framework with various encoding functions on three\ndata imputation datasets, observing that RISE instances always benefit from\nencoders that learn representations for numerical values from the digits into\nwhich they can be decomposed.\n", "versions": [{"version": "v1", "created": "Sat, 17 Apr 2021 14:43:33 GMT"}], "update_date": "2021-04-20", "authors_parsed": [["Garc\u00eda-Dur\u00e1n", "Alberto", ""], ["West", "Robert", ""]]}, {"id": "2104.08615", "submitter": "Kun Wang", "authors": "Kun Wang, Canzhe Zhao, Shuai Li, Shuo Shao", "title": "Conservative Contextual Combinatorial Cascading Bandit", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Conservative mechanism is a desirable property in decision-making problems\nwhich balance the tradeoff between the exploration and exploitation. We propose\nthe novel \\emph{conservative contextual combinatorial cascading bandit\n($C^4$-bandit)}, a cascading online learning game which incorporates the\nconservative mechanism. At each time step, the learning agent is given some\ncontexts and has to recommend a list of items but not worse than the base\nstrategy and then observes the reward by some stopping rules. We design the\n$C^4$-UCB algorithm to solve the problem and prove its n-step upper regret\nbound for two situations: known baseline reward and unknown baseline reward.\nThe regret in both situations can be decomposed into two terms: (a) the upper\nbound for the general contextual combinatorial cascading bandit; and (b) a\nconstant term for the regret from the conservative mechanism. We also improve\nthe bound of the conservative contextual combinatorial bandit as a by-product.\nExperiments on synthetic data demonstrate its advantages and validate our\ntheoretical analysis.\n", "versions": [{"version": "v1", "created": "Sat, 17 Apr 2021 18:42:28 GMT"}, {"version": "v2", "created": "Fri, 23 Apr 2021 05:59:29 GMT"}], "update_date": "2021-04-26", "authors_parsed": [["Wang", "Kun", ""], ["Zhao", "Canzhe", ""], ["Li", "Shuai", ""], ["Shao", "Shuo", ""]]}, {"id": "2104.08708", "submitter": "Haochuan Li", "authors": "Haochuan Li, Yi Tian, Jingzhao Zhang, Ali Jadbabaie", "title": "Complexity Lower Bounds for Nonconvex-Strongly-Concave Min-Max\n  Optimization", "comments": "20 pages, 1 figure", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.OC cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We provide a first-order oracle complexity lower bound for finding stationary\npoints of min-max optimization problems where the objective function is smooth,\nnonconvex in the minimization variable, and strongly concave in the\nmaximization variable. We establish a lower bound of\n$\\Omega\\left(\\sqrt{\\kappa}\\epsilon^{-2}\\right)$ for deterministic oracles,\nwhere $\\epsilon$ defines the level of approximate stationarity and $\\kappa$ is\nthe condition number. Our analysis shows that the upper bound achieved in (Lin\net al., 2020b) is optimal in the $\\epsilon$ and $\\kappa$ dependence up to\nlogarithmic factors. For stochastic oracles, we provide a lower bound of\n$\\Omega\\left(\\sqrt{\\kappa}\\epsilon^{-2} + \\kappa^{1/3}\\epsilon^{-4}\\right)$. It\nsuggests that there is a significant gap between the upper bound\n$\\mathcal{O}(\\kappa^3 \\epsilon^{-4})$ in (Lin et al., 2020a) and our lower\nbound in the condition number dependence.\n", "versions": [{"version": "v1", "created": "Sun, 18 Apr 2021 04:30:01 GMT"}], "update_date": "2021-04-20", "authors_parsed": [["Li", "Haochuan", ""], ["Tian", "Yi", ""], ["Zhang", "Jingzhao", ""], ["Jadbabaie", "Ali", ""]]}, {"id": "2104.08894", "submitter": "Ahmed Abdelkader", "authors": "Phillip Pope, Chen Zhu, Ahmed Abdelkader, Micah Goldblum, Tom\n  Goldstein", "title": "The Intrinsic Dimension of Images and Its Impact on Learning", "comments": "To appear at ICLR 2021 (spotlight), 17 pages with appendix, 15\n  figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG stat.ML", "license": "http://creativecommons.org/licenses/by-sa/4.0/", "abstract": "  It is widely believed that natural image data exhibits low-dimensional\nstructure despite the high dimensionality of conventional pixel\nrepresentations. This idea underlies a common intuition for the remarkable\nsuccess of deep learning in computer vision. In this work, we apply dimension\nestimation tools to popular datasets and investigate the role of\nlow-dimensional structure in deep learning. We find that common natural image\ndatasets indeed have very low intrinsic dimension relative to the high number\nof pixels in the images. Additionally, we find that low dimensional datasets\nare easier for neural networks to learn, and models solving these tasks\ngeneralize better from training to test data. Along the way, we develop a\ntechnique for validating our dimension estimation tools on synthetic data\ngenerated by GANs allowing us to actively manipulate the intrinsic dimension by\ncontrolling the image generation process. Code for our experiments may be found\nhere https://github.com/ppope/dimensions.\n", "versions": [{"version": "v1", "created": "Sun, 18 Apr 2021 16:29:23 GMT"}], "update_date": "2021-04-20", "authors_parsed": [["Pope", "Phillip", ""], ["Zhu", "Chen", ""], ["Abdelkader", "Ahmed", ""], ["Goldblum", "Micah", ""], ["Goldstein", "Tom", ""]]}, {"id": "2104.08903", "submitter": "Lev Utkin", "authors": "Lev V. Utkin and Egor D. Satyukov and Andrei V. Konstantinov", "title": "SurvNAM: The machine learning survival model explanation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A new modification of the Neural Additive Model (NAM) called SurvNAM and its\nmodifications are proposed to explain predictions of the black-box machine\nlearning survival model. The method is based on applying the original NAM to\nsolving the explanation problem in the framework of survival analysis. The\nbasic idea behind SurvNAM is to train the network by means of a specific\nexpected loss function which takes into account peculiarities of the survival\nmodel predictions and is based on approximating the black-box model by the\nextension of the Cox proportional hazards model which uses the well-known\nGeneralized Additive Model (GAM) in place of the simple linear relationship of\ncovariates. The proposed method SurvNAM allows performing the local and global\nexplanation. A set of examples around the explained example is randomly\ngenerated for the local explanation. The global explanation uses the whole\ntraining dataset. The proposed modifications of SurvNAM are based on using the\nLasso-based regularization for functions from GAM and for a special\nrepresentation of the GAM functions using their weighted linear and non-linear\nparts, which is implemented as a shortcut connection. A lot of numerical\nexperiments illustrate the SurvNAM efficiency.\n", "versions": [{"version": "v1", "created": "Sun, 18 Apr 2021 16:40:56 GMT"}], "update_date": "2021-04-20", "authors_parsed": [["Utkin", "Lev V.", ""], ["Satyukov", "Egor D.", ""], ["Konstantinov", "Andrei V.", ""]]}, {"id": "2104.08928", "submitter": "Kan Xu", "authors": "Kan Xu, Xuanyi Zhao, Hamsa Bastani, Osbert Bastani", "title": "Group-Sparse Matrix Factorization for Transfer Learning of Word\n  Embeddings", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.CL cs.LG", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Sparse regression has recently been applied to enable transfer learning from\nvery limited data. We study an extension of this approach to unsupervised\nlearning -- in particular, learning word embeddings from unstructured text\ncorpora using low-rank matrix factorization. Intuitively, when transferring\nword embeddings to a new domain, we expect that the embeddings change for only\na small number of words -- e.g., the ones with novel meanings in that domain.\nWe propose a novel group-sparse penalty that exploits this sparsity to perform\ntransfer learning when there is very little text data available in the target\ndomain -- e.g., a single article of text. We prove generalization bounds for\nour algorithm. Furthermore, we empirically evaluate its effectiveness, both in\nterms of prediction accuracy in downstream tasks as well as the\ninterpretability of the results.\n", "versions": [{"version": "v1", "created": "Sun, 18 Apr 2021 18:19:03 GMT"}], "update_date": "2021-04-20", "authors_parsed": [["Xu", "Kan", ""], ["Zhao", "Xuanyi", ""], ["Bastani", "Hamsa", ""], ["Bastani", "Osbert", ""]]}, {"id": "2104.08959", "submitter": "TrungTin Nguyen", "authors": "TrungTin Nguyen, Faicel Chamroukhi, Hien Duy Nguyen, Florence Forbes", "title": "Non-asymptotic model selection in block-diagonal mixture of polynomial\n  experts models", "comments": "Corrected typos. Extended results from arXiv:2104.02640. arXiv admin\n  note: substantial text overlap with arXiv:2104.02640", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST cs.AI cs.LG stat.ME stat.ML stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Model selection, via penalized likelihood type criteria, is a standard task\nin many statistical inference and machine learning problems. Progress has led\nto deriving criteria with asymptotic consistency results and an increasing\nemphasis on introducing non-asymptotic criteria. We focus on the problem of\nmodeling non-linear relationships in regression data with potential hidden\ngraph-structured interactions between the high-dimensional predictors, within\nthe mixture of experts modeling framework. In order to deal with such a complex\nsituation, we investigate a block-diagonal localized mixture of polynomial\nexperts (BLoMPE) regression model, which is constructed upon an inverse\nregression and block-diagonal structures of the Gaussian expert covariance\nmatrices. We introduce a penalized maximum likelihood selection criterion to\nestimate the unknown conditional density of the regression model. This model\nselection criterion allows us to handle the challenging problem of inferring\nthe number of mixture components, the degree of polynomial mean functions, and\nthe hidden block-diagonal structures of the covariance matrices, which reduces\nthe number of parameters to be estimated and leads to a trade-off between\ncomplexity and sparsity in the model. In particular, we provide a strong\ntheoretical guarantee: a finite-sample oracle inequality satisfied by the\npenalized maximum likelihood estimator with a Jensen-Kullback-Leibler type\nloss, to support the introduced non-asymptotic model selection criterion. The\npenalty shape of this criterion depends on the complexity of the considered\nrandom subcollection of BLoMPE models, including the relevant graph structures,\nthe degree of polynomial mean functions, and the number of mixture components.\n", "versions": [{"version": "v1", "created": "Sun, 18 Apr 2021 21:32:20 GMT"}, {"version": "v2", "created": "Mon, 10 May 2021 21:05:06 GMT"}], "update_date": "2021-05-12", "authors_parsed": [["Nguyen", "TrungTin", ""], ["Chamroukhi", "Faicel", ""], ["Nguyen", "Hien Duy", ""], ["Forbes", "Florence", ""]]}, {"id": "2104.08977", "submitter": "Audrey Huang", "authors": "Audrey Huang, Liu Leqi, Zachary C. Lipton, Kamyar Azizzadenesheli", "title": "Off-Policy Risk Assessment in Contextual Bandits", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Even when unable to run experiments, practitioners can evaluate prospective\npolicies, using previously logged data. However, while the bandits literature\nhas adopted a diverse set of objectives, most research on off-policy evaluation\nto date focuses on the expected reward. In this paper, we introduce Lipschitz\nrisk functionals, a broad class of objectives that subsumes conditional\nvalue-at-risk (CVaR), variance, mean-variance, many distorted risks, and CPT\nrisks, among others. We propose Off-Policy Risk Assessment (OPRA), a framework\nthat first estimates a target policy's CDF and then generates plugin estimates\nfor any collection of Lipschitz risks, providing finite sample guarantees that\nhold simultaneously over the entire class. We instantiate OPRA with both\nimportance sampling and doubly robust estimators. Our primary theoretical\ncontributions are (i) the first uniform concentration inequalities for both CDF\nestimators in contextual bandits and (ii) error bounds on our Lipschitz risk\nestimates, which all converge at a rate of $O(1/\\sqrt{n})$.\n", "versions": [{"version": "v1", "created": "Sun, 18 Apr 2021 23:27:40 GMT"}, {"version": "v2", "created": "Tue, 29 Jun 2021 14:55:52 GMT"}], "update_date": "2021-06-30", "authors_parsed": [["Huang", "Audrey", ""], ["Leqi", "Liu", ""], ["Lipton", "Zachary C.", ""], ["Azizzadenesheli", "Kamyar", ""]]}, {"id": "2104.09011", "submitter": "Tomoharu Iwata", "authors": "Tomoharu Iwata", "title": "Few-shot Learning for Topic Modeling", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Topic models have been successfully used for analyzing text documents.\nHowever, with existing topic models, many documents are required for training.\nIn this paper, we propose a neural network-based few-shot learning method that\ncan learn a topic model from just a few documents. The neural networks in our\nmodel take a small number of documents as inputs, and output topic model\npriors. The proposed method trains the neural networks such that the expected\ntest likelihood is improved when topic model parameters are estimated by\nmaximizing the posterior probability using the priors based on the EM\nalgorithm. Since each step in the EM algorithm is differentiable, the proposed\nmethod can backpropagate the loss through the EM algorithm to train the neural\nnetworks. The expected test likelihood is maximized by a stochastic gradient\ndescent method using a set of multiple text corpora with an episodic training\nframework. In our experiments, we demonstrate that the proposed method achieves\nbetter perplexity than existing methods using three real-world text document\nsets.\n", "versions": [{"version": "v1", "created": "Mon, 19 Apr 2021 01:56:48 GMT"}], "update_date": "2021-04-20", "authors_parsed": [["Iwata", "Tomoharu", ""]]}, {"id": "2104.09185", "submitter": "Sarem Seitz", "authors": "Sarem Seitz", "title": "Mixtures of Gaussian Processes for regression under multiple prior\n  distributions", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  When constructing a Bayesian Machine Learning model, we might be faced with\nmultiple different prior distributions and thus are required to properly\nconsider them in a sensible manner in our model. While this situation is\nreasonably well explored for classical Bayesian Statistics, it appears useful\nto develop a corresponding method for complex Machine Learning problems. Given\ntheir underlying Bayesian framework and their widespread popularity, Gaussian\nProcesses are a good candidate to tackle this task. We therefore extend the\nidea of Mixture models for Gaussian Process regression in order to work with\nmultiple prior beliefs at once - both a analytical regression formula and a\nSparse Variational approach are considered. In addition, we consider the usage\nof our approach to additionally account for the problem of prior\nmisspecification in functional regression problems.\n", "versions": [{"version": "v1", "created": "Mon, 19 Apr 2021 10:19:14 GMT"}], "update_date": "2021-04-20", "authors_parsed": [["Seitz", "Sarem", ""]]}, {"id": "2104.09226", "submitter": "David Plans Dr.", "authors": "Mohammad A. Dabbah, Angus B. Reed, Adam T.C. Booth, Arrash Yassaee,\n  Alex Despotovic, Benjamin Klasmer, Emily Binning, Mert Aral, David Plans,\n  Alain B. Labrique, Diwakar Mohan", "title": "Machine learning approach to dynamic risk modeling of mortality in\n  COVID-19: a UK Biobank study", "comments": "20 pages, 3 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/", "abstract": "  The COVID-19 pandemic has created an urgent need for robust, scalable\nmonitoring tools supporting stratification of high-risk patients. This research\naims to develop and validate prediction models, using the UK Biobank, to\nestimate COVID-19 mortality risk in confirmed cases. From the 11,245\nparticipants testing positive for COVID-19, we develop a data-driven random\nforest classification model with excellent performance (AUC: 0.91), using\nbaseline characteristics, pre-existing conditions, symptoms, and vital signs,\nsuch that the score could dynamically assess mortality risk with disease\ndeterioration. We also identify several significant novel predictors of\nCOVID-19 mortality with equivalent or greater predictive value than established\nhigh-risk comorbidities, such as detailed anthropometrics and prior acute\nkidney failure, urinary tract infection, and pneumonias. The model design and\nfeature selection enables utility in outpatient settings. Possible applications\ninclude supporting individual-level risk profiling and monitoring disease\nprogression across patients with COVID-19 at-scale, especially in\nhospital-at-home settings.\n", "versions": [{"version": "v1", "created": "Mon, 19 Apr 2021 11:51:20 GMT"}], "update_date": "2021-04-20", "authors_parsed": [["Dabbah", "Mohammad A.", ""], ["Reed", "Angus B.", ""], ["Booth", "Adam T. C.", ""], ["Yassaee", "Arrash", ""], ["Despotovic", "Alex", ""], ["Klasmer", "Benjamin", ""], ["Binning", "Emily", ""], ["Aral", "Mert", ""], ["Plans", "David", ""], ["Labrique", "Alain B.", ""], ["Mohan", "Diwakar", ""]]}, {"id": "2104.09240", "submitter": "Benedikt Pf\\\"ulb", "authors": "Benedikt Pf\\\"ulb, Alexander Gepperth, Benedikt Bagus", "title": "Continual Learning with Fully Probabilistic Models", "comments": "Accepted as Findings at the CLVISION2021 workshop, 11 pages, 6\n  figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present an approach for continual learning (CL) that is based on fully\nprobabilistic (or generative) models of machine learning. In contrast to, e.g.,\nGANs that are \"generative\" in the sense that they can generate samples, fully\nprobabilistic models aim at modeling the data distribution directly.\nConsequently, they provide functionalities that are highly relevant for\ncontinual learning, such as density estimation (outlier detection) and sample\ngeneration. As a concrete realization of generative continual learning, we\npropose Gaussian Mixture Replay (GMR). GMR is a pseudo-rehearsal approach using\na Gaussian Mixture Model (GMM) instance for both generator and classifier\nfunctionalities. Relying on the MNIST, FashionMNIST and Devanagari benchmarks,\nwe first demonstrate unsupervised task boundary detection by GMM density\nestimation, which we also use to reject untypical generated samples. In\naddition, we show that GMR is capable of class-conditional sampling in the way\nof a cGAN. Lastly, we verify that GMR, despite its simple structure, achieves\nstate-of-the-art performance on common class-incremental learning problems at\nvery competitive time and memory complexity.\n", "versions": [{"version": "v1", "created": "Mon, 19 Apr 2021 12:26:26 GMT"}], "update_date": "2021-04-20", "authors_parsed": [["Pf\u00fclb", "Benedikt", ""], ["Gepperth", "Alexander", ""], ["Bagus", "Benedikt", ""]]}, {"id": "2104.09311", "submitter": "Yufei Zhang", "authors": "Xin Guo, Anran Hu, Yufei Zhang", "title": "Reinforcement learning for linear-convex models with jumps via stability\n  analysis of feedback controls", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.OC cs.LG stat.ML", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  We study finite-time horizon continuous-time linear-convex reinforcement\nlearning problems in an episodic setting. In this problem, the unknown linear\njump-diffusion process is controlled subject to nonsmooth convex costs. We show\nthat the associated linear-convex control problems admit Lipchitz continuous\noptimal feedback controls and further prove the Lipschitz stability of the\nfeedback controls, i.e., the performance gap between applying feedback controls\nfor an incorrect model and for the true model depends Lipschitz-continuously on\nthe magnitude of perturbations in the model coefficients; the proof relies on a\nstability analysis of the associated forward-backward stochastic differential\nequation. We then propose a novel least-squares algorithm which achieves a\nregret of the order $O(\\sqrt{N\\ln N})$ on linear-convex learning problems with\njumps, where $N$ is the number of learning episodes; the analysis leverages the\nLipschitz stability of feedback controls and concentration properties of\nsub-Weibull random variables.\n", "versions": [{"version": "v1", "created": "Mon, 19 Apr 2021 13:50:52 GMT"}], "update_date": "2021-04-20", "authors_parsed": [["Guo", "Xin", ""], ["Hu", "Anran", ""], ["Zhang", "Yufei", ""]]}, {"id": "2104.09323", "submitter": "Tobias Hatt", "authors": "Tobias Hatt, Stefan Feuerriegel", "title": "Sequential Deconfounding for Causal Inference with Unobserved\n  Confounders", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Using observational data to estimate the effect of a treatment is a powerful\ntool for decision-making when randomized experiments are infeasible or costly.\nHowever, observational data often yields biased estimates of treatment effects,\nsince treatment assignment can be confounded by unobserved variables. A remedy\nis offered by deconfounding methods that adjust for such unobserved\nconfounders. In this paper, we develop the Sequential Deconfounder, a method\nthat enables estimating individualized treatment effects over time in presence\nof unobserved confounders. This is the first deconfounding method that can be\nused in a general sequential setting (i.e., with one or more treatments\nassigned at each timestep). The Sequential Deconfounder uses a novel Gaussian\nprocess latent variable model to infer substitutes for the unobserved\nconfounders, which are then used in conjunction with an outcome model to\nestimate treatment effects over time. We prove that using our method yields\nunbiased estimates of individualized treatment responses over time. Using\nsimulated and real medical data, we demonstrate the efficacy of our method in\ndeconfounding the estimation of treatment responses over time.\n", "versions": [{"version": "v1", "created": "Fri, 16 Apr 2021 09:56:39 GMT"}], "update_date": "2021-04-20", "authors_parsed": [["Hatt", "Tobias", ""], ["Feuerriegel", "Stefan", ""]]}, {"id": "2104.09325", "submitter": "Luis Miralles", "authors": "Andr\\'es L. Su\\'arez-Cetrulo and Ankit Kumar and Luis\n  Miralles-Pechu\\'an", "title": "Modelling the COVID-19 virus evolution with Incremental Machine Learning", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  The investment of time and resources for better strategies and methodologies\nto tackle a potential pandemic is key to deal with potential outbreaks of new\nvariants or other viruses in the future. In this work, we recreated the scene\nof a year ago, 2020, when the pandemic erupted across the world for the fifty\ncountries with more COVID-19 cases reported. We performed some experiments in\nwhich we compare state-of-the-art machine learning algorithms, such as LSTM,\nagainst online incremental machine learning algorithms to adapt them to the\ndaily changes in the spread of the disease and predict future COVID-19 cases.\nTo compare the methods, we performed three experiments: In the first one, we\ntrained the models using only data from the country we predicted. In the second\none, we use data from all fifty countries to train and predict each of them. In\nthe first and second experiment, we used a static hold-out approach for all\nmethods. In the third experiment, we trained the incremental methods\nsequentially, using a prequential evaluation. This scheme is not suitable for\nmost state-of-the-art machine learning algorithms because they need to be\nretrained from scratch for every batch of predictions, causing a computational\nburden. Results show that incremental methods are a promising approach to adapt\nto changes of the disease over time; they are always up to date with the last\nstate of the data distribution, and they have a significantly lower\ncomputational cost than other techniques such as LSTMs.\n", "versions": [{"version": "v1", "created": "Wed, 14 Apr 2021 16:08:35 GMT"}, {"version": "v2", "created": "Wed, 21 Apr 2021 09:13:06 GMT"}], "update_date": "2021-04-22", "authors_parsed": [["Su\u00e1rez-Cetrulo", "Andr\u00e9s L.", ""], ["Kumar", "Ankit", ""], ["Miralles-Pechu\u00e1n", "Luis", ""]]}, {"id": "2104.09327", "submitter": "Michael Hughes", "authors": "Alexandra Hope Lee, Panagiotis Lymperopoulos, Joshua T. Cohen, John B.\n  Wong, and Michael C. Hughes", "title": "Forecasting COVID-19 Counts At A Single Hospital: A Hierarchical\n  Bayesian Approach", "comments": "In ICLR 2021 Workshop on Machine Learning for Preventing and\n  Combating Pandemics", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  We consider the problem of forecasting the daily number of hospitalized\nCOVID-19 patients at a single hospital site, in order to help administrators\nwith logistics and planning. We develop several candidate hierarchical Bayesian\nmodels which directly capture the count nature of data via a generalized\nPoisson likelihood, model time-series dependencies via autoregressive and\nGaussian process latent processes, and share statistical strength across\nrelated sites. We demonstrate our approach on public datasets for 8 hospitals\nin Massachusetts, U.S.A. and 10 hospitals in the United Kingdom. Further\nprospective evaluation compares our approach favorably to baselines currently\nused by stakeholders at 3 related hospitals to forecast 2-week-ahead demand by\nrescaling state-level forecasts.\n", "versions": [{"version": "v1", "created": "Wed, 14 Apr 2021 11:58:54 GMT"}], "update_date": "2021-04-20", "authors_parsed": [["Lee", "Alexandra Hope", ""], ["Lymperopoulos", "Panagiotis", ""], ["Cohen", "Joshua T.", ""], ["Wong", "John B.", ""], ["Hughes", "Michael C.", ""]]}, {"id": "2104.09368", "submitter": "Mingli Chen", "authors": "Mingli Chen, Andreas Joseph, Michael Kumhof, Xinlei Pan, Rui Shi, Xuan\n  Zhou", "title": "Deep Reinforcement Learning in a Monetary Model", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "econ.EM econ.GN q-fin.EC stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose using deep reinforcement learning to solve dynamic stochastic\ngeneral equilibrium models. Agents are represented by deep artificial neural\nnetworks and learn to solve their dynamic optimisation problem by interacting\nwith the model environment, of which they have no a priori knowledge. Deep\nreinforcement learning offers a flexible yet principled way to model bounded\nrationality within this general class of models. We apply our proposed approach\nto a classical model from the adaptive learning literature in macroeconomics\nwhich looks at the interaction of monetary and fiscal policy. We find that,\ncontrary to adaptive learning, the artificially intelligent household can solve\nthe model in all policy regimes.\n", "versions": [{"version": "v1", "created": "Mon, 19 Apr 2021 14:56:44 GMT"}], "update_date": "2021-04-20", "authors_parsed": [["Chen", "Mingli", ""], ["Joseph", "Andreas", ""], ["Kumhof", "Michael", ""], ["Pan", "Xinlei", ""], ["Shi", "Rui", ""], ["Zhou", "Xuan", ""]]}, {"id": "2104.09371", "submitter": "Aniruddha Rajendra Rao", "authors": "Aniruddha Rajendra Rao and Matthew Reimherr", "title": "Non-linear Functional Modeling using Neural Networks", "comments": "3 figures, 8 tables (including supplementary material), 13 pages\n  (including supplementary material)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ME stat.ML", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  We introduce a new class of non-linear models for functional data based on\nneural networks. Deep learning has been very successful in non-linear modeling,\nbut there has been little work done in the functional data setting. We propose\ntwo variations of our framework: a functional neural network with continuous\nhidden layers, called the Functional Direct Neural Network (FDNN), and a second\nversion that utilizes basis expansions and continuous hidden layers, called the\nFunctional Basis Neural Network (FBNN). Both are designed explicitly to exploit\nthe structure inherent in functional data. To fit these models we derive a\nfunctional gradient based optimization algorithm. The effectiveness of the\nproposed methods in handling complex functional models is demonstrated by\ncomprehensive simulation studies and real data examples.\n", "versions": [{"version": "v1", "created": "Mon, 19 Apr 2021 14:59:55 GMT"}], "update_date": "2021-04-20", "authors_parsed": [["Rao", "Aniruddha Rajendra", ""], ["Reimherr", "Matthew", ""]]}, {"id": "2104.09435", "submitter": "Jong Chul Ye", "authors": "Hyoungjun Park, Myeongsu Na, Bumju Kim, Soohyun Park, Ki Hean Kim,\n  Sunghoe Chang, and Jong Chul Ye", "title": "Deep learning enables reference-free isotropic super-resolution for\n  volumetric fluorescence microscopy", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Volumetric imaging by fluorescence microscopy is often limited by anisotropic\nspatial resolution from inferior axial resolution compared to the lateral\nresolution. To address this problem, here we present a deep-learning-enabled\nunsupervised super-resolution technique that enhances anisotropic images in\nvolumetric fluorescence microscopy. In contrast to the existing deep learning\napproaches that require matched high-resolution target volume images, our\nmethod greatly reduces the effort to put into practice as the training of a\nnetwork requires as little as a single 3D image stack, without a priori\nknowledge of the image formation process, registration of training data, or\nseparate acquisition of target data. This is achieved based on the optimal\ntransport driven cycle-consistent generative adversarial network that learns\nfrom an unpaired matching between high-resolution 2D images in lateral image\nplane and low-resolution 2D images in the other planes. Using fluorescence\nconfocal microscopy and light-sheet microscopy, we demonstrate that the trained\nnetwork not only enhances axial resolution, but also restores suppressed visual\ndetails between the imaging planes and removes imaging artifacts.\n", "versions": [{"version": "v1", "created": "Mon, 19 Apr 2021 16:31:12 GMT"}, {"version": "v2", "created": "Mon, 7 Jun 2021 02:45:47 GMT"}], "update_date": "2021-06-08", "authors_parsed": [["Park", "Hyoungjun", ""], ["Na", "Myeongsu", ""], ["Kim", "Bumju", ""], ["Park", "Soohyun", ""], ["Kim", "Ki Hean", ""], ["Chang", "Sunghoe", ""], ["Ye", "Jong Chul", ""]]}, {"id": "2104.09437", "submitter": "Quanquan Gu", "authors": "Difan Zou and Spencer Frei and Quanquan Gu", "title": "Provable Robustness of Adversarial Training for Learning Halfspaces with\n  Noise", "comments": "42 pages, 2 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CR math.OC stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We analyze the properties of adversarial training for learning adversarially\nrobust halfspaces in the presence of agnostic label noise. Denoting\n$\\mathsf{OPT}_{p,r}$ as the best robust classification error achieved by a\nhalfspace that is robust to perturbations of $\\ell_{p}$ balls of radius $r$, we\nshow that adversarial training on the standard binary cross-entropy loss yields\nadversarially robust halfspaces up to (robust) classification error $\\tilde\nO(\\sqrt{\\mathsf{OPT}_{2,r}})$ for $p=2$, and $\\tilde O(d^{1/4}\n\\sqrt{\\mathsf{OPT}_{\\infty, r}} + d^{1/2} \\mathsf{OPT}_{\\infty,r})$ when\n$p=\\infty$. Our results hold for distributions satisfying anti-concentration\nproperties enjoyed by log-concave isotropic distributions among others. We\nadditionally show that if one instead uses a nonconvex sigmoidal loss,\nadversarial training yields halfspaces with an improved robust classification\nerror of $O(\\mathsf{OPT}_{2,r})$ for $p=2$, and $O(d^{1/4}\\mathsf{OPT}_{\\infty,\nr})$ when $p=\\infty$. To the best of our knowledge, this is the first work to\nshow that adversarial training provably yields robust classifiers in the\npresence of noise.\n", "versions": [{"version": "v1", "created": "Mon, 19 Apr 2021 16:35:38 GMT"}], "update_date": "2021-04-20", "authors_parsed": [["Zou", "Difan", ""], ["Frei", "Spencer", ""], ["Gu", "Quanquan", ""]]}, {"id": "2104.09452", "submitter": "Vincent Pisztora", "authors": "Vincent Pisztora, Yanglan Ou, Xiaolei Huang, Francesca Chiaromonte,\n  Jia Li", "title": "Epsilon Consistent Mixup: An Adaptive Consistency-Interpolation Tradeoff", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG stat.AP stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper we propose $\\epsilon$-Consistent Mixup ($\\epsilon$mu).\n$\\epsilon$mu is a data-based structural regularization technique that combines\nMixup's linear interpolation with consistency regularization in the Mixup\ndirection, by compelling a simple adaptive tradeoff between the two. This\nlearnable combination of consistency and interpolation induces a more flexible\nstructure on the evolution of the response across the feature space and is\nshown to improve semi-supervised classification accuracy on the SVHN and\nCIFAR10 benchmark datasets, yielding the largest gains in the most challenging\nlow label-availability scenarios. Empirical studies comparing $\\epsilon$mu and\nMixup are presented and provide insight into the mechanisms behind\n$\\epsilon$mu's effectiveness. In particular, $\\epsilon$mu is found to produce\nmore accurate synthetic labels and more confident predictions than Mixup.\n", "versions": [{"version": "v1", "created": "Mon, 19 Apr 2021 17:10:31 GMT"}], "update_date": "2021-04-20", "authors_parsed": [["Pisztora", "Vincent", ""], ["Ou", "Yanglan", ""], ["Huang", "Xiaolei", ""], ["Chiaromonte", "Francesca", ""], ["Li", "Jia", ""]]}, {"id": "2104.09459", "submitter": "Andrew Wilson", "authors": "Marc Finzi, Max Welling, Andrew Gordon Wilson", "title": "A Practical Method for Constructing Equivariant Multilayer Perceptrons\n  for Arbitrary Matrix Groups", "comments": "Library: https://github.com/mfinzi/equivariant-MLP, Documentation:\n  https://emlp.readthedocs.io/en/latest/, Examples:\n  https://colab.research.google.com/github/mfinzi/equivariant-MLP/blob/master/docs/notebooks/colabs/all.ipynb", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG math.DS stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Symmetries and equivariance are fundamental to the generalization of neural\nnetworks on domains such as images, graphs, and point clouds. Existing work has\nprimarily focused on a small number of groups, such as the translation,\nrotation, and permutation groups. In this work we provide a completely general\nalgorithm for solving for the equivariant layers of matrix groups. In addition\nto recovering solutions from other works as special cases, we construct\nmultilayer perceptrons equivariant to multiple groups that have never been\ntackled before, including $\\mathrm{O}(1,3)$, $\\mathrm{O}(5)$, $\\mathrm{Sp}(n)$,\nand the Rubik's cube group. Our approach outperforms non-equivariant baselines,\nwith applications to particle physics and dynamical systems. We release our\nsoftware library to enable researchers to construct equivariant layers for\narbitrary matrix groups.\n", "versions": [{"version": "v1", "created": "Mon, 19 Apr 2021 17:21:54 GMT"}], "update_date": "2021-04-20", "authors_parsed": [["Finzi", "Marc", ""], ["Welling", "Max", ""], ["Wilson", "Andrew Gordon", ""]]}, {"id": "2104.09460", "submitter": "Willie Neiswanger", "authors": "Willie Neiswanger, Ke Alexander Wang, Stefano Ermon", "title": "Bayesian Algorithm Execution: Estimating Computable Properties of\n  Black-box Functions Using Mutual Information", "comments": "Appears in Proceedings of the 38th International Conference on\n  Machine Learning (ICML), 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.AI cs.IT cs.LG cs.NE math.IT", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In many real-world problems, we want to infer some property of an expensive\nblack-box function $f$, given a budget of $T$ function evaluations. One example\nis budget constrained global optimization of $f$, for which Bayesian\noptimization is a popular method. Other properties of interest include local\noptima, level sets, integrals, or graph-structured information induced by $f$.\nOften, we can find an algorithm $\\mathcal{A}$ to compute the desired property,\nbut it may require far more than $T$ queries to execute. Given such an\n$\\mathcal{A}$, and a prior distribution over $f$, we refer to the problem of\ninferring the output of $\\mathcal{A}$ using $T$ evaluations as Bayesian\nAlgorithm Execution (BAX). To tackle this problem, we present a procedure,\nInfoBAX, that sequentially chooses queries that maximize mutual information\nwith respect to the algorithm's output. Applying this to Dijkstra's algorithm,\nfor instance, we infer shortest paths in synthetic and real-world graphs with\nblack-box edge costs. Using evolution strategies, we yield variants of Bayesian\noptimization that target local, rather than global, optima. On these problems,\nInfoBAX uses up to 500 times fewer queries to $f$ than required by the original\nalgorithm. Our method is closely connected to other Bayesian optimal\nexperimental design procedures such as entropy search methods and optimal\nsensor placement using Gaussian processes.\n", "versions": [{"version": "v1", "created": "Mon, 19 Apr 2021 17:22:11 GMT"}, {"version": "v2", "created": "Tue, 6 Jul 2021 17:56:46 GMT"}], "update_date": "2021-07-07", "authors_parsed": [["Neiswanger", "Willie", ""], ["Wang", "Ke Alexander", ""], ["Ermon", "Stefano", ""]]}, {"id": "2104.09658", "submitter": "Yutao Zhong", "authors": "Pranjal Awasthi and Natalie Frank and Anqi Mao and Mehryar Mohri and\n  Yutao Zhong", "title": "Calibration and Consistency of Adversarial Surrogate Losses", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Adversarial robustness is an increasingly critical property of classifiers in\napplications. The design of robust algorithms relies on surrogate losses since\nthe optimization of the adversarial loss with most hypothesis sets is NP-hard.\nBut which surrogate losses should be used and when do they benefit from\ntheoretical guarantees? We present an extensive study of this question,\nincluding a detailed analysis of the H-calibration and H-consistency of\nadversarial surrogate losses. We show that, under some general assumptions,\nconvex loss functions, or the supremum-based convex losses often used in\napplications, are not H-calibrated for important hypothesis sets such as\ngeneralized linear models or one-layer neural networks. We then give a\ncharacterization of H-calibration and prove that some surrogate losses are\nindeed H-calibrated for the adversarial loss, with these hypothesis sets. Next,\nwe show that H-calibration is not sufficient to guarantee consistency and prove\nthat, in the absence of any distributional assumption, no continuous surrogate\nloss is consistent in the adversarial setting. This, in particular, proves that\na claim presented in a COLT 2020 publication is inaccurate. (Calibration\nresults there are correct modulo subtle definition differences, but the\nconsistency claim does not hold.) Next, we identify natural conditions under\nwhich some surrogate losses that we describe in detail are H-consistent for\nhypothesis sets such as generalized linear models and one-layer neural\nnetworks. We also report a series of empirical results with simulated data,\nwhich show that many H-calibrated surrogate losses are indeed not H-consistent,\nand validate our theoretical assumptions.\n", "versions": [{"version": "v1", "created": "Mon, 19 Apr 2021 21:58:52 GMT"}, {"version": "v2", "created": "Tue, 4 May 2021 14:21:13 GMT"}], "update_date": "2021-05-05", "authors_parsed": [["Awasthi", "Pranjal", ""], ["Frank", "Natalie", ""], ["Mao", "Anqi", ""], ["Mohri", "Mehryar", ""], ["Zhong", "Yutao", ""]]}, {"id": "2104.09665", "submitter": "Allen Liu", "authors": "Allen Liu, Ankur Moitra", "title": "Learning GMMs with Nearly Optimal Robustness Guarantees", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.DS math.ST stat.ML stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this work we solve the problem of robustly learning a high-dimensional\nGaussian mixture model with $k$ components from $\\epsilon$-corrupted samples up\nto accuracy $\\widetilde{O}(\\epsilon)$ in total variation distance for any\nconstant $k$ and with mild assumptions on the mixture. This robustness\nguarantee is optimal up to polylogarithmic factors. At the heart of our\nalgorithm is a new way to relax a system of polynomial equations which\ncorresponds to solving an improper learning problem where we are allowed to\noutput a Gaussian mixture model whose weights are low-degree polynomials.\n", "versions": [{"version": "v1", "created": "Mon, 19 Apr 2021 22:14:54 GMT"}], "update_date": "2021-04-21", "authors_parsed": [["Liu", "Allen", ""], ["Moitra", "Ankur", ""]]}, {"id": "2104.09703", "submitter": "Katsuyuki Hagiwara", "authors": "Katsuyuki Hagiwara", "title": "Bridging between soft and hard thresholding by scaling", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this article, we developed and analyzed a thresholding method in which\nsoft thresholding estimators are independently expanded by empirical scaling\nvalues. The scaling values have a common hyper-parameter that is an order of\nexpansion of an ideal scaling value that achieves hard thresholding. We simply\ncall this estimator a scaled soft thresholding estimator. The scaled soft\nthresholding is a general method that includes the soft thresholding and\nnon-negative garrote as special cases and gives an another derivation of\nadaptive LASSO. We then derived the degree of freedom of the scaled soft\nthresholding by means of the Stein's unbiased risk estimate and found that it\nis decomposed into the degree of freedom of soft thresholding and the reminder\nconnecting to hard thresholding. In this meaning, the scaled soft thresholding\ngives a natural bridge between soft and hard thresholding methods. Since the\ndegree of freedom represents the degree of over-fitting, this result implies\nthat there are two sources of over-fitting in the scaled soft thresholding. The\nfirst source originated from soft thresholding is determined by the number of\nun-removed coefficients and is a natural measure of the degree of over-fitting.\nWe analyzed the second source in a particular case of the scaled soft\nthresholding by referring a known result for hard thresholding. We then found\nthat, in a sparse, large sample and non-parametric setting, the second source\nis largely determined by coefficient estimates whose true values are zeros and\nhas an influence on over-fitting when threshold levels are around noise levels\nin those coefficient estimates. In a simple numerical example, these\ntheoretical implications has well explained the behavior of the degree of\nfreedom. Moreover, based on the results here and some known facts, we explained\nthe behaviors of risks of soft, hard and scaled soft thresholding methods.\n", "versions": [{"version": "v1", "created": "Tue, 20 Apr 2021 00:58:05 GMT"}], "update_date": "2021-04-21", "authors_parsed": [["Hagiwara", "Katsuyuki", ""]]}, {"id": "2104.09732", "submitter": "Lester Mackey", "authors": "Tri Dao, Govinda M Kamath, Vasilis Syrgkanis, Lester Mackey", "title": "Knowledge Distillation as Semiparametric Inference", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A popular approach to model compression is to train an inexpensive student\nmodel to mimic the class probabilities of a highly accurate but cumbersome\nteacher model. Surprisingly, this two-step knowledge distillation process often\nleads to higher accuracy than training the student directly on labeled data. To\nexplain and enhance this phenomenon, we cast knowledge distillation as a\nsemiparametric inference problem with the optimal student model as the target,\nthe unknown Bayes class probabilities as nuisance, and the teacher\nprobabilities as a plug-in nuisance estimate. By adapting modern semiparametric\ntools, we derive new guarantees for the prediction error of standard\ndistillation and develop two enhancements -- cross-fitting and loss correction\n-- to mitigate the impact of teacher overfitting and underfitting on student\nperformance. We validate our findings empirically on both tabular and image\ndata and observe consistent improvements from our knowledge distillation\nenhancements.\n", "versions": [{"version": "v1", "created": "Tue, 20 Apr 2021 03:00:45 GMT"}], "update_date": "2021-04-21", "authors_parsed": [["Dao", "Tri", ""], ["Kamath", "Govinda M", ""], ["Syrgkanis", "Vasilis", ""], ["Mackey", "Lester", ""]]}, {"id": "2104.09855", "submitter": "Rendani Mbuvha", "authors": "Adam Balusik, Jared de Magalhaes and Rendani Mbuvha", "title": "Forecasting The JSE Top 40 Using Long Short-Term Memory Networks", "comments": "17 Pages, 5 Figures, 4 Tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  As a result of the greater availability of big data, as well as the\ndecreasing costs and increasing power of modern computing, the use of\nartificial neural networks for financial time series forecasting is once again\na major topic of discussion and research in the financial world. Despite this\nacademic focus, there are still contrasting opinions and bodies of literature\non which artificial neural networks perform the best and whether or not they\noutperform the forecasting capabilities of conventional time series models.\nThis paper uses a long-short term memory network to perform financial time\nseries forecasting on the return data of the JSE Top 40 index. Furthermore, the\nforecasting performance of the long-short term memory network is compared to\nthe forecasting performance of a seasonal autoregressive integrated moving\naverage model. This paper evaluates the varying approaches presented in the\nexisting literature and ultimately, compares the results to that existing\nliterature. The paper concludes that the long short-term memory network\noutperforms the seasonal autoregressive integrated moving average model when\nforecasting intraday directional movements as well as when forecasting the\nindex close price.\n", "versions": [{"version": "v1", "created": "Tue, 20 Apr 2021 09:39:38 GMT"}], "update_date": "2021-04-21", "authors_parsed": [["Balusik", "Adam", ""], ["de Magalhaes", "Jared", ""], ["Mbuvha", "Rendani", ""]]}, {"id": "2104.09937", "submitter": "Yuge Shi", "authors": "Yuge Shi, Jeffrey Seely, Philip H.S. Torr, N. Siddharth, Awni Hannun,\n  Nicolas Usunier, Gabriel Synnaeve", "title": "Gradient Matching for Domain Generalization", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Machine learning systems typically assume that the distributions of training\nand test sets match closely. However, a critical requirement of such systems in\nthe real world is their ability to generalize to unseen domains. Here, we\npropose an inter-domain gradient matching objective that targets domain\ngeneralization by maximizing the inner product between gradients from different\ndomains. Since direct optimization of the gradient inner product can be\ncomputationally prohibitive -- requires computation of second-order derivatives\n-- we derive a simpler first-order algorithm named Fish that approximates its\noptimization. We demonstrate the efficacy of Fish on 6 datasets from the Wilds\nbenchmark, which captures distribution shift across a diverse range of\nmodalities. Our method produces competitive results on these datasets and\nsurpasses all baselines on 4 of them. We perform experiments on both the Wilds\nbenchmark, which captures distribution shift in the real world, as well as\ndatasets in DomainBed benchmark that focuses more on synthetic-to-real\ntransfer. Our method produces competitive results on both benchmarks,\ndemonstrating its effectiveness across a wide range of domain generalization\ntasks.\n", "versions": [{"version": "v1", "created": "Tue, 20 Apr 2021 12:55:37 GMT"}, {"version": "v2", "created": "Sun, 11 Jul 2021 16:05:22 GMT"}, {"version": "v3", "created": "Wed, 14 Jul 2021 00:07:51 GMT"}], "update_date": "2021-07-15", "authors_parsed": [["Shi", "Yuge", ""], ["Seely", "Jeffrey", ""], ["Torr", "Philip H. S.", ""], ["Siddharth", "N.", ""], ["Hannun", "Awni", ""], ["Usunier", "Nicolas", ""], ["Synnaeve", "Gabriel", ""]]}, {"id": "2104.09958", "submitter": "Martin Engelcke", "authors": "Martin Engelcke, Oiwi Parker Jones, Ingmar Posner", "title": "GENESIS-V2: Inferring Unordered Object Representations without Iterative\n  Refinement", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Advances in object-centric generative models (OCGMs) have culminated in the\ndevelopment of a broad range of methods for unsupervised object segmentation\nand interpretable object-centric scene generation. These methods, however, are\nlimited to simulated and real-world datasets with limited visual complexity.\nMoreover, object representations are often inferred using RNNs which do not\nscale well to large images or iterative refinement which avoids imposing an\nunnatural ordering on objects in an image but requires the a priori\ninitialisation of a fixed number of object representations. In contrast to\nestablished paradigms, this work proposes an embedding-based approach in which\nembeddings of pixels are clustered in a differentiable fashion using a\nstochastic, non-parametric stick-breaking process. Similar to iterative\nrefinement, this clustering procedure also leads to randomly ordered object\nrepresentations, but without the need of initialising a fixed number of\nclusters a priori. This is used to develop a new model, GENESIS-V2, which can\ninfer a variable number of object representations without using RNNs or\niterative refinement. We show that GENESIS-V2 outperforms previous methods for\nunsupervised image segmentation and object-centric scene generation on\nestablished synthetic datasets as well as more complex real-world datasets.\n", "versions": [{"version": "v1", "created": "Tue, 20 Apr 2021 14:59:27 GMT"}, {"version": "v2", "created": "Wed, 21 Apr 2021 14:52:11 GMT"}], "update_date": "2021-04-22", "authors_parsed": [["Engelcke", "Martin", ""], ["Jones", "Oiwi Parker", ""], ["Posner", "Ingmar", ""]]}, {"id": "2104.09987", "submitter": "Alexandre Defossez", "authors": "Alexandre D\\'efossez, Yossi Adi, Gabriel Synnaeve", "title": "Differentiable Model Compression via Pseudo Quantization Noise", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.AI cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose to add independent pseudo quantization noise to model parameters\nduring training to approximate the effect of a quantization operator. This\nmethod, DiffQ, is differentiable both with respect to the unquantized\nparameters, and the number of bits used. Given a single hyper-parameter\nexpressing the desired balance between the quantized model size and accuracy,\nDiffQ can optimize the number of bits used per individual weight or groups of\nweights, in a single training. We experimentally verify that our method\noutperforms state-of-the-art quantization techniques on several benchmarks and\narchitectures for image classification, language modeling, and audio source\nseparation. For instance, on the Wikitext-103 language modeling benchmark,\nDiffQ compresses a 16 layers transformer model by a factor of 8, equivalent to\n4 bits precision, while losing only 0.5 points of perplexity. Code is available\nat: https://github.com/facebookresearch/diffq\n", "versions": [{"version": "v1", "created": "Tue, 20 Apr 2021 14:14:03 GMT"}], "update_date": "2021-04-21", "authors_parsed": [["D\u00e9fossez", "Alexandre", ""], ["Adi", "Yossi", ""], ["Synnaeve", "Gabriel", ""]]}, {"id": "2104.10061", "submitter": "Vincent Schellekens", "authors": "Vincent Schellekens and Laurent Jacques", "title": "Asymmetric compressive learning guarantees with applications to\n  quantized sketches", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  The compressive learning framework reduces the computational cost of training\non large-scale datasets. In a sketching phase, the data is first compressed to\na lightweight sketch vector, obtained by mapping the data samples through a\nwell-chosen feature map, and averaging those contributions. In a learning\nphase, the desired model parameters are then extracted from this sketch by\nsolving an optimization problem, which also involves a feature map. When the\nfeature map is identical during the sketching and learning phases, formal\nstatistical guarantees (excess risk bounds) have been proven.\n  However, the desirable properties of the feature map are different during\nsketching and learning (e.g. quantized outputs, and differentiability,\nrespectively). We thus study the relaxation where this map is allowed to be\ndifferent for each phase. First, we prove that the existing guarantees carry\nover to this asymmetric scheme, up to a controlled error term, provided some\nLimited Projected Distortion (LPD) property holds. We then instantiate this\nframework to the setting of quantized sketches, by proving that the LPD indeed\nholds for binary sketch contributions. Finally, we further validate the\napproach with numerical simulations, including a large-scale application in\naudio event classification.\n", "versions": [{"version": "v1", "created": "Tue, 20 Apr 2021 15:37:59 GMT"}], "update_date": "2021-04-21", "authors_parsed": [["Schellekens", "Vincent", ""], ["Jacques", "Laurent", ""]]}, {"id": "2104.10087", "submitter": "David Plans Dr.", "authors": "D. Morelli, N. Dolezalova, S. Ponzo, M. Colombo and D. Plans", "title": "Development of digitally obtainable 10-year risk scores for depression\n  and anxiety in the general population", "comments": "13 pages, 2 figures, 2 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/", "abstract": "  The burden of depression and anxiety in the world is rising. Identification\nof individuals at increased risk of developing these conditions would help to\ntarget them for prevention and ultimately reduce the healthcare burden. We\ndeveloped a 10-year predictive algorithm for depression and anxiety using the\nfull cohort of over 400,000 UK Biobank (UKB) participants without pre-existing\ndepression or anxiety using digitally obtainable information. From the initial\n204 variables selected from UKB, processed into > 520 features, iterative\nbackward elimination using Cox proportional hazards model was performed to\nselect predictors which account for the majority of its predictive capability.\nBaseline and reduced models were then trained for depression and anxiety using\nboth Cox and DeepSurv, a deep neural network approach to survival analysis. The\nbaseline Cox model achieved concordance of 0.813 and 0.778 on the validation\ndataset for depression and anxiety, respectively. For the DeepSurv model,\nrespective concordance indices were 0.805 and 0.774. After feature selection,\nthe depression model contained 43 predictors and the concordance index was\n0.801 for both Cox and DeepSurv. The reduced anxiety model, with 27 predictors,\nachieved concordance of 0.770 in both models. The final models showed good\ndiscrimination and calibration in the test datasets.We developed predictive\nrisk scores with high discrimination for depression and anxiety using the UKB\ncohort, incorporating predictors which are easily obtainable via smartphone. If\ndeployed in a digital solution, it would allow individuals to track their risk,\nas well as provide some pointers to how to decrease it through lifestyle\nchanges.\n", "versions": [{"version": "v1", "created": "Tue, 20 Apr 2021 16:16:56 GMT"}], "update_date": "2021-04-21", "authors_parsed": [["Morelli", "D.", ""], ["Dolezalova", "N.", ""], ["Ponzo", "S.", ""], ["Colombo", "M.", ""], ["Plans", "D.", ""]]}, {"id": "2104.10093", "submitter": "Gido van de Ven", "authors": "Gido M. van de Ven, Zhe Li, Andreas S. Tolias", "title": "Class-Incremental Learning with Generative Classifiers", "comments": "To appear in the IEEE Conference on Computer Vision and Pattern\n  Recognition Workshop (CVPR-W) on Continual Learning in Computer Vision\n  (CLVision) 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI cs.CV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Incrementally training deep neural networks to recognize new classes is a\nchallenging problem. Most existing class-incremental learning methods store\ndata or use generative replay, both of which have drawbacks, while\n'rehearsal-free' alternatives such as parameter regularization or\nbias-correction methods do not consistently achieve high performance. Here, we\nput forward a new strategy for class-incremental learning: generative\nclassification. Rather than directly learning the conditional distribution\np(y|x), our proposal is to learn the joint distribution p(x,y), factorized as\np(x|y)p(y), and to perform classification using Bayes' rule. As a\nproof-of-principle, here we implement this strategy by training a variational\nautoencoder for each class to be learned and by using importance sampling to\nestimate the likelihoods p(x|y). This simple approach performs very well on a\ndiverse set of continual learning benchmarks, outperforming generative replay\nand other existing baselines that do not store data.\n", "versions": [{"version": "v1", "created": "Tue, 20 Apr 2021 16:26:14 GMT"}, {"version": "v2", "created": "Wed, 28 Apr 2021 09:19:48 GMT"}], "update_date": "2021-04-29", "authors_parsed": [["van de Ven", "Gido M.", ""], ["Li", "Zhe", ""], ["Tolias", "Andreas S.", ""]]}, {"id": "2104.10103", "submitter": "Wanli Qiao", "authors": "Wanli Qiao and Amarda Shehu", "title": "Space Partitioning and Regression Mode Seeking via a Mean-Shift-Inspired\n  Algorithm", "comments": "44 pages, 4 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The mean shift (MS) algorithm is a nonparametric method used to cluster\nsample points and find the local modes of kernel density estimates, using an\nidea based on iterative gradient ascent. In this paper we develop a\nmean-shift-inspired algorithm to estimate the modes of regression functions and\npartition the sample points in the input space. We prove convergence of the\nsequences generated by the algorithm and derive the non-asymptotic rates of\nconvergence of the estimated local modes for the underlying regression model.\nWe also demonstrate the utility of the algorithm for data-enabled discovery\nthrough an application on biomolecular structure data. An extension to subspace\nconstrained mean shift (SCMS) algorithm used to extract ridges of regression\nfunctions is briefly discussed.\n", "versions": [{"version": "v1", "created": "Tue, 20 Apr 2021 16:35:17 GMT"}], "update_date": "2021-04-21", "authors_parsed": [["Qiao", "Wanli", ""], ["Shehu", "Amarda", ""]]}, {"id": "2104.10105", "submitter": "S Chandra Mouli", "authors": "S Chandra Mouli and Bruno Ribeiro", "title": "Neural Networks for Learning Counterfactual G-Invariances from Single\n  Environments", "comments": "ICLR 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Despite -- or maybe because of -- their astonishing capacity to fit data,\nneural networks are believed to have difficulties extrapolating beyond training\ndata distribution. This work shows that, for extrapolations based on finite\ntransformation groups, a model's inability to extrapolate is unrelated to its\ncapacity. Rather, the shortcoming is inherited from a learning hypothesis:\nExamples not explicitly observed with infinitely many training examples have\nunderspecified outcomes in the learner's model. In order to endow neural\nnetworks with the ability to extrapolate over group transformations, we\nintroduce a learning framework counterfactually-guided by the learning\nhypothesis that any group invariance to (known) transformation groups is\nmandatory even without evidence, unless the learner deems it inconsistent with\nthe training data. Unlike existing invariance-driven methods for\n(counterfactual) extrapolations, this framework allows extrapolations from a\nsingle environment. Finally, we introduce sequence and image extrapolation\ntasks that validate our framework and showcase the shortcomings of traditional\napproaches.\n", "versions": [{"version": "v1", "created": "Tue, 20 Apr 2021 16:35:35 GMT"}], "update_date": "2021-04-21", "authors_parsed": [["Mouli", "S Chandra", ""], ["Ribeiro", "Bruno", ""]]}, {"id": "2104.10125", "submitter": "Alexander Bond", "authors": "A. J. Bond, C. B. Beggs", "title": "Bisecting for selecting: using a Laplacian eigenmaps clustering approach\n  to create the new European football Super League", "comments": "24 pages, 9 Figures, 3 Tables, 1 Appendix", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP stat.ME stat.ML stat.OT", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  We use European football performance data to select teams to form the\nproposed European football Super League, using only unsupervised techniques. We\nfirst used random forest regression to select important variables predicting\ngoal difference, which we used to calculate the Euclidian distances between\nteams. Creating a Laplacian eigenmap, we bisected the Fielder vector to\nidentify the five major European football leagues' natural clusters. Our\nresults showed how an unsupervised approach could successfully identify four\nclusters based on five basic performance metrics: shots, shots on target, shots\nconceded, possession, and pass success. The top two clusters identify those\nteams who dominate their respective leagues and are the best candidates to\ncreate the most competitive elite super league.\n", "versions": [{"version": "v1", "created": "Tue, 20 Apr 2021 17:12:31 GMT"}], "update_date": "2021-04-21", "authors_parsed": [["Bond", "A. J.", ""], ["Beggs", "C. B.", ""]]}, {"id": "2104.10132", "submitter": "Claudio Gallicchio", "authors": "Claudio Gallicchio, Alessio Micheli, Luca Silvestri", "title": "Phase Transition Adaptation", "comments": "Accepted at IJCNN 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI stat.ML", "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/", "abstract": "  Artificial Recurrent Neural Networks are a powerful information processing\nabstraction, and Reservoir Computing provides an efficient strategy to build\nrobust implementations by projecting external inputs into high dimensional\ndynamical system trajectories. In this paper, we propose an extension of the\noriginal approach, a local unsupervised learning mechanism we call Phase\nTransition Adaptation, designed to drive the system dynamics towards the `edge\nof stability'. Here, the complex behavior exhibited by the system elicits an\nenhancement in its overall computational capacity. We show experimentally that\nour approach consistently achieves its purpose over several datasets.\n", "versions": [{"version": "v1", "created": "Tue, 20 Apr 2021 17:18:34 GMT"}], "update_date": "2021-04-21", "authors_parsed": [["Gallicchio", "Claudio", ""], ["Micheli", "Alessio", ""], ["Silvestri", "Luca", ""]]}, {"id": "2104.10150", "submitter": "Daniel Kowal", "authors": "Daniel R. Kowal", "title": "Bayesian subset selection and variable importance for interpretable\n  prediction and classification", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG stat.AP stat.CO stat.ME", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Subset selection is a valuable tool for interpretable learning, scientific\ndiscovery, and data compression. However, classical subset selection is often\neschewed due to selection instability, computational bottlenecks, and lack of\npost-selection inference. We address these challenges from a Bayesian\nperspective. Given any Bayesian predictive model $\\mathcal{M}$, we elicit\npredictively-competitive subsets using linear decision analysis. The approach\nis customizable for (local) prediction or classification and provides\ninterpretable summaries of $\\mathcal{M}$. A key quantity is the acceptable\nfamily of subsets, which leverages the predictive distribution from\n$\\mathcal{M}$ to identify subsets that offer nearly-optimal prediction. The\nacceptable family spawns new (co-) variable importance metrics based on whether\nvariables (co-) appear in all, some, or no acceptable subsets. Crucially, the\nlinear coefficients for any subset inherit regularization and predictive\nuncertainty quantification via $\\mathcal{M}$. The proposed approach exhibits\nexcellent prediction, interval estimation, and variable selection for simulated\ndata, including $p=400 > n$. These tools are applied to a large education\ndataset with highly correlated covariates, where the acceptable family is\nespecially useful. Our analysis provides unique insights into the combination\nof environmental, socioeconomic, and demographic factors that predict\neducational outcomes, and features highly competitive prediction with\nremarkable stability.\n", "versions": [{"version": "v1", "created": "Tue, 20 Apr 2021 17:48:34 GMT"}], "update_date": "2021-04-21", "authors_parsed": [["Kowal", "Daniel R.", ""]]}, {"id": "2104.10190", "submitter": "Tim G. J. Rudner", "authors": "Tim G. J. Rudner and Vitchyr H. Pong and Rowan McAllister and Yarin\n  Gal and Sergey Levine", "title": "Outcome-Driven Reinforcement Learning via Variational Inference", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI stat.ME stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  While reinforcement learning algorithms provide automated acquisition of\noptimal policies, practical application of such methods requires a number of\ndesign decisions, such as manually designing reward functions that not only\ndefine the task, but also provide sufficient shaping to accomplish it. In this\npaper, we discuss a new perspective on reinforcement learning, recasting it as\nthe problem of inferring actions that achieve desired outcomes, rather than a\nproblem of maximizing rewards. To solve the resulting outcome-directed\ninference problem, we establish a novel variational inference formulation that\nallows us to derive a well-shaped reward function which can be learned directly\nfrom environment interactions. From the corresponding variational objective, we\nalso derive a new probabilistic Bellman backup operator reminiscent of the\nstandard Bellman backup operator and use it to develop an off-policy algorithm\nto solve goal-directed tasks. We empirically demonstrate that this method\neliminates the need to design reward functions and leads to effective\ngoal-directed behaviors.\n", "versions": [{"version": "v1", "created": "Tue, 20 Apr 2021 18:16:21 GMT"}], "update_date": "2021-04-22", "authors_parsed": [["Rudner", "Tim G. J.", ""], ["Pong", "Vitchyr H.", ""], ["McAllister", "Rowan", ""], ["Gal", "Yarin", ""], ["Levine", "Sergey", ""]]}, {"id": "2104.10201", "submitter": "David Eriksson", "authors": "Ryan Turner, David Eriksson, Michael McCourt, Juha Kiili, Eero\n  Laaksonen, Zhen Xu, Isabelle Guyon", "title": "Bayesian Optimization is Superior to Random Search for Machine Learning\n  Hyperparameter Tuning: Analysis of the Black-Box Optimization Challenge 2020", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper presents the results and insights from the black-box optimization\n(BBO) challenge at NeurIPS 2020 which ran from July-October, 2020. The\nchallenge emphasized the importance of evaluating derivative-free optimizers\nfor tuning the hyperparameters of machine learning models. This was the first\nblack-box optimization challenge with a machine learning emphasis. It was based\non tuning (validation set) performance of standard machine learning models on\nreal datasets. This competition has widespread impact as black-box optimization\n(e.g., Bayesian optimization) is relevant for hyperparameter tuning in almost\nevery machine learning project as well as many applications outside of machine\nlearning. The final leaderboard was determined using the optimization\nperformance on held-out (hidden) objective functions, where the optimizers ran\nwithout human intervention. Baselines were set using the default settings of\nseveral open-source black-box optimization packages as well as random search.\n", "versions": [{"version": "v1", "created": "Tue, 20 Apr 2021 18:44:59 GMT"}], "update_date": "2021-04-22", "authors_parsed": [["Turner", "Ryan", ""], ["Eriksson", "David", ""], ["McCourt", "Michael", ""], ["Kiili", "Juha", ""], ["Laaksonen", "Eero", ""], ["Xu", "Zhen", ""], ["Guyon", "Isabelle", ""]]}, {"id": "2104.10223", "submitter": "Luis Oala", "authors": "Saul Calderon-Ramirez and Luis Oala", "title": "More Than Meets The Eye: Semi-supervised Learning Under Non-IID Data", "comments": "Presented as a RobustML workshop paper at ICLR 2021. Both authors\n  contributed equally. This article extends arXiv:2006.07767", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV stat.ML", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  A common heuristic in semi-supervised deep learning (SSDL) is to select\nunlabelled data based on a notion of semantic similarity to the labelled data.\nFor example, labelled images of numbers should be paired with unlabelled images\nof numbers instead of, say, unlabelled images of cars. We refer to this\npractice as semantic data set matching. In this work, we demonstrate the limits\nof semantic data set matching. We show that it can sometimes even degrade the\nperformance for a state of the art SSDL algorithm. We present and make\navailable a comprehensive simulation sandbox, called non-IID-SSDL, for stress\ntesting an SSDL algorithm under different degrees of distribution mismatch\nbetween the labelled and unlabelled data sets. In addition, we demonstrate that\nsimple density based dissimilarity measures in the feature space of a generic\nclassifier offer a promising and more reliable quantitative matching criterion\nto select unlabelled data before SSDL training.\n", "versions": [{"version": "v1", "created": "Tue, 20 Apr 2021 19:51:10 GMT"}], "update_date": "2021-04-27", "authors_parsed": [["Calderon-Ramirez", "Saul", ""], ["Oala", "Luis", ""]]}, {"id": "2104.10334", "submitter": "Sylvia Klosin", "authors": "Sylvia Klosin", "title": "Automatic Double Machine Learning for Continuous Treatment Effects", "comments": "30 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "econ.EM math.ST stat.ML stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we introduce and prove asymptotic normality for a new\nnonparametric estimator of continuous treatment effects. Specifically, we\nestimate the average dose-response function - the expected value of an outcome\nof interest at a particular level of the treatment level. We utilize tools from\nboth the double debiased machine learning (DML) and the automatic double\nmachine learning (ADML) literatures to construct our estimator. Our estimator\nutilizes a novel debiasing method that leads to nice theoretical stability and\nbalancing properties. In simulations our estimator performs well compared to\ncurrent methods.\n", "versions": [{"version": "v1", "created": "Wed, 21 Apr 2021 03:17:40 GMT"}], "update_date": "2021-04-22", "authors_parsed": [["Klosin", "Sylvia", ""]]}, {"id": "2104.10347", "submitter": "Yali Wan", "authors": "Yali Wan and Marina Meila", "title": "A class of network models recoverable by spectral clustering", "comments": "15 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Finding communities in networks is a problem that remains difficult, in spite\nof the amount of attention it has recently received. The Stochastic Block-Model\n(SBM) is a generative model for graphs with \"communities\" for which, because of\nits simplicity, the theoretical understanding has advanced fast in recent\nyears. In particular, there have been various results showing that simple\nversions of spectral clustering using the Normalized Laplacian of the graph can\nrecover the communities almost perfectly with high probability. Here we show\nthat essentially the same algorithm used for the SBM and for its extension\ncalled Degree-Corrected SBM, works on a wider class of Block-Models, which we\ncall Preference Frame Models, with essentially the same guarantees. Moreover,\nthe parametrization we introduce clearly exhibits the free parameters needed to\nspecify this class of models, and results in bounds that expose with more\nclarity the parameters that control the recovery error in this model class.\n", "versions": [{"version": "v1", "created": "Wed, 21 Apr 2021 04:22:18 GMT"}], "update_date": "2021-04-22", "authors_parsed": [["Wan", "Yali", ""], ["Meila", "Marina", ""]]}, {"id": "2104.10507", "submitter": "Yingbo Gao", "authors": "Yingbo Gao, David Thulke, Alexander Gerstenberger, Khoa Viet Tran,\n  Ralf Schl\\\"uter, Hermann Ney", "title": "On Sampling-Based Training Criteria for Neural Language Modeling", "comments": "Accepted at INTERSPEECH 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.SD eess.AS stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  As the vocabulary size of modern word-based language models becomes ever\nlarger, many sampling-based training criteria are proposed and investigated.\nThe essence of these sampling methods is that the softmax-related traversal\nover the entire vocabulary can be simplified, giving speedups compared to the\nbaseline. A problem we notice about the current landscape of such sampling\nmethods is the lack of a systematic comparison and some myths about preferring\none over another. In this work, we consider Monte Carlo sampling, importance\nsampling, a novel method we call compensated partial summation, and noise\ncontrastive estimation. Linking back to the three traditional criteria, namely\nmean squared error, binary cross-entropy, and cross-entropy, we derive the\ntheoretical solutions to the training problems. Contrary to some common belief,\nwe show that all these sampling methods can perform equally well, as long as we\ncorrect for the intended class posterior probabilities. Experimental results in\nlanguage modeling and automatic speech recognition on Switchboard and\nLibriSpeech support our claim, with all sampling-based methods showing similar\nperplexities and word error rates while giving the expected speedups.\n", "versions": [{"version": "v1", "created": "Wed, 21 Apr 2021 12:55:52 GMT"}, {"version": "v2", "created": "Thu, 17 Jun 2021 10:21:13 GMT"}], "update_date": "2021-06-18", "authors_parsed": [["Gao", "Yingbo", ""], ["Thulke", "David", ""], ["Gerstenberger", "Alexander", ""], ["Tran", "Khoa Viet", ""], ["Schl\u00fcter", "Ralf", ""], ["Ney", "Hermann", ""]]}, {"id": "2104.10527", "submitter": "Mike Huisman", "authors": "Mike Huisman and Aske Plaat and Jan N. van Rijn", "title": "Stateless Neural Meta-Learning using Second-Order Gradients", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Deep learning typically requires large data sets and much compute power for\neach new problem that is learned. Meta-learning can be used to learn a good\nprior that facilitates quick learning, thereby relaxing these requirements so\nthat new tasks can be learned quicker; two popular approaches are MAML and the\nmeta-learner LSTM. In this work, we compare the two and formally show that the\nmeta-learner LSTM subsumes MAML. Combining this insight with recent empirical\nfindings, we construct a new algorithm (dubbed TURTLE) which is simpler than\nthe meta-learner LSTM yet more expressive than MAML. TURTLE outperforms both\ntechniques at few-shot sine wave regression and image classification on\nminiImageNet and CUB without any additional hyperparameter tuning, at a\ncomputational cost that is comparable with second-order MAML. The key to\nTURTLE's success lies in the use of second-order gradients, which also\nsignificantly increases the performance of the meta-learner LSTM by 1-6%\naccuracy.\n", "versions": [{"version": "v1", "created": "Wed, 21 Apr 2021 13:34:31 GMT"}], "update_date": "2021-04-22", "authors_parsed": [["Huisman", "Mike", ""], ["Plaat", "Aske", ""], ["van Rijn", "Jan N.", ""]]}, {"id": "2104.10544", "submitter": "James Townsend", "authors": "James Townsend", "title": "Lossless Compression with Latent Variable Models", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI cs.IT math.IT stat.CO stat.ML", "license": "http://creativecommons.org/publicdomain/zero/1.0/", "abstract": "  We develop a simple and elegant method for lossless compression using latent\nvariable models, which we call 'bits back with asymmetric numeral systems'\n(BB-ANS). The method involves interleaving encode and decode steps, and\nachieves an optimal rate when compressing batches of data. We demonstrate it\nfirstly on the MNIST test set, showing that state-of-the-art lossless\ncompression is possible using a small variational autoencoder (VAE) model. We\nthen make use of a novel empirical insight, that fully convolutional generative\nmodels, trained on small images, are able to generalize to images of arbitrary\nsize, and extend BB-ANS to hierarchical latent variable models, enabling\nstate-of-the-art lossless compression of full-size colour images from the\nImageNet dataset. We describe 'Craystack', a modular software framework which\nwe have developed for rapid prototyping of compression using deep generative\nmodels.\n", "versions": [{"version": "v1", "created": "Wed, 21 Apr 2021 14:03:05 GMT"}, {"version": "v2", "created": "Thu, 22 Apr 2021 09:28:41 GMT"}], "update_date": "2021-04-23", "authors_parsed": [["Townsend", "James", ""]]}, {"id": "2104.10554", "submitter": "Hengrui Cai", "authors": "Hengrui Cai, Wenbin Lu, Rui Song", "title": "Calibrated Optimal Decision Making with Multiple Data Sources and\n  Limited Outcome", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME math.ST stat.AP stat.ML stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider the optimal decision-making problem in a primary sample of\ninterest with multiple auxiliary sources available. The outcome of interest is\nlimited in the sense that it is only observed in the primary sample. In\nreality, such multiple data sources may belong to different populations and\nthus cannot be combined directly. This paper proposes a novel calibrated\noptimal decision rule (CODR) to address the limited outcome, by leveraging the\nshared pattern in multiple data sources. Under a mild and testable assumption\nthat the conditional means of intermediate outcomes in different samples are\nequal given baseline covariates and the treatment information, we can show that\nthe calibrated mean outcome of interest under the CODR is unbiased and more\nefficient than using the primary sample solely. Extensive experiments on\nsimulated datasets demonstrate empirical validity and improvement of the\nproposed CODR, followed by a real application on the MIMIC-III as the primary\nsample with auxiliary data from eICU.\n", "versions": [{"version": "v1", "created": "Wed, 21 Apr 2021 14:24:17 GMT"}], "update_date": "2021-04-22", "authors_parsed": [["Cai", "Hengrui", ""], ["Lu", "Wenbin", ""], ["Song", "Rui", ""]]}, {"id": "2104.10555", "submitter": "John Clemens", "authors": "John Clemens", "title": "MLDS: A Dataset for Weight-Space Analysis of Neural Networks", "comments": "For further information and download links, see\n  https://www.mlcathome.org/", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Neural networks are powerful models that solve a variety of complex\nreal-world problems. However, the stochastic nature of training and large\nnumber of parameters in a typical neural model makes them difficult to evaluate\nvia inspection. Research shows this opacity can hide latent undesirable\nbehavior, be it from poorly representative training data or via malicious\nintent to subvert the behavior of the network, and that this behavior is\ndifficult to detect via traditional indirect evaluation criteria such as loss.\nTherefore, it is time to explore direct ways to evaluate a trained neural model\nvia its structure and weights. In this paper we present MLDS, a new dataset\nconsisting of thousands of trained neural networks with carefully controlled\nparameters and generated via a global volunteer-based distributed computing\nplatform. This dataset enables new insights into both model-to-model and\nmodel-to-training-data relationships. We use this dataset to show clustering of\nmodels in weight-space with identical training data and meaningful divergence\nin weight-space with even a small change to the training data, suggesting that\nweight-space analysis is a viable and effective alternative to loss for\nevaluating neural networks.\n", "versions": [{"version": "v1", "created": "Wed, 21 Apr 2021 14:24:26 GMT"}], "update_date": "2021-04-22", "authors_parsed": [["Clemens", "John", ""]]}, {"id": "2104.10573", "submitter": "Hengrui Cai", "authors": "Hengrui Cai, Rui Song, Wenbin Lu", "title": "GEAR: On Optimal Decision Making with Auxiliary Data", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME math.ST stat.AP stat.ML stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Personalized optimal decision making, finding the optimal decision rule (ODR)\nbased on individual characteristics, has attracted increasing attention\nrecently in many fields, such as education, economics, and medicine. Current\nODR methods usually require the primary outcome of interest in samples for\nassessing treatment effects, namely the experimental sample. However, in many\nstudies, treatments may have a long-term effect, and as such the primary\noutcome of interest cannot be observed in the experimental sample due to the\nlimited duration of experiments, which makes the estimation of ODR impossible.\nThis paper is inspired to address this challenge by making use of an auxiliary\nsample to facilitate the estimation of ODR in the experimental sample. We\npropose an auGmented inverse propensity weighted Experimental and Auxiliary\nsample-based decision Rule (GEAR) by maximizing the augmented inverse\npropensity weighted value estimator over a class of decision rules using the\nexperimental sample, with the primary outcome being imputed based on the\nauxiliary sample. The asymptotic properties of the proposed GEAR estimators and\ntheir associated value estimators are established. Simulation studies are\nconducted to demonstrate its empirical validity with a real AIDS application.\n", "versions": [{"version": "v1", "created": "Wed, 21 Apr 2021 14:59:25 GMT"}], "update_date": "2021-04-22", "authors_parsed": [["Cai", "Hengrui", ""], ["Song", "Rui", ""], ["Lu", "Wenbin", ""]]}, {"id": "2104.10601", "submitter": "Mika Meitz", "authors": "Mika Meitz", "title": "Statistical inference for generative adversarial networks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.ML stat.TH", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  This paper studies generative adversarial networks (GANs) from a statistical\nperspective. A GAN is a popular machine learning method in which the parameters\nof two neural networks, a generator and a discriminator, are estimated to solve\na particular minimax problem. This minimax problem typically has a multitude of\nsolutions and the focus of this paper are the statistical properties of these\nsolutions. We address two key issues for the generator and discriminator\nnetwork parameters, consistent estimation and confidence sets. We first show\nthat the set of solutions to the sample GAN problem is a (Hausdorff) consistent\nestimator of the set of solutions to the corresponding population GAN problem.\nWe then devise a computationally intensive procedure to form confidence sets\nand show that these sets contain the population GAN solutions with the desired\ncoverage probability. The assumptions employed in our results are weak and hold\nin many practical GAN applications. To the best of our knowledge, this paper\nprovides the first results on statistical inference for GANs in the empirically\nrelevant case of multiple solutions.\n", "versions": [{"version": "v1", "created": "Wed, 21 Apr 2021 15:59:12 GMT"}], "update_date": "2021-04-22", "authors_parsed": [["Meitz", "Mika", ""]]}, {"id": "2104.10637", "submitter": "Zhan Yu", "authors": "Zhan Yu, Daniel W. C. Ho, Ding-Xuan Zhou", "title": "Robust Kernel-based Distribution Regression", "comments": "29 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG math.FA stat.ML", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Regularization schemes for regression have been widely studied in learning\ntheory and inverse problems. In this paper, we study distribution regression\n(DR) which involves two stages of sampling, and aims at regressing from\nprobability measures to real-valued responses over a reproducing kernel Hilbert\nspace (RKHS). Recently, theoretical analysis on DR has been carried out via\nkernel ridge regression and several learning behaviors have been observed.\nHowever, the topic has not been explored and understood beyond the least square\nbased DR. By introducing a robust loss function $l_{\\sigma}$ for two-stage\nsampling problems, we present a novel robust distribution regression (RDR)\nscheme. With a windowing function $V$ and a scaling parameter $\\sigma$ which\ncan be appropriately chosen, $l_{\\sigma}$ can include a wide range of popular\nused loss functions that enrich the theme of DR. Moreover, the loss\n$l_{\\sigma}$ is not necessarily convex, hence largely improving the former\nregression class (least square) in the literature of DR. The learning rates\nunder different regularity ranges of the regression function $f_{\\rho}$ are\ncomprehensively studied and derived via integral operator techniques. The\nscaling parameter $\\sigma$ is shown to be crucial in providing robustness and\nsatisfactory learning rates of RDR.\n", "versions": [{"version": "v1", "created": "Wed, 21 Apr 2021 17:03:46 GMT"}], "update_date": "2021-04-22", "authors_parsed": [["Yu", "Zhan", ""], ["Ho", "Daniel W. C.", ""], ["Zhou", "Ding-Xuan", ""]]}, {"id": "2104.10706", "submitter": "Pratyush Maini", "authors": "Pratyush Maini and Mohammad Yaghini and Nicolas Papernot", "title": "Dataset Inference: Ownership Resolution in Machine Learning", "comments": "Published as a conference paper at ICLR 2021 (Spotlight Presentation)", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.CR cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  With increasingly more data and computation involved in their training,\nmachine learning models constitute valuable intellectual property. This has\nspurred interest in model stealing, which is made more practical by advances in\nlearning with partial, little, or no supervision. Existing defenses focus on\ninserting unique watermarks in a model's decision surface, but this is\ninsufficient: the watermarks are not sampled from the training distribution and\nthus are not always preserved during model stealing. In this paper, we make the\nkey observation that knowledge contained in the stolen model's training set is\nwhat is common to all stolen copies. The adversary's goal, irrespective of the\nattack employed, is always to extract this knowledge or its by-products. This\ngives the original model's owner a strong advantage over the adversary: model\nowners have access to the original training data. We thus introduce $dataset$\n$inference$, the process of identifying whether a suspected model copy has\nprivate knowledge from the original model's dataset, as a defense against model\nstealing. We develop an approach for dataset inference that combines\nstatistical testing with the ability to estimate the distance of multiple data\npoints to the decision boundary. Our experiments on CIFAR10, SVHN, CIFAR100 and\nImageNet show that model owners can claim with confidence greater than 99% that\ntheir model (or dataset as a matter of fact) was stolen, despite only exposing\n50 of the stolen model's training points. Dataset inference defends against\nstate-of-the-art attacks even when the adversary is adaptive. Unlike prior\nwork, it does not require retraining or overfitting the defended model.\n", "versions": [{"version": "v1", "created": "Wed, 21 Apr 2021 18:12:18 GMT"}], "update_date": "2021-04-23", "authors_parsed": [["Maini", "Pratyush", ""], ["Yaghini", "Mohammad", ""], ["Papernot", "Nicolas", ""]]}, {"id": "2104.10727", "submitter": "Benny Avelin", "authors": "Benny Avelin and Anders Karlsson", "title": "Deep limits and cut-off phenomena for neural networks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG math.DS stat.ML", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  We consider dynamical and geometrical aspects of deep learning. For many\nstandard choices of layer maps we display semi-invariant metrics which quantify\ndifferences between data or decision functions. This allows us, when\nconsidering random layer maps and using non-commutative ergodic theorems, to\ndeduce that certain limits exist when letting the number of layers tend to\ninfinity. We also examine the random initialization of standard networks where\nwe observe a surprising cut-off phenomenon in terms of the number of layers,\nthe depth of the network. This could be a relevant parameter when choosing an\nappropriate number of layers for a given learning task, or for selecting a good\ninitialization procedure. More generally, we hope that the notions and results\nin this paper can provide a framework, in particular a geometric one, for a\npart of the theoretical understanding of deep neural networks.\n", "versions": [{"version": "v1", "created": "Wed, 21 Apr 2021 19:07:43 GMT"}], "update_date": "2021-04-23", "authors_parsed": [["Avelin", "Benny", ""], ["Karlsson", "Anders", ""]]}, {"id": "2104.10746", "submitter": "Jan Palczewski", "authors": "Lukas Cironis, Jan Palczewski, Georgios Aivaliotis", "title": "Automatic model training under restrictive time constraints", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We develop a hyperparameter optimisation algorithm, Automated Budget\nConstrained Training (AutoBCT), which balances the quality of a model with the\ncomputational cost required to tune it. The relationship between\nhyperparameters, model quality and computational cost must be learnt and this\nlearning is incorporated directly into the optimisation problem. At each\ntraining epoch, the algorithm decides whether to terminate or continue\ntraining, and, in the latter case, what values of hyperparameters to use. This\ndecision weighs optimally potential improvements in the quality with the\nadditional training time and the uncertainty about the learnt quantities. The\nperformance of our algorithm is verified on a number of machine learning\nproblems encompassing random forests and neural networks. Our approach is\nrooted in the theory of Markov decision processes with partial information and\nwe develop a numerical method to compute the value function and an optimal\nstrategy.\n", "versions": [{"version": "v1", "created": "Wed, 21 Apr 2021 20:20:36 GMT"}], "update_date": "2021-04-23", "authors_parsed": [["Cironis", "Lukas", ""], ["Palczewski", "Jan", ""], ["Aivaliotis", "Georgios", ""]]}, {"id": "2104.10751", "submitter": "Hakan Akyuz", "authors": "M. Hakan Aky\\\"uz, \\c{S}. \\.Ilker Birbil", "title": "Discovering Classification Rules for Interpretable Learning with Linear\n  Programming", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Rules embody a set of if-then statements which include one or more conditions\nto classify a subset of samples in a dataset. In various applications such\nclassification rules are considered to be interpretable by the decision makers.\nWe introduce two new algorithms for interpretability and learning. Both\nalgorithms take advantage of linear programming, and hence, they are scalable\nto large data sets. The first algorithm extracts rules for interpretation of\ntrained models that are based on tree/rule ensembles. The second algorithm\ngenerates a set of classification rules through a column generation approach.\nThe proposed algorithms return a set of rules along with their optimal weights\nindicating the importance of each rule for classification. Moreover, our\nalgorithms allow assigning cost coefficients, which could relate to different\nattributes of the rules, such as; rule lengths, estimator weights, number of\nfalse negatives, and so on. Thus, the decision makers can adjust these\ncoefficients to divert the training process and obtain a set of rules that are\nmore appealing for their needs. We have tested the performances of both\nalgorithms on a collection of datasets and presented a case study to elaborate\non optimal rule weights. Our results show that a good compromise between\ninterpretability and accuracy can be obtained by the proposed algorithms.\n", "versions": [{"version": "v1", "created": "Wed, 21 Apr 2021 20:31:28 GMT"}], "update_date": "2021-04-23", "authors_parsed": [["Aky\u00fcz", "M. Hakan", ""], ["Birbil", "\u015e. \u0130lker", ""]]}, {"id": "2104.10770", "submitter": "Zeyu Wei", "authors": "Zeyu Wei and Yen-Chi Chen", "title": "Skeleton Clustering: Dimension-Free Density-based Clustering", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We introduce a density-based clustering method called skeleton clustering\nthat can detect clusters in multivariate and even high-dimensional data with\nirregular shapes. To bypass the curse of dimensionality, we propose surrogate\ndensity measures that are less dependent on the dimension but have intuitive\ngeometric interpretations. The clustering framework constructs a concise\nrepresentation of the given data as an intermediate step and can be thought of\nas a combination of prototype methods, density-based clustering, and\nhierarchical clustering. We show by theoretical analysis and empirical studies\nthat the skeleton clustering leads to reliable clusters in multivariate and\nhigh-dimensional scenarios.\n", "versions": [{"version": "v1", "created": "Wed, 21 Apr 2021 21:25:02 GMT"}], "update_date": "2021-04-23", "authors_parsed": [["Wei", "Zeyu", ""], ["Chen", "Yen-Chi", ""]]}, {"id": "2104.10785", "submitter": "Reza Godaz", "authors": "Reza Godaz, Reza Monsefi, Faezeh Toutounian, Reshad Hosseini", "title": "Accurate and fast matrix factorization for low-rank learning", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper we tackle two important challenges related to the accurate\npartial singular value decomposition (SVD) and numerical rank estimation of a\nhuge matrix to use in low-rank learning problems in a fast way. We use the\nconcepts of Krylov subspaces such as the Golub-Kahan bidiagonalization process\nas well as Ritz vectors to achieve these goals. Our experiments identify\nvarious advantages of the proposed methods compared to traditional and\nrandomized SVD (R-SVD) methods with respect to the accuracy of the singular\nvalues and corresponding singular vectors computed in a similar execution time.\nThe proposed methods are appropriate for applications involving huge matrices\nwhere accuracy in all spectrum of the desired singular values, and also all of\ncorresponding singular vectors is essential. We evaluate our method in the real\napplication of Riemannian similarity learning (RSL) between two various image\ndatasets of MNIST and USPS.\n", "versions": [{"version": "v1", "created": "Wed, 21 Apr 2021 22:35:02 GMT"}, {"version": "v2", "created": "Mon, 3 May 2021 18:27:31 GMT"}, {"version": "v3", "created": "Thu, 15 Jul 2021 18:01:59 GMT"}], "update_date": "2021-07-19", "authors_parsed": [["Godaz", "Reza", ""], ["Monsefi", "Reza", ""], ["Toutounian", "Faezeh", ""], ["Hosseini", "Reshad", ""]]}, {"id": "2104.10790", "submitter": "Richard Zhang", "authors": "Richard Y. Zhang", "title": "Sharp Global Guarantees for Nonconvex Low-Rank Matrix Recovery in the\n  Overparameterized Regime", "comments": "v2 corrects minor typos", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.OC cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We prove that it is possible for nonconvex low-rank matrix recovery to\ncontain no spurious local minima when the rank of the unknown ground truth\n$r^{\\star}<r$ is strictly less than the search rank $r$, and yet for the claim\nto be false when $r^{\\star}=r$. Under the restricted isometry property (RIP),\nwe prove, for the general overparameterized regime with $r^{\\star}\\le r$, that\nan RIP constant of $\\delta<1/(1+\\sqrt{r^{\\star}/r})$ is sufficient for the\ninexistence of spurious local minima, and that\n$\\delta<1/(1+1/\\sqrt{r-r^{\\star}+1})$ is necessary due to existence of\ncounterexamples. Without an explicit control over $r^{\\star}\\le r$, an RIP\nconstant of $\\delta<1/2$ is both necessary and sufficient for the exact\nrecovery of a rank-$r$ ground truth. But if the ground truth is known a priori\nto have $r^{\\star}=1$, then the sharp RIP threshold for exact recovery is\nimproved to $\\delta<1/(1+1/\\sqrt{r})$.\n", "versions": [{"version": "v1", "created": "Wed, 21 Apr 2021 23:07:18 GMT"}, {"version": "v2", "created": "Mon, 26 Apr 2021 23:47:45 GMT"}], "update_date": "2021-04-28", "authors_parsed": [["Zhang", "Richard Y.", ""]]}, {"id": "2104.10840", "submitter": "Ichiro Takeuchi Prof.", "authors": "Toshiaki Tsukurimichi, Yu Inatsu, Vo Nguyen Le Duy, Ichiro Takeuchi", "title": "Conditional Selective Inference for Robust Regression and Outlier\n  Detection using Piecewise-Linear Homotopy Continuation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In practical data analysis under noisy environment, it is common to first use\nrobust methods to identify outliers, and then to conduct further analysis after\nremoving the outliers. In this paper, we consider statistical inference of the\nmodel estimated after outliers are removed, which can be interpreted as a\nselective inference (SI) problem. To use conditional SI framework, it is\nnecessary to characterize the events of how the robust method identifies\noutliers. Unfortunately, the existing methods cannot be directly used here\nbecause they are applicable to the case where the selection events can be\nrepresented by linear/quadratic constraints. In this paper, we propose a\nconditional SI method for popular robust regressions by using homotopy method.\nWe show that the proposed conditional SI method is applicable to a wide class\nof robust regression and outlier detection methods and has good empirical\nperformance on both synthetic data and real data experiments.\n", "versions": [{"version": "v1", "created": "Thu, 22 Apr 2021 03:01:18 GMT"}], "update_date": "2021-04-23", "authors_parsed": [["Tsukurimichi", "Toshiaki", ""], ["Inatsu", "Yu", ""], ["Duy", "Vo Nguyen Le", ""], ["Takeuchi", "Ichiro", ""]]}, {"id": "2104.10911", "submitter": "Ryosuke Shimmura", "authors": "Ryosuke Shimmura and Joe Suzuki", "title": "Converting ADMM to a Proximal Gradient for Convex Optimization Problems", "comments": "15 pages, 8 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.OC stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In machine learning and data science, we often consider efficiency for\nsolving problems. In sparse estimation, such as fused lasso and convex\nclustering, we apply either the proximal gradient method or the alternating\ndirection method of multipliers (ADMM) to solve the problem. It takes time to\ninclude matrix division in the former case, while an efficient method such as\nFISTA (fast iterative shrinkage-thresholding algorithm) has been developed in\nthe latter case. This paper proposes a general method for converting the ADMM\nsolution to the proximal gradient method, assuming that the constraints and\nobjectives are strongly convex. Then, we apply it to sparse estimation\nproblems, such as sparse convex clustering and trend filtering, and we show by\nnumerical experiments that we can obtain a significant improvement in terms of\nefficiency.\n", "versions": [{"version": "v1", "created": "Thu, 22 Apr 2021 07:41:12 GMT"}], "update_date": "2021-04-23", "authors_parsed": [["Shimmura", "Ryosuke", ""], ["Suzuki", "Joe", ""]]}, {"id": "2104.11009", "submitter": "Udit Bhatia", "authors": "Pravin Bhasme, Jenil Vagadiya, Udit Bhatia", "title": "Enhancing predictive skills in physically-consistent way: Physics\n  Informed Machine Learning for Hydrological Processes", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG stat.AP", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Current modeling approaches for hydrological modeling often rely on either\nphysics-based or data-science methods, including Machine Learning (ML)\nalgorithms. While physics-based models tend to rigid structure resulting in\nunrealistic parameter values in certain instances, ML algorithms establish the\ninput-output relationship while ignoring the constraints imposed by well-known\nphysical processes. While there is a notion that the physics model enables\nbetter process understanding and ML algorithms exhibit better predictive\nskills, scientific knowledge that does not add to predictive ability may be\ndeceptive. Hence, there is a need for a hybrid modeling approach to couple ML\nalgorithms and physics-based models in a synergistic manner. Here we develop a\nPhysics Informed Machine Learning (PIML) model that combines the process\nunderstanding of conceptual hydrological model with predictive abilities of\nstate-of-the-art ML models. We apply the proposed model to predict the monthly\ntime series of the target (streamflow) and intermediate variables (actual\nevapotranspiration) in the Narmada river basin in India. Our results show the\ncapability of the PIML model to outperform a purely conceptual model ($abcd$\nmodel) and ML algorithms while ensuring the physical consistency in outputs\nvalidated through water balance analysis. The systematic approach for combining\nconceptual model structure with ML algorithms could be used to improve the\npredictive accuracy of crucial hydrological processes important for flood risk\nassessment.\n", "versions": [{"version": "v1", "created": "Thu, 22 Apr 2021 12:13:42 GMT"}], "update_date": "2021-04-23", "authors_parsed": [["Bhasme", "Pravin", ""], ["Vagadiya", "Jenil", ""], ["Bhatia", "Udit", ""]]}, {"id": "2104.11044", "submitter": "James Lucas", "authors": "James Lucas, Juhan Bae, Michael R. Zhang, Stanislav Fort, Richard\n  Zemel, Roger Grosse", "title": "Analyzing Monotonic Linear Interpolation in Neural Network Loss\n  Landscapes", "comments": "15 pages in main paper, 4 pages of references, 24 pages in appendix.\n  29 figures in total", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Linear interpolation between initial neural network parameters and converged\nparameters after training with stochastic gradient descent (SGD) typically\nleads to a monotonic decrease in the training objective. This Monotonic Linear\nInterpolation (MLI) property, first observed by Goodfellow et al. (2014)\npersists in spite of the non-convex objectives and highly non-linear training\ndynamics of neural networks. Extending this work, we evaluate several\nhypotheses for this property that, to our knowledge, have not yet been\nexplored. Using tools from differential geometry, we draw connections between\nthe interpolated paths in function space and the monotonicity of the network -\nproviding sufficient conditions for the MLI property under mean squared error.\nWhile the MLI property holds under various settings (e.g. network architectures\nand learning problems), we show in practice that networks violating the MLI\nproperty can be produced systematically, by encouraging the weights to move far\nfrom initialization. The MLI property raises important questions about the loss\nlandscape geometry of neural networks and highlights the need to further study\ntheir global properties.\n", "versions": [{"version": "v1", "created": "Thu, 22 Apr 2021 13:22:12 GMT"}, {"version": "v2", "created": "Fri, 23 Apr 2021 17:24:48 GMT"}], "update_date": "2021-04-26", "authors_parsed": [["Lucas", "James", ""], ["Bae", "Juhan", ""], ["Zhang", "Michael R.", ""], ["Fort", "Stanislav", ""], ["Zemel", "Richard", ""], ["Grosse", "Roger", ""]]}, {"id": "2104.11061", "submitter": "Zineb Belkacemi", "authors": "Zineb Belkacemi, Paraskevi Gkeka, Tony Leli\\`evre and Gabriel Stoltz", "title": "Chasing Collective Variables using Autoencoders and biased trajectories", "comments": "49 pages, 8 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "physics.bio-ph cs.LG physics.comp-ph stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In the last decades, free energy biasing methods have proven to be powerful\ntools to accelerate the simulation of important conformational changes of\nmolecules by modifying the sampling measure. However, most of these methods\nrely on the prior knowledge of low-dimensional slow degrees of freedom, i.e.\nCollective Variables (CV). Alternatively, such CVs can be identified using\nmachine learning (ML) and dimensionality reduction algorithms. In this context,\napproaches where the CVs are learned in an iterative way using adaptive biasing\nhave been proposed: at each iteration, the learned CV is used to perform free\nenergy adaptive biasing to generate new data and learn a new CV. This implies\nthat at each iteration, a different measure is sampled, thus the new training\ndata is distributed according to a different distribution. Given that a machine\nlearning model is always dependent on the considered distribution, iterative\nmethods are not guaranteed to converge to a certain CV. This can be remedied by\na reweighting procedure to always fall back to learning with respect to the\nsame unbiased Boltzmann-Gibbs measure, regardless of the biased measure used in\nthe adaptive sampling. In this paper, we introduce a new iterative method\ninvolving CV learning with autoencoders: Free Energy Biasing and Iterative\nLearning with AutoEncoders (FEBILAE). Our method includes the reweighting\nscheme to ensure that the learning model optimizes the same loss, and achieves\nCV convergence. Using a small 2-dimensional toy system and the alanine\ndipeptide system as examples, we present results of our algorithm using the\nextended adaptive biasing force as the free energy adaptive biasing method.\n", "versions": [{"version": "v1", "created": "Thu, 22 Apr 2021 13:44:21 GMT"}], "update_date": "2021-04-23", "authors_parsed": [["Belkacemi", "Zineb", ""], ["Gkeka", "Paraskevi", ""], ["Leli\u00e8vre", "Tony", ""], ["Stoltz", "Gabriel", ""]]}, {"id": "2104.11092", "submitter": "Jayesh Malaviya", "authors": "Jayesh Malaviya", "title": "Survey on Modeling Intensity Function of Hawkes Process Using Neural\n  Models", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://creativecommons.org/licenses/by-sa/4.0/", "abstract": "  The event sequence of many diverse systems is represented as a sequence of\ndiscrete events in a continuous space. Examples of such an event sequence are\nearthquake aftershock events, financial transactions, e-commerce transactions,\nsocial network activity of a user, and the user's web search pattern. Finding\nsuch an intricate pattern helps discover which event will occur in the future\nand when it will occur. A Hawkes process is a mathematical tool used for\nmodeling such time series discrete events. Traditionally, the Hawkes process\nuses a critical component for modeling data as an intensity function with a\nparameterized kernel function. The Hawkes process's intensity function involves\ntwo components: the background intensity and the effect of events' history.\nHowever, such parameterized assumption can not capture future event\ncharacteristics using past events data precisely due to bias in modeling kernel\nfunction. This paper explores the recent advancement using novel deep\nlearning-based methods to model kernel function to remove such parametrized\nkernel function. In the end, we will give potential future research directions\nto improve modeling using the Hawkes process.\n", "versions": [{"version": "v1", "created": "Thu, 22 Apr 2021 14:23:38 GMT"}], "update_date": "2021-04-23", "authors_parsed": [["Malaviya", "Jayesh", ""]]}, {"id": "2104.11142", "submitter": "Martin Huber", "authors": "Martin Huber, David Imhof", "title": "Deep learning for detecting bid rigging: Flagging cartel participants\n  based on convolutional neural networks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Adding to the literature on the data-driven detection of bid-rigging cartels,\nwe propose a novel approach based on deep learning (a subfield of artificial\nintelligence) that flags cartel participants based on their pairwise bidding\ninteractions with other firms. More concisely, we combine a so-called\nconvolutional neural network for image recognition with graphs that in a\npairwise manner plot the normalized bid values of some reference firm against\nthe normalized bids of any other firms participating in the same tenders as the\nreference firm. Based on Japanese and Swiss procurement data, we construct such\ngraphs for both collusive and competitive episodes (i.e when a bid-rigging\ncartel is or is not active) and use a subset of graphs to train the neural\nnetwork such that it learns distinguishing collusive from competitive bidding\npatterns. We use the remaining graphs to test the neural network's\nout-of-sample performance in correctly classifying collusive and competitive\nbidding interactions. We obtain a very decent average accuracy of around 90% or\nslightly higher when either applying the method within Japanese, Swiss, or\nmixed data (in which Swiss and Japanese graphs are pooled). When using data\nfrom one country for training to test the trained model's performance in the\nother country (i.e. transnationally), predictive performance decreases (likely\ndue to institutional differences in procurement procedures across countries),\nbut often remains satisfactorily high. All in all, the generally quite high\naccuracy of the convolutional neural network despite being trained in a rather\nsmall sample of a few 100 graphs points to a large potential of deep learning\napproaches for flagging and fighting bid-rigging cartels.\n", "versions": [{"version": "v1", "created": "Thu, 22 Apr 2021 15:48:12 GMT"}], "update_date": "2021-04-23", "authors_parsed": [["Huber", "Martin", ""], ["Imhof", "David", ""]]}, {"id": "2104.11191", "submitter": "Frederick Matsen IV", "authors": "Michael Karcher, Cheng Zhang, and Frederick A Matsen IV", "title": "Variational Bayesian Supertrees", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "q-bio.PE stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Given overlapping subsets of a set of taxa (e.g. species), and posterior\ndistributions on phylogenetic tree topologies for each of these taxon sets, how\ncan we infer a posterior distribution on phylogenetic tree topologies for the\nentire taxon set? Although the equivalent problem for in the non-Bayesian case\nhas attracted substantial research, the Bayesian case has not attracted the\nattention it deserves. In this paper we develop a variational Bayes approach to\nthis problem and demonstrate its effectiveness.\n", "versions": [{"version": "v1", "created": "Thu, 22 Apr 2021 17:24:00 GMT"}], "update_date": "2021-04-23", "authors_parsed": [["Karcher", "Michael", ""], ["Zhang", "Cheng", ""], ["Matsen", "Frederick A", "IV"]]}, {"id": "2104.11212", "submitter": "Adam \\'Scibior", "authors": "Adam Scibior, Vasileios Lioutas, Daniele Reda, Peyman Bateni, Frank\n  Wood", "title": "Imagining The Road Ahead: Multi-Agent Trajectory Prediction via\n  Differentiable Simulation", "comments": "10 pages, 8 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://creativecommons.org/licenses/by-sa/4.0/", "abstract": "  We develop a deep generative model built on a fully differentiable simulator\nfor multi-agent trajectory prediction. Agents are modeled with conditional\nrecurrent variational neural networks (CVRNNs), which take as input an\nego-centric birdview image representing the current state of the world and\noutput an action, consisting of steering and acceleration, which is used to\nderive the subsequent agent state using a kinematic bicycle model. The full\nsimulation state is then differentiably rendered for each agent, initiating the\nnext time step. We achieve state-of-the-art results on the INTERACTION dataset,\nusing standard neural architectures and a standard variational training\nobjective, producing realistic multi-modal predictions without any ad-hoc\ndiversity-inducing losses. We conduct ablation studies to examine individual\ncomponents of the simulator, finding that both the kinematic bicycle model and\nthe continuous feedback from the birdview image are crucial for achieving this\nlevel of performance. We name our model ITRA, for \"Imagining the Road Ahead\".\n", "versions": [{"version": "v1", "created": "Thu, 22 Apr 2021 17:48:08 GMT"}], "update_date": "2021-04-23", "authors_parsed": [["Scibior", "Adam", ""], ["Lioutas", "Vasileios", ""], ["Reda", "Daniele", ""], ["Bateni", "Peyman", ""], ["Wood", "Frank", ""]]}, {"id": "2104.11216", "submitter": "Sumith Kulal", "authors": "Sumith Kulal, Jiayuan Mao, Alex Aiken, Jiajun Wu", "title": "Hierarchical Motion Understanding via Motion Programs", "comments": "CVPR 2021. First two authors contributed equally. Project page:\n  https://sumith1896.github.io/motion2prog/", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Current approaches to video analysis of human motion focus on raw pixels or\nkeypoints as the basic units of reasoning. We posit that adding higher-level\nmotion primitives, which can capture natural coarser units of motion such as\nbackswing or follow-through, can be used to improve downstream analysis tasks.\nThis higher level of abstraction can also capture key features, such as loops\nof repeated primitives, that are currently inaccessible at lower levels of\nrepresentation. We therefore introduce Motion Programs, a neuro-symbolic,\nprogram-like representation that expresses motions as a composition of\nhigh-level primitives. We also present a system for automatically inducing\nmotion programs from videos of human motion and for leveraging motion programs\nin video synthesis. Experiments show that motion programs can accurately\ndescribe a diverse set of human motions and the inferred programs contain\nsemantically meaningful motion primitives, such as arm swings and jumping\njacks. Our representation also benefits downstream tasks such as video\ninterpolation and video prediction and outperforms off-the-shelf models. We\nfurther demonstrate how these programs can detect diverse kinds of repetitive\nmotion and facilitate interactive video editing.\n", "versions": [{"version": "v1", "created": "Thu, 22 Apr 2021 17:49:59 GMT"}], "update_date": "2021-04-23", "authors_parsed": [["Kulal", "Sumith", ""], ["Mao", "Jiayuan", ""], ["Aiken", "Alex", ""], ["Wu", "Jiajun", ""]]}, {"id": "2104.11283", "submitter": "Yu Yang", "authors": "Hongcheng Liu and Yu Yang", "title": "A Dimension-Insensitive Algorithm for Stochastic Zeroth-Order\n  Optimization", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.OC stat.ME stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper concerns a convex, stochastic zeroth-order optimization (S-ZOO)\nproblem, where the objective is to minimize the expectation of a cost function\nand its gradient is not accessible directly. To solve this problem, traditional\noptimization techniques mostly yield query complexities that grow polynomially\nwith dimensionality, i.e., the number of function evaluations is a polynomial\nfunction of the number of decision variables. Consequently, these methods may\nnot perform well in solving massive-dimensional problems arising in many modern\napplications. Although more recent methods can be provably\ndimension-insensitive, almost all of them work with arguably more stringent\nconditions such as everywhere sparse or compressible gradient. Thus, prior to\nthis research, it was unknown whether dimension-insensitive S-ZOO is possible\nwithout such conditions. In this paper, we give an affirmative answer to this\nquestion by proposing a sparsity-inducing stochastic gradient-free (SI-SGF)\nalgorithm. It is proved to achieve dimension-insensitive query complexity in\nboth convex and strongly convex cases when neither gradient sparsity nor\ngradient compressibility is satisfied. Our numerical results demonstrate the\nstrong potential of the proposed SI-SGF compared with existing alternatives.\n", "versions": [{"version": "v1", "created": "Thu, 22 Apr 2021 18:56:17 GMT"}, {"version": "v2", "created": "Tue, 6 Jul 2021 01:45:14 GMT"}], "update_date": "2021-07-07", "authors_parsed": [["Liu", "Hongcheng", ""], ["Yang", "Yu", ""]]}, {"id": "2104.11315", "submitter": "Sewoong Oh", "authors": "Jonathan Hayase, Weihao Kong, Raghav Somani, Sewoong Oh", "title": "SPECTRE: Defending Against Backdoor Attacks Using Robust Statistics", "comments": "29 pages 19 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI stat.ML", "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/", "abstract": "  Modern machine learning increasingly requires training on a large collection\nof data from multiple sources, not all of which can be trusted. A particularly\nconcerning scenario is when a small fraction of poisoned data changes the\nbehavior of the trained model when triggered by an attacker-specified\nwatermark. Such a compromised model will be deployed unnoticed as the model is\naccurate otherwise. There have been promising attempts to use the intermediate\nrepresentations of such a model to separate corrupted examples from clean ones.\nHowever, these defenses work only when a certain spectral signature of the\npoisoned examples is large enough for detection. There is a wide range of\nattacks that cannot be protected against by the existing defenses. We propose a\nnovel defense algorithm using robust covariance estimation to amplify the\nspectral signature of corrupted data. This defense provides a clean model,\ncompletely removing the backdoor, even in regimes where previous methods have\nno hope of detecting the poisoned examples. Code and pre-trained models are\navailable at https://github.com/SewoongLab/spectre-defense .\n", "versions": [{"version": "v1", "created": "Thu, 22 Apr 2021 20:49:40 GMT"}], "update_date": "2021-04-26", "authors_parsed": [["Hayase", "Jonathan", ""], ["Kong", "Weihao", ""], ["Somani", "Raghav", ""], ["Oh", "Sewoong", ""]]}, {"id": "2104.11375", "submitter": "Tao Sun", "authors": "Tao Sun, Dongsheng Li, Bao Wang", "title": "Decentralized Federated Averaging", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Federated averaging (FedAvg) is a communication efficient algorithm for the\ndistributed training with an enormous number of clients. In FedAvg, clients\nkeep their data locally for privacy protection; a central parameter server is\nused to communicate between clients. This central server distributes the\nparameters to each client and collects the updated parameters from clients.\nFedAvg is mostly studied in centralized fashions, which requires massive\ncommunication between server and clients in each communication. Moreover,\nattacking the central server can break the whole system's privacy. In this\npaper, we study the decentralized FedAvg with momentum (DFedAvgM), which is\nimplemented on clients that are connected by an undirected graph. In DFedAvgM,\nall clients perform stochastic gradient descent with momentum and communicate\nwith their neighbors only. To further reduce the communication cost, we also\nconsider the quantized DFedAvgM. We prove convergence of the (quantized)\nDFedAvgM under trivial assumptions; the convergence rate can be improved when\nthe loss function satisfies the P{\\L} property. Finally, we numerically verify\nthe efficacy of DFedAvgM.\n", "versions": [{"version": "v1", "created": "Fri, 23 Apr 2021 02:01:30 GMT"}], "update_date": "2021-04-26", "authors_parsed": [["Sun", "Tao", ""], ["Li", "Dongsheng", ""], ["Wang", "Bao", ""]]}, {"id": "2104.11496", "submitter": "Lukas Trottner", "authors": "S\\\"oren Christensen, Claudia Strauch and Lukas Trottner", "title": "Learning to reflect: A unifying approach for data-driven stochastic\n  control strategies", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST math.OC math.PR stat.ML stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Stochastic optimal control problems have a long tradition in applied\nprobability, with the questions addressed being of high relevance in a\nmultitude of fields. Even though theoretical solutions are well understood in\nmany scenarios, their practicability suffers from the assumption of known\ndynamics of the underlying stochastic process, raising the statistical\nchallenge of developing purely data-driven strategies. For the mathematically\nseparated classes of continuous diffusion processes and L\\'evy processes, we\nshow that developing efficient strategies for related singular stochastic\ncontrol problems can essentially be reduced to finding rate-optimal estimators\nwith respect to the sup-norm risk of objects associated to the invariant\ndistribution of ergodic processes which determine the theoretical solution of\nthe control problem. From a statistical perspective, we exploit the exponential\n$\\beta$-mixing property as the common factor of both scenarios to drive the\nconvergence analysis, indicating that relying on general stability properties\nof Markov processes is a sufficiently powerful and flexible approach to treat\ncomplex applications requiring statistical methods. We show moreover that in\nthe L\\'evy case $-$ even though per se jump processes are more difficult to\nhandle both in statistics and control theory $-$ a fully data-driven strategy\nwith regret of significantly better order than in the diffusion case can be\nconstructed.\n", "versions": [{"version": "v1", "created": "Fri, 23 Apr 2021 09:33:15 GMT"}], "update_date": "2021-04-26", "authors_parsed": [["Christensen", "S\u00f6ren", ""], ["Strauch", "Claudia", ""], ["Trottner", "Lukas", ""]]}, {"id": "2104.11547", "submitter": "Patrick Forr\\'e", "authors": "Patrick Forr\\'e", "title": "Transitional Conditional Independence", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST math.PR stat.ML stat.OT stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We develope the framework of transitional conditional independence. For this\nwe introduce transition probability spaces and transitional random variables.\nThese constructions will generalize, strengthen and unify previous notions of\n(conditional) random variables and non-stochastic variables, (extended)\nstochastic conditional independence and some form of functional conditional\nindependence. Transitional conditional independence is asymmetric in general\nand it will be shown that it satisfies all desired relevance relations in terms\nof left and right versions of the separoid rules, except symmetry, on standard,\nanalytic and universal measurable spaces. As a preparation we prove a\ndisintegration theorem for transition probabilities, i.e. the existence and\nessential uniqueness of (regular) conditional Markov kernels, on those spaces.\nTransitional conditional independence will be able to express classical\nstatistical concepts like sufficiency, adequacy and ancillarity. As an\napplication, we will then show how transitional conditional independence can be\nused to prove a directed global Markov property for causal graphical models\nthat allow for non-stochastic input variables in strong generality. This will\nthen also allow us to show the main rules of causal do-calculus, relating\nobservational and interventional distributions, in such measure theoretic\ngenerality.\n", "versions": [{"version": "v1", "created": "Fri, 23 Apr 2021 11:52:15 GMT"}], "update_date": "2021-04-26", "authors_parsed": [["Forr\u00e9", "Patrick", ""]]}, {"id": "2104.11688", "submitter": "Quay Au", "authors": "Quay Au, Julia Herbinger, Clemens Stachl, Bernd Bischl, Giuseppe\n  Casalicchio", "title": "Grouped Feature Importance and Combined Features Effect Plot", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Interpretable machine learning has become a very active area of research due\nto the rising popularity of machine learning algorithms and their inherently\nchallenging interpretability. Most work in this area has been focused on the\ninterpretation of single features in a model. However, for researchers and\npractitioners, it is often equally important to quantify the importance or\nvisualize the effect of feature groups. To address this research gap, we\nprovide a comprehensive overview of how existing model-agnostic techniques can\nbe defined for feature groups to assess the grouped feature importance,\nfocusing on permutation-based, refitting, and Shapley-based methods. We also\nintroduce an importance-based sequential procedure that identifies a stable and\nwell-performing combination of features in the grouped feature space.\nFurthermore, we introduce the combined features effect plot, which is a\ntechnique to visualize the effect of a group of features based on a sparse,\ninterpretable linear combination of features. We used simulation studies and a\nreal data example from computational psychology to analyze, compare, and\ndiscuss these methods.\n", "versions": [{"version": "v1", "created": "Fri, 23 Apr 2021 16:27:38 GMT"}], "update_date": "2021-04-26", "authors_parsed": [["Au", "Quay", ""], ["Herbinger", "Julia", ""], ["Stachl", "Clemens", ""], ["Bischl", "Bernd", ""], ["Casalicchio", "Giuseppe", ""]]}, {"id": "2104.11702", "submitter": "Ryan Dew", "authors": "Ryan Dew, Yuhao Fan", "title": "A Gaussian Process Model of Cross-Category Dynamics in Brand Choice", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP econ.EM stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Understanding individual customers' sensitivities to prices, promotions,\nbrand, and other aspects of the marketing mix is fundamental to a wide swath of\nmarketing problems, including targeting and pricing. Companies that operate\nacross many product categories have a unique opportunity, insofar as they can\nuse purchasing data from one category to augment their insights in another.\nSuch cross-category insights are especially crucial in situations where\npurchasing data may be rich in one category, and scarce in another. An\nimportant aspect of how consumers behave across categories is dynamics:\npreferences are not stable over time, and changes in individual-level\npreference parameters in one category may be indicative of changes in other\ncategories, especially if those changes are driven by external factors. Yet,\ndespite the rich history of modeling cross-category preferences, the marketing\nliterature lacks a framework that flexibly accounts for \\textit{correlated\ndynamics}, or the cross-category interlinkages of individual-level sensitivity\ndynamics. In this work, we propose such a framework, leveraging\nindividual-level, latent, multi-output Gaussian processes to build a\nnonparametric Bayesian choice model that allows information sharing of\npreference parameters across customers, time, and categories. We apply our\nmodel to grocery purchase data, and show that our model detects interesting\ndynamics of customers' price sensitivities across multiple categories.\nManagerially, we show that capturing correlated dynamics yields substantial\npredictive gains, relative to benchmarks. Moreover, we find that capturing\ncorrelated dynamics can have implications for understanding changes in\nconsumers preferences over time, and developing targeted marketing strategies\nbased on those dynamics.\n", "versions": [{"version": "v1", "created": "Fri, 23 Apr 2021 16:43:01 GMT"}], "update_date": "2021-04-26", "authors_parsed": [["Dew", "Ryan", ""], ["Fan", "Yuhao", ""]]}, {"id": "2104.11734", "submitter": "Jacob Zavatone-Veth", "authors": "Jacob A. Zavatone-Veth and Cengiz Pehlevan", "title": "Exact priors of finite neural networks", "comments": "12+11 pages, 4 figures; v2: references and figures added", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cond-mat.dis-nn stat.ML", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Bayesian neural networks are theoretically well-understood only in the\ninfinite-width limit, where Gaussian priors over network weights yield Gaussian\npriors over network outputs. Recent work has suggested that finite Bayesian\nnetworks may outperform their infinite counterparts, but their non-Gaussian\noutput priors have been characterized only though perturbative approaches.\nHere, we derive exact solutions for the output priors for individual input\nexamples of a class of finite fully-connected feedforward Bayesian neural\nnetworks. For deep linear networks, the prior has a simple expression in terms\nof the Meijer $G$-function. The prior of a finite ReLU network is a mixture of\nthe priors of linear networks of smaller widths, corresponding to different\nnumbers of active units in each layer. Our results unify previous descriptions\nof finite network priors in terms of their tail decay and large-width behavior.\n", "versions": [{"version": "v1", "created": "Fri, 23 Apr 2021 17:31:42 GMT"}, {"version": "v2", "created": "Tue, 18 May 2021 17:42:44 GMT"}], "update_date": "2021-05-19", "authors_parsed": [["Zavatone-Veth", "Jacob A.", ""], ["Pehlevan", "Cengiz", ""]]}, {"id": "2104.11824", "submitter": "Dheeraj Baby", "authors": "Dheeraj Baby and Yu-Xiang Wang", "title": "Optimal Dynamic Regret in Exp-Concave Online Learning", "comments": "Added a post processing step to Lemma 5; Added Remark 6", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG math.OC stat.ML", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  We consider the problem of the Zinkevich (2003)-style dynamic regret\nminimization in online learning with exp-concave losses. We show that whenever\nimproper learning is allowed, a Strongly Adaptive online learner achieves the\ndynamic regret of $\\tilde O^*(n^{1/3}C_n^{2/3} \\vee 1)$ where $C_n$ is the\ntotal variation (a.k.a. path length) of the an arbitrary sequence of\ncomparators that may not be known to the learner ahead of time. Achieving this\nrate was highly nontrivial even for squared losses in 1D where the best known\nupper bound was $O(\\sqrt{nC_n} \\vee \\log n)$ (Yuan and Lamperski, 2019). Our\nnew proof techniques make elegant use of the intricate structures of the primal\nand dual variables imposed by the KKT conditions and could be of independent\ninterest. Finally, we apply our results to the classical statistical problem of\nlocally adaptive non-parametric regression (Mammen, 1991; Donoho and Johnstone,\n1998) and obtain a stronger and more flexible algorithm that do not require any\nstatistical assumptions or any hyperparameter tuning.\n", "versions": [{"version": "v1", "created": "Fri, 23 Apr 2021 21:36:51 GMT"}, {"version": "v2", "created": "Sat, 3 Jul 2021 04:53:58 GMT"}], "update_date": "2021-07-06", "authors_parsed": [["Baby", "Dheeraj", ""], ["Wang", "Yu-Xiang", ""]]}, {"id": "2104.11833", "submitter": "Eric Bax", "authors": "Eric Bax", "title": "Selecting a number of voters for a voting ensemble", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  For a voting ensemble that selects an odd-sized subset of the ensemble\nclassifiers at random for each example, applies them to the example, and\nreturns the majority vote, we show that any number of voters may minimize the\nerror rate over an out-of-sample distribution. The optimal number of voters\ndepends on the out-of-sample distribution of the number of classifiers in\nerror. To select a number of voters to use, estimating that distribution then\ninferring error rates for numbers of voters gives lower-variance estimates than\ndirectly estimating those error rates.\n", "versions": [{"version": "v1", "created": "Fri, 23 Apr 2021 22:37:02 GMT"}], "update_date": "2021-04-27", "authors_parsed": [["Bax", "Eric", ""]]}, {"id": "2104.11834", "submitter": "Hannes Eriksson", "authors": "Hannes Eriksson, Christos Dimitrakakis, Lars Carlsson", "title": "High-dimensional near-optimal experiment design for drug discovery via\n  Bayesian sparse sampling", "comments": "14 pages, 6 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG q-bio.QM stat.ML", "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/", "abstract": "  We study the problem of performing automated experiment design for drug\nscreening through Bayesian inference and optimisation. In particular, we\ncompare and contrast the behaviour of linear-Gaussian models and Gaussian\nprocesses, when used in conjunction with upper confidence bound algorithms,\nThompson sampling, or bounded horizon tree search. We show that non-myopic\nsophisticated exploration techniques using sparse tree search have a distinct\nadvantage over methods such as Thompson sampling or upper confidence bounds in\nthis setting. We demonstrate the significant superiority of the approach over\nexisting and synthetic datasets of drug toxicity.\n", "versions": [{"version": "v1", "created": "Fri, 23 Apr 2021 22:43:16 GMT"}], "update_date": "2021-05-11", "authors_parsed": [["Eriksson", "Hannes", ""], ["Dimitrakakis", "Christos", ""], ["Carlsson", "Lars", ""]]}, {"id": "2104.11895", "submitter": "Shiyu Liang", "authors": "Shiyu Liang, Ruoyu Sun and R. Srikant", "title": "Achieving Small Test Error in Mildly Overparameterized Neural Networks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG math.OC stat.ML", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Recent theoretical works on over-parameterized neural nets have focused on\ntwo aspects: optimization and generalization. Many existing works that study\noptimization and generalization together are based on neural tangent kernel and\nrequire a very large width. In this work, we are interested in the following\nquestion: for a binary classification problem with two-layer mildly\nover-parameterized ReLU network, can we find a point with small test error in\npolynomial time? We first show that the landscape of loss functions with\nexplicit regularization has the following property: all local minima and\ncertain other points which are only stationary in certain directions achieve\nsmall test error. We then prove that for convolutional neural nets, there is an\nalgorithm which finds one of these points in polynomial time (in the input\ndimension and the number of data points). In addition, we prove that for a\nfully connected neural net, with an additional assumption on the data\ndistribution, there is a polynomial time algorithm.\n", "versions": [{"version": "v1", "created": "Sat, 24 Apr 2021 06:47:20 GMT"}], "update_date": "2021-04-27", "authors_parsed": [["Liang", "Shiyu", ""], ["Sun", "Ruoyu", ""], ["Srikant", "R.", ""]]}, {"id": "2104.12031", "submitter": "Yuetian Luo", "authors": "Yuetian Luo, Anru R. Zhang", "title": "Low-rank Tensor Estimation via Riemannian Gauss-Newton: Statistical\n  Optimality and Second-Order Convergence", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG cs.NA math.NA math.OC stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we consider the estimation of a low Tucker rank tensor from a\nnumber of noisy linear measurements. The general problem covers many specific\nexamples arising from applications, including tensor regression, tensor\ncompletion, and tensor PCA/SVD. We propose a Riemannian Gauss-Newton (RGN)\nmethod with fast implementations for low Tucker rank tensor estimation.\nDifferent from the generic (super)linear convergence guarantee of RGN in the\nliterature, we prove the first quadratic convergence guarantee of RGN for\nlow-rank tensor estimation under some mild conditions. A deterministic\nestimation error lower bound, which matches the upper bound, is provided that\ndemonstrates the statistical optimality of RGN. The merit of RGN is illustrated\nthrough two machine learning applications: tensor regression and tensor SVD.\nFinally, we provide the simulation results to corroborate our theoretical\nfindings.\n", "versions": [{"version": "v1", "created": "Sat, 24 Apr 2021 22:24:14 GMT"}, {"version": "v2", "created": "Tue, 27 Apr 2021 04:01:36 GMT"}], "update_date": "2021-04-28", "authors_parsed": [["Luo", "Yuetian", ""], ["Zhang", "Anru R.", ""]]}, {"id": "2104.12036", "submitter": "Ruimeng Hu", "authors": "Jiequn Han, Ruimeng Hu, Jihao Long", "title": "A Class of Dimensionality-free Metrics for the Convergence of Empirical\n  Measures", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.PR cs.LG stat.ML", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  This paper concerns the convergence of empirical measures in high dimensions.\nWe propose a new class of metrics and show that under such metrics, the\nconvergence is free of the curse of dimensionality (CoD). Such a feature is\ncritical for high-dimensional analysis and stands in contrast to classical\nmetrics ({\\it e.g.}, the Wasserstein distance). The proposed metrics originate\nfrom the maximum mean discrepancy, which we generalize by proposing specific\ncriteria for selecting test function spaces to guarantee the property of being\nfree of CoD. Therefore, we call this class of metrics the generalized maximum\nmean discrepancy (GMMD). Examples of the selected test function spaces include\nthe reproducing kernel Hilbert space, Barron space, and flow-induced function\nspaces. Three applications of the proposed metrics are presented: 1. The\nconvergence of empirical measure in the case of random variables; 2. The\nconvergence of $n$-particle system to the solution to McKean-Vlasov stochastic\ndifferential equation; 3. The construction of an $\\varepsilon$-Nash equilibrium\nfor a homogeneous $n$-player game by its mean-field limit. As a byproduct, we\nprove that, given a distribution close to the target distribution measured by\nGMMD and a certain representation of the target distribution, we can generate a\ndistribution close to the target one in terms of the Wasserstein distance and\nrelative entropy. Overall, we show that the proposed class of metrics is a\npowerful tool to analyze the convergence of empirical measures in high\ndimensions without CoD.\n", "versions": [{"version": "v1", "created": "Sat, 24 Apr 2021 23:27:40 GMT"}, {"version": "v2", "created": "Tue, 27 Apr 2021 16:42:46 GMT"}], "update_date": "2021-04-28", "authors_parsed": [["Han", "Jiequn", ""], ["Hu", "Ruimeng", ""], ["Long", "Jihao", ""]]}, {"id": "2104.12053", "submitter": "Adji Bousso Dieng", "authors": "Adji B. Dieng", "title": "Deep Probabilistic Graphical Modeling", "comments": "This thesis was defended in April 2020 and accepted without revision.\n  The author received her PhD in Statistics from Columbia University on May 20,\n  2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://creativecommons.org/licenses/by-sa/4.0/", "abstract": "  Probabilistic graphical modeling (PGM) provides a framework for formulating\nan interpretable generative process of data and expressing uncertainty about\nunknowns, but it lacks flexibility. Deep learning (DL) is an alternative\nframework for learning from data that has achieved great empirical success in\nrecent years. DL offers great flexibility, but it lacks the interpretability\nand calibration of PGM. This thesis develops deep probabilistic graphical\nmodeling (DPGM.) DPGM consists in leveraging DL to make PGM more flexible. DPGM\nbrings about new methods for learning from data that exhibit the advantages of\nboth PGM and DL.\n  We use DL within PGM to build flexible models endowed with an interpretable\nlatent structure. One model class we develop extends exponential family PCA\nusing neural networks to improve predictive performance while enforcing the\ninterpretability of the latent factors. Another model class we introduce\nenables accounting for long-term dependencies when modeling sequential data,\nwhich is a challenge when using purely DL or PGM approaches. Finally, DPGM\nsuccessfully solves several outstanding problems of probabilistic topic models,\na widely used family of models in PGM.\n  DPGM also brings about new algorithms for learning with complex data. We\ndevelop reweighted expectation maximization, an algorithm that unifies several\nexisting maximum likelihood-based algorithms for learning models parameterized\nby neural networks. This unifying view is made possible using expectation\nmaximization, a canonical inference algorithm in PGM. We also develop\nentropy-regularized adversarial learning, a learning paradigm that deviates\nfrom the traditional maximum likelihood approach used in PGM. From the DL\nperspective, entropy-regularized adversarial learning provides a solution to\nthe long-standing mode collapse problem of generative adversarial networks, a\nwidely used DL approach.\n", "versions": [{"version": "v1", "created": "Sun, 25 Apr 2021 03:48:02 GMT"}], "update_date": "2021-04-27", "authors_parsed": [["Dieng", "Adji B.", ""]]}, {"id": "2104.12055", "submitter": "Md Easin Hasan", "authors": "Fahad B. Mostafa and Md Easin Hasan", "title": "Machine Learning Approaches for Binary Classification to Discover Liver\n  Diseases using Clinical Data", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG stat.AP", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  For a medical diagnosis, health professionals use different kinds of\npathological ways to make a decision for medical reports in terms of patients\nmedical condition. In the modern era, because of the advantage of computers and\ntechnologies, one can collect data and visualize many hidden outcomes from\nthem. Statistical machine learning algorithms based on specific problems can\nassist one to make decisions. Machine learning data driven algorithms can be\nused to validate existing methods and help researchers to suggest potential new\ndecisions. In this paper, multiple imputation by chained equations was applied\nto deal with missing data, and Principal Component Analysis to reduce the\ndimensionality. To reveal significant findings, data visualizations were\nimplemented. We presented and compared many binary classifier machine learning\nalgorithms (Artificial Neural Network, Random Forest, Support Vector Machine)\nwhich were used to classify blood donors and non-blood donors with hepatitis,\nfibrosis and cirrhosis diseases. From the data published in UCI-MLR [1], all\nmentioned techniques were applied to find one better method to classify blood\ndonors and non-blood donors (hepatitis, fibrosis, and cirrhosis) that can help\nhealth professionals in a laboratory to make better decisions. Our proposed\nML-method showed better accuracy score (e.g. 98.23% for SVM). Thus, it improved\nthe quality of classification.\n", "versions": [{"version": "v1", "created": "Sun, 25 Apr 2021 04:10:19 GMT"}, {"version": "v2", "created": "Sat, 5 Jun 2021 19:34:28 GMT"}], "update_date": "2021-06-08", "authors_parsed": [["Mostafa", "Fahad B.", ""], ["Hasan", "Md Easin", ""]]}, {"id": "2104.12119", "submitter": "Christos Merkatas", "authors": "Christos Merkatas and Simo S\\\"arkk\\\"a", "title": "System identification using Bayesian neural networks with nonparametric\n  noise models", "comments": "Submitted to Statistics and Computing; figure and typos corrections", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  System identification is of special interest in science and engineering. This\narticle is concerned with a system identification problem arising in stochastic\ndynamic systems, where the aim is to estimating the parameters of a system\nalong with its unknown noise processes. In particular, we propose a Bayesian\nnonparametric approach for system identification in discrete time nonlinear\nrandom dynamical systems assuming only the order of the Markov process is\nknown. The proposed method replaces the assumption of Gaussian distributed\nerror components with a highly flexible family of probability density functions\nbased on Bayesian nonparametric priors. Additionally, the functional form of\nthe system is estimated by leveraging Bayesian neural networks which also leads\nto flexible uncertainty quantification. Asymptotically on the number of hidden\nneurons, the proposed model converges to full nonparametric Bayesian regression\nmodel. A Gibbs sampler for posterior inference is proposed and its\neffectiveness is illustrated in simulated and real time series.\n", "versions": [{"version": "v1", "created": "Sun, 25 Apr 2021 09:49:50 GMT"}, {"version": "v2", "created": "Fri, 14 May 2021 17:24:40 GMT"}], "update_date": "2021-05-17", "authors_parsed": [["Merkatas", "Christos", ""], ["S\u00e4rkk\u00e4", "Simo", ""]]}, {"id": "2104.12199", "submitter": "Joshua N. Cooper", "authors": "Rory Mitchell, Joshua Cooper, Eibe Frank, Geoffrey Holmes", "title": "Sampling Permutations for Shapley Value Estimation", "comments": "33 pages, 13 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG math.CO", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Game-theoretic attribution techniques based on Shapley values are used\nextensively to interpret black-box machine learning models, but their exact\ncalculation is generally NP-hard, requiring approximation methods for\nnon-trivial models. As the computation of Shapley values can be expressed as a\nsummation over a set of permutations, a common approach is to sample a subset\nof these permutations for approximation. Unfortunately, standard Monte Carlo\nsampling methods can exhibit slow convergence, and more sophisticated quasi\nMonte Carlo methods are not well defined on the space of permutations. To\naddress this, we investigate new approaches based on two classes of\napproximation methods and compare them empirically. First, we demonstrate\nquadrature techniques in a RKHS containing functions of permutations, using the\nMallows kernel to obtain explicit convergence rates of $O(1/n)$, improving on\n$O(1/\\sqrt{n})$ for plain Monte Carlo. The RKHS perspective also leads to quasi\nMonte Carlo type error bounds, with a tractable discrepancy measure defined on\npermutations. Second, we exploit connections between the hypersphere\n$\\mathbb{S}^{d-2}$ and permutations to create practical algorithms for\ngenerating permutation samples with good properties. Experiments show the above\ntechniques provide significant improvements for Shapley value estimates over\nexisting methods, converging to a smaller RMSE in the same number of model\nevaluations.\n", "versions": [{"version": "v1", "created": "Sun, 25 Apr 2021 16:44:18 GMT"}], "update_date": "2021-04-27", "authors_parsed": [["Mitchell", "Rory", ""], ["Cooper", "Joshua", ""], ["Frank", "Eibe", ""], ["Holmes", "Geoffrey", ""]]}, {"id": "2104.12219", "submitter": "Andrew Miller", "authors": "Andrew C. Miller, Nicholas J. Foti, Emily B. Fox", "title": "Breiman's two cultures: You don't have to choose sides", "comments": "Commentary to appear in a special issue of Observational Studies,\n  discussing Leo Breiman's paper \"Statistical Modeling: The Two Cultures\"\n  (https://doi.org/10.1214/ss/1009213726)", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Breiman's classic paper casts data analysis as a choice between two cultures:\ndata modelers and algorithmic modelers. Stated broadly, data modelers use\nsimple, interpretable models with well-understood theoretical properties to\nanalyze data. Algorithmic modelers prioritize predictive accuracy and use more\nflexible function approximations to analyze data. This dichotomy overlooks a\nthird set of models $-$ mechanistic models derived from scientific theories\n(e.g., ODE/SDE simulators). Mechanistic models encode application-specific\nscientific knowledge about the data. And while these categories represent\nextreme points in model space, modern computational and algorithmic tools\nenable us to interpolate between these points, producing flexible,\ninterpretable, and scientifically-informed hybrids that can enjoy accurate and\nrobust predictions, and resolve issues with data analysis that Breiman\ndescribes, such as the Rashomon effect and Occam's dilemma. Challenges still\nremain in finding an appropriate point in model space, with many choices on how\nto compose model components and the degree to which each component informs\ninferences.\n", "versions": [{"version": "v1", "created": "Sun, 25 Apr 2021 17:58:46 GMT"}], "update_date": "2021-04-27", "authors_parsed": [["Miller", "Andrew C.", ""], ["Foti", "Nicholas J.", ""], ["Fox", "Emily B.", ""]]}, {"id": "2104.12225", "submitter": "Priya Donti", "authors": "Priya L. Donti, David Rolnick, J. Zico Kolter", "title": "DC3: A learning method for optimization with hard constraints", "comments": "In ICLR 2021. Code available at https://github.com/locuslab/DC3", "journal-ref": "International Conference on Learning Representations 2021", "doi": null, "report-no": null, "categories": "cs.LG math.OC stat.ML", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Large optimization problems with hard constraints arise in many settings, yet\nclassical solvers are often prohibitively slow, motivating the use of deep\nnetworks as cheap \"approximate solvers.\" Unfortunately, naive deep learning\napproaches typically cannot enforce the hard constraints of such problems,\nleading to infeasible solutions. In this work, we present Deep Constraint\nCompletion and Correction (DC3), an algorithm to address this challenge.\nSpecifically, this method enforces feasibility via a differentiable procedure,\nwhich implicitly completes partial solutions to satisfy equality constraints\nand unrolls gradient-based corrections to satisfy inequality constraints. We\ndemonstrate the effectiveness of DC3 in both synthetic optimization tasks and\nthe real-world setting of AC optimal power flow, where hard constraints encode\nthe physics of the electrical grid. In both cases, DC3 achieves near-optimal\nobjective values while preserving feasibility.\n", "versions": [{"version": "v1", "created": "Sun, 25 Apr 2021 18:21:59 GMT"}], "update_date": "2021-04-27", "authors_parsed": [["Donti", "Priya L.", ""], ["Rolnick", "David", ""], ["Kolter", "J. Zico", ""]]}, {"id": "2104.12231", "submitter": "Andrew Miller", "authors": "Andrew C. Miller, Leon A. Gatys, Joseph Futoma, Emily B. Fox", "title": "Model-based metrics: Sample-efficient estimates of predictive model\n  subpopulation performance", "comments": "27 pages, 8 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG stat.AP stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Machine learning models $-$ now commonly developed to screen, diagnose, or\npredict health conditions $-$ are evaluated with a variety of performance\nmetrics. An important first step in assessing the practical utility of a model\nis to evaluate its average performance over an entire population of interest.\nIn many settings, it is also critical that the model makes good predictions\nwithin predefined subpopulations. For instance, showing that a model is fair or\nequitable requires evaluating the model's performance in different demographic\nsubgroups. However, subpopulation performance metrics are typically computed\nusing only data from that subgroup, resulting in higher variance estimates for\nsmaller groups. We devise a procedure to measure subpopulation performance that\ncan be more sample-efficient than the typical subsample estimates. We propose\nusing an evaluation model $-$ a model that describes the conditional\ndistribution of the predictive model score $-$ to form model-based metric (MBM)\nestimates. Our procedure incorporates model checking and validation, and we\npropose a computationally efficient approximation of the traditional\nnonparametric bootstrap to form confidence intervals. We evaluate MBMs on two\nmain tasks: a semi-synthetic setting where ground truth metrics are available\nand a real-world hospital readmission prediction task. We find that MBMs\nconsistently produce more accurate and lower variance estimates of model\nperformance for small subpopulations.\n", "versions": [{"version": "v1", "created": "Sun, 25 Apr 2021 19:06:34 GMT"}], "update_date": "2021-04-27", "authors_parsed": [["Miller", "Andrew C.", ""], ["Gatys", "Leon A.", ""], ["Futoma", "Joseph", ""], ["Fox", "Emily B.", ""]]}, {"id": "2104.12232", "submitter": "Subhabrata Sen", "authors": "Sumit Mukherjee and Subhabrata Sen", "title": "Variational Inference in high-dimensional linear regression", "comments": "39 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST math.PR stat.ML stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study high-dimensional Bayesian linear regression with product priors.\nUsing the nascent theory of non-linear large deviations (Chatterjee and\nDembo,2016), we derive sufficient conditions for the leading-order correctness\nof the naive mean-field approximation to the log-normalizing constant of the\nposterior distribution. Subsequently, assuming a true linear model for the\nobserved data, we derive a limiting infinite dimensional variational formula\nfor the log normalizing constant of the posterior. Furthermore, we establish\nthat under an additional \"separation\" condition, the variational problem has a\nunique optimizer, and this optimizer governs the probabilistic properties of\nthe posterior distribution. We provide intuitive sufficient conditions for the\nvalidity of this \"separation\" condition. Finally, we illustrate our results on\nconcrete examples with specific design matrices.\n", "versions": [{"version": "v1", "created": "Sun, 25 Apr 2021 19:09:38 GMT"}], "update_date": "2021-04-27", "authors_parsed": [["Mukherjee", "Sumit", ""], ["Sen", "Subhabrata", ""]]}, {"id": "2104.12314", "submitter": "Wanli Qiao", "authors": "Wanli Qiao and Wolfgang Polonik", "title": "Algorithms for ridge estimation with convergence guarantees", "comments": "41 pages, 8 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The extraction of filamentary structure from a point cloud is discussed. The\nfilaments are modeled as ridge lines or higher dimensional ridges of an\nunderlying density. We propose two novel algorithms, and provide theoretical\nguarantees for their convergences. We consider the new algorithms as\nalternatives to the Subspace Constraint Mean Shift (SCMS) algorithm that do not\nsuffer from a shortcoming of the SCMS that is also revealed in this paper.\n", "versions": [{"version": "v1", "created": "Mon, 26 Apr 2021 01:57:04 GMT"}], "update_date": "2021-04-27", "authors_parsed": [["Qiao", "Wanli", ""], ["Polonik", "Wolfgang", ""]]}, {"id": "2104.12368", "submitter": "Minh Ha Quang", "authors": "Minh Ha Quang", "title": "Finite sample approximations of exact and entropic Wasserstein distances\n  between covariance operators and Gaussian processes", "comments": "30 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This work studies finite sample approximations of the exact and entropic\nregularized Wasserstein distances between centered Gaussian processes and, more\ngenerally, covariance operators of functional random processes. We first show\nthat these distances/divergences are fully represented by reproducing kernel\nHilbert space (RKHS) covariance and cross-covariance operators associated with\nthe corresponding covariance functions. Using this representation, we show that\nthe Sinkhorn divergence between two centered Gaussian processes can be\nconsistently and efficiently estimated from the divergence between their\ncorresponding normalized finite-dimensional covariance matrices, or\nalternatively, their sample covariance operators. Consequently, this leads to a\nconsistent and efficient algorithm for estimating the Sinkhorn divergence from\nfinite samples generated by the two processes. For a fixed regularization\nparameter, the convergence rates are {\\it dimension-independent} and of the\nsame order as those for the Hilbert-Schmidt distance. If at least one of the\nRKHS is finite-dimensional, we obtain a {\\it dimension-dependent} sample\ncomplexity for the exact Wasserstein distance between the Gaussian processes.\n", "versions": [{"version": "v1", "created": "Mon, 26 Apr 2021 06:57:14 GMT"}], "update_date": "2021-04-27", "authors_parsed": [["Quang", "Minh Ha", ""]]}, {"id": "2104.12384", "submitter": "Konstantinos Zygalakis", "authors": "J.M. Sanz-Serna, Konstantinos C. Zygalakis", "title": "Wasserstein distance estimates for the distributions of numerical\n  approximations to ergodic stochastic differential equations", "comments": "29 pages, 2 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG cs.NA math.NA math.PR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a framework that allows for the non-asymptotic study of the\n$2$-Wasserstein distance between the invariant distribution of an ergodic\nstochastic differential equation and the distribution of its numerical\napproximation in the strongly log-concave case. This allows us to study in a\nunified way a number of different integrators proposed in the literature for\nthe overdamped and underdamped Langevin dynamics. In addition, we analyse a\nnovel splitting method for the underdamped Langevin dynamics which only\nrequires one gradient evaluation per time step. Under an additional smoothness\nassumption on a $d$--dimensional strongly log-concave distribution with\ncondition number $\\kappa$, the algorithm is shown to produce with an\n$\\mathcal{O}\\big(\\kappa^{5/4} d^{1/4}\\epsilon^{-1/2} \\big)$ complexity samples\nfrom a distribution that, in Wasserstein distance, is at most $\\epsilon>0$ away\nfrom the target distribution.\n", "versions": [{"version": "v1", "created": "Mon, 26 Apr 2021 07:50:04 GMT"}], "update_date": "2021-04-27", "authors_parsed": [["Sanz-Serna", "J. M.", ""], ["Zygalakis", "Konstantinos C.", ""]]}, {"id": "2104.12407", "submitter": "Yuezhou Zhang", "authors": "Yuezhou Zhang, Amos A Folarin, Shaoxiong Sun, Nicholas Cummins,\n  Yatharth Ranjan, Zulqarnain Rashid, Pauline Conde, Callum Stewart, Petroula\n  Laiou, Faith Matcham, Carolin Oetzmann, Femke Lamers, Sara Siddi, Sara\n  Simblett, Aki Rintala, David C Mohr, Inez Myin-Germeys, Til Wykes, Josep\n  Maria Haro, Brenda WJH Pennix, Vaibhav A Narayan, Peter Annas, Matthew\n  Hotopf, Richard JB Dobson", "title": "Predicting Depressive Symptom Severity through Individuals' Nearby\n  Bluetooth Devices Count Data Collected by Mobile Phones: A Preliminary\n  Longitudinal Study", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The Bluetooth sensor embedded in mobile phones provides an unobtrusive,\ncontinuous, and cost-efficient means to capture individuals' proximity\ninformation, such as the nearby Bluetooth devices count (NBDC). The continuous\nNBDC data can partially reflect individuals' behaviors and status, such as\nsocial connections and interactions, working status, mobility, and social\nisolation and loneliness, which were found to be significantly associated with\ndepression by previous survey-based studies. This paper aims to explore the\nNBDC data's value in predicting depressive symptom severity as measured via the\n8-item Patient Health Questionnaire (PHQ-8). The data used in this paper\nincluded 2,886 bi-weekly PHQ-8 records collected from 316 participants\nrecruited from three study sites in the Netherlands, Spain, and the UK as part\nof the EU RADAR-CNS study. From the NBDC data two weeks prior to each PHQ-8\nscore, we extracted 49 Bluetooth features, including statistical features and\nnonlinear features for measuring periodicity and regularity of individuals'\nlife rhythms. Linear mixed-effect models were used to explore associations\nbetween Bluetooth features and the PHQ-8 score. We then applied hierarchical\nBayesian linear regression models to predict the PHQ-8 score from the extracted\nBluetooth features. A number of significant associations were found between\nBluetooth features and depressive symptom severity. Compared with commonly used\nmachine learning models, the proposed hierarchical Bayesian linear regression\nmodel achieved the best prediction metrics, R2= 0.526, and root mean squared\nerror (RMSE) of 3.891. Bluetooth features can explain an extra 18.8% of the\nvariance in the PHQ-8 score relative to the baseline model without Bluetooth\nfeatures (R2=0.338, RMSE = 4.547).\n", "versions": [{"version": "v1", "created": "Mon, 26 Apr 2021 09:06:02 GMT"}], "update_date": "2021-04-27", "authors_parsed": [["Zhang", "Yuezhou", ""], ["Folarin", "Amos A", ""], ["Sun", "Shaoxiong", ""], ["Cummins", "Nicholas", ""], ["Ranjan", "Yatharth", ""], ["Rashid", "Zulqarnain", ""], ["Conde", "Pauline", ""], ["Stewart", "Callum", ""], ["Laiou", "Petroula", ""], ["Matcham", "Faith", ""], ["Oetzmann", "Carolin", ""], ["Lamers", "Femke", ""], ["Siddi", "Sara", ""], ["Simblett", "Sara", ""], ["Rintala", "Aki", ""], ["Mohr", "David C", ""], ["Myin-Germeys", "Inez", ""], ["Wykes", "Til", ""], ["Haro", "Josep Maria", ""], ["Pennix", "Brenda WJH", ""], ["Narayan", "Vaibhav A", ""], ["Annas", "Peter", ""], ["Hotopf", "Matthew", ""], ["Dobson", "Richard JB", ""]]}, {"id": "2104.12437", "submitter": "Darius Afchar", "authors": "Darius Afchar, Romain Hennequin and Vincent Guigue", "title": "Towards Rigorous Interpretations: a Formalisation of Feature Attribution", "comments": "38th International Conference on Machine Learning (ICML 2021)", "journal-ref": "PMLR 139:76-86, 2021", "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Feature attribution is often loosely presented as the process of selecting a\nsubset of relevant features as a rationale of a prediction. Task-dependent by\nnature, precise definitions of \"relevance\" encountered in the literature are\nhowever not always consistent. This lack of clarity stems from the fact that we\nusually do not have access to any notion of ground-truth attribution and from a\nmore general debate on what good interpretations are. In this paper we propose\nto formalise feature selection/attribution based on the concept of relaxed\nfunctional dependence. In particular, we extend our notions to the\ninstance-wise setting and derive necessary properties for candidate selection\nsolutions, while leaving room for task-dependence. By computing ground-truth\nattributions on synthetic datasets, we evaluate many state-of-the-art\nattribution methods and show that, even when optimised, some fail to verify the\nproposed properties and provide wrong solutions.\n", "versions": [{"version": "v1", "created": "Mon, 26 Apr 2021 10:04:44 GMT"}, {"version": "v2", "created": "Mon, 5 Jul 2021 14:27:00 GMT"}], "update_date": "2021-07-12", "authors_parsed": [["Afchar", "Darius", ""], ["Hennequin", "Romain", ""], ["Guigue", "Vincent", ""]]}, {"id": "2104.12476", "submitter": "Zhenliang He", "authors": "Zhenliang He, Meina Kan, Shiguang Shan", "title": "EigenGAN: Layer-Wise Eigen-Learning for GANs", "comments": "Code: https://github.com/LynnHo/EigenGAN-Tensorflow", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recent studies on Generative Adversarial Network (GAN) reveal that different\nlayers of a generative CNN hold different semantics of the synthesized images.\nHowever, few GAN models have explicit dimensions to control the semantic\nattributes represented in a specific layer. This paper proposes EigenGAN which\nis able to unsupervisedly mine interpretable and controllable dimensions from\ndifferent generator layers. Specifically, EigenGAN embeds one linear subspace\nwith orthogonal basis into each generator layer. Via the adversarial training\nto learn a target distribution, these layer-wise subspaces automatically\ndiscover a set of \"eigen-dimensions\" at each layer corresponding to a set of\nsemantic attributes or interpretable variations. By traversing the coefficient\nof a specific eigen-dimension, the generator can produce samples with\ncontinuous changes corresponding to a specific semantic attribute. Taking the\nhuman face for example, EigenGAN can discover controllable dimensions for\nhigh-level concepts such as pose and gender in the subspace of deep layers, as\nwell as low-level concepts such as hue and color in the subspace of shallow\nlayers. Moreover, under the linear circumstance, we theoretically prove that\nour algorithm derives the principal components as PCA does. Codes can be found\nin https://github.com/LynnHo/EigenGAN-Tensorflow.\n", "versions": [{"version": "v1", "created": "Mon, 26 Apr 2021 11:14:37 GMT"}], "update_date": "2021-04-27", "authors_parsed": [["He", "Zhenliang", ""], ["Kan", "Meina", ""], ["Shan", "Shiguang", ""]]}, {"id": "2104.12506", "submitter": "Redouane Lguensat", "authors": "Maike Sonnewald, Redouane Lguensat, Daniel C. Jones, Peter D. Dueben,\n  Julien Brajard, Venkatramani Balaji", "title": "Bridging observation, theory and numerical simulation of the ocean using\n  Machine Learning", "comments": "Topical review submitted to Environmental Research Letters", "journal-ref": null, "doi": null, "report-no": null, "categories": "physics.ao-ph stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Progress within physical oceanography has been concurrent with the increasing\nsophistication of tools available for its study. The incorporation of machine\nlearning (ML) techniques offers exciting possibilities for advancing the\ncapacity and speed of established methods and also for making substantial and\nserendipitous discoveries. Beyond vast amounts of complex data ubiquitous in\nmany modern scientific fields, the study of the ocean poses a combination of\nunique challenges that ML can help address. The observational data available is\nlargely spatially sparse, limited to the surface, and with few time series\nspanning more than a handful of decades. Important timescales span seconds to\nmillennia, with strong scale interactions and numerical modelling efforts\ncomplicated by details such as coastlines. This review covers the current\nscientific insight offered by applying ML and points to where there is imminent\npotential. We cover the main three branches of the field: observations, theory,\nand numerical modelling. Highlighting both challenges and opportunities, we\ndiscuss both the historical context and salient ML tools. We focus on the use\nof ML in situ sampling and satellite observations, and the extent to which ML\napplications can advance theoretical oceanographic exploration, as well as aid\nnumerical simulations. Applications that are also covered include model error\nand bias correction and current and potential use within data assimilation.\nWhile not without risk, there is great interest in the potential benefits of\noceanographic ML applications; this review caters to this interest within the\nresearch community.\n", "versions": [{"version": "v1", "created": "Mon, 26 Apr 2021 12:11:51 GMT"}, {"version": "v2", "created": "Fri, 11 Jun 2021 07:37:33 GMT"}], "update_date": "2021-06-14", "authors_parsed": [["Sonnewald", "Maike", ""], ["Lguensat", "Redouane", ""], ["Jones", "Daniel C.", ""], ["Dueben", "Peter D.", ""], ["Brajard", "Julien", ""], ["Balaji", "Venkatramani", ""]]}, {"id": "2104.12516", "submitter": "Mark Green", "authors": "Mark Green", "title": "Evaluating the performance of personal, social, health-related,\n  biomarker and genetic data for predicting an individuals future health using\n  machine learning: A longitudinal analysis", "comments": "19 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG stat.AP", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  As we gain access to a greater depth and range of health-related information\nabout individuals, three questions arise: (1) Can we build better models to\npredict individual-level risk of ill health? (2) How much data do we need to\neffectively predict ill health? (3) Are new methods required to process the\nadded complexity that new forms of data bring? The aim of the study is to apply\na machine learning approach to identify the relative contribution of personal,\nsocial, health-related, biomarker and genetic data as predictors of future\nhealth in individuals. Using longitudinal data from 6830 individuals in the UK\nfrom Understanding Society (2010-12 to 2015-17), the study compares the\npredictive performance of five types of measures: personal (e.g. age, sex),\nsocial (e.g. occupation, education), health-related (e.g. body weight, grip\nstrength), biomarker (e.g. cholesterol, hormones) and genetic single nucleotide\npolymorphisms (SNPs). The predicted outcome variable was limiting long-term\nillness one and five years from baseline. Two machine learning approaches were\nused to build predictive models: deep learning via neural networks and XGBoost\n(gradient boosting decision trees). Model fit was compared to traditional\nlogistic regression models. Results found that health-related measures had the\nstrongest prediction of future health status, with genetic data performing\npoorly. Machine learning models only offered marginal improvements in model\naccuracy when compared to logistic regression models, but also performed well\non other metrics e.g. neural networks were best on AUC and XGBoost on\nprecision. The study suggests that increasing complexity of data and methods\ndoes not necessarily translate to improved understanding of the determinants of\nhealth or performance of predictive models of ill health.\n", "versions": [{"version": "v1", "created": "Mon, 26 Apr 2021 12:31:40 GMT"}], "update_date": "2021-04-27", "authors_parsed": [["Green", "Mark", ""]]}, {"id": "2104.12576", "submitter": "Yanhang Zhang", "authors": "Yanhang Zhang, Junxian Zhu, Jin Zhu, Xueqin Wang", "title": "Certifiably Polynomial Algorithm for Best Group Subset Selection", "comments": "45 pages, 2 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Best group subset selection aims to choose a small part of non-overlapping\ngroups to achieve the best interpretability on the response variable. It is\npractically attractive for group variable selection; however, due to the\ncomputational intractability in high dimensionality setting, it doesn't catch\nenough attention. To fill the blank of efficient algorithms for best group\nsubset selection, in this paper, we propose a group-splicing algorithm that\niteratively detects effective groups and excludes the helpless ones. Moreover,\ncoupled with a novel Bayesian group information criterion, an adaptive\nalgorithm is developed to determine the true group subset size. It is\ncertifiable that our algorithms enable identifying the optimal group subset in\npolynomial time under mild conditions. We demonstrate the efficiency and\naccuracy of our proposal by comparing state-of-the-art algorithms on both\nsynthetic and real-world datasets.\n", "versions": [{"version": "v1", "created": "Fri, 23 Apr 2021 03:05:11 GMT"}], "update_date": "2021-04-27", "authors_parsed": [["Zhang", "Yanhang", ""], ["Zhu", "Junxian", ""], ["Zhu", "Jin", ""], ["Wang", "Xueqin", ""]]}, {"id": "2104.12586", "submitter": "Alessandro D'Ortenzio", "authors": "A. D'Ortenzio and C. Manes", "title": "Consistency issues in Gaussian Mixture Models reduction algorithms", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.IT cs.LG math.IT math.OC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In many contexts Gaussian Mixtures (GM) are used to approximate probability\ndistributions, possibly time-varying. In some applications the number of GM\ncomponents exponentially increases over time, and reduction procedures are\nrequired to keep them reasonably limited. The GM reduction (GMR) problem can be\nformulated by choosing different measures of the dissimilarity of GMs before\nand after reduction, like the Kullback-Leibler Divergence (KLD) and the\nIntegral Squared Error (ISE). Since in no case the solution is obtained in\nclosed form, many approximate GMR algorithms have been proposed in the past\nthree decades, although none of them provides optimality guarantees. In this\nwork we discuss the importance of the choice of the dissimilarity measure and\nthe issue of consistency of all steps of a reduction algorithm with the chosen\nmeasure. Indeed, most of the existing GMR algorithms are composed by several\nsteps which are not consistent with a unique measure, and for this reason may\nproduce reduced GMs far from optimality. In particular, the use of the KLD, of\nthe ISE and normalized ISE is discussed and compared in this perspective.\n", "versions": [{"version": "v1", "created": "Mon, 26 Apr 2021 13:53:46 GMT"}], "update_date": "2021-04-27", "authors_parsed": [["D'Ortenzio", "A.", ""], ["Manes", "C.", ""]]}, {"id": "2104.12587", "submitter": "Chris Oates", "authors": "Junyang Wang, Jon Cockayne, Oksana Chkrebtii, T. J. Sullivan, Chris.\n  J. Oates", "title": "Bayesian Numerical Methods for Nonlinear Partial Differential Equations", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.NA cs.NA stat.CO stat.ME stat.ML", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  The numerical solution of differential equations can be formulated as an\ninference problem to which formal statistical approaches can be applied.\nHowever, nonlinear partial differential equations (PDEs) pose substantial\nchallenges from an inferential perspective, most notably the absence of\nexplicit conditioning formula. This paper extends earlier work on linear PDEs\nto a general class of initial value problems specified by nonlinear PDEs,\nmotivated by problems for which evaluations of the right-hand-side, initial\nconditions, or boundary conditions of the PDE have a high computational cost.\nThe proposed method can be viewed as exact Bayesian inference under an\napproximate likelihood, which is based on discretisation of the nonlinear\ndifferential operator. Proof-of-concept experimental results demonstrate that\nmeaningful probabilistic uncertainty quantification for the unknown solution of\nthe PDE can be performed, while controlling the number of times the\nright-hand-side, initial and boundary conditions are evaluated. A suitable\nprior model for the solution of the PDE is identified using novel theoretical\nanalysis of the sample path properties of Mat\\'{e}rn processes, which may be of\nindependent interest.\n", "versions": [{"version": "v1", "created": "Thu, 22 Apr 2021 14:02:10 GMT"}, {"version": "v2", "created": "Mon, 3 May 2021 13:26:26 GMT"}], "update_date": "2021-05-04", "authors_parsed": [["Wang", "Junyang", ""], ["Cockayne", "Jon", ""], ["Chkrebtii", "Oksana", ""], ["Sullivan", "T. J.", ""], ["Oates", "Chris. J.", ""]]}, {"id": "2104.12657", "submitter": "Micha{\\l} Narajewski", "authors": "Micha{\\l} Narajewski, Jens Kley-Holsteg, Florian Ziel", "title": "tsrobprep -- an R package for robust preprocessing of time series data", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG cs.MS stat.CO", "license": "http://creativecommons.org/licenses/by-sa/4.0/", "abstract": "  Data cleaning is a crucial part of every data analysis exercise. Yet, the\ncurrently available R packages do not provide fast and robust methods for\ncleaning and preparation of time series data. The open source package tsrobprep\nintroduces efficient methods for handling missing values and outliers using\nmodel based approaches. For data imputation a probabilistic replacement model\nis proposed, which may consist of autoregressive components and external\ninputs. For outlier detection a clustering algorithm based on finite mixture\nmodelling is introduced, which considers typical time series related properties\nas features. By assigning to each observation a probability of being an\noutlying data point, the degree of outlyingness can be determined. The methods\nwork robust and are fully tunable. Moreover, by providing the\nauto_data_cleaning function the data preprocessing can be carried out in one\ncast, without manual tuning and providing suitable results. The primary\nmotivation of the package is the preprocessing of energy system data, however,\nthe package is also suited for other moderate and large sized time series data\nset. We present application for electricity load, wind and solar power data.\n", "versions": [{"version": "v1", "created": "Mon, 26 Apr 2021 15:35:11 GMT"}], "update_date": "2021-04-27", "authors_parsed": [["Narajewski", "Micha\u0142", ""], ["Kley-Holsteg", "Jens", ""], ["Ziel", "Florian", ""]]}, {"id": "2104.12676", "submitter": "Babak Barazandeh", "authors": "Babak Barazandeh, Davoud Ataee Tarzanagh, George Michailidis", "title": "Solving a class of non-convex min-max games using adaptive momentum\n  methods", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.OC cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Adaptive momentum methods have recently attracted a lot of attention for\ntraining of deep neural networks. They use an exponential moving average of\npast gradients of the objective function to update both search directions and\nlearning rates. However, these methods are not suited for solving min-max\noptimization problems that arise in training generative adversarial networks.\nIn this paper, we propose an adaptive momentum min-max algorithm that\ngeneralizes adaptive momentum methods to the non-convex min-max regime.\nFurther, we establish non-asymptotic rates of convergence for the proposed\nalgorithm when used in a reasonably broad class of non-convex min-max\noptimization problems. Experimental results illustrate its superior performance\nvis-a-vis benchmark methods for solving such problems.\n", "versions": [{"version": "v1", "created": "Mon, 26 Apr 2021 16:06:39 GMT"}], "update_date": "2021-04-27", "authors_parsed": [["Barazandeh", "Babak", ""], ["Tarzanagh", "Davoud Ataee", ""], ["Michailidis", "George", ""]]}, {"id": "2104.12696", "submitter": "Isaac Neal", "authors": "Isaac Neal, Sohan Seth, Gary Watmough, Mamadou Saliou Diallo", "title": "Towards Sustainable Census Independent Population Estimation in\n  Mozambique", "comments": "5 pages, 2 figures, published in the AI for Public Health Workshop,\n  ICLR 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Reliable and frequent population estimation is key for making policies around\nvaccination and planning infrastructure delivery. Since censuses lack the\nspatio-temporal resolution required for these tasks, census-independent\napproaches, using remote sensing and microcensus data, have become popular. We\nestimate intercensal population count in two pilot districts in Mozambique. To\nencourage sustainability, we assess the feasibility of using publicly available\ndatasets to estimate population. We also explore transfer learning with\nexisting annotated datasets for predicting building footprints, and training\nwith additional `dot' annotations from regions of interest to enhance these\nestimations. We observe that population predictions improve when using\nfootprint area estimated with this approach versus only publicly available\nfeatures.\n", "versions": [{"version": "v1", "created": "Mon, 26 Apr 2021 16:37:41 GMT"}], "update_date": "2021-04-27", "authors_parsed": [["Neal", "Isaac", ""], ["Seth", "Sohan", ""], ["Watmough", "Gary", ""], ["Diallo", "Mamadou Saliou", ""]]}, {"id": "2104.12733", "submitter": "Ward Haddadin", "authors": "Ward Haddadin", "title": "Invariant polynomials and machine learning", "comments": "27 pages, 5 figures, 3 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "hep-ph cs.LG stat.ML", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  We present an application of invariant polynomials in machine learning. Using\nthe methods developed in previous work, we obtain two types of generators of\nthe Lorentz- and permutation-invariant polynomials in particle momenta; minimal\nalgebra generators and Hironaka decompositions. We discuss and prove some\napproximation theorems to make use of these invariant generators in machine\nlearning algorithms in general and in neural networks specifically. By\nimplementing these generators in neural networks applied to regression tasks,\nwe test the improvements in performance under a wide range of hyperparameter\nchoices and find a reduction of the loss on training data and a significant\nreduction of the loss on validation data. For a different approach on\nquantifying the performance of these neural networks, we treat the problem from\na Bayesian inference perspective and employ nested sampling techniques to\nperform model comparison. Beyond a certain network size, we find that networks\nutilising Hironaka decompositions perform the best.\n", "versions": [{"version": "v1", "created": "Mon, 26 Apr 2021 17:24:29 GMT"}], "update_date": "2021-04-27", "authors_parsed": [["Haddadin", "Ward", ""]]}, {"id": "2104.12852", "submitter": "Christopher Blier-Wong", "authors": "Christopher Blier-Wong and H\\'el\\`ene Cossette and Luc Lamontagne and\n  Etienne Marceau", "title": "Geographic ratemaking with spatial embeddings", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Spatial data is a rich source of information for actuarial applications:\nknowledge of a risk's location could improve an insurance company's ratemaking,\nreserving or risk management processes. Insurance companies with high exposures\nin a territory typically have a competitive advantage since they may use\nhistorical losses in a region to model spatial risk non-parametrically. Relying\non geographic losses is problematic for areas where past loss data is\nunavailable. This paper presents a method based on data (instead of smoothing\nhistorical insurance claim losses) to construct a geographic ratemaking model.\nIn particular, we construct spatial features within a complex representation\nmodel, then use the features as inputs to a simpler predictive model (like a\ngeneralized linear model). Our approach generates predictions with smaller bias\nand smaller variance than other spatial interpolation models such as bivariate\nsplines in most situations. This method also enables us to generate rates in\nterritories with no historical experience.\n", "versions": [{"version": "v1", "created": "Mon, 26 Apr 2021 20:09:45 GMT"}], "update_date": "2021-04-28", "authors_parsed": [["Blier-Wong", "Christopher", ""], ["Cossette", "H\u00e9l\u00e8ne", ""], ["Lamontagne", "Luc", ""], ["Marceau", "Etienne", ""]]}, {"id": "2104.12909", "submitter": "Yusuke Narita", "authors": "Yusuke Narita and Kohei Yata", "title": "Algorithm is Experiment: Machine Learning, Market Design, and Policy\n  Eligibility Rules", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "econ.EM cs.LG stat.ME stat.ML", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Algorithms produce a growing portion of decisions and recommendations both in\npolicy and business. Such algorithmic decisions are natural experiments\n(conditionally quasi-randomly assigned instruments) since the algorithms make\ndecisions based only on observable input variables. We use this observation to\ndevelop a treatment-effect estimator for a class of stochastic and\ndeterministic decision-making algorithms. Our estimator is shown to be\nconsistent and asymptotically normal for well-defined causal effects. A key\nspecial case of our estimator is a multidimensional regression discontinuity\ndesign. We apply our estimator to evaluate the effect of the Coronavirus Aid,\nRelief, and Economic Security (CARES) Act, where more than \\$175 billion worth\nof relief funding is allocated to hospitals via an algorithmic rule. Our\nestimates suggest that the relief funding has little effect on COVID-19-related\nhospital activity levels. Naive OLS and IV estimates exhibit substantial\nselection bias.\n", "versions": [{"version": "v1", "created": "Mon, 26 Apr 2021 23:18:34 GMT"}, {"version": "v2", "created": "Mon, 28 Jun 2021 12:37:09 GMT"}], "update_date": "2021-06-29", "authors_parsed": [["Narita", "Yusuke", ""], ["Yata", "Kohei", ""]]}, {"id": "2104.12949", "submitter": "Michael Burkhart", "authors": "Michael C. Burkhart", "title": "Discriminative Bayesian Filtering Lends Momentum to the Stochastic\n  Newton Method for Minimizing Log-Convex Functions", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG math.OC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  To minimize the average of a set of log-convex functions, the stochastic\nNewton method iteratively updates its estimate using subsampled versions of the\nfull objective's gradient and Hessian. We contextualize this optimization\nproblem as sequential Bayesian inference on a latent state-space model with a\ndiscriminatively-specified observation process. Applying Bayesian filtering\nthen yields a novel optimization algorithm that considers the entire history of\ngradients and Hessians when forming an update. We establish matrix-based\nconditions under which the effect of older observations diminishes over time,\nin a manner analogous to Polyak's heavy ball momentum. We illustrate various\naspects of our approach with an example and review other relevant innovations\nfor the stochastic Newton method.\n", "versions": [{"version": "v1", "created": "Tue, 27 Apr 2021 02:39:21 GMT"}], "update_date": "2021-04-28", "authors_parsed": [["Burkhart", "Michael C.", ""]]}, {"id": "2104.12953", "submitter": "Yuandu Lai", "authors": "Yuandu Lai, Yucheng Shi, Yahong Han, Yunfeng Shao, Meiyu Qi, Bingshuai\n  Li", "title": "Exploring Uncertainty in Deep Learning for Construction of Prediction\n  Intervals", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Deep learning has achieved impressive performance on many tasks in recent\nyears. However, it has been found that it is still not enough for deep neural\nnetworks to provide only point estimates. For high-risk tasks, we need to\nassess the reliability of the model predictions. This requires us to quantify\nthe uncertainty of model prediction and construct prediction intervals. In this\npaper, We explore the uncertainty in deep learning to construct the prediction\nintervals. In general, We comprehensively consider two categories of\nuncertainties: aleatory uncertainty and epistemic uncertainty. We design a\nspecial loss function, which enables us to learn uncertainty without\nuncertainty label. We only need to supervise the learning of regression task.\nWe learn the aleatory uncertainty implicitly from the loss function. And that\nepistemic uncertainty is accounted for in ensembled form. Our method correlates\nthe construction of prediction intervals with the uncertainty estimation.\nImpressive results on some publicly available datasets show that the\nperformance of our method is competitive with other state-of-the-art methods.\n", "versions": [{"version": "v1", "created": "Tue, 27 Apr 2021 02:58:20 GMT"}], "update_date": "2021-04-28", "authors_parsed": [["Lai", "Yuandu", ""], ["Shi", "Yucheng", ""], ["Han", "Yahong", ""], ["Shao", "Yunfeng", ""], ["Qi", "Meiyu", ""], ["Li", "Bingshuai", ""]]}, {"id": "2104.13020", "submitter": "Jose M. Pe\\~na", "authors": "Jose M. Pe\\~na", "title": "Simple yet Sharp Sensitivity Analysis for Unmeasured Confounding", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a method for assessing the sensitivity of the true causal effect\nto unmeasured confounding. The method requires the analyst to specify two\nintuitive parameters. Otherwise, the method is assumption-free. The method\nreturns an interval that contains the true causal effect. Moreover, the bounds\nof the interval are sharp, i.e. attainable. We show experimentally that our\nbounds can be sharper than those obtained by the method of Ding and VanderWeele\n(2016). Finally, we extend our method to bound the natural direct and indirect\neffects when there are measured mediators and unmeasured exposure-outcome\nconfounding.\n", "versions": [{"version": "v1", "created": "Tue, 27 Apr 2021 07:43:52 GMT"}, {"version": "v2", "created": "Fri, 2 Jul 2021 11:44:25 GMT"}], "update_date": "2021-07-05", "authors_parsed": [["Pe\u00f1a", "Jose M.", ""]]}, {"id": "2104.13026", "submitter": "Johan Larsson", "authors": "Johan Larsson, Jonas Wallin", "title": "The Hessian Screening Rule", "comments": "17 pages, 5 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Predictor screening rules, which discard predictors from the design matrix\nbefore fitting a model, have had sizable impacts on the speed with which\n$\\ell_1$-regularized regression problems, such as the lasso, can be solved.\nCurrent state-of-the-art screening rules, however, have difficulties in dealing\nwith highly-correlated predictors, often becoming too conservative. In this\npaper, we present a new screening rule to deal with this issue: the Hessian\nScreening Rule. The rule uses second-order information from the model in order\nto provide more accurate screening as well as higher-quality warm starts. In\nour experiments on $\\ell_1$-regularized least-squares (the lasso) and logistic\nregression, we show that the rule outperforms all other alternatives in\nsimulated experiments with high correlation, as well as in the majority of real\ndatasets that we study.\n", "versions": [{"version": "v1", "created": "Tue, 27 Apr 2021 07:55:29 GMT"}], "update_date": "2021-04-28", "authors_parsed": [["Larsson", "Johan", ""], ["Wallin", "Jonas", ""]]}, {"id": "2104.13101", "submitter": "Felix P. Kemeth", "authors": "Felix P. Kemeth, Tom Bertalan, Nikolaos Evangelou, Tianqi Cui, Saurabh\n  Malani, Ioannis G. Kevrekidis", "title": "Initializing LSTM internal states via manifold learning", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.AI cs.LG nlin.PS", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  We present an approach, based on learning an intrinsic data manifold, for the\ninitialization of the internal state values of LSTM recurrent neural networks,\nensuring consistency with the initial observed input data. Exploiting the\ngeneralized synchronization concept, we argue that the converged, \"mature\"\ninternal states constitute a function on this learned manifold. The dimension\nof this manifold then dictates the length of observed input time series data\nrequired for consistent initialization. We illustrate our approach through a\npartially observed chemical model system, where initializing the internal LSTM\nstates in this fashion yields visibly improved performance. Finally, we show\nthat learning this data manifold enables the transformation of partially\nobserved dynamics into fully observed ones, facilitating alternative\nidentification paths for nonlinear dynamical systems.\n", "versions": [{"version": "v1", "created": "Tue, 27 Apr 2021 10:54:53 GMT"}, {"version": "v2", "created": "Wed, 12 May 2021 06:59:24 GMT"}], "update_date": "2021-05-13", "authors_parsed": [["Kemeth", "Felix P.", ""], ["Bertalan", "Tom", ""], ["Evangelou", "Nikolaos", ""], ["Cui", "Tianqi", ""], ["Malani", "Saurabh", ""], ["Kevrekidis", "Ioannis G.", ""]]}, {"id": "2104.13208", "submitter": "Jean-Jil Duchamps", "authors": "Cl\\'ement Dombry and Jean-Jil Duchamps", "title": "Infinitesimal gradient boosting", "comments": "44 pages, 1 figure", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG math.PR math.ST stat.TH", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  We define infinitesimal gradient boosting as a limit of the popular\ntree-based gradient boosting algorithm from machine learning. The limit is\nconsidered in the vanishing-learning-rate asymptotic, that is when the learning\nrate tends to zero and the number of gradient trees is rescaled accordingly.\nFor this purpose, we introduce a new class of randomized regression trees\nbridging totally randomized trees and Extra Trees and using a softmax\ndistribution for binary splitting. Our main result is the convergence of the\nassociated stochastic algorithm and the characterization of the limiting\nprocedure as the unique solution of a nonlinear ordinary differential equation\nin a infinite dimensional function space. Infinitesimal gradient boosting\ndefines a smooth path in the space of continuous functions along which the\ntraining error decreases, the residuals remain centered and the total variation\nis well controlled.\n", "versions": [{"version": "v1", "created": "Mon, 26 Apr 2021 15:09:05 GMT"}], "update_date": "2021-04-28", "authors_parsed": [["Dombry", "Cl\u00e9ment", ""], ["Duchamps", "Jean-Jil", ""]]}, {"id": "2104.13289", "submitter": "Rita Fioresi", "authors": "Luca Grementieri, Rita Fioresi", "title": "Model-centric Data Manifold: the Data Through the Eyes of the Model", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG math.OC stat.ML", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  We discover that deep ReLU neural network classifiers can see a\nlow-dimensional Riemannian manifold structure on data. Such structure comes via\nthe local data matrix, a variation of the Fisher information matrix, where the\nrole of the model parameters is taken by the data variables. We obtain a\nfoliation of the data domain and we show that the dataset on which the model is\ntrained lies on a leaf, the data leaf, whose dimension is bounded by the number\nof classification labels. We validate our results with some experiments with\nthe MNIST dataset: paths on the data leaf connect valid images, while other\nleaves cover noisy images.\n", "versions": [{"version": "v1", "created": "Mon, 26 Apr 2021 16:03:09 GMT"}], "update_date": "2021-04-28", "authors_parsed": [["Grementieri", "Luca", ""], ["Fioresi", "Rita", ""]]}, {"id": "2104.13326", "submitter": "Yaodong Yu", "authors": "Yaodong Yu, Tianyi Lin, Eric Mazumdar, Michael I. Jordan", "title": "Fast Distributionally Robust Learning with Variance Reduced Min-Max\n  Optimization", "comments": "The first three authors contributed equally to this work; 37 pages,\n  20 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG math.OC stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Distributionally robust supervised learning (DRSL) is emerging as a key\nparadigm for building reliable machine learning systems for real-world\napplications -- reflecting the need for classifiers and predictive models that\nare robust to the distribution shifts that arise from phenomena such as\nselection bias or nonstationarity. Existing algorithms for solving Wasserstein\nDRSL -- one of the most popular DRSL frameworks based around robustness to\nperturbations in the Wasserstein distance -- involve solving complex\nsubproblems or fail to make use of stochastic gradients, limiting their use in\nlarge-scale machine learning problems. We revisit Wasserstein DRSL through the\nlens of min-max optimization and derive scalable and efficiently implementable\nstochastic extra-gradient algorithms which provably achieve faster convergence\nrates than existing approaches. We demonstrate their effectiveness on synthetic\nand real data when compared to existing DRSL approaches. Key to our results is\nthe use of variance reduction and random reshuffling to accelerate stochastic\nmin-max optimization, the analysis of which may be of independent interest.\n", "versions": [{"version": "v1", "created": "Tue, 27 Apr 2021 16:56:09 GMT"}], "update_date": "2021-04-28", "authors_parsed": [["Yu", "Yaodong", ""], ["Lin", "Tianyi", ""], ["Mazumdar", "Eric", ""], ["Jordan", "Michael I.", ""]]}, {"id": "2104.13343", "submitter": "Franco Pellegrini", "authors": "Franco Pellegrini, Giulio Biroli", "title": "Sifting out the features by pruning: Are convolutional networks the\n  winning lottery ticket of fully connected ones?", "comments": "25 pages, 18 figures; typos corrected, references added", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV stat.ML", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Pruning methods can considerably reduce the size of artificial neural\nnetworks without harming their performance. In some cases, they can even\nuncover sub-networks that, when trained in isolation, match or surpass the test\naccuracy of their dense counterparts. Here we study the inductive bias that\npruning imprints in such \"winning lottery tickets\". Focusing on visual tasks,\nwe analyze the architecture resulting from iterative magnitude pruning of a\nsimple fully connected network (FCN). We show that the surviving node\nconnectivity is local in input space, and organized in patterns reminiscent of\nthe ones found in convolutional networks (CNN). We investigate the role played\nby data and tasks in shaping the architecture of pruned sub-networks. Our\nresults show that the winning lottery tickets of FCNs display the key features\nof CNNs. The ability of such automatic network-simplifying procedure to recover\nthe key features \"hand-crafted\" in the design of CNNs suggests interesting\napplications to other datasets and tasks, in order to discover new and\nefficient architectural inductive biases.\n", "versions": [{"version": "v1", "created": "Tue, 27 Apr 2021 17:25:54 GMT"}, {"version": "v2", "created": "Fri, 14 May 2021 10:52:49 GMT"}], "update_date": "2021-05-17", "authors_parsed": [["Pellegrini", "Franco", ""], ["Biroli", "Giulio", ""]]}, {"id": "2104.13369", "submitter": "Michal Yarom", "authors": "Oran Lang, Yossi Gandelsman, Michal Yarom, Yoav Wald, Gal Elidan,\n  Avinatan Hassidim, William T. Freeman, Phillip Isola, Amir Globerson, Michal\n  Irani, Inbar Mosseri", "title": "Explaining in Style: Training a GAN to explain a classifier in\n  StyleSpace", "comments": "First four authors contributed equally. Project page:\n  https://explaining-in-style.github.io/", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG cs.NE eess.IV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Image classification models can depend on multiple different semantic\nattributes of the image. An explanation of the decision of the classifier needs\nto both discover and visualize these properties. Here we present StylEx, a\nmethod for doing this, by training a generative model to specifically explain\nmultiple attributes that underlie classifier decisions. A natural source for\nsuch attributes is the StyleSpace of StyleGAN, which is known to generate\nsemantically meaningful dimensions in the image. However, because standard GAN\ntraining is not dependent on the classifier, it may not represent these\nattributes which are important for the classifier decision, and the dimensions\nof StyleSpace may represent irrelevant attributes. To overcome this, we propose\na training procedure for a StyleGAN, which incorporates the classifier model,\nin order to learn a classifier-specific StyleSpace. Explanatory attributes are\nthen selected from this space. These can be used to visualize the effect of\nchanging multiple attributes per image, thus providing image-specific\nexplanations. We apply StylEx to multiple domains, including animals, leaves,\nfaces and retinal images. For these, we show how an image can be modified in\ndifferent ways to change its classifier output. Our results show that the\nmethod finds attributes that align well with semantic ones, generate meaningful\nimage-specific explanations, and are human-interpretable as measured in\nuser-studies.\n", "versions": [{"version": "v1", "created": "Tue, 27 Apr 2021 17:57:19 GMT"}], "update_date": "2021-04-28", "authors_parsed": [["Lang", "Oran", ""], ["Gandelsman", "Yossi", ""], ["Yarom", "Michal", ""], ["Wald", "Yoav", ""], ["Elidan", "Gal", ""], ["Hassidim", "Avinatan", ""], ["Freeman", "William T.", ""], ["Isola", "Phillip", ""], ["Globerson", "Amir", ""], ["Irani", "Michal", ""], ["Mosseri", "Inbar", ""]]}, {"id": "2104.13417", "submitter": "Weituo Hao", "authors": "Weituo Hao, Mostafa El-Khamy, Jungwon Lee, Jianyi Zhang, Kevin J\n  Liang, Changyou Chen, Lawrence Carin", "title": "Towards Fair Federated Learning with Zero-Shot Data Augmentation", "comments": "Accepted by IEEE CVPR Workshop on Fair, Data Efficient And Trusted\n  Computer Vision", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Federated learning has emerged as an important distributed learning paradigm,\nwhere a server aggregates a global model from many client-trained models while\nhaving no access to the client data. Although it is recognized that statistical\nheterogeneity of the client local data yields slower global model convergence,\nit is less commonly recognized that it also yields a biased federated global\nmodel with a high variance of accuracy across clients. In this work, we aim to\nprovide federated learning schemes with improved fairness. To tackle this\nchallenge, we propose a novel federated learning system that employs zero-shot\ndata augmentation on under-represented data to mitigate statistical\nheterogeneity and encourage more uniform accuracy performance across clients in\nfederated networks. We study two variants of this scheme, Fed-ZDAC (federated\nlearning with zero-shot data augmentation at the clients) and Fed-ZDAS\n(federated learning with zero-shot data augmentation at the server). Empirical\nresults on a suite of datasets demonstrate the effectiveness of our methods on\nsimultaneously improving the test accuracy and fairness.\n", "versions": [{"version": "v1", "created": "Tue, 27 Apr 2021 18:23:54 GMT"}], "update_date": "2021-04-29", "authors_parsed": [["Hao", "Weituo", ""], ["El-Khamy", "Mostafa", ""], ["Lee", "Jungwon", ""], ["Zhang", "Jianyi", ""], ["Liang", "Kevin J", ""], ["Chen", "Changyou", ""], ["Carin", "Lawrence", ""]]}, {"id": "2104.13458", "submitter": "Salvatore Scognamiglio Dr.", "authors": "Vali Asimit, Ioannis Kyriakou, Simone Santoni, Salvatore Scognamiglio\n  and Rui Zhu", "title": "Robust Classification via Support Vector Machines", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://creativecommons.org/publicdomain/zero/1.0/", "abstract": "  The loss function choice for any Support Vector Machine classifier has raised\ngreat interest in the literature due to the lack of robustness of the Hinge\nloss, which is the standard loss choice. In this paper, we plan to robustify\nthe binary classifier by maintaining the overall advantages of the Hinge loss,\nrather than modifying this standard choice. We propose two robust classifiers\nunder data uncertainty. The first is called Single Perturbation SVM (SP-SVM)\nand provides a constructive method by allowing a controlled perturbation to one\nfeature of the data. The second method is called Extreme Empirical Loss SVM\n(EEL-SVM) and is based on a new empirical loss estimate, namely, the Extreme\nEmpirical Loss (EEL), that puts more emphasis on extreme violations of the\nclassification hyper-plane, rather than taking the usual sample average with\nequal importance for all hyper-plane violations. Extensive numerical\ninvestigation reveals the advantages of the two robust classifiers on simulated\ndata and well-known real datasets.\n", "versions": [{"version": "v1", "created": "Tue, 27 Apr 2021 20:20:12 GMT"}], "update_date": "2021-04-29", "authors_parsed": [["Asimit", "Vali", ""], ["Kyriakou", "Ioannis", ""], ["Santoni", "Simone", ""], ["Scognamiglio", "Salvatore", ""], ["Zhu", "Rui", ""]]}, {"id": "2104.13478", "submitter": "Petar Veli\\v{c}kovi\\'c", "authors": "Michael M. Bronstein, Joan Bruna, Taco Cohen, Petar Veli\\v{c}kovi\\'c", "title": "Geometric Deep Learning: Grids, Groups, Graphs, Geodesics, and Gauges", "comments": "156 pages. Work in progress -- comments welcome!", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI cs.CG cs.CV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The last decade has witnessed an experimental revolution in data science and\nmachine learning, epitomised by deep learning methods. Indeed, many\nhigh-dimensional learning tasks previously thought to be beyond reach -- such\nas computer vision, playing Go, or protein folding -- are in fact feasible with\nappropriate computational scale. Remarkably, the essence of deep learning is\nbuilt from two simple algorithmic principles: first, the notion of\nrepresentation or feature learning, whereby adapted, often hierarchical,\nfeatures capture the appropriate notion of regularity for each task, and\nsecond, learning by local gradient-descent type methods, typically implemented\nas backpropagation.\n  While learning generic functions in high dimensions is a cursed estimation\nproblem, most tasks of interest are not generic, and come with essential\npre-defined regularities arising from the underlying low-dimensionality and\nstructure of the physical world. This text is concerned with exposing these\nregularities through unified geometric principles that can be applied\nthroughout a wide spectrum of applications.\n  Such a 'geometric unification' endeavour, in the spirit of Felix Klein's\nErlangen Program, serves a dual purpose: on one hand, it provides a common\nmathematical framework to study the most successful neural network\narchitectures, such as CNNs, RNNs, GNNs, and Transformers. On the other hand,\nit gives a constructive procedure to incorporate prior physical knowledge into\nneural architectures and provide principled way to build future architectures\nyet to be invented.\n", "versions": [{"version": "v1", "created": "Tue, 27 Apr 2021 21:09:51 GMT"}, {"version": "v2", "created": "Sun, 2 May 2021 16:16:03 GMT"}], "update_date": "2021-05-04", "authors_parsed": [["Bronstein", "Michael M.", ""], ["Bruna", "Joan", ""], ["Cohen", "Taco", ""], ["Veli\u010dkovi\u0107", "Petar", ""]]}, {"id": "2104.13479", "submitter": "Prachi Loliencar", "authors": "Prachi Loliencar and Giseon Heo", "title": "Phenotyping OSA: a time series analysis using fuzzy clustering and\n  persistent homology", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG q-bio.QM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Sleep apnea is a disorder that has serious consequences for the pediatric\npopulation. There has been recent concern that traditional diagnosis of the\ndisorder using the apnea-hypopnea index may be ineffective in capturing its\nmulti-faceted outcomes. In this work, we take a first step in addressing this\nissue by phenotyping patients using a clustering analysis of airflow time\nseries. This is approached in three ways: using feature-based fuzzy clustering\nin the time and frequency domains, and using persistent homology to study the\nsignal from a topological perspective. The fuzzy clusters are analyzed in a\nnovel manner using a Dirichlet regression analysis, while the topological\napproach leverages Takens embedding theorem to study the periodicity properties\nof the signals.\n", "versions": [{"version": "v1", "created": "Tue, 27 Apr 2021 21:12:30 GMT"}], "update_date": "2021-04-29", "authors_parsed": [["Loliencar", "Prachi", ""], ["Heo", "Giseon", ""]]}, {"id": "2104.13504", "submitter": "Kevin Kim", "authors": "Kevin Kim and Alex Gittens", "title": "Learning Fair Canonical Polyadical Decompositions using a Kernel\n  Independence Criterion", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This work proposes to learn fair low-rank tensor decompositions by\nregularizing the Canonical Polyadic Decomposition factorization with the kernel\nHilbert-Schmidt independence criterion (KHSIC). It is shown, theoretically and\nempirically, that a small KHSIC between a latent factor and the sensitive\nfeatures guarantees approximate statistical parity. The proposed algorithm\nsurpasses the state-of-the-art algorithm, FATR (Zhu et al., 2018), in\ncontrolling the trade-off between fairness and residual fit on synthetic and\nreal data sets.\n", "versions": [{"version": "v1", "created": "Tue, 27 Apr 2021 23:16:10 GMT"}], "update_date": "2021-04-29", "authors_parsed": [["Kim", "Kevin", ""], ["Gittens", "Alex", ""]]}, {"id": "2104.13517", "submitter": "Ji Oon Lee", "authors": "Ji Hyung Jung, Hye Won Chung, Ji Oon Lee", "title": "Detection of Signal in the Spiked Rectangular Models", "comments": "38 pages, 6 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST cs.LG math.PR stat.ML stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider the problem of detecting signals in the rank-one\nsignal-plus-noise data matrix models that generalize the spiked Wishart\nmatrices. We show that the principal component analysis can be improved by\npre-transforming the matrix entries if the noise is non-Gaussian. As an\nintermediate step, we prove a sharp phase transition of the largest eigenvalues\nof spiked rectangular matrices, which extends the Baik-Ben Arous-P\\'ech\\'e\n(BBP) transition. We also propose a hypothesis test to detect the presence of\nsignal with low computational complexity, based on the linear spectral\nstatistics, which minimizes the sum of the Type-I and Type-II errors when the\nnoise is Gaussian.\n", "versions": [{"version": "v1", "created": "Wed, 28 Apr 2021 01:15:45 GMT"}], "update_date": "2021-04-29", "authors_parsed": [["Jung", "Ji Hyung", ""], ["Chung", "Hye Won", ""], ["Lee", "Ji Oon", ""]]}, {"id": "2104.13621", "submitter": "Antonio Ginart", "authors": "Antonio Ginart, Martin Zhang, James Zou", "title": "MLDemon: Deployment Monitoring for Machine Learning Systems", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Post-deployment monitoring of the performance of ML systems is critical for\nensuring reliability, especially as new user inputs can differ from the\ntraining distribution. Here we propose a novel approach, MLDemon, for ML\nDEployment MONitoring. MLDemon integrates both unlabeled features and a small\namount of on-demand labeled examples over time to produce a real-time estimate\nof the ML model's current performance on a given data stream. Subject to budget\nconstraints, MLDemon decides when to acquire additional, potentially costly,\nsupervised labels to verify the model. On temporal datasets with diverse\ndistribution drifts and models, MLDemon substantially outperforms existing\nmonitoring approaches. Moreover, we provide theoretical analysis to show that\nMLDemon is minimax rate optimal up to logarithmic factors and is provably\nrobust against broad distribution drifts whereas prior approaches are not.\n", "versions": [{"version": "v1", "created": "Wed, 28 Apr 2021 07:59:10 GMT"}, {"version": "v2", "created": "Thu, 29 Apr 2021 06:31:52 GMT"}, {"version": "v3", "created": "Wed, 5 May 2021 18:06:53 GMT"}, {"version": "v4", "created": "Wed, 9 Jun 2021 23:04:50 GMT"}], "update_date": "2021-06-11", "authors_parsed": [["Ginart", "Antonio", ""], ["Zhang", "Martin", ""], ["Zou", "James", ""]]}, {"id": "2104.13626", "submitter": "Paul Viallard", "authors": "Paul Viallard (LHC), Pascal Germain (ULaval), Amaury Habrard (LHC),\n  Emilie Morvant (LHC)", "title": "Self-Bounding Majority Vote Learning Algorithms by the Direct\n  Minimization of a Tight PAC-Bayesian C-Bound", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In the PAC-Bayesian literature, the C-Bound refers to an insightful relation\nbetween the risk of a majority vote classifier (under the zero-one loss) and\nthe first two moments of its margin (i.e., the expected margin and the voters'\ndiversity). Until now, learning algorithms developed in this framework minimize\nthe empirical version of the C-Bound, instead of explicit PAC-Bayesian\ngeneralization bounds. In this paper, by directly optimizing PAC-Bayesian\nguarantees on the C-Bound, we derive self-bounding majority vote learning\nalgorithms. Moreover, our algorithms based on gradient descent are scalable and\nlead to accurate predictors paired with non-vacuous guarantees.\n", "versions": [{"version": "v1", "created": "Wed, 28 Apr 2021 08:23:18 GMT"}], "update_date": "2021-04-29", "authors_parsed": [["Viallard", "Paul", "", "LHC"], ["Germain", "Pascal", "", "ULaval"], ["Habrard", "Amaury", "", "LHC"], ["Morvant", "Emilie", "", "LHC"]]}, {"id": "2104.13628", "submitter": "Quanquan Gu", "authors": "Yuan Cao and Quanquan Gu and Mikhail Belkin", "title": "Risk Bounds for Over-parameterized Maximum Margin Classification on\n  Sub-Gaussian Mixtures", "comments": "35 pages, 3 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG math.ST stat.ML stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Modern machine learning systems such as deep neural networks are often highly\nover-parameterized so that they can fit the noisy training data exactly, yet\nthey can still achieve small test errors in practice. In this paper, we study\nthis \"benign overfitting\" (Bartlett et al. (2020)) phenomenon of the maximum\nmargin classifier for linear classification problems. Specifically, we consider\ndata generated from sub-Gaussian mixtures, and provide a tight risk bound for\nthe maximum margin linear classifier in the over-parameterized setting. Our\nresults precisely characterize the condition under which benign overfitting can\noccur in linear classification problems, and improve on previous work. They\nalso have direct implications for over-parameterized logistic regression.\n", "versions": [{"version": "v1", "created": "Wed, 28 Apr 2021 08:25:16 GMT"}], "update_date": "2021-04-29", "authors_parsed": [["Cao", "Yuan", ""], ["Gu", "Quanquan", ""], ["Belkin", "Mikhail", ""]]}, {"id": "2104.13662", "submitter": "Lucas Theis", "authors": "Lucas Theis and Aaron B. Wagner", "title": "A coding theorem for the rate-distortion-perception function", "comments": null, "journal-ref": "ICLR 2021 Neural Compression Workshop", "doi": null, "report-no": null, "categories": "cs.IT math.IT stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The rate-distortion-perception function (RDPF; Blau and Michaeli, 2019) has\nemerged as a useful tool for thinking about realism and distortion of\nreconstructions in lossy compression. Unlike the rate-distortion function,\nhowever, it is unknown whether encoders and decoders exist that achieve the\nrate suggested by the RDPF. Building on results by Li and El Gamal (2018), we\nshow that the RDPF can indeed be achieved using stochastic, variable-length\ncodes. For this class of codes, we also prove that the RDPF lower-bounds the\nachievable rate\n", "versions": [{"version": "v1", "created": "Wed, 28 Apr 2021 09:33:05 GMT"}], "update_date": "2021-04-29", "authors_parsed": [["Theis", "Lucas", ""], ["Wagner", "Aaron B.", ""]]}, {"id": "2104.13669", "submitter": "Calypso Herrera", "authors": "Calypso Herrera, Florian Krach, Pierre Ruyssen, Josef Teichmann", "title": "Optimal Stopping via Randomized Neural Networks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG cs.NA math.NA math.PR q-fin.CP", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  This paper presents new machine learning approaches to approximate the\nsolution of optimal stopping problems. The key idea of these methods is to use\nneural networks, where the hidden layers are generated randomly and only the\nlast layer is trained, in order to approximate the continuation value. Our\napproaches are applicable for high dimensional problems where the existing\napproaches become increasingly impractical. In addition, since our approaches\ncan be optimized using a simple linear regression, they are very easy to\nimplement and theoretical guarantees can be provided. In Markovian examples our\nrandomized reinforcement learning approach and in non-Markovian examples our\nrandomized recurrent neural network approach outperform the state-of-the-art\nand other relevant machine learning approaches.\n", "versions": [{"version": "v1", "created": "Wed, 28 Apr 2021 09:47:21 GMT"}], "update_date": "2021-04-29", "authors_parsed": [["Herrera", "Calypso", ""], ["Krach", "Florian", ""], ["Ruyssen", "Pierre", ""], ["Teichmann", "Josef", ""]]}, {"id": "2104.13756", "submitter": "Sebastian Gabriel Popescu", "authors": "Sebastian G. Popescu, David J. Sharp, James H. Cole, Konstantinos\n  Kamnitsas, Ben Glocker", "title": "Distributional Gaussian Process Layers for Outlier Detection in Image\n  Segmentation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  We propose a parameter efficient Bayesian layer for hierarchical\nconvolutional Gaussian Processes that incorporates Gaussian Processes operating\nin Wasserstein-2 space to reliably propagate uncertainty. This directly\nreplaces convolving Gaussian Processes with a distance-preserving affine\noperator on distributions. Our experiments on brain tissue-segmentation show\nthat the resulting architecture approaches the performance of well-established\ndeterministic segmentation algorithms (U-Net), which has never been achieved\nwith previous hierarchical Gaussian Processes. Moreover, by applying the same\nsegmentation model to out-of-distribution data (i.e., images with pathology\nsuch as brain tumors), we show that our uncertainty estimates result in\nout-of-distribution detection that outperforms the capabilities of previous\nBayesian networks and reconstruction-based approaches that learn normative\ndistributions.\n", "versions": [{"version": "v1", "created": "Wed, 28 Apr 2021 13:37:10 GMT"}], "update_date": "2021-04-29", "authors_parsed": [["Popescu", "Sebastian G.", ""], ["Sharp", "David J.", ""], ["Cole", "James H.", ""], ["Kamnitsas", "Konstantinos", ""], ["Glocker", "Ben", ""]]}, {"id": "2104.13790", "submitter": "Yangfan Zhou", "authors": "Yangfan Zhou, Kaizhu Huang, Cheng Cheng, Xuguang Wang, Amir Hussain,\n  and Xin Liu", "title": "FastAdaBelief: Improving Convergence Rate for Belief-based Adaptive\n  Optimizers by Exploiting Strong Convexity", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  AdaBelief, one of the current best optimizers, demonstrates superior\ngeneralization ability compared to the popular Adam algorithm by viewing the\nexponential moving average of observed gradients. AdaBelief is theoretically\nappealing in that it has a data-dependent $O(\\sqrt{T})$ regret bound when\nobjective functions are convex, where $T$ is a time horizon. It remains however\nan open problem whether the convergence rate can be further improved without\nsacrificing its generalization ability. %on how to exploit strong convexity to\nfurther improve the convergence rate of AdaBelief. To this end, we make a first\nattempt in this work and design a novel optimization algorithm called\nFastAdaBelief that aims to exploit its strong convexity in order to achieve an\neven faster convergence rate. In particular, by adjusting the step size that\nbetter considers strong convexity and prevents fluctuation, our proposed\nFastAdaBelief demonstrates excellent generalization ability as well as superior\nconvergence. As an important theoretical contribution, we prove that\nFastAdaBelief attains a data-dependant $O(\\log T)$ regret bound, which is\nsubstantially lower than AdaBelief. On the empirical side, we validate our\ntheoretical analysis with extensive experiments in both scenarios of strong and\nnon-strong convexity on three popular baseline models. Experimental results are\nvery encouraging: FastAdaBelief converges the quickest in comparison to all\nmainstream algorithms while maintaining an excellent generalization ability, in\ncases of both strong or non-strong convexity. FastAdaBelief is thus posited as\na new benchmark model for the research community.\n", "versions": [{"version": "v1", "created": "Wed, 28 Apr 2021 14:23:37 GMT"}, {"version": "v2", "created": "Tue, 8 Jun 2021 01:59:50 GMT"}], "update_date": "2021-06-09", "authors_parsed": [["Zhou", "Yangfan", ""], ["Huang", "Kaizhu", ""], ["Cheng", "Cheng", ""], ["Wang", "Xuguang", ""], ["Hussain", "Amir", ""], ["Liu", "Xin", ""]]}, {"id": "2104.13818", "submitter": "Ali Ramezani-Kebrya", "authors": "Ali Ramezani-Kebrya, Fartash Faghri, Ilya Markov, Vitalii Aksenov, Dan\n  Alistarh, Daniel M. Roy", "title": "NUQSGD: Provably Communication-efficient Data-parallel SGD via\n  Nonuniform Quantization", "comments": "This entry is redundant and was created in error. See\n  arXiv:1908.06077 for the latest version", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG math.OC stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  As the size and complexity of models and datasets grow, so does the need for\ncommunication-efficient variants of stochastic gradient descent that can be\ndeployed to perform parallel model training. One popular\ncommunication-compression method for data-parallel SGD is QSGD (Alistarh et\nal., 2017), which quantizes and encodes gradients to reduce communication\ncosts. The baseline variant of QSGD provides strong theoretical guarantees,\nhowever, for practical purposes, the authors proposed a heuristic variant which\nwe call QSGDinf, which demonstrated impressive empirical gains for distributed\ntraining of large neural networks. In this paper, we build on this work to\npropose a new gradient quantization scheme, and show that it has both stronger\ntheoretical guarantees than QSGD, and matches and exceeds the empirical\nperformance of the QSGDinf heuristic and of other compression methods.\n", "versions": [{"version": "v1", "created": "Wed, 28 Apr 2021 15:07:03 GMT"}, {"version": "v2", "created": "Sat, 1 May 2021 20:34:38 GMT"}], "update_date": "2021-05-05", "authors_parsed": [["Ramezani-Kebrya", "Ali", ""], ["Faghri", "Fartash", ""], ["Markov", "Ilya", ""], ["Aksenov", "Vitalii", ""], ["Alistarh", "Dan", ""], ["Roy", "Daniel M.", ""]]}, {"id": "2104.13877", "submitter": "Michael Zhang", "authors": "Michael R. Zhang, Tom Le Paine, Ofir Nachum, Cosmin Paduraru, George\n  Tucker, Ziyu Wang, Mohammad Norouzi", "title": "Autoregressive Dynamics Models for Offline Policy Evaluation and\n  Optimization", "comments": "ICLR 2021. 17 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Standard dynamics models for continuous control make use of feedforward\ncomputation to predict the conditional distribution of next state and reward\ngiven current state and action using a multivariate Gaussian with a diagonal\ncovariance structure. This modeling choice assumes that different dimensions of\nthe next state and reward are conditionally independent given the current state\nand action and may be driven by the fact that fully observable physics-based\nsimulation environments entail deterministic transition dynamics. In this\npaper, we challenge this conditional independence assumption and propose a\nfamily of expressive autoregressive dynamics models that generate different\ndimensions of the next state and reward sequentially conditioned on previous\ndimensions. We demonstrate that autoregressive dynamics models indeed\noutperform standard feedforward models in log-likelihood on heldout\ntransitions. Furthermore, we compare different model-based and model-free\noff-policy evaluation (OPE) methods on RL Unplugged, a suite of offline MuJoCo\ndatasets, and find that autoregressive dynamics models consistently outperform\nall baselines, achieving a new state-of-the-art. Finally, we show that\nautoregressive dynamics models are useful for offline policy optimization by\nserving as a way to enrich the replay buffer through data augmentation and\nimproving performance using model-based planning.\n", "versions": [{"version": "v1", "created": "Wed, 28 Apr 2021 16:48:44 GMT"}], "update_date": "2021-04-29", "authors_parsed": [["Zhang", "Michael R.", ""], ["Paine", "Tom Le", ""], ["Nachum", "Ofir", ""], ["Paduraru", "Cosmin", ""], ["Tucker", "George", ""], ["Wang", "Ziyu", ""], ["Norouzi", "Mohammad", ""]]}, {"id": "2104.13881", "submitter": "Jason Klusowski M", "authors": "Jason M. Klusowski", "title": "Universal Consistency of Decision Trees in High Dimensions", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG math.ST stat.TH", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  This paper shows that decision trees constructed with Classification and\nRegression Trees (CART) methodology are universally consistent in an additive\nmodel context, even when the number of predictor variables scales exponentially\nwith the sample size, under certain $1$-norm sparsity constraints. The\nconsistency is universal in the sense that there are no a priori assumptions on\nthe distribution of the predictor variables. Amazingly, this adaptivity to\n(approximate or exact) sparsity is achieved with a single tree, as opposed to\nwhat might be expected for an ensemble. Finally, we show that these qualitative\nproperties of individual trees are inherited by Breiman's random forests.\nAnother surprise is that consistency holds even when the \"mtry\" tuning\nparameter vanishes as a fraction of the number of predictor variables, thus\nspeeding up computation of the forest. A key step in the analysis is the\nestablishment of an oracle inequality, which precisely characterizes the\ngoodness-of-fit and complexity tradeoff for a misspecified model.\n", "versions": [{"version": "v1", "created": "Wed, 28 Apr 2021 16:59:03 GMT"}, {"version": "v2", "created": "Fri, 7 May 2021 16:04:44 GMT"}, {"version": "v3", "created": "Tue, 25 May 2021 14:32:02 GMT"}, {"version": "v4", "created": "Mon, 21 Jun 2021 01:31:29 GMT"}], "update_date": "2021-06-22", "authors_parsed": [["Klusowski", "Jason M.", ""]]}, {"id": "2104.14012", "submitter": "Leszek Szczecinski", "authors": "Leszek Szczecinski and Rapha\\\"elle Tihon", "title": "Simplified Kalman filter for online rating: one-fits-all approach", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG math.ST stat.TH", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  In this work, we deal with the problem of rating in sports, where the skills\nof the players/teams are inferred from the observed outcomes of the games. Our\nfocus is on the online rating algorithms which estimate the skills after each\nnew game by exploiting the probabilistic models of the relationship between the\nskills and the game outcome. We propose a Bayesian approach which may be seen\nas an approximate Kalman filter and which is generic in the sense that it can\nbe used with any skills-outcome model and can be applied in the individual --\nas well as in the group-sports. We show how the well-know algorithms (such as\nthe Elo, the Glicko, and the TrueSkill algorithms) may be seen as instances of\nthe one-fits-all approach we propose. In order to clarify the conditions under\nwhich the gains of the Bayesian approach over the simpler solutions can\nactually materialize, we critically compare the known and the new algorithms by\nmeans of numerical examples using the synthetic as well as the empirical data.\n", "versions": [{"version": "v1", "created": "Wed, 28 Apr 2021 20:44:10 GMT"}], "update_date": "2021-04-30", "authors_parsed": [["Szczecinski", "Leszek", ""], ["Tihon", "Rapha\u00eblle", ""]]}, {"id": "2104.14014", "submitter": "William Blanzeisky", "authors": "William Blanzeisky, P\\'adraig Cunningham", "title": "Algorithmic Factors Influencing Bias in Machine Learning", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  It is fair to say that many of the prominent examples of bias in Machine\nLearning (ML) arise from bias that is there in the training data. In fact, some\nwould argue that supervised ML algorithms cannot be biased, they reflect the\ndata on which they are trained. In this paper we demonstrate how ML algorithms\ncan misrepresent the training data through underestimation. We show how\nirreducible error, regularization and feature and class imbalance can\ncontribute to this underestimation. The paper concludes with a demonstration of\nhow the careful management of synthetic counterfactuals can ameliorate the\nimpact of this underestimation bias.\n", "versions": [{"version": "v1", "created": "Wed, 28 Apr 2021 20:45:41 GMT"}], "update_date": "2021-04-30", "authors_parsed": [["Blanzeisky", "William", ""], ["Cunningham", "P\u00e1draig", ""]]}, {"id": "2104.14033", "submitter": "Anirbit Mukherjee", "authors": "Anirbit Mukherjee", "title": "A Study of the Mathematics of Deep Learning", "comments": "(A) Our PAC-Bayes risk bounds on neural nets given in Section 6 here\n  does not yet occur in any other file on arXiv. (B) In our paper\n  arXiv:/2005.04211, there is a significantly improved version of Section 3.3\n  of this thesis", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG math.OC stat.AP stat.ML", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  \"Deep Learning\"/\"Deep Neural Nets\" is a technological marvel that is now\nincreasingly deployed at the cutting-edge of artificial intelligence tasks.\nThis dramatic success of deep learning in the last few years has been hinged on\nan enormous amount of heuristics and it has turned out to be a serious\nmathematical challenge to be able to rigorously explain them. In this thesis,\nsubmitted to the Department of Applied Mathematics and Statistics, Johns\nHopkins University we take several steps towards building strong theoretical\nfoundations for these new paradigms of deep-learning. In chapter 2 we show new\ncircuit complexity theorems for deep neural functions and prove classification\ntheorems about these function spaces which in turn lead to exact algorithms for\nempirical risk minimization for depth 2 ReLU nets. We also motivate a measure\nof complexity of neural functions to constructively establish the existence of\nhigh-complexity neural functions. In chapter 3 we give the first algorithm\nwhich can train a ReLU gate in the realizable setting in linear time in an\nalmost distribution free set up. In chapter 4 we give rigorous proofs towards\nexplaining the phenomenon of autoencoders being able to do sparse-coding. In\nchapter 5 we give the first-of-its-kind proofs of convergence for stochastic\nand deterministic versions of the widely used adaptive gradient deep-learning\nalgorithms, RMSProp and ADAM. This chapter also includes a detailed empirical\nstudy on autoencoders of the hyper-parameter values at which modern algorithms\nhave a significant advantage over classical acceleration based methods. In the\nlast chapter 6 we give new and improved PAC-Bayesian bounds for the risk of\nstochastic neural nets. This chapter also includes an experimental\ninvestigation revealing new geometric properties of the paths in weight space\nthat are traced out by the net during the training.\n", "versions": [{"version": "v1", "created": "Wed, 28 Apr 2021 22:05:54 GMT"}], "update_date": "2021-04-30", "authors_parsed": [["Mukherjee", "Anirbit", ""]]}, {"id": "2104.14072", "submitter": "Anthony Gruber", "authors": "Anthony Gruber, Max Gunzburger, Lili Ju, Yuankai Teng, Zhu Wang", "title": "Nonlinear Level Set Learning for Function Approximation on Sparse Data\n  with Applications to Parametric Differential Equations", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG cs.NA math.NA", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A dimension reduction method based on the \"Nonlinear Level set Learning\"\n(NLL) approach is presented for the pointwise prediction of functions which\nhave been sparsely sampled. Leveraging geometric information provided by the\nImplicit Function Theorem, the proposed algorithm effectively reduces the input\ndimension to the theoretical lower bound with minor accuracy loss, providing a\none-dimensional representation of the function which can be used for regression\nand sensitivity analysis. Experiments and applications are presented which\ncompare this modified NLL with the original NLL and the Active Subspaces (AS)\nmethod. While accommodating sparse input data, the proposed algorithm is shown\nto train quickly and provide a much more accurate and informative reduction\nthan either AS or the original NLL on two example functions with\nhigh-dimensional domains, as well as two state-dependent quantities depending\non the solutions to parametric differential equations.\n", "versions": [{"version": "v1", "created": "Thu, 29 Apr 2021 01:54:05 GMT"}], "update_date": "2021-04-30", "authors_parsed": [["Gruber", "Anthony", ""], ["Gunzburger", "Max", ""], ["Ju", "Lili", ""], ["Teng", "Yuankai", ""], ["Wang", "Zhu", ""]]}, {"id": "2104.14129", "submitter": "Jianfei Chen", "authors": "Jianfei Chen, Lianmin Zheng, Zhewei Yao, Dequan Wang, Ion Stoica,\n  Michael W. Mahoney, Joseph E. Gonzalez", "title": "ActNN: Reducing Training Memory Footprint via 2-Bit Activation\n  Compressed Training", "comments": "to be published in ICML 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The increasing size of neural network models has been critical for\nimprovements in their accuracy, but device memory is not growing at the same\nrate. This creates fundamental challenges for training neural networks within\nlimited memory environments. In this work, we propose ActNN, a memory-efficient\ntraining framework that stores randomly quantized activations for back\npropagation. We prove the convergence of ActNN for general network\narchitectures, and we characterize the impact of quantization on the\nconvergence via an exact expression for the gradient variance. Using our\ntheory, we propose novel mixed-precision quantization strategies that exploit\nthe activation's heterogeneity across feature dimensions, samples, and layers.\nThese techniques can be readily applied to existing dynamic graph frameworks,\nsuch as PyTorch, simply by substituting the layers. We evaluate ActNN on\nmainstream computer vision models for classification, detection, and\nsegmentation tasks. On all these tasks, ActNN compresses the activation to 2\nbits on average, with negligible accuracy loss. ActNN reduces the memory\nfootprint of the activation by 12x, and it enables training with a 6.6x to 14x\nlarger batch size.\n", "versions": [{"version": "v1", "created": "Thu, 29 Apr 2021 05:50:54 GMT"}, {"version": "v2", "created": "Tue, 6 Jul 2021 05:22:49 GMT"}], "update_date": "2021-07-07", "authors_parsed": [["Chen", "Jianfei", ""], ["Zheng", "Lianmin", ""], ["Yao", "Zhewei", ""], ["Wang", "Dequan", ""], ["Stoica", "Ion", ""], ["Mahoney", "Michael W.", ""], ["Gonzalez", "Joseph E.", ""]]}, {"id": "2104.14132", "submitter": "Samet Oymak", "authors": "Samet Oymak, Mingchen Li, Mahdi Soltanolkotabi", "title": "Generalization Guarantees for Neural Architecture Search with\n  Train-Validation Split", "comments": "to appear in ICML 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Neural Architecture Search (NAS) is a popular method for automatically\ndesigning optimized architectures for high-performance deep learning. In this\napproach, it is common to use bilevel optimization where one optimizes the\nmodel weights over the training data (lower-level problem) and various\nhyperparameters such as the configuration of the architecture over the\nvalidation data (upper-level problem). This paper explores the statistical\naspects of such problems with train-validation splits. In practice, the\nlower-level problem is often overparameterized and can easily achieve zero\nloss. Thus, a-priori it seems impossible to distinguish the right\nhyperparameters based on training loss alone which motivates a better\nunderstanding of the role of train-validation split. To this aim this work\nestablishes the following results. (1) We show that refined properties of the\nvalidation loss such as risk and hyper-gradients are indicative of those of the\ntrue test loss. This reveals that the upper-level problem helps select the most\ngeneralizable model and prevent overfitting with a near-minimal validation\nsample size. Importantly, this is established for continuous search spaces\nwhich are highly relevant for popular differentiable search schemes. (2) We\nestablish generalization bounds for NAS problems with an emphasis on an\nactivation search problem. When optimized with gradient-descent, we show that\nthe train-validation procedure returns the best (model, architecture) pair even\nif all architectures can perfectly fit the training data to achieve zero error.\n(3) Finally, we highlight rigorous connections between NAS, multiple kernel\nlearning, and low-rank matrix learning. The latter leads to novel algorithmic\ninsights where the solution of the upper problem can be accurately learned via\nefficient spectral methods to achieve near-minimal risk.\n", "versions": [{"version": "v1", "created": "Thu, 29 Apr 2021 06:11:00 GMT"}, {"version": "v2", "created": "Mon, 10 May 2021 09:49:40 GMT"}], "update_date": "2021-05-11", "authors_parsed": [["Oymak", "Samet", ""], ["Li", "Mingchen", ""], ["Soltanolkotabi", "Mahdi", ""]]}, {"id": "2104.14210", "submitter": "Indro Spinelli", "authors": "Indro Spinelli, Simone Scardapane, Amir Hussain, Aurelio Uncini", "title": "Biased Edge Dropout for Enhancing Fairness in Graph Representation\n  Learning", "comments": "Submitted to a journal for the peer-review process", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Graph representation learning has become a ubiquitous component in many\nscenarios, ranging from social network analysis to energy forecasting in smart\ngrids. In several applications, ensuring the fairness of the node (or graph)\nrepresentations with respect to some protected attributes is crucial for their\ncorrect deployment. Yet, fairness in graph deep learning remains\nunder-explored, with few solutions available. In particular, the tendency of\nsimilar nodes to cluster on several real-world graphs (i.e., homophily) can\ndramatically worsen the fairness of these procedures. In this paper, we propose\na biased edge dropout algorithm (FairDrop) to counter-act homophily and improve\nfairness in graph representation learning. FairDrop can be plugged in easily on\nmany existing algorithms, is efficient, adaptable, and can be combined with\nother fairness-inducing solutions. After describing the general algorithm, we\ndemonstrate its application on two benchmark tasks, specifically, as a random\nwalk model for producing node embeddings, and to a graph convolutional network\nfor link prediction. We prove that the proposed algorithm can successfully\nimprove the fairness of all models up to a small or negligible drop in\naccuracy, and compares favourably with existing state-of-the-art solutions. In\nan ablation study, we demonstrate that our algorithm can flexibly interpolate\nbetween biasing towards fairness and an unbiased edge dropout. Furthermore, to\nbetter evaluate the gains, we propose a new dyadic group definition to measure\nthe bias of a link prediction task when paired with group-based fairness\nmetrics. In particular, we extend the metric used to measure the bias in the\nnode embeddings to take into account the graph structure.\n", "versions": [{"version": "v1", "created": "Thu, 29 Apr 2021 08:59:36 GMT"}], "update_date": "2021-04-30", "authors_parsed": [["Spinelli", "Indro", ""], ["Scardapane", "Simone", ""], ["Hussain", "Amir", ""], ["Uncini", "Aurelio", ""]]}, {"id": "2104.14290", "submitter": "Ben Day", "authors": "Ben Day, Alexander Norcliffe, Jacob Moss, Pietro Li\\`o", "title": "Meta-learning using privileged information for dynamics", "comments": "Published as a workshop paper at the Learning to Learn and SimDL\n  workshops at ICLR 2021. 4 pages, 3 pages of appendices", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Neural ODE Processes approach the problem of meta-learning for dynamics using\na latent variable model, which permits a flexible aggregation of contextual\ninformation. This flexibility is inherited from the Neural Process framework\nand allows the model to aggregate sets of context observations of arbitrary\nsize into a fixed-length representation. In the physical sciences, we often\nhave access to structured knowledge in addition to raw observations of a\nsystem, such as the value of a conserved quantity or a description of an\nunderstood component. Taking advantage of the aggregation flexibility, we\nextend the Neural ODE Process model to use additional information within the\nLearning Using Privileged Information setting, and we validate our extension\nwith experiments showing improved accuracy and calibration on simulated\ndynamics tasks.\n", "versions": [{"version": "v1", "created": "Thu, 29 Apr 2021 12:18:02 GMT"}], "update_date": "2021-04-30", "authors_parsed": [["Day", "Ben", ""], ["Norcliffe", "Alexander", ""], ["Moss", "Jacob", ""], ["Li\u00f2", "Pietro", ""]]}, {"id": "2104.14371", "submitter": "Mehmet Caner", "authors": "Mehmet Caner", "title": "Generalized Linear Models with Structured Sparsity Estimators", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG econ.EM", "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/", "abstract": "  In this paper, we introduce structured sparsity estimators in Generalized\nLinear Models. Structured sparsity estimators in the least squares loss are\nintroduced by Stucky and van de Geer (2018) recently for fixed design and\nnormal errors. We extend their results to debiased structured sparsity\nestimators with Generalized Linear Model based loss. Structured sparsity\nestimation means penalized loss functions with a possible sparsity structure\nused in the chosen norm. These include weighted group lasso, lasso and norms\ngenerated from convex cones. The significant difficulty is that it is not clear\nhow to prove two oracle inequalities. The first one is for the initial\npenalized Generalized Linear Model estimator. Since it is not clear how a\nparticular feasible-weighted nodewise regression may fit in an oracle\ninequality for penalized Generalized Linear Model, we need a second oracle\ninequality to get oracle bounds for the approximate inverse for the sample\nestimate of second-order partial derivative of Generalized Linear Model.\n  Our contributions are fivefold: 1. We generalize the existing oracle\ninequality results in penalized Generalized Linear Models by proving the\nunderlying conditions rather than assuming them. One of the key issues is the\nproof of a sample one-point margin condition and its use in an oracle\ninequality. 2. Our results cover even non sub-Gaussian errors and regressors.\n3. We provide a feasible weighted nodewise regression proof which generalizes\nthe results in the literature from a simple l_1 norm usage to norms generated\nfrom convex cones. 4. We realize that norms used in feasible nodewise\nregression proofs should be weaker or equal to the norms in penalized\nGeneralized Linear Model loss. 5. We can debias the first step estimator via\ngetting an approximate inverse of the singular-sample second order partial\nderivative of Generalized Linear Model loss.\n", "versions": [{"version": "v1", "created": "Thu, 29 Apr 2021 14:31:01 GMT"}], "update_date": "2021-04-30", "authors_parsed": [["Caner", "Mehmet", ""]]}, {"id": "2104.14379", "submitter": "Weizhu Qian", "authors": "Weizhu Qian, Bowei Chen, Xiaowei Huang", "title": "Learning Robust Variational Information Bottleneck with Reference", "comments": "8 pages, 5 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  We propose a new approach to train a variational information bottleneck (VIB)\nthat improves its robustness to adversarial perturbations. Unlike the\ntraditional methods where the hard labels are usually used for the\nclassification task, we refine the categorical class information in the\ntraining phase with soft labels which are obtained from a pre-trained reference\nneural network and can reflect the likelihood of the original class labels. We\nalso relax the Gaussian posterior assumption in the VIB implementation by using\nthe mutual information neural estimation. Extensive experiments have been\nperformed with the MNIST and CIFAR-10 datasets, and the results show that our\nproposed approach significantly outperforms the benchmarked models.\n", "versions": [{"version": "v1", "created": "Thu, 29 Apr 2021 14:46:09 GMT"}], "update_date": "2021-04-30", "authors_parsed": [["Qian", "Weizhu", ""], ["Chen", "Bowei", ""], ["Huang", "Xiaowei", ""]]}, {"id": "2104.14401", "submitter": "Bertrand Iooss", "authors": "Bertrand Iooss (EDF R&D PRISME, GdR MASCOT-NUM)", "title": "Sample selection from a given dataset to validate machine learning\n  models", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The selection of a validation basis from a full dataset is often required in\nindustrial use of supervised machine learning algorithm. This validation basis\nwill serve to realize an independent evaluation of the machine learning model.\nTo select this basis, we propose to adopt a \"design of experiments\" point of\nview, by using statistical criteria. We show that the \"support points\" concept,\nbased on Maximum Mean Discrepancy criteria, is particularly relevant. An\nindustrial test case from the company EDF illustrates the practical interest of\nthe methodology.\n", "versions": [{"version": "v1", "created": "Tue, 27 Apr 2021 13:39:32 GMT"}], "update_date": "2021-04-30", "authors_parsed": [["Iooss", "Bertrand", "", "EDF R&D PRISME, GdR MASCOT-NUM"]]}, {"id": "2104.14421", "submitter": "Andrew Wilson", "authors": "Pavel Izmailov, Sharad Vikram, Matthew D. Hoffman, Andrew Gordon\n  Wilson", "title": "What Are Bayesian Neural Network Posteriors Really Like?", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The posterior over Bayesian neural network (BNN) parameters is extremely\nhigh-dimensional and non-convex. For computational reasons, researchers\napproximate this posterior using inexpensive mini-batch methods such as\nmean-field variational inference or stochastic-gradient Markov chain Monte\nCarlo (SGMCMC). To investigate foundational questions in Bayesian deep\nlearning, we instead use full-batch Hamiltonian Monte Carlo (HMC) on modern\narchitectures. We show that (1) BNNs can achieve significant performance gains\nover standard training and deep ensembles; (2) a single long HMC chain can\nprovide a comparable representation of the posterior to multiple shorter\nchains; (3) in contrast to recent studies, we find posterior tempering is not\nneeded for near-optimal performance, with little evidence for a \"cold\nposterior\" effect, which we show is largely an artifact of data augmentation;\n(4) BMA performance is robust to the choice of prior scale, and relatively\nsimilar for diagonal Gaussian, mixture of Gaussian, and logistic priors; (5)\nBayesian neural networks show surprisingly poor generalization under domain\nshift; (6) while cheaper alternatives such as deep ensembles and SGMCMC methods\ncan provide good generalization, they provide distinct predictive distributions\nfrom HMC. Notably, deep ensemble predictive distributions are similarly close\nto HMC as standard SGLD, and closer than standard variational inference.\n", "versions": [{"version": "v1", "created": "Thu, 29 Apr 2021 15:38:46 GMT"}], "update_date": "2021-04-30", "authors_parsed": [["Izmailov", "Pavel", ""], ["Vikram", "Sharad", ""], ["Hoffman", "Matthew D.", ""], ["Wilson", "Andrew Gordon", ""]]}, {"id": "2104.14429", "submitter": "Daniel Hesslow", "authors": "Daniel Hesslow, Alessandro Cappelli, Igor Carron, Laurent Daudet,\n  Rapha\\\"el Lafargue, Kilian M\\\"uller, Ruben Ohana, Gustave Pariente, and\n  Iacopo Poli", "title": "Photonic co-processors in HPC: using LightOn OPUs for Randomized\n  Numerical Linear Algebra", "comments": "Add \"This project has received funding from the European Union's\n  Horizon 2020 research and innovation programme under the Marie\n  Sklodowska-Curie grant agreement No 860830\"", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Randomized Numerical Linear Algebra (RandNLA) is a powerful class of methods,\nwidely used in High Performance Computing (HPC). RandNLA provides approximate\nsolutions to linear algebra functions applied to large signals, at reduced\ncomputational costs. However, the randomization step for dimensionality\nreduction may itself become the computational bottleneck on traditional\nhardware. Leveraging near constant-time linear random projections delivered by\nLightOn Optical Processing Units we show that randomization can be\nsignificantly accelerated, at negligible precision loss, in a wide range of\nimportant RandNLA algorithms, such as RandSVD or trace estimators.\n", "versions": [{"version": "v1", "created": "Thu, 29 Apr 2021 15:48:52 GMT"}, {"version": "v2", "created": "Fri, 7 May 2021 13:03:34 GMT"}], "update_date": "2021-05-10", "authors_parsed": [["Hesslow", "Daniel", ""], ["Cappelli", "Alessandro", ""], ["Carron", "Igor", ""], ["Daudet", "Laurent", ""], ["Lafargue", "Rapha\u00ebl", ""], ["M\u00fcller", "Kilian", ""], ["Ohana", "Ruben", ""], ["Pariente", "Gustave", ""], ["Poli", "Iacopo", ""]]}, {"id": "2104.14526", "submitter": "Tian Tong", "authors": "Tian Tong, Cong Ma, Ashley Prater-Bennette, Erin Tripp, Yuejie Chi", "title": "Scaling and Scalability: Provable Nonconvex Low-Rank Tensor Estimation\n  from Incomplete Measurements", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.IT eess.SP math.IT math.OC stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Tensors, which provide a powerful and flexible model for representing\nmulti-attribute data and multi-way interactions, play an indispensable role in\nmodern data science across various fields in science and engineering. A\nfundamental task is to faithfully recover the tensor from highly incomplete\nmeasurements in a statistically and computationally efficient manner.\nHarnessing the low-rank structure of tensors in the Tucker decomposition, this\npaper develops a scaled gradient descent (ScaledGD) algorithm to directly\nrecover the tensor factors with tailored spectral initializations, and shows\nthat it provably converges at a linear rate independent of the condition number\nof the ground truth tensor for two canonical problems -- tensor completion and\ntensor regression -- as soon as the sample size is above the order of $n^{3/2}$\nignoring other dependencies, where $n$ is the dimension of the tensor. This\nleads to an extremely scalable approach to low-rank tensor estimation compared\nwith prior art, which suffers from at least one of the following drawbacks:\nextreme sensitivity to ill-conditioning, high per-iteration costs in terms of\nmemory and computation, or poor sample complexity guarantees. To the best of\nour knowledge, ScaledGD is the first algorithm that achieves near-optimal\nstatistical and computational complexities simultaneously for low-rank tensor\ncompletion with the Tucker decomposition. Our algorithm highlights the power of\nappropriate preconditioning in accelerating nonconvex statistical estimation,\nwhere the iteration-varying preconditioners promote desirable invariance\nproperties of the trajectory with respect to the underlying symmetry in\nlow-rank tensor factorization.\n", "versions": [{"version": "v1", "created": "Thu, 29 Apr 2021 17:44:49 GMT"}], "update_date": "2021-05-03", "authors_parsed": [["Tong", "Tian", ""], ["Ma", "Cong", ""], ["Prater-Bennette", "Ashley", ""], ["Tripp", "Erin", ""], ["Chi", "Yuejie", ""]]}, {"id": "2104.14527", "submitter": "Virginie Do", "authors": "Virginie Do, Sam Corbett-Davies, Jamal Atif, Nicolas Usunier", "title": "Online certification of preference-based fairness for personalized\n  recommender systems", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI cs.CY stat.ML", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  We propose to assess the fairness of personalized recommender systems in the\nsense of envy-freeness: every (group of) user(s) should prefer their\nrecommendations to the recommendations of other (groups of) users. Auditing for\nenvy-freeness requires probing user preferences to detect potential blind\nspots, which may deteriorate recommendation performance. To control the cost of\nexploration, we propose an auditing algorithm based on pure exploration and\nconservative constraints in multi-armed bandits. We study, both theoretically\nand empirically, the trade-offs achieved by this algorithm.\n", "versions": [{"version": "v1", "created": "Thu, 29 Apr 2021 17:45:27 GMT"}], "update_date": "2021-05-03", "authors_parsed": [["Do", "Virginie", ""], ["Corbett-Davies", "Sam", ""], ["Atif", "Jamal", ""], ["Usunier", "Nicolas", ""]]}, {"id": "2104.14534", "submitter": "Raffaello Camoriano", "authors": "Diego Ferigo, Raffaello Camoriano, Paolo Maria Viceconte, Daniele\n  Calandriello, Silvio Traversaro, Lorenzo Rosasco and Daniele Pucci", "title": "On the Emergence of Whole-body Strategies from Humanoid Robot\n  Push-recovery Learning", "comments": "Co-first authors: Diego Ferigo and Raffaello Camoriano; 8 pages", "journal-ref": "IEEE Robotics and Automation Letters (RA-L) 2021", "doi": "10.1109/LRA.2021.3076955", "report-no": null, "categories": "cs.RO cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Balancing and push-recovery are essential capabilities enabling humanoid\nrobots to solve complex locomotion tasks. In this context, classical control\nsystems tend to be based on simplified physical models and hard-coded\nstrategies. Although successful in specific scenarios, this approach requires\ndemanding tuning of parameters and switching logic between\nspecifically-designed controllers for handling more general perturbations. We\napply model-free Deep Reinforcement Learning for training a general and robust\nhumanoid push-recovery policy in a simulation environment. Our method targets\nhigh-dimensional whole-body humanoid control and is validated on the iCub\nhumanoid. Reward components incorporating expert knowledge on humanoid control\nenable fast learning of several robust behaviors by the same policy, spanning\nthe entire body. We validate our method with extensive quantitative analyses in\nsimulation, including out-of-sample tasks which demonstrate policy robustness\nand generalization, both key requirements towards real-world robot deployment.\n", "versions": [{"version": "v1", "created": "Thu, 29 Apr 2021 17:49:20 GMT"}], "update_date": "2021-05-03", "authors_parsed": [["Ferigo", "Diego", ""], ["Camoriano", "Raffaello", ""], ["Viceconte", "Paolo Maria", ""], ["Calandriello", "Daniele", ""], ["Traversaro", "Silvio", ""], ["Rosasco", "Lorenzo", ""], ["Pucci", "Daniele", ""]]}, {"id": "2104.14538", "submitter": "Aditya Balu", "authors": "Aditya Balu, Sergio Botelho, Biswajit Khara, Vinay Rao, Chinmay Hegde,\n  Soumik Sarkar, Santi Adavani, Adarsh Krishnamurthy, Baskar\n  Ganapathysubramanian", "title": "Distributed Multigrid Neural Solvers on Megavoxel Domains", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/", "abstract": "  We consider the distributed training of large-scale neural networks that\nserve as PDE solvers producing full field outputs. We specifically consider\nneural solvers for the generalized 3D Poisson equation over megavoxel domains.\nA scalable framework is presented that integrates two distinct advances. First,\nwe accelerate training a large model via a method analogous to the multigrid\ntechnique used in numerical linear algebra. Here, the network is trained using\na hierarchy of increasing resolution inputs in sequence, analogous to the 'V',\n'W', 'F', and 'Half-V' cycles used in multigrid approaches. In conjunction with\nthe multi-grid approach, we implement a distributed deep learning framework\nwhich significantly reduces the time to solve. We show the scalability of this\napproach on both GPU (Azure VMs on Cloud) and CPU clusters (PSC Bridges2). This\napproach is deployed to train a generalized 3D Poisson solver that scales well\nto predict output full-field solutions up to the resolution of 512x512x512 for\na high dimensional family of inputs.\n", "versions": [{"version": "v1", "created": "Thu, 29 Apr 2021 17:53:22 GMT"}], "update_date": "2021-04-30", "authors_parsed": [["Balu", "Aditya", ""], ["Botelho", "Sergio", ""], ["Khara", "Biswajit", ""], ["Rao", "Vinay", ""], ["Hegde", "Chinmay", ""], ["Sarkar", "Soumik", ""], ["Adavani", "Santi", ""], ["Krishnamurthy", "Adarsh", ""], ["Ganapathysubramanian", "Baskar", ""]]}, {"id": "2104.14543", "submitter": "Tobias Haug", "authors": "Tobias Haug, M.S. Kim", "title": "Optimal training of variational quantum algorithms without barren\n  plateaus", "comments": "15 pages, 17 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "quant-ph cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Variational quantum algorithms (VQAs) promise efficient use of near-term\nquantum computers. However, training VQAs often requires an extensive amount of\ntime and suffers from the barren plateau problem where the magnitude of the\ngradients vanishes with increasing number of qubits. Here, we show how to\noptimally train VQAs for learning quantum states. Parameterized quantum\ncircuits can form Gaussian kernels, which we use to derive adaptive learning\nrates for gradient ascent. We introduce the generalized quantum natural\ngradient that features stability and optimized movement in parameter space.\nBoth methods together outperform other optimization routines in training VQAs.\nOur methods also excel at numerically optimizing driving protocols for quantum\ncontrol problems. The gradients of the VQA do not vanish when the fidelity\nbetween the initial state and the state to be learned is bounded from below. We\nidentify a VQA for quantum simulation with such a constraint that thus can be\ntrained free of barren plateaus. Finally, we propose the application of\nGaussian kernels for quantum machine learning.\n", "versions": [{"version": "v1", "created": "Thu, 29 Apr 2021 17:54:59 GMT"}, {"version": "v2", "created": "Wed, 12 May 2021 14:26:13 GMT"}, {"version": "v3", "created": "Wed, 23 Jun 2021 13:34:29 GMT"}], "update_date": "2021-06-24", "authors_parsed": [["Haug", "Tobias", ""], ["Kim", "M. S.", ""]]}, {"id": "2104.14562", "submitter": "Wenqiang Pu", "authors": "Wenqiang Pu, Shahana Ibrahim, Xiao Fu, and Mingyi Hong", "title": "Stochastic Mirror Descent for Low-Rank Tensor Decomposition Under\n  Non-Euclidean Losses", "comments": "Submitted to Transaction on Signal Processing", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG eess.SP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This work considers low-rank canonical polyadic decomposition (CPD) under a\nclass of non-Euclidean loss functions that frequently arise in statistical\nmachine learning and signal processing. These loss functions are often used for\ncertain types of tensor data, e.g., count and binary tensors, where the least\nsquares loss is considered unnatural.Compared to the least squares loss, the\nnon-Euclidean losses are generally more challenging to handle. Non-Euclidean\nCPD has attracted considerable interests and a number of prior works exist.\nHowever, pressing computational and theoretical challenges, such as scalability\nand convergence issues, still remain. This work offers a unified stochastic\nalgorithmic framework for large-scale CPD decomposition under a variety of\nnon-Euclidean loss functions. Our key contribution lies in a tensor fiber\nsampling strategy-based flexible stochastic mirror descent framework.\nLeveraging the sampling scheme and the multilinear algebraic structure of\nlow-rank tensors, the proposed lightweight algorithm ensures global convergence\nto a stationary point under reasonable conditions. Numerical results show that\nour framework attains promising non-Euclidean CPD performance. The proposed\nframework also exhibits substantial computational savings compared to\nstate-of-the-art methods.\n", "versions": [{"version": "v1", "created": "Thu, 29 Apr 2021 14:58:25 GMT"}], "update_date": "2021-05-03", "authors_parsed": [["Pu", "Wenqiang", ""], ["Ibrahim", "Shahana", ""], ["Fu", "Xiao", ""], ["Hong", "Mingyi", ""]]}, {"id": "2104.14581", "submitter": "Amanda Muyskens", "authors": "Amanda Muyskens, Benjamin Priest, Im\\`ene Goumiri, and Michael\n  Schneider", "title": "MuyGPs: Scalable Gaussian Process Hyperparameter Estimation Using Local\n  Cross-Validation", "comments": "15 pages, 5 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.CO stat.ML", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Gaussian processes (GPs) are non-linear probabilistic models popular in many\napplications. However, na\\\"ive GP realizations require quadratic memory to\nstore the covariance matrix and cubic computation to perform inference or\nevaluate the likelihood function. These bottlenecks have driven much investment\nin the development of approximate GP alternatives that scale to the large data\nsizes common in modern data-driven applications. We present in this manuscript\nMuyGPs, a novel efficient GP hyperparameter estimation method. MuyGPs builds\nupon prior methods that take advantage of the nearest neighbors structure of\nthe data, and uses leave-one-out cross-validation to optimize covariance\n(kernel) hyperparameters without realizing a possibly expensive likelihood. We\ndescribe our model and methods in detail, and compare our implementations\nagainst the state-of-the-art competitors in a benchmark spatial statistics\nproblem. We show that our method outperforms all known competitors both in\nterms of time-to-solution and the root mean squared error of the predictions.\n", "versions": [{"version": "v1", "created": "Thu, 29 Apr 2021 18:10:21 GMT"}], "update_date": "2021-05-03", "authors_parsed": [["Muyskens", "Amanda", ""], ["Priest", "Benjamin", ""], ["Goumiri", "Im\u00e8ne", ""], ["Schneider", "Michael", ""]]}, {"id": "2104.14672", "submitter": "Trevor Avant", "authors": "Trevor Avant and Kristi A. Morgansen", "title": "Analytical bounds on the local Lipschitz constants of ReLU networks", "comments": "11 pages, 8 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we determine analytical upper bounds on the local Lipschitz\nconstants of feedforward neural networks with ReLU activation functions. We do\nso by deriving Lipschitz constants and bounds for ReLU, affine-ReLU, and max\npooling functions, and combining the results to determine a network-wide bound.\nOur method uses several insights to obtain tight bounds, such as keeping track\nof the zero elements of each layer, and analyzing the composition of affine and\nReLU functions. Furthermore, we employ a careful computational approach which\nallows us to apply our method to large networks such as AlexNet and VGG-16. We\npresent several examples using different networks, which show how our local\nLipschitz bounds are tighter than the global Lipschitz bounds. We also show how\nour method can be applied to provide adversarial bounds for classification\nnetworks. These results show that our method produces the largest known bounds\non minimum adversarial perturbations for large networks such as AlexNet and\nVGG-16.\n", "versions": [{"version": "v1", "created": "Thu, 29 Apr 2021 21:57:47 GMT"}], "update_date": "2021-05-03", "authors_parsed": [["Avant", "Trevor", ""], ["Morgansen", "Kristi A.", ""]]}, {"id": "2104.14847", "submitter": "Samantha Biegel", "authors": "Samantha Biegel, Rafah El-Khatib, Luiz Otavio Vilas Boas Oliveira, Max\n  Baak, Nanne Aben", "title": "Active WeaSuL: Improving Weak Supervision with Active Learning", "comments": "Accepted to the ICLR 2021 Workshop on Weakly Supervised Learning", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  The availability of labelled data is one of the main limitations in machine\nlearning. We can alleviate this using weak supervision: a framework that uses\nexpert-defined rules $\\boldsymbol{\\lambda}$ to estimate probabilistic labels\n$p(y|\\boldsymbol{\\lambda})$ for the entire data set. These rules, however, are\ndependent on what experts know about the problem, and hence may be inaccurate\nor may fail to capture important parts of the problem-space. To mitigate this,\nwe propose Active WeaSuL: an approach that incorporates active learning into\nweak supervision. In Active WeaSuL, experts do not only define rules, but they\nalso iteratively provide the true label for a small set of points where the\nweak supervision model is most likely to be mistaken, which are then used to\nbetter estimate the probabilistic labels. In this way, the weak labels provide\na warm start, which active learning then improves upon. We make two\ncontributions: 1) a modification of the weak supervision loss function, such\nthat the expert-labelled data inform and improve the combination of weak\nlabels; and 2) the maxKL divergence sampling strategy, which determines for\nwhich data points expert labelling is most beneficial. Our experiments show\nthat when the budget for labelling data is limited (e.g. $\\leq 60$ data\npoints), Active WeaSuL outperforms weak supervision, active learning, and\ncompeting strategies, with only a handful of labelled data points. This makes\nActive WeaSuL ideal for situations where obtaining labelled data is difficult.\n", "versions": [{"version": "v1", "created": "Fri, 30 Apr 2021 08:58:26 GMT"}], "update_date": "2021-05-03", "authors_parsed": [["Biegel", "Samantha", ""], ["El-Khatib", "Rafah", ""], ["Oliveira", "Luiz Otavio Vilas Boas", ""], ["Baak", "Max", ""], ["Aben", "Nanne", ""]]}, {"id": "2104.14910", "submitter": "S\\'andor Baran", "authors": "S\\'andor Baran and \\'Agnes Baran", "title": "Calibration of wind speed ensemble forecasts for power generation", "comments": "15 pages, 5 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In the last decades wind power became the second largest energy source in the\nEU covering 16% of its electricity demand. However, due to its volatility,\naccurate short range wind power predictions are required for successful\nintegration of wind energy into the electrical grid. Accurate predictions of\nwind power require accurate hub height wind speed forecasts, where the state of\nthe art method is the probabilistic approach based on ensemble forecasts\nobtained from multiple runs of numerical weather prediction models.\nNonetheless, ensemble forecasts are often uncalibrated and might also be\nbiased, thus require some form of post-processing to improve their predictive\nperformance. We propose a novel flexible machine learning approach for\ncalibrating wind speed ensemble forecasts, which results in a truncated normal\npredictive distribution. In a case study based on 100m wind speed forecasts\nproduced by the operational ensemble prediction system of the Hungarian\nMeteorological Service, the forecast skill of this method is compared with the\npredictive performance of three different ensemble model output statistics\napproaches and the raw ensemble forecasts. We show that compared with the raw\nensemble, post-processing always improves the calibration of probabilistic and\naccuracy of point forecasts and from the four competing methods the novel\nmachine learning based approach results in the best overall performance.\n", "versions": [{"version": "v1", "created": "Fri, 30 Apr 2021 11:18:03 GMT"}, {"version": "v2", "created": "Mon, 28 Jun 2021 10:12:09 GMT"}], "update_date": "2021-06-29", "authors_parsed": [["Baran", "S\u00e1ndor", ""], ["Baran", "\u00c1gnes", ""]]}, {"id": "2104.14929", "submitter": "Matei Moldoveanu", "authors": "Matei Moldoveanu and Abdellatif Zaidi", "title": "On In-network learning. A Comparative Study with Federated and Split\n  Learning", "comments": "Submitted to the 2021 IEEE 22nd International Workshop on Signal\n  Processing Advances in Wireless Communications (SPAWC), special session on\n  Machine learning at the Edge", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.IT cs.LG math.IT", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we consider a problem in which distributively extracted\nfeatures are used for performing inference in wireless networks. We elaborate\non our proposed architecture, which we herein refer to as \"in-network\nlearning\", provide a suitable loss function and discuss its optimization using\nneural networks. We compare its performance with both Federated- and Split\nlearning; and show that this architecture offers both better accuracy and\nbandwidth savings.\n", "versions": [{"version": "v1", "created": "Fri, 30 Apr 2021 11:50:11 GMT"}], "update_date": "2021-05-11", "authors_parsed": [["Moldoveanu", "Matei", ""], ["Zaidi", "Abdellatif", ""]]}, {"id": "2104.14936", "submitter": "Lijun Sun Mr", "authors": "Xinyu Chen, Mengying Lei, Nicolas Saunier, Lijun Sun", "title": "Low-Rank Autoregressive Tensor Completion for Spatiotemporal Traffic\n  Data Imputation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Spatiotemporal traffic time series (e.g., traffic volume/speed) collected\nfrom sensing systems are often incomplete with considerable corruption and\nlarge amounts of missing values, preventing users from harnessing the full\npower of the data. Missing data imputation has been a long-standing research\ntopic and critical application for real-world intelligent transportation\nsystems. A widely applied imputation method is low-rank matrix/tensor\ncompletion; however, the low-rank assumption only preserves the global\nstructure while ignores the strong local consistency in spatiotemporal data. In\nthis paper, we propose a low-rank autoregressive tensor completion (LATC)\nframework by introducing \\textit{temporal variation} as a new regularization\nterm into the completion of a third-order (sensor $\\times$ time of day $\\times$\nday) tensor. The third-order tensor structure allows us to better capture the\nglobal consistency of traffic data, such as the inherent seasonality and\nday-to-day similarity. To achieve local consistency, we design the temporal\nvariation by imposing an AR($p$) model for each time series with coefficients\nas learnable parameters. Different from previous spatial and temporal\nregularization schemes, the minimization of temporal variation can better\ncharacterize temporal generative mechanisms beyond local smoothness, allowing\nus to deal with more challenging scenarios such \"blackout\" missing. To solve\nthe optimization problem in LATC, we introduce an alternating minimization\nscheme that estimates the low-rank tensor and autoregressive coefficients\niteratively. We conduct extensive numerical experiments on several real-world\ntraffic data sets, and our results demonstrate the effectiveness of LATC in\ndiverse missing scenarios.\n", "versions": [{"version": "v1", "created": "Fri, 30 Apr 2021 12:00:57 GMT"}], "update_date": "2021-05-03", "authors_parsed": [["Chen", "Xinyu", ""], ["Lei", "Mengying", ""], ["Saunier", "Nicolas", ""], ["Sun", "Lijun", ""]]}, {"id": "2104.14957", "submitter": "Lena Sembach", "authors": "Lena Sembach, Jan Pablo Burgard, Volker H. Schulz", "title": "A Riemannian Newton Trust-Region Method for Fitting Gaussian Mixture\n  Models", "comments": "30 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG math.OC stat.CO", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Gaussian Mixture Models are a powerful tool in Data Science and Statistics\nthat are mainly used for clustering and density approximation. The task of\nestimating the model parameters is in practice often solved by the Expectation\nMaximization (EM) algorithm which has its benefits in its simplicity and low\nper-iteration costs. However, the EM converges slowly if there is a large share\nof hidden information or overlapping clusters. Recent advances in Manifold\nOptimization for Gaussian Mixture Models have gained increasing interest. We\nintroduce a formula for the Riemannian Hessian for Gaussian Mixture Models. On\ntop, we propose a new Riemannian Newton Trust-Region method which outperforms\ncurrent approaches both in terms of runtime and number of iterations.\n", "versions": [{"version": "v1", "created": "Fri, 30 Apr 2021 12:48:32 GMT"}], "update_date": "2021-05-03", "authors_parsed": [["Sembach", "Lena", ""], ["Burgard", "Jan Pablo", ""], ["Schulz", "Volker H.", ""]]}, {"id": "2104.14958", "submitter": "Laura Davila Pena", "authors": "L. Davila-Pena, Ignacio Garc\\'ia-Jurado, B. Casas-M\\'endez", "title": "Assessment of the influence of features on a classification problem: an\n  application to COVID-19 patients", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG q-bio.PE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper deals with an important subject in classification problems\naddressed by machine learning techniques: the evaluation of the influence of\neach of the features on the classification of individuals. Specifically, a\nmeasure of that influence is introduced using the Shapley value of cooperative\ngames. In addition, an axiomatic characterisation of the proposed measure is\nprovided based on properties of efficiency and balanced contributions.\nFurthermore, some experiments have been designed in order to validate the\nappropriate performance of such measure. Finally, the methodology introduced is\napplied to a sample of COVID-19 patients to study the influence of certain\ndemographic or risk factors on various events of interest related to the\nevolution of the disease.\n", "versions": [{"version": "v1", "created": "Fri, 9 Apr 2021 20:02:05 GMT"}, {"version": "v2", "created": "Wed, 23 Jun 2021 12:19:20 GMT"}], "update_date": "2021-06-24", "authors_parsed": [["Davila-Pena", "L.", ""], ["Garc\u00eda-Jurado", "Ignacio", ""], ["Casas-M\u00e9ndez", "B.", ""]]}, {"id": "2104.14959", "submitter": "Luca Falorsi", "authors": "Luca Falorsi", "title": "Continuous normalizing flows on manifolds", "comments": "Master Thesis", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Normalizing flows are a powerful technique for obtaining reparameterizable\nsamples from complex multimodal distributions. Unfortunately, current\napproaches are only available for the most basic geometries and fall short when\nthe underlying space has a nontrivial topology, limiting their applicability\nfor most real-world data. Using fundamental ideas from differential geometry\nand geometric control theory, we describe how the recently introduced Neural\nODEs and continuous normalizing flows can be extended to arbitrary smooth\nmanifolds. We propose a general methodology for parameterizing vector fields on\nthese spaces and demonstrate how gradient-based learning can be performed.\nAdditionally, we provide a scalable unbiased estimator for the divergence in\nthis generalized setting. Experiments on a diverse selection of spaces\nempirically showcase the defined framework's ability to obtain\nreparameterizable samples from complex distributions.\n", "versions": [{"version": "v1", "created": "Sun, 14 Mar 2021 15:35:19 GMT"}], "update_date": "2021-05-03", "authors_parsed": [["Falorsi", "Luca", ""]]}, {"id": "2104.14962", "submitter": "Yuncong Yu", "authors": "Yuncong Yu, Dylan Kruyff, Tim Becker, Michael Behrisch", "title": "PSEUDo: Interactive Pattern Search in Multivariate Time Series with\n  Locality-Sensitive Hashing and Relevance Feedback", "comments": "11 pages including 2 pages for references, 10 figures including 1\n  teaser figure, sumbitted to IEEE VIS 2021, gitlab repository\n  https://git.science.uu.nl/vig/sublinear-algorithms-for-va/locality-sensitive-hashing-visual-analytics", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI cs.DS stat.ML", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  We present PSEUDo, an adaptive feature learning technique for exploring\nvisual patterns in multi-track sequential data. Our approach is designed with\nthe primary focus to overcome the uneconomic retraining requirements and\ninflexible representation learning in current deep learning-based systems.\nMulti-track time series data are generated on an unprecedented scale due to\nincreased sensors and data storage. These datasets hold valuable patterns, like\nin neuromarketing, where researchers try to link patterns in multivariate\nsequential data from physiological sensors to the purchase behavior of products\nand services. But a lack of ground truth and high variance make automatic\npattern detection unreliable. Our advancements are based on a novel query-aware\nlocality-sensitive hashing technique to create a feature-based representation\nof multivariate time series windows. Most importantly, our algorithm features\nsub-linear training and inference time. We can even accomplish both the\nmodeling and comparison of 10,000 different 64-track time series, each with 100\ntime steps (a typical EEG dataset) under 0.8 seconds. This performance gain\nallows for a rapid relevance feedback-driven adaption of the underlying pattern\nsimilarity model and enables the user to modify the speed-vs-accuracy trade-off\ngradually. We demonstrate superiority of PSEUDo in terms of efficiency,\naccuracy, and steerability through a quantitative performance comparison and a\nqualitative visual quality comparison to the state-of-the-art algorithms in the\nfield. Moreover, we showcase the usability of PSEUDo through a case study\ndemonstrating our visual pattern retrieval concepts in a large meteorological\ndataset. We find that our adaptive models can accurately capture the user's\nnotion of similarity and allow for an understandable exploratory visual pattern\nretrieval in large multivariate time series datasets.\n", "versions": [{"version": "v1", "created": "Fri, 30 Apr 2021 13:00:44 GMT"}, {"version": "v2", "created": "Mon, 10 May 2021 15:04:52 GMT"}], "update_date": "2021-05-11", "authors_parsed": [["Yu", "Yuncong", ""], ["Kruyff", "Dylan", ""], ["Becker", "Tim", ""], ["Behrisch", "Michael", ""]]}, {"id": "2104.14977", "submitter": "Yikun Zhang", "authors": "Yikun Zhang and Yen-Chi Chen", "title": "Linear Convergence of the Subspace Constrained Mean Shift Algorithm:\n  From Euclidean to Directional Data", "comments": "75 pages, 9 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML math.ST stat.ME stat.TH", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  This paper studies linear convergence of the subspace constrained mean shift\n(SCMS) algorithm, a well-known algorithm for identifying a density ridge\ndefined by a kernel density estimator. By arguing that the SCMS algorithm is a\nspecial variant of a subspace constrained gradient ascent (SCGA) algorithm with\nan adaptive step size, we derive linear convergence of such SCGA algorithm.\nWhile the existing research focuses mainly on density ridges in the Euclidean\nspace, we generalize density ridges and the SCMS algorithm to directional data.\nIn particular, we establish the stability theorem of density ridges with\ndirectional data and prove the linear convergence of our proposed directional\nSCMS algorithm.\n", "versions": [{"version": "v1", "created": "Thu, 29 Apr 2021 01:46:35 GMT"}], "update_date": "2021-05-03", "authors_parsed": [["Zhang", "Yikun", ""], ["Chen", "Yen-Chi", ""]]}, {"id": "2104.15010", "submitter": "Johannes Cornelius Schoeman", "authors": "J. C. Schoeman, C. E. van Daalen, J. A. du Preez", "title": "Degenerate Gaussian factors for probabilistic inference", "comments": "Preprint submitted to International Journal of Approximate Reasoning", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/", "abstract": "  In this paper, we propose a parametrised factor that enables inference on\nGaussian networks where linear dependencies exist among the random variables.\nOur factor representation is a generalisation of traditional Gaussian\nparametrisations where the positive-definite constraint (of covariance and\nprecision matrices) has been relaxed. For this purpose, we derive various\nstatistical operations and results (such as marginalisation, multiplication and\naffine transformations of random variables) which extend the capabilities of\nGaussian factors to these degenerate settings. By using this principled factor\ndefinition, degeneracies can be accommodated accurately and automatically at\nlittle additional computational cost. As illustration, we apply our methodology\nto a representative example involving recursive state estimation of cooperative\nmobile robots.\n", "versions": [{"version": "v1", "created": "Fri, 30 Apr 2021 13:58:29 GMT"}], "update_date": "2021-05-03", "authors_parsed": [["Schoeman", "J. C.", ""], ["van Daalen", "C. E.", ""], ["Preez", "J. A. du", ""]]}, {"id": "2104.15046", "submitter": "Simen Eide", "authors": "Simen Eide, David S. Leslie, Arnoldo Frigessi", "title": "Dynamic Slate Recommendation with Gated Recurrent Units and Thompson\n  Sampling", "comments": "The code and the data used in the article are available in the\n  following repository: https://github.com/finn-no/recsys-slates-dataset", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  We consider the problem of recommending relevant content to users of an\ninternet platform in the form of lists of items, called slates. We introduce a\nvariational Bayesian Recurrent Neural Net recommender system that acts on time\nseries of interactions between the internet platform and the user, and which\nscales to real world industrial situations. The recommender system is tested\nboth online on real users, and on an offline dataset collected from a Norwegian\nweb-based marketplace, FINN.no, that is made public for research. This is one\nof the first publicly available datasets which includes all the slates that are\npresented to users as well as which items (if any) in the slates were clicked\non. Such a data set allows us to move beyond the common assumption that\nimplicitly assumes that users are considering all possible items at each\ninteraction. Instead we build our likelihood using the items that are actually\nin the slate, and evaluate the strengths and weaknesses of both approaches\ntheoretically and in experiments. We also introduce a hierarchical prior for\nthe item parameters based on group memberships. Both item parameters and user\npreferences are learned probabilistically. Furthermore, we combine our model\nwith bandit strategies to ensure learning, and introduce `in-slate Thompson\nSampling' which makes use of the slates to maximise explorative opportunities.\nWe show experimentally that explorative recommender strategies perform on par\nor above their greedy counterparts. Even without making use of exploration to\nlearn more effectively, click rates increase simply because of improved\ndiversity in the recommended slates.\n", "versions": [{"version": "v1", "created": "Fri, 30 Apr 2021 15:16:35 GMT"}], "update_date": "2021-05-03", "authors_parsed": [["Eide", "Simen", ""], ["Leslie", "David S.", ""], ["Frigessi", "Arnoldo", ""]]}, {"id": "2104.15061", "submitter": "Haoxi Zhan", "authors": "Haoxi Zhan, Xiaobing Pei", "title": "Black-box Gradient Attack on Graph Neural Networks: Deeper Insights in\n  Graph-based Attack and Defense", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI stat.ML", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Graph Neural Networks (GNNs) have received significant attention due to their\nstate-of-the-art performance on various graph representation learning tasks.\nHowever, recent studies reveal that GNNs are vulnerable to adversarial attacks,\ni.e. an attacker is able to fool the GNNs by perturbing the graph structure or\nnode features deliberately. While being able to successfully decrease the\nperformance of GNNs, most existing attacking algorithms require access to\neither the model parameters or the training data, which is not practical in the\nreal world.\n  In this paper, we develop deeper insights into the Mettack algorithm, which\nis a representative grey-box attacking method, and then we propose a\ngradient-based black-box attacking algorithm. Firstly, we show that the Mettack\nalgorithm will perturb the edges unevenly, thus the attack will be highly\ndependent on a specific training set. As a result, a simple yet useful strategy\nto defense against Mettack is to train the GNN with the validation set.\nSecondly, to overcome the drawbacks, we propose the Black-Box Gradient Attack\n(BBGA) algorithm. Extensive experiments demonstrate that out proposed method is\nable to achieve stable attack performance without accessing the training sets\nof the GNNs. Further results shows that our proposed method is also applicable\nwhen attacking against various defense methods.\n", "versions": [{"version": "v1", "created": "Fri, 30 Apr 2021 15:30:47 GMT"}], "update_date": "2021-05-03", "authors_parsed": [["Zhan", "Haoxi", ""], ["Pei", "Xiaobing", ""]]}, {"id": "2104.15079", "submitter": "Aldo Glielmo Mr.", "authors": "Aldo Glielmo, Claudio Zeni, Bingqing Cheng, Gabor Csanyi, Alessandro\n  Laio", "title": "Ranking the information content of distance measures", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.IT cs.LG math.IT", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Real-world data typically contain a large number of features that are often\nheterogeneous in nature, relevance, and also units of measure. When assessing\nthe similarity between data points, one can build various distance measures\nusing subsets of these features. Using the fewest features but still retaining\nsufficient information about the system is crucial in many statistical learning\napproaches, particularly when data are sparse. We introduce a statistical test\nthat can assess the relative information retained when using two different\ndistance measures, and determine if they are equivalent, independent, or if one\nis more informative than the other. This in turn allows finding the most\ninformative distance measure out of a pool of candidates. The approach is\napplied to find the most relevant policy variables for controlling the Covid-19\nepidemic and to find compact yet informative representations of atomic\nstructures, but its potential applications are wide ranging in many branches of\nscience.\n", "versions": [{"version": "v1", "created": "Fri, 30 Apr 2021 15:57:57 GMT"}], "update_date": "2021-05-03", "authors_parsed": [["Glielmo", "Aldo", ""], ["Zeni", "Claudio", ""], ["Cheng", "Bingqing", ""], ["Csanyi", "Gabor", ""], ["Laio", "Alessandro", ""]]}, {"id": "2104.15090", "submitter": "Sebastian H\\\"onel", "authors": "Sebastian H\\\"onel", "title": "Technical Reports Compilation: Detecting the Fire Drill anti-pattern\n  using Source Code and issue-tracking data", "comments": "208 pages", "journal-ref": null, "doi": "10.13140/RG.2.2.35805.33766/2", "report-no": null, "categories": "cs.SE stat.CO stat.ML", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Detecting the presence of project management anti-patterns (AP) currently\nrequires experts on the matter and is an expensive endeavor. Worse, experts may\nintroduce their individual subjectivity or bias. Using the Fire Drill AP, we\nfirst introduce a novel way to translate descriptions into detectable AP that\nare comprised of arbitrary metrics and events such as logged time or\nmaintenance activities, which are mined from the underlying source code or\nissue-tracking data, thus making the description objective as it becomes\ndata-based. Secondly, we demonstrate a novel method to quantify and score the\ndeviations of real-world projects to data-based AP descriptions. Using nine\nreal-world projects that exhibit a Fire Drill to some degree, we show how to\nfurther enhance the translated AP. The ground truth in these projects was\nextracted from two individual experts and consensus was found between them. Our\nevaluation spans three kinds of pattern, where the first is purely derived from\ndescription, the second type is enhanced by data, and the third kind is derived\nfrom data only. The Fire Drill AP as translated from description only for\neither, source code- or issue-tracking-based detection, shows weak potential of\nconfidently detecting the presence of the anti-pattern in a project. Enriching\nthe AP with data from real-world projects significantly improves detection.\nUsing patterns derived from data only leads to almost perfect correlations of\nthe scores with the ground truth. Some APs share symptoms with the Fire Drill\nAP, and we conclude that the presence of similar patterns is most certainly\ndetectable. Furthermore, any pattern that can be characteristically modeled\nusing the proposed approach is potentially well detectable.\n", "versions": [{"version": "v1", "created": "Fri, 30 Apr 2021 16:16:32 GMT"}, {"version": "v2", "created": "Mon, 3 May 2021 14:52:43 GMT"}, {"version": "v3", "created": "Thu, 24 Jun 2021 12:25:41 GMT"}, {"version": "v4", "created": "Tue, 29 Jun 2021 16:22:47 GMT"}], "update_date": "2021-07-02", "authors_parsed": [["H\u00f6nel", "Sebastian", ""]]}]