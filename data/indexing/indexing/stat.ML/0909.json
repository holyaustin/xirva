[{"id": "0909.0400", "submitter": "Noam Shental", "authors": "Noam Shental, Amnon Amir and Or Zuk", "title": "Rare-Allele Detection Using Compressed Se(que)nsing", "comments": "29 pages, 11 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "q-bio.GN cs.IT cs.LG math.IT q-bio.QM stat.AP stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Detection of rare variants by resequencing is important for the\nidentification of individuals carrying disease variants. Rapid sequencing by\nnew technologies enables low-cost resequencing of target regions, although it\nis still prohibitive to test more than a few individuals. In order to improve\ncost trade-offs, it has recently been suggested to apply pooling designs which\nenable the detection of carriers of rare alleles in groups of individuals.\nHowever, this was shown to hold only for a relatively low number of individuals\nin a pool, and requires the design of pooling schemes for particular cases.\n  We propose a novel pooling design, based on a compressed sensing approach,\nwhich is both general, simple and efficient. We model the experimental\nprocedure and show via computer simulations that it enables the recovery of\nrare allele carriers out of larger groups than were possible before, especially\nin situations where high coverage is obtained for each individual.\n  Our approach can also be combined with barcoding techniques to enhance\nperformance and provide a feasible solution based on current resequencing\ncosts. For example, when targeting a small enough genomic region (~100\nbase-pairs) and using only ~10 sequencing lanes and ~10 distinct barcodes, one\ncan recover the identity of 4 rare allele carriers out of a population of over\n4000 individuals.\n", "versions": [{"version": "v1", "created": "Wed, 2 Sep 2009 13:25:48 GMT"}], "update_date": "2009-09-03", "authors_parsed": [["Shental", "Noam", ""], ["Amir", "Amnon", ""], ["Zuk", "Or", ""]]}, {"id": "0909.0934", "submitter": "Xin Gao Dr.", "authors": "Xin Gao, Daniel Q. Pu, Yuehua Wu and Hong Xu", "title": "Tuning parameter selection for penalized likelihood estimation of\n  inverse covariance matrix", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In a Gaussian graphical model, the conditional independence between two\nvariables are characterized by the corresponding zero entries in the inverse\ncovariance matrix. Maximum likelihood method using the smoothly clipped\nabsolute deviation (SCAD) penalty (Fan and Li, 2001) and the adaptive LASSO\npenalty (Zou, 2006) have been proposed in literature. In this article, we\nestablish the result that using Bayesian information criterion (BIC) to select\nthe tuning parameter in penalized likelihood estimation with both types of\npenalties can lead to consistent graphical model selection. We compare the\nempirical performance of BIC with cross validation method and demonstrate the\nadvantageous performance of BIC criterion for tuning parameter selection\nthrough simulation studies.\n", "versions": [{"version": "v1", "created": "Fri, 4 Sep 2009 18:25:22 GMT"}], "update_date": "2009-09-07", "authors_parsed": [["Gao", "Xin", ""], ["Pu", "Daniel Q.", ""], ["Wu", "Yuehua", ""], ["Xu", "Hong", ""]]}, {"id": "0909.0991", "submitter": "Marco Cuturi", "authors": "Marco Cuturi", "title": "Kernels for Measures Defined on the Gram Matrix of their Support", "comments": "Work in progress, in particular lacks references to very recent\n  literature (2007/2008/2009) as this paper was submitted and rejected @ nips\n  some time ago", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present in this work a new family of kernels to compare positive measures\non arbitrary spaces $\\Xcal$ endowed with a positive kernel $\\kappa$, which\ntranslates naturally into kernels between histograms or clouds of points. We\nfirst cover the case where $\\Xcal$ is Euclidian, and focus on kernels which\ntake into account the variance matrix of the mixture of two measures to compute\ntheir similarity. The kernels we define are semigroup kernels in the sense that\nthey only use the sum of two measures to compare them, and spectral in the\nsense that they only use the eigenspectrum of the variance matrix of this\nmixture. We show that such a family of kernels has close bonds with the laplace\ntransforms of nonnegative-valued functions defined on the cone of positive\nsemidefinite matrices, and we present some closed formulas that can be derived\nas special cases of such integral expressions. By focusing further on functions\nwhich are invariant to the addition of a null eigenvalue to the spectrum of the\nvariance matrix, we can define kernels between atomic measures on arbitrary\nspaces $\\Xcal$ endowed with a kernel $\\kappa$ by using directly the eigenvalues\nof the centered Gram matrix of the joined support of the compared measures. We\nprovide explicit formulas suited for applications and present preliminary\nexperiments to illustrate the interest of the approach.\n", "versions": [{"version": "v1", "created": "Mon, 7 Sep 2009 15:26:47 GMT"}], "update_date": "2009-09-08", "authors_parsed": [["Cuturi", "Marco", ""]]}, {"id": "0909.1234", "submitter": "Rodrigo Labouriau", "authors": "Gabriel C. G. de Abreu, Rodrigo Labouriau and David Edwards", "title": "High-dimensional Graphical Model Search with gRapHD R Package", "comments": "20 pages with 8 figures", "journal-ref": "Journal of Statistical Software, Vol. 37, Issue 1, Nov. 2010", "doi": "10.18637/jss.V037", "report-no": null, "categories": "stat.ML stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper presents the R package gRapHD for efficient selection of\nhigh-dimensional undirected graphical models. The package provides tools for\nselecting trees, forests and decomposable models minimizing information\ncriteria such as AIC or BIC, and for displaying the independence graphs of the\nmodels. It has also some useful tools for analysing graphical structures. It\nsupports the use of discrete, continuous, or both types of variables\nsimultaneously.\n", "versions": [{"version": "v1", "created": "Mon, 7 Sep 2009 12:20:10 GMT"}, {"version": "v2", "created": "Tue, 8 Sep 2009 10:29:31 GMT"}, {"version": "v3", "created": "Tue, 9 Feb 2010 10:33:07 GMT"}, {"version": "v4", "created": "Wed, 22 Sep 2010 08:36:10 GMT"}], "update_date": "2019-09-24", "authors_parsed": [["de Abreu", "Gabriel C. G.", ""], ["Labouriau", "Rodrigo", ""], ["Edwards", "David", ""]]}, {"id": "0909.1373", "submitter": "Seyoung Kim", "authors": "Seyoung Kim, Eric P. Xing", "title": "Tree-guided group lasso for multi-response regression with structured\n  sparsity, with an application to eQTL mapping", "comments": "Published in at http://dx.doi.org/10.1214/12-AOAS549 the Annals of\n  Applied Statistics (http://www.imstat.org/aoas/) by the Institute of\n  Mathematical Statistics (http://www.imstat.org)", "journal-ref": "Annals of Applied Statistics 2012, Vol. 6, No. 3, 1095-1117", "doi": "10.1214/12-AOAS549", "report-no": "IMS-AOAS-AOAS549", "categories": "stat.ML q-bio.GN q-bio.QM stat.AP stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider the problem of estimating a sparse multi-response regression\nfunction, with an application to expression quantitative trait locus (eQTL)\nmapping, where the goal is to discover genetic variations that influence\ngene-expression levels. In particular, we investigate a shrinkage technique\ncapable of capturing a given hierarchical structure over the responses, such as\na hierarchical clustering tree with leaf nodes for responses and internal nodes\nfor clusters of related responses at multiple granularity, and we seek to\nleverage this structure to recover covariates relevant to each\nhierarchically-defined cluster of responses. We propose a tree-guided group\nlasso, or tree lasso, for estimating such structured sparsity under\nmulti-response regression by employing a novel penalty function constructed\nfrom the tree. We describe a systematic weighting scheme for the overlapping\ngroups in the tree-penalty such that each regression coefficient is penalized\nin a balanced manner despite the inhomogeneous multiplicity of group\nmemberships of the regression coefficients due to overlaps among groups. For\nefficient optimization, we employ a smoothing proximal gradient method that was\noriginally developed for a general class of structured-sparsity-inducing\npenalties. Using simulated and yeast data sets, we demonstrate that our method\nshows a superior performance in terms of both prediction errors and recovery of\ntrue sparsity patterns, compared to other methods for learning a\nmultivariate-response regression.\n", "versions": [{"version": "v1", "created": "Tue, 8 Sep 2009 02:29:12 GMT"}, {"version": "v2", "created": "Fri, 11 Sep 2009 19:36:16 GMT"}, {"version": "v3", "created": "Fri, 28 Sep 2012 11:17:56 GMT"}], "update_date": "2012-10-01", "authors_parsed": [["Kim", "Seyoung", ""], ["Xing", "Eric P.", ""]]}, {"id": "0909.1418", "submitter": "Mugizi Rwebangira", "authors": "Mugizi Rwebangira", "title": "On Ranking Senators By Their Votes", "comments": "6 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The problem of ranking a set of objects given some measure of similarity is\none of the most basic in machine learning. Recently Agarwal proposed a method\nbased on techniques in semi-supervised learning utilizing the graph Laplacian.\nIn this work we consider a novel application of this technique to ranking\nbinary choice data and apply it specifically to ranking US Senators by their\nideology.\n", "versions": [{"version": "v1", "created": "Tue, 8 Sep 2009 08:03:09 GMT"}], "update_date": "2009-09-09", "authors_parsed": [["Rwebangira", "Mugizi", ""]]}, {"id": "0909.1440", "submitter": "Rodolphe Jenatton", "authors": "Rodolphe Jenatton (INRIA Rocquencourt), Guillaume Obozinski (INRIA\n  Rocquencourt), Francis Bach (INRIA Rocquencourt)", "title": "Structured Sparse Principal Component Analysis", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present an extension of sparse PCA, or sparse dictionary learning, where\nthe sparsity patterns of all dictionary elements are structured and constrained\nto belong to a prespecified set of shapes. This \\emph{structured sparse PCA} is\nbased on a structured regularization recently introduced by [1]. While\nclassical sparse priors only deal with \\textit{cardinality}, the regularization\nwe use encodes higher-order information about the data. We propose an efficient\nand simple optimization procedure to solve this problem. Experiments with two\npractical tasks, face recognition and the study of the dynamics of a protein\ncomplex, demonstrate the benefits of the proposed structured approach over\nunstructured approaches.\n", "versions": [{"version": "v1", "created": "Tue, 8 Sep 2009 13:42:35 GMT"}], "update_date": "2009-09-09", "authors_parsed": [["Jenatton", "Rodolphe", "", "INRIA Rocquencourt"], ["Obozinski", "Guillaume", "", "INRIA\n  Rocquencourt"], ["Bach", "Francis", "", "INRIA Rocquencourt"]]}, {"id": "0909.1685", "submitter": "Marco Scutari", "authors": "Marco Scutari", "title": "Structure Variability in Bayesian Networks", "comments": "21 pages, 4 figures", "journal-ref": "merged and published as part of Bayesian Analysis 2013, 8(3),\n  505-532", "doi": null, "report-no": "Working Paper 13 - 2009, Department of Statistical Sciences,\n  University of Padova", "categories": "stat.ME math.ST stat.ML stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The structure of a Bayesian network encodes most of the information about the\nprobability distribution of the data, which is uniquely identified given some\ngeneral distributional assumptions. Therefore it's important to study the\nvariability of its network structure, which can be used to compare the\nperformance of different learning algorithms and to measure the strength of any\narbitrary subset of arcs.\n  In this paper we will introduce some descriptive statistics and the\ncorresponding parametric and Monte Carlo tests on the undirected graph\nunderlying the structure of a Bayesian network, modeled as a multivariate\nBernoulli random variable.\n", "versions": [{"version": "v1", "created": "Wed, 9 Sep 2009 11:52:12 GMT"}, {"version": "v2", "created": "Sun, 18 Oct 2009 16:22:16 GMT"}, {"version": "v3", "created": "Fri, 13 Nov 2009 15:14:13 GMT"}, {"version": "v4", "created": "Sun, 16 May 2010 23:13:11 GMT"}], "update_date": "2014-10-15", "authors_parsed": [["Scutari", "Marco", ""]]}, {"id": "0909.1884", "submitter": "Sylvain Arlot", "authors": "Sylvain Arlot (LIENS, INRIA Paris - Rocquencourt), Francis Bach\n  (LIENS, INRIA Paris - Rocquencourt)", "title": "Data-driven calibration of linear estimators with minimal penalties", "comments": "Advances in Neural Information Processing Systems (NIPS 2009),\n  Vancouver : Canada (2009)", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.ME stat.ML stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper tackles the problem of selecting among several linear estimators\nin non-parametric regression; this includes model selection for linear\nregression, the choice of a regularization parameter in kernel ridge\nregression, spline smoothing or locally weighted regression, and the choice of\na kernel in multiple kernel learning. We propose a new algorithm which first\nestimates consistently the variance of the noise, based upon the concept of\nminimal penalty, which was previously introduced in the context of model\nselection. Then, plugging our variance estimate in Mallows' $C_L$ penalty is\nproved to lead to an algorithm satisfying an oracle inequality. Simulation\nexperiments with kernel ridge regression and multiple kernel learning show that\nthe proposed algorithm often improves significantly existing calibration\nprocedures such as generalized cross-validation.\n", "versions": [{"version": "v1", "created": "Thu, 10 Sep 2009 08:14:16 GMT"}, {"version": "v2", "created": "Tue, 13 Sep 2011 07:13:39 GMT"}], "update_date": "2011-09-15", "authors_parsed": [["Arlot", "Sylvain", "", "LIENS, INRIA Paris - Rocquencourt"], ["Bach", "Francis", "", "LIENS, INRIA Paris - Rocquencourt"]]}, {"id": "0909.1933", "submitter": "Liva Ralaivola", "authors": "Liva Ralaivola (LIF), Marie Szafranski (IBISC), Guillaume Stempfel\n  (LIF)", "title": "Chromatic PAC-Bayes Bounds for Non-IID Data: Applications to Ranking and\n  Stationary $\\beta$-Mixing Processes", "comments": "Long version of the AISTATS 09 paper:\n  http://jmlr.csail.mit.edu/proceedings/papers/v5/ralaivola09a/ralaivola09a.pdf", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG math.ST stat.ML stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Pac-Bayes bounds are among the most accurate generalization bounds for\nclassifiers learned from independently and identically distributed (IID) data,\nand it is particularly so for margin classifiers: there have been recent\ncontributions showing how practical these bounds can be either to perform model\nselection (Ambroladze et al., 2007) or even to directly guide the learning of\nlinear classifiers (Germain et al., 2009). However, there are many practical\nsituations where the training data show some dependencies and where the\ntraditional IID assumption does not hold. Stating generalization bounds for\nsuch frameworks is therefore of the utmost interest, both from theoretical and\npractical standpoints. In this work, we propose the first - to the best of our\nknowledge - Pac-Bayes generalization bounds for classifiers trained on data\nexhibiting interdependencies. The approach undertaken to establish our results\nis based on the decomposition of a so-called dependency graph that encodes the\ndependencies within the data, in sets of independent data, thanks to graph\nfractional covers. Our bounds are very general, since being able to find an\nupper bound on the fractional chromatic number of the dependency graph is\nsufficient to get new Pac-Bayes bounds for specific settings. We show how our\nresults can be used to derive bounds for ranking statistics (such as Auc) and\nclassifiers trained on data distributed according to a stationary {\\ss}-mixing\nprocess. In the way, we show how our approach seemlessly allows us to deal with\nU-processes. As a side note, we also provide a Pac-Bayes generalization bound\nfor classifiers learned on data from stationary $\\varphi$-mixing distributions.\n", "versions": [{"version": "v1", "created": "Thu, 10 Sep 2009 11:51:10 GMT"}, {"version": "v2", "created": "Fri, 4 Jun 2010 08:43:38 GMT"}], "update_date": "2010-06-09", "authors_parsed": [["Ralaivola", "Liva", "", "LIF"], ["Szafranski", "Marie", "", "IBISC"], ["Stempfel", "Guillaume", "", "LIF"]]}, {"id": "0909.2332", "submitter": "David Hardoon", "authors": "David R. Hardoon, Zakria Hussain, John Shawe-Taylor", "title": "A Nonconformity Approach to Model Selection for SVMs", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We investigate the issue of model selection and the use of the nonconformity\n(strangeness) measure in batch learning. Using the nonconformity measure we\npropose a new training algorithm that helps avoid the need for Cross-Validation\nor Leave-One-Out model selection strategies. We provide a new generalisation\nerror bound using the notion of nonconformity to upper bound the loss of each\ntest example and show that our proposed approach is comparable to standard\nmodel selection methods, but with theoretical guarantees of success and faster\nconvergence. We demonstrate our novel model selection technique using the\nSupport Vector Machine.\n", "versions": [{"version": "v1", "created": "Sat, 12 Sep 2009 13:31:41 GMT"}], "update_date": "2009-09-15", "authors_parsed": [["Hardoon", "David R.", ""], ["Hussain", "Zakria", ""], ["Shawe-Taylor", "John", ""]]}, {"id": "0909.2353", "submitter": "Ery Arias-Castro", "authors": "Ery Arias-Castro", "title": "Clustering Based on Pairwise Distances When the Data is of Mixed\n  Dimensions", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In the context of clustering, we consider a generative model in a Euclidean\nambient space with clusters of different shapes, dimensions, sizes and\ndensities. In an asymptotic setting where the number of points becomes large,\nwe obtain theoretical guaranties for a few emblematic methods based on pairwise\ndistances: a simple algorithm based on the extraction of connected components\nin a neighborhood graph; the spectral clustering method of Ng, Jordan and\nWeiss; and hierarchical clustering with single linkage. The methods are shown\nto enjoy some near-optimal properties in terms of separation between clusters\nand robustness to outliers. The local scaling method of Zelnik-Manor and Perona\nis shown to lead to a near-optimal choice for the scale in the first two\nmethods. We also provide a lower bound on the spectral gap to consistently\nchoose the correct number of clusters in the spectral method.\n", "versions": [{"version": "v1", "created": "Sat, 12 Sep 2009 17:14:37 GMT"}], "update_date": "2009-09-15", "authors_parsed": [["Arias-Castro", "Ery", ""]]}, {"id": "0909.2904", "submitter": "Shohei Shimizu", "authors": "Yusuke Komatsu, Shohei Shimizu, Hidetoshi Shimodaira", "title": "Computing p-values of LiNGAM outputs via Multiscale Bootstrap", "comments": "11 pages, 3 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Structural equation models and Bayesian networks have been widely used to\nstudy causal relationships between continuous variables. Recently, a\nnon-Gaussian method called LiNGAM was proposed to discover such causal models\nand has been extended in various directions. An important problem with LiNGAM\nis that the results are affected by the random sampling of the data as with any\nstatistical method. Thus, some analysis of the statistical reliability or\nconfidence level should be conducted. A common method to evaluate a confidence\nlevel is a bootstrap method. However, a confidence level computed by ordinary\nbootstrap method is known to be biased as a probability-value ($p$-value) of\nhypothesis testing. In this paper, we propose a new procedure to apply an\nadvanced bootstrap method called multiscale bootstrap to compute confidence\nlevels, i.e., p-values, of LiNGAM outputs. The multiscale bootstrap method\ngives unbiased $p$-values with asymptotic much higher accuracy. Experiments on\nartificial data demonstrate the utility of our approach.\n", "versions": [{"version": "v1", "created": "Wed, 16 Sep 2009 01:34:16 GMT"}, {"version": "v2", "created": "Tue, 22 Jun 2010 04:12:33 GMT"}], "update_date": "2010-06-23", "authors_parsed": [["Komatsu", "Yusuke", ""], ["Shimizu", "Shohei", ""], ["Shimodaira", "Hidetoshi", ""]]}, {"id": "0909.3595", "submitter": "Ikhlef  Bechar", "authors": "Ikhlef Bechar", "title": "A Bernstein-type inequality for stochastic processes of quadratic forms\n  of Gaussian variables", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST math.PR stat.ML stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We introduce a Bernstein-type inequality which serves to uniformly control\nquadratic forms of gaussian variables. The latter can for example be used to\nderive sharp model selection criteria for linear estimation in linear\nregression and linear inverse problems via penalization, and we do not exclude\nthat its scope of application can be made even broader.\n", "versions": [{"version": "v1", "created": "Sat, 19 Sep 2009 20:56:46 GMT"}], "update_date": "2009-09-22", "authors_parsed": [["Bechar", "Ikhlef", ""]]}, {"id": "0909.4370", "submitter": "Tauhid Zaman", "authors": "Devavrat Shah and Tauhid Zaman", "title": "Rumors in a Network: Who's the Culprit?", "comments": "43 pages, 13 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We provide a systematic study of the problem of finding the source of a rumor\nin a network. We model rumor spreading in a network with a variant of the\npopular SIR model and then construct an estimator for the rumor source. This\nestimator is based upon a novel topological quantity which we term\n\\textbf{rumor centrality}. We establish that this is an ML estimator for a\nclass of graphs. We find the following surprising threshold phenomenon: on\ntrees which grow faster than a line, the estimator always has non-trivial\ndetection probability, whereas on trees that grow like a line, the detection\nprobability will go to 0 as the network grows. Simulations performed on\nsynthetic networks such as the popular small-world and scale-free networks, and\non real networks such as an internet AS network and the U.S. electric power\ngrid network, show that the estimator either finds the source exactly or within\na few hops of the true source across different network topologies. We compare\nrumor centrality to another common network centrality notion known as distance\ncentrality. We prove that on trees, the rumor center and distance center are\nequivalent, but on general networks, they may differ. Indeed, simulations show\nthat rumor centrality outperforms distance centrality in finding rumor sources\nin networks which are not tree-like.\n", "versions": [{"version": "v1", "created": "Thu, 24 Sep 2009 17:45:29 GMT"}, {"version": "v2", "created": "Fri, 29 Oct 2010 00:15:33 GMT"}], "update_date": "2010-11-03", "authors_parsed": [["Shah", "Devavrat", ""], ["Zaman", "Tauhid", ""]]}, {"id": "0909.4386", "submitter": "Dominik Janzing", "authors": "Dominik Janzing, Patrik O. Hoyer, Bernhard Schoelkopf", "title": "Telling cause from effect based on high-dimensional observations", "comments": "13 pages, 5 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We describe a method for inferring linear causal relations among\nmulti-dimensional variables. The idea is to use an asymmetry between the\ndistributions of cause and effect that occurs if both the covariance matrix of\nthe cause and the structure matrix mapping cause to the effect are\nindependently chosen. The method works for both stochastic and deterministic\ncausal relations, provided that the dimensionality is sufficiently high (in\nsome experiments, 5 was enough). It is applicable to Gaussian as well as\nnon-Gaussian data.\n", "versions": [{"version": "v1", "created": "Thu, 24 Sep 2009 08:54:15 GMT"}], "update_date": "2009-09-25", "authors_parsed": [["Janzing", "Dominik", ""], ["Hoyer", "Patrik O.", ""], ["Schoelkopf", "Bernhard", ""]]}, {"id": "0909.4395", "submitter": "Laurent Galluccio", "authors": "Laurent Galluccio, Olivier J.J. Michel (GIPSA-lab), Pierre Comon, Eric\n  Slezak (CASSIOPEE), Alfred O. Hero", "title": "Initialization Free Graph Based Clustering", "comments": "16 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper proposes an original approach to cluster multi-component data\nsets, including an estimation of the number of clusters. From the construction\nof a minimal spanning tree with Prim's algorithm, and the assumption that the\nvertices are approximately distributed according to a Poisson distribution, the\nnumber of clusters is estimated by thresholding the Prim's trajectory. The\ncorresponding cluster centroids are then computed in order to initialize the\ngeneralized Lloyd's algorithm, also known as $K$-means, which allows to\ncircumvent initialization problems. Some results are derived for evaluating the\nfalse positive rate of our cluster detection algorithm, with the help of\napproximations relevant in Euclidean spaces. Metrics used for measuring\nsimilarity between multi-dimensional data points are based on symmetrical\ndivergences. The use of these informational divergences together with the\nproposed method leads to better results, compared to other clustering methods\nfor the problem of astrophysical data processing. Some applications of this\nmethod in the multi/hyper-spectral imagery domain to a satellite view of Paris\nand to an image of the Mars planet are also presented. In order to demonstrate\nthe usefulness of divergences in our problem, the method with informational\ndivergence as similarity measure is compared with the same method using\nclassical metrics. In the astrophysics application, we also compare the method\nwith the spectral clustering algorithms.\n", "versions": [{"version": "v1", "created": "Thu, 24 Sep 2009 09:35:10 GMT"}], "update_date": "2009-09-25", "authors_parsed": [["Galluccio", "Laurent", "", "GIPSA-lab"], ["Michel", "Olivier J. J.", "", "GIPSA-lab"], ["Comon", "Pierre", "", "CASSIOPEE"], ["Slezak", "Eric", "", "CASSIOPEE"], ["Hero", "Alfred O.", ""]]}, {"id": "0909.4588", "submitter": "Marcus Hutter", "authors": "Marcus Hutter", "title": "Discrete MDL Predicts in Total Variation", "comments": "15 LaTeX pages", "journal-ref": "Advances in Neural Information Processing Systems 22 (NIPS 2009)\n  pages 817-825", "doi": null, "report-no": null, "categories": "math.PR cs.IT cs.LG math.IT math.ST stat.ML stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The Minimum Description Length (MDL) principle selects the model that has the\nshortest code for data plus model. We show that for a countable class of\nmodels, MDL predictions are close to the true distribution in a strong sense.\nThe result is completely general. No independence, ergodicity, stationarity,\nidentifiability, or other assumption on the model class need to be made. More\nformally, we show that for any countable class of models, the distributions\nselected by MDL (or MAP) asymptotically predict (merge with) the true measure\nin the class in total variation distance. Implications for non-i.i.d. domains\nlike time-series forecasting, discriminative learning, and reinforcement\nlearning are discussed.\n", "versions": [{"version": "v1", "created": "Fri, 25 Sep 2009 02:57:17 GMT"}], "update_date": "2010-12-30", "authors_parsed": [["Hutter", "Marcus", ""]]}, {"id": "0909.5026", "submitter": "Taiji Suzuki", "authors": "Taiji Suzuki and Ryota Tomioka", "title": "SpicyMKL", "comments": "30 pages, 6 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a new optimization algorithm for Multiple Kernel Learning (MKL)\ncalled SpicyMKL, which is applicable to general convex loss functions and\ngeneral types of regularization. The proposed SpicyMKL iteratively solves\nsmooth minimization problems. Thus, there is no need of solving SVM, LP, or QP\ninternally. SpicyMKL can be viewed as a proximal minimization method and\nconverges super-linearly. The cost of inner minimization is roughly\nproportional to the number of active kernels. Therefore, when we aim for a\nsparse kernel combination, our algorithm scales well against increasing number\nof kernels. Moreover, we give a general block-norm formulation of MKL that\nincludes non-sparse regularizations, such as elastic-net and \\ellp -norm\nregularizations. Extending SpicyMKL, we propose an efficient optimization\nmethod for the general regularization framework. Experimental results show that\nour algorithm is faster than existing methods especially when the number of\nkernels is large (> 1000).\n", "versions": [{"version": "v1", "created": "Mon, 28 Sep 2009 07:45:29 GMT"}, {"version": "v2", "created": "Mon, 9 May 2011 03:06:22 GMT"}], "update_date": "2011-05-10", "authors_parsed": [["Suzuki", "Taiji", ""], ["Tomioka", "Ryota", ""]]}, {"id": "0909.5194", "submitter": "Lauren Hannah", "authors": "Lauren A. Hannah, David M. Blei, Warren B. Powell", "title": "Dirichlet Process Mixtures of Generalized Linear Models", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose Dirichlet Process mixtures of Generalized Linear Models (DP-GLM),\na new method of nonparametric regression that accommodates continuous and\ncategorical inputs, and responses that can be modeled by a generalized linear\nmodel. We prove conditions for the asymptotic unbiasedness of the DP-GLM\nregression mean function estimate. We also give examples for when those\nconditions hold, including models for compactly supported continuous\ndistributions and a model with continuous covariates and categorical response.\nWe empirically analyze the properties of the DP-GLM and why it provides better\nresults than existing Dirichlet process mixture regression models. We evaluate\nDP-GLM on several data sets, comparing it to modern methods of nonparametric\nregression like CART, Bayesian trees and Gaussian processes. Compared to\nexisting techniques, the DP-GLM provides a single model (and corresponding\ninference algorithms) that performs well in many regression settings.\n", "versions": [{"version": "v1", "created": "Mon, 28 Sep 2009 20:04:28 GMT"}, {"version": "v2", "created": "Thu, 15 Jul 2010 18:27:01 GMT"}], "update_date": "2010-07-16", "authors_parsed": [["Hannah", "Lauren A.", ""], ["Blei", "David M.", ""], ["Powell", "Warren B.", ""]]}, {"id": "0909.5216", "submitter": "Vincent Tan", "authors": "Vincent Y. F. Tan, Animashree Anandkumar and Alan S. Willsky", "title": "Learning Gaussian Tree Models: Analysis of Error Exponents and Extremal\n  Structures", "comments": "Submitted to Transactions on Signal Processing", "journal-ref": "IEEE Transactions on Signal Processing, May 2010, Volume: 58\n  Issue:5, pages 2701 - 2714", "doi": "10.1109/TSP.2010.2042478", "report-no": null, "categories": "stat.ML cs.IT math.IT math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The problem of learning tree-structured Gaussian graphical models from\nindependent and identically distributed (i.i.d.) samples is considered. The\ninfluence of the tree structure and the parameters of the Gaussian distribution\non the learning rate as the number of samples increases is discussed.\nSpecifically, the error exponent corresponding to the event that the estimated\ntree structure differs from the actual unknown tree structure of the\ndistribution is analyzed. Finding the error exponent reduces to a least-squares\nproblem in the very noisy learning regime. In this regime, it is shown that the\nextremal tree structure that minimizes the error exponent is the star for any\nfixed set of correlation coefficients on the edges of the tree. If the\nmagnitudes of all the correlation coefficients are less than 0.63, it is also\nshown that the tree structure that maximizes the error exponent is the Markov\nchain. In other words, the star and the chain graphs represent the hardest and\nthe easiest structures to learn in the class of tree-structured Gaussian\ngraphical models. This result can also be intuitively explained by correlation\ndecay: pairs of nodes which are far apart, in terms of graph distance, are\nunlikely to be mistaken as edges by the maximum-likelihood estimator in the\nasymptotic regime.\n", "versions": [{"version": "v1", "created": "Mon, 28 Sep 2009 20:49:32 GMT"}, {"version": "v2", "created": "Tue, 5 Jan 2010 02:26:58 GMT"}], "update_date": "2010-05-06", "authors_parsed": [["Tan", "Vincent Y. F.", ""], ["Anandkumar", "Animashree", ""], ["Willsky", "Alan S.", ""]]}, {"id": "0909.5422", "submitter": "Stefano Melacci", "authors": "Stefano Melacci, Mikhail Belkin", "title": "Laplacian Support Vector Machines Trained in the Primal", "comments": "39 pages, 14 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In the last few years, due to the growing ubiquity of unlabeled data, much\neffort has been spent by the machine learning community to develop better\nunderstanding and improve the quality of classifiers exploiting unlabeled data.\nFollowing the manifold regularization approach, Laplacian Support Vector\nMachines (LapSVMs) have shown the state of the art performance in\nsemi--supervised classification. In this paper we present two strategies to\nsolve the primal LapSVM problem, in order to overcome some issues of the\noriginal dual formulation. Whereas training a LapSVM in the dual requires two\nsteps, using the primal form allows us to collapse training to a single step.\nMoreover, the computational complexity of the training algorithm is reduced\nfrom O(n^3) to O(n^2) using preconditioned conjugate gradient, where n is the\ncombined number of labeled and unlabeled examples. We speed up training by\nusing an early stopping strategy based on the prediction on unlabeled data or,\nif available, on labeled validation examples. This allows the algorithm to\nquickly compute approximate solutions with roughly the same classification\naccuracy as the optimal ones, considerably reducing the training time. Due to\nits simplicity, training LapSVM in the primal can be the starting point for\nadditional enhancements of the original LapSVM formulation, such as those for\ndealing with large datasets. We present an extensive experimental evaluation on\nreal world data showing the benefits of the proposed approach.\n", "versions": [{"version": "v1", "created": "Tue, 29 Sep 2009 19:54:05 GMT"}], "update_date": "2009-09-30", "authors_parsed": [["Melacci", "Stefano", ""], ["Belkin", "Mikhail", ""]]}]