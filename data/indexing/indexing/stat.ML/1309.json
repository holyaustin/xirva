[{"id": "1309.0024", "submitter": "Jeffrey Miller", "authors": "Jeffrey W. Miller, Matthew T. Harrison", "title": "Inconsistency of Pitman-Yor process mixtures for the number of\n  components", "comments": "This is a general treatment of the problem discussed in our related\n  article, \"A simple example of Dirichlet process mixture inconsistency for the\n  number of components\", Miller and Harrison (2013) arXiv:1301.2708", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.ML stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In many applications, a finite mixture is a natural model, but it can be\ndifficult to choose an appropriate number of components. To circumvent this\nchoice, investigators are increasingly turning to Dirichlet process mixtures\n(DPMs), and Pitman-Yor process mixtures (PYMs), more generally. While these\nmodels may be well-suited for Bayesian density estimation, many investigators\nare using them for inferences about the number of components, by considering\nthe posterior on the number of components represented in the observed data. We\nshow that this posterior is not consistent --- that is, on data from a finite\nmixture, it does not concentrate at the true number of components. This result\napplies to a large class of nonparametric mixtures, including DPMs and PYMs,\nover a wide variety of families of component distributions, including\nessentially all discrete families, as well as continuous exponential families\nsatisfying mild regularity conditions (such as multivariate Gaussians).\n", "versions": [{"version": "v1", "created": "Fri, 30 Aug 2013 20:43:10 GMT"}], "update_date": "2013-09-03", "authors_parsed": [["Miller", "Jeffrey W.", ""], ["Harrison", "Matthew T.", ""]]}, {"id": "1309.0242", "submitter": "Pontus Svenson", "authors": "Johan Dahlin and Pontus Svenson", "title": "Ensemble approaches for improving community detection methods", "comments": "12 pages, 8 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "physics.soc-ph cs.LG cs.SI stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Statistical estimates can often be improved by fusion of data from several\ndifferent sources. One example is so-called ensemble methods which have been\nsuccessfully applied in areas such as machine learning for classification and\nclustering. In this paper, we present an ensemble method to improve community\ndetection by aggregating the information found in an ensemble of community\nstructures. This ensemble can found by re-sampling methods, multiple runs of a\nstochastic community detection method, or by several different community\ndetection algorithms applied to the same network. The proposed method is\nevaluated using random networks with community structures and compared with two\ncommonly used community detection methods. The proposed method when applied on\na stochastic community detection algorithm performs well with low computational\ncomplexity, thus offering both a new approach to community detection and an\nadditional community detection method.\n", "versions": [{"version": "v1", "created": "Sun, 1 Sep 2013 16:59:55 GMT"}], "update_date": "2013-09-03", "authors_parsed": [["Dahlin", "Johan", ""], ["Svenson", "Pontus", ""]]}, {"id": "1309.0302", "submitter": "Dacheng Tao", "authors": "Tianyi Zhou and Dacheng Tao", "title": "Unmixing Incoherent Structures of Big Data by Randomized or Greedy\n  Decomposition", "comments": "42 pages, 5 figures, 4 tables, 5 algorithms", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.DS cs.LG", "license": "http://creativecommons.org/licenses/by-nc-sa/3.0/", "abstract": "  Learning big data by matrix decomposition always suffers from expensive\ncomputation, mixing of complicated structures and noise. In this paper, we\nstudy more adaptive models and efficient algorithms that decompose a data\nmatrix as the sum of semantic components with incoherent structures. We firstly\nintroduce \"GO decomposition (GoDec)\", an alternating projection method\nestimating the low-rank part $L$ and the sparse part $S$ from data matrix\n$X=L+S+G$ corrupted by noise $G$. Two acceleration strategies are proposed to\nobtain scalable unmixing algorithm on big data: 1) Bilateral random projection\n(BRP) is developed to speed up the update of $L$ in GoDec by a closed-form\nbuilt from left and right random projections of $X-S$ in lower dimensions; 2)\nGreedy bilateral (GreB) paradigm updates the left and right factors of $L$ in a\nmutually adaptive and greedy incremental manner, and achieve significant\nimprovement in both time and sample complexities. Then we proposes three\nnontrivial variants of GoDec that generalizes GoDec to more general data type\nand whose fast algorithms can be derived from the two strategies......\n", "versions": [{"version": "v1", "created": "Mon, 2 Sep 2013 05:07:31 GMT"}], "update_date": "2013-09-03", "authors_parsed": [["Zhou", "Tianyi", ""], ["Tao", "Dacheng", ""]]}, {"id": "1309.0337", "submitter": "Neil Houlsby", "authors": "Neil Houlsby, Massimiliano Ciaramita", "title": "Scalable Probabilistic Entity-Topic Modeling", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.IR cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present an LDA approach to entity disambiguation. Each topic is associated\nwith a Wikipedia article and topics generate either content words or entity\nmentions. Training such models is challenging because of the topic and\nvocabulary size, both in the millions. We tackle these problems using a novel\ndistributed inference and representation framework based on a parallel Gibbs\nsampler guided by the Wikipedia link graph, and pipelines of MapReduce allowing\nfast and memory-frugal processing of large datasets. We report state-of-the-art\nperformance on a public dataset.\n", "versions": [{"version": "v1", "created": "Mon, 2 Sep 2013 09:34:50 GMT"}], "update_date": "2013-09-03", "authors_parsed": [["Houlsby", "Neil", ""], ["Ciaramita", "Massimiliano", ""]]}, {"id": "1309.0787", "submitter": "Furong Huang", "authors": "Furong Huang, U. N. Niranjan, Mohammad Umar Hakeem, Animashree\n  Anandkumar", "title": "Online Tensor Methods for Learning Latent Variable Models", "comments": "JMLR 2014", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.DC cs.SI stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We introduce an online tensor decomposition based approach for two latent\nvariable modeling problems namely, (1) community detection, in which we learn\nthe latent communities that the social actors in social networks belong to, and\n(2) topic modeling, in which we infer hidden topics of text articles. We\nconsider decomposition of moment tensors using stochastic gradient descent. We\nconduct optimization of multilinear operations in SGD and avoid directly\nforming the tensors, to save computational and storage costs. We present\noptimized algorithm in two platforms. Our GPU-based implementation exploits the\nparallelism of SIMD architectures to allow for maximum speed-up by a careful\noptimization of storage and data transfer, whereas our CPU-based implementation\nuses efficient sparse matrix computations and is suitable for large sparse\ndatasets. For the community detection problem, we demonstrate accuracy and\ncomputational efficiency on Facebook, Yelp and DBLP datasets, and for the topic\nmodeling problem, we also demonstrate good performance on the New York Times\ndataset. We compare our results to the state-of-the-art algorithms such as the\nvariational method, and report a gain of accuracy and a gain of several orders\nof magnitude in the execution time.\n", "versions": [{"version": "v1", "created": "Tue, 3 Sep 2013 19:30:55 GMT"}, {"version": "v2", "created": "Tue, 10 Sep 2013 20:56:08 GMT"}, {"version": "v3", "created": "Wed, 16 Oct 2013 01:58:14 GMT"}, {"version": "v4", "created": "Mon, 31 Mar 2014 17:24:07 GMT"}, {"version": "v5", "created": "Sat, 3 Oct 2015 04:26:19 GMT"}], "update_date": "2015-10-06", "authors_parsed": [["Huang", "Furong", ""], ["Niranjan", "U. N.", ""], ["Hakeem", "Mohammad Umar", ""], ["Anandkumar", "Animashree", ""]]}, {"id": "1309.0790", "submitter": "Philip Graff", "authors": "Philip Graff, Farhan Feroz, Michael P. Hobson, Anthony N. Lasenby", "title": "SKYNET: an efficient and robust neural network training tool for machine\n  learning in astronomy", "comments": "19 pages, 21 figures, 7 tables; this version is re-submission to\n  MNRAS in response to referee comments; software available at\n  http://www.mrao.cam.ac.uk/software/skynet/", "journal-ref": null, "doi": "10.1093/mnras/stu642", "report-no": null, "categories": "astro-ph.IM cs.LG cs.NE physics.data-an stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present the first public release of our generic neural network training\nalgorithm, called SkyNet. This efficient and robust machine learning tool is\nable to train large and deep feed-forward neural networks, including\nautoencoders, for use in a wide range of supervised and unsupervised learning\napplications, such as regression, classification, density estimation,\nclustering and dimensionality reduction. SkyNet uses a `pre-training' method to\nobtain a set of network parameters that has empirically been shown to be close\nto a good solution, followed by further optimisation using a regularised\nvariant of Newton's method, where the level of regularisation is determined and\nadjusted automatically; the latter uses second-order derivative information to\nimprove convergence, but without the need to evaluate or store the full Hessian\nmatrix, by using a fast approximate method to calculate Hessian-vector\nproducts. This combination of methods allows for the training of complicated\nnetworks that are difficult to optimise using standard backpropagation\ntechniques. SkyNet employs convergence criteria that naturally prevent\noverfitting, and also includes a fast algorithm for estimating the accuracy of\nnetwork outputs. The utility and flexibility of SkyNet are demonstrated by\napplication to a number of toy problems, and to astronomical problems focusing\non the recovery of structure from blurred and noisy images, the identification\nof gamma-ray bursters, and the compression and denoising of galaxy images. The\nSkyNet software, which is implemented in standard ANSI C and fully parallelised\nusing MPI, is available at http://www.mrao.cam.ac.uk/software/skynet/.\n", "versions": [{"version": "v1", "created": "Tue, 3 Sep 2013 19:33:28 GMT"}, {"version": "v2", "created": "Mon, 27 Jan 2014 19:23:30 GMT"}], "update_date": "2015-06-17", "authors_parsed": [["Graff", "Philip", ""], ["Feroz", "Farhan", ""], ["Hobson", "Michael P.", ""], ["Lasenby", "Anthony N.", ""]]}, {"id": "1309.1193", "submitter": "Evgenia Chunikhina", "authors": "E. Chunikhina, R. Raich, and T. Nguyen", "title": "Confidence-constrained joint sparsity recovery under the Poisson noise\n  model", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Our work is focused on the joint sparsity recovery problem where the common\nsparsity pattern is corrupted by Poisson noise. We formulate the\nconfidence-constrained optimization problem in both least squares (LS) and\nmaximum likelihood (ML) frameworks and study the conditions for perfect\nreconstruction of the original row sparsity and row sparsity pattern. However,\nthe confidence-constrained optimization problem is non-convex. Using convex\nrelaxation, an alternative convex reformulation of the problem is proposed. We\nevaluate the performance of the proposed approach using simulation results on\nsynthetic data and show the effectiveness of proposed row sparsity and row\nsparsity pattern recovery framework.\n", "versions": [{"version": "v1", "created": "Wed, 4 Sep 2013 21:46:55 GMT"}, {"version": "v2", "created": "Wed, 9 Oct 2013 17:59:10 GMT"}], "update_date": "2013-10-10", "authors_parsed": [["Chunikhina", "E.", ""], ["Raich", "R.", ""], ["Nguyen", "T.", ""]]}, {"id": "1309.1194", "submitter": "Dimitris Pados", "authors": "Panos P. Markopoulos, George N. Karystinos, and Dimitris A. Pados", "title": "Some Options for L1-Subspace Signal Processing", "comments": "In Proceedings Tenth Intern. Symposium on Wireless Communication\n  Systems (ISWCS '13), Ilmenau, Germany, Aug. 27-30, 2013 (The 2013 ISWCS Best\n  Paper Award in Physical Layer Comm. and Signal Processing); 5 pages; 3\n  figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We describe ways to define and calculate $L_1$-norm signal subspaces which\nare less sensitive to outlying data than $L_2$-calculated subspaces. We focus\non the computation of the $L_1$ maximum-projection principal component of a\ndata matrix containing N signal samples of dimension D and conclude that the\ngeneral problem is formally NP-hard in asymptotically large N, D. We prove,\nhowever, that the case of engineering interest of fixed dimension D and\nasymptotically large sample support N is not and we present an optimal\nalgorithm of complexity $O(N^D)$. We generalize to multiple\n$L_1$-max-projection components and present an explicit optimal $L_1$ subspace\ncalculation algorithm in the form of matrix nuclear-norm evaluations. We\nconclude with illustrations of $L_1$-subspace signal processing in the fields\nof data dimensionality reduction and direction-of-arrival estimation.\n", "versions": [{"version": "v1", "created": "Wed, 4 Sep 2013 22:00:43 GMT"}], "update_date": "2013-09-09", "authors_parsed": [["Markopoulos", "Panos P.", ""], ["Karystinos", "George N.", ""], ["Pados", "Dimitris A.", ""]]}, {"id": "1309.1233", "submitter": "Yu-Xiang Wang", "authors": "Yu-Xiang Wang and Huan Xu", "title": "Noisy Sparse Subspace Clustering", "comments": "Manuscript currently under review at journal of machine learning\n  research. Previously conference version appeared at ICML'12, and was uploaded\n  to ArXiv by the conference committee", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper considers the problem of subspace clustering under noise.\nSpecifically, we study the behavior of Sparse Subspace Clustering (SSC) when\neither adversarial or random noise is added to the unlabelled input data\npoints, which are assumed to be in a union of low-dimensional subspaces. We\nshow that a modified version of SSC is \\emph{provably effective} in correctly\nidentifying the underlying subspaces, even with noisy data. This extends\ntheoretical guarantee of this algorithm to more practical settings and provides\njustification to the success of SSC in a class of real applications.\n", "versions": [{"version": "v1", "created": "Thu, 5 Sep 2013 04:42:00 GMT"}, {"version": "v2", "created": "Thu, 22 Jan 2015 06:44:45 GMT"}], "update_date": "2015-01-23", "authors_parsed": [["Wang", "Yu-Xiang", ""], ["Xu", "Huan", ""]]}, {"id": "1309.1369", "submitter": "Aleksandr Aravkin", "authors": "Aleksandr Y. Aravkin, Anna Choromanska, Tony Jebara, and Dimitri\n  Kanevsky", "title": "Semistochastic Quadratic Bound Methods", "comments": "11 pages, 1 figure", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG math.NA stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Partition functions arise in a variety of settings, including conditional\nrandom fields, logistic regression, and latent gaussian models. In this paper,\nwe consider semistochastic quadratic bound (SQB) methods for maximum likelihood\ninference based on partition function optimization. Batch methods based on the\nquadratic bound were recently proposed for this class of problems, and\nperformed favorably in comparison to state-of-the-art techniques.\nSemistochastic methods fall in between batch algorithms, which use all the\ndata, and stochastic gradient type methods, which use small random selections\nat each iteration. We build semistochastic quadratic bound-based methods, and\nprove both global convergence (to a stationary point) under very weak\nassumptions, and linear convergence rate under stronger assumptions on the\nobjective. To make the proposed methods faster and more stable, we consider\ninexact subproblem minimization and batch-size selection schemes. The efficacy\nof SQB methods is demonstrated via comparison with several state-of-the-art\ntechniques on commonly used datasets.\n", "versions": [{"version": "v1", "created": "Thu, 5 Sep 2013 15:12:11 GMT"}, {"version": "v2", "created": "Sat, 21 Dec 2013 02:42:50 GMT"}, {"version": "v3", "created": "Sat, 25 Jan 2014 21:00:34 GMT"}, {"version": "v4", "created": "Mon, 17 Feb 2014 22:18:34 GMT"}], "update_date": "2014-02-19", "authors_parsed": [["Aravkin", "Aleksandr Y.", ""], ["Choromanska", "Anna", ""], ["Jebara", "Tony", ""], ["Kanevsky", "Dimitri", ""]]}, {"id": "1309.1392", "submitter": "James P. Crutchfield", "authors": "Christopher C. Strelioff and James P. Crutchfield", "title": "Bayesian Structural Inference for Hidden Processes", "comments": "20 pages, 11 figures, 1 table; supplementary materials, 15 pages, 11\n  figures, 6 tables; http://csc.ucdavis.edu/~cmg/compmech/pubs/bsihp.htm", "journal-ref": "Phys. Rev. E 89, 042119 (2014)", "doi": "10.1103/PhysRevE.89.042119", "report-no": "Santa Fe Institute Working Paper 13-09-027", "categories": "stat.ML cs.LG math.ST nlin.CD physics.data-an stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We introduce a Bayesian approach to discovering patterns in structurally\ncomplex processes. The proposed method of Bayesian Structural Inference (BSI)\nrelies on a set of candidate unifilar HMM (uHMM) topologies for inference of\nprocess structure from a data series. We employ a recently developed exact\nenumeration of topological epsilon-machines. (A sequel then removes the\ntopological restriction.) This subset of the uHMM topologies has the added\nbenefit that inferred models are guaranteed to be epsilon-machines,\nirrespective of estimated transition probabilities. Properties of\nepsilon-machines and uHMMs allow for the derivation of analytic expressions for\nestimating transition probabilities, inferring start states, and comparing the\nposterior probability of candidate model topologies, despite process internal\nstructure being only indirectly present in data. We demonstrate BSI's\neffectiveness in estimating a process's randomness, as reflected by the Shannon\nentropy rate, and its structure, as quantified by the statistical complexity.\nWe also compare using the posterior distribution over candidate models and the\nsingle, maximum a posteriori model for point estimation and show that the\nformer more accurately reflects uncertainty in estimated values. We apply BSI\nto in-class examples of finite- and infinite-order Markov processes, as well to\nan out-of-class, infinite-state hidden process.\n", "versions": [{"version": "v1", "created": "Thu, 5 Sep 2013 16:18:35 GMT"}, {"version": "v2", "created": "Mon, 9 Dec 2013 05:21:31 GMT"}], "update_date": "2014-04-23", "authors_parsed": [["Strelioff", "Christopher C.", ""], ["Crutchfield", "James P.", ""]]}, {"id": "1309.1501", "submitter": "Aleksandr Aravkin", "authors": "Tara N. Sainath, Brian Kingsbury, Abdel-rahman Mohamed, George E.\n  Dahl, George Saon, Hagen Soltau, Tomas Beran, Aleksandr Y. Aravkin, Bhuvana\n  Ramabhadran", "title": "Improvements to deep convolutional neural networks for LVCSR", "comments": "6 pages, 1 figure", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CL cs.NE math.OC stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Deep Convolutional Neural Networks (CNNs) are more powerful than Deep Neural\nNetworks (DNN), as they are able to better reduce spectral variation in the\ninput signal. This has also been confirmed experimentally, with CNNs showing\nimprovements in word error rate (WER) between 4-12% relative compared to DNNs\nacross a variety of LVCSR tasks. In this paper, we describe different methods\nto further improve CNN performance. First, we conduct a deep analysis comparing\nlimited weight sharing and full weight sharing with state-of-the-art features.\nSecond, we apply various pooling strategies that have shown improvements in\ncomputer vision to an LVCSR speech task. Third, we introduce a method to\neffectively incorporate speaker adaptation, namely fMLLR, into log-mel\nfeatures. Fourth, we introduce an effective strategy to use dropout during\nHessian-free sequence training. We find that with these improvements,\nparticularly with fMLLR and dropout, we are able to achieve an additional 2-3%\nrelative improvement in WER on a 50-hour Broadcast News task over our previous\nbest CNN baseline. On a larger 400-hour BN task, we find an additional 4-5%\nrelative improvement over our previous best CNN baseline.\n", "versions": [{"version": "v1", "created": "Thu, 5 Sep 2013 22:06:58 GMT"}, {"version": "v2", "created": "Wed, 11 Sep 2013 14:33:09 GMT"}, {"version": "v3", "created": "Tue, 10 Dec 2013 11:51:39 GMT"}], "update_date": "2013-12-11", "authors_parsed": [["Sainath", "Tara N.", ""], ["Kingsbury", "Brian", ""], ["Mohamed", "Abdel-rahman", ""], ["Dahl", "George E.", ""], ["Saon", "George", ""], ["Soltau", "Hagen", ""], ["Beran", "Tomas", ""], ["Aravkin", "Aleksandr Y.", ""], ["Ramabhadran", "Bhuvana", ""]]}, {"id": "1309.1508", "submitter": "Aleksandr Aravkin", "authors": "Tara N. Sainath, Lior Horesh, Brian Kingsbury, Aleksandr Y. Aravkin,\n  Bhuvana Ramabhadran", "title": "Accelerating Hessian-free optimization for deep neural networks by\n  implicit preconditioning and sampling", "comments": "this paper is not supposed to be posted publically before the\n  conference in December due to company policy. another co-author was not\n  informed of this and posted without the permission of the first author. pls\n  remove", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CL cs.NE math.OC stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Hessian-free training has become a popular parallel second or- der\noptimization technique for Deep Neural Network training. This study aims at\nspeeding up Hessian-free training, both by means of decreasing the amount of\ndata used for training, as well as through reduction of the number of Krylov\nsubspace solver iterations used for implicit estimation of the Hessian. In this\npaper, we develop an L-BFGS based preconditioning scheme that avoids the need\nto access the Hessian explicitly. Since L-BFGS cannot be regarded as a\nfixed-point iteration, we further propose the employment of flexible Krylov\nsubspace solvers that retain the desired theoretical convergence guarantees of\ntheir conventional counterparts. Second, we propose a new sampling algorithm,\nwhich geometrically increases the amount of data utilized for gradient and\nKrylov subspace iteration calculations. On a 50-hr English Broadcast News task,\nwe find that these methodologies provide roughly a 1.5x speed-up, whereas, on a\n300-hr Switchboard task, these techniques provide over a 2.3x speedup, with no\nloss in WER. These results suggest that even further speed-up is expected, as\nproblems scale and complexity grows.\n", "versions": [{"version": "v1", "created": "Thu, 5 Sep 2013 23:21:02 GMT"}, {"version": "v2", "created": "Wed, 11 Sep 2013 14:34:31 GMT"}, {"version": "v3", "created": "Tue, 10 Dec 2013 12:05:51 GMT"}], "update_date": "2013-12-11", "authors_parsed": [["Sainath", "Tara N.", ""], ["Horesh", "Lior", ""], ["Kingsbury", "Brian", ""], ["Aravkin", "Aleksandr Y.", ""], ["Ramabhadran", "Bhuvana", ""]]}, {"id": "1309.1541", "submitter": "Weiran Wang", "authors": "Weiran Wang, Miguel \\'A. Carreira-Perpi\\~n\\'an", "title": "Projection onto the probability simplex: An efficient algorithm with a\n  simple proof, and an application", "comments": "5 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG math.OC stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We provide an elementary proof of a simple, efficient algorithm for computing\nthe Euclidean projection of a point onto the probability simplex. We also show\nan application in Laplacian K-modes clustering.\n", "versions": [{"version": "v1", "created": "Fri, 6 Sep 2013 05:48:40 GMT"}], "update_date": "2013-09-09", "authors_parsed": [["Wang", "Weiran", ""], ["Carreira-Perpi\u00f1\u00e1n", "Miguel \u00c1.", ""]]}, {"id": "1309.1761", "submitter": "Shaun Joseph", "authors": "Shaun N. Joseph and Seif Omar Abu Bakr and Gabriel Lugo", "title": "Convergence of Nearest Neighbor Pattern Classification with Selective\n  Sampling", "comments": "18 pages, 2 figures, mZeal Communications Technical Report", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In the panoply of pattern classification techniques, few enjoy the intuitive\nappeal and simplicity of the nearest neighbor rule: given a set of samples in\nsome metric domain space whose value under some function is known, we estimate\nthe function anywhere in the domain by giving the value of the nearest sample\nper the metric. More generally, one may use the modal value of the m nearest\nsamples, where m is a fixed positive integer (although m=1 is known to be\nadmissible in the sense that no larger value is asymptotically superior in\nterms of prediction error). The nearest neighbor rule is nonparametric and\nextremely general, requiring in principle only that the domain be a metric\nspace. The classic paper on the technique, proving convergence under\nindependent, identically-distributed (iid) sampling, is due to Cover and Hart\n(1967). Because taking samples is costly, there has been much research in\nrecent years on selective sampling, in which each sample is selected from a\npool of candidates ranked by a heuristic; the heuristic tries to guess which\ncandidate would be the most \"informative\" sample. Lindenbaum et al. (2004)\napply selective sampling to the nearest neighbor rule, but their approach\nsacrifices the austere generality of Cover and Hart; furthermore, their\nheuristic algorithm is complex and computationally expensive. Here we report\nrecent results that enable selective sampling in the original Cover-Hart\nsetting. Our results pose three selection heuristics and prove that their\nnearest neighbor rule predictions converge to the true pattern. Two of the\nalgorithms are computationally cheap, with complexity growing linearly in the\nnumber of samples. We believe that these results constitute an important\nadvance in the art.\n", "versions": [{"version": "v1", "created": "Fri, 6 Sep 2013 18:52:16 GMT"}], "update_date": "2013-09-10", "authors_parsed": [["Joseph", "Shaun N.", ""], ["Bakr", "Seif Omar Abu", ""], ["Lugo", "Gabriel", ""]]}, {"id": "1309.1901", "submitter": "Paul McNicholas", "authors": "Sanjeena Subedi and Paul D. McNicholas", "title": "Variational Bayes Approximations for Clustering via Mixtures of Normal\n  Inverse Gaussian Distributions", "comments": null, "journal-ref": null, "doi": "10.1007/s11634-014-0165-7", "report-no": null, "categories": "stat.ME stat.CO stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Parameter estimation for model-based clustering using a finite mixture of\nnormal inverse Gaussian (NIG) distributions is achieved through variational\nBayes approximations. Univariate NIG mixtures and multivariate NIG mixtures are\nconsidered. The use of variational Bayes approximations here is a substantial\ndeparture from the traditional EM approach and alleviates some of the\nassociated computational complexities and uncertainties. Our variational\nalgorithm is applied to simulated and real data. The paper concludes with\ndiscussion and suggestions for future work.\n", "versions": [{"version": "v1", "created": "Sat, 7 Sep 2013 20:29:26 GMT"}], "update_date": "2017-10-09", "authors_parsed": [["Subedi", "Sanjeena", ""], ["McNicholas", "Paul D.", ""]]}, {"id": "1309.1952", "submitter": "Alekh Agarwal", "authors": "Alekh Agarwal and Animashree Anandkumar and Praneeth Netrapalli", "title": "A Clustering Approach to Learn Sparsely-Used Overcomplete Dictionaries", "comments": "Part of this work appears in COLT 2014", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG math.OC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider the problem of learning overcomplete dictionaries in the context\nof sparse coding, where each sample selects a sparse subset of dictionary\nelements. Our main result is a strategy to approximately recover the unknown\ndictionary using an efficient algorithm. Our algorithm is a clustering-style\nprocedure, where each cluster is used to estimate a dictionary element. The\nresulting solution can often be further cleaned up to obtain a high accuracy\nestimate, and we provide one simple scenario where $\\ell_1$-regularized\nregression can be used for such a second stage.\n", "versions": [{"version": "v1", "created": "Sun, 8 Sep 2013 12:55:39 GMT"}, {"version": "v2", "created": "Mon, 7 Jul 2014 05:10:23 GMT"}], "update_date": "2014-07-08", "authors_parsed": [["Agarwal", "Alekh", ""], ["Anandkumar", "Animashree", ""], ["Netrapalli", "Praneeth", ""]]}, {"id": "1309.2074", "submitter": "Qiang Qiu", "authors": "Qiang Qiu, Guillermo Sapiro", "title": "Learning Transformations for Clustering and Classification", "comments": "arXiv admin note: substantial text overlap with arXiv:1308.0273,\n  arXiv:1308.0275", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A low-rank transformation learning framework for subspace clustering and\nclassification is here proposed. Many high-dimensional data, such as face\nimages and motion sequences, approximately lie in a union of low-dimensional\nsubspaces. The corresponding subspace clustering problem has been extensively\nstudied in the literature to partition such high-dimensional data into clusters\ncorresponding to their underlying low-dimensional subspaces. However,\nlow-dimensional intrinsic structures are often violated for real-world\nobservations, as they can be corrupted by errors or deviate from ideal models.\nWe propose to address this by learning a linear transformation on subspaces\nusing matrix rank, via its convex surrogate nuclear norm, as the optimization\ncriteria. The learned linear transformation restores a low-rank structure for\ndata from the same subspace, and, at the same time, forces a a maximally\nseparated structure for data from different subspaces. In this way, we reduce\nvariations within subspaces, and increase separation between subspaces for a\nmore robust subspace clustering. This proposed learned robust subspace\nclustering framework significantly enhances the performance of existing\nsubspace clustering methods. Basic theoretical results here presented help to\nfurther support the underlying framework. To exploit the low-rank structures of\nthe transformed subspaces, we further introduce a fast subspace clustering\ntechnique, which efficiently combines robust PCA with sparse modeling. When\nclass labels are present at the training stage, we show this low-rank\ntransformation framework also significantly enhances classification\nperformance. Extensive experiments using public datasets are presented, showing\nthat the proposed approach significantly outperforms state-of-the-art methods\nfor subspace clustering and classification.\n", "versions": [{"version": "v1", "created": "Mon, 9 Sep 2013 09:16:02 GMT"}, {"version": "v2", "created": "Sun, 9 Mar 2014 18:50:35 GMT"}], "update_date": "2014-03-11", "authors_parsed": [["Qiu", "Qiang", ""], ["Sapiro", "Guillermo", ""]]}, {"id": "1309.2303", "submitter": "Jing Qian", "authors": "Jing Qian, Venkatesh Saligrama", "title": "Spectral Clustering with Imbalanced Data", "comments": "24 pages, 7 figures. arXiv admin note: substantial text overlap with\n  arXiv:1302.5134", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Spectral clustering is sensitive to how graphs are constructed from data\nparticularly when proximal and imbalanced clusters are present. We show that\nRatio-Cut (RCut) or normalized cut (NCut) objectives are not tailored to\nimbalanced data since they tend to emphasize cut sizes over cut values. We\npropose a graph partitioning problem that seeks minimum cut partitions under\nminimum size constraints on partitions to deal with imbalanced data. Our\napproach parameterizes a family of graphs, by adaptively modulating node\ndegrees on a fixed node set, to yield a set of parameter dependent cuts\nreflecting varying levels of imbalance. The solution to our problem is then\nobtained by optimizing over these parameters. We present rigorous limit cut\nanalysis results to justify our approach. We demonstrate the superiority of our\nmethod through unsupervised and semi-supervised experiments on synthetic and\nreal data sets.\n", "versions": [{"version": "v1", "created": "Mon, 9 Sep 2013 20:04:03 GMT"}], "update_date": "2013-09-11", "authors_parsed": [["Qian", "Jing", ""], ["Saligrama", "Venkatesh", ""]]}, {"id": "1309.2350", "submitter": "Shahin Shahrampour", "authors": "Shahin Shahrampour and Ali Jadbabaie", "title": "Exponentially Fast Parameter Estimation in Networks Using Distributed\n  Dual Averaging", "comments": "6 pages, To appear in Conference on Decision and Control 2013", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.SI math.OC stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper we present an optimization-based view of distributed parameter\nestimation and observational social learning in networks. Agents receive a\nsequence of random, independent and identically distributed (i.i.d.) signals,\neach of which individually may not be informative about the underlying true\nstate, but the signals together are globally informative enough to make the\ntrue state identifiable. Using an optimization-based characterization of\nBayesian learning as proximal stochastic gradient descent (with\nKullback-Leibler divergence from a prior as a proximal function), we show how\nto efficiently use a distributed, online variant of Nesterov's dual averaging\nmethod to solve the estimation with purely local information. When the true\nstate is globally identifiable, and the network is connected, we prove that\nagents eventually learn the true parameter using a randomized gossip scheme. We\ndemonstrate that with high probability the convergence is exponentially fast\nwith a rate dependent on the KL divergence of observations under the true state\nfrom observations under the second likeliest state. Furthermore, our work also\nhighlights the possibility of learning under continuous adaptation of network\nwhich is a consequence of employing constant, unit stepsize for the algorithm.\n", "versions": [{"version": "v1", "created": "Tue, 10 Sep 2013 00:36:44 GMT"}], "update_date": "2013-09-11", "authors_parsed": [["Shahrampour", "Shahin", ""], ["Jadbabaie", "Ali", ""]]}, {"id": "1309.2375", "submitter": "Shai Shalev-Shwartz", "authors": "Shai Shalev-Shwartz and Tong Zhang", "title": "Accelerated Proximal Stochastic Dual Coordinate Ascent for Regularized\n  Loss Minimization", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG cs.NA stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We introduce a proximal version of the stochastic dual coordinate ascent\nmethod and show how to accelerate the method using an inner-outer iteration\nprocedure. We analyze the runtime of the framework and obtain rates that\nimprove state-of-the-art results for various key machine learning optimization\nproblems including SVM, logistic regression, ridge regression, Lasso, and\nmulticlass SVM. Experiments validate our theoretical findings.\n", "versions": [{"version": "v1", "created": "Tue, 10 Sep 2013 05:39:25 GMT"}, {"version": "v2", "created": "Tue, 8 Oct 2013 06:06:09 GMT"}], "update_date": "2013-10-09", "authors_parsed": [["Shalev-Shwartz", "Shai", ""], ["Zhang", "Tong", ""]]}, {"id": "1309.2388", "submitter": "Mark Schmidt", "authors": "Mark Schmidt (SIERRA, LIENS), Nicolas Le Roux (SIERRA, LIENS), Francis\n  Bach (SIERRA, LIENS)", "title": "Minimizing Finite Sums with the Stochastic Average Gradient", "comments": "Revision from January 2015 submission. Major changes: updated\n  literature follow and discussion of subsequent work, additional Lemma showing\n  the validity of one of the formulas, somewhat simplified presentation of\n  Lyapunov bound, included code needed for checking proofs rather than the\n  polynomials generated by the code, added error regions to the numerical\n  experiments", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.OC cs.LG stat.CO stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose the stochastic average gradient (SAG) method for optimizing the\nsum of a finite number of smooth convex functions. Like stochastic gradient\n(SG) methods, the SAG method's iteration cost is independent of the number of\nterms in the sum. However, by incorporating a memory of previous gradient\nvalues the SAG method achieves a faster convergence rate than black-box SG\nmethods. The convergence rate is improved from O(1/k^{1/2}) to O(1/k) in\ngeneral, and when the sum is strongly-convex the convergence rate is improved\nfrom the sub-linear O(1/k) to a linear convergence rate of the form O(p^k) for\np \\textless{} 1. Further, in many cases the convergence rate of the new method\nis also faster than black-box deterministic gradient methods, in terms of the\nnumber of gradient evaluations. Numerical experiments indicate that the new\nalgorithm often dramatically outperforms existing SG and deterministic gradient\nmethods, and that the performance may be further improved through the use of\nnon-uniform sampling strategies.\n", "versions": [{"version": "v1", "created": "Tue, 10 Sep 2013 06:49:15 GMT"}, {"version": "v2", "created": "Wed, 11 May 2016 06:51:31 GMT"}], "update_date": "2016-05-12", "authors_parsed": [["Schmidt", "Mark", "", "SIERRA, LIENS"], ["Roux", "Nicolas Le", "", "SIERRA, LIENS"], ["Bach", "Francis", "", "SIERRA, LIENS"]]}, {"id": "1309.2505", "submitter": "Shahzad Gishkori", "authors": "Shahzad Gishkori, Geert Leus", "title": "Compressed Sensing for Block-Sparse Smooth Signals", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.IT math.IT math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present reconstruction algorithms for smooth signals with block sparsity\nfrom their compressed measurements. We tackle the issue of varying group size\nvia group-sparse least absolute shrinkage selection operator (LASSO) as well as\nvia latent group LASSO regularizations. We achieve smoothness in the signal via\nfusion. We develop low-complexity solvers for our proposed formulations through\nthe alternating direction method of multipliers.\n", "versions": [{"version": "v1", "created": "Tue, 10 Sep 2013 13:38:16 GMT"}], "update_date": "2013-09-11", "authors_parsed": [["Gishkori", "Shahzad", ""], ["Leus", "Geert", ""]]}, {"id": "1309.2765", "submitter": "Patoomsiri Songsiri Ms.", "authors": "Patoomsiri Songsiri, Thimaporn Phetkaew, Boonserm Kijsirikul", "title": "Enhancements of Multi-class Support Vector Machine Construction from\n  Binary Learners using Generalization Performance", "comments": "17 pages, 13 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://creativecommons.org/licenses/by/3.0/", "abstract": "  We propose several novel methods for enhancing the multi-class SVMs by\napplying the generalization performance of binary classifiers as the core idea.\nThis concept will be applied on the existing algorithms, i.e., the Decision\nDirected Acyclic Graph (DDAG), the Adaptive Directed Acyclic Graphs (ADAG), and\nMax Wins. Although in the previous approaches there have been many attempts to\nuse some information such as the margin size and the number of support vectors\nas performance estimators for binary SVMs, they may not accurately reflect the\nactual performance of the binary SVMs. We show that the generalization ability\nevaluated via a cross-validation mechanism is more suitable to directly extract\nthe actual performance of binary SVMs. Our methods are built around this\nperformance measure, and each of them is crafted to overcome the weakness of\nthe previous algorithm. The proposed methods include the Reordering Adaptive\nDirected Acyclic Graph (RADAG), Strong Elimination of the classifiers (SE),\nWeak Elimination of the classifiers (WE), and Voting based Candidate Filtering\n(VCF). Experimental results demonstrate that our methods give significantly\nhigher accuracy than all of the traditional ones. Especially, WE provides\nsignificantly superior results compared to Max Wins which is recognized as the\nstate of the art algorithm in terms of both accuracy and classification speed\nwith two times faster in average.\n", "versions": [{"version": "v1", "created": "Wed, 11 Sep 2013 08:59:07 GMT"}], "update_date": "2013-09-12", "authors_parsed": [["Songsiri", "Patoomsiri", ""], ["Phetkaew", "Thimaporn", ""], ["Kijsirikul", "Boonserm", ""]]}, {"id": "1309.2895", "submitter": "Michael Weylandt", "authors": "Genevera I. Allen and Michael Weylandt", "title": "Sparse and Functional Principal Components Analysis", "comments": "The published version of this paper incorrectly thanks \"Luofeng Luo\"\n  instead of \"Luofeng Liao\" in the Acknowledgements", "journal-ref": "DSW 2019: Proceedings of the IEEE Data Science Workshop 2019, pp.\n  11-16", "doi": "10.1109/DSW.2019.8755778", "report-no": null, "categories": "stat.ML", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Regularized variants of Principal Components Analysis, especially Sparse PCA\nand Functional PCA, are among the most useful tools for the analysis of complex\nhigh-dimensional data. Many examples of massive data, have both sparse and\nfunctional (smooth) aspects and may benefit from a regularization scheme that\ncan capture both forms of structure. For example, in neuro-imaging data, the\nbrain's response to a stimulus may be restricted to a discrete region of\nactivation (spatial sparsity), while exhibiting a smooth response within that\nregion. We propose a unified approach to regularized PCA which can induce both\nsparsity and smoothness in both the row and column principal components. Our\nframework generalizes much of the previous literature, with sparse, functional,\ntwo-way sparse, and two-way functional PCA all being special cases of our\napproach. Our method permits flexible combinations of sparsity and smoothness\nthat lead to improvements in feature selection and signal recovery, as well as\nmore interpretable PCA factors. We demonstrate the efficacy of our method on\nsimulated data and a neuroimaging example on EEG data.\n", "versions": [{"version": "v1", "created": "Wed, 11 Sep 2013 17:18:30 GMT"}, {"version": "v2", "created": "Tue, 16 Apr 2019 16:45:03 GMT"}, {"version": "v3", "created": "Thu, 30 May 2019 21:55:30 GMT"}, {"version": "v4", "created": "Mon, 8 Jul 2019 18:41:49 GMT"}, {"version": "v5", "created": "Mon, 19 Aug 2019 20:18:01 GMT"}], "update_date": "2019-08-21", "authors_parsed": [["Allen", "Genevera I.", ""], ["Weylandt", "Michael", ""]]}, {"id": "1309.3103", "submitter": "Alex Susemihl", "authors": "Chris H\\\"ausler, Alex Susemihl, Martin P Nawrot, Manfred Opper", "title": "Temporal Autoencoding Improves Generative Models of Time Series", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Restricted Boltzmann Machines (RBMs) are generative models which can learn\nuseful representations from samples of a dataset in an unsupervised fashion.\nThey have been widely employed as an unsupervised pre-training method in\nmachine learning. RBMs have been modified to model time series in two main\nways: The Temporal RBM stacks a number of RBMs laterally and introduces\ntemporal dependencies between the hidden layer units; The Conditional RBM, on\nthe other hand, considers past samples of the dataset as a conditional bias and\nlearns a representation which takes these into account. Here we propose a new\ntraining method for both the TRBM and the CRBM, which enforces the dynamic\nstructure of temporal datasets. We do so by treating the temporal models as\ndenoising autoencoders, considering past frames of the dataset as corrupted\nversions of the present frame and minimizing the reconstruction error of the\npresent data by the model. We call this approach Temporal Autoencoding. This\nleads to a significant improvement in the performance of both models in a\nfilling-in-frames task across a number of datasets. The error reduction for\nmotion capture data is 56\\% for the CRBM and 80\\% for the TRBM. Taking the\nposterior mean prediction instead of single samples further improves the\nmodel's estimates, decreasing the error by as much as 91\\% for the CRBM on\nmotion capture data. We also trained the model to perform forecasting on a\nlarge number of datasets and have found TA pretraining to consistently improve\nthe performance of the forecasts. Furthermore, by looking at the prediction\nerror across time, we can see that this improvement reflects a better\nrepresentation of the dynamics of the data as opposed to a bias towards\nreconstructing the observed data on a short time scale.\n", "versions": [{"version": "v1", "created": "Thu, 12 Sep 2013 10:39:50 GMT"}], "update_date": "2013-09-13", "authors_parsed": [["H\u00e4usler", "Chris", ""], ["Susemihl", "Alex", ""], ["Nawrot", "Martin P", ""], ["Opper", "Manfred", ""]]}, {"id": "1309.3223", "submitter": "Shayan Oveis Gharan", "authors": "Shayan Oveis Gharan and Luca Trevisan", "title": "Partitioning into Expanders", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS math.SP stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Let G=(V,E) be an undirected graph, lambda_k be the k-th smallest eigenvalue\nof the normalized laplacian matrix of G. There is a basic fact in algebraic\ngraph theory that lambda_k > 0 if and only if G has at most k-1 connected\ncomponents. We prove a robust version of this fact. If lambda_k>0, then for\nsome 1\\leq \\ell\\leq k-1, V can be {\\em partitioned} into l sets P_1,\\ldots,P_l\nsuch that each P_i is a low-conductance set in G and induces a high conductance\ninduced subgraph. In particular, \\phi(P_i)=O(l^3\\sqrt{\\lambda_l}) and\n\\phi(G[P_i]) >= \\lambda_k/k^2).\n  We make our results algorithmic by designing a simple polynomial time\nspectral algorithm to find such partitioning of G with a quadratic loss in the\ninside conductance of P_i's. Unlike the recent results on higher order\nCheeger's inequality [LOT12,LRTV12], our algorithmic results do not use higher\norder eigenfunctions of G. If there is a sufficiently large gap between\nlambda_k and lambda_{k+1}, more precisely, if \\lambda_{k+1} >= \\poly(k)\nlambda_{k}^{1/4} then our algorithm finds a k partitioning of V into sets\nP_1,...,P_k such that the induced subgraph G[P_i] has a significantly larger\nconductance than the conductance of P_i in G. Such a partitioning may represent\nthe best k clustering of G. Our algorithm is a simple local search that only\nuses the Spectral Partitioning algorithm as a subroutine. We expect to see\nfurther applications of this simple algorithm in clustering applications.\n", "versions": [{"version": "v1", "created": "Thu, 12 Sep 2013 17:28:33 GMT"}, {"version": "v2", "created": "Mon, 14 Oct 2013 17:13:41 GMT"}, {"version": "v3", "created": "Fri, 6 Dec 2013 19:00:57 GMT"}], "update_date": "2013-12-09", "authors_parsed": [["Gharan", "Shayan Oveis", ""], ["Trevisan", "Luca", ""]]}, {"id": "1309.3233", "submitter": "Franz J. Kir\\'aly", "authors": "Franz J. Kir\\'aly", "title": "Efficient Orthogonal Tensor Decomposition, with an Application to Latent\n  Variable Model Learning", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Decomposing tensors into orthogonal factors is a well-known task in\nstatistics, machine learning, and signal processing. We study orthogonal outer\nproduct decompositions where the factors in the summands in the decomposition\nare required to be orthogonal across summands, by relating this orthogonal\ndecomposition to the singular value decompositions of the flattenings. We show\nthat it is a non-trivial assumption for a tensor to have such an orthogonal\ndecomposition, and we show that it is unique (up to natural symmetries) in case\nit exists, in which case we also demonstrate how it can be efficiently and\nreliably obtained by a sequence of singular value decompositions. We\ndemonstrate how the factoring algorithm can be applied for parameter\nidentification in latent variable and mixture models.\n", "versions": [{"version": "v1", "created": "Thu, 12 Sep 2013 18:23:33 GMT"}], "update_date": "2013-09-13", "authors_parsed": [["Kir\u00e1ly", "Franz J.", ""]]}, {"id": "1309.3256", "submitter": "Rachel Ward", "authors": "Abhinav Nellore and Rachel Ward", "title": "Recovery guarantees for exemplar-based clustering", "comments": "24 pages, 4 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  For a certain class of distributions, we prove that the linear programming\nrelaxation of $k$-medoids clustering---a variant of $k$-means clustering where\nmeans are replaced by exemplars from within the dataset---distinguishes points\ndrawn from nonoverlapping balls with high probability once the number of points\ndrawn and the separation distance between any two balls are sufficiently large.\nOur results hold in the nontrivial regime where the separation distance is\nsmall enough that points drawn from different balls may be closer to each other\nthan points drawn from the same ball; in this case, clustering by thresholding\npairwise distances between points can fail. We also exhibit numerical evidence\nof high-probability recovery in a substantially more permissive regime.\n", "versions": [{"version": "v1", "created": "Thu, 12 Sep 2013 19:38:18 GMT"}, {"version": "v2", "created": "Mon, 3 Feb 2014 03:56:31 GMT"}], "update_date": "2014-02-04", "authors_parsed": [["Nellore", "Abhinav", ""], ["Ward", "Rachel", ""]]}, {"id": "1309.3533", "submitter": "Emily Fox", "authors": "Emily B. Fox and Michael I. Jordan", "title": "Mixed Membership Models for Time Series", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this article we discuss some of the consequences of the mixed membership\nperspective on time series analysis. In its most abstract form, a mixed\nmembership model aims to associate an individual entity with some set of\nattributes based on a collection of observed data. Although much of the\nliterature on mixed membership models considers the setting in which\nexchangeable collections of data are associated with each member of a set of\nentities, it is equally natural to consider problems in which an entire time\nseries is viewed as an entity and the goal is to characterize the time series\nin terms of a set of underlying dynamic attributes or \"dynamic regimes\".\nIndeed, this perspective is already present in the classical hidden Markov\nmodel, where the dynamic regimes are referred to as \"states\", and the\ncollection of states realized in a sample path of the underlying process can be\nviewed as a mixed membership characterization of the observed time series. Our\ngoal here is to review some of the richer modeling possibilities for time\nseries that are provided by recent developments in the mixed membership\nframework.\n", "versions": [{"version": "v1", "created": "Fri, 13 Sep 2013 18:31:02 GMT"}], "update_date": "2013-09-16", "authors_parsed": [["Fox", "Emily B.", ""], ["Jordan", "Michael I.", ""]]}, {"id": "1309.3676", "submitter": "Nicolae Cleju", "authors": "Nicolae Cleju", "title": "Optimized projections for compressed sensing via rank-constrained\n  nearest correlation matrix", "comments": "25 pages, 13 figures, to appear in Applied and Computational Harmonic\n  Analysis", "journal-ref": null, "doi": "10.1016/j.acha.2013.08.005", "report-no": null, "categories": "cs.IT cs.LG math.IT stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Optimizing the acquisition matrix is useful for compressed sensing of signals\nthat are sparse in overcomplete dictionaries, because the acquisition matrix\ncan be adapted to the particular correlations of the dictionary atoms. In this\npaper a novel formulation of the optimization problem is proposed, in the form\nof a rank-constrained nearest correlation matrix problem. Furthermore,\nimprovements for three existing optimization algorithms are introduced, which\nare shown to be particular instances of the proposed formulation. Simulation\nresults show notable improvements and superior robustness in sparse signal\nrecovery.\n", "versions": [{"version": "v1", "created": "Sat, 14 Sep 2013 15:08:48 GMT"}], "update_date": "2013-09-17", "authors_parsed": [["Cleju", "Nicolae", ""]]}, {"id": "1309.3699", "submitter": "Ravi Ganti", "authors": "Ravi Ganti and Alexander Gray", "title": "Local Support Vector Machines:Formulation and Analysis", "comments": "12 pages, 1 figure", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.AI cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We provide a formulation for Local Support Vector Machines (LSVMs) that\ngeneralizes previous formulations, and brings out the explicit connections to\nlocal polynomial learning used in nonparametric estimation literature. We\ninvestigate the simplest type of LSVMs called Local Linear Support Vector\nMachines (LLSVMs). For the first time we establish conditions under which\nLLSVMs make Bayes consistent predictions at each test point $x_0$. We also\nestablish rates at which the local risk of LLSVMs converges to the minimum\nvalue of expected local risk at each point $x_0$. Using stability arguments we\nestablish generalization error bounds for LLSVMs.\n", "versions": [{"version": "v1", "created": "Sat, 14 Sep 2013 21:06:22 GMT"}], "update_date": "2018-05-23", "authors_parsed": [["Ganti", "Ravi", ""], ["Gray", "Alexander", ""]]}, {"id": "1309.3809", "submitter": "Ishani Chakraborty", "authors": "Ishani Chakraborty and Ahmed Elgammal", "title": "Visual-Semantic Scene Understanding by Sharing Labels in a Context\n  Network", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider the problem of naming objects in complex, natural scenes\ncontaining widely varying object appearance and subtly different names.\nInformed by cognitive research, we propose an approach based on sharing context\nbased object hypotheses between visual and lexical spaces. To this end, we\npresent the Visual Semantic Integration Model (VSIM) that represents object\nlabels as entities shared between semantic and visual contexts and infers a new\nimage by updating labels through context switching. At the core of VSIM is a\nsemantic Pachinko Allocation Model and a visual nearest neighbor Latent\nDirichlet Allocation Model. For inference, we derive an iterative Data\nAugmentation algorithm that pools the label probabilities and maximizes the\njoint label posterior of an image. Our model surpasses the performance of\nstate-of-art methods in several visual tasks on the challenging SUN09 dataset.\n", "versions": [{"version": "v1", "created": "Mon, 16 Sep 2013 00:22:01 GMT"}], "update_date": "2013-09-17", "authors_parsed": [["Chakraborty", "Ishani", ""], ["Elgammal", "Ahmed", ""]]}, {"id": "1309.3832", "submitter": "Mike Ludkovski", "authors": "Robert B. Gramacy and Mike Ludkovski", "title": "Sequential Design for Optimal Stopping Problems", "comments": "24 pages", "journal-ref": null, "doi": "10.1137/140980089", "report-no": null, "categories": "q-fin.CP q-fin.PR stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a new approach to solve optimal stopping problems via simulation.\nWorking within the backward dynamic programming/Snell envelope framework, we\naugment the methodology of Longstaff-Schwartz that focuses on approximating the\nstopping strategy. Namely, we introduce adaptive generation of the stochastic\ngrids anchoring the simulated sample paths of the underlying state process.\nThis allows for active learning of the classifiers partitioning the state space\ninto the continuation and stopping regions. To this end, we examine sequential\ndesign schemes that adaptively place new design points close to the stopping\nboundaries. We then discuss dynamic regression algorithms that can implement\nsuch recursive estimation and local refinement of the classifiers. The new\nalgorithm is illustrated with a variety of numerical experiments, showing that\nan order of magnitude savings in terms of design size can be achieved. We also\ncompare with existing benchmarks in the context of pricing multi-dimensional\nBermudan options.\n", "versions": [{"version": "v1", "created": "Mon, 16 Sep 2013 05:52:17 GMT"}, {"version": "v2", "created": "Tue, 29 Jul 2014 23:25:14 GMT"}], "update_date": "2015-09-04", "authors_parsed": [["Gramacy", "Robert B.", ""], ["Ludkovski", "Mike", ""]]}, {"id": "1309.4111", "submitter": "Tai Qin", "authors": "Tai Qin, Karl Rohe", "title": "Regularized Spectral Clustering under the Degree-Corrected Stochastic\n  Blockmodel", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Spectral clustering is a fast and popular algorithm for finding clusters in\nnetworks. Recently, Chaudhuri et al. (2012) and Amini et al.(2012) proposed\ninspired variations on the algorithm that artificially inflate the node degrees\nfor improved statistical performance. The current paper extends the previous\nstatistical estimation results to the more canonical spectral clustering\nalgorithm in a way that removes any assumption on the minimum degree and\nprovides guidance on the choice of the tuning parameter. Moreover, our results\nshow how the \"star shape\" in the eigenvectors--a common feature of empirical\nnetworks--can be explained by the Degree-Corrected Stochastic Blockmodel and\nthe Extended Planted Partition model, two statistical models that allow for\nhighly heterogeneous degrees. Throughout, the paper characterizes and justifies\nseveral of the variations of the spectral clustering algorithm in terms of\nthese models.\n", "versions": [{"version": "v1", "created": "Mon, 16 Sep 2013 20:47:51 GMT"}], "update_date": "2013-09-18", "authors_parsed": [["Qin", "Tai", ""], ["Rohe", "Karl", ""]]}, {"id": "1309.4306", "submitter": "Raja Giryes", "authors": "Raja Giryes and Michael Elad", "title": "Sparsity Based Poisson Denoising with Dictionary Learning", "comments": "13 pages, 9 figures", "journal-ref": null, "doi": "10.1109/TIP.2014.2362057", "report-no": null, "categories": "cs.CV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The problem of Poisson denoising appears in various imaging applications,\nsuch as low-light photography, medical imaging and microscopy. In cases of high\nSNR, several transformations exist so as to convert the Poisson noise into an\nadditive i.i.d. Gaussian noise, for which many effective algorithms are\navailable. However, in a low SNR regime, these transformations are\nsignificantly less accurate, and a strategy that relies directly on the true\nnoise statistics is required. A recent work by Salmon et al. took this route,\nproposing a patch-based exponential image representation model based on GMM\n(Gaussian mixture model), leading to state-of-the-art results. In this paper,\nwe propose to harness sparse-representation modeling to the image patches,\nadopting the same exponential idea. Our scheme uses a greedy pursuit with\nboot-strapping based stopping condition and dictionary learning within the\ndenoising process. The reconstruction performance of the proposed scheme is\ncompetitive with leading methods in high SNR, and achieving state-of-the-art\nresults in cases of low SNR.\n", "versions": [{"version": "v1", "created": "Tue, 17 Sep 2013 13:46:26 GMT"}, {"version": "v2", "created": "Mon, 31 Mar 2014 16:16:34 GMT"}, {"version": "v3", "created": "Tue, 14 Oct 2014 13:24:34 GMT"}], "update_date": "2015-06-17", "authors_parsed": [["Giryes", "Raja", ""], ["Elad", "Michael", ""]]}, {"id": "1309.4844", "submitter": "Jing Wang", "authors": "Jing Wang, Daniel Rossell, Christos G. Cassandras, and Ioannis Ch.\n  Paschalidis", "title": "Network Anomaly Detection: A Survey and Comparative Analysis of\n  Stochastic and Deterministic Methods", "comments": "7 pages. 1 more figure than final CDC 2013 version", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG cs.NI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present five methods to the problem of network anomaly detection. These\nmethods cover most of the common techniques in the anomaly detection field,\nincluding Statistical Hypothesis Tests (SHT), Support Vector Machines (SVM) and\nclustering analysis. We evaluate all methods in a simulated network that\nconsists of nominal data, three flow-level anomalies and one packet-level\nattack. Through analyzing the results, we point out the advantages and\ndisadvantages of each method and conclude that combining the results of the\nindividual methods can yield improved anomaly detection results.\n", "versions": [{"version": "v1", "created": "Thu, 19 Sep 2013 03:09:33 GMT"}], "update_date": "2013-09-20", "authors_parsed": [["Wang", "Jing", ""], ["Rossell", "Daniel", ""], ["Cassandras", "Christos G.", ""], ["Paschalidis", "Ioannis Ch.", ""]]}, {"id": "1309.4859", "submitter": "Cosma Rohilla Shalizi", "authors": "Cosma Rohilla Shalizi and Aryeh Kontorovich", "title": "Predictive PAC Learning and Process Decompositions", "comments": "9 pages, accepted in NIPS 2013", "journal-ref": "Advances in Neural Information Processing Systems 26 [NIPS 2013],\n  pp.1619--1627", "doi": null, "report-no": null, "categories": "stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We informally call a stochastic process learnable if it admits a\ngeneralization error approaching zero in probability for any concept class with\nfinite VC-dimension (IID processes are the simplest example). A mixture of\nlearnable processes need not be learnable itself, and certainly its\ngeneralization error need not decay at the same rate. In this paper, we argue\nthat it is natural in predictive PAC to condition not on the past observations\nbut on the mixture component of the sample path. This definition not only\nmatches what a realistic learner might demand, but also allows us to sidestep\nseveral otherwise grave problems in learning from dependent data. In\nparticular, we give a novel PAC generalization bound for mixtures of learnable\nprocesses with a generalization error that is not worse than that of each\nmixture component. We also provide a characterization of mixtures of absolutely\nregular ($\\beta$-mixing) processes, of independent probability-theoretic\ninterest.\n", "versions": [{"version": "v1", "created": "Thu, 19 Sep 2013 04:57:59 GMT"}], "update_date": "2015-07-27", "authors_parsed": [["Shalizi", "Cosma Rohilla", ""], ["Kontorovich", "Aryeh", ""]]}, {"id": "1309.5047", "submitter": "Sean Whalen", "authors": "Sean Whalen and Gaurav Pandey", "title": "A Comparative Analysis of Ensemble Classifiers: Case Studies in Genomics", "comments": "10 pages, 3 figures, 8 tables, to appear in Proceedings of the 2013\n  International Conference on Data Mining", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG q-bio.GN stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The combination of multiple classifiers using ensemble methods is\nincreasingly important for making progress in a variety of difficult prediction\nproblems. We present a comparative analysis of several ensemble methods through\ntwo case studies in genomics, namely the prediction of genetic interactions and\nprotein functions, to demonstrate their efficacy on real-world datasets and\ndraw useful conclusions about their behavior. These methods include simple\naggregation, meta-learning, cluster-based meta-learning, and ensemble selection\nusing heterogeneous classifiers trained on resampled data to improve the\ndiversity of their predictions. We present a detailed analysis of these methods\nacross 4 genomics datasets and find the best of these methods offer\nstatistically significant improvements over the state of the art in their\nrespective domains. In addition, we establish a novel connection between\nensemble selection and meta-learning, demonstrating how both of these disparate\nmethods establish a balance between ensemble diversity and performance.\n", "versions": [{"version": "v1", "created": "Thu, 19 Sep 2013 16:45:18 GMT"}], "update_date": "2013-09-20", "authors_parsed": [["Whalen", "Sean", ""], ["Pandey", "Gaurav", ""]]}, {"id": "1309.5211", "submitter": "Nico Goernitz", "authors": "Georg Zeller, Nico Goernitz, Andre Kahles, Jonas Behr, Pramod\n  Mudrakarta, Soeren Sonnenburg, Gunnar Raetsch", "title": "mTim: Rapid and accurate transcript reconstruction from RNA-Seq data", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "q-bio.GN stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recent advances in high-throughput cDNA sequencing (RNA-Seq) technology have\nrevolutionized transcriptome studies. A major motivation for RNA-Seq is to map\nthe structure of expressed transcripts at nucleotide resolution. With accurate\ncomputational tools for transcript reconstruction, this technology may also\nbecome useful for genome (re-)annotation, which has mostly relied on de novo\ngene finding where gene structures are primarily inferred from the genome\nsequence. We developed a machine-learning method, called mTim (margin-based\ntranscript inference method) for transcript reconstruction from RNA-Seq read\nalignments that is based on discriminatively trained hidden Markov support\nvector machines. In addition to features derived from read alignments, it\nutilizes characteristic genomic sequences, e.g. around splice sites, to improve\ntranscript predictions. mTim inferred transcripts that were highly accurate and\nrelatively robust to alignment errors in comparison to those from Cufflinks, a\nwidely used transcript assembly method.\n", "versions": [{"version": "v1", "created": "Fri, 20 Sep 2013 08:53:52 GMT"}], "update_date": "2013-09-23", "authors_parsed": [["Zeller", "Georg", ""], ["Goernitz", "Nico", ""], ["Kahles", "Andre", ""], ["Behr", "Jonas", ""], ["Mudrakarta", "Pramod", ""], ["Sonnenburg", "Soeren", ""], ["Raetsch", "Gunnar", ""]]}, {"id": "1309.5427", "submitter": "Gang Chen", "authors": "Gang Chen", "title": "Latent Fisher Discriminant Analysis", "comments": "12 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV stat.ML", "license": "http://creativecommons.org/licenses/by/3.0/", "abstract": "  Linear Discriminant Analysis (LDA) is a well-known method for dimensionality\nreduction and classification. Previous studies have also extended the\nbinary-class case into multi-classes. However, many applications, such as\nobject detection and keyframe extraction cannot provide consistent\ninstance-label pairs, while LDA requires labels on instance level for training.\nThus it cannot be directly applied for semi-supervised classification problem.\nIn this paper, we overcome this limitation and propose a latent variable Fisher\ndiscriminant analysis model. We relax the instance-level labeling into\nbag-level, is a kind of semi-supervised (video-level labels of event type are\nrequired for semantic frame extraction) and incorporates a data-driven prior\nover the latent variables. Hence, our method combines the latent variable\ninference and dimension reduction in an unified bayesian framework. We test our\nmethod on MUSK and Corel data sets and yield competitive results compared to\nthe baseline approach. We also demonstrate its capacity on the challenging\nTRECVID MED11 dataset for semantic keyframe extraction and conduct a\nhuman-factors ranking-based experimental evaluation, which clearly demonstrates\nour proposed method consistently extracts more semantically meaningful\nkeyframes than challenging baselines.\n", "versions": [{"version": "v1", "created": "Sat, 21 Sep 2013 03:42:04 GMT"}], "update_date": "2013-09-24", "authors_parsed": [["Chen", "Gang", ""]]}, {"id": "1309.5549", "submitter": "Guanghui Lan", "authors": "Saeed Ghadimi, Guanghui Lan", "title": "Stochastic First- and Zeroth-order Methods for Nonconvex Stochastic\n  Programming", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.OC cs.CC stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we introduce a new stochastic approximation (SA) type\nalgorithm, namely the randomized stochastic gradient (RSG) method, for solving\nan important class of nonlinear (possibly nonconvex) stochastic programming\n(SP) problems. We establish the complexity of this method for computing an\napproximate stationary point of a nonlinear programming problem. We also show\nthat this method possesses a nearly optimal rate of convergence if the problem\nis convex. We discuss a variant of the algorithm which consists of applying a\npost-optimization phase to evaluate a short list of solutions generated by\nseveral independent runs of the RSG method, and show that such modification\nallows to improve significantly the large-deviation properties of the\nalgorithm. These methods are then specialized for solving a class of\nsimulation-based optimization problems in which only stochastic zeroth-order\ninformation is available.\n", "versions": [{"version": "v1", "created": "Sun, 22 Sep 2013 01:35:43 GMT"}], "update_date": "2015-10-27", "authors_parsed": [["Ghadimi", "Saeed", ""], ["Lan", "Guanghui", ""]]}, {"id": "1309.5643", "submitter": "Veronika Cheplygina", "authors": "Veronika Cheplygina, David M. J. Tax, and Marco Loog", "title": "Multiple Instance Learning with Bag Dissimilarities", "comments": "Pattern Recognition, in press", "journal-ref": "Pattern Recognition 48.1 (2015): 264-275", "doi": "10.1016/j.patcog.2014.07.022", "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Multiple instance learning (MIL) is concerned with learning from sets (bags)\nof objects (instances), where the individual instance labels are ambiguous. In\nthis setting, supervised learning cannot be applied directly. Often,\nspecialized MIL methods learn by making additional assumptions about the\nrelationship of the bag labels and instance labels. Such assumptions may fit a\nparticular dataset, but do not generalize to the whole range of MIL problems.\nOther MIL methods shift the focus of assumptions from the labels to the overall\n(dis)similarity of bags, and therefore learn from bags directly. We propose to\nrepresent each bag by a vector of its dissimilarities to other bags in the\ntraining set, and treat these dissimilarities as a feature representation. We\nshow several alternatives to define a dissimilarity between bags and discuss\nwhich definitions are more suitable for particular MIL problems. The\nexperimental results show that the proposed approach is computationally\ninexpensive, yet very competitive with state-of-the-art algorithms on a wide\nrange of MIL datasets.\n", "versions": [{"version": "v1", "created": "Sun, 22 Sep 2013 20:24:50 GMT"}, {"version": "v2", "created": "Thu, 6 Feb 2014 13:13:11 GMT"}, {"version": "v3", "created": "Tue, 12 Aug 2014 09:04:32 GMT"}], "update_date": "2014-12-04", "authors_parsed": [["Cheplygina", "Veronika", ""], ["Tax", "David M. J.", ""], ["Loog", "Marco", ""]]}, {"id": "1309.5701", "submitter": "Tomohiko Mizutani", "authors": "Tomohiko Mizutani", "title": "Ellipsoidal Rounding for Nonnegative Matrix Factorization Under Noisy\n  Separability", "comments": "29 pages, 6 figures. Minor revisions; Revised Figure 2; Added Table 1", "journal-ref": "Journal of Machine Learning Research 15, pp. 1011-1039, 2014", "doi": null, "report-no": null, "categories": "stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a numerical algorithm for nonnegative matrix factorization (NMF)\nproblems under noisy separability. An NMF problem under separability can be\nstated as one of finding all vertices of the convex hull of data points. The\nresearch interest of this paper is to find the vectors as close to the vertices\nas possible in a situation in which noise is added to the data points. Our\nalgorithm is designed to capture the shape of the convex hull of data points by\nusing its enclosing ellipsoid. We show that the algorithm has correctness and\nrobustness properties from theoretical and practical perspectives; correctness\nhere means that if the data points do not contain any noise, the algorithm can\nfind the vertices of their convex hull; robustness means that if the data\npoints contain noise, the algorithm can find the near-vertices. Finally, we\napply the algorithm to document clustering, and report the experimental\nresults.\n", "versions": [{"version": "v1", "created": "Mon, 23 Sep 2013 06:19:54 GMT"}, {"version": "v2", "created": "Wed, 12 Feb 2014 11:35:53 GMT"}], "update_date": "2015-03-06", "authors_parsed": [["Mizutani", "Tomohiko", ""]]}, {"id": "1309.5885", "submitter": "Peter Richtarik", "authors": "Olivier Fercoq and Peter Richt\\'arik", "title": "Smooth minimization of nonsmooth functions with parallel coordinate\n  descent methods", "comments": "39 pages, 1 algorithm, 3 figures, 2 tables", "journal-ref": null, "doi": "10.1007/978-3-030-12119-8_4", "report-no": null, "categories": "cs.DC math.OC stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study the performance of a family of randomized parallel coordinate\ndescent methods for minimizing the sum of a nonsmooth and separable convex\nfunctions. The problem class includes as a special case L1-regularized L1\nregression and the minimization of the exponential loss (\"AdaBoost problem\").\nWe assume the input data defining the loss function is contained in a sparse\n$m\\times n$ matrix $A$ with at most $\\omega$ nonzeros in each row. Our methods\nneed $O(n \\beta/\\tau)$ iterations to find an approximate solution with high\nprobability, where $\\tau$ is the number of processors and $\\beta = 1 +\n(\\omega-1)(\\tau-1)/(n-1)$ for the fastest variant. The notation hides\ndependence on quantities such as the required accuracy and confidence levels\nand the distance of the starting iterate from an optimal point. Since\n$\\beta/\\tau$ is a decreasing function of $\\tau$, the method needs fewer\niterations when more processors are used. Certain variants of our algorithms\nperform on average only $O(\\nnz(A)/n)$ arithmetic operations during a single\niteration per processor and, because $\\beta$ decreases when $\\omega$ does,\nfewer iterations are needed for sparser problems.\n", "versions": [{"version": "v1", "created": "Mon, 23 Sep 2013 17:24:00 GMT"}], "update_date": "2019-04-24", "authors_parsed": [["Fercoq", "Olivier", ""], ["Richt\u00e1rik", "Peter", ""]]}, {"id": "1309.5977", "submitter": "Alexander Rakhlin", "authors": "Hariharan Narayanan, Alexander Rakhlin", "title": "Efficient Sampling from Time-Varying Log-Concave Distributions", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a computationally efficient random walk on a convex body which\nrapidly mixes and closely tracks a time-varying log-concave distribution. We\ndevelop general theoretical guarantees on the required number of steps; this\nnumber can be calculated on the fly according to the distance from and the\nshape of the next distribution. We then illustrate the technique on several\nexamples. Within the context of exponential families, the proposed method\nproduces samples from a posterior distribution which is updated as data arrive\nin a streaming fashion. The sampling technique can be used to track\ntime-varying truncated distributions, as well as to obtain samples from a\nchanging mixture model, fitted in a streaming fashion to data. In the setting\nof linear optimization, the proposed method has oracle complexity with best\nknown dependence on the dimension for certain geometries. In the context of\nonline learning and repeated games, the algorithm is an efficient method for\nimplementing no-regret mixture forecasting strategies. Remarkably, in some of\nthese examples, only one step of the random walk is needed to track the next\ndistribution.\n", "versions": [{"version": "v1", "created": "Mon, 23 Sep 2013 20:40:09 GMT"}], "update_date": "2013-09-25", "authors_parsed": [["Narayanan", "Hariharan", ""], ["Rakhlin", "Alexander", ""]]}, {"id": "1309.5979", "submitter": "Ali Mousavi", "authors": "Ali Mousavi, Arian Maleki, Richard G. Baraniuk", "title": "Asymptotic Analysis of LASSOs Solution Path with Implications for\n  Approximate Message Passing", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST cs.IT math.IT stat.ML stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper concerns the performance of the LASSO (also knows as basis pursuit\ndenoising) for recovering sparse signals from undersampled, randomized, noisy\nmeasurements. We consider the recovery of the signal $x_o \\in \\mathbb{R}^N$\nfrom $n$ random and noisy linear observations $y= Ax_o + w$, where $A$ is the\nmeasurement matrix and $w$ is the noise. The LASSO estimate is given by the\nsolution to the optimization problem $x_o$ with $\\hat{x}_{\\lambda} = \\arg\n\\min_x \\frac{1}{2} \\|y-Ax\\|_2^2 + \\lambda \\|x\\|_1$. Despite major progress in\nthe theoretical analysis of the LASSO solution, little is known about its\nbehavior as a function of the regularization parameter $\\lambda$. In this paper\nwe study two questions in the asymptotic setting (i.e., where $N \\rightarrow\n\\infty$, $n \\rightarrow \\infty$ while the ratio $n/N$ converges to a fixed\nnumber in $(0,1)$): (i) How does the size of the active set\n$\\|\\hat{x}_\\lambda\\|_0/N$ behave as a function of $\\lambda$, and (ii) How does\nthe mean square error $\\|\\hat{x}_{\\lambda} - x_o\\|_2^2/N$ behave as a function\nof $\\lambda$? We then employ these results in a new, reliable algorithm for\nsolving LASSO based on approximate message passing (AMP).\n", "versions": [{"version": "v1", "created": "Mon, 23 Sep 2013 20:45:51 GMT"}], "update_date": "2013-09-26", "authors_parsed": [["Mousavi", "Ali", ""], ["Maleki", "Arian", ""], ["Baraniuk", "Richard G.", ""]]}, {"id": "1309.6013", "submitter": "Wen-Xin Zhou", "authors": "T. Tony Cai and Wen-Xin Zhou", "title": "A Max-Norm Constrained Minimization Approach to 1-Bit Matrix Completion", "comments": "33 pages, 3 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider in this paper the problem of noisy 1-bit matrix completion under\na general non-uniform sampling distribution using the max-norm as a convex\nrelaxation for the rank. A max-norm constrained maximum likelihood estimate is\nintroduced and studied. The rate of convergence for the estimate is obtained.\nInformation-theoretical methods are used to establish a minimax lower bound\nunder the general sampling model. The minimax upper and lower bounds together\nyield the optimal rate of convergence for the Frobenius norm loss.\nComputational algorithms and numerical performance are also discussed.\n", "versions": [{"version": "v1", "created": "Tue, 24 Sep 2013 00:31:41 GMT"}], "update_date": "2013-09-25", "authors_parsed": [["Cai", "T. Tony", ""], ["Zhou", "Wen-Xin", ""]]}, {"id": "1309.6024", "submitter": "Zhao Ren", "authors": "Zhao Ren, Tingni Sun, Cun-Hui Zhang, Harrison H. Zhou", "title": "Asymptotic normality and optimalities in estimation of large Gaussian\n  graphical models", "comments": "Published at http://dx.doi.org/10.1214/14-AOS1286 in the Annals of\n  Statistics (http://www.imstat.org/aos/) by the Institute of Mathematical\n  Statistics (http://www.imstat.org)", "journal-ref": "Annals of Statistics 2015, Vol. 43, No. 3, 991-1026", "doi": "10.1214/14-AOS1286", "report-no": "IMS-AOS-AOS1286", "categories": "math.ST stat.ME stat.ML stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The Gaussian graphical model, a popular paradigm for studying relationship\namong variables in a wide range of applications, has attracted great attention\nin recent years. This paper considers a fundamental question: When is it\npossible to estimate low-dimensional parameters at parametric square-root rate\nin a large Gaussian graphical model? A novel regression approach is proposed to\nobtain asymptotically efficient estimation of each entry of a precision matrix\nunder a sparseness condition relative to the sample size. When the precision\nmatrix is not sufficiently sparse, or equivalently the sample size is not\nsufficiently large, a lower bound is established to show that it is no longer\npossible to achieve the parametric rate in the estimation of each entry. This\nlower bound result, which provides an answer to the delicate sample size\nquestion, is established with a novel construction of a subset of sparse\nprecision matrices in an application of Le Cam's lemma. Moreover, the proposed\nestimator is proven to have optimal convergence rate when the parametric rate\ncannot be achieved, under a minimal sample requirement. The proposed estimator\nis applied to test the presence of an edge in the Gaussian graphical model or\nto recover the support of the entire model, to obtain adaptive rate-optimal\nestimation of the entire precision matrix as measured by the matrix $\\ell_q$\noperator norm and to make inference in latent variables in the graphical model.\nAll of this is achieved under a sparsity condition on the precision matrix and\na side condition on the range of its spectrum. This significantly relaxes the\ncommonly imposed uniform signal strength condition on the precision matrix,\nirrepresentability condition on the Hessian tensor operator of the covariance\nmatrix or the $\\ell_1$ constraint on the precision matrix. Numerical results\nconfirm our theoretical findings. The ROC curve of the proposed algorithm,\nAsymptotic Normal Thresholding (ANT), for support recovery significantly\noutperforms that of the popular GLasso algorithm.\n", "versions": [{"version": "v1", "created": "Tue, 24 Sep 2013 01:58:23 GMT"}, {"version": "v2", "created": "Wed, 25 Sep 2013 18:16:51 GMT"}, {"version": "v3", "created": "Wed, 3 Jun 2015 05:08:24 GMT"}], "update_date": "2015-06-04", "authors_parsed": [["Ren", "Zhao", ""], ["Sun", "Tingni", ""], ["Zhang", "Cun-Hui", ""], ["Zhou", "Harrison H.", ""]]}, {"id": "1309.6158", "submitter": "Giovanni Montana", "authors": "Aaron Sim, Dimosthenis Tsagkrasoulis, Giovanni Montana", "title": "Random Forests on Distance Matrices for Imaging Genetics Studies", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a non-parametric regression methodology, Random Forests on\nDistance Matrices (RFDM), for detecting genetic variants associated to\nquantitative phenotypes representing the human brain's structure or function,\nand obtained using neuroimaging techniques. RFDM, which is an extension of\ndecision forests, requires a distance matrix as response that encodes all\npair-wise phenotypic distances in the random sample. We discuss ways to learn\nsuch distances directly from the data using manifold learning techniques, and\nhow to define such distances when the phenotypes are non-vectorial objects such\nas brain connectivity networks. We also describe an extension of RFDM to detect\nespistatic effects while keeping the computational complexity low. Extensive\nsimulation results and an application to an imaging genetics study of\nAlzheimer's Disease are presented and discussed.\n", "versions": [{"version": "v1", "created": "Tue, 24 Sep 2013 13:58:16 GMT"}], "update_date": "2013-09-25", "authors_parsed": [["Sim", "Aaron", ""], ["Tsagkrasoulis", "Dimosthenis", ""], ["Montana", "Giovanni", ""]]}, {"id": "1309.6301", "submitter": "Xiangrong Zeng", "authors": "Xiangrong Zeng and M\\'ario A. T. Figueiredo", "title": "Solving OSCAR regularization problems by proximal splitting algorithms", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The OSCAR (octagonal selection and clustering algorithm for regression)\nregularizer consists of a L_1 norm plus a pair-wise L_inf norm (responsible for\nits grouping behavior) and was proposed to encourage group sparsity in\nscenarios where the groups are a priori unknown. The OSCAR regularizer has a\nnon-trivial proximity operator, which limits its applicability. We reformulate\nthis regularizer as a weighted sorted L_1 norm, and propose its grouping\nproximity operator (GPO) and approximate proximity operator (APO), thus making\nstate-of-the-art proximal splitting algorithms (PSAs) available to solve\ninverse problems with OSCAR regularization. The GPO is in fact the APO followed\nby additional grouping and averaging operations, which are costly in time and\nstorage, explaining the reason why algorithms with APO are much faster than\nthat with GPO. The convergences of PSAs with GPO are guaranteed since GPO is an\nexact proximity operator. Although convergence of PSAs with APO is may not be\nguaranteed, we have experimentally found that APO behaves similarly to GPO when\nthe regularization parameter of the pair-wise L_inf norm is set to an\nappropriately small value. Experiments on recovery of group-sparse signals\n(with unknown groups) show that PSAs with APO are very fast and accurate.\n", "versions": [{"version": "v1", "created": "Tue, 24 Sep 2013 19:48:56 GMT"}, {"version": "v2", "created": "Fri, 27 Sep 2013 19:36:41 GMT"}], "update_date": "2013-09-30", "authors_parsed": [["Zeng", "Xiangrong", ""], ["Figueiredo", "M\u00e1rio A. T.", ""]]}, {"id": "1309.6415", "submitter": "Henrik Nyman", "authors": "Henrik Nyman, Johan Pensar, Timo Koski, Jukka Corander", "title": "Stratified Graphical Models - Context-Specific Independence in Graphical\n  Models", "comments": "19 pages, 7 png figures. In version two the women and mathematics\n  example is replaced with a parliament election data example. Version two\n  contains 22 pages and 8 png figures", "journal-ref": null, "doi": "10.1214/14-BA882", "report-no": null, "categories": "stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Theory of graphical models has matured over more than three decades to\nprovide the backbone for several classes of models that are used in a myriad of\napplications such as genetic mapping of diseases, credit risk evaluation,\nreliability and computer security, etc. Despite of their generic applicability\nand wide adoptance, the constraints imposed by undirected graphical models and\nBayesian networks have also been recognized to be unnecessarily stringent under\ncertain circumstances. This observation has led to the proposal of several\ngeneralizations that aim at more relaxed constraints by which the models can\nimpose local or context-specific dependence structures. Here we consider an\nadditional class of such models, termed as stratified graphical models. We\ndevelop a method for Bayesian learning of these models by deriving an\nanalytical expression for the marginal likelihood of data under a specific\nsubclass of decomposable stratified models. A non-reversible Markov chain Monte\nCarlo approach is further used to identify models that are highly supported by\nthe posterior distribution over the model space. Our method is illustrated and\ncompared with ordinary graphical models through application to several real and\nsynthetic datasets.\n", "versions": [{"version": "v1", "created": "Wed, 25 Sep 2013 07:30:18 GMT"}, {"version": "v2", "created": "Thu, 14 Nov 2013 15:28:48 GMT"}], "update_date": "2014-11-13", "authors_parsed": [["Nyman", "Henrik", ""], ["Pensar", "Johan", ""], ["Koski", "Timo", ""], ["Corander", "Jukka", ""]]}, {"id": "1309.6487", "submitter": "Xi Peng", "authors": "Xi Peng, Huajin Tang, Lei Zhang, Zhang Yi, and Shijie Xiao", "title": "A Unified Framework for Representation-based Subspace Clustering of\n  Out-of-sample and Large-scale Data", "comments": "in IEEE Trans. on Neural Networks and Learning Systems, 2015", "journal-ref": "IEEE Trans. on Neural Networks and Learning Systems, vol. 27, no.\n  12, pp. 2499-2512, Dec. 2016", "doi": "10.1109/TNNLS.2015.2490080", "report-no": null, "categories": "cs.LG cs.CV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Under the framework of spectral clustering, the key of subspace clustering is\nbuilding a similarity graph which describes the neighborhood relations among\ndata points. Some recent works build the graph using sparse, low-rank, and\n$\\ell_2$-norm-based representation, and have achieved state-of-the-art\nperformance. However, these methods have suffered from the following two\nlimitations. First, the time complexities of these methods are at least\nproportional to the cube of the data size, which make those methods inefficient\nfor solving large-scale problems. Second, they cannot cope with out-of-sample\ndata that are not used to construct the similarity graph. To cluster each\nout-of-sample datum, the methods have to recalculate the similarity graph and\nthe cluster membership of the whole data set. In this paper, we propose a\nunified framework which makes representation-based subspace clustering\nalgorithms feasible to cluster both out-of-sample and large-scale data. Under\nour framework, the large-scale problem is tackled by converting it as\nout-of-sample problem in the manner of \"sampling, clustering, coding, and\nclassifying\". Furthermore, we give an estimation for the error bounds by\ntreating each subspace as a point in a hyperspace. Extensive experimental\nresults on various benchmark data sets show that our methods outperform several\nrecently-proposed scalable methods in clustering large-scale data set.\n", "versions": [{"version": "v1", "created": "Wed, 25 Sep 2013 12:53:13 GMT"}, {"version": "v2", "created": "Fri, 30 Oct 2015 14:43:50 GMT"}], "update_date": "2017-05-17", "authors_parsed": [["Peng", "Xi", ""], ["Tang", "Huajin", ""], ["Zhang", "Lei", ""], ["Yi", "Zhang", ""], ["Xiao", "Shijie", ""]]}, {"id": "1309.6707", "submitter": "Cem Tekin", "authors": "Cem Tekin, Simpson Zhang, Mihaela van der Schaar", "title": "Distributed Online Learning in Social Recommender Systems", "comments": null, "journal-ref": "Selected Topics in Signal Processing, IEEE Journal of , vol.8,\n  no.4, pp.638,652, Aug. 2014", "doi": "10.1109/JSTSP.2014.2299517", "report-no": null, "categories": "cs.SI cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we consider decentralized sequential decision making in\ndistributed online recommender systems, where items are recommended to users\nbased on their search query as well as their specific background including\nhistory of bought items, gender and age, all of which comprise the context\ninformation of the user. In contrast to centralized recommender systems, in\nwhich there is a single centralized seller who has access to the complete\ninventory of items as well as the complete record of sales and user\ninformation, in decentralized recommender systems each seller/learner only has\naccess to the inventory of items and user information for its own products and\nnot the products and user information of other sellers, but can get commission\nif it sells an item of another seller. Therefore the sellers must distributedly\nfind out for an incoming user which items to recommend (from the set of own\nitems or items of another seller), in order to maximize the revenue from own\nsales and commissions. We formulate this problem as a cooperative contextual\nbandit problem, analytically bound the performance of the sellers compared to\nthe best recommendation strategy given the complete realization of user\narrivals and the inventory of items, as well as the context-dependent purchase\nprobabilities of each item, and verify our results via numerical examples on a\ndistributed data set adapted based on Amazon data. We evaluate the dependence\nof the performance of a seller on the inventory of items the seller has, the\nnumber of connections it has with the other sellers, and the commissions which\nthe seller gets by selling items of other sellers to its users.\n", "versions": [{"version": "v1", "created": "Thu, 26 Sep 2013 02:01:44 GMT"}, {"version": "v2", "created": "Wed, 22 Jan 2014 02:42:52 GMT"}], "update_date": "2015-03-25", "authors_parsed": [["Tekin", "Cem", ""], ["Zhang", "Simpson", ""], ["van der Schaar", "Mihaela", ""]]}, {"id": "1309.6779", "submitter": "Jonas Peters", "authors": "Jonas Peters, Joris Mooij, Dominik Janzing, Bernhard Sch\\\"olkopf", "title": "Causal Discovery with Continuous Additive Noise Models", "comments": null, "journal-ref": "Journal of Machine Learning Research 15:2009-2053, 2014", "doi": null, "report-no": null, "categories": "stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider the problem of learning causal directed acyclic graphs from an\nobservational joint distribution. One can use these graphs to predict the\noutcome of interventional experiments, from which data are often not available.\nWe show that if the observational distribution follows a structural equation\nmodel with an additive noise structure, the directed acyclic graph becomes\nidentifiable from the distribution under mild conditions. This constitutes an\ninteresting alternative to traditional methods that assume faithfulness and\nidentify only the Markov equivalence class of the graph, thus leaving some\nedges undirected. We provide practical algorithms for finitely many samples,\nRESIT (Regression with Subsequent Independence Test) and two methods based on\nan independence score. We prove that RESIT is correct in the population setting\nand provide an empirical evaluation.\n", "versions": [{"version": "v1", "created": "Thu, 26 Sep 2013 10:04:45 GMT"}, {"version": "v2", "created": "Sat, 16 Nov 2013 20:29:36 GMT"}, {"version": "v3", "created": "Mon, 13 Jan 2014 19:24:27 GMT"}, {"version": "v4", "created": "Sun, 6 Apr 2014 14:41:28 GMT"}], "update_date": "2016-08-18", "authors_parsed": [["Peters", "Jonas", ""], ["Mooij", "Joris", ""], ["Janzing", "Dominik", ""], ["Sch\u00f6lkopf", "Bernhard", ""]]}, {"id": "1309.6786", "submitter": "Ulrich Paquet", "authors": "Ulrich Paquet, Noam Koenigstein", "title": "One-class Collaborative Filtering with Random Graphs: Annotated Version", "comments": "11 pages, 7 figures. Detailed, annotated and expanded version of\n  conference paper \"One-class Collaborative Filtering with Random Graphs\" (WWW\n  2013)", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The bane of one-class collaborative filtering is interpreting and modelling\nthe latent signal from the missing class. In this paper we present a novel\nBayesian generative model for implicit collaborative filtering. It forms a core\ncomponent of the Xbox Live architecture, and unlike previous approaches,\ndelineates the odds of a user disliking an item from simply not considering it.\nThe latent signal is treated as an unobserved random graph connecting users\nwith items they might have encountered. We demonstrate how large-scale\ndistributed learning can be achieved through a combination of stochastic\ngradient descent and mean field variational inference over random graph\nsamples. A fine-grained comparison is done against a state of the art baseline\non real world data.\n", "versions": [{"version": "v1", "created": "Thu, 26 Sep 2013 10:32:43 GMT"}, {"version": "v2", "created": "Fri, 25 Oct 2013 13:45:46 GMT"}, {"version": "v3", "created": "Mon, 21 Jul 2014 08:50:30 GMT"}, {"version": "v4", "created": "Wed, 24 Sep 2014 09:25:09 GMT"}], "update_date": "2014-09-25", "authors_parsed": [["Paquet", "Ulrich", ""], ["Koenigstein", "Noam", ""]]}, {"id": "1309.6811", "submitter": "Tameem Adel", "authors": "Tameem Adel, Benn Smith, Ruth Urner, Daniel Stashuk, Daniel J. Lizotte", "title": "Generative Multiple-Instance Learning Models For Quantitative\n  Electromyography", "comments": "Appears in Proceedings of the Twenty-Ninth Conference on Uncertainty\n  in Artificial Intelligence (UAI2013)", "journal-ref": null, "doi": null, "report-no": "UAI-P-2013-PG-2-11", "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a comprehensive study of the use of generative modeling approaches\nfor Multiple-Instance Learning (MIL) problems. In MIL a learner receives\ntraining instances grouped together into bags with labels for the bags only\n(which might not be correct for the comprised instances). Our work was\nmotivated by the task of facilitating the diagnosis of neuromuscular disorders\nusing sets of motor unit potential trains (MUPTs) detected within a muscle\nwhich can be cast as a MIL problem. Our approach leads to a state-of-the-art\nsolution to the problem of muscle classification. By introducing and analyzing\ngenerative models for MIL in a general framework and examining a variety of\nmodel structures and components, our work also serves as a methodological guide\nto modelling MIL tasks. We evaluate our proposed methods both on MUPT datasets\nand on the MUSK1 dataset, one of the most widely used benchmarks for MIL.\n", "versions": [{"version": "v1", "created": "Thu, 26 Sep 2013 12:26:53 GMT"}], "update_date": "2013-09-27", "authors_parsed": [["Adel", "Tameem", ""], ["Smith", "Benn", ""], ["Urner", "Ruth", ""], ["Stashuk", "Daniel", ""], ["Lizotte", "Daniel J.", ""]]}, {"id": "1309.6812", "submitter": "Saeed Amizadeh", "authors": "Saeed Amizadeh, Bo Thiesson, Milos Hauskrecht", "title": "The Bregman Variational Dual-Tree Framework", "comments": "Appears in Proceedings of the Twenty-Ninth Conference on Uncertainty\n  in Artificial Intelligence (UAI2013)", "journal-ref": null, "doi": null, "report-no": "UAI-P-2013-PG-22-31", "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Graph-based methods provide a powerful tool set for many non-parametric\nframeworks in Machine Learning. In general, the memory and computational\ncomplexity of these methods is quadratic in the number of examples in the data\nwhich makes them quickly infeasible for moderate to large scale datasets. A\nsignificant effort to find more efficient solutions to the problem has been\nmade in the literature. One of the state-of-the-art methods that has been\nrecently introduced is the Variational Dual-Tree (VDT) framework. Despite some\nof its unique features, VDT is currently restricted only to Euclidean spaces\nwhere the Euclidean distance quantifies the similarity. In this paper, we\nextend the VDT framework beyond the Euclidean distance to more general Bregman\ndivergences that include the Euclidean distance as a special case. By\nexploiting the properties of the general Bregman divergence, we show how the\nnew framework can maintain all the pivotal features of the VDT framework and\nyet significantly improve its performance in non-Euclidean domains. We apply\nthe proposed framework to different text categorization problems and\ndemonstrate its benefits over the original VDT.\n", "versions": [{"version": "v1", "created": "Thu, 26 Sep 2013 12:28:35 GMT"}], "update_date": "2013-09-27", "authors_parsed": [["Amizadeh", "Saeed", ""], ["Thiesson", "Bo", ""], ["Hauskrecht", "Milos", ""]]}, {"id": "1309.6813", "submitter": "Stephen Bach", "authors": "Stephen Bach, Bert Huang, Ben London, Lise Getoor", "title": "Hinge-loss Markov Random Fields: Convex Inference for Structured\n  Prediction", "comments": "Appears in Proceedings of the Twenty-Ninth Conference on Uncertainty\n  in Artificial Intelligence (UAI2013)", "journal-ref": null, "doi": null, "report-no": "UAI-P-2013-PG-32-41", "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Graphical models for structured domains are powerful tools, but the\ncomputational complexities of combinatorial prediction spaces can force\nrestrictions on models, or require approximate inference in order to be\ntractable. Instead of working in a combinatorial space, we use hinge-loss\nMarkov random fields (HL-MRFs), an expressive class of graphical models with\nlog-concave density functions over continuous variables, which can represent\nconfidences in discrete predictions. This paper demonstrates that HL-MRFs are\ngeneral tools for fast and accurate structured prediction. We introduce the\nfirst inference algorithm that is both scalable and applicable to the full\nclass of HL-MRFs, and show how to train HL-MRFs with several learning\nalgorithms. Our experiments show that HL-MRFs match or surpass the predictive\nperformance of state-of-the-art methods, including discrete models, in four\napplication domains.\n", "versions": [{"version": "v1", "created": "Thu, 26 Sep 2013 12:28:52 GMT"}], "update_date": "2013-09-27", "authors_parsed": [["Bach", "Stephen", ""], ["Huang", "Bert", ""], ["London", "Ben", ""], ["Getoor", "Lise", ""]]}, {"id": "1309.6814", "submitter": "Krishnakumar Balasubramanian", "authors": "Krishnakumar Balasubramanian, Kai Yu, Tong Zhang", "title": "High-dimensional Joint Sparsity Random Effects Model for Multi-task\n  Learning", "comments": "Appears in Proceedings of the Twenty-Ninth Conference on Uncertainty\n  in Artificial Intelligence (UAI2013)", "journal-ref": null, "doi": null, "report-no": "UAI-P-2013-PG-42-51", "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Joint sparsity regularization in multi-task learning has attracted much\nattention in recent years. The traditional convex formulation employs the group\nLasso relaxation to achieve joint sparsity across tasks. Although this approach\nleads to a simple convex formulation, it suffers from several issues due to the\nlooseness of the relaxation. To remedy this problem, we view jointly sparse\nmulti-task learning as a specialized random effects model, and derive a convex\nrelaxation approach that involves two steps. The first step learns the\ncovariance matrix of the coefficients using a convex formulation which we refer\nto as sparse covariance coding; the second step solves a ridge regression\nproblem with a sparse quadratic regularizer based on the covariance matrix\nobtained in the first step. It is shown that this approach produces an\nasymptotically optimal quadratic regularizer in the multitask learning setting\nwhen the number of tasks approaches infinity. Experimental results demonstrate\nthat the convex formulation obtained via the proposed model significantly\noutperforms group Lasso (and related multi-stage formulations\n", "versions": [{"version": "v1", "created": "Thu, 26 Sep 2013 12:29:19 GMT"}], "update_date": "2013-09-27", "authors_parsed": [["Balasubramanian", "Krishnakumar", ""], ["Yu", "Kai", ""], ["Zhang", "Tong", ""]]}, {"id": "1309.6818", "submitter": "Jakramate Bootkrajang", "authors": "Jakramate Bootkrajang, Ata Kaban", "title": "Boosting in the presence of label noise", "comments": "Appears in Proceedings of the Twenty-Ninth Conference on Uncertainty\n  in Artificial Intelligence (UAI2013)", "journal-ref": null, "doi": null, "report-no": "UAI-P-2013-PG-82-91", "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Boosting is known to be sensitive to label noise. We studied two approaches\nto improve AdaBoost's robustness against labelling errors. One is to employ a\nlabel-noise robust classifier as a base learner, while the other is to modify\nthe AdaBoost algorithm to be more robust. Empirical evaluation shows that a\ncommittee of robust classifiers, although converges faster than non label-noise\naware AdaBoost, is still susceptible to label noise. However, pairing it with\nthe new robust Boosting algorithm we propose here results in a more resilient\nalgorithm under mislabelling.\n", "versions": [{"version": "v1", "created": "Thu, 26 Sep 2013 12:35:03 GMT"}], "update_date": "2013-09-27", "authors_parsed": [["Bootkrajang", "Jakramate", ""], ["Kaban", "Ata", ""]]}, {"id": "1309.6819", "submitter": "Byron Boots", "authors": "Byron Boots, Geoffrey Gordon, Arthur Gretton", "title": "Hilbert Space Embeddings of Predictive State Representations", "comments": "Appears in Proceedings of the Twenty-Ninth Conference on Uncertainty\n  in Artificial Intelligence (UAI2013)", "journal-ref": null, "doi": null, "report-no": "UAI-P-2013-PG-92-101", "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Predictive State Representations (PSRs) are an expressive class of models for\ncontrolled stochastic processes. PSRs represent state as a set of predictions\nof future observable events. Because PSRs are defined entirely in terms of\nobservable data, statistically consistent estimates of PSR parameters can be\nlearned efficiently by manipulating moments of observed training data. Most\nlearning algorithms for PSRs have assumed that actions and observations are\nfinite with low cardinality. In this paper, we generalize PSRs to infinite sets\nof observations and actions, using the recent concept of Hilbert space\nembeddings of distributions. The essence is to represent the state as a\nnonparametric conditional embedding operator in a Reproducing Kernel Hilbert\nSpace (RKHS) and leverage recent work in kernel methods to estimate, predict,\nand update the representation. We show that these Hilbert space embeddings of\nPSRs are able to gracefully handle continuous actions and observations, and\nthat our learned models outperform competing system identification algorithms\non several prediction benchmarks.\n", "versions": [{"version": "v1", "created": "Thu, 26 Sep 2013 12:35:19 GMT"}], "update_date": "2013-09-27", "authors_parsed": [["Boots", "Byron", ""], ["Gordon", "Geoffrey", ""], ["Gretton", "Arthur", ""]]}, {"id": "1309.6820", "submitter": "Eliot Brenner", "authors": "Eliot Brenner, David Sontag", "title": "SparsityBoost: A New Scoring Function for Learning Bayesian Network\n  Structure", "comments": "Appears in Proceedings of the Twenty-Ninth Conference on Uncertainty\n  in Artificial Intelligence (UAI2013)", "journal-ref": null, "doi": null, "report-no": "UAI-P-2013-PG-112-121", "categories": "cs.LG cs.AI stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We give a new consistent scoring function for structure learning of Bayesian\nnetworks. In contrast to traditional approaches to scorebased structure\nlearning, such as BDeu or MDL, the complexity penalty that we propose is\ndata-dependent and is given by the probability that a conditional independence\ntest correctly shows that an edge cannot exist. What really distinguishes this\nnew scoring function from earlier work is that it has the property of becoming\ncomputationally easier to maximize as the amount of data increases. We prove a\npolynomial sample complexity result, showing that maximizing this score is\nguaranteed to correctly learn a structure with no false edges and a\ndistribution close to the generating distribution, whenever there exists a\nBayesian network which is a perfect map for the data generating distribution.\nAlthough the new score can be used with any search algorithm, we give empirical\nresults showing that it is particularly effective when used together with a\nlinear programming relaxation approach to Bayesian network structure learning.\n", "versions": [{"version": "v1", "created": "Thu, 26 Sep 2013 12:35:41 GMT"}], "update_date": "2013-09-27", "authors_parsed": [["Brenner", "Eliot", ""], ["Sontag", "David", ""]]}, {"id": "1309.6821", "submitter": "Emma Brunskill", "authors": "Emma Brunskill, Lihong Li", "title": "Sample Complexity of Multi-task Reinforcement Learning", "comments": "Appears in Proceedings of the Twenty-Ninth Conference on Uncertainty\n  in Artificial Intelligence (UAI2013)", "journal-ref": null, "doi": null, "report-no": "UAI-P-2013-PG-122-131", "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Transferring knowledge across a sequence of reinforcement-learning tasks is\nchallenging, and has a number of important applications. Though there is\nencouraging empirical evidence that transfer can improve performance in\nsubsequent reinforcement-learning tasks, there has been very little theoretical\nanalysis. In this paper, we introduce a new multi-task algorithm for a sequence\nof reinforcement-learning tasks when each task is sampled independently from\n(an unknown) distribution over a finite set of Markov decision processes whose\nparameters are initially unknown. For this setting, we prove under certain\nassumptions that the per-task sample complexity of exploration is reduced\nsignificantly due to transfer compared to standard single-task algorithms. Our\nmulti-task algorithm also has the desired characteristic that it is guaranteed\nnot to exhibit negative transfer: in the worst case its per-task sample\ncomplexity is comparable to the corresponding single-task algorithm.\n", "versions": [{"version": "v1", "created": "Thu, 26 Sep 2013 12:36:00 GMT"}], "update_date": "2013-09-27", "authors_parsed": [["Brunskill", "Emma", ""], ["Li", "Lihong", ""]]}, {"id": "1309.6823", "submitter": "Hao Cheng", "authors": "Hao Cheng, Xinhua Zhang, Dale Schuurmans", "title": "Convex Relaxations of Bregman Divergence Clustering", "comments": "Appears in Proceedings of the Twenty-Ninth Conference on Uncertainty\n  in Artificial Intelligence (UAI2013)", "journal-ref": null, "doi": null, "report-no": "UAI-P-2013-PG-162-171", "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Although many convex relaxations of clustering have been proposed in the past\ndecade, current formulations remain restricted to spherical Gaussian or\ndiscriminative models and are susceptible to imbalanced clusters. To address\nthese shortcomings, we propose a new class of convex relaxations that can be\nflexibly applied to more general forms of Bregman divergence clustering. By\nbasing these new formulations on normalized equivalence relations we retain\nadditional control on relaxation quality, which allows improvement in\nclustering quality. We furthermore develop optimization methods that improve\nscalability by exploiting recent implicit matrix norm methods. In practice, we\nfind that the new formulations are able to efficiently produce tighter\nclusterings that improve the accuracy of state of the art methods.\n", "versions": [{"version": "v1", "created": "Thu, 26 Sep 2013 12:36:30 GMT"}], "update_date": "2013-09-27", "authors_parsed": [["Cheng", "Hao", ""], ["Zhang", "Xinhua", ""], ["Schuurmans", "Dale", ""]]}, {"id": "1309.6829", "submitter": "Qiang Fu", "authors": "Qiang Fu, Huahua Wang, Arindam Banerjee", "title": "Bethe-ADMM for Tree Decomposition based Parallel MAP Inference", "comments": "Appears in Proceedings of the Twenty-Ninth Conference on Uncertainty\n  in Artificial Intelligence (UAI2013)", "journal-ref": null, "doi": null, "report-no": "UAI-P-2013-PG-222-231", "categories": "cs.AI cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider the problem of maximum a posteriori (MAP) inference in discrete\ngraphical models. We present a parallel MAP inference algorithm called\nBethe-ADMM based on two ideas: tree-decomposition of the graph and the\nalternating direction method of multipliers (ADMM). However, unlike the\nstandard ADMM, we use an inexact ADMM augmented with a Bethe-divergence based\nproximal function, which makes each subproblem in ADMM easy to solve in\nparallel using the sum-product algorithm. We rigorously prove global\nconvergence of Bethe-ADMM. The proposed algorithm is extensively evaluated on\nboth synthetic and real datasets to illustrate its effectiveness. Further, the\nparallel Bethe-ADMM is shown to scale almost linearly with increasing number of\ncores.\n", "versions": [{"version": "v1", "created": "Thu, 26 Sep 2013 12:38:09 GMT"}], "update_date": "2013-09-27", "authors_parsed": [["Fu", "Qiang", ""], ["Wang", "Huahua", ""], ["Banerjee", "Arindam", ""]]}, {"id": "1309.6830", "submitter": "Ravi Ganti", "authors": "Ravi Ganti, Alexander G. Gray", "title": "Building Bridges: Viewing Active Learning from the Multi-Armed Bandit\n  Lens", "comments": "Appears in Proceedings of the Twenty-Ninth Conference on Uncertainty\n  in Artificial Intelligence (UAI2013)", "journal-ref": null, "doi": null, "report-no": "UAI-P-2013-PG-232-241", "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper we propose a multi-armed bandit inspired, pool based active\nlearning algorithm for the problem of binary classification. By carefully\nconstructing an analogy between active learning and multi-armed bandits, we\nutilize ideas such as lower confidence bounds, and self-concordant\nregularization from the multi-armed bandit literature to design our proposed\nalgorithm. Our algorithm is a sequential algorithm, which in each round assigns\na sampling distribution on the pool, samples one point from this distribution,\nand queries the oracle for the label of this sampled point. The design of this\nsampling distribution is also inspired by the analogy between active learning\nand multi-armed bandits. We show how to derive lower confidence bounds required\nby our algorithm. Experimental comparisons to previously proposed active\nlearning algorithms show superior performance on some standard UCI datasets.\n", "versions": [{"version": "v1", "created": "Thu, 26 Sep 2013 12:39:01 GMT"}], "update_date": "2013-09-27", "authors_parsed": [["Ganti", "Ravi", ""], ["Gray", "Alexander G.", ""]]}, {"id": "1309.6831", "submitter": "Alborz Geramifard", "authors": "Alborz Geramifard, Thomas J. Walsh, Nicholas Roy, Jonathan How", "title": "Batch-iFDD for Representation Expansion in Large MDPs", "comments": "Appears in Proceedings of the Twenty-Ninth Conference on Uncertainty\n  in Artificial Intelligence (UAI2013)", "journal-ref": null, "doi": null, "report-no": "UAI-P-2013-PG-242-251", "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Matching pursuit (MP) methods are a promising class of feature construction\nalgorithms for value function approximation. Yet existing MP methods require\ncreating a pool of potential features, mandating expert knowledge or\nenumeration of a large feature pool, both of which hinder scalability. This\npaper introduces batch incremental feature dependency discovery (Batch-iFDD) as\nan MP method that inherits a provable convergence property. Additionally,\nBatch-iFDD does not require a large pool of features, leading to lower\ncomputational complexity. Empirical policy evaluation results across three\ndomains with up to one million states highlight the scalability of Batch-iFDD\nover the previous state of the art MP algorithm.\n", "versions": [{"version": "v1", "created": "Thu, 26 Sep 2013 12:39:19 GMT"}], "update_date": "2013-09-27", "authors_parsed": [["Geramifard", "Alborz", ""], ["Walsh", "Thomas J.", ""], ["Roy", "Nicholas", ""], ["How", "Jonathan", ""]]}, {"id": "1309.6833", "submitter": "Hossein Hajimirsadeghi", "authors": "Hossein Hajimirsadeghi, Jinling Li, Greg Mori, Mohammad Zaki, Tarek\n  Sayed", "title": "Multiple Instance Learning by Discriminative Training of Markov Networks", "comments": "Appears in Proceedings of the Twenty-Ninth Conference on Uncertainty\n  in Artificial Intelligence (UAI2013)", "journal-ref": null, "doi": null, "report-no": "UAI-P-2013-PG-262-271", "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We introduce a graphical framework for multiple instance learning (MIL) based\non Markov networks. This framework can be used to model the traditional MIL\ndefinition as well as more general MIL definitions. Different levels of\nambiguity -- the portion of positive instances in a bag -- can be explored in\nweakly supervised data. To train these models, we propose a discriminative\nmax-margin learning algorithm leveraging efficient inference for\ncardinality-based cliques. The efficacy of the proposed framework is evaluated\non a variety of data sets. Experimental results verify that encoding or\nlearning the degree of ambiguity can improve classification performance.\n", "versions": [{"version": "v1", "created": "Thu, 26 Sep 2013 12:40:19 GMT"}], "update_date": "2013-09-27", "authors_parsed": [["Hajimirsadeghi", "Hossein", ""], ["Li", "Jinling", ""], ["Mori", "Greg", ""], ["Zaki", "Mohammad", ""], ["Sayed", "Tarek", ""]]}, {"id": "1309.6834", "submitter": "Yonatan Halpern", "authors": "Yonatan Halpern, David Sontag", "title": "Unsupervised Learning of Noisy-Or Bayesian Networks", "comments": "Appears in Proceedings of the Twenty-Ninth Conference on Uncertainty\n  in Artificial Intelligence (UAI2013)", "journal-ref": null, "doi": null, "report-no": "UAI-P-2013-PG-272-281", "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper considers the problem of learning the parameters in Bayesian\nnetworks of discrete variables with known structure and hidden variables.\nPrevious approaches in these settings typically use expectation maximization;\nwhen the network has high treewidth, the required expectations might be\napproximated using Monte Carlo or variational methods. We show how to avoid\ninference altogether during learning by giving a polynomial-time algorithm\nbased on the method-of-moments, building upon recent work on learning\ndiscrete-valued mixture models. In particular, we show how to learn the\nparameters for a family of bipartite noisy-or Bayesian networks. In our\nexperimental results, we demonstrate an application of our algorithm to\nlearning QMR-DT, a large Bayesian network used for medical diagnosis. We show\nthat it is possible to fully learn the parameters of QMR-DT even when only the\nfindings are observed in the training data (ground truth diseases unknown).\n", "versions": [{"version": "v1", "created": "Thu, 26 Sep 2013 12:40:36 GMT"}], "update_date": "2013-09-27", "authors_parsed": [["Halpern", "Yonatan", ""], ["Sontag", "David", ""]]}, {"id": "1309.6835", "submitter": "James Hensman", "authors": "James Hensman, Nicolo Fusi, Neil D. Lawrence", "title": "Gaussian Processes for Big Data", "comments": "Appears in Proceedings of the Twenty-Ninth Conference on Uncertainty\n  in Artificial Intelligence (UAI2013)", "journal-ref": null, "doi": null, "report-no": "UAI-P-2013-PG-282-290", "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We introduce stochastic variational inference for Gaussian process models.\nThis enables the application of Gaussian process (GP) models to data sets\ncontaining millions of data points. We show how GPs can be vari- ationally\ndecomposed to depend on a set of globally relevant inducing variables which\nfactorize the model in the necessary manner to perform variational inference.\nOur ap- proach is readily extended to models with non-Gaussian likelihoods and\nlatent variable models based around Gaussian processes. We demonstrate the\napproach on a simple toy problem and two real world data sets.\n", "versions": [{"version": "v1", "created": "Thu, 26 Sep 2013 12:41:06 GMT"}], "update_date": "2013-09-27", "authors_parsed": [["Hensman", "James", ""], ["Fusi", "Nicolo", ""], ["Lawrence", "Neil D.", ""]]}, {"id": "1309.6838", "submitter": "Jean Honorio", "authors": "Jean Honorio, Tommi S. Jaakkola", "title": "Inverse Covariance Estimation for High-Dimensional Data in Linear Time\n  and Space: Spectral Methods for Riccati and Sparse Models", "comments": "Appears in Proceedings of the Twenty-Ninth Conference on Uncertainty\n  in Artificial Intelligence (UAI2013)", "journal-ref": "Uncertainty in Artificial Intelligence (UAI), 2013", "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose maximum likelihood estimation for learning Gaussian graphical\nmodels with a Gaussian (ell_2^2) prior on the parameters. This is in contrast\nto the commonly used Laplace (ell_1) prior for encouraging sparseness. We show\nthat our optimization problem leads to a Riccati matrix equation, which has a\nclosed form solution. We propose an efficient algorithm that performs a\nsingular value decomposition of the training data. Our algorithm is\nO(NT^2)-time and O(NT)-space for N variables and T samples. Our method is\ntailored to high-dimensional problems (N gg T), in which sparseness promoting\nmethods become intractable. Furthermore, instead of obtaining a single solution\nfor a specific regularization parameter, our algorithm finds the whole solution\npath. We show that the method has logarithmic sample complexity under the\nspiked covariance model. We also propose sparsification of the dense solution\nwith provable performance guarantees. We provide techniques for using our\nlearnt models, such as removing unimportant variables, computing likelihoods\nand conditional distributions. Finally, we show promising results in several\ngene expressions datasets.\n", "versions": [{"version": "v1", "created": "Thu, 26 Sep 2013 12:41:38 GMT"}], "update_date": "2018-11-16", "authors_parsed": [["Honorio", "Jean", ""], ["Jaakkola", "Tommi S.", ""]]}, {"id": "1309.6840", "submitter": "Oluwasanmi Koyejo", "authors": "Oluwasanmi Koyejo, Joydeep Ghosh", "title": "Constrained Bayesian Inference for Low Rank Multitask Learning", "comments": "Appears in Proceedings of the Twenty-Ninth Conference on Uncertainty\n  in Artificial Intelligence (UAI2013)", "journal-ref": null, "doi": null, "report-no": "UAI-P-2013-PG-341-350", "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a novel approach for constrained Bayesian inference. Unlike\ncurrent methods, our approach does not require convexity of the constraint set.\nWe reduce the constrained variational inference to a parametric optimization\nover the feasible set of densities and propose a general recipe for such\nproblems. We apply the proposed constrained Bayesian inference approach to\nmultitask learning subject to rank constraints on the weight matrix. Further,\nconstrained parameter estimation is applied to recover the sparse conditional\nindependence structure encoded by prior precision matrices. Our approach is\nmotivated by reverse inference for high dimensional functional neuroimaging, a\ndomain where the high dimensionality and small number of examples requires the\nuse of constraints to ensure meaningful and effective models. For this\napplication, we propose a model that jointly learns a weight matrix and the\nprior inverse covariance structure between different tasks. We present\nexperimental validation showing that the proposed approach outperforms strong\nbaseline models in terms of predictive performance and structure recovery.\n", "versions": [{"version": "v1", "created": "Thu, 26 Sep 2013 12:42:25 GMT"}], "update_date": "2013-09-27", "authors_parsed": [["Koyejo", "Oluwasanmi", ""], ["Ghosh", "Joydeep", ""]]}, {"id": "1309.6847", "submitter": "Ofer Meshi", "authors": "Ofer Meshi, Elad Eban, Gal Elidan, Amir Globerson", "title": "Learning Max-Margin Tree Predictors", "comments": "Appears in Proceedings of the Twenty-Ninth Conference on Uncertainty\n  in Artificial Intelligence (UAI2013)", "journal-ref": null, "doi": null, "report-no": "UAI-P-2013-PG-411-420", "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Structured prediction is a powerful framework for coping with joint\nprediction of interacting outputs. A central difficulty in using this framework\nis that often the correct label dependence structure is unknown. At the same\ntime, we would like to avoid an overly complex structure that will lead to\nintractable prediction. In this work we address the challenge of learning tree\nstructured predictive models that achieve high accuracy while at the same time\nfacilitate efficient (linear time) inference. We start by proving that this\ntask is in general NP-hard, and then suggest an approximate alternative.\nBriefly, our CRANK approach relies on a novel Circuit-RANK regularizer that\npenalizes non-tree structures and that can be optimized using a CCCP procedure.\nWe demonstrate the effectiveness of our approach on several domains and show\nthat, despite the relative simplicity of the structure, prediction accuracy is\ncompetitive with a fully connected model that is computationally costly at\nprediction time.\n", "versions": [{"version": "v1", "created": "Thu, 26 Sep 2013 12:45:00 GMT"}], "update_date": "2013-09-27", "authors_parsed": [["Meshi", "Ofer", ""], ["Eban", "Elad", ""], ["Elidan", "Gal", ""], ["Globerson", "Amir", ""]]}, {"id": "1309.6849", "submitter": "Joris Mooij", "authors": "Joris Mooij, Tom Heskes", "title": "Cyclic Causal Discovery from Continuous Equilibrium Data", "comments": "Appears in Proceedings of the Twenty-Ninth Conference on Uncertainty\n  in Artificial Intelligence (UAI2013)", "journal-ref": null, "doi": null, "report-no": "UAI-P-2013-PG-431-439", "categories": "cs.LG cs.AI stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a method for learning cyclic causal models from a combination of\nobservational and interventional equilibrium data. Novel aspects of the\nproposed method are its ability to work with continuous data (without assuming\nlinearity) and to deal with feedback loops. Within the context of biochemical\nreactions, we also propose a novel way of modeling interventions that modify\nthe activity of compounds instead of their abundance. For computational\nreasons, we approximate the nonlinear causal mechanisms by (coupled) local\nlinearizations, one for each experimental condition. We apply the method to\nreconstruct a cellular signaling network from the flow cytometry data measured\nby Sachs et al. (2005). We show that our method finds evidence in the data for\nfeedback loops and that it gives a more accurate quantitative description of\nthe data at comparable model complexity.\n", "versions": [{"version": "v1", "created": "Thu, 26 Sep 2013 12:45:43 GMT"}], "update_date": "2013-09-27", "authors_parsed": [["Mooij", "Joris", ""], ["Heskes", "Tom", ""]]}, {"id": "1309.6850", "submitter": "Kiyohito Nagano", "authors": "Kiyohito Nagano, Yoshinobu Kawahara", "title": "Structured Convex Optimization under Submodular Constraints", "comments": "Appears in Proceedings of the Twenty-Ninth Conference on Uncertainty\n  in Artificial Intelligence (UAI2013)", "journal-ref": null, "doi": null, "report-no": "UAI-P-2013-PG-459-468", "categories": "cs.LG cs.DS stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A number of discrete and continuous optimization problems in machine learning\nare related to convex minimization problems under submodular constraints. In\nthis paper, we deal with a submodular function with a directed graph structure,\nand we show that a wide range of convex optimization problems under submodular\nconstraints can be solved much more efficiently than general submodular\noptimization methods by a reduction to a maximum flow problem. Furthermore, we\ngive some applications, including sparse optimization methods, in which the\nproposed methods are effective. Additionally, we evaluate the performance of\nthe proposed method through computational experiments.\n", "versions": [{"version": "v1", "created": "Thu, 26 Sep 2013 12:45:59 GMT"}], "update_date": "2013-09-27", "authors_parsed": [["Nagano", "Kiyohito", ""], ["Kawahara", "Yoshinobu", ""]]}, {"id": "1309.6852", "submitter": "Shuzi Niu", "authors": "Shuzi Niu, Yanyan Lan, Jiafeng Guo, Xueqi Cheng", "title": "Stochastic Rank Aggregation", "comments": "Appears in Proceedings of the Twenty-Ninth Conference on Uncertainty\n  in Artificial Intelligence (UAI2013)", "journal-ref": null, "doi": null, "report-no": "UAI-P-2013-PG-478-487", "categories": "cs.LG cs.IR stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper addresses the problem of rank aggregation, which aims to find a\nconsensus ranking among multiple ranking inputs. Traditional rank aggregation\nmethods are deterministic, and can be categorized into explicit and implicit\nmethods depending on whether rank information is explicitly or implicitly\nutilized. Surprisingly, experimental results on real data sets show that\nexplicit rank aggregation methods would not work as well as implicit methods,\nalthough rank information is critical for the task. Our analysis indicates that\nthe major reason might be the unreliable rank information from incomplete\nranking inputs. To solve this problem, we propose to incorporate uncertainty\ninto rank aggregation and tackle the problem in both unsupervised and\nsupervised scenario. We call this novel framework {stochastic rank aggregation}\n(St.Agg for short). Specifically, we introduce a prior distribution on ranks,\nand transform the ranking functions or objectives in traditional explicit\nmethods to their expectations over this distribution. Our experiments on\nbenchmark data sets show that the proposed St.Agg outperforms the baselines in\nboth unsupervised and supervised scenarios.\n", "versions": [{"version": "v1", "created": "Thu, 26 Sep 2013 12:46:39 GMT"}], "update_date": "2013-09-27", "authors_parsed": [["Niu", "Shuzi", ""], ["Lan", "Yanyan", ""], ["Guo", "Jiafeng", ""], ["Cheng", "Xueqi", ""]]}, {"id": "1309.6858", "submitter": "Novi Quadrianto", "authors": "Novi Quadrianto, Viktoriia Sharmanska, David A. Knowles, Zoubin\n  Ghahramani", "title": "The Supervised IBP: Neighbourhood Preserving Infinite Latent Feature\n  Models", "comments": "Appears in Proceedings of the Twenty-Ninth Conference on Uncertainty\n  in Artificial Intelligence (UAI2013)", "journal-ref": null, "doi": null, "report-no": "UAI-P-2013-PG-527-536", "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a probabilistic model to infer supervised latent variables in the\nHamming space from observed data. Our model allows simultaneous inference of\nthe number of binary latent variables, and their values. The latent variables\npreserve neighbourhood structure of the data in a sense that objects in the\nsame semantic concept have similar latent values, and objects in different\nconcepts have dissimilar latent values. We formulate the supervised infinite\nlatent variable problem based on an intuitive principle of pulling objects\ntogether if they are of the same type, and pushing them apart if they are not.\nWe then combine this principle with a flexible Indian Buffet Process prior on\nthe latent variables. We show that the inferred supervised latent variables can\nbe directly used to perform a nearest neighbour search for the purpose of\nretrieval. We introduce a new application of dynamically extending hash codes,\nand show how to effectively couple the structure of the hash codes with\ncontinuously growing structure of the neighbourhood preserving infinite latent\nfeature space.\n", "versions": [{"version": "v1", "created": "Thu, 26 Sep 2013 12:49:02 GMT"}], "update_date": "2013-09-27", "authors_parsed": [["Quadrianto", "Novi", ""], ["Sharmanska", "Viktoriia", ""], ["Knowles", "David A.", ""], ["Ghahramani", "Zoubin", ""]]}, {"id": "1309.6860", "submitter": "Eleni Sgouritsa", "authors": "Eleni Sgouritsa, Dominik Janzing, Jonas Peters, Bernhard Schoelkopf", "title": "Identifying Finite Mixtures of Nonparametric Product Distributions and\n  Causal Inference of Confounders", "comments": "Appears in Proceedings of the Twenty-Ninth Conference on Uncertainty\n  in Artificial Intelligence (UAI2013)", "journal-ref": null, "doi": null, "report-no": "UAI-P-2013-PG-556-565", "categories": "cs.LG cs.AI stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a kernel method to identify finite mixtures of nonparametric\nproduct distributions. It is based on a Hilbert space embedding of the joint\ndistribution. The rank of the constructed tensor is equal to the number of\nmixture components. We present an algorithm to recover the components by\npartitioning the data points into clusters such that the variables are jointly\nconditionally independent given the cluster. This method can be used to\nidentify finite confounders.\n", "versions": [{"version": "v1", "created": "Thu, 26 Sep 2013 12:49:46 GMT"}], "update_date": "2013-09-27", "authors_parsed": [["Sgouritsa", "Eleni", ""], ["Janzing", "Dominik", ""], ["Peters", "Jonas", ""], ["Schoelkopf", "Bernhard", ""]]}, {"id": "1309.6862", "submitter": "Amar Shah", "authors": "Amar Shah, Zoubin Ghahramani", "title": "Determinantal Clustering Processes - A Nonparametric Bayesian Approach\n  to Kernel Based Semi-Supervised Clustering", "comments": "Appears in Proceedings of the Twenty-Ninth Conference on Uncertainty\n  in Artificial Intelligence (UAI2013)", "journal-ref": null, "doi": null, "report-no": "UAI-P-2013-PG-566-575", "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Semi-supervised clustering is the task of clustering data points into\nclusters where only a fraction of the points are labelled. The true number of\nclusters in the data is often unknown and most models require this parameter as\nan input. Dirichlet process mixture models are appealing as they can infer the\nnumber of clusters from the data. However, these models do not deal with high\ndimensional data well and can encounter difficulties in inference. We present a\nnovel nonparameteric Bayesian kernel based method to cluster data points\nwithout the need to prespecify the number of clusters or to model complicated\ndensities from which data points are assumed to be generated from. The key\ninsight is to use determinants of submatrices of a kernel matrix as a measure\nof how close together a set of points are. We explore some theoretical\nproperties of the model and derive a natural Gibbs based algorithm with MCMC\nhyperparameter learning. The model is implemented on a variety of synthetic and\nreal world data sets.\n", "versions": [{"version": "v1", "created": "Thu, 26 Sep 2013 12:50:04 GMT"}], "update_date": "2013-09-27", "authors_parsed": [["Shah", "Amar", ""], ["Ghahramani", "Zoubin", ""]]}, {"id": "1309.6863", "submitter": "Ilya Shpitser", "authors": "Ilya Shpitser, Robin J. Evans, Thomas S. Richardson, James M. Robins", "title": "Sparse Nested Markov models with Log-linear Parameters", "comments": "Appears in Proceedings of the Twenty-Ninth Conference on Uncertainty\n  in Artificial Intelligence (UAI2013)", "journal-ref": null, "doi": null, "report-no": "UAI-P-2013-PG-576-585", "categories": "cs.LG cs.AI stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Hidden variables are ubiquitous in practical data analysis, and therefore\nmodeling marginal densities and doing inference with the resulting models is an\nimportant problem in statistics, machine learning, and causal inference.\nRecently, a new type of graphical model, called the nested Markov model, was\ndeveloped which captures equality constraints found in marginals of directed\nacyclic graph (DAG) models. Some of these constraints, such as the so called\n`Verma constraint', strictly generalize conditional independence. To make\nmodeling and inference with nested Markov models practical, it is necessary to\nlimit the number of parameters in the model, while still correctly capturing\nthe constraints in the marginal of a DAG model. Placing such limits is similar\nin spirit to sparsity methods for undirected graphical models, and regression\nmodels. In this paper, we give a log-linear parameterization which allows\nsparse modeling with nested Markov models. We illustrate the advantages of this\nparameterization with a simulation study.\n", "versions": [{"version": "v1", "created": "Thu, 26 Sep 2013 12:50:19 GMT"}], "update_date": "2013-09-27", "authors_parsed": [["Shpitser", "Ilya", ""], ["Evans", "Robin J.", ""], ["Richardson", "Thomas S.", ""], ["Robins", "James M.", ""]]}, {"id": "1309.6865", "submitter": "Nitish Srivastava", "authors": "Nitish Srivastava, Ruslan R Salakhutdinov, Geoffrey E. Hinton", "title": "Modeling Documents with Deep Boltzmann Machines", "comments": "Appears in Proceedings of the Twenty-Ninth Conference on Uncertainty\n  in Artificial Intelligence (UAI2013)", "journal-ref": null, "doi": null, "report-no": "UAI-P-2013-PG-616-624", "categories": "cs.LG cs.IR stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We introduce a Deep Boltzmann Machine model suitable for modeling and\nextracting latent semantic representations from a large unstructured collection\nof documents. We overcome the apparent difficulty of training a DBM with\njudicious parameter tying. This parameter tying enables an efficient\npretraining algorithm and a state initialization scheme that aids inference.\nThe model can be trained just as efficiently as a standard Restricted Boltzmann\nMachine. Our experiments show that the model assigns better log probability to\nunseen data than the Replicated Softmax model. Features extracted from our\nmodel outperform LDA, Replicated Softmax, and DocNADE models on document\nretrieval and document classification tasks.\n", "versions": [{"version": "v1", "created": "Thu, 26 Sep 2013 12:50:54 GMT"}], "update_date": "2013-09-27", "authors_parsed": [["Srivastava", "Nitish", ""], ["Salakhutdinov", "Ruslan R", ""], ["Hinton", "Geoffrey E.", ""]]}, {"id": "1309.6868", "submitter": "Charles Tripp", "authors": "Charles Tripp, Ross D. Shachter", "title": "Approximate Kalman Filter Q-Learning for Continuous State-Space MDPs", "comments": "Appears in Proceedings of the Twenty-Ninth Conference on Uncertainty\n  in Artificial Intelligence (UAI2013)", "journal-ref": null, "doi": null, "report-no": "UAI-P-2013-PG-644-653", "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We seek to learn an effective policy for a Markov Decision Process (MDP) with\ncontinuous states via Q-Learning. Given a set of basis functions over state\naction pairs we search for a corresponding set of linear weights that minimizes\nthe mean Bellman residual. Our algorithm uses a Kalman filter model to estimate\nthose weights and we have developed a simpler approximate Kalman filter model\nthat outperforms the current state of the art projected TD-Learning methods on\nseveral standard benchmark problems.\n", "versions": [{"version": "v1", "created": "Thu, 26 Sep 2013 12:51:47 GMT"}], "update_date": "2013-09-27", "authors_parsed": [["Tripp", "Charles", ""], ["Shachter", "Ross D.", ""]]}, {"id": "1309.6869", "submitter": "Michal Valko", "authors": "Michal Valko, Nathaniel Korda, Remi Munos, Ilias Flaounas, Nelo\n  Cristianini", "title": "Finite-Time Analysis of Kernelised Contextual Bandits", "comments": "Appears in Proceedings of the Twenty-Ninth Conference on Uncertainty\n  in Artificial Intelligence (UAI2013)", "journal-ref": null, "doi": null, "report-no": "UAI-P-2013-PG-654-663", "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We tackle the problem of online reward maximisation over a large finite set\nof actions described by their contexts. We focus on the case when the number of\nactions is too big to sample all of them even once. However we assume that we\nhave access to the similarities between actions' contexts and that the expected\nreward is an arbitrary linear function of the contexts' images in the related\nreproducing kernel Hilbert space (RKHS). We propose KernelUCB, a kernelised UCB\nalgorithm, and give a cumulative regret bound through a frequentist analysis.\nFor contextual bandits, the related algorithm GP-UCB turns out to be a special\ncase of our algorithm, and our finite-time analysis improves the regret bound\nof GP-UCB for the agnostic case, both in the terms of the kernel-dependent\nquantity and the RKHS norm of the reward function. Moreover, for the linear\nkernel, our regret bound matches the lower bound for contextual linear bandits.\n", "versions": [{"version": "v1", "created": "Thu, 26 Sep 2013 12:52:20 GMT"}], "update_date": "2013-09-27", "authors_parsed": [["Valko", "Michal", ""], ["Korda", "Nathaniel", ""], ["Munos", "Remi", ""], ["Flaounas", "Ilias", ""], ["Cristianini", "Nelo", ""]]}, {"id": "1309.6874", "submitter": "Pengtao Xie", "authors": "Pengtao Xie, Eric P. Xing", "title": "Integrating Document Clustering and Topic Modeling", "comments": "Appears in Proceedings of the Twenty-Ninth Conference on Uncertainty\n  in Artificial Intelligence (UAI2013)", "journal-ref": null, "doi": null, "report-no": "UAI-P-2013-PG-694-703", "categories": "cs.LG cs.CL cs.IR stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Document clustering and topic modeling are two closely related tasks which\ncan mutually benefit each other. Topic modeling can project documents into a\ntopic space which facilitates effective document clustering. Cluster labels\ndiscovered by document clustering can be incorporated into topic models to\nextract local topics specific to each cluster and global topics shared by all\nclusters. In this paper, we propose a multi-grain clustering topic model\n(MGCTM) which integrates document clustering and topic modeling into a unified\nframework and jointly performs the two tasks to achieve the overall best\nperformance. Our model tightly couples two components: a mixture component used\nfor discovering latent groups in document collection and a topic model\ncomponent used for mining multi-grain topics including local topics specific to\neach cluster and global topics shared across clusters.We employ variational\ninference to approximate the posterior of hidden variables and learn model\nparameters. Experiments on two datasets demonstrate the effectiveness of our\nmodel.\n", "versions": [{"version": "v1", "created": "Thu, 26 Sep 2013 12:54:02 GMT"}], "update_date": "2013-09-27", "authors_parsed": [["Xie", "Pengtao", ""], ["Xing", "Eric P.", ""]]}, {"id": "1309.6875", "submitter": "Peilin Zhao", "authors": "Peilin Zhao, Steven Hoi, Jinfeng Zhuang", "title": "Active Learning with Expert Advice", "comments": "Appears in Proceedings of the Twenty-Ninth Conference on Uncertainty\n  in Artificial Intelligence (UAI2013)", "journal-ref": null, "doi": null, "report-no": "UAI-P-2013-PG-704-713", "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Conventional learning with expert advice methods assumes a learner is always\nreceiving the outcome (e.g., class labels) of every incoming training instance\nat the end of each trial. In real applications, acquiring the outcome from\noracle can be costly or time consuming. In this paper, we address a new problem\nof active learning with expert advice, where the outcome of an instance is\ndisclosed only when it is requested by the online learner. Our goal is to learn\nan accurate prediction model by asking the oracle the number of questions as\nsmall as possible. To address this challenge, we propose a framework of active\nforecasters for online active learning with expert advice, which attempts to\nextend two regular forecasters, i.e., Exponentially Weighted Average Forecaster\nand Greedy Forecaster, to tackle the task of active learning with expert\nadvice. We prove that the proposed algorithms satisfy the Hannan consistency\nunder some proper assumptions, and validate the efficacy of our technique by an\nextensive set of experiments.\n", "versions": [{"version": "v1", "created": "Thu, 26 Sep 2013 12:54:31 GMT"}], "update_date": "2013-09-27", "authors_parsed": [["Zhao", "Peilin", ""], ["Hoi", "Steven", ""], ["Zhuang", "Jinfeng", ""]]}, {"id": "1309.6876", "submitter": "Chao Zhang", "authors": "Chao Zhang", "title": "Bennett-type Generalization Bounds: Large-deviation Case and Faster Rate\n  of Convergence", "comments": "Appears in Proceedings of the Twenty-Ninth Conference on Uncertainty\n  in Artificial Intelligence (UAI2013)", "journal-ref": null, "doi": null, "report-no": "UAI-P-2013-PG-714-722", "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we present the Bennett-type generalization bounds of the\nlearning process for i.i.d. samples, and then show that the generalization\nbounds have a faster rate of convergence than the traditional results. In\nparticular, we first develop two types of Bennett-type deviation inequality for\nthe i.i.d. learning process: one provides the generalization bounds based on\nthe uniform entropy number; the other leads to the bounds based on the\nRademacher complexity. We then adopt a new method to obtain the alternative\nexpressions of the Bennett-type generalization bounds, which imply that the\nbounds have a faster rate o(N^{-1/2}) of convergence than the traditional\nresults O(N^{-1/2}). Additionally, we find that the rate of the bounds will\nbecome faster in the large-deviation case, which refers to a situation where\nthe empirical risk is far away from (at least not close to) the expected risk.\nFinally, we analyze the asymptotical convergence of the learning process and\ncompare our analysis with the existing results.\n", "versions": [{"version": "v1", "created": "Thu, 26 Sep 2013 12:54:57 GMT"}], "update_date": "2013-09-27", "authors_parsed": [["Zhang", "Chao", ""]]}, {"id": "1309.6933", "submitter": "Larry Wasserman", "authors": "Larry Wasserman, Mladen Kolar and Alessandro Rinaldo", "title": "Estimating Undirected Graphs Under Weak Assumptions", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST cs.LG stat.ML stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider the problem of providing nonparametric confidence guarantees for\nundirected graphs under weak assumptions. In particular, we do not assume\nsparsity, incoherence or Normality. We allow the dimension $D$ to increase with\nthe sample size $n$. First, we prove lower bounds that show that if we want\naccurate inferences with low assumptions then there are limitations on the\ndimension as a function of sample size. When the dimension increases slowly\nwith sample size, we show that methods based on Normal approximations and on\nthe bootstrap lead to valid inferences and we provide Berry-Esseen bounds on\nthe accuracy of the Normal approximation. When the dimension is large relative\nto sample size, accurate inferences for graphs under low assumptions are not\npossible. Instead we propose to estimate something less demanding than the\nentire partial correlation graph. In particular, we consider: cluster graphs,\nrestricted partial correlation graphs and correlation graphs.\n", "versions": [{"version": "v1", "created": "Thu, 26 Sep 2013 15:18:22 GMT"}], "update_date": "2013-09-27", "authors_parsed": [["Wasserman", "Larry", ""], ["Kolar", "Mladen", ""], ["Rinaldo", "Alessandro", ""]]}, {"id": "1309.7004", "submitter": "Peter L. Spirtes", "authors": "Peter L. Spirtes", "title": "Calculation of Entailed Rank Constraints in Partially Non-Linear and\n  Cyclic Models", "comments": "Appears in Proceedings of the Twenty-Ninth Conference on Uncertainty\n  in Artificial Intelligence (UAI2013)", "journal-ref": null, "doi": null, "report-no": "UAI-P-2013-PG-606-615", "categories": "cs.AI stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The Trek Separation Theorem (Sullivant et al. 2010) states necessary and\nsufficient conditions for a linear directed acyclic graphical model to entail\nfor all possible values of its linear coefficients that the rank of various\nsub-matrices of the covariance matrix is less than or equal to n, for any given\nn. In this paper, I extend the Trek Separation Theorem in two ways: I prove\nthat the same necessary and sufficient conditions apply even when the\ngenerating model is partially non-linear and contains some cycles. This\njustifies application of constraint-based causal search algorithms such as the\nBuildPureClusters algorithm (Silva et al. 2006) for discovering the causal\nstructure of latent variable models to data generated by a wider class of\ncausal models that may contain non-linear and cyclic relations among the latent\nvariables.\n", "versions": [{"version": "v1", "created": "Tue, 17 Sep 2013 13:26:01 GMT"}], "update_date": "2013-09-27", "authors_parsed": [["Spirtes", "Peter L.", ""]]}, {"id": "1309.7311", "submitter": "Peter Orchard", "authors": "Peter Orchard, Felix Agakov, Amos Storkey", "title": "Bayesian Inference in Sparse Gaussian Graphical Models", "comments": null, "journal-ref": null, "doi": "10.1017/S0956796814000057", "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  One of the fundamental tasks of science is to find explainable relationships\nbetween observed phenomena. One approach to this task that has received\nattention in recent years is based on probabilistic graphical modelling with\nsparsity constraints on model structures. In this paper, we describe two new\napproaches to Bayesian inference of sparse structures of Gaussian graphical\nmodels (GGMs). One is based on a simple modification of the cutting-edge block\nGibbs sampler for sparse GGMs, which results in significant computational gains\nin high dimensions. The other method is based on a specific construction of the\nHamiltonian Monte Carlo sampler, which results in further significant\nimprovements. We compare our fully Bayesian approaches with the popular\nregularisation-based graphical LASSO, and demonstrate significant advantages of\nthe Bayesian treatment under the same computing costs. We apply the methods to\na broad range of simulated data sets, and a real-life financial data set.\n", "versions": [{"version": "v1", "created": "Fri, 27 Sep 2013 17:53:57 GMT"}], "update_date": "2014-04-16", "authors_parsed": [["Orchard", "Peter", ""], ["Agakov", "Felix", ""], ["Storkey", "Amos", ""]]}, {"id": "1309.7512", "submitter": "Alexander Fix", "authors": "Alexander Fix and Thorsten Joachims and Sam Park and Ramin Zabih", "title": "Structured learning of sum-of-submodular higher order energy functions", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Submodular functions can be exactly minimized in polynomial time, and the\nspecial case that graph cuts solve with max flow \\cite{KZ:PAMI04} has had\nsignificant impact in computer vision\n\\cite{BVZ:PAMI01,Kwatra:SIGGRAPH03,Rother:GrabCut04}. In this paper we address\nthe important class of sum-of-submodular (SoS) functions\n\\cite{Arora:ECCV12,Kolmogorov:DAM12}, which can be efficiently minimized via a\nvariant of max flow called submodular flow \\cite{Edmonds:ADM77}. SoS functions\ncan naturally express higher order priors involving, e.g., local image patches;\nhowever, it is difficult to fully exploit their expressive power because they\nhave so many parameters. Rather than trying to formulate existing higher order\npriors as an SoS function, we take a discriminative learning approach,\neffectively searching the space of SoS functions for a higher order prior that\nperforms well on our training set. We adopt a structural SVM approach\n\\cite{Joachims/etal/09a,Tsochantaridis/etal/04} and formulate the training\nproblem in terms of quadratic programming; as a result we can efficiently\nsearch the space of SoS priors via an extended cutting-plane algorithm. We also\nshow how the state-of-the-art max flow method for vision problems\n\\cite{Goldberg:ESA11} can be modified to efficiently solve the submodular flow\nproblem. Experimental comparisons are made against the OpenCV implementation of\nthe GrabCut interactive segmentation technique \\cite{Rother:GrabCut04}, which\nuses hand-tuned parameters instead of machine learning. On a standard dataset\n\\cite{Gulshan:CVPR10} our method learns higher order priors with hundreds of\nparameter values, and produces significantly better segmentations. While our\nfocus is on binary labeling problems, we show that our techniques can be\nnaturally generalized to handle more than two labels.\n", "versions": [{"version": "v1", "created": "Sat, 28 Sep 2013 23:55:01 GMT"}, {"version": "v2", "created": "Tue, 1 Oct 2013 02:45:20 GMT"}], "update_date": "2013-10-02", "authors_parsed": [["Fix", "Alexander", ""], ["Joachims", "Thorsten", ""], ["Park", "Sam", ""], ["Zabih", "Ramin", ""]]}, {"id": "1309.7676", "submitter": "Eric Christiansen", "authors": "Eric Christiansen", "title": "An upper bound on prototype set size for condensed nearest neighbor", "comments": "This was submitted to the journal Artificial Intelligence in 2009,\n  and while it was considered technically sound, it was also believed to be of\n  minor importance. My research has since moved on, so I'm unlikely to attempt\n  a resubmission", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://creativecommons.org/licenses/by/3.0/", "abstract": "  The condensed nearest neighbor (CNN) algorithm is a heuristic for reducing\nthe number of prototypical points stored by a nearest neighbor classifier,\nwhile keeping the classification rule given by the reduced prototypical set\nconsistent with the full set. I present an upper bound on the number of\nprototypical points accumulated by CNN. The bound originates in a bound on the\nnumber of times the decision rule is updated during training in the multiclass\nperceptron algorithm, and thus is independent of training set size.\n", "versions": [{"version": "v1", "created": "Sun, 29 Sep 2013 23:45:59 GMT"}], "update_date": "2013-10-01", "authors_parsed": [["Christiansen", "Eric", ""]]}, {"id": "1309.7733", "submitter": "Madan Kundu", "authors": "Madan Gopal Kundu and Jaroslaw Harezlak", "title": "Regression Trees for Longitudinal Data", "comments": null, "journal-ref": "Biostatistics & Epidemiology, 2019", "doi": "10.1080/24709360.2018.1557797", "report-no": null, "categories": "stat.ME stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  While studying response trajectory, often the population of interest may be\ndiverse enough to exist distinct subgroups within it and the longitudinal\nchange in response may not be uniform in these subgroups. That is, the\ntimeslope and/or influence of covariates in longitudinal profile may vary among\nthese different subgroups. For example, Raudenbush (2001) used depression as an\nexample to argue that it is incorrect to assume that all the people in a given\npopulation would be experiencing either increasing or decreasing levels of\ndepression. In such cases, traditional linear mixed effects model (assuming\ncommon parametric form for covariates and time) is not directly applicable for\nthe entire population as a group-averaged trajectory can mask important\nsubgroup differences. Our aim is to identify and characterize longitudinally\nhomogeneous subgroups based on the combination of baseline covariates in the\nmost parsimonious way. This goal can be achieved via constructing regression\ntree for longitudinal data using baseline covariates as partitioning variables.\nWe have proposed LongCART algorithm to construct regression tree for the\nlongitudinal data. In each node, the proposed LongCART algorithm determines the\nneed for further splitting (i.e. whether parameter(s) of longitudinal profile\nis influenced by any baseline attributes) via parameter instability tests and\nthus the decision of further splitting is type-I error controlled. We have\nobtained the asymptotic results for the proposed instability test and examined\nfinite sample behavior of the whole algorithm through simulation studies.\nFinally, we have applied the LongCART algorithm to study the longitudinal\nchanges in choline level among HIV patients.\n", "versions": [{"version": "v1", "created": "Mon, 30 Sep 2013 06:26:50 GMT"}, {"version": "v2", "created": "Tue, 1 Oct 2013 01:07:40 GMT"}, {"version": "v3", "created": "Sun, 13 Jul 2014 14:58:35 GMT"}], "update_date": "2020-06-30", "authors_parsed": [["Kundu", "Madan Gopal", ""], ["Harezlak", "Jaroslaw", ""]]}, {"id": "1309.7804", "submitter": "Michael I. Jordan", "authors": "Michael I. Jordan", "title": "On statistics, computation and scalability", "comments": "Published in at http://dx.doi.org/10.3150/12-BEJSP17 the Bernoulli\n  (http://isi.cbs.nl/bernoulli/) by the International Statistical\n  Institute/Bernoulli Society (http://isi.cbs.nl/BS/bshome.htm)", "journal-ref": "Bernoulli 2013, Vol. 19, No. 4, 1378-1390", "doi": "10.3150/12-BEJSP17", "report-no": "IMS-BEJ-BEJSP17", "categories": "stat.ML cs.LG math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  How should statistical procedures be designed so as to be scalable\ncomputationally to the massive datasets that are increasingly the norm? When\ncoupled with the requirement that an answer to an inferential question be\ndelivered within a certain time budget, this question has significant\nrepercussions for the field of statistics. With the goal of identifying\n\"time-data tradeoffs,\" we investigate some of the statistical consequences of\ncomputational perspectives on scability, in particular divide-and-conquer\nmethodology and hierarchies of convex relaxations.\n", "versions": [{"version": "v1", "created": "Mon, 30 Sep 2013 11:51:23 GMT"}], "update_date": "2013-10-01", "authors_parsed": [["Jordan", "Michael I.", ""]]}, {"id": "1309.7821", "submitter": "Bereket Kindo", "authors": "Bereket P. Kindo, Hao Wang and Edsel A. Pe\\~na", "title": "MPBART - Multinomial Probit Bayesian Additive Regression Trees", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This article proposes Multinomial Probit Bayesian Additive Regression Trees\n(MPBART) as a multinomial probit extension of BART - Bayesian Additive\nRegression Trees (Chipman et al (2010)). MPBART is flexible to allow inclusion\nof predictors that describe the observed units as well as the available choice\nalternatives. Through two simulation studies and four real data examples, we\nshow that MPBART exhibits very good predictive performance in comparison to\nother discrete choice and multiclass classification methods. To implement\nMPBART, we have developed an R package mpbart available freely from CRAN\nrepositories.\n", "versions": [{"version": "v1", "created": "Mon, 30 Sep 2013 12:34:09 GMT"}, {"version": "v2", "created": "Sat, 6 Feb 2016 19:16:06 GMT"}], "update_date": "2016-02-09", "authors_parsed": [["Kindo", "Bereket P.", ""], ["Wang", "Hao", ""], ["Pe\u00f1a", "Edsel A.", ""]]}, {"id": "1309.7857", "submitter": "Aleksandr Aravkin", "authors": "Aleksandr Y. Aravkin, James V. Burke, and Gianluigi Pillonetto", "title": "Generalized system identification with stable spline kernels", "comments": "23 pages, 6 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML math.OC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Regularized least-squares approaches have been successfully applied to linear\nsystem identification. Recent approaches use quadratic penalty terms on the\nunknown impulse response defined by stable spline kernels, which control model\nspace complexity by leveraging regularity and bounded-input bounded-output\nstability. This paper extends linear system identification to a wide class of\nnonsmooth stable spline estimators, where regularization functionals and data\nmisfits can be selected from a rich set of piecewise linear-quadratic (PLQ)\npenalties. This class includes the 1-norm, Huber, and Vapnik, in addition to\nthe least-squares penalty.\n  By representing penalties through their conjugates, the modeler can specify\nany piecewise linear-quadratic penalty for misfit and regularizer, as well as\ninequality constraints on the response. The interior-point solver we implement\n(IPsolve) is locally quadratically convergent, with $O(\\min(m,n)^2(m+n))$\narithmetic operations per iteration, where $n$ the number of unknown impulse\nresponse coefficients and $m$ the number of observed output measurements.\nIPsolve is competitive with available alternatives for system identification.\nThis is shown by a comparison with TFOCS, libSVM, and the FISTA algorithm. The\ncode is open source (https://github.com/saravkin/IPsolve).\n  The impact of the approach for system identification is illustrated with\nnumerical experiments featuring robust formulations for contaminated data,\nrelaxation systems, nonnegativity and unimodality constraints on the impulse\nresponse, and sparsity promoting regularization. Incorporating constraints\nyields particularly significant improvements.\n", "versions": [{"version": "v1", "created": "Mon, 30 Sep 2013 14:30:43 GMT"}, {"version": "v2", "created": "Sat, 5 Mar 2016 20:32:59 GMT"}, {"version": "v3", "created": "Fri, 25 Mar 2016 15:35:29 GMT"}, {"version": "v4", "created": "Wed, 25 Jul 2018 22:02:08 GMT"}], "update_date": "2018-07-27", "authors_parsed": [["Aravkin", "Aleksandr Y.", ""], ["Burke", "James V.", ""], ["Pillonetto", "Gianluigi", ""]]}]