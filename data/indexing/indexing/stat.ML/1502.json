[{"id": "1502.00062", "submitter": "Mallenahalli Naresh Kumar Prof. Dr.", "authors": "Vadrevu Sree Hari Rao and Mallenahalli Naresh Kumar", "title": "A New Intelligence Based Approach for Computer-Aided Diagnosis of Dengue\n  Fever", "comments": "7 pages, 5 figures. arXiv admin note: substantial text overlap with\n  arXiv:1501.07093", "journal-ref": "Information Technology in Biomedicine, IEEE Transactions on ,\n  vol.16, no.1, pp.112,118, Jan. 2012", "doi": "10.1109/TITB.2011.2171978", "report-no": null, "categories": "stat.ML cs.AI cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Identification of the influential clinical symptoms and laboratory features\nthat help in the diagnosis of dengue fever in early phase of the illness would\naid in designing effective public health management and virological\nsurveillance strategies. Keeping this as our main objective we develop in this\npaper, a new computational intelligence based methodology that predicts the\ndiagnosis in real time, minimizing the number of false positives and false\nnegatives. Our methodology consists of three major components (i) a novel\nmissing value imputation procedure that can be applied on any data set\nconsisting of categorical (nominal) and/or numeric (real or integer) (ii) a\nwrapper based features selection method with genetic search for extracting a\nsubset of most influential symptoms that can diagnose the illness and (iii) an\nalternating decision tree method that employs boosting for generating highly\naccurate decision rules. The predictive models developed using our methodology\nare found to be more accurate than the state-of-the-art methodologies used in\nthe diagnosis of the dengue fever.\n", "versions": [{"version": "v1", "created": "Sat, 31 Jan 2015 03:15:12 GMT"}], "update_date": "2015-02-03", "authors_parsed": [["Rao", "Vadrevu Sree Hari", ""], ["Kumar", "Mallenahalli Naresh", ""]]}, {"id": "1502.00093", "submitter": "Sotetsu Koyamada", "authors": "Sotetsu Koyamada and Yumi Shikauchi and Ken Nakae and Masanori Koyama\n  and Shin Ishii", "title": "Deep learning of fMRI big data: a novel approach to subject-transfer\n  decoding", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG q-bio.NC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  As a technology to read brain states from measurable brain activities, brain\ndecoding are widely applied in industries and medical sciences. In spite of\nhigh demands in these applications for a universal decoder that can be applied\nto all individuals simultaneously, large variation in brain activities across\nindividuals has limited the scope of many studies to the development of\nindividual-specific decoders. In this study, we used deep neural network (DNN),\na nonlinear hierarchical model, to construct a subject-transfer decoder. Our\ndecoder is the first successful DNN-based subject-transfer decoder. When\napplied to a large-scale functional magnetic resonance imaging (fMRI) database,\nour DNN-based decoder achieved higher decoding accuracy than other baseline\nmethods, including support vector machine (SVM). In order to analyze the\nknowledge acquired by this decoder, we applied principal sensitivity analysis\n(PSA) to the decoder and visualized the discriminative features that are common\nto all subjects in the dataset. Our PSA successfully visualized the\nsubject-independent features contributing to the subject-transferability of the\ntrained decoder.\n", "versions": [{"version": "v1", "created": "Sat, 31 Jan 2015 11:58:26 GMT"}], "update_date": "2015-02-03", "authors_parsed": [["Koyamada", "Sotetsu", ""], ["Shikauchi", "Yumi", ""], ["Nakae", "Ken", ""], ["Koyama", "Masanori", ""], ["Ishii", "Shin", ""]]}, {"id": "1502.00133", "submitter": "Sumeet Katariya", "authors": "Kevin Jamieson, Sumeet Katariya, Atul Deshpande and Robert Nowak", "title": "Sparse Dueling Bandits", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The dueling bandit problem is a variation of the classical multi-armed bandit\nin which the allowable actions are noisy comparisons between pairs of arms.\nThis paper focuses on a new approach for finding the \"best\" arm according to\nthe Borda criterion using noisy comparisons. We prove that in the absence of\nstructural assumptions, the sample complexity of this problem is proportional\nto the sum of the inverse squared gaps between the Borda scores of each\nsuboptimal arm and the best arm. We explore this dependence further and\nconsider structural constraints on the pairwise comparison matrix (a particular\nform of sparsity natural to this problem) that can significantly reduce the\nsample complexity. This motivates a new algorithm called Successive Elimination\nwith Comparison Sparsity (SECS) that exploits sparsity to find the Borda winner\nusing fewer samples than standard algorithms. We also evaluate the new\nalgorithm experimentally with synthetic and real data. The results show that\nthe sparsity model and the new algorithm can provide significant improvements\nover standard approaches.\n", "versions": [{"version": "v1", "created": "Sat, 31 Jan 2015 16:18:14 GMT"}], "update_date": "2015-02-03", "authors_parsed": [["Jamieson", "Kevin", ""], ["Katariya", "Sumeet", ""], ["Deshpande", "Atul", ""], ["Nowak", "Robert", ""]]}, {"id": "1502.00141", "submitter": "Mathieu Lagrange", "authors": "Mathieu Lagrange, Gr\\'egoire Lafay, Mathias Rossignol, Emmanouil\n  Benetos, Axel Roebel", "title": "An evaluation framework for event detection using a morphological model\n  of acoustic scenes", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.SD", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper introduces a model of environmental acoustic scenes which adopts a\nmorphological approach by ab-stracting temporal structures of acoustic scenes.\nTo demonstrate its potential, this model is employed to evaluate the\nperformance of a large set of acoustic events detection systems. This model\nallows us to explicitly control key morphological aspects of the acoustic scene\nand isolate their impact on the performance of the system under evaluation.\nThus, more information can be gained on the behavior of evaluated systems,\nproviding guidance for further improvements. The proposed model is validated\nusing submitted systems from the IEEE DCASE Challenge; results indicate that\nthe proposed scheme is able to successfully build datasets useful for\nevaluating some aspects the performance of event detection systems, more\nparticularly their robustness to new listening conditions and the increasing\nlevel of background sounds.\n", "versions": [{"version": "v1", "created": "Sat, 31 Jan 2015 18:12:34 GMT"}], "update_date": "2015-02-03", "authors_parsed": [["Lagrange", "Mathieu", ""], ["Lafay", "Gr\u00e9goire", ""], ["Rossignol", "Mathias", ""], ["Benetos", "Emmanouil", ""], ["Roebel", "Axel", ""]]}, {"id": "1502.00182", "submitter": "Mostafa Rahmani", "authors": "Mostafa Rahmani, George Atia", "title": "High Dimensional Low Rank plus Sparse Matrix Decomposition", "comments": "IEEE Transactions on Signal Processing", "journal-ref": null, "doi": "10.1109/TSP.2017.2649482", "report-no": null, "categories": "cs.NA cs.DS cs.LG math.NA stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper is concerned with the problem of low rank plus sparse matrix\ndecomposition for big data. Conventional algorithms for matrix decomposition\nuse the entire data to extract the low-rank and sparse components, and are\nbased on optimization problems with complexity that scales with the dimension\nof the data, which limits their scalability. Furthermore, existing randomized\napproaches mostly rely on uniform random sampling, which is quite inefficient\nfor many real world data matrices that exhibit additional structures (e.g.\nclustering). In this paper, a scalable subspace-pursuit approach that\ntransforms the decomposition problem to a subspace learning problem is\nproposed. The decomposition is carried out using a small data sketch formed\nfrom sampled columns/rows. Even when the data is sampled uniformly at random,\nit is shown that the sufficient number of sampled columns/rows is roughly\nO(r\\mu), where \\mu is the coherency parameter and r the rank of the low rank\ncomponent. In addition, adaptive sampling algorithms are proposed to address\nthe problem of column/row sampling from structured data. We provide an analysis\nof the proposed method with adaptive sampling and show that adaptive sampling\nmakes the required number of sampled columns/rows invariant to the distribution\nof the data. The proposed approach is amenable to online implementation and an\nonline scheme is proposed.\n", "versions": [{"version": "v1", "created": "Sun, 1 Feb 2015 00:57:57 GMT"}, {"version": "v2", "created": "Sat, 13 Feb 2016 03:56:48 GMT"}, {"version": "v3", "created": "Thu, 16 Mar 2017 06:41:34 GMT"}], "update_date": "2017-03-17", "authors_parsed": [["Rahmani", "Mostafa", ""], ["Atia", "George", ""]]}, {"id": "1502.00186", "submitter": "Haiping Huang", "authors": "Haiping Huang and Taro Toyoizumi", "title": "Advanced Mean Field Theory of Restricted Boltzmann Machine", "comments": "5 pages, 4 figures, accepted by Phys Rev E (Rapid Communication)", "journal-ref": "Phys. Rev. E 91, 050101 (2015)", "doi": "10.1103/PhysRevE.91.050101", "report-no": null, "categories": "cond-mat.stat-mech cs.LG q-bio.NC stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Learning in restricted Boltzmann machine is typically hard due to the\ncomputation of gradients of log-likelihood function. To describe the network\nstate statistics of the restricted Boltzmann machine, we develop an advanced\nmean field theory based on the Bethe approximation. Our theory provides an\nefficient message passing based method that evaluates not only the partition\nfunction (free energy) but also its gradients without requiring statistical\nsampling. The results are compared with those obtained by the computationally\nexpensive sampling based method.\n", "versions": [{"version": "v1", "created": "Sun, 1 Feb 2015 02:23:12 GMT"}, {"version": "v2", "created": "Fri, 13 Feb 2015 04:57:37 GMT"}, {"version": "v3", "created": "Sat, 2 May 2015 03:54:30 GMT"}], "update_date": "2015-05-27", "authors_parsed": [["Huang", "Haiping", ""], ["Toyoizumi", "Taro", ""]]}, {"id": "1502.00231", "submitter": "Yishi Zhang", "authors": "Zhijun Chen, Chaozhong Wu, Yishi Zhang, Zhen Huang, Bin Ran, Ming\n  Zhong, Nengchao Lyu", "title": "Feature Selection with Redundancy-complementariness Dispersion", "comments": "28 pages, 13 figures, 7 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Feature selection has attracted significant attention in data mining and\nmachine learning in the past decades. Many existing feature selection methods\neliminate redundancy by measuring pairwise inter-correlation of features,\nwhereas the complementariness of features and higher inter-correlation among\nmore than two features are ignored. In this study, a modification item\nconcerning the complementariness of features is introduced in the evaluation\ncriterion of features. Additionally, in order to identify the interference\neffect of already-selected False Positives (FPs), the\nredundancy-complementariness dispersion is also taken into account to adjust\nthe measurement of pairwise inter-correlation of features. To illustrate the\neffectiveness of proposed method, classification experiments are applied with\nfour frequently used classifiers on ten datasets. Classification results verify\nthe superiority of proposed method compared with five representative feature\nselection methods.\n", "versions": [{"version": "v1", "created": "Sun, 1 Feb 2015 10:44:26 GMT"}], "update_date": "2015-02-03", "authors_parsed": [["Chen", "Zhijun", ""], ["Wu", "Chaozhong", ""], ["Zhang", "Yishi", ""], ["Huang", "Zhen", ""], ["Ran", "Bin", ""], ["Zhong", "Ming", ""], ["Lyu", "Nengchao", ""]]}, {"id": "1502.00354", "submitter": "Nesreen Ahmed", "authors": "Nesreen K. Ahmed and Ryan A. Rossi", "title": "A Web-based Interactive Visual Graph Analytics Platform", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.SI cs.HC stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper proposes a web-based visual graph analytics platform for\ninteractive graph mining, visualization, and real-time exploration of networks.\nGraphVis is fast, intuitive, and flexible, combining interactive visualizations\nwith analytic techniques to reveal important patterns and insights for sense\nmaking, reasoning, and decision making. Networks can be visualized and explored\nwithin seconds by simply drag-and-dropping a graph file into the web browser.\nThe structure, properties, and patterns of the network are computed\nautomatically and can be instantly explored in real-time. At the heart of\nGraphVis lies a multi-level interactive network visualization and analytics\nengine that allows for real-time graph mining and exploration across multiple\nlevels of granularity simultaneously. Both the graph analytic and visualization\ntechniques (at each level of granularity) are dynamic and interactive, with\nimmediate and continuous visual feedback upon every user interaction (e.g.,\nchange of a slider for filtering). Furthermore, nodes, edges, and subgraphs are\neasily inserted, deleted or exported via a number of novel techniques and tools\nthat make it extremely easy and flexible for exploring, testing hypothesis, and\nunderstanding networks in real-time over the web. A number of interactive\nvisual graph analytic techniques are also proposed including interactive role\ndiscovery methods, community detection, as well as a number of novel block\nmodels for generating graphs with community structure. Finally, we also\nhighlight other key aspects including filtering, querying, ranking,\nmanipulating, exporting, partitioning, as well as tools for dynamic network\nanalysis and visualization, interactive graph generators, and a variety of\nmulti-level network analysis, summarization, and statistical techniques.\n", "versions": [{"version": "v1", "created": "Mon, 2 Feb 2015 04:26:01 GMT"}], "update_date": "2015-02-03", "authors_parsed": [["Ahmed", "Nesreen K.", ""], ["Rossi", "Ryan A.", ""]]}, {"id": "1502.00524", "submitter": "Ricard Marxer", "authors": "Ricard Marxer and Hendrik Purwins", "title": "Unsupervised Incremental Learning and Prediction of Music Signals", "comments": "13 pages, 10 figures", "journal-ref": null, "doi": "10.1109/TASLP.2016.2530409", "report-no": null, "categories": "cs.SD cs.IR cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A system is presented that segments, clusters and predicts musical audio in\nan unsupervised manner, adjusting the number of (timbre) clusters\ninstantaneously to the audio input. A sequence learning algorithm adapts its\nstructure to a dynamically changing clustering tree. The flow of the system is\nas follows: 1) segmentation by onset detection, 2) timbre representation of\neach segment by Mel frequency cepstrum coefficients, 3) discretization by\nincremental clustering, yielding a tree of different sound classes (e.g.\ninstruments) that can grow or shrink on the fly driven by the instantaneous\nsound events, resulting in a discrete symbol sequence, 4) extraction of\nstatistical regularities of the symbol sequence, using hierarchical N-grams and\nthe newly introduced conceptual Boltzmann machine, and 5) prediction of the\nnext sound event in the sequence. The system's robustness is assessed with\nrespect to complexity and noisiness of the signal. Clustering in isolation\nyields an adjusted Rand index (ARI) of 82.7% / 85.7% for data sets of singing\nvoice and drums. Onset detection jointly with clustering achieve an ARI of\n81.3% / 76.3% and the prediction of the entire system yields an ARI of 27.2% /\n39.2%.\n", "versions": [{"version": "v1", "created": "Mon, 2 Feb 2015 15:45:38 GMT"}, {"version": "v2", "created": "Fri, 23 Oct 2015 14:37:45 GMT"}], "update_date": "2020-05-21", "authors_parsed": [["Marxer", "Ricard", ""], ["Purwins", "Hendrik", ""]]}, {"id": "1502.00725", "submitter": "Hongwei Li", "authors": "Hongwei Li and Qiang Liu", "title": "Cheaper and Better: Selecting Good Workers for Crowdsourcing", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.AI cs.LG stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Crowdsourcing provides a popular paradigm for data collection at scale. We\nstudy the problem of selecting subsets of workers from a given worker pool to\nmaximize the accuracy under a budget constraint. One natural question is\nwhether we should hire as many workers as the budget allows, or restrict on a\nsmall number of top-quality workers. By theoretically analyzing the error rate\nof a typical setting in crowdsourcing, we frame the worker selection problem\ninto a combinatorial optimization problem and propose an algorithm to solve it\nefficiently. Empirical results on both simulated and real-world datasets show\nthat our algorithm is able to select a small number of high-quality workers,\nand performs as good as, sometimes even better than, the much larger crowds as\nthe budget allows.\n", "versions": [{"version": "v1", "created": "Tue, 3 Feb 2015 03:45:48 GMT"}], "update_date": "2015-02-04", "authors_parsed": [["Li", "Hongwei", ""], ["Liu", "Qiang", ""]]}, {"id": "1502.00727", "submitter": "Daniel Korenblum", "authors": "Daniel Korenblum", "title": "Laplacian Mixture Modeling for Network Analysis and Unsupervised\n  Learning on Graphs", "comments": "13 figures, 35 references", "journal-ref": "PLOS ONE, 13:1-33 (2018),\n  https://doi.org/10.1371/journal.pone.0204096 (Public Library of Science)", "doi": "10.1371/journal.pone.0204096", "report-no": null, "categories": "stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Laplacian mixture models identify overlapping regions of influence in\nunlabeled graph and network data in a scalable and computationally efficient\nway, yielding useful low-dimensional representations. By combining Laplacian\neigenspace and finite mixture modeling methods, they provide probabilistic or\nfuzzy dimensionality reductions or domain decompositions for a variety of input\ndata types, including mixture distributions, feature vectors, and graphs or\nnetworks. Provable optimal recovery using the algorithm is analytically shown\nfor a nontrivial class of cluster graphs. Heuristic approximations for scalable\nhigh-performance implementations are described and empirically tested.\nConnections to PageRank and community detection in network analysis demonstrate\nthe wide applicability of this approach. The origins of fuzzy spectral methods,\nbeginning with generalized heat or diffusion equations in physics, are reviewed\nand summarized. Comparisons to other dimensionality reduction and clustering\nmethods for challenging unsupervised machine learning problems are also\ndiscussed.\n", "versions": [{"version": "v1", "created": "Tue, 3 Feb 2015 03:50:16 GMT"}, {"version": "v2", "created": "Wed, 8 Apr 2015 13:53:15 GMT"}, {"version": "v3", "created": "Tue, 21 Jul 2015 06:20:45 GMT"}, {"version": "v4", "created": "Wed, 5 Jul 2017 07:46:30 GMT"}, {"version": "v5", "created": "Thu, 6 Jul 2017 11:53:25 GMT"}, {"version": "v6", "created": "Sun, 1 Jul 2018 00:48:44 GMT"}, {"version": "v7", "created": "Mon, 1 Oct 2018 20:10:54 GMT"}], "update_date": "2018-10-03", "authors_parsed": [["Korenblum", "Daniel", ""]]}, {"id": "1502.00916", "submitter": "Diane Oyen", "authors": "Jason K. Johnson, Diane Oyen, Michael Chertkov, Praneeth Netrapalli", "title": "Learning Planar Ising Models", "comments": "14 pages, 6 figures", "journal-ref": null, "doi": null, "report-no": "LA-UR-15-20740", "categories": "stat.ML", "license": "http://creativecommons.org/licenses/publicdomain/", "abstract": "  Inference and learning of graphical models are both well-studied problems in\nstatistics and machine learning that have found many applications in science\nand engineering. However, exact inference is intractable in general graphical\nmodels, which suggests the problem of seeking the best approximation to a\ncollection of random variables within some tractable family of graphical\nmodels. In this paper, we focus on the class of planar Ising models, for which\nexact inference is tractable using techniques of statistical physics. Based on\nthese techniques and recent methods for planarity testing and planar embedding,\nwe propose a simple greedy algorithm for learning the best planar Ising model\nto approximate an arbitrary collection of binary random variables (possibly\nfrom sample data). Given the set of all pairwise correlations among variables,\nwe select a planar graph and optimal planar Ising model defined on this graph\nto best approximate that set of correlations. We demonstrate our method in\nsimulations and for the application of modeling senate voting records.\n", "versions": [{"version": "v1", "created": "Tue, 3 Feb 2015 16:35:43 GMT"}], "update_date": "2015-02-04", "authors_parsed": [["Johnson", "Jason K.", ""], ["Oyen", "Diane", ""], ["Chertkov", "Michael", ""], ["Netrapalli", "Praneeth", ""]]}, {"id": "1502.01068", "submitter": "Quoc Tran-Dinh", "authors": "Quoc Tran-Dinh, Yen-Huan Li, and Volkan Cevher", "title": "Composite convex minimization involving self-concordant-like cost\n  functions", "comments": "19 pages, 5 figures", "journal-ref": null, "doi": null, "report-no": "LIONS-EPFL-2015A", "categories": "math.OC stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The self-concordant-like property of a smooth convex function is a new\nanalytical structure that generalizes the self-concordant notion. While a wide\nvariety of important applications feature the self-concordant-like property,\nthis concept has heretofore remained unexploited in convex optimization. To\nthis end, we develop a variable metric framework of minimizing the sum of a\n\"simple\" convex function and a self-concordant-like function. We introduce a\nnew analytic step-size selection procedure and prove that the basic gradient\nalgorithm has improved convergence guarantees as compared to \"fast\" algorithms\nthat rely on the Lipschitz gradient property. Our numerical tests with\nreal-data sets shows that the practice indeed follows the theory.\n", "versions": [{"version": "v1", "created": "Wed, 4 Feb 2015 00:40:23 GMT"}, {"version": "v2", "created": "Sat, 20 Jan 2018 05:51:27 GMT"}], "update_date": "2018-01-23", "authors_parsed": [["Tran-Dinh", "Quoc", ""], ["Li", "Yen-Huan", ""], ["Cevher", "Volkan", ""]]}, {"id": "1502.01094", "submitter": "Soheil Bahrampour", "authors": "Soheil Bahrampour, Nasser M. Nasrabadi, Asok Ray, W. Kenneth Jenkins", "title": "Multimodal Task-Driven Dictionary Learning for Image Classification", "comments": "To appear at IEEE Transactions on Image Processing", "journal-ref": null, "doi": "10.1109/TIP.2015.2496275", "report-no": null, "categories": "stat.ML cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Dictionary learning algorithms have been successfully used for both\nreconstructive and discriminative tasks, where an input signal is represented\nwith a sparse linear combination of dictionary atoms. While these methods are\nmostly developed for single-modality scenarios, recent studies have\ndemonstrated the advantages of feature-level fusion based on the joint sparse\nrepresentation of the multimodal inputs. In this paper, we propose a multimodal\ntask-driven dictionary learning algorithm under the joint sparsity constraint\n(prior) to enforce collaborations among multiple homogeneous/heterogeneous\nsources of information. In this task-driven formulation, the multimodal\ndictionaries are learned simultaneously with their corresponding classifiers.\nThe resulting multimodal dictionaries can generate discriminative latent\nfeatures (sparse codes) from the data that are optimized for a given task such\nas binary or multiclass classification. Moreover, we present an extension of\nthe proposed formulation using a mixed joint and independent sparsity prior\nwhich facilitates more flexible fusion of the modalities at feature level. The\nefficacy of the proposed algorithms for multimodal classification is\nillustrated on four different applications -- multimodal face recognition,\nmulti-view face recognition, multi-view action recognition, and multimodal\nbiometric recognition. It is also shown that, compared to the counterpart\nreconstructive-based dictionary learning algorithms, the task-driven\nformulations are more computationally efficient in the sense that they can be\nequipped with more compact dictionaries and still achieve superior performance.\n", "versions": [{"version": "v1", "created": "Wed, 4 Feb 2015 05:17:50 GMT"}, {"version": "v2", "created": "Tue, 27 Oct 2015 07:26:59 GMT"}], "update_date": "2016-01-20", "authors_parsed": [["Bahrampour", "Soheil", ""], ["Nasrabadi", "Nasser M.", ""], ["Ray", "Asok", ""], ["Jenkins", "W. Kenneth", ""]]}, {"id": "1502.01176", "submitter": "Ethan Fetaya", "authors": "Ethan Fetaya and Shimon Ullman", "title": "Learning Local Invariant Mahalanobis Distances", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  For many tasks and data types, there are natural transformations to which the\ndata should be invariant or insensitive. For instance, in visual recognition,\nnatural images should be insensitive to rotation and translation. This\nrequirement and its implications have been important in many machine learning\napplications, and tolerance for image transformations was primarily achieved by\nusing robust feature vectors. In this paper we propose a novel and\ncomputationally efficient way to learn a local Mahalanobis metric per datum,\nand show how we can learn a local invariant metric to any transformation in\norder to improve performance.\n", "versions": [{"version": "v1", "created": "Wed, 4 Feb 2015 12:27:04 GMT"}], "update_date": "2015-07-28", "authors_parsed": [["Fetaya", "Ethan", ""], ["Ullman", "Shimon", ""]]}, {"id": "1502.01368", "submitter": "Cencheng Shen", "authors": "Cencheng Shen, Li Chen, Yuexiao Dong, Carey E. Priebe", "title": "Sparse Representation Classification Beyond L1 Minimization and the\n  Subspace Assumption", "comments": "15 pages, 4 figures, 3 tables", "journal-ref": "IEEE Transactions on Information Theory, vol. 66(8), pp 5061-5071,\n  2020", "doi": "10.1109/TIT.2020.2981309", "report-no": null, "categories": "stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The sparse representation classifier (SRC) has been utilized in various\nclassification problems, which makes use of L1 minimization and works well for\nimage recognition satisfying a subspace assumption. In this paper we propose a\nnew implementation of SRC via screening, establish its equivalence to the\noriginal SRC under regularity conditions, and prove its classification\nconsistency under a latent subspace model and contamination. The results are\ndemonstrated via simulations and real data experiments, where the new algorithm\nachieves comparable numerical performance and significantly faster.\n", "versions": [{"version": "v1", "created": "Wed, 4 Feb 2015 21:50:55 GMT"}, {"version": "v2", "created": "Tue, 9 Jun 2015 01:24:39 GMT"}, {"version": "v3", "created": "Thu, 28 Dec 2017 21:49:13 GMT"}, {"version": "v4", "created": "Wed, 8 Jan 2020 22:22:00 GMT"}], "update_date": "2021-05-18", "authors_parsed": [["Shen", "Cencheng", ""], ["Chen", "Li", ""], ["Dong", "Yuexiao", ""], ["Priebe", "Carey E.", ""]]}, {"id": "1502.01403", "submitter": "Yuchen Zhang", "authors": "Yuchen Zhang, Martin J. Wainwright, Michael I. Jordan", "title": "Distributed Estimation of Generalized Matrix Rank: Efficient Algorithms\n  and Lower Bounds", "comments": "23 pages, 5 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.CC stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study the following generalized matrix rank estimation problem: given an\n$n \\times n$ matrix and a constant $c \\geq 0$, estimate the number of\neigenvalues that are greater than $c$. In the distributed setting, the matrix\nof interest is the sum of $m$ matrices held by separate machines. We show that\nany deterministic algorithm solving this problem must communicate $\\Omega(n^2)$\nbits, which is order-equivalent to transmitting the whole matrix. In contrast,\nwe propose a randomized algorithm that communicates only $\\widetilde O(n)$\nbits. The upper bound is matched by an $\\Omega(n)$ lower bound on the\nrandomized communication complexity. We demonstrate the practical effectiveness\nof the proposed algorithm with some numerical experiments.\n", "versions": [{"version": "v1", "created": "Thu, 5 Feb 2015 00:53:01 GMT"}, {"version": "v2", "created": "Fri, 6 Feb 2015 18:51:23 GMT"}], "update_date": "2015-02-09", "authors_parsed": [["Zhang", "Yuchen", ""], ["Wainwright", "Martin J.", ""], ["Jordan", "Michael I.", ""]]}, {"id": "1502.01418", "submitter": "Cem Tekin", "authors": "Cem Tekin and Mihaela van der Schaar", "title": "RELEAF: An Algorithm for Learning and Exploiting Relevance", "comments": "to appear in IEEE Journal of Selected Topics in Signal Processing,\n  2015", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recommender systems, medical diagnosis, network security, etc., require\non-going learning and decision-making in real time. These -- and many others --\nrepresent perfect examples of the opportunities and difficulties presented by\nBig Data: the available information often arrives from a variety of sources and\nhas diverse features so that learning from all the sources may be valuable but\nintegrating what is learned is subject to the curse of dimensionality. This\npaper develops and analyzes algorithms that allow efficient learning and\ndecision-making while avoiding the curse of dimensionality. We formalize the\ninformation available to the learner/decision-maker at a particular time as a\ncontext vector which the learner should consider when taking actions. In\ngeneral the context vector is very high dimensional, but in many settings, the\nmost relevant information is embedded into only a few relevant dimensions. If\nthese relevant dimensions were known in advance, the problem would be simple --\nbut they are not. Moreover, the relevant dimensions may be different for\ndifferent actions. Our algorithm learns the relevant dimensions for each\naction, and makes decisions based in what it has learned. Formally, we build on\nthe structure of a contextual multi-armed bandit by adding and exploiting a\nrelevance relation. We prove a general regret bound for our algorithm whose\ntime order depends only on the maximum number of relevant dimensions among all\nthe actions, which in the special case where the relevance relation is\nsingle-valued (a function), reduces to $\\tilde{O}(T^{2(\\sqrt{2}-1)})$; in the\nabsence of a relevance relation, the best known contextual bandit algorithms\nachieve regret $\\tilde{O}(T^{(D+1)/(D+2)})$, where $D$ is the full dimension of\nthe context vector.\n", "versions": [{"version": "v1", "created": "Thu, 5 Feb 2015 03:03:16 GMT"}, {"version": "v2", "created": "Sat, 7 Feb 2015 08:29:14 GMT"}], "update_date": "2015-02-10", "authors_parsed": [["Tekin", "Cem", ""], ["van der Schaar", "Mihaela", ""]]}, {"id": "1502.01425", "submitter": "Guang Cheng", "authors": "Will Wei Sun and Junwei Lu and Han Liu and Guang Cheng", "title": "Provable Sparse Tensor Decomposition", "comments": "To Appear in JRSS-B", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a novel sparse tensor decomposition method, namely Tensor\nTruncated Power (TTP) method, that incorporates variable selection into the\nestimation of decomposition components. The sparsity is achieved via an\nefficient truncation step embedded in the tensor power iteration. Our method\napplies to a broad family of high dimensional latent variable models, including\nhigh dimensional Gaussian mixture and mixtures of sparse regressions. A\nthorough theoretical investigation is further conducted. In particular, we show\nthat the final decomposition estimator is guaranteed to achieve a local\nstatistical rate, and further strengthen it to the global statistical rate by\nintroducing a proper initialization procedure. In high dimensional regimes, the\nobtained statistical rate significantly improves those shown in the existing\nnon-sparse decomposition methods. The empirical advantages of TTP are confirmed\nin extensive simulated results and two real applications of click-through rate\nprediction and high-dimensional gene clustering.\n", "versions": [{"version": "v1", "created": "Thu, 5 Feb 2015 03:37:40 GMT"}, {"version": "v2", "created": "Fri, 25 Dec 2015 09:47:23 GMT"}, {"version": "v3", "created": "Mon, 2 May 2016 23:46:16 GMT"}], "update_date": "2016-05-04", "authors_parsed": [["Sun", "Will Wei", ""], ["Lu", "Junwei", ""], ["Liu", "Han", ""], ["Cheng", "Guang", ""]]}, {"id": "1502.01493", "submitter": "Pierre Dupont", "authors": "Samuel Branders, Roberto D'Ambrosio and Pierre Dupont", "title": "A mixture Cox-Logistic model for feature selection from survival and\n  classification data", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper presents an original approach for jointly fitting survival times\nand classifying samples into subgroups. The Coxlogit model is a generalized\nlinear model with a common set of selected features for both tasks. Survival\ntimes and class labels are here assumed to be conditioned by a common risk\nscore which depends on those features. Learning is then naturally expressed as\nmaximizing the joint probability of subgroup labels and the ordering of\nsurvival events, conditioned to a common weight vector. The model is estimated\nby minimizing a regularized log-likelihood through a coordinate descent\nalgorithm.\n  Validation on synthetic and breast cancer data shows that the proposed\napproach outperforms a standard Cox model or logistic regression when both\npredicting the survival times and classifying new samples into subgroups. It is\nalso better at selecting informative features for both tasks.\n", "versions": [{"version": "v1", "created": "Thu, 5 Feb 2015 10:45:54 GMT"}], "update_date": "2015-02-06", "authors_parsed": [["Branders", "Samuel", ""], ["D'Ambrosio", "Roberto", ""], ["Dupont", "Pierre", ""]]}, {"id": "1502.01563", "submitter": "Emanuele Frandi", "authors": "Emanuele Frandi, Ricardo Nanculef, Johan A. K. Suykens", "title": "A PARTAN-Accelerated Frank-Wolfe Algorithm for Large-Scale SVM\n  Classification", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG math.OC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Frank-Wolfe algorithms have recently regained the attention of the Machine\nLearning community. Their solid theoretical properties and sparsity guarantees\nmake them a suitable choice for a wide range of problems in this field. In\naddition, several variants of the basic procedure exist that improve its\ntheoretical properties and practical performance. In this paper, we investigate\nthe application of some of these techniques to Machine Learning, focusing in\nparticular on a Parallel Tangent (PARTAN) variant of the FW algorithm that has\nnot been previously suggested or studied for this type of problems. We provide\nexperiments both in a standard setting and using a stochastic speed-up\ntechnique, showing that the considered algorithms obtain promising results on\nseveral medium and large-scale benchmark datasets for SVM classification.\n", "versions": [{"version": "v1", "created": "Thu, 5 Feb 2015 14:17:55 GMT"}], "update_date": "2015-10-27", "authors_parsed": [["Frandi", "Emanuele", ""], ["Nanculef", "Ricardo", ""], ["Suykens", "Johan A. K.", ""]]}, {"id": "1502.01664", "submitter": "Lewis Evans Mr", "authors": "Lewis P. G. Evans and Niall M. Adams and Christoforos Anagnostopoulos", "title": "Estimating Optimal Active Learning via Model Retraining Improvement", "comments": "arXiv admin note: substantial text overlap with arXiv:1407.8042", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A central question for active learning (AL) is: \"what is the optimal\nselection?\" Defining optimality by classifier loss produces a new\ncharacterisation of optimal AL behaviour, by treating expected loss reduction\nas a statistical target for estimation. This target forms the basis of model\nretraining improvement (MRI), a novel approach providing a statistical\nestimation framework for AL. This framework is constructed to address the\ncentral question of AL optimality, and to motivate the design of estimation\nalgorithms. MRI allows the exploration of optimal AL behaviour, and the\nexamination of AL heuristics, showing precisely how they make sub-optimal\nselections. The abstract formulation of MRI is used to provide a new guarantee\nfor AL, that an unbiased MRI estimator should outperform random selection. This\nMRI framework reveals intricate estimation issues that in turn motivate the\nconstruction of new statistical AL algorithms. One new algorithm in particular\nperforms strongly in a large-scale experimental study, compared to standard AL\nmethods. This competitive performance suggests that practical efforts to\nminimise estimation bias may be important for AL applications.\n", "versions": [{"version": "v1", "created": "Thu, 5 Feb 2015 18:13:34 GMT"}], "update_date": "2015-02-06", "authors_parsed": [["Evans", "Lewis P. G.", ""], ["Adams", "Niall M.", ""], ["Anagnostopoulos", "Christoforos", ""]]}, {"id": "1502.01682", "submitter": "Michael Bloodgood", "authors": "Kathryn Baker, Michael Bloodgood, Bonnie J. Dorr, Chris\n  Callison-Burch, Nathaniel W. Filardo, Christine Piatko, Lori Levin and Scott\n  Miller", "title": "Use of Modality and Negation in Semantically-Informed Syntactic MT", "comments": "28 pages, 13 figures, 2 tables; appeared in Computational\n  Linguistics, 38(2):411-438, 2012", "journal-ref": "Computational Linguistics, 38(2):411-438, 2012", "doi": null, "report-no": null, "categories": "cs.CL cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper describes the resource- and system-building efforts of an\neight-week Johns Hopkins University Human Language Technology Center of\nExcellence Summer Camp for Applied Language Exploration (SCALE-2009) on\nSemantically-Informed Machine Translation (SIMT). We describe a new\nmodality/negation (MN) annotation scheme, the creation of a (publicly\navailable) MN lexicon, and two automated MN taggers that we built using the\nannotation scheme and lexicon. Our annotation scheme isolates three components\nof modality and negation: a trigger (a word that conveys modality or negation),\na target (an action associated with modality or negation) and a holder (an\nexperiencer of modality). We describe how our MN lexicon was semi-automatically\nproduced and we demonstrate that a structure-based MN tagger results in\nprecision around 86% (depending on genre) for tagging of a standard LDC data\nset.\n  We apply our MN annotation scheme to statistical machine translation using a\nsyntactic framework that supports the inclusion of semantic annotations.\nSyntactic tags enriched with semantic annotations are assigned to parse trees\nin the target-language training texts through a process of tree grafting. While\nthe focus of our work is modality and negation, the tree grafting procedure is\ngeneral and supports other types of semantic information. We exploit this\ncapability by including named entities, produced by a pre-existing tagger, in\naddition to the MN elements produced by the taggers described in this paper.\nThe resulting system significantly outperformed a linguistically naive baseline\nmodel (Hiero), and reached the highest scores yet reported on the NIST 2009\nUrdu-English test set. This finding supports the hypothesis that both syntactic\nand semantic information can improve translation quality.\n", "versions": [{"version": "v1", "created": "Thu, 5 Feb 2015 19:10:26 GMT"}], "update_date": "2015-02-06", "authors_parsed": [["Baker", "Kathryn", ""], ["Bloodgood", "Michael", ""], ["Dorr", "Bonnie J.", ""], ["Callison-Burch", "Chris", ""], ["Filardo", "Nathaniel W.", ""], ["Piatko", "Christine", ""], ["Levin", "Lori", ""], ["Miller", "Scott", ""]]}, {"id": "1502.01684", "submitter": "Nicolas Goix", "authors": "Nicolas Goix (LTCI), Anne Sabourin (LTCI), St\\'ephan Cl\\'emen\\c{c}on\n  (LTCI)", "title": "On Anomaly Ranking and Excess-Mass Curves", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML math.PR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Learning how to rank multivariate unlabeled observations depending on their\ndegree of abnormality/novelty is a crucial problem in a wide range of\napplications. In practice, it generally consists in building a real valued\n\"scoring\" function on the feature space so as to quantify to which extent\nobservations should be considered as abnormal. In the 1-d situation,\nmeasurements are generally considered as \"abnormal\" when they are remote from\ncentral measures such as the mean or the median. Anomaly detection then relies\non tail analysis of the variable of interest. Extensions to the multivariate\nsetting are far from straightforward and it is precisely the main purpose of\nthis paper to introduce a novel and convenient (functional) criterion for\nmeasuring the performance of a scoring function regarding the anomaly ranking\ntask, referred to as the Excess-Mass curve (EM curve). In addition, an adaptive\nalgorithm for building a scoring function based on unlabeled data X1 , . . . ,\nXn with a nearly optimal EM is proposed and is analyzed from a statistical\nperspective.\n", "versions": [{"version": "v1", "created": "Thu, 5 Feb 2015 19:16:18 GMT"}], "update_date": "2015-02-06", "authors_parsed": [["Goix", "Nicolas", "", "LTCI"], ["Sabourin", "Anne", "", "LTCI"], ["Cl\u00e9men\u00e7on", "St\u00e9phan", "", "LTCI"]]}, {"id": "1502.01705", "submitter": "Xiaozhao Zhao", "authors": "Xiaozhao Zhao, Yuexian Hou, Dawei Song, Wenjie Li", "title": "A Confident Information First Principle for Parametric Reduction and\n  Model Selection of Boltzmann Machines", "comments": "16pages. arXiv admin note: substantial text overlap with\n  arXiv:1302.3931", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Typical dimensionality reduction (DR) methods are often data-oriented,\nfocusing on directly reducing the number of random variables (features) while\nretaining the maximal variations in the high-dimensional data. In unsupervised\nsituations, one of the main limitations of these methods lies in their\ndependency on the scale of data features. This paper aims to address the\nproblem from a new perspective and considers model-oriented dimensionality\nreduction in parameter spaces of binary multivariate distributions.\n  Specifically, we propose a general parameter reduction criterion, called\nConfident-Information-First (CIF) principle, to maximally preserve confident\nparameters and rule out less confident parameters. Formally, the confidence of\neach parameter can be assessed by its contribution to the expected Fisher\ninformation distance within the geometric manifold over the neighbourhood of\nthe underlying real distribution.\n  We then revisit Boltzmann machines (BM) from a model selection perspective\nand theoretically show that both the fully visible BM (VBM) and the BM with\nhidden units can be derived from the general binary multivariate distribution\nusing the CIF principle. This can help us uncover and formalize the essential\nparts of the target density that BM aims to capture and the non-essential parts\nthat BM should discard. Guided by the theoretical analysis, we develop a\nsample-specific CIF for model selection of BM that is adaptive to the observed\nsamples. The method is studied in a series of density estimation experiments\nand has been shown effective in terms of the estimate accuracy.\n", "versions": [{"version": "v1", "created": "Thu, 5 Feb 2015 20:28:01 GMT"}], "update_date": "2015-02-06", "authors_parsed": [["Zhao", "Xiaozhao", ""], ["Hou", "Yuexian", ""], ["Song", "Dawei", ""], ["Li", "Wenjie", ""]]}, {"id": "1502.01753", "submitter": "Peter Wittek", "authors": "Peter Wittek, S\\'andor Dar\\'anyi, Efstratios Kontopoulos, Theodoros\n  Moysiadis, Ioannis Kompatsiaris", "title": "Monitoring Term Drift Based on Semantic Consistency in an Evolving\n  Vector Field", "comments": "8 pages, 1 figure. Code used to conduct the experiments is available\n  at https://github.com/peterwittek/concept_drifts", "journal-ref": "Proceedings of IJCNN-15, International Joint Conference on Neural\n  Networks, pages 1--8, 2015", "doi": "10.1109/IJCNN.2015.7280766", "report-no": null, "categories": "cs.CL cs.LG cs.NE stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Based on the Aristotelian concept of potentiality vs. actuality allowing for\nthe study of energy and dynamics in language, we propose a field approach to\nlexical analysis. Falling back on the distributional hypothesis to\nstatistically model word meaning, we used evolving fields as a metaphor to\nexpress time-dependent changes in a vector space model by a combination of\nrandom indexing and evolving self-organizing maps (ESOM). To monitor semantic\ndrifts within the observation period, an experiment was carried out on the term\nspace of a collection of 12.8 million Amazon book reviews. For evaluation, the\nsemantic consistency of ESOM term clusters was compared with their respective\nneighbourhoods in WordNet, and contrasted with distances among term vectors by\nrandom indexing. We found that at 0.05 level of significance, the terms in the\nclusters showed a high level of semantic consistency. Tracking the drift of\ndistributional patterns in the term space across time periods, we found that\nconsistency decreased, but not at a statistically significant level. Our method\nis highly scalable, with interpretations in philosophy.\n", "versions": [{"version": "v1", "created": "Thu, 5 Feb 2015 22:51:45 GMT"}], "update_date": "2016-11-22", "authors_parsed": [["Wittek", "Peter", ""], ["Dar\u00e1nyi", "S\u00e1ndor", ""], ["Kontopoulos", "Efstratios", ""], ["Moysiadis", "Theodoros", ""], ["Kompatsiaris", "Ioannis", ""]]}, {"id": "1502.01783", "submitter": "Venkatesh Saligrama", "authors": "Jing Qian, Jonathan Root, Venkatesh Saligrama", "title": "Learning Efficient Anomaly Detectors from $K$-NN Graphs", "comments": "arXiv admin note: text overlap with arXiv:1405.0530", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a non-parametric anomaly detection algorithm for high dimensional\ndata. We score each datapoint by its average $K$-NN distance, and rank them\naccordingly. We then train limited complexity models to imitate these scores\nbased on the max-margin learning-to-rank framework. A test-point is declared as\nan anomaly at $\\alpha$-false alarm level if the predicted score is in the\n$\\alpha$-percentile. The resulting anomaly detector is shown to be\nasymptotically optimal in that for any false alarm rate $\\alpha$, its decision\nregion converges to the $\\alpha$-percentile minimum volume level set of the\nunknown underlying density. In addition, we test both the statistical\nperformance and computational efficiency of our algorithm on a number of\nsynthetic and real-data experiments. Our results demonstrate the superiority of\nour algorithm over existing $K$-NN based anomaly detection algorithms, with\nsignificant computational savings.\n", "versions": [{"version": "v1", "created": "Fri, 6 Feb 2015 03:36:51 GMT"}], "update_date": "2015-02-09", "authors_parsed": [["Qian", "Jing", ""], ["Root", "Jonathan", ""], ["Saligrama", "Venkatesh", ""]]}, {"id": "1502.01908", "submitter": "Andreas Svensson", "authors": "Andreas Svensson, Johan Dahlin and Thomas B. Sch\\\"on", "title": "Marginalizing Gaussian Process Hyperparameters using Sequential Monte\n  Carlo", "comments": "Accepted to the 6th IEEE international workshop on computational\n  advances in multi-sensor adaptive processing (CAMSAP), Cancun, Mexico,\n  December 2015", "journal-ref": null, "doi": "10.1109/CAMSAP.2015.7383840", "report-no": null, "categories": "stat.ML stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Gaussian process regression is a popular method for non-parametric\nprobabilistic modeling of functions. The Gaussian process prior is\ncharacterized by so-called hyperparameters, which often have a large influence\non the posterior model and can be difficult to tune. This work provides a\nmethod for numerical marginalization of the hyperparameters, relying on the\nrigorous framework of sequential Monte Carlo. Our method is well suited for\nonline problems, and we demonstrate its ability to handle real-world problems\nwith several dimensions and compare it to other marginalization methods. We\nalso conclude that our proposed method is a competitive alternative to the\ncommonly used point estimates maximizing the likelihood, both in terms of\ncomputational load and its ability to handle multimodal posteriors.\n", "versions": [{"version": "v1", "created": "Fri, 6 Feb 2015 14:52:01 GMT"}, {"version": "v2", "created": "Fri, 2 Oct 2015 11:05:17 GMT"}], "update_date": "2016-11-18", "authors_parsed": [["Svensson", "Andreas", ""], ["Dahlin", "Johan", ""], ["Sch\u00f6n", "Thomas B.", ""]]}, {"id": "1502.01943", "submitter": "Przemys{\\l}aw Spurek", "authors": "P. Spurek, J. Tabor, P. Markowicz", "title": "Active Function Cross-Entropy Clustering", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Gaussian Mixture Models (GMM) have found many applications in density\nestimation and data clustering. However, the model does not adapt well to\ncurved and strongly nonlinear data. Recently there appeared an improvement\ncalled AcaGMM (Active curve axis Gaussian Mixture Model), which fits Gaussians\nalong curves using an EM-like (Expectation Maximization) approach.\n  Using the ideas standing behind AcaGMM, we build an alternative active\nfunction model of clustering, which has some advantages over AcaGMM. In\nparticular it is naturally defined in arbitrary dimensions and enables an easy\nadaptation to clustering of complicated datasets along the predefined family of\nfunctions. Moreover, it does not need external methods to determine the number\nof clusters as it automatically reduces the number of groups on-line.\n", "versions": [{"version": "v1", "created": "Fri, 6 Feb 2015 16:40:06 GMT"}], "update_date": "2015-02-09", "authors_parsed": [["Spurek", "P.", ""], ["Tabor", "J.", ""], ["Markowicz", "P.", ""]]}, {"id": "1502.01953", "submitter": "Arunselvan Ramaswamy", "authors": "Arunselvan Ramaswamy, Shalabh Bhatnagar", "title": "A Generalization of the Borkar-Meyn Theorem for Stochastic Recursive\n  Inclusions", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.SY math.DS stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper the stability theorem of Borkar and Meyn is extended to include\nthe case when the mean field is a differential inclusion. Two different sets of\nsufficient conditions are presented that guarantee the stability and\nconvergence of stochastic recursive inclusions. Our work builds on the works of\nBenaim, Hofbauer and Sorin as well as Borkar and Meyn. As a corollary to one of\nthe main theorems, a natural generalization of the Borkar and Meyn Theorem\nfollows. In addition, the original theorem of Borkar and Meyn is shown to hold\nunder slightly relaxed assumptions. Finally, as an application to one of the\nmain theorems we discuss a solution to the approximate drift problem.\n", "versions": [{"version": "v1", "created": "Fri, 6 Feb 2015 16:56:27 GMT"}, {"version": "v2", "created": "Mon, 2 Mar 2015 04:30:53 GMT"}, {"version": "v3", "created": "Tue, 27 Sep 2016 16:03:38 GMT"}], "update_date": "2016-09-28", "authors_parsed": [["Ramaswamy", "Arunselvan", ""], ["Bhatnagar", "Shalabh", ""]]}, {"id": "1502.01956", "submitter": "Arunselvan Ramaswamy", "authors": "Arunselvan Ramaswamy, Shalabh Bhatnagar", "title": "Stochastic recursive inclusion in two timescales with an application to\n  the Lagrangian dual problem", "comments": null, "journal-ref": "Stochastics 2016", "doi": "10.1080/17442508.2016.1215450", "report-no": null, "categories": "cs.SY math.DS stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper we present a framework to analyze the asymptotic behavior of\ntwo timescale stochastic approximation algorithms including those with\nset-valued mean fields. This paper builds on the works of Borkar and Perkins &\nLeslie. The framework presented herein is more general as compared to the\nsynchronous two timescale framework of Perkins \\& Leslie, however the\nassumptions involved are easily verifiable. As an application, we use this\nframework to analyze the two timescale stochastic approximation algorithm\ncorresponding to the Lagrangian dual problem in optimization theory.\n", "versions": [{"version": "v1", "created": "Fri, 6 Feb 2015 17:01:14 GMT"}, {"version": "v2", "created": "Fri, 9 Oct 2015 07:05:10 GMT"}], "update_date": "2016-09-28", "authors_parsed": [["Ramaswamy", "Arunselvan", ""], ["Bhatnagar", "Shalabh", ""]]}, {"id": "1502.01988", "submitter": "Tengyuan Liang", "authors": "T. Tony Cai, Tengyuan Liang and Alexander Rakhlin", "title": "Computational and Statistical Boundaries for Submatrix Localization in a\n  Large Noisy Matrix", "comments": "37 pages, 1 figure", "journal-ref": "The Annals of Statistics 45 (2017) 1403-1430", "doi": "10.1214/16-AOS1488", "report-no": null, "categories": "math.ST stat.ML stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The interplay between computational efficiency and statistical accuracy in\nhigh-dimensional inference has drawn increasing attention in the literature. In\nthis paper, we study computational and statistical boundaries for submatrix\nlocalization. Given one observation of (one or multiple non-overlapping) signal\nsubmatrix (of magnitude $\\lambda$ and size $k_m \\times k_n$) contaminated with\na noise matrix (of size $m \\times n$), we establish two transition thresholds\nfor the signal to noise $\\lambda/\\sigma$ ratio in terms of $m$, $n$, $k_m$, and\n$k_n$. The first threshold, $\\sf SNR_c$, corresponds to the computational\nboundary. Below this threshold, it is shown that no polynomial time algorithm\ncan succeed in identifying the submatrix, under the \\textit{hidden clique\nhypothesis}. We introduce adaptive linear time spectral algorithms that\nidentify the submatrix with high probability when the signal strength is above\nthe threshold $\\sf SNR_c$. The second threshold, $\\sf SNR_s$, captures the\nstatistical boundary, below which no method can succeed with probability going\nto one in the minimax sense. The exhaustive search method successfully finds\nthe submatrix above this threshold. The results show an interesting phenomenon\nthat $\\sf SNR_c$ is always significantly larger than $\\sf SNR_s$, which implies\nan essential gap between statistical optimality and computational efficiency\nfor submatrix localization.\n", "versions": [{"version": "v1", "created": "Fri, 6 Feb 2015 18:58:28 GMT"}, {"version": "v2", "created": "Wed, 21 Oct 2015 17:51:11 GMT"}], "update_date": "2020-07-27", "authors_parsed": [["Cai", "T. Tony", ""], ["Liang", "Tengyuan", ""], ["Rakhlin", "Alexander", ""]]}, {"id": "1502.02072", "submitter": "Bharath Ramsundar", "authors": "Bharath Ramsundar, Steven Kearnes, Patrick Riley, Dale Webster, David\n  Konerding, Vijay Pande", "title": "Massively Multitask Networks for Drug Discovery", "comments": "Preliminary work. Under review by the International Conference on\n  Machine Learning (ICML)", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Massively multitask neural architectures provide a learning framework for\ndrug discovery that synthesizes information from many distinct biological\nsources. To train these architectures at scale, we gather large amounts of data\nfrom public sources to create a dataset of nearly 40 million measurements\nacross more than 200 biological targets. We investigate several aspects of the\nmultitask framework by performing a series of empirical studies and obtain some\ninteresting results: (1) massively multitask networks obtain predictive\naccuracies significantly better than single-task methods, (2) the predictive\npower of multitask networks improves as additional tasks and data are added,\n(3) the total amount of data and the total number of tasks both contribute\nsignificantly to multitask improvement, and (4) multitask networks afford\nlimited transferability to tasks not in the training set. Our results\nunderscore the need for greater data sharing and further algorithmic innovation\nto accelerate the drug discovery process.\n", "versions": [{"version": "v1", "created": "Fri, 6 Feb 2015 23:04:01 GMT"}], "update_date": "2015-02-10", "authors_parsed": [["Ramsundar", "Bharath", ""], ["Kearnes", "Steven", ""], ["Riley", "Patrick", ""], ["Webster", "Dale", ""], ["Konerding", "David", ""], ["Pande", "Vijay", ""]]}, {"id": "1502.02089", "submitter": "Mauricio A. \\'Alvarez", "authors": "Sebasti\\'an G\\'omez-Gonz\\'alez, Mauricio A. \\'Alvarez, Hern\\'an Felipe\n  Garc\\'ia", "title": "Discriminative training for Convolved Multiple-Output Gaussian processes", "comments": "11 pages; 6 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Multi-output Gaussian processes (MOGP) are probability distributions over\nvector-valued functions, and have been previously used for multi-output\nregression and for multi-class classification. A less explored facet of the\nmulti-output Gaussian process is that it can be used as a generative model for\nvector-valued random fields in the context of pattern recognition. As a\ngenerative model, the multi-output GP is able to handle vector-valued functions\nwith continuous inputs, as opposed, for example, to hidden Markov models. It\nalso offers the ability to model multivariate random functions with high\ndimensional inputs. In this report, we use a discriminative training criteria\nknown as Minimum Classification Error to fit the parameters of a multi-output\nGaussian process. We compare the performance of generative training and\ndiscriminative training of MOGP in emotion recognition, activity recognition,\nand face recognition. We also compare the proposed methodology against hidden\nMarkov models trained in a generative and in a discriminative way.\n", "versions": [{"version": "v1", "created": "Sat, 7 Feb 2015 01:45:55 GMT"}], "update_date": "2015-02-10", "authors_parsed": [["G\u00f3mez-Gonz\u00e1lez", "Sebasti\u00e1n", ""], ["\u00c1lvarez", "Mauricio A.", ""], ["Garc\u00eda", "Hern\u00e1n Felipe", ""]]}, {"id": "1502.02127", "submitter": "Marc Claesen", "authors": "Marc Claesen and Bart De Moor", "title": "Hyperparameter Search in Machine Learning", "comments": "5 pages, accepted for MIC 2015: The XI Metaheuristics International\n  Conference in Agadir, Morocco", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We introduce the hyperparameter search problem in the field of machine\nlearning and discuss its main challenges from an optimization perspective.\nMachine learning methods attempt to build models that capture some element of\ninterest based on given data. Most common learning algorithms feature a set of\nhyperparameters that must be determined before training commences. The choice\nof hyperparameters can significantly affect the resulting model's performance,\nbut determining good values can be complex; hence a disciplined, theoretically\nsound search strategy is essential.\n", "versions": [{"version": "v1", "created": "Sat, 7 Feb 2015 11:46:22 GMT"}, {"version": "v2", "created": "Mon, 6 Apr 2015 15:44:52 GMT"}], "update_date": "2015-04-07", "authors_parsed": [["Claesen", "Marc", ""], ["De Moor", "Bart", ""]]}, {"id": "1502.02206", "submitter": "Kai-Wei Chang", "authors": "Kai-Wei Chang, Akshay Krishnamurthy, Alekh Agarwal, Hal Daum\\'e III,\n  John Langford", "title": "Learning to Search Better Than Your Teacher", "comments": "In ICML 2015", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Methods for learning to search for structured prediction typically imitate a\nreference policy, with existing theoretical guarantees demonstrating low regret\ncompared to that reference. This is unsatisfactory in many applications where\nthe reference policy is suboptimal and the goal of learning is to improve upon\nit. Can learning to search work even when the reference is poor?\n  We provide a new learning to search algorithm, LOLS, which does well relative\nto the reference policy, but additionally guarantees low regret compared to\ndeviations from the learned policy: a local-optimality guarantee. Consequently,\nLOLS can improve upon the reference policy, unlike previous algorithms. This\nenables us to develop structured contextual bandits, a partial information\nstructured prediction setting with many potential applications.\n", "versions": [{"version": "v1", "created": "Sun, 8 Feb 2015 03:18:50 GMT"}, {"version": "v2", "created": "Wed, 20 May 2015 05:48:10 GMT"}], "update_date": "2015-05-21", "authors_parsed": [["Chang", "Kai-Wei", ""], ["Krishnamurthy", "Akshay", ""], ["Agarwal", "Alekh", ""], ["Daum\u00e9", "Hal", "III"], ["Langford", "John", ""]]}, {"id": "1502.02251", "submitter": "Marc Deisenroth", "authors": "Niklas Wahlstr\\\"om and Thomas B. Sch\\\"on and Marc Peter Deisenroth", "title": "From Pixels to Torques: Policy Learning with Deep Dynamical Models", "comments": "9 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG cs.RO cs.SY", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Data-efficient learning in continuous state-action spaces using very\nhigh-dimensional observations remains a key challenge in developing fully\nautonomous systems. In this paper, we consider one instance of this challenge,\nthe pixels to torques problem, where an agent must learn a closed-loop control\npolicy from pixel information only. We introduce a data-efficient, model-based\nreinforcement learning algorithm that learns such a closed-loop policy directly\nfrom pixel information. The key ingredient is a deep dynamical model that uses\ndeep auto-encoders to learn a low-dimensional embedding of images jointly with\na predictive model in this low-dimensional feature space. Joint learning\nensures that not only static but also dynamic properties of the data are\naccounted for. This is crucial for long-term predictions, which lie at the core\nof the adaptive model predictive control strategy that we use for closed-loop\ncontrol. Compared to state-of-the-art reinforcement learning methods for\ncontinuous states and actions, our approach learns quickly, scales to\nhigh-dimensional state spaces and is an important step toward fully autonomous\nlearning from pixels to torques.\n", "versions": [{"version": "v1", "created": "Sun, 8 Feb 2015 13:57:59 GMT"}, {"version": "v2", "created": "Tue, 10 Feb 2015 11:27:12 GMT"}, {"version": "v3", "created": "Thu, 18 Jun 2015 16:59:43 GMT"}], "update_date": "2015-06-19", "authors_parsed": [["Wahlstr\u00f6m", "Niklas", ""], ["Sch\u00f6n", "Thomas B.", ""], ["Deisenroth", "Marc Peter", ""]]}, {"id": "1502.02259", "submitter": "Assaf Hallak", "authors": "Assaf Hallak, Dotan Di Castro and Shie Mannor", "title": "Contextual Markov Decision Processes", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider a planning problem where the dynamics and rewards of the\nenvironment depend on a hidden static parameter referred to as the context. The\nobjective is to learn a strategy that maximizes the accumulated reward across\nall contexts. The new model, called Contextual Markov Decision Process (CMDP),\ncan model a customer's behavior when interacting with a website (the learner).\nThe customer's behavior depends on gender, age, location, device, etc. Based on\nthat behavior, the website objective is to determine customer characteristics,\nand to optimize the interaction between them. Our work focuses on one basic\nscenario--finite horizon with a small known number of possible contexts. We\nsuggest a family of algorithms with provable guarantees that learn the\nunderlying models and the latent contexts, and optimize the CMDPs. Bounds are\nobtained for specific naive implementations, and extensions of the framework\nare discussed, laying the ground for future research.\n", "versions": [{"version": "v1", "created": "Sun, 8 Feb 2015 14:58:50 GMT"}], "update_date": "2015-02-10", "authors_parsed": [["Hallak", "Assaf", ""], ["Di Castro", "Dotan", ""], ["Mannor", "Shie", ""]]}, {"id": "1502.02309", "submitter": "Giovanni Montana", "authors": "Ricardo Pio Monti, Romy Lorenz, Christoforos Anagnostopoulos, Robert\n  Leech, Giovanni Montana", "title": "Measuring the functional connectome \"on-the-fly\": towards a new control\n  signal for fMRI-based brain-computer interfaces", "comments": "37 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  There has been an explosion of interest in functional Magnetic Resonance\nImaging (MRI) during the past two decades. Naturally, this has been accompanied\nby many major advances in the understanding of the human connectome. These\nadvances have served to pose novel challenges as well as open new avenues for\nresearch. One of the most promising and exciting of such avenues is the study\nof functional MRI in real-time. Such studies have recently gained momentum and\nhave been applied in a wide variety of settings; ranging from training of\nhealthy subjects to self-regulate neuronal activity to being suggested as\npotential treatments for clinical populations. To date, the vast majority of\nthese studies have focused on a single region at a time. This is due in part to\nthe many challenges faced when estimating dynamic functional connectivity\nnetworks in real-time. In this work we propose a novel methodology with which\nto accurately track changes in functional connectivity networks in real-time.\nWe adapt the recently proposed SINGLE algorithm for estimating sparse and\ntemporally homo- geneous dynamic networks to be applicable in real-time. The\nproposed method is applied to motor task data from the Human Connectome Project\nas well as to real-time data ob- tained while exploring a virtual environment.\nWe show that the algorithm is able to estimate significant task-related changes\nin network structure quickly enough to be useful in future brain-computer\ninterface applications.\n", "versions": [{"version": "v1", "created": "Sun, 8 Feb 2015 22:09:17 GMT"}], "update_date": "2016-10-31", "authors_parsed": [["Monti", "Ricardo Pio", ""], ["Lorenz", "Romy", ""], ["Anagnostopoulos", "Christoforos", ""], ["Leech", "Robert", ""], ["Montana", "Giovanni", ""]]}, {"id": "1502.02330", "submitter": "Dacheng Tao", "authors": "Yong Luo, Dacheng Tao, Yonggang Wen, Kotagiri Ramamohanarao, Chao Xu", "title": "Tensor Canonical Correlation Analysis for Multi-view Dimension Reduction", "comments": "20 pages, 10 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Canonical correlation analysis (CCA) has proven an effective tool for\ntwo-view dimension reduction due to its profound theoretical foundation and\nsuccess in practical applications. In respect of multi-view learning, however,\nit is limited by its capability of only handling data represented by two-view\nfeatures, while in many real-world applications, the number of views is\nfrequently many more. Although the ad hoc way of simultaneously exploring all\npossible pairs of features can numerically deal with multi-view data, it\nignores the high order statistics (correlation information) which can only be\ndiscovered by simultaneously exploring all features.\n  Therefore, in this work, we develop tensor CCA (TCCA) which straightforwardly\nyet naturally generalizes CCA to handle the data of an arbitrary number of\nviews by analyzing the covariance tensor of the different views. TCCA aims to\ndirectly maximize the canonical correlation of multiple (more than two) views.\nCrucially, we prove that the multi-view canonical correlation maximization\nproblem is equivalent to finding the best rank-1 approximation of the data\ncovariance tensor, which can be solved efficiently using the well-known\nalternating least squares (ALS) algorithm. As a consequence, the high order\ncorrelation information contained in the different views is explored and thus a\nmore reliable common subspace shared by all features can be obtained. In\naddition, a non-linear extension of TCCA is presented. Experiments on various\nchallenge tasks, including large scale biometric structure prediction, internet\nadvertisement classification and web image annotation, demonstrate the\neffectiveness of the proposed method.\n", "versions": [{"version": "v1", "created": "Mon, 9 Feb 2015 01:58:27 GMT"}], "update_date": "2015-02-10", "authors_parsed": [["Luo", "Yong", ""], ["Tao", "Dacheng", ""], ["Wen", "Yonggang", ""], ["Ramamohanarao", "Kotagiri", ""], ["Xu", "Chao", ""]]}, {"id": "1502.02344", "submitter": "Ichiro Takeuchi Prof.", "authors": "Atsushi Shibagaki, Yoshiki Suzuki, Masayuki Karasuyama, Ichiro\n  Takeuchi", "title": "Regularization Path of Cross-Validation Error Lower Bounds", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Careful tuning of a regularization parameter is indispensable in many machine\nlearning tasks because it has a significant impact on generalization\nperformances. Nevertheless, current practice of regularization parameter tuning\nis more of an art than a science, e.g., it is hard to tell how many grid-points\nwould be needed in cross-validation (CV) for obtaining a solution with\nsufficiently small CV error. In this paper we propose a novel framework for\ncomputing a lower bound of the CV errors as a function of the regularization\nparameter, which we call regularization path of CV error lower bounds. The\nproposed framework can be used for providing a theoretical approximation\nguarantee on a set of solutions in the sense that how far the CV error of the\ncurrent best solution could be away from best possible CV error in the entire\nrange of the regularization parameters. We demonstrate through numerical\nexperiments that a theoretically guaranteed a choice of regularization\nparameter in the above sense is possible with reasonable computational costs.\n", "versions": [{"version": "v1", "created": "Mon, 9 Feb 2015 03:35:38 GMT"}, {"version": "v2", "created": "Mon, 22 Jun 2015 08:50:09 GMT"}], "update_date": "2015-06-23", "authors_parsed": [["Shibagaki", "Atsushi", ""], ["Suzuki", "Yoshiki", ""], ["Karasuyama", "Masayuki", ""], ["Takeuchi", "Ichiro", ""]]}, {"id": "1502.02347", "submitter": "Quanquan Gu", "authors": "Quanquan Gu, Yuan Cao, Yang Ning, Han Liu", "title": "Local and Global Inference for High Dimensional Nonparanormal Graphical\n  Models", "comments": "58 pages, 4 figures, 2 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper proposes a unified framework to quantify local and global\ninferential uncertainty for high dimensional nonparanormal graphical models. In\nparticular, we consider the problems of testing the presence of a single edge\nand constructing a uniform confidence subgraph. Due to the presence of unknown\nmarginal transformations, we propose a pseudo likelihood based inferential\napproach. In sharp contrast to the existing high dimensional score test method,\nour method is free of tuning parameters given an initial estimator, and extends\nthe scope of the existing likelihood based inferential framework. Furthermore,\nwe propose a U-statistic multiplier bootstrap method to construct the\nconfidence subgraph. We show that the constructed subgraph is contained in the\ntrue graph with probability greater than a given nominal level. Compared with\nexisting methods for constructing confidence subgraphs, our method does not\nrely on Gaussian or sub-Gaussian assumptions. The theoretical properties of the\nproposed inferential methods are verified by thorough numerical experiments and\nreal data analysis.\n", "versions": [{"version": "v1", "created": "Mon, 9 Feb 2015 04:06:43 GMT"}, {"version": "v2", "created": "Tue, 30 Jun 2015 19:46:49 GMT"}], "update_date": "2015-07-01", "authors_parsed": [["Gu", "Quanquan", ""], ["Cao", "Yuan", ""], ["Ning", "Yang", ""], ["Liu", "Han", ""]]}, {"id": "1502.02355", "submitter": "Shuheng Zhou", "authors": "Mark Rudelson and Shuheng Zhou", "title": "High dimensional errors-in-variables models with dependent measurements", "comments": "61 pages", "journal-ref": null, "doi": null, "report-no": "Department of Statistics TR 538, University of Michigan", "categories": "math.ST stat.ML stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Suppose that we observe $y \\in \\mathbb{R}^f$ and $X \\in \\mathbb{R}^{f \\times\nm}$ in the following errors-in-variables model: \\begin{eqnarray*} y & = & X_0\n\\beta^* + \\epsilon \\\\ X & = & X_0 + W \\end{eqnarray*} where $X_0$ is a $f\n\\times m$ design matrix with independent subgaussian row vectors, $\\epsilon \\in\n\\mathbb{R}^f$ is a noise vector and $W$ is a mean zero $f \\times m$ random\nnoise matrix with independent subgaussian column vectors, independent of $X_0$\nand $\\epsilon$. This model is significantly different from those analyzed in\nthe literature in the sense that we allow the measurement error for each\ncovariate to be a dependent vector across its $f$ observations. Such error\nstructures appear in the science literature when modeling the trial-to-trial\nfluctuations in response strength shared across a set of neurons.\n  Under sparsity and restrictive eigenvalue type of conditions, we show that\none is able to recover a sparse vector $\\beta^* \\in \\mathbb{R}^m$ from the\nmodel given a single observation matrix $X$ and the response vector $y$. We\nestablish consistency in estimating $\\beta^*$ and obtain the rates of\nconvergence in the $\\ell_q$ norm, where $q = 1, 2$ for the Lasso-type\nestimator, and for $q \\in [1, 2]$ for a Dantzig-type conic programming\nestimator. We show error bounds which approach that of the regular Lasso and\nthe Dantzig selector in case the errors in $W$ are tending to 0.\n", "versions": [{"version": "v1", "created": "Mon, 9 Feb 2015 04:43:58 GMT"}, {"version": "v2", "created": "Fri, 18 Dec 2015 16:26:56 GMT"}], "update_date": "2015-12-21", "authors_parsed": [["Rudelson", "Mark", ""], ["Zhou", "Shuheng", ""]]}, {"id": "1502.02362", "submitter": "Adith Swaminathan", "authors": "Adith Swaminathan and Thorsten Joachims", "title": "Counterfactual Risk Minimization: Learning from Logged Bandit Feedback", "comments": "10 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We develop a learning principle and an efficient algorithm for batch learning\nfrom logged bandit feedback. This learning setting is ubiquitous in online\nsystems (e.g., ad placement, web search, recommendation), where an algorithm\nmakes a prediction (e.g., ad ranking) for a given input (e.g., query) and\nobserves bandit feedback (e.g., user clicks on presented ads). We first address\nthe counterfactual nature of the learning problem through propensity scoring.\nNext, we prove generalization error bounds that account for the variance of the\npropensity-weighted empirical risk estimator. These constructive bounds give\nrise to the Counterfactual Risk Minimization (CRM) principle. We show how CRM\ncan be used to derive a new learning method -- called Policy Optimizer for\nExponential Models (POEM) -- for learning stochastic linear rules for\nstructured output prediction. We present a decomposition of the POEM objective\nthat enables efficient stochastic gradient optimization. POEM is evaluated on\nseveral multi-label classification problems showing substantially improved\nrobustness and generalization performance compared to the state-of-the-art.\n", "versions": [{"version": "v1", "created": "Mon, 9 Feb 2015 05:09:25 GMT"}, {"version": "v2", "created": "Wed, 20 May 2015 23:29:49 GMT"}], "update_date": "2015-05-22", "authors_parsed": [["Swaminathan", "Adith", ""], ["Joachims", "Thorsten", ""]]}, {"id": "1502.02367", "submitter": "Junyoung Chung", "authors": "Junyoung Chung, Caglar Gulcehre, Kyunghyun Cho and Yoshua Bengio", "title": "Gated Feedback Recurrent Neural Networks", "comments": "9 pages, removed appendix", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NE cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this work, we propose a novel recurrent neural network (RNN) architecture.\nThe proposed RNN, gated-feedback RNN (GF-RNN), extends the existing approach of\nstacking multiple recurrent layers by allowing and controlling signals flowing\nfrom upper recurrent layers to lower layers using a global gating unit for each\npair of layers. The recurrent signals exchanged between layers are gated\nadaptively based on the previous hidden states and the current input. We\nevaluated the proposed GF-RNN with different types of recurrent units, such as\ntanh, long short-term memory and gated recurrent units, on the tasks of\ncharacter-level language modeling and Python program evaluation. Our empirical\nevaluation of different RNN units, revealed that in both tasks, the GF-RNN\noutperforms the conventional approaches to build deep stacked RNNs. We suggest\nthat the improvement arises because the GF-RNN can adaptively assign different\nlayers to different timescales and layer-to-layer interactions (including the\ntop-down ones which are not usually present in a stacked RNN) by learning to\ngate these interactions.\n", "versions": [{"version": "v1", "created": "Mon, 9 Feb 2015 05:25:54 GMT"}, {"version": "v2", "created": "Thu, 12 Feb 2015 19:18:07 GMT"}, {"version": "v3", "created": "Wed, 18 Feb 2015 11:34:38 GMT"}, {"version": "v4", "created": "Wed, 17 Jun 2015 06:26:21 GMT"}], "update_date": "2015-06-18", "authors_parsed": [["Chung", "Junyoung", ""], ["Gulcehre", "Caglar", ""], ["Cho", "Kyunghyun", ""], ["Bengio", "Yoshua", ""]]}, {"id": "1502.02377", "submitter": "Jim Jing-Yan Wang", "authors": "Mohua Zhang, Jianhua Peng, Xuejie Liu, Jim Jing-Yan Wang", "title": "Sparse Coding with Earth Mover's Distance for Multi-Instance Histogram\n  Representation", "comments": null, "journal-ref": null, "doi": "10.1007/s00521-016-2269-9", "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Sparse coding (Sc) has been studied very well as a powerful data\nrepresentation method. It attempts to represent the feature vector of a data\nsample by reconstructing it as the sparse linear combination of some basic\nelements, and a $L_2$ norm distance function is usually used as the loss\nfunction for the reconstruction error. In this paper, we investigate using Sc\nas the representation method within multi-instance learning framework, where a\nsample is given as a bag of instances, and further represented as a histogram\nof the quantized instances. We argue that for the data type of histogram, using\n$L_2$ norm distance is not suitable, and propose to use the earth mover's\ndistance (EMD) instead of $L_2$ norm distance as a measure of the\nreconstruction error. By minimizing the EMD between the histogram of a sample\nand the its reconstruction from some basic histograms, a novel sparse coding\nmethod is developed, which is refereed as SC-EMD. We evaluate its performances\nas a histogram representation method in tow multi-instance learning problems\n--- abnormal image detection in wireless capsule endoscopy videos, and protein\nbinding site retrieval. The encouraging results demonstrate the advantages of\nthe new method over the traditional method using $L_2$ norm distance.\n", "versions": [{"version": "v1", "created": "Mon, 9 Feb 2015 06:42:02 GMT"}, {"version": "v2", "created": "Mon, 14 Mar 2016 18:13:28 GMT"}], "update_date": "2016-03-15", "authors_parsed": [["Zhang", "Mohua", ""], ["Peng", "Jianhua", ""], ["Liu", "Xuejie", ""], ["Wang", "Jim Jing-Yan", ""]]}, {"id": "1502.02398", "submitter": "David Lopez-Paz", "authors": "David Lopez-Paz, Krikamol Muandet, Bernhard Sch\\\"olkopf, Ilya\n  Tolstikhin", "title": "Towards a Learning Theory of Cause-Effect Inference", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML math.PR math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We pose causal inference as the problem of learning to classify probability\ndistributions. In particular, we assume access to a collection\n$\\{(S_i,l_i)\\}_{i=1}^n$, where each $S_i$ is a sample drawn from the\nprobability distribution of $X_i \\times Y_i$, and $l_i$ is a binary label\nindicating whether \"$X_i \\to Y_i$\" or \"$X_i \\leftarrow Y_i$\". Given these data,\nwe build a causal inference rule in two steps. First, we featurize each $S_i$\nusing the kernel mean embedding associated with some characteristic kernel.\nSecond, we train a binary classifier on such embeddings to distinguish between\ncausal directions. We present generalization bounds showing the statistical\nconsistency and learning rates of the proposed approach, and provide a simple\nimplementation that achieves state-of-the-art cause-effect inference.\nFurthermore, we extend our ideas to infer causal relationships between more\nthan two variables.\n", "versions": [{"version": "v1", "created": "Mon, 9 Feb 2015 08:49:26 GMT"}, {"version": "v2", "created": "Mon, 18 May 2015 21:45:37 GMT"}], "update_date": "2015-05-20", "authors_parsed": [["Lopez-Paz", "David", ""], ["Muandet", "Krikamol", ""], ["Sch\u00f6lkopf", "Bernhard", ""], ["Tolstikhin", "Ilya", ""]]}, {"id": "1502.02445", "submitter": "Giovanni Montana", "authors": "Alexandre de Brebisson, Giovanni Montana", "title": "Deep Neural Networks for Anatomical Brain Segmentation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG stat.AP stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a novel approach to automatically segment magnetic resonance (MR)\nimages of the human brain into anatomical regions. Our methodology is based on\na deep artificial neural network that assigns each voxel in an MR image of the\nbrain to its corresponding anatomical region. The inputs of the network capture\ninformation at different scales around the voxel of interest: 3D and orthogonal\n2D intensity patches capture the local spatial context while large, compressed\n2D orthogonal patches and distances to the regional centroids enforce global\nspatial consistency. Contrary to commonly used segmentation methods, our\ntechnique does not require any non-linear registration of the MR images. To\nbenchmark our model, we used the dataset provided for the MICCAI 2012 challenge\non multi-atlas labelling, which consists of 35 manually segmented MR images of\nthe brain. We obtained competitive results (mean dice coefficient 0.725, error\nrate 0.163) showing the potential of our approach. To our knowledge, our\ntechnique is the first to tackle the anatomical segmentation of the whole brain\nusing deep neural networks.\n", "versions": [{"version": "v1", "created": "Mon, 9 Feb 2015 11:48:42 GMT"}, {"version": "v2", "created": "Thu, 25 Jun 2015 16:19:44 GMT"}], "update_date": "2015-06-26", "authors_parsed": [["de Brebisson", "Alexandre", ""], ["Montana", "Giovanni", ""]]}, {"id": "1502.02506", "submitter": "Giovanni Montana", "authors": "Adrien Payan, Giovanni Montana", "title": "Predicting Alzheimer's disease: a neuroimaging study with 3D\n  convolutional neural networks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG stat.AP stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Pattern recognition methods using neuroimaging data for the diagnosis of\nAlzheimer's disease have been the subject of extensive research in recent\nyears. In this paper, we use deep learning methods, and in particular sparse\nautoencoders and 3D convolutional neural networks, to build an algorithm that\ncan predict the disease status of a patient, based on an MRI scan of the brain.\nWe report on experiments using the ADNI data set involving 2,265 historical\nscans. We demonstrate that 3D convolutional neural networks outperform several\nother classifiers reported in the literature and produce state-of-art results.\n", "versions": [{"version": "v1", "created": "Mon, 9 Feb 2015 14:46:40 GMT"}], "update_date": "2015-02-10", "authors_parsed": [["Payan", "Adrien", ""], ["Montana", "Giovanni", ""]]}, {"id": "1502.02513", "submitter": "Manuel Martin", "authors": "M.P. Martin, T.G. Orton, E. Lacarce, J. Meersmans, N.P.A. Saby, J.B.\n  Paroissien, C. Jolivet, L. Boulonne, D. Arrouays", "title": "Evaluation of modelling approaches for predicting the spatial\n  distribution of soil organic carbon stocks at the national scale", "comments": null, "journal-ref": "Geoderma, Volumes 223-225, July 2014, Pages 97-107", "doi": "10.1016/j.geoderma.2014.01.005", "report-no": null, "categories": "stat.AP stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Soil organic carbon (SOC) plays a major role in the global carbon budget. It\ncan act as a source or a sink of atmospheric carbon, thereby possibly\ninfluencing the course of climate change. Improving the tools that model the\nspatial distributions of SOC stocks at national scales is a priority, both for\nmonitoring changes in SOC and as an input for global carbon cycles studies. In\nthis paper, we compare and evaluate two recent and promising modelling\napproaches. First, we considered several increasingly complex boosted\nregression trees (BRT), a convenient and efficient multiple regression model\nfrom the statistical learning field. Further, we considered a robust\ngeostatistical approach coupled to the BRT models. Testing the different\napproaches was performed on the dataset from the French Soil Monitoring\nNetwork, with a consistent cross-validation procedure. We showed that when a\nlimited number of predictors were included in the BRT model, the standalone BRT\npredictions were significantly improved by robust geostatistical modelling of\nthe residuals. However, when data for several SOC drivers were included, the\nstandalone BRT model predictions were not significantly improved by\ngeostatistical modelling. Therefore, in this latter situation, the BRT\npredictions might be considered adequate without the need for geostatistical\nmodelling, provided that i) care is exercised in model fitting and validating,\nand ii) the dataset does not allow for modelling of local spatial\nautocorrelations, as is the case for many national systematic sampling schemes.\n", "versions": [{"version": "v1", "created": "Mon, 9 Feb 2015 15:00:13 GMT"}], "update_date": "2015-02-10", "authors_parsed": [["Martin", "M. P.", ""], ["Orton", "T. G.", ""], ["Lacarce", "E.", ""], ["Meersmans", "J.", ""], ["Saby", "N. P. A.", ""], ["Paroissien", "J. B.", ""], ["Jolivet", "C.", ""], ["Boulonne", "L.", ""], ["Arrouays", "D.", ""]]}, {"id": "1502.02536", "submitter": "Christian A. Naesseth", "authors": "Christian A. Naesseth and Fredrik Lindsten and Thomas B. Sch\\\"on", "title": "Nested Sequential Monte Carlo Methods", "comments": "Extended version of paper published in Proceedings of the 32nd\n  International Conference on Machine Learning (ICML), Lille, France, 2015", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.CO stat.ME stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose nested sequential Monte Carlo (NSMC), a methodology to sample from\nsequences of probability distributions, even where the random variables are\nhigh-dimensional. NSMC generalises the SMC framework by requiring only\napproximate, properly weighted, samples from the SMC proposal distribution,\nwhile still resulting in a correct SMC algorithm. Furthermore, NSMC can in\nitself be used to produce such properly weighted samples. Consequently, one\nNSMC sampler can be used to construct an efficient high-dimensional proposal\ndistribution for another NSMC sampler, and this nesting of the algorithm can be\ndone to an arbitrary degree. This allows us to consider complex and\nhigh-dimensional models using SMC. We show results that motivate the efficacy\nof our approach on several filtering problems with dimensions in the order of\n100 to 1 000.\n", "versions": [{"version": "v1", "created": "Mon, 9 Feb 2015 16:15:31 GMT"}, {"version": "v2", "created": "Wed, 3 Jun 2015 07:04:58 GMT"}, {"version": "v3", "created": "Fri, 11 Sep 2015 11:01:25 GMT"}], "update_date": "2015-09-14", "authors_parsed": [["Naesseth", "Christian A.", ""], ["Lindsten", "Fredrik", ""], ["Sch\u00f6n", "Thomas B.", ""]]}, {"id": "1502.02551", "submitter": "Suyog Gupta", "authors": "Suyog Gupta, Ankur Agrawal, Kailash Gopalakrishnan, Pritish Narayanan", "title": "Deep Learning with Limited Numerical Precision", "comments": "10 pages, 6 figures, 1 table", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.NE stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Training of large-scale deep neural networks is often constrained by the\navailable computational resources. We study the effect of limited precision\ndata representation and computation on neural network training. Within the\ncontext of low-precision fixed-point computations, we observe the rounding\nscheme to play a crucial role in determining the network's behavior during\ntraining. Our results show that deep networks can be trained using only 16-bit\nwide fixed-point number representation when using stochastic rounding, and\nincur little to no degradation in the classification accuracy. We also\ndemonstrate an energy-efficient hardware accelerator that implements\nlow-precision fixed-point arithmetic with stochastic rounding.\n", "versions": [{"version": "v1", "created": "Mon, 9 Feb 2015 16:37:29 GMT"}], "update_date": "2015-02-11", "authors_parsed": [["Gupta", "Suyog", ""], ["Agrawal", "Ankur", ""], ["Gopalakrishnan", "Kailash", ""], ["Narayanan", "Pritish", ""]]}, {"id": "1502.02558", "submitter": "Mijung Park", "authors": "Mijung Park and Wittawat Jitkrittum and Dino Sejdinovic", "title": "K2-ABC: Approximate Bayesian Computation with Kernel Embeddings", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Complicated generative models often result in a situation where computing the\nlikelihood of observed data is intractable, while simulating from the\nconditional density given a parameter value is relatively easy. Approximate\nBayesian Computation (ABC) is a paradigm that enables simulation-based\nposterior inference in such cases by measuring the similarity between simulated\nand observed data in terms of a chosen set of summary statistics. However,\nthere is no general rule to construct sufficient summary statistics for complex\nmodels. Insufficient summary statistics will \"leak\" information, which leads to\nABC algorithms yielding samples from an incorrect (partial) posterior. In this\npaper, we propose a fully nonparametric ABC paradigm which circumvents the need\nfor manually selecting summary statistics. Our approach, K2-ABC, uses maximum\nmean discrepancy (MMD) as a dissimilarity measure between the distributions\nover observed and simulated data. MMD is easily estimated as the squared\ndifference between their empirical kernel embeddings. Experiments on a\nsimulated scenario and a real-world biological problem illustrate the\neffectiveness of the proposed algorithm.\n", "versions": [{"version": "v1", "created": "Mon, 9 Feb 2015 16:49:31 GMT"}, {"version": "v2", "created": "Mon, 8 Jun 2015 13:21:39 GMT"}, {"version": "v3", "created": "Thu, 24 Dec 2015 13:04:48 GMT"}, {"version": "v4", "created": "Sat, 26 Dec 2015 16:57:55 GMT"}], "update_date": "2015-12-29", "authors_parsed": [["Park", "Mijung", ""], ["Jitkrittum", "Wittawat", ""], ["Sejdinovic", "Dino", ""]]}, {"id": "1502.02590", "submitter": "Alhussein Fawzi", "authors": "Alhussein Fawzi, Omar Fawzi, Pascal Frossard", "title": "Analysis of classifiers' robustness to adversarial perturbations", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The goal of this paper is to analyze an intriguing phenomenon recently\ndiscovered in deep networks, namely their instability to adversarial\nperturbations (Szegedy et. al., 2014). We provide a theoretical framework for\nanalyzing the robustness of classifiers to adversarial perturbations, and show\nfundamental upper bounds on the robustness of classifiers. Specifically, we\nestablish a general upper bound on the robustness of classifiers to adversarial\nperturbations, and then illustrate the obtained upper bound on the families of\nlinear and quadratic classifiers. In both cases, our upper bound depends on a\ndistinguishability measure that captures the notion of difficulty of the\nclassification task. Our results for both classes imply that in tasks involving\nsmall distinguishability, no classifier in the considered set will be robust to\nadversarial perturbations, even if a good accuracy is achieved. Our theoretical\nframework moreover suggests that the phenomenon of adversarial instability is\ndue to the low flexibility of classifiers, compared to the difficulty of the\nclassification task (captured by the distinguishability). Moreover, we show the\nexistence of a clear distinction between the robustness of a classifier to\nrandom noise and its robustness to adversarial perturbations. Specifically, the\nformer is shown to be larger than the latter by a factor that is proportional\nto \\sqrt{d} (with d being the signal dimension) for linear classifiers. This\nresult gives a theoretical explanation for the discrepancy between the two\nrobustness properties in high dimensional problems, which was empirically\nobserved in the context of neural networks. To the best of our knowledge, our\nresults provide the first theoretical work that addresses the phenomenon of\nadversarial instability recently observed for deep networks. Our analysis is\ncomplemented by experimental results on controlled and real-world data.\n", "versions": [{"version": "v1", "created": "Mon, 9 Feb 2015 18:20:00 GMT"}, {"version": "v2", "created": "Wed, 18 Nov 2015 20:57:10 GMT"}, {"version": "v3", "created": "Thu, 7 Jan 2016 18:55:41 GMT"}, {"version": "v4", "created": "Mon, 28 Mar 2016 22:50:52 GMT"}], "update_date": "2016-03-30", "authors_parsed": [["Fawzi", "Alhussein", ""], ["Fawzi", "Omar", ""], ["Frossard", "Pascal", ""]]}, {"id": "1502.02613", "submitter": "Renliang Gu", "authors": "Renliang Gu and Aleksandar Dogand\\v{z}i\\'c", "title": "Projected Nesterov's Proximal-Gradient Algorithm for Sparse Signal\n  Reconstruction with a Convex Constraint", "comments": null, "journal-ref": "IEEE Trans. Signal Processing, vol, 65, no. 2, (2017) 3510-3525", "doi": "10.1109/TSP.2017.2691661", "report-no": null, "categories": "stat.CO stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We develop a projected Nesterov's proximal-gradient (PNPG) approach for\nsparse signal reconstruction that combines adaptive step size with Nesterov's\nmomentum acceleration. The objective function that we wish to minimize is the\nsum of a convex differentiable data-fidelity (negative log-likelihood (NLL))\nterm and a convex regularization term. We apply sparse signal regularization\nwhere the signal belongs to a closed convex set within the closure of the\ndomain of the NLL; the convex-set constraint facilitates flexible NLL domains\nand accurate signal recovery. Signal sparsity is imposed using the\n$\\ell_1$-norm penalty on the signal's linear transform coefficients or gradient\nmap, respectively. The PNPG approach employs projected Nesterov's acceleration\nstep with restart and an inner iteration to compute the proximal mapping. We\npropose an adaptive step-size selection scheme to obtain a good local\nmajorizing function of the NLL and reduce the time spent backtracking. Thanks\nto step-size adaptation, PNPG does not require Lipschitz continuity of the\ngradient of the NLL. We present an integrated derivation of the momentum\nacceleration and its $\\mathcal{O}(k^{-2})$ convergence-rate and iterate\nconvergence proofs, which account for adaptive step-size selection, inexactness\nof the iterative proximal mapping, and the convex-set constraint. The tuning of\nPNPG is largely application-independent. Tomographic and compressed-sensing\nreconstruction experiments with Poisson generalized linear and Gaussian linear\nmeasurement models demonstrate the performance of the proposed approach.\n", "versions": [{"version": "v1", "created": "Mon, 9 Feb 2015 19:19:46 GMT"}, {"version": "v2", "created": "Sat, 21 Feb 2015 21:25:37 GMT"}, {"version": "v3", "created": "Tue, 31 Mar 2015 16:25:34 GMT"}, {"version": "v4", "created": "Sun, 15 May 2016 19:41:15 GMT"}, {"version": "v5", "created": "Wed, 25 May 2016 21:42:27 GMT"}, {"version": "v6", "created": "Wed, 5 Oct 2016 08:04:30 GMT"}], "update_date": "2017-05-09", "authors_parsed": [["Gu", "Renliang", ""], ["Dogand\u017ei\u0107", "Aleksandar", ""]]}, {"id": "1502.02761", "submitter": "Yujia Li", "authors": "Yujia Li, Kevin Swersky and Richard Zemel", "title": "Generative Moment Matching Networks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider the problem of learning deep generative models from data. We\nformulate a method that generates an independent sample via a single\nfeedforward pass through a multilayer perceptron, as in the recently proposed\ngenerative adversarial networks (Goodfellow et al., 2014). Training a\ngenerative adversarial network, however, requires careful optimization of a\ndifficult minimax program. Instead, we utilize a technique from statistical\nhypothesis testing known as maximum mean discrepancy (MMD), which leads to a\nsimple objective that can be interpreted as matching all orders of statistics\nbetween a dataset and samples from the model, and can be trained by\nbackpropagation. We further boost the performance of this approach by combining\nour generative network with an auto-encoder network, using MMD to learn to\ngenerate codes that can then be decoded to produce samples. We show that the\ncombination of these techniques yields excellent generative models compared to\nbaseline approaches as measured on MNIST and the Toronto Face Database.\n", "versions": [{"version": "v1", "created": "Tue, 10 Feb 2015 02:54:58 GMT"}], "update_date": "2015-02-11", "authors_parsed": [["Li", "Yujia", ""], ["Swersky", "Kevin", ""], ["Zemel", "Richard", ""]]}, {"id": "1502.02763", "submitter": "Branislav Kveton", "authors": "Branislav Kveton, Csaba Szepesvari, Zheng Wen, and Azin Ashkan", "title": "Cascading Bandits: Learning to Rank in the Cascade Model", "comments": "Proceedings of the 32nd International Conference on Machine Learning", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A search engine usually outputs a list of $K$ web pages. The user examines\nthis list, from the first web page to the last, and chooses the first\nattractive page. This model of user behavior is known as the cascade model. In\nthis paper, we propose cascading bandits, a learning variant of the cascade\nmodel where the objective is to identify $K$ most attractive items. We\nformulate our problem as a stochastic combinatorial partial monitoring problem.\nWe propose two algorithms for solving it, CascadeUCB1 and CascadeKL-UCB. We\nalso prove gap-dependent upper bounds on the regret of these algorithms and\nderive a lower bound on the regret in cascading bandits. The lower bound\nmatches the upper bound of CascadeKL-UCB up to a logarithmic factor. We\nexperiment with our algorithms on several problems. The algorithms perform\nsurprisingly well even when our modeling assumptions are violated.\n", "versions": [{"version": "v1", "created": "Tue, 10 Feb 2015 02:56:04 GMT"}, {"version": "v2", "created": "Mon, 18 May 2015 19:03:38 GMT"}], "update_date": "2015-05-19", "authors_parsed": [["Kveton", "Branislav", ""], ["Szepesvari", "Csaba", ""], ["Wen", "Zheng", ""], ["Ashkan", "Azin", ""]]}, {"id": "1502.02843", "submitter": "Marc Deisenroth", "authors": "Marc Peter Deisenroth and Jun Wei Ng", "title": "Distributed Gaussian Processes", "comments": "10 pages, 5 figures. Appears in Proceedings of ICML 2015", "journal-ref": "JMLR W&CP, vol 37, 2015", "doi": null, "report-no": null, "categories": "stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  To scale Gaussian processes (GPs) to large data sets we introduce the robust\nBayesian Committee Machine (rBCM), a practical and scalable product-of-experts\nmodel for large-scale distributed GP regression. Unlike state-of-the-art sparse\nGP approximations, the rBCM is conceptually simple and does not rely on\ninducing or variational parameters. The key idea is to recursively distribute\ncomputations to independent computational units and, subsequently, recombine\nthem to form an overall result. Efficient closed-form inference allows for\nstraightforward parallelisation and distributed computations with a small\nmemory footprint. The rBCM is independent of the computational graph and can be\nused on heterogeneous computing infrastructures, ranging from laptops to\nclusters. With sufficient computing resources our distributed GP model can\nhandle arbitrarily large data sets.\n", "versions": [{"version": "v1", "created": "Tue, 10 Feb 2015 10:31:41 GMT"}, {"version": "v2", "created": "Sun, 17 May 2015 07:54:02 GMT"}, {"version": "v3", "created": "Fri, 22 May 2015 06:46:11 GMT"}], "update_date": "2015-05-25", "authors_parsed": [["Deisenroth", "Marc Peter", ""], ["Ng", "Jun Wei", ""]]}, {"id": "1502.02846", "submitter": "Maren Mahsereci", "authors": "Maren Mahsereci and Philipp Hennig", "title": "Probabilistic Line Searches for Stochastic Optimization", "comments": "12 pages, including supplements", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG math.OC stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In deterministic optimization, line searches are a standard tool ensuring\nstability and efficiency. Where only stochastic gradients are available, no\ndirect equivalent has so far been formulated, because uncertain gradients do\nnot allow for a strict sequence of decisions collapsing the search space. We\nconstruct a probabilistic line search by combining the structure of existing\ndeterministic methods with notions from Bayesian optimization. Our method\nretains a Gaussian process surrogate of the univariate optimization objective,\nand uses a probabilistic belief over the Wolfe conditions to monitor the\ndescent. The algorithm has very low computational cost, and no user-controlled\nparameters. Experiments show that it effectively removes the need to define a\nlearning rate for stochastic gradient descent.\n", "versions": [{"version": "v1", "created": "Tue, 10 Feb 2015 10:36:25 GMT"}, {"version": "v2", "created": "Thu, 2 Jul 2015 15:47:59 GMT"}, {"version": "v3", "created": "Fri, 4 Dec 2015 14:47:40 GMT"}, {"version": "v4", "created": "Mon, 18 Jan 2016 13:17:08 GMT"}], "update_date": "2016-01-19", "authors_parsed": [["Mahsereci", "Maren", ""], ["Hennig", "Philipp", ""]]}, {"id": "1502.02860", "submitter": "Marc Deisenroth", "authors": "Marc Peter Deisenroth, Dieter Fox and Carl Edward Rasmussen", "title": "Gaussian Processes for Data-Efficient Learning in Robotics and Control", "comments": "20 pages, 29 figures; fixed a typo in equation on page 8", "journal-ref": "IEEE Transactions on Pattern Analysis and Machine Intelligence,\n  vol. 37, issue no 2, pages 408-423, February 2015", "doi": "10.1109/TPAMI.2013.218", "report-no": null, "categories": "stat.ML cs.LG cs.RO cs.SY", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Autonomous learning has been a promising direction in control and robotics\nfor more than a decade since data-driven learning allows to reduce the amount\nof engineering knowledge, which is otherwise required. However, autonomous\nreinforcement learning (RL) approaches typically require many interactions with\nthe system to learn controllers, which is a practical limitation in real\nsystems, such as robots, where many interactions can be impractical and time\nconsuming. To address this problem, current learning approaches typically\nrequire task-specific knowledge in form of expert demonstrations, realistic\nsimulators, pre-shaped policies, or specific knowledge about the underlying\ndynamics. In this article, we follow a different approach and speed up learning\nby extracting more information from data. In particular, we learn a\nprobabilistic, non-parametric Gaussian process transition model of the system.\nBy explicitly incorporating model uncertainty into long-term planning and\ncontroller learning our approach reduces the effects of model errors, a key\nproblem in model-based learning. Compared to state-of-the art RL our\nmodel-based policy search method achieves an unprecedented speed of learning.\nWe demonstrate its applicability to autonomous learning in real robot and\ncontrol tasks.\n", "versions": [{"version": "v1", "created": "Tue, 10 Feb 2015 11:09:38 GMT"}, {"version": "v2", "created": "Tue, 10 Oct 2017 18:25:45 GMT"}], "update_date": "2017-10-12", "authors_parsed": [["Deisenroth", "Marc Peter", ""], ["Fox", "Dieter", ""], ["Rasmussen", "Carl Edward", ""]]}, {"id": "1502.03032", "submitter": "Jiyan Yang", "authors": "Jiyan Yang, Xiangrui Meng, Michael W. Mahoney", "title": "Implementing Randomized Matrix Algorithms in Parallel and Distributed\n  Environments", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.DS math.NA stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this era of large-scale data, distributed systems built on top of clusters\nof commodity hardware provide cheap and reliable storage and scalable\nprocessing of massive data. Here, we review recent work on developing and\nimplementing randomized matrix algorithms in large-scale parallel and\ndistributed environments. Randomized algorithms for matrix problems have\nreceived a great deal of attention in recent years, thus far typically either\nin theory or in machine learning applications or with implementations on a\nsingle machine. Our main focus is on the underlying theory and practical\nimplementation of random projection and random sampling algorithms for very\nlarge very overdetermined (i.e., overconstrained) $\\ell_1$ and $\\ell_2$\nregression problems. Randomization can be used in one of two related ways:\neither to construct sub-sampled problems that can be solved, exactly or\napproximately, with traditional numerical methods; or to construct\npreconditioned versions of the original full problem that are easier to solve\nwith traditional iterative algorithms. Theoretical results demonstrate that in\nnear input-sparsity time and with only a few passes through the data one can\nobtain very strong relative-error approximate solutions, with high probability.\nEmpirical results highlight the importance of various trade-offs (e.g., between\nthe time to construct an embedding and the conditioning quality of the\nembedding, between the relative importance of computation versus communication,\netc.) and demonstrate that $\\ell_1$ and $\\ell_2$ regression problems can be\nsolved to low, medium, or high precision in existing distributed systems on up\nto terabyte-sized data.\n", "versions": [{"version": "v1", "created": "Tue, 10 Feb 2015 18:38:44 GMT"}, {"version": "v2", "created": "Mon, 27 Jul 2015 07:15:33 GMT"}], "update_date": "2015-07-28", "authors_parsed": [["Yang", "Jiyan", ""], ["Meng", "Xiangrui", ""], ["Mahoney", "Michael W.", ""]]}, {"id": "1502.03042", "submitter": "Leo Duan", "authors": "Leo L. Duan, Xia Wang and Rhonda D. Szczesniak", "title": "Functional Gaussian Process Model for Bayesian Nonparametric Analysis", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML stat.CO stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Gaussian process is a theoretically appealing model for nonparametric\nanalysis, but its computational cumbersomeness hinders its use in large scale\nand the existing reduced-rank solutions are usually heuristic. In this work, we\npropose a novel construction of Gaussian process as a projection from fixed\ndiscrete frequencies to any continuous location. This leads to a valid\nstochastic process that has a theoretic support with the reduced rank in the\nspectral density, as well as a high-speed computing algorithm. Our method\nprovides accurate estimates for the covariance parameters and concise form of\npredictive distribution for spatial prediction. For non-stationary data, we\nadopt the mixture framework with a customized spectral dependency structure.\nThis enables clustering based on local stationarity, while maintains the joint\nGaussianness. Our work is directly applicable in solving some of the challenges\nin the spatial data, such as large scale computation, anisotropic covariance,\nspatio-temporal modeling, etc. We illustrate the uses of the model via\nsimulations and an application on a massive dataset.\n", "versions": [{"version": "v1", "created": "Tue, 10 Feb 2015 18:57:58 GMT"}, {"version": "v2", "created": "Tue, 24 Nov 2015 03:15:06 GMT"}], "update_date": "2015-11-25", "authors_parsed": [["Duan", "Leo L.", ""], ["Wang", "Xia", ""], ["Szczesniak", "Rhonda D.", ""]]}, {"id": "1502.03126", "submitter": "Soheil Bahrampour", "authors": "Soheil Bahrampour and Nasser M. Nasrabadi and Asok Ray and Kenneth W.\n  Jenkins", "title": "Kernel Task-Driven Dictionary Learning for Hyperspectral Image\n  Classification", "comments": "5 pages, IEEE International Conference on Acoustics, Speech, and\n  Signal Processing (ICASSP), 2015", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Dictionary learning algorithms have been successfully used in both\nreconstructive and discriminative tasks, where the input signal is represented\nby a linear combination of a few dictionary atoms. While these methods are\nusually developed under $\\ell_1$ sparsity constrain (prior) in the input\ndomain, recent studies have demonstrated the advantages of sparse\nrepresentation using structured sparsity priors in the kernel domain. In this\npaper, we propose a supervised dictionary learning algorithm in the kernel\ndomain for hyperspectral image classification. In the proposed formulation, the\ndictionary and classifier are obtained jointly for optimal classification\nperformance. The supervised formulation is task-driven and provides learned\nfeatures from the hyperspectral data that are well suited for the\nclassification task. Moreover, the proposed algorithm uses a joint\n($\\ell_{12}$) sparsity prior to enforce collaboration among the neighboring\npixels. The simulation results illustrate the efficiency of the proposed\ndictionary learning algorithm.\n", "versions": [{"version": "v1", "created": "Tue, 10 Feb 2015 21:27:27 GMT"}], "update_date": "2015-02-12", "authors_parsed": [["Bahrampour", "Soheil", ""], ["Nasrabadi", "Nasser M.", ""], ["Ray", "Asok", ""], ["Jenkins", "Kenneth W.", ""]]}, {"id": "1502.03163", "submitter": "Yuancheng Luo", "authors": "Yuancheng Luo, Dmitry N. Zotkin, Ramani Duraiswami", "title": "Gaussian Process Models for HRTF based Sound-Source Localization and\n  Active-Learning", "comments": "11 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.SD cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  From a machine learning perspective, the human ability localize sounds can be\nmodeled as a non-parametric and non-linear regression problem between binaural\nspectral features of sound received at the ears (input) and their sound-source\ndirections (output). The input features can be summarized in terms of the\nindividual's head-related transfer functions (HRTFs) which measure the spectral\nresponse between the listener's eardrum and an external point in $3$D. Based on\nthese viewpoints, two related problems are considered: how can one achieve an\noptimal sampling of measurements for training sound-source localization (SSL)\nmodels, and how can SSL models be used to infer the subject's HRTFs in\nlistening tests. First, we develop a class of binaural SSL models based on\nGaussian process regression and solve a \\emph{forward selection} problem that\nfinds a subset of input-output samples that best generalize to all SSL\ndirections. Second, we use an \\emph{active-learning} approach that updates an\nonline SSL model for inferring the subject's SSL errors via headphones and a\ngraphical user interface. Experiments show that only a small fraction of HRTFs\nare required for $5^{\\circ}$ localization accuracy and that the learned HRTFs\nare localized closer to their intended directions than non-individualized\nHRTFs.\n", "versions": [{"version": "v1", "created": "Wed, 11 Feb 2015 00:55:14 GMT"}], "update_date": "2015-02-12", "authors_parsed": [["Luo", "Yuancheng", ""], ["Zotkin", "Dmitry N.", ""], ["Duraiswami", "Ramani", ""]]}, {"id": "1502.03175", "submitter": "Brandon Willard", "authors": "Nicholas G. Polson, James G. Scott and Brandon T. Willard", "title": "Proximal Algorithms in Statistics and Machine Learning", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper we develop proximal methods for statistical learning. Proximal\npoint algorithms are useful in statistics and machine learning for obtaining\noptimization solutions for composite functions. Our approach exploits\nclosed-form solutions of proximal operators and envelope representations based\non the Moreau, Forward-Backward, Douglas-Rachford and Half-Quadratic envelopes.\nEnvelope representations lead to novel proximal algorithms for statistical\noptimisation of composite objective functions which include both non-smooth and\nnon-convex objectives. We illustrate our methodology with regularized Logistic\nand Poisson regression and non-convex bridge penalties with a fused lasso norm.\nWe provide a discussion of convergence of non-descent algorithms with\nacceleration and for non-convex functions. Finally, we provide directions for\nfuture research.\n", "versions": [{"version": "v1", "created": "Wed, 11 Feb 2015 02:21:49 GMT"}, {"version": "v2", "created": "Thu, 5 Mar 2015 18:22:40 GMT"}, {"version": "v3", "created": "Sat, 30 May 2015 22:01:39 GMT"}], "update_date": "2015-06-02", "authors_parsed": [["Polson", "Nicholas G.", ""], ["Scott", "James G.", ""], ["Willard", "Brandon T.", ""]]}, {"id": "1502.03211", "submitter": "Fang Han", "authors": "Cheng Zhou, Fang Han, Xinsheng Zhang, Han Liu", "title": "An Extreme-Value Approach for Testing the Equality of Large U-Statistic\n  Based Correlation Matrices", "comments": "to appear in Bernoulli", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.ML stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  There has been an increasing interest in testing the equality of large\nPearson's correlation matrices. However, in many applications it is more\nimportant to test the equality of large rank-based correlation matrices since\nthey are more robust to outliers and nonlinearity. Unlike the Pearson's case,\ntesting the equality of large rank-based statistics has not been well explored\nand requires us to develop new methods and theory. In this paper, we provide a\nframework for testing the equality of two large U-statistic based correlation\nmatrices, which include the rank-based correlation matrices as special cases.\nOur approach exploits extreme value statistics and the Jackknife estimator for\nuncertainty assessment and is valid under a fully nonparametric model.\nTheoretically, we develop a theory for testing the equality of U-statistic\nbased correlation matrices. We then apply this theory to study the problem of\ntesting large Kendall's tau correlation matrices and demonstrate its\noptimality. For proving this optimality, a novel construction of least\nfavourable distributions is developed for the correlation matrix comparison.\n", "versions": [{"version": "v1", "created": "Wed, 11 Feb 2015 07:54:25 GMT"}, {"version": "v2", "created": "Fri, 30 Mar 2018 16:51:19 GMT"}], "update_date": "2018-04-02", "authors_parsed": [["Zhou", "Cheng", ""], ["Han", "Fang", ""], ["Zhang", "Xinsheng", ""], ["Liu", "Han", ""]]}, {"id": "1502.03255", "submitter": "Fran\\c{c}ois Schnitzler", "authors": "Assaf Hallak and Fran\\c{c}ois Schnitzler and Timothy Mann and Shie\n  Mannor", "title": "Off-policy evaluation for MDPs with unknown structure", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Off-policy learning in dynamic decision problems is essential for providing\nstrong evidence that a new policy is better than the one in use. But how can we\nprove superiority without testing the new policy? To answer this question, we\nintroduce the G-SCOPE algorithm that evaluates a new policy based on data\ngenerated by the existing policy. Our algorithm is both computationally and\nsample efficient because it greedily learns to exploit factored structure in\nthe dynamics of the environment. We present a finite sample analysis of our\napproach and show through experiments that the algorithm scales well on\nhigh-dimensional problems with few samples.\n", "versions": [{"version": "v1", "created": "Wed, 11 Feb 2015 10:42:40 GMT"}], "update_date": "2015-02-12", "authors_parsed": [["Hallak", "Assaf", ""], ["Schnitzler", "Fran\u00e7ois", ""], ["Mann", "Timothy", ""], ["Mannor", "Shie", ""]]}, {"id": "1502.03365", "submitter": "Jiaming Xu", "authors": "Marc Lelarge and Laurent Massouli\\'e and Jiaming Xu", "title": "Reconstruction in the Labeled Stochastic Block Model", "comments": "A preliminary version of this paper appeared in the Proceedings of\n  the 2013 Information Theory Workshop", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The labeled stochastic block model is a random graph model representing\nnetworks with community structure and interactions of multiple types. In its\nsimplest form, it consists of two communities of approximately equal size, and\nthe edges are drawn and labeled at random with probability depending on whether\ntheir two endpoints belong to the same community or not.\n  It has been conjectured in \\cite{Heimlicher12} that correlated reconstruction\n(i.e.\\ identification of a partition correlated with the true partition into\nthe underlying communities) would be feasible if and only if a model parameter\nexceeds a threshold. We prove one half of this conjecture, i.e., reconstruction\nis impossible when below the threshold. In the positive direction, we introduce\na weighted graph to exploit the label information. With a suitable choice of\nweight function, we show that when above the threshold by a specific constant,\nreconstruction is achieved by (1) minimum bisection, (2) a semidefinite\nrelaxation of minimum bisection, and (3) a spectral method combined with\nremoval of edges incident to vertices of high degree. Furthermore, we show that\nhypothesis testing between the labeled stochastic block model and the labeled\nErd\\H{o}s-R\\'enyi random graph model exhibits a phase transition at the\nconjectured reconstruction threshold.\n", "versions": [{"version": "v1", "created": "Wed, 11 Feb 2015 16:33:19 GMT"}], "update_date": "2015-02-12", "authors_parsed": [["Lelarge", "Marc", ""], ["Massouli\u00e9", "Laurent", ""], ["Xu", "Jiaming", ""]]}, {"id": "1502.03391", "submitter": "Vince Lyzinski", "authors": "Vince Lyzinski, Youngser Park, Carey E. Priebe, Michael W. Trosset", "title": "Fast Embedding for JOFC Using the Raw Stress Criterion", "comments": "43 pages, 10 figures, 3 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The Joint Optimization of Fidelity and Commensurability (JOFC) manifold\nmatching methodology embeds an omnibus dissimilarity matrix consisting of\nmultiple dissimilarities on the same set of objects. One approach to this\nembedding optimizes the preservation of fidelity to each individual\ndissimilarity matrix together with commensurability of each given observation\nacross modalities via iterative majorization of a raw stress error criterion by\nsuccessive Guttman transforms. In this paper, we exploit the special structure\ninherent to JOFC to exactly and efficiently compute the successive Guttman\ntransforms, and as a result we are able to greatly speed up the JOFC procedure\nfor both in-sample and out-of-sample embedding. We demonstrate the scalability\nof our implementation on both real and simulated data examples.\n", "versions": [{"version": "v1", "created": "Wed, 11 Feb 2015 17:49:00 GMT"}, {"version": "v2", "created": "Mon, 19 Oct 2015 05:09:38 GMT"}, {"version": "v3", "created": "Mon, 31 Oct 2016 18:28:11 GMT"}], "update_date": "2016-11-01", "authors_parsed": [["Lyzinski", "Vince", ""], ["Park", "Youngser", ""], ["Priebe", "Carey E.", ""], ["Trosset", "Michael W.", ""]]}, {"id": "1502.03465", "submitter": "Javier Movellan", "authors": "Javier R. Movellan", "title": "Variable and Fixed Interval Exponential Smoothing", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML math.OC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Exponential smoothers are a simple and memory efficient way to compute\nrunning averages of time series. Here we define and describe practical\nproperties of exponential smoothers for signals observed at constant and\nvariable intervals.\n", "versions": [{"version": "v1", "created": "Wed, 11 Feb 2015 21:49:26 GMT"}], "update_date": "2015-02-13", "authors_parsed": [["Movellan", "Javier R.", ""]]}, {"id": "1502.03466", "submitter": "Alexander Vandenberg-Rodes", "authors": "Alexander Vandenberg-Rodes and Babak Shahbaba", "title": "Dependent Mat\\'ern Processes for Multivariate Time Series", "comments": "10 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  For the challenging task of modeling multivariate time series, we propose a\nnew class of models that use dependent Mat\\'ern processes to capture the\nunderlying structure of data, explain their interdependencies, and predict\ntheir unknown values. Although similar models have been proposed in the\neconometric, statistics, and machine learning literature, our approach has\nseveral advantages that distinguish it from existing methods: 1) it is flexible\nto provide high prediction accuracy, yet its complexity is controlled to avoid\noverfitting; 2) its interpretability separates it from black-box methods; 3)\nfinally, its computational efficiency makes it scalable for high-dimensional\ntime series. In this paper, we use several simulated and real data sets to\nillustrate these advantages. We will also briefly discuss some extensions of\nour model.\n", "versions": [{"version": "v1", "created": "Wed, 11 Feb 2015 21:56:13 GMT"}], "update_date": "2015-02-13", "authors_parsed": [["Vandenberg-Rodes", "Alexander", ""], ["Shahbaba", "Babak", ""]]}, {"id": "1502.03473", "submitter": "Shuai Li", "authors": "Shuai Li and Alexandros Karatzoglou and Claudio Gentile", "title": "Collaborative Filtering Bandits", "comments": "The 39th SIGIR (SIGIR 2016)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Classical collaborative filtering, and content-based filtering methods try to\nlearn a static recommendation model given training data. These approaches are\nfar from ideal in highly dynamic recommendation domains such as news\nrecommendation and computational advertisement, where the set of items and\nusers is very fluid. In this work, we investigate an adaptive clustering\ntechnique for content recommendation based on exploration-exploitation\nstrategies in contextual multi-armed bandit settings. Our algorithm takes into\naccount the collaborative effects that arise due to the interaction of the\nusers with the items, by dynamically grouping users based on the items under\nconsideration and, at the same time, grouping items based on the similarity of\nthe clusterings induced over the users. The resulting algorithm thus takes\nadvantage of preference patterns in the data in a way akin to collaborative\nfiltering methods. We provide an empirical analysis on medium-size real-world\ndatasets, showing scalability and increased prediction performance (as measured\nby click-through rate) over state-of-the-art methods for clustering bandits. We\nalso provide a regret analysis within a standard linear stochastic noise\nsetting.\n", "versions": [{"version": "v1", "created": "Wed, 11 Feb 2015 22:28:14 GMT"}, {"version": "v2", "created": "Tue, 17 Mar 2015 17:51:41 GMT"}, {"version": "v3", "created": "Thu, 7 May 2015 17:03:39 GMT"}, {"version": "v4", "created": "Thu, 24 Dec 2015 17:24:07 GMT"}, {"version": "v5", "created": "Wed, 30 Mar 2016 10:29:12 GMT"}, {"version": "v6", "created": "Wed, 11 May 2016 15:17:30 GMT"}, {"version": "v7", "created": "Tue, 31 May 2016 18:47:03 GMT"}], "update_date": "2016-06-01", "authors_parsed": [["Li", "Shuai", ""], ["Karatzoglou", "Alexandros", ""], ["Gentile", "Claudio", ""]]}, {"id": "1502.03475", "submitter": "M. Sadegh Talebi", "authors": "Richard Combes and M. Sadegh Talebi and Alexandre Proutiere and Marc\n  Lelarge", "title": "Combinatorial Bandits Revisited", "comments": "30 pages, Advances in Neural Information Processing Systems 28 (NIPS\n  2015)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG math.OC stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper investigates stochastic and adversarial combinatorial multi-armed\nbandit problems. In the stochastic setting under semi-bandit feedback, we\nderive a problem-specific regret lower bound, and discuss its scaling with the\ndimension of the decision space. We propose ESCB, an algorithm that efficiently\nexploits the structure of the problem and provide a finite-time analysis of its\nregret. ESCB has better performance guarantees than existing algorithms, and\nsignificantly outperforms these algorithms in practice. In the adversarial\nsetting under bandit feedback, we propose \\textsc{CombEXP}, an algorithm with\nthe same regret scaling as state-of-the-art algorithms, but with lower\ncomputational complexity for some combinatorial problems.\n", "versions": [{"version": "v1", "created": "Wed, 11 Feb 2015 22:35:50 GMT"}, {"version": "v2", "created": "Tue, 7 Apr 2015 15:07:54 GMT"}, {"version": "v3", "created": "Fri, 6 Nov 2015 00:53:37 GMT"}], "update_date": "2015-11-09", "authors_parsed": [["Combes", "Richard", ""], ["Talebi", "M. Sadegh", ""], ["Proutiere", "Alexandre", ""], ["Lelarge", "Marc", ""]]}, {"id": "1502.03491", "submitter": "Allen Lavoie", "authors": "Mithun Chakraborty, Sanmay Das, Allen Lavoie", "title": "How to show a probabilistic model is better", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a simple theoretical framework, and corresponding practical\nprocedures, for comparing probabilistic models on real data in a traditional\nmachine learning setting. This framework is based on the theory of proper\nscoring rules, but requires only basic algebra and probability theory to\nunderstand and verify. The theoretical concepts presented are well-studied,\nprimarily in the statistics literature. The goal of this paper is to advocate\ntheir wider adoption for performance evaluation in empirical machine learning.\n", "versions": [{"version": "v1", "created": "Wed, 11 Feb 2015 23:44:02 GMT"}], "update_date": "2015-02-13", "authors_parsed": [["Chakraborty", "Mithun", ""], ["Das", "Sanmay", ""], ["Lavoie", "Allen", ""]]}, {"id": "1502.03492", "submitter": "David Duvenaud", "authors": "Dougal Maclaurin, David Duvenaud, Ryan P. Adams", "title": "Gradient-based Hyperparameter Optimization through Reversible Learning", "comments": "10 figures. Submitted to ICML", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Tuning hyperparameters of learning algorithms is hard because gradients are\nusually unavailable. We compute exact gradients of cross-validation performance\nwith respect to all hyperparameters by chaining derivatives backwards through\nthe entire training procedure. These gradients allow us to optimize thousands\nof hyperparameters, including step-size and momentum schedules, weight\ninitialization distributions, richly parameterized regularization schemes, and\nneural network architectures. We compute hyperparameter gradients by exactly\nreversing the dynamics of stochastic gradient descent with momentum.\n", "versions": [{"version": "v1", "created": "Wed, 11 Feb 2015 23:52:36 GMT"}, {"version": "v2", "created": "Fri, 13 Feb 2015 19:26:39 GMT"}, {"version": "v3", "created": "Thu, 2 Apr 2015 17:40:44 GMT"}], "update_date": "2015-04-03", "authors_parsed": [["Maclaurin", "Dougal", ""], ["Duvenaud", "David", ""], ["Adams", "Ryan P.", ""]]}, {"id": "1502.03496", "submitter": "Yu Cheng", "authors": "Dehua Cheng, Yu Cheng, Yan Liu, Richard Peng, Shang-Hua Teng", "title": "Spectral Sparsification of Random-Walk Matrix Polynomials", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.DM cs.LG cs.SI stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider a fundamental algorithmic question in spectral graph theory:\nCompute a spectral sparsifier of random-walk matrix-polynomial\n$$L_\\alpha(G)=D-\\sum_{r=1}^d\\alpha_rD(D^{-1}A)^r$$ where $A$ is the adjacency\nmatrix of a weighted, undirected graph, $D$ is the diagonal matrix of weighted\ndegrees, and $\\alpha=(\\alpha_1...\\alpha_d)$ are nonnegative coefficients with\n$\\sum_{r=1}^d\\alpha_r=1$. Recall that $D^{-1}A$ is the transition matrix of\nrandom walks on the graph. The sparsification of $L_\\alpha(G)$ appears to be\nalgorithmically challenging as the matrix power $(D^{-1}A)^r$ is defined by all\npaths of length $r$, whose precise calculation would be prohibitively\nexpensive.\n  In this paper, we develop the first nearly linear time algorithm for this\nsparsification problem: For any $G$ with $n$ vertices and $m$ edges, $d$\ncoefficients $\\alpha$, and $\\epsilon > 0$, our algorithm runs in time\n$O(d^2m\\log^2n/\\epsilon^{2})$ to construct a Laplacian matrix\n$\\tilde{L}=D-\\tilde{A}$ with $O(n\\log n/\\epsilon^{2})$ non-zeros such that\n$\\tilde{L}\\approx_{\\epsilon}L_\\alpha(G)$.\n  Matrix polynomials arise in mathematical analysis of matrix functions as well\nas numerical solutions of matrix equations. Our work is particularly motivated\nby the algorithmic problems for speeding up the classic Newton's method in\napplications such as computing the inverse square-root of the precision matrix\nof a Gaussian random field, as well as computing the $q$th-root transition (for\n$q\\geq1$) in a time-reversible Markov model. The key algorithmic step for both\napplications is the construction of a spectral sparsifier of a constant degree\nrandom-walk matrix-polynomials introduced by Newton's method. Our algorithm can\nalso be used to build efficient data structures for effective resistances for\nmulti-step time-reversible Markov models, and we anticipate that it could be\nuseful for other tasks in network analysis.\n", "versions": [{"version": "v1", "created": "Thu, 12 Feb 2015 00:25:32 GMT"}], "update_date": "2015-02-13", "authors_parsed": [["Cheng", "Dehua", ""], ["Cheng", "Yu", ""], ["Liu", "Yan", ""], ["Peng", "Richard", ""], ["Teng", "Shang-Hua", ""]]}, {"id": "1502.03509", "submitter": "Iain Murray", "authors": "Mathieu Germain, Karol Gregor, Iain Murray, Hugo Larochelle", "title": "MADE: Masked Autoencoder for Distribution Estimation", "comments": "9 pages and 1 page of supplementary material. Updated to match\n  published version", "journal-ref": "Proceedings of the 32nd International Conference on Machine\n  Learning, JMLR W&CP 37:881-889, 2015", "doi": null, "report-no": null, "categories": "cs.LG cs.NE stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  There has been a lot of recent interest in designing neural network models to\nestimate a distribution from a set of examples. We introduce a simple\nmodification for autoencoder neural networks that yields powerful generative\nmodels. Our method masks the autoencoder's parameters to respect autoregressive\nconstraints: each input is reconstructed only from previous inputs in a given\nordering. Constrained this way, the autoencoder outputs can be interpreted as a\nset of conditional probabilities, and their product, the full joint\nprobability. We can also train a single network that can decompose the joint\nprobability in multiple different orderings. Our simple framework can be\napplied to multiple architectures, including deep ones. Vectorized\nimplementations, such as on GPUs, are simple and fast. Experiments demonstrate\nthat this approach is competitive with state-of-the-art tractable distribution\nestimators. At test time, the method is significantly faster and scales better\nthan other autoregressive estimators.\n", "versions": [{"version": "v1", "created": "Thu, 12 Feb 2015 02:06:07 GMT"}, {"version": "v2", "created": "Fri, 5 Jun 2015 14:37:32 GMT"}], "update_date": "2015-06-08", "authors_parsed": [["Germain", "Mathieu", ""], ["Gregor", "Karol", ""], ["Murray", "Iain", ""], ["Larochelle", "Hugo", ""]]}, {"id": "1502.03520", "submitter": "Yingyu Liang", "authors": "Sanjeev Arora, Yuanzhi Li, Yingyu Liang, Tengyu Ma, Andrej Risteski", "title": "A Latent Variable Model Approach to PMI-based Word Embeddings", "comments": "Appear in Transactions of the Association for Computational\n  Linguistics (TACL), 2016", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CL stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Semantic word embeddings represent the meaning of a word via a vector, and\nare created by diverse methods. Many use nonlinear operations on co-occurrence\nstatistics, and have hand-tuned hyperparameters and reweighting methods.\n  This paper proposes a new generative model, a dynamic version of the\nlog-linear topic model of~\\citet{mnih2007three}. The methodological novelty is\nto use the prior to compute closed form expressions for word statistics. This\nprovides a theoretical justification for nonlinear models like PMI, word2vec,\nand GloVe, as well as some hyperparameter choices. It also helps explain why\nlow-dimensional semantic embeddings contain linear algebraic structure that\nallows solution of word analogies, as shown by~\\citet{mikolov2013efficient} and\nmany subsequent papers.\n  Experimental support is provided for the generative model assumptions, the\nmost important of which is that latent word vectors are fairly uniformly\ndispersed in space.\n", "versions": [{"version": "v1", "created": "Thu, 12 Feb 2015 02:50:08 GMT"}, {"version": "v2", "created": "Tue, 3 Mar 2015 00:49:42 GMT"}, {"version": "v3", "created": "Tue, 21 Apr 2015 21:14:43 GMT"}, {"version": "v4", "created": "Wed, 22 Jul 2015 23:51:24 GMT"}, {"version": "v5", "created": "Wed, 14 Oct 2015 04:27:00 GMT"}, {"version": "v6", "created": "Wed, 24 Feb 2016 01:28:03 GMT"}, {"version": "v7", "created": "Fri, 22 Jul 2016 20:09:25 GMT"}, {"version": "v8", "created": "Wed, 19 Jun 2019 21:54:20 GMT"}], "update_date": "2019-06-21", "authors_parsed": [["Arora", "Sanjeev", ""], ["Li", "Yuanzhi", ""], ["Liang", "Yingyu", ""], ["Ma", "Tengyu", ""], ["Risteski", "Andrej", ""]]}, {"id": "1502.03536", "submitter": "Vamsi Ithapu", "authors": "Chris Hinrichs, Vamsi K Ithapu, Qinyuan Sun, Sterling C Johnson, Vikas\n  Singh", "title": "Speeding up Permutation Testing in Neuroimaging", "comments": "NIPS 13", "journal-ref": "Advances in neural information processing systems (2013), pp.\n  890-898", "doi": null, "report-no": null, "categories": "stat.CO cs.AI stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Multiple hypothesis testing is a significant problem in nearly all\nneuroimaging studies. In order to correct for this phenomena, we require a\nreliable estimate of the Family-Wise Error Rate (FWER). The well known\nBonferroni correction method, while simple to implement, is quite conservative,\nand can substantially under-power a study because it ignores dependencies\nbetween test statistics. Permutation testing, on the other hand, is an exact,\nnon-parametric method of estimating the FWER for a given $\\alpha$-threshold,\nbut for acceptably low thresholds the computational burden can be prohibitive.\nIn this paper, we show that permutation testing in fact amounts to populating\nthe columns of a very large matrix ${\\bf P}$. By analyzing the spectrum of this\nmatrix, under certain conditions, we see that ${\\bf P}$ has a low-rank plus a\nlow-variance residual decomposition which makes it suitable for highly\nsub--sampled --- on the order of $0.5\\%$ --- matrix completion methods. Based\non this observation, we propose a novel permutation testing methodology which\noffers a large speedup, without sacrificing the fidelity of the estimated FWER.\nOur evaluations on four different neuroimaging datasets show that a\ncomputational speedup factor of roughly $50\\times$ can be achieved while\nrecovering the FWER distribution up to very high accuracy. Further, we show\nthat the estimated $\\alpha$-threshold is also recovered faithfully, and is\nstable.\n", "versions": [{"version": "v1", "created": "Thu, 12 Feb 2015 04:30:06 GMT"}], "update_date": "2015-02-17", "authors_parsed": [["Hinrichs", "Chris", ""], ["Ithapu", "Vamsi K", ""], ["Sun", "Qinyuan", ""], ["Johnson", "Sterling C", ""], ["Singh", "Vikas", ""]]}, {"id": "1502.03571", "submitter": "Jiyan Yang", "authors": "Jiyan Yang, Yin-Lam Chow, Christopher R\\'e, Michael W. Mahoney", "title": "Weighted SGD for $\\ell_p$ Regression with Randomized Preconditioning", "comments": "A conference version of this paper appears under the same title in\n  Proceedings of ACM-SIAM Symposium on Discrete Algorithms, Arlington, VA, 2016", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.OC stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In recent years, stochastic gradient descent (SGD) methods and randomized\nlinear algebra (RLA) algorithms have been applied to many large-scale problems\nin machine learning and data analysis. We aim to bridge the gap between these\ntwo methods in solving constrained overdetermined linear regression\nproblems---e.g., $\\ell_2$ and $\\ell_1$ regression problems. We propose a hybrid\nalgorithm named pwSGD that uses RLA techniques for preconditioning and\nconstructing an importance sampling distribution, and then performs an SGD-like\niterative process with weighted sampling on the preconditioned system. We prove\nthat pwSGD inherits faster convergence rates that only depend on the lower\ndimension of the linear system, while maintaining low computation complexity.\nParticularly, when solving $\\ell_1$ regression with size $n$ by $d$, pwSGD\nreturns an approximate solution with $\\epsilon$ relative error in the objective\nvalue in $\\mathcal{O}(\\log n \\cdot \\text{nnz}(A) + \\text{poly}(d)/\\epsilon^2)$\ntime. This complexity is uniformly better than that of RLA methods in terms of\nboth $\\epsilon$ and $d$ when the problem is unconstrained. For $\\ell_2$\nregression, pwSGD returns an approximate solution with $\\epsilon$ relative\nerror in the objective value and the solution vector measured in prediction\nnorm in $\\mathcal{O}(\\log n \\cdot \\text{nnz}(A) + \\text{poly}(d)\n\\log(1/\\epsilon) /\\epsilon)$ time. We also provide lower bounds on the coreset\ncomplexity for more general regression problems, indicating that still new\nideas will be needed to extend similar RLA preconditioning ideas to weighted\nSGD algorithms for more general regression problems. Finally, the effectiveness\nof such algorithms is illustrated numerically on both synthetic and real\ndatasets.\n", "versions": [{"version": "v1", "created": "Thu, 12 Feb 2015 09:11:58 GMT"}, {"version": "v2", "created": "Mon, 27 Jul 2015 08:17:08 GMT"}, {"version": "v3", "created": "Fri, 6 May 2016 05:25:27 GMT"}, {"version": "v4", "created": "Sun, 31 Jul 2016 07:59:39 GMT"}, {"version": "v5", "created": "Mon, 10 Jul 2017 05:46:01 GMT"}], "update_date": "2017-07-11", "authors_parsed": [["Yang", "Jiyan", ""], ["Chow", "Yin-Lam", ""], ["R\u00e9", "Christopher", ""], ["Mahoney", "Michael W.", ""]]}, {"id": "1502.03655", "submitter": "Johan Dahlin Mr.", "authors": "Manon Kok, Johan Dahlin, Thomas B. Sch\\\"on, Adrian Wills", "title": "Newton-based maximum likelihood estimation in nonlinear state space\n  models", "comments": "17 pages, 2 figures. Accepted for the 17th IFAC Symposium on System\n  Identification (SYSID), Beijing, China, October 2015", "journal-ref": null, "doi": "10.1016/j.ifacol.2015.12.160", "report-no": null, "categories": "stat.CO stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Maximum likelihood (ML) estimation using Newton's method in nonlinear state\nspace models (SSMs) is a challenging problem due to the analytical\nintractability of the log-likelihood and its gradient and Hessian. We estimate\nthe gradient and Hessian using Fisher's identity in combination with a\nsmoothing algorithm. We explore two approximations of the log-likelihood and of\nthe solution of the smoothing problem. The first is a linearization\napproximation which is computationally cheap, but the accuracy typically varies\nbetween models. The second is a sampling approximation which is asymptotically\nvalid for any SSM but is more computationally costly. We demonstrate our\napproach for ML parameter estimation on simulated data from two different SSMs\nwith encouraging results.\n", "versions": [{"version": "v1", "created": "Thu, 12 Feb 2015 13:47:23 GMT"}, {"version": "v2", "created": "Fri, 11 Sep 2015 12:05:38 GMT"}], "update_date": "2016-03-11", "authors_parsed": [["Kok", "Manon", ""], ["Dahlin", "Johan", ""], ["Sch\u00f6n", "Thomas B.", ""], ["Wills", "Adrian", ""]]}, {"id": "1502.03656", "submitter": "Johan Dahlin Mr.", "authors": "Johan Dahlin, Fredrik Lindsten, Thomas B. Sch\\\"on", "title": "Quasi-Newton particle Metropolis-Hastings", "comments": "23 pages, 5 figures. Accepted for the 17th IFAC Symposium on System\n  Identification (SYSID), Beijing, China, October 2015", "journal-ref": null, "doi": "10.1016/j.ifacol.2015.12.258", "report-no": null, "categories": "stat.CO q-fin.CP stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Particle Metropolis-Hastings enables Bayesian parameter inference in general\nnonlinear state space models (SSMs). However, in many implementations a random\nwalk proposal is used and this can result in poor mixing if not tuned correctly\nusing tedious pilot runs. Therefore, we consider a new proposal inspired by\nquasi-Newton algorithms that may achieve similar (or better) mixing with less\ntuning. An advantage compared to other Hessian based proposals, is that it only\nrequires estimates of the gradient of the log-posterior. A possible application\nis parameter inference in the challenging class of SSMs with intractable\nlikelihoods. We exemplify this application and the benefits of the new proposal\nby modelling log-returns of future contracts on coffee by a stochastic\nvolatility model with $\\alpha$-stable observations.\n", "versions": [{"version": "v1", "created": "Thu, 12 Feb 2015 13:48:01 GMT"}, {"version": "v2", "created": "Wed, 2 Sep 2015 14:10:59 GMT"}], "update_date": "2016-03-11", "authors_parsed": [["Dahlin", "Johan", ""], ["Lindsten", "Fredrik", ""], ["Sch\u00f6n", "Thomas B.", ""]]}, {"id": "1502.03696", "submitter": "Andreas Hula", "authors": "Andreas Hula and P. Read Montague and Peter Dayan", "title": "Monte Carlo Planning method estimates planning horizons during\n  interactive social exchange", "comments": null, "journal-ref": null, "doi": "10.1371/journal.pcbi.1004254", "report-no": null, "categories": "stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Reciprocating interactions represent a central feature of all human\nexchanges. They have been the target of various recent experiments, with\nhealthy participants and psychiatric populations engaging as dyads in\nmulti-round exchanges such as a repeated trust task. Behaviour in such\nexchanges involves complexities related to each agent's preference for equity\nwith their partner, beliefs about the partner's appetite for equity, beliefs\nabout the partner's model of their partner, and so on. Agents may also plan\ndifferent numbers of steps into the future. Providing a computationally precise\naccount of the behaviour is an essential step towards understanding what\nunderlies choices. A natural framework for this is that of an interactive\npartially observable Markov decision process (IPOMDP). However, the various\ncomplexities make IPOMDPs inordinately computationally challenging. Here, we\nshow how to approximate the solution for the multi-round trust task using a\nvariant of the Monte-Carlo tree search algorithm. We demonstrate that the\nalgorithm is efficient and effective, and therefore can be used to invert\nobservations of behavioural choices. We use generated behaviour to elucidate\nthe richness and sophistication of interactive inference.\n", "versions": [{"version": "v1", "created": "Thu, 12 Feb 2015 15:18:43 GMT"}, {"version": "v2", "created": "Tue, 10 Mar 2015 14:58:32 GMT"}, {"version": "v3", "created": "Wed, 18 Mar 2015 17:09:52 GMT"}, {"version": "v4", "created": "Thu, 19 Mar 2015 09:44:51 GMT"}, {"version": "v5", "created": "Tue, 7 Apr 2015 09:34:08 GMT"}, {"version": "v6", "created": "Mon, 13 Apr 2015 12:17:43 GMT"}, {"version": "v7", "created": "Tue, 26 May 2015 09:39:56 GMT"}], "update_date": "2015-06-23", "authors_parsed": [["Hula", "Andreas", ""], ["Montague", "P. Read", ""], ["Dayan", "Peter", ""]]}, {"id": "1502.03919", "submitter": "Aviv Tamar", "authors": "Aviv Tamar, Yinlam Chow, Mohammad Ghavamzadeh, Shie Mannor", "title": "Policy Gradient for Coherent Risk Measures", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Several authors have recently developed risk-sensitive policy gradient\nmethods that augment the standard expected cost minimization problem with a\nmeasure of variability in cost. These studies have focused on specific\nrisk-measures, such as the variance or conditional value at risk (CVaR). In\nthis work, we extend the policy gradient method to the whole class of coherent\nrisk measures, which is widely accepted in finance and operations research,\namong other fields. We consider both static and time-consistent dynamic risk\nmeasures. For static risk measures, our approach is in the spirit of policy\ngradient algorithms and combines a standard sampling approach with convex\nprogramming. For dynamic risk measures, our approach is actor-critic style and\ninvolves explicit approximation of value function. Most importantly, our\ncontribution presents a unified approach to risk-sensitive reinforcement\nlearning that generalizes and extends previous results.\n", "versions": [{"version": "v1", "created": "Fri, 13 Feb 2015 09:16:24 GMT"}, {"version": "v2", "created": "Mon, 8 Jun 2015 06:31:42 GMT"}], "update_date": "2015-06-09", "authors_parsed": [["Tamar", "Aviv", ""], ["Chow", "Yinlam", ""], ["Ghavamzadeh", "Mohammad", ""], ["Mannor", "Shie", ""]]}, {"id": "1502.03939", "submitter": "Bruno Sudret", "authors": "R. Schoebi, B. Sudret, J. Wiart", "title": "Polynomial-Chaos-based Kriging", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.CO stat.ME stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Computer simulation has become the standard tool in many engineering fields\nfor designing and optimizing systems, as well as for assessing their\nreliability. To cope with demanding analysis such as optimization and\nreliability, surrogate models (a.k.a meta-models) have been increasingly\ninvestigated in the last decade. Polynomial Chaos Expansions (PCE) and Kriging\nare two popular non-intrusive meta-modelling techniques. PCE surrogates the\ncomputational model with a series of orthonormal polynomials in the input\nvariables where polynomials are chosen in coherency with the probability\ndistributions of those input variables. On the other hand, Kriging assumes that\nthe computer model behaves as a realization of a Gaussian random process whose\nparameters are estimated from the available computer runs, i.e. input vectors\nand response values. These two techniques have been developed more or less in\nparallel so far with little interaction between the researchers in the two\nfields. In this paper, PC-Kriging is derived as a new non-intrusive\nmeta-modeling approach combining PCE and Kriging. A sparse set of orthonormal\npolynomials (PCE) approximates the global behavior of the computational model\nwhereas Kriging manages the local variability of the model output. An adaptive\nalgorithm similar to the least angle regression algorithm determines the\noptimal sparse set of polynomials. PC-Kriging is validated on various benchmark\nanalytical functions which are easy to sample for reference results. From the\nnumerical investigations it is concluded that PC-Kriging performs better than\nor at least as good as the two distinct meta-modeling techniques. A larger gain\nin accuracy is obtained when the experimental design has a limited size, which\nis an asset when dealing with demanding computational models.\n", "versions": [{"version": "v1", "created": "Fri, 13 Feb 2015 10:53:52 GMT"}], "update_date": "2015-02-16", "authors_parsed": [["Schoebi", "R.", ""], ["Sudret", "B.", ""], ["Wiart", "J.", ""]]}, {"id": "1502.04033", "submitter": "Tobias Reitmaier", "authors": "Tobias Reitmaier and Bernhard Sick", "title": "The Responsibility Weighted Mahalanobis Kernel for Semi-Supervised\n  Training of Support Vector Machines for Classification", "comments": null, "journal-ref": null, "doi": "10.1016/j.ins.2015.06.027", "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Kernel functions in support vector machines (SVM) are needed to assess the\nsimilarity of input samples in order to classify these samples, for instance.\nBesides standard kernels such as Gaussian (i.e., radial basis function, RBF) or\npolynomial kernels, there are also specific kernels tailored to consider\nstructure in the data for similarity assessment. In this article, we will\ncapture structure in data by means of probabilistic mixture density models, for\nexample Gaussian mixtures in the case of real-valued input spaces. From the\ndistance measures that are inherently contained in these models, e.g.,\nMahalanobis distances in the case of Gaussian mixtures, we derive a new kernel,\nthe responsibility weighted Mahalanobis (RWM) kernel. Basically, this kernel\nemphasizes the influence of model components from which any two samples that\nare compared are assumed to originate (that is, the \"responsible\" model\ncomponents). We will see that this kernel outperforms the RBF kernel and other\nkernels capturing structure in data (such as the LAP kernel in Laplacian SVM)\nin many applications where partially labeled data are available, i.e., for\nsemi-supervised training of SVM. Other key advantages are that the RWM kernel\ncan easily be used with standard SVM implementations and training algorithms\nsuch as sequential minimal optimization, and heuristics known for the\nparametrization of RBF kernels in a C-SVM can easily be transferred to this new\nkernel. Properties of the RWM kernel are demonstrated with 20 benchmark data\nsets and an increasing percentage of labeled samples in the training data.\n", "versions": [{"version": "v1", "created": "Fri, 13 Feb 2015 15:48:00 GMT"}, {"version": "v2", "created": "Mon, 16 Feb 2015 13:02:05 GMT"}], "update_date": "2015-07-03", "authors_parsed": [["Reitmaier", "Tobias", ""], ["Sick", "Bernhard", ""]]}, {"id": "1502.04081", "submitter": "David Belanger", "authors": "David Belanger and Sham Kakade", "title": "A Linear Dynamical System Model for Text", "comments": "Accepted at International Conference of Machine Learning 2015", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.CL cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Low dimensional representations of words allow accurate NLP models to be\ntrained on limited annotated data. While most representations ignore words'\nlocal context, a natural way to induce context-dependent representations is to\nperform inference in a probabilistic latent-variable sequence model. Given the\nrecent success of continuous vector space word representations, we provide such\nan inference procedure for continuous states, where words' representations are\ngiven by the posterior mean of a linear dynamical system. Here, efficient\ninference can be performed using Kalman filtering. Our learning algorithm is\nextremely scalable, operating on simple cooccurrence counts for both parameter\ninitialization using the method of moments and subsequent iterations of EM. In\nour experiments, we employ our inferred word embeddings as features in standard\ntagging tasks, obtaining significant accuracy improvements. Finally, the Kalman\nfilter updates can be seen as a linear recurrent neural network. We demonstrate\nthat using the parameters of our model to initialize a non-linear recurrent\nneural network language model reduces its training time by a day and yields\nlower perplexity.\n", "versions": [{"version": "v1", "created": "Fri, 13 Feb 2015 18:39:29 GMT"}, {"version": "v2", "created": "Sun, 31 May 2015 20:04:53 GMT"}], "update_date": "2015-06-02", "authors_parsed": [["Belanger", "David", ""], ["Kakade", "Sham", ""]]}, {"id": "1502.04148", "submitter": "James Voss", "authors": "James Voss, Mikhail Belkin, and Luis Rademacher", "title": "A Pseudo-Euclidean Iteration for Optimal Recovery in Noisy ICA", "comments": "17 pages, 2 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Independent Component Analysis (ICA) is a popular model for blind signal\nseparation. The ICA model assumes that a number of independent source signals\nare linearly mixed to form the observed signals. We propose a new algorithm,\nPEGI (for pseudo-Euclidean Gradient Iteration), for provable model recovery for\nICA with Gaussian noise. The main technical innovation of the algorithm is to\nuse a fixed point iteration in a pseudo-Euclidean (indefinite \"inner product\")\nspace. The use of this indefinite \"inner product\" resolves technical issues\ncommon to several existing algorithms for noisy ICA. This leads to an algorithm\nwhich is conceptually simple, efficient and accurate in testing.\n  Our second contribution is combining PEGI with the analysis of objectives for\noptimal recovery in the noisy ICA model. It has been observed that the direct\napproach of demixing with the inverse of the mixing matrix is suboptimal for\nsignal recovery in terms of the natural Signal to Interference plus Noise Ratio\n(SINR) criterion. There have been several partial solutions proposed in the ICA\nliterature. It turns out that any solution to the mixing matrix reconstruction\nproblem can be used to construct an SINR-optimal ICA demixing, despite the fact\nthat SINR itself cannot be computed from data. That allows us to obtain a\npractical and provably SINR-optimal recovery method for ICA with arbitrary\nGaussian noise.\n", "versions": [{"version": "v1", "created": "Fri, 13 Feb 2015 23:18:35 GMT"}, {"version": "v2", "created": "Thu, 1 Oct 2015 16:05:56 GMT"}], "update_date": "2015-10-02", "authors_parsed": [["Voss", "James", ""], ["Belkin", "Mikhail", ""], ["Rademacher", "Luis", ""]]}, {"id": "1502.04168", "submitter": "Shaobo Lin", "authors": "Shaobo Lin", "title": "Nonparametric regression using needlet kernels for spherical data", "comments": "21 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Needlets have been recognized as state-of-the-art tools to tackle spherical\ndata, due to their excellent localization properties in both spacial and\nfrequency domains.\n  This paper considers developing kernel methods associated with the needlet\nkernel for nonparametric regression problems whose predictor variables are\ndefined on a sphere. Due to the localization property in the frequency domain,\nwe prove that the regularization parameter of the kernel ridge regression\nassociated with the needlet kernel can decrease arbitrarily fast. A natural\nconsequence is that the regularization term for the kernel ridge regression is\nnot necessary in the sense of rate optimality. Based on the excellent\nlocalization property in the spacial domain further, we also prove that all the\n$l^{q}$ $(01\\leq q < \\infty)$ kernel regularization estimates associated with\nthe needlet kernel, including the kernel lasso estimate and the kernel bridge\nestimate, possess almost the same generalization capability for a large range\nof regularization parameters in the sense of rate optimality.\n  This finding tentatively reveals that, if the needlet kernel is utilized,\nthen the choice of $q$ might not have a strong impact in terms of the\ngeneralization capability in some modeling contexts. From this perspective, $q$\ncan be arbitrarily specified, or specified merely by other no generalization\ncriteria like smoothness, computational complexity, sparsity, etc..\n", "versions": [{"version": "v1", "created": "Sat, 14 Feb 2015 05:37:32 GMT"}, {"version": "v2", "created": "Thu, 10 Sep 2015 12:34:58 GMT"}], "update_date": "2015-09-11", "authors_parsed": [["Lin", "Shaobo", ""]]}, {"id": "1502.04269", "submitter": "Berk Ustun", "authors": "Berk Ustun and Cynthia Rudin", "title": "Supersparse Linear Integer Models for Optimized Medical Scoring Systems", "comments": "This version reflects our findings on SLIM as of January 2016\n  (arXiv:1306.5860 and arXiv:1405.4047 are out-of-date). The final published\n  version of this articled is available at http://www.springerlink.com", "journal-ref": null, "doi": "10.1007/s10994-015-5528-6", "report-no": null, "categories": "stat.ML cs.DM cs.LG stat.AP stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Scoring systems are linear classification models that only require users to\nadd, subtract and multiply a few small numbers in order to make a prediction.\nThese models are in widespread use by the medical community, but are difficult\nto learn from data because they need to be accurate and sparse, have coprime\ninteger coefficients, and satisfy multiple operational constraints. We present\na new method for creating data-driven scoring systems called a Supersparse\nLinear Integer Model (SLIM). SLIM scoring systems are built by solving an\ninteger program that directly encodes measures of accuracy (the 0-1 loss) and\nsparsity (the $\\ell_0$-seminorm) while restricting coefficients to coprime\nintegers. SLIM can seamlessly incorporate a wide range of operational\nconstraints related to accuracy and sparsity, and can produce highly tailored\nmodels without parameter tuning. We provide bounds on the testing and training\naccuracy of SLIM scoring systems, and present a new data reduction technique\nthat can improve scalability by eliminating a portion of the training data\nbeforehand. Our paper includes results from a collaboration with the\nMassachusetts General Hospital Sleep Laboratory, where SLIM was used to create\na highly tailored scoring system for sleep apnea screening\n", "versions": [{"version": "v1", "created": "Sun, 15 Feb 2015 01:26:41 GMT"}, {"version": "v2", "created": "Mon, 18 May 2015 14:46:40 GMT"}, {"version": "v3", "created": "Tue, 26 Jan 2016 17:34:21 GMT"}], "update_date": "2017-11-07", "authors_parsed": [["Ustun", "Berk", ""], ["Rudin", "Cynthia", ""]]}, {"id": "1502.04315", "submitter": "Felipe Llinares", "authors": "Felipe Llinares L\\'opez, Mahito Sugiyama, Laetitia Papaxanthos,\n  Karsten M. Borgwardt", "title": "Fast and Memory-Efficient Significant Pattern Mining via Permutation\n  Testing", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a novel algorithm, Westfall-Young light, for detecting patterns,\nsuch as itemsets and subgraphs, which are statistically significantly enriched\nin one of two classes. Our method corrects rigorously for multiple hypothesis\ntesting and correlations between patterns through the Westfall-Young\npermutation procedure, which empirically estimates the null distribution of\npattern frequencies in each class via permutations. In our experiments,\nWestfall-Young light dramatically outperforms the current state-of-the-art\napproach in terms of both runtime and memory efficiency on popular real-world\nbenchmark datasets for pattern mining. The key to this efficiency is that\nunlike all existing methods, our algorithm neither needs to solve the\nunderlying frequent itemset mining problem anew for each permutation nor needs\nto store the occurrence list of all frequent patterns. Westfall-Young light\nopens the door to significant pattern mining on large datasets that previously\nled to prohibitive runtime or memory costs.\n", "versions": [{"version": "v1", "created": "Sun, 15 Feb 2015 14:46:13 GMT"}], "update_date": "2015-02-17", "authors_parsed": [["L\u00f3pez", "Felipe Llinares", ""], ["Sugiyama", "Mahito", ""], ["Papaxanthos", "Laetitia", ""], ["Borgwardt", "Karsten M.", ""]]}, {"id": "1502.04416", "submitter": "Bohan Liu", "authors": "Bohan Liu and Ernest Fokoue", "title": "Random Subspace Learning Approach to High-Dimensional Outliers Detection", "comments": "13 pages, 18 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We introduce and develop a novel approach to outlier detection based on\nadaptation of random subspace learning. Our proposed method handles both\nhigh-dimension low-sample size and traditional low-dimensional high-sample size\ndatasets. Essentially, we avoid the computational bottleneck of techniques like\nminimum covariance determinant (MCD) by computing the needed determinants and\nassociated measures in much lower dimensional subspaces. Both theoretical and\ncomputational development of our approach reveal that it is computationally\nmore efficient than the regularized methods in high-dimensional low-sample\nsize, and often competes favorably with existing methods as far as the\npercentage of correct outlier detection is concerned.\n", "versions": [{"version": "v1", "created": "Mon, 16 Feb 2015 03:31:02 GMT"}, {"version": "v2", "created": "Mon, 23 Mar 2015 02:21:36 GMT"}, {"version": "v3", "created": "Thu, 26 Mar 2015 19:18:03 GMT"}, {"version": "v4", "created": "Tue, 28 Apr 2015 18:28:31 GMT"}, {"version": "v5", "created": "Sun, 3 May 2015 21:56:36 GMT"}], "update_date": "2015-05-05", "authors_parsed": [["Liu", "Bohan", ""], ["Fokoue", "Ernest", ""]]}, {"id": "1502.04434", "submitter": "Sergey Demyanov", "authors": "Sergey Demyanov, James Bailey, Ramamohanarao Kotagiri, Christopher\n  Leckie", "title": "Invariant backpropagation: how to train a transformation-invariant\n  neural network", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In many classification problems a classifier should be robust to small\nvariations in the input vector. This is a desired property not only for\nparticular transformations, such as translation and rotation in image\nclassification problems, but also for all others for which the change is small\nenough to retain the object perceptually indistinguishable. We propose two\nextensions of the backpropagation algorithm that train a neural network to be\nrobust to variations in the feature vector. While the first of them enforces\nrobustness of the loss function to all variations, the second method trains the\npredictions to be robust to a particular variation which changes the loss\nfunction the most. The second methods demonstrates better results, but is\nslightly slower. We analytically compare the proposed algorithm with two the\nmost similar approaches (Tangent BP and Adversarial Training), and propose\ntheir fast versions. In the experimental part we perform comparison of all\nalgorithms in terms of classification accuracy and robustness to noise on MNIST\nand CIFAR-10 datasets. Additionally we analyze how the performance of the\nproposed algorithm depends on the dataset size and data augmentation.\n", "versions": [{"version": "v1", "created": "Mon, 16 Feb 2015 06:28:35 GMT"}, {"version": "v2", "created": "Mon, 2 Nov 2015 11:44:59 GMT"}, {"version": "v3", "created": "Fri, 15 Jan 2016 04:49:00 GMT"}], "update_date": "2016-01-18", "authors_parsed": [["Demyanov", "Sergey", ""], ["Bailey", "James", ""], ["Kotagiri", "Ramamohanarao", ""], ["Leckie", "Christopher", ""]]}, {"id": "1502.04502", "submitter": "Teng Qiu", "authors": "Teng Qiu, Yongjie Li", "title": "Clustering by Descending to the Nearest Neighbor in the Delaunay Graph\n  Space", "comments": "7 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.CV cs.LG", "license": "http://creativecommons.org/licenses/by-nc-sa/3.0/", "abstract": "  In our previous works, we proposed a physically-inspired rule to organize the\ndata points into an in-tree (IT) structure, in which some undesired edges are\nallowed to occur. By removing those undesired or redundant edges, this IT\nstructure is divided into several separate parts, each representing one\ncluster. In this work, we seek to prevent the undesired edges from arising at\nthe source. Before using the physically-inspired rule, data points are at first\norganized into a proximity graph which restricts each point to select the\noptimal directed neighbor just among its neighbors. Consequently, separated\nin-trees or clusters automatically arise, without redundant edges requiring to\nbe removed.\n", "versions": [{"version": "v1", "created": "Mon, 16 Feb 2015 11:50:42 GMT"}], "update_date": "2015-02-17", "authors_parsed": [["Qiu", "Teng", ""], ["Li", "Yongjie", ""]]}, {"id": "1502.04622", "submitter": "Balaji Lakshminarayanan", "authors": "Balaji Lakshminarayanan, Daniel M. Roy and Yee Whye Teh", "title": "Particle Gibbs for Bayesian Additive Regression Trees", "comments": null, "journal-ref": "Proceedings of the 18th International Conference on Artificial\n  Intelligence and Statistics (AISTATS) 2015, San Diego, CA, USA. JMLR: W&CP\n  volume 38", "doi": null, "report-no": null, "categories": "stat.ML cs.LG stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Additive regression trees are flexible non-parametric models and popular\noff-the-shelf tools for real-world non-linear regression. In application\ndomains, such as bioinformatics, where there is also demand for probabilistic\npredictions with measures of uncertainty, the Bayesian additive regression\ntrees (BART) model, introduced by Chipman et al. (2010), is increasingly\npopular. As data sets have grown in size, however, the standard\nMetropolis-Hastings algorithms used to perform inference in BART are proving\ninadequate. In particular, these Markov chains make local changes to the trees\nand suffer from slow mixing when the data are high-dimensional or the best\nfitting trees are more than a few layers deep. We present a novel sampler for\nBART based on the Particle Gibbs (PG) algorithm (Andrieu et al., 2010) and a\ntop-down particle filtering algorithm for Bayesian decision trees\n(Lakshminarayanan et al., 2013). Rather than making local changes to individual\ntrees, the PG sampler proposes a complete tree to fit the residual. Experiments\nshow that the PG sampler outperforms existing samplers in many settings.\n", "versions": [{"version": "v1", "created": "Mon, 16 Feb 2015 16:48:30 GMT"}], "update_date": "2015-02-17", "authors_parsed": [["Lakshminarayanan", "Balaji", ""], ["Roy", "Daniel M.", ""], ["Teh", "Yee Whye", ""]]}, {"id": "1502.04631", "submitter": "Jiaming Xu", "authors": "Rui Wu, Jiaming Xu, R. Srikant, Laurent Massouli\\'e, Marc Lelarge,\n  Bruce Hajek", "title": "Clustering and Inference From Pairwise Comparisons", "comments": "Corrected typos in the abstract", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Given a set of pairwise comparisons, the classical ranking problem computes a\nsingle ranking that best represents the preferences of all users. In this\npaper, we study the problem of inferring individual preferences, arising in the\ncontext of making personalized recommendations. In particular, we assume that\nthere are $n$ users of $r$ types; users of the same type provide similar\npairwise comparisons for $m$ items according to the Bradley-Terry model. We\npropose an efficient algorithm that accurately estimates the individual\npreferences for almost all users, if there are $r \\max \\{m, n\\}\\log m \\log^2 n$\npairwise comparisons per type, which is near optimal in sample complexity when\n$r$ only grows logarithmically with $m$ or $n$. Our algorithm has three steps:\nfirst, for each user, compute the \\emph{net-win} vector which is a projection\nof its $\\binom{m}{2}$-dimensional vector of pairwise comparisons onto an\n$m$-dimensional linear subspace; second, cluster the users based on the net-win\nvectors; third, estimate a single preference for each cluster separately. The\nnet-win vectors are much less noisy than the high dimensional vectors of\npairwise comparisons and clustering is more accurate after the projection as\nconfirmed by numerical experiments. Moreover, we show that, when a cluster is\nonly approximately correct, the maximum likelihood estimation for the\nBradley-Terry model is still close to the true preference.\n", "versions": [{"version": "v1", "created": "Mon, 16 Feb 2015 17:08:48 GMT"}, {"version": "v2", "created": "Thu, 17 Dec 2015 05:42:44 GMT"}], "update_date": "2015-12-18", "authors_parsed": [["Wu", "Rui", ""], ["Xu", "Jiaming", ""], ["Srikant", "R.", ""], ["Massouli\u00e9", "Laurent", ""], ["Lelarge", "Marc", ""], ["Hajek", "Bruce", ""]]}, {"id": "1502.04635", "submitter": "Paul Reverdy", "authors": "Paul Reverdy and Naomi E. Leonard", "title": "Parameter estimation in softmax decision-making models with linear\n  objective functions", "comments": "In press", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.OC cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  With an eye towards human-centered automation, we contribute to the\ndevelopment of a systematic means to infer features of human decision-making\nfrom behavioral data. Motivated by the common use of softmax selection in\nmodels of human decision-making, we study the maximum likelihood parameter\nestimation problem for softmax decision-making models with linear objective\nfunctions. We present conditions under which the likelihood function is convex.\nThese allow us to provide sufficient conditions for convergence of the\nresulting maximum likelihood estimator and to construct its asymptotic\ndistribution. In the case of models with nonlinear objective functions, we show\nhow the estimator can be applied by linearizing about a nominal parameter\nvalue. We apply the estimator to fit the stochastic UCL (Upper Credible Limit)\nmodel of human decision-making to human subject data. We show statistically\nsignificant differences in behavior across related, but distinct, tasks.\n", "versions": [{"version": "v1", "created": "Mon, 16 Feb 2015 17:17:24 GMT"}, {"version": "v2", "created": "Sat, 29 Aug 2015 19:50:20 GMT"}], "update_date": "2015-09-01", "authors_parsed": [["Reverdy", "Paul", ""], ["Leonard", "Naomi E.", ""]]}, {"id": "1502.04689", "submitter": "Zemin Zhang", "authors": "Zemin Zhang, Shuchin Aeron", "title": "Exact tensor completion using t-SVD", "comments": "16 pages, 5 figures, 2 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.NA stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper we focus on the problem of completion of multidimensional\narrays (also referred to as tensors) from limited sampling. Our approach is\nbased on a recently proposed tensor-Singular Value Decomposition (t-SVD) [1].\nUsing this factorization one can derive notion of tensor rank, referred to as\nthe tensor tubal rank, which has optimality properties similar to that of\nmatrix rank derived from SVD. As shown in [2] some multidimensional data, such\nas panning video sequences exhibit low tensor tubal rank and we look at the\nproblem of completing such data under random sampling of the data cube. We show\nthat by solving a convex optimization problem, which minimizes the tensor\nnuclear norm obtained as the convex relaxation of tensor tubal rank, one can\nguarantee recovery with overwhelming probability as long as samples in\nproportion to the degrees of freedom in t-SVD are observed. In this sense our\nresults are order-wise optimal. The conditions under which this result holds\nare very similar to the incoherency conditions for the matrix completion,\nalbeit we define incoherency under the algebraic set-up of t-SVD. We show the\nperformance of the algorithm on some real data sets and compare it with other\nexisting approaches based on tensor flattening and Tucker decomposition.\n", "versions": [{"version": "v1", "created": "Mon, 16 Feb 2015 20:37:35 GMT"}, {"version": "v2", "created": "Fri, 27 Feb 2015 19:31:25 GMT"}], "update_date": "2015-03-02", "authors_parsed": [["Zhang", "Zemin", ""], ["Aeron", "Shuchin", ""]]}, {"id": "1502.04726", "submitter": "Hojjat Seyed Mousavi", "authors": "Hojjat S. Mousavi, Vishal Monga, Trac D. Tran", "title": "ICR: Iterative Convex Refinement for Sparse Signal Recovery Using Spike\n  and Slab Priors", "comments": "Submitted to IEEE Signal Processing Letters, Feb 2015", "journal-ref": null, "doi": "10.1109/LSP.2015.2438255", "report-no": null, "categories": "stat.ML cs.CV math.OC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this letter, we address sparse signal recovery using spike and slab\npriors. In particular, we focus on a Bayesian framework where sparsity is\nenforced on reconstruction coefficients via probabilistic priors. The\noptimization resulting from spike and slab prior maximization is known to be a\nhard non-convex problem, and existing solutions involve simplifying assumptions\nand/or relaxations. We propose an approach called Iterative Convex Refinement\n(ICR) that aims to solve the aforementioned optimization problem directly\nallowing for greater generality in the sparse structure. Essentially, ICR\nsolves a sequence of convex optimization problems such that sequence of\nsolutions converges to a sub-optimal solution of the original hard optimization\nproblem. We propose two versions of our algorithm: a.) an unconstrained\nversion, and b.) with a non-negativity constraint on sparse coefficients, which\nmay be required in some real-world problems. Experimental validation is\nperformed on both synthetic data and for a real-world image recovery problem,\nwhich illustrates merits of ICR over state of the art alternatives.\n", "versions": [{"version": "v1", "created": "Mon, 16 Feb 2015 21:17:52 GMT"}], "update_date": "2015-05-28", "authors_parsed": [["Mousavi", "Hojjat S.", ""], ["Monga", "Vishal", ""], ["Tran", "Trac D.", ""]]}, {"id": "1502.04742", "submitter": "Necla Gunduz", "authors": "Necla Gunduz and Ernest Fokoue", "title": "On the Predictive Properties of Binary Link Functions", "comments": "17 pages, 10 figures. arXiv admin note: text overlap with\n  arXiv:math-ph/0607066 by other authors", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper provides a theoretical and computational justification of the long\nheld claim that of the similarity of the probit and logit link functions often\nused in binary classification. Despite this widespread recognition of the\nstrong similarities between these two link functions, very few (if any)\nresearchers have dedicated time to carry out a formal study aimed at\nestablishing and characterizing firmly all the aspects of the similarities and\ndifferences. This paper proposes a definition of both structural and predictive\nequivalence of link functions-based binary regression models, and explores the\nvarious ways in which they are either similar or dissimilar. From a predictive\nanalytics perspective, it turns out that not only are probit and logit\nperfectly predictively concordant, but the other link functions like cauchit\nand complementary log log enjoy very high percentage of predictive equivalence.\nThroughout this paper, simulated and real life examples demonstrate all the\nequivalence results that we prove theoretically.\n", "versions": [{"version": "v1", "created": "Mon, 16 Feb 2015 22:39:57 GMT"}], "update_date": "2015-02-18", "authors_parsed": [["Gunduz", "Necla", ""], ["Fokoue", "Ernest", ""]]}, {"id": "1502.04837", "submitter": "Teng Qiu", "authors": "Teng Qiu, Yongjie Li", "title": "Nonparametric Nearest Neighbor Descent Clustering based on Delaunay\n  Triangulation", "comments": "7 pages; 6 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.CV cs.LG", "license": "http://creativecommons.org/licenses/by-nc-sa/3.0/", "abstract": "  In our physically inspired in-tree (IT) based clustering algorithm and the\nseries after it, there is only one free parameter involved in computing the\npotential value of each point. In this work, based on the Delaunay\nTriangulation or its dual Voronoi tessellation, we propose a nonparametric\nprocess to compute potential values by the local information. This computation,\nthough nonparametric, is relatively very rough, and consequently, many local\nextreme points will be generated. However, unlike those gradient-based methods,\nour IT-based methods are generally insensitive to those local extremes. This\npositively demonstrates the superiority of these parametric (previous) and\nnonparametric (in this work) IT-based methods.\n", "versions": [{"version": "v1", "created": "Tue, 17 Feb 2015 09:27:03 GMT"}, {"version": "v2", "created": "Wed, 18 Mar 2015 12:17:46 GMT"}], "update_date": "2015-03-19", "authors_parsed": [["Qiu", "Teng", ""], ["Li", "Yongjie", ""]]}, {"id": "1502.04868", "submitter": "Rafael Boloix-Tortosa", "authors": "Rafael Boloix-Tortosa, F. Javier Pay\\'an-Somet, Eva Arias-de-Reyna and\n  Juan Jos\\'e Murillo-Fuentes", "title": "Proper Complex Gaussian Processes for Regression", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Complex-valued signals are used in the modeling of many systems in\nengineering and science, hence being of fundamental interest. Often, random\ncomplex-valued signals are considered to be proper. A proper complex random\nvariable or process is uncorrelated with its complex conjugate. This assumption\nis a good model of the underlying physics in many problems, and simplifies the\ncomputations. While linear processing and neural networks have been widely\nstudied for these signals, the development of complex-valued nonlinear kernel\napproaches remains an open problem. In this paper we propose Gaussian processes\nfor regression as a framework to develop 1) a solution for proper\ncomplex-valued kernel regression and 2) the design of the reproducing kernel\nfor complex-valued inputs, using the convolutional approach for\ncross-covariances. In this design we pay attention to preserve, in the complex\ndomain, the measure of similarity between near inputs. The hyperparameters of\nthe kernel are learned maximizing the marginal likelihood using Wirtinger\nderivatives. Besides, the approach is connected to the multiple output learning\nscenario. In the experiments included, we first solve a proper complex Gaussian\nprocess where the cross-covariance does not cancel, a challenging scenario when\ndealing with proper complex signals. Then we successfully use these novel\nresults to solve some problems previously proposed in the literature as\nbenchmarks, reporting a remarkable improvement in the estimation error.\n", "versions": [{"version": "v1", "created": "Tue, 17 Feb 2015 11:59:44 GMT"}, {"version": "v2", "created": "Wed, 18 Feb 2015 09:33:34 GMT"}], "update_date": "2015-02-19", "authors_parsed": [["Boloix-Tortosa", "Rafael", ""], ["Pay\u00e1n-Somet", "F. Javier", ""], ["Arias-de-Reyna", "Eva", ""], ["Murillo-Fuentes", "Juan Jos\u00e9", ""]]}, {"id": "1502.04874", "submitter": "Sebastien Gadat", "authors": "S\\'ebastien Gadat and Fabien Panloup and Sofiane Saadane", "title": "Regret bounds for Narendra-Shapiro bandit algorithms", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.PR math.ST stat.ML stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Narendra-Shapiro (NS) algorithms are bandit-type algorithms that have been\nintroduced in the sixties (with a view to applications in Psychology or\nlearning automata), whose convergence has been intensively studied in the\nstochastic algorithm literature. In this paper, we adress the following\nquestion: are the Narendra-Shapiro (NS) bandit algorithms competitive from a\n\\textit{regret} point of view? In our main result, we show that some\ncompetitive bounds can be obtained for such algorithms in their penalized\nversion (introduced in \\cite{Lamberton_Pages}). More precisely, up to an\nover-penalization modification, the pseudo-regret $\\bar{R}_n$ related to the\npenalized two-armed bandit algorithm is uniformly bounded by $C \\sqrt{n}$\n(where $C$ is made explicit in the paper). \\noindent We also generalize\nexisting convergence and rates of convergence results to the multi-armed case\nof the over-penalized bandit algorithm, including the convergence toward the\ninvariant measure of a Piecewise Deterministic Markov Process (PDMP) after a\nsuitable renormalization. Finally, ergodic properties of this PDMP are given in\nthe multi-armed case.\n", "versions": [{"version": "v1", "created": "Tue, 17 Feb 2015 12:49:01 GMT"}, {"version": "v2", "created": "Sat, 16 Jan 2016 13:13:35 GMT"}], "update_date": "2016-01-19", "authors_parsed": [["Gadat", "S\u00e9bastien", ""], ["Panloup", "Fabien", ""], ["Saadane", "Sofiane", ""]]}, {"id": "1502.04993", "submitter": "Yury Zaytsev", "authors": "Yury V. Zaytsev, Abigail Morrison and Moritz Deger", "title": "Reconstruction of recurrent synaptic connectivity of thousands of\n  neurons from simulated spiking activity", "comments": "This is the final version of the manuscript from the publisher which\n  supersedes our original pre-print version. The spike data used in this paper\n  and the code that implements our connectivity reconstruction method are\n  publicly available for download at http://dx.doi.org/10.5281/zenodo.17662 and\n  http://dx.doi.org/10.5281/zenodo.17663 respectively", "journal-ref": "J. Comput Neurosci 39(1), 77-103 (2015)", "doi": "10.1007/s10827-015-0565-5", "report-no": null, "categories": "q-bio.NC q-bio.QM stat.ML", "license": "http://creativecommons.org/licenses/by/3.0/", "abstract": "  Dynamics and function of neuronal networks are determined by their synaptic\nconnectivity. Current experimental methods to analyze synaptic network\nstructure on the cellular level, however, cover only small fractions of\nfunctional neuronal circuits, typically without a simultaneous record of\nneuronal spiking activity. Here we present a method for the reconstruction of\nlarge recurrent neuronal networks from thousands of parallel spike train\nrecordings. We employ maximum likelihood estimation of a generalized linear\nmodel of the spiking activity in continuous time. For this model the point\nprocess likelihood is concave, such that a global optimum of the parameters can\nbe obtained by gradient ascent. Previous methods, including those of the same\nclass, did not allow recurrent networks of that order of magnitude to be\nreconstructed due to prohibitive computational cost and numerical\ninstabilities. We describe a minimal model that is optimized for large networks\nand an efficient scheme for its parallelized numerical optimization on generic\ncomputing clusters. For a simulated balanced random network of 1000 neurons,\nsynaptic connectivity is recovered with a misclassification error rate of less\nthan 1% under ideal conditions. We show that the error rate remains low in a\nseries of example cases under progressively less ideal conditions. Finally, we\nsuccessfully reconstruct the connectivity of a hidden synfire chain that is\nembedded in a random network, which requires clustering of the network\nconnectivity to reveal the synfire groups. Our results demonstrate how synaptic\nconnectivity could potentially be inferred from large-scale parallel spike\ntrain recordings.\n", "versions": [{"version": "v1", "created": "Tue, 17 Feb 2015 18:43:58 GMT"}, {"version": "v2", "created": "Sun, 7 Jun 2015 11:09:27 GMT"}], "update_date": "2015-07-28", "authors_parsed": [["Zaytsev", "Yury V.", ""], ["Morrison", "Abigail", ""], ["Deger", "Moritz", ""]]}, {"id": "1502.05023", "submitter": "Srinadh Bhojanapalli", "authors": "Srinadh Bhojanapalli, Sujay Sanghavi", "title": "A New Sampling Technique for Tensors", "comments": "29 pages,3 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.DS cs.IT cs.LG math.IT", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper we propose new techniques to sample arbitrary third-order\ntensors, with an objective of speeding up tensor algorithms that have recently\ngained popularity in machine learning. Our main contribution is a new way to\nselect, in a biased random way, only $O(n^{1.5}/\\epsilon^2)$ of the possible\n$n^3$ elements while still achieving each of the three goals: \\\\ {\\em (a)\ntensor sparsification}: for a tensor that has to be formed from arbitrary\nsamples, compute very few elements to get a good spectral approximation, and\nfor arbitrary orthogonal tensors {\\em (b) tensor completion:} recover an\nexactly low-rank tensor from a small number of samples via alternating least\nsquares, or {\\em (c) tensor factorization:} approximating factors of a low-rank\ntensor corrupted by noise. \\\\ Our sampling can be used along with existing\ntensor-based algorithms to speed them up, removing the computational bottleneck\nin these methods.\n", "versions": [{"version": "v1", "created": "Tue, 17 Feb 2015 20:23:13 GMT"}, {"version": "v2", "created": "Thu, 19 Feb 2015 21:05:53 GMT"}], "update_date": "2015-02-23", "authors_parsed": [["Bhojanapalli", "Srinadh", ""], ["Sanghavi", "Sujay", ""]]}, {"id": "1502.05312", "submitter": "Jos\\'e Miguel Hern\\'andez-Lobato", "authors": "Jos\\'e Miguel Hern\\'andez-Lobato, Michael A. Gelbart, Matthew W.\n  Hoffman, Ryan P. Adams and Zoubin Ghahramani", "title": "Predictive Entropy Search for Bayesian Optimization with Unknown\n  Constraints", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Unknown constraints arise in many types of expensive black-box optimization\nproblems. Several methods have been proposed recently for performing Bayesian\noptimization with constraints, based on the expected improvement (EI)\nheuristic. However, EI can lead to pathologies when used with constraints. For\nexample, in the case of decoupled constraints---i.e., when one can\nindependently evaluate the objective or the constraints---EI can encounter a\npathology that prevents exploration. Additionally, computing EI requires a\ncurrent best solution, which may not exist if none of the data collected so far\nsatisfy the constraints. By contrast, information-based approaches do not\nsuffer from these failure modes. In this paper, we present a new\ninformation-based method called Predictive Entropy Search with Constraints\n(PESC). We analyze the performance of PESC and show that it compares favorably\nto EI-based approaches on synthetic and benchmark problems, as well as several\nreal-world examples. We demonstrate that PESC is an effective algorithm that\nprovides a promising direction towards a unified solution for constrained\nBayesian optimization.\n", "versions": [{"version": "v1", "created": "Wed, 18 Feb 2015 17:39:30 GMT"}, {"version": "v2", "created": "Wed, 15 Jul 2015 10:34:52 GMT"}], "update_date": "2015-07-16", "authors_parsed": [["Hern\u00e1ndez-Lobato", "Jos\u00e9 Miguel", ""], ["Gelbart", "Michael A.", ""], ["Hoffman", "Matthew W.", ""], ["Adams", "Ryan P.", ""], ["Ghahramani", "Zoubin", ""]]}, {"id": "1502.05313", "submitter": "Taichi Kiwaki Mr", "authors": "Taichi Kiwaki", "title": "Variational Optimization of Annealing Schedules", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Annealed importance sampling (AIS) is a common algorithm to estimate\npartition functions of useful stochastic models. One important problem for\nobtaining accurate AIS estimates is the selection of an annealing schedule.\nConventionally, an annealing schedule is often determined heuristically or is\nsimply set as a linearly increasing sequence. In this paper, we propose an\nalgorithm for the optimal schedule by deriving a functional that dominates the\nAIS estimation error and by numerically minimizing this functional. We\nexperimentally demonstrate that the proposed algorithm mostly outperforms\nconventional scheduling schemes with large quantization numbers.\n", "versions": [{"version": "v1", "created": "Wed, 18 Feb 2015 17:45:44 GMT"}, {"version": "v2", "created": "Fri, 27 Mar 2015 15:02:25 GMT"}], "update_date": "2015-03-30", "authors_parsed": [["Kiwaki", "Taichi", ""]]}, {"id": "1502.05336", "submitter": "Jos\\'e Miguel Hern\\'andez-Lobato", "authors": "Jos\\'e Miguel Hern\\'andez-Lobato and Ryan P. Adams", "title": "Probabilistic Backpropagation for Scalable Learning of Bayesian Neural\n  Networks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Large multilayer neural networks trained with backpropagation have recently\nachieved state-of-the-art results in a wide range of problems. However, using\nbackprop for neural net learning still has some disadvantages, e.g., having to\ntune a large number of hyperparameters to the data, lack of calibrated\nprobabilistic predictions, and a tendency to overfit the training data. In\nprinciple, the Bayesian approach to learning neural networks does not have\nthese problems. However, existing Bayesian techniques lack scalability to large\ndataset and network sizes. In this work we present a novel scalable method for\nlearning Bayesian neural networks, called probabilistic backpropagation (PBP).\nSimilar to classical backpropagation, PBP works by computing a forward\npropagation of probabilities through the network and then doing a backward\ncomputation of gradients. A series of experiments on ten real-world datasets\nshow that PBP is significantly faster than other techniques, while offering\ncompetitive predictive abilities. Our experiments also show that PBP provides\naccurate estimates of the posterior variance on the network weights.\n", "versions": [{"version": "v1", "created": "Wed, 18 Feb 2015 18:45:17 GMT"}, {"version": "v2", "created": "Wed, 15 Jul 2015 10:14:49 GMT"}], "update_date": "2015-07-16", "authors_parsed": [["Hern\u00e1ndez-Lobato", "Jos\u00e9 Miguel", ""], ["Adams", "Ryan P.", ""]]}, {"id": "1502.05503", "submitter": "Michael Gutmann", "authors": "Michael U. Gutmann, Jukka Corander, Ritabrata Dutta, and Samuel Kaski", "title": "Classification and Bayesian Optimization for Likelihood-Free Inference", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.CO stat.ME stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Some statistical models are specified via a data generating process for which\nthe likelihood function cannot be computed in closed form. Standard\nlikelihood-based inference is then not feasible but the model parameters can be\ninferred by finding the values which yield simulated data that resemble the\nobserved data. This approach faces at least two major difficulties: The first\ndifficulty is the choice of the discrepancy measure which is used to judge\nwhether the simulated data resemble the observed data. The second difficulty is\nthe computationally efficient identification of regions in the parameter space\nwhere the discrepancy is low. We give here an introduction to our recent work\nwhere we tackle the two difficulties through classification and Bayesian\noptimization.\n", "versions": [{"version": "v1", "created": "Thu, 19 Feb 2015 09:09:27 GMT"}], "update_date": "2015-02-20", "authors_parsed": [["Gutmann", "Michael U.", ""], ["Corander", "Jukka", ""], ["Dutta", "Ritabrata", ""], ["Kaski", "Samuel", ""]]}, {"id": "1502.05556", "submitter": "Lucas Maystre", "authors": "Lucas Maystre, Matthias Grossglauser", "title": "Just Sort It! A Simple and Effective Approach to Active Preference\n  Learning", "comments": "Accepted at ICML 2017", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  We address the problem of learning a ranking by using adaptively chosen\npairwise comparisons. Our goal is to recover the ranking accurately but to\nsample the comparisons sparingly. If all comparison outcomes are consistent\nwith the ranking, the optimal solution is to use an efficient sorting\nalgorithm, such as Quicksort. But how do sorting algorithms behave if some\ncomparison outcomes are inconsistent with the ranking? We give favorable\nguarantees for Quicksort for the popular Bradley-Terry model, under natural\nassumptions on the parameters. Furthermore, we empirically demonstrate that\nsorting algorithms lead to a very simple and effective active learning\nstrategy: repeatedly sort the items. This strategy performs as well as\nstate-of-the-art methods (and much better than random sampling) at a minuscule\nfraction of the computational cost.\n", "versions": [{"version": "v1", "created": "Thu, 19 Feb 2015 12:50:13 GMT"}, {"version": "v2", "created": "Thu, 15 Jun 2017 15:19:38 GMT"}], "update_date": "2017-06-16", "authors_parsed": [["Maystre", "Lucas", ""], ["Grossglauser", "Matthias", ""]]}, {"id": "1502.05571", "submitter": "Ashley Prater", "authors": "Ashley Prater, Lixin Shen, Bruce W. Suter", "title": "Finding Dantzig selectors with a proximity operator based fixed-point\n  algorithm", "comments": "15 pages, 5 figures. Submitted to Computational Statistics and Data\n  Analysis", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.NA stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we study a simple iterative method for finding the Dantzig\nselector, which was designed for linear regression problems. The method\nconsists of two main stages. The first stage is to approximate the Dantzig\nselector through a fixed-point formulation of solutions to the Dantzig selector\nproblem. The second stage is to construct a new estimator by regressing data\nonto the support of the approximated Dantzig selector. We compare our method to\nan alternating direction method, and present the results of numerical\nsimulations using both the proposed method and the alternating direction method\non synthetic and real data sets. The numerical simulations demonstrate that the\ntwo methods produce results of similar quality, however the proposed method\ntends to be significantly faster.\n", "versions": [{"version": "v1", "created": "Thu, 19 Feb 2015 13:50:18 GMT"}], "update_date": "2015-02-20", "authors_parsed": [["Prater", "Ashley", ""], ["Shen", "Lixin", ""], ["Suter", "Bruce W.", ""]]}, {"id": "1502.05675", "submitter": "Malik Magdon-Ismail", "authors": "Malik Magdon-Ismail", "title": "NP-Hardness and Inapproximability of Sparse PCA", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CC cs.DS math.CO stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We give a reduction from {\\sc clique} to establish that sparse PCA is\nNP-hard. The reduction has a gap which we use to exclude an FPTAS for sparse\nPCA (unless P=NP). Under weaker complexity assumptions, we also exclude\npolynomial constant-factor approximation algorithms.\n", "versions": [{"version": "v1", "created": "Thu, 19 Feb 2015 19:30:46 GMT"}, {"version": "v2", "created": "Fri, 20 Feb 2015 13:00:17 GMT"}], "update_date": "2015-02-23", "authors_parsed": [["Magdon-Ismail", "Malik", ""]]}, {"id": "1502.05680", "submitter": "Andrea Montanari", "authors": "Andrea Montanari", "title": "Finding One Community in a Sparse Graph", "comments": "30 pages, 8 pdf figures", "journal-ref": null, "doi": "10.1007/s10955-015-1338-2", "report-no": null, "categories": "stat.ML cond-mat.stat-mech cs.SI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider a random sparse graph with bounded average degree, in which a\nsubset of vertices has higher connectivity than the background. In particular,\nthe average degree inside this subset of vertices is larger than outside (but\nstill bounded). Given a realization of such graph, we aim at identifying the\nhidden subset of vertices. This can be regarded as a model for the problem of\nfinding a tightly knitted community in a social network, or a cluster in a\nrelational dataset.\n  In this paper we present two sets of contributions: $(i)$ We use the cavity\nmethod from spin glass theory to derive an exact phase diagram for the\nreconstruction problem. In particular, as the difference in edge probability\nincreases, the problem undergoes two phase transitions, a static phase\ntransition and a dynamic one. $(ii)$ We establish rigorous bounds on the\ndynamic phase transition and prove that, above a certain threshold, a local\nalgorithm (belief propagation) correctly identify most of the hidden set. Below\nthe same threshold \\emph{no local algorithm} can achieve this goal. However, in\nthis regime the subset can be identified by exhaustive search.\n  For small hidden sets and large average degree, the phase transition for\nlocal algorithms takes an intriguingly simple form. Local algorithms succeed\nwith high probability for ${\\rm deg}_{\\rm in} - {\\rm deg}_{\\rm out} >\n\\sqrt{{\\rm deg}_{\\rm out}/e}$ and fail for ${\\rm deg}_{\\rm in} - {\\rm deg}_{\\rm\nout} < \\sqrt{{\\rm deg}_{\\rm out}/e}$ (with ${\\rm deg}_{\\rm in}$, ${\\rm\ndeg}_{\\rm out}$ the average degrees inside and outside the community). We argue\nthat spectral algorithms are also ineffective in the latter regime.\n  It is an open problem whether any polynomial time algorithms might succeed\nfor ${\\rm deg}_{\\rm in} - {\\rm deg}_{\\rm out} < \\sqrt{{\\rm deg}_{\\rm out}/e}$.\n", "versions": [{"version": "v1", "created": "Thu, 19 Feb 2015 19:50:09 GMT"}, {"version": "v2", "created": "Thu, 30 Jul 2015 19:46:13 GMT"}], "update_date": "2015-09-02", "authors_parsed": [["Montanari", "Andrea", ""]]}, {"id": "1502.05698", "submitter": "Jason  Weston", "authors": "Jason Weston, Antoine Bordes, Sumit Chopra, Alexander M. Rush, Bart\n  van Merri\\\"enboer, Armand Joulin, Tomas Mikolov", "title": "Towards AI-Complete Question Answering: A Set of Prerequisite Toy Tasks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.CL stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  One long-term goal of machine learning research is to produce methods that\nare applicable to reasoning and natural language, in particular building an\nintelligent dialogue agent. To measure progress towards that goal, we argue for\nthe usefulness of a set of proxy tasks that evaluate reading comprehension via\nquestion answering. Our tasks measure understanding in several ways: whether a\nsystem is able to answer questions via chaining facts, simple induction,\ndeduction and many more. The tasks are designed to be prerequisites for any\nsystem that aims to be capable of conversing with a human. We believe many\nexisting learning systems can currently not solve them, and hence our aim is to\nclassify these tasks into skill sets, so that researchers can identify (and\nthen rectify) the failings of their systems. We also extend and improve the\nrecently introduced Memory Networks model, and show it is able to solve some,\nbut not all, of the tasks.\n", "versions": [{"version": "v1", "created": "Thu, 19 Feb 2015 20:46:10 GMT"}, {"version": "v10", "created": "Thu, 31 Dec 2015 13:08:14 GMT"}, {"version": "v2", "created": "Tue, 24 Feb 2015 18:35:12 GMT"}, {"version": "v3", "created": "Wed, 25 Feb 2015 17:50:21 GMT"}, {"version": "v4", "created": "Sat, 14 Mar 2015 14:03:33 GMT"}, {"version": "v5", "created": "Fri, 10 Apr 2015 14:51:46 GMT"}, {"version": "v6", "created": "Tue, 2 Jun 2015 21:58:20 GMT"}, {"version": "v7", "created": "Wed, 7 Oct 2015 19:36:24 GMT"}, {"version": "v8", "created": "Sat, 21 Nov 2015 23:23:02 GMT"}, {"version": "v9", "created": "Sun, 29 Nov 2015 06:24:27 GMT"}], "update_date": "2016-01-01", "authors_parsed": [["Weston", "Jason", ""], ["Bordes", "Antoine", ""], ["Chopra", "Sumit", ""], ["Rush", "Alexander M.", ""], ["van Merri\u00ebnboer", "Bart", ""], ["Joulin", "Armand", ""], ["Mikolov", "Tomas", ""]]}, {"id": "1502.05700", "submitter": "Jasper Snoek", "authors": "Jasper Snoek, Oren Rippel, Kevin Swersky, Ryan Kiros, Nadathur Satish,\n  Narayanan Sundaram, Md. Mostofa Ali Patwary, Prabhat, Ryan P. Adams", "title": "Scalable Bayesian Optimization Using Deep Neural Networks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Bayesian optimization is an effective methodology for the global optimization\nof functions with expensive evaluations. It relies on querying a distribution\nover functions defined by a relatively cheap surrogate model. An accurate model\nfor this distribution over functions is critical to the effectiveness of the\napproach, and is typically fit using Gaussian processes (GPs). However, since\nGPs scale cubically with the number of observations, it has been challenging to\nhandle objectives whose optimization requires many evaluations, and as such,\nmassively parallelizing the optimization.\n  In this work, we explore the use of neural networks as an alternative to GPs\nto model distributions over functions. We show that performing adaptive basis\nfunction regression with a neural network as the parametric form performs\ncompetitively with state-of-the-art GP-based approaches, but scales linearly\nwith the number of data rather than cubically. This allows us to achieve a\npreviously intractable degree of parallelism, which we apply to large scale\nhyperparameter optimization, rapidly finding competitive models on benchmark\nobject recognition tasks using convolutional networks, and image caption\ngeneration using neural language models.\n", "versions": [{"version": "v1", "created": "Thu, 19 Feb 2015 20:51:27 GMT"}, {"version": "v2", "created": "Mon, 13 Jul 2015 15:47:13 GMT"}], "update_date": "2015-07-14", "authors_parsed": [["Snoek", "Jasper", ""], ["Rippel", "Oren", ""], ["Swersky", "Kevin", ""], ["Kiros", "Ryan", ""], ["Satish", "Nadathur", ""], ["Sundaram", "Narayanan", ""], ["Patwary", "Md. Mostofa Ali", ""], ["Prabhat", "", ""], ["Adams", "Ryan P.", ""]]}, {"id": "1502.05752", "submitter": "Zhiwu Lu", "authors": "Zhenyong Fu and Zhiwu Lu", "title": "Pairwise Constraint Propagation: A Survey", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  As one of the most important types of (weaker) supervised information in\nmachine learning and pattern recognition, pairwise constraint, which specifies\nwhether a pair of data points occur together, has recently received significant\nattention, especially the problem of pairwise constraint propagation. At least\ntwo reasons account for this trend: the first is that compared to the data\nlabel, pairwise constraints are more general and easily to collect, and the\nsecond is that since the available pairwise constraints are usually limited,\nthe constraint propagation problem is thus important.\n  This paper provides an up-to-date critical survey of pairwise constraint\npropagation research. There are two underlying motivations for us to write this\nsurvey paper: the first is to provide an up-to-date review of the existing\nliterature, and the second is to offer some insights into the studies of\npairwise constraint propagation. To provide a comprehensive survey, we not only\ncategorize existing propagation techniques but also present detailed\ndescriptions of representative methods within each category.\n", "versions": [{"version": "v1", "created": "Thu, 19 Feb 2015 23:59:48 GMT"}], "update_date": "2015-02-23", "authors_parsed": [["Fu", "Zhenyong", ""], ["Lu", "Zhiwu", ""]]}, {"id": "1502.05767", "submitter": "Atilim Gunes Baydin", "authors": "Atilim Gunes Baydin, Barak A. Pearlmutter, Alexey Andreyevich Radul,\n  Jeffrey Mark Siskind", "title": "Automatic differentiation in machine learning: a survey", "comments": "43 pages, 5 figures", "journal-ref": "Atilim Gunes Baydin, Barak A. Pearlmutter, Alexey Andreyevich\n  Radul, Jeffrey Mark Siskind. Automatic differentiation in machine learning: a\n  survey. The Journal of Machine Learning Research, 18(153):1--43, 2018", "doi": null, "report-no": null, "categories": "cs.SC cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Derivatives, mostly in the form of gradients and Hessians, are ubiquitous in\nmachine learning. Automatic differentiation (AD), also called algorithmic\ndifferentiation or simply \"autodiff\", is a family of techniques similar to but\nmore general than backpropagation for efficiently and accurately evaluating\nderivatives of numeric functions expressed as computer programs. AD is a small\nbut established field with applications in areas including computational fluid\ndynamics, atmospheric sciences, and engineering design optimization. Until very\nrecently, the fields of machine learning and AD have largely been unaware of\neach other and, in some cases, have independently discovered each other's\nresults. Despite its relevance, general-purpose AD has been missing from the\nmachine learning toolbox, a situation slowly changing with its ongoing adoption\nunder the names \"dynamic computational graphs\" and \"differentiable\nprogramming\". We survey the intersection of AD and machine learning, cover\napplications where AD has direct relevance, and address the main implementation\ntechniques. By precisely defining the main differentiation techniques and their\ninterrelationships, we aim to bring clarity to the usage of the terms\n\"autodiff\", \"automatic differentiation\", and \"symbolic differentiation\" as\nthese are encountered more and more in machine learning settings.\n", "versions": [{"version": "v1", "created": "Fri, 20 Feb 2015 04:20:47 GMT"}, {"version": "v2", "created": "Sun, 19 Apr 2015 16:49:13 GMT"}, {"version": "v3", "created": "Thu, 17 Aug 2017 16:45:07 GMT"}, {"version": "v4", "created": "Mon, 5 Feb 2018 15:57:57 GMT"}], "update_date": "2018-07-18", "authors_parsed": [["Baydin", "Atilim Gunes", ""], ["Pearlmutter", "Barak A.", ""], ["Radul", "Alexey Andreyevich", ""], ["Siskind", "Jeffrey Mark", ""]]}, {"id": "1502.05774", "submitter": "Bo Waggoner", "authors": "Jacob Abernethy, Yiling Chen, Chien-Ju Ho, Bo Waggoner", "title": "Low-Cost Learning via Active Data Procurement", "comments": "Full version of EC 2015 paper. Color recommended for figures but\n  nonessential. 36 pages, of which 12 appendix", "journal-ref": null, "doi": "10.1145/2764468.2764519", "report-no": null, "categories": "cs.GT cs.AI cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We design mechanisms for online procurement of data held by strategic agents\nfor machine learning tasks. The challenge is to use past data to actively price\nfuture data and give learning guarantees even when an agent's cost for\nrevealing her data may depend arbitrarily on the data itself. We achieve this\ngoal by showing how to convert a large class of no-regret algorithms into\nonline posted-price and learning mechanisms. Our results in a sense parallel\nclassic sample complexity guarantees, but with the key resource being money\nrather than quantity of data: With a budget constraint $B$, we give robust risk\n(predictive error) bounds on the order of $1/\\sqrt{B}$. Because we use an\nactive approach, we can often guarantee to do significantly better by\nleveraging correlations between costs and data.\n  Our algorithms and analysis go through a model of no-regret learning with $T$\narriving pairs (cost, data) and a budget constraint of $B$. Our regret bounds\nfor this model are on the order of $T/\\sqrt{B}$ and we give lower bounds on the\nsame order.\n", "versions": [{"version": "v1", "created": "Fri, 20 Feb 2015 05:11:44 GMT"}, {"version": "v2", "created": "Sat, 6 Jun 2015 03:24:36 GMT"}], "update_date": "2015-06-09", "authors_parsed": [["Abernethy", "Jacob", ""], ["Chen", "Yiling", ""], ["Ho", "Chien-Ju", ""], ["Waggoner", "Bo", ""]]}, {"id": "1502.05890", "submitter": "Akshay Krishnamurthy", "authors": "Akshay Krishnamurthy, Alekh Agarwal, Miroslav Dudik", "title": "Contextual Semibandits via Supervised Learning Oracles", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study an online decision making problem where on each round a learner\nchooses a list of items based on some side information, receives a scalar\nfeedback value for each individual item, and a reward that is linearly related\nto this feedback. These problems, known as contextual semibandits, arise in\ncrowdsourcing, recommendation, and many other domains. This paper reduces\ncontextual semibandits to supervised learning, allowing us to leverage powerful\nsupervised learning methods in this partial-feedback setting. Our first\nreduction applies when the mapping from feedback to reward is known and leads\nto a computationally efficient algorithm with near-optimal regret. We show that\nthis algorithm outperforms state-of-the-art approaches on real-world\nlearning-to-rank datasets, demonstrating the advantage of oracle-based\nalgorithms. Our second reduction applies to the previously unstudied setting\nwhen the linear mapping from feedback to reward is unknown. Our regret\nguarantees are superior to prior techniques that ignore the feedback.\n", "versions": [{"version": "v1", "created": "Fri, 20 Feb 2015 14:55:41 GMT"}, {"version": "v2", "created": "Thu, 5 Mar 2015 01:38:23 GMT"}, {"version": "v3", "created": "Tue, 14 Jun 2016 00:43:13 GMT"}, {"version": "v4", "created": "Fri, 4 Nov 2016 19:28:07 GMT"}], "update_date": "2016-11-07", "authors_parsed": [["Krishnamurthy", "Akshay", ""], ["Agarwal", "Alekh", ""], ["Dudik", "Miroslav", ""]]}, {"id": "1502.05925", "submitter": "Feng Nan", "authors": "Feng Nan, Joseph Wang, Venkatesh Saligrama", "title": "Feature-Budgeted Random Forest", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We seek decision rules for prediction-time cost reduction, where complete\ndata is available for training, but during prediction-time, each feature can\nonly be acquired for an additional cost. We propose a novel random forest\nalgorithm to minimize prediction error for a user-specified {\\it average}\nfeature acquisition budget. While random forests yield strong generalization\nperformance, they do not explicitly account for feature costs and furthermore\nrequire low correlation among trees, which amplifies costs. Our random forest\ngrows trees with low acquisition cost and high strength based on greedy minimax\ncost-weighted-impurity splits. Theoretically, we establish near-optimal\nacquisition cost guarantees for our algorithm. Empirically, on a number of\nbenchmark datasets we demonstrate superior accuracy-cost curves against\nstate-of-the-art prediction-time algorithms.\n", "versions": [{"version": "v1", "created": "Fri, 20 Feb 2015 16:42:40 GMT"}], "update_date": "2015-02-23", "authors_parsed": [["Nan", "Feng", ""], ["Wang", "Joseph", ""], ["Saligrama", "Venkatesh", ""]]}, {"id": "1502.06064", "submitter": "Ken Miura", "authors": "Ken Miura, Tetsuaki Mano, Atsushi Kanehira, Yuichiro Tsuchiya and\n  Tatsuya Harada", "title": "MILJS : Brand New JavaScript Libraries for Matrix Calculation and\n  Machine Learning", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG cs.MS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  MILJS is a collection of state-of-the-art, platform-independent, scalable,\nfast JavaScript libraries for matrix calculation and machine learning. Our core\nlibrary offering a matrix calculation is called Sushi, which exhibits far\nbetter performance than any other leading machine learning libraries written in\nJavaScript. Especially, our matrix multiplication is 177 times faster than the\nfastest JavaScript benchmark. Based on Sushi, a machine learning library called\nTempura is provided, which supports various algorithms widely used in machine\nlearning research. We also provide Soba as a visualization library. The\nimplementations of our libraries are clearly written, properly documented and\nthus can are easy to get started with, as long as there is a web browser. These\nlibraries are available from http://mil-tokyo.github.io/ under the MIT license.\n", "versions": [{"version": "v1", "created": "Sat, 21 Feb 2015 04:29:41 GMT"}], "update_date": "2015-02-24", "authors_parsed": [["Miura", "Ken", ""], ["Mano", "Tetsuaki", ""], ["Kanehira", "Atsushi", ""], ["Tsuchiya", "Yuichiro", ""], ["Harada", "Tatsuya", ""]]}, {"id": "1502.06134", "submitter": "Tengyuan Liang", "authors": "Tengyuan Liang, Alexander Rakhlin, Karthik Sridharan", "title": "Learning with Square Loss: Localization through Offset Rademacher\n  Complexity", "comments": "21 pages, 1 figure", "journal-ref": "Proceedings of the 28th Conference on Learning Theory 40 (2015)\n  1260-1285", "doi": null, "report-no": null, "categories": "stat.ML cs.LG math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider regression with square loss and general classes of functions\nwithout the boundedness assumption. We introduce a notion of offset Rademacher\ncomplexity that provides a transparent way to study localization both in\nexpectation and in high probability. For any (possibly non-convex) class, the\nexcess loss of a two-step estimator is shown to be upper bounded by this offset\ncomplexity through a novel geometric inequality. In the convex case, the\nestimator reduces to an empirical risk minimizer. The method recovers the\nresults of \\citep{RakSriTsy15} for the bounded case while also providing\nguarantees without the boundedness assumption.\n", "versions": [{"version": "v1", "created": "Sat, 21 Feb 2015 19:20:44 GMT"}, {"version": "v2", "created": "Thu, 26 Feb 2015 16:10:05 GMT"}, {"version": "v3", "created": "Mon, 15 Jun 2015 15:20:08 GMT"}], "update_date": "2020-07-27", "authors_parsed": [["Liang", "Tengyuan", ""], ["Rakhlin", "Alexander", ""], ["Sridharan", "Karthik", ""]]}, {"id": "1502.06161", "submitter": "Thiago Marzag\\~ao", "authors": "Thiago Marzag\\~ao", "title": "Using NLP to measure democracy", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.IR cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper uses natural language processing to create the first machine-coded\ndemocracy index, which I call Automated Democracy Scores (ADS). The ADS are\nbased on 42 million news articles from 6,043 different sources and cover all\nindependent countries in the 1993-2012 period. Unlike the democracy indices we\nhave today the ADS are replicable and have standard errors small enough to\nactually distinguish between cases.\n  The ADS are produced with supervised learning. Three approaches are tried: a)\na combination of Latent Semantic Analysis and tree-based regression methods; b)\na combination of Latent Dirichlet Allocation and tree-based regression methods;\nand c) the Wordscores algorithm. The Wordscores algorithm outperforms the\nalternatives, so it is the one on which the ADS are based.\n  There is a web application where anyone can change the training set and see\nhow the results change: democracy-scores.org\n", "versions": [{"version": "v1", "created": "Sun, 22 Feb 2015 01:30:32 GMT"}], "update_date": "2015-02-24", "authors_parsed": [["Marzag\u00e3o", "Thiago", ""]]}, {"id": "1502.06189", "submitter": "Hamed Firouzi", "authors": "Hamed Firouzi, Alfred Hero, Bala Rajaratnam", "title": "Two-stage Sampling, Prediction and Adaptive Regression via Correlation\n  Screening (SPARCS)", "comments": "To appear in IEEE Transactions on Information Theory. 40 Pages. arXiv\n  admin note: text overlap with arXiv:1303.2378", "journal-ref": null, "doi": "10.1109/TIT.2016.2621111", "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper proposes a general adaptive procedure for budget-limited predictor\ndesign in high dimensions called two-stage Sampling, Prediction and Adaptive\nRegression via Correlation Screening (SPARCS). SPARCS can be applied to high\ndimensional prediction problems in experimental science, medicine, finance, and\nengineering, as illustrated by the following. Suppose one wishes to run a\nsequence of experiments to learn a sparse multivariate predictor of a dependent\nvariable $Y$ (disease prognosis for instance) based on a $p$ dimensional set of\nindependent variables $\\mathbf X=[X_1,\\ldots, X_p]^T$ (assayed biomarkers).\nAssume that the cost of acquiring the full set of variables $\\mathbf X$\nincreases linearly in its dimension. SPARCS breaks the data collection into two\nstages in order to achieve an optimal tradeoff between sampling cost and\npredictor performance. In the first stage we collect a few ($n$) expensive\nsamples $\\{y_i,\\mathbf x_i\\}_{i=1}^n$, at the full dimension $p\\gg n$ of\n$\\mathbf X$, winnowing the number of variables down to a smaller dimension $l <\np$ using a type of cross-correlation or regression coefficient screening. In\nthe second stage we collect a larger number $(t-n)$ of cheaper samples of the\n$l$ variables that passed the screening of the first stage. At the second\nstage, a low dimensional predictor is constructed by solving the standard\nregression problem using all $t$ samples of the selected variables. SPARCS is\nan adaptive online algorithm that implements false positive control on the\nselected variables, is well suited to small sample sizes, and is scalable to\nhigh dimensions. We establish asymptotic bounds for the Familywise Error Rate\n(FWER), specify high dimensional convergence rates for support recovery, and\nestablish optimal sample allocation rules to the first and second stages.\n", "versions": [{"version": "v1", "created": "Sun, 22 Feb 2015 06:44:18 GMT"}, {"version": "v2", "created": "Sun, 2 Oct 2016 01:09:34 GMT"}], "update_date": "2016-11-18", "authors_parsed": [["Firouzi", "Hamed", ""], ["Hero", "Alfred", ""], ["Rajaratnam", "Bala", ""]]}, {"id": "1502.06309", "submitter": "Yu-Xiang Wang", "authors": "Yu-Xiang Wang, Jing Lei, Stephen E. Fienberg", "title": "Learning with Differential Privacy: Stability, Learnability and the\n  Sufficiency and Necessity of ERM Principle", "comments": "to appear, Journal of Machine Learning Research, 2016", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.CR cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  While machine learning has proven to be a powerful data-driven solution to\nmany real-life problems, its use in sensitive domains has been limited due to\nprivacy concerns. A popular approach known as **differential privacy** offers\nprovable privacy guarantees, but it is often observed in practice that it could\nsubstantially hamper learning accuracy. In this paper we study the learnability\n(whether a problem can be learned by any algorithm) under Vapnik's general\nlearning setting with differential privacy constraint, and reveal some\nintricate relationships between privacy, stability and learnability.\n  In particular, we show that a problem is privately learnable **if an only\nif** there is a private algorithm that asymptotically minimizes the empirical\nrisk (AERM). In contrast, for non-private learning AERM alone is not sufficient\nfor learnability. This result suggests that when searching for private learning\nalgorithms, we can restrict the search to algorithms that are AERM. In light of\nthis, we propose a conceptual procedure that always finds a universally\nconsistent algorithm whenever the problem is learnable under privacy\nconstraint. We also propose a generic and practical algorithm and show that\nunder very general conditions it privately learns a wide class of learning\nproblems. Lastly, we extend some of the results to the more practical\n$(\\epsilon,\\delta)$-differential privacy and establish the existence of a\nphase-transition on the class of problems that are approximately privately\nlearnable with respect to how small $\\delta$ needs to be.\n", "versions": [{"version": "v1", "created": "Mon, 23 Feb 2015 03:52:08 GMT"}, {"version": "v2", "created": "Tue, 23 Jun 2015 13:02:20 GMT"}, {"version": "v3", "created": "Wed, 27 Apr 2016 19:55:21 GMT"}], "update_date": "2016-04-28", "authors_parsed": [["Wang", "Yu-Xiang", ""], ["Lei", "Jing", ""], ["Fienberg", "Stephen E.", ""]]}, {"id": "1502.06354", "submitter": "Gergely Neu", "authors": "Gergely Neu", "title": "First-order regret bounds for combinatorial semi-bandits", "comments": "To appear at COLT 2015", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider the problem of online combinatorial optimization under\nsemi-bandit feedback, where a learner has to repeatedly pick actions from a\ncombinatorial decision set in order to minimize the total losses associated\nwith its decisions. After making each decision, the learner observes the losses\nassociated with its action, but not other losses. For this problem, there are\nseveral learning algorithms that guarantee that the learner's expected regret\ngrows as $\\widetilde{O}(\\sqrt{T})$ with the number of rounds $T$. In this\npaper, we propose an algorithm that improves this scaling to\n$\\widetilde{O}(\\sqrt{{L_T^*}})$, where $L_T^*$ is the total loss of the best\naction. Our algorithm is among the first to achieve such guarantees in a\npartial-feedback scheme, and the first one to do so in a combinatorial setting.\n", "versions": [{"version": "v1", "created": "Mon, 23 Feb 2015 09:12:26 GMT"}, {"version": "v2", "created": "Wed, 10 Jun 2015 11:51:51 GMT"}], "update_date": "2015-06-11", "authors_parsed": [["Neu", "Gergely", ""]]}, {"id": "1502.06464", "submitter": "Djork-Arn\\'e Clevert", "authors": "Djork-Arn\\'e Clevert, Andreas Mayr, Thomas Unterthiner, Sepp\n  Hochreiter", "title": "Rectified Factor Networks", "comments": "9 pages + 49 pages supplement", "journal-ref": "Advances in Neural Information Processing Systems 28 (NIPS 2015)", "doi": null, "report-no": null, "categories": "cs.LG cs.CV cs.NE stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose rectified factor networks (RFNs) to efficiently construct very\nsparse, non-linear, high-dimensional representations of the input. RFN models\nidentify rare and small events in the input, have a low interference between\ncode units, have a small reconstruction error, and explain the data covariance\nstructure. RFN learning is a generalized alternating minimization algorithm\nderived from the posterior regularization method which enforces non-negative\nand normalized posterior means. We proof convergence and correctness of the RFN\nlearning algorithm. On benchmarks, RFNs are compared to other unsupervised\nmethods like autoencoders, RBMs, factor analysis, ICA, and PCA. In contrast to\nprevious sparse coding methods, RFNs yield sparser codes, capture the data's\ncovariance structure more precisely, and have a significantly smaller\nreconstruction error. We test RFNs as pretraining technique for deep networks\non different vision datasets, where RFNs were superior to RBMs and\nautoencoders. On gene expression data from two pharmaceutical drug discovery\nstudies, RFNs detected small and rare gene modules that revealed highly\nrelevant new biological insights which were so far missed by other unsupervised\nmethods.\n", "versions": [{"version": "v1", "created": "Mon, 23 Feb 2015 15:44:37 GMT"}, {"version": "v2", "created": "Thu, 11 Jun 2015 21:27:53 GMT"}], "update_date": "2018-01-31", "authors_parsed": [["Clevert", "Djork-Arn\u00e9", ""], ["Mayr", "Andreas", ""], ["Unterthiner", "Thomas", ""], ["Hochreiter", "Sepp", ""]]}, {"id": "1502.06470", "submitter": "Eric Tramel", "authors": "Eric W. Tramel and Ang\\'elique Dr\\'emeau and Florent Krzakala", "title": "Approximate Message Passing with Restricted Boltzmann Machine Priors", "comments": null, "journal-ref": "J. Stat. Mech. (2016) 073401", "doi": "10.1088/1742-5468/2016/07/073401", "report-no": null, "categories": "cs.IT cond-mat.dis-nn math.IT physics.data-an stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Approximate Message Passing (AMP) has been shown to be an excellent\nstatistical approach to signal inference and compressed sensing problem. The\nAMP framework provides modularity in the choice of signal prior; here we\npropose a hierarchical form of the Gauss-Bernouilli prior which utilizes a\nRestricted Boltzmann Machine (RBM) trained on the signal support to push\nreconstruction performance beyond that of simple iid priors for signals whose\nsupport can be well represented by a trained binary RBM. We present and analyze\ntwo methods of RBM factorization and demonstrate how these affect signal\nreconstruction performance within our proposed algorithm. Finally, using the\nMNIST handwritten digit dataset, we show experimentally that using an RBM\nallows AMP to approach oracle-support performance.\n", "versions": [{"version": "v1", "created": "Mon, 23 Feb 2015 15:51:07 GMT"}, {"version": "v2", "created": "Tue, 9 Jun 2015 14:05:45 GMT"}, {"version": "v3", "created": "Thu, 10 Dec 2015 03:45:32 GMT"}], "update_date": "2016-07-11", "authors_parsed": [["Tramel", "Eric W.", ""], ["Dr\u00e9meau", "Ang\u00e9lique", ""], ["Krzakala", "Florent", ""]]}, {"id": "1502.06531", "submitter": "Josip Djolonga", "authors": "Josip Djolonga and Andreas Krause", "title": "Scalable Variational Inference in Log-supermodular Models", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider the problem of approximate Bayesian inference in log-supermodular\nmodels. These models encompass regular pairwise MRFs with binary variables, but\nallow to capture high-order interactions, which are intractable for existing\napproximate inference techniques such as belief propagation, mean field, and\nvariants. We show that a recently proposed variational approach to inference in\nlog-supermodular models -L-FIELD- reduces to the widely-studied minimum norm\nproblem for submodular minimization. This insight allows to leverage powerful\nexisting tools, and hence to solve the variational problem orders of magnitude\nmore efficiently than previously possible. We then provide another natural\ninterpretation of L-FIELD, demonstrating that it exactly minimizes a specific\ntype of R\\'enyi divergence measure. This insight sheds light on the nature of\nthe variational approximations produced by L-FIELD. Furthermore, we show how to\nperform parallel inference as message passing in a suitable factor graph at a\nlinear convergence rate, without having to sum up over all the configurations\nof the factor. Finally, we apply our approach to a challenging image\nsegmentation task. Our experiments confirm scalability of our approach, high\nquality of the marginals, and the benefit of incorporating higher-order\npotentials.\n", "versions": [{"version": "v1", "created": "Mon, 23 Feb 2015 18:08:07 GMT"}, {"version": "v2", "created": "Tue, 24 Feb 2015 16:43:05 GMT"}], "update_date": "2015-02-25", "authors_parsed": [["Djolonga", "Josip", ""], ["Krause", "Andreas", ""]]}, {"id": "1502.06557", "submitter": "Florian Ziel", "authors": "Florian Ziel", "title": "Iteratively reweighted adaptive lasso for conditional heteroscedastic\n  time series with applications to AR-ARCH type processes", "comments": null, "journal-ref": "Computational Statistics & Data Analysis, 100 (2016) 773-793", "doi": "10.1016/j.csda.2015.11.016", "report-no": null, "categories": "stat.ME q-fin.CP stat.AP stat.CO stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Shrinkage algorithms are of great importance in almost every area of\nstatistics due to the increasing impact of big data. Especially time series\nanalysis benefits from efficient and rapid estimation techniques such as the\nlasso. However, currently lasso type estimators for autoregressive time series\nmodels still focus on models with homoscedastic residuals. Therefore, an\niteratively reweighted adaptive lasso algorithm for the estimation of time\nseries models under conditional heteroscedasticity is presented in a\nhigh-dimensional setting. The asymptotic behaviour of the resulting estimator\nis analysed. It is found that the proposed estimation procedure performs\nsubstantially better than its homoscedastic counterpart. A special case of the\nalgorithm is suitable to compute the estimated multivariate AR-ARCH type models\nefficiently. Extensions to the model like periodic AR-ARCH, threshold AR-ARCH\nor ARMA-GARCH are discussed. Finally, different simulation results and\napplications to electricity market data and returns of metal prices are shown.\n", "versions": [{"version": "v1", "created": "Mon, 23 Feb 2015 19:14:39 GMT"}, {"version": "v2", "created": "Sat, 5 Dec 2015 23:05:43 GMT"}], "update_date": "2016-06-01", "authors_parsed": [["Ziel", "Florian", ""]]}, {"id": "1502.06590", "submitter": "Yash Deshpande", "authors": "Yash Deshpande and Andrea Montanari", "title": "Improved Sum-of-Squares Lower Bounds for Hidden Clique and Hidden\n  Submatrix Problems", "comments": "40 pages, 1 table, conference", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CC cs.IT math.IT math.ST stat.ML stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Given a large data matrix $A\\in\\mathbb{R}^{n\\times n}$, we consider the\nproblem of determining whether its entries are i.i.d. with some known marginal\ndistribution $A_{ij}\\sim P_0$, or instead $A$ contains a principal submatrix\n$A_{{\\sf Q},{\\sf Q}}$ whose entries have marginal distribution $A_{ij}\\sim\nP_1\\neq P_0$. As a special case, the hidden (or planted) clique problem\nrequires to find a planted clique in an otherwise uniformly random graph.\n  Assuming unbounded computational resources, this hypothesis testing problem\nis statistically solvable provided $|{\\sf Q}|\\ge C \\log n$ for a suitable\nconstant $C$. However, despite substantial effort, no polynomial time algorithm\nis known that succeeds with high probability when $|{\\sf Q}| = o(\\sqrt{n})$.\nRecently Meka and Wigderson \\cite{meka2013association}, proposed a method to\nestablish lower bounds within the Sum of Squares (SOS) semidefinite hierarchy.\n  Here we consider the degree-$4$ SOS relaxation, and study the construction of\n\\cite{meka2013association} to prove that SOS fails unless $k\\ge C\\,\nn^{1/3}/\\log n$. An argument presented by Barak implies that this lower bound\ncannot be substantially improved unless the witness construction is changed in\nthe proof. Our proof uses the moments method to bound the spectrum of a certain\nrandom association scheme, i.e. a symmetric random matrix whose rows and\ncolumns are indexed by the edges of an Erd\\\"os-Renyi random graph.\n", "versions": [{"version": "v1", "created": "Mon, 23 Feb 2015 20:45:11 GMT"}], "update_date": "2015-02-24", "authors_parsed": [["Deshpande", "Yash", ""], ["Montanari", "Andrea", ""]]}, {"id": "1502.06626", "submitter": "Malik Magdon-Ismail", "authors": "Malik Magdon-Ismail, Christos Boutsidis", "title": "Optimal Sparse Linear Auto-Encoders and Sparse PCA", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI cs.IT math.IT stat.CO stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Principal components analysis (PCA) is the optimal linear auto-encoder of\ndata, and it is often used to construct features. Enforcing sparsity on the\nprincipal components can promote better generalization, while improving the\ninterpretability of the features. We study the problem of constructing optimal\nsparse linear auto-encoders. Two natural questions in such a setting are: i)\nGiven a level of sparsity, what is the best approximation to PCA that can be\nachieved? ii) Are there low-order polynomial-time algorithms which can\nasymptotically achieve this optimal tradeoff between the sparsity and the\napproximation quality?\n  In this work, we answer both questions by giving efficient low-order\npolynomial-time algorithms for constructing asymptotically \\emph{optimal}\nlinear auto-encoders (in particular, sparse features with near-PCA\nreconstruction error) and demonstrate the performance of our algorithms on real\ndata.\n", "versions": [{"version": "v1", "created": "Mon, 23 Feb 2015 21:06:39 GMT"}], "update_date": "2015-02-25", "authors_parsed": [["Magdon-Ismail", "Malik", ""], ["Boutsidis", "Christos", ""]]}, {"id": "1502.06644", "submitter": "Robert Vandermeulen", "authors": "Robert A. Vandermeulen and Clayton D. Scott", "title": "On The Identifiability of Mixture Models from Grouped Samples", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Finite mixture models are statistical models which appear in many problems in\nstatistics and machine learning. In such models it is assumed that data are\ndrawn from random probability measures, called mixture components, which are\nthemselves drawn from a probability measure P over probability measures. When\nestimating mixture models, it is common to make assumptions on the mixture\ncomponents, such as parametric assumptions. In this paper, we make no\nassumption on the mixture components, and instead assume that observations from\nthe mixture model are grouped, such that observations in the same group are\nknown to be drawn from the same component. We show that any mixture of m\nprobability measures can be uniquely identified provided there are 2m-1\nobservations per group. Moreover we show that, for any m, there exists a\nmixture of m probability measures that cannot be uniquely identified when\ngroups have 2m-2 observations. Our results hold for any sample space with more\nthan one element.\n", "versions": [{"version": "v1", "created": "Mon, 23 Feb 2015 22:24:26 GMT"}], "update_date": "2015-02-25", "authors_parsed": [["Vandermeulen", "Robert A.", ""], ["Scott", "Clayton D.", ""]]}, {"id": "1502.06689", "submitter": "Sonia Bhaskar", "authors": "Sonia Bhaskar and Adel Javanmard", "title": "1-Bit Matrix Completion under Exact Low-Rank Constraint", "comments": "6 pages, 3 figures, to appear in CISS 2015", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider the problem of noisy 1-bit matrix completion under an exact rank\nconstraint on the true underlying matrix $M^*$. Instead of observing a subset\nof the noisy continuous-valued entries of a matrix $M^*$, we observe a subset\nof noisy 1-bit (or binary) measurements generated according to a probabilistic\nmodel. We consider constrained maximum likelihood estimation of $M^*$, under a\nconstraint on the entry-wise infinity-norm of $M^*$ and an exact rank\nconstraint. This is in contrast to previous work which has used convex\nrelaxations for the rank. We provide an upper bound on the matrix estimation\nerror under this model. Compared to the existing results, our bound has faster\nconvergence rate with matrix dimensions when the fraction of revealed 1-bit\nobservations is fixed, independent of the matrix dimensions. We also propose an\niterative algorithm for solving our nonconvex optimization with a certificate\nof global optimality of the limiting point. This algorithm is based on low rank\nfactorization of $M^*$. We validate the method on synthetic and real data with\nimproved performance over existing methods.\n", "versions": [{"version": "v1", "created": "Tue, 24 Feb 2015 05:38:31 GMT"}], "update_date": "2015-02-25", "authors_parsed": [["Bhaskar", "Sonia", ""], ["Javanmard", "Adel", ""]]}, {"id": "1502.06800", "submitter": "Francis Bach", "authors": "Francis Bach (LIENS, SIERRA)", "title": "On the Equivalence between Kernel Quadrature Rules and Random Feature\n  Expansions", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG math.NA stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We show that kernel-based quadrature rules for computing integrals can be\nseen as a special case of random feature expansions for positive definite\nkernels, for a particular decomposition that always exists for such kernels. We\nprovide a theoretical analysis of the number of required samples for a given\napproximation error, leading to both upper and lower bounds that are based\nsolely on the eigenvalues of the associated integral operator and match up to\nlogarithmic terms. In particular, we show that the upper bound may be obtained\nfrom independent and identically distributed samples from a specific\nnon-uniform distribution, while the lower bound if valid for any set of points.\nApplying our results to kernel-based quadrature, while our results are fairly\ngeneral, we recover known upper and lower bounds for the special cases of\nSobolev spaces. Moreover, our results extend to the more general problem of\nfull function approximations (beyond simply computing an integral), with\nresults in L2- and L$\\infty$-norm that match known results for special cases.\nApplying our results to random features, we show an improvement of the number\nof random features needed to preserve the generalization guarantees for\nlearning with Lipschitz-continuous losses.\n", "versions": [{"version": "v1", "created": "Tue, 24 Feb 2015 13:12:51 GMT"}, {"version": "v2", "created": "Mon, 9 Nov 2015 14:29:04 GMT"}], "update_date": "2015-11-10", "authors_parsed": [["Bach", "Francis", "", "LIENS, SIERRA"]]}, {"id": "1502.06811", "submitter": "Ngoc Duong", "authors": "Ngoc Q. K. Duong, Hien-Thanh Duong (HUMG)", "title": "A Review of Audio Features and Statistical Models Exploited for Voice\n  Pattern Design", "comments": "http://www.iaria.org/conferences2015/PATTERNS15.html ; Seventh\n  International Conferences on Pervasive Patterns and Applications (PATTERNS\n  2015), Mar 2015, Nice, France", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.SD stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Audio fingerprinting, also named as audio hashing, has been well-known as a\npowerful technique to perform audio identification and synchronization. It\nbasically involves two major steps: fingerprint (voice pattern) design and\nmatching search. While the first step concerns the derivation of a robust and\ncompact audio signature, the second step usually requires knowledge about\ndatabase and quick-search algorithms. Though this technique offers a wide range\nof real-world applications, to the best of the authors' knowledge, a\ncomprehensive survey of existing algorithms appeared more than eight years ago.\nThus, in this paper, we present a more up-to-date review and, for emphasizing\non the audio signal processing aspect, we focus our state-of-the-art survey on\nthe fingerprint design step for which various audio features and their\ntractable statistical models are discussed.\n", "versions": [{"version": "v1", "created": "Tue, 24 Feb 2015 13:47:45 GMT"}], "update_date": "2015-02-25", "authors_parsed": [["Duong", "Ngoc Q. K.", "", "HUMG"], ["Duong", "Hien-Thanh", "", "HUMG"]]}, {"id": "1502.06895", "submitter": "Xiangyu Wang", "authors": "Xiangyu Wang, Chenlei Leng, David B. Dunson", "title": "On the consistency theory of high dimensional variable screening", "comments": "adding comments on REC", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST cs.LG stat.ML stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Variable screening is a fast dimension reduction technique for assisting high\ndimensional feature selection. As a preselection method, it selects a moderate\nsize subset of candidate variables for further refining via feature selection\nto produce the final model. The performance of variable screening depends on\nboth computational efficiency and the ability to dramatically reduce the number\nof variables without discarding the important ones. When the data dimension $p$\nis substantially larger than the sample size $n$, variable screening becomes\ncrucial as 1) Faster feature selection algorithms are needed; 2) Conditions\nguaranteeing selection consistency might fail to hold. This article studies a\nclass of linear screening methods and establishes consistency theory for this\nspecial class. In particular, we prove the restricted diagonally dominant (RDD)\ncondition is a necessary and sufficient condition for strong screening\nconsistency. As concrete examples, we show two screening methods $SIS$ and\n$HOLP$ are both strong screening consistent (subject to additional constraints)\nwith large probability if $n > O((\\rho s + \\sigma/\\tau)^2\\log p)$ under random\ndesigns. In addition, we relate the RDD condition to the irrepresentable\ncondition, and highlight limitations of $SIS$.\n", "versions": [{"version": "v1", "created": "Tue, 24 Feb 2015 17:52:20 GMT"}, {"version": "v2", "created": "Tue, 31 Mar 2015 18:38:42 GMT"}, {"version": "v3", "created": "Sat, 6 Jun 2015 07:33:02 GMT"}], "update_date": "2015-06-09", "authors_parsed": [["Wang", "Xiangyu", ""], ["Leng", "Chenlei", ""], ["Dunson", "David B.", ""]]}, {"id": "1502.06919", "submitter": "Jean Lafond", "authors": "Jean Lafond (LTCI)", "title": "Low Rank Matrix Completion with Exponential Family Noise", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.ML stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The matrix completion problem consists in reconstructing a matrix from a\nsample of entries, possibly observed with noise. A popular class of estimator,\nknown as nuclear norm penalized estimators, are based on minimizing the sum of\na data fitting term and a nuclear norm penalization. Here, we investigate the\ncase where the noise distribution belongs to the exponential family and is\nsub-exponential. Our framework alllows for a general sampling scheme. We first\nconsider an estimator defined as the minimizer of the sum of a log-likelihood\nterm and a nuclear norm penalization and prove an upper bound on the Frobenius\nprediction risk. The rate obtained improves on previous works on matrix\ncompletion for exponential family. When the sampling distribution is known, we\npropose another estimator and prove an oracle inequality w.r.t. the\nKullback-Leibler prediction risk, which translates immediatly into an upper\nbound on the Frobenius prediction risk. Finally, we show that all the rates\nobtained are minimax optimal up to a logarithmic factor.\n", "versions": [{"version": "v1", "created": "Tue, 24 Feb 2015 19:27:06 GMT"}, {"version": "v2", "created": "Mon, 20 Apr 2015 09:02:08 GMT"}], "update_date": "2015-04-21", "authors_parsed": [["Lafond", "Jean", "", "LTCI"]]}, {"id": "1502.06930", "submitter": "Oscar Hernan Madrid Padilla", "authors": "Oscar Hernan Madrid Padilla and James G. Scott", "title": "Tensor decomposition with generalized lasso penalties", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.CO stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present an approach for penalized tensor decomposition (PTD) that\nestimates smoothly varying latent factors in multi-way data. This generalizes\nexisting work on sparse tensor decomposition and penalized matrix\ndecompositions, in a manner parallel to the generalized lasso for regression\nand smoothing problems. Our approach presents many nontrivial challenges at the\nintersection of modeling and computation, which are studied in detail. An\nefficient coordinate-wise optimization algorithm for (PTD) is presented, and\nits convergence properties are characterized. The method is applied both to\nsimulated data and real data on flu hospitalizations in Texas. These results\nshow that our penalized tensor decomposition can offer major improvements on\nexisting methods for analyzing multi-way data that exhibit smooth spatial or\ntemporal features.\n", "versions": [{"version": "v1", "created": "Tue, 24 Feb 2015 20:03:05 GMT"}, {"version": "v2", "created": "Thu, 17 Dec 2015 05:19:53 GMT"}, {"version": "v3", "created": "Fri, 13 May 2016 01:40:38 GMT"}], "update_date": "2016-05-16", "authors_parsed": [["Padilla", "Oscar Hernan Madrid", ""], ["Scott", "James G.", ""]]}, {"id": "1502.06952", "submitter": "Wanjie Wang", "authors": "Jiashun Jin, Zheng Tracy Ke, Wanjie Wang", "title": "Phase Transitions for High Dimensional Clustering and Related Problems", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.ML stat.TH", "license": "http://creativecommons.org/publicdomain/zero/1.0/", "abstract": "  Consider a two-class clustering problem where we observe $X_i = \\ell_i \\mu +\nZ_i$, $Z_i \\stackrel{iid}{\\sim} N(0, I_p)$, $1 \\leq i \\leq n$. The feature\nvector $\\mu\\in R^p$ is unknown but is presumably sparse. The class labels\n$\\ell_i\\in\\{-1, 1\\}$ are also unknown and the main interest is to estimate\nthem.\n  We are interested in the statistical limits. In the two-dimensional phase\nspace calibrating the rarity and strengths of useful features, we find the\nprecise demarcation for the Region of Impossibility and Region of Possibility.\nIn the former, useful features are too rare/weak for successful clustering. In\nthe latter, useful features are strong enough to allow successful clustering.\nThe results are extended to the case of colored noise using Le Cam's idea on\ncomparison of experiments.\n  We also extend the study on statistical limits for clustering to that for\nsignal recovery and that for hypothesis testing. We compare the statistical\nlimits for three problems and expose some interesting insight.\n  We propose classical PCA and Important Features PCA (IF-PCA) for clustering.\nFor a threshold $t > 0$, IF-PCA clusters by applying classical PCA to all\ncolumns of $X$ with an $L^2$-norm larger than $t$. We also propose two\naggregation methods. For any parameter in the Region of Possibility, some of\nthese methods yield successful clustering. We find an interesting phase\ntransition for IF-PCA.\n  Our results require delicate analysis, especially on post-selection Random\nMatrix Theory and on lower bound arguments.\n", "versions": [{"version": "v1", "created": "Tue, 24 Feb 2015 20:58:44 GMT"}, {"version": "v2", "created": "Wed, 25 Mar 2015 16:52:44 GMT"}, {"version": "v3", "created": "Mon, 28 Dec 2015 21:20:45 GMT"}, {"version": "v4", "created": "Wed, 8 Jun 2016 19:29:50 GMT"}], "update_date": "2016-06-09", "authors_parsed": [["Jin", "Jiashun", ""], ["Ke", "Zheng Tracy", ""], ["Wang", "Wanjie", ""]]}, {"id": "1502.07017", "submitter": "Swayambhoo Jain", "authors": "Swayambhoo Jain and Jarvis Haupt", "title": "On Convolutional Approximations to Linear Dimensionality Reduction\n  Operators for Large Scale Data Processing", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we examine the problem of approximating a general linear\ndimensionality reduction (LDR) operator, represented as a matrix $A \\in\n\\mathbb{R}^{m \\times n}$ with $m < n$, by a partial circulant matrix with rows\nrelated by circular shifts. Partial circulant matrices admit fast\nimplementations via Fourier transform methods and subsampling operations; our\ninvestigation here is motivated by a desire to leverage these potential\ncomputational improvements in large-scale data processing tasks. We establish a\nfundamental result, that most large LDR matrices (whose row spaces are\nuniformly distributed) in fact cannot be well approximated by partial circulant\nmatrices. Then, we propose a natural generalization of the partial circulant\napproximation framework that entails approximating the range space of a given\nLDR operator $A$ over a restricted domain of inputs, using a matrix formed as a\nproduct of a partial circulant matrix having $m '> m$ rows and a $m \\times k$\n'post processing' matrix. We introduce a novel algorithmic technique, based on\nsparse matrix factorization, for identifying the factors comprising such\napproximations, and provide preliminary evidence to demonstrate the potential\nof this approach.\n", "versions": [{"version": "v1", "created": "Wed, 25 Feb 2015 00:45:41 GMT"}], "update_date": "2015-02-26", "authors_parsed": [["Jain", "Swayambhoo", ""], ["Haupt", "Jarvis", ""]]}, {"id": "1502.07097", "submitter": "Shahar Mendelson", "authors": "Shahar Mendelson", "title": "On aggregation for heavy-tailed classes", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.ML stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We introduce an alternative to the notion of `fast rate' in Learning Theory,\nwhich coincides with the optimal error rate when the given class happens to be\nconvex and regular in some sense. While it is well known that such a rate\ncannot always be attained by a learning procedure (i.e., a procedure that\nselects a function in the given class), we introduce an aggregation procedure\nthat attains that rate under rather minimal assumptions -- for example, that\nthe $L_q$ and $L_2$ norms are equivalent on the linear span of the class for\nsome $q>2$, and the target random variable is square-integrable.\n", "versions": [{"version": "v1", "created": "Wed, 25 Feb 2015 09:33:49 GMT"}], "update_date": "2015-02-26", "authors_parsed": [["Mendelson", "Shahar", ""]]}, {"id": "1502.07104", "submitter": "Tom Diethe", "authors": "Tom Diethe", "title": "A Note on the Kullback-Leibler Divergence for the von Mises-Fisher\n  distribution", "comments": "8 pages 1 figure", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a derivation of the Kullback Leibler (KL)-Divergence (also known\nas Relative Entropy) for the von Mises Fisher (VMF) Distribution in\n$d$-dimensions.\n", "versions": [{"version": "v1", "created": "Wed, 25 Feb 2015 10:08:34 GMT"}], "update_date": "2015-02-26", "authors_parsed": [["Diethe", "Tom", ""]]}, {"id": "1502.07190", "submitter": "Linda S. L. Tan", "authors": "Linda S. L. Tan, Aik Hui Chan and Tian Zheng", "title": "Topic-adjusted visibility metric for scientific articles", "comments": null, "journal-ref": "Annals of Applied Statistics, Volume 10, Number 1 (2016), 1-31", "doi": "10.1214/15-AOAS887", "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Measuring the impact of scientific articles is important for evaluating the\nresearch output of individual scientists, academic institutions and journals.\nWhile citations are raw data for constructing impact measures, there exist\nbiases and potential issues if factors affecting citation patterns are not\nproperly accounted for. In this work, we address the problem of field variation\nand introduce an article level metric useful for evaluating individual\narticles' visibility. This measure derives from joint probabilistic modeling of\nthe content in the articles and the citations amongst them using latent\nDirichlet allocation (LDA) and the mixed membership stochastic blockmodel\n(MMSB). Our proposed model provides a visibility metric for individual articles\nadjusted for field variation in citation rates, a structural understanding of\ncitation behavior in different fields, and article recommendations which take\ninto account article visibility and citation patterns. We develop an efficient\nalgorithm for model fitting using variational methods. To scale up to large\nnetworks, we develop an online variant using stochastic gradient methods and\ncase-control likelihood approximation. We apply our methods to the benchmark\nKDD Cup 2003 dataset with approximately 30,000 high energy physics papers.\n", "versions": [{"version": "v1", "created": "Wed, 25 Feb 2015 14:57:55 GMT"}, {"version": "v2", "created": "Sat, 13 Jun 2015 15:08:27 GMT"}, {"version": "v3", "created": "Fri, 16 Oct 2015 13:50:03 GMT"}], "update_date": "2019-04-24", "authors_parsed": [["Tan", "Linda S. L.", ""], ["Chan", "Aik Hui", ""], ["Zheng", "Tian", ""]]}, {"id": "1502.07229", "submitter": "Yiming Ying", "authors": "Yiming Ying and Ding-Xuan Zhou", "title": "Online Pairwise Learning Algorithms with Kernels", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Pairwise learning usually refers to a learning task which involves a loss\nfunction depending on pairs of examples, among which most notable ones include\nranking, metric learning and AUC maximization. In this paper, we study an\nonline algorithm for pairwise learning with a least-square loss function in an\nunconstrained setting of a reproducing kernel Hilbert space (RKHS), which we\nrefer to as the Online Pairwise lEaRning Algorithm (OPERA). In contrast to\nexisting works \\cite{Kar,Wang} which require that the iterates are restricted\nto a bounded domain or the loss function is strongly-convex, OPERA is\nassociated with a non-strongly convex objective function and learns the target\nfunction in an unconstrained RKHS. Specifically, we establish a general theorem\nwhich guarantees the almost surely convergence for the last iterate of OPERA\nwithout any assumptions on the underlying distribution. Explicit convergence\nrates are derived under the condition of polynomially decaying step sizes. We\nalso establish an interesting property for a family of widely-used kernels in\nthe setting of pairwise learning and illustrate the above convergence results\nusing such kernels. Our methodology mainly depends on the characterization of\nRKHSs using its associated integral operators and probability inequalities for\nrandom variables with values in a Hilbert space.\n", "versions": [{"version": "v1", "created": "Wed, 25 Feb 2015 16:26:33 GMT"}], "update_date": "2015-02-26", "authors_parsed": [["Ying", "Yiming", ""], ["Zhou", "Ding-Xuan", ""]]}, {"id": "1502.07334", "submitter": "Milad Kharratzadeh", "authors": "Milad Kharratzadeh, Mark Coates", "title": "Sparse Multivariate Factor Regression", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider the problem of multivariate regression in a setting where the\nrelevant predictors could be shared among different responses. We propose an\nalgorithm which decomposes the coefficient matrix into the product of a long\nmatrix and a wide matrix, with an elastic net penalty on the former and an\n$\\ell_1$ penalty on the latter. The first matrix linearly transforms the\npredictors to a set of latent factors, and the second one regresses the\nresponses on these factors. Our algorithm simultaneously performs dimension\nreduction and coefficient estimation and automatically estimates the number of\nlatent factors from the data. Our formulation results in a non-convex\noptimization problem, which despite its flexibility to impose effective\nlow-dimensional structure, is difficult, or even impossible, to solve exactly\nin a reasonable time. We specify an optimization algorithm based on alternating\nminimization with three different sets of updates to solve this non-convex\nproblem and provide theoretical results on its convergence and optimality.\nFinally, we demonstrate the effectiveness of our algorithm via experiments on\nsimulated and real data.\n", "versions": [{"version": "v1", "created": "Wed, 25 Feb 2015 20:40:30 GMT"}, {"version": "v2", "created": "Fri, 31 Jul 2015 14:35:38 GMT"}, {"version": "v3", "created": "Thu, 13 Aug 2015 20:47:59 GMT"}, {"version": "v4", "created": "Fri, 26 Feb 2016 17:16:03 GMT"}, {"version": "v5", "created": "Mon, 29 Feb 2016 22:23:46 GMT"}], "update_date": "2016-03-02", "authors_parsed": [["Kharratzadeh", "Milad", ""], ["Coates", "Mark", ""]]}, {"id": "1502.07645", "submitter": "Yu-Xiang Wang", "authors": "Yu-Xiang Wang, Stephen E. Fienberg, Alex Smola", "title": "Privacy for Free: Posterior Sampling and Stochastic Gradient Monte Carlo", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider the problem of Bayesian learning on sensitive datasets and\npresent two simple but somewhat surprising results that connect Bayesian\nlearning to \"differential privacy:, a cryptographic approach to protect\nindividual-level privacy while permiting database-level utility. Specifically,\nwe show that that under standard assumptions, getting one single sample from a\nposterior distribution is differentially private \"for free\". We will see that\nestimator is statistically consistent, near optimal and computationally\ntractable whenever the Bayesian model of interest is consistent, optimal and\ntractable. Similarly but separately, we show that a recent line of works that\nuse stochastic gradient for Hybrid Monte Carlo (HMC) sampling also preserve\ndifferentially privacy with minor or no modifications of the algorithmic\nprocedure at all, these observations lead to an \"anytime\" algorithm for\nBayesian learning under privacy constraint. We demonstrate that it performs\nmuch better than the state-of-the-art differential private methods on synthetic\nand real datasets.\n", "versions": [{"version": "v1", "created": "Thu, 26 Feb 2015 17:38:47 GMT"}, {"version": "v2", "created": "Sun, 12 Apr 2015 02:53:05 GMT"}], "update_date": "2015-04-14", "authors_parsed": [["Wang", "Yu-Xiang", ""], ["Fienberg", "Stephen E.", ""], ["Smola", "Alex", ""]]}, {"id": "1502.07685", "submitter": "Ryan Giordano", "authors": "Ryan Giordano, Tamara Broderick", "title": "Covariance Matrices and Influence Scores for Mean Field Variational\n  Bayes", "comments": "28 pages, 5 figures, submitted to ICML 2015", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Mean field variational Bayes (MFVB) is a popular posterior approximation\nmethod due to its fast runtime on large-scale data sets. However, it is well\nknown that a major failing of MFVB is that it underestimates the uncertainty of\nmodel variables (sometimes severely) and provides no information about model\nvariable covariance. We develop a fast, general methodology for exponential\nfamilies that augments MFVB to deliver accurate uncertainty estimates for model\nvariables -- both for individual variables and coherently across variables.\nMFVB for exponential families defines a fixed-point equation in the means of\nthe approximating posterior, and our approach yields a covariance estimate by\nperturbing this fixed point. Inspired by linear response theory, we call our\nmethod linear response variational Bayes (LRVB). We also show how LRVB can be\nused to quickly calculate a measure of the influence of individual data points\non parameter point estimates. We demonstrate the accuracy and scalability of\nour method by learning Gaussian mixture models for both simulated and real\ndata.\n", "versions": [{"version": "v1", "created": "Thu, 26 Feb 2015 19:08:39 GMT"}], "update_date": "2015-02-27", "authors_parsed": [["Giordano", "Ryan", ""], ["Broderick", "Tamara", ""]]}, {"id": "1502.07697", "submitter": "Sebastien Gerchinovitz", "authors": "Pierre Gaillard (GREGHEC, EDF R\\&D), S\\'ebastien Gerchinovitz (IMT,\n  UPS)", "title": "A Chaining Algorithm for Online Nonparametric Regression", "comments": "Published in the proceedings of COLT 2015:\n  http://jmlr.org/proceedings/papers/v40/Gaillard15.html", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider the problem of online nonparametric regression with arbitrary\ndeterministic sequences. Using ideas from the chaining technique, we design an\nalgorithm that achieves a Dudley-type regret bound similar to the one obtained\nin a non-constructive fashion by Rakhlin and Sridharan (2014). Our regret bound\nis expressed in terms of the metric entropy in the sup norm, which yields\noptimal guarantees when the metric and sequential entropies are of the same\norder of magnitude. In particular our algorithm is the first one that achieves\noptimal rates for online regression over H{\\\"o}lder balls. In addition we show\nfor this example how to adapt our chaining algorithm to get a reasonable\ncomputational efficiency with similar regret guarantees (up to a log factor).\n", "versions": [{"version": "v1", "created": "Thu, 26 Feb 2015 19:47:41 GMT"}, {"version": "v2", "created": "Wed, 1 Jul 2015 18:37:24 GMT"}], "update_date": "2015-07-02", "authors_parsed": [["Gaillard", "Pierre", "", "GREGHEC, EDF R\\&D"], ["Gerchinovitz", "S\u00e9bastien", "", "IMT,\n  UPS"]]}, {"id": "1502.07738", "submitter": "Jiaming Xu", "authors": "Bruce Hajek and Yihong Wu and Jiaming Xu", "title": "Achieving Exact Cluster Recovery Threshold via Semidefinite Programming:\n  Extensions", "comments": "This paper was accepted to IEEE Transactions on Information Theory on\n  April 25, 2016. The material was presented in part at the 2015 49th Asilomar\n  Conference on Signals, Systems and Computers and the 2015 IEEE Information\n  Theory Workshop. This work was also in part presented at the Workshop on\n  Community Detection, February 26-27, Institut Henri Poincar\\'e, Paris", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.SI math.PR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Resolving a conjecture of Abbe, Bandeira and Hall, the authors have recently\nshown that the semidefinite programming (SDP) relaxation of the maximum\nlikelihood estimator achieves the sharp threshold for exactly recovering the\ncommunity structure under the binary stochastic block model of two equal-sized\nclusters. The same was shown for the case of a single cluster and outliers.\nExtending the proof techniques, in this paper it is shown that SDP relaxations\nalso achieve the sharp recovery threshold in the following cases: (1) Binary\nstochastic block model with two clusters of sizes proportional to network size\nbut not necessarily equal; (2) Stochastic block model with a fixed number of\nequal-sized clusters; (3) Binary censored block model with the background graph\nbeing Erd\\H{o}s-R\\'enyi. Furthermore, a sufficient condition is given for an\nSDP procedure to achieve exact recovery for the general case of a fixed number\nof clusters plus outliers. These results demonstrate the versatility of SDP\nrelaxation as a simple, general purpose, computationally feasible methodology\nfor community detection.\n", "versions": [{"version": "v1", "created": "Thu, 26 Feb 2015 20:56:48 GMT"}, {"version": "v2", "created": "Tue, 26 May 2015 19:46:18 GMT"}, {"version": "v3", "created": "Wed, 15 Jun 2016 00:28:12 GMT"}], "update_date": "2016-06-16", "authors_parsed": [["Hajek", "Bruce", ""], ["Wu", "Yihong", ""], ["Xu", "Jiaming", ""]]}, {"id": "1502.07813", "submitter": "Parthan Kasarapu Mr", "authors": "Parthan Kasarapu and Lloyd Allison", "title": "Minimum message length estimation of mixtures of multivariate Gaussian\n  and von Mises-Fisher distributions", "comments": "46 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Mixture modelling involves explaining some observed evidence using a\ncombination of probability distributions. The crux of the problem is the\ninference of an optimal number of mixture components and their corresponding\nparameters. This paper discusses unsupervised learning of mixture models using\nthe Bayesian Minimum Message Length (MML) criterion. To demonstrate the\neffectiveness of search and inference of mixture parameters using the proposed\napproach, we select two key probability distributions, each handling\nfundamentally different types of data: the multivariate Gaussian distribution\nto address mixture modelling of data distributed in Euclidean space, and the\nmultivariate von Mises-Fisher (vMF) distribution to address mixture modelling\nof directional data distributed on a unit hypersphere. The key contributions of\nthis paper, in addition to the general search and inference methodology,\ninclude the derivation of MML expressions for encoding the data using\nmultivariate Gaussian and von Mises-Fisher distributions, and the analytical\nderivation of the MML estimates of the parameters of the two distributions. Our\napproach is tested on simulated and real world data sets. For instance, we\ninfer vMF mixtures that concisely explain experimentally determined\nthree-dimensional protein conformations, providing an effective null model\ndescription of protein structures that is central to many inference problems in\nstructural bioinformatics. The experimental results demonstrate that the\nperformance of our proposed search and inference method along with the encoding\nschemes improve on the state of the art mixture modelling techniques.\n", "versions": [{"version": "v1", "created": "Fri, 27 Feb 2015 03:32:49 GMT"}], "update_date": "2015-03-02", "authors_parsed": [["Kasarapu", "Parthan", ""], ["Allison", "Lloyd", ""]]}, {"id": "1502.07943", "submitter": "Ameet Talwalkar", "authors": "Kevin Jamieson, Ameet Talwalkar", "title": "Non-stochastic Best Arm Identification and Hyperparameter Optimization", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Motivated by the task of hyperparameter optimization, we introduce the\nnon-stochastic best-arm identification problem. Within the multi-armed bandit\nliterature, the cumulative regret objective enjoys algorithms and analyses for\nboth the non-stochastic and stochastic settings while to the best of our\nknowledge, the best-arm identification framework has only been considered in\nthe stochastic setting. We introduce the non-stochastic setting under this\nframework, identify a known algorithm that is well-suited for this setting, and\nanalyze its behavior. Next, by leveraging the iterative nature of standard\nmachine learning algorithms, we cast hyperparameter optimization as an instance\nof non-stochastic best-arm identification, and empirically evaluate our\nproposed algorithm on this task. Our empirical results show that, by allocating\nmore resources to promising hyperparameter settings, we typically achieve\ncomparable test accuracies an order of magnitude faster than baseline methods.\n", "versions": [{"version": "v1", "created": "Fri, 27 Feb 2015 15:58:45 GMT"}], "update_date": "2015-03-02", "authors_parsed": [["Jamieson", "Kevin", ""], ["Talwalkar", "Ameet", ""]]}, {"id": "1502.08009", "submitter": "Wouter Koolen", "authors": "Wouter M. Koolen and Tim van Erven", "title": "Second-order Quantile Methods for Experts and Combinatorial Games", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We aim to design strategies for sequential decision making that adjust to the\ndifficulty of the learning problem. We study this question both in the setting\nof prediction with expert advice, and for more general combinatorial decision\ntasks. We are not satisfied with just guaranteeing minimax regret rates, but we\nwant our algorithms to perform significantly better on easy data. Two popular\nways to formalize such adaptivity are second-order regret bounds and quantile\nbounds. The underlying notions of 'easy data', which may be paraphrased as \"the\nlearning problem has small variance\" and \"multiple decisions are useful\", are\nsynergetic. But even though there are sophisticated algorithms that exploit one\nof the two, no existing algorithm is able to adapt to both.\n  In this paper we outline a new method for obtaining such adaptive algorithms,\nbased on a potential function that aggregates a range of learning rates (which\nare essential tuning parameters). By choosing the right prior we construct\nefficient algorithms and show that they reap both benefits by proving the first\nbounds that are both second-order and incorporate quantiles.\n", "versions": [{"version": "v1", "created": "Fri, 27 Feb 2015 18:56:45 GMT"}], "update_date": "2015-03-02", "authors_parsed": [["Koolen", "Wouter M.", ""], ["van Erven", "Tim", ""]]}, {"id": "1502.08029", "submitter": "Li Yao", "authors": "Li Yao, Atousa Torabi, Kyunghyun Cho, Nicolas Ballas, Christopher Pal,\n  Hugo Larochelle, Aaron Courville", "title": "Describing Videos by Exploiting Temporal Structure", "comments": "Accepted to ICCV15. This version comes with code release and\n  supplementary material", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.AI cs.CL cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recent progress in using recurrent neural networks (RNNs) for image\ndescription has motivated the exploration of their application for video\ndescription. However, while images are static, working with videos requires\nmodeling their dynamic temporal structure and then properly integrating that\ninformation into a natural language description. In this context, we propose an\napproach that successfully takes into account both the local and global\ntemporal structure of videos to produce descriptions. First, our approach\nincorporates a spatial temporal 3-D convolutional neural network (3-D CNN)\nrepresentation of the short temporal dynamics. The 3-D CNN representation is\ntrained on video action recognition tasks, so as to produce a representation\nthat is tuned to human motion and behavior. Second we propose a temporal\nattention mechanism that allows to go beyond local temporal modeling and learns\nto automatically select the most relevant temporal segments given the\ntext-generating RNN. Our approach exceeds the current state-of-art for both\nBLEU and METEOR metrics on the Youtube2Text dataset. We also present results on\na new, larger and more challenging dataset of paired video and natural language\ndescriptions.\n", "versions": [{"version": "v1", "created": "Fri, 27 Feb 2015 19:30:40 GMT"}, {"version": "v2", "created": "Wed, 4 Mar 2015 17:24:47 GMT"}, {"version": "v3", "created": "Tue, 10 Mar 2015 15:27:08 GMT"}, {"version": "v4", "created": "Sat, 25 Apr 2015 20:32:27 GMT"}, {"version": "v5", "created": "Thu, 1 Oct 2015 00:12:46 GMT"}], "update_date": "2015-10-02", "authors_parsed": [["Yao", "Li", ""], ["Torabi", "Atousa", ""], ["Cho", "Kyunghyun", ""], ["Ballas", "Nicolas", ""], ["Pal", "Christopher", ""], ["Larochelle", "Hugo", ""], ["Courville", "Aaron", ""]]}, {"id": "1502.08053", "submitter": "Dominik Csiba", "authors": "Dominik Csiba, Zheng Qu, Peter Richt\\'arik", "title": "Stochastic Dual Coordinate Ascent with Adaptive Probabilities", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.OC cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper introduces AdaSDCA: an adaptive variant of stochastic dual\ncoordinate ascent (SDCA) for solving the regularized empirical risk\nminimization problems. Our modification consists in allowing the method\nadaptively change the probability distribution over the dual variables\nthroughout the iterative process. AdaSDCA achieves provably better complexity\nbound than SDCA with the best fixed probability distribution, known as\nimportance sampling. However, it is of a theoretical character as it is\nexpensive to implement. We also propose AdaSDCA+: a practical variant which in\nour experiments outperforms existing non-adaptive methods.\n", "versions": [{"version": "v1", "created": "Fri, 27 Feb 2015 20:54:03 GMT"}], "update_date": "2015-03-02", "authors_parsed": [["Csiba", "Dominik", ""], ["Qu", "Zheng", ""], ["Richt\u00e1rik", "Peter", ""]]}]