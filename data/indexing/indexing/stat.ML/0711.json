[{"id": "0711.0660", "submitter": "Hannes Leeb", "authors": "Benedikt M. Potscher, Hannes Leeb", "title": "On the Distribution of Penalized Maximum Likelihood Estimators: The\n  LASSO, SCAD, and Thresholding", "comments": null, "journal-ref": "J. Multivariate Anal. 100 (2009) 2065-2082", "doi": "10.1016/j.jmva.2009.06.010", "report-no": null, "categories": "math.ST stat.ME stat.ML stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study the distributions of the LASSO, SCAD, and thresholding estimators,\nin finite samples and in the large-sample limit. The asymptotic distributions\nare derived for both the case where the estimators are tuned to perform\nconsistent model selection and for the case where the estimators are tuned to\nperform conservative model selection. Our findings complement those of Knight\nand Fu (2000) and Fan and Li (2001). We show that the distributions are\ntypically highly nonnormal regardless of how the estimator is tuned, and that\nthis property persists in large samples. The uniform convergence rate of these\nestimators is also obtained, and is shown to be slower than 1/root(n) in case\nthe estimator is tuned to perform consistent model selection. An impossibility\nresult regarding estimation of the estimators' distribution function is also\nprovided.\n", "versions": [{"version": "v1", "created": "Mon, 5 Nov 2007 15:27:39 GMT"}, {"version": "v2", "created": "Fri, 17 Apr 2009 12:13:16 GMT"}], "update_date": "2009-09-07", "authors_parsed": [["Potscher", "Benedikt M.", ""], ["Leeb", "Hannes", ""]]}, {"id": "0711.0928", "submitter": "Alexey Koloydenko", "authors": "J. Lember, A. Koloydenko", "title": "Infinite Viterbi alignments in the two state hidden Markov models", "comments": "Several minor changes and corrections have been made in the arguments\n  as suggested by anonymous reviewers, which should hopefully improve\n  readability. Abstract has been added", "journal-ref": "Acta et Commentationes Universitatis Tartuensis de Mathematica,\n  Volume 12, 2008, pp. 109-124", "doi": null, "report-no": null, "categories": "math.ST math.PR stat.ML stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Since the early days of digital communication, Hidden Markov Models (HMMs)\nhave now been routinely used in speech recognition, processing of natural\nlanguages, images, and in bioinformatics. An HMM $(X_i,Y_i)_{i\\ge 1}$ assumes\nobservations $X_1,X_2,...$ to be conditionally independent given an\n\"explanotary\" Markov process $Y_1,Y_2,...$, which itself is not observed;\nmoreover, the conditional distribution of $X_i$ depends solely on $Y_i$.\nCentral to the theory and applications of HMM is the Viterbi algorithm to find\n{\\em a maximum a posteriori} estimate $q_{1:n}=(q_1,q_2,...,q_n)$ of $Y_{1:n}$\ngiven the observed data $x_{1:n}$. Maximum {\\em a posteriori} paths are also\ncalled Viterbi paths or alignments. Recently, attempts have been made to study\nthe behavior of Viterbi alignments of HMMs with two hidden states when $n$\ntends to infinity. It has indeed been shown that in some special cases a\nwell-defined limiting Viterbi alignment exists. While innovative, these\nattempts have relied on rather strong assumptions. This work proves the\nexistence of infinite Viterbi alignments for virtually any HMM with two hidden\nstates.\n", "versions": [{"version": "v1", "created": "Tue, 6 Nov 2007 17:34:18 GMT"}, {"version": "v2", "created": "Thu, 5 Feb 2009 21:03:57 GMT"}], "update_date": "2009-02-06", "authors_parsed": [["Lember", "J.", ""], ["Koloydenko", "A.", ""]]}, {"id": "0711.2434", "submitter": "Hemant Ishwaran", "authors": "Hemant Ishwaran", "title": "Variable importance in binary regression trees and forests", "comments": "Published in at http://dx.doi.org/10.1214/07-EJS039 the Electronic\n  Journal of Statistics (http://www.i-journals.org/ejs/) by the Institute of\n  Mathematical Statistics (http://www.imstat.org)", "journal-ref": "Electronic Journal of Statistics 2007, Vol. 1, 519-537", "doi": "10.1214/07-EJS039", "report-no": "IMS-EJS-EJS_2007_39", "categories": "stat.ML", "license": null, "abstract": "  We characterize and study variable importance (VIMP) and pairwise variable\nassociations in binary regression trees. A key component involves the node mean\nsquared error for a quantity we refer to as a maximal subtree. The theory\nnaturally extends from single trees to ensembles of trees and applies to\nmethods like random forests. This is useful because while importance values\nfrom random forests are used to screen variables, for example they are used to\nfilter high throughput genomic data in Bioinformatics, very little theory\nexists about their properties.\n", "versions": [{"version": "v1", "created": "Thu, 15 Nov 2007 15:09:41 GMT"}], "update_date": "2009-09-29", "authors_parsed": [["Ishwaran", "Hemant", ""]]}, {"id": "0711.4983", "submitter": "Longhai Li", "authors": "Longhai Li and Radford M. Neal", "title": "A Method for Compressing Parameters in Bayesian Models with Application\n  to Logistic Sequence Prediction Models", "comments": "29 pages", "journal-ref": "Bayesian Analysis, 2008, 3(4), 793-822", "doi": "10.1214/08-BA330", "report-no": null, "categories": "stat.ML stat.ME", "license": null, "abstract": "  Bayesian classification and regression with high order interactions is\nlargely infeasible because Markov chain Monte Carlo (MCMC) would need to be\napplied with a great many parameters, whose number increases rapidly with the\norder. In this paper we show how to make it feasible by effectively reducing\nthe number of parameters, exploiting the fact that many interactions have the\nsame values for all training cases. Our method uses a single ``compressed''\nparameter to represent the sum of all parameters associated with a set of\npatterns that have the same value for all training cases. Using symmetric\nstable distributions as the priors of the original parameters, we can easily\nfind the priors of these compressed parameters. We therefore need to deal only\nwith a much smaller number of compressed parameters when training the model\nwith MCMC. The number of compressed parameters may have converged before\nconsidering the highest possible order. After training the model, we can split\nthese compressed parameters into the original ones as needed to make\npredictions for test cases. We show in detail how to compress parameters for\nlogistic sequence prediction models. Experiments on both simulated and real\ndata demonstrate that a huge number of parameters can indeed be reduced by our\ncompression method.\n", "versions": [{"version": "v1", "created": "Fri, 30 Nov 2007 17:24:41 GMT"}], "update_date": "2017-04-28", "authors_parsed": [["Li", "Longhai", ""], ["Neal", "Radford M.", ""]]}]