[{"id": "1211.0025", "submitter": "Vladimir Vovk", "authors": "Vladimir Vovk and Ivan Petej", "title": "Venn-Abers predictors", "comments": "18 pages; to appear in the UAI 2014 Proceedings", "journal-ref": null, "doi": null, "report-no": "OCM07", "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper continues study, both theoretical and empirical, of the method of\nVenn prediction, concentrating on binary prediction problems. Venn predictors\nproduce probability-type predictions for the labels of test objects which are\nguaranteed to be well calibrated under the standard assumption that the\nobservations are generated independently from the same distribution. We give a\nsimple formalization and proof of this property. We also introduce Venn-Abers\npredictors, a new class of Venn predictors based on the idea of isotonic\nregression, and report promising empirical results both for Venn-Abers\npredictors and for their more computationally efficient simplified version.\n", "versions": [{"version": "v1", "created": "Wed, 31 Oct 2012 20:38:04 GMT"}, {"version": "v2", "created": "Sat, 21 Jun 2014 13:47:44 GMT"}], "update_date": "2014-06-24", "authors_parsed": [["Vovk", "Vladimir", ""], ["Petej", "Ivan", ""]]}, {"id": "1211.0028", "submitter": "Qirong Ho", "authors": "Qirong Ho, Rong Yan, Rajat Raina, Eric P. Xing", "title": "Understanding the Interaction between Interests, Conversations and\n  Friendships in Facebook", "comments": null, "journal-ref": null, "doi": null, "report-no": "CMU-ML-12-109", "categories": "cs.SI cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we explore salient questions about user interests,\nconversations and friendships in the Facebook social network, using a novel\nlatent space model that integrates several data types. A key challenge of\nstudying Facebook's data is the wide range of data modalities such as text,\nnetwork links, and categorical labels. Our latent space model seamlessly\ncombines all three data modalities over millions of users, allowing us to study\nthe interplay between user friendships, interests, and higher-order\nnetwork-wide social trends on Facebook. The recovered insights not only answer\nour initial questions, but also reveal surprising facts about user interests in\nthe context of Facebook's ecosystem. We also confirm that our results are\nsignificant with respect to evidential information from the study subjects.\n", "versions": [{"version": "v1", "created": "Wed, 31 Oct 2012 20:56:16 GMT"}], "update_date": "2012-11-02", "authors_parsed": [["Ho", "Qirong", ""], ["Yan", "Rong", ""], ["Raina", "Rajat", ""], ["Xing", "Eric P.", ""]]}, {"id": "1211.0056", "submitter": "Zhaosong Lu", "authors": "Zhaosong Lu", "title": "Iterative Hard Thresholding Methods for $l_0$ Regularized Convex Cone\n  Programming", "comments": "25 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.OC cs.LG math.NA stat.CO stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper we consider $l_0$ regularized convex cone programming problems.\nIn particular, we first propose an iterative hard thresholding (IHT) method and\nits variant for solving $l_0$ regularized box constrained convex programming.\nWe show that the sequence generated by these methods converges to a local\nminimizer. Also, we establish the iteration complexity of the IHT method for\nfinding an $\\epsilon$-local-optimal solution. We then propose a method for\nsolving $l_0$ regularized convex cone programming by applying the IHT method to\nits quadratic penalty relaxation and establish its iteration complexity for\nfinding an $\\epsilon$-approximate local minimizer. Finally, we propose a\nvariant of this method in which the associated penalty parameter is dynamically\nupdated, and show that every accumulation point is a local minimizer of the\nproblem.\n", "versions": [{"version": "v1", "created": "Wed, 31 Oct 2012 23:47:04 GMT"}, {"version": "v2", "created": "Fri, 2 Nov 2012 04:04:35 GMT"}], "update_date": "2012-11-05", "authors_parsed": [["Lu", "Zhaosong", ""]]}, {"id": "1211.0174", "submitter": "Aki Vehtari", "authors": "Jaakko Riihim\\\"aki and Aki Vehtari", "title": "Laplace approximation for logistic Gaussian process density estimation\n  and regression", "comments": "The v2 and v3 files are the same, but the v3 metadata now has the\n  correct title and abstract", "journal-ref": "Bayesian analysis, 9(2):425-448, 2014", "doi": null, "report-no": null, "categories": "stat.CO stat.ME stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Logistic Gaussian process (LGP) priors provide a flexible alternative for\nmodelling unknown densities. The smoothness properties of the density estimates\ncan be controlled through the prior covariance structure of the LGP, but the\nchallenge is the analytically intractable inference. In this paper, we present\napproximate Bayesian inference for LGP density estimation in a grid using\nLaplace's method to integrate over the non-Gaussian posterior distribution of\nlatent function values and to determine the covariance function parameters with\ntype-II maximum a posteriori (MAP) estimation. We demonstrate that Laplace's\nmethod with MAP is sufficiently fast for practical interactive visualisation of\n1D and 2D densities. Our experiments with simulated and real 1D data sets show\nthat the estimation accuracy is close to a Markov chain Monte Carlo\napproximation and state-of-the-art hierarchical infinite Gaussian mixture\nmodels. We also construct a reduced-rank approximation to speed up the\ncomputations for dense 2D grids, and demonstrate density regression with the\nproposed Laplace approach.\n", "versions": [{"version": "v1", "created": "Thu, 1 Nov 2012 13:31:17 GMT"}, {"version": "v2", "created": "Fri, 4 Oct 2013 14:19:51 GMT"}, {"version": "v3", "created": "Mon, 7 Oct 2013 10:57:33 GMT"}], "update_date": "2016-11-01", "authors_parsed": [["Riihim\u00e4ki", "Jaakko", ""], ["Vehtari", "Aki", ""]]}, {"id": "1211.0358", "submitter": "Andreas Damianou Mr", "authors": "Andreas C. Damianou, Neil D. Lawrence", "title": "Deep Gaussian Processes", "comments": "9 pages, 8 figures. Appearing in Proceedings of the 16th\n  International Conference on Artificial Intelligence and Statistics (AISTATS)\n  2013", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG math.PR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper we introduce deep Gaussian process (GP) models. Deep GPs are a\ndeep belief network based on Gaussian process mappings. The data is modeled as\nthe output of a multivariate GP. The inputs to that Gaussian process are then\ngoverned by another GP. A single layer model is equivalent to a standard GP or\nthe GP latent variable model (GP-LVM). We perform inference in the model by\napproximate variational marginalization. This results in a strict lower bound\non the marginal likelihood of the model which we use for model selection\n(number of layers and nodes per layer). Deep belief networks are typically\napplied to relatively large data sets using stochastic gradient descent for\noptimization. Our fully Bayesian treatment allows for the application of deep\nmodels even when data is scarce. Model selection by our variational bound shows\nthat a five layer hierarchy is justified even when modelling a digit data set\ncontaining only 150 examples.\n", "versions": [{"version": "v1", "created": "Fri, 2 Nov 2012 03:13:08 GMT"}, {"version": "v2", "created": "Sat, 23 Mar 2013 01:23:34 GMT"}], "update_date": "2013-03-26", "authors_parsed": [["Damianou", "Andreas C.", ""], ["Lawrence", "Neil D.", ""]]}, {"id": "1211.0373", "submitter": "Vincent Q. Vu", "authors": "Vincent Q. Vu, Jing Lei", "title": "Minimax sparse principal subspace estimation in high dimensions", "comments": "Published in at http://dx.doi.org/10.1214/13-AOS1151 the Annals of\n  Statistics (http://www.imstat.org/aos/) by the Institute of Mathematical\n  Statistics (http://www.imstat.org)", "journal-ref": "Annals of Statistics 2013, Vol. 41, No. 6, 2905-2947", "doi": "10.1214/13-AOS1151", "report-no": "IMS-AOS-AOS1151", "categories": "math.ST stat.ML stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study sparse principal components analysis in high dimensions, where $p$\n(the number of variables) can be much larger than $n$ (the number of\nobservations), and analyze the problem of estimating the subspace spanned by\nthe principal eigenvectors of the population covariance matrix. We introduce\ntwo complementary notions of $\\ell_q$ subspace sparsity: row sparsity and\ncolumn sparsity. We prove nonasymptotic lower and upper bounds on the minimax\nsubspace estimation error for $0\\leq q\\leq1$. The bounds are optimal for row\nsparse subspaces and nearly optimal for column sparse subspaces, they apply to\ngeneral classes of covariance matrices, and they show that $\\ell_q$ constrained\nestimates can achieve optimal minimax rates without restrictive spiked\ncovariance conditions. Interestingly, the form of the rates matches known\nresults for sparse regression when the effective noise variance is defined\nappropriately. Our proof employs a novel variational $\\sin\\Theta$ theorem that\nmay be useful in other regularized spectral estimation problems.\n", "versions": [{"version": "v1", "created": "Fri, 2 Nov 2012 06:05:47 GMT"}, {"version": "v2", "created": "Mon, 10 Jun 2013 20:58:42 GMT"}, {"version": "v3", "created": "Wed, 12 Jun 2013 01:59:54 GMT"}, {"version": "v4", "created": "Fri, 3 Jan 2014 07:00:33 GMT"}], "update_date": "2014-01-06", "authors_parsed": [["Vu", "Vincent Q.", ""], ["Lei", "Jing", ""]]}, {"id": "1211.0439", "submitter": "Peter Sollich", "authors": "Simon R. F. Ashton and Peter Sollich", "title": "Learning curves for multi-task Gaussian process regression", "comments": "9 pages, to appear in Advances in Neural Information Processing\n  Systems 25", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cond-mat.dis-nn stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study the average case performance of multi-task Gaussian process (GP)\nregression as captured in the learning curve, i.e. the average Bayes error for\na chosen task versus the total number of examples $n$ for all tasks. For GP\ncovariances that are the product of an input-dependent covariance function and\na free-form inter-task covariance matrix, we show that accurate approximations\nfor the learning curve can be obtained for an arbitrary number of tasks $T$. We\nuse these to study the asymptotic learning behaviour for large $n$.\nSurprisingly, multi-task learning can be asymptotically essentially useless, in\nthe sense that examples from other tasks help only when the degree of\ninter-task correlation, $\\rho$, is near its maximal value $\\rho=1$. This effect\nis most extreme for learning of smooth target functions as described by e.g.\nsquared exponential kernels. We also demonstrate that when learning many tasks,\nthe learning curves separate into an initial phase, where the Bayes error on\neach task is reduced down to a plateau value by \"collective learning\" even\nthough most tasks have not seen examples, and a final decay that occurs once\nthe number of examples is proportional to the number of tasks.\n", "versions": [{"version": "v1", "created": "Fri, 2 Nov 2012 12:46:24 GMT"}], "update_date": "2012-11-05", "authors_parsed": [["Ashton", "Simon R. F.", ""], ["Sollich", "Peter", ""]]}, {"id": "1211.0587", "submitter": "Joel Veness", "authors": "Joel Veness, Martha White, Michael Bowling, Andr\\'as Gy\\\"orgy", "title": "Partition Tree Weighting", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IT cs.LG math.IT stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper introduces the Partition Tree Weighting technique, an efficient\nmeta-algorithm for piecewise stationary sources. The technique works by\nperforming Bayesian model averaging over a large class of possible partitions\nof the data into locally stationary segments. It uses a prior, closely related\nto the Context Tree Weighting technique of Willems, that is well suited to data\ncompression applications. Our technique can be applied to any coding\ndistribution at an additional time and space cost only logarithmic in the\nsequence length. We provide a competitive analysis of the redundancy of our\nmethod, and explore its application in a variety of settings. The order of the\nredundancy and the complexity of our algorithm matches those of the best\ncompetitors available in the literature, and the new algorithm exhibits a\nsuperior complexity-performance trade-off in our experiments.\n", "versions": [{"version": "v1", "created": "Sat, 3 Nov 2012 00:41:46 GMT"}, {"version": "v2", "created": "Wed, 21 Nov 2012 12:52:44 GMT"}], "update_date": "2012-11-22", "authors_parsed": [["Veness", "Joel", ""], ["White", "Martha", ""], ["Bowling", "Michael", ""], ["Gy\u00f6rgy", "Andr\u00e1s", ""]]}, {"id": "1211.0632", "submitter": "Hua Ouyang", "authors": "Hua Ouyang, Niao He, Alexander Gray", "title": "Stochastic ADMM for Nonsmooth Optimization", "comments": "A short version of this paper appears in the 5th NIPS Workshop on\n  Optimization for Machine Learning, Lake Tahoe, Nevada, USA, 2012", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG math.OC stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a stochastic setting for optimization problems with nonsmooth\nconvex separable objective functions over linear equality constraints. To solve\nsuch problems, we propose a stochastic Alternating Direction Method of\nMultipliers (ADMM) algorithm. Our algorithm applies to a more general class of\nnonsmooth convex functions that does not necessarily have a closed-form\nsolution by minimizing the augmented function directly. We also demonstrate the\nrates of convergence for our algorithm under various structural assumptions of\nthe stochastic functions: $O(1/\\sqrt{t})$ for convex functions and $O(\\log\nt/t)$ for strongly convex functions. Compared to previous literature, we\nestablish the convergence rate of ADMM algorithm, for the first time, in terms\nof both the objective value and the feasibility violation.\n", "versions": [{"version": "v1", "created": "Sat, 3 Nov 2012 19:05:56 GMT"}, {"version": "v2", "created": "Tue, 22 Jan 2013 17:07:37 GMT"}], "update_date": "2013-01-23", "authors_parsed": [["Ouyang", "Hua", ""], ["He", "Niao", ""], ["Gray", "Alexander", ""]]}, {"id": "1211.0757", "submitter": "Ju Sun", "authors": "Ju Sun, Yuqian Zhang, John Wright", "title": "Efficient Point-to-Subspace Query in $\\ell^1$: Theory and Applications\n  in Computer Vision", "comments": "To appear in NIPS workshop on big learning, 2012", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.CV stat.AP", "license": "http://creativecommons.org/licenses/by/3.0/", "abstract": "  Motivated by vision tasks such as robust face and object recognition, we\nconsider the following general problem: given a collection of low-dimensional\nlinear subspaces in a high-dimensional ambient (image) space and a query point\n(image), efficiently determine the nearest subspace to the query in $\\ell^1$\ndistance. We show in theory that Cauchy random embedding of the objects into\nsignificantly-lower-dimensional spaces helps preserve the identity of the\nnearest subspace with constant probability. This offers the possibility of\nefficiently selecting several candidates for accurate search. We sketch\npreliminary experiments on robust face and digit recognition to corroborate our\ntheory.\n", "versions": [{"version": "v1", "created": "Mon, 5 Nov 2012 04:15:25 GMT"}], "update_date": "2012-11-06", "authors_parsed": [["Sun", "Ju", ""], ["Zhang", "Yuqian", ""], ["Wright", "John", ""]]}, {"id": "1211.0801", "submitter": "Ming Yuan", "authors": "Ming Yuan", "title": "Discussion: Latent variable graphical model selection via convex\n  optimization", "comments": "Published in at http://dx.doi.org/10.1214/12-AOS979 the Annals of\n  Statistics (http://www.imstat.org/aos/) by the Institute of Mathematical\n  Statistics (http://www.imstat.org)", "journal-ref": "Annals of Statistics 2012, Vol. 40, No. 4, 1968-1972", "doi": "10.1214/12-AOS979", "report-no": "IMS-AOS-AOS979", "categories": "math.ST cs.LG stat.ML stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Discussion of \"Latent variable graphical model selection via convex\noptimization\" by Venkat Chandrasekaran, Pablo A. Parrilo and Alan S. Willsky\n[arXiv:1008.1290].\n", "versions": [{"version": "v1", "created": "Mon, 5 Nov 2012 09:36:40 GMT"}], "update_date": "2012-11-06", "authors_parsed": [["Yuan", "Ming", ""]]}, {"id": "1211.0806", "submitter": "Nicolai Meinshausen", "authors": "Steffen Lauritzen, Nicolai Meinshausen", "title": "Discussion: Latent variable graphical model selection via convex\n  optimization", "comments": "Published in at http://dx.doi.org/10.1214/12-AOS980 the Annals of\n  Statistics (http://www.imstat.org/aos/) by the Institute of Mathematical\n  Statistics (http://www.imstat.org)", "journal-ref": "Annals of Statistics 2012, Vol. 40, No. 4, 1973-1977", "doi": "10.1214/12-AOS980", "report-no": "IMS-AOS-AOS980", "categories": "math.ST cs.LG stat.ML stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Discussion of \"Latent variable graphical model selection via convex\noptimization\" by Venkat Chandrasekaran, Pablo A. Parrilo and Alan S. Willsky\n[arXiv:1008.1290].\n", "versions": [{"version": "v1", "created": "Mon, 5 Nov 2012 09:51:07 GMT"}], "update_date": "2012-11-06", "authors_parsed": [["Lauritzen", "Steffen", ""], ["Meinshausen", "Nicolai", ""]]}, {"id": "1211.0808", "submitter": "Martin J. Wainwright", "authors": "Martin J. Wainwright", "title": "Discussion: Latent variable graphical model selection via convex\n  optimization", "comments": "Published in at http://dx.doi.org/10.1214/12-AOS981 the Annals of\n  Statistics (http://www.imstat.org/aos/) by the Institute of Mathematical\n  Statistics (http://www.imstat.org)", "journal-ref": "Annals of Statistics 2012, Vol. 40, No. 4, 1978-1983", "doi": "10.1214/12-AOS981", "report-no": "IMS-AOS-AOS981", "categories": "math.ST cs.LG stat.ML stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Discussion of \"Latent variable graphical model selection via convex\noptimization\" by Venkat Chandrasekaran, Pablo A. Parrilo and Alan S. Willsky\n[arXiv:1008.1290].\n", "versions": [{"version": "v1", "created": "Mon, 5 Nov 2012 09:59:33 GMT"}], "update_date": "2012-11-06", "authors_parsed": [["Wainwright", "Martin J.", ""]]}, {"id": "1211.0817", "submitter": "Emmanuel J. Cand\\'{e}s", "authors": "Emmanuel J. Cand\\'es, Mahdi Soltanolkotabi", "title": "Discussion: Latent variable graphical model selection via convex\n  optimization", "comments": "Published in at http://dx.doi.org/10.1214/12-AOS1001 the Annals of\n  Statistics (http://www.imstat.org/aos/) by the Institute of Mathematical\n  Statistics (http://www.imstat.org)", "journal-ref": "Annals of Statistics 2012, Vol. 40, No. 4, 1997-2004", "doi": "10.1214/12-AOS1001", "report-no": "IMS-AOS-AOS1001", "categories": "math.ST cs.LG stat.ML stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Discussion of \"Latent variable graphical model selection via convex\noptimization\" by Venkat Chandrasekaran, Pablo A. Parrilo and Alan S. Willsky\n[arXiv:1008.1290].\n", "versions": [{"version": "v1", "created": "Mon, 5 Nov 2012 10:32:57 GMT"}], "update_date": "2012-11-06", "authors_parsed": [["Cand\u00e9s", "Emmanuel J.", ""], ["Soltanolkotabi", "Mahdi", ""]]}, {"id": "1211.0835", "submitter": "Venkat Chandrasekaran", "authors": "Venkat Chandrasekaran, Pablo A. Parrilo, Alan S. Willsky", "title": "Rejoinder: Latent variable graphical model selection via convex\n  optimization", "comments": "Published in at http://dx.doi.org/10.1214/12-AOS1020 the Annals of\n  Statistics (http://www.imstat.org/aos/) by the Institute of Mathematical\n  Statistics (http://www.imstat.org)", "journal-ref": "Annals of Statistics 2012, Vol. 40, No. 4, 2005-2013", "doi": "10.1214/12-AOS1020", "report-no": "IMS-AOS-AOS1020", "categories": "math.ST cs.LG stat.ML stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Rejoinder to \"Latent variable graphical model selection via convex\noptimization\" by Venkat Chandrasekaran, Pablo A. Parrilo and Alan S. Willsky\n[arXiv:1008.1290].\n", "versions": [{"version": "v1", "created": "Mon, 5 Nov 2012 11:33:03 GMT"}], "update_date": "2012-11-06", "authors_parsed": [["Chandrasekaran", "Venkat", ""], ["Parrilo", "Pablo A.", ""], ["Willsky", "Alan S.", ""]]}, {"id": "1211.0879", "submitter": "Yanshan Shi", "authors": "Yanshan Shi", "title": "Comparing K-Nearest Neighbors and Potential Energy Method in\n  classification problem. A case study using KNN applet by E.M. Mirkes and real\n  life benchmark data sets", "comments": "23 pages, 27 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://creativecommons.org/licenses/by/3.0/", "abstract": "  K-nearest neighbors (KNN) method is used in many supervised learning\nclassification problems. Potential Energy (PE) method is also developed for\nclassification problems based on its physical metaphor. The energy potential\nused in the experiments are Yukawa potential and Gaussian Potential. In this\npaper, I use both applet and MATLAB program with real life benchmark data to\nanalyze the performances of KNN and PE method in classification problems. The\nresults show that in general, KNN and PE methods have similar performance. In\nparticular, PE with Yukawa potential has worse performance than KNN when the\ndensity of the data is higher in the distribution of the database. When the\nGaussian potential is applied, the results from PE and KNN have similar\nbehavior. The indicators used are correlation coefficients and information\ngain.\n", "versions": [{"version": "v1", "created": "Mon, 5 Nov 2012 14:48:15 GMT"}], "update_date": "2012-11-06", "authors_parsed": [["Shi", "Yanshan", ""]]}, {"id": "1211.0889", "submitter": "Yi Yu", "authors": "Yi Yu and Yang Feng", "title": "APPLE: Approximate Path for Penalized Likelihood Estimators", "comments": "24 pages, 9 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In high-dimensional data analysis, penalized likelihood estimators are shown\nto provide superior results in both variable selection and parameter\nestimation. A new algorithm, APPLE, is proposed for calculating the Approximate\nPath for Penalized Likelihood Estimators. Both the convex penalty (such as\nLASSO) and the nonconvex penalty (such as SCAD and MCP) cases are considered.\nThe APPLE efficiently computes the solution path for the penalized likelihood\nestimator using a hybrid of the modified predictor-corrector method and the\ncoordinate-descent algorithm. APPLE is compared with several well-known\npackages via simulation and analysis of two gene expression data sets.\n", "versions": [{"version": "v1", "created": "Fri, 2 Nov 2012 07:42:54 GMT"}, {"version": "v2", "created": "Thu, 2 May 2013 14:53:21 GMT"}, {"version": "v3", "created": "Sat, 4 May 2013 06:22:23 GMT"}], "update_date": "2013-05-07", "authors_parsed": [["Yu", "Yi", ""], ["Feng", "Yang", ""]]}, {"id": "1211.0906", "submitter": "Frank Hutter", "authors": "Frank Hutter, Lin Xu, Holger H. Hoos, Kevin Leyton-Brown", "title": "Algorithm Runtime Prediction: Methods & Evaluation", "comments": "51 pages, 13 figures, 8 tables. Added references, feature cost, and\n  experiments with subsets of features; reworded Sections 1&2", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.LG cs.PF stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Perhaps surprisingly, it is possible to predict how long an algorithm will\ntake to run on a previously unseen input, using machine learning techniques to\nbuild a model of the algorithm's runtime as a function of problem-specific\ninstance features. Such models have important applications to algorithm\nanalysis, portfolio-based algorithm selection, and the automatic configuration\nof parameterized algorithms. Over the past decade, a wide variety of techniques\nhave been studied for building such models. Here, we describe extensions and\nimprovements of existing models, new families of models, and -- perhaps most\nimportantly -- a much more thorough treatment of algorithm parameters as model\ninputs. We also comprehensively describe new and existing features for\npredicting algorithm runtime for propositional satisfiability (SAT), travelling\nsalesperson (TSP) and mixed integer programming (MIP) problems. We evaluate\nthese innovations through the largest empirical analysis of its kind, comparing\nto a wide range of runtime modelling techniques from the literature. Our\nexperiments consider 11 algorithms and 35 instance distributions; they also\nspan a very wide range of SAT, MIP, and TSP instances, with the least\nstructured having been generated uniformly at random and the most structured\nhaving emerged from real industrial applications. Overall, we demonstrate that\nour new models yield substantially better runtime predictions than previous\napproaches in terms of their generalization to new problem instances, to new\nalgorithms from a parameterized space, and to both simultaneously.\n", "versions": [{"version": "v1", "created": "Mon, 5 Nov 2012 16:15:16 GMT"}, {"version": "v2", "created": "Sat, 26 Oct 2013 09:00:50 GMT"}], "update_date": "2013-10-29", "authors_parsed": [["Hutter", "Frank", ""], ["Xu", "Lin", ""], ["Hoos", "Holger H.", ""], ["Leyton-Brown", "Kevin", ""]]}, {"id": "1211.0919", "submitter": "Majid Janzamin", "authors": "Majid Janzamin, Animashree Anandkumar", "title": "High-Dimensional Covariance Decomposition into Sparse Markov and\n  Independence Models", "comments": "42 pages. Experiments are updated, and the optimization program is\n  implemented by an ADMM technique which makes it faster", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Fitting high-dimensional data involves a delicate tradeoff between faithful\nrepresentation and the use of sparse models. Too often, sparsity assumptions on\nthe fitted model are too restrictive to provide a faithful representation of\nthe observed data. In this paper, we present a novel framework incorporating\nsparsity in different domains.We decompose the observed covariance matrix into\na sparse Gaussian Markov model (with a sparse precision matrix) and a sparse\nindependence model (with a sparse covariance matrix). Our framework\nincorporates sparse covariance and sparse precision estimation as special cases\nand thus introduces a richer class of high-dimensional models. We characterize\nsufficient conditions for identifiability of the two models, \\viz Markov and\nindependence models. We propose an efficient decomposition method based on a\nmodification of the popular $\\ell_1$-penalized maximum-likelihood estimator\n($\\ell_1$-MLE). We establish that our estimator is consistent in both the\ndomains, i.e., it successfully recovers the supports of both Markov and\nindependence models, when the number of samples $n$ scales as $n = \\Omega(d^2\n\\log p)$, where $p$ is the number of variables and $d$ is the maximum node\ndegree in the Markov model. Our experiments validate these results and also\ndemonstrate that our models have better inference accuracy under simple\nalgorithms such as loopy belief propagation.\n", "versions": [{"version": "v1", "created": "Mon, 5 Nov 2012 16:54:44 GMT"}, {"version": "v2", "created": "Sat, 14 Dec 2013 07:46:48 GMT"}], "update_date": "2013-12-17", "authors_parsed": [["Janzamin", "Majid", ""], ["Anandkumar", "Animashree", ""]]}, {"id": "1211.0932", "submitter": "Guido F.  Montufar", "authors": "Guido F. Montufar and Jason Morton", "title": "Kernels and Submodels of Deep Belief Networks", "comments": "13 pages, 4 figures, deep learning and unsupervised feature learning\n  nips workshop 2012", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study the mixtures of factorizing probability distributions represented as\nvisible marginal distributions in stochastic layered networks. We take the\nperspective of kernel transitions of distributions, which gives a unified\npicture of distributed representations arising from Deep Belief Networks (DBN)\nand other networks without lateral connections. We describe combinatorial and\ngeometric properties of the set of kernels and products of kernels realizable\nby DBNs as the network parameters vary. We describe explicit classes of\nprobability distributions, including exponential families, that can be learned\nby DBNs. We use these submodels to bound the maximal and the expected\nKullback-Leibler approximation errors of DBNs from above depending on the\nnumber of hidden layers and units that they contain.\n", "versions": [{"version": "v1", "created": "Mon, 5 Nov 2012 17:13:51 GMT"}], "update_date": "2012-11-06", "authors_parsed": [["Montufar", "Guido F.", ""], ["Morton", "Jason", ""]]}, {"id": "1211.1043", "submitter": "Jose Hernandez-Orallo", "authors": "Jose Hernandez-Orallo", "title": "Soft (Gaussian CDE) regression models and loss functions", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Regression, unlike classification, has lacked a comprehensive and effective\napproach to deal with cost-sensitive problems by the reuse (and not a\nre-training) of general regression models. In this paper, a wide variety of\ncost-sensitive problems in regression (such as bids, asymmetric losses and\nrejection rules) can be solved effectively by a lightweight but powerful\napproach, consisting of: (1) the conversion of any traditional one-parameter\ncrisp regression model into a two-parameter soft regression model, seen as a\nnormal conditional density estimator, by the use of newly-introduced enrichment\nmethods; and (2) the reframing of an enriched soft regression model to new\ncontexts by an instance-dependent optimisation of the expected loss derived\nfrom the conditional normal distribution.\n", "versions": [{"version": "v1", "created": "Mon, 5 Nov 2012 21:40:38 GMT"}], "update_date": "2012-11-07", "authors_parsed": [["Hernandez-Orallo", "Jose", ""]]}, {"id": "1211.1082", "submitter": "Phil Long", "authors": "Maria Florina Balcan and Philip M. Long", "title": "Active and passive learning of linear separators under log-concave\n  distributions", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG math.ST stat.ML stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We provide new results concerning label efficient, polynomial time, passive\nand active learning of linear separators. We prove that active learning\nprovides an exponential improvement over PAC (passive) learning of homogeneous\nlinear separators under nearly log-concave distributions. Building on this, we\nprovide a computationally efficient PAC algorithm with optimal (up to a\nconstant factor) sample complexity for such problems. This resolves an open\nquestion concerning the sample complexity of efficient PAC algorithms under the\nuniform distribution in the unit ball. Moreover, it provides the first bound\nfor a polynomial-time PAC algorithm that is tight for an interesting infinite\nclass of hypothesis functions under a general and natural class of\ndata-distributions, providing significant progress towards a longstanding open\nquestion.\n  We also provide new bounds for active and passive learning in the case that\nthe data might not be linearly separable, both in the agnostic case and and\nunder the Tsybakov low-noise condition. To derive our results, we provide new\nstructural results for (nearly) log-concave distributions, which might be of\nindependent interest as well.\n", "versions": [{"version": "v1", "created": "Tue, 6 Nov 2012 00:21:32 GMT"}, {"version": "v2", "created": "Tue, 5 Mar 2013 00:08:44 GMT"}, {"version": "v3", "created": "Fri, 26 Apr 2013 17:50:21 GMT"}], "update_date": "2013-04-29", "authors_parsed": [["Balcan", "Maria Florina", ""], ["Long", "Philip M.", ""]]}, {"id": "1211.1275", "submitter": "Mehmet G\\\"onen", "authors": "Mehmet G\\\"onen and Suleiman A. Khan and Samuel Kaski", "title": "Kernelized Bayesian Matrix Factorization", "comments": "Proceedings of the 30th International Conference on Machine Learning\n  (2013)", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We extend kernelized matrix factorization with a fully Bayesian treatment and\nwith an ability to work with multiple side information sources expressed as\ndifferent kernels. Kernel functions have been introduced to matrix\nfactorization to integrate side information about the rows and columns (e.g.,\nobjects and users in recommender systems), which is necessary for making\nout-of-matrix (i.e., cold start) predictions. We discuss specifically bipartite\ngraph inference, where the output matrix is binary, but extensions to more\ngeneral matrices are straightforward. We extend the state of the art in two key\naspects: (i) A fully conjugate probabilistic formulation of the kernelized\nmatrix factorization problem enables an efficient variational approximation,\nwhereas fully Bayesian treatments are not computationally feasible in the\nearlier approaches. (ii) Multiple side information sources are included,\ntreated as different kernels in multiple kernel learning that additionally\nreveals which side information sources are informative. Our method outperforms\nalternatives in predicting drug-protein interactions on two data sets. We then\nshow that our framework can also be used for solving multilabel learning\nproblems by considering samples and labels as the two domains where matrix\nfactorization operates on. Our algorithm obtains the lowest Hamming loss values\non 10 out of 14 multilabel classification data sets compared to five\nstate-of-the-art multilabel learning algorithms.\n", "versions": [{"version": "v1", "created": "Tue, 6 Nov 2012 15:54:07 GMT"}, {"version": "v2", "created": "Fri, 15 Feb 2013 08:42:56 GMT"}, {"version": "v3", "created": "Wed, 8 May 2013 21:02:52 GMT"}], "update_date": "2013-05-10", "authors_parsed": [["G\u00f6nen", "Mehmet", ""], ["Khan", "Suleiman A.", ""], ["Kaski", "Samuel", ""]]}, {"id": "1211.1323", "submitter": "Claudia Beleites", "authors": "Claudia Beleites and Ute Neugebauer and Thomas Bocklitz and Christoph\n  Krafft and J\\\"urgen Popp", "title": "Sample Size Planning for Classification Models", "comments": "The paper is published in Analytica Chimica Acta (special issue\n  \"CAC2012\"). This is a reformatted version of the accepted manuscript with few\n  typos corrected and links to the official publicaion, including the\n  supplementary material (pages 11 - 16 and supplementary-* files in the\n  source). The slides of the presentation at Clircon (2015-04-22, Exeter, UK)\n  are available as ancillary pdf file", "journal-ref": "Analytica Chimica Acta, 760 (2013) 25 - 33", "doi": "10.1016/j.aca.2012.11.007", "report-no": null, "categories": "stat.AP stat.ME stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In biospectroscopy, suitably annotated and statistically independent samples\n(e. g. patients, batches, etc.) for classifier training and testing are scarce\nand costly. Learning curves show the model performance as function of the\ntraining sample size and can help to determine the sample size needed to train\ngood classifiers. However, building a good model is actually not enough: the\nperformance must also be proven. We discuss learning curves for typical small\nsample size situations with 5 - 25 independent samples per class. Although the\nclassification models achieve acceptable performance, the learning curve can be\ncompletely masked by the random testing uncertainty due to the equally limited\ntest sample size. In consequence, we determine test sample sizes necessary to\nachieve reasonable precision in the validation and find that 75 - 100 samples\nwill usually be needed to test a good but not perfect classifier. Such a data\nset will then allow refined sample size planning on the basis of the achieved\nperformance. We also demonstrate how to calculate necessary sample sizes in\norder to show the superiority of one classifier over another: this often\nrequires hundreds of statistically independent test samples or is even\ntheoretically impossible. We demonstrate our findings with a data set of ca.\n2550 Raman spectra of single cells (five classes: erythrocytes, leukocytes and\nthree tumour cell lines BT-20, MCF-7 and OCI-AML3) as well as by an extensive\nsimulation that allows precise determination of the actual performance of the\nmodels in question.\n", "versions": [{"version": "v1", "created": "Tue, 6 Nov 2012 17:42:00 GMT"}, {"version": "v2", "created": "Fri, 4 Jan 2013 13:28:52 GMT"}, {"version": "v3", "created": "Sun, 3 May 2015 09:09:38 GMT"}], "update_date": "2015-05-05", "authors_parsed": [["Beleites", "Claudia", ""], ["Neugebauer", "Ute", ""], ["Bocklitz", "Thomas", ""], ["Krafft", "Christoph", ""], ["Popp", "J\u00fcrgen", ""]]}, {"id": "1211.1328", "submitter": "Matthew Urry Dr", "authors": "Matthew Urry and Peter Sollich", "title": "Random walk kernels and learning curves for Gaussian process regression\n  on random graphs", "comments": null, "journal-ref": "JMLR(14):1801-1835 2013", "doi": null, "report-no": null, "categories": "stat.ML cond-mat.dis-nn cond-mat.stat-mech cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider learning on graphs, guided by kernels that encode similarity\nbetween vertices. Our focus is on random walk kernels, the analogues of squared\nexponential kernels in Euclidean spaces. We show that on large, locally\ntreelike, graphs these have some counter-intuitive properties, specifically in\nthe limit of large kernel lengthscales. We consider using these kernels as\ncovariance matrices of e.g.\\ Gaussian processes (GPs). In this situation one\ntypically scales the prior globally to normalise the average of the prior\nvariance across vertices. We demonstrate that, in contrast to the Euclidean\ncase, this generically leads to significant variation in the prior variance\nacross vertices, which is undesirable from the probabilistic modelling point of\nview. We suggest the random walk kernel should be normalised locally, so that\neach vertex has the same prior variance, and analyse the consequences of this\nby studying learning curves for Gaussian process regression. Numerical\ncalculations as well as novel theoretical predictions for the learning curves\nusing belief propagation make it clear that one obtains distinctly different\nprobabilistic models depending on the choice of normalisation. Our method for\npredicting the learning curves using belief propagation is significantly more\naccurate than previous approximations and should become exact in the limit of\nlarge random graphs.\n", "versions": [{"version": "v1", "created": "Tue, 6 Nov 2012 17:58:39 GMT"}, {"version": "v2", "created": "Mon, 30 Sep 2013 10:36:51 GMT"}], "update_date": "2013-10-01", "authors_parsed": [["Urry", "Matthew", ""], ["Sollich", "Peter", ""]]}, {"id": "1211.1642", "submitter": "Stoyan Georgiev", "authors": "Stoyan Georgiev and Sayan Mukherjee", "title": "Randomized Dimension Reduction on Massive Data", "comments": "31 pages, 6 figures, Key Words:dimension reduction, generalized\n  eigendecompositon, low-rank, supervised, inverse regression, random\n  projections, randomized algorithms, Krylov subspace methods", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Scalability of statistical estimators is of increasing importance in modern\napplications and dimension reduction is often used to extract relevant\ninformation from data. A variety of popular dimension reduction approaches can\nbe framed as symmetric generalized eigendecomposition problems. In this paper\nwe outline how taking into account the low rank structure assumption implicit\nin these dimension reduction approaches provides both computational and\nstatistical advantages. We adapt recent randomized low-rank approximation\nalgorithms to provide efficient solutions to three dimension reduction methods:\nPrincipal Component Analysis (PCA), Sliced Inverse Regression (SIR), and\nLocalized Sliced Inverse Regression (LSIR). A key observation in this paper is\nthat randomization serves a dual role, improving both computational and\nstatistical performance. This point is highlighted in our experiments on real\nand simulated data.\n", "versions": [{"version": "v1", "created": "Wed, 7 Nov 2012 19:21:48 GMT"}, {"version": "v2", "created": "Tue, 5 Nov 2013 21:17:40 GMT"}], "update_date": "2013-11-07", "authors_parsed": [["Georgiev", "Stoyan", ""], ["Mukherjee", "Sayan", ""]]}, {"id": "1211.1716", "submitter": "James Voss", "authors": "Mikhail Belkin, Luis Rademacher, James Voss", "title": "Blind Signal Separation in the Presence of Gaussian Noise", "comments": "16 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.DS stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A prototypical blind signal separation problem is the so-called cocktail\nparty problem, with n people talking simultaneously and n different microphones\nwithin a room. The goal is to recover each speech signal from the microphone\ninputs. Mathematically this can be modeled by assuming that we are given\nsamples from an n-dimensional random variable X=AS, where S is a vector whose\ncoordinates are independent random variables corresponding to each speaker. The\nobjective is to recover the matrix A^{-1} given random samples from X. A range\nof techniques collectively known as Independent Component Analysis (ICA) have\nbeen proposed to address this problem in the signal processing and machine\nlearning literature. Many of these techniques are based on using the kurtosis\nor other cumulants to recover the components.\n  In this paper we propose a new algorithm for solving the blind signal\nseparation problem in the presence of additive Gaussian noise, when we are\ngiven samples from X=AS+\\eta, where \\eta is drawn from an unknown, not\nnecessarily spherical n-dimensional Gaussian distribution. Our approach is\nbased on a method for decorrelating a sample with additive Gaussian noise under\nthe assumption that the underlying distribution is a linear transformation of a\ndistribution with independent components. Our decorrelation routine is based on\nthe properties of cumulant tensors and can be combined with any standard\ncumulant-based method for ICA to get an algorithm that is provably robust in\nthe presence of Gaussian noise. We derive polynomial bounds for the sample\ncomplexity and error propagation of our method.\n", "versions": [{"version": "v1", "created": "Wed, 7 Nov 2012 22:50:51 GMT"}, {"version": "v2", "created": "Sun, 9 Jun 2013 04:43:53 GMT"}], "update_date": "2013-06-11", "authors_parsed": [["Belkin", "Mikhail", ""], ["Rademacher", "Luis", ""], ["Voss", "James", ""]]}, {"id": "1211.2073", "submitter": "Yang Lu", "authors": "Yang Lu and Mengying Wang and Kenny Q. Zhu and Bo Yuan", "title": "LAGE: A Java Framework to reconstruct Gene Regulatory Networks from\n  Large-Scale Continues Expression Data", "comments": "2 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CE q-bio.QM stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  LAGE is a systematic framework developed in Java. The motivation of LAGE is\nto provide a scalable and parallel solution to reconstruct Gene Regulatory\nNetworks (GRNs) from continuous gene expression data for very large amount of\ngenes. The basic idea of our framework is motivated by the philosophy of\ndivideand-conquer. Specifically, LAGE recursively partitions genes into\nmultiple overlapping communities with much smaller sizes, learns\nintra-community GRNs respectively before merge them altogether. Besides, the\ncomplete information of overlapping communities serves as the byproduct, which\ncould be used to mine meaningful functional modules in biological networks.\n", "versions": [{"version": "v1", "created": "Fri, 9 Nov 2012 08:34:25 GMT"}], "update_date": "2012-11-12", "authors_parsed": [["Lu", "Yang", ""], ["Wang", "Mengying", ""], ["Zhu", "Kenny Q.", ""], ["Yuan", "Bo", ""]]}, {"id": "1211.2190", "submitter": "Luca Martino", "authors": "Jesse Read, Luca Martino, David Luengo", "title": "Efficient Monte Carlo Methods for Multi-Dimensional Learning with\n  Classifier Chains", "comments": "Submitted to Pattern Recognition", "journal-ref": "Pattern Recognition, Volume 47, Issue 3, Pages: 1535-1546, 2014", "doi": "10.1016/j.patcog.2013.10.006", "report-no": null, "categories": "cs.LG stat.CO stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Multi-dimensional classification (MDC) is the supervised learning problem\nwhere an instance is associated with multiple classes, rather than with a\nsingle class, as in traditional classification problems. Since these classes\nare often strongly correlated, modeling the dependencies between them allows\nMDC methods to improve their performance - at the expense of an increased\ncomputational cost. In this paper we focus on the classifier chains (CC)\napproach for modeling dependencies, one of the most popular and highest-\nperforming methods for multi-label classification (MLC), a particular case of\nMDC which involves only binary classes (i.e., labels). The original CC\nalgorithm makes a greedy approximation, and is fast but tends to propagate\nerrors along the chain. Here we present novel Monte Carlo schemes, both for\nfinding a good chain sequence and performing efficient inference. Our\nalgorithms remain tractable for high-dimensional data sets and obtain the best\npredictive performance across several real data sets.\n", "versions": [{"version": "v1", "created": "Fri, 9 Nov 2012 17:21:48 GMT"}, {"version": "v2", "created": "Mon, 17 Dec 2012 18:39:35 GMT"}, {"version": "v3", "created": "Mon, 15 Apr 2013 16:43:20 GMT"}, {"version": "v4", "created": "Sat, 7 Sep 2013 13:10:06 GMT"}], "update_date": "2014-05-20", "authors_parsed": [["Read", "Jesse", ""], ["Martino", "Luca", ""], ["Luengo", "David", ""]]}, {"id": "1211.2227", "submitter": "Joseph Anderson", "authors": "Joseph Anderson, Navin Goyal, Luis Rademacher", "title": "Efficient learning of simplices", "comments": "New author added to this version, Joseph Anderson. New results:\n  reductions from learning a simplex and a linearly transformed l_p ball to ICA\n  (sections 7 and 8)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.DS stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We show an efficient algorithm for the following problem: Given uniformly\nrandom points from an arbitrary n-dimensional simplex, estimate the simplex.\nThe size of the sample and the number of arithmetic operations of our algorithm\nare polynomial in n. This answers a question of Frieze, Jerrum and Kannan\n[FJK]. Our result can also be interpreted as efficiently learning the\nintersection of n+1 half-spaces in R^n in the model where the intersection is\nbounded and we are given polynomially many uniform samples from it. Our proof\nuses the local search technique from Independent Component Analysis (ICA), also\nused by [FJK]. Unlike these previous algorithms, which were based on analyzing\nthe fourth moment, ours is based on the third moment.\n  We also show a direct connection between the problem of learning a simplex\nand ICA: a simple randomized reduction to ICA from the problem of learning a\nsimplex. The connection is based on a known representation of the uniform\nmeasure on a simplex. Similar representations lead to a reduction from the\nproblem of learning an affine transformation of an n-dimensional l_p ball to\nICA.\n", "versions": [{"version": "v1", "created": "Fri, 9 Nov 2012 20:47:23 GMT"}, {"version": "v2", "created": "Fri, 22 Feb 2013 22:54:40 GMT"}, {"version": "v3", "created": "Thu, 6 Jun 2013 02:52:50 GMT"}], "update_date": "2013-06-07", "authors_parsed": [["Anderson", "Joseph", ""], ["Goyal", "Navin", ""], ["Rademacher", "Luis", ""]]}, {"id": "1211.2264", "submitter": "Tingni Sun", "authors": "Tingni Sun, Cun-Hui Zhang", "title": "Calibrated Elastic Regularization in Matrix Completion", "comments": "9 pages; Advances in Neural Information Processing Systems, NIPS 2012", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.ML stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper concerns the problem of matrix completion, which is to estimate a\nmatrix from observations in a small subset of indices. We propose a calibrated\nspectrum elastic net method with a sum of the nuclear and Frobenius penalties\nand develop an iterative algorithm to solve the convex minimization problem.\nThe iterative algorithm alternates between imputing the missing entries in the\nincomplete matrix by the current guess and estimating the matrix by a scaled\nsoft-thresholding singular value decomposition of the imputed matrix until the\nresulting matrix converges. A calibration step follows to correct the bias\ncaused by the Frobenius penalty. Under proper coherence conditions and for\nsuitable penalties levels, we prove that the proposed estimator achieves an\nerror bound of nearly optimal order and in proportion to the noise level. This\nprovides a unified analysis of the noisy and noiseless matrix completion\nproblems. Simulation results are presented to compare our proposal with\nprevious ones.\n", "versions": [{"version": "v1", "created": "Fri, 9 Nov 2012 23:17:30 GMT"}], "update_date": "2012-11-13", "authors_parsed": [["Sun", "Tingni", ""], ["Zhang", "Cun-Hui", ""]]}, {"id": "1211.2304", "submitter": "Ayan Acharya", "authors": "Ayan Acharya, Eduardo R. Hruschka, Joydeep Ghosh, Badrul Sarwar,\n  Jean-David Ruvini", "title": "Probabilistic Combination of Classifier and Cluster Ensembles for\n  Non-transductive Learning", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Unsupervised models can provide supplementary soft constraints to help\nclassify new target data under the assumption that similar objects in the\ntarget set are more likely to share the same class label. Such models can also\nhelp detect possible differences between training and target distributions,\nwhich is useful in applications where concept drift may take place. This paper\ndescribes a Bayesian framework that takes as input class labels from existing\nclassifiers (designed based on labeled data from the source domain), as well as\ncluster labels from a cluster ensemble operating solely on the target data to\nbe classified, and yields a consensus labeling of the target data. This\nframework is particularly useful when the statistics of the target data drift\nor change from those of the training data. We also show that the proposed\nframework is privacy-aware and allows performing distributed learning when\ndata/models have sharing restrictions. Experiments show that our framework can\nyield superior results to those provided by applying classifier ensembles only.\n", "versions": [{"version": "v1", "created": "Sat, 10 Nov 2012 07:37:44 GMT"}], "update_date": "2015-03-13", "authors_parsed": [["Acharya", "Ayan", ""], ["Hruschka", "Eduardo R.", ""], ["Ghosh", "Joydeep", ""], ["Sarwar", "Badrul", ""], ["Ruvini", "Jean-David", ""]]}, {"id": "1211.2315", "submitter": "Chlo\\'e-Agathe Azencott", "authors": "Chlo\\'e-Agathe Azencott, Dominik Grimm, Mahito Sugiyama, Yoshinobu\n  Kawahara and Karsten M. Borgwardt", "title": "Efficient network-guided multi-locus association mapping with graph cuts", "comments": "20 pages, 6 figures, accepted at ISMB (International Conference on\n  Intelligent Systems for Molecular Biology) 2013", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML q-bio.QM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  As an increasing number of genome-wide association studies reveal the\nlimitations of attempting to explain phenotypic heritability by single genetic\nloci, there is growing interest for associating complex phenotypes with sets of\ngenetic loci. While several methods for multi-locus mapping have been proposed,\nit is often unclear how to relate the detected loci to the growing knowledge\nabout gene pathways and networks. The few methods that take biological pathways\nor networks into account are either restricted to investigating a limited\nnumber of predetermined sets of loci, or do not scale to genome-wide settings.\n  We present SConES, a new efficient method to discover sets of genetic loci\nthat are maximally associated with a phenotype, while being connected in an\nunderlying network. Our approach is based on a minimum cut reformulation of the\nproblem of selecting features under sparsity and connectivity constraints that\ncan be solved exactly and rapidly.\n  SConES outperforms state-of-the-art competitors in terms of runtime, scales\nto hundreds of thousands of genetic loci, and exhibits higher power in\ndetecting causal SNPs in simulation studies than existing methods. On flowering\ntime phenotypes and genotypes from Arabidopsis thaliana, SConES detects loci\nthat enable accurate phenotype prediction and that are supported by the\nliterature.\n  Matlab code for SConES is available at\nhttp://webdav.tuebingen.mpg.de/u/karsten/Forschung/scones/\n", "versions": [{"version": "v1", "created": "Sat, 10 Nov 2012 10:35:53 GMT"}, {"version": "v2", "created": "Wed, 9 Jan 2013 14:24:12 GMT"}, {"version": "v3", "created": "Mon, 18 Mar 2013 10:57:17 GMT"}, {"version": "v4", "created": "Wed, 17 Apr 2013 08:26:08 GMT"}, {"version": "v5", "created": "Thu, 18 Apr 2013 08:39:51 GMT"}], "update_date": "2013-04-19", "authors_parsed": [["Azencott", "Chlo\u00e9-Agathe", ""], ["Grimm", "Dominik", ""], ["Sugiyama", "Mahito", ""], ["Kawahara", "Yoshinobu", ""], ["Borgwardt", "Karsten M.", ""]]}, {"id": "1211.2459", "submitter": "Luis  Sanchez Giraldo", "authors": "Luis G. Sanchez Giraldo and Murali Rao and Jose C. Principe", "title": "Measures of Entropy from Data Using Infinitely Divisible Kernels", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.IT math.IT stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Information theory provides principled ways to analyze different inference\nand learning problems such as hypothesis testing, clustering, dimensionality\nreduction, classification, among others. However, the use of information\ntheoretic quantities as test statistics, that is, as quantities obtained from\nempirical data, poses a challenging estimation problem that often leads to\nstrong simplifications such as Gaussian models, or the use of plug in density\nestimators that are restricted to certain representation of the data. In this\npaper, a framework to non-parametrically obtain measures of entropy directly\nfrom data using operators in reproducing kernel Hilbert spaces defined by\ninfinitely divisible kernels is presented. The entropy functionals, which bear\nresemblance with quantum entropies, are defined on positive definite matrices\nand satisfy similar axioms to those of Renyi's definition of entropy.\nConvergence of the proposed estimators follows from concentration results on\nthe difference between the ordered spectrum of the Gram matrices and the\nintegral operators associated to the population quantities. In this way,\ncapitalizing on both the axiomatic definition of entropy and on the\nrepresentation power of positive definite kernels, the proposed measure of\nentropy avoids the estimation of the probability distribution underlying the\ndata. Moreover, estimators of kernel-based conditional entropy and mutual\ninformation are also defined. Numerical experiments on independence tests\ncompare favourably with state of the art.\n", "versions": [{"version": "v1", "created": "Sun, 11 Nov 2012 20:49:28 GMT"}, {"version": "v2", "created": "Mon, 9 Dec 2013 19:38:31 GMT"}, {"version": "v3", "created": "Mon, 1 Sep 2014 21:52:55 GMT"}], "update_date": "2014-09-03", "authors_parsed": [["Giraldo", "Luis G. Sanchez", ""], ["Rao", "Murali", ""], ["Principe", "Jose C.", ""]]}, {"id": "1211.2476", "submitter": "Hossein Azari Soufiani", "authors": "Hossein Azari Soufiani, David C. Parkes, Lirong Xia", "title": "Random Utility Theory for Social Choice", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.MA cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Random utility theory models an agent's preferences on alternatives by\ndrawing a real-valued score on each alternative (typically independently) from\na parameterized distribution, and then ranking the alternatives according to\nscores. A special case that has received significant attention is the\nPlackett-Luce model, for which fast inference methods for maximum likelihood\nestimators are available. This paper develops conditions on general random\nutility models that enable fast inference within a Bayesian framework through\nMC-EM, providing concave loglikelihood functions and bounded sets of global\nmaxima solutions. Results on both real-world and simulated data provide support\nfor the scalability of the approach and capability for model selection among\ngeneral random utility models including Plackett-Luce.\n", "versions": [{"version": "v1", "created": "Sun, 11 Nov 2012 23:09:02 GMT"}], "update_date": "2012-11-13", "authors_parsed": [["Soufiani", "Hossein Azari", ""], ["Parkes", "David C.", ""], ["Xia", "Lirong", ""]]}, {"id": "1211.2532", "submitter": "Benjamin Rolfs", "authors": "Dominique Guillot and Bala Rajaratnam and Benjamin T. Rolfs and Arian\n  Maleki and Ian Wong", "title": "Iterative Thresholding Algorithm for Sparse Inverse Covariance\n  Estimation", "comments": "25 pages, 1 figure, 4 tables. Conference paper", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.CO cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The L1-regularized maximum likelihood estimation problem has recently become\na topic of great interest within the machine learning, statistics, and\noptimization communities as a method for producing sparse inverse covariance\nestimators. In this paper, a proximal gradient method (G-ISTA) for performing\nL1-regularized covariance matrix estimation is presented. Although numerous\nalgorithms have been proposed for solving this problem, this simple proximal\ngradient method is found to have attractive theoretical and numerical\nproperties. G-ISTA has a linear rate of convergence, resulting in an O(log e)\niteration complexity to reach a tolerance of e. This paper gives eigenvalue\nbounds for the G-ISTA iterates, providing a closed-form linear convergence\nrate. The rate is shown to be closely related to the condition number of the\noptimal point. Numerical convergence results and timing comparisons for the\nproposed method are presented. G-ISTA is shown to perform very well, especially\nwhen the optimal point is well-conditioned.\n", "versions": [{"version": "v1", "created": "Mon, 12 Nov 2012 08:35:26 GMT"}, {"version": "v2", "created": "Wed, 14 Nov 2012 01:22:30 GMT"}, {"version": "v3", "created": "Tue, 27 Nov 2012 04:48:51 GMT"}], "update_date": "2012-11-28", "authors_parsed": [["Guillot", "Dominique", ""], ["Rajaratnam", "Bala", ""], ["Rolfs", "Benjamin T.", ""], ["Maleki", "Arian", ""], ["Wong", "Ian", ""]]}, {"id": "1211.2556", "submitter": "Fatai Anifowose", "authors": "Fatai Adesina Anifowose", "title": "A Comparative Study of Gaussian Mixture Model and Radial Basis Function\n  for Voice Recognition", "comments": "9 pages, 10 figures; International Journal of Advanced Computer\n  Science and Applications (IJACSA), Vol. 1, No.3, September 2010", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A comparative study of the application of Gaussian Mixture Model (GMM) and\nRadial Basis Function (RBF) in biometric recognition of voice has been carried\nout and presented. The application of machine learning techniques to biometric\nauthentication and recognition problems has gained a widespread acceptance. In\nthis research, a GMM model was trained, using Expectation Maximization (EM)\nalgorithm, on a dataset containing 10 classes of vowels and the model was used\nto predict the appropriate classes using a validation dataset. For experimental\nvalidity, the model was compared to the performance of two different versions\nof RBF model using the same learning and validation datasets. The results\nshowed very close recognition accuracy between the GMM and the standard RBF\nmodel, but with GMM performing better than the standard RBF by less than 1% and\nthe two models outperformed similar models reported in literature. The DTREG\nversion of RBF outperformed the other two models by producing 94.8% recognition\naccuracy. In terms of recognition time, the standard RBF was found to be the\nfastest among the three models.\n", "versions": [{"version": "v1", "created": "Mon, 12 Nov 2012 10:42:58 GMT"}], "update_date": "2012-11-13", "authors_parsed": [["Anifowose", "Fatai Adesina", ""]]}, {"id": "1211.2717", "submitter": "Tong Zhang", "authors": "Shai Shalev-Shwartz and Tong Zhang", "title": "Proximal Stochastic Dual Coordinate Ascent", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG math.OC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We introduce a proximal version of dual coordinate ascent method. We\ndemonstrate how the derived algorithmic framework can be used for numerous\nregularized loss minimization problems, including $\\ell_1$ regularization and\nstructured output SVM. The convergence rates we obtain match, and sometimes\nimprove, state-of-the-art results.\n", "versions": [{"version": "v1", "created": "Mon, 12 Nov 2012 18:08:34 GMT"}], "update_date": "2012-11-13", "authors_parsed": [["Shalev-Shwartz", "Shai", ""], ["Zhang", "Tong", ""]]}, {"id": "1211.2881", "submitter": "Junyoung Chung", "authors": "Junyoung Chung, Donghoon Lee, Youngjoo Seo, and Chang D. Yoo", "title": "Deep Attribute Networks", "comments": "This paper has been withdrawn by the author due to a crucial\n  grammatical errors", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Obtaining compact and discriminative features is one of the major challenges\nin many of the real-world image classification tasks such as face verification\nand object recognition. One possible approach is to represent input image on\nthe basis of high-level features that carry semantic meaning which humans can\nunderstand. In this paper, a model coined deep attribute network (DAN) is\nproposed to address this issue. For an input image, the model outputs the\nattributes of the input image without performing any classification. The\nefficacy of the proposed model is evaluated on unconstrained face verification\nand real-world object recognition tasks using the LFW and the a-PASCAL\ndatasets. We demonstrate the potential of deep learning for attribute-based\nclassification by showing comparable results with existing state-of-the-art\nresults. Once properly trained, the DAN is fast and does away with calculating\nlow-level features which are maybe unreliable and computationally expensive.\n", "versions": [{"version": "v1", "created": "Tue, 13 Nov 2012 03:41:31 GMT"}, {"version": "v2", "created": "Tue, 20 Nov 2012 11:30:46 GMT"}, {"version": "v3", "created": "Wed, 28 Nov 2012 08:39:03 GMT"}], "update_date": "2012-11-29", "authors_parsed": [["Chung", "Junyoung", ""], ["Lee", "Donghoon", ""], ["Seo", "Youngjoo", ""], ["Yoo", "Chang D.", ""]]}, {"id": "1211.2891", "submitter": "Lior Rokach", "authors": "Ariel Bar, Lior Rokach, Guy Shani, Bracha Shapira, Alon Schclar", "title": "Boosting Simple Collaborative Filtering Models Using Ensemble Methods", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper we examine the effect of applying ensemble learning to the\nperformance of collaborative filtering methods. We present several systematic\napproaches for generating an ensemble of collaborative filtering models based\non a single collaborative filtering algorithm (single-model or homogeneous\nensemble). We present an adaptation of several popular ensemble techniques in\nmachine learning for the collaborative filtering domain, including bagging,\nboosting, fusion and randomness injection. We evaluate the proposed approach on\nseveral types of collaborative filtering base models: k- NN, matrix\nfactorization and a neighborhood matrix factorization model. Empirical\nevaluation shows a prediction improvement compared to all base CF algorithms.\nIn particular, we show that the performance of an ensemble of simple (weak) CF\nmodels such as k-NN is competitive compared with a single strong CF model (such\nas matrix factorization) while requiring an order of magnitude less\ncomputational cost.\n", "versions": [{"version": "v1", "created": "Tue, 13 Nov 2012 05:30:36 GMT"}], "update_date": "2012-11-14", "authors_parsed": [["Bar", "Ariel", ""], ["Rokach", "Lior", ""], ["Shani", "Guy", ""], ["Shapira", "Bracha", ""], ["Schclar", "Alon", ""]]}, {"id": "1211.2958", "submitter": "Juha Karvanen", "authors": "Juha Karvanen", "title": "Study design in causal models", "comments": "The example on the MORGAM Project extended is in this version", "journal-ref": "Scandinavian Journal of Statistics, Volume 42, Issue 2, pages\n  361-377, 2015", "doi": "10.1111/sjos.12110", "report-no": null, "categories": "stat.ME stat.AP stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The causal assumptions, the study design and the data are the elements\nrequired for scientific inference in empirical research. The research is\nadequately communicated only if all of these elements and their relations are\ndescribed precisely. Causal models with design describe the study design and\nthe missing data mechanism together with the causal structure and allow the\ndirect application of causal calculus in the estimation of the causal effects.\nThe flow of the study is visualized by ordering the nodes of the causal diagram\nin two dimensions by their causal order and the time of the observation.\nConclusions whether a causal or observational relationship can be estimated\nfrom the collected incomplete data can be made directly from the graph. Causal\nmodels with design offer a systematic and unifying view scientific inference\nand increase the clarity and speed of communication. Examples on the causal\nmodels for a case-control study, a nested case-control study, a clinical trial\nand a two-stage case-cohort study are presented.\n", "versions": [{"version": "v1", "created": "Tue, 13 Nov 2012 11:33:47 GMT"}, {"version": "v2", "created": "Thu, 14 Mar 2013 13:32:40 GMT"}, {"version": "v3", "created": "Tue, 31 Dec 2013 11:26:41 GMT"}, {"version": "v4", "created": "Thu, 24 Apr 2014 05:13:52 GMT"}], "update_date": "2015-05-01", "authors_parsed": [["Karvanen", "Juha", ""]]}, {"id": "1211.3010", "submitter": "Sriharsha Veeramachaneni", "authors": "Sriharsha Veeramachaneni", "title": "Time-series Scenario Forecasting", "comments": "16 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Many applications require the ability to judge uncertainty of time-series\nforecasts. Uncertainty is often specified as point-wise error bars around a\nmean or median forecast. Due to temporal dependencies, such a method obscures\nsome information. We would ideally have a way to query the posterior\nprobability of the entire time-series given the predictive variables, or at a\nminimum, be able to draw samples from this distribution. We use a Bayesian\ndictionary learning algorithm to statistically generate an ensemble of\nforecasts. We show that the algorithm performs as well as a physics-based\nensemble method for temperature forecasts for Houston. We conclude that the\nmethod shows promise for scenario forecasting where physics-based methods are\nabsent.\n", "versions": [{"version": "v1", "created": "Tue, 13 Nov 2012 14:54:47 GMT"}], "update_date": "2012-11-14", "authors_parsed": [["Veeramachaneni", "Sriharsha", ""]]}, {"id": "1211.3038", "submitter": "Karthik Gurumoorthy", "authors": "Karthik S. Gurumoorthy and Anand Rangarajan and John Corring", "title": "Gradient density estimation in arbitrary finite dimensions using the\n  method of stationary phase", "comments": "This work is partly supported by EADS Prize Postdoctoral Fellowship", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We prove that the density function of the gradient of a sufficiently smooth\nfunction $S : \\Omega \\subset \\mathbb{R}^d \\rightarrow \\mathbb{R}$, obtained via\na random variable transformation of a uniformly distributed random variable, is\nincreasingly closely approximated by the normalized power spectrum of\n$\\phi=\\exp\\left(\\frac{iS}{\\tau}\\right)$ as the free parameter $\\tau \\rightarrow\n0$. The result is shown using the stationary phase approximation and standard\nintegration techniques and requires proper ordering of limits. We highlight a\nrelationship with the well-known characteristic function approach to density\nestimation, and detail why our result is distinct from this approach.\n", "versions": [{"version": "v1", "created": "Tue, 13 Nov 2012 16:19:31 GMT"}, {"version": "v2", "created": "Fri, 1 Nov 2013 05:43:52 GMT"}, {"version": "v3", "created": "Mon, 14 Apr 2014 08:22:49 GMT"}, {"version": "v4", "created": "Thu, 4 Sep 2014 15:38:51 GMT"}], "update_date": "2017-05-30", "authors_parsed": [["Gurumoorthy", "Karthik S.", ""], ["Rangarajan", "Anand", ""], ["Corring", "John", ""]]}, {"id": "1211.3295", "submitter": "Diego Colombo", "authors": "Diego Colombo and Marloes H. Maathuis", "title": "Order-independent constraint-based causal structure learning", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider constraint-based methods for causal structure learning, such as\nthe PC-, FCI-, RFCI- and CCD- algorithms (Spirtes et al. (2000, 1993),\nRichardson (1996), Colombo et al. (2012), Claassen et al. (2013)). The first\nstep of all these algorithms consists of the PC-algorithm. This algorithm is\nknown to be order-dependent, in the sense that the output can depend on the\norder in which the variables are given. This order-dependence is a minor issue\nin low-dimensional settings. We show, however, that it can be very pronounced\nin high-dimensional settings, where it can lead to highly variable results. We\npropose several modifications of the PC-algorithm (and hence also of the other\nalgorithms) that remove part or all of this order-dependence. All proposed\nmodifications are consistent in high-dimensional settings under the same\nconditions as their original counterparts. We compare the PC-, FCI-, and\nRFCI-algorithms and their modifications in simulation studies and on a yeast\ngene expression data set. We show that our modifications yield similar\nperformance in low-dimensional settings and improved performance in\nhigh-dimensional settings. All software is implemented in the R-package pcalg.\n", "versions": [{"version": "v1", "created": "Wed, 14 Nov 2012 12:56:06 GMT"}, {"version": "v2", "created": "Fri, 27 Sep 2013 15:56:21 GMT"}], "update_date": "2013-09-30", "authors_parsed": [["Colombo", "Diego", ""], ["Maathuis", "Marloes H.", ""]]}, {"id": "1211.3412", "submitter": "Nesreen Ahmed", "authors": "Nesreen K. Ahmed and Jennifer Neville and Ramana Kompella", "title": "Network Sampling: From Static to Streaming Graphs", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.SI cs.DS cs.LG physics.soc-ph stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Network sampling is integral to the analysis of social, information, and\nbiological networks. Since many real-world networks are massive in size,\ncontinuously evolving, and/or distributed in nature, the network structure is\noften sampled in order to facilitate study. For these reasons, a more thorough\nand complete understanding of network sampling is critical to support the field\nof network science. In this paper, we outline a framework for the general\nproblem of network sampling, by highlighting the different objectives,\npopulation and units of interest, and classes of network sampling methods. In\naddition, we propose a spectrum of computational models for network sampling\nmethods, ranging from the traditionally studied model based on the assumption\nof a static domain to a more challenging model that is appropriate for\nstreaming domains. We design a family of sampling methods based on the concept\nof graph induction that generalize across the full spectrum of computational\nmodels (from static to streaming) while efficiently preserving many of the\ntopological properties of the input graphs. Furthermore, we demonstrate how\ntraditional static sampling algorithms can be modified for graph streams for\neach of the three main classes of sampling methods: node, edge, and\ntopology-based sampling. Our experimental results indicate that our proposed\nfamily of sampling methods more accurately preserves the underlying properties\nof the graph for both static and streaming graphs. Finally, we study the impact\nof network sampling algorithms on the parameter estimation and performance\nevaluation of relational classification algorithms.\n", "versions": [{"version": "v1", "created": "Wed, 14 Nov 2012 01:48:37 GMT"}], "update_date": "2012-11-16", "authors_parsed": [["Ahmed", "Nesreen K.", ""], ["Neville", "Jennifer", ""], ["Kompella", "Ramana", ""]]}, {"id": "1211.3444", "submitter": "Deanna Needell", "authors": "B. Cung, T. Jin, J. Ramirez, A. Thompson, C. Boutsidis and D. Needell", "title": "Spectral Clustering: An empirical study of Approximation Algorithms and\n  its Application to the Attrition Problem", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG math.NA stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Clustering is the problem of separating a set of objects into groups (called\nclusters) so that objects within the same cluster are more similar to each\nother than to those in different clusters. Spectral clustering is a now\nwell-known method for clustering which utilizes the spectrum of the data\nsimilarity matrix to perform this separation. Since the method relies on\nsolving an eigenvector problem, it is computationally expensive for large\ndatasets. To overcome this constraint, approximation methods have been\ndeveloped which aim to reduce running time while maintaining accurate\nclassification. In this article, we summarize and experimentally evaluate\nseveral approximation methods for spectral clustering. From an applications\nstandpoint, we employ spectral clustering to solve the so-called attrition\nproblem, where one aims to identify from a set of employees those who are\nlikely to voluntarily leave the company from those who are not. Our study sheds\nlight on the empirical performance of existing approximate spectral clustering\nmethods and shows the applicability of these methods in an important business\noptimization related problem.\n", "versions": [{"version": "v1", "created": "Wed, 14 Nov 2012 22:05:09 GMT"}], "update_date": "2012-11-16", "authors_parsed": [["Cung", "B.", ""], ["Jin", "T.", ""], ["Ramirez", "J.", ""], ["Thompson", "A.", ""], ["Boutsidis", "C.", ""], ["Needell", "D.", ""]]}, {"id": "1211.3589", "submitter": "Abdul-Saboor Sheikh", "authors": "Abdul-Saboor Sheikh, Jacquelyn A. Shelton and J\\\"org L\\\"ucke", "title": "A Truncated EM Approach for Spike-and-Slab Sparse Coding", "comments": "To appear in JMLR (2014)", "journal-ref": "Journal of Machine Learning Research, 15:2653-2687, 2014", "doi": null, "report-no": null, "categories": "stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study inference and learning based on a sparse coding model with\n`spike-and-slab' prior. As in standard sparse coding, the model used assumes\nindependent latent sources that linearly combine to generate data points.\nHowever, instead of using a standard sparse prior such as a Laplace\ndistribution, we study the application of a more flexible `spike-and-slab'\ndistribution which models the absence or presence of a source's contribution\nindependently of its strength if it contributes. We investigate two approaches\nto optimize the parameters of spike-and-slab sparse coding: a novel truncated\nEM approach and, for comparison, an approach based on standard factored\nvariational distributions. The truncated approach can be regarded as a\nvariational approach with truncated posteriors as variational distributions. In\napplications to source separation we find that both approaches improve the\nstate-of-the-art in a number of standard benchmarks, which argues for the use\nof `spike-and-slab' priors for the corresponding data domains. Furthermore, we\nfind that the truncated EM approach improves on the standard factored approach\nin source separation tasks$-$which hints to biases introduced by assuming\nposterior independence in the factored variational approach. Likewise, on a\nstandard benchmark for image denoising, we find that the truncated EM approach\nimproves on the factored variational approach. While the performance of the\nfactored approach saturates with increasing numbers of hidden dimensions, the\nperformance of the truncated approach improves the state-of-the-art for higher\nnoise levels.\n", "versions": [{"version": "v1", "created": "Thu, 15 Nov 2012 12:34:07 GMT"}, {"version": "v2", "created": "Fri, 27 Dec 2013 11:01:59 GMT"}, {"version": "v3", "created": "Wed, 3 Sep 2014 09:54:03 GMT"}], "update_date": "2014-10-08", "authors_parsed": [["Sheikh", "Abdul-Saboor", ""], ["Shelton", "Jacquelyn A.", ""], ["L\u00fccke", "J\u00f6rg", ""]]}, {"id": "1211.3601", "submitter": "Daniel Sussman", "authors": "Carey E. Priebe, Daniel L. Sussman, Minh Tang, and Joshua T.\n  Vogelstein", "title": "Statistical inference on errorfully observed graphs", "comments": "30 pages, 8 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Statistical inference on graphs is a burgeoning field in the applied and\ntheoretical statistics communities, as well as throughout the wider world of\nscience, engineering, business, etc. In many applications, we are faced with\nthe reality of errorfully observed graphs. That is, the existence of an edge\nbetween two vertices is based on some imperfect assessment. In this paper, we\nconsider a graph $G = (V,E)$. We wish to perform an inference task -- the\ninference task considered here is \"vertex classification\". However, we do not\nobserve $G$; rather, for each potential edge $uv \\in {{V}\\choose{2}}$ we\nobserve an \"edge-feature\" which we use to classify $uv$ as edge/not-edge. Thus\nwe errorfully observe $G$ when we observe the graph $\\widetilde{G} =\n(V,\\widetilde{E})$ as the edges in $\\widetilde{E}$ arise from the\nclassifications of the \"edge-features\", and are expected to be errorful.\nMoreover, we face a quantity/quality trade-off regarding the edge-features we\nobserve -- more informative edge-features are more expensive, and hence the\nnumber of potential edges that can be assessed decreases with the quality of\nthe edge-features. We studied this problem by formulating a quantity/quality\ntradeoff for a simple class of random graphs model, namely the stochastic\nblockmodel. We then consider a simple but optimal vertex classifier for\nclassifying $v$ and we derive the optimal quantity/quality operating point for\nsubsequent graph inference in the face of this trade-off. The optimal operating\npoints for the quantity/quality trade-off are surprising and illustrate the\nissue that methods for intermediate tasks should be chosen to maximize\nperformance for the ultimate inference task. Finally, we investigate the\nquantity/quality tradeoff for errorful obesrvations of the {\\it C.\\ elegans}\nconnectome graph.\n", "versions": [{"version": "v1", "created": "Thu, 15 Nov 2012 13:22:09 GMT"}, {"version": "v2", "created": "Fri, 25 Oct 2013 19:48:19 GMT"}, {"version": "v3", "created": "Sun, 23 Feb 2014 18:57:54 GMT"}, {"version": "v4", "created": "Mon, 21 Jul 2014 13:12:33 GMT"}], "update_date": "2014-07-22", "authors_parsed": [["Priebe", "Carey E.", ""], ["Sussman", "Daniel L.", ""], ["Tang", "Minh", ""], ["Vogelstein", "Joshua T.", ""]]}, {"id": "1211.3711", "submitter": "Alex Graves", "authors": "Alex Graves", "title": "Sequence Transduction with Recurrent Neural Networks", "comments": "First published in the International Conference of Machine Learning\n  (ICML) 2012 Workshop on Representation Learning", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NE cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Many machine learning tasks can be expressed as the transformation---or\n\\emph{transduction}---of input sequences into output sequences: speech\nrecognition, machine translation, protein secondary structure prediction and\ntext-to-speech to name but a few. One of the key challenges in sequence\ntransduction is learning to represent both the input and output sequences in a\nway that is invariant to sequential distortions such as shrinking, stretching\nand translating. Recurrent neural networks (RNNs) are a powerful sequence\nlearning architecture that has proven capable of learning such representations.\nHowever RNNs traditionally require a pre-defined alignment between the input\nand output sequences to perform transduction. This is a severe limitation since\n\\emph{finding} the alignment is the most difficult aspect of many sequence\ntransduction problems. Indeed, even determining the length of the output\nsequence is often challenging. This paper introduces an end-to-end,\nprobabilistic sequence transduction system, based entirely on RNNs, that is in\nprinciple able to transform any input sequence into any finite, discrete output\nsequence. Experimental results for phoneme recognition are provided on the\nTIMIT speech corpus.\n", "versions": [{"version": "v1", "created": "Wed, 14 Nov 2012 19:25:21 GMT"}], "update_date": "2012-11-16", "authors_parsed": [["Graves", "Alex", ""]]}, {"id": "1211.3760", "submitter": "Georg M. Goerg", "authors": "Georg M. Goerg and Cosma Rohilla Shalizi", "title": "Mixed LICORS: A Nonparametric Algorithm for Predictive State\n  Reconstruction", "comments": "11 pages; AISTATS 2013", "journal-ref": "AISTATS 2013, pp. 289--297", "doi": null, "report-no": null, "categories": "stat.ME stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We introduce 'mixed LICORS', an algorithm for learning nonlinear,\nhigh-dimensional dynamics from spatio-temporal data, suitable for both\nprediction and simulation. Mixed LICORS extends the recent LICORS algorithm\n(Goerg and Shalizi, 2012) from hard clustering of predictive distributions to a\nnon-parametric, EM-like soft clustering. This retains the asymptotic predictive\noptimality of LICORS, but, as we show in simulations, greatly improves\nout-of-sample forecasts with limited data. The new method is implemented in the\npublicly-available R package \"LICORS\"\n(http://cran.r-project.org/web/packages/LICORS/).\n", "versions": [{"version": "v1", "created": "Thu, 15 Nov 2012 21:45:15 GMT"}, {"version": "v2", "created": "Fri, 3 May 2013 02:03:00 GMT"}], "update_date": "2015-07-27", "authors_parsed": [["Goerg", "Georg M.", ""], ["Shalizi", "Cosma Rohilla", ""]]}, {"id": "1211.3831", "submitter": "Youhei Akimoto", "authors": "Youhei Akimoto (INRIA Saclay - Ile de France), Yann Ollivier (LRI)", "title": "Objective Improvement in Information-Geometric Optimization", "comments": null, "journal-ref": "Foundations of Genetic Algorithms XII (2013)", "doi": null, "report-no": null, "categories": "cs.LG cs.AI math.OC stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Information-Geometric Optimization (IGO) is a unified framework of stochastic\nalgorithms for optimization problems. Given a family of probability\ndistributions, IGO turns the original optimization problem into a new\nmaximization problem on the parameter space of the probability distributions.\nIGO updates the parameter of the probability distribution along the natural\ngradient, taken with respect to the Fisher metric on the parameter manifold,\naiming at maximizing an adaptive transform of the objective function. IGO\nrecovers several known algorithms as particular instances: for the family of\nBernoulli distributions IGO recovers PBIL, for the family of Gaussian\ndistributions the pure rank-mu CMA-ES update is recovered, and for exponential\nfamilies in expectation parametrization the cross-entropy/ML method is\nrecovered. This article provides a theoretical justification for the IGO\nframework, by proving that any step size not greater than 1 guarantees monotone\nimprovement over the course of optimization, in terms of q-quantile values of\nthe objective function f. The range of admissible step sizes is independent of\nf and its domain. We extend the result to cover the case of different step\nsizes for blocks of the parameters in the IGO algorithm. Moreover, we prove\nthat expected fitness improves over time when fitness-proportional selection is\napplied, in which case the RPP algorithm is recovered.\n", "versions": [{"version": "v1", "created": "Fri, 16 Nov 2012 08:54:08 GMT"}, {"version": "v2", "created": "Thu, 29 Nov 2012 12:57:45 GMT"}, {"version": "v3", "created": "Thu, 7 Mar 2013 13:36:08 GMT"}], "update_date": "2013-03-08", "authors_parsed": [["Akimoto", "Youhei", "", "INRIA Saclay - Ile de France"], ["Ollivier", "Yann", "", "LRI"]]}, {"id": "1211.3907", "submitter": "Eric Chi", "authors": "Eric C. Chi, Hua Zhou, and Kenneth Lange", "title": "Distance Majorization and Its Applications", "comments": "29 pages, 6 figures", "journal-ref": "Mathematical Programming Series A, 146:409-436, 2014", "doi": "10.1007/s10107-013-0697-1", "report-no": null, "categories": "math.OC stat.CO stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The problem of minimizing a continuously differentiable convex function over\nan intersection of closed convex sets is ubiquitous in applied mathematics. It\nis particularly interesting when it is easy to project onto each separate set,\nbut nontrivial to project onto their intersection. Algorithms based on Newton's\nmethod such as the interior point method are viable for small to medium-scale\nproblems. However, modern applications in statistics, engineering, and machine\nlearning are posing problems with potentially tens of thousands of parameters\nor more. We revisit this convex programming problem and propose an algorithm\nthat scales well with dimensionality. Our proposal is an instance of a\nsequential unconstrained minimization technique and revolves around three\nideas: the majorization-minimization (MM) principle, the classical penalty\nmethod for constrained optimization, and quasi-Newton acceleration of\nfixed-point algorithms. The performance of our distance majorization algorithms\nis illustrated in several applications.\n", "versions": [{"version": "v1", "created": "Fri, 16 Nov 2012 14:47:43 GMT"}, {"version": "v2", "created": "Wed, 9 Jan 2013 19:25:09 GMT"}, {"version": "v3", "created": "Mon, 11 Mar 2013 14:29:39 GMT"}, {"version": "v4", "created": "Thu, 23 May 2013 17:30:10 GMT"}, {"version": "v5", "created": "Tue, 11 Jun 2013 23:08:53 GMT"}], "update_date": "2014-08-06", "authors_parsed": [["Chi", "Eric C.", ""], ["Zhou", "Hua", ""], ["Lange", "Kenneth", ""]]}, {"id": "1211.3966", "submitter": "Jie  Wang", "authors": "Jie Wang, Peter Wonka, Jieping Ye", "title": "Lasso Screening Rules via Dual Polytope Projection", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://creativecommons.org/licenses/by/3.0/", "abstract": "  Lasso is a widely used regression technique to find sparse representations.\nWhen the dimension of the feature space and the number of samples are extremely\nlarge, solving the Lasso problem remains challenging. To improve the efficiency\nof solving large-scale Lasso problems, El Ghaoui and his colleagues have\nproposed the SAFE rules which are able to quickly identify the inactive\npredictors, i.e., predictors that have $0$ components in the solution vector.\nThen, the inactive predictors or features can be removed from the optimization\nproblem to reduce its scale. By transforming the standard Lasso to its dual\nform, it can be shown that the inactive predictors include the set of inactive\nconstraints on the optimal dual solution. In this paper, we propose an\nefficient and effective screening rule via Dual Polytope Projections (DPP),\nwhich is mainly based on the uniqueness and nonexpansiveness of the optimal\ndual solution due to the fact that the feasible set in the dual space is a\nconvex and closed polytope. Moreover, we show that our screening rule can be\nextended to identify inactive groups in group Lasso. To the best of our\nknowledge, there is currently no \"exact\" screening rule for group Lasso. We\nhave evaluated our screening rule using synthetic and real data sets. Results\nshow that our rule is more effective in identifying inactive predictors than\nexisting state-of-the-art screening rules for Lasso.\n", "versions": [{"version": "v1", "created": "Fri, 16 Nov 2012 17:48:42 GMT"}, {"version": "v2", "created": "Sat, 1 Feb 2014 00:12:10 GMT"}, {"version": "v3", "created": "Wed, 15 Oct 2014 20:18:33 GMT"}], "update_date": "2014-10-17", "authors_parsed": [["Wang", "Jie", ""], ["Wonka", "Peter", ""], ["Ye", "Jieping", ""]]}, {"id": "1211.4116", "submitter": "Louis Theran", "authors": "Franz J. Kir\\'aly, Louis Theran, Ryota Tomioka", "title": "The Algebraic Combinatorial Approach for Low-Rank Matrix Completion", "comments": "37 pages, with an appendix by Takeaki Uno", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.NA math.AG math.CO stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a novel algebraic combinatorial view on low-rank matrix completion\nbased on studying relations between a few entries with tools from algebraic\ngeometry and matroid theory. The intrinsic locality of the approach allows for\nthe treatment of single entries in a closed theoretical and practical\nframework. More specifically, apart from introducing an algebraic combinatorial\ntheory of low-rank matrix completion, we present probability-one algorithms to\ndecide whether a particular entry of the matrix can be completed. We also\ndescribe methods to complete that entry from a few others, and to estimate the\nerror which is incurred by any method completing that entry. Furthermore, we\nshow how known results on matrix completion and their sampling assumptions can\nbe related to our new perspective and interpreted in terms of a completability\nphase transition.\n", "versions": [{"version": "v1", "created": "Sat, 17 Nov 2012 12:23:36 GMT"}, {"version": "v2", "created": "Mon, 26 Nov 2012 00:07:26 GMT"}, {"version": "v3", "created": "Wed, 17 Apr 2013 06:43:36 GMT"}, {"version": "v4", "created": "Tue, 19 Aug 2014 15:00:30 GMT"}], "update_date": "2014-08-20", "authors_parsed": [["Kir\u00e1ly", "Franz J.", ""], ["Theran", "Louis", ""], ["Tomioka", "Ryota", ""]]}, {"id": "1211.4142", "submitter": "Shaina Race", "authors": "Ralph Abbey, Jeremy Diepenbrock, Amy Langville, Carl Meyer, Shaina\n  Race, Dexin Zhou", "title": "Data Clustering via Principal Direction Gap Partitioning", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We explore the geometrical interpretation of the PCA based clustering\nalgorithm Principal Direction Divisive Partitioning (PDDP). We give several\nexamples where this algorithm breaks down, and suggest a new method, gap\npartitioning, which takes into account natural gaps in the data between\nclusters. Geometric features of the PCA space are derived and illustrated and\nexperimental results are given which show our method is comparable on the\ndatasets used in the original paper on PDDP.\n", "versions": [{"version": "v1", "created": "Sat, 17 Nov 2012 18:28:30 GMT"}], "update_date": "2012-11-20", "authors_parsed": [["Abbey", "Ralph", ""], ["Diepenbrock", "Jeremy", ""], ["Langville", "Amy", ""], ["Meyer", "Carl", ""], ["Race", "Shaina", ""], ["Zhou", "Dexin", ""]]}, {"id": "1211.4246", "submitter": "Yoshua Bengio", "authors": "Guillaume Alain and Yoshua Bengio", "title": "What Regularized Auto-Encoders Learn from the Data Generating\n  Distribution", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  What do auto-encoders learn about the underlying data generating\ndistribution? Recent work suggests that some auto-encoder variants do a good\njob of capturing the local manifold structure of data. This paper clarifies\nsome of these previous observations by showing that minimizing a particular\nform of regularized reconstruction error yields a reconstruction function that\nlocally characterizes the shape of the data generating density. We show that\nthe auto-encoder captures the score (derivative of the log-density with respect\nto the input). It contradicts previous interpretations of reconstruction error\nas an energy function. Unlike previous results, the theorems provided here are\ncompletely generic and do not depend on the parametrization of the\nauto-encoder: they show what the auto-encoder would tend to if given enough\ncapacity and examples. These results are for a contractive training criterion\nwe show to be similar to the denoising auto-encoder training criterion with\nsmall corruption noise, but with contraction applied on the whole\nreconstruction function rather than just encoder. Similarly to score matching,\none can consider the proposed training criterion as a convenient alternative to\nmaximum likelihood because it does not involve a partition function. Finally,\nwe show how an approximate Metropolis-Hastings MCMC can be setup to recover\nsamples from the estimated distribution, and this is confirmed in sampling\nexperiments.\n", "versions": [{"version": "v1", "created": "Sun, 18 Nov 2012 19:06:37 GMT"}, {"version": "v2", "created": "Wed, 16 Jan 2013 03:11:05 GMT"}, {"version": "v3", "created": "Tue, 12 Mar 2013 18:40:20 GMT"}, {"version": "v4", "created": "Sat, 6 Jul 2013 11:56:20 GMT"}, {"version": "v5", "created": "Tue, 19 Aug 2014 15:12:19 GMT"}], "update_date": "2014-08-20", "authors_parsed": [["Alain", "Guillaume", ""], ["Bengio", "Yoshua", ""]]}, {"id": "1211.4289", "submitter": "Loc Tran H", "authors": "Loc Tran", "title": "Application of three graph Laplacian based semi-supervised learning\n  methods to protein function prediction problem", "comments": "16 pages, 9 tables", "journal-ref": null, "doi": "10.5121/ijbb.2013.3202", "report-no": null, "categories": "cs.LG cs.CE q-bio.QM stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Protein function prediction is the important problem in modern biology. In\nthis paper, the un-normalized, symmetric normalized, and random walk graph\nLaplacian based semi-supervised learning methods will be applied to the\nintegrated network combined from multiple networks to predict the functions of\nall yeast proteins in these multiple networks. These multiple networks are\nnetwork created from Pfam domain structure, co-participation in a protein\ncomplex, protein-protein interaction network, genetic interaction network, and\nnetwork created from cell cycle gene expression measurements. Multiple networks\nare combined with fixed weights instead of using convex optimization to\ndetermine the combination weights due to high time complexity of convex\noptimization method. This simple combination method will not affect the\naccuracy performance measures of the three semi-supervised learning methods.\nExperiment results show that the un-normalized and symmetric normalized graph\nLaplacian based methods perform slightly better than random walk graph\nLaplacian based method for integrated network. Moreover, the accuracy\nperformance measures of these three semi-supervised learning methods for\nintegrated network are much better than the best accuracy performance measures\nof these three methods for the individual network.\n", "versions": [{"version": "v1", "created": "Mon, 19 Nov 2012 02:59:14 GMT"}, {"version": "v2", "created": "Mon, 3 Dec 2012 11:36:19 GMT"}, {"version": "v3", "created": "Thu, 11 Jul 2013 10:29:29 GMT"}], "update_date": "2013-07-12", "authors_parsed": [["Tran", "Loc", ""]]}, {"id": "1211.4321", "submitter": "Francois Caron", "authors": "Francois Caron (INRIA Bordeaux - Sud-Ouest, IMB), Yee Whye Teh", "title": "Bayesian nonparametric models for ranked data", "comments": "NIPS - Neural Information Processing Systems (2012)", "journal-ref": null, "doi": null, "report-no": "RR-8140", "categories": "stat.ML cs.LG stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We develop a Bayesian nonparametric extension of the popular Plackett-Luce\nchoice model that can handle an infinite number of choice items. Our framework\nis based on the theory of random atomic measures, with the prior specified by a\ngamma process. We derive a posterior characterization and a simple and\neffective Gibbs sampler for posterior simulation. We develop a time-varying\nextension of our model, and apply it to the New York Times lists of weekly\nbestselling books.\n", "versions": [{"version": "v1", "created": "Mon, 19 Nov 2012 07:40:51 GMT"}], "update_date": "2012-11-20", "authors_parsed": [["Caron", "Francois", "", "INRIA Bordeaux - Sud-Ouest, IMB"], ["Teh", "Yee Whye", ""]]}, {"id": "1211.4410", "submitter": "Sotirios Chatzis", "authors": "Emmanouil A. Platanios and Sotirios P. Chatzis", "title": "Mixture Gaussian Process Conditional Heteroscedasticity", "comments": "Technical Report, under preparation", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Generalized autoregressive conditional heteroscedasticity (GARCH) models have\nlong been considered as one of the most successful families of approaches for\nvolatility modeling in financial return series. In this paper, we propose an\nalternative approach based on methodologies widely used in the field of\nstatistical machine learning. Specifically, we propose a novel nonparametric\nBayesian mixture of Gaussian process regression models, each component of which\nmodels the noise variance process that contaminates the observed data as a\nseparate latent Gaussian process driven by the observed data. This way, we\nessentially obtain a mixture Gaussian process conditional heteroscedasticity\n(MGPCH) model for volatility modeling in financial return series. We impose a\nnonparametric prior with power-law nature over the distribution of the model\nmixture components, namely the Pitman-Yor process prior, to allow for better\ncapturing modeled data distributions with heavy tails and skewness. Finally, we\nprovide a copula- based approach for obtaining a predictive posterior for the\ncovariances over the asset returns modeled by means of a postulated MGPCH\nmodel. We evaluate the efficacy of our approach in a number of benchmark\nscenarios, and compare its performance to state-of-the-art methodologies.\n", "versions": [{"version": "v1", "created": "Mon, 19 Nov 2012 13:33:55 GMT"}, {"version": "v2", "created": "Sun, 2 Dec 2012 20:58:33 GMT"}, {"version": "v3", "created": "Thu, 6 Dec 2012 11:48:29 GMT"}, {"version": "v4", "created": "Fri, 25 Jan 2013 22:03:25 GMT"}], "update_date": "2013-01-29", "authors_parsed": [["Platanios", "Emmanouil A.", ""], ["Chatzis", "Sotirios P.", ""]]}, {"id": "1211.4601", "submitter": "Aleksandr Aravkin", "authors": "Aleksandr Y. Aravkin and James V. Burke", "title": "Smoothing Dynamic Systems with State-Dependent Covariance Matrices", "comments": "8 pages, 1 figure", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.OC stat.CO stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Kalman filtering and smoothing algorithms are used in many areas, including\ntracking and navigation, medical applications, and financial trend filtering.\nOne of the basic assumptions required to apply the Kalman smoothing framework\nis that error covariance matrices are known and given. In this paper, we study\na general class of inference problems where covariance matrices can depend\nfunctionally on unknown parameters. In the Kalman framework, this allows\nmodeling situations where covariance matrices may depend functionally on the\nstate sequence being estimated. We present an extended formulation and\ngeneralized Gauss-Newton (GGN) algorithm for inference in this context. When\napplied to dynamic systems inference, we show the algorithm can be implemented\nto preserve the computational efficiency of the classic Kalman smoother. The\nnew approach is illustrated with a synthetic numerical example.\n", "versions": [{"version": "v1", "created": "Mon, 19 Nov 2012 21:42:27 GMT"}, {"version": "v2", "created": "Thu, 20 Mar 2014 19:47:51 GMT"}], "update_date": "2014-03-21", "authors_parsed": [["Aravkin", "Aleksandr Y.", ""], ["Burke", "James V.", ""]]}, {"id": "1211.4657", "submitter": "Chen Chen", "authors": "Chen Chen and Yeqing Li and Junzhou Huang", "title": "Forest Sparsity for Multi-channel Compressive Sensing", "comments": "Accepted by IEEE Transactions on Signal Processing, 2014", "journal-ref": null, "doi": "10.1109/TSP.2014.2318138", "report-no": null, "categories": "cs.LG cs.CV cs.IT math.IT stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we investigate a new compressive sensing model for\nmulti-channel sparse data where each channel can be represented as a\nhierarchical tree and different channels are highly correlated. Therefore, the\nfull data could follow the forest structure and we call this property as\n\\emph{forest sparsity}. It exploits both intra- and inter- channel correlations\nand enriches the family of existing model-based compressive sensing theories.\nThe proposed theory indicates that only $\\mathcal{O}(Tk+\\log(N/k))$\nmeasurements are required for multi-channel data with forest sparsity, where\n$T$ is the number of channels, $N$ and $k$ are the length and sparsity number\nof each channel respectively. This result is much better than\n$\\mathcal{O}(Tk+T\\log(N/k))$ of tree sparsity, $\\mathcal{O}(Tk+k\\log(N/k))$ of\njoint sparsity, and far better than $\\mathcal{O}(Tk+Tk\\log(N/k))$ of standard\nsparsity. In addition, we extend the forest sparsity theory to the multiple\nmeasurement vectors problem, where the measurement matrix is a block-diagonal\nmatrix. The result shows that the required measurement bound can be the same as\nthat for dense random measurement matrix, when the data shares equal energy in\neach channel. A new algorithm is developed and applied on four example\napplications to validate the benefit of the proposed model. Extensive\nexperiments demonstrate the effectiveness and efficiency of the proposed theory\nand algorithm.\n", "versions": [{"version": "v1", "created": "Tue, 20 Nov 2012 03:22:45 GMT"}, {"version": "v2", "created": "Thu, 1 May 2014 15:56:00 GMT"}], "update_date": "2014-05-02", "authors_parsed": [["Chen", "Chen", ""], ["Li", "Yeqing", ""], ["Huang", "Junzhou", ""]]}, {"id": "1211.4706", "submitter": "A. Gokcen Mahmutoglu", "authors": "A. Gokcen Mahmutoglu, Alper T. Erdogan, Alper Demir", "title": "Random Input Sampling for Complex Models Using Markov Chain Monte Carlo", "comments": "12 pages, 4 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Many random processes can be simulated as the output of a deterministic model\naccepting random inputs. Such a model usually describes a complex mathematical\nor physical stochastic system and the randomness is introduced in the input\nvariables of the model. When the statistics of the output event are known,\nthese input variables have to be chosen in a specific way for the output to\nhave the prescribed statistics. Because the probability distribution of the\ninput random variables is not directly known but dictated implicitly by the\nstatistics of the output random variables, this problem is usually intractable\nfor classical sampling methods. Based on Markov Chain Monte Carlo we propose a\nnovel method to sample random inputs to such models by introducing a\nmodification to the standard Metropolis-Hastings algorithm. As an example we\nconsider a system described by a stochastic differential equation (sde) and\ndemonstrate how sample paths of a random process satisfying this sde can be\ngenerated with our technique.\n", "versions": [{"version": "v1", "created": "Tue, 20 Nov 2012 10:45:25 GMT"}], "update_date": "2012-11-21", "authors_parsed": [["Mahmutoglu", "A. Gokcen", ""], ["Erdogan", "Alper T.", ""], ["Demir", "Alper", ""]]}, {"id": "1211.4753", "submitter": "Nicholas Foti", "authors": "Nicholas J. Foti, Joseph D. Futoma, Daniel N. Rockmore, Sinead\n  Williamson", "title": "A unifying representation for a class of dependent random measures", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a general construction for dependent random measures based on\nthinning Poisson processes on an augmented space. The framework is not\nrestricted to dependent versions of a specific nonparametric model, but can be\napplied to all models that can be represented using completely random measures.\nSeveral existing dependent random measures can be seen as specific cases of\nthis framework. Interesting properties of the resulting measures are derived\nand the efficacy of the framework is demonstrated by constructing a\ncovariate-dependent latent feature model and topic model that obtain superior\npredictive performance.\n", "versions": [{"version": "v1", "created": "Tue, 20 Nov 2012 14:22:07 GMT"}], "update_date": "2012-11-21", "authors_parsed": [["Foti", "Nicholas J.", ""], ["Futoma", "Joseph D.", ""], ["Rockmore", "Daniel N.", ""], ["Williamson", "Sinead", ""]]}, {"id": "1211.4798", "submitter": "Nicholas Foti", "authors": "Nicholas J. Foti, Sinead Williamson", "title": "A survey of non-exchangeable priors for Bayesian nonparametric models", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Dependent nonparametric processes extend distributions over measures, such as\nthe Dirichlet process and the beta process, to give distributions over\ncollections of measures, typically indexed by values in some covariate space.\nSuch models are appropriate priors when exchangeability assumptions do not\nhold, and instead we want our model to vary fluidly with some set of\ncovariates. Since the concept of dependent nonparametric processes was\nformalized by MacEachern [1], there have been a number of models proposed and\nused in the statistics and machine learning literatures. Many of these models\nexhibit underlying similarities, an understanding of which, we hope, will help\nin selecting an appropriate prior, developing new models, and leveraging\ninference techniques.\n", "versions": [{"version": "v1", "created": "Tue, 20 Nov 2012 16:29:13 GMT"}], "update_date": "2012-11-21", "authors_parsed": [["Foti", "Nicholas J.", ""], ["Williamson", "Sinead", ""]]}, {"id": "1211.4860", "submitter": "Oscar Beijbom Mr", "authors": "Oscar Beijbom", "title": "Domain Adaptations for Computer Vision Applications", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A basic assumption of statistical learning theory is that train and test data\nare drawn from the same underlying distribution. Unfortunately, this assumption\ndoesn't hold in many applications. Instead, ample labeled data might exist in a\nparticular `source' domain while inference is needed in another, `target'\ndomain. Domain adaptation methods leverage labeled data from both domains to\nimprove classification on unseen data in the target domain. In this work we\nsurvey domain transfer learning methods for various application domains with\nfocus on recent work in Computer Vision.\n", "versions": [{"version": "v1", "created": "Tue, 20 Nov 2012 20:54:30 GMT"}], "update_date": "2012-11-21", "authors_parsed": [["Beijbom", "Oscar", ""]]}, {"id": "1211.4888", "submitter": "Tuhin Sahai", "authors": "Tuhin Sahai, Stefan Klus and Michael Dellnitz", "title": "A Traveling Salesman Learns Bayesian Networks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Structure learning of Bayesian networks is an important problem that arises\nin numerous machine learning applications. In this work, we present a novel\napproach for learning the structure of Bayesian networks using the solution of\nan appropriately constructed traveling salesman problem. In our approach, one\ncomputes an optimal ordering (partially ordered set) of random variables using\nmethods for the traveling salesman problem. This ordering significantly reduces\nthe search space for the subsequent greedy optimization that computes the final\nstructure of the Bayesian network. We demonstrate our approach of learning\nBayesian networks on real world census and weather datasets. In both cases, we\ndemonstrate that the approach very accurately captures dependencies between\nrandom variables. We check the accuracy of the predictions based on independent\nstudies in both application domains.\n", "versions": [{"version": "v1", "created": "Tue, 20 Nov 2012 21:50:22 GMT"}], "update_date": "2012-11-22", "authors_parsed": [["Sahai", "Tuhin", ""], ["Klus", "Stefan", ""], ["Dellnitz", "Michael", ""]]}, {"id": "1211.4909", "submitter": "Benyuan Liu", "authors": "Benyuan Liu, Zhilin Zhang, Hongqi Fan, Qiang Fu", "title": "Fast Marginalized Block Sparse Bayesian Learning Algorithm", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IT cs.LG math.IT stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The performance of sparse signal recovery from noise corrupted,\nunderdetermined measurements can be improved if both sparsity and correlation\nstructure of signals are exploited. One typical correlation structure is the\nintra-block correlation in block sparse signals. To exploit this structure, a\nframework, called block sparse Bayesian learning (BSBL), has been proposed\nrecently. Algorithms derived from this framework showed superior performance\nbut they are not very fast, which limits their applications. This work derives\nan efficient algorithm from this framework, using a marginalized likelihood\nmaximization method. Compared to existing BSBL algorithms, it has close\nrecovery performance but is much faster. Therefore, it is more suitable for\nlarge scale datasets and applications requiring real-time implementation.\n", "versions": [{"version": "v1", "created": "Wed, 21 Nov 2012 01:06:49 GMT"}, {"version": "v2", "created": "Thu, 29 Nov 2012 01:24:49 GMT"}, {"version": "v3", "created": "Fri, 18 Jan 2013 02:28:09 GMT"}, {"version": "v4", "created": "Tue, 22 Jan 2013 01:51:17 GMT"}, {"version": "v5", "created": "Mon, 4 Mar 2013 02:07:31 GMT"}, {"version": "v6", "created": "Mon, 16 Sep 2013 22:58:17 GMT"}, {"version": "v7", "created": "Sun, 29 Sep 2013 15:56:47 GMT"}], "update_date": "2013-10-01", "authors_parsed": [["Liu", "Benyuan", ""], ["Zhang", "Zhilin", ""], ["Fan", "Hongqi", ""], ["Fu", "Qiang", ""]]}, {"id": "1211.5037", "submitter": "Fran\\c{c}ois Caron", "authors": "Fran\\c{c}ois Caron, Yee Whye Teh, Thomas Brendan Murphy", "title": "Bayesian nonparametric Plackett-Luce models for the analysis of\n  preferences for college degree programmes", "comments": "Published in at http://dx.doi.org/10.1214/14-AOAS717 the Annals of\n  Applied Statistics (http://www.imstat.org/aoas/) by the Institute of\n  Mathematical Statistics (http://www.imstat.org)", "journal-ref": "Annals of Applied Statistics 2014, Vol. 8, No. 2, 1145-1181", "doi": "10.1214/14-AOAS717", "report-no": "IMS-AOAS-AOAS717", "categories": "stat.ML cs.LG stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper we propose a Bayesian nonparametric model for clustering\npartial ranking data. We start by developing a Bayesian nonparametric extension\nof the popular Plackett-Luce choice model that can handle an infinite number of\nchoice items. Our framework is based on the theory of random atomic measures,\nwith the prior specified by a completely random measure. We characterise the\nposterior distribution given data, and derive a simple and effective Gibbs\nsampler for posterior simulation. We then develop a Dirichlet process mixture\nextension of our model and apply it to investigate the clustering of\npreferences for college degree programmes amongst Irish secondary school\ngraduates. The existence of clusters of applicants who have similar preferences\nfor degree programmes is established and we determine that subject matter and\ngeographical location of the third level institution characterise these\nclusters.\n", "versions": [{"version": "v1", "created": "Wed, 21 Nov 2012 14:09:56 GMT"}, {"version": "v2", "created": "Tue, 14 Jan 2014 19:34:49 GMT"}, {"version": "v3", "created": "Fri, 1 Aug 2014 06:34:00 GMT"}], "update_date": "2014-08-04", "authors_parsed": [["Caron", "Fran\u00e7ois", ""], ["Teh", "Yee Whye", ""], ["Murphy", "Thomas Brendan", ""]]}, {"id": "1211.5194", "submitter": "Jinzhu Jia", "authors": "Junyang Qian, Jinzhu Jia", "title": "On pattern recovery of the fused Lasso", "comments": "25 pages, 5 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study the property of the Fused Lasso Signal Approximator (FLSA) for\nestimating a blocky signal sequence with additive noise. We transform the FLSA\nto an ordinary Lasso problem. By studying the property of the design matrix in\nthe transformed Lasso problem, we find that the irrepresentable condition might\nnot hold, in which case we show that the FLSA might not be able to recover the\nsignal pattern. We then apply the newly developed preconditioning method --\nPuffer Transformation [Jia and Rohe, 2012] on the transformed Lasso problem. We\ncall the new method the preconditioned fused Lasso and we give non-asymptotic\nresults for this method. Results show that when the signal jump strength\n(signal difference between two neighboring groups) is big and the noise level\nis small, our preconditioned fused Lasso estimator gives the correct pattern\nwith high probability. Theoretical results give insight on what controls the\nsignal pattern recovery ability -- it is the noise level {instead of} the\nlength of the sequence. Simulations confirm our theorems and show significant\nimprovement of the preconditioned fused Lasso estimator over the vanilla FLSA.\n", "versions": [{"version": "v1", "created": "Thu, 22 Nov 2012 03:13:05 GMT"}], "update_date": "2012-11-26", "authors_parsed": [["Qian", "Junyang", ""], ["Jia", "Jinzhu", ""]]}, {"id": "1211.5414", "submitter": "Daniel Hsu", "authors": "Daniel Hsu and Sham M. Kakade and Tong Zhang", "title": "Analysis of a randomized approximation scheme for matrix multiplication", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.LG cs.NA stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This note gives a simple analysis of a randomized approximation scheme for\nmatrix multiplication proposed by Sarlos (2006) based on a random rotation\nfollowed by uniform column sampling. The result follows from a matrix version\nof Bernstein's inequality and a tail inequality for quadratic forms in\nsubgaussian random vectors.\n", "versions": [{"version": "v1", "created": "Fri, 23 Nov 2012 06:11:54 GMT"}], "update_date": "2012-11-26", "authors_parsed": [["Hsu", "Daniel", ""], ["Kakade", "Sham M.", ""], ["Zhang", "Tong", ""]]}, {"id": "1211.5687", "submitter": "Heng Luo", "authors": "Heng Luo, Pierre Luc Carrier, Aaron Courville, Yoshua Bengio", "title": "Texture Modeling with Convolutional Spike-and-Slab RBMs and Deep\n  Extensions", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We apply the spike-and-slab Restricted Boltzmann Machine (ssRBM) to texture\nmodeling. The ssRBM with tiled-convolution weight sharing (TssRBM) achieves or\nsurpasses the state-of-the-art on texture synthesis and inpainting by\nparametric models. We also develop a novel RBM model with a spike-and-slab\nvisible layer and binary variables in the hidden layer. This model is designed\nto be stacked on top of the TssRBM. We show the resulting deep belief network\n(DBN) is a powerful generative model that improves on single-layer models and\nis capable of modeling not only single high-resolution and challenging textures\nbut also multiple textures.\n", "versions": [{"version": "v1", "created": "Sat, 24 Nov 2012 17:51:57 GMT"}], "update_date": "2012-11-27", "authors_parsed": [["Luo", "Heng", ""], ["Carrier", "Pierre Luc", ""], ["Courville", "Aaron", ""], ["Bengio", "Yoshua", ""]]}, {"id": "1211.5901", "submitter": "Nicolas Chopin", "authors": "Sumeetpal S. Singh and Nicolas Chopin and Nick Whiteley", "title": "Bayesian learning of noisy Markov decision processes", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider the inverse reinforcement learning problem, that is, the problem\nof learning from, and then predicting or mimicking a controller based on\nstate/action data. We propose a statistical model for such data, derived from\nthe structure of a Markov decision process. Adopting a Bayesian approach to\ninference, we show how latent variables of the model can be estimated, and how\npredictions about actions can be made, in a unified framework. A new Markov\nchain Monte Carlo (MCMC) sampler is devised for simulation from the posterior\ndistribution. This step includes a parameter expansion step, which is shown to\nbe essential for good convergence properties of the MCMC sampler. As an\nillustration, the method is applied to learning a human controller.\n", "versions": [{"version": "v1", "created": "Mon, 26 Nov 2012 09:55:27 GMT"}], "update_date": "2012-11-27", "authors_parsed": [["Singh", "Sumeetpal S.", ""], ["Chopin", "Nicolas", ""], ["Whiteley", "Nick", ""]]}, {"id": "1211.6085", "submitter": "Saurabh Paul", "authors": "Saurabh Paul, Christos Boutsidis, Malik Magdon-Ismail, Petros Drineas", "title": "Random Projections for Linear Support Vector Machines", "comments": "To appear in ACM TKDD, 2014. Shorter version appeared at AISTATS 2013", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Let X be a data matrix of rank \\rho, whose rows represent n points in\nd-dimensional space. The linear support vector machine constructs a hyperplane\nseparator that maximizes the 1-norm soft margin. We develop a new oblivious\ndimension reduction technique which is precomputed and can be applied to any\ninput matrix X. We prove that, with high probability, the margin and minimum\nenclosing ball in the feature space are preserved to within \\epsilon-relative\nerror, ensuring comparable generalization as in the original space in the case\nof classification. For regression, we show that the margin is preserved to\n\\epsilon-relative error with high probability. We present extensive experiments\nwith real and synthetic data to support our theory.\n", "versions": [{"version": "v1", "created": "Mon, 26 Nov 2012 20:35:12 GMT"}, {"version": "v2", "created": "Wed, 28 Nov 2012 16:26:48 GMT"}, {"version": "v3", "created": "Sat, 20 Apr 2013 21:42:22 GMT"}, {"version": "v4", "created": "Tue, 8 Oct 2013 23:57:41 GMT"}, {"version": "v5", "created": "Thu, 17 Apr 2014 19:07:11 GMT"}], "update_date": "2014-04-18", "authors_parsed": [["Paul", "Saurabh", ""], ["Boutsidis", "Christos", ""], ["Magdon-Ismail", "Malik", ""], ["Drineas", "Petros", ""]]}, {"id": "1211.6158", "submitter": "Ankan Saha", "authors": "Ankan Saha and Prateek Jain and Ambuj Tewari", "title": "The Interplay Between Stability and Regret in Online Learning", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper considers the stability of online learning algorithms and its\nimplications for learnability (bounded regret). We introduce a novel quantity\ncalled {\\em forward regret} that intuitively measures how good an online\nlearning algorithm is if it is allowed a one-step look-ahead into the future.\nWe show that given stability, bounded forward regret is equivalent to bounded\nregret. We also show that the existence of an algorithm with bounded regret\nimplies the existence of a stable algorithm with bounded regret and bounded\nforward regret. The equivalence results apply to general, possibly non-convex\nproblems. To the best of our knowledge, our analysis provides the first general\nconnection between stability and regret in the online setting that is not\nrestricted to a particular class of algorithms. Our stability-regret connection\nprovides a simple recipe for analyzing regret incurred by any online learning\nalgorithm. Using our framework, we analyze several existing online learning\nalgorithms as well as the \"approximate\" versions of algorithms like RDA that\nsolve an optimization problem at each iteration. Our proofs are simpler than\nexisting analysis for the respective algorithms, show a clear trade-off between\nstability and forward regret, and provide tighter regret bounds in some cases.\nFurthermore, using our recipe, we analyze \"approximate\" versions of several\nalgorithms such as follow-the-regularized-leader (FTRL) that requires solving\nan optimization problem at each step.\n", "versions": [{"version": "v1", "created": "Mon, 26 Nov 2012 23:13:23 GMT"}], "update_date": "2012-11-28", "authors_parsed": [["Saha", "Ankan", ""], ["Jain", "Prateek", ""], ["Tewari", "Ambuj", ""]]}, {"id": "1211.6248", "submitter": "Arnim Bleier", "authors": "Arnim Bleier", "title": "A simple non-parametric Topic Mixture for Authors and Documents", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This article reviews the Author-Topic Model and presents a new non-parametric\nextension based on the Hierarchical Dirichlet Process. The extension is\nespecially suitable when no prior information about the number of components\nnecessary is available. A blocked Gibbs sampler is described and focus put on\nstaying as close as possible to the original model with only the minimum of\ntheoretical and implementation overhead necessary.\n", "versions": [{"version": "v1", "created": "Tue, 27 Nov 2012 09:36:22 GMT"}, {"version": "v2", "created": "Tue, 4 Dec 2012 13:50:19 GMT"}], "update_date": "2012-12-05", "authors_parsed": [["Bleier", "Arnim", ""]]}, {"id": "1211.6302", "submitter": "Francis Bach", "authors": "Francis Bach (INRIA Paris - Rocquencourt, LIENS)", "title": "Duality between subgradient and conditional gradient methods", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG math.OC stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Given a convex optimization problem and its dual, there are many possible\nfirst-order algorithms. In this paper, we show the equivalence between mirror\ndescent algorithms and algorithms generalizing the conditional gradient method.\nThis is done through convex duality, and implies notably that for certain\nproblems, such as for supervised machine learning problems with non-smooth\nlosses or problems regularized by non-smooth regularizers, the primal\nsubgradient method and the dual conditional gradient method are formally\nequivalent. The dual interpretation leads to a form of line search for mirror\ndescent, as well as guarantees of convergence for primal-dual certificates.\n", "versions": [{"version": "v1", "created": "Tue, 27 Nov 2012 13:46:59 GMT"}, {"version": "v2", "created": "Wed, 2 Jan 2013 17:48:58 GMT"}, {"version": "v3", "created": "Fri, 18 Oct 2013 17:02:13 GMT"}], "update_date": "2013-10-21", "authors_parsed": [["Bach", "Francis", "", "INRIA Paris - Rocquencourt, LIENS"]]}, {"id": "1211.6451", "submitter": "Paul McNicholas", "authors": "Sakyajit Bhattacharya and Paul D. McNicholas", "title": "A LASSO-Penalized BIC for Mixture Model Selection", "comments": null, "journal-ref": null, "doi": "10.1007/s11634-013-0155-1", "report-no": null, "categories": "stat.ME math.ST stat.CO stat.ML stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The efficacy of family-based approaches to mixture model-based clustering and\nclassification depends on the selection of parsimonious models. Current wisdom\nsuggests the Bayesian information criterion (BIC) for mixture model selection.\nHowever, the BIC has well-known limitations, including a tendency to\noverestimate the number of components as well as a proclivity for, often\ndrastically, underestimating the number of components in higher dimensions.\nWhile the former problem might be soluble through merging components, the\nlatter is impossible to mitigate in clustering and classification applications.\nIn this paper, a LASSO-penalized BIC (LPBIC) is introduced to overcome this\nproblem. This approach is illustrated based on applications of extensions of\nmixtures of factor analyzers, where the LPBIC is used to select both the number\nof components and the number of latent factors. The LPBIC is shown to match or\noutperform the BIC in several situations.\n", "versions": [{"version": "v1", "created": "Tue, 27 Nov 2012 21:11:41 GMT"}], "update_date": "2013-11-12", "authors_parsed": [["Bhattacharya", "Sakyajit", ""], ["McNicholas", "Paul D.", ""]]}, {"id": "1211.6653", "submitter": "Yuyang Wang", "authors": "Yuyang Wang, Roni Khardon", "title": "Nonparametric Bayesian Mixed-effect Model: a Sparse Gaussian Process\n  Approach", "comments": "Preliminary version appeared in ECML2012", "journal-ref": null, "doi": "10.1007/978-3-642-33460-3_51", "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Multi-task learning models using Gaussian processes (GP) have been developed\nand successfully applied in various applications. The main difficulty with this\napproach is the computational cost of inference using the union of examples\nfrom all tasks. Therefore sparse solutions, that avoid using the entire data\ndirectly and instead use a set of informative \"representatives\" are desirable.\nThe paper investigates this problem for the grouped mixed-effect GP model where\neach individual response is given by a fixed-effect, taken from one of a set of\nunknown groups, plus a random individual effect function that captures\nvariations among individuals. Such models have been widely used in previous\nwork but no sparse solutions have been developed. The paper presents the first\nsparse solution for such problems, showing how the sparse approximation can be\nobtained by maximizing a variational lower bound on the marginal likelihood,\ngeneralizing ideas from single-task Gaussian processes to handle the\nmixed-effect model as well as grouping. Experiments using artificial and real\ndata validate the approach showing that it can recover the performance of\ninference with the full sample, that it outperforms baseline methods, and that\nit outperforms state of the art sparse solutions for other multi-task GP\nformulations.\n", "versions": [{"version": "v1", "created": "Wed, 28 Nov 2012 16:50:23 GMT"}], "update_date": "2012-11-29", "authors_parsed": [["Wang", "Yuyang", ""], ["Khardon", "Roni", ""]]}, {"id": "1211.6675", "submitter": "Dalton Lunga", "authors": "Dalton Lunga 'and' Okan Ersoy", "title": "Nonlinear Dynamic Field Embedding: On Hyperspectral Scene Visualization", "comments": "49 pages, 18 figures", "journal-ref": null, "doi": null, "report-no": "TR-ECE-12-14", "categories": "cs.CV cs.CE stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Graph embedding techniques are useful to characterize spectral signature\nrelations for hyperspectral images. However, such images consists of disjoint\nclasses due to spatial details that are often ignored by existing graph\ncomputing tools. Robust parameter estimation is a challenge for kernel\nfunctions that compute such graphs. Finding a corresponding high quality\ncoordinate system to map signature relations remains an open research question.\nWe answer positively on these challenges by first proposing a kernel function\nof spatial and spectral information in computing neighborhood graphs. Secondly,\nthe study exploits the force field interpretation from mechanics and devise a\nunifying nonlinear graph embedding framework. The generalized framework leads\nto novel unsupervised multidimensional artificial field embedding techniques\nthat rely on the simple additive assumption of pair-dependent attraction and\nrepulsion functions. The formulations capture long range and short range\ndistance related effects often associated with living organisms and help to\nestablish algorithmic properties that mimic mutual behavior for the purpose of\ndimensionality reduction. The main benefits from the proposed models includes\nthe ability to preserve the local topology of data and produce quality\nvisualizations i.e. maintaining disjoint meaningful neighborhoods. As part of\nevaluation, visualization, gradient field trajectories, and semisupervised\nclassification experiments are conducted for image scenes acquired by multiple\nsensors at various spatial resolutions over different types of objects. The\nresults demonstrate the superiority of the proposed embedding framework over\nvarious widely used methods.\n", "versions": [{"version": "v1", "created": "Wed, 28 Nov 2012 17:39:16 GMT"}], "update_date": "2012-11-29", "authors_parsed": [["Ersoy", "Dalton Lunga 'and' Okan", ""]]}, {"id": "1211.6687", "submitter": "Nicolas Gillis", "authors": "Nicolas Gillis", "title": "Robustness Analysis of Hottopixx, a Linear Programming Model for\n  Factoring Nonnegative Matrices", "comments": "23 pages; new numerical results; Comparison with Arora et al.;\n  Accepted in SIAM J. Mat. Anal. Appl", "journal-ref": "SIAM J. Matrix Anal. & Appl. 34 (3), pp. 1189-1212, 2013", "doi": "10.1137/120900629", "report-no": null, "categories": "stat.ML cs.LG cs.NA math.OC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Although nonnegative matrix factorization (NMF) is NP-hard in general, it has\nbeen shown very recently that it is tractable under the assumption that the\ninput nonnegative data matrix is close to being separable (separability\nrequires that all columns of the input matrix belongs to the cone spanned by a\nsmall subset of these columns). Since then, several algorithms have been\ndesigned to handle this subclass of NMF problems. In particular, Bittorf,\nRecht, R\\'e and Tropp (`Factoring nonnegative matrices with linear programs',\nNIPS 2012) proposed a linear programming model, referred to as Hottopixx. In\nthis paper, we provide a new and more general robustness analysis of their\nmethod. In particular, we design a provably more robust variant using a\npost-processing strategy which allows us to deal with duplicates and near\nduplicates in the dataset.\n", "versions": [{"version": "v1", "created": "Wed, 28 Nov 2012 18:05:56 GMT"}, {"version": "v2", "created": "Tue, 4 Dec 2012 16:06:55 GMT"}, {"version": "v3", "created": "Sun, 17 Feb 2013 08:53:06 GMT"}, {"version": "v4", "created": "Fri, 31 May 2013 15:06:57 GMT"}], "update_date": "2013-08-19", "authors_parsed": [["Gillis", "Nicolas", ""]]}, {"id": "1211.6807", "submitter": "Sungmin Kim", "authors": "Sungmin Kim and Tao Shi", "title": "Scalable Spectral Algorithms for Community Detection in Directed\n  Networks", "comments": "Single column, 40 pages, 6 figures and 7 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.SI physics.soc-ph stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Community detection has been one of the central problems in network studies\nand directed network is particularly challenging due to asymmetry among its\nlinks. In this paper, we found that incorporating the direction of links\nreveals new perspectives on communities regarding to two different roles,\nsource and terminal, that a node plays in each community. Intriguingly, such\ncommunities appear to be connected with unique spectral property of the graph\nLaplacian of the adjacency matrix and we exploit this connection by using\nregularized SVD methods. We propose harvesting algorithms, coupled with\nregularized SVDs, that are linearly scalable for efficient identification of\ncommunities in huge directed networks. The proposed algorithm shows great\nperformance and scalability on benchmark networks in simulations and\nsuccessfully recovers communities in real network applications.\n", "versions": [{"version": "v1", "created": "Thu, 29 Nov 2012 03:35:17 GMT"}, {"version": "v2", "created": "Mon, 23 Sep 2013 17:15:15 GMT"}], "update_date": "2013-09-24", "authors_parsed": [["Kim", "Sungmin", ""], ["Shi", "Tao", ""]]}, {"id": "1211.6851", "submitter": "Chiheb-Eddine Ben n'cir C.B.N'cir", "authors": "Chiheb-Eddine Ben N'Cir and Nadia Essoussi", "title": "Classification Recouvrante Bas\\'ee sur les M\\'ethodes \\`a Noyau", "comments": "Les 43\\`emes Journ\\'ees de Statistique", "journal-ref": "Les 43\\`emes Journ\\'ees de Statistique 2011", "doi": null, "report-no": null, "categories": "cs.LG stat.CO stat.ME stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Overlapping clustering problem is an important learning issue in which\nclusters are not mutually exclusive and each object may belongs simultaneously\nto several clusters. This paper presents a kernel based method that produces\noverlapping clusters on a high feature space using mercer kernel techniques to\nimprove separability of input patterns. The proposed method, called\nOKM-K(Overlapping $k$-means based kernel method), extends OKM (Overlapping\n$k$-means) method to produce overlapping schemes. Experiments are performed on\noverlapping dataset and empirical results obtained with OKM-K outperform\nresults obtained with OKM.\n", "versions": [{"version": "v1", "created": "Thu, 29 Nov 2012 09:22:19 GMT"}], "update_date": "2012-11-30", "authors_parsed": [["N'Cir", "Chiheb-Eddine Ben", ""], ["Essoussi", "Nadia", ""]]}, {"id": "1211.6859", "submitter": "Chiheb-Eddine Ben n'cir C.B.N'cir", "authors": "Chiheb-Eddine Ben N'Cir and Nadia Essoussi and Patrice Bertrand", "title": "Overlapping clustering based on kernel similarity metric", "comments": "Second Meeting on Statistics and Data Mining 2010", "journal-ref": "Second Meeting on Statistics and Data Mining Second Meeting on\n  Statistics and Data Mining March 11-12, 2010", "doi": null, "report-no": null, "categories": "stat.ML cs.LG stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Producing overlapping schemes is a major issue in clustering. Recent proposed\noverlapping methods relies on the search of an optimal covering and are based\non different metrics, such as Euclidean distance and I-Divergence, used to\nmeasure closeness between observations. In this paper, we propose the use of\nanother measure for overlapping clustering based on a kernel similarity metric\n.We also estimate the number of overlapped clusters using the Gram matrix.\nExperiments on both Iris and EachMovie datasets show the correctness of the\nestimation of number of clusters and show that measure based on kernel\nsimilarity metric improves the precision, recall and f-measure in overlapping\nclustering.\n", "versions": [{"version": "v1", "created": "Thu, 29 Nov 2012 09:35:30 GMT"}], "update_date": "2012-11-30", "authors_parsed": [["N'Cir", "Chiheb-Eddine Ben", ""], ["Essoussi", "Nadia", ""], ["Bertrand", "Patrice", ""]]}, {"id": "1211.6950", "submitter": "Gonzalo Mateos", "authors": "Gonzalo Mateos and Ketan Rajawat", "title": "Dynamic Network Cartography", "comments": "To appear in the IEEE Signal Processing Magazine - Special Issue on\n  Adaptation and Learning over Complex Networks", "journal-ref": null, "doi": "10.1109/MSP.2012.2232355", "report-no": null, "categories": "cs.NI cs.IT cs.MA math.IT stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Communication networks have evolved from specialized, research and tactical\ntransmission systems to large-scale and highly complex interconnections of\nintelligent devices, increasingly becoming more commercial, consumer-oriented,\nand heterogeneous. Propelled by emergent social networking services and\nhigh-definition streaming platforms, network traffic has grown explosively\nthanks to the advances in processing speed and storage capacity of\nstate-of-the-art communication technologies. As \"netizens\" demand a seamless\nnetworking experience that entails not only higher speeds, but also resilience\nand robustness to failures and malicious cyber-attacks, ample opportunities for\nsignal processing (SP) research arise. The vision is for ubiquitous smart\nnetwork devices to enable data-driven statistical learning algorithms for\ndistributed, robust, and online network operation and management, adaptable to\nthe dynamically-evolving network landscape with minimal need for human\nintervention. The present paper aims at delineating the analytical background\nand the relevance of SP tools to dynamic network monitoring, introducing the SP\nreadership to the concept of dynamic network cartography -- a framework to\nconstruct maps of the dynamic network state in an efficient and scalable manner\ntailored to large-scale heterogeneous networks.\n", "versions": [{"version": "v1", "created": "Thu, 29 Nov 2012 15:26:23 GMT"}], "update_date": "2013-10-01", "authors_parsed": [["Mateos", "Gonzalo", ""], ["Rajawat", "Ketan", ""]]}, {"id": "1211.7120", "submitter": "Sinead Williamson", "authors": "Sinead A. Williamson, Avinava Dubey and Eric P. Xing", "title": "Exact and Efficient Parallel Inference for Nonparametric Mixture Models", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Nonparametric mixture models based on the Dirichlet process are an elegant\nalternative to finite models when the number of underlying components is\nunknown, but inference in such models can be slow. Existing attempts to\nparallelize inference in such models have relied on introducing approximations,\nwhich can lead to inaccuracies in the posterior estimate. In this paper, we\ndescribe auxiliary variable representations for the Dirichlet process and the\nhierarchical Dirichlet process that allow us to sample from the true posterior\nin a distributed manner. We show that our approach allows scalable inference\nwithout the deterioration in estimate quality that accompanies existing\nmethods.\n", "versions": [{"version": "v1", "created": "Thu, 29 Nov 2012 23:39:00 GMT"}], "update_date": "2012-12-03", "authors_parsed": [["Williamson", "Sinead A.", ""], ["Dubey", "Avinava", ""], ["Xing", "Eric P.", ""]]}, {"id": "1211.7219", "submitter": "Qian Zhao", "authors": "Qian Zhao and Deyu Meng and Zongben Xu", "title": "A recursive divide-and-conquer approach for sparse principal component\n  analysis", "comments": "35 pages, 4 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, a new method is proposed for sparse PCA based on the recursive\ndivide-and-conquer methodology. The main idea is to separate the original\nsparse PCA problem into a series of much simpler sub-problems, each having a\nclosed-form solution. By recursively solving these sub-problems in an\nanalytical way, an efficient algorithm is constructed to solve the sparse PCA\nproblem. The algorithm only involves simple computations and is thus easy to\nimplement. The proposed method can also be very easily extended to other sparse\nPCA problems with certain constraints, such as the nonnegative sparse PCA\nproblem. Furthermore, we have shown that the proposed algorithm converges to a\nstationary point of the problem, and its computational complexity is\napproximately linear in both data size and dimensionality. The effectiveness of\nthe proposed method is substantiated by extensive experiments implemented on a\nseries of synthetic and real data in both reconstruction-error-minimization and\ndata-variance-maximization viewpoints.\n", "versions": [{"version": "v1", "created": "Fri, 30 Nov 2012 11:50:21 GMT"}], "update_date": "2012-12-03", "authors_parsed": [["Zhao", "Qian", ""], ["Meng", "Deyu", ""], ["Xu", "Zongben", ""]]}, {"id": "1211.7276", "submitter": "Duc Son Pham", "authors": "Duc Son Pham and Svetha Venkatesh", "title": "Efficient algorithms for robust recovery of images from compressed data", "comments": "Sequel of a related IEEE Transactions on Image Processing paper", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IT cs.LG math.IT stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Compressed sensing (CS) is an important theory for sub-Nyquist sampling and\nrecovery of compressible data. Recently, it has been extended by Pham and\nVenkatesh to cope with the case where corruption to the CS data is modeled as\nimpulsive noise. The new formulation, termed as robust CS, combines robust\nstatistics and CS into a single framework to suppress outliers in the CS\nrecovery. To solve the newly formulated robust CS problem, Pham and Venkatesh\nsuggested a scheme that iteratively solves a number of CS problems, the\nsolutions from which converge to the true robust compressed sensing solution.\nHowever, this scheme is rather inefficient as it has to use existing CS solvers\nas a proxy. To overcome limitation with the original robust CS algorithm, we\npropose to solve the robust CS problem directly in this paper and drive more\ncomputationally efficient algorithms by following latest advances in\nlarge-scale convex optimization for non-smooth regularization. Furthermore, we\nalso extend the robust CS formulation to various settings, including additional\naffine constraints, $\\ell_1$-norm loss function, mixed-norm regularization, and\nmulti-tasking, so as to further improve robust CS. We also derive simple but\neffective algorithms to solve these extensions. We demonstrate that the new\nalgorithms provide much better computational advantage over the original robust\nCS formulation, and effectively solve more sophisticated extensions where the\noriginal methods simply cannot. We demonstrate the usefulness of the extensions\non several CS imaging tasks.\n", "versions": [{"version": "v1", "created": "Mon, 26 Nov 2012 15:01:15 GMT"}], "update_date": "2012-12-03", "authors_parsed": [["Pham", "Duc Son", ""], ["Venkatesh", "Svetha", ""]]}, {"id": "1211.7369", "submitter": "Franz J. Kir\\'aly", "authors": "Franz J. Kir\\'aly and Andreas Ziehe", "title": "Approximate Rank-Detecting Factorization of Low-Rank Tensors", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG math.NA", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present an algorithm, AROFAC2, which detects the (CP-)rank of a degree 3\ntensor and calculates its factorization into rank-one components. We provide\ngenerative conditions for the algorithm to work and demonstrate on both\nsynthetic and real world data that AROFAC2 is a potentially outperforming\nalternative to the gold standard PARAFAC over which it has the advantages that\nit can intrinsically detect the true rank, avoids spurious components, and is\nstable with respect to outliers and non-Gaussian noise.\n", "versions": [{"version": "v1", "created": "Fri, 30 Nov 2012 20:50:40 GMT"}], "update_date": "2012-12-03", "authors_parsed": [["Kir\u00e1ly", "Franz J.", ""], ["Ziehe", "Andreas", ""]]}]