[{"id": "0811.0146", "submitter": "Alain Lifchitz", "authors": "Alain Lifchitz (LIP6), Sandra Jhean-Larose (LPC), Guy Denhi\\`ere (LPC)", "title": "Effect of Tuned Parameters on a LSA MCQ Answering Model", "comments": "9 pages", "journal-ref": "Behavior Research Methods, 41 (4), p. 1201-1209, November 2009", "doi": "10.3758/BRM.41.4.1201", "report-no": null, "categories": "cs.LG cs.AI stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper presents the current state of a work in progress, whose objective\nis to better understand the effects of factors that significantly influence the\nperformance of Latent Semantic Analysis (LSA). A difficult task, which consists\nin answering (French) biology Multiple Choice Questions, is used to test the\nsemantic properties of the truncated singular space and to study the relative\ninfluence of main parameters. A dedicated software has been designed to fine\ntune the LSA semantic space for the Multiple Choice Questions task. With\noptimal parameters, the performances of our simple model are quite surprisingly\nequal or superior to those of 7th and 8th grades students. This indicates that\nsemantic spaces were quite good despite their low dimensions and the small\nsizes of training data sets. Besides, we present an original entropy global\nweighting of answers' terms of each question of the Multiple Choice Questions\nwhich was necessary to achieve the model's success.\n", "versions": [{"version": "v1", "created": "Sun, 2 Nov 2008 09:21:40 GMT"}, {"version": "v2", "created": "Mon, 17 Nov 2008 19:54:50 GMT"}, {"version": "v3", "created": "Thu, 14 May 2009 12:51:44 GMT"}], "update_date": "2009-12-10", "authors_parsed": [["Lifchitz", "Alain", "", "LIP6"], ["Jhean-Larose", "Sandra", "", "LPC"], ["Denhi\u00e8re", "Guy", "", "LPC"]]}, {"id": "0811.0484", "submitter": "Aaron Clauset", "authors": "Aaron Clauset and Cristopher Moore and M.E.J. Newman", "title": "Hierarchical structure and the prediction of missing links in networks", "comments": "8 pages, 7 figures, 1 table, includes Supplementary Information", "journal-ref": "Nature 453, 98 - 101 (2008)", "doi": "10.1038/nature06830", "report-no": null, "categories": "stat.ML physics.soc-ph q-bio.MN", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Networks have in recent years emerged as an invaluable tool for describing\nand quantifying complex systems in many branches of science. Recent studies\nsuggest that networks often exhibit hierarchical organization, where vertices\ndivide into groups that further subdivide into groups of groups, and so forth\nover multiple scales. In many cases these groups are found to correspond to\nknown functional units, such as ecological niches in food webs, modules in\nbiochemical networks (protein interaction networks, metabolic networks, or\ngenetic regulatory networks), or communities in social networks. Here we\npresent a general technique for inferring hierarchical structure from network\ndata and demonstrate that the existence of hierarchy can simultaneously explain\nand quantitatively reproduce many commonly observed topological properties of\nnetworks, such as right-skewed degree distributions, high clustering\ncoefficients, and short path lengths. We further show that knowledge of\nhierarchical structure can be used to predict missing connections in partially\nknown networks with high accuracy, and for more general network structures than\ncompeting techniques. Taken together, our results suggest that hierarchy is a\ncentral organizing principle of complex networks, capable of offering insight\ninto many network phenomena.\n", "versions": [{"version": "v1", "created": "Tue, 4 Nov 2008 15:30:37 GMT"}], "update_date": "2008-11-05", "authors_parsed": [["Clauset", "Aaron", ""], ["Moore", "Cristopher", ""], ["Newman", "M. E. J.", ""]]}, {"id": "0811.1067", "submitter": "Lek-Heng Lim", "authors": "Xiaoye Jiang, Lek-Heng Lim, Yuan Yao, Yinyu Ye", "title": "Statistical ranking and combinatorial Hodge theory", "comments": "42 pages; minor changes throughout; numerical experiments added", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.DM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a number of techniques for obtaining a global ranking from data\nthat may be incomplete and imbalanced -- characteristics almost universal to\nmodern datasets coming from e-commerce and internet applications. We are\nprimarily interested in score or rating-based cardinal data. From raw ranking\ndata, we construct pairwise rankings, represented as edge flows on an\nappropriate graph. Our statistical ranking method uses the graph Helmholtzian,\nthe graph theoretic analogue of the Helmholtz operator or vector Laplacian, in\nmuch the same way the graph Laplacian is an analogue of the Laplace operator or\nscalar Laplacian. We study the graph Helmholtzian using combinatorial Hodge\ntheory: we show that every edge flow representing pairwise ranking can be\nresolved into two orthogonal components, a gradient flow that represents the\nL2-optimal global ranking and a divergence-free flow (cyclic) that measures the\nvalidity of the global ranking obtained -- if this is large, then the data does\nnot have a meaningful global ranking. This divergence-free flow can be further\ndecomposed orthogonally into a curl flow (locally cyclic) and a harmonic flow\n(locally acyclic but globally cyclic); these provides information on whether\ninconsistency arises locally or globally. An obvious advantage over the NP-hard\nKemeny optimization is that discrete Hodge decomposition may be computed via a\nlinear least squares regression. We also investigated the L1-projection of edge\nflows, showing that this is dual to correlation maximization over bounded\ndivergence-free flows, and the L1-approximate sparse cyclic ranking, showing\nthat this is dual to correlation maximization over bounded curl-free flows. We\ndiscuss relations with Kemeny optimization, Borda count, and Kendall-Smith\nconsistency index from social choice theory and statistics.\n", "versions": [{"version": "v1", "created": "Fri, 7 Nov 2008 01:23:09 GMT"}, {"version": "v2", "created": "Mon, 10 Aug 2009 10:34:29 GMT"}], "update_date": "2009-08-10", "authors_parsed": [["Jiang", "Xiaoye", ""], ["Lim", "Lek-Heng", ""], ["Yao", "Yuan", ""], ["Ye", "Yinyu", ""]]}, {"id": "0811.1239", "submitter": "Mladen Kolar", "authors": "M. Kolar, E. P. Xing", "title": "Improved Estimation of High-dimensional Ising Models", "comments": "8 pages, 3 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider the problem of jointly estimating the parameters as well as the\nstructure of binary valued Markov Random Fields, in contrast to earlier work\nthat focus on one of the two problems. We formulate the problem as a\nmaximization of $\\ell_1$-regularized surrogate likelihood that allows us to\nfind a sparse solution. Our optimization technique efficiently incorporates the\ncutting-plane algorithm in order to obtain a tighter outer bound on the\nmarginal polytope, which results in improvement of both parameter estimates and\napproximation to marginals. On synthetic data, we compare our algorithm on the\ntwo estimation tasks to the other existing methods. We analyze the method in\nthe high-dimensional setting, where the number of dimensions $p$ is allowed to\ngrow with the number of observations $n$. The rate of convergence of the\nestimate is demonstrated to depend explicitly on the sparsity of the underlying\ngraph.\n", "versions": [{"version": "v1", "created": "Fri, 7 Nov 2008 23:33:40 GMT"}], "update_date": "2008-11-11", "authors_parsed": [["Kolar", "M.", ""], ["Xing", "E. P.", ""]]}, {"id": "0811.2026", "submitter": "Seyoung Kim", "authors": "Seyoung Kim, Kyung-Ah Sohn and Eric P. Xing", "title": "A Multivariate Regression Approach to Association Analysis of\n  Quantitative Trait Network", "comments": "Submitted to The American Journal of Human Genetics", "journal-ref": null, "doi": null, "report-no": "CMU-ML-08-113", "categories": "stat.ML q-bio.GN q-bio.MN q-bio.QM stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Many complex disease syndromes such as asthma consist of a large number of\nhighly related, rather than independent, clinical phenotypes, raising a new\ntechnical challenge in identifying genetic variations associated simultaneously\nwith correlated traits. In this study, we propose a new statistical framework\ncalled graph-guided fused lasso (GFlasso) to address this issue in a principled\nway. Our approach explicitly represents the dependency structure among the\nquantitative traits as a network, and leverages this trait network to encode\nstructured regularizations in a multivariate regression model over the\ngenotypes and traits, so that the genetic markers that jointly influence\nsubgroups of highly correlated traits can be detected with high sensitivity and\nspecificity. While most of the traditional methods examined each phenotype\nindependently and combined the results afterwards, our approach analyzes all of\nthe traits jointly in a single statistical method, and borrow information\nacross correlated phenotypes to discover the genetic markers that perturbe a\nsubset of correlated triats jointly rather than a single trait. Using simulated\ndatasets based on the HapMap consortium data and an asthma dataset, we compare\nthe performance of our method with the single-marker analysis, and other sparse\nregression methods such as the ridge regression and the lasso that do not use\nany structural information in the traits. Our results show that there is a\nsignificant advantage in detecting the true causal SNPs when we incorporate the\ncorrelation pattern in traits using our proposed methods.\n", "versions": [{"version": "v1", "created": "Thu, 13 Nov 2008 03:44:48 GMT"}], "update_date": "2008-11-16", "authors_parsed": [["Kim", "Seyoung", ""], ["Sohn", "Kyung-Ah", ""], ["Xing", "Eric P.", ""]]}, {"id": "0811.2177", "submitter": "Nicolai Meinshausen", "authors": "Nicolai Meinshausen, Lukas Meier and Peter B\\\"uhlmann", "title": "P-values for high-dimensional regression", "comments": "25 pages, 4 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Assigning significance in high-dimensional regression is challenging. Most\ncomputationally efficient selection algorithms cannot guard against inclusion\nof noise variables. Asymptotically valid p-values are not available. An\nexception is a recent proposal by Wasserman and Roeder (2008) which splits the\ndata into two parts. The number of variables is then reduced to a manageable\nsize using the first split, while classical variable selection techniques can\nbe applied to the remaining variables, using the data from the second split.\nThis yields asymptotic error control under minimal conditions. It involves,\nhowever, a one-time random split of the data. Results are sensitive to this\narbitrary choice: it amounts to a `p-value lottery' and makes it difficult to\nreproduce results. Here, we show that inference across multiple random splits\ncan be aggregated, while keeping asymptotic control over the inclusion of noise\nvariables. We show that the resulting p-values can be used for control of both\nfamily-wise error (FWER) and false discovery rate (FDR). In addition, the\nproposed aggregation is shown to improve power while reducing the number of\nfalsely selected variables substantially.\n", "versions": [{"version": "v1", "created": "Thu, 13 Nov 2008 20:15:30 GMT"}, {"version": "v2", "created": "Wed, 25 Feb 2009 14:07:07 GMT"}, {"version": "v3", "created": "Fri, 12 Jun 2009 12:50:33 GMT"}], "update_date": "2009-06-12", "authors_parsed": [["Meinshausen", "Nicolai", ""], ["Meier", "Lukas", ""], ["B\u00fchlmann", "Peter", ""]]}, {"id": "0811.3174", "submitter": "Ravi Venkatesan", "authors": "R. C. Venkatesan and A. Plastino", "title": "Deformed Statistics Formulation of the Information Bottleneck Method", "comments": "6 pages. Expanded analysis, typographical corrections, 1 reference\n  added", "journal-ref": null, "doi": null, "report-no": null, "categories": "cond-mat.stat-mech physics.data-an stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The theoretical basis for a candidate variational principle for the\ninformation bottleneck (IB) method is formulated within the ambit of the\ngeneralized nonadditive statistics of Tsallis. Given a nonadditivity parameter\n$ q $, the role of the \\textit{additive duality} of nonadditive statistics ($\nq^*=2-q $) in relating Tsallis entropies for ranges of the nonadditivity\nparameter $ q < 1 $ and $ q > 1 $ is described. Defining $ X $, $ \\tilde X $,\nand $ Y $ to be the source alphabet, the compressed reproduction alphabet, and,\nthe \\textit{relevance variable} respectively, it is demonstrated that\nminimization of a generalized IB (gIB) Lagrangian defined in terms of the\nnonadditivity parameter $ q^* $ self-consistently yields the\n\\textit{nonadditive effective distortion measure} to be the \\textit{$ q\n$-deformed} generalized Kullback-Leibler divergence: $\nD_{K-L}^{q}[p(Y|X)||p(Y|\\tilde X)] $. This result is achieved without enforcing\nany \\textit{a-priori} assumptions. Next, it is proven that the $q^*-deformed $\nnonadditive free energy of the system is non-negative and convex. Finally, the\nupdate equations for the gIB method are derived. These results generalize\ncritical features of the IB method to the case of Tsallis statistics.\n", "versions": [{"version": "v1", "created": "Wed, 19 Nov 2008 20:39:23 GMT"}, {"version": "v2", "created": "Sat, 22 Nov 2008 19:39:55 GMT"}, {"version": "v3", "created": "Thu, 15 Jan 2009 07:53:55 GMT"}, {"version": "v4", "created": "Fri, 1 May 2009 04:17:16 GMT"}], "update_date": "2009-05-01", "authors_parsed": [["Venkatesan", "R. C.", ""], ["Plastino", "A.", ""]]}, {"id": "0811.3499", "submitter": "Steffen Kuehn", "authors": "Steffen Kuehn", "title": "Kernel Regression by Mode Calculation of the Conditional Probability\n  Distribution", "comments": "11 pages, 5 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The most direct way to express arbitrary dependencies in datasets is to\nestimate the joint distribution and to apply afterwards the argmax-function to\nobtain the mode of the corresponding conditional distribution. This method is\nin practice difficult, because it requires a global optimization of a\ncomplicated function, the joint distribution by fixed input variables. This\narticle proposes a method for finding global maxima if the joint distribution\nis modeled by a kernel density estimation. Some experiments show advantages and\nshortcomings of the resulting regression method in comparison to the standard\nNadaraya-Watson regression technique, which approximates the optimum by the\nexpectation value.\n", "versions": [{"version": "v1", "created": "Fri, 21 Nov 2008 10:17:00 GMT"}], "update_date": "2008-11-24", "authors_parsed": [["Kuehn", "Steffen", ""]]}, {"id": "0811.3579", "submitter": "Korbinian Strimmer", "authors": "Jean Hausser and Korbinian Strimmer", "title": "Entropy inference and the James-Stein estimator, with application to\n  nonlinear gene association networks", "comments": "18 pages, 3 figures, 1 table", "journal-ref": "Journal of Machine Learning Research 10: 1469-1484 (2009)", "doi": null, "report-no": null, "categories": "stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a procedure for effective estimation of entropy and mutual\ninformation from small-sample data, and apply it to the problem of inferring\nhigh-dimensional gene association networks. Specifically, we develop a\nJames-Stein-type shrinkage estimator, resulting in a procedure that is highly\nefficient statistically as well as computationally. Despite its simplicity, we\nshow that it outperforms eight other entropy estimation procedures across a\ndiverse range of sampling scenarios and data-generating models, even in cases\nof severe undersampling. We illustrate the approach by analyzing E. coli gene\nexpression data and computing an entropy-based gene-association network from\ngene expression data. A computer program is available that implements the\nproposed shrinkage estimator.\n", "versions": [{"version": "v1", "created": "Fri, 21 Nov 2008 16:47:58 GMT"}, {"version": "v2", "created": "Wed, 31 Dec 2008 15:30:45 GMT"}, {"version": "v3", "created": "Wed, 22 Jul 2009 22:17:26 GMT"}], "update_date": "2009-08-08", "authors_parsed": [["Hausser", "Jean", ""], ["Strimmer", "Korbinian", ""]]}, {"id": "0811.3619", "submitter": "Robin Genuer", "authors": "Robin Genuer (LM-Orsay), Jean-Michel Poggi (LM-Orsay), Christine\n  Tuleau (JAD)", "title": "Random Forests: some methodological insights", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper examines from an experimental perspective random forests, the\nincreasingly used statistical method for classification and regression problems\nintroduced by Leo Breiman in 2001. It first aims at confirming, known but\nsparse, advice for using random forests and at proposing some complementary\nremarks for both standard problems as well as high dimensional ones for which\nthe number of variables hugely exceeds the sample size. But the main\ncontribution of this paper is twofold: to provide some insights about the\nbehavior of the variable importance index based on random forests and in\naddition, to propose to investigate two classical issues of variable selection.\nThe first one is to find important variables for interpretation and the second\none is more restrictive and try to design a good prediction model. The strategy\ninvolves a ranking of explanatory variables using the random forests score of\nimportance and a stepwise ascending variable introduction strategy.\n", "versions": [{"version": "v1", "created": "Fri, 21 Nov 2008 19:45:15 GMT"}], "update_date": "2008-11-24", "authors_parsed": [["Genuer", "Robin", "", "LM-Orsay"], ["Poggi", "Jean-Michel", "", "LM-Orsay"], ["Tuleau", "Christine", "", "JAD"]]}, {"id": "0811.3628", "submitter": "Pradeep Ravikumar", "authors": "Pradeep Ravikumar, Martin J. Wainwright, Garvesh Raskutti and Bin Yu", "title": "High-dimensional covariance estimation by minimizing $\\ell_1$-penalized\n  log-determinant divergence", "comments": "35 pages, 9 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Given i.i.d. observations of a random vector $X \\in \\mathbb{R}^p$, we study\nthe problem of estimating both its covariance matrix $\\Sigma^*$, and its\ninverse covariance or concentration matrix {$\\Theta^* = (\\Sigma^*)^{-1}$.} We\nestimate $\\Theta^*$ by minimizing an $\\ell_1$-penalized log-determinant Bregman\ndivergence; in the multivariate Gaussian case, this approach corresponds to\n$\\ell_1$-penalized maximum likelihood, and the structure of $\\Theta^*$ is\nspecified by the graph of an associated Gaussian Markov random field. We\nanalyze the performance of this estimator under high-dimensional scaling, in\nwhich the number of nodes in the graph $p$, the number of edges $s$ and the\nmaximum node degree $d$, are allowed to grow as a function of the sample size\n$n$. In addition to the parameters $(p,s,d)$, our analysis identifies other key\nquantities covariance matrix $\\Sigma^*$; and (b) the $\\ell_\\infty$ operator\nnorm of the sub-matrix $\\Gamma^*_{S S}$, where $S$ indexes the graph edges, and\n$\\Gamma^* = (\\Theta^*)^{-1} \\otimes (\\Theta^*)^{-1}$; and (c) a mutual\nincoherence or irrepresentability measure on the matrix $\\Gamma^*$ and (d) the\nrate of decay $1/f(n,\\delta)$ on the probabilities $ \\{|\\hat{\\Sigma}^n_{ij}-\n\\Sigma^*_{ij}| > \\delta \\}$, where $\\hat{\\Sigma}^n$ is the sample covariance\nbased on $n$ samples. Our first result establishes consistency of our estimate\n$\\hat{\\Theta}$ in the elementwise maximum-norm. This in turn allows us to\nderive convergence rates in Frobenius and spectral norms, with improvements\nupon existing results for graphs with maximum node degrees $d = o(\\sqrt{s})$.\nIn our second result, we show that with probability converging to one, the\nestimate $\\hat{\\Theta}$ correctly specifies the zero pattern of the\nconcentration matrix $\\Theta^*$.\n", "versions": [{"version": "v1", "created": "Fri, 21 Nov 2008 20:38:18 GMT"}], "update_date": "2008-11-24", "authors_parsed": [["Ravikumar", "Pradeep", ""], ["Wainwright", "Martin J.", ""], ["Raskutti", "Garvesh", ""], ["Yu", "Bin", ""]]}, {"id": "0811.4167", "submitter": "Dabao Zhang", "authors": "Dabao Zhang, Yanzhu Lin and Min Zhang", "title": "Penalized Orthogonal-Components Regression for Large p Small n Data", "comments": "12 pages", "journal-ref": null, "doi": "10.1214/09-EJS354", "report-no": null, "categories": "stat.ME stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a penalized orthogonal-components regression (POCRE) for large p\nsmall n data. Orthogonal components are sequentially constructed to maximize,\nupon standardization, their correlation to the response residuals. A new\npenalization framework, implemented via empirical Bayes thresholding, is\npresented to effectively identify sparse predictors of each component. POCRE is\ncomputationally efficient owing to its sequential construction of leading\nsparse principal components. In addition, such construction offers other\nproperties such as grouping highly correlated predictors and allowing for\ncollinear or nearly collinear predictors. With multivariate responses, POCRE\ncan construct common components and thus build up latent-variable models for\nlarge p small n data.\n", "versions": [{"version": "v1", "created": "Tue, 25 Nov 2008 19:53:33 GMT"}, {"version": "v2", "created": "Mon, 1 Dec 2008 15:10:07 GMT"}, {"version": "v3", "created": "Thu, 18 Dec 2008 01:45:00 GMT"}], "update_date": "2013-04-18", "authors_parsed": [["Zhang", "Dabao", ""], ["Lin", "Yanzhu", ""], ["Zhang", "Min", ""]]}, {"id": "0811.4208", "submitter": "Anil Raj", "authors": "Anil Raj and Chris H. Wiggins", "title": "An information-theoretic derivation of min-cut based clustering", "comments": "7 pages, 3 figures, two-column, submitted to IEEE Transactions on\n  Pattern Analysis and Machine Intelligence", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Min-cut clustering, based on minimizing one of two heuristic cost-functions\nproposed by Shi and Malik, has spawned tremendous research, both analytic and\nalgorithmic, in the graph partitioning and image segmentation communities over\nthe last decade. It is however unclear if these heuristics can be derived from\na more general principle facilitating generalization to new problem settings.\nMotivated by an existing graph partitioning framework, we derive relationships\nbetween optimizing relevance information, as defined in the Information\nBottleneck method, and the regularized cut in a K-partitioned graph. For fast\nmixing graphs, we show that the cost functions introduced by Shi and Malik can\nbe well approximated as the rate of loss of predictive information about the\nlocation of random walkers on the graph. For graphs generated from a stochastic\nalgorithm designed to model community structure, the optimal information\ntheoretic partition and the optimal min-cut partition are shown to be the same\nwith high probability.\n", "versions": [{"version": "v1", "created": "Wed, 26 Nov 2008 14:56:26 GMT"}], "update_date": "2008-11-27", "authors_parsed": [["Raj", "Anil", ""], ["Wiggins", "Chris H.", ""]]}]