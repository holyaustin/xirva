[{"id": "1403.0289", "submitter": "C\\'edric Richard", "authors": "Rita Ammanouil, Andr\\'e Ferrari, C\\'edric Richard, David Mary", "title": "Blind and fully constrained unmixing of hyperspectral images", "comments": null, "journal-ref": null, "doi": "10.1109/TIP.2014.2362056", "report-no": null, "categories": "stat.AP stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper addresses the problem of blind and fully constrained unmixing of\nhyperspectral images. Unmixing is performed without the use of any dictionary,\nand assumes that the number of constituent materials in the scene and their\nspectral signatures are unknown. The estimated abundances satisfy the desired\nsum-to-one and nonnegativity constraints. Two models with increasing complexity\nare developed to achieve this challenging task, depending on how noise\ninteracts with hyperspectral data. The first one leads to a convex optimization\nproblem, and is solved with the Alternating Direction Method of Multipliers.\nThe second one accounts for signal-dependent noise, and is addressed with a\nReweighted Least Squares algorithm. Experiments on synthetic and real data\ndemonstrate the effectiveness of our approach.\n", "versions": [{"version": "v1", "created": "Mon, 3 Mar 2014 02:06:36 GMT"}], "update_date": "2015-06-18", "authors_parsed": [["Ammanouil", "Rita", ""], ["Ferrari", "Andr\u00e9", ""], ["Richard", "C\u00e9dric", ""], ["Mary", "David", ""]]}, {"id": "1403.0309", "submitter": "Conrad Sanderson", "authors": "Sareh Shirazi, Mehrtash T. Harandi, Brian C. Lovell, Conrad Sanderson", "title": "Object Tracking via Non-Euclidean Geometry: A Grassmann Approach", "comments": "IEEE Winter Conference on Applications of Computer Vision (WACV),\n  2014", "journal-ref": null, "doi": "10.1109/WACV.2014.6836008", "report-no": null, "categories": "cs.CV math.MG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A robust visual tracking system requires an object appearance model that is\nable to handle occlusion, pose, and illumination variations in the video\nstream. This can be difficult to accomplish when the model is trained using\nonly a single image. In this paper, we first propose a tracking approach based\non affine subspaces (constructed from several images) which are able to\naccommodate the abovementioned variations. We use affine subspaces not only to\nrepresent the object, but also the candidate areas that the object may occupy.\nWe furthermore propose a novel approach to measure affine subspace-to-subspace\ndistance via the use of non-Euclidean geometry of Grassmann manifolds. The\ntracking problem is then considered as an inference task in a Markov Chain\nMonte Carlo framework via particle filtering. Quantitative evaluation on\nchallenging video sequences indicates that the proposed approach obtains\nconsiderably better performance than several recent state-of-the-art methods\nsuch as Tracking-Learning-Detection and MILtrack.\n", "versions": [{"version": "v1", "created": "Mon, 3 Mar 2014 04:46:44 GMT"}], "update_date": "2014-08-27", "authors_parsed": [["Shirazi", "Sareh", ""], ["Harandi", "Mehrtash T.", ""], ["Lovell", "Brian C.", ""], ["Sanderson", "Conrad", ""]]}, {"id": "1403.0320", "submitter": "Conrad Sanderson", "authors": "Shaokang Chen, Arnold Wiliem, Conrad Sanderson, Brian C. Lovell", "title": "Matching Image Sets via Adaptive Multi Convex Hull", "comments": "IEEE Winter Conference on Applications of Computer Vision (WACV),\n  2014", "journal-ref": null, "doi": "10.1109/WACV.2014.6835985", "report-no": null, "categories": "cs.CV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Traditional nearest points methods use all the samples in an image set to\nconstruct a single convex or affine hull model for classification. However,\nstrong artificial features and noisy data may be generated from combinations of\ntraining samples when significant intra-class variations and/or noise occur in\nthe image set. Existing multi-model approaches extract local models by\nclustering each image set individually only once, with fixed clusters used for\nmatching with various image sets. This may not be optimal for discrimination,\nas undesirable environmental conditions (eg. illumination and pose variations)\nmay result in the two closest clusters representing different characteristics\nof an object (eg. frontal face being compared to non-frontal face). To address\nthe above problem, we propose a novel approach to enhance nearest points based\nmethods by integrating affine/convex hull classification with an adapted\nmulti-model approach. We first extract multiple local convex hulls from a query\nimage set via maximum margin clustering to diminish the artificial variations\nand constrain the noise in local convex hulls. We then propose adaptive\nreference clustering (ARC) to constrain the clustering of each gallery image\nset by forcing the clusters to have resemblance to the clusters in the query\nimage set. By applying ARC, noisy clusters in the query set can be discarded.\nExperiments on Honda, MoBo and ETH-80 datasets show that the proposed method\noutperforms single model approaches and other recent techniques, such as Sparse\nApproximated Nearest Points, Mutual Subspace Method and Manifold Discriminant\nAnalysis.\n", "versions": [{"version": "v1", "created": "Mon, 3 Mar 2014 06:19:45 GMT"}], "update_date": "2014-08-27", "authors_parsed": [["Chen", "Shaokang", ""], ["Wiliem", "Arnold", ""], ["Sanderson", "Conrad", ""], ["Lovell", "Brian C.", ""]]}, {"id": "1403.0388", "submitter": "Mohammadzaman Zamani", "authors": "Mohammadzaman Zamani, Hamid Beigy, and Amirreza Shaban", "title": "Cascading Randomized Weighted Majority: A New Online Ensemble Learning\n  Algorithm", "comments": "15 pages, 3 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  With the increasing volume of data in the world, the best approach for\nlearning from this data is to exploit an online learning algorithm. Online\nensemble methods are online algorithms which take advantage of an ensemble of\nclassifiers to predict labels of data. Prediction with expert advice is a\nwell-studied problem in the online ensemble learning literature. The Weighted\nMajority algorithm and the randomized weighted majority (RWM) are the most\nwell-known solutions to this problem, aiming to converge to the best expert.\nSince among some expert, the best one does not necessarily have the minimum\nerror in all regions of data space, defining specific regions and converging to\nthe best expert in each of these regions will lead to a better result. In this\npaper, we aim to resolve this defect of RWM algorithms by proposing a novel\nonline ensemble algorithm to the problem of prediction with expert advice. We\npropose a cascading version of RWM to achieve not only better experimental\nresults but also a better error bound for sufficiently large datasets.\n", "versions": [{"version": "v1", "created": "Mon, 3 Mar 2014 11:05:10 GMT"}, {"version": "v2", "created": "Fri, 5 Dec 2014 17:57:03 GMT"}, {"version": "v3", "created": "Sun, 4 Jan 2015 03:01:38 GMT"}, {"version": "v4", "created": "Mon, 2 Feb 2015 17:18:43 GMT"}], "update_date": "2015-02-03", "authors_parsed": [["Zamani", "Mohammadzaman", ""], ["Beigy", "Hamid", ""], ["Shaban", "Amirreza", ""]]}, {"id": "1403.0408", "submitter": "Jonas Peters", "authors": "Jonas Peters", "title": "On the Intersection Property of Conditional Independence and its\n  Application to Causal Discovery", "comments": null, "journal-ref": "Journal of Causal Inference 3:97-108, 2015", "doi": null, "report-no": null, "categories": "math.PR stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This work investigates the intersection property of conditional independence.\nIt states that for random variables $A,B,C$ and $X$ we have that $X$\nindependent of $A$ given $B,C$ and $X$ independent of $B$ given $A,C$ implies\n$X$ independent of $(A,B)$ given $C$. Under the assumption that the joint\ndistribution has a continuous density, we provide necessary and sufficient\nconditions under which the intersection property holds. The result has direct\napplications to causal inference: it leads to strictly weaker conditions under\nwhich the graphical structure becomes identifiable from the joint distribution\nof an additive noise model.\n", "versions": [{"version": "v1", "created": "Mon, 3 Mar 2014 12:31:05 GMT"}, {"version": "v2", "created": "Tue, 4 Mar 2014 10:25:06 GMT"}], "update_date": "2016-08-18", "authors_parsed": [["Peters", "Jonas", ""]]}, {"id": "1403.0481", "submitter": "Arindam Chaudhuri AC", "authors": "Arindam Chaudhuri", "title": "Support Vector Machine Model for Currency Crisis Discrimination", "comments": "Book Chapter Selected Works in Infrastructural Finance, Rudra P.\n  Pradhan, Indian Institute of Technology Kharagpur, Editor, Macmillan\n  Publishers, India, pp 249 - 256, 2011", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Support Vector Machine (SVM) is powerful classification technique based on\nthe idea of structural risk minimization. Use of kernel function enables curse\nof dimensionality to be addressed. However, proper kernel function for certain\nproblem is dependent on specific dataset and as such there is no good method on\nchoice of kernel function. In this paper, SVM is used to build empirical models\nof currency crisis in Argentina. An estimation technique is developed by\ntraining model on real life data set which provides reasonably accurate model\noutputs and helps policy makers to identify situations in which currency crisis\nmay happen. The third and fourth order polynomial kernel is generally best\nchoice to achieve high generalization of classifier performance. SVM has high\nlevel of maturity with algorithms that are simple, easy to implement, tolerates\ncurse of dimensionality and good empirical performance. The satisfactory\nresults show that currency crisis situation is properly emulated using only\nsmall fraction of database and could be used as an evaluation tool as well as\nan early warning system. To the best of knowledge this is the first work on SVM\napproach for currency crisis evaluation of Argentina.\n", "versions": [{"version": "v1", "created": "Mon, 3 Mar 2014 16:34:38 GMT"}], "update_date": "2014-03-04", "authors_parsed": [["Chaudhuri", "Arindam", ""]]}, {"id": "1403.0504", "submitter": "Brooks Paige", "authors": "Brooks Paige and Frank Wood", "title": "A Compilation Target for Probabilistic Programming Languages", "comments": "In Proceedings of the 31st International Conference on Machine\n  Learning (ICML), 2014", "journal-ref": "JMLR W&CP 32 (1) : 1935-1943, 2014", "doi": null, "report-no": null, "categories": "cs.AI cs.PL stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Forward inference techniques such as sequential Monte Carlo and particle\nMarkov chain Monte Carlo for probabilistic programming can be implemented in\nany programming language by creative use of standardized operating system\nfunctionality including processes, forking, mutexes, and shared memory.\nExploiting this we have defined, developed, and tested a probabilistic\nprogramming language intermediate representation language we call probabilistic\nC, which itself can be compiled to machine code by standard compilers and\nlinked to operating system libraries yielding an efficient, scalable, portable\nprobabilistic programming compilation target. This opens up a new hardware and\nsystems research path for optimizing probabilistic programming systems.\n", "versions": [{"version": "v1", "created": "Mon, 3 Mar 2014 18:08:57 GMT"}, {"version": "v2", "created": "Thu, 10 Jul 2014 17:41:10 GMT"}], "update_date": "2014-07-11", "authors_parsed": [["Paige", "Brooks", ""], ["Wood", "Frank", ""]]}, {"id": "1403.0515", "submitter": "Xiliang Lu", "authors": "Yuling Jiao, Bangti Jin, Xiliang Lu", "title": "A Primal Dual Active Set with Continuation Algorithm for the\n  \\ell^0-Regularized Optimization Problem", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.OC cs.IT math.IT stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We develop a primal dual active set with continuation algorithm for solving\nthe \\ell^0-regularized least-squares problem that frequently arises in\ncompressed sensing. The algorithm couples the the primal dual active set method\nwith a continuation strategy on the regularization parameter. At each inner\niteration, it first identifies the active set from both primal and dual\nvariables, and then updates the primal variable by solving a (typically small)\nleast-squares problem defined on the active set, from which the dual variable\ncan be updated explicitly. Under certain conditions on the sensing matrix,\ni.e., mutual incoherence property or restricted isometry property, and the\nnoise level, the finite step global convergence of the algorithm is\nestablished. Extensive numerical examples are presented to illustrate the\nefficiency and accuracy of the algorithm and the convergence analysis.\n", "versions": [{"version": "v1", "created": "Mon, 3 Mar 2014 18:33:57 GMT"}], "update_date": "2014-03-04", "authors_parsed": [["Jiao", "Yuling", ""], ["Jin", "Bangti", ""], ["Lu", "Xiliang", ""]]}, {"id": "1403.0648", "submitter": "Jinli Hu Mr", "authors": "Jinli Hu and Amos Storkey", "title": "Multi-period Trading Prediction Markets with Connections to Machine\n  Learning", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.GT cs.LG q-fin.TR stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a new model for prediction markets, in which we use risk measures\nto model agents and introduce a market maker to describe the trading process.\nThis specific choice on modelling tools brings us mathematical convenience. The\nanalysis shows that the whole market effectively approaches a global objective,\ndespite that the market is designed such that each agent only cares about its\nown goal. Additionally, the market dynamics provides a sensible algorithm for\noptimising the global objective. An intimate connection between machine\nlearning and our markets is thus established, such that we could 1) analyse a\nmarket by applying machine learning methods to the global objective, and 2)\nsolve machine learning problems by setting up and running certain markets.\n", "versions": [{"version": "v1", "created": "Tue, 4 Mar 2014 01:14:40 GMT"}], "update_date": "2014-03-05", "authors_parsed": [["Hu", "Jinli", ""], ["Storkey", "Amos", ""]]}, {"id": "1403.0667", "submitter": "James Voss", "authors": "James Voss, Mikhail Belkin, Luis Rademacher", "title": "The Hidden Convexity of Spectral Clustering", "comments": "22 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In recent years, spectral clustering has become a standard method for data\nanalysis used in a broad range of applications. In this paper we propose a new\nclass of algorithms for multiway spectral clustering based on optimization of a\ncertain \"contrast function\" over the unit sphere. These algorithms, partly\ninspired by certain Independent Component Analysis techniques, are simple, easy\nto implement and efficient.\n  Geometrically, the proposed algorithms can be interpreted as hidden basis\nrecovery by means of function optimization. We give a complete characterization\nof the contrast functions admissible for provable basis recovery. We show how\nthese conditions can be interpreted as a \"hidden convexity\" of our optimization\nproblem on the sphere; interestingly, we use efficient convex maximization\nrather than the more common convex minimization. We also show encouraging\nexperimental results on real and simulated data.\n", "versions": [{"version": "v1", "created": "Tue, 4 Mar 2014 02:48:20 GMT"}, {"version": "v2", "created": "Thu, 1 Oct 2015 21:59:05 GMT"}, {"version": "v3", "created": "Wed, 4 May 2016 18:10:13 GMT"}], "update_date": "2016-05-05", "authors_parsed": [["Voss", "James", ""], ["Belkin", "Mikhail", ""], ["Rademacher", "Luis", ""]]}, {"id": "1403.0699", "submitter": "Conrad Sanderson", "authors": "Azadeh Alavi, Yan Yang, Mehrtash Harandi, Conrad Sanderson", "title": "Multi-Shot Person Re-Identification via Relational Stein Divergence", "comments": "IEEE International Conference on Image Processing (ICIP), 2013", "journal-ref": null, "doi": "10.1109/ICIP.2013.6738731", "report-no": null, "categories": "cs.CV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Person re-identification is particularly challenging due to significant\nappearance changes across separate camera views. In order to re-identify\npeople, a representative human signature should effectively handle differences\nin illumination, pose and camera parameters. While general appearance-based\nmethods are modelled in Euclidean spaces, it has been argued that some\napplications in image and video analysis are better modelled via non-Euclidean\nmanifold geometry. To this end, recent approaches represent images as\ncovariance matrices, and interpret such matrices as points on Riemannian\nmanifolds. As direct classification on such manifolds can be difficult, in this\npaper we propose to represent each manifold point as a vector of similarities\nto class representers, via a recently introduced form of Bregman matrix\ndivergence known as the Stein divergence. This is followed by using a\ndiscriminative mapping of similarity vectors for final classification. The use\nof similarity vectors is in contrast to the traditional approach of embedding\nmanifolds into tangent spaces, which can suffer from representing the manifold\nstructure inaccurately. Comparative evaluations on benchmark ETHZ and iLIDS\ndatasets for the person re-identification task show that the proposed approach\nobtains better performance than recent techniques such as Histogram Plus\nEpitome, Partial Least Squares, and Symmetry-Driven Accumulation of Local\nFeatures.\n", "versions": [{"version": "v1", "created": "Tue, 4 Mar 2014 06:44:17 GMT"}], "update_date": "2014-03-05", "authors_parsed": [["Alavi", "Azadeh", ""], ["Yang", "Yan", ""], ["Harandi", "Mehrtash", ""], ["Sanderson", "Conrad", ""]]}, {"id": "1403.0700", "submitter": "Conrad Sanderson", "authors": "Azadeh Alavi, Arnold Wiliem, Kun Zhao, Brian C. Lovell, Conrad\n  Sanderson", "title": "Random Projections on Manifolds of Symmetric Positive Definite Matrices\n  for Image Classification", "comments": "IEEE Winter Conference on Applications of Computer Vision (WACV),\n  2014", "journal-ref": null, "doi": "10.1109/WACV.2014.6836085", "report-no": null, "categories": "cs.CV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recent advances suggest that encoding images through Symmetric Positive\nDefinite (SPD) matrices and then interpreting such matrices as points on\nRiemannian manifolds can lead to increased classification performance. Taking\ninto account manifold geometry is typically done via (1) embedding the\nmanifolds in tangent spaces, or (2) embedding into Reproducing Kernel Hilbert\nSpaces (RKHS). While embedding into tangent spaces allows the use of existing\nEuclidean-based learning algorithms, manifold shape is only approximated which\ncan cause loss of discriminatory information. The RKHS approach retains more of\nthe manifold structure, but may require non-trivial effort to kernelise\nEuclidean-based learning algorithms. In contrast to the above approaches, in\nthis paper we offer a novel solution that allows SPD matrices to be used with\nunmodified Euclidean-based learning algorithms, with the true manifold shape\nwell-preserved. Specifically, we propose to project SPD matrices using a set of\nrandom projection hyperplanes over RKHS into a random projection space, which\nleads to representing each matrix as a vector of projection coefficients.\nExperiments on face recognition, person re-identification and texture\nclassification show that the proposed approach outperforms several recent\nmethods, such as Tensor Sparse Coding, Histogram Plus Epitome, Riemannian\nLocality Preserving Projection and Relational Divergence Classification.\n", "versions": [{"version": "v1", "created": "Tue, 4 Mar 2014 06:57:50 GMT"}], "update_date": "2014-08-27", "authors_parsed": [["Alavi", "Azadeh", ""], ["Wiliem", "Arnold", ""], ["Zhao", "Kun", ""], ["Lovell", "Brian C.", ""], ["Sanderson", "Conrad", ""]]}, {"id": "1403.0736", "submitter": "Marc Claesen", "authors": "Marc Claesen, Frank De Smet, Johan A.K. Suykens, Bart De Moor", "title": "Fast Prediction with SVM Models Containing RBF Kernels", "comments": "9 pages, 1 figure, 3 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present an approximation scheme for support vector machine models that use\nan RBF kernel. A second-order Maclaurin series approximation is used for\nexponentials of inner products between support vectors and test instances. The\napproximation is applicable to all kernel methods featuring sums of kernel\nevaluations and makes no assumptions regarding data normalization. The\nprediction speed of approximated models no longer relates to the amount of\nsupport vectors but is quadratic in terms of the number of input dimensions. If\nthe number of input dimensions is small compared to the amount of support\nvectors, the approximated model is significantly faster in prediction and has a\nsmaller memory footprint. An optimized C++ implementation was made to assess\nthe gain in prediction speed in a set of practical tests. We additionally\nprovide a method to verify the approximation accuracy, prior to training models\nor during run-time, to ensure the loss in accuracy remains acceptable and\nwithin known bounds.\n", "versions": [{"version": "v1", "created": "Tue, 4 Mar 2014 10:47:45 GMT"}, {"version": "v2", "created": "Sun, 13 Apr 2014 08:43:17 GMT"}, {"version": "v3", "created": "Fri, 3 Oct 2014 14:45:41 GMT"}], "update_date": "2014-10-06", "authors_parsed": [["Claesen", "Marc", ""], ["De Smet", "Frank", ""], ["Suykens", "Johan A. K.", ""], ["De Moor", "Bart", ""]]}, {"id": "1403.0745", "submitter": "Marc Claesen", "authors": "Marc Claesen, Frank De Smet, Johan Suykens, Bart De Moor", "title": "EnsembleSVM: A Library for Ensemble Learning Using Support Vector\n  Machines", "comments": "5 pages, 1 table", "journal-ref": "Journal of Machine Learning Research. 15 (2014) 141-145", "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  EnsembleSVM is a free software package containing efficient routines to\nperform ensemble learning with support vector machine (SVM) base models. It\ncurrently offers ensemble methods based on binary SVM models. Our\nimplementation avoids duplicate storage and evaluation of support vectors which\nare shared between constituent models. Experimental results show that using\nensemble approaches can drastically reduce training complexity while\nmaintaining high predictive accuracy. The EnsembleSVM software package is\nfreely available online at http://esat.kuleuven.be/stadius/ensemblesvm.\n", "versions": [{"version": "v1", "created": "Tue, 4 Mar 2014 11:28:59 GMT"}], "update_date": "2014-03-05", "authors_parsed": [["Claesen", "Marc", ""], ["De Smet", "Frank", ""], ["Suykens", "Johan", ""], ["De Moor", "Bart", ""]]}, {"id": "1403.0829", "submitter": "Weifeng Liu", "authors": "W. Liu, H. Liu, D. Tao, Y. Wang, Ke Lu", "title": "Multiview Hessian regularized logistic regression for action recognition", "comments": "13 pages,2 figures, submitted to signal processing", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  With the rapid development of social media sharing, people often need to\nmanage the growing volume of multimedia data such as large scale video\nclassification and annotation, especially to organize those videos containing\nhuman activities. Recently, manifold regularized semi-supervised learning\n(SSL), which explores the intrinsic data probability distribution and then\nimproves the generalization ability with only a small number of labeled data,\nhas emerged as a promising paradigm for semiautomatic video classification. In\naddition, human action videos often have multi-modal content and different\nrepresentations. To tackle the above problems, in this paper we propose\nmultiview Hessian regularized logistic regression (mHLR) for human action\nrecognition. Compared with existing work, the advantages of mHLR lie in three\nfolds: (1) mHLR combines multiple Hessian regularization, each of which\nobtained from a particular representation of instance, to leverage the\nexploring of local geometry; (2) mHLR naturally handle multi-view instances\nwith multiple representations; (3) mHLR employs a smooth loss function and then\ncan be effectively optimized. We carefully conduct extensive experiments on the\nunstructured social activity attribute (USAA) dataset and the experimental\nresults demonstrate the effectiveness of the proposed multiview Hessian\nregularized logistic regression for human action recognition.\n", "versions": [{"version": "v1", "created": "Mon, 3 Mar 2014 01:11:40 GMT"}], "update_date": "2014-03-05", "authors_parsed": [["Liu", "W.", ""], ["Liu", "H.", ""], ["Tao", "D.", ""], ["Wang", "Y.", ""], ["Lu", "Ke", ""]]}, {"id": "1403.0873", "submitter": "Franz J. Kir\\'aly", "authors": "Franz J Kir\\'aly and Louis Theran", "title": "Matroid Regression", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST cs.DM cs.LG stat.ME stat.ML stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose an algebraic combinatorial method for solving large sparse linear\nsystems of equations locally - that is, a method which can compute single\nevaluations of the signal without computing the whole signal. The method scales\nonly in the sparsity of the system and not in its size, and allows to provide\nerror estimates for any solution method. At the heart of our approach is the\nso-called regression matroid, a combinatorial object associated to sparsity\npatterns, which allows to replace inversion of the large matrix with the\ninversion of a kernel matrix that is constant size. We show that our method\nprovides the best linear unbiased estimator (BLUE) for this setting and the\nminimum variance unbiased estimator (MVUE) under Gaussian noise assumptions,\nand furthermore we show that the size of the kernel matrix which is to be\ninverted can be traded off with accuracy.\n", "versions": [{"version": "v1", "created": "Tue, 4 Mar 2014 17:54:37 GMT"}], "update_date": "2014-03-05", "authors_parsed": [["Kir\u00e1ly", "Franz J", ""], ["Theran", "Louis", ""]]}, {"id": "1403.0989", "submitter": "Leto Peel", "authors": "Leto Peel and Aaron Clauset", "title": "Detecting change points in the large-scale structure of evolving\n  networks", "comments": null, "journal-ref": "Proc. of the 29th International Conference on Artificial\n  Intelligence (AAAI), 2914-2920 (2015)", "doi": null, "report-no": null, "categories": "cs.SI physics.soc-ph stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Interactions among people or objects are often dynamic in nature and can be\nrepresented as a sequence of networks, each providing a snapshot of the\ninteractions over a brief period of time. An important task in analyzing such\nevolving networks is change-point detection, in which we both identify the\ntimes at which the large-scale pattern of interactions changes fundamentally\nand quantify how large and what kind of change occurred. Here, we formalize for\nthe first time the network change-point detection problem within an online\nprobabilistic learning framework and introduce a method that can reliably solve\nit. This method combines a generalized hierarchical random graph model with a\nBayesian hypothesis test to quantitatively determine if, when, and precisely\nhow a change point has occurred. We analyze the detectability of our method\nusing synthetic data with known change points of different types and\nmagnitudes, and show that this method is more accurate than several previously\nused alternatives. Applied to two high-resolution evolving social networks,\nthis method identifies a sequence of change points that align with known\nexternal \"shocks\" to these networks.\n", "versions": [{"version": "v1", "created": "Wed, 5 Mar 2014 02:28:38 GMT"}, {"version": "v2", "created": "Fri, 14 Nov 2014 19:40:26 GMT"}], "update_date": "2016-06-17", "authors_parsed": [["Peel", "Leto", ""], ["Clauset", "Aaron", ""]]}, {"id": "1403.1124", "submitter": "Juha Karvanen", "authors": "Juha Karvanen", "title": "Estimating complex causal effects from incomplete observational data", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME cs.LG math.ST stat.ML stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Despite the major advances taken in causal modeling, causality is still an\nunfamiliar topic for many statisticians. In this paper, it is demonstrated from\nthe beginning to the end how causal effects can be estimated from observational\ndata assuming that the causal structure is known. To make the problem more\nchallenging, the causal effects are highly nonlinear and the data are missing\nat random. The tools used in the estimation include causal models with design,\ncausal calculus, multiple imputation and generalized additive models. The main\nmessage is that a trained statistician can estimate causal effects by\njudiciously combining existing tools.\n", "versions": [{"version": "v1", "created": "Wed, 5 Mar 2014 13:40:29 GMT"}, {"version": "v2", "created": "Wed, 2 Jul 2014 08:12:09 GMT"}], "update_date": "2014-07-03", "authors_parsed": [["Karvanen", "Juha", ""]]}, {"id": "1403.1345", "submitter": "Yun Yang", "authors": "Yun Yang and David B. Dunson", "title": "Minimax Optimal Bayesian Aggregation", "comments": "37 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.ME stat.ML stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  It is generally believed that ensemble approaches, which combine multiple\nalgorithms or models, can outperform any single algorithm at machine learning\ntasks, such as prediction. In this paper, we propose Bayesian convex and linear\naggregation approaches motivated by regression applications. We show that the\nproposed approach is minimax optimal when the true data-generating model is a\nconvex or linear combination of models in the list. Moreover, the method can\nadapt to sparsity structure in which certain models should receive zero\nweights, and the method is tuning parameter free unlike competitors. More\ngenerally, under an M-open view when the truth falls outside the space of all\nconvex/linear combinations, our theory suggests that the posterior measure\ntends to concentrate on the best approximation of the truth at the minimax\nrate. We illustrate the method through simulation studies and several\napplications.\n", "versions": [{"version": "v1", "created": "Thu, 6 Mar 2014 04:57:38 GMT"}], "update_date": "2014-03-07", "authors_parsed": [["Yang", "Yun", ""], ["Dunson", "David B.", ""]]}, {"id": "1403.1430", "submitter": "Zhenfang Hu", "authors": "Zhenfang Hu, Gang Pan, Yueming Wang, and Zhaohui Wu", "title": "Sparse Principal Component Analysis via Rotation and Truncation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Sparse principal component analysis (sparse PCA) aims at finding a sparse\nbasis to improve the interpretability over the dense basis of PCA, meanwhile\nthe sparse basis should cover the data subspace as much as possible. In\ncontrast to most of existing work which deal with the problem by adding some\nsparsity penalties on various objectives of PCA, in this paper, we propose a\nnew method SPCArt, whose motivation is to find a rotation matrix and a sparse\nbasis such that the sparse basis approximates the basis of PCA after the\nrotation. The algorithm of SPCArt consists of three alternating steps: rotate\nPCA basis, truncate small entries, and update the rotation matrix. Its\nperformance bounds are also given. SPCArt is efficient, with each iteration\nscaling linearly with the data dimension. It is easy to choose parameters in\nSPCArt, due to its explicit physical explanations. Besides, we give a unified\nview to several existing sparse PCA methods and discuss the connection with\nSPCArt. Some ideas in SPCArt are extended to GPower, a popular sparse PCA\nalgorithm, to overcome its drawback. Experimental results demonstrate that\nSPCArt achieves the state-of-the-art performance. It also achieves a good\ntradeoff among various criteria, including sparsity, explained variance,\northogonality, balance of sparsity among loadings, and computational speed.\n", "versions": [{"version": "v1", "created": "Thu, 6 Mar 2014 12:37:49 GMT"}, {"version": "v2", "created": "Thu, 1 May 2014 04:05:18 GMT"}], "update_date": "2014-05-02", "authors_parsed": [["Hu", "Zhenfang", ""], ["Pan", "Gang", ""], ["Wang", "Yueming", ""], ["Wu", "Zhaohui", ""]]}, {"id": "1403.1481", "submitter": "Andrew McDonald", "authors": "Andrew M. McDonald, Massimiliano Pontil, Dimitris Stamos", "title": "New Perspectives on k-Support and Cluster Norms", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The $k$-support norm is a regularizer which has been successfully applied to\nsparse vector prediction problems. We show that it belongs to a general class\nof norms which can be formulated as a parameterized infimum over quadratics. We\nfurther extend the $k$-support norm to matrices, and we observe that it is a\nspecial case of the matrix cluster norm. Using this formulation we derive an\nefficient algorithm to compute the proximity operator of both norms. This\nimproves upon the standard algorithm for the $k$-support norm and allows us to\napply proximal gradient methods to the cluster norm. We also describe how to\nsolve regularization problems which employ centered versions of these norms.\nFinally, we apply the matrix regularizers to different matrix completion and\nmultitask learning datasets. Our results indicate that the spectral $k$-support\nnorm and the cluster norm give state of the art performance on these problems,\nsignificantly outperforming trace norm and elastic net penalties.\n", "versions": [{"version": "v1", "created": "Thu, 6 Mar 2014 16:25:05 GMT"}], "update_date": "2014-03-07", "authors_parsed": [["McDonald", "Andrew M.", ""], ["Pontil", "Massimiliano", ""], ["Stamos", "Dimitris", ""]]}, {"id": "1403.1600", "submitter": "Kai Zhu", "authors": "Kai Zhu, Rui Wu, Lei Ying, R. Srikant", "title": "Collaborative Filtering with Information-Rich and Information-Sparse\n  Entities", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.IT cs.LG math.IT", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we consider a popular model for collaborative filtering in\nrecommender systems where some users of a website rate some items, such as\nmovies, and the goal is to recover the ratings of some or all of the unrated\nitems of each user. In particular, we consider both the clustering model, where\nonly users (or items) are clustered, and the co-clustering model, where both\nusers and items are clustered, and further, we assume that some users rate many\nitems (information-rich users) and some users rate only a few items\n(information-sparse users). When users (or items) are clustered, our algorithm\ncan recover the rating matrix with $\\omega(MK \\log M)$ noisy entries while $MK$\nentries are necessary, where $K$ is the number of clusters and $M$ is the\nnumber of items. In the case of co-clustering, we prove that $K^2$ entries are\nnecessary for recovering the rating matrix, and our algorithm achieves this\nlower bound within a logarithmic factor when $K$ is sufficiently large. We\ncompare our algorithms with a well-known algorithms called alternating\nminimization (AM), and a similarity score-based algorithm known as the\npopularity-among-friends (PAF) algorithm by applying all three to the MovieLens\nand Netflix data sets. Our co-clustering algorithm and AM have similar overall\nerror rates when recovering the rating matrix, both of which are lower than the\nerror rate under PAF. But more importantly, the error rate of our co-clustering\nalgorithm is significantly lower than AM and PAF in the scenarios of interest\nin recommender systems: when recommending a few items to each user or when\nrecommending items to users who only rated a few items (these users are the\nmajority of the total user population). The performance difference increases\neven more when noise is added to the datasets.\n", "versions": [{"version": "v1", "created": "Thu, 6 Mar 2014 21:51:48 GMT"}], "update_date": "2014-03-10", "authors_parsed": [["Zhu", "Kai", ""], ["Wu", "Rui", ""], ["Ying", "Lei", ""], ["Srikant", "R.", ""]]}, {"id": "1403.1891", "submitter": "Lihong Li", "authors": "Lihong Li and Shunbao Chen and Jim Kleban and Ankur Gupta", "title": "Counterfactual Estimation and Optimization of Click Metrics for Search\n  Engines", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI stat.AP stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Optimizing an interactive system against a predefined online metric is\nparticularly challenging, when the metric is computed from user feedback such\nas clicks and payments. The key challenge is the counterfactual nature: in the\ncase of Web search, any change to a component of the search engine may result\nin a different search result page for the same query, but we normally cannot\ninfer reliably from search log how users would react to the new result page.\nConsequently, it appears impossible to accurately estimate online metrics that\ndepend on user feedback, unless the new engine is run to serve users and\ncompared with a baseline in an A/B test. This approach, while valid and\nsuccessful, is unfortunately expensive and time-consuming. In this paper, we\npropose to address this problem using causal inference techniques, under the\ncontextual-bandit framework. This approach effectively allows one to run\n(potentially infinitely) many A/B tests offline from search log, making it\npossible to estimate and optimize online metrics quickly and inexpensively.\nFocusing on an important component in a commercial search engine, we show how\nthese ideas can be instantiated and applied, and obtain very promising results\nthat suggest the wide applicability of these techniques.\n", "versions": [{"version": "v1", "created": "Fri, 7 Mar 2014 22:54:52 GMT"}, {"version": "v2", "created": "Wed, 12 Mar 2014 06:36:02 GMT"}], "update_date": "2014-03-13", "authors_parsed": [["Li", "Lihong", ""], ["Chen", "Shunbao", ""], ["Kleban", "Jim", ""], ["Gupta", "Ankur", ""]]}, {"id": "1403.1893", "submitter": "Michael Smith", "authors": "Michael R. Smith and Tony Martinez", "title": "Becoming More Robust to Label Noise with Classifier Diversity", "comments": "37 pages, 10 tables, 2 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.AI cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  It is widely known in the machine learning community that class noise can be\n(and often is) detrimental to inducing a model of the data. Many current\napproaches use a single, often biased, measurement to determine if an instance\nis noisy. A biased measure may work well on certain data sets, but it can also\nbe less effective on a broader set of data sets. In this paper, we present\nnoise identification using classifier diversity (NICD) -- a method for deriving\na less biased noise measurement and integrating it into the learning process.\nTo lessen the bias of the noise measure, NICD selects a diverse set of\nclassifiers (based on their predictions of novel instances) to determine which\ninstances are noisy. We examine NICD as a technique for filtering, instance\nweighting, and selecting the base classifiers of a voting ensemble. We compare\nNICD with several other noise handling techniques that do not consider\nclassifier diversity on a set of 54 data sets and 5 learning algorithms. NICD\nsignificantly increases the classification accuracy over the other considered\napproaches and is effective across a broad set of data sets and learning\nalgorithms.\n", "versions": [{"version": "v1", "created": "Fri, 7 Mar 2014 22:58:48 GMT"}], "update_date": "2014-03-11", "authors_parsed": [["Smith", "Michael R.", ""], ["Martinez", "Tony", ""]]}, {"id": "1403.1944", "submitter": "Ping Li PhD", "authors": "Ping Li and Hong Li and Min Wu", "title": "Multi-label ensemble based on variable pairwise constraint projection", "comments": "19 pages,5 tables, 2 figures; Published with Information Sciences\n  (INS)", "journal-ref": "Information Sciences, 222, 2013, pp.269-281.(Available online 7\n  August 2012)", "doi": "10.1016/j.ins.2012.07.066", "report-no": null, "categories": "cs.LG cs.CV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Multi-label classification has attracted an increasing amount of attention in\nrecent years. To this end, many algorithms have been developed to classify\nmulti-label data in an effective manner. However, they usually do not consider\nthe pairwise relations indicated by sample labels, which actually play\nimportant roles in multi-label classification. Inspired by this, we naturally\nextend the traditional pairwise constraints to the multi-label scenario via a\nflexible thresholding scheme. Moreover, to improve the generalization ability\nof the classifier, we adopt a boosting-like strategy to construct a multi-label\nensemble from a group of base classifiers. To achieve these goals, this paper\npresents a novel multi-label classification framework named Variable Pairwise\nConstraint projection for Multi-label Ensemble (VPCME). Specifically, we take\nadvantage of the variable pairwise constraint projection to learn a\nlower-dimensional data representation, which preserves the correlations between\nsamples and labels. Thereafter, the base classifiers are trained in the new\ndata space. For the boosting-like strategy, we employ both the variable\npairwise constraints and the bootstrap steps to diversify the base classifiers.\nEmpirical studies have shown the superiority of the proposed method in\ncomparison with other approaches.\n", "versions": [{"version": "v1", "created": "Sat, 8 Mar 2014 07:20:05 GMT"}], "update_date": "2014-03-11", "authors_parsed": [["Li", "Ping", ""], ["Li", "Hong", ""], ["Wu", "Min", ""]]}, {"id": "1403.2073", "submitter": "Wei Liu Dr", "authors": "Wei Liu", "title": "Generalized Canonical Correlation Analysis and Its Application to Blind\n  Source Separation Based on a Dual-Linear Predictor Structure", "comments": "7 pages and 5 figures. The main aim is to show the inherent\n  relationship between generalised canonical correlation analysis and the\n  dual-linear predictor approach presented in two separate conference papers\n  (references [15] and [16])", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.NA stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Blind source separation (BSS) is one of the most important and established\nresearch topics in signal processing and many algorithms have been proposed\nbased on different statistical properties of the source signals. For\nsecond-order statistics (SOS) based methods, canonical correlation analysis\n(CCA) has been proved to be an effective solution to the problem. In this work,\nthe CCA approach is generalized to accommodate the case with added white noise\nand it is then applied to the BSS problem for noisy mixtures. In this approach,\nthe noise component is assumed to be spatially and temporally white, but the\nvariance information of noise is not required. An adaptive blind source\nextraction algorithm is derived based on this idea and a further extension is\nproposed by employing a dual-linear predictor structure for blind source\nextraction (BSE).\n", "versions": [{"version": "v1", "created": "Sun, 9 Mar 2014 16:28:38 GMT"}], "update_date": "2014-03-11", "authors_parsed": [["Liu", "Wei", ""]]}, {"id": "1403.2150", "submitter": "Sofia Triantafillou", "authors": "Sofia Triantafillou, Ioannis Tsamardinos", "title": "Constraint-based Causal Discovery from Multiple Interventions over\n  Overlapping Variable Sets", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Scientific practice typically involves repeatedly studying a system, each\ntime trying to unravel a different perspective. In each study, the scientist\nmay take measurements under different experimental conditions (interventions,\nmanipulations, perturbations) and measure different sets of quantities\n(variables). The result is a collection of heterogeneous data sets coming from\ndifferent data distributions. In this work, we present algorithm COmbINE, which\naccepts a collection of data sets over overlapping variable sets under\ndifferent experimental conditions; COmbINE then outputs a summary of all causal\nmodels indicating the invariant and variant structural characteristics of all\nmodels that simultaneously fit all of the input data sets. COmbINE converts\nestimated dependencies and independencies in the data into path constraints on\nthe data-generating causal model and encodes them as a SAT instance. The\nalgorithm is sound and complete in the sample limit. To account for conflicting\nconstraints arising from statistical errors, we introduce a general method for\nsorting constraints in order of confidence, computed as a function of their\ncorresponding p-values. In our empirical evaluation, COmbINE outperforms in\nterms of efficiency the only pre-existing similar algorithm; the latter\nadditionally admits feedback cycles, but does not admit conflicting constraints\nwhich hinders the applicability on real data. As a proof-of-concept, COmbINE is\nemployed to co-analyze 4 real, mass-cytometry data sets measuring\nphosphorylated protein concentrations of overlapping protein sets under 3\ndifferent interventions.\n", "versions": [{"version": "v1", "created": "Mon, 10 Mar 2014 07:24:20 GMT"}], "update_date": "2014-03-11", "authors_parsed": [["Triantafillou", "Sofia", ""], ["Tsamardinos", "Ioannis", ""]]}, {"id": "1403.2301", "submitter": "Radu Balan", "authors": "Radu Balan and Dongmian Zou", "title": "Phase Retrieval using Lipschitz Continuous Maps", "comments": "12 pages, 1 figure", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.FA cs.IT math.IT stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this note we prove that reconstruction from magnitudes of frame\ncoefficients (the so called \"phase retrieval problem\") can be performed using\nLipschitz continuous maps. Specifically we show that when the nonlinear\nanalysis map $\\alpha:{\\mathcal H}\\rightarrow\\mathbb{R}^m$ is injective, with\n$(\\alpha(x))_k=|<x,f_k>|^2$, where $\\{f_1,\\ldots,f_m\\}$ is a frame for the\nHilbert space ${\\mathcal H}$, then there exists a left inverse map\n$\\omega:\\mathbb{R}^m\\rightarrow {\\mathcal H}$ that is Lipschitz continuous.\nAdditionally we obtain the Lipschitz constant of this inverse map in terms of\nthe lower Lipschitz constant of $\\alpha$. Surprisingly the increase in\nLipschitz constant is independent of the space dimension or frame redundancy.\n", "versions": [{"version": "v1", "created": "Mon, 10 Mar 2014 17:01:06 GMT"}], "update_date": "2014-03-11", "authors_parsed": [["Balan", "Radu", ""], ["Zou", "Dongmian", ""]]}, {"id": "1403.2310", "submitter": "Qing Zhou", "authors": "Jiaying Gu, Fei Fu, and Qing Zhou", "title": "Penalized Estimation of Directed Acyclic Graphs From Discrete Data", "comments": "To appear in Statistics and Computing", "journal-ref": null, "doi": "10.1007/s11222-018-9801-y", "report-no": null, "categories": "stat.ME stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Bayesian networks, with structure given by a directed acyclic graph (DAG),\nare a popular class of graphical models. However, learning Bayesian networks\nfrom discrete or categorical data is particularly challenging, due to the large\nparameter space and the difficulty in searching for a sparse structure. In this\narticle, we develop a maximum penalized likelihood method to tackle this\nproblem. Instead of the commonly used multinomial distribution, we model the\nconditional distribution of a node given its parents by multi-logit regression,\nin which an edge is parameterized by a set of coefficient vectors with dummy\nvariables encoding the levels of a node. To obtain a sparse DAG, a group norm\npenalty is employed, and a blockwise coordinate descent algorithm is developed\nto maximize the penalized likelihood subject to the acyclicity constraint of a\nDAG. When interventional data are available, our method constructs a causal\nnetwork, in which a directed edge represents a causal relation. We apply our\nmethod to various simulated and real data sets. The results show that our\nmethod is very competitive, compared to many existing methods, in DAG\nestimation from both interventional and high-dimensional observational data.\n", "versions": [{"version": "v1", "created": "Mon, 10 Mar 2014 17:26:40 GMT"}, {"version": "v2", "created": "Wed, 19 Aug 2015 16:49:21 GMT"}, {"version": "v3", "created": "Wed, 4 May 2016 18:40:58 GMT"}, {"version": "v4", "created": "Fri, 2 Feb 2018 19:49:44 GMT"}], "update_date": "2018-02-06", "authors_parsed": [["Gu", "Jiaying", ""], ["Fu", "Fei", ""], ["Zhou", "Qing", ""]]}, {"id": "1403.2433", "submitter": "Mark Reid", "authors": "Mark D. Reid and Rafael M. Frongillo and Robert C. Williamson", "title": "Generalised Mixability, Constant Regret, and Bayesian Updating", "comments": "12 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Mixability of a loss is known to characterise when constant regret bounds are\nachievable in games of prediction with expert advice through the use of Vovk's\naggregating algorithm. We provide a new interpretation of mixability via convex\nanalysis that highlights the role of the Kullback-Leibler divergence in its\ndefinition. This naturally generalises to what we call $\\Phi$-mixability where\nthe Bregman divergence $D_\\Phi$ replaces the KL divergence. We prove that\nlosses that are $\\Phi$-mixable also enjoy constant regret bounds via a\ngeneralised aggregating algorithm that is similar to mirror descent.\n", "versions": [{"version": "v1", "created": "Mon, 10 Mar 2014 22:55:11 GMT"}], "update_date": "2014-03-12", "authors_parsed": [["Reid", "Mark D.", ""], ["Frongillo", "Rafael M.", ""], ["Williamson", "Robert C.", ""]]}, {"id": "1403.2732", "submitter": "Seth Myers", "authors": "Seth A. Myers and Jure Leskovec", "title": "The Bursty Dynamics of the Twitter Information Network", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.SI physics.soc-ph stat.ML", "license": "http://creativecommons.org/licenses/publicdomain/", "abstract": "  In online social media systems users are not only posting, consuming, and\nresharing content, but also creating new and destroying existing connections in\nthe underlying social network. While each of these two types of dynamics has\nindividually been studied in the past, much less is known about the connection\nbetween the two. How does user information posting and seeking behavior\ninteract with the evolution of the underlying social network structure?\n  Here, we study ways in which network structure reacts to users posting and\nsharing content. We examine the complete dynamics of the Twitter information\nnetwork, where users post and reshare information while they also create and\ndestroy connections. We find that the dynamics of network structure can be\ncharacterized by steady rates of change, interrupted by sudden bursts.\nInformation diffusion in the form of cascades of post re-sharing often creates\nsuch sudden bursts of new connections, which significantly change users' local\nnetwork structure. These bursts transform users' networks of followers to\nbecome structurally more cohesive as well as more homogenous in terms of\nfollower interests. We also explore the effect of the information content on\nthe dynamics of the network and find evidence that the appearance of new topics\nand real-world events can lead to significant changes in edge creations and\ndeletions. Lastly, we develop a model that quantifies the dynamics of the\nnetwork and the occurrence of these bursts as a function of the information\nspreading through the network. The model can successfully predict which\ninformation diffusion events will lead to bursts in network dynamics.\n", "versions": [{"version": "v1", "created": "Tue, 11 Mar 2014 20:06:02 GMT"}], "update_date": "2014-03-13", "authors_parsed": [["Myers", "Seth A.", ""], ["Leskovec", "Jure", ""]]}, {"id": "1403.2877", "submitter": "Carlos Oscar Sorzano S.", "authors": "C.O.S. Sorzano, J. Vargas, A. Pascual Montano", "title": "A survey of dimensionality reduction techniques", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG q-bio.QM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Experimental life sciences like biology or chemistry have seen in the recent\ndecades an explosion of the data available from experiments. Laboratory\ninstruments become more and more complex and report hundreds or thousands\nmeasurements for a single experiment and therefore the statistical methods face\nchallenging tasks when dealing with such high dimensional data. However, much\nof the data is highly redundant and can be efficiently brought down to a much\nsmaller number of variables without a significant loss of information. The\nmathematical procedures making possible this reduction are called\ndimensionality reduction techniques; they have widely been developed by fields\nlike Statistics or Machine Learning, and are currently a hot research topic. In\nthis review we categorize the plethora of dimension reduction techniques\navailable and give the mathematical insight behind them.\n", "versions": [{"version": "v1", "created": "Wed, 12 Mar 2014 10:35:15 GMT"}], "update_date": "2014-03-13", "authors_parsed": [["Sorzano", "C. O. S.", ""], ["Vargas", "J.", ""], ["Montano", "A. Pascual", ""]]}, {"id": "1403.2933", "submitter": "Daniel Larremore", "authors": "Daniel B. Larremore, Aaron Clauset, Abigail Z. Jacobs", "title": "Efficiently inferring community structure in bipartite networks", "comments": "12 pages, 9 figures", "journal-ref": "Physical Review E 90(1): 012805 (2014)", "doi": "10.1103/PhysRevE.90.012805", "report-no": null, "categories": "cs.SI physics.data-an physics.soc-ph q-bio.QM stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Bipartite networks are a common type of network data in which there are two\ntypes of vertices, and only vertices of different types can be connected. While\nbipartite networks exhibit community structure like their unipartite\ncounterparts, existing approaches to bipartite community detection have\ndrawbacks, including implicit parameter choices, loss of information through\none-mode projections, and lack of interpretability. Here we solve the community\ndetection problem for bipartite networks by formulating a bipartite stochastic\nblock model, which explicitly includes vertex type information and may be\ntrivially extended to $k$-partite networks. This bipartite stochastic block\nmodel yields a projection-free and statistically principled method for\ncommunity detection that makes clear assumptions and parameter choices and\nyields interpretable results. We demonstrate this model's ability to\nefficiently and accurately find community structure in synthetic bipartite\nnetworks with known structure and in real-world bipartite networks with unknown\nstructure, and we characterize its performance in practical contexts.\n", "versions": [{"version": "v1", "created": "Wed, 12 Mar 2014 13:54:06 GMT"}, {"version": "v2", "created": "Thu, 10 Jul 2014 21:38:16 GMT"}], "update_date": "2014-07-14", "authors_parsed": [["Larremore", "Daniel B.", ""], ["Clauset", "Aaron", ""], ["Jacobs", "Abigail Z.", ""]]}, {"id": "1403.3080", "submitter": "Xi Chen", "authors": "Xi Chen, Qihang Lin, Dengyong Zhou", "title": "Statistical Decision Making for Optimal Budget Allocation in Crowd\n  Labeling", "comments": "39 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG math.OC stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In crowd labeling, a large amount of unlabeled data instances are outsourced\nto a crowd of workers. Workers will be paid for each label they provide, but\nthe labeling requester usually has only a limited amount of the budget. Since\ndata instances have different levels of labeling difficulty and workers have\ndifferent reliability, it is desirable to have an optimal policy to allocate\nthe budget among all instance-worker pairs such that the overall labeling\naccuracy is maximized. We consider categorical labeling tasks and formulate the\nbudget allocation problem as a Bayesian Markov decision process (MDP), which\nsimultaneously conducts learning and decision making. Using the dynamic\nprogramming (DP) recurrence, one can obtain the optimal allocation policy.\nHowever, DP quickly becomes computationally intractable when the size of the\nproblem increases. To solve this challenge, we propose a computationally\nefficient approximate policy, called optimistic knowledge gradient policy. Our\nMDP is a quite general framework, which applies to both pull crowdsourcing\nmarketplaces with homogeneous workers and push marketplaces with heterogeneous\nworkers. It can also incorporate the contextual information of instances when\nthey are available. The experiments on both simulated and real data show that\nthe proposed policy achieves a higher labeling accuracy than other existing\npolicies at the same budget level.\n", "versions": [{"version": "v1", "created": "Wed, 12 Mar 2014 19:55:00 GMT"}, {"version": "v2", "created": "Thu, 24 Apr 2014 08:52:28 GMT"}], "update_date": "2014-04-25", "authors_parsed": [["Chen", "Xi", ""], ["Lin", "Qihang", ""], ["Zhou", "Dengyong", ""]]}, {"id": "1403.3100", "submitter": "Ashton Anderson", "authors": "Ashton Anderson, Daniel Huttenlocher, Jon Kleinberg, Jure Leskovec", "title": "Engaging with Massive Online Courses", "comments": "WWW 2014", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.SI physics.soc-ph stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The Web has enabled one of the most visible recent developments in\neducation---the deployment of massive open online courses. With their global\nreach and often staggering enrollments, MOOCs have the potential to become a\nmajor new mechanism for learning. Despite this early promise, however, MOOCs\nare still relatively unexplored and poorly understood.\n  In a MOOC, each student's complete interaction with the course materials\ntakes place on the Web, thus providing a record of learner activity of\nunprecedented scale and resolution. In this work, we use such trace data to\ndevelop a conceptual framework for understanding how users currently engage\nwith MOOCs. We develop a taxonomy of individual behavior, examine the different\nbehavioral patterns of high- and low-achieving students, and investigate how\nforum participation relates to other parts of the course.\n  We also report on a large-scale deployment of badges as incentives for\nengagement in a MOOC, including randomized experiments in which the\npresentation of badges was varied across sub-populations. We find that making\nbadges more salient produced increases in forum engagement.\n", "versions": [{"version": "v1", "created": "Wed, 12 Mar 2014 20:01:27 GMT"}, {"version": "v2", "created": "Fri, 14 Mar 2014 02:17:18 GMT"}], "update_date": "2014-03-17", "authors_parsed": [["Anderson", "Ashton", ""], ["Huttenlocher", "Daniel", ""], ["Kleinberg", "Jon", ""], ["Leskovec", "Jure", ""]]}, {"id": "1403.3342", "submitter": "Michael Smith", "authors": "Michael R. Smith and Tony Martinez and Christophe Giraud-Carrier", "title": "The Potential Benefits of Filtering Versus Hyper-Parameter Optimization", "comments": "11 pages, 4 tables, 3 Figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The quality of an induced model by a learning algorithm is dependent on the\nquality of the training data and the hyper-parameters supplied to the learning\nalgorithm. Prior work has shown that improving the quality of the training data\n(i.e., by removing low quality instances) or tuning the learning algorithm\nhyper-parameters can significantly improve the quality of an induced model. A\ncomparison of the two methods is lacking though. In this paper, we estimate and\ncompare the potential benefits of filtering and hyper-parameter optimization.\nEstimating the potential benefit gives an overly optimistic estimate but also\nempirically demonstrates an approximation of the maximum potential benefit of\neach method. We find that, while both significantly improve the induced model,\nimproving the quality of the training set has a greater potential effect than\nhyper-parameter optimization.\n", "versions": [{"version": "v1", "created": "Thu, 13 Mar 2014 17:48:19 GMT"}], "update_date": "2014-03-14", "authors_parsed": [["Smith", "Michael R.", ""], ["Martinez", "Tony", ""], ["Giraud-Carrier", "Christophe", ""]]}, {"id": "1403.3378", "submitter": "Siong Thye Goh", "authors": "Siong Thye Goh, Cynthia Rudin", "title": "Box Drawings for Learning with Imbalanced Data", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The vast majority of real world classification problems are imbalanced,\nmeaning there are far fewer data from the class of interest (the positive\nclass) than from other classes. We propose two machine learning algorithms to\nhandle highly imbalanced classification problems. The classifiers constructed\nby both methods are created as unions of parallel axis rectangles around the\npositive examples, and thus have the benefit of being interpretable. The first\nalgorithm uses mixed integer programming to optimize a weighted balance between\npositive and negative class accuracies. Regularization is introduced to improve\ngeneralization performance. The second method uses an approximation in order to\nassist with scalability. Specifically, it follows a \\textit{characterize then\ndiscriminate} approach, where the positive class is characterized first by\nboxes, and then each box boundary becomes a separate discriminative classifier.\nThis method has the computational advantages that it can be easily\nparallelized, and considers only the relevant regions of feature space.\n", "versions": [{"version": "v1", "created": "Thu, 13 Mar 2014 19:28:48 GMT"}, {"version": "v2", "created": "Sat, 7 Jun 2014 15:01:07 GMT"}], "update_date": "2014-06-10", "authors_parsed": [["Goh", "Siong Thye", ""], ["Rudin", "Cynthia", ""]]}, {"id": "1403.3438", "submitter": "Reinhard Heckel", "authors": "Reinhard Heckel, Eirikur Agustsson, Helmut B\\\"olcskei", "title": "Neighborhood Selection for Thresholding-based Subspace Clustering", "comments": "ICASSP 2014", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.IT math.IT", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Subspace clustering refers to the problem of clustering high-dimensional data\npoints into a union of low-dimensional linear subspaces, where the number of\nsubspaces, their dimensions and orientations are all unknown. In this paper, we\npropose a variation of the recently introduced thresholding-based subspace\nclustering (TSC) algorithm, which applies spectral clustering to an adjacency\nmatrix constructed from the nearest neighbors of each data point with respect\nto the spherical distance measure. The new element resides in an individual and\ndata-driven choice of the number of nearest neighbors. Previous performance\nresults for TSC, as well as for other subspace clustering algorithms based on\nspectral clustering, come in terms of an intermediate performance measure,\nwhich does not address the clustering error directly. Our main analytical\ncontribution is a performance analysis of the modified TSC algorithm (as well\nas the original TSC algorithm) in terms of the clustering error directly.\n", "versions": [{"version": "v1", "created": "Thu, 13 Mar 2014 21:19:30 GMT"}], "update_date": "2014-03-17", "authors_parsed": [["Heckel", "Reinhard", ""], ["Agustsson", "Eirikur", ""], ["B\u00f6lcskei", "Helmut", ""]]}, {"id": "1403.3707", "submitter": "Nesreen Ahmed", "authors": "Nesreen K. Ahmed, Christopher Cole, Jennifer Neville", "title": "Learning the Latent State Space of Time-Varying Graphs", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.SI cs.LG physics.soc-ph stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  From social networks to Internet applications, a wide variety of electronic\ncommunication tools are producing streams of graph data; where the nodes\nrepresent users and the edges represent the contacts between them over time.\nThis has led to an increased interest in mechanisms to model the dynamic\nstructure of time-varying graphs. In this work, we develop a framework for\nlearning the latent state space of a time-varying email graph. We show how the\nframework can be used to find subsequences that correspond to global real-time\nevents in the Email graph (e.g. vacations, breaks, ...etc.). These events\nimpact the underlying graph process to make its characteristics non-stationary.\nWithin the framework, we compare two different representations of the temporal\nrelationships; discrete vs. probabilistic. We use the two representations as\ninputs to a mixture model to learn the latent state transitions that correspond\nto important changes in the Email graph structure over time.\n", "versions": [{"version": "v1", "created": "Fri, 14 Mar 2014 20:37:06 GMT"}], "update_date": "2014-03-18", "authors_parsed": [["Ahmed", "Nesreen K.", ""], ["Cole", "Christopher", ""], ["Neville", "Jennifer", ""]]}, {"id": "1403.3741", "submitter": "Ian Osband", "authors": "Ian Osband, Benjamin Van Roy", "title": "Near-optimal Reinforcement Learning in Factored MDPs", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Any reinforcement learning algorithm that applies to all Markov decision\nprocesses (MDPs) will suffer $\\Omega(\\sqrt{SAT})$ regret on some MDP, where $T$\nis the elapsed time and $S$ and $A$ are the cardinalities of the state and\naction spaces. This implies $T = \\Omega(SA)$ time to guarantee a near-optimal\npolicy. In many settings of practical interest, due to the curse of\ndimensionality, $S$ and $A$ can be so enormous that this learning time is\nunacceptable. We establish that, if the system is known to be a \\emph{factored}\nMDP, it is possible to achieve regret that scales polynomially in the number of\n\\emph{parameters} encoding the factored MDP, which may be exponentially smaller\nthan $S$ or $A$. We provide two algorithms that satisfy near-optimal regret\nbounds in this context: posterior sampling reinforcement learning (PSRL) and an\nupper confidence bound algorithm (UCRL-Factored).\n", "versions": [{"version": "v1", "created": "Sat, 15 Mar 2014 01:56:02 GMT"}, {"version": "v2", "created": "Fri, 6 Jun 2014 23:17:54 GMT"}, {"version": "v3", "created": "Fri, 31 Oct 2014 23:34:32 GMT"}], "update_date": "2014-11-04", "authors_parsed": [["Osband", "Ian", ""], ["Van Roy", "Benjamin", ""]]}, {"id": "1403.4017", "submitter": "Longqi Yang", "authors": "Longqi Yang, Yibing Wang, Zhisong Pan and Guyu Hu", "title": "Multi-task Feature Selection based Anomaly Detection", "comments": "6 pages, 5 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Network anomaly detection is still a vibrant research area. As the fast\ngrowth of network bandwidth and the tremendous traffic on the network, there\narises an extremely challengeable question: How to efficiently and accurately\ndetect the anomaly on multiple traffic? In multi-task learning, the traffic\nconsisting of flows at different time periods is considered as a task. Multiple\ntasks at different time periods performed simultaneously to detect anomalies.\nIn this paper, we apply the multi-task feature selection in network anomaly\ndetection area which provides a powerful method to gather information from\nmultiple traffic and detect anomalies on it simultaneously. In particular, the\nmulti-task feature selection includes the well-known l1-norm based feature\nselection as a special case given only one task. Moreover, we show that the\nmulti-task feature selection is more accurate by utilizing more information\nsimultaneously than the l1-norm based method. At the evaluation stage, we\npreprocess the raw data trace from trans-Pacific backbone link between Japan\nand the United States, label with anomaly communities, and generate a\n248-feature dataset. We show empirically that the multi-task feature selection\noutperforms independent l1-norm based feature selection on real traffic\ndataset.\n", "versions": [{"version": "v1", "created": "Mon, 17 Mar 2014 08:04:41 GMT"}], "update_date": "2014-03-18", "authors_parsed": [["Yang", "Longqi", ""], ["Wang", "Yibing", ""], ["Pan", "Zhisong", ""], ["Hu", "Guyu", ""]]}, {"id": "1403.4206", "submitter": "Konstantina Palla Miss", "authors": "Konstantina Palla, David A. Knowles, Zoubin Ghahramani", "title": "A reversible infinite HMM using normalised random measures", "comments": "9 pages, 6 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a nonparametric prior over reversible Markov chains. We use\ncompletely random measures, specifically gamma processes, to construct a\ncountably infinite graph with weighted edges. By enforcing symmetry to make the\nedges undirected we define a prior over random walks on graphs that results in\na reversible Markov chain. The resulting prior over infinite transition\nmatrices is closely related to the hierarchical Dirichlet process but enforces\nreversibility. A reinforcement scheme has recently been proposed with similar\nproperties, but the de Finetti measure is not well characterised. We take the\nalternative approach of explicitly constructing the mixing measure, which\nallows more straightforward and efficient inference at the cost of no longer\nhaving a closed form predictive distribution. We use our process to construct a\nreversible infinite HMM which we apply to two real datasets, one from\nepigenomics and one ion channel recording.\n", "versions": [{"version": "v1", "created": "Mon, 17 Mar 2014 18:41:54 GMT"}], "update_date": "2014-03-18", "authors_parsed": [["Palla", "Konstantina", ""], ["Knowles", "David A.", ""], ["Ghahramani", "Zoubin", ""]]}, {"id": "1403.4544", "submitter": "Cheryl Flynn", "authors": "Cheryl J. Flynn, Clifford M. Hurvich, Jeffrey S. Simonoff", "title": "On the Sensitivity of the Lasso to the Number of Predictor Variables", "comments": null, "journal-ref": null, "doi": "10.1214/16-STS586", "report-no": null, "categories": "stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The Lasso is a computationally efficient regression regularization procedure\nthat can produce sparse estimators when the number of predictors (p) is large.\nOracle inequalities provide probability loss bounds for the Lasso estimator at\na deterministic choice of the regularization parameter. These bounds tend to\nzero if p is appropriately controlled, and are thus commonly cited as\ntheoretical justification for the Lasso and its ability to handle\nhigh-dimensional settings. Unfortunately, in practice the regularization\nparameter is not selected to be a deterministic quantity, but is instead chosen\nusing a random, data-dependent procedure. To address this shortcoming of\nprevious theoretical work, we study the loss of the Lasso estimator when tuned\noptimally for prediction. Assuming orthonormal predictors and a sparse true\nmodel, we prove that the probability that the best possible predictive\nperformance of the Lasso deteriorates as p increases is positive and can be\narbitrarily close to one given a sufficiently high signal to noise ratio and\nsufficiently large p. We further demonstrate empirically that the amount of\ndeterioration in performance can be far worse than the oracle inequalities\nsuggest and provide a real data example where deterioration is observed.\n", "versions": [{"version": "v1", "created": "Tue, 18 Mar 2014 17:32:01 GMT"}, {"version": "v2", "created": "Fri, 8 Apr 2016 22:02:47 GMT"}, {"version": "v3", "created": "Fri, 27 May 2016 17:50:39 GMT"}], "update_date": "2017-07-21", "authors_parsed": [["Flynn", "Cheryl J.", ""], ["Hurvich", "Clifford M.", ""], ["Simonoff", "Jeffrey S.", ""]]}, {"id": "1403.4608", "submitter": "Justin Cheng", "authors": "Justin Cheng, Lada A. Adamic, P. Alex Dow, Jon Kleinberg, Jure\n  Leskovec", "title": "Can Cascades be Predicted?", "comments": null, "journal-ref": null, "doi": "10.1145/2566486.2567997", "report-no": null, "categories": "cs.SI physics.soc-ph stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  On many social networking web sites such as Facebook and Twitter, resharing\nor reposting functionality allows users to share others' content with their own\nfriends or followers. As content is reshared from user to user, large cascades\nof reshares can form. While a growing body of research has focused on analyzing\nand characterizing such cascades, a recent, parallel line of work has argued\nthat the future trajectory of a cascade may be inherently unpredictable. In\nthis work, we develop a framework for addressing cascade prediction problems.\nOn a large sample of photo reshare cascades on Facebook, we find strong\nperformance in predicting whether a cascade will continue to grow in the\nfuture. We find that the relative growth of a cascade becomes more predictable\nas we observe more of its reshares, that temporal and structural features are\nkey predictors of cascade size, and that initially, breadth, rather than depth\nin a cascade is a better indicator of larger cascades. This prediction\nperformance is robust in the sense that multiple distinct classes of features\nall achieve similar performance. We also discover that temporal features are\npredictive of a cascade's eventual shape. Observing independent cascades of the\nsame content, we find that while these cascades differ greatly in size, we are\nstill able to predict which ends up the largest.\n", "versions": [{"version": "v1", "created": "Tue, 18 Mar 2014 20:00:55 GMT"}], "update_date": "2014-03-20", "authors_parsed": [["Cheng", "Justin", ""], ["Adamic", "Lada A.", ""], ["Dow", "P. Alex", ""], ["Kleinberg", "Jon", ""], ["Leskovec", "Jure", ""]]}, {"id": "1403.4626", "submitter": "Kevin H. Knuth", "authors": "Kevin H. Knuth, Man Kit Tse, Joshua Choinsky, Haley A. Maunu, Duane F.\n  Carbon", "title": "Bayesian Source Separation Applied to Identifying Complex Organic\n  Molecules in Space", "comments": "5 pages, 6 Figures, Presented at the IEEE Statistical Signal\n  Processing Workshop, Madison WI, August 2007, 346-350", "journal-ref": "Proceedings of the IEEE Statistical Signal Processing Workshop,\n  Madison WI, August 2007, 346-350", "doi": "10.1109/SSP.2007.4301277", "report-no": null, "categories": "astro-ph.IM physics.data-an stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Emission from a class of benzene-based molecules known as Polycyclic Aromatic\nHydrocarbons (PAHs) dominates the infrared spectrum of star-forming regions.\nThe observed emission appears to arise from the combined emission of numerous\nPAH species, each with its unique spectrum. Linear superposition of the PAH\nspectra identifies this problem as a source separation problem. It is, however,\nof a formidable class of source separation problems given that different PAH\nsources potentially number in the hundreds, even thousands, and there is only\none measured spectral signal for a given astrophysical site. Fortunately, the\nsource spectra of the PAHs are known, but the signal is also contaminated by\nother spectral sources. We describe our ongoing work in developing Bayesian\nsource separation techniques relying on nested sampling in conjunction with an\nON/OFF mechanism enabling simultaneous estimation of the probability that a\nparticular PAH species is present and its contribution to the spectrum.\n", "versions": [{"version": "v1", "created": "Tue, 18 Mar 2014 21:40:13 GMT"}], "update_date": "2014-03-20", "authors_parsed": [["Knuth", "Kevin H.", ""], ["Tse", "Man Kit", ""], ["Choinsky", "Joshua", ""], ["Maunu", "Haley A.", ""], ["Carbon", "Duane F.", ""]]}, {"id": "1403.4640", "submitter": "Nabeel Gillani", "authors": "Nabeel Gillani, Rebecca Eynon, Michael Osborne, Isis Hjorth, Stephen\n  Roberts", "title": "Communication Communities in MOOCs", "comments": "10 pages, 3 figures, 1 table. Submitted for review to UAI 2014", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CY cs.SI stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Massive Open Online Courses (MOOCs) bring together thousands of people from\ndifferent geographies and demographic backgrounds -- but to date, little is\nknown about how they learn or communicate. We introduce a new content-analysed\nMOOC dataset and use Bayesian Non-negative Matrix Factorization (BNMF) to\nextract communities of learners based on the nature of their online forum\nposts. We see that BNMF yields a superior probabilistic generative model for\nonline discussions when compared to other models, and that the communities it\nlearns are differentiated by their composite students' demographic and course\nperformance indicators. These findings suggest that computationally efficient\nprobabilistic generative modelling of MOOCs can reveal important insights for\neducational researchers and practitioners and help to develop more intelligent\nand responsive online learning environments.\n", "versions": [{"version": "v1", "created": "Tue, 18 Mar 2014 22:57:24 GMT"}, {"version": "v2", "created": "Wed, 16 Apr 2014 15:50:48 GMT"}], "update_date": "2014-04-17", "authors_parsed": [["Gillani", "Nabeel", ""], ["Eynon", "Rebecca", ""], ["Osborne", "Michael", ""], ["Hjorth", "Isis", ""], ["Roberts", "Stephen", ""]]}, {"id": "1403.4698", "submitter": "Xi Luo", "authors": "Xi Luo", "title": "A Hierarchical Graphical Model for Big Inverse Covariance Estimation\n  with an Application to fMRI", "comments": "An R package of the proposed method will be publicly available on\n  CRAN. This paper has been presented orally at Yale University on Feburary 18,\n  2014, and at the Eastern North American Region Meeting of the International\n  Biometric Society on March 18, 2014", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.AP stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Brain networks has attracted the interests of many neuroscientists. From\nfunctional MRI (fMRI) data, statistical tools have been developed to recover\nbrain networks. However, the dimensionality of whole-brain fMRI, usually in\nhundreds of thousands, challenges the applicability of these methods. We\ndevelop a hierarchical graphical model (HGM) to remediate this difficulty. This\nmodel introduces a hidden layer of networks based on sparse Gaussian graphical\nmodels, and the observed data are sampled from individual network nodes. In\nfMRI, the network layer models the underlying signals of different brain\nfunctional units, and how these units directly interact with each other. The\nintroduction of this hierarchical structure not only provides a formal and\ninterpretable approach, but also enables efficient computation for inferring\nbig networks with hundreds of thousands of nodes. Based on the conditional\nconvexity of our formulation, we develop an alternating update algorithm to\ncompute the HGM model parameters simultaneously. The effectiveness of this\napproach is demonstrated on simulated data and a real dataset from a stop/go\nfMRI experiment.\n", "versions": [{"version": "v1", "created": "Wed, 19 Mar 2014 05:43:43 GMT"}, {"version": "v2", "created": "Mon, 7 Apr 2014 18:49:10 GMT"}], "update_date": "2014-04-08", "authors_parsed": [["Luo", "Xi", ""]]}, {"id": "1403.4699", "submitter": "Lin Xiao", "authors": "Lin Xiao and Tong Zhang", "title": "A Proximal Stochastic Gradient Method with Progressive Variance\n  Reduction", "comments": null, "journal-ref": null, "doi": null, "report-no": "MSR-TR-2014-38", "categories": "math.OC stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider the problem of minimizing the sum of two convex functions: one is\nthe average of a large number of smooth component functions, and the other is a\ngeneral convex function that admits a simple proximal mapping. We assume the\nwhole objective function is strongly convex. Such problems often arise in\nmachine learning, known as regularized empirical risk minimization. We propose\nand analyze a new proximal stochastic gradient method, which uses a multi-stage\nscheme to progressively reduce the variance of the stochastic gradient. While\neach iteration of this algorithm has similar cost as the classical stochastic\ngradient method (or incremental gradient method), we show that the expected\nobjective value converges to the optimum at a geometric rate. The overall\ncomplexity of this method is much lower than both the proximal full gradient\nmethod and the standard proximal stochastic gradient method.\n", "versions": [{"version": "v1", "created": "Wed, 19 Mar 2014 06:02:55 GMT"}], "update_date": "2014-03-20", "authors_parsed": [["Xiao", "Lin", ""], ["Zhang", "Tong", ""]]}, {"id": "1403.4781", "submitter": "Subhadip Mukherjee", "authors": "Subhadip Mukherjee and Chandra Sekhar Seelamantula", "title": "A Split-and-Merge Dictionary Learning Algorithm for Sparse\n  Representation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In big data image/video analytics, we encounter the problem of learning an\novercomplete dictionary for sparse representation from a large training\ndataset, which can not be processed at once because of storage and\ncomputational constraints. To tackle the problem of dictionary learning in such\nscenarios, we propose an algorithm for parallel dictionary learning. The\nfundamental idea behind the algorithm is to learn a sparse representation in\ntwo phases. In the first phase, the whole training dataset is partitioned into\nsmall non-overlapping subsets, and a dictionary is trained independently on\neach small database. In the second phase, the dictionaries are merged to form a\nglobal dictionary. We show that the proposed algorithm is efficient in its\nusage of memory and computational complexity, and performs on par with the\nstandard learning strategy operating on the entire data at a time. As an\napplication, we consider the problem of image denoising. We present a\ncomparative analysis of our algorithm with the standard learning techniques,\nthat use the entire database at a time, in terms of training and denoising\nperformance. We observe that the split-and-merge algorithm results in a\nremarkable reduction of training time, without significantly affecting the\ndenoising performance.\n", "versions": [{"version": "v1", "created": "Wed, 19 Mar 2014 12:16:17 GMT"}], "update_date": "2014-03-20", "authors_parsed": [["Mukherjee", "Subhadip", ""], ["Seelamantula", "Chandra Sekhar", ""]]}, {"id": "1403.5045", "submitter": "Branislav Kveton", "authors": "Branislav Kveton, Zheng Wen, Azin Ashkan, Hoda Eydgahi, Brian Eriksson", "title": "Matroid Bandits: Fast Combinatorial Optimization with Learning", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI cs.SY stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A matroid is a notion of independence in combinatorial optimization which is\nclosely related to computational efficiency. In particular, it is well known\nthat the maximum of a constrained modular function can be found greedily if and\nonly if the constraints are associated with a matroid. In this paper, we bring\ntogether the ideas of bandits and matroids, and propose a new class of\ncombinatorial bandits, matroid bandits. The objective in these problems is to\nlearn how to maximize a modular function on a matroid. This function is\nstochastic and initially unknown. We propose a practical algorithm for solving\nour problem, Optimistic Matroid Maximization (OMM); and prove two upper bounds,\ngap-dependent and gap-free, on its regret. Both bounds are sublinear in time\nand at most linear in all other quantities of interest. The gap-dependent upper\nbound is tight and we prove a matching lower bound on a partition matroid\nbandit. Finally, we evaluate our method on three real-world problems and show\nthat it is practical.\n", "versions": [{"version": "v1", "created": "Thu, 20 Mar 2014 05:52:43 GMT"}, {"version": "v2", "created": "Fri, 11 Apr 2014 06:25:22 GMT"}, {"version": "v3", "created": "Mon, 16 Jun 2014 20:23:34 GMT"}], "update_date": "2015-04-15", "authors_parsed": [["Kveton", "Branislav", ""], ["Wen", "Zheng", ""], ["Ashkan", "Azin", ""], ["Eydgahi", "Hoda", ""], ["Eriksson", "Brian", ""]]}, {"id": "1403.5112", "submitter": "Matthias Seibert", "authors": "Matthias Seibert, Martin Kleinsteuber, R\\'emi Gribonval, Rodolphe\n  Jenatton, Francis Bach", "title": "On The Sample Complexity of Sparse Dictionary Learning", "comments": "4 pages, submitted to Statistical Signal Processing Workshop 2014", "journal-ref": null, "doi": "10.1109/SSP.2014.6884621", "report-no": null, "categories": "stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In the synthesis model signals are represented as a sparse combinations of\natoms from a dictionary. Dictionary learning describes the acquisition process\nof the underlying dictionary for a given set of training samples. While ideally\nthis would be achieved by optimizing the expectation of the factors over the\nunderlying distribution of the training data, in practice the necessary\ninformation about the distribution is not available. Therefore, in real world\napplications it is achieved by minimizing an empirical average over the\navailable samples. The main goal of this paper is to provide a sample\ncomplexity estimate that controls to what extent the empirical average deviates\nfrom the cost function. This estimate then provides a suitable estimate to the\naccuracy of the representation of the learned dictionary. The presented\napproach exemplifies the general results proposed by the authors in Sample\nComplexity of Dictionary Learning and other Matrix Factorizations, Gribonval et\nal. and gives more concrete bounds of the sample complexity of dictionary\nlearning. We cover a variety of sparsity measures employed in the learning\nprocedure.\n", "versions": [{"version": "v1", "created": "Thu, 20 Mar 2014 12:30:54 GMT"}], "update_date": "2015-03-11", "authors_parsed": [["Seibert", "Matthias", ""], ["Kleinsteuber", "Martin", ""], ["Gribonval", "R\u00e9mi", ""], ["Jenatton", "Rodolphe", ""], ["Bach", "Francis", ""]]}, {"id": "1403.5177", "submitter": "Ichigaku Takigawa", "authors": "Ichigaku Takigawa and Hiroshi Mamitsuka", "title": "Sparse Learning over Infinite Subgraph Features", "comments": "42 pages, 24 figures, 4 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a supervised-learning algorithm from graph data (a set of graphs)\nfor arbitrary twice-differentiable loss functions and sparse linear models over\nall possible subgraph features. To date, it has been shown that under all\npossible subgraph features, several types of sparse learning, such as Adaboost,\nLPBoost, LARS/LASSO, and sparse PLS regression, can be performed. Particularly\nemphasis is placed on simultaneous learning of relevant features from an\ninfinite set of candidates. We first generalize techniques used in all these\npreceding studies to derive an unifying bounding technique for arbitrary\nseparable functions. We then carefully use this bounding to make block\ncoordinate gradient descent feasible over infinite subgraph features, resulting\nin a fast converging algorithm that can solve a wider class of sparse learning\nproblems over graph data. We also empirically study the differences from the\nexisting approaches in convergence property, selected subgraph features, and\nsearch-space sizes. We further discuss several unnoticed issues in sparse\nlearning over all possible subgraph features.\n", "versions": [{"version": "v1", "created": "Thu, 20 Mar 2014 15:44:56 GMT"}], "update_date": "2014-03-21", "authors_parsed": [["Takigawa", "Ichigaku", ""], ["Mamitsuka", "Hiroshi", ""]]}, {"id": "1403.5370", "submitter": "Mathieu Dubois", "authors": "Mathieu Dubois (LIMSI), Frenoux Emmanuelle (LIMSI), Philippe Tarroux\n  (LIMSI)", "title": "Using n-grams models for visual semantic place recognition", "comments": "VISAPP (2013)", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The aim of this paper is to present a new method for visual place\nrecognition. Our system combines global image characterization and visual\nwords, which allows to use efficient Bayesian filtering methods to integrate\nseveral images. More precisely, we extend the classical HMM model with\ntechniques inspired by the field of Natural Language Processing. This paper\npresents our system and the Bayesian filtering algorithm. The performance of\nour system and the influence of the main parameters are evaluated on a standard\ndatabase. The discussion highlights the interest of using such models and\nproposes improvements.\n", "versions": [{"version": "v1", "created": "Fri, 21 Mar 2014 05:23:17 GMT"}], "update_date": "2014-03-24", "authors_parsed": [["Dubois", "Mathieu", "", "LIMSI"], ["Emmanuelle", "Frenoux", "", "LIMSI"], ["Tarroux", "Philippe", "", "LIMSI"]]}, {"id": "1403.5607", "submitter": "Michael Gelbart", "authors": "Michael A. Gelbart, Jasper Snoek, Ryan P. Adams", "title": "Bayesian Optimization with Unknown Constraints", "comments": "14 pages, 3 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recent work on Bayesian optimization has shown its effectiveness in global\noptimization of difficult black-box objective functions. Many real-world\noptimization problems of interest also have constraints which are unknown a\npriori. In this paper, we study Bayesian optimization for constrained problems\nin the general case that noise may be present in the constraint functions, and\nthe objective and constraints may be evaluated independently. We provide\nmotivating practical examples, and present a general framework to solve such\nproblems. We demonstrate the effectiveness of our approach on optimizing the\nperformance of online latent Dirichlet allocation subject to topic sparsity\nconstraints, tuning a neural network given test-time memory constraints, and\noptimizing Hamiltonian Monte Carlo to achieve maximal effectiveness in a fixed\ntime, subject to passing standard convergence diagnostics.\n", "versions": [{"version": "v1", "created": "Sat, 22 Mar 2014 03:35:00 GMT"}], "update_date": "2014-03-25", "authors_parsed": [["Gelbart", "Michael A.", ""], ["Snoek", "Jasper", ""], ["Adams", "Ryan P.", ""]]}, {"id": "1403.5647", "submitter": "Miao Xu", "authors": "Rong Jin, Shenghuo Zhu", "title": "CUR Algorithm with Incomplete Matrix Observation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  CUR matrix decomposition is a randomized algorithm that can efficiently\ncompute the low rank approximation for a given rectangle matrix. One limitation\nwith the existing CUR algorithms is that they require an access to the full\nmatrix A for computing U. In this work, we aim to alleviate this limitation. In\nparticular, we assume that besides having an access to randomly sampled d rows\nand d columns from A, we only observe a subset of randomly sampled entries from\nA. Our goal is to develop a low rank approximation algorithm, similar to CUR,\nbased on (i) randomly sampled rows and columns from A, and (ii) randomly\nsampled entries from A. The proposed algorithm is able to perfectly recover the\ntarget matrix A with only O(rn log n) number of observed entries. In addition,\ninstead of having to solve an optimization problem involved trace norm\nregularization, the proposed algorithm only needs to solve a standard\nregression problem. Finally, unlike most matrix completion theories that hold\nonly when the target matrix is of low rank, we show a strong guarantee for the\nproposed algorithm even when the target matrix is not low rank.\n", "versions": [{"version": "v1", "created": "Sat, 22 Mar 2014 11:15:01 GMT"}], "update_date": "2014-03-25", "authors_parsed": [["Jin", "Rong", ""], ["Zhu", "Shenghuo", ""]]}, {"id": "1403.5693", "submitter": "Dougal Maclaurin", "authors": "Dougal Maclaurin and Ryan P. Adams", "title": "Firefly Monte Carlo: Exact MCMC with Subsets of Data", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Markov chain Monte Carlo (MCMC) is a popular and successful general-purpose\ntool for Bayesian inference. However, MCMC cannot be practically applied to\nlarge data sets because of the prohibitive cost of evaluating every likelihood\nterm at every iteration. Here we present Firefly Monte Carlo (FlyMC) an\nauxiliary variable MCMC algorithm that only queries the likelihoods of a\npotentially small subset of the data at each iteration yet simulates from the\nexact posterior distribution, in contrast to recent proposals that are\napproximate even in the asymptotic limit. FlyMC is compatible with a wide\nvariety of modern MCMC algorithms, and only requires a lower bound on the\nper-datum likelihood factors. In experiments, we find that FlyMC generates\nsamples from the posterior more than an order of magnitude faster than regular\nMCMC, opening up MCMC methods to larger datasets than were previously\nconsidered feasible.\n", "versions": [{"version": "v1", "created": "Sat, 22 Mar 2014 18:21:29 GMT"}], "update_date": "2014-03-25", "authors_parsed": [["Maclaurin", "Dougal", ""], ["Adams", "Ryan P.", ""]]}, {"id": "1403.5787", "submitter": "Pan Zhang", "authors": "Pan Zhang and Cristopher Moore", "title": "Scalable detection of statistically significant communities and\n  hierarchies, using message-passing for modularity", "comments": null, "journal-ref": "Proceedings of National Academy of Sciences, 111, 18144 (2014)", "doi": "10.1073/pnas.1409770111", "report-no": null, "categories": "physics.soc-ph cond-mat.stat-mech cs.SI stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Modularity is a popular measure of community structure. However, maximizing\nthe modularity can lead to many competing partitions, with almost the same\nmodularity, that are poorly correlated with each other. It can also produce\nillusory \"communities\" in random graphs where none exist. We address this\nproblem by using the modularity as a Hamiltonian at finite temperature, and\nusing an efficient Belief Propagation algorithm to obtain the consensus of many\npartitions with high modularity, rather than looking for a single partition\nthat maximizes it. We show analytically and numerically that the proposed\nalgorithm works all the way down to the detectability transition in networks\ngenerated by the stochastic block model. It also performs well on real-world\nnetworks, revealing large communities in some networks where previous work has\nclaimed no communities exist. Finally we show that by applying our algorithm\nrecursively, subdividing communities until no statistically-significant\nsubcommunities can be found, we can detect hierarchical structure in real-world\nnetworks more efficiently than previous methods.\n", "versions": [{"version": "v1", "created": "Sun, 23 Mar 2014 18:41:32 GMT"}, {"version": "v2", "created": "Tue, 22 Apr 2014 22:15:51 GMT"}, {"version": "v3", "created": "Sat, 27 Dec 2014 10:10:21 GMT"}], "update_date": "2014-12-30", "authors_parsed": [["Zhang", "Pan", ""], ["Moore", "Cristopher", ""]]}, {"id": "1403.5877", "submitter": "Anastasios Kyrillidis", "authors": "Anastasios Kyrillidis and Anastasios Zouzias", "title": "Non-uniform Feature Sampling for Decision Tree Ensembles", "comments": "7 pages, 7 figures, 1 table", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.IT cs.LG math.IT stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study the effectiveness of non-uniform randomized feature selection in\ndecision tree classification. We experimentally evaluate two feature selection\nmethodologies, based on information extracted from the provided dataset: $(i)$\n\\emph{leverage scores-based} and $(ii)$ \\emph{norm-based} feature selection.\nExperimental evaluation of the proposed feature selection techniques indicate\nthat such approaches might be more effective compared to naive uniform feature\nselection and moreover having comparable performance to the random forest\nalgorithm [3]\n", "versions": [{"version": "v1", "created": "Mon, 24 Mar 2014 08:26:19 GMT"}], "update_date": "2014-03-25", "authors_parsed": [["Kyrillidis", "Anastasios", ""], ["Zouzias", "Anastasios", ""]]}, {"id": "1403.5994", "submitter": "Jason Gejie Liu", "authors": "Jason Gejie Liu, Shuchin Aeron", "title": "First Order Methods for Robust Non-negative Matrix Factorization for\n  Large Scale Noisy Data", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Nonnegative matrix factorization (NMF) has been shown to be identifiable\nunder the separability assumption, under which all the columns(or rows) of the\ninput data matrix belong to the convex cone generated by only a few of these\ncolumns(or rows) [1]. In real applications, however, such separability\nassumption is hard to satisfy. Following [4] and [5], in this paper, we look at\nthe Linear Programming (LP) based reformulation to locate the extreme rays of\nthe convex cone but in a noisy setting. Furthermore, in order to deal with the\nlarge scale data, we employ First-Order Methods (FOM) to mitigate the\ncomputational complexity of LP, which primarily results from a large number of\nconstraints. We show the performance of the algorithm on real and synthetic\ndata sets.\n", "versions": [{"version": "v1", "created": "Mon, 24 Mar 2014 15:22:12 GMT"}], "update_date": "2014-03-25", "authors_parsed": [["Liu", "Jason Gejie", ""], ["Aeron", "Shuchin", ""]]}, {"id": "1403.5997", "submitter": "Niko Br\\\"ummer", "authors": "Niko Br\\\"ummer and Albert Swart", "title": "Bayesian calibration for forensic evidence reporting", "comments": "accepted for Interspeech 2014", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We introduce a Bayesian solution for the problem in forensic speaker\nrecognition, where there may be very little background material for estimating\nscore calibration parameters. We work within the Bayesian paradigm of evidence\nreporting and develop a principled probabilistic treatment of the problem,\nwhich results in a Bayesian likelihood-ratio as the vehicle for reporting\nweight of evidence. We show in contrast, that reporting a likelihood-ratio\ndistribution does not solve this problem. Our solution is experimentally\nexercised on a simulated forensic scenario, using NIST SRE'12 scores, which\ndemonstrates a clear advantage for the proposed method compared to the\ntraditional plugin calibration recipe.\n", "versions": [{"version": "v1", "created": "Mon, 24 Mar 2014 15:25:59 GMT"}, {"version": "v2", "created": "Tue, 25 Mar 2014 07:27:21 GMT"}, {"version": "v3", "created": "Tue, 10 Jun 2014 08:18:06 GMT"}], "update_date": "2017-10-03", "authors_parsed": [["Br\u00fcmmer", "Niko", ""], ["Swart", "Albert", ""]]}, {"id": "1403.6095", "submitter": "Irina Gaynanova", "authors": "Irina Gaynanova, James G. Booth and Martin T. Wells", "title": "Simultaneous sparse estimation of canonical vectors in the p>>N setting", "comments": "Accepted to JASA, 2015", "journal-ref": "Journal of the American Statistical Association 2016, Vol. 111,\n  No. 514, 696-706", "doi": "10.1080/01621459.2015.1034318", "report-no": null, "categories": "stat.ME stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This article considers the problem of sparse estimation of canonical vectors\nin linear discriminant analysis when $p\\gg N$. Several methods have been\nproposed in the literature that estimate one canonical vector in the two-group\ncase. However, $G-1$ canonical vectors can be considered if the number of\ngroups is $G$. In the multi-group context, it is common to estimate canonical\nvectors in a sequential fashion. Moreover, separate prior estimation of the\ncovariance structure is often required. We propose a novel methodology for\ndirect estimation of canonical vectors. In contrast to existing techniques, the\nproposed method estimates all canonical vectors at once, performs variable\nselection across all the vectors and comes with theoretical guarantees on the\nvariable selection and classification consistency. First, we highlight the fact\nthat in the $N>p$ setting the canonical vectors can be expressed in a closed\nform up to an orthogonal transformation. Secondly, we propose an extension of\nthis form to the $p\\gg N$ setting and achieve feature selection by using a\ngroup penalty. The resulting optimization problem is convex and can be solved\nusing a block-coordinate descent algorithm. The practical performance of the\nmethod is evaluated through simulation studies as well as real data\napplications.\n", "versions": [{"version": "v1", "created": "Mon, 24 Mar 2014 19:28:37 GMT"}, {"version": "v2", "created": "Tue, 1 Apr 2014 14:11:02 GMT"}, {"version": "v3", "created": "Tue, 4 Nov 2014 15:50:04 GMT"}, {"version": "v4", "created": "Thu, 30 Apr 2015 18:44:27 GMT"}], "update_date": "2021-04-01", "authors_parsed": [["Gaynanova", "Irina", ""], ["Booth", "James G.", ""], ["Wells", "Martin T.", ""]]}, {"id": "1403.6212", "submitter": "Yiyuan She", "authors": "Yiyuan She", "title": "Selective Factor Extraction in High Dimensions", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper studies simultaneous feature selection and extraction in\nsupervised and unsupervised learning. We propose and investigate selective\nreduced rank regression for constructing optimal explanatory factors from a\nparsimonious subset of input features. The proposed estimators enjoy sharp\noracle inequalities, and with a predictive information criterion for model\nselection, they adapt to unknown sparsity by controlling both rank and row\nsupport of the coefficient matrix. A class of algorithms is developed that can\naccommodate various convex and nonconvex sparsity-inducing penalties, and can\nbe used for rank-constrained variable screening in high-dimensional\nmultivariate data. The paper also showcases applications in macroeconomics and\ncomputer vision to demonstrate how low-dimensional data structures can be\neffectively captured by joint variable selection and projection.\n", "versions": [{"version": "v1", "created": "Tue, 25 Mar 2014 02:40:41 GMT"}, {"version": "v2", "created": "Sun, 7 Dec 2014 01:25:39 GMT"}, {"version": "v3", "created": "Thu, 20 Oct 2016 05:35:29 GMT"}, {"version": "v4", "created": "Tue, 25 Oct 2016 22:51:03 GMT"}], "update_date": "2016-10-27", "authors_parsed": [["She", "Yiyuan", ""]]}, {"id": "1403.6355", "submitter": "Dejan Slep\\v{c}ev", "authors": "Nicol\\'as Garc\\'ia Trillos and Dejan Slep\\v{c}ev", "title": "Continuum limit of total variation on point clouds", "comments": null, "journal-ref": null, "doi": "10.1007/s00205-015-0929-z", "report-no": null, "categories": "math.ST math.AP stat.ML stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider point clouds obtained as random samples of a measure on a\nEuclidean domain. A graph representing the point cloud is obtained by assigning\nweights to edges based on the distance between the points they connect. Our\ngoal is to develop mathematical tools needed to study the consistency, as the\nnumber of available data points increases, of graph-based machine learning\nalgorithms for tasks such as clustering. In particular, we study when is the\ncut capacity, and more generally total variation, on these graphs a good\napproximation of the perimeter (total variation) in the continuum setting. We\naddress this question in the setting of $\\Gamma$-convergence. We obtain almost\noptimal conditions on the scaling, as number of points increases, of the size\nof the neighborhood over which the points are connected by an edge for the\n$\\Gamma$-convergence to hold. Taking the limit is enabled by a transportation\nbased metric which allows to suitably compare functionals defined on different\npoint clouds.\n", "versions": [{"version": "v1", "created": "Tue, 25 Mar 2014 14:22:21 GMT"}, {"version": "v2", "created": "Fri, 4 Jul 2014 08:55:53 GMT"}, {"version": "v3", "created": "Wed, 17 Sep 2014 03:21:43 GMT"}], "update_date": "2015-10-28", "authors_parsed": [["Trillos", "Nicol\u00e1s Garc\u00eda", ""], ["Slep\u010dev", "Dejan", ""]]}, {"id": "1403.6384", "submitter": "Fabrizio De Vico Fallani", "authors": "Daria La Rocca, Patrizio Campisi, Balazs Vegso, Peter Cserti, Gyorgy\n  Kozmann, Fabio Babiloni, Fabrizio De Vico Fallani", "title": "Human brain distinctiveness based on EEG spectral coherence connectivity", "comments": "Key words: EEG, Resting state, Biometrics, Spectral coherence, Match\n  score fusion", "journal-ref": null, "doi": "10.1109/TBME.2014.2317881", "report-no": null, "categories": "q-bio.NC stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The use of EEG biometrics, for the purpose of automatic people recognition,\nhas received increasing attention in the recent years. Most of current analysis\nrely on the extraction of features characterizing the activity of single brain\nregions, like power-spectrum estimates, thus neglecting possible temporal\ndependencies between the generated EEG signals. However, important\nphysiological information can be extracted from the way different brain regions\nare functionally coupled. In this study, we propose a novel approach that fuses\nspectral coherencebased connectivity between different brain regions as a\npossibly viable biometric feature. The proposed approach is tested on a large\ndataset of subjects (N=108) during eyes-closed (EC) and eyes-open (EO) resting\nstate conditions. The obtained recognition performances show that using brain\nconnectivity leads to higher distinctiveness with respect to power-spectrum\nmeasurements, in both the experimental conditions. Notably, a 100% recognition\naccuracy is obtained in EC and EO when integrating functional connectivity\nbetween regions in the frontal lobe, while a lower 97.41% is obtained in EC\n(96.26% in EO) when fusing power spectrum information from centro-parietal\nregions. Taken together, these results suggest that functional connectivity\npatterns represent effective features for improving EEG-based biometric\nsystems.\n", "versions": [{"version": "v1", "created": "Sun, 23 Mar 2014 21:22:15 GMT"}], "update_date": "2014-09-10", "authors_parsed": [["La Rocca", "Daria", ""], ["Campisi", "Patrizio", ""], ["Vegso", "Balazs", ""], ["Cserti", "Peter", ""], ["Kozmann", "Gyorgy", ""], ["Babiloni", "Fabio", ""], ["Fallani", "Fabrizio De Vico", ""]]}, {"id": "1403.6499", "submitter": "Dong Xia", "authors": "Dong Xia", "title": "Optimal Schatten-q and Ky-Fan-k Norm Rate of Low Rank Matrix Estimation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we consider low rank matrix estimation using either\nmatrix-version Dantzig Selector $\\hat{A}_{\\lambda}^d$ or matrix-version LASSO\nestimator $\\hat{A}_{\\lambda}^L$. We consider sub-Gaussian measurements, $i.e.$,\nthe measurements $X_1,\\ldots,X_n\\in\\mathbb{R}^{m\\times m}$ have $i.i.d.$\nsub-Gaussian entries. Suppose $\\textrm{rank}(A_0)=r$. We proved that, when\n$n\\geq Cm[r^2\\vee r\\log(m)\\log(n)]$ for some $C>0$, both $\\hat{A}_{\\lambda}^d$\nand $\\hat{A}_{\\lambda}^L$ can obtain optimal upper bounds(except some\nlogarithmic terms) for estimation accuracy under spectral norm. By applying\nmetric entropy of Grassmann manifolds, we construct (near) matching minimax\nlower bound for estimation accuracy under spectral norm. We also give upper\nbounds and matching minimax lower bound(except some logarithmic terms) for\nestimation accuracy under Schatten-q norm for every $1\\leq q\\leq\\infty$. As a\ndirect corollary, we show both upper bounds and minimax lower bounds of\nestimation accuracy under Ky-Fan-k norms for every $1\\leq k\\leq m$.\n", "versions": [{"version": "v1", "created": "Tue, 25 Mar 2014 20:33:34 GMT"}, {"version": "v2", "created": "Thu, 3 Apr 2014 20:20:10 GMT"}], "update_date": "2014-04-07", "authors_parsed": [["Xia", "Dong", ""]]}, {"id": "1403.6530", "submitter": "L.A. Prashanth", "authors": "Prashanth L.A. and Mohammad Ghavamzadeh", "title": "Variance-Constrained Actor-Critic Algorithms for Discounted and Average\n  Reward MDPs", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG math.OC stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In many sequential decision-making problems we may want to manage risk by\nminimizing some measure of variability in rewards in addition to maximizing a\nstandard criterion. Variance related risk measures are among the most common\nrisk-sensitive criteria in finance and operations research. However, optimizing\nmany such criteria is known to be a hard problem. In this paper, we consider\nboth discounted and average reward Markov decision processes. For each\nformulation, we first define a measure of variability for a policy, which in\nturn gives us a set of risk-sensitive criteria to optimize. For each of these\ncriteria, we derive a formula for computing its gradient. We then devise\nactor-critic algorithms that operate on three timescales - a TD critic on the\nfastest timescale, a policy gradient (actor) on the intermediate timescale, and\na dual ascent for Lagrange multipliers on the slowest timescale. In the\ndiscounted setting, we point out the difficulty in estimating the gradient of\nthe variance of the return and incorporate simultaneous perturbation approaches\nto alleviate this. The average setting, on the other hand, allows for an actor\nupdate using compatible features to estimate the gradient of the variance. We\nestablish the convergence of our algorithms to locally risk-sensitive optimal\npolicies. Finally, we demonstrate the usefulness of our algorithms in a traffic\nsignal control application.\n", "versions": [{"version": "v1", "created": "Tue, 25 Mar 2014 23:00:50 GMT"}, {"version": "v2", "created": "Wed, 18 Mar 2015 15:42:31 GMT"}], "update_date": "2015-03-19", "authors_parsed": [["A.", "Prashanth L.", ""], ["Ghavamzadeh", "Mohammad", ""]]}, {"id": "1403.6706", "submitter": "Karthikeyan Natesan Ramamurthy", "authors": "Karthikeyan Natesan Ramamurthy, Aleksandr Y. Aravkin, Jayaraman J.\n  Thiagarajan", "title": "Beyond L2-Loss Functions for Learning Sparse Models", "comments": "10 pages, 6 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.CV cs.LG math.OC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Incorporating sparsity priors in learning tasks can give rise to simple, and\ninterpretable models for complex high dimensional data. Sparse models have\nfound widespread use in structure discovery, recovering data from corruptions,\nand a variety of large scale unsupervised and supervised learning problems.\nAssuming the availability of sufficient data, these methods infer dictionaries\nfor sparse representations by optimizing for high-fidelity reconstruction. In\nmost scenarios, the reconstruction quality is measured using the squared\nEuclidean distance, and efficient algorithms have been developed for both batch\nand online learning cases. However, new application domains motivate looking\nbeyond conventional loss functions. For example, robust loss functions such as\n$\\ell_1$ and Huber are useful in learning outlier-resilient models, and the\nquantile loss is beneficial in discovering structures that are the\nrepresentative of a particular quantile. These new applications motivate our\nwork in generalizing sparse learning to a broad class of convex loss functions.\nIn particular, we consider the class of piecewise linear quadratic (PLQ) cost\nfunctions that includes Huber, as well as $\\ell_1$, quantile, Vapnik, hinge\nloss, and smoothed variants of these penalties. We propose an algorithm to\nlearn dictionaries and obtain sparse codes when the data reconstruction\nfidelity is measured using any smooth PLQ cost function. We provide convergence\nguarantees for the proposed algorithm, and demonstrate the convergence behavior\nusing empirical experiments. Furthermore, we present three case studies that\nrequire the use of PLQ cost functions: (i) robust image modeling, (ii) tag\nrefinement for image annotation and retrieval and (iii) computing empirical\nconfidence limits for subspace clustering.\n", "versions": [{"version": "v1", "created": "Wed, 26 Mar 2014 15:16:56 GMT"}], "update_date": "2014-03-27", "authors_parsed": [["Ramamurthy", "Karthikeyan Natesan", ""], ["Aravkin", "Aleksandr Y.", ""], ["Thiagarajan", "Jayaraman J.", ""]]}, {"id": "1403.7084", "submitter": "David Van Leeuwen", "authors": "David A. van Leeuwen and Niko Br\\\"ummer", "title": "Constrained speaker linking", "comments": "Submitted to Interspeech 2014, some typos fixed", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.SD", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper we study speaker linking (a.k.a.\\ partitioning) given\nconstraints of the distribution of speaker identities over speech recordings.\nSpecifically, we show that the intractable partitioning problem becomes\ntractable when the constraints pre-partition the data in smaller cliques with\nnon-overlapping speakers. The surprisingly common case where speakers in\ntelephone conversations are known, but the assignment of channels to identities\nis unspecified, is treated in a Bayesian way. We show that for the Dutch CGN\ndatabase, where this channel assignment task is at hand, a lightweight speaker\nrecognition system can quite effectively solve the channel assignment problem,\nwith 93% of the cliques solved. We further show that the posterior distribution\nover channel assignment configurations is well calibrated.\n", "versions": [{"version": "v1", "created": "Wed, 26 Mar 2014 14:51:31 GMT"}, {"version": "v2", "created": "Wed, 2 Apr 2014 12:19:03 GMT"}], "update_date": "2014-04-03", "authors_parsed": [["van Leeuwen", "David A.", ""], ["Br\u00fcmmer", "Niko", ""]]}, {"id": "1403.7265", "submitter": "Elaine Angelino", "authors": "Elaine Angelino, Eddie Kohler, Amos Waterland, Margo Seltzer and Ryan\n  P. Adams", "title": "Accelerating MCMC via Parallel Predictive Prefetching", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a general framework for accelerating a large class of widely used\nMarkov chain Monte Carlo (MCMC) algorithms. Our approach exploits fast,\niterative approximations to the target density to speculatively evaluate many\npotential future steps of the chain in parallel. The approach can accelerate\ncomputation of the target distribution of a Bayesian inference problem, without\ncompromising exactness, by exploiting subsets of data. It takes advantage of\nwhatever parallel resources are available, but produces results exactly\nequivalent to standard serial execution. In the initial burn-in phase of chain\nevaluation, it achieves speedup over serial evaluation that is close to linear\nin the number of available cores.\n", "versions": [{"version": "v1", "created": "Fri, 28 Mar 2014 01:28:52 GMT"}], "update_date": "2014-03-31", "authors_parsed": [["Angelino", "Elaine", ""], ["Kohler", "Eddie", ""], ["Waterland", "Amos", ""], ["Seltzer", "Margo", ""], ["Adams", "Ryan P.", ""]]}, {"id": "1403.7267", "submitter": "Roberto Aldave", "authors": "Roberto Aldave and Jean-Pierre Dussault", "title": "Systematic Ensemble Learning for Regression", "comments": "38 pages, 6 figures. Submitted to Machine Learning", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The motivation of this work is to improve the performance of standard\nstacking approaches or ensembles, which are composed of simple, heterogeneous\nbase models, through the integration of the generation and selection stages for\nregression problems. We propose two extensions to the standard stacking\napproach. In the first extension we combine a set of standard stacking\napproaches into an ensemble of ensembles using a two-step ensemble learning in\nthe regression setting. The second extension consists of two parts. In the\ninitial part a diversity mechanism is injected into the original training data\nset, systematically generating different training subsets or partitions, and\ncorresponding ensembles of ensembles. In the final part after measuring the\nquality of the different partitions or ensembles, a max-min rule-based\nselection algorithm is used to select the most appropriate ensemble/partition\non which to make the final prediction. We show, based on experiments over a\nbroad range of data sets, that the second extension performs better than the\nbest of the standard stacking approaches, and is as good as the oracle of\ndatabases, which has the best base model selected by cross-validation for each\ndata set. In addition to that, the second extension performs better than two\nstate-of-the-art ensemble methods for regression, and it is as good as a third\nstate-of-the-art ensemble method.\n", "versions": [{"version": "v1", "created": "Fri, 28 Mar 2014 01:36:27 GMT"}], "update_date": "2014-03-31", "authors_parsed": [["Aldave", "Roberto", ""], ["Dussault", "Jean-Pierre", ""]]}, {"id": "1403.7304", "submitter": "Yu Nishiyama", "authors": "Yu Nishiyama and Kenji Fukumizu", "title": "Characteristic Kernels and Infinitely Divisible Distributions", "comments": null, "journal-ref": "Journal of Machine Learning Research 17(180):1-28, 2016", "doi": null, "report-no": null, "categories": "stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We connect shift-invariant characteristic kernels to infinitely divisible\ndistributions on $\\mathbb{R}^{d}$. Characteristic kernels play an important\nrole in machine learning applications with their kernel means to distinguish\nany two probability measures. The contribution of this paper is two-fold.\nFirst, we show, using the L\\'evy-Khintchine formula, that any shift-invariant\nkernel given by a bounded, continuous and symmetric probability density\nfunction (pdf) of an infinitely divisible distribution on $\\mathbb{R}^d$ is\ncharacteristic. We also present some closure property of such characteristic\nkernels under addition, pointwise product, and convolution. Second, in\ndeveloping various kernel mean algorithms, it is fundamental to compute the\nfollowing values: (i) kernel mean values $m_P(x)$, $x \\in \\mathcal{X}$, and\n(ii) kernel mean RKHS inner products ${\\left\\langle m_P, m_Q\n\\right\\rangle_{\\mathcal{H}}}$, for probability measures $P, Q$. If $P, Q$, and\nkernel $k$ are Gaussians, then computation (i) and (ii) results in Gaussian\npdfs that is tractable. We generalize this Gaussian combination to more general\ncases in the class of infinitely divisible distributions. We then introduce a\n{\\it conjugate} kernel and {\\it convolution trick}, so that the above (i) and\n(ii) have the same pdf form, expecting tractable computation at least in some\ncases. As specific instances, we explore $\\alpha$-stable distributions and a\nrich class of generalized hyperbolic distributions, where the Laplace, Cauchy\nand Student-t distributions are included.\n", "versions": [{"version": "v1", "created": "Fri, 28 Mar 2014 08:41:28 GMT"}, {"version": "v2", "created": "Wed, 18 May 2016 10:53:08 GMT"}, {"version": "v3", "created": "Tue, 25 Oct 2016 03:35:10 GMT"}], "update_date": "2016-10-26", "authors_parsed": [["Nishiyama", "Yu", ""], ["Fukumizu", "Kenji", ""]]}, {"id": "1403.7308", "submitter": "Marko Robnik-\\v{S}ikonja", "authors": "Marko Robnik-\\v{S}ikonja", "title": "Data Generators for Learning Systems Based on RBF Networks", "comments": null, "journal-ref": "IEEE Transaction on Neural Networks and Learning Systems,\n  27(5):926-938, 2016", "doi": "10.1109/TNNLS.2015.2429711", "report-no": null, "categories": "stat.ML cs.AI cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  There are plenty of problems where the data available is scarce and\nexpensive. We propose a generator of semi-artificial data with similar\nproperties to the original data which enables development and testing of\ndifferent data mining algorithms and optimization of their parameters. The\ngenerated data allow a large scale experimentation and simulations without\ndanger of overfitting. The proposed generator is based on RBF networks, which\nlearn sets of Gaussian kernels. These Gaussian kernels can be used in a\ngenerative mode to generate new data from the same distributions. To assess\nquality of the generated data we evaluated the statistical properties of the\ngenerated data, structural similarity and predictive similarity using\nsupervised and unsupervised learning techniques. To determine usability of the\nproposed generator we conducted a large scale evaluation using 51 UCI data\nsets. The results show a considerable similarity between the original and\ngenerated data and indicate that the method can be useful in several\ndevelopment and simulation scenarios. We analyze possible improvements in\nclassification performance by adding different amounts of generated data to the\ntraining set, performance on high dimensional data sets, and conditions when\nthe proposed approach is successful.\n", "versions": [{"version": "v1", "created": "Fri, 28 Mar 2014 08:55:21 GMT"}, {"version": "v2", "created": "Sun, 19 Jul 2020 21:49:33 GMT"}], "update_date": "2020-07-21", "authors_parsed": [["Robnik-\u0160ikonja", "Marko", ""]]}, {"id": "1403.7550", "submitter": "Ce Zhang", "authors": "Ce Zhang and Christopher R\\'e", "title": "DimmWitted: A Study of Main-Memory Statistical Analytics", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB cs.LG math.OC stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We perform the first study of the tradeoff space of access methods and\nreplication to support statistical analytics using first-order methods executed\nin the main memory of a Non-Uniform Memory Access (NUMA) machine. Statistical\nanalytics systems differ from conventional SQL-analytics in the amount and\ntypes of memory incoherence they can tolerate. Our goal is to understand\ntradeoffs in accessing the data in row- or column-order and at what granularity\none should share the model and data for a statistical task. We study this new\ntradeoff space, and discover there are tradeoffs between hardware and\nstatistical efficiency. We argue that our tradeoff study may provide valuable\ninformation for designers of analytics engines: for each system we consider,\nour prototype engine can run at least one popular task at least 100x faster. We\nconduct our study across five architectures using popular models including\nSVMs, logistic regression, Gibbs sampling, and neural networks.\n", "versions": [{"version": "v1", "created": "Fri, 28 Mar 2014 21:48:00 GMT"}, {"version": "v2", "created": "Thu, 1 May 2014 06:14:29 GMT"}, {"version": "v3", "created": "Mon, 7 Jul 2014 17:20:20 GMT"}], "update_date": "2014-07-08", "authors_parsed": [["Zhang", "Ce", ""], ["R\u00e9", "Christopher", ""]]}, {"id": "1403.7588", "submitter": "Cun Mu", "authors": "Cun Mu, Yuqian Zhang, John Wright, Donald Goldfarb", "title": "Scalable Robust Matrix Recovery: Frank-Wolfe Meets Proximal Methods", "comments": null, "journal-ref": "SIAM Journal on Scientific Computing, 2016, Vol. 38, No. 5 : pp.\n  A3291-A3317", "doi": "10.1137/15M101628X", "report-no": null, "categories": "math.OC cs.CV cs.NA stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recovering matrices from compressive and grossly corrupted observations is a\nfundamental problem in robust statistics, with rich applications in computer\nvision and machine learning. In theory, under certain conditions, this problem\ncan be solved in polynomial time via a natural convex relaxation, known as\nCompressive Principal Component Pursuit (CPCP). However, all existing provable\nalgorithms for CPCP suffer from superlinear per-iteration cost, which severely\nlimits their applicability to large scale problems. In this paper, we propose\nprovable, scalable and efficient methods to solve CPCP with (essentially)\nlinear per-iteration cost. Our method combines classical ideas from Frank-Wolfe\nand proximal methods. In each iteration, we mainly exploit Frank-Wolfe to\nupdate the low-rank component with rank-one SVD and exploit the proximal step\nfor the sparse term. Convergence results and implementation details are also\ndiscussed. We demonstrate the scalability of the proposed approach with\npromising numerical experiments on visual data.\n", "versions": [{"version": "v1", "created": "Sat, 29 Mar 2014 04:04:43 GMT"}, {"version": "v2", "created": "Mon, 29 May 2017 21:16:42 GMT"}], "update_date": "2017-05-31", "authors_parsed": [["Mu", "Cun", ""], ["Zhang", "Yuqian", ""], ["Wright", "John", ""], ["Goldfarb", "Donald", ""]]}, {"id": "1403.7683", "submitter": "Anastasios Kyrillidis", "authors": "Anastasios Kyrillidis and Michail Vlachos and Anastasios Zouzias", "title": "Approximate Matrix Multiplication with Application to Linear Embeddings", "comments": "8 pages, International Symposium on Information Theory", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST cs.IT math.IT stat.ML stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we study the problem of approximately computing the product of\ntwo real matrices. In particular, we analyze a dimensionality-reduction-based\napproximation algorithm due to Sarlos [1], introducing the notion of nuclear\nrank as the ratio of the nuclear norm over the spectral norm. The presented\nbound has improved dependence with respect to the approximation error (as\ncompared to previous approaches), whereas the subspace -- on which we project\nthe input matrices -- has dimensions proportional to the maximum of their\nnuclear rank and it is independent of the input dimensions. In addition, we\nprovide an application of this result to linear low-dimensional embeddings.\nNamely, we show that any Euclidean point-set with bounded nuclear rank is\namenable to projection onto number of dimensions that is independent of the\ninput dimensionality, while achieving additive error guarantees.\n", "versions": [{"version": "v1", "created": "Sun, 30 Mar 2014 00:24:21 GMT"}], "update_date": "2014-04-01", "authors_parsed": [["Kyrillidis", "Anastasios", ""], ["Vlachos", "Michail", ""], ["Zouzias", "Anastasios", ""]]}, {"id": "1403.7737", "submitter": "Shusen Wang", "authors": "Shusen Wang", "title": "Sharpened Error Bounds for Random Sampling Based $\\ell_2$ Regression", "comments": "unpublished manuscript", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.NA stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Given a data matrix $X \\in R^{n\\times d}$ and a response vector $y \\in\nR^{n}$, suppose $n>d$, it costs $O(n d^2)$ time and $O(n d)$ space to solve the\nleast squares regression (LSR) problem. When $n$ and $d$ are both large,\nexactly solving the LSR problem is very expensive. When $n \\gg d$, one feasible\napproach to speeding up LSR is to randomly embed $y$ and all columns of $X$\ninto a smaller subspace $R^c$; the induced LSR problem has the same number of\ncolumns but much fewer number of rows, and it can be solved in $O(c d^2)$ time\nand $O(c d)$ space.\n  We discuss in this paper two random sampling based methods for solving LSR\nmore efficiently. Previous work showed that the leverage scores based sampling\nbased LSR achieves $1+\\epsilon$ accuracy when $c \\geq O(d \\epsilon^{-2} \\log\nd)$. In this paper we sharpen this error bound, showing that $c = O(d \\log d +\nd \\epsilon^{-1})$ is enough for achieving $1+\\epsilon$ accuracy. We also show\nthat when $c \\geq O(\\mu d \\epsilon^{-2} \\log d)$, the uniform sampling based\nLSR attains a $2+\\epsilon$ bound with positive probability.\n", "versions": [{"version": "v1", "created": "Sun, 30 Mar 2014 11:21:39 GMT"}, {"version": "v2", "created": "Sat, 5 Apr 2014 05:56:04 GMT"}], "update_date": "2014-04-08", "authors_parsed": [["Wang", "Shusen", ""]]}, {"id": "1403.7890", "submitter": "Xiangyu Chang", "authors": "Xiangyu Chang, Yu Wang, Rongjian Li, Zongben Xu", "title": "Sparse K-Means with $\\ell_{\\infty}/\\ell_0$ Penalty for High-Dimensional\n  Data Clustering", "comments": "36 pages, 4 figures, Present the paper at ICSA 2013", "journal-ref": "Statistica Sinica 28 (2018)1265-1284", "doi": null, "report-no": "SS-2015-0261", "categories": "stat.ML cs.LG stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Sparse clustering, which aims to find a proper partition of an extremely\nhigh-dimensional data set with redundant noise features, has been attracted\nmore and more interests in recent years. The existing studies commonly solve\nthe problem in a framework of maximizing the weighted feature contributions\nsubject to a $\\ell_2/\\ell_1$ penalty. Nevertheless, this framework has two\nserious drawbacks: One is that the solution of the framework unavoidably\ninvolves a considerable portion of redundant noise features in many situations,\nand the other is that the framework neither offers intuitive explanations on\nwhy this framework can select relevant features nor leads to any theoretical\nguarantee for feature selection consistency.\n  In this article, we attempt to overcome those drawbacks through developing a\nnew sparse clustering framework which uses a $\\ell_{\\infty}/\\ell_0$ penalty.\nFirst, we introduce new concepts on optimal partitions and noise features for\nthe high-dimensional data clustering problems, based on which the previously\nknown framework can be intuitively explained in principle. Then, we apply the\nsuggested $\\ell_{\\infty}/\\ell_0$ framework to formulate a new sparse k-means\nmodel with the $\\ell_{\\infty}/\\ell_0$ penalty ($\\ell_0$-k-means for short). We\npropose an efficient iterative algorithm for solving the $\\ell_0$-k-means. To\ndeeply understand the behavior of $\\ell_0$-k-means, we prove that the solution\nyielded by the $\\ell_0$-k-means algorithm has feature selection consistency\nwhenever the data matrix is generated from a high-dimensional Gaussian mixture\nmodel. Finally, we provide experiments with both synthetic data and the Allen\nDeveloping Mouse Brain Atlas data to support that the proposed $\\ell_0$-k-means\nexhibits better noise feature detection capacity over the previously known\nsparse k-means with the $\\ell_2/\\ell_1$ penalty ($\\ell_1$-k-means for short).\n", "versions": [{"version": "v1", "created": "Mon, 31 Mar 2014 07:18:55 GMT"}], "update_date": "2019-02-25", "authors_parsed": [["Chang", "Xiangyu", ""], ["Wang", "Yu", ""], ["Li", "Rongjian", ""], ["Xu", "Zongben", ""]]}, {"id": "1403.8098", "submitter": "Miguel Sim\\~oes", "authors": "Miguel Sim\\~oes, Jos\\'e Bioucas-Dias, Luis B. Almeida, Jocelyn\n  Chanussot", "title": "Hyperspectral image superresolution: An edge-preserving convex\n  formulation", "comments": "International Conference on Image Processing (ICIP), 2014 - accepted", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV physics.data-an stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Hyperspectral remote sensing images (HSIs) are characterized by having a low\nspatial resolution and a high spectral resolution, whereas multispectral images\n(MSIs) are characterized by low spectral and high spatial resolutions. These\ncomplementary characteristics have stimulated active research in the inference\nof images with high spatial and spectral resolutions from HSI-MSI pairs.\n  In this paper, we formulate this data fusion problem as the minimization of a\nconvex objective function containing two data-fitting terms and an\nedge-preserving regularizer. The data-fitting terms are quadratic and account\nfor blur, different spatial resolutions, and additive noise; the regularizer, a\nform of vector Total Variation, promotes aligned discontinuities across the\nreconstructed hyperspectral bands.\n  The optimization described above is rather hard, owing to its\nnon-diagonalizable linear operators, to the non-quadratic and non-smooth nature\nof the regularizer, and to the very large size of the image to be inferred. We\ntackle these difficulties by tailoring the Split Augmented Lagrangian Shrinkage\nAlgorithm (SALSA)---an instance of the Alternating Direction Method of\nMultipliers (ADMM)---to this optimization problem. By using a convenient\nvariable splitting and by exploiting the fact that HSIs generally \"live\" in a\nlow-dimensional subspace, we obtain an effective algorithm that yields\nstate-of-the-art results, as illustrated by experiments.\n", "versions": [{"version": "v1", "created": "Mon, 31 Mar 2014 17:18:48 GMT"}, {"version": "v2", "created": "Tue, 10 Jun 2014 11:58:55 GMT"}], "update_date": "2014-06-11", "authors_parsed": [["Sim\u00f5es", "Miguel", ""], ["Bioucas-Dias", "Jos\u00e9", ""], ["Almeida", "Luis B.", ""], ["Chanussot", "Jocelyn", ""]]}]