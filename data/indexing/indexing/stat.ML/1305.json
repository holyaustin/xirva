[{"id": "1305.0015", "submitter": "Balaji Lakshminarayanan", "authors": "Balaji Lakshminarayanan and Yee Whye Teh", "title": "Inferring ground truth from multi-annotator ordinal data: a\n  probabilistic approach", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A popular approach for large scale data annotation tasks is crowdsourcing,\nwherein each data point is labeled by multiple noisy annotators. We consider\nthe problem of inferring ground truth from noisy ordinal labels obtained from\nmultiple annotators of varying and unknown expertise levels. Annotation models\nfor ordinal data have been proposed mostly as extensions of their\nbinary/categorical counterparts and have received little attention in the\ncrowdsourcing literature. We propose a new model for crowdsourced ordinal data\nthat accounts for instance difficulty as well as annotator expertise, and\nderive a variational Bayesian inference algorithm for parameter estimation. We\nanalyze the ordinal extensions of several state-of-the-art annotator models for\nbinary/categorical labels and evaluate the performance of all the models on two\nreal world datasets containing ordinal query-URL relevance scores, collected\nthrough Amazon's Mechanical Turk. Our results indicate that the proposed model\nperforms better or as well as existing state-of-the-art methods and is more\nresistant to `spammy' annotators (i.e., annotators who assign labels randomly\nwithout actually looking at the instance) than popular baselines such as mean,\nmedian, and majority vote which do not account for annotator expertise.\n", "versions": [{"version": "v1", "created": "Tue, 30 Apr 2013 20:12:01 GMT"}], "update_date": "2013-05-02", "authors_parsed": [["Lakshminarayanan", "Balaji", ""], ["Teh", "Yee Whye", ""]]}, {"id": "1305.0030", "submitter": "Anthony Nouy", "authors": "Mathilde Chevreuil, R\\'egis Lebrun, Anthony Nouy, Prashant Rai", "title": "A least-squares method for sparse low rank approximation of multivariate\n  functions", "comments": null, "journal-ref": "SIAM/ASA Journal on Uncertainty Quantification}, 3(1):897--921,\n  2015", "doi": "10.1137/13091899X", "report-no": null, "categories": "math.NA stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we propose a low-rank approximation method based on discrete\nleast-squares for the approximation of a multivariate function from random,\nnoisy-free observations. Sparsity inducing regularization techniques are used\nwithin classical algorithms for low-rank approximation in order to exploit the\npossible sparsity of low-rank approximations. Sparse low-rank approximations\nare constructed with a robust updated greedy algorithm which includes an\noptimal selection of regularization parameters and approximation ranks using\ncross validation techniques. Numerical examples demonstrate the capability of\napproximating functions of many variables even when very few function\nevaluations are available, thus proving the interest of the proposed algorithm\nfor the propagation of uncertainties through complex computational models.\n", "versions": [{"version": "v1", "created": "Tue, 30 Apr 2013 21:25:54 GMT"}, {"version": "v2", "created": "Wed, 9 Jul 2014 15:13:01 GMT"}], "update_date": "2015-12-09", "authors_parsed": [["Chevreuil", "Mathilde", ""], ["Lebrun", "R\u00e9gis", ""], ["Nouy", "Anthony", ""], ["Rai", "Prashant", ""]]}, {"id": "1305.0047", "submitter": "Ji Liu", "authors": "Ji Liu and Lei Yuan and Jieping Ye", "title": "Dictionary LASSO: Guaranteed Sparse Recovery under Linear Transformation", "comments": "26 pages, 3 figures, ICML2013", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider the following signal recovery problem: given a measurement matrix\n$\\Phi\\in \\mathbb{R}^{n\\times p}$ and a noisy observation vector $c\\in\n\\mathbb{R}^{n}$ constructed from $c = \\Phi\\theta^* + \\epsilon$ where\n$\\epsilon\\in \\mathbb{R}^{n}$ is the noise vector whose entries follow i.i.d.\ncentered sub-Gaussian distribution, how to recover the signal $\\theta^*$ if\n$D\\theta^*$ is sparse {\\rca under a linear transformation}\n$D\\in\\mathbb{R}^{m\\times p}$? One natural method using convex optimization is\nto solve the following problem: $$\\min_{\\theta} {1\\over 2}\\|\\Phi\\theta - c\\|^2\n+ \\lambda\\|D\\theta\\|_1.$$ This paper provides an upper bound of the estimate\nerror and shows the consistency property of this method by assuming that the\ndesign matrix $\\Phi$ is a Gaussian random matrix. Specifically, we show 1) in\nthe noiseless case, if the condition number of $D$ is bounded and the\nmeasurement number $n\\geq \\Omega(s\\log(p))$ where $s$ is the sparsity number,\nthen the true solution can be recovered with high probability; and 2) in the\nnoisy case, if the condition number of $D$ is bounded and the measurement\nincreases faster than $s\\log(p)$, that is, $s\\log(p)=o(n)$, the estimate error\nconverges to zero with probability 1 when $p$ and $s$ go to infinity. Our\nresults are consistent with those for the special case $D=\\bold{I}_{p\\times p}$\n(equivalently LASSO) and improve the existing analysis. The condition number of\n$D$ plays a critical role in our analysis. We consider the condition numbers in\ntwo cases including the fused LASSO and the random graph: the condition number\nin the fused LASSO case is bounded by a constant, while the condition number in\nthe random graph case is bounded with high probability if $m\\over p$ (i.e.,\n$#text{edge}\\over #text{vertex}$) is larger than a certain constant. Numerical\nsimulations are consistent with our theoretical results.\n", "versions": [{"version": "v1", "created": "Tue, 30 Apr 2013 22:48:54 GMT"}, {"version": "v2", "created": "Sat, 20 Jul 2013 17:14:06 GMT"}], "update_date": "2013-07-23", "authors_parsed": [["Liu", "Ji", ""], ["Yuan", "Lei", ""], ["Ye", "Jieping", ""]]}, {"id": "1305.0051", "submitter": "Kevin Xu", "authors": "Kevin S. Xu, Mark Kliger, Yilun Chen, Peter J. Woolf, and Alfred O.\n  Hero III", "title": "Revealing social networks of spammers through spectral clustering", "comments": "Source code and data available at\n  http://tbayes.eecs.umich.edu/xukevin/spam_icc09 Proceedings of the IEEE\n  International Conference on Communications (2009)", "journal-ref": null, "doi": "10.1109/ICC.2009.5199418", "report-no": null, "categories": "cs.SI cs.LG physics.soc-ph stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  To date, most studies on spam have focused only on the spamming phase of the\nspam cycle and have ignored the harvesting phase, which consists of the mass\nacquisition of email addresses. It has been observed that spammers conceal\ntheir identity to a lesser degree in the harvesting phase, so it may be\npossible to gain new insights into spammers' behavior by studying the behavior\nof harvesters, which are individuals or bots that collect email addresses. In\nthis paper, we reveal social networks of spammers by identifying communities of\nharvesters with high behavioral similarity using spectral clustering. The data\nanalyzed was collected through Project Honey Pot, a distributed system for\nmonitoring harvesting and spamming. Our main findings are (1) that most\nspammers either send only phishing emails or no phishing emails at all, (2)\nthat most communities of spammers also send only phishing emails or no phishing\nemails at all, and (3) that several groups of spammers within communities\nexhibit coherent temporal behavior and have similar IP addresses. Our findings\nreveal some previously unknown behavior of spammers and suggest that there is\nindeed social structure between spammers to be discovered.\n", "versions": [{"version": "v1", "created": "Tue, 30 Apr 2013 22:57:12 GMT"}], "update_date": "2013-05-02", "authors_parsed": [["Xu", "Kevin S.", ""], ["Kliger", "Mark", ""], ["Chen", "Yilun", ""], ["Woolf", "Peter J.", ""], ["Hero", "Alfred O.", "III"]]}, {"id": "1305.0087", "submitter": "Jiyan Yang", "authors": "Jiyan Yang and Xiangrui Meng and Michael W. Mahoney", "title": "Quantile Regression for Large-scale Applications", "comments": "35 pages; long version of a paper appearing in the 2013 ICML. Version\n  to appear in the SIAM Journal on Scientific Computing", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.DC cs.NA stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Quantile regression is a method to estimate the quantiles of the conditional\ndistribution of a response variable, and as such it permits a much more\naccurate portrayal of the relationship between the response variable and\nobserved covariates than methods such as Least-squares or Least Absolute\nDeviations regression. It can be expressed as a linear program, and, with\nappropriate preprocessing, interior-point methods can be used to find a\nsolution for moderately large problems. Dealing with very large problems,\n\\emph(e.g.), involving data up to and beyond the terabyte regime, remains a\nchallenge. Here, we present a randomized algorithm that runs in nearly linear\ntime in the size of the input and that, with constant probability, computes a\n$(1+\\epsilon)$ approximate solution to an arbitrary quantile regression\nproblem. As a key step, our algorithm computes a low-distortion\nsubspace-preserving embedding with respect to the loss function of quantile\nregression. Our empirical evaluation illustrates that our algorithm is\ncompetitive with the best previous work on small to medium-sized problems, and\nthat in addition it can be implemented in MapReduce-like environments and\napplied to terabyte-sized problems.\n", "versions": [{"version": "v1", "created": "Wed, 1 May 2013 05:21:03 GMT"}, {"version": "v2", "created": "Mon, 14 Oct 2013 18:18:50 GMT"}, {"version": "v3", "created": "Tue, 7 Jan 2014 00:33:30 GMT"}], "update_date": "2014-01-08", "authors_parsed": [["Yang", "Jiyan", ""], ["Meng", "Xiangrui", ""], ["Mahoney", "Michael W.", ""]]}, {"id": "1305.0213", "submitter": "Akshay Krishnamurthy", "authors": "Akshay Krishnamurthy, James Sharpnack, Aarti Singh", "title": "Recovering Graph-Structured Activations using Adaptive Compressive\n  Measurements", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.IT math.IT", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study the localization of a cluster of activated vertices in a graph, from\nadaptively designed compressive measurements. We propose a hierarchical\npartitioning of the graph that groups the activated vertices into few\npartitions, so that a top-down sensing procedure can identify these partitions,\nand hence the activations, using few measurements. By exploiting the cluster\nstructure, we are able to provide localization guarantees at weaker signal to\nnoise ratios than in the unstructured setting. We complement this performance\nguarantee with an information theoretic lower bound, providing a necessary\nsignal-to-noise ratio for any algorithm to successfully localize the cluster.\nWe verify our analysis with some simulations, demonstrating the practicality of\nour algorithm.\n", "versions": [{"version": "v1", "created": "Wed, 1 May 2013 16:06:20 GMT"}, {"version": "v2", "created": "Fri, 20 Sep 2013 19:00:10 GMT"}, {"version": "v3", "created": "Fri, 14 Feb 2014 00:59:04 GMT"}], "update_date": "2014-02-17", "authors_parsed": [["Krishnamurthy", "Akshay", ""], ["Sharpnack", "James", ""], ["Singh", "Aarti", ""]]}, {"id": "1305.0258", "submitter": "Nathan Monnig", "authors": "Nathan D. Monnig, Bengt Fornberg, and Francois G. Meyer", "title": "Inverting Nonlinear Dimensionality Reduction with Scale-Free Radial\n  Basis Function Interpolation", "comments": "Accepted for publication in Applied and Computational Harmonic\n  Analysis", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.NA cs.NA physics.data-an stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Nonlinear dimensionality reduction embeddings computed from datasets do not\nprovide a mechanism to compute the inverse map. In this paper, we address the\nproblem of computing a stable inverse map to such a general bi-Lipschitz map.\nOur approach relies on radial basis functions (RBFs) to interpolate the inverse\nmap everywhere on the low-dimensional image of the forward map. We demonstrate\nthat the scale-free cubic RBF kernel performs better than the Gaussian kernel:\nit does not suffer from ill-conditioning, and does not require the choice of a\nscale. The proposed construction is shown to be similar to the Nystr\\\"om\nextension of the eigenvectors of the symmetric normalized graph Laplacian\nmatrix. Based on this observation, we provide a new interpretation of the\nNystr\\\"om extension with suggestions for improvement.\n", "versions": [{"version": "v1", "created": "Wed, 1 May 2013 19:55:06 GMT"}, {"version": "v2", "created": "Tue, 5 Nov 2013 15:49:52 GMT"}], "update_date": "2013-11-06", "authors_parsed": [["Monnig", "Nathan D.", ""], ["Fornberg", "Bengt", ""], ["Meyer", "Francois G.", ""]]}, {"id": "1305.0319", "submitter": "Adrian Barbu", "authors": "Adrian Barbu, Tianfu Wu, Ying Nian Wu", "title": "Learning Mixtures of Bernoulli Templates by Two-Round EM with\n  Performance Guarantee", "comments": "27 pages, 8 figures", "journal-ref": "Electronic Journal of Statistics, Vol. 8, No. 2, pp 3004-3030,\n  2014", "doi": "10.1214/14-EJS981", "report-no": null, "categories": "stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Dasgupta and Shulman showed that a two-round variant of the EM algorithm can\nlearn mixture of Gaussian distributions with near optimal precision with high\nprobability if the Gaussian distributions are well separated and if the\ndimension is sufficiently high. In this paper, we generalize their theory to\nlearning mixture of high-dimensional Bernoulli templates. Each template is a\nbinary vector, and a template generates examples by randomly switching its\nbinary components independently with a certain probability. In computer vision\napplications, a binary vector is a feature map of an image, where each binary\ncomponent indicates whether a local feature or structure is present or absent\nwithin a certain cell of the image domain. A Bernoulli template can be\nconsidered as a statistical model for images of objects (or parts of objects)\nfrom the same category. We show that the two-round EM algorithm can learn\nmixture of Bernoulli templates with near optimal precision with high\nprobability, if the Bernoulli templates are sufficiently different and if the\nnumber of features is sufficiently high. We illustrate the theoretical results\nby synthetic and real examples.\n", "versions": [{"version": "v1", "created": "Thu, 2 May 2013 00:31:45 GMT"}, {"version": "v2", "created": "Fri, 3 May 2013 09:55:22 GMT"}, {"version": "v3", "created": "Thu, 5 Sep 2013 16:46:21 GMT"}, {"version": "v4", "created": "Tue, 10 Dec 2013 16:44:37 GMT"}, {"version": "v5", "created": "Wed, 13 Aug 2014 21:23:36 GMT"}, {"version": "v6", "created": "Tue, 20 Jan 2015 03:16:21 GMT"}], "update_date": "2015-01-21", "authors_parsed": [["Barbu", "Adrian", ""], ["Wu", "Tianfu", ""], ["Wu", "Ying Nian", ""]]}, {"id": "1305.0355", "submitter": "Adel Javanmard", "authors": "Adel Javanmard and Andrea Montanari", "title": "Model Selection for High-Dimensional Regression under the Generalized\n  Irrepresentability Condition", "comments": "32 pages, 3 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST cs.IT cs.LG math.IT stat.ME stat.ML stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In the high-dimensional regression model a response variable is linearly\nrelated to $p$ covariates, but the sample size $n$ is smaller than $p$. We\nassume that only a small subset of covariates is `active' (i.e., the\ncorresponding coefficients are non-zero), and consider the model-selection\nproblem of identifying the active covariates. A popular approach is to estimate\nthe regression coefficients through the Lasso ($\\ell_1$-regularized least\nsquares). This is known to correctly identify the active set only if the\nirrelevant covariates are roughly orthogonal to the relevant ones, as\nquantified through the so called `irrepresentability' condition. In this paper\nwe study the `Gauss-Lasso' selector, a simple two-stage method that first\nsolves the Lasso, and then performs ordinary least squares restricted to the\nLasso active set. We formulate `generalized irrepresentability condition'\n(GIC), an assumption that is substantially weaker than irrepresentability. We\nprove that, under GIC, the Gauss-Lasso correctly recovers the active set.\n", "versions": [{"version": "v1", "created": "Thu, 2 May 2013 07:25:52 GMT"}], "update_date": "2013-05-03", "authors_parsed": [["Javanmard", "Adel", ""], ["Montanari", "Andrea", ""]]}, {"id": "1305.0395", "submitter": "Andrzej  Cichocki", "authors": "Andrzej Cichocki", "title": "Tensor Decompositions: A New Concept in Brain Data Analysis?", "comments": null, "journal-ref": "Control Measurement, and System Integration (SICE), special issue;\n  Measurement of Brain Functions and Bio-Signals, 7, 507-517, (2011)", "doi": null, "report-no": null, "categories": "cs.NA cs.LG q-bio.NC stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Matrix factorizations and their extensions to tensor factorizations and\ndecompositions have become prominent techniques for linear and multilinear\nblind source separation (BSS), especially multiway Independent Component\nAnalysis (ICA), NonnegativeMatrix and Tensor Factorization (NMF/NTF), Smooth\nComponent Analysis (SmoCA) and Sparse Component Analysis (SCA). Moreover,\ntensor decompositions have many other potential applications beyond multilinear\nBSS, especially feature extraction, classification, dimensionality reduction\nand multiway clustering. In this paper, we briefly overview new and emerging\nmodels and approaches for tensor decompositions in applications to group and\nlinked multiway BSS/ICA, feature extraction, classification andMultiway Partial\nLeast Squares (MPLS) regression problems. Keywords: Multilinear BSS, linked\nmultiway BSS/ICA, tensor factorizations and decompositions, constrained Tucker\nand CP models, Penalized Tensor Decompositions (PTD), feature extraction,\nclassification, multiway PLS and CCA.\n", "versions": [{"version": "v1", "created": "Thu, 2 May 2013 11:17:47 GMT"}], "update_date": "2013-05-03", "authors_parsed": [["Cichocki", "Andrzej", ""]]}, {"id": "1305.0423", "submitter": "Somayeh Danafar", "authors": "Somayeh Danafar, Paola M.V. Rancoita, Tobias Glasmachers, Kevin\n  Whittingstall, Juergen Schmidhuber", "title": "Testing Hypotheses by Regularized Maximum Mean Discrepancy", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Do two data samples come from different distributions? Recent studies of this\nfundamental problem focused on embedding probability distributions into\nsufficiently rich characteristic Reproducing Kernel Hilbert Spaces (RKHSs), to\ncompare distributions by the distance between their embeddings. We show that\nRegularized Maximum Mean Discrepancy (RMMD), our novel measure for kernel-based\nhypothesis testing, yields substantial improvements even when sample sizes are\nsmall, and excels at hypothesis tests involving multiple comparisons with power\ncontrol. We derive asymptotic distributions under the null and alternative\nhypotheses, and assess power control. Outstanding results are obtained on:\nchallenging EEG data, MNIST, the Berkley Covertype, and the Flare-Solar\ndataset.\n", "versions": [{"version": "v1", "created": "Thu, 2 May 2013 13:03:53 GMT"}], "update_date": "2013-05-03", "authors_parsed": [["Danafar", "Somayeh", ""], ["Rancoita", "Paola M. V.", ""], ["Glasmachers", "Tobias", ""], ["Whittingstall", "Kevin", ""], ["Schmidhuber", "Juergen", ""]]}, {"id": "1305.0626", "submitter": "Fuqiang Chen", "authors": "Fuqiang Chen", "title": "An Improved EM algorithm", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we firstly give a brief introduction of expectation\nmaximization (EM) algorithm, and then discuss the initial value sensitivity of\nexpectation maximization algorithm. Subsequently, we give a short proof of EM's\nconvergence. Then, we implement experiments with the expectation maximization\nalgorithm (We implement all the experiments on Gaussion mixture model (GMM)).\nOur experiment with expectation maximization is performed in the following\nthree cases: initialize randomly; initialize with result of K-means; initialize\nwith result of K-medoids. The experiment result shows that expectation\nmaximization algorithm depend on its initial state or parameters. And we found\nthat EM initialized with K-medoids performed better than both the one\ninitialized with K-means and the one initialized randomly.\n", "versions": [{"version": "v1", "created": "Fri, 3 May 2013 06:25:41 GMT"}], "update_date": "2013-05-06", "authors_parsed": [["Chen", "Fuqiang", ""]]}, {"id": "1305.0630", "submitter": "S\\'ebastien Loustau", "authors": "S\\'ebastien Loustau", "title": "Anisotropic oracle inequalities in noisy quantization", "comments": "30 pages. arXiv admin note: text overlap with arXiv:1205.1417", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.ML stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The effect of errors in variables in quantization is investigated. We prove\ngeneral exact and non-exact oracle inequalities with fast rates for an\nempirical minimization based on a noisy sample\n$Z_i=X_i+\\epsilon_i,i=1,\\ldots,n$, where $X_i$ are i.i.d. with density $f$ and\n$\\epsilon_i$ are i.i.d. with density $\\eta$. These rates depend on the geometry\nof the density $f$ and the asymptotic behaviour of the characteristic function\nof $\\eta$.\n  This general study can be applied to the problem of $k$-means clustering with\nnoisy data. For this purpose, we introduce a deconvolution $k$-means stochastic\nminimization which reaches fast rates of convergence under standard Pollard's\nregularity assumptions.\n", "versions": [{"version": "v1", "created": "Fri, 3 May 2013 07:37:15 GMT"}], "update_date": "2013-05-06", "authors_parsed": [["Loustau", "S\u00e9bastien", ""]]}, {"id": "1305.0638", "submitter": "Deqing Wang", "authors": "Deqing Wang, Hui Zhang, Rui Liu, Weifeng Lv", "title": "Feature Selection Based on Term Frequency and T-Test for Text\n  Categorization", "comments": "5pages 9 figures CIKM2012 paper", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.IR stat.ML", "license": "http://creativecommons.org/licenses/by-nc-sa/3.0/", "abstract": "  Much work has been done on feature selection. Existing methods are based on\ndocument frequency, such as Chi-Square Statistic, Information Gain etc.\nHowever, these methods have two shortcomings: one is that they are not reliable\nfor low-frequency terms, and the other is that they only count whether one term\noccurs in a document and ignore the term frequency. Actually, high-frequency\nterms within a specific category are often regards as discriminators.\n  This paper focuses on how to construct the feature selection function based\non term frequency, and proposes a new approach based on $t$-test, which is used\nto measure the diversity of the distributions of a term between the specific\ncategory and the entire corpus. Extensive comparative experiments on two text\ncorpora using three classifiers show that our new approach is comparable to or\nor slightly better than the state-of-the-art feature selection methods (i.e.,\n$\\chi^2$, and IG) in terms of macro-$F_1$ and micro-$F_1$.\n", "versions": [{"version": "v1", "created": "Fri, 3 May 2013 08:26:05 GMT"}], "update_date": "2013-05-06", "authors_parsed": [["Wang", "Deqing", ""], ["Zhang", "Hui", ""], ["Liu", "Rui", ""], ["Lv", "Weifeng", ""]]}, {"id": "1305.0751", "submitter": "Jose M. Pe\\~na", "authors": "Jose M. Pe\\~na", "title": "Marginal AMP Chain Graphs", "comments": "Changes from v1 to v2: Discussion section got extended. Changes from\n  v2 to v3: New Sections 3 and 5. Changes from v3 to v4: Example 4 added to\n  discussion section. Changes from v4 to v5: None. Changes from v5 to v6: Some\n  minor and major errors have been corrected. The latter include the\n  definitions of descending route and pairwise separation base, and the proofs\n  of Theorems 5 and 6", "journal-ref": "International Journal of Approximate Reasoning, 55 (5), 1185-1206,\n  2014", "doi": null, "report-no": null, "categories": "stat.ML cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a new family of models that is based on graphs that may have\nundirected, directed and bidirected edges. We name these new models marginal\nAMP (MAMP) chain graphs because each of them is Markov equivalent to some AMP\nchain graph under marginalization of some of its nodes. However, MAMP chain\ngraphs do not only subsume AMP chain graphs but also multivariate regression\nchain graphs. We describe global and pairwise Markov properties for MAMP chain\ngraphs and prove their equivalence for compositional graphoids. We also\ncharacterize when two MAMP chain graphs are Markov equivalent.\n  For Gaussian probability distributions, we also show that every MAMP chain\ngraph is Markov equivalent to some directed and acyclic graph with\ndeterministic nodes under marginalization and conditioning on some of its\nnodes. This is important because it implies that the independence model\nrepresented by a MAMP chain graph can be accounted for by some data generating\nprocess that is partially observed and has selection bias. Finally, we modify\nMAMP chain graphs so that they are closed under marginalization for Gaussian\nprobability distributions. This is a desirable feature because it guarantees\nparsimonious models under marginalization.\n", "versions": [{"version": "v1", "created": "Fri, 3 May 2013 15:35:50 GMT"}, {"version": "v2", "created": "Thu, 16 May 2013 13:08:01 GMT"}, {"version": "v3", "created": "Thu, 3 Oct 2013 12:16:24 GMT"}, {"version": "v4", "created": "Wed, 26 Feb 2014 15:50:00 GMT"}, {"version": "v5", "created": "Sat, 8 Mar 2014 12:42:39 GMT"}, {"version": "v6", "created": "Fri, 7 Nov 2014 10:02:12 GMT"}], "update_date": "2014-11-10", "authors_parsed": [["Pe\u00f1a", "Jose M.", ""]]}, {"id": "1305.0855", "submitter": "Xiaohui Xie", "authors": "Yifei Chen, Xiaohui Xie", "title": "Inference in Kingman's Coalescent with Particle Markov Chain Monte Carlo\n  Method", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML q-bio.PE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a new algorithm to do posterior sampling of Kingman's coalescent,\nbased upon the Particle Markov Chain Monte Carlo methodology. Specifically, the\nalgorithm is an instantiation of the Particle Gibbs Sampling method, which\nalternately samples coalescent times conditioned on coalescent tree structures,\nand tree structures conditioned on coalescent times via the conditional\nSequential Monte Carlo procedure. We implement our algorithm as a C++ package,\nand demonstrate its utility via a parameter estimation task in population\ngenetics on both single- and multiple-locus data. The experiment results show\nthat the proposed algorithm performs comparable to or better than several\nwell-developed methods.\n", "versions": [{"version": "v1", "created": "Fri, 3 May 2013 23:01:50 GMT"}], "update_date": "2013-05-07", "authors_parsed": [["Chen", "Yifei", ""], ["Xie", "Xiaohui", ""]]}, {"id": "1305.1002", "submitter": "Ji Won Yoon", "authors": "Ji Won Yoon and Nial Friel", "title": "Efficient Estimation of the number of neighbours in Probabilistic K\n  Nearest Neighbour Classification", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Probabilistic k-nearest neighbour (PKNN) classification has been introduced\nto improve the performance of original k-nearest neighbour (KNN) classification\nalgorithm by explicitly modelling uncertainty in the classification of each\nfeature vector. However, an issue common to both KNN and PKNN is to select the\noptimal number of neighbours, $k$. The contribution of this paper is to\nincorporate the uncertainty in $k$ into the decision making, and in so doing\nuse Bayesian model averaging to provide improved classification. Indeed the\nproblem of assessing the uncertainty in $k$ can be viewed as one of statistical\nmodel selection which is one of the most important technical issues in the\nstatistics and machine learning domain. In this paper, a new functional\napproximation algorithm is proposed to reconstruct the density of the model\n(order) without relying on time consuming Monte Carlo simulations. In addition,\nthis algorithm avoids cross validation by adopting Bayesian framework. The\nperformance of this algorithm yielded very good performance on several real\nexperimental datasets.\n", "versions": [{"version": "v1", "created": "Sun, 5 May 2013 09:44:08 GMT"}], "update_date": "2013-05-07", "authors_parsed": [["Yoon", "Ji Won", ""], ["Friel", "Nial", ""]]}, {"id": "1305.1027", "submitter": "Mohammad Gheshlaghi Azar", "authors": "Mohammad Gheshlaghi Azar and Alessandro Lazaric and Emma Brunskill", "title": "Regret Bounds for Reinforcement Learning with Policy Advice", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In some reinforcement learning problems an agent may be provided with a set\nof input policies, perhaps learned from prior experience or provided by\nadvisors. We present a reinforcement learning with policy advice (RLPA)\nalgorithm which leverages this input set and learns to use the best policy in\nthe set for the reinforcement learning task at hand. We prove that RLPA has a\nsub-linear regret of \\tilde O(\\sqrt{T}) relative to the best input policy, and\nthat both this regret and its computational complexity are independent of the\nsize of the state and action space. Our empirical simulations support our\ntheoretical analysis. This suggests RLPA may offer significant advantages in\nlarge domains where some prior good policies are provided.\n", "versions": [{"version": "v1", "created": "Sun, 5 May 2013 16:59:58 GMT"}, {"version": "v2", "created": "Wed, 17 Jul 2013 21:07:37 GMT"}], "update_date": "2013-07-19", "authors_parsed": [["Azar", "Mohammad Gheshlaghi", ""], ["Lazaric", "Alessandro", ""], ["Brunskill", "Emma", ""]]}, {"id": "1305.1040", "submitter": "Ting-Li Chen", "authors": "Ting-Li Chen", "title": "On the Convergence and Consistency of the Blurring Mean-Shift Process", "comments": "arXiv admin note: text overlap with arXiv:1201.1979", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The mean-shift algorithm is a popular algorithm in computer vision and image\nprocessing. It can also be cast as a minimum gamma-divergence estimation. In\nthis paper we focus on the \"blurring\" mean shift algorithm, which is one\nversion of the mean-shift process that successively blurs the dataset. The\nanalysis of the blurring mean-shift is relatively more complicated compared to\nthe nonblurring version, yet the algorithm convergence and the estimation\nconsistency have not been well studied in the literature. In this paper we\nprove both the convergence and the consistency of the blurring mean-shift. We\nalso perform simulation studies to compare the efficiency of the blurring and\nthe nonblurring versions of the mean-shift algorithms. Our results show that\nthe blurring mean-shift has more efficiency.\n", "versions": [{"version": "v1", "created": "Sun, 5 May 2013 18:51:24 GMT"}], "update_date": "2013-05-07", "authors_parsed": [["Chen", "Ting-Li", ""]]}, {"id": "1305.1396", "submitter": "Marcelo Fiori", "authors": "Mat\\'ias Di Martino, Guzman Hern\\'andez, Marcelo Fiori, Alicia\n  Fern\\'andez", "title": "A new framework for optimal classifier design", "comments": null, "journal-ref": "Pattern Recognition, Volume 46, Issue 8, August 2013, Pages\n  2249-2255", "doi": "10.1016/j.patcog.2013.01.006", "report-no": null, "categories": "cs.CV cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The use of alternative measures to evaluate classifier performance is gaining\nattention, specially for imbalanced problems. However, the use of these\nmeasures in the classifier design process is still unsolved. In this work we\npropose a classifier designed specifically to optimize one of these alternative\nmeasures, namely, the so-called F-measure. Nevertheless, the technique is\ngeneral, and it can be used to optimize other evaluation measures. An algorithm\nto train the novel classifier is proposed, and the numerical scheme is tested\nwith several databases, showing the optimality and robustness of the presented\nclassifier.\n", "versions": [{"version": "v1", "created": "Tue, 7 May 2013 04:05:24 GMT"}, {"version": "v2", "created": "Thu, 12 Sep 2013 16:09:55 GMT"}], "update_date": "2013-09-13", "authors_parsed": [["Di Martino", "Mat\u00edas", ""], ["Hern\u00e1ndez", "Guzman", ""], ["Fiori", "Marcelo", ""], ["Fern\u00e1ndez", "Alicia", ""]]}, {"id": "1305.1704", "submitter": "Lei Li", "authors": "Yusuf Erol and Lei Li and Bharath Ramsundar and Stuart J. Russell", "title": "The Extended Parameter Filter", "comments": null, "journal-ref": "ICML 2013", "doi": null, "report-no": "UCB/EECS-2013-48", "categories": "stat.ML cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The parameters of temporal models, such as dynamic Bayesian networks, may be\nmodelled in a Bayesian context as static or atemporal variables that influence\ntransition probabilities at every time step. Particle filters fail for models\nthat include such variables, while methods that use Gibbs sampling of parameter\nvariables may incur a per-sample cost that grows linearly with the length of\nthe observation sequence. Storvik devised a method for incremental computation\nof exact sufficient statistics that, for some cases, reduces the per-sample\ncost to a constant. In this paper, we demonstrate a connection between\nStorvik's filter and a Kalman filter in parameter space and establish more\ngeneral conditions under which Storvik's filter works. Drawing on an analogy to\nthe extended Kalman filter, we develop and analyze, both theoretically and\nexperimentally, a Taylor approximation to the parameter posterior that allows\nStorvik's method to be applied to a broader class of models. Our experiments on\nboth synthetic examples and real applications show improvement over existing\nmethods.\n", "versions": [{"version": "v1", "created": "Wed, 8 May 2013 03:21:31 GMT"}], "update_date": "2013-05-09", "authors_parsed": [["Erol", "Yusuf", ""], ["Li", "Lei", ""], ["Ramsundar", "Bharath", ""], ["Russell", "Stuart J.", ""]]}, {"id": "1305.1809", "submitter": "Christos Dimitrakakis", "authors": "Nikolaos Tziortziotis and Christos Dimitrakakis and Konstantinos\n  Blekas", "title": "Cover Tree Bayesian Reinforcement Learning", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper proposes an online tree-based Bayesian approach for reinforcement\nlearning. For inference, we employ a generalised context tree model. This\ndefines a distribution on multivariate Gaussian piecewise-linear models, which\ncan be updated in closed form. The tree structure itself is constructed using\nthe cover tree method, which remains efficient in high dimensional spaces. We\ncombine the model with Thompson sampling and approximate dynamic programming to\nobtain effective exploration policies in unknown environments. The flexibility\nand computational simplicity of the model render it suitable for many\nreinforcement learning problems in continuous state spaces. We demonstrate this\nin an experimental comparison with least squares policy iteration.\n", "versions": [{"version": "v1", "created": "Wed, 8 May 2013 13:11:52 GMT"}, {"version": "v2", "created": "Fri, 2 May 2014 09:44:45 GMT"}], "update_date": "2014-05-05", "authors_parsed": [["Tziortziotis", "Nikolaos", ""], ["Dimitrakakis", "Christos", ""], ["Blekas", "Konstantinos", ""]]}, {"id": "1305.1956", "submitter": "Andrew Lan", "authors": "Andrew S. Lan, Christoph Studer, Andrew E. Waters and Richard G.\n  Baraniuk", "title": "Joint Topic Modeling and Factor Analysis of Textual Information and\n  Graded Response Data", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Modern machine learning methods are critical to the development of\nlarge-scale personalized learning systems that cater directly to the needs of\nindividual learners. The recently developed SPARse Factor Analysis (SPARFA)\nframework provides a new statistical model and algorithms for machine\nlearning-based learning analytics, which estimate a learner's knowledge of the\nlatent concepts underlying a domain, and content analytics, which estimate the\nrelationships among a collection of questions and the latent concepts. SPARFA\nestimates these quantities given only the binary-valued graded responses to a\ncollection of questions. In order to better interpret the estimated latent\nconcepts, SPARFA relies on a post-processing step that utilizes user-defined\ntags (e.g., topics or keywords) available for each question. In this paper, we\nrelax the need for user-defined tags by extending SPARFA to jointly process\nboth graded learner responses and the text of each question and its associated\nanswer(s) or other feedback. Our purely data-driven approach (i) enhances the\ninterpretability of the estimated latent concepts without the need of\nexplicitly generating a set of tags or performing a post-processing step, (ii)\nimproves the prediction performance of SPARFA, and (iii) scales to large\ntest/assessments where human annotation would prove burdensome. We demonstrate\nthe efficacy of the proposed approach on two real educational datasets.\n", "versions": [{"version": "v1", "created": "Wed, 8 May 2013 20:44:55 GMT"}, {"version": "v2", "created": "Fri, 10 May 2013 01:05:09 GMT"}], "update_date": "2013-05-13", "authors_parsed": [["Lan", "Andrew S.", ""], ["Studer", "Christoph", ""], ["Waters", "Andrew E.", ""], ["Baraniuk", "Richard G.", ""]]}, {"id": "1305.1998", "submitter": "John Zech", "authors": "John Zech, Frank Wood", "title": "Inferring Team Strengths Using a Discrete Markov Random Field", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose an original model for inferring team strengths using a Markov\nRandom Field, which can be used to generate historical estimates of the\noffensive and defensive strengths of a team over time. This model was designed\nto be applied to sports such as soccer or hockey, in which contest outcomes\ntake value in a limited discrete space. We perform inference using a\ncombination of Expectation Maximization and Loopy Belief Propagation. The\nchallenges of working with a non-convex optimization problem and a\nhigh-dimensional parameter space are discussed. The performance of the model is\ndemonstrated on professional soccer data from the English Premier League.\n", "versions": [{"version": "v1", "created": "Thu, 9 May 2013 03:15:19 GMT"}], "update_date": "2013-05-10", "authors_parsed": [["Zech", "John", ""], ["Wood", "Frank", ""]]}, {"id": "1305.2038", "submitter": "Patrick Meyer E.", "authors": "Patrick E. Meyer", "title": "A Rank Minrelation - Majrelation Coefficient", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.AI", "license": "http://creativecommons.org/licenses/by-nc-sa/3.0/", "abstract": "  Improving the detection of relevant variables using a new bivariate measure\ncould importantly impact variable selection and large network inference\nmethods. In this paper, we propose a new statistical coefficient that we call\nthe rank minrelation coefficient. We define a minrelation of X to Y (or\nequivalently a majrelation of Y to X) as a measure that estimate p(Y > X) when\nX and Y are continuous random variables. The approach is similar to Lin's\nconcordance coefficient that rather focuses on estimating p(X = Y). In other\nwords, if a variable X exhibits a minrelation to Y then, as X increases, Y is\nlikely to increases too. However, on the contrary to concordance or\ncorrelation, the minrelation is not symmetric. More explicitly, if X decreases,\nlittle can be said on Y values (except that the uncertainty on Y actually\nincreases). In this paper, we formally define this new kind of bivariate\ndependencies and propose a new statistical coefficient in order to detect those\ndependencies. We show through several key examples that this new coefficient\nhas many interesting properties in order to select relevant variables, in\nparticular when compared to correlation.\n", "versions": [{"version": "v1", "created": "Thu, 9 May 2013 09:09:19 GMT"}], "update_date": "2013-05-10", "authors_parsed": [["Meyer", "Patrick E.", ""]]}, {"id": "1305.2238", "submitter": "Tuo Zhao", "authors": "Han Liu and Lie Wang and Tuo Zhao", "title": "Calibrated Multivariate Regression with Application to Neural Semantic\n  Basis Discovery", "comments": "Journal of Machine Learning Research, 2015", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a calibrated multivariate regression method named CMR for fitting\nhigh dimensional multivariate regression models. Compared with existing\nmethods, CMR calibrates regularization for each regression task with respect to\nits noise level so that it simultaneously attains improved finite-sample\nperformance and tuning insensitiveness. Theoretically, we provide sufficient\nconditions under which CMR achieves the optimal rate of convergence in\nparameter estimation. Computationally, we propose an efficient smoothed\nproximal gradient algorithm with a worst-case numerical rate of convergence\n$\\cO(1/\\epsilon)$, where $\\epsilon$ is a pre-specified accuracy of the\nobjective function value. We conduct thorough numerical simulations to\nillustrate that CMR consistently outperforms other high dimensional\nmultivariate regression methods. We also apply CMR to solve a brain activity\nprediction problem and find that it is as competitive as a handcrafted model\ncreated by human experts. The R package \\texttt{camel} implementing the\nproposed method is available on the Comprehensive R Archive Network\n\\url{http://cran.r-project.org/web/packages/camel/}.\n", "versions": [{"version": "v1", "created": "Fri, 10 May 2013 01:08:36 GMT"}, {"version": "v2", "created": "Thu, 28 Jul 2016 05:05:18 GMT"}], "update_date": "2017-06-06", "authors_parsed": [["Liu", "Han", ""], ["Wang", "Lie", ""], ["Zhao", "Tuo", ""]]}, {"id": "1305.2362", "submitter": "David Wipf", "authors": "David Wipf and Haichao Zhang", "title": "Revisiting Bayesian Blind Deconvolution", "comments": "This paper has been submitted to JMLR. A conference version will\n  appear at EMMCVPR 2013", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Blind deconvolution involves the estimation of a sharp signal or image given\nonly a blurry observation. Because this problem is fundamentally ill-posed,\nstrong priors on both the sharp image and blur kernel are required to\nregularize the solution space. While this naturally leads to a standard MAP\nestimation framework, performance is compromised by unknown trade-off parameter\nsettings, optimization heuristics, and convergence issues stemming from\nnon-convexity and/or poor prior selections. To mitigate some of these problems,\na number of authors have recently proposed substituting a variational Bayesian\n(VB) strategy that marginalizes over the high-dimensional image space leading\nto better estimates of the blur kernel. However, the underlying cost function\nnow involves both integrals with no closed-form solution and complex,\nfunction-valued arguments, thus losing the transparency of MAP. Beyond standard\nBayesian-inspired intuitions, it thus remains unclear by exactly what mechanism\nthese methods are able to operate, rendering understanding, improvements and\nextensions more difficult. To elucidate these issues, we demonstrate that the\nVB methodology can be recast as an unconventional MAP problem with a very\nparticular penalty/prior that couples the image, blur kernel, and noise level\nin a principled way. This unique penalty has a number of useful characteristics\npertaining to relative concavity, local minima avoidance, and scale-invariance\nthat allow us to rigorously explain the success of VB including its existing\nimplementational heuristics and approximations. It also provides strict\ncriteria for choosing the optimal image prior that, perhaps\ncounter-intuitively, need not reflect the statistics of natural scenes. In so\ndoing we challenge the prevailing notion of why VB is successful for blind\ndeconvolution while providing a transparent platform for introducing\nenhancements.\n", "versions": [{"version": "v1", "created": "Fri, 10 May 2013 15:09:11 GMT"}], "update_date": "2013-05-13", "authors_parsed": [["Wipf", "David", ""], ["Zhang", "Haichao", ""]]}, {"id": "1305.2436", "submitter": "Po-Ling Loh", "authors": "Po-Ling Loh, Martin J. Wainwright", "title": "Regularized M-estimators with nonconvexity: Statistical and algorithmic\n  theory for local optima", "comments": "58 pages, 13 figures. To appear in JMLR", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST cs.IT math.IT stat.ML stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We provide novel theoretical results regarding local optima of regularized\n$M$-estimators, allowing for nonconvexity in both loss and penalty functions.\nUnder restricted strong convexity on the loss and suitable regularity\nconditions on the penalty, we prove that \\emph{any stationary point} of the\ncomposite objective function will lie within statistical precision of the\nunderlying parameter vector. Our theory covers many nonconvex objective\nfunctions of interest, including the corrected Lasso for errors-in-variables\nlinear models; regression for generalized linear models with nonconvex\npenalties such as SCAD, MCP, and capped-$\\ell_1$; and high-dimensional\ngraphical model estimation. We quantify statistical accuracy by providing\nbounds on the $\\ell_1$-, $\\ell_2$-, and prediction error between stationary\npoints and the population-level optimum. We also propose a simple modification\nof composite gradient descent that may be used to obtain a near-global optimum\nwithin statistical precision $\\epsilon$ in $\\log(1/\\epsilon)$ steps, which is\nthe fastest possible rate of any first-order method. We provide simulation\nstudies illustrating the sharpness of our theoretical results.\n", "versions": [{"version": "v1", "created": "Fri, 10 May 2013 20:46:41 GMT"}, {"version": "v2", "created": "Thu, 1 Jan 2015 21:24:26 GMT"}], "update_date": "2015-01-05", "authors_parsed": [["Loh", "Po-Ling", ""], ["Wainwright", "Martin J.", ""]]}, {"id": "1305.2473", "submitter": "Takafumi Kanamori Dr.", "authors": "Takafumi Kanamori and Hironori Fujisawa", "title": "Affine Invariant Divergences associated with Composite Scores and its\n  Applications", "comments": "24 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.ML stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In statistical analysis, measuring a score of predictive performance is an\nimportant task. In many scientific fields, appropriate scores were tailored to\ntackle the problems at hand. A proper score is a popular tool to obtain\nstatistically consistent forecasts. Furthermore, a mathematical\ncharacterization of the proper score was studied. As a result, it was revealed\nthat the proper score corresponds to a Bregman divergence, which is an\nextension of the squared distance over the set of probability distributions. In\nthe present paper, we introduce composite scores as an extension of the typical\nscores in order to obtain a wider class of probabilistic forecasting. Then, we\npropose a class of composite scores, named Holder scores, that induce\nequivariant estimators. The equivariant estimators have a favorable property,\nimplying that the estimator is transformed in a consistent way, when the data\nis transformed. In particular, we deal with the affine transformation of the\ndata. By using the equivariant estimators under the affine transformation, one\ncan obtain estimators that do no essentially depend on the choice of the system\nof units in the measurement. Conversely, we prove that the Holder score is\ncharacterized by the invariance property under the affine transformations.\nFurthermore, we investigate statistical properties of the estimators using\nHolder scores for the statistical problems including estimation of regression\nfunctions and robust parameter estimation, and illustrate the usefulness of the\nnewly introduced scores for statistical forecasting.\n", "versions": [{"version": "v1", "created": "Sat, 11 May 2013 05:23:20 GMT"}], "update_date": "2013-05-14", "authors_parsed": [["Kanamori", "Takafumi", ""], ["Fujisawa", "Hironori", ""]]}, {"id": "1305.2505", "submitter": "Purushottam Kar", "authors": "Purushottam Kar, Bharath K Sriperumbudur, Prateek Jain and Harish C\n  Karnick", "title": "On the Generalization Ability of Online Learning Algorithms for Pairwise\n  Loss Functions", "comments": "To appear in proceedings of the 30th International Conference on\n  Machine Learning (ICML 2013)", "journal-ref": "Journal of Machine Learning Research, W&CP 28(3) (2013)", "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we study the generalization properties of online learning\nbased stochastic methods for supervised learning problems where the loss\nfunction is dependent on more than one training sample (e.g., metric learning,\nranking). We present a generic decoupling technique that enables us to provide\nRademacher complexity-based generalization error bounds. Our bounds are in\ngeneral tighter than those obtained by Wang et al (COLT 2012) for the same\nproblem. Using our decoupling technique, we are further able to obtain fast\nconvergence rates for strongly convex pairwise loss functions. We are also able\nto analyze a class of memory efficient online learning algorithms for pairwise\nlearning problems that use only a bounded subset of past training samples to\nupdate the hypothesis at each step. Finally, in order to complement our\ngeneralization bounds, we propose a novel memory efficient online learning\nalgorithm for higher order learning problems with bounded regret guarantees.\n", "versions": [{"version": "v1", "created": "Sat, 11 May 2013 13:52:37 GMT"}], "update_date": "2013-05-14", "authors_parsed": [["Kar", "Purushottam", ""], ["Sriperumbudur", "Bharath K", ""], ["Jain", "Prateek", ""], ["Karnick", "Harish C", ""]]}, {"id": "1305.2524", "submitter": "Rina Foygel", "authors": "Rina Foygel and Lester Mackey", "title": "Corrupted Sensing: Novel Guarantees for Separating Structured Signals", "comments": "http://ieeexplore.ieee.org/xpl/articleDetails.jsp?arnumber=6712045", "journal-ref": "IEEE Transactions on Information Theory, 60(2): 1223-1247 (2014)", "doi": "10.1109/TIT.2013.2293654", "report-no": null, "categories": "cs.IT math.IT math.OC stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study the problem of corrupted sensing, a generalization of compressed\nsensing in which one aims to recover a signal from a collection of corrupted or\nunreliable measurements. While an arbitrary signal cannot be recovered in the\nface of arbitrary corruption, tractable recovery is possible when both signal\nand corruption are suitably structured. We quantify the relationship between\nsignal recovery and two geometric measures of structure, the Gaussian\ncomplexity of a tangent cone and the Gaussian distance to a subdifferential. We\ntake a convex programming approach to disentangling signal and corruption,\nanalyzing both penalized programs that trade off between signal and corruption\ncomplexity, and constrained programs that bound the complexity of signal or\ncorruption when prior information is available. In each case, we provide\nconditions for exact signal recovery from structured corruption and stable\nsignal recovery from structured corruption with added unstructured noise. Our\nsimulations demonstrate close agreement between our theoretical recovery bounds\nand the sharp phase transitions observed in practice. In addition, we provide\nnew interpretable bounds for the Gaussian complexity of sparse vectors,\nblock-sparse vectors, and low-rank matrices, which lead to sharper guarantees\nof recovery when combined with our results and those in the literature.\n", "versions": [{"version": "v1", "created": "Sat, 11 May 2013 16:01:25 GMT"}, {"version": "v2", "created": "Tue, 4 Feb 2014 17:43:00 GMT"}], "update_date": "2014-02-05", "authors_parsed": [["Foygel", "Rina", ""], ["Mackey", "Lester", ""]]}, {"id": "1305.2532", "submitter": "Stephane Ross", "authors": "Stephane Ross, Jiaji Zhou, Yisong Yue, Debadeepta Dey, J. Andrew\n  Bagnell", "title": "Learning Policies for Contextual Submodular Prediction", "comments": "13 pages. To appear in proceedings of the International Conference on\n  Machine Learning (ICML), 2013", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Many prediction domains, such as ad placement, recommendation, trajectory\nprediction, and document summarization, require predicting a set or list of\noptions. Such lists are often evaluated using submodular reward functions that\nmeasure both quality and diversity. We propose a simple, efficient, and\nprovably near-optimal approach to optimizing such prediction problems based on\nno-regret learning. Our method leverages a surprising result from online\nsubmodular optimization: a single no-regret online learner can compete with an\noptimal sequence of predictions. Compared to previous work, which either learn\na sequence of classifiers or rely on stronger assumptions such as\nrealizability, we ensure both data-efficiency as well as performance guarantees\nin the fully agnostic setting. Experiments validate the efficiency and\napplicability of the approach on a wide range of problems including manipulator\ntrajectory optimization, news recommendation and document summarization.\n", "versions": [{"version": "v1", "created": "Sat, 11 May 2013 18:09:52 GMT"}], "update_date": "2013-05-14", "authors_parsed": [["Ross", "Stephane", ""], ["Zhou", "Jiaji", ""], ["Yue", "Yisong", ""], ["Dey", "Debadeepta", ""], ["Bagnell", "J. Andrew", ""]]}, {"id": "1305.2581", "submitter": "Tong Zhang", "authors": "Shai Shalev-Shwartz and Tong Zhang", "title": "Accelerated Mini-Batch Stochastic Dual Coordinate Ascent", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Stochastic dual coordinate ascent (SDCA) is an effective technique for\nsolving regularized loss minimization problems in machine learning. This paper\nconsiders an extension of SDCA under the mini-batch setting that is often used\nin practice. Our main contribution is to introduce an accelerated mini-batch\nversion of SDCA and prove a fast convergence rate for this method. We discuss\nan implementation of our method over a parallel computing system, and compare\nthe results to both the vanilla stochastic dual coordinate ascent and to the\naccelerated deterministic gradient descent method of\n\\cite{nesterov2007gradient}.\n", "versions": [{"version": "v1", "created": "Sun, 12 May 2013 12:46:25 GMT"}], "update_date": "2013-05-14", "authors_parsed": [["Shalev-Shwartz", "Shai", ""], ["Zhang", "Tong", ""]]}, {"id": "1305.2648", "submitter": "Matus Telgarsky", "authors": "Matus Telgarsky", "title": "Boosting with the Logistic Loss is Consistent", "comments": "To appear, COLT 2013", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This manuscript provides optimization guarantees, generalization bounds, and\nstatistical consistency results for AdaBoost variants which replace the\nexponential loss with the logistic and similar losses (specifically, twice\ndifferentiable convex losses which are Lipschitz and tend to zero on one side).\n  The heart of the analysis is to show that, in lieu of explicit regularization\nand constraints, the structure of the problem is fairly rigidly controlled by\nthe source distribution itself. The first control of this type is in the\nseparable case, where a distribution-dependent relaxed weak learning rate\ninduces speedy convergence with high probability over any sample. Otherwise, in\nthe nonseparable case, the convex surrogate risk itself exhibits\ndistribution-dependent levels of curvature, and consequently the algorithm's\noutput has small norm with high probability.\n", "versions": [{"version": "v1", "created": "Mon, 13 May 2013 00:15:14 GMT"}], "update_date": "2013-05-14", "authors_parsed": [["Telgarsky", "Matus", ""]]}, {"id": "1305.2667", "submitter": "Jan Luts", "authors": "Jan Luts, John T. Ormerod", "title": "Mean field variational Bayesian inference for support vector machine\n  classification", "comments": "18 pages, 4 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A mean field variational Bayes approach to support vector machines (SVMs)\nusing the latent variable representation on Polson & Scott (2012) is presented.\nThis representation allows circumvention of many of the shortcomings associated\nwith classical SVMs including automatic penalty parameter selection, the\nability to handle dependent samples, missing data and variable selection. We\ndemonstrate on simulated and real datasets that our approach is easily\nextendable to non-standard situations and outperforms the classical SVM\napproach whilst remaining computationally efficient.\n", "versions": [{"version": "v1", "created": "Mon, 13 May 2013 03:31:15 GMT"}], "update_date": "2013-05-14", "authors_parsed": [["Luts", "Jan", ""], ["Ormerod", "John T.", ""]]}, {"id": "1305.3120", "submitter": "Julien Mairal", "authors": "Julien Mairal (INRIA Grenoble Rh\\^one-Alpes / LJK Laboratoire Jean\n  Kuntzmann)", "title": "Optimization with First-Order Surrogate Functions", "comments": "to appear in the proceedings of ICML 2013; the arxiv paper contains\n  the 9 pages main text followed by 26 pages of supplemental material.\n  International Conference on Machine Learning (ICML 2013) (2013)", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG math.OC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we study optimization methods consisting of iteratively\nminimizing surrogates of an objective function. By proposing several\nalgorithmic variants and simple convergence analyses, we make two main\ncontributions. First, we provide a unified viewpoint for several first-order\noptimization techniques such as accelerated proximal gradient, block coordinate\ndescent, or Frank-Wolfe algorithms. Second, we introduce a new incremental\nscheme that experimentally matches or outperforms state-of-the-art solvers for\nlarge-scale optimization problems typically arising in machine learning.\n", "versions": [{"version": "v1", "created": "Tue, 14 May 2013 11:49:34 GMT"}], "update_date": "2013-05-15", "authors_parsed": [["Mairal", "Julien", "", "INRIA Grenoble Rh\u00f4ne-Alpes / LJK Laboratoire Jean\n  Kuntzmann"]]}, {"id": "1305.3207", "submitter": "Ilias Diakonikolas", "authors": "Siu-On Chan, Ilias Diakonikolas, Rocco A. Servedio, Xiaorui Sun", "title": "Efficient Density Estimation via Piecewise Polynomial Approximation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.DS stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We give a highly efficient \"semi-agnostic\" algorithm for learning univariate\nprobability distributions that are well approximated by piecewise polynomial\ndensity functions. Let $p$ be an arbitrary distribution over an interval $I$\nwhich is $\\tau$-close (in total variation distance) to an unknown probability\ndistribution $q$ that is defined by an unknown partition of $I$ into $t$\nintervals and $t$ unknown degree-$d$ polynomials specifying $q$ over each of\nthe intervals. We give an algorithm that draws $\\tilde{O}(t\\new{(d+1)}/\\eps^2)$\nsamples from $p$, runs in time $\\poly(t,d,1/\\eps)$, and with high probability\noutputs a piecewise polynomial hypothesis distribution $h$ that is\n$(O(\\tau)+\\eps)$-close (in total variation distance) to $p$. This sample\ncomplexity is essentially optimal; we show that even for $\\tau=0$, any\nalgorithm that learns an unknown $t$-piecewise degree-$d$ probability\ndistribution over $I$ to accuracy $\\eps$ must use $\\Omega({\\frac {t(d+1)}\n{\\poly(1 + \\log(d+1))}} \\cdot {\\frac 1 {\\eps^2}})$ samples from the\ndistribution, regardless of its running time. Our algorithm combines tools from\napproximation theory, uniform convergence, linear programming, and dynamic\nprogramming.\n  We apply this general algorithm to obtain a wide range of results for many\nnatural problems in density estimation over both continuous and discrete\ndomains. These include state-of-the-art results for learning mixtures of\nlog-concave distributions; mixtures of $t$-modal distributions; mixtures of\nMonotone Hazard Rate distributions; mixtures of Poisson Binomial Distributions;\nmixtures of Gaussians; and mixtures of $k$-monotone densities. Our general\ntechnique yields computationally efficient algorithms for all these problems,\nin many cases with provably optimal sample complexities (up to logarithmic\nfactors) in all parameters.\n", "versions": [{"version": "v1", "created": "Tue, 14 May 2013 16:54:10 GMT"}], "update_date": "2013-05-15", "authors_parsed": [["Chan", "Siu-On", ""], ["Diakonikolas", "Ilias", ""], ["Servedio", "Rocco A.", ""], ["Sun", "Xiaorui", ""]]}, {"id": "1305.3334", "submitter": "Cem Tekin", "authors": "Cem Tekin and Mingyan Liu", "title": "Online Learning in a Contract Selection Problem", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.GT math.OC stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In an online contract selection problem there is a seller which offers a set\nof contracts to sequentially arriving buyers whose types are drawn from an\nunknown distribution. If there exists a profitable contract for the buyer in\nthe offered set, i.e., a contract with payoff higher than the payoff of not\naccepting any contracts, the buyer chooses the contract that maximizes its\npayoff. In this paper we consider the online contract selection problem to\nmaximize the sellers profit. Assuming that a structural property called ordered\npreferences holds for the buyer's payoff function, we propose online learning\nalgorithms that have sub-linear regret with respect to the best set of\ncontracts given the distribution over the buyer's type. This problem has many\napplications including spectrum contracts, wireless service provider data plans\nand recommendation systems.\n", "versions": [{"version": "v1", "created": "Wed, 15 May 2013 01:22:34 GMT"}], "update_date": "2013-05-16", "authors_parsed": [["Tekin", "Cem", ""], ["Liu", "Mingyan", ""]]}, {"id": "1305.3486", "submitter": "Reinhard Heckel", "authors": "Reinhard Heckel and Helmut B\\\"olcskei", "title": "Noisy Subspace Clustering via Thresholding", "comments": "Presented at the IEEE Int. Symp. Inf. Theory (ISIT) 2013, Istanbul,\n  Turkey. The version posted here corrects a minor error in the published\n  version. Specifically, the exponent -c n_l in the success probability of\n  Theorem 1 and in the corresponding proof outline has been corrected to\n  -c(n_l-1)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IT cs.LG math.IT math.ST stat.ML stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider the problem of clustering noisy high-dimensional data points into\na union of low-dimensional subspaces and a set of outliers. The number of\nsubspaces, their dimensions, and their orientations are unknown. A\nprobabilistic performance analysis of the thresholding-based subspace\nclustering (TSC) algorithm introduced recently in [1] shows that TSC succeeds\nin the noisy case, even when the subspaces intersect. Our results reveal an\nexplicit tradeoff between the allowed noise level and the affinity of the\nsubspaces. We furthermore find that the simple outlier detection scheme\nintroduced in [1] provably succeeds in the noisy case.\n", "versions": [{"version": "v1", "created": "Wed, 15 May 2013 14:12:50 GMT"}, {"version": "v2", "created": "Thu, 18 Jul 2013 11:04:58 GMT"}], "update_date": "2013-07-19", "authors_parsed": [["Heckel", "Reinhard", ""], ["B\u00f6lcskei", "Helmut", ""]]}, {"id": "1305.3616", "submitter": "Manuel Gomez Rodriguez", "authors": "Manuel Gomez Rodriguez, Jure Leskovec, Bernhard Schoelkopf", "title": "Modeling Information Propagation with Survival Theory", "comments": "To appear at ICML '13", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.SI cs.DS physics.soc-ph stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Networks provide a skeleton for the spread of contagions, like, information,\nideas, behaviors and diseases. Many times networks over which contagions\ndiffuse are unobserved and need to be inferred. Here we apply survival theory\nto develop general additive and multiplicative risk models under which the\nnetwork inference problems can be solved efficiently by exploiting their\nconvexity. Our additive risk model generalizes several existing network\ninference models. We show all these models are particular cases of our more\ngeneral model. Our multiplicative model allows for modeling scenarios in which\na node can either increase or decrease the risk of activation of another node,\nin contrast with previous approaches, which consider only positive risk\nincrements. We evaluate the performance of our network inference algorithms on\nlarge synthetic and real cascade datasets, and show that our models are able to\npredict the length and duration of cascades in real data.\n", "versions": [{"version": "v1", "created": "Wed, 15 May 2013 20:01:06 GMT"}], "update_date": "2013-05-17", "authors_parsed": [["Rodriguez", "Manuel Gomez", ""], ["Leskovec", "Jure", ""], ["Schoelkopf", "Bernhard", ""]]}, {"id": "1305.3640", "submitter": "Jan-Willem van de Meent", "authors": "Jan-Willem van de Meent, Jonathan E. Bronson, Frank Wood, Ruben L.\n  Gonzalez Jr., Chris H. Wiggins", "title": "Hierarchically-coupled hidden Markov models for learning kinetic rates\n  from single-molecule data", "comments": "9 pages, 5 figures", "journal-ref": "International Conference on Machine Learning 2013", "doi": null, "report-no": null, "categories": "stat.ML physics.bio-ph q-bio.QM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We address the problem of analyzing sets of noisy time-varying signals that\nall report on the same process but confound straightforward analyses due to\ncomplex inter-signal heterogeneities and measurement artifacts. In particular\nwe consider single-molecule experiments which indirectly measure the distinct\nsteps in a biomolecular process via observations of noisy time-dependent\nsignals such as a fluorescence intensity or bead position. Straightforward\nhidden Markov model (HMM) analyses attempt to characterize such processes in\nterms of a set of conformational states, the transitions that can occur between\nthese states, and the associated rates at which those transitions occur; but\nrequire ad-hoc post-processing steps to combine multiple signals. Here we\ndevelop a hierarchically coupled HMM that allows experimentalists to deal with\ninter-signal variability in a principled and automatic way. Our approach is a\ngeneralized expectation maximization hyperparameter point estimation procedure\nwith variational Bayes at the level of individual time series that learns an\nsingle interpretable representation of the overall data generating process.\n", "versions": [{"version": "v1", "created": "Wed, 15 May 2013 21:19:08 GMT"}], "update_date": "2013-05-17", "authors_parsed": [["van de Meent", "Jan-Willem", ""], ["Bronson", "Jonathan E.", ""], ["Wood", "Frank", ""], ["Gonzalez", "Ruben L.", "Jr."], ["Wiggins", "Chris H.", ""]]}, {"id": "1305.3794", "submitter": "Gabriel Kronberger", "authors": "Gabriel Kronberger and Michael Kommenda", "title": "Evolution of Covariance Functions for Gaussian Process Regression using\n  Genetic Programming", "comments": "Presented at the Workshop \"Theory and Applications of Metaheuristic\n  Algorithms\", EUROCAST2013. To appear in selected papers of Computer Aided\n  Systems Theory - EUROCAST 2013; Volumes Editors: Roberto Moreno-D\\'iaz, Franz\n  R. Pichler, Alexis Quesada-Arencibia; LNCS Springer", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NE cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this contribution we describe an approach to evolve composite covariance\nfunctions for Gaussian processes using genetic programming. A critical aspect\nof Gaussian processes and similar kernel-based models such as SVM is, that the\ncovariance function should be adapted to the modeled data. Frequently, the\nsquared exponential covariance function is used as a default. However, this can\nlead to a misspecified model, which does not fit the data well. In the proposed\napproach we use a grammar for the composition of covariance functions and\ngenetic programming to search over the space of sentences that can be derived\nfrom the grammar. We tested the proposed approach on synthetic data from\ntwo-dimensional test functions, and on the Mauna Loa CO2 time series. The\nresults show, that our approach is feasible, finding covariance functions that\nperform much better than a default covariance function. For the CO2 data set a\ncomposite covariance function is found, that matches the performance of a\nhand-tuned covariance function.\n", "versions": [{"version": "v1", "created": "Thu, 16 May 2013 13:25:20 GMT"}, {"version": "v2", "created": "Wed, 22 May 2013 09:28:04 GMT"}], "update_date": "2013-05-23", "authors_parsed": [["Kronberger", "Gabriel", ""], ["Kommenda", "Michael", ""]]}, {"id": "1305.4152", "submitter": "Botond Cseke", "authors": "Botond Cseke, Andrew Zammit Mangion, Tom Heskes, and Guido Sanguinetti", "title": "Sparse Approximate Inference for Spatio-Temporal Point Process Models", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Spatio-temporal point process models play a central role in the analysis of\nspatially distributed systems in several disciplines. Yet, scalable inference\nremains computa- tionally challenging both due to the high resolution modelling\ngenerally required and the analytically intractable likelihood function. Here,\nwe exploit the sparsity structure typical of (spatially) discretised\nlog-Gaussian Cox process models by using approximate message-passing\nalgorithms. The proposed algorithms scale well with the state dimension and the\nlength of the temporal horizon with moderate loss in distributional accuracy.\nThey hence provide a flexible and faster alternative to both non-linear\nfiltering-smoothing type algorithms and to approaches that implement the\nLaplace method or expectation propagation on (block) sparse latent Gaussian\nmodels. We infer the parameters of the latent Gaussian model using a structured\nvariational Bayes approach. We demonstrate the proposed framework on simulation\nstudies with both Gaussian and point-process observations and use it to\nreconstruct the conflict intensity and dynamics in Afghanistan from the\nWikiLeaks Afghan War Diary.\n", "versions": [{"version": "v1", "created": "Fri, 17 May 2013 18:40:53 GMT"}, {"version": "v2", "created": "Tue, 21 May 2013 18:40:08 GMT"}, {"version": "v3", "created": "Thu, 15 Aug 2013 13:35:42 GMT"}, {"version": "v4", "created": "Sat, 9 Aug 2014 14:03:39 GMT"}, {"version": "v5", "created": "Mon, 6 Jul 2015 14:21:58 GMT"}], "update_date": "2015-07-07", "authors_parsed": [["Cseke", "Botond", ""], ["Mangion", "Andrew Zammit", ""], ["Heskes", "Tom", ""], ["Sanguinetti", "Guido", ""]]}, {"id": "1305.4153", "submitter": "Botond Cseke", "authors": "Botond Cseke and Guido Sanguinetti", "title": "Factored expectation propagation for input-output FHMM models in systems\n  biology", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider the problem of joint modelling of metabolic signals and gene\nexpression in systems biology applications. We propose an approach based on\ninput-output factorial hidden Markov models and propose a structured\nvariational inference approach to infer the structure and states of the model.\nWe start from the classical free form structured variational mean field\napproach and use a expectation propagation to approximate the expectations\nneeded in the variational loop. We show that this corresponds to a factored\nexpectation constrained approximate inference. We validate our model through\nextensive simulations and demonstrate its applicability on a real world\nbacterial data set.\n", "versions": [{"version": "v1", "created": "Fri, 17 May 2013 18:44:50 GMT"}], "update_date": "2013-05-20", "authors_parsed": [["Cseke", "Botond", ""], ["Sanguinetti", "Guido", ""]]}, {"id": "1305.4232", "submitter": "Hau-tieng Wu", "authors": "Hau-tieng Wu", "title": "Embedding Riemannian Manifolds by the Heat Kernel of the Connection\n  Laplacian", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.DG math.SP math.ST stat.ML stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Given a class of closed Riemannian manifolds with prescribed geometric\nconditions, we introduce an embedding of the manifolds into $\\ell^2$ based on\nthe heat kernel of the Connection Laplacian associated with the Levi-Civita\nconnection on the tangent bundle. As a result, we can construct a distance in\nthis class which leads to a pre-compactness theorem on the class under\nconsideration.\n", "versions": [{"version": "v1", "created": "Sat, 18 May 2013 06:17:39 GMT"}, {"version": "v2", "created": "Wed, 13 Sep 2017 00:01:37 GMT"}], "update_date": "2017-09-14", "authors_parsed": [["Wu", "Hau-tieng", ""]]}, {"id": "1305.4268", "submitter": "Jos\\'e Miguel Hern\\'andez-Lobato", "authors": "Yue Wu, Jos\\'e Miguel Hern\\'andez-Lobato and Zoubin Ghahramani", "title": "Dynamic Covariance Models for Multivariate Financial Time Series", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The accurate prediction of time-changing covariances is an important problem\nin the modeling of multivariate financial data. However, some of the most\npopular models suffer from a) overfitting problems and multiple local optima,\nb) failure to capture shifts in market conditions and c) large computational\ncosts. To address these problems we introduce a novel dynamic model for\ntime-changing covariances. Over-fitting and local optima are avoided by\nfollowing a Bayesian approach instead of computing point estimates. Changes in\nmarket conditions are captured by assuming a diffusion process in parameter\nvalues, and finally computationally efficient and scalable inference is\nperformed using particle filters. Experiments with financial data show\nexcellent performance of the proposed method with respect to current standard\nmodels.\n", "versions": [{"version": "v1", "created": "Sat, 18 May 2013 14:08:12 GMT"}, {"version": "v2", "created": "Sun, 2 Jun 2013 18:29:10 GMT"}], "update_date": "2013-06-04", "authors_parsed": [["Wu", "Yue", ""], ["Hern\u00e1ndez-Lobato", "Jos\u00e9 Miguel", ""], ["Ghahramani", "Zoubin", ""]]}, {"id": "1305.4324", "submitter": "Fares Hedayati Fares Hedayati", "authors": "Peter Bartlett, Peter Grunwald, Peter Harremoes, Fares Hedayati,\n  Wojciech Kotlowski", "title": "Horizon-Independent Optimal Prediction with Log-Loss in Exponential\n  Families", "comments": "23 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study online learning under logarithmic loss with regular parametric\nmodels. Hedayati and Bartlett (2012b) showed that a Bayesian prediction\nstrategy with Jeffreys prior and sequential normalized maximum likelihood\n(SNML) coincide and are optimal if and only if the latter is exchangeable, and\nif and only if the optimal strategy can be calculated without knowing the time\nhorizon in advance. They put forward the question what families have\nexchangeable SNML strategies. This paper fully answers this open problem for\none-dimensional exponential families. The exchangeability can happen only for\nthree classes of natural exponential family distributions, namely the Gaussian,\nGamma, and the Tweedie exponential family of order 3/2. Keywords: SNML\nExchangeability, Exponential Family, Online Learning, Logarithmic Loss,\nBayesian Strategy, Jeffreys Prior, Fisher Information1\n", "versions": [{"version": "v1", "created": "Sun, 19 May 2013 04:56:05 GMT"}], "update_date": "2013-05-21", "authors_parsed": [["Bartlett", "Peter", ""], ["Grunwald", "Peter", ""], ["Harremoes", "Peter", ""], ["Hedayati", "Fares", ""], ["Kotlowski", "Wojciech", ""]]}, {"id": "1305.4325", "submitter": "Shu Tanaka", "authors": "Issei Sato, Shu Tanaka, Kenichi Kurihara, Seiji Miyashita, and Hiroshi\n  Nakagawa", "title": "Quantum Annealing for Dirichlet Process Mixture Models with Applications\n  to Network Clustering", "comments": "12 pages, 6 figures, accepted in Neurocomputing", "journal-ref": "Neurocomputing, Vol. 121, 523 (2013)", "doi": "10.1016/j.neucom.2013.05.019", "report-no": null, "categories": "cond-mat.dis-nn cond-mat.stat-mech quant-ph stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We developed a new quantum annealing (QA) algorithm for Dirichlet process\nmixture (DPM) models based on the Chinese restaurant process (CRP). QA is a\nparallelized extension of simulated annealing (SA), i.e., it is a parallel\nstochastic optimization technique. Existing approaches [Kurihara et al.\nUAI2009, Sato et al. UAI2009] and cannot be applied to the CRP because their QA\nframework is formulated using a fixed number of mixture components. The\nproposed QA algorithm can handle an unfixed number of classes in mixture\nmodels. We applied QA to a DPM model for clustering vertices in a network where\na CRP seating arrangement indicates a network partition. A multi core processor\nwas used for running QA in experiments, the results of which show that QA is\nbetter than SA, Markov chain Monte Carlo inference, and beam search at finding\na maximum a posteriori estimation of a seating arrangement in the CRP. Since\nour QA algorithm is as easy as to implement the SA algorithm, it is suitable\nfor a wide range of applications.\n", "versions": [{"version": "v1", "created": "Sun, 19 May 2013 05:54:10 GMT"}], "update_date": "2013-09-10", "authors_parsed": [["Sato", "Issei", ""], ["Tanaka", "Shu", ""], ["Kurihara", "Kenichi", ""], ["Miyashita", "Seiji", ""], ["Nakagawa", "Hiroshi", ""]]}, {"id": "1305.4433", "submitter": "Xiangnan Kong", "authors": "Xiangnan Kong, Bokai Cao, Philip S. Yu, Ying Ding and David J. Wild", "title": "Meta Path-Based Collective Classification in Heterogeneous Information\n  Networks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Collective classification has been intensively studied due to its impact in\nmany important applications, such as web mining, bioinformatics and citation\nanalysis. Collective classification approaches exploit the dependencies of a\ngroup of linked objects whose class labels are correlated and need to be\npredicted simultaneously. In this paper, we focus on studying the collective\nclassification problem in heterogeneous networks, which involves multiple types\nof data objects interconnected by multiple types of links. Intuitively, two\nobjects are correlated if they are linked by many paths in the network.\nHowever, most existing approaches measure the dependencies among objects\nthrough directly links or indirect links without considering the different\nsemantic meanings behind different paths. In this paper, we study the\ncollective classification problem taht is defined among the same type of\nobjects in heterogenous networks. Moreover, by considering different linkage\npaths in the network, one can capture the subtlety of different types of\ndependencies among objects. We introduce the concept of meta-path based\ndependencies among objects, where a meta path is a path consisting a certain\nsequence of linke types. We show that the quality of collective classification\nresults strongly depends upon the meta paths used. To accommodate the large\nnetwork size, a novel solution, called HCC (meta-path based Heterogenous\nCollective Classification), is developed to effectively assign labels to a\ngroup of instances that are interconnected through different meta-paths. The\nproposed HCC model can capture different types of dependencies among objects\nwith respect to different meta paths. Empirical studies on real-world networks\ndemonstrate that effectiveness of the proposed meta path-based collective\nclassification approach.\n", "versions": [{"version": "v1", "created": "Mon, 20 May 2013 04:05:23 GMT"}], "update_date": "2013-05-21", "authors_parsed": [["Kong", "Xiangnan", ""], ["Cao", "Bokai", ""], ["Yu", "Philip S.", ""], ["Ding", "Ying", ""], ["Wild", "David J.", ""]]}, {"id": "1305.4723", "submitter": "Zhaosong Lu", "authors": "Zhaosong Lu and Lin Xiao", "title": "On the Complexity Analysis of Randomized Block-Coordinate Descent\n  Methods", "comments": "26 pages (submitted)", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.OC cs.LG cs.NA math.NA stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper we analyze the randomized block-coordinate descent (RBCD)\nmethods proposed in [8,11] for minimizing the sum of a smooth convex function\nand a block-separable convex function. In particular, we extend Nesterov's\ntechnique developed in [8] for analyzing the RBCD method for minimizing a\nsmooth convex function over a block-separable closed convex set to the\naforementioned more general problem and obtain a sharper expected-value type of\nconvergence rate than the one implied in [11]. Also, we obtain a better\nhigh-probability type of iteration complexity, which improves upon the one in\n[11] by at least the amount $O(n/\\epsilon)$, where $\\epsilon$ is the target\nsolution accuracy and $n$ is the number of problem blocks. In addition, for\nunconstrained smooth convex minimization, we develop a new technique called\n{\\it randomized estimate sequence} to analyze the accelerated RBCD method\nproposed by Nesterov [11] and establish a sharper expected-value type of\nconvergence rate than the one given in [11].\n", "versions": [{"version": "v1", "created": "Tue, 21 May 2013 06:12:42 GMT"}], "update_date": "2013-05-22", "authors_parsed": [["Lu", "Zhaosong", ""], ["Xiao", "Lin", ""]]}, {"id": "1305.4893", "submitter": "Minh Tang", "authors": "Minh Tang and Youngser Park and Carey E. Priebe", "title": "Out-of-sample Extension for Latent Position Graphs", "comments": "26 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider the problem of vertex classification for graphs constructed from\nthe latent position model. It was shown previously that the approach of\nembedding the graphs into some Euclidean space followed by classification in\nthat space can yields a universally consistent vertex classifier. However, a\nmajor technical difficulty of the approach arises when classifying unlabeled\nout-of-sample vertices without including them in the embedding stage. In this\npaper, we studied the out-of-sample extension for the graph embedding step and\nits impact on the subsequent inference tasks. We show that, under the latent\nposition graph model and for sufficiently large $n$, the mapping of the\nout-of-sample vertices is close to its true latent position. We then\ndemonstrate that successful inference for the out-of-sample vertices is\npossible.\n", "versions": [{"version": "v1", "created": "Tue, 21 May 2013 17:24:57 GMT"}], "update_date": "2013-05-22", "authors_parsed": [["Tang", "Minh", ""], ["Park", "Youngser", ""], ["Priebe", "Carey E.", ""]]}, {"id": "1305.4987", "submitter": "Julie Tibshirani", "authors": "Julie Tibshirani and Christopher D. Manning", "title": "Robust Logistic Regression using Shift Parameters (Long Version)", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Annotation errors can significantly hurt classifier performance, yet datasets\nare only growing noisier with the increased use of Amazon Mechanical Turk and\ntechniques like distant supervision that automatically generate labels. In this\npaper, we present a robust extension of logistic regression that incorporates\nthe possibility of mislabelling directly into the objective. Our model can be\ntrained through nearly the same means as logistic regression, and retains its\nefficiency on high-dimensional datasets. Through named entity recognition\nexperiments, we demonstrate that our approach can provide a significant\nimprovement over the standard model when annotation errors are present.\n", "versions": [{"version": "v1", "created": "Tue, 21 May 2013 23:36:18 GMT"}, {"version": "v2", "created": "Tue, 29 Apr 2014 07:32:58 GMT"}], "update_date": "2014-04-30", "authors_parsed": [["Tibshirani", "Julie", ""], ["Manning", "Christopher D.", ""]]}, {"id": "1305.5017", "submitter": "Luke Bornn", "authors": "Luke Bornn", "title": "PAWL-Forced Simulated Tempering", "comments": "Proceedings of BAYSM, 2013", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.CO stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this short note, we show how the parallel adaptive Wang-Landau (PAWL)\nalgorithm of Bornn et al. (2013) can be used to automate and improve simulated\ntempering algorithms. While Wang-Landau and other stochastic approximation\nmethods have frequently been applied within the simulated tempering framework,\nthis note demonstrates through a simple example the additional improvements\nbrought about by parallelization, adaptive proposals and automated bin\nsplitting.\n", "versions": [{"version": "v1", "created": "Wed, 22 May 2013 04:01:38 GMT"}], "update_date": "2013-05-23", "authors_parsed": [["Bornn", "Luke", ""]]}, {"id": "1305.5029", "submitter": "Yuchen Zhang", "authors": "Yuchen Zhang and John C. Duchi and Martin J. Wainwright", "title": "Divide and Conquer Kernel Ridge Regression: A Distributed Algorithm with\n  Minimax Optimal Rates", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST cs.LG stat.ML stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We establish optimal convergence rates for a decomposition-based scalable\napproach to kernel ridge regression. The method is simple to describe: it\nrandomly partitions a dataset of size N into m subsets of equal size, computes\nan independent kernel ridge regression estimator for each subset, then averages\nthe local solutions into a global predictor. This partitioning leads to a\nsubstantial reduction in computation time versus the standard approach of\nperforming kernel ridge regression on all N samples. Our two main theorems\nestablish that despite the computational speed-up, statistical optimality is\nretained: as long as m is not too large, the partition-based estimator achieves\nthe statistical minimax rate over all estimators using the set of N samples. As\nconcrete examples, our theory guarantees that the number of processors m may\ngrow nearly linearly for finite-rank kernels and Gaussian kernels and\npolynomially in N for Sobolev spaces, which in turn allows for substantial\nreductions in computational cost. We conclude with experiments on both\nsimulated data and a music-prediction task that complement our theoretical\nresults, exhibiting the computational and statistical benefits of our approach.\n", "versions": [{"version": "v1", "created": "Wed, 22 May 2013 06:30:46 GMT"}, {"version": "v2", "created": "Tue, 29 Apr 2014 22:02:35 GMT"}], "update_date": "2014-05-01", "authors_parsed": [["Zhang", "Yuchen", ""], ["Duchi", "John C.", ""], ["Wainwright", "Martin J.", ""]]}, {"id": "1305.5306", "submitter": "Yin Zheng", "authors": "Yin Zheng, Yu-Jin Zhang, Hugo Larochelle", "title": "A Supervised Neural Autoregressive Topic Model for Simultaneous Image\n  Classification and Annotation", "comments": "13 pages, 5 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Topic modeling based on latent Dirichlet allocation (LDA) has been a\nframework of choice to perform scene recognition and annotation. Recently, a\nnew type of topic model called the Document Neural Autoregressive Distribution\nEstimator (DocNADE) was proposed and demonstrated state-of-the-art performance\nfor document modeling. In this work, we show how to successfully apply and\nextend this model to the context of visual scene modeling. Specifically, we\npropose SupDocNADE, a supervised extension of DocNADE, that increases the\ndiscriminative power of the hidden topic features by incorporating label\ninformation into the training objective of the model. We also describe how to\nleverage information about the spatial position of the visual words and how to\nembed additional image annotations, so as to simultaneously perform image\nclassification and annotation. We test our model on the Scene15, LabelMe and\nUIUC-Sports datasets and show that it compares favorably to other topic models\nsuch as the supervised variant of LDA.\n", "versions": [{"version": "v1", "created": "Thu, 23 May 2013 03:35:31 GMT"}], "update_date": "2013-05-24", "authors_parsed": [["Zheng", "Yin", ""], ["Zhang", "Yu-Jin", ""], ["Larochelle", "Hugo", ""]]}, {"id": "1305.5399", "submitter": "Gilles Stoltz", "authors": "Shie Mannor (EE-Technion), Vianney Perchet (LPMA), Gilles Stoltz\n  (INRIA Paris - Rocquencourt, DMA, GREGH)", "title": "A Primal Condition for Approachability with Partial Monitoring", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.OC cs.GT cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In approachability with full monitoring there are two types of conditions\nthat are known to be equivalent for convex sets: a primal and a dual condition.\nThe primal one is of the form: a set C is approachable if and only all\ncontaining half-spaces are approachable in the one-shot game; while the dual\none is of the form: a convex set C is approachable if and only if it intersects\nall payoff sets of a certain form. We consider approachability in games with\npartial monitoring. In previous works (Perchet 2011; Mannor et al. 2011) we\nprovided a dual characterization of approachable convex sets; we also exhibited\nefficient strategies in the case where C is a polytope. In this paper we\nprovide primal conditions on a convex set to be approachable with partial\nmonitoring. They depend on a modified reward function and lead to\napproachability strategies, based on modified payoff functions, that proceed by\nprojections similarly to Blackwell's (1956) strategy; this is in contrast with\npreviously studied strategies in this context that relied mostly on the\nsignaling structure and aimed at estimating well the distributions of the\nsignals received. Our results generalize classical results by Kohlberg 1975\n(see also Mertens et al. 1994) and apply to games with arbitrary signaling\nstructure as well as to arbitrary convex sets.\n", "versions": [{"version": "v1", "created": "Thu, 23 May 2013 12:44:29 GMT"}], "update_date": "2013-05-24", "authors_parsed": [["Mannor", "Shie", "", "EE-Technion"], ["Perchet", "Vianney", "", "LPMA"], ["Stoltz", "Gilles", "", "INRIA Paris - Rocquencourt, DMA, GREGH"]]}, {"id": "1305.5734", "submitter": "Yin Song", "authors": "Yin Song, Longbing Cao, Xuhui Fan, Wei Cao and Jian Zhang", "title": "Characterizing A Database of Sequential Behaviors with Latent Dirichlet\n  Hidden Markov Models", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper proposes a generative model, the latent Dirichlet hidden Markov\nmodels (LDHMM), for characterizing a database of sequential behaviors\n(sequences). LDHMMs posit that each sequence is generated by an underlying\nMarkov chain process, which are controlled by the corresponding parameters\n(i.e., the initial state vector, transition matrix and the emission matrix).\nThese sequence-level latent parameters for each sequence are modeled as latent\nDirichlet random variables and parameterized by a set of deterministic\ndatabase-level hyper-parameters. Through this way, we expect to model the\nsequence in two levels: the database level by deterministic hyper-parameters\nand the sequence-level by latent parameters. To learn the deterministic\nhyper-parameters and approximate posteriors of parameters in LDHMMs, we propose\nan iterative algorithm under the variational EM framework, which consists of E\nand M steps. We examine two different schemes, the fully-factorized and\npartially-factorized forms, for the framework, based on different assumptions.\nWe present empirical results of behavior modeling and sequence classification\non three real-world data sets, and compare them to other related models. The\nexperimental results prove that the proposed LDHMMs produce better\ngeneralization performance in terms of log-likelihood and deliver competitive\nresults on the sequence classification problem.\n", "versions": [{"version": "v1", "created": "Fri, 24 May 2013 13:51:20 GMT"}], "update_date": "2013-05-27", "authors_parsed": [["Song", "Yin", ""], ["Cao", "Longbing", ""], ["Fan", "Xuhui", ""], ["Cao", "Wei", ""], ["Zhang", "Jian", ""]]}, {"id": "1305.5782", "submitter": "Christopher Aicher", "authors": "Christopher Aicher, Abigail Z. Jacobs, Aaron Clauset", "title": "Adapting the Stochastic Block Model to Edge-Weighted Networks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG cs.SI physics.data-an", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We generalize the stochastic block model to the important case in which edges\nare annotated with weights drawn from an exponential family distribution. This\ngeneralization introduces several technical difficulties for model estimation,\nwhich we solve using a Bayesian approach. We introduce a variational algorithm\nthat efficiently approximates the model's posterior distribution for dense\ngraphs. In specific numerical experiments on edge-weighted networks, this\nweighted stochastic block model outperforms the common approach of first\napplying a single threshold to all weights and then applying the classic\nstochastic block model, which can obscure latent block structure in networks.\nThis model will enable the recovery of latent structure in a broader range of\nnetwork data than was previously possible.\n", "versions": [{"version": "v1", "created": "Fri, 24 May 2013 16:32:10 GMT"}], "update_date": "2013-05-27", "authors_parsed": [["Aicher", "Christopher", ""], ["Jacobs", "Abigail Z.", ""], ["Clauset", "Aaron", ""]]}, {"id": "1305.5826", "submitter": "Kian Hsiang Low", "authors": "Jie Chen, Nannan Cao, Kian Hsiang Low, Ruofei Ouyang, Colin Keng-Yan\n  Tan, Patrick Jaillet", "title": "Parallel Gaussian Process Regression with Low-Rank Covariance Matrix\n  Approximations", "comments": "29th Conference on Uncertainty in Artificial Intelligence (UAI 2013),\n  Extended version with proofs, 13 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.DC cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Gaussian processes (GP) are Bayesian non-parametric models that are widely\nused for probabilistic regression. Unfortunately, it cannot scale well with\nlarge data nor perform real-time predictions due to its cubic time cost in the\ndata size. This paper presents two parallel GP regression methods that exploit\nlow-rank covariance matrix approximations for distributing the computational\nload among parallel machines to achieve time efficiency and scalability. We\ntheoretically guarantee the predictive performances of our proposed parallel\nGPs to be equivalent to that of some centralized approximate GP regression\nmethods: The computation of their centralized counterparts can be distributed\namong parallel machines, hence achieving greater time efficiency and\nscalability. We analytically compare the properties of our parallel GPs such as\ntime, space, and communication complexity. Empirical evaluation on two\nreal-world datasets in a cluster of 20 computing nodes shows that our parallel\nGPs are significantly more time-efficient and scalable than their centralized\ncounterparts and exact/full GP while achieving predictive performances\ncomparable to full GP.\n", "versions": [{"version": "v1", "created": "Fri, 24 May 2013 19:00:28 GMT"}], "update_date": "2013-05-27", "authors_parsed": [["Chen", "Jie", ""], ["Cao", "Nannan", ""], ["Low", "Kian Hsiang", ""], ["Ouyang", "Ruofei", ""], ["Tan", "Colin Keng-Yan", ""], ["Jaillet", "Patrick", ""]]}, {"id": "1305.6213", "submitter": "Jean-Francois Bercher", "authors": "Jean-Fran\\c{c}ois Bercher (LIGM)", "title": "Some results on a $\\chi$-divergence, an~extended~Fisher information\n  and~generalized~Cram\\'er-Rao inequalities", "comments": null, "journal-ref": "Geometric Sciences of Information, Paris : France (2013)", "doi": null, "report-no": null, "categories": "cs.IT math.IT stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a modified $\\chi^{\\beta}$-divergence, give some of its properties,\nand show that this leads to the definition of a generalized Fisher information.\nWe give generalized Cram\\'er-Rao inequalities, involving this Fisher\ninformation, an extension of the Fisher information matrix, and arbitrary norms\nand power of the estimation error. In the case of a location parameter, we\nobtain new characterizations of the generalized $q$-Gaussians, for instance as\nthe distribution with a given moment that minimizes the generalized Fisher\ninformation. Finally we indicate how the generalized Fisher information can\nlead to new uncertainty relations.\n", "versions": [{"version": "v1", "created": "Mon, 27 May 2013 13:35:52 GMT"}], "update_date": "2013-05-28", "authors_parsed": [["Bercher", "Jean-Fran\u00e7ois", "", "LIGM"]]}, {"id": "1305.6215", "submitter": "Jean-Francois Bercher", "authors": "Jean-Fran\\c{c}ois Bercher (LIGM)", "title": "On some interrelations of generalized $q$-entropies and a generalized\n  Fisher information, including a Cram\\'er-Rao inequality", "comments": null, "journal-ref": "Applied Stochastic Models and Data Analysis, Mataro (Barcelona) :\n  Spain (2013)", "doi": null, "report-no": null, "categories": "cs.IT cond-mat.other math.IT stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this communication, we describe some interrelations between generalized\n$q$-entropies and a generalized version of Fisher information. In information\ntheory, the de Bruijn identity links the Fisher information and the derivative\nof the entropy. We show that this identity can be extended to generalized\nversions of entropy and Fisher information. More precisely, a generalized\nFisher information naturally pops up in the expression of the derivative of the\nTsallis entropy. This generalized Fisher information also appears as a special\ncase of a generalized Fisher information for estimation problems. Indeed, we\nderive here a new Cram\\'er-Rao inequality for the estimation of a parameter,\nwhich involves a generalized form of Fisher information. This generalized\nFisher information reduces to the standard Fisher information as a particular\ncase. In the case of a translation parameter, the general Cram\\'er-Rao\ninequality leads to an inequality for distributions which is saturated by\ngeneralized $q$-Gaussian distributions. These generalized $q$-Gaussians are\nimportant in several areas of physics and mathematics. They are known to\nmaximize the $q$-entropies subject to a moment constraint. The Cram\\'er-Rao\ninequality shows that the generalized $q$-Gaussians also minimize the\ngeneralized Fisher information among distributions with a fixed moment.\nSimilarly, the generalized $q$-Gaussians also minimize the generalized Fisher\ninformation among distributions with a given $q$-entropy.\n", "versions": [{"version": "v1", "created": "Mon, 27 May 2013 13:36:21 GMT"}], "update_date": "2013-05-28", "authors_parsed": [["Bercher", "Jean-Fran\u00e7ois", "", "LIGM"]]}, {"id": "1305.6526", "submitter": "Marten Wegkamp", "authors": "Marten Wegkamp, Yue Zhao", "title": "Adaptive estimation of the copula correlation matrix for semiparametric\n  elliptical copulas", "comments": "Published at http://dx.doi.org/10.3150/14-BEJ690 in the Bernoulli\n  (http://isi.cbs.nl/bernoulli/) by the International Statistical\n  Institute/Bernoulli Society (http://isi.cbs.nl/BS/bshome.htm)", "journal-ref": "Bernoulli 2016, Vol. 22, No. 2, 1184-1226", "doi": "10.3150/14-BEJ690", "report-no": "IMS-BEJ-BEJ690", "categories": "stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study the adaptive estimation of copula correlation matrix $\\Sigma$ for\nthe semi-parametric elliptical copula model. In this context, the correlations\nare connected to Kendall's tau through a sine function transformation. Hence, a\nnatural estimate for $\\Sigma$ is the plug-in estimator $\\hat{\\Sigma}$ with\nKendall's tau statistic. We first obtain a sharp bound on the operator norm of\n$\\hat{\\Sigma}-\\Sigma$. Then we study a factor model of $\\Sigma$, for which we\npropose a refined estimator $\\widetilde{\\Sigma}$ by fitting a low-rank matrix\nplus a diagonal matrix to $\\hat{\\Sigma}$ using least squares with a nuclear\nnorm penalty on the low-rank matrix. The bound on the operator norm of\n$\\hat{\\Sigma}-\\Sigma$ serves to scale the penalty term, and we obtain finite\nsample oracle inequalities for $\\widetilde{\\Sigma}$. We also consider an\nelementary factor copula model of $\\Sigma$, for which we propose closed-form\nestimators. All of our estimation procedures are entirely data-driven.\n", "versions": [{"version": "v1", "created": "Tue, 28 May 2013 15:14:58 GMT"}, {"version": "v2", "created": "Mon, 6 Jan 2014 14:28:43 GMT"}, {"version": "v3", "created": "Mon, 15 Feb 2016 06:24:48 GMT"}], "update_date": "2016-02-16", "authors_parsed": [["Wegkamp", "Marten", ""], ["Zhao", "Yue", ""]]}, {"id": "1305.6568", "submitter": "Arthur Carvalho", "authors": "Arthur Carvalho and Renato Oliveira", "title": "Reinforcement Learning for the Soccer Dribbling Task", "comments": null, "journal-ref": null, "doi": "10.1109/CIG.2011.6031994", "report-no": null, "categories": "cs.LG cs.RO stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a reinforcement learning solution to the \\emph{soccer dribbling\ntask}, a scenario in which a soccer agent has to go from the beginning to the\nend of a region keeping possession of the ball, as an adversary attempts to\ngain possession. While the adversary uses a stationary policy, the dribbler\nlearns the best action to take at each decision point. After defining\nmeaningful variables to represent the state space, and high-level macro-actions\nto incorporate domain knowledge, we describe our application of the\nreinforcement learning algorithm \\emph{Sarsa} with CMAC for function\napproximation. Our experiments show that, after the training period, the\ndribbler is able to accomplish its task against a strong adversary around 58%\nof the time.\n", "versions": [{"version": "v1", "created": "Tue, 28 May 2013 17:47:08 GMT"}], "update_date": "2013-05-29", "authors_parsed": [["Carvalho", "Arthur", ""], ["Oliveira", "Renato", ""]]}, {"id": "1305.6646", "submitter": "Paul Mineiro", "authors": "Stephane Ross and Paul Mineiro and John Langford", "title": "Normalized Online Learning", "comments": "Appears in Proceedings of the Twenty-Ninth Conference on Uncertainty\n  in Artificial Intelligence (UAI2013)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We introduce online learning algorithms which are independent of feature\nscales, proving regret bounds dependent on the ratio of scales existent in the\ndata rather than the absolute scale. This has several useful effects: there is\nno need to pre-normalize data, the test-time and test-space complexity are\nreduced, and the algorithms are more robust.\n", "versions": [{"version": "v1", "created": "Tue, 28 May 2013 22:12:59 GMT"}], "update_date": "2013-05-30", "authors_parsed": [["Ross", "Stephane", ""], ["Mineiro", "Paul", ""], ["Langford", "John", ""]]}, {"id": "1305.6659", "submitter": "Trevor Campbell", "authors": "Trevor Campbell, Miao Liu, Brian Kulis, Jonathan P. How, Lawrence\n  Carin", "title": "Dynamic Clustering via Asymptotics of the Dependent Dirichlet Process\n  Mixture", "comments": "This paper is from NIPS 2013. Please use the following BibTeX\n  citation: @inproceedings{Campbell13_NIPS, Author = {Trevor Campbell and Miao\n  Liu and Brian Kulis and Jonathan P. How and Lawrence Carin}, Title = {Dynamic\n  Clustering via Asymptotics of the Dependent Dirichlet Process}, Booktitle =\n  {Advances in Neural Information Processing Systems (NIPS)}, Year = {2013}}", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper presents a novel algorithm, based upon the dependent Dirichlet\nprocess mixture model (DDPMM), for clustering batch-sequential data containing\nan unknown number of evolving clusters. The algorithm is derived via a\nlow-variance asymptotic analysis of the Gibbs sampling algorithm for the DDPMM,\nand provides a hard clustering with convergence guarantees similar to those of\nthe k-means algorithm. Empirical results from a synthetic test with moving\nGaussian clusters and a test with real ADS-B aircraft trajectory data\ndemonstrate that the algorithm requires orders of magnitude less computational\ntime than contemporary probabilistic and hard clustering algorithms, while\nproviding higher accuracy on the examined datasets.\n", "versions": [{"version": "v1", "created": "Tue, 28 May 2013 23:59:16 GMT"}, {"version": "v2", "created": "Fri, 1 Nov 2013 18:25:39 GMT"}], "update_date": "2013-11-04", "authors_parsed": [["Campbell", "Trevor", ""], ["Liu", "Miao", ""], ["Kulis", "Brian", ""], ["How", "Jonathan P.", ""], ["Carin", "Lawrence", ""]]}, {"id": "1305.6883", "submitter": "Joscha Diehl", "authors": "Joscha Diehl", "title": "Rotation invariants of two dimensional curves based on iterated\n  integrals", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We introduce a novel class of rotation invariants of two dimensional curves\nbased on iterated integrals. The invariants we present are in some sense\ncomplete and we describe an algorithm to calculate them, giving explicit\ncomputations up to order six. We present an application to online\n(stroke-trajectory based) character recognition. This seems to be the first\ntime in the literature that the use of iterated integrals of a curve is\nproposed for (invariant) feature extraction in machine learning applications.\n", "versions": [{"version": "v1", "created": "Wed, 29 May 2013 17:58:04 GMT"}], "update_date": "2013-05-30", "authors_parsed": [["Diehl", "Joscha", ""]]}, {"id": "1305.6916", "submitter": "Fang Han", "authors": "Fang Han, Han Liu", "title": "Statistical analysis of latent generalized correlation matrix estimation\n  in transelliptical distribution", "comments": "Published at http://dx.doi.org/10.3150/15-BEJ702 in the Bernoulli\n  (http://isi.cbs.nl/bernoulli/) by the International Statistical\n  Institute/Bernoulli Society (http://isi.cbs.nl/BS/bshome.htm)", "journal-ref": "Bernoulli 2017, Vol. 23, No. 1, 23-57", "doi": "10.3150/15-BEJ702", "report-no": "IMS-BEJ-BEJ702", "categories": "stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Correlation matrices play a key role in many multivariate methods (e.g.,\ngraphical model estimation and factor analysis). The current state-of-the-art\nin estimating large correlation matrices focuses on the use of Pearson's sample\ncorrelation matrix. Although Pearson's sample correlation matrix enjoys various\ngood properties under Gaussian models, it is not an effective estimator when\nfacing heavy-tailed distributions. As a robust alternative, Han and Liu [J. Am.\nStat. Assoc. 109 (2015) 275-287] advocated the use of a transformed version of\nthe Kendall's tau sample correlation matrix in estimating high dimensional\nlatent generalized correlation matrix under the transelliptical distribution\nfamily (or elliptical copula). The transelliptical family assumes that after\nunspecified marginal monotone transformations, the data follow an elliptical\ndistribution. In this paper, we study the theoretical properties of the\nKendall's tau sample correlation matrix and its transformed version proposed in\nHan and Liu [J. Am. Stat. Assoc. 109 (2015) 275-287] for estimating the\npopulation Kendall's tau correlation matrix and the latent Pearson's\ncorrelation matrix under both spectral and restricted spectral norms. With\nregard to the spectral norm, we highlight the role of \"effective rank\" in\nquantifying the rate of convergence. With regard to the restricted spectral\nnorm, we for the first time present a \"sign sub-Gaussian condition\" which is\nsufficient to guarantee that the rank-based correlation matrix estimator\nattains the fast rate of convergence. In both cases, we do not need any moment\ncondition.\n", "versions": [{"version": "v1", "created": "Wed, 29 May 2013 19:39:56 GMT"}, {"version": "v2", "created": "Mon, 11 Nov 2013 00:25:41 GMT"}, {"version": "v3", "created": "Wed, 13 Nov 2013 16:51:32 GMT"}, {"version": "v4", "created": "Wed, 28 Sep 2016 12:38:14 GMT"}], "update_date": "2016-09-29", "authors_parsed": [["Han", "Fang", ""], ["Liu", "Han", ""]]}, {"id": "1305.7057", "submitter": "Sahar Mokhtar", "authors": "Sahar A. Mokhtar and Alaa. M. Elsayad", "title": "Predicting the Severity of Breast Masses with Data Mining Methods", "comments": null, "journal-ref": "International Journal of Computer Science Issues (IJCSI) March\n  2013 Issue (Volume 10, Issue 2)", "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Mammography is the most effective and available tool for breast cancer\nscreening. However, the low positive predictive value of breast biopsy\nresulting from mammogram interpretation leads to approximately 70% unnecessary\nbiopsies with benign outcomes. Data mining algorithms could be used to help\nphysicians in their decisions to perform a breast biopsy on a suspicious lesion\nseen in a mammogram image or to perform a short term follow-up examination\ninstead. In this research paper data mining classification algorithms; Decision\nTree (DT), Artificial Neural Network (ANN), and Support Vector Machine (SVM)\nare analyzed on mammographic masses data set. The purpose of this study is to\nincrease the ability of physicians to determine the severity (benign or\nmalignant) of a mammographic mass lesion from BI-RADS attributes and the\npatient,s age. The whole data set is divided for training the models and test\nthem by the ratio of 70:30% respectively and the performances of classification\nalgorithms are compared through three statistical measures; sensitivity,\nspecificity, and classification accuracy. Accuracy of DT, ANN and SVM are\n78.12%, 80.56% and 81.25% of test samples respectively. Our analysis shows that\nout of these three classification models SVM predicts severity of breast cancer\nwith least error rate and highest accuracy.\n", "versions": [{"version": "v1", "created": "Thu, 30 May 2013 10:44:41 GMT"}], "update_date": "2013-06-04", "authors_parsed": [["Mokhtar", "Sahar A.", ""], ["Elsayad", "Alaa. M.", ""]]}, {"id": "1305.7169", "submitter": "Shawn Mankad", "authors": "Shawn Mankad and George Michailidis", "title": "Structural and Functional Discovery in Dynamic Networks with\n  Non-negative Matrix Factorization", "comments": "16 pages, 17 figures", "journal-ref": null, "doi": "10.1103/PhysRevE.88.042812", "report-no": null, "categories": "cs.SI physics.soc-ph stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Time series of graphs are increasingly prevalent in modern data and pose\nunique challenges to visual exploration and pattern extraction. This paper\ndescribes the development and application of matrix factorizations for\nexploration and time-varying community detection in time-evolving graph\nsequences. The matrix factorization model allows the user to home in on and\ndisplay interesting, underlying structure and its evolution over time. The\nmethods are scalable to weighted networks with a large number of time points or\nnodes, and can accommodate sudden changes to graph topology. Our techniques are\ndemonstrated with several dynamic graph series from both synthetic and real\nworld data, including citation and trade networks. These examples illustrate\nhow users can steer the techniques and combine them with existing methods to\ndiscover and display meaningful patterns in sizable graphs over many time\npoints.\n", "versions": [{"version": "v1", "created": "Thu, 30 May 2013 17:34:38 GMT"}], "update_date": "2015-06-16", "authors_parsed": [["Mankad", "Shawn", ""], ["Michailidis", "George", ""]]}, {"id": "1305.7255", "submitter": "Marina Meila", "authors": "Dominique Perraul-Joncas and Marina Meila", "title": "Non-linear dimensionality reduction: Riemannian metric estimation and\n  the problem of geometric discovery", "comments": "32 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In recent years, manifold learning has become increasingly popular as a tool\nfor performing non-linear dimensionality reduction. This has led to the\ndevelopment of numerous algorithms of varying degrees of complexity that aim to\nrecover man ifold geometry using either local or global features of the data.\n  Building on the Laplacian Eigenmap and Diffusionmaps framework, we propose a\nnew paradigm that offers a guarantee, under reasonable assumptions, that any\nmanifo ld learning algorithm will preserve the geometry of a data set. Our\napproach is based on augmenting the output of embedding algorithms with\ngeometric informatio n embodied in the Riemannian metric of the manifold. We\nprovide an algorithm for estimating the Riemannian metric from data and\ndemonstrate possible application s of our approach in a variety of examples.\n", "versions": [{"version": "v1", "created": "Thu, 30 May 2013 21:16:04 GMT"}], "update_date": "2013-06-03", "authors_parsed": [["Perraul-Joncas", "Dominique", ""], ["Meila", "Marina", ""]]}, {"id": "1305.7344", "submitter": "Geoffrey McLachlan", "authors": "Saumyadipta Pyne, Kui Wang, Jonathan Irish, Pablo Tamayo, Marc-Danie\n  Nazaire, Tarn Duong, Sharon Lee, Shu-Kay Ng, David Hafler, Ronald Levy, Garry\n  Nolan, Jill Mesirov, and Geoffrey J. McLachlan", "title": "Joint Modeling and Registration of Cell Populations in Cohorts of\n  High-Dimensional Flow Cytometric Data", "comments": null, "journal-ref": null, "doi": "10.1371/journal.pone.0100334", "report-no": null, "categories": "stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In systems biomedicine, an experimenter encounters different potential\nsources of variation in data such as individual samples, multiple experimental\nconditions, and multi-variable network-level responses. In multiparametric\ncytometry, which is often used for analyzing patient samples, such issues are\ncritical. While computational methods can identify cell populations in\nindividual samples, without the ability to automatically match them across\nsamples, it is difficult to compare and characterize the populations in typical\nexperiments, such as those responding to various stimulations or distinctive of\nparticular patients or time-points, especially when there are many samples.\nJoint Clustering and Matching (JCM) is a multi-level framework for simultaneous\nmodeling and registration of populations across a cohort. JCM models every\npopulation with a robust multivariate probability distribution. Simultaneously,\nJCM fits a random-effects model to construct an overall batch template -- used\nfor registering populations across samples, and classifying new samples. By\ntackling systems-level variation, JCM supports practical biomedical\napplications involving large cohorts.\n", "versions": [{"version": "v1", "created": "Fri, 31 May 2013 09:53:53 GMT"}], "update_date": "2015-06-16", "authors_parsed": [["Pyne", "Saumyadipta", ""], ["Wang", "Kui", ""], ["Irish", "Jonathan", ""], ["Tamayo", "Pablo", ""], ["Nazaire", "Marc-Danie", ""], ["Duong", "Tarn", ""], ["Lee", "Sharon", ""], ["Ng", "Shu-Kay", ""], ["Hafler", "David", ""], ["Levy", "Ronald", ""], ["Nolan", "Garry", ""], ["Mesirov", "Jill", ""], ["McLachlan", "Geoffrey J.", ""]]}, {"id": "1305.7388", "submitter": "Minh Tang", "authors": "Avanti Athreya, Vince Lyzinski, David J. Marchette, Carey E. Priebe,\n  Daniel L. Sussman, Minh Tang", "title": "A central limit theorem for scaled eigenvectors of random dot product\n  graphs", "comments": "24 pages, 2 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.ML stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We prove a central limit theorem for the components of the largest\neigenvectors of the adjacency matrix of a finite-dimensional random dot product\ngraph whose true latent positions are unknown. In particular, we follow the\nmethodology outlined in \\citet{sussman2012universally} to construct consistent\nestimates for the latent positions, and we show that the appropriately scaled\ndifferences between the estimated and true latent positions converge to a\nmixture of Gaussian random variables. As a corollary, we obtain a central limit\ntheorem for the first eigenvector of the adjacency matrix of an Erd\\\"os-Renyi\nrandom graph.\n", "versions": [{"version": "v1", "created": "Fri, 31 May 2013 13:12:35 GMT"}, {"version": "v2", "created": "Mon, 23 Dec 2013 19:42:19 GMT"}], "update_date": "2013-12-24", "authors_parsed": [["Athreya", "Avanti", ""], ["Lyzinski", "Vince", ""], ["Marchette", "David J.", ""], ["Priebe", "Carey E.", ""], ["Sussman", "Daniel L.", ""], ["Tang", "Minh", ""]]}, {"id": "1305.7454", "submitter": "Uwe Aickelin", "authors": "Jan Feyereisl, Uwe Aickelin", "title": "Privileged Information for Data Clustering", "comments": "Information Sciences 194, 4-23, 2012", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Many machine learning algorithms assume that all input samples are\nindependently and identically distributed from some common distribution on\neither the input space X, in the case of unsupervised learning, or the input\nand output space X x Y in the case of supervised and semi-supervised learning.\nIn the last number of years the relaxation of this assumption has been explored\nand the importance of incorporation of additional information within machine\nlearning algorithms became more apparent. Traditionally such fusion of\ninformation was the domain of semi-supervised learning. More recently the\ninclusion of knowledge from separate hypothetical spaces has been proposed by\nVapnik as part of the supervised setting. In this work we are interested in\nexploring Vapnik's idea of master-class learning and the associated learning\nusing privileged information, however within the unsupervised setting. Adoption\nof the advanced supervised learning paradigm for the unsupervised setting\ninstigates investigation into the difference between privileged and technical\ndata. By means of our proposed aRi-MAX method stability of the KMeans algorithm\nis improved and identification of the best clustering solution is achieved on\nan artificial dataset. Subsequently an information theoretic dot product based\nalgorithm called P-Dot is proposed. This method has the ability to utilize a\nwide variety of clustering techniques, individually or in combination, while\nfusing privileged and technical data for improved clustering. Application of\nthe P-Dot method to the task of digit recognition confirms our findings in a\nreal-world scenario.\n", "versions": [{"version": "v1", "created": "Fri, 31 May 2013 15:28:44 GMT"}], "update_date": "2013-06-03", "authors_parsed": [["Feyereisl", "Jan", ""], ["Aickelin", "Uwe", ""]]}, {"id": "1305.7477", "submitter": "Jason Lee", "authors": "Jason D. Lee, Yuekai Sun, Jonathan E. Taylor", "title": "On model selection consistency of regularized M-estimators", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST cs.LG math.OC stat.ME stat.ML stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Regularized M-estimators are used in diverse areas of science and engineering\nto fit high-dimensional models with some low-dimensional structure. Usually the\nlow-dimensional structure is encoded by the presence of the (unknown)\nparameters in some low-dimensional model subspace. In such settings, it is\ndesirable for estimates of the model parameters to be \\emph{model selection\nconsistent}: the estimates also fall in the model subspace. We develop a\ngeneral framework for establishing consistency and model selection consistency\nof regularized M-estimators and show how it applies to some special cases of\ninterest in statistical learning. Our analysis identifies two key properties of\nregularized M-estimators, referred to as geometric decomposability and\nirrepresentability, that ensure the estimators are consistent and model\nselection consistent.\n", "versions": [{"version": "v1", "created": "Fri, 31 May 2013 16:24:17 GMT"}, {"version": "v2", "created": "Mon, 3 Jun 2013 16:59:21 GMT"}, {"version": "v3", "created": "Wed, 3 Jul 2013 23:47:12 GMT"}, {"version": "v4", "created": "Fri, 26 Jul 2013 19:44:45 GMT"}, {"version": "v5", "created": "Wed, 23 Oct 2013 19:34:33 GMT"}, {"version": "v6", "created": "Tue, 10 Dec 2013 17:52:09 GMT"}, {"version": "v7", "created": "Wed, 12 Feb 2014 08:52:10 GMT"}, {"version": "v8", "created": "Sat, 11 Oct 2014 05:54:58 GMT"}], "update_date": "2014-10-14", "authors_parsed": [["Lee", "Jason D.", ""], ["Sun", "Yuekai", ""], ["Taylor", "Jonathan E.", ""]]}]