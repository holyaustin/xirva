[{"id": "1512.00001", "submitter": "Stan Hatko", "authors": "Stan Hatko", "title": "k-Nearest Neighbour Classification of Datasets with a Family of\n  Distances", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The $k$-nearest neighbour ($k$-NN) classifier is one of the oldest and most\nimportant supervised learning algorithms for classifying datasets.\nTraditionally the Euclidean norm is used as the distance for the $k$-NN\nclassifier. In this thesis we investigate the use of alternative distances for\nthe $k$-NN classifier.\n  We start by introducing some background notions in statistical machine\nlearning. We define the $k$-NN classifier and discuss Stone's theorem and the\nproof that $k$-NN is universally consistent on the normed space $R^d$. We then\nprove that $k$-NN is universally consistent if we take a sequence of random\nnorms (that are independent of the sample and the query) from a family of norms\nthat satisfies a particular boundedness condition. We extend this result by\nreplacing norms with distances based on uniformly locally Lipschitz functions\nthat satisfy certain conditions. We discuss the limitations of Stone's lemma\nand Stone's theorem, particularly with respect to quasinorms and adaptively\nchoosing a distance for $k$-NN based on the labelled sample. We show the\nuniversal consistency of a two stage $k$-NN type classifier where we select the\ndistance adaptively based on a split labelled sample and the query. We conclude\nby giving some examples of improvements of the accuracy of classifying various\ndatasets using the above techniques.\n", "versions": [{"version": "v1", "created": "Sun, 29 Nov 2015 01:52:34 GMT"}], "update_date": "2015-12-02", "authors_parsed": [["Hatko", "Stan", ""]]}, {"id": "1512.00150", "submitter": "Chao Gao", "authors": "Chao Gao, Yu Lu, Zongming Ma, Harrison H. Zhou", "title": "Optimal Estimation and Completion of Matrices with Biclustering\n  Structures", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.ML stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Biclustering structures in data matrices were first formalized in a seminal\npaper by John Hartigan (1972) where one seeks to cluster cases and variables\nsimultaneously. Such structures are also prevalent in block modeling of\nnetworks. In this paper, we develop a unified theory for the estimation and\ncompletion of matrices with biclustering structures, where the data is a\npartially observed and noise contaminated data matrix with a certain\nbiclustering structure. In particular, we show that a constrained least squares\nestimator achieves minimax rate-optimal performance in several of the most\nimportant scenarios. To this end, we derive unified high probability upper\nbounds for all sub-Gaussian data and also provide matching minimax lower bounds\nin both Gaussian and binary cases. Due to the close connection of graphon to\nstochastic block models, an immediate consequence of our general results is a\nminimax rate-optimal estimator for sparse graphons.\n", "versions": [{"version": "v1", "created": "Tue, 1 Dec 2015 05:54:23 GMT"}, {"version": "v2", "created": "Mon, 22 Oct 2018 19:54:21 GMT"}], "update_date": "2018-10-24", "authors_parsed": [["Gao", "Chao", ""], ["Lu", "Yu", ""], ["Ma", "Zongming", ""], ["Zhou", "Harrison H.", ""]]}, {"id": "1512.00315", "submitter": "Jaak Simm", "authors": "Adam Arany, Jaak Simm, Pooya Zakeri, Tom Haber, J\\\"org K. Wegner,\n  Vladimir Chupakhin, Hugo Ceulemans, Yves Moreau", "title": "Highly Scalable Tensor Factorization for Prediction of Drug-Protein\n  Interaction Type", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  The understanding of the type of inhibitory interaction plays an important\nrole in drug design. Therefore, researchers are interested to know whether a\ndrug has competitive or non-competitive interaction to particular protein\ntargets.\n  Method: to analyze the interaction types we propose factorization method\nMacau which allows us to combine different measurement types into a single\ntensor together with proteins and compounds. The compounds are characterized by\nhigh dimensional 2D ECFP fingerprints. The novelty of the proposed method is\nthat using a specially designed noise injection MCMC sampler it can incorporate\nhigh dimensional side information, i.e., millions of unique 2D ECFP compound\nfeatures, even for large scale datasets of millions of compounds. Without the\nside information, in this case, the tensor factorization would be practically\nfutile.\n  Results: using public IC50 and Ki data from ChEMBL we trained a model from\nwhere we can identify the latent subspace separating the two measurement types\n(IC50 and Ki). The results suggest the proposed method can detect the\ncompetitive inhibitory activity between compounds and proteins.\n", "versions": [{"version": "v1", "created": "Tue, 1 Dec 2015 16:03:14 GMT"}], "update_date": "2015-12-02", "authors_parsed": [["Arany", "Adam", ""], ["Simm", "Jaak", ""], ["Zakeri", "Pooya", ""], ["Haber", "Tom", ""], ["Wegner", "J\u00f6rg K.", ""], ["Chupakhin", "Vladimir", ""], ["Ceulemans", "Hugo", ""], ["Moreau", "Yves", ""]]}, {"id": "1512.00442", "submitter": "Ke Li", "authors": "Ke Li, Jitendra Malik", "title": "Fast k-Nearest Neighbour Search via Dynamic Continuous Indexing", "comments": "13 pages, 6 figures; International Conference on Machine Learning\n  (ICML), 2016. This version corrects a typo in the pseudocode", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.AI cs.IR cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Existing methods for retrieving k-nearest neighbours suffer from the curse of\ndimensionality. We argue this is caused in part by inherent deficiencies of\nspace partitioning, which is the underlying strategy used by most existing\nmethods. We devise a new strategy that avoids partitioning the vector space and\npresent a novel randomized algorithm that runs in time linear in dimensionality\nof the space and sub-linear in the intrinsic dimensionality and the size of the\ndataset and takes space constant in dimensionality of the space and linear in\nthe size of the dataset. The proposed algorithm allows fine-grained control\nover accuracy and speed on a per-query basis, automatically adapts to\nvariations in data density, supports dynamic updates to the dataset and is\neasy-to-implement. We show appealing theoretical properties and demonstrate\nempirically that the proposed algorithm outperforms locality-sensitivity\nhashing (LSH) in terms of approximation quality, speed and space efficiency.\n", "versions": [{"version": "v1", "created": "Tue, 1 Dec 2015 20:53:16 GMT"}, {"version": "v2", "created": "Fri, 10 Jun 2016 18:47:10 GMT"}, {"version": "v3", "created": "Thu, 6 Apr 2017 06:51:49 GMT"}], "update_date": "2017-04-07", "authors_parsed": [["Li", "Ke", ""], ["Malik", "Jitendra", ""]]}, {"id": "1512.00486", "submitter": "Maksim Lapin", "authors": "Maksim Lapin, Matthias Hein, Bernt Schiele", "title": "Loss Functions for Top-k Error: Analysis and Insights", "comments": "In Computer Vision and Pattern Recognition (CVPR), 2016", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In order to push the performance on realistic computer vision tasks, the\nnumber of classes in modern benchmark datasets has significantly increased in\nrecent years. This increase in the number of classes comes along with increased\nambiguity between the class labels, raising the question if top-1 error is the\nright performance measure. In this paper, we provide an extensive comparison\nand evaluation of established multiclass methods comparing their top-k\nperformance both from a practical as well as from a theoretical perspective.\nMoreover, we introduce novel top-k loss functions as modifications of the\nsoftmax and the multiclass SVM losses and provide efficient optimization\nschemes for them. In the experiments, we compare on various datasets all of the\nproposed and established methods for top-k error optimization. An interesting\ninsight of this paper is that the softmax loss yields competitive top-k\nperformance for all k simultaneously. For a specific top-k error, our new top-k\nlosses lead typically to further improvements while being faster to train than\nthe softmax.\n", "versions": [{"version": "v1", "created": "Tue, 1 Dec 2015 21:22:35 GMT"}, {"version": "v2", "created": "Wed, 13 Apr 2016 15:12:01 GMT"}], "update_date": "2016-04-14", "authors_parsed": [["Lapin", "Maksim", ""], ["Hein", "Matthias", ""], ["Schiele", "Bernt", ""]]}, {"id": "1512.00743", "submitter": "Amogh Gudi", "authors": "Amogh Gudi", "title": "Recognizing Semantic Features in Faces using Deep Learning", "comments": "Thesis, M.Sc. Artificial Intelligence, University of Amsterdam, 2014", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI cs.CV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The human face constantly conveys information, both consciously and\nsubconsciously. However, as basic as it is for humans to visually interpret\nthis information, it is quite a big challenge for machines. Conventional\nsemantic facial feature recognition and analysis techniques are already in use\nand are based on physiological heuristics, but they suffer from lack of\nrobustness and high computation time. This thesis aims to explore ways for\nmachines to learn to interpret semantic information available in faces in an\nautomated manner without requiring manual design of feature detectors, using\nthe approach of Deep Learning. This thesis provides a study of the effects of\nvarious factors and hyper-parameters of deep neural networks in the process of\ndetermining an optimal network configuration for the task of semantic facial\nfeature recognition. This thesis explores the effectiveness of the system to\nrecognize the various semantic features (like emotions, age, gender, ethnicity\netc.) present in faces. Furthermore, the relation between the effect of\nhigh-level concepts on low level features is explored through an analysis of\nthe similarities in low-level descriptors of different semantic features. This\nthesis also demonstrates a novel idea of using a deep network to generate 3-D\nActive Appearance Models of faces from real-world 2-D images.\n  For a more detailed report on this work, please see [arXiv:1512.00743v1].\n", "versions": [{"version": "v1", "created": "Wed, 2 Dec 2015 15:46:26 GMT"}, {"version": "v2", "created": "Wed, 19 Oct 2016 13:33:44 GMT"}], "update_date": "2016-10-21", "authors_parsed": [["Gudi", "Amogh", ""]]}, {"id": "1512.00792", "submitter": "Rebecca Steorts", "authors": "Jeffrey Miller, Brenda Betancourt, Abbas Zaidi, Hanna Wallach, and\n  Rebecca C. Steorts", "title": "Microclustering: When the Cluster Sizes Grow Sublinearly with the Size\n  of the Data Set", "comments": "8 pages, 3 figures, NIPS Bayesian Nonparametrics: The Next Generation\n  Workshop Series", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.AP stat.CO stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Most generative models for clustering implicitly assume that the number of\ndata points in each cluster grows linearly with the total number of data\npoints. Finite mixture models, Dirichlet process mixture models, and\nPitman--Yor process mixture models make this assumption, as do all other\ninfinitely exchangeable clustering models. However, for some tasks, this\nassumption is undesirable. For example, when performing entity resolution, the\nsize of each cluster is often unrelated to the size of the data set.\nConsequently, each cluster contains a negligible fraction of the total number\nof data points. Such tasks therefore require models that yield clusters whose\nsizes grow sublinearly with the size of the data set. We address this\nrequirement by defining the \\emph{microclustering property} and introducing a\nnew model that exhibits this property. We compare this model to several\ncommonly used clustering models by checking model fit using real and simulated\ndata sets.\n", "versions": [{"version": "v1", "created": "Wed, 2 Dec 2015 18:08:48 GMT"}], "update_date": "2015-12-03", "authors_parsed": [["Miller", "Jeffrey", ""], ["Betancourt", "Brenda", ""], ["Zaidi", "Abbas", ""], ["Wallach", "Hanna", ""], ["Steorts", "Rebecca C.", ""]]}, {"id": "1512.00809", "submitter": "Korbinian Strimmer", "authors": "Agnan Kessy, Alex Lewin, and Korbinian Strimmer", "title": "Optimal whitening and decorrelation", "comments": "14 pages, 2 tables", "journal-ref": "The American Statistician 2018, Vol. 72, No. 4, pp. 309-314", "doi": "10.1080/00031305.2016.1277159", "report-no": null, "categories": "stat.ME stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Whitening, or sphering, is a common preprocessing step in statistical\nanalysis to transform random variables to orthogonality. However, due to\nrotational freedom there are infinitely many possible whitening procedures.\nConsequently, there is a diverse range of sphering methods in use, for example\nbased on principal component analysis (PCA), Cholesky matrix decomposition and\nzero-phase component analysis (ZCA), among others.\n  Here we provide an overview of the underlying theory and discuss five natural\nwhitening procedures. Subsequently, we demonstrate that investigating the\ncross-covariance and the cross-correlation matrix between sphered and original\nvariables allows to break the rotational invariance and to identify optimal\nwhitening transformations. As a result we recommend two particular approaches:\nZCA-cor whitening to produce sphered variables that are maximally similar to\nthe original variables, and PCA-cor whitening to obtain sphered variables that\nmaximally compress the original variables.\n", "versions": [{"version": "v1", "created": "Wed, 2 Dec 2015 18:54:53 GMT"}, {"version": "v2", "created": "Thu, 26 May 2016 16:36:44 GMT"}, {"version": "v3", "created": "Thu, 15 Dec 2016 11:27:22 GMT"}, {"version": "v4", "created": "Sun, 18 Dec 2016 00:17:54 GMT"}], "update_date": "2018-11-15", "authors_parsed": [["Kessy", "Agnan", ""], ["Lewin", "Alex", ""], ["Strimmer", "Korbinian", ""]]}, {"id": "1512.00907", "submitter": "Mostafa Rahmani", "authors": "Mostafa Rahmani, George Atia", "title": "Innovation Pursuit: A New Approach to Subspace Clustering", "comments": null, "journal-ref": "IEEE Transactions on Signal Processing ( Volume: 65, Issue: 23,\n  Dec.1, 1 2017 )", "doi": "10.1109/TSP.2017.2749206", "report-no": null, "categories": "cs.CV cs.IR cs.IT cs.LG math.IT stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In subspace clustering, a group of data points belonging to a union of\nsubspaces are assigned membership to their respective subspaces. This paper\npresents a new approach dubbed Innovation Pursuit (iPursuit) to the problem of\nsubspace clustering using a new geometrical idea whereby subspaces are\nidentified based on their relative novelties. We present two frameworks in\nwhich the idea of innovation pursuit is used to distinguish the subspaces.\nUnderlying the first framework is an iterative method that finds the subspaces\nconsecutively by solving a series of simple linear optimization problems, each\nsearching for a direction of innovation in the span of the data potentially\northogonal to all subspaces except for the one to be identified in one step of\nthe algorithm. A detailed mathematical analysis is provided establishing\nsufficient conditions for iPursuit to correctly cluster the data. The proposed\napproach can provably yield exact clustering even when the subspaces have\nsignificant intersections. It is shown that the complexity of the iterative\napproach scales only linearly in the number of data points and subspaces, and\nquadratically in the dimension of the subspaces. The second framework\nintegrates iPursuit with spectral clustering to yield a new variant of\nspectral-clustering-based algorithms. The numerical simulations with both real\nand synthetic data demonstrate that iPursuit can often outperform the\nstate-of-the-art subspace clustering algorithms, more so for subspaces with\nsignificant intersections, and that it significantly improves the\nstate-of-the-art result for subspace-segmentation-based face clustering.\n", "versions": [{"version": "v1", "created": "Wed, 2 Dec 2015 23:52:43 GMT"}, {"version": "v2", "created": "Fri, 2 Dec 2016 05:26:58 GMT"}, {"version": "v3", "created": "Wed, 17 May 2017 23:12:17 GMT"}, {"version": "v4", "created": "Wed, 14 Jun 2017 03:29:06 GMT"}, {"version": "v5", "created": "Sun, 26 Nov 2017 15:24:33 GMT"}], "update_date": "2017-11-28", "authors_parsed": [["Rahmani", "Mostafa", ""], ["Atia", "George", ""]]}, {"id": "1512.00927", "submitter": "Muneki Yasuda", "authors": "Chako Takahashi and Muneki Yasuda", "title": "Mean-Field Inference in Gaussian Restricted Boltzmann Machine", "comments": null, "journal-ref": "J. Phys. Soc. Jpn., Vol.85, No.3, Article ID: 034001, 2016", "doi": "10.7566/JPSJ.85.034001", "report-no": null, "categories": "stat.ML physics.data-an", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A Gaussian restricted Boltzmann machine (GRBM) is a Boltzmann machine defined\non a bipartite graph and is an extension of usual restricted Boltzmann\nmachines. A GRBM consists of two different layers: a visible layer composed of\ncontinuous visible variables and a hidden layer composed of discrete hidden\nvariables. In this paper, we derive two different inference algorithms for\nGRBMs based on the naive mean-field approximation (NMFA). One is an inference\nalgorithm for whole variables in a GRBM, and the other is an inference\nalgorithm for partial variables in a GBRBM. We compare the two methods\nanalytically and numerically and show that the latter method is better.\n", "versions": [{"version": "v1", "created": "Thu, 3 Dec 2015 02:20:55 GMT"}, {"version": "v2", "created": "Fri, 18 Mar 2016 03:58:39 GMT"}], "update_date": "2016-03-21", "authors_parsed": [["Takahashi", "Chako", ""], ["Yasuda", "Muneki", ""]]}, {"id": "1512.00933", "submitter": "Francois-Xavier Briol", "authors": "Fran\\c{c}ois-Xavier Briol, Chris. J. Oates, Mark Girolami, Michael A.\n  Osborne and Dino Sejdinovic", "title": "Probabilistic Integration: A Role in Statistical Computation?", "comments": "Several improvements suggested by reviewers, including additional\n  experiments on uncertainty quantification properties. Change of title:\n  previously \"Probabilistic Integration: A Role for Statisticians in Numerical\n  Analysis?\"", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.NA math.NA math.ST stat.CO stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A research frontier has emerged in scientific computation, wherein numerical\nerror is regarded as a source of epistemic uncertainty that can be modelled.\nThis raises several statistical challenges, including the design of statistical\nmethods that enable the coherent propagation of probabilities through a\n(possibly deterministic) computational work-flow. This paper examines the case\nfor probabilistic numerical methods in routine statistical computation. Our\nfocus is on numerical integration, where a probabilistic integrator is equipped\nwith a full distribution over its output that reflects the presence of an\nunknown numerical error. Our main technical contribution is to establish, for\nthe first time, rates of posterior contraction for these methods. These show\nthat probabilistic integrators can in principle enjoy the \"best of both\nworlds\", leveraging the sampling efficiency of Monte Carlo methods whilst\nproviding a principled route to assess the impact of numerical error on\nscientific conclusions. Several substantial applications are provided for\nillustration and critical evaluation, including examples from statistical\nmodelling, computer graphics and a computer model for an oil reservoir.\n", "versions": [{"version": "v1", "created": "Thu, 3 Dec 2015 02:52:33 GMT"}, {"version": "v2", "created": "Mon, 4 Apr 2016 22:29:14 GMT"}, {"version": "v3", "created": "Wed, 6 Apr 2016 06:09:18 GMT"}, {"version": "v4", "created": "Mon, 11 Apr 2016 09:15:20 GMT"}, {"version": "v5", "created": "Thu, 20 Oct 2016 08:44:17 GMT"}, {"version": "v6", "created": "Wed, 18 Oct 2017 14:15:40 GMT"}], "update_date": "2017-10-19", "authors_parsed": [["Briol", "Fran\u00e7ois-Xavier", ""], ["Oates", "Chris. J.", ""], ["Girolami", "Mark", ""], ["Osborne", "Michael A.", ""], ["Sejdinovic", "Dino", ""]]}, {"id": "1512.00947", "submitter": "Momiao Xiong", "authors": "Panpan Wang, Mohammad Rahman, Li Jin and Momiao Xiong", "title": "A New Statistical Framework for Genetic Pleiotropic Analysis of High\n  Dimensional Phenotype Data", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML q-bio.GN stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The widely used genetic pleiotropic analysis of multiple phenotypes are often\ndesigned for examining the relationship between common variants and a few\nphenotypes. They are not suited for both high dimensional phenotypes and high\ndimensional genotype (next-generation sequencing) data. To overcome these\nlimitations, we develop sparse structural equation models (SEMs) as a general\nframework for a new paradigm of genetic analysis of multiple phenotypes. To\nincorporate both common and rare variants into the analysis, we extend the\ntraditional multivariate SEMs to sparse functional SEMs. To deal with high\ndimensional phenotype and genotype data, we employ functional data analysis and\nthe alternative direction methods of multiplier (ADMM) techniques to reduce\ndata dimension and improve computational efficiency. Using large scale\nsimulations we showed that the proposed methods have higher power to detect\ntrue causal genetic pleiotropic structure than other existing methods.\nSimulations also demonstrate that the gene-based pleiotropic analysis has\nhigher power than the single variant-based pleiotropic analysis. The proposed\nmethod is applied to exome sequence data from the NHLBI Exome Sequencing\nProject (ESP) with 11 phenotypes, which identifies a network with 137 genes\nconnected to 11 phenotypes and 341 edges. Among them, 114 genes showed\npleiotropic genetic effects and 45 genes were reported to be associated with\nphenotypes in the analysis or other cardiovascular disease (CVD) related\nphenotypes in the literature.\n", "versions": [{"version": "v1", "created": "Thu, 3 Dec 2015 04:49:42 GMT"}], "update_date": "2015-12-04", "authors_parsed": [["Wang", "Panpan", ""], ["Rahman", "Mohammad", ""], ["Jin", "Li", ""], ["Xiong", "Momiao", ""]]}, {"id": "1512.00984", "submitter": "Quanming Yao", "authors": "Quanming Yao, James T. Kwok, Wenliang Zhong", "title": "Fast Low-Rank Matrix Learning with Nonconvex Regularization", "comments": "Long version of conference paper appeared ICDM 2015", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NA cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Low-rank modeling has a lot of important applications in machine learning,\ncomputer vision and social network analysis. While the matrix rank is often\napproximated by the convex nuclear norm, the use of nonconvex low-rank\nregularizers has demonstrated better recovery performance. However, the\nresultant optimization problem is much more challenging. A very recent\nstate-of-the-art is based on the proximal gradient algorithm. However, it\nrequires an expensive full SVD in each proximal step. In this paper, we show\nthat for many commonly-used nonconvex low-rank regularizers, a cutoff can be\nderived to automatically threshold the singular values obtained from the\nproximal operator. This allows the use of power method to approximate the SVD\nefficiently. Besides, the proximal operator can be reduced to that of a much\nsmaller matrix projected onto this leading subspace. Convergence, with a rate\nof O(1/T) where T is the number of iterations, can be guaranteed. Extensive\nexperiments are performed on matrix completion and robust principal component\nanalysis. The proposed method achieves significant speedup over the\nstate-of-the-art. Moreover, the matrix solution obtained is more accurate and\nhas a lower rank than that of the traditional nuclear norm regularizer.\n", "versions": [{"version": "v1", "created": "Thu, 3 Dec 2015 08:32:17 GMT"}], "update_date": "2016-05-02", "authors_parsed": [["Yao", "Quanming", ""], ["Kwok", "James T.", ""], ["Zhong", "Wenliang", ""]]}, {"id": "1512.00994", "submitter": "Xinggang Wang", "authors": "Hanqiang Song and Zhuotun Zhu and Xinggang Wang", "title": "Bag Reference Vector for Multi-instance Learning", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Multi-instance learning (MIL) has a wide range of applications due to its\ndistinctive characteristics. Although many state-of-the-art algorithms have\nachieved decent performances, a plurality of existing methods solve the problem\nonly in instance level rather than excavating relations among bags. In this\npaper, we propose an efficient algorithm to describe each bag by a\ncorresponding feature vector via comparing it with other bags. In other words,\nthe crucial information of a bag is extracted from the similarity between that\nbag and other reference bags. In addition, we apply extensions of Hausdorff\ndistance to representing the similarity, to a certain extent, overcoming the\nkey challenge of MIL problem, the ambiguity of instances' labels in positive\nbags. Experimental results on benchmarks and text categorization tasks show\nthat the proposed method outperforms the previous state-of-the-art by a large\nmargin.\n", "versions": [{"version": "v1", "created": "Thu, 3 Dec 2015 09:03:05 GMT"}], "update_date": "2015-12-04", "authors_parsed": [["Song", "Hanqiang", ""], ["Zhu", "Zhuotun", ""], ["Wang", "Xinggang", ""]]}, {"id": "1512.01139", "submitter": "Vivak Patel", "authors": "Vivak Patel", "title": "Kalman-based Stochastic Gradient Method with Stop Condition and\n  Insensitivity to Conditioning", "comments": null, "journal-ref": "SIAM J. Optim. 26 (2016) 2620-2648", "doi": "10.1137/15M1048239", "report-no": null, "categories": "math.OC stat.CO stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Modern proximal and stochastic gradient descent (SGD) methods are believed to\nefficiently minimize large composite objective functions, but such methods have\ntwo algorithmic challenges: (1) a lack of fast or justified stop conditions,\nand (2) sensitivity to the objective function's conditioning. In response to\nthe first challenge, modern proximal and SGD methods guarantee convergence only\nafter multiple epochs, but such a guarantee renders proximal and SGD methods\ninfeasible when the number of component functions is very large or infinite. In\nresponse to the second challenge, second order SGD methods have been developed,\nbut they are marred by the complexity of their analysis. In this work, we\naddress these challenges on the limited, but important, linear regression\nproblem by introducing and analyzing a second order proximal/SGD method based\non Kalman Filtering (kSGD). Through our analysis, we show kSGD is\nasymptotically optimal, develop a fast algorithm for very large, infinite or\nstreaming data sources with a justified stop condition, prove that kSGD is\ninsensitive to the problem's conditioning, and develop a unique approach for\nanalyzing the complex second order dynamics. Our theoretical results are\nsupported by numerical experiments on three regression problems (linear,\nnonparametric wavelet, and logistic) using three large publicly available\ndatasets. Moreover, our analysis and experiments lay a foundation for embedding\nkSGD in multiple epoch algorithms, extending kSGD to other problem classes, and\ndeveloping parallel and low memory kSGD implementations.\n", "versions": [{"version": "v1", "created": "Thu, 3 Dec 2015 16:13:33 GMT"}, {"version": "v2", "created": "Fri, 10 Jun 2016 21:51:38 GMT"}], "update_date": "2017-01-05", "authors_parsed": [["Patel", "Vivak", ""]]}, {"id": "1512.01255", "submitter": "Sebastian Weichwald", "authors": "Sebastian Weichwald, Moritz Grosse-Wentrup, Arthur Gretton", "title": "MERLiN: Mixture Effect Recovery in Linear Networks", "comments": null, "journal-ref": "IEEE Journal of Selected Topics in Signal Processing, 10(7),\n  1254-1266, 2016", "doi": "10.1109/JSTSP.2016.2601144", "report-no": null, "categories": "stat.ME q-bio.NC stat.AP stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Causal inference concerns the identification of cause-effect relationships\nbetween variables, e.g. establishing whether a stimulus affects activity in a\ncertain brain region. The observed variables themselves often do not constitute\nmeaningful causal variables, however, and linear combinations need to be\nconsidered. In electroencephalographic studies, for example, one is not\ninterested in establishing cause-effect relationships between electrode signals\n(the observed variables), but rather between cortical signals (the causal\nvariables) which can be recovered as linear combinations of electrode signals.\n  We introduce MERLiN (Mixture Effect Recovery in Linear Networks), a family of\ncausal inference algorithms that implement a novel means of constructing causal\nvariables from non-causal variables. We demonstrate through application to EEG\ndata how the basic MERLiN algorithm can be extended for application to\ndifferent (neuroimaging) data modalities. Given an observed linear mixture, the\nalgorithms can recover a causal variable that is a linear effect of another\ngiven variable. That is, MERLiN allows us to recover a cortical signal that is\naffected by activity in a certain brain region, while not being a direct effect\nof the stimulus. The Python/Matlab implementation for all presented algorithms\nis available on https://github.com/sweichwald/MERLiN\n", "versions": [{"version": "v1", "created": "Thu, 3 Dec 2015 21:29:30 GMT"}, {"version": "v2", "created": "Fri, 8 Jul 2016 17:43:18 GMT"}, {"version": "v3", "created": "Tue, 27 Sep 2016 21:27:57 GMT"}], "update_date": "2020-09-03", "authors_parsed": [["Weichwald", "Sebastian", ""], ["Grosse-Wentrup", "Moritz", ""], ["Gretton", "Arthur", ""]]}, {"id": "1512.01272", "submitter": "Vikash Mansinghka", "authors": "Vikash Mansinghka, Patrick Shafto, Eric Jonas, Cap Petschulat, Max\n  Gasner, Joshua B. Tenenbaum", "title": "CrossCat: A Fully Bayesian Nonparametric Method for Analyzing\n  Heterogeneous, High Dimensional Data", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI stat.CO stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  There is a widespread need for statistical methods that can analyze\nhigh-dimensional datasets with- out imposing restrictive or opaque modeling\nassumptions. This paper describes a domain-general data analysis method called\nCrossCat. CrossCat infers multiple non-overlapping views of the data, each\nconsisting of a subset of the variables, and uses a separate nonparametric\nmixture to model each view. CrossCat is based on approximately Bayesian\ninference in a hierarchical, nonparamet- ric model for data tables. This model\nconsists of a Dirichlet process mixture over the columns of a data table in\nwhich each mixture component is itself an independent Dirichlet process mixture\nover the rows; the inner mixture components are simple parametric models whose\nform depends on the types of data in the table. CrossCat combines strengths of\nmixture modeling and Bayesian net- work structure learning. Like mixture\nmodeling, CrossCat can model a broad class of distributions by positing latent\nvariables, and produces representations that can be efficiently conditioned and\nsampled from for prediction. Like Bayesian networks, CrossCat represents the\ndependencies and independencies between variables, and thus remains accurate\nwhen there are multiple statistical signals. Inference is done via a scalable\nGibbs sampling scheme; this paper shows that it works well in practice. This\npaper also includes empirical results on heterogeneous tabular data of up to 10\nmillion cells, such as hospital cost and quality measures, voting records,\nunemployment rates, gene expression measurements, and images of handwritten\ndigits. CrossCat infers structure that is consistent with accepted findings and\ncommon-sense knowledge in multiple domains and yields predictive accuracy\ncompetitive with generative, discriminative, and model-free alternatives.\n", "versions": [{"version": "v1", "created": "Thu, 3 Dec 2015 22:39:37 GMT"}], "update_date": "2015-12-07", "authors_parsed": [["Mansinghka", "Vikash", ""], ["Shafto", "Patrick", ""], ["Jonas", "Eric", ""], ["Petschulat", "Cap", ""], ["Gasner", "Max", ""], ["Tenenbaum", "Joshua B.", ""]]}, {"id": "1512.01286", "submitter": "Simone Romano", "authors": "Simone Romano and Nguyen Xuan Vinh and James Bailey and Karin Verspoor", "title": "Adjusting for Chance Clustering Comparison Measures", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Adjusted for chance measures are widely used to compare\npartitions/clusterings of the same data set. In particular, the Adjusted Rand\nIndex (ARI) based on pair-counting, and the Adjusted Mutual Information (AMI)\nbased on Shannon information theory are very popular in the clustering\ncommunity. Nonetheless it is an open problem as to what are the best\napplication scenarios for each measure and guidelines in the literature for\ntheir usage are sparse, with the result that users often resort to using both.\nGeneralized Information Theoretic (IT) measures based on the Tsallis entropy\nhave been shown to link pair-counting and Shannon IT measures. In this paper,\nwe aim to bridge the gap between adjustment of measures based on pair-counting\nand measures based on information theory. We solve the key technical challenge\nof analytically computing the expected value and variance of generalized IT\nmeasures. This allows us to propose adjustments of generalized IT measures,\nwhich reduce to well known adjusted clustering comparison measures as special\ncases. Using the theory of generalized IT measures, we are able to propose the\nfollowing guidelines for using ARI and AMI as external validation indices: ARI\nshould be used when the reference clustering has large equal sized clusters;\nAMI should be used when the reference clustering is unbalanced and there exist\nsmall clusters.\n", "versions": [{"version": "v1", "created": "Thu, 3 Dec 2015 23:56:55 GMT"}], "update_date": "2015-12-07", "authors_parsed": [["Romano", "Simone", ""], ["Vinh", "Nguyen Xuan", ""], ["Bailey", "James", ""], ["Verspoor", "Karin", ""]]}, {"id": "1512.01408", "submitter": "John Pearson", "authors": "Xin (Cindy) Chen, Jeffrey M Beck and John M Pearson", "title": "Neuron's Eye View: Inferring Features of Complex Stimuli from Neural\n  Responses", "comments": "Updated author list and added new results", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML q-bio.NC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Experiments that study neural encoding of stimuli at the level of individual\nneurons typically choose a small set of features present in the world ---\ncontrast and luminance for vision, pitch and intensity for sound --- and\nassemble a stimulus set that systematically varies along these dimensions.\nSubsequent analysis of neural responses to these stimuli typically focuses on\nregression models, with experimenter-controlled features as predictors and\nspike counts or firing rates as responses. Unfortunately, this approach\nrequires knowledge in advance about the relevant features coded by a given\npopulation of neurons. For domains as complex as social interaction or natural\nmovement, however, the relevant feature space is poorly understood, and an\narbitrary \\emph{a priori} choice of features may give rise to confirmation\nbias. Here, we present a Bayesian model for exploratory data analysis that is\ncapable of automatically identifying the features present in unstructured\nstimuli based solely on neuronal responses. Our approach is unique within the\nclass of latent state space models of neural activity in that it assumes that\nfiring rates of neurons are sensitive to multiple discrete time-varying\nfeatures tied to the \\emph{stimulus}, each of which has Markov (or semi-Markov)\ndynamics. That is, we are modeling neural activity as driven by multiple\nsimultaneous stimulus features rather than intrinsic neural dynamics. We derive\na fast variational Bayesian inference algorithm and show that it correctly\nrecovers hidden features in synthetic data, as well as ground-truth stimulus\nfeatures in a prototypical neural dataset. To demonstrate the utility of the\nalgorithm, we also apply it to cluster neural responses and demonstrate\nsuccessful recovery of features corresponding to monkeys and faces in the image\nset.\n", "versions": [{"version": "v1", "created": "Fri, 4 Dec 2015 14:00:43 GMT"}, {"version": "v2", "created": "Mon, 21 Nov 2016 20:19:50 GMT"}], "update_date": "2016-11-22", "authors_parsed": [["Xin", "", "", "Cindy"], ["Chen", "", ""], ["Beck", "Jeffrey M", ""], ["Pearson", "John M", ""]]}, {"id": "1512.01631", "submitter": "Xiaohan Yan", "authors": "Xiaohan Yan and Jacob Bien", "title": "Hierarchical Sparse Modeling: A Choice of Two Group Lasso Formulations", "comments": "30 pages, 13 figures", "journal-ref": "Statist. Sci. 32 (2017), no. 4, 531--560", "doi": "10.1214/17-STS622", "report-no": null, "categories": "stat.ME math.ST stat.CO stat.ML stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Demanding sparsity in estimated models has become a routine practice in\nstatistics. In many situations, we wish to require that the sparsity patterns\nattained honor certain problem-specific constraints. Hierarchical sparse\nmodeling (HSM) refers to situations in which these constraints specify that one\nset of parameters be set to zero whenever another is set to zero. In recent\nyears, numerous papers have developed convex regularizers for this form of\nsparsity structure, which arises in many areas of statistics including\ninteraction modeling, time series analysis, and covariance estimation. In this\npaper, we observe that these methods fall into two frameworks, the group lasso\n(GL) and latent overlapping group lasso (LOG), which have not been\nsystematically compared in the context of HSM. The purpose of this paper is to\nprovide a side-by-side comparison of these two frameworks for HSM in terms of\ntheir statistical properties and computational efficiency. We call special\nattention to GL's more aggressive shrinkage of parameters deep in the\nhierarchy, a property not shared by LOG. In terms of computation, we introduce\na finite-step algorithm that exactly solves the proximal operator of LOG for a\ncertain simple HSM structure; we later exploit this to develop a novel\npath-based block coordinate descent scheme for general HSM structures. Both\nalgorithms greatly improve the computational performance of LOG. Finally, we\ncompare the two methods in the context of covariance estimation, where we\nintroduce a new sparsely-banded estimator using LOG, which we show achieves the\nstatistical advantages of an existing GL-based method but is simpler to express\nand more efficient to compute.\n", "versions": [{"version": "v1", "created": "Sat, 5 Dec 2015 07:00:54 GMT"}, {"version": "v2", "created": "Tue, 29 Nov 2016 02:13:49 GMT"}, {"version": "v3", "created": "Mon, 3 Jul 2017 04:03:45 GMT"}, {"version": "v4", "created": "Wed, 29 Nov 2017 20:05:56 GMT"}], "update_date": "2017-12-04", "authors_parsed": [["Yan", "Xiaohan", ""], ["Bien", "Jacob", ""]]}, {"id": "1512.01639", "submitter": "Krzysztof Wo{\\l}k", "authors": "Krzysztof Wo{\\l}k, Krzysztof Marasek", "title": "PJAIT Systems for the IWSLT 2015 Evaluation Campaign Enhanced by\n  Comparable Corpora", "comments": null, "journal-ref": "Proceedings of the 12th International Workshop on Spoken Language\n  Translation, Da Nang, Vietnam, December 3-4, 2015, p.101-104", "doi": null, "report-no": null, "categories": "cs.CL stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we attempt to improve Statistical Machine Translation (SMT)\nsystems on a very diverse set of language pairs (in both directions): Czech -\nEnglish, Vietnamese - English, French - English and German - English. To\naccomplish this, we performed translation model training, created adaptations\nof training settings for each language pair, and obtained comparable corpora\nfor our SMT systems. Innovative tools and data adaptation techniques were\nemployed. The TED parallel text corpora for the IWSLT 2015 evaluation campaign\nwere used to train language models, and to develop, tune, and test the system.\nIn addition, we prepared Wikipedia-based comparable corpora for use with our\nSMT system. This data was specified as permissible for the IWSLT 2015\nevaluation. We explored the use of domain adaptation techniques, symmetrized\nword alignment models, the unsupervised transliteration models and the KenLM\nlanguage modeling tool. To evaluate the effects of different preparations on\ntranslation results, we conducted experiments and used the BLEU, NIST and TER\nmetrics. Our results indicate that our approach produced a positive impact on\nSMT quality.\n", "versions": [{"version": "v1", "created": "Sat, 5 Dec 2015 08:55:31 GMT"}], "update_date": "2015-12-08", "authors_parsed": [["Wo\u0142k", "Krzysztof", ""], ["Marasek", "Krzysztof", ""]]}, {"id": "1512.01641", "submitter": "Krzysztof Wo{\\l}k", "authors": "Krzysztof Wo{\\l}k, Krzysztof Marasek", "title": "Unsupervised comparable corpora preparation and exploration for\n  bi-lingual translation equivalents", "comments": "arXiv admin note: text overlap with arXiv:1509.08639", "journal-ref": "Proceedings of the 12th IWSLT, Da Nang, Vietnam, December 3-4,\n  2015, p.118-125", "doi": null, "report-no": null, "categories": "cs.CL stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The multilingual nature of the world makes translation a crucial requirement\ntoday. Parallel dictionaries constructed by humans are a widely-available\nresource, but they are limited and do not provide enough coverage for good\nquality translation purposes, due to out-of-vocabulary words and neologisms.\nThis motivates the use of statistical translation systems, which are\nunfortunately dependent on the quantity and quality of training data. Such\nsystems have a very limited availability especially for some languages and very\nnarrow text domains. In this research we present our improvements to current\ncomparable corpora mining methodologies by re- implementation of the comparison\nalgorithms (using Needleman-Wunch algorithm), introduction of a tuning script\nand computation time improvement by GPU acceleration. Experiments are carried\nout on bilingual data extracted from the Wikipedia, on various domains. For the\nWikipedia itself, additional cross-lingual comparison heuristics were\nintroduced. The modifications made a positive impact on the quality and\nquantity of mined data and on the translation quality.\n", "versions": [{"version": "v1", "created": "Sat, 5 Dec 2015 08:59:28 GMT"}], "update_date": "2015-12-08", "authors_parsed": [["Wo\u0142k", "Krzysztof", ""], ["Marasek", "Krzysztof", ""]]}, {"id": "1512.01665", "submitter": "Pengyu Wang", "authors": "Pengyu Wang and Phil Blunsom", "title": "Stochastic Collapsed Variational Inference for Hidden Markov Models", "comments": "NIPS Workshop on Time Series, 2015", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Stochastic variational inference for collapsed models has recently been\nsuccessfully applied to large scale topic modelling. In this paper, we propose\na stochastic collapsed variational inference algorithm for hidden Markov\nmodels, in a sequential data setting. Given a collapsed hidden Markov Model, we\nbreak its long Markov chain into a set of short subchains. We propose a novel\nsum-product algorithm to update the posteriors of the subchains, taking into\naccount their boundary transitions due to the sequential dependencies. Our\nexperiments on two discrete datasets show that our collapsed algorithm is\nscalable to very large datasets, memory efficient and significantly more\naccurate than the existing uncollapsed algorithm.\n", "versions": [{"version": "v1", "created": "Sat, 5 Dec 2015 13:39:18 GMT"}], "update_date": "2015-12-08", "authors_parsed": [["Wang", "Pengyu", ""], ["Blunsom", "Phil", ""]]}, {"id": "1512.01666", "submitter": "Pengyu Wang", "authors": "Pengyu Wang and Phil Blunsom", "title": "Stochastic Collapsed Variational Inference for Sequential Data", "comments": "NIPS Workshop on Advances in Approximate Bayesian Inference, 2015", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Stochastic variational inference for collapsed models has recently been\nsuccessfully applied to large scale topic modelling. In this paper, we propose\na stochastic collapsed variational inference algorithm in the sequential data\nsetting. Our algorithm is applicable to both finite hidden Markov models and\nhierarchical Dirichlet process hidden Markov models, and to any datasets\ngenerated by emission distributions in the exponential family. Our experiment\nresults on two discrete datasets show that our inference is both more efficient\nand more accurate than its uncollapsed version, stochastic variational\ninference.\n", "versions": [{"version": "v1", "created": "Sat, 5 Dec 2015 13:45:47 GMT"}], "update_date": "2015-12-08", "authors_parsed": [["Wang", "Pengyu", ""], ["Blunsom", "Phil", ""]]}, {"id": "1512.01708", "submitter": "Soham De", "authors": "Soham De, Gavin Taylor, Tom Goldstein", "title": "Variance Reduction for Distributed Stochastic Gradient Descent", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.DC math.OC stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Variance reduction (VR) methods boost the performance of stochastic gradient\ndescent (SGD) by enabling the use of larger, constant stepsizes and preserving\nlinear convergence rates. However, current variance reduced SGD methods require\neither high memory usage or an exact gradient computation (using the entire\ndataset) at the end of each epoch. This limits the use of VR methods in\npractical distributed settings. In this paper, we propose a variance reduction\nmethod, called VR-lite, that does not require full gradient computations or\nextra storage. We explore distributed synchronous and asynchronous variants\nthat are scalable and remain stable with low communication frequency. We\nempirically compare both the sequential and distributed algorithms to\nstate-of-the-art stochastic optimization methods, and find that our proposed\nalgorithms perform favorably to other stochastic methods.\n", "versions": [{"version": "v1", "created": "Sat, 5 Dec 2015 22:48:40 GMT"}, {"version": "v2", "created": "Fri, 7 Apr 2017 04:07:29 GMT"}], "update_date": "2017-04-10", "authors_parsed": [["De", "Soham", ""], ["Taylor", "Gavin", ""], ["Goldstein", "Tom", ""]]}, {"id": "1512.01834", "submitter": "SueYeon Chung", "authors": "SueYeon Chung, Daniel D. Lee, Haim Sompolinsky", "title": "Linear Readout of Object Manifolds", "comments": "5 pages, 3 figures, accepted in Physical Review E as Rapid\n  Communication on 14th May. 2016", "journal-ref": "Phys. Rev. E 93, 060301 (R) (2016)", "doi": "10.1103/PhysRevE.93.060301", "report-no": null, "categories": "cond-mat.dis-nn cond-mat.stat-mech cs.NE q-bio.NC stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Objects are represented in sensory systems by continuous manifolds due to\nsensitivity of neuronal responses to changes in physical features such as\nlocation, orientation, and intensity. What makes certain sensory\nrepresentations better suited for invariant decoding of objects by downstream\nnetworks? We present a theory that characterizes the ability of a linear\nreadout network, the perceptron, to classify objects from variable neural\nresponses. We show how the readout perceptron capacity depends on the\ndimensionality, size, and shape of the object manifolds in its input neural\nrepresentation.\n", "versions": [{"version": "v1", "created": "Sun, 6 Dec 2015 21:00:02 GMT"}, {"version": "v2", "created": "Sun, 21 Aug 2016 05:50:13 GMT"}], "update_date": "2016-08-23", "authors_parsed": [["Chung", "SueYeon", ""], ["Lee", "Daniel D.", ""], ["Sompolinsky", "Haim", ""]]}, {"id": "1512.01845", "submitter": "Alex Beutel", "authors": "Chao-Yuan Wu, Alex Beutel, Amr Ahmed, Alexander J. Smola", "title": "Explaining reviews and ratings with PACO: Poisson Additive Co-Clustering", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Understanding a user's motivations provides valuable information beyond the\nability to recommend items. Quite often this can be accomplished by perusing\nboth ratings and review texts, since it is the latter where the reasoning for\nspecific preferences is explicitly expressed.\n  Unfortunately matrix factorization approaches to recommendation result in\nlarge, complex models that are difficult to interpret and give recommendations\nthat are hard to clearly explain to users. In contrast, in this paper, we\nattack this problem through succinct additive co-clustering. We devise a novel\nBayesian technique for summing co-clusterings of Poisson distributions. With\nthis novel technique we propose a new Bayesian model for joint collaborative\nfiltering of ratings and text reviews through a sum of simple co-clusterings.\nThe simple structure of our model yields easily interpretable recommendations.\nEven with a simple, succinct structure, our model outperforms competitors in\nterms of predicting ratings with reviews.\n", "versions": [{"version": "v1", "created": "Sun, 6 Dec 2015 22:13:46 GMT"}], "update_date": "2015-12-08", "authors_parsed": [["Wu", "Chao-Yuan", ""], ["Beutel", "Alex", ""], ["Ahmed", "Amr", ""], ["Smola", "Alexander J.", ""]]}, {"id": "1512.01904", "submitter": "Chengtao Li", "authors": "Chengtao Li, Suvrit Sra and Stefanie Jegelka", "title": "Gauss quadrature for matrix inverse forms with applications", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.NA", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a framework for accelerating a spectrum of machine learning\nalgorithms that require computation of bilinear inverse forms $u^\\top A^{-1}u$,\nwhere $A$ is a positive definite matrix and $u$ a given vector. Our framework\nis built on Gauss-type quadrature and easily scales to large, sparse matrices.\nFurther, it allows retrospective computation of lower and upper bounds on\n$u^\\top A^{-1}u$, which in turn accelerates several algorithms. We prove that\nthese bounds tighten iteratively and converge at a linear (geometric) rate. To\nour knowledge, ours is the first work to demonstrate these key properties of\nGauss-type quadrature, which is a classical and deeply studied topic. We\nillustrate empirical consequences of our results by using quadrature to\naccelerate machine learning tasks involving determinantal point processes and\nsubmodular optimization, and observe tremendous speedups in several instances.\n", "versions": [{"version": "v1", "created": "Mon, 7 Dec 2015 04:13:45 GMT"}, {"version": "v2", "created": "Sat, 28 May 2016 04:12:21 GMT"}], "update_date": "2016-05-31", "authors_parsed": [["Li", "Chengtao", ""], ["Sra", "Suvrit", ""], ["Jegelka", "Stefanie", ""]]}, {"id": "1512.01947", "submitter": "Ricardo Pio Monti", "authors": "Ricardo Pio Monti, Christoforos Anagnostopoulos, Giovanni Montana", "title": "Learning population and subject-specific brain connectivity networks via\n  Mixed Neighborhood Selection", "comments": "32 pages, 6 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In neuroimaging data analysis, Gaussian graphical models are often used to\nmodel statistical dependencies across spatially remote brain regions known as\nfunctional connectivity. Typically, data is collected across a cohort of\nsubjects and the scientific objectives consist of estimating population and\nsubject-specific graphical models. A third objective that is often overlooked\ninvolves quantifying inter-subject variability and thus identifying regions or\nsub-networks that demonstrate heterogeneity across subjects. Such information\nis fundamental in order to thoroughly understand the human connectome. We\npropose Mixed Neighborhood Selection in order to simultaneously address the\nthree aforementioned objectives. By recasting covariance selection as a\nneighborhood selection problem we are able to efficiently learn the topology of\neach node. We introduce an additional mixed effect component to neighborhood\nselection in order to simultaneously estimate a graphical model for the\npopulation of subjects as well as for each individual subject. The proposed\nmethod is validated empirically through a series of simulations and applied to\nresting state data for healthy subjects taken from the ABIDE consortium.\n", "versions": [{"version": "v1", "created": "Mon, 7 Dec 2015 09:07:35 GMT"}], "update_date": "2015-12-08", "authors_parsed": [["Monti", "Ricardo Pio", ""], ["Anagnostopoulos", "Christoforos", ""], ["Montana", "Giovanni", ""]]}, {"id": "1512.02016", "submitter": "Bei Chen", "authors": "Bei Chen, Ning Chen, Jun Zhu, Jiaming Song, Bo Zhang", "title": "Discriminative Nonparametric Latent Feature Relational Models with Data\n  Augmentation", "comments": "Accepted by AAAI 2016", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a discriminative nonparametric latent feature relational model\n(LFRM) for link prediction to automatically infer the dimensionality of latent\nfeatures. Under the generic RegBayes (regularized Bayesian inference)\nframework, we handily incorporate the prediction loss with probabilistic\ninference of a Bayesian model; set distinct regularization parameters for\ndifferent types of links to handle the imbalance issue in real networks; and\nunify the analysis of both the smooth logistic log-loss and the piecewise\nlinear hinge loss. For the nonconjugate posterior inference, we present a\nsimple Gibbs sampler via data augmentation, without making restricting\nassumptions as done in variational methods. We further develop an approximate\nsampler using stochastic gradient Langevin dynamics to handle large networks\nwith hundreds of thousands of entities and millions of links, orders of\nmagnitude larger than what existing LFRM models can process. Extensive studies\non various real networks show promising performance.\n", "versions": [{"version": "v1", "created": "Mon, 7 Dec 2015 12:37:41 GMT"}], "update_date": "2015-12-08", "authors_parsed": [["Chen", "Bei", ""], ["Chen", "Ning", ""], ["Zhu", "Jun", ""], ["Song", "Jiaming", ""], ["Zhang", "Bo", ""]]}, {"id": "1512.02063", "submitter": "Guilherme Fran\\c{c}a", "authors": "Guilherme Fran\\c{c}a, Jos\\'e Bento", "title": "An Explicit Rate Bound for the Over-Relaxed ADMM", "comments": "IEEE International Symposium on Information Theory (ISIT), 2016", "journal-ref": null, "doi": "10.1109/ISIT.2016.7541670", "report-no": null, "categories": "stat.ML math.OC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The framework of Integral Quadratic Constraints of Lessard et al. (2014)\nreduces the computation of upper bounds on the convergence rate of several\noptimization algorithms to semi-definite programming (SDP). Followup work by\nNishihara et al. (2015) applies this technique to the entire family of\nover-relaxed Alternating Direction Method of Multipliers (ADMM). Unfortunately,\nthey only provide an explicit error bound for sufficiently large values of some\nof the parameters of the problem, leaving the computation for the general case\nas a numerical optimization problem. In this paper we provide an exact\nanalytical solution to this SDP and obtain a general and explicit upper bound\non the convergence rate of the entire family of over-relaxed ADMM. Furthermore,\nwe demonstrate that it is not possible to extract from this SDP a general bound\nbetter than ours. We end with a few numerical illustrations of our result and a\ncomparison between the convergence rate we obtain for the ADMM with known\nconvergence rates for the Gradient Descent.\n", "versions": [{"version": "v1", "created": "Mon, 7 Dec 2015 14:31:29 GMT"}, {"version": "v2", "created": "Thu, 31 Mar 2016 18:11:21 GMT"}, {"version": "v3", "created": "Mon, 5 Mar 2018 14:33:10 GMT"}], "update_date": "2018-03-06", "authors_parsed": [["Fran\u00e7a", "Guilherme", ""], ["Bento", "Jos\u00e9", ""]]}, {"id": "1512.02097", "submitter": "Teng Qiu", "authors": "Teng Qiu, Yongjie Li", "title": "Clustering by Deep Nearest Neighbor Descent (D-NND): A Density-based\n  Parameter-Insensitive Clustering Method", "comments": "28 pages, 14 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.CV cs.LG stat.CO stat.ME", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Most density-based clustering methods largely rely on how well the underlying\ndensity is estimated. However, density estimation itself is also a challenging\nproblem, especially the determination of the kernel bandwidth. A large\nbandwidth could lead to the over-smoothed density estimation in which the\nnumber of density peaks could be less than the true clusters, while a small\nbandwidth could lead to the under-smoothed density estimation in which spurious\ndensity peaks, or called the \"ripple noise\", would be generated in the\nestimated density. In this paper, we propose a density-based hierarchical\nclustering method, called the Deep Nearest Neighbor Descent (D-NND), which\ncould learn the underlying density structure layer by layer and capture the\ncluster structure at the same time. The over-smoothed density estimation could\nbe largely avoided and the negative effect of the under-estimated cases could\nbe also largely reduced. Overall, D-NND presents not only the strong capability\nof discovering the underlying cluster structure but also the remarkable\nreliability due to its insensitivity to parameters.\n", "versions": [{"version": "v1", "created": "Mon, 7 Dec 2015 15:47:49 GMT"}], "update_date": "2015-12-08", "authors_parsed": [["Qiu", "Teng", ""], ["Li", "Yongjie", ""]]}, {"id": "1512.02134", "submitter": "Nikolaus Mayer", "authors": "Nikolaus Mayer, Eddy Ilg, Philip H\\\"ausser, Philipp Fischer, Daniel\n  Cremers, Alexey Dosovitskiy, Thomas Brox", "title": "A Large Dataset to Train Convolutional Networks for Disparity, Optical\n  Flow, and Scene Flow Estimation", "comments": "Includes supplementary material", "journal-ref": null, "doi": "10.1109/CVPR.2016.438", "report-no": null, "categories": "cs.CV cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recent work has shown that optical flow estimation can be formulated as a\nsupervised learning task and can be successfully solved with convolutional\nnetworks. Training of the so-called FlowNet was enabled by a large\nsynthetically generated dataset. The present paper extends the concept of\noptical flow estimation via convolutional networks to disparity and scene flow\nestimation. To this end, we propose three synthetic stereo video datasets with\nsufficient realism, variation, and size to successfully train large networks.\nOur datasets are the first large-scale datasets to enable training and\nevaluating scene flow methods. Besides the datasets, we present a convolutional\nnetwork for real-time disparity estimation that provides state-of-the-art\nresults. By combining a flow and disparity estimation network and training it\njointly, we demonstrate the first scene flow estimation with a convolutional\nnetwork.\n", "versions": [{"version": "v1", "created": "Mon, 7 Dec 2015 17:35:00 GMT"}], "update_date": "2018-01-18", "authors_parsed": [["Mayer", "Nikolaus", ""], ["Ilg", "Eddy", ""], ["H\u00e4usser", "Philip", ""], ["Fischer", "Philipp", ""], ["Cremers", "Daniel", ""], ["Dosovitskiy", "Alexey", ""], ["Brox", "Thomas", ""]]}, {"id": "1512.02188", "submitter": "Tae-Hyun Oh", "authors": "Tae-Hyun Oh, Yasuyuki Matsushita, In So Kweon, David Wipf", "title": "Pseudo-Bayesian Robust PCA: Algorithms and Analyses", "comments": "Journal version of NIPS 2016. Submitted to TPAMI", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Commonly used in computer vision and other applications, robust PCA\nrepresents an algorithmic attempt to reduce the sensitivity of classical PCA to\noutliers. The basic idea is to learn a decomposition of some data matrix of\ninterest into low rank and sparse components, the latter representing unwanted\noutliers. Although the resulting optimization problem is typically NP-hard,\nconvex relaxations provide a computationally-expedient alternative with\ntheoretical support. However, in practical regimes performance guarantees break\ndown and a variety of non-convex alternatives, including Bayesian-inspired\nmodels, have been proposed to boost estimation quality. Unfortunately though,\nwithout additional a priori knowledge none of these methods can significantly\nexpand the critical operational range such that exact principal subspace\nrecovery is possible. Into this mix we propose a novel pseudo-Bayesian\nalgorithm that explicitly compensates for design weaknesses in many existing\nnon-convex approaches leading to state-of-the-art performance with a sound\nanalytical foundation. Surprisingly, our algorithm can even outperform convex\nmatrix completion despite the fact that the latter is provided with perfect\nknowledge of which entries are not corrupted.\n", "versions": [{"version": "v1", "created": "Mon, 7 Dec 2015 19:43:54 GMT"}, {"version": "v2", "created": "Fri, 7 Oct 2016 16:08:25 GMT"}], "update_date": "2016-10-10", "authors_parsed": [["Oh", "Tae-Hyun", ""], ["Matsushita", "Yasuyuki", ""], ["Kweon", "In So", ""], ["Wipf", "David", ""]]}, {"id": "1512.02271", "submitter": "Damon McDougall", "authors": "Damon McDougall and Richard Moore", "title": "Optimal strategies for the control of autonomous vehicles in data\n  assimilation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.OC stat.CO stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a method to compute optimal control paths for autonomous vehicles\ndeployed for the purpose of inferring a velocity field. In addition to being\nadvected by the flow, the vehicles are able to effect a fixed relative speed\nwith arbitrary control over direction. It is this direction that is used as the\nbasis for the locally optimal control algorithm presented here, with objective\nformed from the variance trace of the expected posterior distribution. We\npresent results for linear flows near hyperbolic fixed points.\n", "versions": [{"version": "v1", "created": "Mon, 7 Dec 2015 22:31:40 GMT"}], "update_date": "2015-12-09", "authors_parsed": [["McDougall", "Damon", ""], ["Moore", "Richard", ""]]}, {"id": "1512.02306", "submitter": "Barbara Engelhardt", "authors": "Ashlee Valente, Geoffrey Ginsburg, Barbara E Engelhardt", "title": "Nonparametric Reduced-Rank Regression for Multi-SNP, Multi-Trait\n  Association Mapping", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP q-bio.GN stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Genome-wide association studies have proven to be essential for understanding\nthe genetic basis of disease. However, many complex traits---personality\ntraits, facial features, disease subtyping---are inherently high-dimensional,\nimpeding simple approaches to association mapping. We developed a nonparametric\nBayesian reduced rank regression model for multi-SNP, multi-trait association\nmapping that does not require the rank of the linear subspace to be specified.\nWe show in simulations and real data that our model shares strength over SNPs\nand over correlated traits, improving statistical power to identify genetic\nassociations with an interpretable, SNP-supervised low-dimensional linear\nprojection of the high-dimensional phenotype. On the HapMap phase 3 gene\nexpression QTL study data, we identify pleiotropic expression QTLs that\nclassical univariate tests are underpowered to find and that two step\napproaches cannot recover. Our Python software, BERRRI, is publicly available\nat GitHub: https://github.com/ashlee1031/BERRRI.\n", "versions": [{"version": "v1", "created": "Tue, 8 Dec 2015 02:25:12 GMT"}], "update_date": "2015-12-09", "authors_parsed": [["Valente", "Ashlee", ""], ["Ginsburg", "Geoffrey", ""], ["Engelhardt", "Barbara E", ""]]}, {"id": "1512.02337", "submitter": "David Steurer", "authors": "Samuel B. Hopkins, Tselil Schramm, Jonathan Shi, David Steurer", "title": "Fast spectral algorithms from sum-of-squares proofs: tensor\n  decomposition and planted sparse vectors", "comments": "62 pages, title changed, to appear at STOC 2016", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.CC cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider two problems that arise in machine learning applications: the\nproblem of recovering a planted sparse vector in a random linear subspace and\nthe problem of decomposing a random low-rank overcomplete 3-tensor. For both\nproblems, the best known guarantees are based on the sum-of-squares method. We\ndevelop new algorithms inspired by analyses of the sum-of-squares method. Our\nalgorithms achieve the same or similar guarantees as sum-of-squares for these\nproblems but the running time is significantly faster.\n  For the planted sparse vector problem, we give an algorithm with running time\nnearly linear in the input size that approximately recovers a planted sparse\nvector with up to constant relative sparsity in a random subspace of $\\mathbb\nR^n$ of dimension up to $\\tilde \\Omega(\\sqrt n)$. These recovery guarantees\nmatch the best known ones of Barak, Kelner, and Steurer (STOC 2014) up to\nlogarithmic factors.\n  For tensor decomposition, we give an algorithm with running time close to\nlinear in the input size (with exponent $\\approx 1.086$) that approximately\nrecovers a component of a random 3-tensor over $\\mathbb R^n$ of rank up to\n$\\tilde \\Omega(n^{4/3})$. The best previous algorithm for this problem due to\nGe and Ma (RANDOM 2015) works up to rank $\\tilde \\Omega(n^{3/2})$ but requires\nquasipolynomial time.\n", "versions": [{"version": "v1", "created": "Tue, 8 Dec 2015 05:49:07 GMT"}, {"version": "v2", "created": "Wed, 3 Feb 2016 18:01:12 GMT"}], "update_date": "2016-02-04", "authors_parsed": [["Hopkins", "Samuel B.", ""], ["Schramm", "Tselil", ""], ["Shi", "Jonathan", ""], ["Steurer", "David", ""]]}, {"id": "1512.02479", "submitter": "Gr\\'egoire Montavon", "authors": "Gr\\'egoire Montavon, Sebastian Bach, Alexander Binder, Wojciech Samek,\n  Klaus-Robert M\\\"uller", "title": "Explaining NonLinear Classification Decisions with Deep Taylor\n  Decomposition", "comments": "20 pages, 15 figures", "journal-ref": null, "doi": "10.1016/j.patcog.2016.11.008", "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Nonlinear methods such as Deep Neural Networks (DNNs) are the gold standard\nfor various challenging machine learning problems, e.g., image classification,\nnatural language processing or human action recognition. Although these methods\nperform impressively well, they have a significant disadvantage, the lack of\ntransparency, limiting the interpretability of the solution and thus the scope\nof application in practice. Especially DNNs act as black boxes due to their\nmultilayer nonlinear structure. In this paper we introduce a novel methodology\nfor interpreting generic multilayer neural networks by decomposing the network\nclassification decision into contributions of its input elements. Although our\nfocus is on image classification, the method is applicable to a broad set of\ninput data, learning tasks and network architectures. Our method is based on\ndeep Taylor decomposition and efficiently utilizes the structure of the network\nby backpropagating the explanations from the output to the input layer. We\nevaluate the proposed method empirically on the MNIST and ILSVRC data sets.\n", "versions": [{"version": "v1", "created": "Tue, 8 Dec 2015 14:25:29 GMT"}], "update_date": "2017-11-15", "authors_parsed": [["Montavon", "Gr\u00e9goire", ""], ["Bach", "Sebastian", ""], ["Binder", "Alexander", ""], ["Samek", "Wojciech", ""], ["M\u00fcller", "Klaus-Robert", ""]]}, {"id": "1512.02543", "submitter": "Creighton Heaukulani", "authors": "Creighton Heaukulani and Daniel M. Roy", "title": "Gibbs-type Indian buffet processes", "comments": "27 pages, 5 figures", "journal-ref": "Advanced publication. Bayesian Analysis (2019)", "doi": "10.1214/19-BA1166", "report-no": null, "categories": "stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We investigate a class of feature allocation models that generalize the\nIndian buffet process and are parameterized by Gibbs-type random measures. Two\nexisting classes are contained as special cases: the original two-parameter\nIndian buffet process, corresponding to the Dirichlet process, and the stable\n(or three-parameter) Indian buffet process, corresponding to the Pitman--Yor\nprocess. Asymptotic behavior of the Gibbs-type partitions, such as power laws\nholding for the number of latent clusters, translates into analogous\ncharacteristics for this class of Gibbs-type feature allocation models. Despite\ncontaining several different distinct subclasses, the properties of Gibbs-type\npartitions allow us to develop a black-box procedure for posterior inference\nwithin any subclass of models. Through numerical experiments, we compare and\ncontrast a few of these subclasses and highlight the utility of varying\npower-law behaviors in the latent features.\n", "versions": [{"version": "v1", "created": "Tue, 8 Dec 2015 17:01:05 GMT"}, {"version": "v2", "created": "Sun, 10 Nov 2019 08:36:49 GMT"}], "update_date": "2019-11-12", "authors_parsed": [["Heaukulani", "Creighton", ""], ["Roy", "Daniel M.", ""]]}, {"id": "1512.02565", "submitter": "William Fithian", "authors": "William Fithian, Jonathan Taylor, Robert Tibshirani, and Ryan\n  Tibshirani", "title": "Selective Sequential Model Selection", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Many model selection algorithms produce a path of fits specifying a sequence\nof increasingly complex models. Given such a sequence and the data used to\nproduce them, we consider the problem of choosing the least complex model that\nis not falsified by the data. Extending the selected-model tests of Fithian et\nal. (2014), we construct p-values for each step in the path which account for\nthe adaptive selection of the model path using the data. In the case of linear\nregression, we propose two specific tests, the max-t test for forward stepwise\nregression (generalizing a proposal of Buja and Brown (2014)), and the\nnext-entry test for the lasso. These tests improve on the power of the\nsaturated-model test of Tibshirani et al. (2014), sometimes dramatically. In\naddition, our framework extends beyond linear regression to a much more general\nclass of parametric and nonparametric model selection problems.\n  To select a model, we can feed our single-step p-values as inputs into\nsequential stopping rules such as those proposed by G'Sell et al. (2013) and Li\nand Barber (2015), achieving control of the familywise error rate or false\ndiscovery rate (FDR) as desired. The FDR-controlling rules require the null\np-values to be independent of each other and of the non-null p-values, a\ncondition not satisfied by the saturated-model p-values of Tibshirani et al.\n(2014). We derive intuitive and general sufficient conditions for independence,\nand show that our proposed constructions yield independent p-values.\n", "versions": [{"version": "v1", "created": "Tue, 8 Dec 2015 17:52:16 GMT"}], "update_date": "2015-12-09", "authors_parsed": [["Fithian", "William", ""], ["Taylor", "Jonathan", ""], ["Tibshirani", "Robert", ""], ["Tibshirani", "Ryan", ""]]}, {"id": "1512.02728", "submitter": "Abhimanu Kumar", "authors": "Abhimanu Kumar and Pengtao Xie and Junming Yin and Eric P. Xing", "title": "Distributed Training of Deep Neural Networks with Theoretical Analysis:\n  Under SSP Setting", "comments": "The paper needs more refinement", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG math.OC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a distributed approach to train deep neural networks (DNNs), which\nhas guaranteed convergence theoretically and great scalability empirically:\nclose to 6 times faster on instance of ImageNet data set when run with 6\nmachines. The proposed scheme is close to optimally scalable in terms of number\nof machines, and guaranteed to converge to the same optima as the undistributed\nsetting. The convergence and scalability of the distributed setting is shown\nempirically across different datasets (TIMIT and ImageNet) and machine learning\ntasks (image classification and phoneme extraction). The convergence analysis\nprovides novel insights into this complex learning scheme, including: 1)\nlayerwise convergence, and 2) convergence of the weights in probability.\n", "versions": [{"version": "v1", "created": "Wed, 9 Dec 2015 02:46:28 GMT"}, {"version": "v2", "created": "Fri, 11 Dec 2015 21:51:51 GMT"}], "update_date": "2016-10-04", "authors_parsed": [["Kumar", "Abhimanu", ""], ["Xie", "Pengtao", ""], ["Yin", "Junming", ""], ["Xing", "Eric P.", ""]]}, {"id": "1512.02752", "submitter": "Qi Mao", "authors": "Qi Mao, Li Wang, Ivor W. Tsang, Yijun Sun", "title": "A Novel Regularized Principal Graph Learning Framework on Explicit Graph\n  Representation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Many scientific datasets are of high dimension, and the analysis usually\nrequires visual manipulation by retaining the most important structures of\ndata. Principal curve is a widely used approach for this purpose. However, many\nexisting methods work only for data with structures that are not\nself-intersected, which is quite restrictive for real applications. A few\nmethods can overcome the above problem, but they either require complicated\nhuman-made rules for a specific task with lack of convergence guarantee and\nadaption flexibility to different tasks, or cannot obtain explicit structures\nof data. To address these issues, we develop a new regularized principal graph\nlearning framework that captures the local information of the underlying graph\nstructure based on reversed graph embedding. As showcases, models that can\nlearn a spanning tree or a weighted undirected $\\ell_1$ graph are proposed, and\na new learning algorithm is developed that learns a set of principal points and\na graph structure from data, simultaneously. The new algorithm is simple with\nguaranteed convergence. We then extend the proposed framework to deal with\nlarge-scale data. Experimental results on various synthetic and six real world\ndatasets show that the proposed method compares favorably with baselines and\ncan uncover the underlying structure correctly.\n", "versions": [{"version": "v1", "created": "Wed, 9 Dec 2015 04:57:18 GMT"}, {"version": "v2", "created": "Sun, 17 Jan 2016 14:34:14 GMT"}], "update_date": "2016-01-19", "authors_parsed": [["Mao", "Qi", ""], ["Wang", "Li", ""], ["Tsang", "Ivor W.", ""], ["Sun", "Yijun", ""]]}, {"id": "1512.02866", "submitter": "Liran Szlak", "authors": "Jonathan Rosenski, Ohad Shamir, Liran Szlak", "title": "Multi-Player Bandits -- a Musical Chairs Approach", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider a variant of the stochastic multi-armed bandit problem, where\nmultiple players simultaneously choose from the same set of arms and may\ncollide, receiving no reward. This setting has been motivated by problems\narising in cognitive radio networks, and is especially challenging under the\nrealistic assumption that communication between players is limited. We provide\na communication-free algorithm (Musical Chairs) which attains constant regret\nwith high probability, as well as a sublinear-regret, communication-free\nalgorithm (Dynamic Musical Chairs) for the more difficult setting of players\ndynamically entering and leaving throughout the game. Moreover, both algorithms\ndo not require prior knowledge of the number of players. To the best of our\nknowledge, these are the first communication-free algorithms with these types\nof formal guarantees. We also rigorously compare our algorithms to previous\nworks, and complement our theoretical findings with experiments.\n", "versions": [{"version": "v1", "created": "Wed, 9 Dec 2015 14:18:16 GMT"}], "update_date": "2015-12-10", "authors_parsed": [["Rosenski", "Jonathan", ""], ["Shamir", "Ohad", ""], ["Szlak", "Liran", ""]]}, {"id": "1512.02896", "submitter": "Farid M. Naini", "authors": "Farid M. Naini, Jayakrishnan Unnikrishnan, Patrick Thiran, Martin\n  Vetterli", "title": "Where You Are Is Who You Are: User Identification by Matching Statistics", "comments": null, "journal-ref": null, "doi": "10.1109/TIFS.2015.2498131", "report-no": null, "categories": "cs.LG cs.CR cs.SI stat.AP stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Most users of online services have unique behavioral or usage patterns. These\nbehavioral patterns can be exploited to identify and track users by using only\nthe observed patterns in the behavior. We study the task of identifying users\nfrom statistics of their behavioral patterns. Specifically, we focus on the\nsetting in which we are given histograms of users' data collected during two\ndifferent experiments. We assume that, in the first dataset, the users'\nidentities are anonymized or hidden and that, in the second dataset, their\nidentities are known. We study the task of identifying the users by matching\nthe histograms of their data in the first dataset with the histograms from the\nsecond dataset. In recent works, the optimal algorithm for this user\nidentification task is introduced. In this paper, we evaluate the effectiveness\nof this method on three different types of datasets and in multiple scenarios.\nUsing datasets such as call data records, web browsing histories, and GPS\ntrajectories, we show that a large fraction of users can be easily identified\ngiven only histograms of their data; hence these histograms can act as users'\nfingerprints. We also verify that simultaneous identification of users achieves\nbetter performance compared to one-by-one user identification. We show that\nusing the optimal method for identification gives higher identification\naccuracy than heuristics-based approaches in practical scenarios. The accuracy\nobtained under this optimal method can thus be used to quantify the maximum\nlevel of user identification that is possible in such settings. We show that\nthe key factors affecting the accuracy of the optimal identification algorithm\nare the duration of the data collection, the number of users in the anonymized\ndataset, and the resolution of the dataset. We analyze the effectiveness of\nk-anonymization in resisting user identification attacks on these datasets.\n", "versions": [{"version": "v1", "created": "Wed, 9 Dec 2015 15:23:33 GMT"}], "update_date": "2015-12-10", "authors_parsed": [["Naini", "Farid M.", ""], ["Unnikrishnan", "Jayakrishnan", ""], ["Thiran", "Patrick", ""], ["Vetterli", "Martin", ""]]}, {"id": "1512.02970", "submitter": "Soham De", "authors": "Soham De and Tom Goldstein", "title": "Efficient Distributed SGD with Variance Reduction", "comments": "In Proceedings of 2016 IEEE International Conference on Data Mining\n  (ICDM)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.DC math.OC stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Stochastic Gradient Descent (SGD) has become one of the most popular\noptimization methods for training machine learning models on massive datasets.\nHowever, SGD suffers from two main drawbacks: (i) The noisy gradient updates\nhave high variance, which slows down convergence as the iterates approach the\noptimum, and (ii) SGD scales poorly in distributed settings, typically\nexperiencing rapidly decreasing marginal benefits as the number of workers\nincreases. In this paper, we propose a highly parallel method, CentralVR, that\nuses error corrections to reduce the variance of SGD gradient updates, and\nscales linearly with the number of worker nodes. CentralVR enjoys low iteration\ncomplexity, provably linear convergence rates, and exhibits linear performance\ngains up to hundreds of cores for massive datasets. We compare CentralVR to\nstate-of-the-art parallel stochastic optimization methods on a variety of\nmodels and datasets, and find that our proposed methods exhibit stronger\nscaling than other SGD variants.\n", "versions": [{"version": "v1", "created": "Wed, 9 Dec 2015 17:57:31 GMT"}, {"version": "v2", "created": "Tue, 4 Oct 2016 16:03:51 GMT"}, {"version": "v3", "created": "Fri, 7 Apr 2017 02:54:14 GMT"}], "update_date": "2017-04-10", "authors_parsed": [["De", "Soham", ""], ["Goldstein", "Tom", ""]]}, {"id": "1512.03025", "submitter": "Ilia Zintchenko", "authors": "Ilia Zintchenko, Matthew Hastings, Nathan Wiebe, Ethan Brown, Matthias\n  Troyer", "title": "Partial Reinitialisation for Optimisers", "comments": "8 pages, 5 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG cs.NE math.OC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Heuristic optimisers which search for an optimal configuration of variables\nrelative to an objective function often get stuck in local optima where the\nalgorithm is unable to find further improvement. The standard approach to\ncircumvent this problem involves periodically restarting the algorithm from\nrandom initial configurations when no further improvement can be found. We\npropose a method of partial reinitialization, whereby, in an attempt to find a\nbetter solution, only sub-sets of variables are re-initialised rather than the\nwhole configuration. Much of the information gained from previous runs is hence\nretained. This leads to significant improvements in the quality of the solution\nfound in a given time for a variety of optimisation problems in machine\nlearning.\n", "versions": [{"version": "v1", "created": "Wed, 9 Dec 2015 20:08:43 GMT"}], "update_date": "2015-12-10", "authors_parsed": [["Zintchenko", "Ilia", ""], ["Hastings", "Matthew", ""], ["Wiebe", "Nathan", ""], ["Brown", "Ethan", ""], ["Troyer", "Matthias", ""]]}, {"id": "1512.03081", "submitter": "Mingyuan Zhou", "authors": "Mingyuan Zhou, Yulai Cong, Bo Chen", "title": "Gamma Belief Networks", "comments": "44 pages, 24 figures", "journal-ref": "Journal of Machine Learning Research, 17(163):1-44, September 2016", "doi": null, "report-no": null, "categories": "stat.ML stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  To infer multilayer deep representations of high-dimensional discrete and\nnonnegative real vectors, we propose an augmentable gamma belief network (GBN)\nthat factorizes each of its hidden layers into the product of a sparse\nconnection weight matrix and the nonnegative real hidden units of the next\nlayer. The GBN's hidden layers are jointly trained with an upward-downward\nGibbs sampler that solves each layer with the same subroutine. The\ngamma-negative binomial process combined with a layer-wise training strategy\nallows inferring the width of each layer given a fixed budget on the width of\nthe first layer. Example results illustrate interesting relationships between\nthe width of the first layer and the inferred network structure, and\ndemonstrate that the GBN can add more layers to improve its performance in both\nunsupervisedly extracting features and predicting heldout data. For exploratory\ndata analysis, we extract trees and subnetworks from the learned deep network\nto visualize how the very specific factors discovered at the first hidden layer\nand the increasingly more general factors discovered at deeper hidden layers\nare related to each other, and we generate synthetic data by propagating random\nvariables through the deep network from the top hidden layer back to the bottom\ndata layer.\n", "versions": [{"version": "v1", "created": "Wed, 9 Dec 2015 21:21:09 GMT"}, {"version": "v2", "created": "Thu, 6 Oct 2016 04:25:18 GMT"}], "update_date": "2016-10-07", "authors_parsed": [["Zhou", "Mingyuan", ""], ["Cong", "Yulai", ""], ["Chen", "Bo", ""]]}, {"id": "1512.03107", "submitter": "Tianbao Yang", "authors": "Tianbao Yang, Qihang Lin", "title": "RSG: Beating Subgradient Method without Smoothness and Strong Convexity", "comments": "Final version accepted by JMLR", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.OC stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we study the efficiency of a {\\bf R}estarted {\\bf S}ub{\\bf\nG}radient (RSG) method that periodically restarts the standard subgradient\nmethod (SG). We show that, when applied to a broad class of convex optimization\nproblems, RSG method can find an $\\epsilon$-optimal solution with a lower\ncomplexity than the SG method. In particular, we first show that RSG can reduce\nthe dependence of SG's iteration complexity on the distance between the initial\nsolution and the optimal set to that between the $\\epsilon$-level set and the\noptimal set {multiplied by a logarithmic factor}. Moreover, we show the\nadvantages of RSG over SG in solving three different families of convex\noptimization problems. (a) For the problems whose epigraph is a polyhedron, RSG\nis shown to converge linearly. (b) For the problems with local quadratic growth\nproperty in the $\\epsilon$-sublevel set, RSG has an\n$O(\\frac{1}{\\epsilon}\\log(\\frac{1}{\\epsilon}))$ iteration complexity. (c) For\nthe problems that admit a local Kurdyka-\\L ojasiewicz property with a power\nconstant of $\\beta\\in[0,1)$, RSG has an\n$O(\\frac{1}{\\epsilon^{2\\beta}}\\log(\\frac{1}{\\epsilon}))$ iteration complexity.\nThe novelty of our analysis lies at exploiting the lower bound of the\nfirst-order optimality residual at the $\\epsilon$-level set. It is this novelty\nthat allows us to explore the local properties of functions (e.g., local\nquadratic growth property, local Kurdyka-\\L ojasiewicz property, more generally\nlocal error bound conditions) to develop the improved convergence of RSG. { We\nalso develop a practical variant of RSG enjoying faster convergence than the SG\nmethod, which can be run without knowing the involved parameters in the local\nerror bound condition.} We demonstrate the effectiveness of the proposed\nalgorithms on several machine learning tasks including regression,\nclassification and matrix completion.\n", "versions": [{"version": "v1", "created": "Wed, 9 Dec 2015 22:58:21 GMT"}, {"version": "v10", "created": "Thu, 23 Jun 2016 05:08:01 GMT"}, {"version": "v11", "created": "Thu, 11 Aug 2016 23:09:32 GMT"}, {"version": "v12", "created": "Tue, 29 Nov 2016 06:12:35 GMT"}, {"version": "v13", "created": "Wed, 18 Apr 2018 20:57:48 GMT"}, {"version": "v14", "created": "Mon, 12 Nov 2018 05:23:28 GMT"}, {"version": "v2", "created": "Sun, 13 Dec 2015 23:40:38 GMT"}, {"version": "v3", "created": "Mon, 4 Jan 2016 20:36:11 GMT"}, {"version": "v4", "created": "Tue, 5 Jan 2016 03:33:59 GMT"}, {"version": "v5", "created": "Wed, 3 Feb 2016 05:56:29 GMT"}, {"version": "v6", "created": "Mon, 29 Feb 2016 05:37:18 GMT"}, {"version": "v7", "created": "Fri, 4 Mar 2016 23:57:06 GMT"}, {"version": "v8", "created": "Tue, 5 Apr 2016 03:40:59 GMT"}, {"version": "v9", "created": "Wed, 4 May 2016 04:07:35 GMT"}], "update_date": "2018-11-13", "authors_parsed": [["Yang", "Tianbao", ""], ["Lin", "Qihang", ""]]}, {"id": "1512.03219", "submitter": "Vladislav Malyshkin", "authors": "Vladislav Gennadievich Malyshkin", "title": "Norm-Free Radon-Nikodym Approach to Machine Learning", "comments": "Cluster localization measure added. Quantum mechanics analogy\n  improved and expanded (density matrix exact expression added). Coverage\n  calculation via matrix spectrum added", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  For Machine Learning (ML) classification problem, where a vector of\n$\\mathbf{x}$--observations (values of attributes) is mapped to a single $y$\nvalue (class label), a generalized Radon--Nikodym type of solution is proposed.\nQuantum--mechanics --like probability states $\\psi^2(\\mathbf{x})$ are\nconsidered and \"Cluster Centers\", corresponding to the extremums of\n$<y\\psi^2(\\mathbf{x})>/<\\psi^2(\\mathbf{x})>$, are found from generalized\neigenvalues problem. The eigenvalues give possible $y^{[i]}$ outcomes and\ncorresponding to them eigenvectors $\\psi^{[i]}(\\mathbf{x})$ define \"Cluster\nCenters\". The projection of a $\\psi$ state, localized at given $\\mathbf{x}$ to\nclassify, on these eigenvectors define the probability of $y^{[i]}$ outcome,\nthus avoiding using a norm ($L^2$ or other types), required for \"quality\ncriteria\" in a typical Machine Learning technique. A coverage of each `Cluster\nCenter\" is calculated, what potentially allows to separate system properties\n(described by $y^{[i]}$ outcomes) and system testing conditions (described by\n$C^{[i]}$ coverage). As an example of such application $y$ distribution\nestimator is proposed in a form of pairs $(y^{[i]},C^{[i]})$, that can be\nconsidered as Gauss quadratures generalization. This estimator allows to\nperform $y$ probability distribution estimation in a strongly non--Gaussian\ncase.\n", "versions": [{"version": "v1", "created": "Thu, 10 Dec 2015 11:24:26 GMT"}, {"version": "v2", "created": "Tue, 15 Dec 2015 19:01:18 GMT"}], "update_date": "2015-12-16", "authors_parsed": [["Malyshkin", "Vladislav Gennadievich", ""]]}, {"id": "1512.03300", "submitter": "Khoat Than", "authors": "Khoat Than, Tu Bao Ho", "title": "Inference in topic models: sparsity and trade-off", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Topic models are popular for modeling discrete data (e.g., texts, images,\nvideos, links), and provide an efficient way to discover hidden\nstructures/semantics in massive data. One of the core problems in this field is\nthe posterior inference for individual data instances. This problem is\nparticularly important in streaming environments, but is often intractable. In\nthis paper, we investigate the use of the Frank-Wolfe algorithm (FW) for\nrecovering sparse solutions to posterior inference. From detailed elucidation\nof both theoretical and practical aspects, FW exhibits many interesting\nproperties which are beneficial to topic modeling. We then employ FW to design\nfast methods, including ML-FW, for learning latent Dirichlet allocation (LDA)\nat large scales. Extensive experiments show that to reach the same\npredictiveness level, ML-FW can perform tens to thousand times faster than\nexisting state-of-the-art methods for learning LDA from massive/streaming data.\n", "versions": [{"version": "v1", "created": "Thu, 10 Dec 2015 16:12:10 GMT"}], "update_date": "2015-12-11", "authors_parsed": [["Than", "Khoat", ""], ["Ho", "Tu Bao", ""]]}, {"id": "1512.03308", "submitter": "Khoat Than", "authors": "Khoat Than, Tung Doan", "title": "Guaranteed inference in topic models", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  One of the core problems in statistical models is the estimation of a\nposterior distribution. For topic models, the problem of posterior inference\nfor individual texts is particularly important, especially when dealing with\ndata streams, but is often intractable in the worst case. As a consequence,\nexisting methods for posterior inference are approximate and do not have any\nguarantee on neither quality nor convergence rate. In this paper, we introduce\na provably fast algorithm, namely Online Maximum a Posteriori Estimation (OPE),\nfor posterior inference in topic models. OPE has more attractive properties\nthan existing inference approaches, including theoretical guarantees on quality\nand fast rate of convergence to a local maximal/stationary point of the\ninference problem. The discussions about OPE are very general and hence can be\neasily employed in a wide range of contexts. Finally, we employ OPE to design\nthree methods for learning Latent Dirichlet Allocation from text streams or\nlarge corpora. Extensive experiments demonstrate some superior behaviors of OPE\nand of our new learning methods.\n", "versions": [{"version": "v1", "created": "Thu, 10 Dec 2015 16:24:44 GMT"}, {"version": "v2", "created": "Wed, 17 Aug 2016 06:46:30 GMT"}], "update_date": "2016-08-18", "authors_parsed": [["Than", "Khoat", ""], ["Doan", "Tung", ""]]}, {"id": "1512.03396", "submitter": "Yuting Ma", "authors": "Yuting Ma, Tian Zheng", "title": "Boosted Sparse Non-linear Distance Metric Learning", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper proposes a boosting-based solution addressing metric learning\nproblems for high-dimensional data. Distance measures have been used as natural\nmeasures of (dis)similarity and served as the foundation of various learning\nmethods. The efficiency of distance-based learning methods heavily depends on\nthe chosen distance metric. With increasing dimensionality and complexity of\ndata, however, traditional metric learning methods suffer from poor scalability\nand the limitation due to linearity as the true signals are usually embedded\nwithin a low-dimensional nonlinear subspace. In this paper, we propose a\nnonlinear sparse metric learning algorithm via boosting. We restructure a\nglobal optimization problem into a forward stage-wise learning of weak learners\nbased on a rank-one decomposition of the weight matrix in the Mahalanobis\ndistance metric. A gradient boosting algorithm is devised to obtain a sparse\nrank-one update of the weight matrix at each step. Nonlinear features are\nlearned by a hierarchical expansion of interactions incorporated within the\nboosting algorithm. Meanwhile, an early stopping rule is imposed to control the\noverall complexity of the learned metric. As a result, our approach guarantees\nthree desirable properties of the final metric: positive semi-definiteness, low\nrank and element-wise sparsity. Numerical experiments show that our learning\nmodel compares favorably with the state-of-the-art methods in the current\nliterature of metric learning.\n", "versions": [{"version": "v1", "created": "Thu, 10 Dec 2015 20:19:01 GMT"}], "update_date": "2015-12-11", "authors_parsed": [["Ma", "Yuting", ""], ["Zheng", "Tian", ""]]}, {"id": "1512.03397", "submitter": "Rina Foygel Barber", "authors": "Rina Foygel Barber and Aaditya Ramdas", "title": "The p-filter: multi-layer FDR control for grouped hypotheses", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In many practical applications of multiple hypothesis testing using the False\nDiscovery Rate (FDR), the given hypotheses can be naturally partitioned into\ngroups, and one may not only want to control the number of false discoveries\n(wrongly rejected null hypotheses), but also the number of falsely discovered\ngroups of hypotheses (we say a group is falsely discovered if at least one\nhypothesis within that group is rejected, when in reality the group contains\nonly nulls). In this paper, we introduce the p-filter, a procedure which\nunifies and generalizes the standard FDR procedure by Benjamini and Hochberg\nand global null testing procedure by Simes. We first prove that our proposed\nmethod can simultaneously control the overall FDR at the finest level\n(individual hypotheses treated separately) and the group FDR at coarser levels\n(when such groups are user-specified). We then generalize the p-filter\nprocedure even further to handle multiple partitions of hypotheses, since that\nmight be natural in many applications. For example, in neuroscience\nexperiments, we may have a hypothesis for every (discretized) location in the\nbrain, and at every (discretized) timepoint: does the stimulus correlate with\nactivity in location x at time t after the stimulus was presented? In this\nsetting, one might want to group hypotheses by location and by time.\nImportantly, our procedure can handle multiple partitions which are\nnonhierarchical (i.e. one partition may arrange p-values by voxel, and another\npartition arranges them by time point; neither one is nested inside the other).\nWe prove that our procedure controls FDR simultaneously across these multiple\nlay- ers, under assumptions that are standard in the literature: we do not need\nthe hypotheses to be independent, but require a nonnegative dependence\ncondition known as PRDS.\n", "versions": [{"version": "v1", "created": "Thu, 10 Dec 2015 20:23:16 GMT"}, {"version": "v2", "created": "Mon, 22 Feb 2016 14:57:01 GMT"}, {"version": "v3", "created": "Sat, 29 Oct 2016 01:53:16 GMT"}], "update_date": "2016-11-01", "authors_parsed": [["Barber", "Rina Foygel", ""], ["Ramdas", "Aaditya", ""]]}, {"id": "1512.03443", "submitter": "Abhimanu Kumar", "authors": "Abhimanu Kumar and Shriphani Palakodety and Chong Wang and Carolyn P.\n  Rose and Eric P. Xing and Miaomiao Wen", "title": "Scalable Modeling of Conversational-role based Self-presentation\n  Characteristics in Large Online Forums", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.SI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Online discussion forums are complex webs of overlapping subcommunities\n(macrolevel structure, across threads) in which users enact different roles\ndepending on which subcommunity they are participating in within a particular\ntime point (microlevel structure, within threads). This sub-network structure\nis implicit in massive collections of threads. To uncover this structure, we\ndevelop a scalable algorithm based on stochastic variational inference and\nleverage topic models (LDA) along with mixed membership stochastic block (MMSB)\nmodels. We evaluate our model on three large-scale datasets,\nCancer-ThreadStarter (22K users and 14.4K threads), Cancer-NameMention(15.1K\nusers and 12.4K threads) and StackOverFlow (1.19 million users and 4.55 million\nthreads). Qualitatively, we demonstrate that our model can provide useful\nexplanations of microlevel and macrolevel user presentation characteristics in\ndifferent communities using the topics discovered from posts. Quantitatively,\nwe show that our model does better than MMSB and LDA in predicting user reply\nstructure within threads. In addition, we demonstrate via synthetic data\nexperiments that the proposed active sub-network discovery model is stable and\nrecovers the original parameters of the experimental setup with high\nprobability.\n", "versions": [{"version": "v1", "created": "Thu, 10 Dec 2015 21:19:42 GMT"}], "update_date": "2015-12-14", "authors_parsed": [["Kumar", "Abhimanu", ""], ["Palakodety", "Shriphani", ""], ["Wang", "Chong", ""], ["Rose", "Carolyn P.", ""], ["Xing", "Eric P.", ""], ["Wen", "Miaomiao", ""]]}, {"id": "1512.03444", "submitter": "Amichai Painsky", "authors": "Amichai Painsky and Saharon Rosset", "title": "Cross-Validated Variable Selection in Tree-Based Methods Improves\n  Predictive Performance", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recursive partitioning approaches producing tree-like models are a long\nstanding staple of predictive modeling, in the last decade mostly as\n``sub-learners'' within state of the art ensemble methods like Boosting and\nRandom Forest. However, a fundamental flaw in the partitioning (or splitting)\nrule of commonly used tree building methods precludes them from treating\ndifferent types of variables equally. This most clearly manifests in these\nmethods' inability to properly utilize categorical variables with a large\nnumber of categories, which are ubiquitous in the new age of big data. Such\nvariables can often be very informative, but current tree methods essentially\nleave us a choice of either not using them, or exposing our models to severe\noverfitting. We propose a conceptual framework to splitting using leave-one-out\n(LOO) cross validation for selecting the splitting variable, then performing a\nregular split (in our case, following CART's approach) for the selected\nvariable. The most important consequence of our approach is that categorical\nvariables with many categories can be safely used in tree building and are only\nchosen if they contribute to predictive power. We demonstrate in extensive\nsimulation and real data analysis that our novel splitting approach\nsignificantly improves the performance of both single tree models and ensemble\nmethods that utilize trees. Importantly, we design an algorithm for LOO\nsplitting variable selection which under reasonable assumptions does not\nincrease the overall computational complexity compared to CART for two-class\nclassification. For regression tasks, our approach carries an increased\ncomputational burden, replacing a O(log(n)) factor in CART splitting rule\nsearch with an O(n) term.\n", "versions": [{"version": "v1", "created": "Thu, 10 Dec 2015 21:20:14 GMT"}], "update_date": "2015-12-14", "authors_parsed": [["Painsky", "Amichai", ""], ["Rosset", "Saharon", ""]]}, {"id": "1512.03518", "submitter": "Zirui Zhou", "authors": "Zirui Zhou, Anthony Man-Cho So", "title": "A Unified Approach to Error Bounds for Structured Convex Optimization\n  Problems", "comments": "32 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.OC cs.LG math.NA stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Error bounds, which refer to inequalities that bound the distance of vectors\nin a test set to a given set by a residual function, have proven to be\nextremely useful in analyzing the convergence rates of a host of iterative\nmethods for solving optimization problems. In this paper, we present a new\nframework for establishing error bounds for a class of structured convex\noptimization problems, in which the objective function is the sum of a smooth\nconvex function and a general closed proper convex function. Such a class\nencapsulates not only fairly general constrained minimization problems but also\nvarious regularized loss minimization formulations in machine learning, signal\nprocessing, and statistics. Using our framework, we show that a number of\nexisting error bound results can be recovered in a unified and transparent\nmanner. To further demonstrate the power of our framework, we apply it to a\nclass of nuclear-norm regularized loss minimization problems and establish a\nnew error bound for this class under a strict complementarity-type regularity\ncondition. We then complement this result by constructing an example to show\nthat the said error bound could fail to hold without the regularity condition.\nConsequently, we obtain a rather complete answer to a question raised by Tseng.\nWe believe that our approach will find further applications in the study of\nerror bounds for structured convex optimization problems.\n", "versions": [{"version": "v1", "created": "Fri, 11 Dec 2015 04:32:30 GMT"}], "update_date": "2015-12-14", "authors_parsed": [["Zhou", "Zirui", ""], ["So", "Anthony Man-Cho", ""]]}, {"id": "1512.03542", "submitter": "Zhengping Che", "authors": "Zhengping Che, Sanjay Purushotham, Robinder Khemani, Yan Liu", "title": "Distilling Knowledge from Deep Networks with Applications to Healthcare\n  Domain", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Exponential growth in Electronic Healthcare Records (EHR) has resulted in new\nopportunities and urgent needs for discovery of meaningful data-driven\nrepresentations and patterns of diseases in Computational Phenotyping research.\nDeep Learning models have shown superior performance for robust prediction in\ncomputational phenotyping tasks, but suffer from the issue of model\ninterpretability which is crucial for clinicians involved in decision-making.\nIn this paper, we introduce a novel knowledge-distillation approach called\nInterpretable Mimic Learning, to learn interpretable phenotype features for\nmaking robust prediction while mimicking the performance of deep learning\nmodels. Our framework uses Gradient Boosting Trees to learn interpretable\nfeatures from deep learning models such as Stacked Denoising Autoencoder and\nLong Short-Term Memory. Exhaustive experiments on a real-world clinical\ntime-series dataset show that our method obtains similar or better performance\nthan the deep learning models, and it provides interpretable phenotypes for\nclinical decision making.\n", "versions": [{"version": "v1", "created": "Fri, 11 Dec 2015 07:38:12 GMT"}], "update_date": "2015-12-14", "authors_parsed": [["Che", "Zhengping", ""], ["Purushotham", "Sanjay", ""], ["Khemani", "Robinder", ""], ["Liu", "Yan", ""]]}, {"id": "1512.03844", "submitter": "Alexander Wong", "authors": "Mohammad Javad Shafiee, Parthipan Siva, Paul Fieguth, and Alexander\n  Wong", "title": "Efficient Deep Feature Learning and Extraction via StochasticNets", "comments": "10 pages. arXiv admin note: substantial text overlap with\n  arXiv:1508.05463", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Deep neural networks are a powerful tool for feature learning and extraction\ngiven their ability to model high-level abstractions in highly complex data.\nOne area worth exploring in feature learning and extraction using deep neural\nnetworks is efficient neural connectivity formation for faster feature learning\nand extraction. Motivated by findings of stochastic synaptic connectivity\nformation in the brain as well as the brain's uncanny ability to efficiently\nrepresent information, we propose the efficient learning and extraction of\nfeatures via StochasticNets, where sparsely-connected deep neural networks can\nbe formed via stochastic connectivity between neurons. To evaluate the\nfeasibility of such a deep neural network architecture for feature learning and\nextraction, we train deep convolutional StochasticNets to learn abstract\nfeatures using the CIFAR-10 dataset, and extract the learned features from\nimages to perform classification on the SVHN and STL-10 datasets. Experimental\nresults show that features learned using deep convolutional StochasticNets,\nwith fewer neural connections than conventional deep convolutional neural\nnetworks, can allow for better or comparable classification accuracy than\nconventional deep neural networks: relative test error decrease of ~4.5% for\nclassification on the STL-10 dataset and ~1% for classification on the SVHN\ndataset. Furthermore, it was shown that the deep features extracted using deep\nconvolutional StochasticNets can provide comparable classification accuracy\neven when only 10% of the training data is used for feature learning. Finally,\nit was also shown that significant gains in feature extraction speed can be\nachieved in embedded applications using StochasticNets. As such, StochasticNets\nallow for faster feature learning and extraction performance while facilitate\nfor better or comparable accuracy performances.\n", "versions": [{"version": "v1", "created": "Fri, 11 Dec 2015 22:47:34 GMT"}], "update_date": "2015-12-15", "authors_parsed": [["Shafiee", "Mohammad Javad", ""], ["Siva", "Parthipan", ""], ["Fieguth", "Paul", ""], ["Wong", "Alexander", ""]]}, {"id": "1512.03880", "submitter": "Jinyang  Gao", "authors": "Jinyang Gao, H.V.Jagadish, Beng Chin Ooi", "title": "Active Sampler: Light-weight Accelerator for Complex Data Analytics at\n  Scale", "comments": "12 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recent years have witnessed amazing outcomes from \"Big Models\" trained by\n\"Big Data\". Most popular algorithms for model training are iterative. Due to\nthe surging volumes of data, we can usually afford to process only a fraction\nof the training data in each iteration. Typically, the data are either\nuniformly sampled or sequentially accessed.\n  In this paper, we study how the data access pattern can affect model\ntraining. We propose an Active Sampler algorithm, where training data with more\n\"learning value\" to the model are sampled more frequently. The goal is to focus\ntraining effort on valuable instances near the classification boundaries,\nrather than evident cases, noisy data or outliers. We show the correctness and\noptimality of Active Sampler in theory, and then develop a light-weight\nvectorized implementation. Active Sampler is orthogonal to most approaches\noptimizing the efficiency of large-scale data analytics, and can be applied to\nmost analytics models trained by stochastic gradient descent (SGD) algorithm.\nExtensive experimental evaluations demonstrate that Active Sampler can speed up\nthe training procedure of SVM, feature selection and deep learning, for\ncomparable training quality by 1.6-2.2x.\n", "versions": [{"version": "v1", "created": "Sat, 12 Dec 2015 06:32:33 GMT"}], "update_date": "2015-12-15", "authors_parsed": [["Gao", "Jinyang", ""], ["Jagadish", "H. V.", ""], ["Ooi", "Beng Chin", ""]]}, {"id": "1512.03883", "submitter": "Yiyuan She", "authors": "Qiaoya Zhang, Yiyuan She", "title": "Sparse Generalized Principal Component Analysis for Large-scale\n  Applications beyond Gaussianity", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.CO stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Principal Component Analysis (PCA) is a dimension reduction technique. It\nproduces inconsistent estimators when the dimensionality is moderate to high,\nwhich is often the problem in modern large-scale applications where algorithm\nscalability and model interpretability are difficult to achieve, not to mention\nthe prevalence of missing values. While existing sparse PCA methods alleviate\ninconsistency, they are constrained to the Gaussian assumption of classical PCA\nand fail to address algorithm scalability issues. We generalize sparse PCA to\nthe broad exponential family distributions under high-dimensional setup, with\nbuilt-in treatment for missing values. Meanwhile we propose a family of\niterative sparse generalized PCA (SG-PCA) algorithms such that despite the\nnon-convexity and non-smoothness of the optimization task, the loss function\ndecreases in every iteration. In terms of ease and intuitive parameter tuning,\nour sparsity-inducing regularization is far superior to the popular Lasso.\nFurthermore, to promote overall scalability, accelerated gradient is integrated\nfor fast convergence, while a progressive screening technique gradually\nsqueezes out nuisance dimensions of a large-scale problem for feasible\noptimization. High-dimensional simulation and real data experiments demonstrate\nthe efficiency and efficacy of SG-PCA.\n", "versions": [{"version": "v1", "created": "Sat, 12 Dec 2015 06:45:05 GMT"}, {"version": "v2", "created": "Thu, 28 Jan 2016 02:36:12 GMT"}], "update_date": "2016-01-29", "authors_parsed": [["Zhang", "Qiaoya", ""], ["She", "Yiyuan", ""]]}, {"id": "1512.03929", "submitter": "Joseph Fitzsimons", "authors": "Zhikuan Zhao, Jack K. Fitzsimons and Joseph F. Fitzsimons", "title": "Quantum assisted Gaussian process regression", "comments": "4 pages. Comments welcome", "journal-ref": "Phys. Rev. A 99, 052331 (2019)", "doi": "10.1103/PhysRevA.99.052331", "report-no": null, "categories": "quant-ph cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Gaussian processes (GP) are a widely used model for regression problems in\nsupervised machine learning. Implementation of GP regression typically requires\n$O(n^3)$ logic gates. We show that the quantum linear systems algorithm [Harrow\net al., Phys. Rev. Lett. 103, 150502 (2009)] can be applied to Gaussian process\nregression (GPR), leading to an exponential reduction in computation time in\nsome instances. We show that even in some cases not ideally suited to the\nquantum linear systems algorithm, a polynomial increase in efficiency still\noccurs.\n", "versions": [{"version": "v1", "created": "Sat, 12 Dec 2015 16:19:35 GMT"}], "update_date": "2019-05-29", "authors_parsed": [["Zhao", "Zhikuan", ""], ["Fitzsimons", "Jack K.", ""], ["Fitzsimons", "Joseph F.", ""]]}, {"id": "1512.03965", "submitter": "Ohad Shamir", "authors": "Ronen Eldan and Ohad Shamir", "title": "The Power of Depth for Feedforward Neural Networks", "comments": "Accepted to COLT 2016; Fixed a bug in the proof of claim 2 (now\n  requiring the mild assumption that the activations are polynomially bounded);\n  Other minor revisions", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.NE stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We show that there is a simple (approximately radial) function on $\\reals^d$,\nexpressible by a small 3-layer feedforward neural networks, which cannot be\napproximated by any 2-layer network, to more than a certain constant accuracy,\nunless its width is exponential in the dimension. The result holds for\nvirtually all known activation functions, including rectified linear units,\nsigmoids and thresholds, and formally demonstrates that depth -- even if\nincreased by 1 -- can be exponentially more valuable than width for standard\nfeedforward neural networks. Moreover, compared to related results in the\ncontext of Boolean functions, our result requires fewer assumptions, and the\nproof techniques and construction are very different.\n", "versions": [{"version": "v1", "created": "Sat, 12 Dec 2015 21:41:24 GMT"}, {"version": "v2", "created": "Thu, 24 Dec 2015 16:21:24 GMT"}, {"version": "v3", "created": "Fri, 25 Dec 2015 03:47:28 GMT"}, {"version": "v4", "created": "Mon, 9 May 2016 02:16:54 GMT"}], "update_date": "2016-05-10", "authors_parsed": [["Eldan", "Ronen", ""], ["Shamir", "Ohad", ""]]}, {"id": "1512.03990", "submitter": "Mauricio Santillana", "authors": "Mauricio Santillana, Andre Nguyen, Tamara Louie, Anna Zink, Josh Gray,\n  Iyue Sung, John S. Brownstein", "title": "Cloud-based Electronic Health Records for Real-time, Region-specific\n  Influenza Surveillance", "comments": null, "journal-ref": null, "doi": "10.1038/srep25732", "report-no": null, "categories": "stat.AP stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Accurate real-time monitoring systems of influenza outbreaks help public\nhealth officials make informed decisions that may help save lives. We show that\ninformation extracted from cloud-based electronic health records databases, in\ncombination with machine learning techniques and historical epidemiological\ninformation, have the potential to accurately and reliably provide near\nreal-time regional predictions of flu outbreaks in the United States.\n", "versions": [{"version": "v1", "created": "Sun, 13 Dec 2015 02:51:36 GMT"}], "update_date": "2018-12-11", "authors_parsed": [["Santillana", "Mauricio", ""], ["Nguyen", "Andre", ""], ["Louie", "Tamara", ""], ["Zink", "Anna", ""], ["Gray", "Josh", ""], ["Sung", "Iyue", ""], ["Brownstein", "John S.", ""]]}, {"id": "1512.04052", "submitter": "Fionn Murtagh", "authors": "Fionn Murtagh", "title": "Big Data Scaling through Metric Mapping: Exploiting the Remarkable\n  Simplicity of Very High Dimensional Spaces using Correspondence Analysis", "comments": "13 pages, 3 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present new findings in regard to data analysis in very high dimensional\nspaces. We use dimensionalities up to around one million. A particular benefit\nof Correspondence Analysis is its suitability for carrying out an orthonormal\nmapping, or scaling, of power law distributed data. Power law distributed data\nare found in many domains. Correspondence factor analysis provides a latent\nsemantic or principal axes mapping. Our experiments use data from digital\nchemistry and finance, and other statistically generated data.\n", "versions": [{"version": "v1", "created": "Sun, 13 Dec 2015 13:34:32 GMT"}], "update_date": "2015-12-15", "authors_parsed": [["Murtagh", "Fionn", ""]]}, {"id": "1512.04152", "submitter": "Chansoo Lee", "authors": "Jacob Abernethy, Chansoo Lee, Ambuj Tewari", "title": "Fighting Bandits with a New Kind of Smoothness", "comments": "In Proceedings of NIPS, 2015", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.GT stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We define a novel family of algorithms for the adversarial multi-armed bandit\nproblem, and provide a simple analysis technique based on convex smoothing. We\nprove two main results. First, we show that regularization via the\n\\emph{Tsallis entropy}, which includes EXP3 as a special case, achieves the\n$\\Theta(\\sqrt{TN})$ minimax regret. Second, we show that a wide class of\nperturbation methods achieve a near-optimal regret as low as $O(\\sqrt{TN \\log\nN})$ if the perturbation distribution has a bounded hazard rate. For example,\nthe Gumbel, Weibull, Frechet, Pareto, and Gamma distributions all satisfy this\nkey property.\n", "versions": [{"version": "v1", "created": "Mon, 14 Dec 2015 01:57:02 GMT"}], "update_date": "2015-12-15", "authors_parsed": [["Abernethy", "Jacob", ""], ["Lee", "Chansoo", ""], ["Tewari", "Ambuj", ""]]}, {"id": "1512.04202", "submitter": "Xi-Lin Li", "authors": "Xi-Lin Li", "title": "Preconditioned Stochastic Gradient Descent", "comments": "13 pages, 9 figures. To appear in IEEE Transactions on Neural\n  Networks and Learning Systems. Supplemental materials on\n  https://sites.google.com/site/lixilinx/home/psgd", "journal-ref": null, "doi": "10.1109/TNNLS.2017.2672978", "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Stochastic gradient descent (SGD) still is the workhorse for many practical\nproblems. However, it converges slow, and can be difficult to tune. It is\npossible to precondition SGD to accelerate its convergence remarkably. But many\nattempts in this direction either aim at solving specialized problems, or\nresult in significantly more complicated methods than SGD. This paper proposes\na new method to estimate a preconditioner such that the amplitudes of\nperturbations of preconditioned stochastic gradient match that of the\nperturbations of parameters to be optimized in a way comparable to Newton\nmethod for deterministic optimization. Unlike the preconditioners based on\nsecant equation fitting as done in deterministic quasi-Newton methods, which\nassume positive definite Hessian and approximate its inverse, the new\npreconditioner works equally well for both convex and non-convex optimizations\nwith exact or noisy gradients. When stochastic gradient is used, it can\nnaturally damp the gradient noise to stabilize SGD. Efficient preconditioner\nestimation methods are developed, and with reasonable simplifications, they are\napplicable to large scaled problems. Experimental results demonstrate that\nequipped with the new preconditioner, without any tuning effort, preconditioned\nSGD can efficiently solve many challenging problems like the training of a deep\nneural network or a recurrent neural network requiring extremely long term\nmemories.\n", "versions": [{"version": "v1", "created": "Mon, 14 Dec 2015 07:14:09 GMT"}, {"version": "v2", "created": "Wed, 14 Sep 2016 21:51:02 GMT"}, {"version": "v3", "created": "Wed, 22 Feb 2017 06:58:30 GMT"}], "update_date": "2017-02-23", "authors_parsed": [["Li", "Xi-Lin", ""]]}, {"id": "1512.04274", "submitter": "Sebastian Weichwald", "authors": "Sebastian Weichwald, Timm Meyer, Bernhard Sch\\\"olkopf, Tonio Ball,\n  Moritz Grosse-Wentrup", "title": "Decoding index finger position from EEG using random forests", "comments": "accepted manuscript", "journal-ref": "Cognitive Information Processing (CIP), 2014 4th International\n  Workshop on, 1-6, 2014", "doi": "10.1109/CIP.2014.6844513", "report-no": null, "categories": "stat.ML q-bio.NC q-bio.QM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  While invasively recorded brain activity is known to provide detailed\ninformation on motor commands, it is an open question at what level of detail\ninformation about positions of body parts can be decoded from non-invasively\nacquired signals. In this work it is shown that index finger positions can be\ndifferentiated from non-invasive electroencephalographic (EEG) recordings in\nhealthy human subjects. Using a leave-one-subject-out cross-validation\nprocedure, a random forest distinguished different index finger positions on a\nnumerical keyboard above chance-level accuracy. Among the different spectral\nfeatures investigated, high $\\beta$-power (20-30 Hz) over contralateral\nsensorimotor cortex carried most information about finger position. Thus, these\nfindings indicate that finger position is in principle decodable from\nnon-invasive features of brain activity that generalize across individuals.\n", "versions": [{"version": "v1", "created": "Mon, 14 Dec 2015 12:19:31 GMT"}], "update_date": "2015-12-15", "authors_parsed": [["Weichwald", "Sebastian", ""], ["Meyer", "Timm", ""], ["Sch\u00f6lkopf", "Bernhard", ""], ["Ball", "Tonio", ""], ["Grosse-Wentrup", "Moritz", ""]]}, {"id": "1512.04387", "submitter": "Yura Perov N", "authors": "Yura N Perov, Tuan Anh Le, Frank Wood", "title": "Data-driven Sequential Monte Carlo in Probabilistic Programming", "comments": "Black Box Learning and Inference, NIPS 2015 Workshop", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI stat.AP stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Most of Markov Chain Monte Carlo (MCMC) and sequential Monte Carlo (SMC)\nalgorithms in existing probabilistic programming systems suboptimally use only\nmodel priors as proposal distributions. In this work, we describe an approach\nfor training a discriminative model, namely a neural network, in order to\napproximate the optimal proposal by using posterior estimates from previous\nruns of inference. We show an example that incorporates a data-driven proposal\nfor use in a non-parametric model in the Anglican probabilistic programming\nsystem. Our results show that data-driven proposals can significantly improve\ninference performance so that considerably fewer particles are necessary to\nperform a good posterior estimation.\n", "versions": [{"version": "v1", "created": "Mon, 14 Dec 2015 16:18:32 GMT"}, {"version": "v2", "created": "Mon, 16 May 2016 19:34:52 GMT"}], "update_date": "2016-05-17", "authors_parsed": [["Perov", "Yura N", ""], ["Le", "Tuan Anh", ""], ["Wood", "Frank", ""]]}, {"id": "1512.04481", "submitter": "Phaedon-Stelios Koutsourelakis", "authors": "Isabell M. Franck, P.S. Koutsourelakis", "title": "Multimodal, high-dimensional, model-based, Bayesian inverse problems\n  with applications in biomechanics", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.CO stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper is concerned with the numerical solution of model-based, Bayesian\ninverse problems. We are particularly interested in cases where the cost of\neach likelihood evaluation (forward-model call) is expensive and the number of\nun- known (latent) variables is high. This is the setting in many problems in\ncom- putational physics where forward models with nonlinear PDEs are used and\nthe parameters to be calibrated involve spatio-temporarily varying\ncoefficients, which upon discretization give rise to a high-dimensional vector\nof unknowns. One of the consequences of the well-documented ill-posedness of\ninverse prob- lems is the possibility of multiple solutions. While such\ninformation is contained in the posterior density in Bayesian formulations, the\ndiscovery of a single mode, let alone multiple, is a formidable task. The goal\nof the present paper is two- fold. On one hand, we propose approximate,\nadaptive inference strategies using mixture densities to capture multi-modal\nposteriors, and on the other, to ex- tend our work in [1] with regards to\neffective dimensionality reduction techniques that reveal low-dimensional\nsubspaces where the posterior variance is mostly concentrated. We validate the\nmodel proposed by employing Importance Sam- pling which confirms that the bias\nintroduced is small and can be efficiently corrected if the analyst wishes to\ndo so. We demonstrate the performance of the proposed strategy in nonlinear\nelastography where the identification of the mechanical properties of\nbiological materials can inform non-invasive, medical di- agnosis. The\ndiscovery of multiple modes (solutions) in such problems is critical in\nachieving the diagnostic objectives.\n", "versions": [{"version": "v1", "created": "Mon, 14 Dec 2015 19:37:29 GMT"}, {"version": "v2", "created": "Tue, 15 Dec 2015 12:10:55 GMT"}, {"version": "v3", "created": "Thu, 21 Jul 2016 20:27:50 GMT"}], "update_date": "2016-07-25", "authors_parsed": [["Franck", "Isabell M.", ""], ["Koutsourelakis", "P. S.", ""]]}, {"id": "1512.04564", "submitter": "Hung Nien", "authors": "Hung Nien and Jeffrey A. Fessler", "title": "Relaxed Linearized Algorithms for Faster X-Ray CT Image Reconstruction", "comments": "Submitted to IEEE Transactions on Medical Imaging", "journal-ref": "IEEE Transactions on Medical Imaging 35(4):1090-8 Apr 2016", "doi": "10.1109/TMI.2015.2508780", "report-no": null, "categories": "math.OC cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Statistical image reconstruction (SIR) methods are studied extensively for\nX-ray computed tomography (CT) due to the potential of acquiring CT scans with\nreduced X-ray dose while maintaining image quality. However, the longer\nreconstruction time of SIR methods hinders their use in X-ray CT in practice.\nTo accelerate statistical methods, many optimization techniques have been\ninvestigated. Over-relaxation is a common technique to speed up convergence of\niterative algorithms. For instance, using a relaxation parameter that is close\nto two in alternating direction method of multipliers (ADMM) has been shown to\nspeed up convergence significantly. This paper proposes a relaxed linearized\naugmented Lagrangian (AL) method that shows theoretical faster convergence rate\nwith over-relaxation and applies the proposed relaxed linearized AL method to\nX-ray CT image reconstruction problems. Experimental results with both\nsimulated and real CT scan data show that the proposed relaxed algorithm (with\nordered-subsets [OS] acceleration) is about twice as fast as the existing\nunrelaxed fast algorithms, with negligible computation and memory overhead.\n", "versions": [{"version": "v1", "created": "Mon, 14 Dec 2015 21:31:23 GMT"}], "update_date": "2019-06-14", "authors_parsed": [["Nien", "Hung", ""], ["Fessler", "Jeffrey A.", ""]]}, {"id": "1512.04754", "submitter": "Ulugbek Kamilov", "authors": "Ulugbek S. Kamilov and Hassan Mansour", "title": "Learning optimal nonlinearities for iterative thresholding algorithms", "comments": null, "journal-ref": null, "doi": "10.1109/LSP.2016.2548245", "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Iterative shrinkage/thresholding algorithm (ISTA) is a well-studied method\nfor finding sparse solutions to ill-posed inverse problems. In this letter, we\npresent a data-driven scheme for learning optimal thresholding functions for\nISTA. The proposed scheme is obtained by relating iterations of ISTA to layers\nof a simple deep neural network (DNN) and developing a corresponding error\nbackpropagation algorithm that allows to fine-tune the thresholding functions.\nSimulations on sparse statistical signals illustrate potential gains in\nestimation quality due to the proposed data adaptive ISTA.\n", "versions": [{"version": "v1", "created": "Tue, 15 Dec 2015 12:20:17 GMT"}], "update_date": "2016-05-04", "authors_parsed": [["Kamilov", "Ulugbek S.", ""], ["Mansour", "Hassan", ""]]}, {"id": "1512.04808", "submitter": "Sebastian Weichwald", "authors": "Sebastian Weichwald, Bernhard Sch\\\"olkopf, Tonio Ball, Moritz\n  Grosse-Wentrup", "title": "Causal and anti-causal learning in pattern recognition for neuroimaging", "comments": "accepted manuscript", "journal-ref": "Pattern Recognition in Neuroimaging, 2014 International Workshop\n  on, 1-4, 2014", "doi": "10.1109/PRNI.2014.6858551", "report-no": null, "categories": "stat.ML cs.LG q-bio.NC stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Pattern recognition in neuroimaging distinguishes between two types of\nmodels: encoding- and decoding models. This distinction is based on the insight\nthat brain state features, that are found to be relevant in an experimental\nparadigm, carry a different meaning in encoding- than in decoding models. In\nthis paper, we argue that this distinction is not sufficient: Relevant features\nin encoding- and decoding models carry a different meaning depending on whether\nthey represent causal- or anti-causal relations. We provide a theoretical\njustification for this argument and conclude that causal inference is essential\nfor interpretation in neuroimaging.\n", "versions": [{"version": "v1", "created": "Tue, 15 Dec 2015 15:05:00 GMT"}], "update_date": "2015-12-16", "authors_parsed": [["Weichwald", "Sebastian", ""], ["Sch\u00f6lkopf", "Bernhard", ""], ["Ball", "Tonio", ""], ["Grosse-Wentrup", "Moritz", ""]]}, {"id": "1512.04829", "submitter": "Wouter Kouw", "authors": "Wouter M. Kouw, Jesse H. Krijthe, Marco Loog and Laurens J.P. van der\n  Maaten", "title": "Feature-Level Domain Adaptation", "comments": "32 pages, 13 figures, 9 tables", "journal-ref": "JMLR 17:171 (2016) 1-32", "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Domain adaptation is the supervised learning setting in which the training\nand test data are sampled from different distributions: training data is\nsampled from a source domain, whilst test data is sampled from a target domain.\nThis paper proposes and studies an approach, called feature-level domain\nadaptation (FLDA), that models the dependence between the two domains by means\nof a feature-level transfer model that is trained to describe the transfer from\nsource to target domain. Subsequently, we train a domain-adapted classifier by\nminimizing the expected loss under the resulting transfer model. For linear\nclassifiers and a large family of loss functions and transfer models, this\nexpected loss can be computed or approximated analytically, and minimized\nefficiently. Our empirical evaluation of FLDA focuses on problems comprising\nbinary and count data in which the transfer can be naturally modeled via a\ndropout distribution, which allows the classifier to adapt to differences in\nthe marginal probability of features in the source and the target domain. Our\nexperiments on several real-world problems show that FLDA performs on par with\nstate-of-the-art domain-adaptation techniques.\n", "versions": [{"version": "v1", "created": "Tue, 15 Dec 2015 15:55:55 GMT"}, {"version": "v2", "created": "Tue, 7 Jun 2016 20:06:47 GMT"}], "update_date": "2016-10-21", "authors_parsed": [["Kouw", "Wouter M.", ""], ["Krijthe", "Jesse H.", ""], ["Loog", "Marco", ""], ["van der Maaten", "Laurens J. P.", ""]]}, {"id": "1512.04848", "submitter": "Travis Dick", "authors": "Travis Dick, Mu Li, Venkata Krishna Pillutla, Colin White, Maria\n  Florina Balcan, Alex Smola", "title": "Data Driven Resource Allocation for Distributed Learning", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.DS stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In distributed machine learning, data is dispatched to multiple machines for\nprocessing. Motivated by the fact that similar data points often belong to the\nsame or similar classes, and more generally, classification rules of high\naccuracy tend to be \"locally simple but globally complex\" (Vapnik & Bottou\n1993), we propose data dependent dispatching that takes advantage of such\nstructure. We present an in-depth analysis of this model, providing new\nalgorithms with provable worst-case guarantees, analysis proving existing\nscalable heuristics perform well in natural non worst-case conditions, and\ntechniques for extending a dispatching rule from a small sample to the entire\ndistribution. We overcome novel technical challenges to satisfy important\nconditions for accurate distributed learning, including fault tolerance and\nbalancedness. We empirically compare our approach with baselines based on\nrandom partitioning, balanced partition trees, and locality sensitive hashing,\nshowing that we achieve significantly higher accuracy on both synthetic and\nreal world image and advertising datasets. We also demonstrate that our\ntechnique strongly scales with the available computing power.\n", "versions": [{"version": "v1", "created": "Tue, 15 Dec 2015 16:41:42 GMT"}, {"version": "v2", "created": "Thu, 15 Dec 2016 20:45:52 GMT"}], "update_date": "2016-12-16", "authors_parsed": [["Dick", "Travis", ""], ["Li", "Mu", ""], ["Pillutla", "Venkata Krishna", ""], ["White", "Colin", ""], ["Balcan", "Maria Florina", ""], ["Smola", "Alex", ""]]}, {"id": "1512.04937", "submitter": "Amin Jalali", "authors": "Amin Jalali, Qiyang Han, Ioana Dumitriu, Maryam Fazel", "title": "Relative Density and Exact Recovery in Heterogeneous Stochastic Block\n  Models", "comments": "1 figure", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The Stochastic Block Model (SBM) is a widely used random graph model for\nnetworks with communities. Despite the recent burst of interest in recovering\ncommunities in the SBM from statistical and computational points of view, there\nare still gaps in understanding the fundamental information theoretic and\ncomputational limits of recovery. In this paper, we consider the SBM in its\nfull generality, where there is no restriction on the number and sizes of\ncommunities or how they grow with the number of nodes, as well as on the\nconnection probabilities inside or across communities. This generality allows\nus to move past the artifacts of homogenous SBM, and understand the right\nparameters (such as the relative densities of communities) that define the\nvarious recovery thresholds. We outline the implications of our generalizations\nvia a set of illustrative examples. For instance, $\\log n$ is considered to be\nthe standard lower bound on the cluster size for exact recovery via convex\nmethods, for homogenous SBM. We show that it is possible, in the right\ncircumstances (when sizes are spread and the smaller the cluster, the denser),\nto recover very small clusters (up to $\\sqrt{\\log n}$ size), if there are just\na few of them (at most polylogarithmic in $n$).\n", "versions": [{"version": "v1", "created": "Tue, 15 Dec 2015 20:57:28 GMT"}], "update_date": "2015-12-16", "authors_parsed": [["Jalali", "Amin", ""], ["Han", "Qiyang", ""], ["Dumitriu", "Ioana", ""], ["Fazel", "Maryam", ""]]}, {"id": "1512.05010", "submitter": "Dejan Slep\\v{c}ev", "authors": "Slav Kirov and Dejan Slep\\v{c}ev", "title": "Multiple penalized principal curves: analysis and computation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.AP cs.CV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study the problem of finding the one-dimensional structure in a given data\nset. In other words we consider ways to approximate a given measure (data) by\ncurves. We consider an objective functional whose minimizers are a\nregularization of principal curves and introduce a new functional which allows\nfor multiple curves. We prove the existence of minimizers and establish their\nbasic properties. We develop an efficient algorithm for obtaining (near)\nminimizers of the functional. While both of the functionals used are nonconvex,\nwe argue that enlarging the configuration space to allow for multiple curves\nleads to a simpler energy landscape with fewer undesirable (high-energy) local\nminima. Furthermore we note that the approach proposed is able to find the\none-dimensional structure even for data with considerable amount of noise.\n", "versions": [{"version": "v1", "created": "Tue, 15 Dec 2015 23:26:50 GMT"}, {"version": "v2", "created": "Tue, 30 Aug 2016 13:19:05 GMT"}], "update_date": "2016-08-31", "authors_parsed": [["Kirov", "Slav", ""], ["Slep\u010dev", "Dejan", ""]]}, {"id": "1512.05059", "submitter": "Mina Ghashami", "authors": "Mina Ghashami, Daniel Perry, Jeff M. Phillips", "title": "Streaming Kernel Principal Component Analysis", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Kernel principal component analysis (KPCA) provides a concise set of basis\nvectors which capture non-linear structures within large data sets, and is a\ncentral tool in data analysis and learning. To allow for non-linear relations,\ntypically a full $n \\times n$ kernel matrix is constructed over $n$ data\npoints, but this requires too much space and time for large values of $n$.\nTechniques such as the Nystr\\\"om method and random feature maps can help\ntowards this goal, but they do not explicitly maintain the basis vectors in a\nstream and take more space than desired. We propose a new approach for\nstreaming KPCA which maintains a small set of basis elements in a stream,\nrequiring space only logarithmic in $n$, and also improves the dependence on\nthe error parameter. Our technique combines together random feature maps with\nrecent advances in matrix sketching, it has guaranteed spectral norm error\nbounds with respect to the original kernel matrix, and it compares favorably in\npractice to state-of-the-art approaches.\n", "versions": [{"version": "v1", "created": "Wed, 16 Dec 2015 06:12:55 GMT"}], "update_date": "2015-12-17", "authors_parsed": [["Ghashami", "Mina", ""], ["Perry", "Daniel", ""], ["Phillips", "Jeff M.", ""]]}, {"id": "1512.05073", "submitter": "Anish Mukherjee", "authors": "Ayanendranath Basu, Smarajit Bose, Amita Pal, Anish Mukherjee,\n  Debasmita Das", "title": "A Novel Minimum Divergence Approach to Robust Speaker Identification", "comments": "22 pages, 2 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.SD stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this work, a novel solution to the speaker identification problem is\nproposed through minimization of statistical divergences between the\nprobability distribution (g). of feature vectors from the test utterance and\nthe probability distributions of the feature vector corresponding to the\nspeaker classes. This approach is made more robust to the presence of outliers,\nthrough the use of suitably modified versions of the standard divergence\nmeasures. The relevant solutions to the minimum distance methods are referred\nto as the minimum rescaled modified distance estimators (MRMDEs). Three\nmeasures were considered - the likelihood disparity, the Hellinger distance and\nPearson's chi-square distance. The proposed approach is motivated by the\nobservation that, in the case of the likelihood disparity, when the empirical\ndistribution function is used to estimate g, it becomes equivalent to maximum\nlikelihood classification with Gaussian Mixture Models (GMMs) for speaker\nclasses, a highly effective approach used, for example, by Reynolds [22] based\non Mel Frequency Cepstral Coefficients (MFCCs) as features. Significant\nimprovement in classification accuracy is observed under this approach on the\nbenchmark speech corpus NTIMIT and a new bilingual speech corpus NISIS, with\nMFCC features, both in isolation and in combination with delta MFCC features.\nMoreover, the ubiquitous principal component transformation, by itself and in\nconjunction with the principle of classifier combination, is found to further\nenhance the performance.\n", "versions": [{"version": "v1", "created": "Wed, 16 Dec 2015 07:29:53 GMT"}], "update_date": "2015-12-17", "authors_parsed": [["Basu", "Ayanendranath", ""], ["Bose", "Smarajit", ""], ["Pal", "Amita", ""], ["Mukherjee", "Anish", ""], ["Das", "Debasmita", ""]]}, {"id": "1512.05219", "submitter": "Yizhe Zhang", "authors": "Yizhe Zhang, Ricardo Henao, Lawrence Carin, Jianling Zhong and\n  Alexander J. Hartemink", "title": "Learning a Hybrid Architecture for Sequence Regression and Annotation", "comments": "AAAI 2016", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  When learning a hidden Markov model (HMM), sequen- tial observations can\noften be complemented by real-valued summary response variables generated from\nthe path of hid- den states. Such settings arise in numerous domains, includ-\ning many applications in biology, like motif discovery and genome annotation.\nIn this paper, we present a flexible frame- work for jointly modeling both\nlatent sequence features and the functional mapping that relates the summary\nresponse variables to the hidden state sequence. The algorithm is com- patible\nwith a rich set of mapping functions. Results show that the availability of\nadditional continuous response vari- ables can simultaneously improve the\nannotation of the se- quential observations and yield good prediction\nperformance in both synthetic data and real-world datasets.\n", "versions": [{"version": "v1", "created": "Wed, 16 Dec 2015 15:48:40 GMT"}], "update_date": "2015-12-17", "authors_parsed": [["Zhang", "Yizhe", ""], ["Henao", "Ricardo", ""], ["Carin", "Lawrence", ""], ["Zhong", "Jianling", ""], ["Hartemink", "Alexander J.", ""]]}, {"id": "1512.05287", "submitter": "Yarin Gal", "authors": "Yarin Gal, Zoubin Ghahramani", "title": "A Theoretically Grounded Application of Dropout in Recurrent Neural\n  Networks", "comments": "Added clarifications; Published in NIPS 2016", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recurrent neural networks (RNNs) stand at the forefront of many recent\ndevelopments in deep learning. Yet a major difficulty with these models is\ntheir tendency to overfit, with dropout shown to fail when applied to recurrent\nlayers. Recent results at the intersection of Bayesian modelling and deep\nlearning offer a Bayesian interpretation of common deep learning techniques\nsuch as dropout. This grounding of dropout in approximate Bayesian inference\nsuggests an extension of the theoretical results, offering insights into the\nuse of dropout with RNN models. We apply this new variational inference based\ndropout technique in LSTM and GRU models, assessing it on language modelling\nand sentiment analysis tasks. The new approach outperforms existing techniques,\nand to the best of our knowledge improves on the single model state-of-the-art\nin language modelling with the Penn Treebank (73.4 test perplexity). This\nextends our arsenal of variational tools in deep learning.\n", "versions": [{"version": "v1", "created": "Wed, 16 Dec 2015 19:18:43 GMT"}, {"version": "v2", "created": "Thu, 11 Feb 2016 19:27:53 GMT"}, {"version": "v3", "created": "Wed, 25 May 2016 17:45:04 GMT"}, {"version": "v4", "created": "Tue, 4 Oct 2016 16:44:17 GMT"}, {"version": "v5", "created": "Wed, 5 Oct 2016 15:09:30 GMT"}], "update_date": "2016-10-06", "authors_parsed": [["Gal", "Yarin", ""], ["Ghahramani", "Zoubin", ""]]}, {"id": "1512.05294", "submitter": "Harini Suresh", "authors": "Harini Suresh", "title": "Feature Representation for ICU Mortality", "comments": "This article has been withdrawn due by the author due to the need for\n  more testing to verify results", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Good predictors of ICU Mortality have the potential to identify high-risk\npatients earlier, improve ICU resource allocation, or create more accurate\npopulation-level risk models. Machine learning practitioners typically make\nchoices about how to represent features in a particular model, but these\nchoices are seldom evaluated quantitatively. This study compares the\nperformance of different representations of clinical event data from MIMIC II\nin a logistic regression model to predict 36-hour ICU mortality. The most\ncommon representations are linear (normalized counts) and binary (yes/no).\nThese, along with a new representation termed \"hill\", are compared using both\nL1 and L2 regularization. Results indicate that the introduced \"hill\"\nrepresentation outperforms both the binary and linear representations, the hill\nrepresentation thus has the potential to improve existing models of ICU\nmortality.\n", "versions": [{"version": "v1", "created": "Wed, 16 Dec 2015 19:36:06 GMT"}, {"version": "v2", "created": "Sun, 7 Feb 2016 21:59:58 GMT"}], "update_date": "2016-02-09", "authors_parsed": [["Suresh", "Harini", ""]]}, {"id": "1512.05469", "submitter": "Matthew Kusner", "authors": "Matt J. Kusner, Yu Sun, Karthik Sridharan, Kilian Q. Weinberger", "title": "Private Causal Inference", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Causal inference deals with identifying which random variables \"cause\" or\ncontrol other random variables. Recent advances on the topic of causal\ninference based on tools from statistical estimation and machine learning have\nresulted in practical algorithms for causal inference. Causal inference has the\npotential to have significant impact on medical research, prevention and\ncontrol of diseases, and identifying factors that impact economic changes to\nname just a few. However, these promising applications for causal inference are\noften ones that involve sensitive or personal data of users that need to be\nkept private (e.g., medical records, personal finances, etc). Therefore, there\nis a need for the development of causal inference methods that preserve data\nprivacy. We study the problem of inferring causality using the current, popular\ncausal inference framework, the additive noise model (ANM) while simultaneously\nensuring privacy of the users. Our framework provides differential privacy\nguarantees for a variety of ANM variants. We run extensive experiments, and\ndemonstrate that our techniques are practical and easy to implement.\n", "versions": [{"version": "v1", "created": "Thu, 17 Dec 2015 05:46:56 GMT"}, {"version": "v2", "created": "Sat, 20 Aug 2016 10:55:34 GMT"}], "update_date": "2016-08-23", "authors_parsed": [["Kusner", "Matt J.", ""], ["Sun", "Yu", ""], ["Sridharan", "Karthik", ""], ["Weinberger", "Kilian Q.", ""]]}, {"id": "1512.05610", "submitter": "Sami Remes", "authors": "Sami Remes and Tommi Mononen and Samuel Kaski", "title": "Classification of weak multi-view signals by sharing factors in a\n  mixture of Bayesian group factor analyzers", "comments": "Presented at MLINI-2015 workshop, 2015 (arXiv:1605.04435)", "journal-ref": null, "doi": null, "report-no": "MLINI/2015/03", "categories": "stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a novel classification model for weak signal data, building upon a\nrecent model for Bayesian multi-view learning, Group Factor Analysis (GFA).\nInstead of assuming all data to come from a single GFA model, we allow latent\nclusters, each having a different GFA model and producing a different class\ndistribution. We show that sharing information across the clusters, by sharing\nfactors, increases the classification accuracy considerably; the shared factors\nessentially form a flexible noise model that explains away the part of data not\nrelated to classification. Motivation for the setting comes from single-trial\nfunctional brain imaging data, having a very low signal-to-noise ratio and a\nnatural multi-view setting, with the different sensors, measurement modalities\n(EEG, MEG, fMRI) and possible auxiliary information as views. We demonstrate\nour model on a MEG dataset.\n", "versions": [{"version": "v1", "created": "Thu, 17 Dec 2015 14:46:20 GMT"}, {"version": "v2", "created": "Tue, 7 Jun 2016 18:37:29 GMT"}], "update_date": "2016-06-08", "authors_parsed": [["Remes", "Sami", ""], ["Mononen", "Tommi", ""], ["Kaski", "Samuel", ""]]}, {"id": "1512.05665", "submitter": "Ulrich Schaechtle", "authors": "Ulrich Schaechtle, Ben Zinberg, Alexey Radul, Kostas Stathis and\n  Vikash K. Mansinghka", "title": "Probabilistic Programming with Gaussian Process Memoization", "comments": "36 pages, 9 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Gaussian Processes (GPs) are widely used tools in statistics, machine\nlearning, robotics, computer vision, and scientific computation. However,\ndespite their popularity, they can be difficult to apply; all but the simplest\nclassification or regression applications require specification and inference\nover complex covariance functions that do not admit simple analytical\nposteriors. This paper shows how to embed Gaussian processes in any\nhigher-order probabilistic programming language, using an idiom based on\nmemoization, and demonstrates its utility by implementing and extending classic\nand state-of-the-art GP applications. The interface to Gaussian processes,\ncalled gpmem, takes an arbitrary real-valued computational process as input and\nreturns a statistical emulator that automatically improve as the original\nprocess is invoked and its input-output behavior is recorded. The flexibility\nof gpmem is illustrated via three applications: (i) robust GP regression with\nhierarchical hyper-parameter learning, (ii) discovering symbolic expressions\nfrom time-series data by fully Bayesian structure learning over kernels\ngenerated by a stochastic grammar, and (iii) a bandit formulation of Bayesian\noptimization with automatic inference and action selection. All applications\nshare a single 50-line Python library and require fewer than 20 lines of\nprobabilistic code each.\n", "versions": [{"version": "v1", "created": "Thu, 17 Dec 2015 16:46:10 GMT"}, {"version": "v2", "created": "Tue, 5 Jan 2016 10:55:02 GMT"}], "update_date": "2016-01-06", "authors_parsed": [["Schaechtle", "Ulrich", ""], ["Zinberg", "Ben", ""], ["Radul", "Alexey", ""], ["Stathis", "Kostas", ""], ["Mansinghka", "Vikash K.", ""]]}, {"id": "1512.05698", "submitter": "Wojciech Rejchel", "authors": "Wojciech Rejchel", "title": "Oracle inequalities for ranking and U-processes with Lasso penalty", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We investigate properties of estimators obtained by minimization of\nU-processes with the Lasso penalty in high-dimensional settings. Our attention\nis focused on the ranking problem that is popular in machine learning. It is\nrelated to guessing the ordering between objects on the basis of their observed\npredictors. We prove the oracle inequality for the excess risk of the\nconsidered estimator as well as the bound for the l1 distance between the\nestimator and the oracle.\n", "versions": [{"version": "v1", "created": "Thu, 17 Dec 2015 17:57:21 GMT"}], "update_date": "2015-12-18", "authors_parsed": [["Rejchel", "Wojciech", ""]]}, {"id": "1512.05742", "submitter": "Iulian Vlad Serban", "authors": "Iulian Vlad Serban, Ryan Lowe, Peter Henderson, Laurent Charlin,\n  Joelle Pineau", "title": "A Survey of Available Corpora for Building Data-Driven Dialogue Systems", "comments": "56 pages including references and appendix, 5 tables and 1 figure;\n  Under review for the Dialogue & Discourse journal. Update: paper has been\n  rewritten and now includes several new datasets", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI cs.HC cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  During the past decade, several areas of speech and language understanding\nhave witnessed substantial breakthroughs from the use of data-driven models. In\nthe area of dialogue systems, the trend is less obvious, and most practical\nsystems are still built through significant engineering and expert knowledge.\nNevertheless, several recent results suggest that data-driven approaches are\nfeasible and quite promising. To facilitate research in this area, we have\ncarried out a wide survey of publicly available datasets suitable for\ndata-driven learning of dialogue systems. We discuss important characteristics\nof these datasets, how they can be used to learn diverse dialogue strategies,\nand their other potential uses. We also examine methods for transfer learning\nbetween datasets and the use of external knowledge. Finally, we discuss\nappropriate choice of evaluation metrics for the learning objective.\n", "versions": [{"version": "v1", "created": "Thu, 17 Dec 2015 19:52:39 GMT"}, {"version": "v2", "created": "Tue, 22 Dec 2015 04:58:05 GMT"}, {"version": "v3", "created": "Tue, 21 Mar 2017 01:15:32 GMT"}], "update_date": "2017-03-22", "authors_parsed": [["Serban", "Iulian Vlad", ""], ["Lowe", "Ryan", ""], ["Henderson", "Peter", ""], ["Charlin", "Laurent", ""], ["Pineau", "Joelle", ""]]}, {"id": "1512.05840", "submitter": "Pau Perng-Hwa Kung", "authors": "Pau Perng-Hwa Kung", "title": "Deep Poisson Factorization Machines: factor analysis for mapping\n  behaviors in journalist ecosystem", "comments": "Incomplete work, will re-upload once the details and implementations\n  are straightened out", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CY cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Newsroom in online ecosystem is difficult to untangle. With prevalence of\nsocial media, interactions between journalists and individuals become visible,\nbut lack of understanding to inner processing of information feedback loop in\npublic sphere leave most journalists baffled. Can we provide an organized view\nto characterize journalist behaviors on individual level to know better of the\necosystem? To this end, I propose Poisson Factorization Machine (PFM), a\nBayesian analogue to matrix factorization that assumes Poisson distribution for\ngenerative process. The model generalizes recent studies on Poisson Matrix\nFactorization to account temporal interaction which involves tensor-like\nstructure, and label information. Two inference procedures are designed, one\nbased on batch variational EM and another stochastic variational inference\nscheme that efficiently scales with data size. An important novelty in this\nnote is that I show how to stack layers of PFM to introduce a deep\narchitecture. This work discusses some potential results applying the model and\nexplains how such latent factors may be useful for analyzing latent behaviors\nfor data exploration.\n", "versions": [{"version": "v1", "created": "Fri, 18 Dec 2015 01:25:47 GMT"}, {"version": "v2", "created": "Fri, 29 Dec 2017 21:35:25 GMT"}], "update_date": "2018-01-03", "authors_parsed": [["Kung", "Pau Perng-Hwa", ""]]}, {"id": "1512.05844", "submitter": "Alexander Wong", "authors": "Mohammad Javad Shafiee, Parthipan Siva, Paul Fieguth, and Alexander\n  Wong", "title": "Domain Adaptation and Transfer Learning in StochasticNets", "comments": null, "journal-ref": "Vision Letters, Vol. 1, No. 1, pp. VL115, 2015", "doi": "10.15353/vsnl.v1i1.44", "report-no": null, "categories": "cs.CV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Transfer learning is a recent field of machine learning research that aims to\nresolve the challenge of dealing with insufficient training data in the domain\nof interest. This is a particular issue with traditional deep neural networks\nwhere a large amount of training data is needed. Recently, StochasticNets was\nproposed to take advantage of sparse connectivity in order to decrease the\nnumber of parameters that needs to be learned, which in turn may relax training\ndata size requirements. In this paper, we study the efficacy of transfer\nlearning on StochasticNet frameworks. Experimental results show ~7% improvement\non StochasticNet performance when the transfer learning is applied in training\nstep.\n", "versions": [{"version": "v1", "created": "Fri, 18 Dec 2015 03:09:29 GMT"}], "update_date": "2015-12-21", "authors_parsed": [["Shafiee", "Mohammad Javad", ""], ["Siva", "Parthipan", ""], ["Fieguth", "Paul", ""], ["Wong", "Alexander", ""]]}, {"id": "1512.06061", "submitter": "Brijnesh Jain", "authors": "Brijnesh Jain", "title": "Asymptotic Behavior of Mean Partitions in Consensus Clustering", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Although consistency is a minimum requirement of any estimator, little is\nknown about consistency of the mean partition approach in consensus clustering.\nThis contribution studies the asymptotic behavior of mean partitions. We show\nthat under normal assumptions, the mean partition approach is consistent and\nasymptotic normal. To derive both results, we represent partitions as points of\nsome geometric space, called orbit space. Then we draw on results from the\ntheory of Fr\\'echet means and stochastic programming. The asymptotic properties\nhold for continuous extensions of standard cluster criteria (indices). The\nresults justify consensus clustering using finite but sufficiently large sample\nsizes. Furthermore, the orbit space framework provides a mathematical\nfoundation for studying further statistical, geometrical, and analytical\nproperties of sets of partitions.\n", "versions": [{"version": "v1", "created": "Fri, 18 Dec 2015 17:59:49 GMT"}], "update_date": "2015-12-21", "authors_parsed": [["Jain", "Brijnesh", ""]]}, {"id": "1512.06086", "submitter": "Nicolas Dobigeon", "authors": "Cl\\'ement Elvira, Pierre Chainais and Nicolas Dobigeon", "title": "Bayesian anti-sparse coding", "comments": null, "journal-ref": null, "doi": "10.1109/TSP.2016.2645543", "report-no": null, "categories": "stat.ML physics.data-an stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Sparse representations have proven their efficiency in solving a wide class\nof inverse problems encountered in signal and image processing. Conversely,\nenforcing the information to be spread uniformly over representation\ncoefficients exhibits relevant properties in various applications such as\ndigital communications. Anti-sparse regularization can be naturally expressed\nthrough an $\\ell_{\\infty}$-norm penalty. This paper derives a probabilistic\nformulation of such representations. A new probability distribution, referred\nto as the democratic prior, is first introduced. Its main properties as well as\nthree random variate generators for this distribution are derived. Then this\nprobability distribution is used as a prior to promote anti-sparsity in a\nGaussian linear inverse problem, yielding a fully Bayesian formulation of\nanti-sparse coding. Two Markov chain Monte Carlo (MCMC) algorithms are proposed\nto generate samples according to the posterior distribution. The first one is a\nstandard Gibbs sampler. The second one uses Metropolis-Hastings moves that\nexploit the proximity mapping of the log-posterior distribution. These samples\nare used to approximate maximum a posteriori and minimum mean square error\nestimators of both parameters and hyperparameters. Simulations on synthetic\ndata illustrate the performances of the two proposed samplers, for both\ncomplete and over-complete dictionaries. All results are compared to the recent\ndeterministic variational FITRA algorithm.\n", "versions": [{"version": "v1", "created": "Fri, 18 Dec 2015 19:37:24 GMT"}], "update_date": "2017-12-29", "authors_parsed": [["Elvira", "Cl\u00e9ment", ""], ["Chainais", "Pierre", ""], ["Dobigeon", "Nicolas", ""]]}, {"id": "1512.06098", "submitter": "Botond Cseke", "authors": "Botond Cseke, David Schnoerr, Manfred Opper, Guido Sanguinetti", "title": "Expectation propagation for continuous time stochastic processes", "comments": null, "journal-ref": null, "doi": "10.1088/1751-8113/49/49/494002", "report-no": null, "categories": "stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider the inverse problem of reconstructing the posterior measure over\nthe trajec- tories of a diffusion process from discrete time observations and\ncontinuous time constraints. We cast the problem in a Bayesian framework and\nderive approximations to the posterior distributions of single time marginals\nusing variational approximate inference. We then show how the approximation can\nbe extended to a wide class of discrete-state Markov jump pro- cesses by making\nuse of the chemical Langevin equation. Our empirical results show that the\nproposed method is computationally efficient and provides good approximations\nfor these classes of inverse problems.\n", "versions": [{"version": "v1", "created": "Fri, 18 Dec 2015 20:26:00 GMT"}, {"version": "v2", "created": "Tue, 28 Jun 2016 13:48:21 GMT"}], "update_date": "2016-12-21", "authors_parsed": [["Cseke", "Botond", ""], ["Schnoerr", "David", ""], ["Opper", "Manfred", ""], ["Sanguinetti", "Guido", ""]]}, {"id": "1512.06171", "submitter": "Alex Gibberd Mr", "authors": "Alexander J. Gibberd and James D. B. Nelson", "title": "Regularized Estimation of Piecewise Constant Gaussian Graphical Models:\n  The Group-Fused Graphical Lasso", "comments": "32 pages, 9 figures", "journal-ref": "Journal of Computational and Graphical Statistics, 2017, Volume\n  26, Number 3, pp 623--634", "doi": "10.1080/10618600.2017.1302340", "report-no": null, "categories": "stat.ME stat.CO stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The time-evolving precision matrix of a piecewise-constant Gaussian graphical\nmodel encodes the dynamic conditional dependency structure of a multivariate\ntime-series. Traditionally, graphical models are estimated under the assumption\nthat data is drawn identically from a generating distribution. Introducing\nsparsity and sparse-difference inducing priors we relax these assumptions and\npropose a novel regularized M-estimator to jointly estimate both the graph and\nchangepoint structure. The resulting estimator possesses the ability to\ntherefore favor sparse dependency structures and/or smoothly evolving graph\nstructures, as required. Moreover, our approach extends current methods to\nallow estimation of changepoints that are grouped across multiple dependencies\nin a system. An efficient algorithm for estimating structure is proposed. We\nstudy the empirical recovery properties in a synthetic setting. The qualitative\neffect of grouped changepoint estimation is then demonstrated by applying the\nmethod on two real-world data-sets.\n", "versions": [{"version": "v1", "created": "Sat, 19 Dec 2015 00:53:58 GMT"}, {"version": "v2", "created": "Tue, 31 Oct 2017 16:28:38 GMT"}], "update_date": "2017-11-01", "authors_parsed": [["Gibberd", "Alexander J.", ""], ["Nelson", "James D. B.", ""]]}, {"id": "1512.06228", "submitter": "Abhijit Sharang", "authors": "Abhijit Sharang and Chetan Rao", "title": "Using machine learning for medium frequency derivative portfolio trading", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "q-fin.TR cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We use machine learning for designing a medium frequency trading strategy for\na portfolio of 5 year and 10 year US Treasury note futures. We formulate this\nas a classification problem where we predict the weekly direction of movement\nof the portfolio using features extracted from a deep belief network trained on\ntechnical indicators of the portfolio constituents. The experimentation shows\nthat the resulting pipeline is effective in making a profitable trade.\n", "versions": [{"version": "v1", "created": "Sat, 19 Dec 2015 11:45:13 GMT"}], "update_date": "2015-12-22", "authors_parsed": [["Sharang", "Abhijit", ""], ["Rao", "Chetan", ""]]}, {"id": "1512.06293", "submitter": "Thomas Wiatowski", "authors": "Thomas Wiatowski and Helmut B\\\"olcskei", "title": "A Mathematical Theory of Deep Convolutional Neural Networks for Feature\n  Extraction", "comments": "IEEE Transactions on Information Theory, to appear", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IT cs.AI cs.LG math.FA math.IT stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Deep convolutional neural networks have led to breakthrough results in\nnumerous practical machine learning tasks such as classification of images in\nthe ImageNet data set, control-policy-learning to play Atari games or the board\ngame Go, and image captioning. Many of these applications first perform feature\nextraction and then feed the results thereof into a trainable classifier. The\nmathematical analysis of deep convolutional neural networks for feature\nextraction was initiated by Mallat, 2012. Specifically, Mallat considered\nso-called scattering networks based on a wavelet transform followed by the\nmodulus non-linearity in each network layer, and proved translation invariance\n(asymptotically in the wavelet scale parameter) and deformation stability of\nthe corresponding feature extractor. This paper complements Mallat's results by\ndeveloping a theory that encompasses general convolutional transforms, or in\nmore technical parlance, general semi-discrete frames (including\nWeyl-Heisenberg filters, curvelets, shearlets, ridgelets, wavelets, and learned\nfilters), general Lipschitz-continuous non-linearities (e.g., rectified linear\nunits, shifted logistic sigmoids, hyperbolic tangents, and modulus functions),\nand general Lipschitz-continuous pooling operators emulating, e.g.,\nsub-sampling and averaging. In addition, all of these elements can be different\nin different network layers. For the resulting feature extractor we prove a\ntranslation invariance result of vertical nature in the sense of the features\nbecoming progressively more translation-invariant with increasing network\ndepth, and we establish deformation sensitivity bounds that apply to signal\nclasses such as, e.g., band-limited functions, cartoon functions, and Lipschitz\nfunctions.\n", "versions": [{"version": "v1", "created": "Sat, 19 Dec 2015 22:31:24 GMT"}, {"version": "v2", "created": "Thu, 1 Sep 2016 10:12:30 GMT"}, {"version": "v3", "created": "Tue, 24 Oct 2017 06:44:21 GMT"}], "update_date": "2017-10-25", "authors_parsed": [["Wiatowski", "Thomas", ""], ["B\u00f6lcskei", "Helmut", ""]]}, {"id": "1512.06452", "submitter": "Hossein Soleimani", "authors": "Hossein Soleimani, David J. Miller", "title": "ATD: Anomalous Topic Discovery in High Dimensional Discrete Data", "comments": null, "journal-ref": null, "doi": "10.1109/TKDE.2016.2561288", "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose an algorithm for detecting patterns exhibited by anomalous\nclusters in high dimensional discrete data. Unlike most anomaly detection (AD)\nmethods, which detect individual anomalies, our proposed method detects groups\n(clusters) of anomalies; i.e. sets of points which collectively exhibit\nabnormal patterns. In many applications this can lead to better understanding\nof the nature of the atypical behavior and to identifying the sources of the\nanomalies. Moreover, we consider the case where the atypical patterns exhibit\non only a small (salient) subset of the very high dimensional feature space.\nIndividual AD techniques and techniques that detect anomalies using all the\nfeatures typically fail to detect such anomalies, but our method can detect\nsuch instances collectively, discover the shared anomalous patterns exhibited\nby them, and identify the subsets of salient features. In this paper, we focus\non detecting anomalous topics in a batch of text documents, developing our\nalgorithm based on topic models. Results of our experiments show that our\nmethod can accurately detect anomalous topics and salient features (words)\nunder each such topic in a synthetic data set and two real-world text corpora\nand achieves better performance compared to both standard group AD and\nindividual AD techniques. All required code to reproduce our experiments is\navailable from https://github.com/hsoleimani/ATD\n", "versions": [{"version": "v1", "created": "Sun, 20 Dec 2015 22:55:39 GMT"}, {"version": "v2", "created": "Fri, 20 May 2016 17:39:57 GMT"}], "update_date": "2016-05-23", "authors_parsed": [["Soleimani", "Hossein", ""], ["Miller", "David J.", ""]]}, {"id": "1512.06730", "submitter": "Shuchin Aeron", "authors": "Eric Kernfeld, Nathan Majumder, Shuchin Aeron, Misha Kilmer", "title": "Multilinear Subspace Clustering", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IT cs.CV cs.LG math.IT stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper we present a new model and an algorithm for unsupervised\nclustering of 2-D data such as images. We assume that the data comes from a\nunion of multilinear subspaces (UOMS) model, which is a specific structured\ncase of the much studied union of subspaces (UOS) model. For segmentation under\nthis model, we develop Multilinear Subspace Clustering (MSC) algorithm and\nevaluate its performance on the YaleB and Olivietti image data sets. We show\nthat MSC is highly competitive with existing algorithms employing the UOS model\nin terms of clustering performance while enjoying improvement in computational\ncomplexity.\n", "versions": [{"version": "v1", "created": "Mon, 21 Dec 2015 17:53:35 GMT"}], "update_date": "2015-12-22", "authors_parsed": [["Kernfeld", "Eric", ""], ["Majumder", "Nathan", ""], ["Aeron", "Shuchin", ""], ["Kilmer", "Misha", ""]]}, {"id": "1512.06789", "submitter": "Pedro Alejandro Ortega", "authors": "Pedro A. Ortega, Daniel A. Braun, Justin Dyer, Kee-Eung Kim and\n  Naftali Tishby", "title": "Information-Theoretic Bounded Rationality", "comments": "47 pages, 19 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.AI cs.SY math.OC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Bounded rationality, that is, decision-making and planning under resource\nlimitations, is widely regarded as an important open problem in artificial\nintelligence, reinforcement learning, computational neuroscience and economics.\nThis paper offers a consolidated presentation of a theory of bounded\nrationality based on information-theoretic ideas. We provide a conceptual\njustification for using the free energy functional as the objective function\nfor characterizing bounded-rational decisions. This functional possesses three\ncrucial properties: it controls the size of the solution space; it has Monte\nCarlo planners that are exact, yet bypass the need for exhaustive search; and\nit captures model uncertainty arising from lack of evidence or from interacting\nwith other agents having unknown intentions. We discuss the single-step\ndecision-making case, and show how to extend it to sequential decisions using\nequivalence transformations. This extension yields a very general class of\ndecision problems that encompass classical decision rules (e.g. EXPECTIMAX and\nMINIMAX) as limit cases, as well as trust- and risk-sensitive planning.\n", "versions": [{"version": "v1", "created": "Mon, 21 Dec 2015 19:58:46 GMT"}], "update_date": "2015-12-22", "authors_parsed": [["Ortega", "Pedro A.", ""], ["Braun", "Daniel A.", ""], ["Dyer", "Justin", ""], ["Kim", "Kee-Eung", ""], ["Tishby", "Naftali", ""]]}, {"id": "1512.06888", "submitter": "Peter Landgren", "authors": "Peter Landgren, Vaibhav Srivastava, Naomi Ehrich Leonard", "title": "On Distributed Cooperative Decision-Making in Multiarmed Bandits", "comments": "This revision provides a correction to the original paper, which\n  appeared in the Proceedings of the 2016 European Control Conference (ECC).\n  The second statement of Proposition 1, Theorem 1 and their proofs are new.\n  The new Theorem 1 is used to prove the regret bounds in Theorem 2", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.SY cs.MA math.OC stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study the explore-exploit tradeoff in distributed cooperative\ndecision-making using the context of the multiarmed bandit (MAB) problem. For\nthe distributed cooperative MAB problem, we design the cooperative UCB\nalgorithm that comprises two interleaved distributed processes: (i) running\nconsensus algorithms for estimation of rewards, and (ii)\nupper-confidence-bound-based heuristics for selection of arms. We rigorously\nanalyze the performance of the cooperative UCB algorithm and characterize the\ninfluence of communication graph structure on the decision-making performance\nof the group.\n", "versions": [{"version": "v1", "created": "Mon, 21 Dec 2015 21:56:04 GMT"}, {"version": "v2", "created": "Mon, 16 May 2016 19:23:17 GMT"}, {"version": "v3", "created": "Mon, 16 Sep 2019 05:08:05 GMT"}], "update_date": "2019-09-17", "authors_parsed": [["Landgren", "Peter", ""], ["Srivastava", "Vaibhav", ""], ["Leonard", "Naomi Ehrich", ""]]}, {"id": "1512.06929", "submitter": "Anthony Scopatz", "authors": "Anthony Scopatz", "title": "Facility Deployment Decisions through Warp Optimizaton of Regressed\n  Gaussian Processes", "comments": "Number of Pages: 35, Number of Tables: 0, Number of Figures: 11", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.OC physics.data-an stat.ML", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  A method for quickly determining deployment schedules that meet a given fuel\ncycle demand is presented here. This algorithm is fast enough to perform in\nsitu within low-fidelity fuel cycle simulators. It uses Gaussian process\nregression models to predict the production curve as a function of time and the\nnumber of deployed facilities. Each of these predictions is measured against\nthe demand curve using the dynamic time warping distance. The minimum distance\ndeployment schedule is evaluated in a full fuel cycle simulation, whose\ngenerated production curve then informs the model on the next optimization\niteration. The method converges within five to ten iterations to a distance\nthat is less than one percent of the total deployable production. A\nrepresentative once-through fuel cycle is used to demonstrate the methodology\nfor reactor deployment.\n", "versions": [{"version": "v1", "created": "Tue, 22 Dec 2015 02:06:31 GMT"}], "update_date": "2015-12-23", "authors_parsed": [["Scopatz", "Anthony", ""]]}, {"id": "1512.06992", "submitter": "Christos Dimitrakakis", "authors": "Zuhe Zhang, Benjamin Rubinstein, Christos Dimitrakakis", "title": "On the Differential Privacy of Bayesian Inference", "comments": "AAAI 2016, Feb 2016, Phoenix, Arizona, United States", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.CR cs.LG math.ST stat.ML stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study how to communicate findings of Bayesian inference to third parties,\nwhile preserving the strong guarantee of differential privacy. Our main\ncontributions are four different algorithms for private Bayesian inference on\nproba-bilistic graphical models. These include two mechanisms for adding noise\nto the Bayesian updates, either directly to the posterior parameters, or to\ntheir Fourier transform so as to preserve update consistency. We also utilise a\nrecently introduced posterior sampling mechanism, for which we prove bounds for\nthe specific but general case of discrete Bayesian networks; and we introduce a\nmaximum-a-posteriori private mechanism. Our analysis includes utility and\nprivacy bounds, with a novel focus on the influence of graph structure on\nprivacy. Worked examples and experiments with Bayesian na{\\\"i}ve Bayes and\nBayesian linear regression illustrate the application of our mechanisms.\n", "versions": [{"version": "v1", "created": "Tue, 22 Dec 2015 09:22:39 GMT"}], "update_date": "2015-12-23", "authors_parsed": [["Zhang", "Zuhe", ""], ["Rubinstein", "Benjamin", ""], ["Dimitrakakis", "Christos", ""]]}, {"id": "1512.06999", "submitter": "Gael Varoquaux", "authors": "Ga\\\"el Varoquaux (PARIETAL), Michael Eickenberg (PARIETAL), Elvis\n  Dohmatob (PARIETAL), Bertand Thirion (PARIETAL)", "title": "FAASTA: A fast solver for total-variation regularization of\n  ill-conditioned problems with application to brain imaging", "comments": null, "journal-ref": "Colloque GRETSI, Sep 2015, Lyon, France. Gretsi, 2015,\n  http://www.gretsi.fr/colloque2015/myGretsi/programme.php", "doi": null, "report-no": null, "categories": "q-bio.NC cs.LG stat.CO stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The total variation (TV) penalty, as many other analysis-sparsity problems,\ndoes not lead to separable factors or a proximal operatorwith a closed-form\nexpression, such as soft thresholding for the $\\ell\\_1$ penalty. As a result,\nin a variational formulation of an inverse problem or statisticallearning\nestimation, it leads to challenging non-smooth optimization problemsthat are\noften solved with elaborate single-step first-order methods. When thedata-fit\nterm arises from empirical measurements, as in brain imaging, it isoften very\nill-conditioned and without simple structure. In this situation, in proximal\nsplitting methods, the computation cost of thegradient step can easily dominate\neach iteration. Thus it is beneficialto minimize the number of gradient\nsteps.We present fAASTA, a variant of FISTA, that relies on an internal solver\nforthe TV proximal operator, and refines its tolerance to balance\ncomputationalcost of the gradient and the proximal steps. We give benchmarks\nandillustrations on \"brain decoding\": recovering brain maps from\nnoisymeasurements to predict observed behavior. The algorithm as well as\ntheempirical study of convergence speed are valuable for any non-exact\nproximaloperator, in particular analysis-sparsity problems.\n", "versions": [{"version": "v1", "created": "Tue, 22 Dec 2015 09:35:55 GMT"}], "update_date": "2015-12-23", "authors_parsed": [["Varoquaux", "Ga\u00ebl", "", "PARIETAL"], ["Eickenberg", "Michael", "", "PARIETAL"], ["Dohmatob", "Elvis", "", "PARIETAL"], ["Thirion", "Bertand", "", "PARIETAL"]]}, {"id": "1512.07041", "submitter": "Andrey Makarenko", "authors": "A.V. Makarenko, M.G. Volovik", "title": "Implementation of deep learning algorithm for automatic detection of\n  brain tumors using intraoperative IR-thermal mapping data", "comments": "7 pages, 5 figures, 2 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG q-bio.QM stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The efficiency of deep machine learning for automatic delineation of tumor\nareas has been demonstrated for intraoperative neuronavigation using active\nIR-mapping with the use of the cold test. The proposed approach employs a\nmatrix IR-imager to remotely register the space-time distribution of surface\ntemperature pattern, which is determined by the dynamics of local cerebral\nblood flow. The advantages of this technique are non-invasiveness, zero risks\nfor the health of patients and medical staff, low implementation and\noperational costs, ease and speed of use. Traditional IR-diagnostic technique\nhas a crucial limitation - it involves a diagnostician who determines the\nboundaries of tumor areas, which gives rise to considerable uncertainty, which\ncan lead to diagnosis errors that are difficult to control. The current study\ndemonstrates that implementing deep learning algorithms allows to eliminate the\nexplained drawback.\n", "versions": [{"version": "v1", "created": "Tue, 22 Dec 2015 11:52:26 GMT"}], "update_date": "2015-12-23", "authors_parsed": [["Makarenko", "A. V.", ""], ["Volovik", "M. G.", ""]]}, {"id": "1512.07146", "submitter": "Steve Hanneke", "authors": "Steve Hanneke", "title": "Refined Error Bounds for Several Learning Algorithms", "comments": null, "journal-ref": "Journal of Machine Learning Research, Vol. 17 (2016), No. 135, pp.\n  1-55", "doi": null, "report-no": null, "categories": "cs.LG math.ST stat.ML stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This article studies the achievable guarantees on the error rates of certain\nlearning algorithms, with particular focus on refining logarithmic factors.\nMany of the results are based on a general technique for obtaining bounds on\nthe error rates of sample-consistent classifiers with monotonic error regions,\nin the realizable case. We prove bounds of this type expressed in terms of\neither the VC dimension or the sample compression size. This general technique\nalso enables us to derive several new bounds on the error rates of general\nsample-consistent learning algorithms, as well as refined bounds on the label\ncomplexity of the CAL active learning algorithm. Additionally, we establish a\nsimple necessary and sufficient condition for the existence of a\ndistribution-free bound on the error rates of all sample-consistent learning\nrules, converging at a rate inversely proportional to the sample size. We also\nstudy learning in the presence of classification noise, deriving a new excess\nerror rate guarantee for general VC classes under Tsybakov's noise condition,\nand establishing a simple and general necessary and sufficient condition for\nthe minimax excess risk under bounded noise to converge at a rate inversely\nproportional to the sample size.\n", "versions": [{"version": "v1", "created": "Tue, 22 Dec 2015 16:17:43 GMT"}, {"version": "v2", "created": "Sat, 10 Sep 2016 15:11:33 GMT"}], "update_date": "2016-09-13", "authors_parsed": [["Hanneke", "Steve", ""]]}, {"id": "1512.07336", "submitter": "Pengtao Xie", "authors": "Pengtao Xie, Yuntian Deng, Eric Xing", "title": "Latent Variable Modeling with Diversity-Inducing Mutual Angular\n  Regularization", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Latent Variable Models (LVMs) are a large family of machine learning models\nproviding a principled and effective way to extract underlying patterns,\nstructure and knowledge from observed data. Due to the dramatic growth of\nvolume and complexity of data, several new challenges have emerged and cannot\nbe effectively addressed by existing LVMs: (1) How to capture long-tail\npatterns that carry crucial information when the popularity of patterns is\ndistributed in a power-law fashion? (2) How to reduce model complexity and\ncomputational cost without compromising the modeling power of LVMs? (3) How to\nimprove the interpretability and reduce the redundancy of discovered patterns?\nTo addresses the three challenges discussed above, we develop a novel\nregularization technique for LVMs, which controls the geometry of the latent\nspace during learning to enable the learned latent components of LVMs to be\ndiverse in the sense that they are favored to be mutually different from each\nother, to accomplish long-tail coverage, low redundancy, and better\ninterpretability. We propose a mutual angular regularizer (MAR) to encourage\nthe components in LVMs to have larger mutual angles. The MAR is non-convex and\nnon-smooth, entailing great challenges for optimization. To cope with this\nissue, we derive a smooth lower bound of the MAR and optimize the lower bound\ninstead. We show that the monotonicity of the lower bound is closely aligned\nwith the MAR to qualify the lower bound as a desirable surrogate of the MAR.\nUsing neural network (NN) as an instance, we analyze how the MAR affects the\ngeneralization performance of NN. On two popular latent variable models ---\nrestricted Boltzmann machine and distance metric learning, we demonstrate that\nMAR can effectively capture long-tail patterns, reduce model complexity without\nsacrificing expressivity and improve interpretability.\n", "versions": [{"version": "v1", "created": "Wed, 23 Dec 2015 02:29:39 GMT"}], "update_date": "2015-12-24", "authors_parsed": [["Xie", "Pengtao", ""], ["Deng", "Yuntian", ""], ["Xing", "Eric", ""]]}, {"id": "1512.07344", "submitter": "Xin Yuan", "authors": "Yunchen Pu, Xin Yuan, Andrew Stevens, Chunyuan Li, Lawrence Carin", "title": "A Deep Generative Deconvolutional Image Model", "comments": "10 pages, 7 figures. Appearing in Proceedings of the 19th\n  International Conference on Artificial Intelligence and Statistics (AISTATS)\n  2016, Cadiz, Spain. JMLR: W&CP volume 41", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A deep generative model is developed for representation and analysis of\nimages, based on a hierarchical convolutional dictionary-learning framework.\nStochastic {\\em unpooling} is employed to link consecutive layers in the model,\nyielding top-down image generation. A Bayesian support vector machine is linked\nto the top-layer features, yielding max-margin discrimination. Deep\ndeconvolutional inference is employed when testing, to infer the latent\nfeatures, and the top-layer features are connected with the max-margin\nclassifier for discrimination tasks. The model is efficiently trained using a\nMonte Carlo expectation-maximization (MCEM) algorithm, with implementation on\ngraphical processor units (GPUs) for efficient large-scale learning, and fast\ntesting. Excellent results are obtained on several benchmark datasets,\nincluding ImageNet, demonstrating that the proposed model achieves results that\nare highly competitive with similarly sized convolutional neural networks.\n", "versions": [{"version": "v1", "created": "Wed, 23 Dec 2015 03:10:29 GMT"}], "update_date": "2015-12-25", "authors_parsed": [["Pu", "Yunchen", ""], ["Yuan", "Xin", ""], ["Stevens", "Andrew", ""], ["Li", "Chunyuan", ""], ["Carin", "Lawrence", ""]]}, {"id": "1512.07349", "submitter": "Pin-Yu Chen", "authors": "Pin-Yu Chen, Baichuan Zhang, Mohammad Al Hasan, Alfred O. Hero", "title": "Incremental Method for Spectral Clustering of Increasing Orders", "comments": "in KDD workshop on mining and learning graph, 2016\n  http://www.mlgworkshop.org/2016/", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.SI cs.NA stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The smallest eigenvalues and the associated eigenvectors (i.e., eigenpairs)\nof a graph Laplacian matrix have been widely used for spectral clustering and\ncommunity detection. However, in real-life applications the number of clusters\nor communities (say, $K$) is generally unknown a-priori. Consequently, the\nmajority of the existing methods either choose $K$ heuristically or they repeat\nthe clustering method with different choices of $K$ and accept the best\nclustering result. The first option, more often, yields suboptimal result,\nwhile the second option is computationally expensive. In this work, we propose\nan incremental method for constructing the eigenspectrum of the graph Laplacian\nmatrix. This method leverages the eigenstructure of graph Laplacian matrix to\nobtain the $K$-th eigenpairs of the Laplacian matrix given a collection of all\nthe $K-1$ smallest eigenpairs. Our proposed method adapts the Laplacian matrix\nsuch that the batch eigenvalue decomposition problem transforms into an\nefficient sequential leading eigenpair computation problem. As a practical\napplication, we consider user-guided spectral clustering. Specifically, we\ndemonstrate that users can utilize the proposed incremental method for\neffective eigenpair computation and determining the desired number of clusters\nbased on multiple clustering metrics.\n", "versions": [{"version": "v1", "created": "Wed, 23 Dec 2015 03:55:24 GMT"}, {"version": "v2", "created": "Fri, 12 Feb 2016 21:16:01 GMT"}, {"version": "v3", "created": "Thu, 26 May 2016 03:21:30 GMT"}, {"version": "v4", "created": "Sat, 13 Aug 2016 20:30:35 GMT"}], "update_date": "2018-01-24", "authors_parsed": [["Chen", "Pin-Yu", ""], ["Zhang", "Baichuan", ""], ["Hasan", "Mohammad Al", ""], ["Hero", "Alfred O.", ""]]}, {"id": "1512.07372", "submitter": "Pin-Yu Chen", "authors": "Pin-Yu Chen, Sutanay Choudhury, Alfred O. Hero", "title": "Multi-centrality Graph Spectral Decompositions and their Application to\n  Cyber Intrusion Detection", "comments": "To appear in ICASSP 2016", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.SI cs.CR stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Many modern datasets can be represented as graphs and hence spectral\ndecompositions such as graph principal component analysis (PCA) can be useful.\nDistinct from previous graph decomposition approaches based on subspace\nprojection of a single topological feature, e.g., the Fiedler vector of\ncentered graph adjacency matrix (graph Laplacian), we propose spectral\ndecomposition approaches to graph PCA and graph dictionary learning that\nintegrate multiple features, including graph walk statistics, centrality\nmeasures and graph distances to reference nodes. In this paper we propose a new\nPCA method for single graph analysis, called multi-centrality graph PCA\n(MC-GPCA), and a new dictionary learning method for ensembles of graphs, called\nmulti-centrality graph dictionary learning (MC-GDL), both based on spectral\ndecomposition of multi-centrality matrices. As an application to cyber\nintrusion detection, MC-GPCA can be an effective indicator of anomalous\nconnectivity pattern and MC-GDL can provide discriminative basis for attack\nclassification.\n", "versions": [{"version": "v1", "created": "Wed, 23 Dec 2015 07:13:27 GMT"}, {"version": "v2", "created": "Thu, 10 Mar 2016 03:31:25 GMT"}], "update_date": "2016-03-11", "authors_parsed": [["Chen", "Pin-Yu", ""], ["Choudhury", "Sutanay", ""], ["Hero", "Alfred O.", ""]]}, {"id": "1512.07422", "submitter": "Rodolphe Jenatton", "authors": "Rodolphe Jenatton, Jim Huang, C\\'edric Archambeau", "title": "Adaptive Algorithms for Online Convex Optimization with Long-term\n  Constraints", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG math.OC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present an adaptive online gradient descent algorithm to solve online\nconvex optimization problems with long-term constraints , which are constraints\nthat need to be satisfied when accumulated over a finite number of rounds T ,\nbut can be violated in intermediate rounds. For some user-defined trade-off\nparameter $\\beta$ $\\in$ (0, 1), the proposed algorithm achieves cumulative\nregret bounds of O(T^max{$\\beta$,1--$\\beta$}) and O(T^(1--$\\beta$/2)) for the\nloss and the constraint violations respectively. Our results hold for convex\nlosses and can handle arbitrary convex constraints without requiring knowledge\nof the number of rounds in advance. Our contributions improve over the best\nknown cumulative regret bounds by Mahdavi, et al. (2012) that are respectively\nO(T^1/2) and O(T^3/4) for general convex domains, and respectively O(T^2/3) and\nO(T^2/3) when further restricting to polyhedral domains. We supplement the\nanalysis with experiments validating the performance of our algorithm in\npractice.\n", "versions": [{"version": "v1", "created": "Wed, 23 Dec 2015 10:32:09 GMT"}], "update_date": "2015-12-24", "authors_parsed": [["Jenatton", "Rodolphe", ""], ["Huang", "Jim", ""], ["Archambeau", "C\u00e9dric", ""]]}, {"id": "1512.07446", "submitter": "Cem Tekin", "authors": "Cem Tekin, Jinsung Yoon, Mihaela van der Schaar", "title": "Adaptive Ensemble Learning with Confidence Bounds", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Extracting actionable intelligence from distributed, heterogeneous,\ncorrelated and high-dimensional data sources requires run-time processing and\nlearning both locally and globally. In the last decade, a large number of\nmeta-learning techniques have been proposed in which local learners make online\npredictions based on their locally-collected data instances, and feed these\npredictions to an ensemble learner, which fuses them and issues a global\nprediction. However, most of these works do not provide performance guarantees\nor, when they do, these guarantees are asymptotic. None of these existing works\nprovide confidence estimates about the issued predictions or rate of learning\nguarantees for the ensemble learner. In this paper, we provide a systematic\nensemble learning method called Hedged Bandits, which comes with both long run\n(asymptotic) and short run (rate of learning) performance guarantees. Moreover,\nour approach yields performance guarantees with respect to the optimal local\nprediction strategy, and is also able to adapt its predictions in a data-driven\nmanner. We illustrate the performance of Hedged Bandits in the context of\nmedical informatics and show that it outperforms numerous online and offline\nensemble learning methods.\n", "versions": [{"version": "v1", "created": "Wed, 23 Dec 2015 12:08:15 GMT"}, {"version": "v2", "created": "Thu, 30 Jun 2016 21:36:37 GMT"}, {"version": "v3", "created": "Sun, 30 Oct 2016 12:55:05 GMT"}], "update_date": "2016-11-01", "authors_parsed": [["Tekin", "Cem", ""], ["Yoon", "Jinsung", ""], ["van der Schaar", "Mihaela", ""]]}, {"id": "1512.07548", "submitter": "Christian Bauckhage", "authors": "Christian Bauckhage", "title": "k-Means Clustering Is Matrix Factorization", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We show that the objective function of conventional k-means clustering can be\nexpressed as the Frobenius norm of the difference of a data matrix and a low\nrank approximation of that data matrix. In short, we show that k-means\nclustering is a matrix factorization problem. These notes are meant as a\nreference and intended to provide a guided tour towards a result that is often\nmentioned but seldom made explicit in the literature.\n", "versions": [{"version": "v1", "created": "Wed, 23 Dec 2015 17:12:06 GMT"}], "update_date": "2015-12-24", "authors_parsed": [["Bauckhage", "Christian", ""]]}, {"id": "1512.07587", "submitter": "Rajasekaran Masatran", "authors": "Rajasekaran Masatran", "title": "A Latent-Variable Lattice Model", "comments": "6 pages, with 4 figures, 8 algorithms, and 1 table", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Markov random field (MRF) learning is intractable, and its approximation\nalgorithms are computationally expensive. We target a small subset of MRF that\nis used frequently in computer vision. We characterize this subset with three\nconcepts: Lattice, Homogeneity, and Inertia; and design a non-markov model as\nan alternative. Our goal is robust learning from small datasets. Our learning\nalgorithm uses vector quantization and, at time complexity O(U log U) for a\ndataset of U pixels, is much faster than that of general-purpose MRF.\n", "versions": [{"version": "v1", "created": "Wed, 23 Dec 2015 19:01:03 GMT"}, {"version": "v2", "created": "Thu, 28 Jan 2016 16:57:50 GMT"}, {"version": "v3", "created": "Mon, 8 Feb 2016 08:48:46 GMT"}, {"version": "v4", "created": "Sat, 5 Mar 2016 13:07:09 GMT"}, {"version": "v5", "created": "Fri, 20 May 2016 08:30:02 GMT"}, {"version": "v6", "created": "Wed, 25 May 2016 09:17:23 GMT"}, {"version": "v7", "created": "Wed, 8 Jun 2016 03:25:09 GMT"}], "update_date": "2016-06-09", "authors_parsed": [["Masatran", "Rajasekaran", ""]]}, {"id": "1512.07638", "submitter": "Paul Reverdy", "authors": "Paul Reverdy and Vaibhav Srivastava and Naomi Ehrich Leonard", "title": "Satisficing in multi-armed bandit problems", "comments": "To appear in IEEE Transactions on Automatic Control", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG math.OC stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Satisficing is a relaxation of maximizing and allows for less risky decision\nmaking in the face of uncertainty. We propose two sets of satisficing\nobjectives for the multi-armed bandit problem, where the objective is to\nachieve reward-based decision-making performance above a given threshold. We\nshow that these new problems are equivalent to various standard multi-armed\nbandit problems with maximizing objectives and use the equivalence to find\nbounds on performance. The different objectives can result in qualitatively\ndifferent behavior; for example, agents explore their options continually in\none case and only a finite number of times in another. For the case of Gaussian\nrewards we show an additional equivalence between the two sets of satisficing\nobjectives that allows algorithms developed for one set to be applied to the\nother. We then develop variants of the Upper Credible Limit (UCL) algorithm\nthat solve the problems with satisficing objectives and show that these\nmodified UCL algorithms achieve efficient satisficing performance.\n", "versions": [{"version": "v1", "created": "Wed, 23 Dec 2015 21:05:16 GMT"}, {"version": "v2", "created": "Mon, 19 Dec 2016 17:16:07 GMT"}], "update_date": "2016-12-20", "authors_parsed": [["Reverdy", "Paul", ""], ["Srivastava", "Vaibhav", ""], ["Leonard", "Naomi Ehrich", ""]]}, {"id": "1512.07650", "submitter": "Yahel David", "authors": "Yahel David and Nahum Shimkin", "title": "The Max $K$-Armed Bandit: PAC Lower Bounds and Efficient Algorithms", "comments": "arXiv admin note: substantial text overlap with arXiv:1508.05608", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.AI cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider the Max $K$-Armed Bandit problem, where a learning agent is faced\nwith several stochastic arms, each a source of i.i.d. rewards of unknown\ndistribution. At each time step the agent chooses an arm, and observes the\nreward of the obtained sample. Each sample is considered here as a separate\nitem with the reward designating its value, and the goal is to find an item\nwith the highest possible value. Our basic assumption is a known lower bound on\nthe {\\em tail function} of the reward distributions. Under the PAC framework,\nwe provide a lower bound on the sample complexity of any\n$(\\epsilon,\\delta)$-correct algorithm, and propose an algorithm that attains\nthis bound up to logarithmic factors. We analyze the robustness of the proposed\nalgorithm and in addition, we compare the performance of this algorithm to the\nvariant in which the arms are not distinguishable by the agent and are chosen\nrandomly at each stage. Interestingly, when the maximal rewards of the arms\nhappen to be similar, the latter approach may provide better performance.\n", "versions": [{"version": "v1", "created": "Wed, 23 Dec 2015 22:11:02 GMT"}], "update_date": "2015-12-25", "authors_parsed": [["David", "Yahel", ""], ["Shimkin", "Nahum", ""]]}, {"id": "1512.07662", "submitter": "Chunyuan Li", "authors": "Chunyuan Li, Changyou Chen, Kai Fan and Lawrence Carin", "title": "High-Order Stochastic Gradient Thermostats for Bayesian Learning of Deep\n  Models", "comments": "AAAI 2016", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Learning in deep models using Bayesian methods has generated significant\nattention recently. This is largely because of the feasibility of modern\nBayesian methods to yield scalable learning and inference, while maintaining a\nmeasure of uncertainty in the model parameters. Stochastic gradient MCMC\nalgorithms (SG-MCMC) are a family of diffusion-based sampling methods for\nlarge-scale Bayesian learning. In SG-MCMC, multivariate stochastic gradient\nthermostats (mSGNHT) augment each parameter of interest, with a momentum and a\nthermostat variable to maintain stationary distributions as target posterior\ndistributions. As the number of variables in a continuous-time diffusion\nincreases, its numerical approximation error becomes a practical bottleneck, so\nbetter use of a numerical integrator is desirable. To this end, we propose use\nof an efficient symmetric splitting integrator in mSGNHT, instead of the\ntraditional Euler integrator. We demonstrate that the proposed scheme is more\naccurate, robust, and converges faster. These properties are demonstrated to be\ndesirable in Bayesian deep learning. Extensive experiments on two canonical\nmodels and their deep extensions demonstrate that the proposed scheme improves\ngeneral Bayesian posterior sampling, particularly for deep models.\n", "versions": [{"version": "v1", "created": "Wed, 23 Dec 2015 23:21:40 GMT"}], "update_date": "2015-12-25", "authors_parsed": [["Li", "Chunyuan", ""], ["Chen", "Changyou", ""], ["Fan", "Kai", ""], ["Carin", "Lawrence", ""]]}, {"id": "1512.07666", "submitter": "Chunyuan Li", "authors": "Chunyuan Li, Changyou Chen, David Carlson and Lawrence Carin", "title": "Preconditioned Stochastic Gradient Langevin Dynamics for Deep Neural\n  Networks", "comments": "AAAI 2016", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Effective training of deep neural networks suffers from two main issues. The\nfirst is that the parameter spaces of these models exhibit pathological\ncurvature. Recent methods address this problem by using adaptive\npreconditioning for Stochastic Gradient Descent (SGD). These methods improve\nconvergence by adapting to the local geometry of parameter space. A second\nissue is overfitting, which is typically addressed by early stopping. However,\nrecent work has demonstrated that Bayesian model averaging mitigates this\nproblem. The posterior can be sampled by using Stochastic Gradient Langevin\nDynamics (SGLD). However, the rapidly changing curvature renders default SGLD\nmethods inefficient. Here, we propose combining adaptive preconditioners with\nSGLD. In support of this idea, we give theoretical properties on asymptotic\nconvergence and predictive risk. We also provide empirical results for Logistic\nRegression, Feedforward Neural Nets, and Convolutional Neural Nets,\ndemonstrating that our preconditioned SGLD method gives state-of-the-art\nperformance on these models.\n", "versions": [{"version": "v1", "created": "Wed, 23 Dec 2015 23:45:03 GMT"}], "update_date": "2015-12-25", "authors_parsed": [["Li", "Chunyuan", ""], ["Chen", "Changyou", ""], ["Carlson", "David", ""], ["Carin", "Lawrence", ""]]}, {"id": "1512.07679", "submitter": "Gabriel Dulac-Arnold", "authors": "Gabriel Dulac-Arnold and Richard Evans and Hado van Hasselt and Peter\n  Sunehag and Timothy Lillicrap and Jonathan Hunt and Timothy Mann and\n  Theophane Weber and Thomas Degris and Ben Coppin", "title": "Deep Reinforcement Learning in Large Discrete Action Spaces", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.LG cs.NE stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Being able to reason in an environment with a large number of discrete\nactions is essential to bringing reinforcement learning to a larger class of\nproblems. Recommender systems, industrial plants and language models are only\nsome of the many real-world tasks involving large numbers of discrete actions\nfor which current methods are difficult or even often impossible to apply. An\nability to generalize over the set of actions as well as sub-linear complexity\nrelative to the size of the set are both necessary to handle such tasks.\nCurrent approaches are not able to provide both of these, which motivates the\nwork in this paper. Our proposed approach leverages prior information about the\nactions to embed them in a continuous space upon which it can generalize.\nAdditionally, approximate nearest-neighbor methods allow for logarithmic-time\nlookup complexity relative to the number of actions, which is necessary for\ntime-wise tractable training. This combined approach allows reinforcement\nlearning methods to be applied to large-scale learning problems previously\nintractable with current methods. We demonstrate our algorithm's abilities on a\nseries of tasks having up to one million actions.\n", "versions": [{"version": "v1", "created": "Thu, 24 Dec 2015 01:31:40 GMT"}, {"version": "v2", "created": "Mon, 4 Apr 2016 11:27:36 GMT"}], "update_date": "2016-04-05", "authors_parsed": [["Dulac-Arnold", "Gabriel", ""], ["Evans", "Richard", ""], ["van Hasselt", "Hado", ""], ["Sunehag", "Peter", ""], ["Lillicrap", "Timothy", ""], ["Hunt", "Jonathan", ""], ["Mann", "Timothy", ""], ["Weber", "Theophane", ""], ["Degris", "Thomas", ""], ["Coppin", "Ben", ""]]}, {"id": "1512.07797", "submitter": "Jiaqian Yu", "authors": "Jiaqian Yu (CVC, GALEN), Matthew Blaschko", "title": "The Lov\\'asz Hinge: A Novel Convex Surrogate for Submodular Losses", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Learning with non-modular losses is an important problem when sets of\npredictions are made simultaneously. The main tools for constructing convex\nsurrogate loss functions for set prediction are margin rescaling and slack\nrescaling. In this work, we show that these strategies lead to tight convex\nsurrogates iff the underlying loss function is increasing in the number of\nincorrect predictions. However, gradient or cutting-plane computation for these\nfunctions is NP-hard for non-supermodular loss functions. We propose instead a\nnovel surrogate loss function for submodular losses, the Lov\\'asz hinge, which\nleads to O(p log p) complexity with O(p) oracle accesses to the loss function\nto compute a gradient or cutting-plane. We prove that the Lov\\'asz hinge is\nconvex and yields an extension. As a result, we have developed the first\ntractable convex surrogates in the literature for submodular losses. We\ndemonstrate the utility of this novel convex surrogate through several set\nprediction tasks, including on the PASCAL VOC and Microsoft COCO datasets.\n", "versions": [{"version": "v1", "created": "Thu, 24 Dec 2015 11:49:47 GMT"}, {"version": "v2", "created": "Mon, 15 May 2017 11:25:31 GMT"}], "update_date": "2017-05-16", "authors_parsed": [["Yu", "Jiaqian", "", "CVC, GALEN"], ["Blaschko", "Matthew", ""]]}, {"id": "1512.07839", "submitter": "Sacha Sokoloski", "authors": "Sacha Sokoloski", "title": "Implementing a Bayes Filter in a Neural Circuit: The Case of Unknown\n  Stimulus Dynamics", "comments": "This is the final version, and has been accepted for publication in\n  Neural Computation", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In order to interact intelligently with objects in the world, animals must\nfirst transform neural population responses into estimates of the dynamic,\nunknown stimuli which caused them. The Bayesian solution to this problem is\nknown as a Bayes filter, which applies Bayes' rule to combine population\nresponses with the predictions of an internal model. In this paper we present a\nmethod for learning to approximate a Bayes filter when the stimulus dynamics\nare unknown. To do this we use the inferential properties of probabilistic\npopulation codes to compute Bayes' rule, and train a neural network to compute\napproximate predictions by the method of maximum likelihood. In particular, we\nperform stochastic gradient descent on the negative log-likelihood with a novel\napproximation of the gradient. We demonstrate our methods on a finite-state, a\nlinear, and a nonlinear filtering problem, and show how the hidden layer of the\nneural network develops tuning curves which are consistent with findings in\nexperimental neuroscience.\n", "versions": [{"version": "v1", "created": "Tue, 22 Dec 2015 14:52:14 GMT"}, {"version": "v2", "created": "Mon, 16 May 2016 10:35:55 GMT"}, {"version": "v3", "created": "Thu, 17 Nov 2016 20:46:39 GMT"}, {"version": "v4", "created": "Wed, 7 Jun 2017 00:34:49 GMT"}], "update_date": "2017-06-08", "authors_parsed": [["Sokoloski", "Sacha", ""]]}, {"id": "1512.07942", "submitter": "Krzysztof Chalupka", "authors": "Krzysztof Chalupka, Pietro Perona and Frederick Eberhardt", "title": "Multi-Level Cause-Effect Systems", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a domain-general account of causation that applies to settings in\nwhich macro-level causal relations between two systems are of interest, but the\nrelevant causal features are poorly understood and have to be aggregated from\nvast arrays of micro-measurements. Our approach generalizes that of Chalupka et\nal. (2015) to the setting in which the macro-level effect is not specified. We\nformalize the connection between micro- and macro-variables in such situations\nand provide a coherent framework describing causal relations at multiple levels\nof analysis. We present an algorithm that discovers macro-variable causes and\neffects from micro-level measurements obtained from an experiment. We further\nshow how to design experiments to discover macro-variables from observational\nmicro-variable data. Finally, we show that under specific conditions, one can\nidentify multiple levels of causal structure. Throughout the article, we use a\nsimulated neuroscience multi-unit recording experiment to illustrate the ideas\nand the algorithms.\n", "versions": [{"version": "v1", "created": "Fri, 25 Dec 2015 01:08:07 GMT"}], "update_date": "2015-12-29", "authors_parsed": [["Chalupka", "Krzysztof", ""], ["Perona", "Pietro", ""], ["Eberhardt", "Frederick", ""]]}, {"id": "1512.07960", "submitter": "Hideaki Kim", "authors": "Hideaki Kim, Hiroshi Sawada", "title": "Histogram Meets Topic Model: Density Estimation by Mixture of Histograms", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The histogram method is a powerful non-parametric approach for estimating the\nprobability density function of a continuous variable. But the construction of\na histogram, compared to the parametric approaches, demands a large number of\nobservations to capture the underlying density function. Thus it is not\nsuitable for analyzing a sparse data set, a collection of units with a small\nsize of data. In this paper, by employing the probabilistic topic model, we\ndevelop a novel Bayesian approach to alleviating the sparsity problem in the\nconventional histogram estimation. Our method estimates a unit's density\nfunction as a mixture of basis histograms, in which the number of bins for each\nbasis, as well as their heights, is determined automatically. The estimation\nprocedure is performed by using the fast and easy-to-implement collapsed Gibbs\nsampling. We apply the proposed method to synthetic data, showing that it\nperforms well.\n", "versions": [{"version": "v1", "created": "Fri, 25 Dec 2015 05:30:20 GMT"}], "update_date": "2015-12-29", "authors_parsed": [["Kim", "Hideaki", ""], ["Sawada", "Hiroshi", ""]]}, {"id": "1512.07962", "submitter": "Changyou Chen", "authors": "Changyou Chen, David Carlson, Zhe Gan, Chunyuan Li and Lawrence Carin", "title": "Bridging the Gap between Stochastic Gradient MCMC and Stochastic\n  Optimization", "comments": "Merry Christmas from the Santa (algorithm). AISTATS 2016", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Stochastic gradient Markov chain Monte Carlo (SG-MCMC) methods are Bayesian\nanalogs to popular stochastic optimization methods; however, this connection is\nnot well studied. We explore this relationship by applying simulated annealing\nto an SGMCMC algorithm. Furthermore, we extend recent SG-MCMC methods with two\nkey components: i) adaptive preconditioners (as in ADAgrad or RMSprop), and ii)\nadaptive element-wise momentum weights. The zero-temperature limit gives a\nnovel stochastic optimization method with adaptive element-wise momentum\nweights, while conventional optimization methods only have a shared, static\nmomentum weight. Under certain assumptions, our theoretical analysis suggests\nthe proposed simulated annealing approach converges close to the global optima.\nExperiments on several deep neural network models show state-of-the-art results\ncompared to related stochastic optimization algorithms.\n", "versions": [{"version": "v1", "created": "Fri, 25 Dec 2015 06:01:44 GMT"}, {"version": "v2", "created": "Sat, 27 Feb 2016 16:26:22 GMT"}, {"version": "v3", "created": "Fri, 5 Aug 2016 14:49:57 GMT"}], "update_date": "2016-08-08", "authors_parsed": [["Chen", "Changyou", ""], ["Carlson", "David", ""], ["Gan", "Zhe", ""], ["Li", "Chunyuan", ""], ["Carin", "Lawrence", ""]]}, {"id": "1512.08064", "submitter": "Steve Hanneke", "authors": "Steve Hanneke, Liu Yang", "title": "Statistical Learning under Nonstationary Mixing Processes", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study a special case of the problem of statistical learning without the\ni.i.d. assumption. Specifically, we suppose a learning method is presented with\na sequence of data points, and required to make a prediction (e.g., a\nclassification) for each one, and can then observe the loss incurred by this\nprediction. We go beyond traditional analyses, which have focused on stationary\nmixing processes or nonstationary product processes, by combining these two\nrelaxations to allow nonstationary mixing processes. We are particularly\ninterested in the case of $\\beta$-mixing processes, with the sum of changes in\nmarginal distributions growing sublinearly in the number of samples. Under\nthese conditions, we propose a learning method, and establish that for bounded\nVC subgraph classes, the cumulative excess risk grows sublinearly in the number\nof predictions, at a quantified rate.\n", "versions": [{"version": "v1", "created": "Sat, 26 Dec 2015 01:33:55 GMT"}, {"version": "v2", "created": "Sun, 20 May 2018 18:14:27 GMT"}], "update_date": "2018-05-22", "authors_parsed": [["Hanneke", "Steve", ""], ["Yang", "Liu", ""]]}, {"id": "1512.08065", "submitter": "Ming Jin", "authors": "Ming Jin, Andreas Damianou, Pieter Abbeel, Costas Spanos", "title": "Inverse Reinforcement Learning via Deep Gaussian Process", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.RO stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a new approach to inverse reinforcement learning (IRL) based on\nthe deep Gaussian process (deep GP) model, which is capable of learning\ncomplicated reward structures with few demonstrations. Our model stacks\nmultiple latent GP layers to learn abstract representations of the state\nfeature space, which is linked to the demonstrations through the Maximum\nEntropy learning framework. Incorporating the IRL engine into the nonlinear\nlatent structure renders existing deep GP inference approaches intractable. To\ntackle this, we develop a non-standard variational approximation framework\nwhich extends previous inference schemes. This allows for approximate Bayesian\ntreatment of the feature space and guards against overfitting. Carrying out\nrepresentation and inverse reinforcement learning simultaneously within our\nmodel outperforms state-of-the-art approaches, as we demonstrate with\nexperiments on standard benchmarks (\"object world\",\"highway driving\") and a new\nbenchmark (\"binary world\").\n", "versions": [{"version": "v1", "created": "Sat, 26 Dec 2015 01:40:37 GMT"}, {"version": "v2", "created": "Thu, 30 Mar 2017 03:36:37 GMT"}, {"version": "v3", "created": "Tue, 2 May 2017 03:11:45 GMT"}, {"version": "v4", "created": "Thu, 4 May 2017 23:20:24 GMT"}], "update_date": "2017-05-08", "authors_parsed": [["Jin", "Ming", ""], ["Damianou", "Andreas", ""], ["Abbeel", "Pieter", ""], ["Spanos", "Costas", ""]]}, {"id": "1512.08204", "submitter": "Massimiliano Pontil", "authors": "Andrew M. McDonald, Massimiliano Pontil, Dimitris Stamos", "title": "New Perspectives on $k$-Support and Cluster Norms", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study a regularizer which is defined as a parameterized infimum of\nquadratics, and which we call the box-norm. We show that the k-support norm, a\nregularizer proposed by [Argyriou et al, 2012] for sparse vector prediction\nproblems, belongs to this family, and the box-norm can be generated as a\nperturbation of the former. We derive an improved algorithm to compute the\nproximity operator of the squared box-norm, and we provide a method to compute\nthe norm. We extend the norms to matrices, introducing the spectral k-support\nnorm and spectral box-norm. We note that the spectral box-norm is essentially\nequivalent to the cluster norm, a multitask learning regularizer introduced by\n[Jacob et al. 2009a], and which in turn can be interpreted as a perturbation of\nthe spectral k-support norm. Centering the norm is important for multitask\nlearning and we also provide a method to use centered versions of the norms as\nregularizers. Numerical experiments indicate that the spectral k-support and\nbox-norms and their centered variants provide state of the art performance in\nmatrix completion and multitask learning problems respectively.\n", "versions": [{"version": "v1", "created": "Sun, 27 Dec 2015 11:30:37 GMT"}], "update_date": "2016-01-12", "authors_parsed": [["McDonald", "Andrew M.", ""], ["Pontil", "Massimiliano", ""], ["Stamos", "Dimitris", ""]]}, {"id": "1512.08240", "submitter": "Jesse Krijthe", "authors": "Jesse H. Krijthe and Marco Loog", "title": "Robust Semi-supervised Least Squares Classification by Implicit\n  Constraints", "comments": "Appeared as Pattern Recognition Volume 63, March 2017, Pages 115-126.\n  This version of the manuscript fixes some typos in the equations on page 9\n  that are incorrect in the published version", "journal-ref": "Pattern Recognition Volume 63, March 2017, Pages 115-126", "doi": "10.1016/j.patcog.2016.09.009", "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We introduce the implicitly constrained least squares (ICLS) classifier, a\nnovel semi-supervised version of the least squares classifier. This classifier\nminimizes the squared loss on the labeled data among the set of parameters\nimplied by all possible labelings of the unlabeled data. Unlike other\ndiscriminative semi-supervised methods, this approach does not introduce\nexplicit additional assumptions into the objective function, but leverages\nimplicit assumptions already present in the choice of the supervised least\nsquares classifier. This method can be formulated as a quadratic programming\nproblem and its solution can be found using a simple gradient descent\nprocedure. We prove that, in a limited 1-dimensional setting, this approach\nnever leads to performance worse than the supervised classifier. Experimental\nresults show that also in the general multidimensional case performance\nimprovements can be expected, both in terms of the squared loss that is\nintrinsic to the classifier, as well as in terms of the expected classification\nerror.\n", "versions": [{"version": "v1", "created": "Sun, 27 Dec 2015 16:44:06 GMT"}, {"version": "v2", "created": "Fri, 27 Jan 2017 20:42:38 GMT"}], "update_date": "2017-01-31", "authors_parsed": [["Krijthe", "Jesse H.", ""], ["Loog", "Marco", ""]]}, {"id": "1512.08269", "submitter": "Fanny Yang", "authors": "Fanny Yang, Sivaraman Balakrishnan, Martin J. Wainwright", "title": "Statistical and Computational Guarantees for the Baum-Welch Algorithm", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.IT math.IT math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The Hidden Markov Model (HMM) is one of the mainstays of statistical modeling\nof discrete time series, with applications including speech recognition,\ncomputational biology, computer vision and econometrics. Estimating an HMM from\nits observation process is often addressed via the Baum-Welch algorithm, which\nis known to be susceptible to local optima. In this paper, we first give a\ngeneral characterization of the basin of attraction associated with any global\noptimum of the population likelihood. By exploiting this characterization, we\nprovide non-asymptotic finite sample guarantees on the Baum-Welch updates,\nguaranteeing geometric convergence to a small ball of radius on the order of\nthe minimax rate around a global optimum. As a concrete example, we prove a\nlinear rate of convergence for a hidden Markov mixture of two isotropic\nGaussians given a suitable mean separation and an initialization within a ball\nof large radius around (one of) the true parameters. To our knowledge, these\nare the first rigorous local convergence guarantees to global optima for the\nBaum-Welch algorithm in a setting where the likelihood function is nonconvex.\nWe complement our theoretical results with thorough numerical simulations\nstudying the convergence of the Baum-Welch algorithm and illustrating the\naccuracy of our predictions.\n", "versions": [{"version": "v1", "created": "Sun, 27 Dec 2015 20:10:20 GMT"}], "update_date": "2015-12-29", "authors_parsed": [["Yang", "Fanny", ""], ["Balakrishnan", "Sivaraman", ""], ["Wainwright", "Martin J.", ""]]}, {"id": "1512.08298", "submitter": "Junwei Lu", "authors": "Junwei Lu, Mladen Kolar and Han Liu", "title": "Post-Regularization Inference for Time-Varying Nonparanormal Graphical\n  Models", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a novel class of time-varying nonparanormal graphical models,\nwhich allows us to model high dimensional heavy-tailed systems and the\nevolution of their latent network structures. Under this model, we develop\nstatistical tests for presence of edges both locally at a fixed index value and\nglobally over a range of values. The tests are developed for a high-dimensional\nregime, are robust to model selection mistakes and do not require commonly\nassumed minimum signal strength. The testing procedures are based on a high\ndimensional, debiasing-free moment estimator, which uses a novel kernel\nsmoothed Kendall's tau correlation matrix as an input statistic. The estimator\nconsistently estimates the latent inverse Pearson correlation matrix uniformly\nin both the index variable and kernel bandwidth. Its rate of convergence is\nshown to be minimax optimal. Our method is supported by thorough numerical\nsimulations and an application to a neural imaging data set.\n", "versions": [{"version": "v1", "created": "Mon, 28 Dec 2015 01:27:07 GMT"}, {"version": "v2", "created": "Wed, 30 Dec 2015 07:22:31 GMT"}, {"version": "v3", "created": "Mon, 12 Feb 2018 23:22:59 GMT"}], "update_date": "2018-02-14", "authors_parsed": [["Lu", "Junwei", ""], ["Kolar", "Mladen", ""], ["Liu", "Han", ""]]}, {"id": "1512.08425", "submitter": "Jiaming Xu", "authors": "Yudong Chen and Xiaodong Li and Jiaming Xu", "title": "Convexified Modularity Maximization for Degree-corrected Stochastic\n  Block Models", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST cs.LG cs.SI stat.ML stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The stochastic block model (SBM) is a popular framework for studying\ncommunity detection in networks. This model is limited by the assumption that\nall nodes in the same community are statistically equivalent and have equal\nexpected degrees. The degree-corrected stochastic block model (DCSBM) is a\nnatural extension of SBM that allows for degree heterogeneity within\ncommunities. This paper proposes a convexified modularity maximization approach\nfor estimating the hidden communities under DCSBM. Our approach is based on a\nconvex programming relaxation of the classical (generalized) modularity\nmaximization formulation, followed by a novel doubly-weighted $ \\ell_1 $-norm $\nk $-median procedure. We establish non-asymptotic theoretical guarantees for\nboth approximate clustering and perfect clustering. Our approximate clustering\nresults are insensitive to the minimum degree, and hold even in sparse regime\nwith bounded average degrees. In the special case of SBM, these theoretical\nresults match the best-known performance guarantees of computationally feasible\nalgorithms. Numerically, we provide an efficient implementation of our\nalgorithm, which is applied to both synthetic and real-world networks.\nExperiment results show that our method enjoys competitive performance compared\nto the state of the art in the literature.\n", "versions": [{"version": "v1", "created": "Mon, 28 Dec 2015 14:48:03 GMT"}, {"version": "v2", "created": "Tue, 19 Jan 2016 17:17:32 GMT"}], "update_date": "2016-01-20", "authors_parsed": [["Chen", "Yudong", ""], ["Li", "Xiaodong", ""], ["Xu", "Jiaming", ""]]}, {"id": "1512.08571", "submitter": "Sajid Anwar", "authors": "Sajid Anwar, Kyuyeon Hwang and Wonyong Sung", "title": "Structured Pruning of Deep Convolutional Neural Networks", "comments": "11 pages, 8 figures, 1 table", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NE cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Real time application of deep learning algorithms is often hindered by high\ncomputational complexity and frequent memory accesses. Network pruning is a\npromising technique to solve this problem. However, pruning usually results in\nirregular network connections that not only demand extra representation efforts\nbut also do not fit well on parallel computation. We introduce structured\nsparsity at various scales for convolutional neural networks, which are channel\nwise, kernel wise and intra kernel strided sparsity. This structured sparsity\nis very advantageous for direct computational resource savings on embedded\ncomputers, parallel computing environments and hardware based systems. To\ndecide the importance of network connections and paths, the proposed method\nuses a particle filtering approach. The importance weight of each particle is\nassigned by computing the misclassification rate with corresponding\nconnectivity pattern. The pruned network is re-trained to compensate for the\nlosses due to pruning. While implementing convolutions as matrix products, we\nparticularly show that intra kernel strided sparsity with a simple constraint\ncan significantly reduce the size of kernel and feature map matrices. The\npruned network is finally fixed point optimized with reduced word length\nprecision. This results in significant reduction in the total storage size\nproviding advantages for on-chip memory based implementations of deep neural\nnetworks.\n", "versions": [{"version": "v1", "created": "Tue, 29 Dec 2015 01:21:08 GMT"}], "update_date": "2015-12-31", "authors_parsed": [["Anwar", "Sajid", ""], ["Hwang", "Kyuyeon", ""], ["Sung", "Wonyong", ""]]}, {"id": "1512.08643", "submitter": "Eugene Belilovsky", "authors": "Eugene Belilovsky (GALEN, CVN), Ga\\\"el Varoquaux (PARIETAL), Matthew\n  B. Blaschko", "title": "Testing for Differences in Gaussian Graphical Models: Applications to\n  Brain Connectivity", "comments": null, "journal-ref": "Neural Information Processing Systems (NIPS) 2016, Dec 2016,\n  Barcelona, Spain", "doi": null, "report-no": null, "categories": "stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Functional brain networks are well described and estimated from data with\nGaussian Graphical Models (GGMs), e.g. using sparse inverse covariance\nestimators. Comparing functional connectivity of subjects in two populations\ncalls for comparing these estimated GGMs. Our goal is to identify differences\nin GGMs known to have similar structure. We characterize the uncertainty of\ndifferences with confidence intervals obtained using a parametric distribution\non parameters of a sparse estimator. Sparse penalties enable statistical\nguarantees and interpretable models even in high-dimensional and low-sample\nsettings. Characterizing the distributions of sparse models is inherently\nchallenging as the penalties produce a biased estimator. Recent work invokes\nthe sparsity assumptions to effectively remove the bias from a sparse estimator\nsuch as the lasso. These distributions can be used to give confidence intervals\non edges in GGMs, and by extension their differences. However, in the case of\ncomparing GGMs, these estimators do not make use of any assumed joint structure\namong the GGMs. Inspired by priors from brain functional connectivity we derive\nthe distribution of parameter differences under a joint penalty when parameters\nare known to be sparse in the difference. This leads us to introduce the\ndebiased multi-task fused lasso, whose distribution can be characterized in an\nefficient manner. We then show how the debiased lasso and multi-task fused\nlasso can be used to obtain confidence intervals on edge differences in GGMs.\nWe validate the techniques proposed on a set of synthetic examples as well as\nneuro-imaging dataset created for the study of autism.\n", "versions": [{"version": "v1", "created": "Tue, 29 Dec 2015 10:07:20 GMT"}, {"version": "v2", "created": "Fri, 18 Nov 2016 13:14:54 GMT"}], "update_date": "2016-11-21", "authors_parsed": [["Belilovsky", "Eugene", "", "GALEN, CVN"], ["Varoquaux", "Ga\u00ebl", "", "PARIETAL"], ["Blaschko", "Matthew B.", ""]]}, {"id": "1512.08673", "submitter": "Mathukumalli Vidyasagar", "authors": "M. Eren Ahsen and M. Vidyasagar", "title": "Error Bounds for Compressed Sensing Algorithms With Group Sparsity: A\n  Unified Approach", "comments": "28 pages, final version of 1401.6623, accepted for publication. arXiv\n  admin note: substantial text overlap with arXiv:1401.6623", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In compressed sensing, in order to recover a sparse or nearly sparse vector\nfrom possibly noisy measurements, the most popular approach is $\\ell_1$-norm\nminimization. Upper bounds for the $\\ell_2$- norm of the error between the true\nand estimated vectors are given in [1] and reviewed in [2], while bounds for\nthe $\\ell_1$-norm are given in [3]. When the unknown vector is not\nconventionally sparse but is \"group sparse\" instead, a variety of alternatives\nto the $\\ell_1$-norm have been proposed in the literature, including the group\nLASSO, sparse group LASSO, and group LASSO with tree structured overlapping\ngroups. However, no error bounds are available for any of these modified\nobjective functions. In the present paper, a unified approach is presented for\nderiving upper bounds on the error between the true vector and its\napproximation, based on the notion of decomposable and $\\gamma$-decomposable\nnorms. The bounds presented cover all of the norms mentioned above, and also\nprovide a guideline for choosing norms in future to accommodate alternate forms\nof sparsity.\n", "versions": [{"version": "v1", "created": "Tue, 29 Dec 2015 13:10:25 GMT"}], "update_date": "2015-12-31", "authors_parsed": [["Ahsen", "M. Eren", ""], ["Vidyasagar", "M.", ""]]}, {"id": "1512.08787", "submitter": "Ravi Ganti", "authors": "Ravi Ganti, Laura Balzano, Rebecca Willett", "title": "Matrix Completion Under Monotonic Single Index Models", "comments": "21 pages, 5 figures, 1 table. Accepted for publication at NIPS 2015", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Most recent results in matrix completion assume that the matrix under\nconsideration is low-rank or that the columns are in a union of low-rank\nsubspaces. In real-world settings, however, the linear structure underlying\nthese models is distorted by a (typically unknown) nonlinear transformation.\nThis paper addresses the challenge of matrix completion in the face of such\nnonlinearities. Given a few observations of a matrix that are obtained by\napplying a Lipschitz, monotonic function to a low rank matrix, our task is to\nestimate the remaining unobserved entries. We propose a novel matrix completion\nmethod that alternates between low-rank matrix estimation and monotonic\nfunction estimation to estimate the missing matrix elements. Mean squared error\nbounds provide insight into how well the matrix can be estimated based on the\nsize, rank of the matrix and properties of the nonlinear transformation.\nEmpirical results on synthetic and real-world datasets demonstrate the\ncompetitiveness of the proposed approach.\n", "versions": [{"version": "v1", "created": "Tue, 29 Dec 2015 20:52:41 GMT"}], "update_date": "2015-12-31", "authors_parsed": [["Ganti", "Ravi", ""], ["Balzano", "Laura", ""], ["Willett", "Rebecca", ""]]}, {"id": "1512.08806", "submitter": "Uri Shaham", "authors": "Uri Shaham, Roy Lederman", "title": "Common Variable Learning and Invariant Representation Learning using\n  Siamese Neural Networks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider the statistical problem of learning common source of variability\nin data which are synchronously captured by multiple sensors, and demonstrate\nthat Siamese neural networks can be naturally applied to this problem. This\napproach is useful in particular in exploratory, data-driven applications,\nwhere neither a model nor label information is available. In recent years, many\nresearchers have successfully applied Siamese neural networks to obtain an\nembedding of data which corresponds to a \"semantic similarity\". We present an\ninterpretation of this \"semantic similarity\" as learning of equivalence\nclasses. We discuss properties of the embedding obtained by Siamese networks\nand provide empirical results that demonstrate the ability of Siamese networks\nto learn common variability.\n", "versions": [{"version": "v1", "created": "Tue, 29 Dec 2015 22:06:00 GMT"}, {"version": "v2", "created": "Tue, 26 Apr 2016 03:28:46 GMT"}, {"version": "v3", "created": "Wed, 11 May 2016 17:56:32 GMT"}], "update_date": "2016-05-12", "authors_parsed": [["Shaham", "Uri", ""], ["Lederman", "Roy", ""]]}, {"id": "1512.08808", "submitter": "Kerstin Bunte", "authors": "Kerstin Bunte, Eemeli Lepp\\\"aaho, Inka Saarinen, Samuel Kaski", "title": "Sparse group factor analysis for biclustering of multiple data sources", "comments": "7 pages, 5 figures, 1 table in Bioinformatics 2016", "journal-ref": "Bioinformatics Volume 32, Issue 16 Pp. 2457-2463, 2016", "doi": "10.1093/bioinformatics/btw207", "report-no": null, "categories": "cs.LG cs.IR stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Motivation: Modelling methods that find structure in data are necessary with\nthe current large volumes of genomic data, and there have been various efforts\nto find subsets of genes exhibiting consistent patterns over subsets of\ntreatments. These biclustering techniques have focused on one data source,\noften gene expression data. We present a Bayesian approach for joint\nbiclustering of multiple data sources, extending a recent method Group Factor\nAnalysis (GFA) to have a biclustering interpretation with additional sparsity\nassumptions. The resulting method enables data-driven detection of linear\nstructure present in parts of the data sources. Results: Our simulation studies\nshow that the proposed method reliably infers bi-clusters from heterogeneous\ndata sources. We tested the method on data from the NCI-DREAM drug sensitivity\nprediction challenge, resulting in an excellent prediction accuracy. Moreover,\nthe predictions are based on several biclusters which provide insight into the\ndata sources, in this case on gene expression, DNA methylation, protein\nabundance, exome sequence, functional connectivity fingerprints and drug\nsensitivity.\n", "versions": [{"version": "v1", "created": "Tue, 29 Dec 2015 22:07:35 GMT"}, {"version": "v2", "created": "Thu, 21 Apr 2016 10:23:53 GMT"}], "update_date": "2016-09-15", "authors_parsed": [["Bunte", "Kerstin", ""], ["Lepp\u00e4aho", "Eemeli", ""], ["Saarinen", "Inka", ""], ["Kaski", "Samuel", ""]]}, {"id": "1512.08819", "submitter": "Lingzhou Xue", "authors": "Danning Li and Lingzhou Xue", "title": "Joint limiting laws for high-dimensional independence tests", "comments": "31 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.ME stat.ML stat.OT stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Testing independence is of significant interest in many important areas of\nlarge-scale inference. Using extreme-value form statistics to test against\nsparse alternatives and using quadratic form statistics to test against dense\nalternatives are two important testing procedures for high-dimensional\nindependence. However, quadratic form statistics suffer from low power against\nsparse alternatives, and extreme-value form statistics suffer from low power\nagainst dense alternatives with small disturbances and may have size\ndistortions due to its slow convergence. For real-world applications, it is\nimportant to derive powerful testing procedures against more general\nalternatives. Based on intermediate limiting distributions, we derive\n(model-free) joint limiting laws of extreme-value form and quadratic form\nstatistics, and surprisingly, we prove that they are asymptotically\nindependent. Given such asymptotic independencies, we propose (model-free)\ntesting procedures to boost the power against general alternatives and also\nretain the correct asymptotic size. Under the high-dimensional setting, we\nderive the closed-form limiting null distributions, and obtain their explicit\nrates of uniform convergence. We prove their consistent statistical powers\nagainst general alternatives. We demonstrate the performance of our proposed\ntest statistics in simulation studies. Our work provides very helpful insights\nto high-dimensional independence tests, and fills an important gap.\n", "versions": [{"version": "v1", "created": "Wed, 30 Dec 2015 00:30:19 GMT"}], "update_date": "2015-12-31", "authors_parsed": [["Li", "Danning", ""], ["Xue", "Lingzhou", ""]]}, {"id": "1512.08861", "submitter": "Han Liu", "authors": "Zhaoran Wang, Quanquan Gu, Han Liu", "title": "Sharp Computational-Statistical Phase Transitions via Oracle\n  Computational Model", "comments": "57 pages, 4 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study the fundamental tradeoffs between computational tractability and\nstatistical accuracy for a general family of hypothesis testing problems with\ncombinatorial structures. Based upon an oracle model of computation, which\ncaptures the interactions between algorithms and data, we establish a general\nlower bound that explicitly connects the minimum testing risk under\ncomputational budget constraints with the intrinsic probabilistic and\ncombinatorial structures of statistical problems. This lower bound mirrors the\nclassical statistical lower bound by Le Cam (1986) and allows us to quantify\nthe optimal statistical performance achievable given limited computational\nbudgets in a systematic fashion. Under this unified framework, we sharply\ncharacterize the statistical-computational phase transition for two testing\nproblems, namely, normal mean detection and sparse principal component\ndetection. For normal mean detection, we consider two combinatorial structures,\nnamely, sparse set and perfect matching. For these problems we identify\nsignificant gaps between the optimal statistical accuracy that is achievable\nunder computational tractability constraints and the classical statistical\nlower bounds. Compared with existing works on computational lower bounds for\nstatistical problems, which consider general polynomial-time algorithms on\nTuring machines, and rely on computational hardness hypotheses on problems like\nplanted clique detection, we focus on the oracle computational model, which\ncovers a broad range of popular algorithms, and do not rely on unproven\nhypotheses. Moreover, our result provides an intuitive and concrete\ninterpretation for the intrinsic computational intractability of\nhigh-dimensional statistical problems. One byproduct of our result is a lower\nbound for a strict generalization of the matrix permanent problem, which is of\nindependent interest.\n", "versions": [{"version": "v1", "created": "Wed, 30 Dec 2015 06:16:46 GMT"}], "update_date": "2015-12-31", "authors_parsed": [["Wang", "Zhaoran", ""], ["Gu", "Quanquan", ""], ["Liu", "Han", ""]]}, {"id": "1512.08887", "submitter": "Farhad Pourkamali-Anaraki", "authors": "Farhad Pourkamali-Anaraki", "title": "Estimation of the sample covariance matrix from compressive measurements", "comments": "IET Signal Processing", "journal-ref": "IET Sig. Process. 10 (2016) 1089-1095", "doi": "10.1049/iet-spr.2016.0169", "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper focuses on the estimation of the sample covariance matrix from\nlow-dimensional random projections of data known as compressive measurements.\nIn particular, we present an unbiased estimator to extract the covariance\nstructure from compressive measurements obtained by a general class of random\nprojection matrices consisting of i.i.d. zero-mean entries and finite first\nfour moments. In contrast to previous works, we make no structural assumptions\nabout the underlying covariance matrix such as being low-rank. In fact, our\nanalysis is based on a non-Bayesian data setting which requires no\ndistributional assumptions on the set of data samples. Furthermore, inspired by\nthe generality of the projection matrices, we propose an approach to covariance\nestimation that utilizes sparse Rademacher matrices. Therefore, our algorithm\ncan be used to estimate the covariance matrix in applications with limited\nmemory and computation power at the acquisition devices. Experimental results\ndemonstrate that our approach allows for accurate estimation of the sample\ncovariance matrix on several real-world data sets, including video data.\n", "versions": [{"version": "v1", "created": "Wed, 30 Dec 2015 09:18:18 GMT"}, {"version": "v2", "created": "Fri, 19 Aug 2016 05:53:51 GMT"}, {"version": "v3", "created": "Fri, 28 Apr 2017 03:22:44 GMT"}], "update_date": "2017-05-01", "authors_parsed": [["Pourkamali-Anaraki", "Farhad", ""]]}, {"id": "1512.08949", "submitter": "Nihar Shah", "authors": "Nihar B. Shah and Martin J. Wainwright", "title": "Simple, Robust and Optimal Ranking from Pairwise Comparisons", "comments": "Changes in version 2: In addition to recovery in the exact and\n  Hamming metrics, v2 analyzes a general, abstract recovery criterion based on\n  a notion of \"allowed sets\"", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI cs.IT math.IT stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider data in the form of pairwise comparisons of n items, with the\ngoal of precisely identifying the top k items for some value of k < n, or\nalternatively, recovering a ranking of all the items. We analyze the Copeland\ncounting algorithm that ranks the items in order of the number of pairwise\ncomparisons won, and show it has three attractive features: (a) its\ncomputational efficiency leads to speed-ups of several orders of magnitude in\ncomputation time as compared to prior work; (b) it is robust in that\ntheoretical guarantees impose no conditions on the underlying matrix of\npairwise-comparison probabilities, in contrast to some prior work that applies\nonly to the BTL parametric model; and (c) it is an optimal method up to\nconstant factors, meaning that it achieves the information-theoretic limits for\nrecovering the top k-subset. We extend our results to obtain sharp guarantees\nfor approximate recovery under the Hamming distortion metric, and more\ngenerally, to any arbitrary error requirement that satisfies a simple and\nnatural monotonicity condition.\n", "versions": [{"version": "v1", "created": "Wed, 30 Dec 2015 14:25:23 GMT"}, {"version": "v2", "created": "Wed, 27 Apr 2016 03:52:36 GMT"}], "update_date": "2016-04-28", "authors_parsed": [["Shah", "Nihar B.", ""], ["Wainwright", "Martin J.", ""]]}, {"id": "1512.08996", "submitter": "Mingyuan Zhou", "authors": "Ayan Acharya, Joydeep Ghosh, Mingyuan Zhou", "title": "Nonparametric Bayesian Factor Analysis for Dynamic Count Matrices", "comments": "Appeared in Artificial Intelligence and Statistics (AISTATS), May\n  2015. The ArXiv version fixes a typo in (8), the equation right above Section\n  3.2 in Page 4 of http://www.jmlr.org/proceedings/papers/v38/acharya15.pdf", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML stat.AP stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A gamma process dynamic Poisson factor analysis model is proposed to\nfactorize a dynamic count matrix, whose columns are sequentially observed count\nvectors. The model builds a novel Markov chain that sends the latent gamma\nrandom variables at time $(t-1)$ as the shape parameters of those at time $t$,\nwhich are linked to observed or latent counts under the Poisson likelihood. The\nsignificant challenge of inferring the gamma shape parameters is fully\naddressed, using unique data augmentation and marginalization techniques for\nthe negative binomial distribution. The same nonparametric Bayesian model also\napplies to the factorization of a dynamic binary matrix, via a\nBernoulli-Poisson link that connects a binary observation to a latent count,\nwith closed-form conditional posteriors for the latent counts and efficient\ncomputation for sparse observations. We apply the model to text and music\nanalysis, with state-of-the-art results.\n", "versions": [{"version": "v1", "created": "Wed, 30 Dec 2015 16:28:55 GMT"}], "update_date": "2015-12-31", "authors_parsed": [["Acharya", "Ayan", ""], ["Ghosh", "Joydeep", ""], ["Zhou", "Mingyuan", ""]]}, {"id": "1512.09103", "submitter": "Zeyuan Allen-Zhu", "authors": "Zeyuan Allen-Zhu, Zheng Qu, Peter Richt\\'arik, Yang Yuan", "title": "Even Faster Accelerated Coordinate Descent Using Non-Uniform Sampling", "comments": "same result, but polished writing", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.OC cs.DS math.NA stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Accelerated coordinate descent is widely used in optimization due to its\ncheap per-iteration cost and scalability to large-scale problems. Up to a\nprimal-dual transformation, it is also the same as accelerated stochastic\ngradient descent that is one of the central methods used in machine learning.\n  In this paper, we improve the best known running time of accelerated\ncoordinate descent by a factor up to $\\sqrt{n}$. Our improvement is based on a\nclean, novel non-uniform sampling that selects each coordinate with a\nprobability proportional to the square root of its smoothness parameter. Our\nproof technique also deviates from the classical estimation sequence technique\nused in prior work. Our speed-up applies to important problems such as\nempirical risk minimization and solving linear systems, both in theory and in\npractice.\n", "versions": [{"version": "v1", "created": "Wed, 30 Dec 2015 20:30:07 GMT"}, {"version": "v2", "created": "Wed, 13 Jan 2016 20:54:11 GMT"}, {"version": "v3", "created": "Fri, 27 May 2016 19:09:29 GMT"}], "update_date": "2016-05-30", "authors_parsed": [["Allen-Zhu", "Zeyuan", ""], ["Qu", "Zheng", ""], ["Richt\u00e1rik", "Peter", ""], ["Yuan", "Yang", ""]]}, {"id": "1512.09204", "submitter": "Weici Hu", "authors": "Weici Hu, Peter I. Frazier", "title": "Bayes-Optimal Effort Allocation in Crowdsourcing: Bounds and Index\n  Policies", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider effort allocation in crowdsourcing, where we wish to assign\nlabeling tasks to imperfect homogeneous crowd workers to maximize overall\naccuracy in a continuous-time Bayesian setting, subject to budget and time\nconstraints. The Bayes-optimal policy for this problem is the solution to a\npartially observable Markov decision process, but the curse of dimensionality\nrenders the computation infeasible. Based on the Lagrangian Relaxation\ntechnique in Adelman & Mersereau (2008), we provide a computationally tractable\ninstance-specific upper bound on the value of this Bayes-optimal policy, which\ncan in turn be used to bound the optimality gap of any other sub-optimal\npolicy. In an approach similar in spirit to the Whittle index for restless\nmultiarmed bandits, we provide an index policy for effort allocation in\ncrowdsourcing and demonstrate numerically that it outperforms other stateof-\narts and performs close to optimal solution.\n", "versions": [{"version": "v1", "created": "Thu, 31 Dec 2015 03:09:33 GMT"}], "update_date": "2016-01-01", "authors_parsed": [["Hu", "Weici", ""], ["Frazier", "Peter I.", ""]]}, {"id": "1512.09206", "submitter": "Kevin Lee", "authors": "Kevin Lee and Lingzhou Xue", "title": "Nonparametric mixture of Gaussian graphical models", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Graphical model has been widely used to investigate the complex dependence\nstructure of high-dimensional data, and it is common to assume that observed\ndata follow a homogeneous graphical model. However, observations usually come\nfrom different resources and have heterogeneous hidden commonality in\nreal-world applications. Thus, it is of great importance to estimate\nheterogeneous dependencies and discover subpopulation with certain commonality\nacross the whole population. In this work, we introduce a novel regularized\nestimation scheme for learning nonparametric mixture of Gaussian graphical\nmodels, which extends the methodology and applicability of Gaussian graphical\nmodels and mixture models. We propose a unified penalized likelihood approach\nto effectively estimate nonparametric functional parameters and heterogeneous\ngraphical parameters. We further design an efficient generalized effective EM\nalgorithm to address three significant challenges: high-dimensionality,\nnon-convexity, and label switching. Theoretically, we study both the\nalgorithmic convergence of our proposed algorithm and the asymptotic properties\nof our proposed estimators. Numerically, we demonstrate the performance of our\nmethod in simulation studies and a real application to estimate human brain\nfunctional connectivity from ADHD imaging data, where two heterogeneous\nconditional dependencies are explained through profiling demographic variables\nand supported by existing scientific findings.\n", "versions": [{"version": "v1", "created": "Thu, 31 Dec 2015 03:41:38 GMT"}], "update_date": "2016-01-01", "authors_parsed": [["Lee", "Kevin", ""], ["Xue", "Lingzhou", ""]]}, {"id": "1512.09251", "submitter": "Wolfgang Konen K", "authors": "Samineh Bagheri and Wolfgang Konen and Michael Emmerich and Thomas\n  B\\\"ack", "title": "Solving the G-problems in less than 500 iterations: Improved efficient\n  constrained optimization by surrogate modeling and adaptive parameter control", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.OC cs.NE stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Constrained optimization of high-dimensional numerical problems plays an\nimportant role in many scientific and industrial applications. Function\nevaluations in many industrial applications are severely limited and no\nanalytical information about objective function and constraint functions is\navailable. For such expensive black-box optimization tasks, the constraint\noptimization algorithm COBRA was proposed, making use of RBF surrogate modeling\nfor both the objective and the constraint functions. COBRA has shown remarkable\nsuccess in solving reliably complex benchmark problems in less than 500\nfunction evaluations. Unfortunately, COBRA requires careful adjustment of\nparameters in order to do so.\n  In this work we present a new self-adjusting algorithm SACOBRA, which is\nbased on COBRA and capable to achieve high-quality results with very few\nfunction evaluations and no parameter tuning. It is shown with the help of\nperformance profiles on a set of benchmark problems (G-problems, MOPTA08) that\nSACOBRA consistently outperforms any COBRA algorithm with fixed parameter\nsetting. We analyze the importance of the several new elements in SACOBRA and\nfind that each element of SACOBRA plays a role to boost up the overall\noptimization performance. We discuss the reasons behind and get in this way a\nbetter understanding of high-quality RBF surrogate modeling.\n", "versions": [{"version": "v1", "created": "Thu, 31 Dec 2015 10:30:21 GMT"}], "update_date": "2016-01-01", "authors_parsed": [["Bagheri", "Samineh", ""], ["Konen", "Wolfgang", ""], ["Emmerich", "Michael", ""], ["B\u00e4ck", "Thomas", ""]]}, {"id": "1512.09295", "submitter": "Qirong Ho", "authors": "Eric P. Xing, Qirong Ho, Pengtao Xie, Wei Dai", "title": "Strategies and Principles of Distributed Machine Learning on Big Data", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.DC cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The rise of Big Data has led to new demands for Machine Learning (ML) systems\nto learn complex models with millions to billions of parameters, that promise\nadequate capacity to digest massive datasets and offer powerful predictive\nanalytics thereupon. In order to run ML algorithms at such scales, on a\ndistributed cluster with 10s to 1000s of machines, it is often the case that\nsignificant engineering efforts are required --- and one might fairly ask if\nsuch engineering truly falls within the domain of ML research or not. Taking\nthe view that Big ML systems can benefit greatly from ML-rooted statistical and\nalgorithmic insights --- and that ML researchers should therefore not shy away\nfrom such systems design --- we discuss a series of principles and strategies\ndistilled from our recent efforts on industrial-scale ML solutions. These\nprinciples and strategies span a continuum from application, to engineering,\nand to theoretical research and development of Big ML systems and\narchitectures, with the goal of understanding how to make them efficient,\ngenerally-applicable, and supported with convergence and scaling guarantees.\nThey concern four key questions which traditionally receive little attention in\nML research: How to distribute an ML program over a cluster? How to bridge ML\ncomputation with inter-machine communication? How to perform such\ncommunication? What should be communicated between machines? By exposing\nunderlying statistical and algorithmic characteristics unique to ML programs\nbut not typically seen in traditional computer programs, and by dissecting\nsuccessful cases to reveal how we have harnessed these principles to design and\ndevelop both high-performance distributed ML software as well as\ngeneral-purpose ML frameworks, we present opportunities for ML researchers and\npractitioners to further shape and grow the area that lies between ML and\nsystems.\n", "versions": [{"version": "v1", "created": "Thu, 31 Dec 2015 14:33:53 GMT"}], "update_date": "2016-01-01", "authors_parsed": [["Xing", "Eric P.", ""], ["Ho", "Qirong", ""], ["Xie", "Pengtao", ""], ["Dai", "Wei", ""]]}, {"id": "1512.09300", "submitter": "Anders Boesen Lindbo Larsen", "authors": "Anders Boesen Lindbo Larsen, S{\\o}ren Kaae S{\\o}nderby, Hugo\n  Larochelle, Ole Winther", "title": "Autoencoding beyond pixels using a learned similarity metric", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present an autoencoder that leverages learned representations to better\nmeasure similarities in data space. By combining a variational autoencoder with\na generative adversarial network we can use learned feature representations in\nthe GAN discriminator as basis for the VAE reconstruction objective. Thereby,\nwe replace element-wise errors with feature-wise errors to better capture the\ndata distribution while offering invariance towards e.g. translation. We apply\nour method to images of faces and show that it outperforms VAEs with\nelement-wise similarity measures in terms of visual fidelity. Moreover, we show\nthat the method learns an embedding in which high-level abstract visual\nfeatures (e.g. wearing glasses) can be modified using simple arithmetic.\n", "versions": [{"version": "v1", "created": "Thu, 31 Dec 2015 14:53:39 GMT"}, {"version": "v2", "created": "Wed, 10 Feb 2016 21:18:27 GMT"}], "update_date": "2016-02-12", "authors_parsed": [["Larsen", "Anders Boesen Lindbo", ""], ["S\u00f8nderby", "S\u00f8ren Kaae", ""], ["Larochelle", "Hugo", ""], ["Winther", "Ole", ""]]}, {"id": "1512.09302", "submitter": "Bo Wen", "authors": "Bo Wen, Xiaojun Chen, Ting Kei Pong", "title": "Linear Convergence of Proximal Gradient Algorithm with Extrapolation for\n  a Class of Nonconvex Nonsmooth Minimization Problems", "comments": "We have replaced the blanket assumptions on $f+g$ by the (weaker)\n  assumptions that the optimal value of (1.1) is finite and attained. Section\n  3.4 has been deleted", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.OC stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we study the proximal gradient algorithm with extrapolation\nfor minimizing the sum of a Lipschitz differentiable function and a proper\nclosed convex function. Under the error bound condition used in [19] for\nanalyzing the convergence of the proximal gradient algorithm, we show that\nthere exists a threshold such that if the extrapolation coefficients are chosen\nbelow this threshold, then the sequence generated converges $R$-linearly to a\nstationary point of the problem. Moreover, the corresponding sequence of\nobjective values is also $R$-linearly convergent. In addition, the threshold\nreduces to $1$ for convex problems and, as a consequence, we obtain the\n$R$-linear convergence of the sequence generated by FISTA with fixed restart.\nFinally, we present some numerical experiments to illustrate our results.\n", "versions": [{"version": "v1", "created": "Thu, 31 Dec 2015 14:57:03 GMT"}, {"version": "v2", "created": "Mon, 1 Aug 2016 02:48:11 GMT"}], "update_date": "2016-08-02", "authors_parsed": [["Wen", "Bo", ""], ["Chen", "Xiaojun", ""], ["Pong", "Ting Kei", ""]]}, {"id": "1512.09327", "submitter": "Thibaut Lienart", "authors": "Leonard Hasenclever, Stefan Webb, Thibaut Lienart, Sebastian Vollmer,\n  Balaji Lakshminarayanan, Charles Blundell, Yee Whye Teh", "title": "Distributed Bayesian Learning with Stochastic Natural-gradient\n  Expectation Propagation and the Posterior Server", "comments": "37 pages, 7 figures", "journal-ref": "Journal of Machine Learning Research 18 (2017) 1-37", "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper makes two contributions to Bayesian machine learning algorithms.\nFirstly, we propose stochastic natural gradient expectation propagation (SNEP),\na novel alternative to expectation propagation (EP), a popular variational\ninference algorithm. SNEP is a black box variational algorithm, in that it does\nnot require any simplifying assumptions on the distribution of interest, beyond\nthe existence of some Monte Carlo sampler for estimating the moments of the EP\ntilted distributions. Further, as opposed to EP which has no guarantee of\nconvergence, SNEP can be shown to be convergent, even when using Monte Carlo\nmoment estimates. Secondly, we propose a novel architecture for distributed\nBayesian learning which we call the posterior server. The posterior server\nallows scalable and robust Bayesian learning in cases where a data set is\nstored in a distributed manner across a cluster, with each compute node\ncontaining a disjoint subset of data. An independent Monte Carlo sampler is run\non each compute node, with direct access only to the local data subset, but\nwhich targets an approximation to the global posterior distribution given all\ndata across the whole cluster. This is achieved by using a distributed\nasynchronous implementation of SNEP to pass messages across the cluster. We\ndemonstrate SNEP and the posterior server on distributed Bayesian learning of\nlogistic regression and neural networks.\n  Keywords: Distributed Learning, Large Scale Learning, Deep Learning, Bayesian\nLearn- ing, Variational Inference, Expectation Propagation, Stochastic\nApproximation, Natural Gradient, Markov chain Monte Carlo, Parameter Server,\nPosterior Server.\n", "versions": [{"version": "v1", "created": "Thu, 31 Dec 2015 17:30:45 GMT"}, {"version": "v2", "created": "Thu, 1 Sep 2016 16:02:19 GMT"}, {"version": "v3", "created": "Thu, 27 Oct 2016 12:45:58 GMT"}, {"version": "v4", "created": "Thu, 7 Sep 2017 18:36:51 GMT"}], "update_date": "2017-09-11", "authors_parsed": [["Hasenclever", "Leonard", ""], ["Webb", "Stefan", ""], ["Lienart", "Thibaut", ""], ["Vollmer", "Sebastian", ""], ["Lakshminarayanan", "Balaji", ""], ["Blundell", "Charles", ""], ["Teh", "Yee Whye", ""]]}]