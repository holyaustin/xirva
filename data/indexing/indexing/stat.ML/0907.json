[{"id": "0907.0781", "submitter": "Hal Daum\\'e III", "authors": "Yee Whye Teh and Hal Daum\\'e III and Daniel Roy", "title": "Bayesian Agglomerative Clustering with Coalescents", "comments": "NIPS 2008", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We introduce a new Bayesian model for hierarchical clustering based on a\nprior over trees called Kingman's coalescent. We develop novel greedy and\nsequential Monte Carlo inferences which operate in a bottom-up agglomerative\nfashion. We show experimentally the superiority of our algorithms over others,\nand demonstrate our approach in document clustering and phylolinguistics.\n", "versions": [{"version": "v1", "created": "Sat, 4 Jul 2009 18:24:06 GMT"}], "update_date": "2009-07-07", "authors_parsed": [["Teh", "Yee Whye", ""], ["Daum\u00e9", "Hal", "III"], ["Roy", "Daniel", ""]]}, {"id": "0907.1013", "submitter": "David Blei", "authors": "David M. Blei and John D. Lafferty", "title": "Visualizing Topics with Multi-Word Expressions", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We describe a new method for visualizing topics, the distributions over terms\nthat are automatically extracted from large text corpora using latent variable\nmodels. Our method finds significant $n$-grams related to a topic, which are\nthen used to help understand and interpret the underlying distribution.\nCompared with the usual visualization, which simply lists the most probable\ntopical terms, the multi-word expressions provide a better intuitive impression\nfor what a topic is \"about.\" Our approach is based on a language model of\narbitrary length expressions, for which we develop a new methodology based on\nnested permutation tests to find significant phrases. We show that this method\noutperforms the more standard use of $\\chi^2$ and likelihood ratio tests. We\nillustrate the topic presentations on corpora of scientific abstracts and news\narticles.\n", "versions": [{"version": "v1", "created": "Mon, 6 Jul 2009 15:29:00 GMT"}], "update_date": "2009-07-07", "authors_parsed": [["Blei", "David M.", ""], ["Lafferty", "John D.", ""]]}, {"id": "0907.1531", "submitter": "Mikhail Zaslavskiy", "authors": "Brice Hoffmann (CBIO), Mikhail Zaslavskiy (CBIO, CMM), Jean-Philippe\n  Vert (CBIO), V\\'eronique Stoven (CBIO)", "title": "A new protein binding pocket similarity measure based on comparison of\n  3D atom clouds: application to ligand prediction", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML q-bio.BM q-bio.QM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Motivation: Prediction of ligands for proteins of known 3D structure is\nimportant to understand structure-function relationship, predict molecular\nfunction, or design new drugs. Results: We explore a new approach for ligand\nprediction in which binding pockets are represented by atom clouds. Each target\npocket is compared to an ensemble of pockets of known ligands. Pockets are\naligned in 3D space with further use of convolution kernels between clouds of\npoints. Performance of the new method for ligand prediction is compared to\nthose of other available measures and to docking programs. We discuss two\ncriteria to compare the quality of similarity measures: area under ROC curve\n(AUC) and classification based scores. We show that the latter is better suited\nto evaluate the methods with respect to ligand prediction. Our results on\nexisting and new benchmarks indicate that the new method outperforms other\napproaches, including docking. Availability: The new method is available at\nhttp://cbio.ensmp.fr/paris/ Contact: mikhail.zaslavskiy@mines-paristech.fr\n", "versions": [{"version": "v1", "created": "Thu, 9 Jul 2009 13:10:09 GMT"}], "update_date": "2009-07-10", "authors_parsed": [["Hoffmann", "Brice", "", "CBIO"], ["Zaslavskiy", "Mikhail", "", "CBIO, CMM"], ["Vert", "Jean-Philippe", "", "CBIO"], ["Stoven", "V\u00e9ronique", "", "CBIO"]]}, {"id": "0907.2079", "submitter": "Yong Zhang", "authors": "Zhaosong Lu and Yong Zhang", "title": "An Augmented Lagrangian Approach for Sparse Principal Component Analysis", "comments": "42 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.OC cs.LG math.ST stat.AP stat.CO stat.ME stat.ML stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Principal component analysis (PCA) is a widely used technique for data\nanalysis and dimension reduction with numerous applications in science and\nengineering. However, the standard PCA suffers from the fact that the principal\ncomponents (PCs) are usually linear combinations of all the original variables,\nand it is thus often difficult to interpret the PCs. To alleviate this\ndrawback, various sparse PCA approaches were proposed in literature [15, 6, 17,\n28, 8, 25, 18, 7, 16]. Despite success in achieving sparsity, some important\nproperties enjoyed by the standard PCA are lost in these methods such as\nuncorrelation of PCs and orthogonality of loading vectors. Also, the total\nexplained variance that they attempt to maximize can be too optimistic. In this\npaper we propose a new formulation for sparse PCA, aiming at finding sparse and\nnearly uncorrelated PCs with orthogonal loading vectors while explaining as\nmuch of the total variance as possible. We also develop a novel augmented\nLagrangian method for solving a class of nonsmooth constrained optimization\nproblems, which is well suited for our formulation of sparse PCA. We show that\nit converges to a feasible point, and moreover under some regularity\nassumptions, it converges to a stationary point. Additionally, we propose two\nnonmonotone gradient methods for solving the augmented Lagrangian subproblems,\nand establish their global and local convergence. Finally, we compare our\nsparse PCA approach with several existing methods on synthetic, random, and\nreal data, respectively. The computational results demonstrate that the sparse\nPCs produced by our approach substantially outperform those by other methods in\nterms of total explained variance, correlation of PCs, and orthogonality of\nloading vectors.\n", "versions": [{"version": "v1", "created": "Mon, 13 Jul 2009 00:45:51 GMT"}], "update_date": "2009-07-14", "authors_parsed": [["Lu", "Zhaosong", ""], ["Zhang", "Yong", ""]]}, {"id": "0907.2337", "submitter": "Mladen Kolar", "authors": "Mladen Kolar, Eric P. Xing", "title": "Sparsistent Estimation of Time-Varying Discrete Markov Random Fields", "comments": "Updated references. Reorganized proofs. Added simulation studies", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Network models have been popular for modeling and representing complex\nrelationships and dependencies between observed variables. When data comes from\na dynamic stochastic process, a single static network model cannot adequately\ncapture transient dependencies, such as, gene regulatory dependencies\nthroughout a developmental cycle of an organism. Kolar et al (2010b) proposed a\nmethod based on kernel-smoothing l1-penalized logistic regression for\nestimating time-varying networks from nodal observations collected from a\ntime-series of observational data. In this paper, we establish conditions under\nwhich the proposed method consistently recovers the structure of a time-varying\nnetwork. This work complements previous empirical findings by providing sound\ntheoretical guarantees for the proposed estimation procedure. For completeness,\nwe include numerical simulations in the paper.\n", "versions": [{"version": "v1", "created": "Tue, 14 Jul 2009 12:09:33 GMT"}, {"version": "v2", "created": "Tue, 2 Apr 2013 14:30:31 GMT"}], "update_date": "2013-04-03", "authors_parsed": [["Kolar", "Mladen", ""], ["Xing", "Eric P.", ""]]}, {"id": "0907.3220", "submitter": "Ula\\c{s} Ba\\u{g}ci", "authors": "Ulas Bagci and Engin Erzin", "title": "Inter Genre Similarity Modelling For Automatic Music Genre\n  Classification", "comments": "Dafx 2006 submission", "journal-ref": "9th International Conference on Digital Audio Effects (DAFx-06),\n  Montreal, Canada, September 18-20, 2006", "doi": null, "report-no": null, "categories": "cs.SD cs.AI stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Music genre classification is an essential tool for music information\nretrieval systems and it has been finding critical applications in various\nmedia platforms. Two important problems of the automatic music genre\nclassification are feature extraction and classifier design. This paper\ninvestigates inter-genre similarity modelling (IGS) to improve the performance\nof automatic music genre classification. Inter-genre similarity information is\nextracted over the mis-classified feature population. Once the inter-genre\nsimilarity is modelled, elimination of the inter-genre similarity reduces the\ninter-genre confusion and improves the identification rates. Inter-genre\nsimilarity modelling is further improved with iterative IGS modelling(IIGS) and\nscore modelling for IGS elimination(SMIGS). Experimental results with promising\nclassification improvements are provided.\n", "versions": [{"version": "v1", "created": "Sat, 18 Jul 2009 13:25:59 GMT"}], "update_date": "2018-10-18", "authors_parsed": [["Bagci", "Ulas", ""], ["Erzin", "Engin", ""]]}, {"id": "0907.3426", "submitter": "Theodore Alexandrov", "authors": "Theodore Alexandrov, Klaus Steinhorst, Oliver Keszoecze, Stefan\n  Schiffler", "title": "SparseCodePicking: feature extraction in mass spectrometry using sparse\n  coding algorithms", "comments": "10 pages, 6 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML physics.med-ph stat.AP stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Mass spectrometry (MS) is an important technique for chemical profiling which\ncalculates for a sample a high dimensional histogram-like spectrum. A crucial\nstep of MS data processing is the peak picking which selects peaks containing\ninformation about molecules with high concentrations which are of interest in\nan MS investigation. We present a new procedure of the peak picking based on a\nsparse coding algorithm. Given a set of spectra of different classes, i.e. with\ndifferent positions and heights of the peaks, this procedure can extract peaks\nby means of unsupervised learning. Instead of an $l_1$-regularization penalty\nterm used in the original sparse coding algorithm we propose using an\nelastic-net penalty term for better regularization. The evaluation is done by\nmeans of simulation. We show that for a large region of parameters the proposed\npeak picking method based on the sparse coding features outperforms a mean\nspectrum-based method. Moreover, we demonstrate the procedure applying it to\ntwo real-life datasets.\n", "versions": [{"version": "v1", "created": "Mon, 20 Jul 2009 15:50:22 GMT"}, {"version": "v2", "created": "Mon, 5 Oct 2009 08:58:10 GMT"}], "update_date": "2009-10-05", "authors_parsed": [["Alexandrov", "Theodore", ""], ["Steinhorst", "Klaus", ""], ["Keszoecze", "Oliver", ""], ["Schiffler", "Stefan", ""]]}, {"id": "0907.3740", "submitter": "Massimiliano Pontil", "authors": "Andreas Maurer and Massimiliano Pontil", "title": "Empirical Bernstein Bounds and Sample Variance Penalization", "comments": "10 pages, 1 figure, Proc. Computational Learning Theory Conference\n  (COLT 2009)", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We give improved constants for data dependent and variance sensitive\nconfidence bounds, called empirical Bernstein bounds, and extend these\ninequalities to hold uniformly over classes of functionswhose growth function\nis polynomial in the sample size n. The bounds lead us to consider sample\nvariance penalization, a novel learning method which takes into account the\nempirical variance of the loss function. We give conditions under which sample\nvariance penalization is effective. In particular, we present a bound on the\nexcess risk incurred by the method. Using this, we argue that there are\nsituations in which the excess risk of our method is of order 1/n, while the\nexcess risk of empirical risk minimization is of order 1/sqrt/{n}. We show some\nexperimental results, which confirm the theory. Finally, we discuss the\npotential application of our results to sample compression schemes.\n", "versions": [{"version": "v1", "created": "Tue, 21 Jul 2009 20:21:38 GMT"}], "update_date": "2009-07-23", "authors_parsed": [["Maurer", "Andreas", ""], ["Pontil", "Massimiliano", ""]]}, {"id": "0907.4643", "submitter": "Dariusz Plewczynski", "authors": "Dariusz Plewczynski", "title": "Mean-Field Theory of Meta-Learning", "comments": "23 pages", "journal-ref": null, "doi": "10.1088/1742-5468/2009/11/P11003", "report-no": null, "categories": "stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We discuss here the mean-field theory for a cellular automata model of\nmeta-learning. The meta-learning is the process of combining outcomes of\nindividual learning procedures in order to determine the final decision with\nhigher accuracy than any single learning method. Our method is constructed from\nan ensemble of interacting, learning agents, that acquire and process incoming\ninformation using various types, or different versions of machine learning\nalgorithms. The abstract learning space, where all agents are located, is\nconstructed here using a fully connected model that couples all agents with\nrandom strength values. The cellular automata network simulates the higher\nlevel integration of information acquired from the independent learning trials.\nThe final classification of incoming input data is therefore defined as the\nstationary state of the meta-learning system using simple majority rule, yet\nthe minority clusters that share opposite classification outcome can be\nobserved in the system. Therefore, the probability of selecting proper class\nfor a given input data, can be estimated even without the prior knowledge of\nits affiliation. The fuzzy logic can be easily introduced into the system, even\nif learning agents are build from simple binary classification machine learning\nalgorithms by calculating the percentage of agreeing agents.\n", "versions": [{"version": "v1", "created": "Mon, 27 Jul 2009 14:43:22 GMT"}, {"version": "v2", "created": "Fri, 9 Oct 2009 11:31:24 GMT"}], "update_date": "2015-05-13", "authors_parsed": [["Plewczynski", "Dariusz", ""]]}, {"id": "0907.4728", "submitter": "Alain Celisse", "authors": "Sylvain Arlot (LIENS), Alain Celisse (MIA)", "title": "A survey of cross-validation procedures for model selection", "comments": null, "journal-ref": "Statistics Surveys 4 (2010) 40--79", "doi": "10.1214/09-SS054", "report-no": null, "categories": "math.ST stat.AP stat.ME stat.ML stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Used to estimate the risk of an estimator or to perform model selection,\ncross-validation is a widespread strategy because of its simplicity and its\napparent universality. Many results exist on the model selection performances\nof cross-validation procedures. This survey intends to relate these results to\nthe most recent advances of model selection theory, with a particular emphasis\non distinguishing empirical statements from rigorous theoretical results. As a\nconclusion, guidelines are provided for choosing the best cross-validation\nprocedure according to the particular features of the problem in hand.\n", "versions": [{"version": "v1", "created": "Mon, 27 Jul 2009 18:37:23 GMT"}], "update_date": "2011-02-01", "authors_parsed": [["Arlot", "Sylvain", "", "LIENS"], ["Celisse", "Alain", "", "MIA"]]}, {"id": "0907.5309", "submitter": "Bharath Sriperumbudur", "authors": "Bharath K. Sriperumbudur, Arthur Gretton, Kenji Fukumizu, Bernhard\n  Sch\\\"olkopf and Gert R. G. Lanckriet", "title": "Hilbert space embeddings and metrics on probability measures", "comments": "48 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A Hilbert space embedding for probability measures has recently been\nproposed, with applications including dimensionality reduction, homogeneity\ntesting, and independence testing. This embedding represents any probability\nmeasure as a mean element in a reproducing kernel Hilbert space (RKHS). A\npseudometric on the space of probability measures can be defined as the\ndistance between distribution embeddings: we denote this as $\\gamma_k$, indexed\nby the kernel function $k$ that defines the inner product in the RKHS.\n  We present three theoretical properties of $\\gamma_k$. First, we consider the\nquestion of determining the conditions on the kernel $k$ for which $\\gamma_k$\nis a metric: such $k$ are denoted {\\em characteristic kernels}. Unlike\npseudometrics, a metric is zero only when two distributions coincide, thus\nensuring the RKHS embedding maps all distributions uniquely (i.e., the\nembedding is injective). While previously published conditions may apply only\nin restricted circumstances (e.g. on compact domains), and are difficult to\ncheck, our conditions are straightforward and intuitive: bounded continuous\nstrictly positive definite kernels are characteristic. Alternatively, if a\nbounded continuous kernel is translation-invariant on $\\bb{R}^d$, then it is\ncharacteristic if and only if the support of its Fourier transform is the\nentire $\\bb{R}^d$. Second, we show that there exist distinct distributions that\nare arbitrarily close in $\\gamma_k$. Third, to understand the nature of the\ntopology induced by $\\gamma_k$, we relate $\\gamma_k$ to other popular metrics\non probability measures, and present conditions on the kernel $k$ under which\n$\\gamma_k$ metrizes the weak topology.\n", "versions": [{"version": "v1", "created": "Thu, 30 Jul 2009 11:20:24 GMT"}, {"version": "v2", "created": "Wed, 12 Aug 2009 20:31:39 GMT"}, {"version": "v3", "created": "Sat, 30 Jan 2010 02:42:08 GMT"}], "update_date": "2010-01-30", "authors_parsed": [["Sriperumbudur", "Bharath K.", ""], ["Gretton", "Arthur", ""], ["Fukumizu", "Kenji", ""], ["Sch\u00f6lkopf", "Bernhard", ""], ["Lanckriet", "Gert R. G.", ""]]}, {"id": "0907.5494", "submitter": "Sebastien Bubeck", "authors": "Sebastien Bubeck, Marina Meila, Ulrike von Luxburg", "title": "How the initialization affects the stability of the k-means algorithm", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We investigate the role of the initialization for the stability of the\nk-means clustering algorithm. As opposed to other papers, we consider the\nactual k-means algorithm and do not ignore its property of getting stuck in\nlocal optima. We are interested in the actual clustering, not only in the costs\nof the solution. We analyze when different initializations lead to the same\nlocal optimum, and when they lead to different local optima. This enables us to\nprove that it is reasonable to select the number of clusters based on stability\nscores.\n", "versions": [{"version": "v1", "created": "Fri, 31 Jul 2009 09:19:49 GMT"}], "update_date": "2009-08-03", "authors_parsed": [["Bubeck", "Sebastien", ""], ["Meila", "Marina", ""], ["von Luxburg", "Ulrike", ""]]}]