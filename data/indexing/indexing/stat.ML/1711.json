[{"id": "1711.00001", "submitter": "Haoze Wu", "authors": "Haoze Wu, Yangyu Zhou", "title": "Gene Ontology (GO) Prediction using Machine Learning Methods", "comments": "The results in this paper result from a biased test set, and is\n  therefore not reliable", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CE q-bio.QM stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We applied machine learning to predict whether a gene is involved in axon\nregeneration. We extracted 31 features from different databases and trained\nfive machine learning models. Our optimal model, a Random Forest Classifier\nwith 50 submodels, yielded a test score of 85.71%, which is 4.1% higher than\nthe baseline score. We concluded that our models have some predictive\ncapability. Similar methodology and features could be applied to predict other\nGene Ontology (GO) terms.\n", "versions": [{"version": "v1", "created": "Mon, 30 Oct 2017 19:02:13 GMT"}, {"version": "v2", "created": "Thu, 26 Sep 2019 16:47:43 GMT"}], "update_date": "2019-09-27", "authors_parsed": [["Wu", "Haoze", ""], ["Zhou", "Yangyu", ""]]}, {"id": "1711.00064", "submitter": "Chandler Zuo", "authors": "Chandler Zuo", "title": "Calibration for Stratified Classification Models", "comments": "14 pages, 12 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In classification problems, sampling bias between training data and testing\ndata is critical to the ranking performance of classification scores. Such bias\ncan be both unintentionally introduced by data collection and intentionally\nintroduced by the algorithm, such as under-sampling or weighting techniques\napplied to imbalanced data. When such sampling bias exists, using the raw\nclassification score to rank observations in the testing data can lead to\nsuboptimal results. In this paper, I investigate the optimal calibration\nstrategy in general settings, and develop a practical solution for one specific\nsampling bias case, where the sampling bias is introduced by stratified\nsampling. The optimal solution is developed by analytically solving the problem\nof optimizing the ROC curve. For practical data, I propose a ranking algorithm\nfor general classification models with stratified data. Numerical experiments\ndemonstrate that the proposed algorithm effectively addresses the stratified\nsampling bias issue. Interestingly, the proposed method shows its potential\napplicability in two other machine learning areas: unsupervised learning and\nmodel ensembling, which can be future research topics.\n", "versions": [{"version": "v1", "created": "Tue, 31 Oct 2017 19:23:57 GMT"}], "update_date": "2017-11-02", "authors_parsed": [["Zuo", "Chandler", ""]]}, {"id": "1711.00066", "submitter": "Konrad Zolna", "authors": "Konrad Zolna, Devansh Arpit, Dendi Suhubdy, Yoshua Bengio", "title": "Fraternal Dropout", "comments": "Accepted to ICLR 2018. Extended appendix. Added official GitHub code\n  for replication: https://github.com/kondiz/fraternal-dropout . Added\n  references. Corrected typos", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.AI cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recurrent neural networks (RNNs) are important class of architectures among\nneural networks useful for language modeling and sequential prediction.\nHowever, optimizing RNNs is known to be harder compared to feed-forward neural\nnetworks. A number of techniques have been proposed in literature to address\nthis problem. In this paper we propose a simple technique called fraternal\ndropout that takes advantage of dropout to achieve this goal. Specifically, we\npropose to train two identical copies of an RNN (that share parameters) with\ndifferent dropout masks while minimizing the difference between their\n(pre-softmax) predictions. In this way our regularization encourages the\nrepresentations of RNNs to be invariant to dropout mask, thus being robust. We\nshow that our regularization term is upper bounded by the expectation-linear\ndropout objective which has been shown to address the gap due to the difference\nbetween the train and inference phases of dropout. We evaluate our model and\nachieve state-of-the-art results in sequence modeling tasks on two benchmark\ndatasets - Penn Treebank and Wikitext-2. We also show that our approach leads\nto performance improvement by a significant margin in image captioning\n(Microsoft COCO) and semi-supervised (CIFAR-10) tasks.\n", "versions": [{"version": "v1", "created": "Tue, 31 Oct 2017 19:32:45 GMT"}, {"version": "v2", "created": "Thu, 16 Nov 2017 16:40:34 GMT"}, {"version": "v3", "created": "Wed, 31 Jan 2018 00:12:47 GMT"}, {"version": "v4", "created": "Wed, 28 Mar 2018 15:50:58 GMT"}], "update_date": "2018-03-29", "authors_parsed": [["Zolna", "Konrad", ""], ["Arpit", "Devansh", ""], ["Suhubdy", "Dendi", ""], ["Bengio", "Yoshua", ""]]}, {"id": "1711.00070", "submitter": "Anna Korba", "authors": "Stephan Cl\\'emen\\c{c}on, Anna Korba, Eric Sibony", "title": "Ranking Median Regression: Learning to Order through Local Consensus", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.ML stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This article is devoted to the problem of predicting the value taken by a\nrandom permutation $\\Sigma$, describing the preferences of an individual over a\nset of numbered items $\\{1,\\; \\ldots,\\; n\\}$ say, based on the observation of\nan input/explanatory r.v. $X$ e.g. characteristics of the individual), when\nerror is measured by the Kendall $\\tau$ distance. In the probabilistic\nformulation of the 'Learning to Order' problem we propose, which extends the\nframework for statistical Kemeny ranking aggregation developped in\n\\citet{CKS17}, this boils down to recovering conditional Kemeny medians of\n$\\Sigma$ given $X$ from i.i.d. training examples $(X_1, \\Sigma_1),\\; \\ldots,\\;\n(X_N, \\Sigma_N)$. For this reason, this statistical learning problem is\nreferred to as \\textit{ranking median regression} here. Our contribution is\ntwofold. We first propose a probabilistic theory of ranking median regression:\nthe set of optimal elements is characterized, the performance of empirical risk\nminimizers is investigated in this context and situations where fast learning\nrates can be achieved are also exhibited. Next we introduce the concept of\nlocal consensus/median, in order to derive efficient methods for ranking median\nregression. The major advantage of this local learning approach lies in its\nclose connection with the widely studied Kemeny aggregation problem. From an\nalgorithmic perspective, this permits to build predictive rules for ranking\nmedian regression by implementing efficient techniques for (approximate) Kemeny\nmedian computations at a local level in a tractable manner. In particular,\nversions of $k$-nearest neighbor and tree-based methods, tailored to ranking\nmedian regression, are investigated. Accuracy of piecewise constant ranking\nmedian regression rules is studied under a specific smoothness assumption for\n$\\Sigma$'s conditional distribution given $X$.\n", "versions": [{"version": "v1", "created": "Tue, 31 Oct 2017 19:40:40 GMT"}, {"version": "v2", "created": "Mon, 18 Dec 2017 21:36:32 GMT"}], "update_date": "2017-12-20", "authors_parsed": [["Cl\u00e9men\u00e7on", "Stephan", ""], ["Korba", "Anna", ""], ["Sibony", "Eric", ""]]}, {"id": "1711.00083", "submitter": "Alejandro Schuler", "authors": "Alejandro Schuler, Ken Jung, Robert Tibshirani, Trevor Hastie, Nigam\n  Shah", "title": "Synth-Validation: Selecting the Best Causal Inference Method for a Given\n  Dataset", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Many decisions in healthcare, business, and other policy domains are made\nwithout the support of rigorous evidence due to the cost and complexity of\nperforming randomized experiments. Using observational data to answer causal\nquestions is risky: subjects who receive different treatments also differ in\nother ways that affect outcomes. Many causal inference methods have been\ndeveloped to mitigate these biases. However, there is no way to know which\nmethod might produce the best estimate of a treatment effect in a given study.\nIn analogy to cross-validation, which estimates the prediction error of\npredictive models applied to a given dataset, we propose synth-validation, a\nprocedure that estimates the estimation error of causal inference methods\napplied to a given dataset. In synth-validation, we use the observed data to\nestimate generative distributions with known treatment effects. We apply each\ncausal inference method to datasets sampled from these distributions and\ncompare the effect estimates with the known effects to estimate error. Using\nsimulations, we show that using synth-validation to select a causal inference\nmethod for each study lowers the expected estimation error relative to\nconsistently using any single method.\n", "versions": [{"version": "v1", "created": "Tue, 31 Oct 2017 20:05:27 GMT"}], "update_date": "2017-11-02", "authors_parsed": [["Schuler", "Alejandro", ""], ["Jung", "Ken", ""], ["Tibshirani", "Robert", ""], ["Hastie", "Trevor", ""], ["Shah", "Nigam", ""]]}, {"id": "1711.00108", "submitter": "Elliot Meyerson", "authors": "Elliot Meyerson and Risto Miikkulainen", "title": "Beyond Shared Hierarchies: Deep Multitask Learning through Soft Layer\n  Ordering", "comments": "14 pages (main paper: 10 pages). Published as a conference paper at\n  ICLR 2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Existing deep multitask learning (MTL) approaches align layers shared between\ntasks in a parallel ordering. Such an organization significantly constricts the\ntypes of shared structure that can be learned. The necessity of parallel\nordering for deep MTL is first tested by comparing it with permuted ordering of\nshared layers. The results indicate that a flexible ordering can enable more\neffective sharing, thus motivating the development of a soft ordering approach,\nwhich learns how shared layers are applied in different ways for different\ntasks. Deep MTL with soft ordering outperforms parallel ordering methods across\na series of domains. These results suggest that the power of deep MTL comes\nfrom learning highly general building blocks that can be assembled to meet the\ndemands of each task.\n", "versions": [{"version": "v1", "created": "Tue, 31 Oct 2017 20:55:06 GMT"}, {"version": "v2", "created": "Tue, 13 Feb 2018 02:05:34 GMT"}], "update_date": "2018-02-14", "authors_parsed": [["Meyerson", "Elliot", ""], ["Miikkulainen", "Risto", ""]]}, {"id": "1711.00126", "submitter": "Abolfazl Hashemi", "authors": "Abolfazl Hashemi and Haris Vikalo", "title": "Accelerated Sparse Subspace Clustering", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  State-of-the-art algorithms for sparse subspace clustering perform spectral\nclustering on a similarity matrix typically obtained by representing each data\npoint as a sparse combination of other points using either basis pursuit (BP)\nor orthogonal matching pursuit (OMP). BP-based methods are often prohibitive in\npractice while the performance of OMP-based schemes are unsatisfactory,\nespecially in settings where data points are highly similar. In this paper, we\npropose a novel algorithm that exploits an accelerated variant of orthogonal\nleast-squares to efficiently find the underlying subspaces. We show that under\ncertain conditions the proposed algorithm returns a subspace-preserving\nsolution. Simulation results illustrate that the proposed method compares\nfavorably with BP-based method in terms of running time while being\nsignificantly more accurate than OMP-based schemes.\n", "versions": [{"version": "v1", "created": "Tue, 31 Oct 2017 22:05:47 GMT"}], "update_date": "2017-11-02", "authors_parsed": [["Hashemi", "Abolfazl", ""], ["Vikalo", "Haris", ""]]}, {"id": "1711.00137", "submitter": "Jacob Schreiber", "authors": "Jacob Schreiber", "title": "Pomegranate: fast and flexible probabilistic modeling in python", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present pomegranate, an open source machine learning package for\nprobabilistic modeling in Python. Probabilistic modeling encompasses a wide\nrange of methods that explicitly describe uncertainty using probability\ndistributions. Three widely used probabilistic models implemented in\npomegranate are general mixture models, hidden Markov models, and Bayesian\nnetworks. A primary focus of pomegranate is to abstract away the complexities\nof training models from their definition. This allows users to focus on\nspecifying the correct model for their application instead of being limited by\ntheir understanding of the underlying algorithms. An aspect of this focus\ninvolves the collection of additive sufficient statistics from data sets as a\nstrategy for training models. This approach trivially enables many useful\nlearning strategies, such as out-of-core learning, minibatch learning, and\nsemi-supervised learning, without requiring the user to consider how to\npartition data or modify the algorithms to handle these tasks themselves.\npomegranate is written in Cython to speed up calculations and releases the\nglobal interpreter lock to allow for built-in multithreaded parallelism, making\nit competitive with---or outperform---other implementations of similar\nalgorithms. This paper presents an overview of the design choices in\npomegranate, and how they have enabled complex features to be supported by\nsimple code.\n", "versions": [{"version": "v1", "created": "Tue, 31 Oct 2017 22:53:20 GMT"}, {"version": "v2", "created": "Tue, 27 Feb 2018 23:43:16 GMT"}], "update_date": "2018-03-01", "authors_parsed": [["Schreiber", "Jacob", ""]]}, {"id": "1711.00141", "submitter": "Haoyang Zeng", "authors": "Constantinos Daskalakis, Andrew Ilyas, Vasilis Syrgkanis, Haoyang Zeng", "title": "Training GANs with Optimism", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.GT stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We address the issue of limit cycling behavior in training Generative\nAdversarial Networks and propose the use of Optimistic Mirror Decent (OMD) for\ntraining Wasserstein GANs. Recent theoretical results have shown that\noptimistic mirror decent (OMD) can enjoy faster regret rates in the context of\nzero-sum games. WGANs is exactly a context of solving a zero-sum game with\nsimultaneous no-regret dynamics. Moreover, we show that optimistic mirror\ndecent addresses the limit cycling problem in training WGANs. We formally show\nthat in the case of bi-linear zero-sum games the last iterate of OMD dynamics\nconverges to an equilibrium, in contrast to GD dynamics which are bound to\ncycle. We also portray the huge qualitative difference between GD and OMD\ndynamics with toy examples, even when GD is modified with many adaptations\nproposed in the recent literature, such as gradient penalty or momentum. We\napply OMD WGAN training to a bioinformatics problem of generating DNA\nsequences. We observe that models trained with OMD achieve consistently smaller\nKL divergence with respect to the true underlying distribution, than models\ntrained with GD variants. Finally, we introduce a new algorithm, Optimistic\nAdam, which is an optimistic variant of Adam. We apply it to WGAN training on\nCIFAR10 and observe improved performance in terms of inception score as\ncompared to Adam.\n", "versions": [{"version": "v1", "created": "Tue, 31 Oct 2017 23:09:08 GMT"}, {"version": "v2", "created": "Tue, 13 Feb 2018 16:33:55 GMT"}], "update_date": "2018-02-14", "authors_parsed": [["Daskalakis", "Constantinos", ""], ["Ilyas", "Andrew", ""], ["Syrgkanis", "Vasilis", ""], ["Zeng", "Haoyang", ""]]}, {"id": "1711.00142", "submitter": "Abolfazl Hashemi", "authors": "Abolfazl Hashemi, Rasoul Shafipour, Haris Vikalo, and Gonzalo Mateos", "title": "Sampling and Reconstruction of Graph Signals via Weak Submodularity and\n  Semidefinite Relaxation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study the problem of sampling a bandlimited graph signal in the presence\nof noise, where the objective is to select a node subset of prescribed\ncardinality that minimizes the signal reconstruction mean squared error (MSE).\nTo that end, we formulate the task at hand as the minimization of MSE subject\nto binary constraints, and approximate the resulting NP-hard problem via\nsemidefinite programming (SDP) relaxation. Moreover, we provide an alternative\nformulation based on maximizing a monotone weak submodular function and propose\na randomized-greedy algorithm to find a sub-optimal subset. We then derive a\nworst-case performance guarantee on the MSE returned by the randomized greedy\nalgorithm for general non-stationary graph signals. The efficacy of the\nproposed methods is illustrated through numerical simulations on synthetic and\nreal-world graphs. Notably, the randomized greedy algorithm yields an\norder-of-magnitude speedup over state-of-the-art greedy sampling schemes, while\nincurring only a marginal MSE performance loss.\n", "versions": [{"version": "v1", "created": "Tue, 31 Oct 2017 23:29:25 GMT"}], "update_date": "2017-11-02", "authors_parsed": [["Hashemi", "Abolfazl", ""], ["Shafipour", "Rasoul", ""], ["Vikalo", "Haris", ""], ["Mateos", "Gonzalo", ""]]}, {"id": "1711.00165", "submitter": "Yasaman Bahri", "authors": "Jaehoon Lee, Yasaman Bahri, Roman Novak, Samuel S. Schoenholz, Jeffrey\n  Pennington, Jascha Sohl-Dickstein", "title": "Deep Neural Networks as Gaussian Processes", "comments": "Published version in ICLR 2018. 10 pages + appendix", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  It has long been known that a single-layer fully-connected neural network\nwith an i.i.d. prior over its parameters is equivalent to a Gaussian process\n(GP), in the limit of infinite network width. This correspondence enables exact\nBayesian inference for infinite width neural networks on regression tasks by\nmeans of evaluating the corresponding GP. Recently, kernel functions which\nmimic multi-layer random neural networks have been developed, but only outside\nof a Bayesian framework. As such, previous work has not identified that these\nkernels can be used as covariance functions for GPs and allow fully Bayesian\nprediction with a deep neural network.\n  In this work, we derive the exact equivalence between infinitely wide deep\nnetworks and GPs. We further develop a computationally efficient pipeline to\ncompute the covariance function for these GPs. We then use the resulting GPs to\nperform Bayesian inference for wide deep neural networks on MNIST and CIFAR-10.\nWe observe that trained neural network accuracy approaches that of the\ncorresponding GP with increasing layer width, and that the GP uncertainty is\nstrongly correlated with trained network prediction error. We further find that\ntest performance increases as finite-width trained networks are made wider and\nmore similar to a GP, and thus that GP predictions typically outperform those\nof finite-width networks. Finally we connect the performance of these GPs to\nthe recent theory of signal propagation in random neural networks.\n", "versions": [{"version": "v1", "created": "Wed, 1 Nov 2017 02:13:25 GMT"}, {"version": "v2", "created": "Wed, 17 Jan 2018 02:50:16 GMT"}, {"version": "v3", "created": "Sat, 3 Mar 2018 00:45:00 GMT"}], "update_date": "2018-03-06", "authors_parsed": [["Lee", "Jaehoon", ""], ["Bahri", "Yasaman", ""], ["Novak", "Roman", ""], ["Schoenholz", "Samuel S.", ""], ["Pennington", "Jeffrey", ""], ["Sohl-Dickstein", "Jascha", ""]]}, {"id": "1711.00221", "submitter": "Haibin Yu", "authors": "Haibin Yu, Trong Nghia Hoang, Kian Hsiang Low, Patrick Jaillet", "title": "Stochastic Variational Inference for Bayesian Sparse Gaussian Process\n  Regression", "comments": "To appear in Proceedings of the International Joint Conference on\n  Neural Networks 2019 (IJCNN'19)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper presents a novel variational inference framework for deriving a\nfamily of Bayesian sparse Gaussian process regression (SGPR) models whose\napproximations are variationally optimal with respect to the full-rank GPR\nmodel enriched with various corresponding correlation structures of the\nobservation noises. Our variational Bayesian SGPR (VBSGPR) models jointly treat\nboth the distributions of the inducing variables and hyperparameters as\nvariational parameters, which enables the decomposability of the variational\nlower bound that in turn can be exploited for stochastic optimization. Such a\nstochastic optimization involves iteratively following the stochastic gradient\nof the variational lower bound to improve its estimates of the optimal\nvariational distributions of the inducing variables and hyperparameters (and\nhence the predictive distribution) of our VBSGPR models and is guaranteed to\nachieve asymptotic convergence to them. We show that the stochastic gradient is\nan unbiased estimator of the exact gradient and can be computed in constant\ntime per iteration, hence achieving scalability to big data. We empirically\nevaluate the performance of our proposed framework on two real-world, massive\ndatasets.\n", "versions": [{"version": "v1", "created": "Wed, 1 Nov 2017 06:30:03 GMT"}, {"version": "v2", "created": "Thu, 21 Mar 2019 00:06:02 GMT"}, {"version": "v3", "created": "Fri, 22 Mar 2019 06:46:30 GMT"}], "update_date": "2019-03-25", "authors_parsed": [["Yu", "Haibin", ""], ["Hoang", "Trong Nghia", ""], ["Low", "Kian Hsiang", ""], ["Jaillet", "Patrick", ""]]}, {"id": "1711.00258", "submitter": "Yucen Luo", "authors": "Yucen Luo, Jun Zhu, Mengxi Li, Yong Ren, Bo Zhang", "title": "Smooth Neighbors on Teacher Graphs for Semi-supervised Learning", "comments": "Accept as Spotlight in Computer Vision and Pattern Recognition 2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.NE stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The recently proposed self-ensembling methods have achieved promising results\nin deep semi-supervised learning, which penalize inconsistent predictions of\nunlabeled data under different perturbations. However, they only consider\nadding perturbations to each single data point, while ignoring the connections\nbetween data samples. In this paper, we propose a novel method, called Smooth\nNeighbors on Teacher Graphs (SNTG). In SNTG, a graph is constructed based on\nthe predictions of the teacher model, i.e., the implicit self-ensemble of\nmodels. Then the graph serves as a similarity measure with respect to which the\nrepresentations of \"similar\" neighboring points are learned to be smooth on the\nlow-dimensional manifold. We achieve state-of-the-art results on\nsemi-supervised learning benchmarks. The error rates are 9.89%, 3.99% for\nCIFAR-10 with 4000 labels, SVHN with 500 labels, respectively. In particular,\nthe improvements are significant when the labels are fewer. For the\nnon-augmented MNIST with only 20 labels, the error rate is reduced from\nprevious 4.81% to 1.36%. Our method also shows robustness to noisy labels.\n", "versions": [{"version": "v1", "created": "Wed, 1 Nov 2017 09:10:07 GMT"}, {"version": "v2", "created": "Wed, 28 Mar 2018 14:53:05 GMT"}], "update_date": "2018-03-29", "authors_parsed": [["Luo", "Yucen", ""], ["Zhu", "Jun", ""], ["Li", "Mengxi", ""], ["Ren", "Yong", ""], ["Zhang", "Bo", ""]]}, {"id": "1711.00313", "submitter": "Mostafa Dehghani", "authors": "Mostafa Dehghani, Aliaksei Severyn, Sascha Rothe, Jaap Kamps", "title": "Avoiding Your Teacher's Mistakes: Training Neural Networks with\n  Controlled Weak Supervision", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CL cs.NE stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Training deep neural networks requires massive amounts of training data, but\nfor many tasks only limited labeled data is available. This makes weak\nsupervision attractive, using weak or noisy signals like the output of\nheuristic methods or user click-through data for training. In a semi-supervised\nsetting, we can use a large set of data with weak labels to pretrain a neural\nnetwork and then fine-tune the parameters with a small amount of data with true\nlabels. This feels intuitively sub-optimal as these two independent stages\nleave the model unaware about the varying label quality. What if we could\nsomehow inform the model about the label quality? In this paper, we propose a\nsemi-supervised learning method where we train two neural networks in a\nmulti-task fashion: a \"target network\" and a \"confidence network\". The target\nnetwork is optimized to perform a given task and is trained using a large set\nof unlabeled data that are weakly annotated. We propose to weight the gradient\nupdates to the target network using the scores provided by the second\nconfidence network, which is trained on a small amount of supervised data. Thus\nwe avoid that the weight updates computed from noisy labels harm the quality of\nthe target network model. We evaluate our learning strategy on two different\ntasks: document ranking and sentiment classification. The results demonstrate\nthat our approach not only enhances the performance compared to the baselines\nbut also speeds up the learning process from weak labels.\n", "versions": [{"version": "v1", "created": "Wed, 1 Nov 2017 12:38:59 GMT"}, {"version": "v2", "created": "Thu, 7 Dec 2017 14:30:18 GMT"}], "update_date": "2017-12-08", "authors_parsed": [["Dehghani", "Mostafa", ""], ["Severyn", "Aliaksei", ""], ["Rothe", "Sascha", ""], ["Kamps", "Jaap", ""]]}, {"id": "1711.00342", "submitter": "Lester Mackey", "authors": "Lester Mackey, Vasilis Syrgkanis, Ilias Zadik", "title": "Orthogonal Machine Learning: Power and Limitations", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG econ.EM math.ST stat.ML stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Double machine learning provides $\\sqrt{n}$-consistent estimates of\nparameters of interest even when high-dimensional or nonparametric nuisance\nparameters are estimated at an $n^{-1/4}$ rate. The key is to employ\nNeyman-orthogonal moment equations which are first-order insensitive to\nperturbations in the nuisance parameters. We show that the $n^{-1/4}$\nrequirement can be improved to $n^{-1/(2k+2)}$ by employing a $k$-th order\nnotion of orthogonality that grants robustness to more complex or\nhigher-dimensional nuisance parameters. In the partially linear regression\nsetting popular in causal inference, we show that we can construct second-order\northogonal moments if and only if the treatment residual is not normally\ndistributed. Our proof relies on Stein's lemma and may be of independent\ninterest. We conclude by demonstrating the robustness benefits of an explicit\ndoubly-orthogonal estimation procedure for treatment effect.\n", "versions": [{"version": "v1", "created": "Wed, 1 Nov 2017 13:42:54 GMT"}, {"version": "v2", "created": "Tue, 21 Nov 2017 22:49:28 GMT"}, {"version": "v3", "created": "Thu, 21 Dec 2017 03:16:15 GMT"}, {"version": "v4", "created": "Tue, 12 Jun 2018 02:25:49 GMT"}, {"version": "v5", "created": "Sat, 16 Jun 2018 05:07:05 GMT"}, {"version": "v6", "created": "Wed, 1 Aug 2018 18:40:19 GMT"}], "update_date": "2018-08-03", "authors_parsed": [["Mackey", "Lester", ""], ["Syrgkanis", "Vasilis", ""], ["Zadik", "Ilias", ""]]}, {"id": "1711.00382", "submitter": "Khalil Elkhalil", "authors": "Khalil Elkhalil, Abla Kammoun, Romain Couillet, Tareq Y. Al-Naffouri,\n  Mohamed-Slim Alouini", "title": "A Large Dimensional Study of Regularized Discriminant Analysis\n  Classifiers", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This article carries out a large dimensional analysis of standard regularized\ndiscriminant analysis classifiers designed on the assumption that data arise\nfrom a Gaussian mixture model with different means and covariances. The\nanalysis relies on fundamental results from random matrix theory (RMT) when\nboth the number of features and the cardinality of the training data within\neach class grow large at the same pace. Under mild assumptions, we show that\nthe asymptotic classification error approaches a deterministic quantity that\ndepends only on the means and covariances associated with each class as well as\nthe problem dimensions. Such a result permits a better understanding of the\nperformance of regularized discriminant analsysis, in practical large but\nfinite dimensions, and can be used to determine and pre-estimate the optimal\nregularization parameter that minimizes the misclassification error\nprobability. Despite being theoretically valid only for Gaussian data, our\nfindings are shown to yield a high accuracy in predicting the performances\nachieved with real data sets drawn from the popular USPS data base, thereby\nmaking an interesting connection between theory and practice.\n", "versions": [{"version": "v1", "created": "Wed, 1 Nov 2017 14:52:06 GMT"}, {"version": "v2", "created": "Mon, 12 Feb 2018 12:39:18 GMT"}, {"version": "v3", "created": "Sun, 18 Feb 2018 12:10:00 GMT"}, {"version": "v4", "created": "Tue, 18 Jun 2019 09:49:04 GMT"}], "update_date": "2019-06-19", "authors_parsed": [["Elkhalil", "Khalil", ""], ["Kammoun", "Abla", ""], ["Couillet", "Romain", ""], ["Al-Naffouri", "Tareq Y.", ""], ["Alouini", "Mohamed-Slim", ""]]}, {"id": "1711.00388", "submitter": "Lunjia Hu", "authors": "Avrim Blum, Lunjia Hu", "title": "Active Tolerant Testing", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this work, we give the first algorithms for tolerant testing of nontrivial\nclasses in the active model: estimating the distance of a target function to a\nhypothesis class C with respect to some arbitrary distribution D, using only a\nsmall number of label queries to a polynomial-sized pool of unlabeled examples\ndrawn from D. Specifically, we show that for the class D of unions of d\nintervals on the line, we can estimate the error rate of the best hypothesis in\nthe class to an additive error epsilon from only $O(\\frac{1}{\\epsilon^6}\\log\n\\frac{1}{\\epsilon})$ label queries to an unlabeled pool of size\n$O(\\frac{d}{\\epsilon^2}\\log \\frac{1}{\\epsilon})$. The key point here is the\nnumber of labels needed is independent of the VC-dimension of the class. This\nextends the work of Balcan et al. [2012] who solved the non-tolerant testing\nproblem for this class (distinguishing the zero-error case from the case that\nthe best hypothesis in the class has error greater than epsilon).\n  We also consider the related problem of estimating the performance of a given\nlearning algorithm A in this setting. That is, given a large pool of unlabeled\nexamples drawn from distribution D, can we, from only a few label queries,\nestimate how well A would perform if the entire dataset were labeled? We focus\non k-Nearest Neighbor style algorithms, and also show how our results can be\napplied to the problem of hyperparameter tuning (selecting the best value of k\nfor the given learning problem).\n", "versions": [{"version": "v1", "created": "Wed, 1 Nov 2017 15:20:13 GMT"}], "update_date": "2017-11-02", "authors_parsed": [["Blum", "Avrim", ""], ["Hu", "Lunjia", ""]]}, {"id": "1711.00400", "submitter": "Richard Combes", "authors": "Richard Combes, Stefan Magureanu and Alexandre Proutiere", "title": "Minimal Exploration in Structured Stochastic Bandits", "comments": "13 pages, NIPS 2017", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.AI cs.LG math.OC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper introduces and addresses a wide class of stochastic bandit\nproblems where the function mapping the arm to the corresponding reward\nexhibits some known structural properties. Most existing structures (e.g.\nlinear, Lipschitz, unimodal, combinatorial, dueling, ...) are covered by our\nframework. We derive an asymptotic instance-specific regret lower bound for\nthese problems, and develop OSSB, an algorithm whose regret matches this\nfundamental limit. OSSB is not based on the classical principle of \"optimism in\nthe face of uncertainty\" or on Thompson sampling, and rather aims at matching\nthe minimal exploration rates of sub-optimal arms as characterized in the\nderivation of the regret lower bound. We illustrate the efficiency of OSSB\nusing numerical experiments in the case of the linear bandit problem and show\nthat OSSB outperforms existing algorithms, including Thompson sampling.\n", "versions": [{"version": "v1", "created": "Wed, 1 Nov 2017 15:40:26 GMT"}], "update_date": "2017-11-02", "authors_parsed": [["Combes", "Richard", ""], ["Magureanu", "Stefan", ""], ["Proutiere", "Alexandre", ""]]}, {"id": "1711.00436", "submitter": "Hanxiao Liu", "authors": "Hanxiao Liu, Karen Simonyan, Oriol Vinyals, Chrisantha Fernando, Koray\n  Kavukcuoglu", "title": "Hierarchical Representations for Efficient Architecture Search", "comments": "Accepted as a conference paper at ICLR 2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV cs.NE stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We explore efficient neural architecture search methods and show that a\nsimple yet powerful evolutionary algorithm can discover new architectures with\nexcellent performance. Our approach combines a novel hierarchical genetic\nrepresentation scheme that imitates the modularized design pattern commonly\nadopted by human experts, and an expressive search space that supports complex\ntopologies. Our algorithm efficiently discovers architectures that outperform a\nlarge number of manually designed models for image classification, obtaining\ntop-1 error of 3.6% on CIFAR-10 and 20.3% when transferred to ImageNet, which\nis competitive with the best existing neural architecture search approaches. We\nalso present results using random search, achieving 0.3% less top-1 accuracy on\nCIFAR-10 and 0.1% less on ImageNet whilst reducing the search time from 36\nhours down to 1 hour.\n", "versions": [{"version": "v1", "created": "Wed, 1 Nov 2017 16:46:27 GMT"}, {"version": "v2", "created": "Thu, 22 Feb 2018 22:31:30 GMT"}], "update_date": "2018-02-26", "authors_parsed": [["Liu", "Hanxiao", ""], ["Simonyan", "Karen", ""], ["Vinyals", "Oriol", ""], ["Fernando", "Chrisantha", ""], ["Kavukcuoglu", "Koray", ""]]}, {"id": "1711.00449", "submitter": "Angus Galloway", "authors": "Angus Galloway, Graham W. Taylor, Medhat Moussa", "title": "Attacking Binarized Neural Networks", "comments": "Published as a conference paper at ICLR 2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Neural networks with low-precision weights and activations offer compelling\nefficiency advantages over their full-precision equivalents. The two most\nfrequently discussed benefits of quantization are reduced memory consumption,\nand a faster forward pass when implemented with efficient bitwise operations.\nWe propose a third benefit of very low-precision neural networks: improved\nrobustness against some adversarial attacks, and in the worst case, performance\nthat is on par with full-precision models. We focus on the very low-precision\ncase where weights and activations are both quantized to $\\pm$1, and note that\nstochastically quantizing weights in just one layer can sharply reduce the\nimpact of iterative attacks. We observe that non-scaled binary neural networks\nexhibit a similar effect to the original defensive distillation procedure that\nled to gradient masking, and a false notion of security. We address this by\nconducting both black-box and white-box experiments with binary models that do\nnot artificially mask gradients.\n", "versions": [{"version": "v1", "created": "Wed, 1 Nov 2017 17:28:26 GMT"}, {"version": "v2", "created": "Wed, 31 Jan 2018 17:03:31 GMT"}], "update_date": "2018-02-01", "authors_parsed": [["Galloway", "Angus", ""], ["Taylor", "Graham W.", ""], ["Moussa", "Medhat", ""]]}, {"id": "1711.00464", "submitter": "Alexander Alemi", "authors": "Alexander A. Alemi, Ben Poole, Ian Fischer, Joshua V. Dillon, Rif A.\n  Saurous, Kevin Murphy", "title": "Fixing a Broken ELBO", "comments": "21 pages, 9 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recent work in unsupervised representation learning has focused on learning\ndeep directed latent-variable models. Fitting these models by maximizing the\nmarginal likelihood or evidence is typically intractable, thus a common\napproximation is to maximize the evidence lower bound (ELBO) instead. However,\nmaximum likelihood training (whether exact or approximate) does not necessarily\nresult in a good latent representation, as we demonstrate both theoretically\nand empirically. In particular, we derive variational lower and upper bounds on\nthe mutual information between the input and the latent variable, and use these\nbounds to derive a rate-distortion curve that characterizes the tradeoff\nbetween compression and reconstruction accuracy. Using this framework, we\ndemonstrate that there is a family of models with identical ELBO, but different\nquantitative and qualitative characteristics. Our framework also suggests a\nsimple new method to ensure that latent variable models with powerful\nstochastic decoders do not ignore their latent code.\n", "versions": [{"version": "v1", "created": "Wed, 1 Nov 2017 17:58:43 GMT"}, {"version": "v2", "created": "Fri, 5 Jan 2018 19:34:58 GMT"}, {"version": "v3", "created": "Tue, 13 Feb 2018 20:54:38 GMT"}], "update_date": "2018-02-15", "authors_parsed": [["Alemi", "Alexander A.", ""], ["Poole", "Ben", ""], ["Fischer", "Ian", ""], ["Dillon", "Joshua V.", ""], ["Saurous", "Rif A.", ""], ["Murphy", "Kevin", ""]]}, {"id": "1711.00487", "submitter": "Ilia Kisil Mr", "authors": "Ilia Kisil, Giuseppe G. Calvi, Danilo P. Mandic", "title": "Tensor Valued Common and Individual Feature Extraction:\n  Multi-dimensional Perspective", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.SP stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A novel method for common and individual feature analysis from exceedingly\nlarge-scale data is proposed, in order to ensure the tractability of both the\ncomputation and storage and thus mitigate the curse of dimensionality, a major\nbottleneck in modern data science. This is achieved by making use of the\ninherent redundancy in so-called multi-block data structures, which represent\nmultiple observations of the same phenomenon taken at different times, angles\nor recording conditions. Upon providing an intrinsic link between the\nproperties of the outer vector product and extracted features in tensor\ndecompositions (TDs), the proposed common and individual information extraction\nfrom multi-block data is performed through imposing physical meaning to\notherwise unconstrained factorisation approaches. This is shown to dramatically\nreduce the dimensionality of search spaces for subsequent classification\nprocedures and to yield greatly enhanced accuracy. Simulations on a multi-class\nclassification task of large-scale extraction of individual features from a\ncollection of partially related real-world images demonstrate the advantages of\nthe \"blessing of dimensionality\" associated with TDs.\n", "versions": [{"version": "v1", "created": "Wed, 1 Nov 2017 18:03:07 GMT"}], "update_date": "2017-11-03", "authors_parsed": [["Kisil", "Ilia", ""], ["Calvi", "Giuseppe G.", ""], ["Mandic", "Danilo P.", ""]]}, {"id": "1711.00489", "submitter": "Samuel L. Smith", "authors": "Samuel L. Smith, Pieter-Jan Kindermans, Chris Ying and Quoc V. Le", "title": "Don't Decay the Learning Rate, Increase the Batch Size", "comments": "11 pages, 8 figures. Published as a conference paper at ICLR 2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV cs.DC stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  It is common practice to decay the learning rate. Here we show one can\nusually obtain the same learning curve on both training and test sets by\ninstead increasing the batch size during training. This procedure is successful\nfor stochastic gradient descent (SGD), SGD with momentum, Nesterov momentum,\nand Adam. It reaches equivalent test accuracies after the same number of\ntraining epochs, but with fewer parameter updates, leading to greater\nparallelism and shorter training times. We can further reduce the number of\nparameter updates by increasing the learning rate $\\epsilon$ and scaling the\nbatch size $B \\propto \\epsilon$. Finally, one can increase the momentum\ncoefficient $m$ and scale $B \\propto 1/(1-m)$, although this tends to slightly\nreduce the test accuracy. Crucially, our techniques allow us to repurpose\nexisting training schedules for large batch training with no hyper-parameter\ntuning. We train ResNet-50 on ImageNet to $76.1\\%$ validation accuracy in under\n30 minutes.\n", "versions": [{"version": "v1", "created": "Wed, 1 Nov 2017 18:04:31 GMT"}, {"version": "v2", "created": "Sat, 24 Feb 2018 00:16:12 GMT"}], "update_date": "2018-02-27", "authors_parsed": [["Smith", "Samuel L.", ""], ["Kindermans", "Pieter-Jan", ""], ["Ying", "Chris", ""], ["Le", "Quoc V.", ""]]}, {"id": "1711.00501", "submitter": "Tengyu Ma", "authors": "Rong Ge, Jason D. Lee, Tengyu Ma", "title": "Learning One-hidden-layer Neural Networks with Landscape Design", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.DS math.OC stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider the problem of learning a one-hidden-layer neural network: we\nassume the input $x\\in \\mathbb{R}^d$ is from Gaussian distribution and the\nlabel $y = a^\\top \\sigma(Bx) + \\xi$, where $a$ is a nonnegative vector in\n$\\mathbb{R}^m$ with $m\\le d$, $B\\in \\mathbb{R}^{m\\times d}$ is a full-rank\nweight matrix, and $\\xi$ is a noise vector. We first give an analytic formula\nfor the population risk of the standard squared loss and demonstrate that it\nimplicitly attempts to decompose a sequence of low-rank tensors simultaneously.\n  Inspired by the formula, we design a non-convex objective function $G(\\cdot)$\nwhose landscape is guaranteed to have the following properties: 1. All local\nminima of $G$ are also global minima.\n  2. All global minima of $G$ correspond to the ground truth parameters.\n  3. The value and gradient of $G$ can be estimated using samples.\n  With these properties, stochastic gradient descent on $G$ provably converges\nto the global minimum and learn the ground-truth parameters. We also prove\nfinite sample complexity result and validate the results by simulations.\n", "versions": [{"version": "v1", "created": "Wed, 1 Nov 2017 18:27:42 GMT"}, {"version": "v2", "created": "Fri, 3 Nov 2017 00:19:35 GMT"}], "update_date": "2017-11-06", "authors_parsed": [["Ge", "Rong", ""], ["Lee", "Jason D.", ""], ["Ma", "Tengyu", ""]]}, {"id": "1711.00629", "submitter": "Xin Zhang", "authors": "Xin Zhang, Weixuan Kou, Eric I-Chao Chang, He Gao, Yubo Fan and Yan Xu", "title": "Sleep Stage Classification Based on Multi-level Feature Learning and\n  Recurrent Neural Networks via Wearable Device", "comments": "11 pages, 10 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG q-bio.NC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper proposes a practical approach for automatic sleep stage\nclassification based on a multi-level feature learning framework and Recurrent\nNeural Network (RNN) classifier using heart rate and wrist actigraphy derived\nfrom a wearable device. The feature learning framework is designed to extract\nlow- and mid-level features. Low-level features capture temporal and frequency\ndomain properties and mid-level features learn compositions and structural\ninformation of signals. Since sleep staging is a sequential problem with\nlong-term dependencies, we take advantage of RNNs with Bidirectional Long\nShort-Term Memory (BLSTM) architectures for sequence data learning. To simulate\nthe actual situation of daily sleep, experiments are conducted with a resting\ngroup in which sleep is recorded in resting state, and a comprehensive group in\nwhich both resting sleep and non-resting sleep are included.We evaluate the\nalgorithm based on an eight-fold cross validation to classify five sleep stages\n(W, N1, N2, N3, and REM). The proposed algorithm achieves weighted precision,\nrecall and F1 score of 58.0%, 60.3%, and 58.2% in the resting group and 58.5%,\n61.1%, and 58.5% in the comprehensive group, respectively. Various comparison\nexperiments demonstrate the effectiveness of feature learning and BLSTM. We\nfurther explore the influence of depth and width of RNNs on performance. Our\nmethod is specially proposed for wearable devices and is expected to be\napplicable for long-term sleep monitoring at home. Without using too much prior\ndomain knowledge, our method has the potential to generalize sleep disorder\ndetection.\n", "versions": [{"version": "v1", "created": "Thu, 2 Nov 2017 06:28:45 GMT"}], "update_date": "2017-11-03", "authors_parsed": [["Zhang", "Xin", ""], ["Kou", "Weixuan", ""], ["Chang", "Eric I-Chao", ""], ["Gao", "He", ""], ["Fan", "Yubo", ""], ["Xu", "Yan", ""]]}, {"id": "1711.00636", "submitter": "Patrick McDermott", "authors": "Patrick L. McDermott and Christopher K. Wikle", "title": "Bayesian Recurrent Neural Network Models for Forecasting and Quantifying\n  Uncertainty in Spatial-Temporal Data", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recurrent neural networks (RNNs) are nonlinear dynamical models commonly used\nin the machine learning and dynamical systems literature to represent complex\ndynamical or sequential relationships between variables. More recently, as deep\nlearning models have become more common, RNNs have been used to forecast\nincreasingly complicated systems. Dynamical spatio-temporal processes represent\na class of complex systems that can potentially benefit from these types of\nmodels. Although the RNN literature is expansive and highly developed,\nuncertainty quantification is often ignored. Even when considered, the\nuncertainty is generally quantified without the use of a rigorous framework,\nsuch as a fully Bayesian setting. Here we attempt to quantify uncertainty in a\nmore formal framework while maintaining the forecast accuracy that makes these\nmodels appealing, by presenting a Bayesian RNN model for nonlinear\nspatio-temporal forecasting. Additionally, we make simple modifications to the\nbasic RNN to help accommodate the unique nature of nonlinear spatio-temporal\ndata. The proposed model is applied to a Lorenz simulation and two real-world\nnonlinear spatio-temporal forecasting applications.\n", "versions": [{"version": "v1", "created": "Thu, 2 Nov 2017 07:27:19 GMT"}, {"version": "v2", "created": "Wed, 7 Feb 2018 02:30:21 GMT"}], "update_date": "2018-02-08", "authors_parsed": [["McDermott", "Patrick L.", ""], ["Wikle", "Christopher K.", ""]]}, {"id": "1711.00658", "submitter": "Lei Han", "authors": "Lei Han, Yiheng Huang and Tong Zhang", "title": "Candidates vs. Noises Estimation for Large Multi-Class Classification\n  Problem", "comments": "Published in ICML 2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper proposes a method for multi-class classification problems, where\nthe number of classes K is large. The method, referred to as Candidates vs.\nNoises Estimation (CANE), selects a small subset of candidate classes and\nsamples the remaining classes. We show that CANE is always consistent and\ncomputationally efficient. Moreover, the resulting estimator has low\nstatistical variance approaching that of the maximum likelihood estimator, when\nthe observed label belongs to the selected candidates with high probability. In\npractice, we use a tree structure with leaves as classes to promote fast beam\nsearch for candidate selection. We further apply the CANE method to estimate\nword probabilities in learning large neural language models. Extensive\nexperimental results show that CANE achieves better prediction accuracy over\nthe Noise-Contrastive Estimation (NCE), its variants and a number of the\nstate-of-the-art tree classifiers, while it gains significant speedup compared\nto standard O(K) methods.\n", "versions": [{"version": "v1", "created": "Thu, 2 Nov 2017 09:23:11 GMT"}, {"version": "v2", "created": "Thu, 13 Sep 2018 04:16:06 GMT"}], "update_date": "2018-09-14", "authors_parsed": [["Han", "Lei", ""], ["Huang", "Yiheng", ""], ["Zhang", "Tong", ""]]}, {"id": "1711.00659", "submitter": "Alain Rakotomamonjy", "authors": "Rafael Will M de Araujo (USP), Roberto Hirata (USP), Alain\n  Rakotomamonjy (LITIS)", "title": "Concave losses for robust dictionary learning", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Traditional dictionary learning methods are based on quadratic convex loss\nfunction and thus are sensitive to outliers. In this paper, we propose a\ngeneric framework for robust dictionary learning based on concave losses. We\nprovide results on composition of concave functions, notably regarding\nsuper-gradient computations, that are key for developing generic dictionary\nlearning algorithms applicable to smooth and non-smooth losses. In order to\nimprove identification of outliers, we introduce an initialization heuristic\nbased on undercomplete dictionary learning. Experimental results using\nsynthetic and real data demonstrate that our method is able to better detect\noutliers, is capable of generating better dictionaries, outperforming\nstate-of-the-art methods such as K-SVD and LC-KSVD.\n", "versions": [{"version": "v1", "created": "Thu, 2 Nov 2017 09:27:58 GMT"}], "update_date": "2017-11-03", "authors_parsed": [["de Araujo", "Rafael Will M", "", "USP"], ["Hirata", "Roberto", "", "USP"], ["Rakotomamonjy", "Alain", "", "LITIS"]]}, {"id": "1711.00673", "submitter": "Binxin Ru", "authors": "Binxin Ru, Mark McLeod, Diego Granziol, Michael A. Osborne", "title": "Fast Information-theoretic Bayesian Optimisation", "comments": "Main Paper: 9 pages, 6 figures, 2 tables; Accepted by ICML 2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Information-theoretic Bayesian optimisation techniques have demonstrated\nstate-of-the-art performance in tackling important global optimisation\nproblems. However, current information-theoretic approaches require many\napproximations in implementation, introduce often-prohibitive computational\noverhead and limit the choice of kernels available to model the objective. We\ndevelop a fast information-theoretic Bayesian Optimisation method, FITBO, that\navoids the need for sampling the global minimiser, thus significantly reducing\ncomputational overhead. Moreover, in comparison with existing approaches, our\nmethod faces fewer constraints on kernel choice and enjoys the merits of\ndealing with the output space. We demonstrate empirically that FITBO inherits\nthe performance associated with information-theoretic Bayesian optimisation,\nwhile being even faster than simpler Bayesian optimisation approaches, such as\nExpected Improvement.\n", "versions": [{"version": "v1", "created": "Thu, 2 Nov 2017 10:09:09 GMT"}, {"version": "v2", "created": "Wed, 8 Nov 2017 14:57:57 GMT"}, {"version": "v3", "created": "Sun, 26 Nov 2017 18:15:23 GMT"}, {"version": "v4", "created": "Mon, 12 Mar 2018 21:10:07 GMT"}, {"version": "v5", "created": "Wed, 6 Jun 2018 16:18:12 GMT"}], "update_date": "2018-06-07", "authors_parsed": [["Ru", "Binxin", ""], ["McLeod", "Mark", ""], ["Granziol", "Diego", ""], ["Osborne", "Michael A.", ""]]}, {"id": "1711.00695", "submitter": "Iliyan Zarov", "authors": "Laura Douglas, Iliyan Zarov, Konstantinos Gourgoulias, Chris Lucas,\n  Chris Hart, Adam Baker, Maneesh Sahani, Yura Perov, Saurabh Johri", "title": "A Universal Marginalizer for Amortized Inference in Generative Models", "comments": "Submitted to the NIPS 2017 Workshop on Advances in Approximate\n  Bayesian Inference", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider the problem of inference in a causal generative model where the\nset of available observations differs between data instances. We show how\ncombining samples drawn from the graphical model with an appropriate masking\nfunction makes it possible to train a single neural network to approximate all\nthe corresponding conditional marginal distributions and thus amortize the cost\nof inference. We further demonstrate that the efficiency of importance sampling\nmay be improved by basing proposals on the output of the neural network. We\nalso outline how the same network can be used to generate samples from an\napproximate joint posterior via a chain decomposition of the graph.\n", "versions": [{"version": "v1", "created": "Thu, 2 Nov 2017 11:40:09 GMT"}], "update_date": "2017-11-03", "authors_parsed": [["Douglas", "Laura", ""], ["Zarov", "Iliyan", ""], ["Gourgoulias", "Konstantinos", ""], ["Lucas", "Chris", ""], ["Hart", "Chris", ""], ["Baker", "Adam", ""], ["Sahani", "Maneesh", ""], ["Perov", "Yura", ""], ["Johri", "Saurabh", ""]]}, {"id": "1711.00721", "submitter": "Nikola Markovic", "authors": "Przemys{\\l}aw Seku{\\l}a, Nikola Markovi\\'c, Zachary Vander Laan, Kaveh\n  Farokhi Sadabadi", "title": "Estimating Historical Hourly Traffic Volumes via Machine Learning and\n  Vehicle Probe Data: A Maryland Case Study", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.CY", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper focuses on the problem of estimating historical traffic volumes\nbetween sparsely-located traffic sensors, which transportation agencies need to\naccurately compute statewide performance measures. To this end, the paper\nexamines applications of vehicle probe data, automatic traffic recorder counts,\nand neural network models to estimate hourly volumes in the Maryland highway\nnetwork, and proposes a novel approach that combines neural networks with an\nexisting profiling method. On average, the proposed approach yields 24% more\naccurate estimates than volume profiles, which are currently used by\ntransportation agencies across the US to compute statewide performance\nmeasures. The paper also quantifies the value of using vehicle probe data in\nestimating hourly traffic volumes, which provides important managerial insights\nto transportation agencies interested in acquiring this type of data. For\nexample, results show that volumes can be estimated with a mean absolute\npercent error of about 21% at locations where average number of observed probes\nis between 30 and 47 vehicles/hr, which provides a useful guideline for\nassessing the value of probe vehicle data from different vendors.\n", "versions": [{"version": "v1", "created": "Thu, 2 Nov 2017 13:14:08 GMT"}, {"version": "v2", "created": "Wed, 17 Oct 2018 20:07:47 GMT"}], "update_date": "2018-10-19", "authors_parsed": [["Seku\u0142a", "Przemys\u0142aw", ""], ["Markovi\u0107", "Nikola", ""], ["Laan", "Zachary Vander", ""], ["Sadabadi", "Kaveh Farokhi", ""]]}, {"id": "1711.00753", "submitter": "Kristin Branson", "authors": "Mayank Kabra and Kristin Branson", "title": "Network-size independent covering number bounds for deep networks", "comments": "We found a possible error in our analysis. We are re-evaluating, and\n  may resubmit", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We give a covering number bound for deep learning networks that is\nindependent of the size of the network. The key for the simple analysis is that\nfor linear classifiers, rotating the data doesn't affect the covering number.\nThus, we can ignore the rotation part of each layer's linear transformation,\nand get the covering number bound by concentrating on the scaling part.\n", "versions": [{"version": "v1", "created": "Thu, 2 Nov 2017 14:18:19 GMT"}, {"version": "v2", "created": "Wed, 8 Nov 2017 19:00:05 GMT"}], "update_date": "2017-11-10", "authors_parsed": [["Kabra", "Mayank", ""], ["Branson", "Kristin", ""]]}, {"id": "1711.00765", "submitter": "Barak Sober", "authors": "Barak Sober, Yariv Aizenbud, David Levin", "title": "Approximation of Functions over Manifolds: A Moving Least-Squares\n  Approach", "comments": "arXiv admin note: text overlap with arXiv:1606.07104", "journal-ref": null, "doi": "10.1016/j.cam.2020.113140", "report-no": null, "categories": "stat.ML cs.CG cs.GR cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present an algorithm for approximating a function defined over a\n$d$-dimensional manifold utilizing only noisy function values at locations\nsampled from the manifold with noise. To produce the approximation we do not\nrequire any knowledge regarding the manifold other than its dimension $d$. We\nuse the Manifold Moving Least-Squares approach of (Sober and Levin 2016) to\nreconstruct the atlas of charts and the approximation is built on-top of those\ncharts. The resulting approximant is shown to be a function defined over a\nneighborhood of a manifold, approximating the originally sampled manifold. In\nother words, given a new point, located near the manifold, the approximation\ncan be evaluated directly on that point. We prove that our construction yields\na smooth function, and in case of noiseless samples the approximation order is\n$\\mathcal{O}(h^{m+1})$, where $h$ is a local density of sample parameter (i.e.,\nthe fill distance) and $m$ is the degree of a local polynomial approximation,\nused in our algorithm. In addition, the proposed algorithm has linear time\ncomplexity with respect to the ambient-space's dimension. Thus, we are able to\navoid the computational complexity, commonly encountered in high dimensional\napproximations, without having to perform non-linear dimension reduction, which\ninevitably introduces distortions to the geometry of the data. Additionaly, we\nshow numerical experiments that the proposed approach compares favorably to\nstatistical approaches for regression over manifolds and show its potential.\n", "versions": [{"version": "v1", "created": "Thu, 2 Nov 2017 14:37:04 GMT"}, {"version": "v2", "created": "Tue, 23 Jan 2018 07:49:10 GMT"}, {"version": "v3", "created": "Wed, 8 May 2019 00:05:59 GMT"}, {"version": "v4", "created": "Fri, 17 Jan 2020 02:15:41 GMT"}], "update_date": "2020-08-13", "authors_parsed": [["Sober", "Barak", ""], ["Aizenbud", "Yariv", ""], ["Levin", "David", ""]]}, {"id": "1711.00789", "submitter": "Li Ma", "authors": "Meng Li and Li Ma", "title": "Learning Asymmetric and Local Features in Multi-Dimensional Data through\n  Wavelets with Recursive Partitioning", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.CO stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Effective learning of asymmetric and local features in images and other data\nobserved on multi-dimensional grids is a challenging objective critical for a\nwide range of image processing applications involving biomedical and natural\nimages. It requires methods that are sensitive to local details while fast\nenough to handle massive numbers of images of ever increasing sizes. We\nintroduce a probabilistic model-based framework that achieves these objectives\nby incorporating adaptivity into discrete wavelet transforms (DWT) through\nBayesian hierarchical modeling, thereby allowing wavelet bases to adapt to the\ngeometric structure of the data while maintaining the high computational\nscalability of wavelet methods---linear in the sample size (e.g., the\nresolution of an image). We derive a recursive representation of the Bayesian\nposterior model which leads to an exact message passing algorithm to complete\nlearning and inference. While our framework is applicable to a range of\nproblems including multi-dimensional signal processing, compression, and\nstructural learning, we illustrate its work and evaluate its performance in the\ncontext of image reconstruction using real images from the ImageNet database,\ntwo widely used benchmark datasets, and a dataset from retinal optical\ncoherence tomography and compare its performance to state-of-the-art methods\nbased on basis transforms and deep learning.\n", "versions": [{"version": "v1", "created": "Thu, 2 Nov 2017 15:51:16 GMT"}, {"version": "v2", "created": "Mon, 18 Jun 2018 12:22:56 GMT"}, {"version": "v3", "created": "Tue, 10 Jul 2018 02:05:25 GMT"}, {"version": "v4", "created": "Mon, 29 Apr 2019 22:53:35 GMT"}, {"version": "v5", "created": "Fri, 6 Nov 2020 19:23:52 GMT"}], "update_date": "2020-11-10", "authors_parsed": [["Li", "Meng", ""], ["Ma", "Li", ""]]}, {"id": "1711.00799", "submitter": "Roman F\\\"oll", "authors": "Roman F\\\"oll, Bernard Haasdonk, Markus Hanselmann, Holger Ulmer", "title": "Deep Recurrent Gaussian Process with Variational Sparse Spectrum\n  Approximation", "comments": "51 pages, 7 figures, preprint", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Modeling sequential data has become more and more important in practice. Some\napplications are autonomous driving, virtual sensors and weather forecasting.\nTo model such systems so called recurrent models are used. In this article we\nintroduce two new Deep Recurrent Gaussian Process (DRGP) models based on the\nSparse Spectrum Gaussian Process (SSGP) and the improved variational version\ncalled Variational Sparse Spectrum Gaussian Process (VSSGP). We follow the\nrecurrent structure given by an existing DRGP based on a specific sparse\nNystr\\\"om approximation. Therefore, we also variationally integrate out the\ninput-space and hence can propagate uncertainty through the layers. We can show\nthat for the resulting lower bound an optimal variational distribution exists.\nTraining is realized through optimizing the variational lower bound. Using\nDistributed Variational Inference (DVI), we can reduce the computational\ncomplexity. We improve over current state of the art methods in prediction\naccuracy for experimental data-sets used for their evaluation and introduce a\nnew data-set for engine control, named Emission. Furthermore, our method can\neasily be adapted for unsupervised learning, e.g. the latent variable model and\nits deep version.\n", "versions": [{"version": "v1", "created": "Thu, 2 Nov 2017 16:18:43 GMT"}, {"version": "v2", "created": "Sun, 19 Nov 2017 20:30:37 GMT"}], "update_date": "2017-11-21", "authors_parsed": [["F\u00f6ll", "Roman", ""], ["Haasdonk", "Bernard", ""], ["Hanselmann", "Markus", ""], ["Ulmer", "Holger", ""]]}, {"id": "1711.00817", "submitter": "Martin Zhang", "authors": "Vivek Bagaria, Govinda M. Kamath, Vasilis Ntranos, Martin J. Zhang,\n  and David Tse", "title": "Medoids in almost linear time via multi-armed bandits", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.DS cs.IT cs.LG math.IT", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Computing the medoid of a large number of points in high-dimensional space is\nan increasingly common operation in many data science problems. We present an\nalgorithm Med-dit which uses O(n log n) distance evaluations to compute the\nmedoid with high probability. Med-dit is based on a connection with the\nmulti-armed bandit problem. We evaluate the performance of Med-dit empirically\non the Netflix-prize and the single-cell RNA-Seq datasets, containing hundreds\nof thousands of points living in tens of thousands of dimensions, and observe a\n5-10x improvement in performance over the current state of the art. Med-dit is\navailable at https://github.com/bagavi/Meddit\n", "versions": [{"version": "v1", "created": "Thu, 2 Nov 2017 17:00:05 GMT"}, {"version": "v2", "created": "Sun, 5 Nov 2017 01:58:25 GMT"}, {"version": "v3", "created": "Tue, 7 Nov 2017 07:15:42 GMT"}], "update_date": "2017-11-08", "authors_parsed": [["Bagaria", "Vivek", ""], ["Kamath", "Govinda M.", ""], ["Ntranos", "Vasilis", ""], ["Zhang", "Martin J.", ""], ["Tse", "David", ""]]}, {"id": "1711.00837", "submitter": "Felix Last", "authors": "Felix Last, Georgios Douzas, Fernando Bacao", "title": "Oversampling for Imbalanced Learning Based on K-Means and SMOTE", "comments": "19 pages, 8 figures", "journal-ref": "Information Sciences 465 (2018) 1-20", "doi": "10.1016/j.ins.2018.06.056", "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Learning from class-imbalanced data continues to be a common and challenging\nproblem in supervised learning as standard classification algorithms are\ndesigned to handle balanced class distributions. While different strategies\nexist to tackle this problem, methods which generate artificial data to achieve\na balanced class distribution are more versatile than modifications to the\nclassification algorithm. Such techniques, called oversamplers, modify the\ntraining data, allowing any classifier to be used with class-imbalanced\ndatasets. Many algorithms have been proposed for this task, but most are\ncomplex and tend to generate unnecessary noise. This work presents a simple and\neffective oversampling method based on k-means clustering and SMOTE\noversampling, which avoids the generation of noise and effectively overcomes\nimbalances between and within classes. Empirical results of extensive\nexperiments with 71 datasets show that training data oversampled with the\nproposed method improves classification results. Moreover, k-means SMOTE\nconsistently outperforms other popular oversampling methods. An implementation\nis made available in the python programming language.\n", "versions": [{"version": "v1", "created": "Thu, 2 Nov 2017 17:43:03 GMT"}, {"version": "v2", "created": "Tue, 12 Dec 2017 18:33:14 GMT"}], "update_date": "2020-03-06", "authors_parsed": [["Last", "Felix", ""], ["Douzas", "Georgios", ""], ["Bacao", "Fernando", ""]]}, {"id": "1711.00843", "submitter": "Mike Ludkovski", "authors": "Sergio Rodriguez and Michael Ludkovski", "title": "Generalized Probabilistic Bisection for Stochastic Root-Finding", "comments": "27 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider numerical schemes for root finding of noisy responses through\ngeneralizing the Probabilistic Bisection Algorithm (PBA) to the more practical\ncontext where the sampling distribution is unknown and location-dependent. As\nin standard PBA, we rely on a knowledge state for the approximate posterior of\nthe root location. To implement the corresponding Bayesian updating, we also\ncarry out inference of oracle accuracy, namely learning the probability of\ncorrect response. To this end we utilize batched querying in combination with a\nvariety of frequentist and Bayesian estimators based on majority vote, as well\nas the underlying functional responses, if available. For guiding sampling\nselection we investigate both Information Directed sampling, as well as\nQuantile sampling. Our numerical experiments show that these strategies perform\nquite differently; in particular we demonstrate the efficiency of randomized\nquantile sampling which is reminiscent of Thompson sampling. Our work is\nmotivated by the root-finding sub-routine in pricing of Bermudan financial\nderivatives, illustrated in the last section of the paper.\n", "versions": [{"version": "v1", "created": "Thu, 2 Nov 2017 17:55:48 GMT"}], "update_date": "2017-11-03", "authors_parsed": [["Rodriguez", "Sergio", ""], ["Ludkovski", "Michael", ""]]}, {"id": "1711.00848", "submitter": "Abhishek Kumar", "authors": "Abhishek Kumar, Prasanna Sattigeri, Avinash Balakrishnan", "title": "Variational Inference of Disentangled Latent Concepts from Unlabeled\n  Observations", "comments": "ICLR 2018 Version", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI cs.CV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Disentangled representations, where the higher level data generative factors\nare reflected in disjoint latent dimensions, offer several benefits such as\nease of deriving invariant representations, transferability to other tasks,\ninterpretability, etc. We consider the problem of unsupervised learning of\ndisentangled representations from large pool of unlabeled observations, and\npropose a variational inference based approach to infer disentangled latent\nfactors. We introduce a regularizer on the expectation of the approximate\nposterior over observed data that encourages the disentanglement. We also\npropose a new disentanglement metric which is better aligned with the\nqualitative disentanglement observed in the decoder's output. We empirically\nobserve significant improvement over existing methods in terms of both\ndisentanglement and data likelihood (reconstruction quality).\n", "versions": [{"version": "v1", "created": "Thu, 2 Nov 2017 17:57:43 GMT"}, {"version": "v2", "created": "Tue, 7 Nov 2017 21:29:36 GMT"}, {"version": "v3", "created": "Thu, 27 Dec 2018 19:25:22 GMT"}], "update_date": "2018-12-31", "authors_parsed": [["Kumar", "Abhishek", ""], ["Sattigeri", "Prasanna", ""], ["Balakrishnan", "Avinash", ""]]}, {"id": "1711.00867", "submitter": "Pieter-Jan Kindermans", "authors": "Pieter-Jan Kindermans, Sara Hooker, Julius Adebayo, Maximilian Alber,\n  Kristof T. Sch\\\"utt, Sven D\\\"ahne, Dumitru Erhan, Been Kim", "title": "The (Un)reliability of saliency methods", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Saliency methods aim to explain the predictions of deep neural networks.\nThese methods lack reliability when the explanation is sensitive to factors\nthat do not contribute to the model prediction. We use a simple and common\npre-processing step ---adding a constant shift to the input data--- to show\nthat a transformation with no effect on the model can cause numerous methods to\nincorrectly attribute. In order to guarantee reliability, we posit that methods\nshould fulfill input invariance, the requirement that a saliency method mirror\nthe sensitivity of the model with respect to transformations of the input. We\nshow, through several examples, that saliency methods that do not satisfy input\ninvariance result in misleading attribution.\n", "versions": [{"version": "v1", "created": "Thu, 2 Nov 2017 18:01:30 GMT"}], "update_date": "2017-11-06", "authors_parsed": [["Kindermans", "Pieter-Jan", ""], ["Hooker", "Sara", ""], ["Adebayo", "Julius", ""], ["Alber", "Maximilian", ""], ["Sch\u00fctt", "Kristof T.", ""], ["D\u00e4hne", "Sven", ""], ["Erhan", "Dumitru", ""], ["Kim", "Been", ""]]}, {"id": "1711.00882", "submitter": "Gil Tabak", "authors": "Gil Tabak, Minjie Fan, Samuel J. Yang, Stephan Hoyer, Geoff Davis", "title": "Correcting Nuisance Variation using Wasserstein Distance", "comments": "21 pages, 20 figures, 6 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Profiling cellular phenotypes from microscopic imaging can provide meaningful\nbiological information resulting from various factors affecting the cells. One\nmotivating application is drug development: morphological cell features can be\ncaptured from images, from which similarities between different drug compounds\napplied at different doses can be quantified. The general approach is to find a\nfunction mapping the images to an embedding space of manageable dimensionality\nwhose geometry captures relevant features of the input images. An important\nknown issue for such methods is separating relevant biological signal from\nnuisance variation. For example, the embedding vectors tend to be more\ncorrelated for cells that were cultured and imaged during the same week than\nfor those from different weeks, despite having identical drug compounds applied\nin both cases. In this case, the particular batch in which a set of experiments\nwere conducted constitutes the domain of the data; an ideal set of image\nembeddings should contain only the relevant biological information (e.g. drug\neffects). We develop a general framework for adjusting the image embeddings in\norder to `forget' domain-specific information while preserving relevant\nbiological information. To achieve this, we minimize a loss function based on\ndistances between marginal distributions (such as the Wasserstein distance) of\nembeddings across domains for each replicated treatment. For the dataset we\npresent results with, the only replicated treatment happens to be the negative\ncontrol treatment, for which we do not expect any treatment-induced cell\nmorphology changes. We find that for our transformed embeddings (i) the\nunderlying geometric structure is not only preserved but the embeddings also\ncarry improved biological signal; and (ii) less domain-specific information is\npresent.\n", "versions": [{"version": "v1", "created": "Thu, 2 Nov 2017 18:42:40 GMT"}, {"version": "v2", "created": "Mon, 17 Jun 2019 18:51:20 GMT"}], "update_date": "2019-06-19", "authors_parsed": [["Tabak", "Gil", ""], ["Fan", "Minjie", ""], ["Yang", "Samuel J.", ""], ["Hoyer", "Stephan", ""], ["Davis", "Geoff", ""]]}, {"id": "1711.00905", "submitter": "Xuehang Zheng", "authors": "Xuehang Zheng, Il Yong Chun, Zhipeng Li, Yong Long, Jeffrey A. Fessler", "title": "Sparse-View X-Ray CT Reconstruction Using $\\ell_1$ Prior with Learned\n  Transform", "comments": "The first two authors contributed equally to this work", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG physics.med-ph", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A major challenge in X-ray computed tomography (CT) is reducing radiation\ndose while maintaining high quality of reconstructed images. To reduce the\nradiation dose, one can reduce the number of projection views (sparse-view CT);\nhowever, it becomes difficult to achieve high-quality image reconstruction as\nthe number of projection views decreases. Researchers have applied the concept\nof learning sparse representations from (high-quality) CT image dataset to the\nsparse-view CT reconstruction. We propose a new statistical CT reconstruction\nmodel that combines penalized weighted-least squares (PWLS) and $\\ell_1$ prior\nwith learned sparsifying transform (PWLS-ST-$\\ell_1$), and a corresponding\nefficient algorithm based on Alternating Direction Method of Multipliers\n(ADMM). To moderate the difficulty of tuning ADMM parameters, we propose a new\nADMM parameter selection scheme based on approximated condition numbers. We\ninterpret the proposed model by analyzing the minimum mean square error of its\n($\\ell_2$-norm relaxed) image update estimator. Our results with the extended\ncardiac-torso (XCAT) phantom data and clinical chest data show that, for\nsparse-view 2D fan-beam CT and 3D axial cone-beam CT, PWLS-ST-$\\ell_1$ improves\nthe quality of reconstructed images compared to the CT reconstruction methods\nusing edge-preserving regularizer and $\\ell_2$ prior with learned ST. These\nresults also show that, for sparse-view 2D fan-beam CT, PWLS-ST-$\\ell_1$\nachieves comparable or better image quality and requires much shorter runtime\nthan PWLS-DL using a learned overcomplete dictionary. Our results with clinical\nchest data show that, methods using the unsupervised learned prior generalize\nbetter than a state-of-the-art deep \"denoising\" neural network that does not\nuse a physical imaging model.\n", "versions": [{"version": "v1", "created": "Thu, 2 Nov 2017 19:46:45 GMT"}, {"version": "v2", "created": "Sat, 23 Feb 2019 02:54:07 GMT"}, {"version": "v3", "created": "Mon, 16 Sep 2019 02:25:53 GMT"}], "update_date": "2019-09-17", "authors_parsed": [["Zheng", "Xuehang", ""], ["Chun", "Il Yong", ""], ["Li", "Zhipeng", ""], ["Long", "Yong", ""], ["Fessler", "Jeffrey A.", ""]]}, {"id": "1711.00922", "submitter": "Ari Pakman", "authors": "Ari Pakman", "title": "Binary Bouncy Particle Sampler", "comments": "4 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.CO stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The Bouncy Particle Sampler is a novel rejection-free non-reversible sampler\nfor differentiable probability distributions over continuous variables. We\ngeneralize the algorithm to piecewise differentiable distributions and apply it\nto generic binary distributions using a piecewise differentiable augmentation.\nWe illustrate the new algorithm in a binary Markov Random Field example, and\ncompare it to binary Hamiltonian Monte Carlo. Our results suggest that binary\nBPS samplers are better for easy to mix distributions.\n", "versions": [{"version": "v1", "created": "Thu, 2 Nov 2017 20:36:43 GMT"}], "update_date": "2017-11-06", "authors_parsed": [["Pakman", "Ari", ""]]}, {"id": "1711.00946", "submitter": "Cyril Zhang", "authors": "Elad Hazan, Karan Singh, Cyril Zhang", "title": "Learning Linear Dynamical Systems via Spectral Filtering", "comments": "Published as a conference paper at NIPS 2017", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.SY math.OC stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present an efficient and practical algorithm for the online prediction of\ndiscrete-time linear dynamical systems with a symmetric transition matrix. We\ncircumvent the non-convex optimization problem using improper learning:\ncarefully overparameterize the class of LDSs by a polylogarithmic factor, in\nexchange for convexity of the loss functions. From this arises a\npolynomial-time algorithm with a near-optimal regret guarantee, with an\nanalogous sample complexity bound for agnostic learning. Our algorithm is based\non a novel filtering technique, which may be of independent interest: we\nconvolve the time series with the eigenvectors of a certain Hankel matrix.\n", "versions": [{"version": "v1", "created": "Thu, 2 Nov 2017 21:30:00 GMT"}], "update_date": "2017-11-08", "authors_parsed": [["Hazan", "Elad", ""], ["Singh", "Karan", ""], ["Zhang", "Cyril", ""]]}, {"id": "1711.00950", "submitter": "Rebecca E. Morrison", "authors": "Rebecca E. Morrison, Ricardo Baptista, Youssef Marzouk", "title": "Beyond normality: Learning sparse probabilistic graphical models in the\n  non-Gaussian setting", "comments": "Accepted in NIPS 2017", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present an algorithm to identify sparse dependence structure in continuous\nand non-Gaussian probability distributions, given a corresponding set of data.\nThe conditional independence structure of an arbitrary distribution can be\nrepresented as an undirected graph (or Markov random field), but most\nalgorithms for learning this structure are restricted to the discrete or\nGaussian cases. Our new approach allows for more realistic and accurate\ndescriptions of the distribution in question, and in turn better estimates of\nits sparse Markov structure. Sparsity in the graph is of interest as it can\naccelerate inference, improve sampling methods, and reveal important\ndependencies between variables. The algorithm relies on exploiting the\nconnection between the sparsity of the graph and the sparsity of transport\nmaps, which deterministically couple one probability measure to another.\n", "versions": [{"version": "v1", "created": "Thu, 2 Nov 2017 21:45:07 GMT"}, {"version": "v2", "created": "Mon, 6 Nov 2017 16:30:25 GMT"}], "update_date": "2017-11-07", "authors_parsed": [["Morrison", "Rebecca E.", ""], ["Baptista", "Ricardo", ""], ["Marzouk", "Youssef", ""]]}, {"id": "1711.00970", "submitter": "Shibani Santurkar", "authors": "Shibani Santurkar, Ludwig Schmidt and Aleksander M\\k{a}dry", "title": "A Classification-Based Study of Covariate Shift in GAN Distributions", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV cs.NE stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A basic, and still largely unanswered, question in the context of Generative\nAdversarial Networks (GANs) is whether they are truly able to capture all the\nfundamental characteristics of the distributions they are trained on. In\nparticular, evaluating the diversity of GAN distributions is challenging and\nexisting methods provide only a partial understanding of this issue. In this\npaper, we develop quantitative and scalable tools for assessing the diversity\nof GAN distributions. Specifically, we take a classification-based perspective\nand view loss of diversity as a form of covariate shift introduced by GANs. We\nexamine two specific forms of such shift: mode collapse and boundary\ndistortion. In contrast to prior work, our methods need only minimal human\nsupervision and can be readily applied to state-of-the-art GANs on large,\ncanonical datasets. Examining popular GANs using our tools indicates that these\nGANs have significant problems in reproducing the more distributional\nproperties of their training dataset.\n", "versions": [{"version": "v1", "created": "Thu, 2 Nov 2017 23:13:39 GMT"}, {"version": "v2", "created": "Sat, 11 Nov 2017 14:41:30 GMT"}, {"version": "v3", "created": "Sun, 19 Nov 2017 16:46:20 GMT"}, {"version": "v4", "created": "Wed, 11 Apr 2018 02:07:59 GMT"}, {"version": "v5", "created": "Wed, 30 May 2018 03:23:47 GMT"}, {"version": "v6", "created": "Sat, 2 Jun 2018 03:49:25 GMT"}, {"version": "v7", "created": "Wed, 6 Jun 2018 02:51:11 GMT"}], "update_date": "2018-06-07", "authors_parsed": [["Santurkar", "Shibani", ""], ["Schmidt", "Ludwig", ""], ["M\u0105dry", "Aleksander", ""]]}, {"id": "1711.00982", "submitter": "Varun Kanade", "authors": "Cheng Li, Felix Wong, Zhenming Liu, Varun Kanade", "title": "From which world is your graph?", "comments": "To appear in NIPS 2017", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.SI stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Discovering statistical structure from links is a fundamental problem in the\nanalysis of social networks. Choosing a misspecified model, or equivalently, an\nincorrect inference algorithm will result in an invalid analysis or even\nfalsely uncover patterns that are in fact artifacts of the model. This work\nfocuses on unifying two of the most widely used link-formation models: the\nstochastic blockmodel (SBM) and the small world (or latent space) model (SWM).\nIntegrating techniques from kernel learning, spectral graph theory, and\nnonlinear dimensionality reduction, we develop the first statistically sound\npolynomial-time algorithm to discover latent patterns in sparse graphs for both\nmodels. When the network comes from an SBM, the algorithm outputs a block\nstructure. When it is from an SWM, the algorithm outputs estimates of each\nnode's latent position.\n", "versions": [{"version": "v1", "created": "Fri, 3 Nov 2017 00:24:39 GMT"}], "update_date": "2017-11-06", "authors_parsed": [["Li", "Cheng", ""], ["Wong", "Felix", ""], ["Liu", "Zhenming", ""], ["Kanade", "Varun", ""]]}, {"id": "1711.00987", "submitter": "Bin Hu", "authors": "Bin Hu, Peter Seiler, Laurent Lessard", "title": "Analysis of Biased Stochastic Gradient Descent Using Sequential\n  Semidefinite Programs", "comments": "Accepted to Mathematical Programming", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.OC stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a convergence rate analysis for biased stochastic gradient descent\n(SGD), where individual gradient updates are corrupted by computation errors.\nWe develop stochastic quadratic constraints to formulate a small linear matrix\ninequality (LMI) whose feasible points lead to convergence bounds of biased\nSGD. Based on this LMI condition, we develop a sequential minimization approach\nto analyze the intricate trade-offs that couple stepsize selection, convergence\nrate, optimization accuracy, and robustness to gradient inaccuracy. We also\nprovide feasible points for this LMI and obtain theoretical formulas that\nquantify the convergence properties of biased SGD under various assumptions on\nthe loss functions.\n", "versions": [{"version": "v1", "created": "Fri, 3 Nov 2017 00:59:03 GMT"}, {"version": "v2", "created": "Wed, 11 Mar 2020 22:17:36 GMT"}, {"version": "v3", "created": "Sat, 28 Mar 2020 16:09:56 GMT"}], "update_date": "2020-03-31", "authors_parsed": [["Hu", "Bin", ""], ["Seiler", "Peter", ""], ["Lessard", "Laurent", ""]]}, {"id": "1711.00991", "submitter": "Arash Ali Amini", "authors": "Arash A. Amini, Bryon Aragam, Qing Zhou", "title": "The neighborhood lattice for encoding partial correlations in a Hilbert\n  space", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST cs.LG stat.ML stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Neighborhood regression has been a successful approach in graphical and\nstructural equation modeling, with applications to learning undirected and\ndirected graphical models. We extend these ideas by defining and studying an\nalgebraic structure called the neighborhood lattice based on a generalized\nnotion of neighborhood regression. We show that this algebraic structure has\nthe potential to provide an economic encoding of all conditional independence\nstatements in a Gaussian distribution (or conditional uncorrelatedness in\ngeneral), even in the cases where no graphical model exists that could\n\"perfectly\" encode all such statements. We study the computational complexity\nof computing these structures and show that under a sparsity assumption, they\ncan be computed in polynomial time, even in the absence of the assumption of\nperfectness to a graph. On the other hand, assuming perfectness, we show how\nthese neighborhood lattices may be \"graphically\" computed using the separation\nproperties of the so-called partial correlation graph. We also draw connections\nwith directed acyclic graphical models and Bayesian networks. We derive these\nresults using an abstract generalization of partial uncorrelatedness, called\npartial orthogonality, which allows us to use algebraic properties of\nprojection operators on Hilbert spaces to significantly simplify and extend\nexisting ideas and arguments. Consequently, our results apply to a wide range\nof random objects and data structures, such as random vectors, data matrices,\nand functions.\n", "versions": [{"version": "v1", "created": "Fri, 3 Nov 2017 01:45:53 GMT"}, {"version": "v2", "created": "Wed, 6 Feb 2019 06:05:21 GMT"}], "update_date": "2019-02-07", "authors_parsed": [["Amini", "Arash A.", ""], ["Aragam", "Bryon", ""], ["Zhou", "Qing", ""]]}, {"id": "1711.01012", "submitter": "Tanmay Gangwani", "authors": "Tanmay Gangwani, Jian Peng", "title": "Policy Optimization by Genetic Distillation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Genetic algorithms have been widely used in many practical optimization\nproblems. Inspired by natural selection, operators, including mutation,\ncrossover and selection, provide effective heuristics for search and black-box\noptimization. However, they have not been shown useful for deep reinforcement\nlearning, possibly due to the catastrophic consequence of parameter crossovers\nof neural networks. Here, we present Genetic Policy Optimization (GPO), a new\ngenetic algorithm for sample-efficient deep policy optimization. GPO uses\nimitation learning for policy crossover in the state space and applies policy\ngradient methods for mutation. Our experiments on MuJoCo tasks show that GPO as\na genetic algorithm is able to provide superior performance over the\nstate-of-the-art policy gradient methods and achieves comparable or higher\nsample efficiency.\n", "versions": [{"version": "v1", "created": "Fri, 3 Nov 2017 03:29:32 GMT"}, {"version": "v2", "created": "Mon, 12 Mar 2018 20:18:21 GMT"}], "update_date": "2018-03-14", "authors_parsed": [["Gangwani", "Tanmay", ""], ["Peng", "Jian", ""]]}, {"id": "1711.01131", "submitter": "Vincent Adam", "authors": "Vincent Adam", "title": "Structured Variational Inference for Coupled Gaussian Processes", "comments": "* Updated references. * Corrected typos", "journal-ref": "Advances in Approximate Bayesian Inference, NIPS 2017 Workshop", "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Sparse variational approximations allow for principled and scalable inference\nin Gaussian Process (GP) models. In settings where several GPs are part of the\ngenerative model, theses GPs are a posteriori coupled. For many applications\nsuch as regression where predictive accuracy is the quantity of interest, this\ncoupling is not crucial. Howewer if one is interested in posterior uncertainty,\nit cannot be ignored. A key element of variational inference schemes is the\nchoice of the approximate posterior parameterization. When the number of latent\nvariables is large, mean field (MF) methods provide fast and accurate posterior\nmeans while more structured posterior lead to inference algorithm of greater\ncomputational complexity. Here, we extend previous sparse GP approximations and\npropose a novel parameterization of variational posteriors in the multi-GP\nsetting allowing for fast and scalable inference capturing posterior\ndependencies.\n", "versions": [{"version": "v1", "created": "Fri, 3 Nov 2017 12:44:50 GMT"}, {"version": "v2", "created": "Wed, 29 Nov 2017 12:13:54 GMT"}], "update_date": "2017-11-30", "authors_parsed": [["Adam", "Vincent", ""]]}, {"id": "1711.01134", "submitter": "Finale Doshi-Velez", "authors": "Finale Doshi-Velez, Mason Kortz, Ryan Budish, Chris Bavitz, Sam\n  Gershman, David O'Brien, Kate Scott, Stuart Schieber, James Waldo, David\n  Weinberger, Adrian Weller, and Alexandra Wood", "title": "Accountability of AI Under the Law: The Role of Explanation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The ubiquity of systems using artificial intelligence or \"AI\" has brought\nincreasing attention to how those systems should be regulated. The choice of\nhow to regulate AI systems will require care. AI systems have the potential to\nsynthesize large amounts of data, allowing for greater levels of\npersonalization and precision than ever before---applications range from\nclinical decision support to autonomous driving and predictive policing. That\nsaid, there exist legitimate concerns about the intentional and unintentional\nnegative consequences of AI systems. There are many ways to hold AI systems\naccountable. In this work, we focus on one: explanation. Questions about a\nlegal right to explanation from AI systems was recently debated in the EU\nGeneral Data Protection Regulation, and thus thinking carefully about when and\nhow explanation from AI systems might improve accountability is timely. In this\nwork, we review contexts in which explanation is currently required under the\nlaw, and then list the technical considerations that must be considered if we\ndesired AI systems that could provide kinds of explanations that are currently\nrequired of humans.\n", "versions": [{"version": "v1", "created": "Fri, 3 Nov 2017 12:54:51 GMT"}, {"version": "v2", "created": "Tue, 21 Nov 2017 02:06:17 GMT"}, {"version": "v3", "created": "Fri, 20 Dec 2019 20:03:56 GMT"}], "update_date": "2019-12-24", "authors_parsed": [["Doshi-Velez", "Finale", ""], ["Kortz", "Mason", ""], ["Budish", "Ryan", ""], ["Bavitz", "Chris", ""], ["Gershman", "Sam", ""], ["O'Brien", "David", ""], ["Scott", "Kate", ""], ["Schieber", "Stuart", ""], ["Waldo", "James", ""], ["Weinberger", "David", ""], ["Weller", "Adrian", ""], ["Wood", "Alexandra", ""]]}, {"id": "1711.01191", "submitter": "Addison Bohannon", "authors": "Addison Bohannon, Brian Sadler, Radu Balan", "title": "Learning flexible representations of stochastic processes on graphs", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.SP stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Graph convolutional networks adapt the architecture of convolutional neural\nnetworks to learn rich representations of data supported on arbitrary graphs by\nreplacing the convolution operations of convolutional neural networks with\ngraph-dependent linear operations. However, these graph-dependent linear\noperations are developed for scalar functions supported on undirected graphs.\nWe propose a class of linear operations for stochastic (time-varying) processes\non directed (or undirected) graphs to be used in graph convolutional networks.\nWe propose a parameterization of such linear operations using functional\ncalculus to achieve arbitrarily low learning complexity. The proposed approach\nis shown to model richer behaviors and display greater flexibility in learning\nrepresentations than product graph methods.\n", "versions": [{"version": "v1", "created": "Fri, 3 Nov 2017 14:45:50 GMT"}, {"version": "v2", "created": "Tue, 13 Mar 2018 14:09:56 GMT"}], "update_date": "2018-03-14", "authors_parsed": [["Bohannon", "Addison", ""], ["Sadler", "Brian", ""], ["Balan", "Radu", ""]]}, {"id": "1711.01204", "submitter": "Nutan Chen", "authors": "Nutan Chen, Alexej Klushyn, Richard Kurle, Xueyan Jiang, Justin Bayer,\n  Patrick van der Smagt", "title": "Metrics for Deep Generative Models", "comments": "Published on the 21st International Conference on Artificial\n  Intelligence and Statistics (AISTATS), 2018", "journal-ref": "The 21st International Conference on Artificial Intelligence and\n  Statistics, 2018", "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Neural samplers such as variational autoencoders (VAEs) or generative\nadversarial networks (GANs) approximate distributions by transforming samples\nfrom a simple random source---the latent space---to samples from a more complex\ndistribution represented by a dataset. While the manifold hypothesis implies\nthat the density induced by a dataset contains large regions of low density,\nthe training criterions of VAEs and GANs will make the latent space densely\ncovered. Consequently points that are separated by low-density regions in\nobservation space will be pushed together in latent space, making stationary\ndistances poor proxies for similarity. We transfer ideas from Riemannian\ngeometry to this setting, letting the distance between two points be the\nshortest path on a Riemannian manifold induced by the transformation. The\nmethod yields a principled distance measure, provides a tool for visual\ninspection of deep generative models, and an alternative to linear\ninterpolation in latent space. In addition, it can be applied for robot\nmovement generalization using previously learned skills. The method is\nevaluated on a synthetic dataset with known ground truth; on a simulated robot\narm dataset; on human motion capture data; and on a generative model of\nhandwritten digits.\n", "versions": [{"version": "v1", "created": "Fri, 3 Nov 2017 15:18:10 GMT"}, {"version": "v2", "created": "Thu, 8 Feb 2018 14:53:43 GMT"}], "update_date": "2018-02-09", "authors_parsed": [["Chen", "Nutan", ""], ["Klushyn", "Alexej", ""], ["Kurle", "Richard", ""], ["Jiang", "Xueyan", ""], ["Bayer", "Justin", ""], ["van der Smagt", "Patrick", ""]]}, {"id": "1711.01244", "submitter": "Ron Amit", "authors": "Ron Amit and Ron Meir", "title": "Meta-Learning by Adjusting Priors Based on Extended PAC-Bayes Theory", "comments": "Accepted to ICML 2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.AI cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In meta-learning an agent extracts knowledge from observed tasks, aiming to\nfacilitate learning of novel future tasks. Under the assumption that future\ntasks are 'related' to previous tasks, the accumulated knowledge should be\nlearned in a way which captures the common structure across learned tasks,\nwhile allowing the learner sufficient flexibility to adapt to novel aspects of\nnew tasks. We present a framework for meta-learning that is based on\ngeneralization error bounds, allowing us to extend various PAC-Bayes bounds to\nmeta-learning. Learning takes place through the construction of a distribution\nover hypotheses based on the observed tasks, and its utilization for learning a\nnew task. Thus, prior knowledge is incorporated through setting an\nexperience-dependent prior for novel tasks. We develop a gradient-based\nalgorithm which minimizes an objective function derived from the bounds and\ndemonstrate its effectiveness numerically with deep neural networks. In\naddition to establishing the improved performance available through\nmeta-learning, we demonstrate the intuitive way by which prior information is\nmanifested at different levels of the network.\n", "versions": [{"version": "v1", "created": "Fri, 3 Nov 2017 17:14:14 GMT"}, {"version": "v2", "created": "Thu, 28 Dec 2017 19:38:13 GMT"}, {"version": "v3", "created": "Tue, 13 Feb 2018 05:26:27 GMT"}, {"version": "v4", "created": "Thu, 17 May 2018 18:33:49 GMT"}, {"version": "v5", "created": "Tue, 22 May 2018 08:43:18 GMT"}, {"version": "v6", "created": "Fri, 8 Jun 2018 09:34:51 GMT"}, {"version": "v7", "created": "Tue, 31 Jul 2018 09:44:24 GMT"}, {"version": "v8", "created": "Mon, 20 May 2019 10:29:06 GMT"}], "update_date": "2019-05-21", "authors_parsed": [["Amit", "Ron", ""], ["Meir", "Ron", ""]]}, {"id": "1711.01297", "submitter": "Nick Pawlowski", "authors": "Nick Pawlowski, Andrew Brock, Matthew C.H. Lee, Martin Rajchl, Ben\n  Glocker", "title": "Implicit Weight Uncertainty in Neural Networks", "comments": "Submitted to NIPS 2018, under review", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Modern neural networks tend to be overconfident on unseen, noisy or\nincorrectly labelled data and do not produce meaningful uncertainty measures.\nBayesian deep learning aims to address this shortcoming with variational\napproximations (such as Bayes by Backprop or Multiplicative Normalising Flows).\nHowever, current approaches have limitations regarding flexibility and\nscalability. We introduce Bayes by Hypernet (BbH), a new method of variational\napproximation that interprets hypernetworks as implicit distributions. It\nnaturally uses neural networks to model arbitrarily complex distributions and\nscales to modern deep learning architectures. In our experiments, we\ndemonstrate that our method achieves competitive accuracies and predictive\nuncertainties on MNIST and a CIFAR5 task, while being the most robust against\nadversarial attacks.\n", "versions": [{"version": "v1", "created": "Fri, 3 Nov 2017 18:49:04 GMT"}, {"version": "v2", "created": "Fri, 25 May 2018 07:00:07 GMT"}], "update_date": "2018-05-28", "authors_parsed": [["Pawlowski", "Nick", ""], ["Brock", "Andrew", ""], ["Lee", "Matthew C. H.", ""], ["Rajchl", "Martin", ""], ["Glocker", "Ben", ""]]}, {"id": "1711.01312", "submitter": "Martin Zhang", "authors": "Fei Xia, Martin J. Zhang, James Zou, David Tse", "title": "NeuralFDR: Learning Discovery Thresholds from Hypothesis Features", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.AP stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  As datasets grow richer, an important challenge is to leverage the full\nfeatures in the data to maximize the number of useful discoveries while\ncontrolling for false positives. We address this problem in the context of\nmultiple hypotheses testing, where for each hypothesis, we observe a p-value\nalong with a set of features specific to that hypothesis. For example, in\ngenetic association studies, each hypothesis tests the correlation between a\nvariant and the trait. We have a rich set of features for each variant (e.g.\nits location, conservation, epigenetics etc.) which could inform how likely the\nvariant is to have a true association. However popular testing approaches, such\nas Benjamini-Hochberg's procedure (BH) and independent hypothesis weighting\n(IHW), either ignore these features or assume that the features are categorical\nor uni-variate. We propose a new algorithm, NeuralFDR, which automatically\nlearns a discovery threshold as a function of all the hypothesis features. We\nparametrize the discovery threshold as a neural network, which enables flexible\nhandling of multi-dimensional discrete and continuous features as well as\nefficient end-to-end optimization. We prove that NeuralFDR has strong false\ndiscovery rate (FDR) guarantees, and show that it makes substantially more\ndiscoveries in synthetic and real datasets. Moreover, we demonstrate that the\nlearned discovery threshold is directly interpretable.\n", "versions": [{"version": "v1", "created": "Fri, 3 Nov 2017 19:27:11 GMT"}, {"version": "v2", "created": "Tue, 7 Nov 2017 08:22:26 GMT"}, {"version": "v3", "created": "Wed, 8 Nov 2017 06:01:26 GMT"}, {"version": "v4", "created": "Sat, 18 Nov 2017 20:44:38 GMT"}], "update_date": "2017-11-21", "authors_parsed": [["Xia", "Fei", ""], ["Zhang", "Martin J.", ""], ["Zou", "James", ""], ["Tse", "David", ""]]}, {"id": "1711.01341", "submitter": "Jason Xu", "authors": "Jason Xu, Eric C. Chi, Kenneth Lange", "title": "Generalized Linear Model Regression under Distance-to-set Penalties", "comments": "5 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Estimation in generalized linear models (GLM) is complicated by the presence\nof constraints. One can handle constraints by maximizing a penalized\nlog-likelihood. Penalties such as the lasso are effective in high dimensions,\nbut often lead to unwanted shrinkage. This paper explores instead penalizing\nthe squared distance to constraint sets. Distance penalties are more flexible\nthan algebraic and regularization penalties, and avoid the drawback of\nshrinkage. To optimize distance penalized objectives, we make use of the\nmajorization-minimization principle. Resulting algorithms constructed within\nthis framework are amenable to acceleration and come with global convergence\nguarantees. Applications to shape constraints, sparse regression, and\nrank-restricted matrix regression on synthetic and real data showcase strong\nempirical performance, even under non-convex constraints.\n", "versions": [{"version": "v1", "created": "Fri, 3 Nov 2017 21:37:23 GMT"}], "update_date": "2017-11-07", "authors_parsed": [["Xu", "Jason", ""], ["Chi", "Eric C.", ""], ["Lange", "Kenneth", ""]]}, {"id": "1711.01348", "submitter": "Sebastian Urban", "authors": "Sebastian Urban, Patrick van der Smagt", "title": "Automatic Differentiation for Tensor Algebras", "comments": "Technical Report", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.SC stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Kjolstad et. al. proposed a tensor algebra compiler. It takes expressions\nthat define a tensor element-wise, such as $f_{ij}(a,b,c,d) =\n\\exp\\left[-\\sum_{k=0}^4 \\left((a_{ik}+b_{jk})^2\\, c_{ii} + d_{i+k}^3 \\right)\n\\right]$, and generates the corresponding compute kernel code.\n  For machine learning, especially deep learning, it is often necessary to\ncompute the gradient of a loss function $l(a,b,c,d)=l(f(a,b,c,d))$ with respect\nto parameters $a,b,c,d$. If tensor compilers are to be applied in this field,\nit is necessary to derive expressions for the derivatives of element-wise\ndefined tensors, i.e. expressions for $(da)_{ik}=\\partial l/\\partial a_{ik}$.\n  When the mapping between function indices and argument indices is not 1:1,\nspecial attention is required. For the function $f_{ij} (x) = x_i^2$, the\nderivative of the loss is $(dx)_i=\\partial l/\\partial x_i=\\sum_j\n(df)_{ij}2x_i$; the sum is necessary because index $j$ does not appear in the\nindices of $f$. Another example is $f_{i}(x)=x_{ii}^2$, where $x$ is a matrix;\nhere we have $(dx)_{ij}=\\delta_{ij}(df)_i2x_{ii}$; the Kronecker delta is\nnecessary because the derivative is zero for off-diagonal elements. Another\nindexing scheme is used by $f_{ij}(x)=\\exp x_{i+j}$; here the correct\nderivative is $(dx)_{k}=\\sum_i (df)_{i,k-i} \\exp x_{k}$, where the range of the\nsum must be chosen appropriately.\n  In this publication we present an algorithm that can handle any case in which\nthe indices of an argument are an arbitrary linear combination of the indices\nof the function, thus all the above examples can be handled. Sums (and their\nranges) and Kronecker deltas are automatically inserted into the derivatives as\nnecessary. Additionally, the indices are transformed, if required (as in the\nlast example). The algorithm outputs a symbolic expression that can be\nsubsequently fed into a tensor algebra compiler.\n  Source code is provided.\n", "versions": [{"version": "v1", "created": "Fri, 3 Nov 2017 22:24:47 GMT"}], "update_date": "2017-11-07", "authors_parsed": [["Urban", "Sebastian", ""], ["van der Smagt", "Patrick", ""]]}, {"id": "1711.01416", "submitter": "Vasily Pestun", "authors": "Vasily Pestun, John Terilla, Yiannis Vlassopoulos", "title": "Language as a matrix product state", "comments": "10 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cond-mat.dis-nn cs.LG cs.NE stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a statistical model for natural language that begins by\nconsidering language as a monoid, then representing it in complex matrices with\na compatible translation invariant probability measure. We interpret the\nprobability measure as arising via the Born rule from a translation invariant\nmatrix product state.\n", "versions": [{"version": "v1", "created": "Sat, 4 Nov 2017 09:11:18 GMT"}], "update_date": "2017-11-07", "authors_parsed": [["Pestun", "Vasily", ""], ["Terilla", "John", ""], ["Vlassopoulos", "Yiannis", ""]]}, {"id": "1711.01431", "submitter": "Johan Loeckx", "authors": "Johan Loeckx", "title": "The Case for Meta-Cognitive Machine Learning: On Model Entropy and\n  Concept Formation in Deep Learning", "comments": "5 pages, 5 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Machine learning is usually defined in behaviourist terms, where external\nvalidation is the primary mechanism of learning. In this paper, I argue for a\nmore holistic interpretation in which finding more probable, efficient and\nabstract representations is as central to learning as performance. In other\nwords, machine learning should be extended with strategies to reason over its\nown learning process, leading to so-called meta-cognitive machine learning. As\nsuch, the de facto definition of machine learning should be reformulated in\nthese intrinsically multi-objective terms, taking into account not only the\ntask performance but also internal learning objectives. To this end, we suggest\na \"model entropy function\" to be defined that quantifies the efficiency of the\ninternal learning processes. It is conjured that the minimization of this model\nentropy leads to concept formation. Besides philosophical aspects, some initial\nillustrations are included to support the claims.\n", "versions": [{"version": "v1", "created": "Sat, 4 Nov 2017 12:54:35 GMT"}], "update_date": "2017-11-07", "authors_parsed": [["Loeckx", "Johan", ""]]}, {"id": "1711.01514", "submitter": "Karthikeyan Natesan Ramamurthy", "authors": "Dennis Wei and Karthikeyan Natesan Ramamurthy and Kush R. Varshney", "title": "Distribution-Preserving k-Anonymity", "comments": "Portions of this work were first presented at the 2015 SIAM\n  International Conference on Data Mining", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Preserving the privacy of individuals by protecting their sensitive\nattributes is an important consideration during microdata release. However, it\nis equally important to preserve the quality or utility of the data for at\nleast some targeted workloads. We propose a novel framework for privacy\npreservation based on the k-anonymity model that is ideally suited for\nworkloads that require preserving the probability distribution of the\nquasi-identifier variables in the data. Our framework combines the principles\nof distribution-preserving quantization and k-member clustering, and we\nspecialize it to two variants that respectively use intra-cluster and Gaussian\ndithering of cluster centers to achieve distribution preservation. We perform\ntheoretical analysis of the proposed schemes in terms of distribution\npreservation, and describe their utility in workloads such as covariate shift\nand transfer learning where such a property is necessary. Using extensive\nexperiments on real-world Medical Expenditure Panel Survey data, we demonstrate\nthe merits of our algorithms over standard k-anonymization for a hallmark\nhealth care application where an insurance company wishes to understand the\nrisk in entering a new market. Furthermore, by empirically quantifying the\nreidentification risk, we also show that the proposed approaches indeed\nmaintain k-anonymity.\n", "versions": [{"version": "v1", "created": "Sun, 5 Nov 2017 01:31:25 GMT"}], "update_date": "2017-11-07", "authors_parsed": [["Wei", "Dennis", ""], ["Ramamurthy", "Karthikeyan Natesan", ""], ["Varshney", "Kush R.", ""]]}, {"id": "1711.01530", "submitter": "James Stokes", "authors": "Tengyuan Liang, Tomaso Poggio, Alexander Rakhlin and James Stokes", "title": "Fisher-Rao Metric, Geometry, and Complexity of Neural Networks", "comments": "To appear in the proceedings of the 22nd International Conference on\n  Artificial Intelligence and Statistics (AISTATS) 2019", "journal-ref": "The 22nd International Conference on Artificial Intelligence and\n  Statistics 89 (2019) 888-896", "doi": null, "report-no": null, "categories": "cs.LG cs.AI stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study the relationship between geometry and capacity measures for deep\nneural networks from an invariance viewpoint. We introduce a new notion of\ncapacity --- the Fisher-Rao norm --- that possesses desirable invariance\nproperties and is motivated by Information Geometry. We discover an analytical\ncharacterization of the new capacity measure, through which we establish\nnorm-comparison inequalities and further show that the new measure serves as an\numbrella for several existing norm-based complexity measures. We discuss upper\nbounds on the generalization error induced by the proposed measure. Extensive\nnumerical experiments on CIFAR-10 support our theoretical findings. Our\ntheoretical analysis rests on a key structural lemma about partial derivatives\nof multi-layer rectifier networks.\n", "versions": [{"version": "v1", "created": "Sun, 5 Nov 2017 04:32:59 GMT"}, {"version": "v2", "created": "Sat, 23 Feb 2019 21:27:30 GMT"}], "update_date": "2020-07-27", "authors_parsed": [["Liang", "Tengyuan", ""], ["Poggio", "Tomaso", ""], ["Rakhlin", "Alexander", ""], ["Stokes", "James", ""]]}, {"id": "1711.01558", "submitter": "Ilya Tolstikhin", "authors": "Ilya Tolstikhin and Olivier Bousquet and Sylvain Gelly and Bernhard\n  Schoelkopf", "title": "Wasserstein Auto-Encoders", "comments": "Published at ICLR 2018.. Included much wider hyperparameter sweep: in\n  significant improvements in FIDs on CelebA", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose the Wasserstein Auto-Encoder (WAE)---a new algorithm for building\na generative model of the data distribution. WAE minimizes a penalized form of\nthe Wasserstein distance between the model distribution and the target\ndistribution, which leads to a different regularizer than the one used by the\nVariational Auto-Encoder (VAE). This regularizer encourages the encoded\ntraining distribution to match the prior. We compare our algorithm with several\nother techniques and show that it is a generalization of adversarial\nauto-encoders (AAE). Our experiments show that WAE shares many of the\nproperties of VAEs (stable training, encoder-decoder architecture, nice latent\nmanifold structure) while generating samples of better quality, as measured by\nthe FID score.\n", "versions": [{"version": "v1", "created": "Sun, 5 Nov 2017 10:18:27 GMT"}, {"version": "v2", "created": "Tue, 19 Dec 2017 08:24:54 GMT"}, {"version": "v3", "created": "Mon, 12 Mar 2018 15:06:34 GMT"}, {"version": "v4", "created": "Thu, 5 Dec 2019 10:27:44 GMT"}], "update_date": "2019-12-06", "authors_parsed": [["Tolstikhin", "Ilya", ""], ["Bousquet", "Olivier", ""], ["Gelly", "Sylvain", ""], ["Schoelkopf", "Bernhard", ""]]}, {"id": "1711.01559", "submitter": "Louis-Serge Bouchard", "authors": "K. Youssef, Louis-S. Bouchard, K.Z. Haigh, H. Krovi, J. Silovsky, C.P.\n  Vander Valk", "title": "Machine Learning Approach to RF Transmitter Identification", "comments": "14 pages, 14 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.SP cs.LG cs.NE stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  With the development and widespread use of wireless devices in recent years\n(mobile phones, Internet of Things, Wi-Fi), the electromagnetic spectrum has\nbecome extremely crowded. In order to counter security threats posed by rogue\nor unknown transmitters, it is important to identify RF transmitters not by the\ndata content of the transmissions but based on the intrinsic physical\ncharacteristics of the transmitters. RF waveforms represent a particular\nchallenge because of the extremely high data rates involved and the potentially\nlarge number of transmitters present in a given location. These factors outline\nthe need for rapid fingerprinting and identification methods that go beyond the\ntraditional hand-engineered approaches. In this study, we investigate the use\nof machine learning (ML) strategies to the classification and identification\nproblems, and the use of wavelets to reduce the amount of data required. Four\ndifferent ML strategies are evaluated: deep neural nets (DNN), convolutional\nneural nets (CNN), support vector machines (SVM), and multi-stage training\n(MST) using accelerated Levenberg-Marquardt (A-LM) updates. The A-LM MST method\npreconditioned by wavelets was by far the most accurate, achieving 100%\nclassification accuracy of transmitters, as tested using data originating from\n12 different transmitters. We discuss strategies for extension of MST to a much\nlarger number of transmitters.\n", "versions": [{"version": "v1", "created": "Sun, 5 Nov 2017 10:41:05 GMT"}, {"version": "v2", "created": "Tue, 7 Nov 2017 06:14:22 GMT"}], "update_date": "2017-11-09", "authors_parsed": [["Youssef", "K.", ""], ["Bouchard", "Louis-S.", ""], ["Haigh", "K. Z.", ""], ["Krovi", "H.", ""], ["Silovsky", "J.", ""], ["Valk", "C. P. Vander", ""]]}, {"id": "1711.01566", "submitter": "Mohammad Reza Karimi", "authors": "Mohammad Reza Karimi and Mario Lucic and Hamed Hassani and Andreas\n  Krause", "title": "Stochastic Submodular Maximization: The Case of Coverage Functions", "comments": "31st Conference on Neural Information Processing Systems (NIPS 2017)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.DM stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Stochastic optimization of continuous objectives is at the heart of modern\nmachine learning. However, many important problems are of discrete nature and\noften involve submodular objectives. We seek to unleash the power of stochastic\ncontinuous optimization, namely stochastic gradient descent and its variants,\nto such discrete problems. We first introduce the problem of stochastic\nsubmodular optimization, where one needs to optimize a submodular objective\nwhich is given as an expectation. Our model captures situations where the\ndiscrete objective arises as an empirical risk (e.g., in the case of\nexemplar-based clustering), or is given as an explicit stochastic model (e.g.,\nin the case of influence maximization in social networks). By exploiting that\ncommon extensions act linearly on the class of submodular functions, we employ\nprojected stochastic gradient ascent and its variants in the continuous domain,\nand perform rounding to obtain discrete solutions. We focus on the rich and\nwidely used family of weighted coverage functions. We show that our approach\nyields solutions that are guaranteed to match the optimal approximation\nguarantees, while reducing the computational cost by several orders of\nmagnitude, as we demonstrate empirically.\n", "versions": [{"version": "v1", "created": "Sun, 5 Nov 2017 11:58:55 GMT"}], "update_date": "2017-11-07", "authors_parsed": [["Karimi", "Mohammad Reza", ""], ["Lucic", "Mario", ""], ["Hassani", "Hamed", ""], ["Krause", "Andreas", ""]]}, {"id": "1711.01569", "submitter": "Markus Dumke", "authors": "Markus Dumke", "title": "Double Q($\\sigma$) and Q($\\sigma, \\lambda$): Unifying Reinforcement\n  Learning Control Algorithms", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Temporal-difference (TD) learning is an important field in reinforcement\nlearning. Sarsa and Q-Learning are among the most used TD algorithms. The\nQ($\\sigma$) algorithm (Sutton and Barto (2017)) unifies both. This paper\nextends the Q($\\sigma$) algorithm to an online multi-step algorithm Q($\\sigma,\n\\lambda$) using eligibility traces and introduces Double Q($\\sigma$) as the\nextension of Q($\\sigma$) to double learning. Experiments suggest that the new\nQ($\\sigma, \\lambda$) algorithm can outperform the classical TD control methods\nSarsa($\\lambda$), Q($\\lambda$) and Q($\\sigma$).\n", "versions": [{"version": "v1", "created": "Sun, 5 Nov 2017 12:05:31 GMT"}], "update_date": "2017-11-07", "authors_parsed": [["Dumke", "Markus", ""]]}, {"id": "1711.01577", "submitter": "Zhen He", "authors": "Zhen He, Shaobing Gao, Liang Xiao, Daxue Liu, Hangen He, David Barber", "title": "Wider and Deeper, Cheaper and Faster: Tensorized LSTMs for Sequence\n  Learning", "comments": "Accepted by NIPS 2017", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.AI cs.LG cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Long Short-Term Memory (LSTM) is a popular approach to boosting the ability\nof Recurrent Neural Networks to store longer term temporal information. The\ncapacity of an LSTM network can be increased by widening and adding layers.\nHowever, usually the former introduces additional parameters, while the latter\nincreases the runtime. As an alternative we propose the Tensorized LSTM in\nwhich the hidden states are represented by tensors and updated via a\ncross-layer convolution. By increasing the tensor size, the network can be\nwidened efficiently without additional parameters since the parameters are\nshared across different locations in the tensor; by delaying the output, the\nnetwork can be deepened implicitly with little additional runtime since deep\ncomputations for each timestep are merged into temporal computations of the\nsequence. Experiments conducted on five challenging sequence learning tasks\nshow the potential of the proposed model.\n", "versions": [{"version": "v1", "created": "Sun, 5 Nov 2017 12:30:35 GMT"}, {"version": "v2", "created": "Tue, 7 Nov 2017 10:41:28 GMT"}, {"version": "v3", "created": "Wed, 13 Dec 2017 02:14:14 GMT"}], "update_date": "2017-12-14", "authors_parsed": [["He", "Zhen", ""], ["Gao", "Shaobing", ""], ["Xiao", "Liang", ""], ["Liu", "Daxue", ""], ["He", "Hangen", ""], ["Barber", "David", ""]]}, {"id": "1711.01598", "submitter": "Xuan Bi", "authors": "Xuan Bi, Annie Qu and Xiaotong Shen", "title": "Multilayer tensor factorization with applications to recommender systems", "comments": "Accepted by the Annals of Statistics", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML stat.AP stat.ME stat.OT", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recommender systems have been widely adopted by electronic commerce and\nentertainment industries for individualized prediction and recommendation,\nwhich benefit consumers and improve business intelligence. In this article, we\npropose an innovative method, namely the recommendation engine of multilayers\n(REM), for tensor recommender systems. The proposed method utilizes the\nstructure of a tensor response to integrate information from multiple modes,\nand creates an additional layer of nested latent factors to accommodate\nbetween-subjects dependency. One major advantage is that the proposed method is\nable to address the \"cold-start\" issue in the absence of information from new\ncustomers, new products or new contexts. Specifically, it provides more\neffective recommendations through sub-group information. To achieve scalable\ncomputation, we develop a new algorithm for the proposed method, which\nincorporates a maximum block improvement strategy into the cyclic\nblockwise-coordinate-descent algorithm. In theory, we investigate both\nalgorithmic properties for global and local convergence, along with the\nasymptotic consistency of estimated parameters. Finally, the proposed method is\napplied in simulations and IRI marketing data with 116 million observations of\nproduct sales. Numerical studies demonstrate that the proposed method\noutperforms existing competitors in the literature.\n", "versions": [{"version": "v1", "created": "Sun, 5 Nov 2017 14:40:07 GMT"}], "update_date": "2017-11-07", "authors_parsed": [["Bi", "Xuan", ""], ["Qu", "Annie", ""], ["Shen", "Xiaotong", ""]]}, {"id": "1711.01655", "submitter": "Elchanan Mossel", "authors": "Vishesh Jain, Frederic Koehler, Elchanan Mossel", "title": "Approximating Partition Functions in Constant Time", "comments": "This preprint is completely subsumed by preprints arXiv:1802.06126\n  and arXiv:1802.06129 by the same authors which also include important\n  references that are missing in the current preprint", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.DS stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study approximations of the partition function of dense graphical models.\nPartition functions of graphical models play a fundamental role is statistical\nphysics, in statistics and in machine learning. Two of the main methods for\napproximating the partition function are Markov Chain Monte Carlo and\nVariational Methods. An impressive body of work in mathematics, physics and\ntheoretical computer science provides conditions under which Markov Chain Monte\nCarlo methods converge in polynomial time. These methods often lead to\npolynomial time approximation algorithms for the partition function in cases\nwhere the underlying model exhibits correlation decay. There are very few\ntheoretical guarantees for the performance of variational methods. One\nexception is recent results by Risteski (2016) who considered dense graphical\nmodels and showed that using variational methods, it is possible to find an\n$O(\\epsilon n)$ additive approximation to the log partition function in time\n$n^{O(1/\\epsilon^2)}$ even in a regime where correlation decay does not hold.\n  We show that under essentially the same conditions, an $O(\\epsilon n)$\nadditive approximation of the log partition function can be found in constant\ntime, independent of $n$. In particular, our results cover dense Ising and\nPotts models as well as dense graphical models with $k$-wise interaction. They\nalso apply for low threshold rank models.\n", "versions": [{"version": "v1", "created": "Sun, 5 Nov 2017 20:06:01 GMT"}, {"version": "v2", "created": "Tue, 20 Feb 2018 16:31:41 GMT"}], "update_date": "2018-02-21", "authors_parsed": [["Jain", "Vishesh", ""], ["Koehler", "Frederic", ""], ["Mossel", "Elchanan", ""]]}, {"id": "1711.01661", "submitter": "Alex Groce", "authors": "Alex Groce and Josie Holmes", "title": "Provenance and Pseudo-Provenance for Seeded Learning-Based Automated\n  Test Generation", "comments": "Presented at NIPS 2017 Symposium on Interpretable Machine Learning", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.SE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Many methods for automated software test generation, including some that\nexplicitly use machine learning (and some that use ML more broadly conceived)\nderive new tests from existing tests (often referred to as seeds). Often, the\nseed tests from which new tests are derived are manually constructed, or at\nleast simpler than the tests that are produced as the final outputs of such\ntest generators. We propose annotation of generated tests with a provenance\n(trail) showing how individual generated tests of interest (especially failing\ntests) derive from seed tests, and how the population of generated tests\nrelates to the original seed tests. In some cases, post-processing of generated\ntests can invalidate provenance information, in which case we also propose a\nmethod for attempting to construct \"pseudo-provenance\" describing how the tests\ncould have been (partly) generated from seeds.\n", "versions": [{"version": "v1", "created": "Sun, 5 Nov 2017 21:08:01 GMT"}, {"version": "v2", "created": "Wed, 15 Nov 2017 09:32:38 GMT"}], "update_date": "2017-11-16", "authors_parsed": [["Groce", "Alex", ""], ["Holmes", "Josie", ""]]}, {"id": "1711.01682", "submitter": "Andrea Montanari", "authors": "Andrea Montanari and Ramji Venkataramanan", "title": "Estimation of Low-Rank Matrices via Approximate Message Passing", "comments": "76 pages, 6 pdf figures; Version 4 expands the introductory material\n  and the applications to statistical inference", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.ML stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Consider the problem of estimating a low-rank matrix when its entries are\nperturbed by Gaussian noise. If the empirical distribution of the entries of\nthe spikes is known, optimal estimators that exploit this knowledge can\nsubstantially outperform simple spectral approaches. Recent work characterizes\nthe asymptotic accuracy of Bayes-optimal estimators in the high-dimensional\nlimit. In this paper we present a practical algorithm that can achieve\nBayes-optimal accuracy above the spectral threshold. A bold conjecture from\nstatistical physics posits that no polynomial-time algorithm achieves optimal\nerror below the same threshold (unless the best estimator is trivial). Our\napproach uses Approximate Message Passing (AMP) in conjunction with a spectral\ninitialization. AMP algorithms have proved successful in a variety of\nstatistical estimation tasks, and are amenable to exact asymptotic analysis via\nstate evolution. Unfortunately, state evolution is uninformative when the\nalgorithm is initialized near an unstable fixed point, as often happens in\nlow-rank matrix estimation. We develop a new analysis of AMP that allows for\nspectral initializations. Our main theorem is general and applies beyond matrix\nestimation. However, we use it to derive detailed predictions for the problem\nof estimating a rank-one matrix in noise. Special cases of this problem are\nclosely related---via universality arguments---to the network community\ndetection problem for two asymmetric communities. For general rank-one models,\nwe show that AMP can be used to construct confidence intervals and control\nfalse discovery rate. We provide illustrations of the general methodology by\nconsidering the cases of sparse low-rank matrices and of block-constant\nlow-rank matrices with symmetric blocks (we refer to the latter as to the\n`Gaussian Block Model').\n", "versions": [{"version": "v1", "created": "Mon, 6 Nov 2017 00:30:08 GMT"}, {"version": "v2", "created": "Tue, 2 Jan 2018 00:28:54 GMT"}, {"version": "v3", "created": "Sat, 19 May 2018 19:04:32 GMT"}, {"version": "v4", "created": "Wed, 7 Aug 2019 17:15:36 GMT"}], "update_date": "2019-08-08", "authors_parsed": [["Montanari", "Andrea", ""], ["Venkataramanan", "Ramji", ""]]}, {"id": "1711.01742", "submitter": "Ji Chen", "authors": "Ji Chen, Xiaodong Li", "title": "Model-free Nonconvex Matrix Completion: Local Minima Analysis and\n  Applications in Memory-efficient Kernel PCA", "comments": "Main theorem improved", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.OC cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This work studies low-rank approximation of a positive semidefinite matrix\nfrom partial entries via nonconvex optimization. We characterized how well\nlocal-minimum based low-rank factorization approximates a fixed positive\nsemidefinite matrix without any assumptions on the rank-matching, the condition\nnumber or eigenspace incoherence parameter. Furthermore, under certain\nassumptions on rank-matching and well-boundedness of condition numbers and\neigenspace incoherence parameters, a corollary of our main theorem improves the\nstate-of-the-art sampling rate results for nonconvex matrix completion with no\nspurious local minima in Ge et al. [2016, 2017]. In addition, we investigated\nwhen the proposed nonconvex optimization results in accurate low-rank\napproximations even in presence of large condition numbers, large incoherence\nparameters, or rank mismatching. We also propose to apply the nonconvex\noptimization to memory-efficient Kernel PCA. Compared to the well-known\nNystr\\\"{o}m methods, numerical experiments indicate that the proposed nonconvex\noptimization approach yields more stable results in both low-rank approximation\nand clustering.\n", "versions": [{"version": "v1", "created": "Mon, 6 Nov 2017 06:24:59 GMT"}, {"version": "v2", "created": "Tue, 26 Dec 2017 07:10:00 GMT"}, {"version": "v3", "created": "Thu, 4 Apr 2019 23:08:24 GMT"}], "update_date": "2019-04-08", "authors_parsed": [["Chen", "Ji", ""], ["Li", "Xiaodong", ""]]}, {"id": "1711.01744", "submitter": "Minh Trung Le", "authors": "Trung Le, Tu Dinh Nguyen, Dinh Phung", "title": "KGAN: How to Break The Minimax Game in GAN", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Generative Adversarial Networks (GANs) were intuitively and attractively\nexplained under the perspective of game theory, wherein two involving parties\nare a discriminator and a generator. In this game, the task of the\ndiscriminator is to discriminate the real and generated (i.e., fake) data,\nwhilst the task of the generator is to generate the fake data that maximally\nconfuses the discriminator. In this paper, we propose a new viewpoint for GANs,\nwhich is termed as the minimizing general loss viewpoint. This viewpoint shows\na connection between the general loss of a classification problem regarding a\nconvex loss function and a f-divergence between the true and fake data\ndistributions. Mathematically, we proposed a setting for the classification\nproblem of the true and fake data, wherein we can prove that the general loss\nof this classification problem is exactly the negative f-divergence for a\ncertain convex function f. This allows us to interpret the problem of learning\nthe generator for dismissing the f-divergence between the true and fake data\ndistributions as that of maximizing the general loss which is equivalent to the\nmin-max problem in GAN if the Logistic loss is used in the classification\nproblem. However, this viewpoint strengthens GANs in two ways. First, it allows\nus to employ any convex loss function for the discriminator. Second, it\nsuggests that rather than limiting ourselves in NN-based discriminators, we can\nalternatively utilize other powerful families. Bearing this viewpoint, we then\npropose using the kernel-based family for discriminators. This family has two\nappealing features: i) a powerful capacity in classifying non-linear nature\ndata and ii) being convex in the feature space. Using the convexity of this\nfamily, we can further develop Fenchel duality to equivalently transform the\nmax-min problem to the max-max dual problem.\n", "versions": [{"version": "v1", "created": "Mon, 6 Nov 2017 06:33:01 GMT"}], "update_date": "2017-11-07", "authors_parsed": [["Le", "Trung", ""], ["Nguyen", "Tu Dinh", ""], ["Phung", "Dinh", ""]]}, {"id": "1711.01761", "submitter": "Alexandre Defossez", "authors": "Alexandre D\\'efossez (FAIR), Francis Bach (SIERRA)", "title": "AdaBatch: Efficient Gradient Aggregation Rules for Sequential and\n  Parallel Stochastic Gradient Methods", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG math.OC stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study a new aggregation operator for gradients coming from a mini-batch\nfor stochastic gradient (SG) methods that allows a significant speed-up in the\ncase of sparse optimization problems. We call this method AdaBatch and it only\nrequires a few lines of code change compared to regular mini-batch SGD\nalgorithms. We provide a theoretical insight to understand how this new class\nof algorithms is performing and show that it is equivalent to an implicit\nper-coordinate rescaling of the gradients, similarly to what Adagrad methods\ncan do. In theory and in practice, this new aggregation allows to keep the same\nsample efficiency of SG methods while increasing the batch size.\nExperimentally, we also show that in the case of smooth convex optimization,\nour procedure can even obtain a better loss when increasing the batch size for\na fixed number of samples. We then apply this new algorithm to obtain a\nparallelizable stochastic gradient method that is synchronous but allows\nspeed-up on par with Hogwild! methods as convergence does not deteriorate with\nthe increase of the batch size. The same approach can be used to make\nmini-batch provably efficient for variance-reduced SG methods such as SVRG.\n", "versions": [{"version": "v1", "created": "Mon, 6 Nov 2017 07:52:34 GMT"}], "update_date": "2017-11-07", "authors_parsed": [["D\u00e9fossez", "Alexandre", "", "FAIR"], ["Bach", "Francis", "", "SIERRA"]]}, {"id": "1711.01768", "submitter": "Seong Joon Oh", "authors": "Seong Joon Oh, Max Augustin, Bernt Schiele, Mario Fritz", "title": "Towards Reverse-Engineering Black-Box Neural Networks", "comments": "20 pages, 12 figures, to appear at ICLR'18. Code:\n  https://goo.gl/MbYfsv", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.CR cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Many deployed learned models are black boxes: given input, returns output.\nInternal information about the model, such as the architecture, optimisation\nprocedure, or training data, is not disclosed explicitly as it might contain\nproprietary information or make the system more vulnerable. This work shows\nthat such attributes of neural networks can be exposed from a sequence of\nqueries. This has multiple implications. On the one hand, our work exposes the\nvulnerability of black-box neural networks to different types of attacks -- we\nshow that the revealed internal information helps generate more effective\nadversarial examples against the black box model. On the other hand, this\ntechnique can be used for better protection of private content from automatic\nrecognition models using adversarial examples. Our paper suggests that it is\nactually hard to draw a line between white box and black box models.\n", "versions": [{"version": "v1", "created": "Mon, 6 Nov 2017 07:58:48 GMT"}, {"version": "v2", "created": "Wed, 7 Feb 2018 14:04:23 GMT"}, {"version": "v3", "created": "Wed, 14 Feb 2018 18:50:53 GMT"}], "update_date": "2018-02-15", "authors_parsed": [["Oh", "Seong Joon", ""], ["Augustin", "Max", ""], ["Schiele", "Bernt", ""], ["Fritz", "Mario", ""]]}, {"id": "1711.01790", "submitter": "Hang Xiao", "authors": "Hang Xiao, Zhengli Xing, Linxiao Yang, Jun Fang, Yanlun Wu", "title": "Simultaneous Block-Sparse Signal Recovery Using Pattern-Coupled Sparse\n  Bayesian Learning", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.IT eess.SP math.IT stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we consider the block-sparse signals recovery problem in the\ncontext of multiple measurement vectors (MMV) with common row sparsity\npatterns. We develop a new method for recovery of common row sparsity MMV\nsignals, where a pattern-coupled hierarchical Gaussian prior model is\nintroduced to characterize both the block-sparsity of the coefficients and the\nstatistical dependency between neighboring coefficients of the common row\nsparsity MMV signals. Unlike many other methods, the proposed method is able to\nautomatically capture the block sparse structure of the unknown signal. Our\nmethod is developed using an expectation-maximization (EM) framework.\nSimulation results show that our proposed method offers competitive performance\nin recovering block-sparse common row sparsity pattern MMV signals.\n", "versions": [{"version": "v1", "created": "Mon, 6 Nov 2017 09:00:00 GMT"}], "update_date": "2017-11-07", "authors_parsed": [["Xiao", "Hang", ""], ["Xing", "Zhengli", ""], ["Yang", "Linxiao", ""], ["Fang", "Jun", ""], ["Wu", "Yanlun", ""]]}, {"id": "1711.01796", "submitter": "Masaaki Takada Mr.", "authors": "Masaaki Takada, Taiji Suzuki, Hironori Fujisawa", "title": "Independently Interpretable Lasso: A New Regularizer for Sparse\n  Regression with Uncorrelated Variables", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Sparse regularization such as $\\ell_1$ regularization is a quite powerful and\nwidely used strategy for high dimensional learning problems. The effectiveness\nof sparse regularization has been supported practically and theoretically by\nseveral studies. However, one of the biggest issues in sparse regularization is\nthat its performance is quite sensitive to correlations between features.\nOrdinary $\\ell_1$ regularization can select variables correlated with each\nother, which results in deterioration of not only its generalization error but\nalso interpretability. In this paper, we propose a new regularization method,\n\"Independently Interpretable Lasso\" (IILasso). Our proposed regularizer\nsuppresses selecting correlated variables, and thus each active variable\nindependently affects the objective variable in the model. Hence, we can\ninterpret regression coefficients intuitively and also improve the performance\nby avoiding overfitting. We analyze theoretical property of IILasso and show\nthat the proposed method is much advantageous for its sign recovery and\nachieves almost minimax optimal convergence rate. Synthetic and real data\nanalyses also indicate the effectiveness of IILasso.\n", "versions": [{"version": "v1", "created": "Mon, 6 Nov 2017 09:22:21 GMT"}, {"version": "v2", "created": "Thu, 22 Feb 2018 16:41:28 GMT"}], "update_date": "2018-02-23", "authors_parsed": [["Takada", "Masaaki", ""], ["Suzuki", "Taiji", ""], ["Fujisawa", "Hironori", ""]]}, {"id": "1711.01846", "submitter": "Artur Speiser", "authors": "Artur Speiser, Jinyao Yan, Evan Archer, Lars Buesing, Srinivas C.\n  Turaga, Jakob H. Macke", "title": "Fast amortized inference of neural activity from calcium imaging data\n  with variational autoencoders", "comments": "NIPS 2017", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG q-bio.NC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Calcium imaging permits optical measurement of neural activity. Since\nintracellular calcium concentration is an indirect measurement of neural\nactivity, computational tools are necessary to infer the true underlying\nspiking activity from fluorescence measurements. Bayesian model inversion can\nbe used to solve this problem, but typically requires either computationally\nexpensive MCMC sampling, or faster but approximate maximum-a-posteriori\noptimization. Here, we introduce a flexible algorithmic framework for fast,\nefficient and accurate extraction of neural spikes from imaging data. Using the\nframework of variational autoencoders, we propose to amortize inference by\ntraining a deep neural network to perform model inversion efficiently. The\nrecognition network is trained to produce samples from the posterior\ndistribution over spike trains. Once trained, performing inference amounts to a\nfast single forward pass through the network, without the need for iterative\noptimization or sampling. We show that amortization can be applied flexibly to\na wide range of nonlinear generative models and significantly improves upon the\nstate of the art in computation time, while achieving competitive accuracy. Our\nframework is also able to represent posterior distributions over spike-trains.\nWe demonstrate the generality of our method by proposing the first\nprobabilistic approach for separating backpropagating action potentials from\nputative synaptic inputs in calcium imaging of dendritic spines.\n", "versions": [{"version": "v1", "created": "Mon, 6 Nov 2017 11:57:54 GMT"}], "update_date": "2017-11-07", "authors_parsed": [["Speiser", "Artur", ""], ["Yan", "Jinyao", ""], ["Archer", "Evan", ""], ["Buesing", "Lars", ""], ["Turaga", "Srinivas C.", ""], ["Macke", "Jakob H.", ""]]}, {"id": "1711.01847", "submitter": "Marcel Nonnenmacher", "authors": "Marcel Nonnenmacher, Srinivas C. Turaga, Jakob H. Macke", "title": "Extracting low-dimensional dynamics from multiple large-scale neural\n  population recordings by learning to predict correlations", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A powerful approach for understanding neural population dynamics is to\nextract low-dimensional trajectories from population recordings using\ndimensionality reduction methods. Current approaches for dimensionality\nreduction on neural data are limited to single population recordings, and can\nnot identify dynamics embedded across multiple measurements. We propose an\napproach for extracting low-dimensional dynamics from multiple, sequential\nrecordings. Our algorithm scales to data comprising millions of observed\ndimensions, making it possible to access dynamics distributed across large\npopulations or multiple brain areas. Building on subspace-identification\napproaches for dynamical systems, we perform parameter estimation by minimizing\na moment-matching objective using a scalable stochastic gradient descent\nalgorithm: The model is optimized to predict temporal covariations across\nneurons and across time. We show how this approach naturally handles missing\ndata and multiple partial recordings, and can identify dynamics and predict\ncorrelations even in the presence of severe subsampling and small overlap\nbetween recordings. We demonstrate the effectiveness of the approach both on\nsimulated data and a whole-brain larval zebrafish imaging dataset.\n", "versions": [{"version": "v1", "created": "Mon, 6 Nov 2017 11:59:19 GMT"}], "update_date": "2017-11-07", "authors_parsed": [["Nonnenmacher", "Marcel", ""], ["Turaga", "Srinivas C.", ""], ["Macke", "Jakob H.", ""]]}, {"id": "1711.01861", "submitter": "Pedro J Goncalves", "authors": "Jan-Matthis Lueckmann, Pedro J. Goncalves, Giacomo Bassetto, Kaan\n  \\\"Ocal, Marcel Nonnenmacher, Jakob H. Macke", "title": "Flexible statistical inference for mechanistic models of neural dynamics", "comments": "NIPS 2017. The first two authors contributed equally", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Mechanistic models of single-neuron dynamics have been extensively studied in\ncomputational neuroscience. However, identifying which models can\nquantitatively reproduce empirically measured data has been challenging. We\npropose to overcome this limitation by using likelihood-free inference\napproaches (also known as Approximate Bayesian Computation, ABC) to perform\nfull Bayesian inference on single-neuron models. Our approach builds on recent\nadvances in ABC by learning a neural network which maps features of the\nobserved data to the posterior distribution over parameters. We learn a\nBayesian mixture-density network approximating the posterior over multiple\nrounds of adaptively chosen simulations. Furthermore, we propose an efficient\napproach for handling missing features and parameter settings for which the\nsimulator fails, as well as a strategy for automatically learning relevant\nfeatures using recurrent neural networks. On synthetic data, our approach\nefficiently estimates posterior distributions and recovers ground-truth\nparameters. On in-vitro recordings of membrane voltages, we recover\nmultivariate posteriors over biophysical parameters, which yield\nmodel-predicted voltage traces that accurately match empirical data. Our\napproach will enable neuroscientists to perform Bayesian inference on complex\nneuron models without having to design model-specific algorithms, closing the\ngap between mechanistic and statistical approaches to single-neuron modelling.\n", "versions": [{"version": "v1", "created": "Mon, 6 Nov 2017 12:36:07 GMT"}], "update_date": "2017-11-07", "authors_parsed": [["Lueckmann", "Jan-Matthis", ""], ["Goncalves", "Pedro J.", ""], ["Bassetto", "Giacomo", ""], ["\u00d6cal", "Kaan", ""], ["Nonnenmacher", "Marcel", ""], ["Macke", "Jakob H.", ""]]}, {"id": "1711.01870", "submitter": "Snehasis Banerjee", "authors": "Snehasis Banerjee and Tanushyam Chattopadhyay and Ayan Mukherjee", "title": "Interpretable Feature Recommendation for Signal Analytics", "comments": "4 pages, Interpretable Data Mining Workshop, CIKM 2017", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper presents an automated approach for interpretable feature\nrecommendation for solving signal data analytics problems. The method has been\ntested by performing experiments on datasets in the domain of prognostics where\ninterpretation of features is considered very important. The proposed approach\nis based on Wide Learning architecture and provides means for interpretation of\nthe recommended features. It is to be noted that such an interpretation is not\navailable with feature learning approaches like Deep Learning (such as\nConvolutional Neural Network) or feature transformation approaches like\nPrincipal Component Analysis. Results show that the feature recommendation and\ninterpretation techniques are quite effective for the problems at hand in terms\nof performance and drastic reduction in time to develop a solution. It is\nfurther shown by an example, how this human-in-loop interpretation system can\nbe used as a prescriptive system.\n", "versions": [{"version": "v1", "created": "Mon, 6 Nov 2017 13:03:37 GMT"}], "update_date": "2017-11-07", "authors_parsed": [["Banerjee", "Snehasis", ""], ["Chattopadhyay", "Tanushyam", ""], ["Mukherjee", "Ayan", ""]]}, {"id": "1711.01921", "submitter": "Rakshith Shetty", "authors": "Rakshith Shetty and Bernt Schiele and Mario Fritz", "title": "$A^{4}NT$: Author Attribute Anonymity by Adversarial Training of Neural\n  Machine Translation", "comments": "16 pages, 10 figures and 8 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CR cs.CL cs.CY cs.SI stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Text-based analysis methods allow to reveal privacy relevant author\nattributes such as gender, age and identify of the text's author. Such methods\ncan compromise the privacy of an anonymous author even when the author tries to\nremove privacy sensitive content. In this paper, we propose an automatic\nmethod, called Adversarial Author Attribute Anonymity Neural Translation\n($A^4NT$), to combat such text-based adversaries. We combine\nsequence-to-sequence language models used in machine translation and generative\nadversarial networks to obfuscate author attributes. Unlike machine translation\ntechniques which need paired data, our method can be trained on unpaired\ncorpora of text containing different authors. Importantly, we propose and\nevaluate techniques to impose constraints on our $A^4NT$ to preserve the\nsemantics of the input text. $A^4NT$ learns to make minimal changes to the\ninput text to successfully fool author attribute classifiers, while aiming to\nmaintain the meaning of the input. We show through experiments on two different\ndatasets and three settings that our proposed method is effective in fooling\nthe author attribute classifiers and thereby improving the anonymity of\nauthors.\n", "versions": [{"version": "v1", "created": "Mon, 6 Nov 2017 14:54:56 GMT"}, {"version": "v2", "created": "Tue, 7 Nov 2017 16:56:09 GMT"}, {"version": "v3", "created": "Mon, 19 Feb 2018 10:37:27 GMT"}], "update_date": "2018-02-20", "authors_parsed": [["Shetty", "Rakshith", ""], ["Schiele", "Bernt", ""], ["Fritz", "Mario", ""]]}, {"id": "1711.01944", "submitter": "Tianbao Yang", "authors": "Yi Xu, Rong Jin, Tianbao Yang", "title": "First-order Stochastic Algorithms for Escaping From Saddle Points in\n  Almost Linear Time", "comments": "40 pages; updated some proofs, included some new results", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.OC stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Two classes of methods have been proposed for escaping from saddle points\nwith one using the second-order information carried by the Hessian and the\nother adding the noise into the first-order information. The existing analysis\nfor algorithms using noise in the first-order information is quite involved and\nhides the essence of added noise, which hinder further improvements of these\nalgorithms. In this paper, we present a novel perspective of noise-adding\ntechnique, i.e., adding the noise into the first-order information can help\nextract the negative curvature from the Hessian matrix, and provide a formal\nreasoning of this perspective by analyzing a simple first-order procedure. More\nimportantly, the proposed procedure enables one to design purely first-order\nstochastic algorithms for escaping from non-degenerate saddle points with a\nmuch better time complexity (almost linear time in terms of the problem's\ndimensionality). In particular, we develop a {\\bf first-order stochastic\nalgorithm} based on our new technique and an existing algorithm that only\nconverges to a first-order stationary point to enjoy a time complexity of\n{$\\widetilde O(d/\\epsilon^{3.5})$ for finding a nearly second-order stationary\npoint $\\bf{x}$ such that $\\|\\nabla F(bf{x})\\|\\leq \\epsilon$ and $\\nabla^2\nF(bf{x})\\geq -\\sqrt{\\epsilon}I$ (in high probability), where $F(\\cdot)$ denotes\nthe objective function and $d$ is the dimensionality of the problem. To the\nbest of our knowledge, this is the best theoretical result of first-order\nalgorithms for stochastic non-convex optimization, which is even competitive\nwith if not better than existing stochastic algorithms hinging on the\nsecond-order information.\n", "versions": [{"version": "v1", "created": "Fri, 3 Nov 2017 05:18:50 GMT"}, {"version": "v2", "created": "Tue, 7 Nov 2017 22:25:00 GMT"}, {"version": "v3", "created": "Thu, 1 Mar 2018 22:35:24 GMT"}], "update_date": "2018-03-05", "authors_parsed": [["Xu", "Yi", ""], ["Jin", "Rong", ""], ["Yang", "Tianbao", ""]]}, {"id": "1711.01968", "submitter": "Jiajun Zhang", "authors": "Jiajun Zhang, Zhiguo Shi", "title": "Deformable Deep Convolutional Generative Adversarial Network in\n  Microwave Based Hand Gesture Recognition System", "comments": "Accepted by International Conference on Wireless Communications and\n  Signal Processing 2017", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Traditional vision-based hand gesture recognition systems is limited under\ndark circumstances. In this paper, we build a hand gesture recognition system\nbased on microwave transceiver and deep learning algorithm. A Doppler radar\nsensor with dual receiving channels at 5.8GHz is used to acquire a big database\nof hand gestures signals. The received hand gesture signals are then processed\nwith time-frequency analysis. Based on these big databases of hand gesture, we\npropose a new machine learning architecture called deformable deep\nconvolutional generative adversarial network. Experimental results show the new\narchitecture can upgrade the recognition rate by 10% and the deformable kernel\ncan reduce the testing time cost by 30%.\n", "versions": [{"version": "v1", "created": "Mon, 6 Nov 2017 15:44:28 GMT"}, {"version": "v2", "created": "Wed, 22 Nov 2017 12:05:19 GMT"}], "update_date": "2017-11-23", "authors_parsed": [["Zhang", "Jiajun", ""], ["Shi", "Zhiguo", ""]]}, {"id": "1711.01970", "submitter": "Eirikur Agustsson", "authors": "Eirikur Agustsson, Alexander Sage, Radu Timofte, Luc Van Gool", "title": "Optimal transport maps for distribution preserving operations on latent\n  spaces of Generative Models", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Generative models such as Variational Auto Encoders (VAEs) and Generative\nAdversarial Networks (GANs) are typically trained for a fixed prior\ndistribution in the latent space, such as uniform or Gaussian. After a trained\nmodel is obtained, one can sample the Generator in various forms for\nexploration and understanding, such as interpolating between two samples,\nsampling in the vicinity of a sample or exploring differences between a pair of\nsamples applied to a third sample. In this paper, we show that the latent space\noperations used in the literature so far induce a distribution mismatch between\nthe resulting outputs and the prior distribution the model was trained on. To\naddress this, we propose to use distribution matching transport maps to ensure\nthat such latent space operations preserve the prior distribution, while\nminimally modifying the original operation. Our experimental results validate\nthat the proposed operations give higher quality samples compared to the\noriginal operations.\n", "versions": [{"version": "v1", "created": "Mon, 6 Nov 2017 15:51:29 GMT"}, {"version": "v2", "created": "Wed, 24 Jan 2018 21:55:52 GMT"}], "update_date": "2018-01-26", "authors_parsed": [["Agustsson", "Eirikur", ""], ["Sage", "Alexander", ""], ["Timofte", "Radu", ""], ["Van Gool", "Luc", ""]]}, {"id": "1711.02033", "submitter": "Siamak Ravanbakhsh", "authors": "Siamak Ravanbakhsh and Junier Oliva and Sebastien Fromenteau and Layne\n  C. Price and Shirley Ho and Jeff Schneider and Barnabas Poczos", "title": "Estimating Cosmological Parameters from the Dark Matter Distribution", "comments": "ICML 2016", "journal-ref": null, "doi": null, "report-no": null, "categories": "astro-ph.CO cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A grand challenge of the 21st century cosmology is to accurately estimate the\ncosmological parameters of our Universe. A major approach to estimating the\ncosmological parameters is to use the large-scale matter distribution of the\nUniverse. Galaxy surveys provide the means to map out cosmic large-scale\nstructure in three dimensions. Information about galaxy locations is typically\nsummarized in a \"single\" function of scale, such as the galaxy correlation\nfunction or power-spectrum. We show that it is possible to estimate these\ncosmological parameters directly from the distribution of matter. This paper\npresents the application of deep 3D convolutional networks to volumetric\nrepresentation of dark-matter simulations as well as the results obtained using\na recently proposed distribution regression framework, showing that machine\nlearning techniques are comparable to, and can sometimes outperform,\nmaximum-likelihood point estimates using \"cosmological models\". This opens the\nway to estimating the parameters of our Universe with higher accuracy.\n", "versions": [{"version": "v1", "created": "Mon, 6 Nov 2017 17:37:43 GMT"}], "update_date": "2017-11-07", "authors_parsed": [["Ravanbakhsh", "Siamak", ""], ["Oliva", "Junier", ""], ["Fromenteau", "Sebastien", ""], ["Price", "Layne C.", ""], ["Ho", "Shirley", ""], ["Schneider", "Jeff", ""], ["Poczos", "Barnabas", ""]]}, {"id": "1711.02036", "submitter": "Damian Straszak", "authors": "Damian Straszak and Nisheeth K. Vishnoi", "title": "Maximum Entropy Distributions: Bit Complexity and Stability", "comments": "To appear in COLT 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.IT math.IT math.OC stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Maximum entropy distributions with discrete support in $m$ dimensions arise\nin machine learning, statistics, information theory, and theoretical computer\nscience. While structural and computational properties of max-entropy\ndistributions have been extensively studied, basic questions such as: Do\nmax-entropy distributions over a large support (e.g., $2^m$) with a specified\nmarginal vector have succinct descriptions (polynomial-size in the input\ndescription)? and: Are entropy maximizing distributions \"stable\" under the\nperturbation of the marginal vector? have resisted a rigorous resolution.\n  Here we show that these questions are related and resolve both of them. Our\nmain result shows a ${\\rm poly}(m, \\log 1/\\varepsilon)$ bound on the bit\ncomplexity of $\\varepsilon$-optimal dual solutions to the maximum entropy\nconvex program -- for very general support sets and with no restriction on the\nmarginal vector. Applications of this result include polynomial time algorithms\nto compute max-entropy distributions over several new and old polytopes for any\nmarginal vector in a unified manner, a polynomial time algorithm to compute the\nBrascamp-Lieb constant in the rank-1 case. The proof of this result allows us\nto show that changing the marginal vector by $\\delta$ changes the max-entropy\ndistribution in the total variation distance roughly by a factor of ${\\rm\npoly}(m, \\log 1/\\delta)\\sqrt{\\delta}$ -- even when the size of the support set\nis exponential. Together, our results put max-entropy distributions on a\nmathematically sound footing -- these distributions are robust and\ncomputationally feasible models for data.\n", "versions": [{"version": "v1", "created": "Mon, 6 Nov 2017 17:42:18 GMT"}, {"version": "v2", "created": "Sun, 2 Jun 2019 20:26:05 GMT"}], "update_date": "2019-06-04", "authors_parsed": [["Straszak", "Damian", ""], ["Vishnoi", "Nisheeth K.", ""]]}, {"id": "1711.02037", "submitter": "N. Benjamin Erichson", "authors": "N. Benjamin Erichson and Ariana Mendible and Sophie Wihlborn and J.\n  Nathan Kutz", "title": "Randomized Nonnegative Matrix Factorization", "comments": "This is an extended and revised version of the paper which appeared\n  in JPRL", "journal-ref": "Pattern Recognition Letters, Volume 104, 2018, Pages 1-7", "doi": "10.1016/j.patrec.2018.01.007", "report-no": null, "categories": "stat.ML cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Nonnegative matrix factorization (NMF) is a powerful tool for data mining.\nHowever, the emergence of `big data' has severely challenged our ability to\ncompute this fundamental decomposition using deterministic algorithms. This\npaper presents a randomized hierarchical alternating least squares (HALS)\nalgorithm to compute the NMF. By deriving a smaller matrix from the nonnegative\ninput data, a more efficient nonnegative decomposition can be computed. Our\nalgorithm scales to big data applications while attaining a near-optimal\nfactorization. The proposed algorithm is evaluated using synthetic and real\nworld data and shows substantial speedups compared to deterministic HALS.\n", "versions": [{"version": "v1", "created": "Mon, 6 Nov 2017 17:42:47 GMT"}, {"version": "v2", "created": "Mon, 30 Apr 2018 19:20:32 GMT"}], "update_date": "2018-05-02", "authors_parsed": [["Erichson", "N. Benjamin", ""], ["Mendible", "Ariana", ""], ["Wihlborn", "Sophie", ""], ["Kutz", "J. Nathan", ""]]}, {"id": "1711.02038", "submitter": "Xun Gao", "authors": "Xun Gao, Zhengyu Zhang, Luming Duan", "title": "An efficient quantum algorithm for generative machine learning", "comments": "7+15 pages, 3+6 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "quant-ph cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A central task in the field of quantum computing is to find applications\nwhere quantum computer could provide exponential speedup over any classical\ncomputer. Machine learning represents an important field with broad\napplications where quantum computer may offer significant speedup. Several\nquantum algorithms for discriminative machine learning have been found based on\nefficient solving of linear algebraic problems, with potential exponential\nspeedup in runtime under the assumption of effective input from a quantum\nrandom access memory. In machine learning, generative models represent another\nlarge class which is widely used for both supervised and unsupervised learning.\nHere, we propose an efficient quantum algorithm for machine learning based on a\nquantum generative model. We prove that our proposed model is exponentially\nmore powerful to represent probability distributions compared with classical\ngenerative models and has exponential speedup in training and inference at\nleast for some instances under a reasonable assumption in computational\ncomplexity theory. Our result opens a new direction for quantum machine\nlearning and offers a remarkable example in which a quantum algorithm shows\nexponential improvement over any classical algorithm in an important\napplication field.\n", "versions": [{"version": "v1", "created": "Mon, 6 Nov 2017 17:44:03 GMT"}], "update_date": "2017-11-07", "authors_parsed": [["Gao", "Xun", ""], ["Zhang", "Zhengyu", ""], ["Duan", "Luming", ""]]}, {"id": "1711.02068", "submitter": "Sandeep Vidyapu", "authors": "Vidyapu Sandeep, V Vijaya Saradhi, Samit Bhattacharya", "title": "From Multimodal to Unimodal Webpages for Developing Countries", "comments": "Presented at NIPS 2017 Workshop on Machine Learning for the\n  Developing World", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The multimodal web elements such as text and images are associated with\ninherent memory costs to store and transfer over the Internet. With the limited\nnetwork connectivity in developing countries, webpage rendering gets delayed in\nthe presence of high-memory demanding elements such as images (relative to\ntext). To overcome this limitation, we propose a Canonical Correlation Analysis\n(CCA) based computational approach to replace high-cost modality with an\nequivalent low-cost modality. Our model learns a common subspace for low-cost\nand high-cost modalities that maximizes the correlation between their visual\nfeatures. The obtained common subspace is used for determining the low-cost\n(text) element of a given high-cost (image) element for the replacement. We\nanalyze the cost-saving performance of the proposed approach through an\neye-tracking experiment conducted on real-world webpages. Our approach reduces\nthe memory-cost by at least 83.35% by replacing images with text.\n", "versions": [{"version": "v1", "created": "Mon, 6 Nov 2017 18:32:59 GMT"}], "update_date": "2017-11-09", "authors_parsed": [["Sandeep", "Vidyapu", ""], ["Saradhi", "V Vijaya", ""], ["Bhattacharya", "Samit", ""]]}, {"id": "1711.02074", "submitter": "Dufan Wu", "authors": "Dufan Wu, Kyungsang Kim, Bin Dong, Georges El Fakhri and Quanzheng Li", "title": "End-to-end Lung Nodule Detection in Computed Tomography", "comments": "published at MLMI 2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Computer aided diagnostic (CAD) system is crucial for modern med-ical\nimaging. But almost all CAD systems operate on reconstructed images, which were\noptimized for radiologists. Computer vision can capture features that is subtle\nto human observers, so it is desirable to design a CAD system op-erating on the\nraw data. In this paper, we proposed a deep-neural-network-based detection\nsystem for lung nodule detection in computed tomography (CT). A\nprimal-dual-type deep reconstruction network was applied first to convert the\nraw data to the image space, followed by a 3-dimensional convolutional neural\nnetwork (3D-CNN) for the nodule detection. For efficient network training, the\ndeep reconstruction network and the CNN detector was trained sequentially\nfirst, then followed by one epoch of end-to-end fine tuning. The method was\nevaluated on the Lung Image Database Consortium image collection (LIDC-IDRI)\nwith simulated forward projections. With 144 multi-slice fanbeam pro-jections,\nthe proposed end-to-end detector could achieve comparable sensitivity with the\nreference detector, which was trained and applied on the fully-sampled image\ndata. It also demonstrated superior detection performance compared to detectors\ntrained on the reconstructed images. The proposed method is general and could\nbe expanded to most detection tasks in medical imaging.\n", "versions": [{"version": "v1", "created": "Mon, 6 Nov 2017 18:41:18 GMT"}, {"version": "v2", "created": "Tue, 2 Oct 2018 18:15:45 GMT"}], "update_date": "2018-10-04", "authors_parsed": [["Wu", "Dufan", ""], ["Kim", "Kyungsang", ""], ["Dong", "Bin", ""], ["Fakhri", "Georges El", ""], ["Li", "Quanzheng", ""]]}, {"id": "1711.02114", "submitter": "Thiago Serra", "authors": "Thiago Serra and Christian Tjandraatmadja and Srikumar Ramalingam", "title": "Bounding and Counting Linear Regions of Deep Neural Networks", "comments": "ICML 2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI cs.NE math.OC stat.ML", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  We investigate the complexity of deep neural networks (DNN) that represent\npiecewise linear (PWL) functions. In particular, we study the number of linear\nregions, i.e. pieces, that a PWL function represented by a DNN can attain, both\ntheoretically and empirically. We present (i) tighter upper and lower bounds\nfor the maximum number of linear regions on rectifier networks, which are exact\nfor inputs of dimension one; (ii) a first upper bound for multi-layer maxout\nnetworks; and (iii) a first method to perform exact enumeration or counting of\nthe number of regions by modeling the DNN with a mixed-integer linear\nformulation. These bounds come from leveraging the dimension of the space\ndefining each linear region. The results also indicate that a deep rectifier\nnetwork can only have more linear regions than every shallow counterpart with\nsame number of neurons if that number exceeds the dimension of the input.\n", "versions": [{"version": "v1", "created": "Mon, 6 Nov 2017 19:06:12 GMT"}, {"version": "v2", "created": "Sat, 6 Jan 2018 13:18:46 GMT"}, {"version": "v3", "created": "Sat, 9 Jun 2018 10:30:13 GMT"}, {"version": "v4", "created": "Sun, 16 Sep 2018 01:36:41 GMT"}], "update_date": "2018-09-18", "authors_parsed": [["Serra", "Thiago", ""], ["Tjandraatmadja", "Christian", ""], ["Ramalingam", "Srikumar", ""]]}, {"id": "1711.02159", "submitter": "Anirban Roychowdhury", "authors": "Anirban Roychowdhury and Srinivasan Parthasarathy", "title": "Adaptive Bayesian Sampling with Monte Carlo EM", "comments": "In Proc. 30th Advances in Neural Information Processing Systems\n  (NIPS), 2017 (to appear)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a novel technique for learning the mass matrices in samplers\nobtained from discretized dynamics that preserve some energy function. Existing\nadaptive samplers use Riemannian preconditioning techniques, where the mass\nmatrices are functions of the parameters being sampled. This leads to\nsignificant complexities in the energy reformulations and resultant dynamics,\noften leading to implicit systems of equations and requiring inversion of\nhigh-dimensional matrices in the leapfrog steps. Our approach provides a\nsimpler alternative, by using existing dynamics in the sampling step of a Monte\nCarlo EM framework, and learning the mass matrices in the M step with a novel\nonline technique. We also propose a way to adaptively set the number of samples\ngathered in the E step, using sampling error estimates from the leapfrog\ndynamics. Along with a novel stochastic sampler based on Nos\\'{e}-Poincar\\'{e}\ndynamics, we use this framework with standard Hamiltonian Monte Carlo (HMC) as\nwell as newer stochastic algorithms such as SGHMC and SGNHT, and show strong\nperformance on synthetic and real high-dimensional sampling scenarios; we\nachieve sampling accuracies comparable to Riemannian samplers while being\nsignificantly faster.\n", "versions": [{"version": "v1", "created": "Mon, 6 Nov 2017 20:23:24 GMT"}], "update_date": "2017-11-08", "authors_parsed": [["Roychowdhury", "Anirban", ""], ["Parthasarathy", "Srinivasan", ""]]}, {"id": "1711.02195", "submitter": "Hunter Lang", "authors": "Hunter Lang, David Sontag, Aravindan Vijayaraghavan", "title": "Optimality of Approximate Inference Algorithms on Stable Instances", "comments": "13 pages, 2 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.AI cs.DS cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Approximate algorithms for structured prediction problems---such as LP\nrelaxations and the popular alpha-expansion algorithm (Boykov et al.\n2001)---typically far exceed their theoretical performance guarantees on\nreal-world instances. These algorithms often find solutions that are very close\nto optimal. The goal of this paper is to partially explain the performance of\nalpha-expansion and an LP relaxation algorithm on MAP inference in\nFerromagnetic Potts models (FPMs). Our main results give stability conditions\nunder which these two algorithms provably recover the optimal MAP solution.\nThese theoretical results complement numerous empirical observations of good\nperformance.\n", "versions": [{"version": "v1", "created": "Mon, 6 Nov 2017 22:14:34 GMT"}, {"version": "v2", "created": "Mon, 23 Apr 2018 16:02:44 GMT"}], "update_date": "2018-04-24", "authors_parsed": [["Lang", "Hunter", ""], ["Sontag", "David", ""], ["Vijayaraghavan", "Aravindan", ""]]}, {"id": "1711.02198", "submitter": "Mina Karzand", "authors": "Guy Bresler and Mina Karzand", "title": "Regret Bounds and Regimes of Optimality for User-User and Item-Item\n  Collaborative Filtering", "comments": "51 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.IT cs.LG math.IT", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider an online model for recommendation systems, with each user being\nrecommended an item at each time-step and providing 'like' or 'dislike'\nfeedback. Each user may be recommended a given item at most once. A latent\nvariable model specifies the user preferences: both users and items are\nclustered into types. All users of a given type have identical preferences for\nthe items, and similarly, items of a given type are either all liked or all\ndisliked by a given user. We assume that the matrix encoding the preferences of\neach user type for each item type is randomly generated; in this way, the model\ncaptures structure in both the item and user spaces, the amount of structure\ndepending on the number of each of the types. The measure of performance of the\nrecommendation system is the expected number of disliked recommendations per\nuser, defined as expected regret. We propose two algorithms inspired by\nuser-user and item-item collaborative filtering (CF), modified to explicitly\nmake exploratory recommendations, and prove performance guarantees in terms of\ntheir expected regret. For two regimes of model parameters, with structure only\nin item space or only in user space, we prove information-theoretic lower\nbounds on regret that match our upper bounds up to logarithmic factors. Our\nanalysis elucidates system operating regimes in which existing CF algorithms\nare nearly optimal.\n", "versions": [{"version": "v1", "created": "Mon, 6 Nov 2017 22:25:43 GMT"}, {"version": "v2", "created": "Tue, 7 May 2019 17:40:59 GMT"}], "update_date": "2019-05-08", "authors_parsed": [["Bresler", "Guy", ""], ["Karzand", "Mina", ""]]}, {"id": "1711.02209", "submitter": "Aren Jansen", "authors": "Aren Jansen, Manoj Plakal, Ratheet Pandya, Daniel P. W. Ellis, Shawn\n  Hershey, Jiayang Liu, R. Channing Moore, Rif A. Saurous", "title": "Unsupervised Learning of Semantic Audio Representations", "comments": "Submitted to ICASSP 2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.SD eess.AS stat.ML", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Even in the absence of any explicit semantic annotation, vast collections of\naudio recordings provide valuable information for learning the categorical\nstructure of sounds. We consider several class-agnostic semantic constraints\nthat apply to unlabeled nonspeech audio: (i) noise and translations in time do\nnot change the underlying sound category, (ii) a mixture of two sound events\ninherits the categories of the constituents, and (iii) the categories of events\nin close temporal proximity are likely to be the same or related. Without\nlabels to ground them, these constraints are incompatible with classification\nloss functions. However, they may still be leveraged to identify geometric\ninequalities needed for triplet loss-based training of convolutional neural\nnetworks. The result is low-dimensional embeddings of the input spectrograms\nthat recover 41% and 84% of the performance of their fully-supervised\ncounterparts when applied to downstream query-by-example sound retrieval and\nsound event classification tasks, respectively. Moreover, in\nlimited-supervision settings, our unsupervised embeddings double the\nstate-of-the-art classification performance.\n", "versions": [{"version": "v1", "created": "Mon, 6 Nov 2017 22:54:01 GMT"}], "update_date": "2017-11-08", "authors_parsed": [["Jansen", "Aren", ""], ["Plakal", "Manoj", ""], ["Pandya", "Ratheet", ""], ["Ellis", "Daniel P. W.", ""], ["Hershey", "Shawn", ""], ["Liu", "Jiayang", ""], ["Moore", "R. Channing", ""], ["Saurous", "Rif A.", ""]]}, {"id": "1711.02213", "submitter": "Xin Wang", "authors": "Urs K\\\"oster, Tristan J. Webb, Xin Wang, Marcel Nassar, Arjun K.\n  Bansal, William H. Constable, O\\u{g}uz H. Elibol, Scott Gray, Stewart Hall,\n  Luke Hornof, Amir Khosrowshahi, Carey Kloss, Ruby J. Pai, Naveen Rao", "title": "Flexpoint: An Adaptive Numerical Format for Efficient Training of Deep\n  Neural Networks", "comments": "14 pages, 5 figures, accepted in Neural Information Processing\n  Systems 2017", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.NA stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Deep neural networks are commonly developed and trained in 32-bit floating\npoint format. Significant gains in performance and energy efficiency could be\nrealized by training and inference in numerical formats optimized for deep\nlearning. Despite advances in limited precision inference in recent years,\ntraining of neural networks in low bit-width remains a challenging problem.\nHere we present the Flexpoint data format, aiming at a complete replacement of\n32-bit floating point format training and inference, designed to support modern\ndeep network topologies without modifications. Flexpoint tensors have a shared\nexponent that is dynamically adjusted to minimize overflows and maximize\navailable dynamic range. We validate Flexpoint by training AlexNet, a deep\nresidual network and a generative adversarial network, using a simulator\nimplemented with the neon deep learning framework. We demonstrate that 16-bit\nFlexpoint closely matches 32-bit floating point in training all three models,\nwithout any need for tuning of model hyperparameters. Our results suggest\nFlexpoint as a promising numerical format for future hardware for training and\ninference.\n", "versions": [{"version": "v1", "created": "Mon, 6 Nov 2017 23:05:10 GMT"}, {"version": "v2", "created": "Sat, 2 Dec 2017 17:08:50 GMT"}], "update_date": "2017-12-05", "authors_parsed": [["K\u00f6ster", "Urs", ""], ["Webb", "Tristan J.", ""], ["Wang", "Xin", ""], ["Nassar", "Marcel", ""], ["Bansal", "Arjun K.", ""], ["Constable", "William H.", ""], ["Elibol", "O\u011fuz H.", ""], ["Gray", "Scott", ""], ["Hall", "Stewart", ""], ["Hornof", "Luke", ""], ["Khosrowshahi", "Amir", ""], ["Kloss", "Carey", ""], ["Pai", "Ruby J.", ""], ["Rao", "Naveen", ""]]}, {"id": "1711.02226", "submitter": "Tatsunori Hashimoto", "authors": "Tatsunori B. Hashimoto, John C. Duchi, Percy Liang", "title": "Unsupervised Transformation Learning via Convex Relaxations", "comments": "To appear at NIPS 2017", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Our goal is to extract meaningful transformations from raw images, such as\nvarying the thickness of lines in handwriting or the lighting in a portrait. We\npropose an unsupervised approach to learn such transformations by attempting to\nreconstruct an image from a linear combination of transformations of its\nnearest neighbors. On handwritten digits and celebrity portraits, we show that\neven with linear transformations, our method generates visually high-quality\nmodified images. Moreover, since our method is semiparametric and does not\nmodel the data distribution, the learned transformations extrapolate off the\ntraining data and can be applied to new types of images.\n", "versions": [{"version": "v1", "created": "Mon, 6 Nov 2017 23:56:41 GMT"}], "update_date": "2017-11-08", "authors_parsed": [["Hashimoto", "Tatsunori B.", ""], ["Duchi", "John C.", ""], ["Liang", "Percy", ""]]}, {"id": "1711.02282", "submitter": "Nan Rosemary Ke", "authors": "Anirudh Goyal, Nan Rosemary Ke, Surya Ganguli, Yoshua Bengio", "title": "Variational Walkback: Learning a Transition Operator as a Stochastic\n  Recurrent Net", "comments": "To appear at NIPS 2017", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a novel method to directly learn a stochastic transition operator\nwhose repeated application provides generated samples. Traditional undirected\ngraphical models approach this problem indirectly by learning a Markov chain\nmodel whose stationary distribution obeys detailed balance with respect to a\nparameterized energy function. The energy function is then modified so the\nmodel and data distributions match, with no guarantee on the number of steps\nrequired for the Markov chain to converge. Moreover, the detailed balance\ncondition is highly restrictive: energy based models corresponding to neural\nnetworks must have symmetric weights, unlike biological neural circuits. In\ncontrast, we develop a method for directly learning arbitrarily parameterized\ntransition operators capable of expressing non-equilibrium stationary\ndistributions that violate detailed balance, thereby enabling us to learn more\nbiologically plausible asymmetric neural networks and more general non-energy\nbased dynamical systems. The proposed training objective, which we derive via\nprincipled variational methods, encourages the transition operator to \"walk\nback\" in multi-step trajectories that start at data-points, as quickly as\npossible back to the original data points. We present a series of experimental\nresults illustrating the soundness of the proposed approach, Variational\nWalkback (VW), on the MNIST, CIFAR-10, SVHN and CelebA datasets, demonstrating\nsuperior samples compared to earlier attempts to learn a transition operator.\nWe also show that although each rapid training trajectory is limited to a\nfinite but variable number of steps, our transition operator continues to\ngenerate good samples well past the length of such trajectories, thereby\ndemonstrating the match of its non-equilibrium stationary distribution to the\ndata distribution. Source Code: http://github.com/anirudh9119/walkback_nips17\n", "versions": [{"version": "v1", "created": "Tue, 7 Nov 2017 04:45:13 GMT"}], "update_date": "2017-11-08", "authors_parsed": [["Goyal", "Anirudh", ""], ["Ke", "Nan Rosemary", ""], ["Ganguli", "Surya", ""], ["Bengio", "Yoshua", ""]]}, {"id": "1711.02283", "submitter": "Vivien Seguy", "authors": "Vivien Seguy, Bharath Bhushan Damodaran, R\\'emi Flamary, Nicolas\n  Courty, Antoine Rolet and Mathieu Blondel", "title": "Large-Scale Optimal Transport and Mapping Estimation", "comments": "15 pages, 4 figures. To appear in the Proceedings of the\n  International Conference on Learning Representations (ICLR) 2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper presents a novel two-step approach for the fundamental problem of\nlearning an optimal map from one distribution to another. First, we learn an\noptimal transport (OT) plan, which can be thought as a one-to-many map between\nthe two distributions. To that end, we propose a stochastic dual approach of\nregularized OT, and show empirically that it scales better than a recent\nrelated approach when the amount of samples is very large. Second, we estimate\na \\textit{Monge map} as a deep neural network learned by approximating the\nbarycentric projection of the previously-obtained OT plan. This\nparameterization allows generalization of the mapping outside the support of\nthe input measure. We prove two theoretical stability results of regularized OT\nwhich show that our estimations converge to the OT plan and Monge map between\nthe underlying continuous measures. We showcase our proposed approach on two\napplications: domain adaptation and generative modeling.\n", "versions": [{"version": "v1", "created": "Tue, 7 Nov 2017 04:53:07 GMT"}, {"version": "v2", "created": "Mon, 26 Feb 2018 01:25:22 GMT"}], "update_date": "2018-02-27", "authors_parsed": [["Seguy", "Vivien", ""], ["Damodaran", "Bharath Bhushan", ""], ["Flamary", "R\u00e9mi", ""], ["Courty", "Nicolas", ""], ["Rolet", "Antoine", ""], ["Blondel", "Mathieu", ""]]}, {"id": "1711.02301", "submitter": "Maithra Raghu", "authors": "Maithra Raghu, Alex Irpan, Jacob Andreas, Robert Kleinberg, Quoc V.\n  Le, Jon Kleinberg", "title": "Can Deep Reinforcement Learning Solve Erdos-Selfridge-Spencer Games?", "comments": "Accepted to ICML 2018, code opensourced at:\n  https://github.com/rubai5/ESS_Game", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.NE stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Deep reinforcement learning has achieved many recent successes, but our\nunderstanding of its strengths and limitations is hampered by the lack of rich\nenvironments in which we can fully characterize optimal behavior, and\ncorrespondingly diagnose individual actions against such a characterization.\nHere we consider a family of combinatorial games, arising from work of Erdos,\nSelfridge, and Spencer, and we propose their use as environments for evaluating\nand comparing different approaches to reinforcement learning. These games have\na number of appealing features: they are challenging for current learning\napproaches, but they form (i) a low-dimensional, simply parametrized\nenvironment where (ii) there is a linear closed form solution for optimal\nbehavior from any state, and (iii) the difficulty of the game can be tuned by\nchanging environment parameters in an interpretable way. We use these\nErdos-Selfridge-Spencer games not only to compare different algorithms, but\ntest for generalization, make comparisons to supervised learning, analyse\nmultiagent play, and even develop a self play algorithm. Code can be found at:\nhttps://github.com/rubai5/ESS_Game\n", "versions": [{"version": "v1", "created": "Tue, 7 Nov 2017 06:16:56 GMT"}, {"version": "v2", "created": "Sun, 17 Dec 2017 00:51:17 GMT"}, {"version": "v3", "created": "Mon, 5 Mar 2018 21:05:00 GMT"}, {"version": "v4", "created": "Wed, 27 Jun 2018 03:24:25 GMT"}, {"version": "v5", "created": "Fri, 29 Jun 2018 00:18:48 GMT"}], "update_date": "2018-07-02", "authors_parsed": [["Raghu", "Maithra", ""], ["Irpan", "Alex", ""], ["Andreas", "Jacob", ""], ["Kleinberg", "Robert", ""], ["Le", "Quoc V.", ""], ["Kleinberg", "Jon", ""]]}, {"id": "1711.02305", "submitter": "Kai Sheng Tai", "authors": "Kai Sheng Tai, Vatsal Sharan, Peter Bailis, Gregory Valiant", "title": "Sketching Linear Classifiers over Data Streams", "comments": "Full version of paper appearing at SIGMOD 2018 with more detailed\n  proofs of theoretical results. Code available at\n  https://github.com/stanford-futuredata/wmsketch", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.DS stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We introduce a new sub-linear space sketch---the Weight-Median Sketch---for\nlearning compressed linear classifiers over data streams while supporting the\nefficient recovery of large-magnitude weights in the model. This enables\nmemory-limited execution of several statistical analyses over streams,\nincluding online feature selection, streaming data explanation, relative\ndeltoid detection, and streaming estimation of pointwise mutual information.\nUnlike related sketches that capture the most frequently-occurring features (or\nitems) in a data stream, the Weight-Median Sketch captures the features that\nare most discriminative of one stream (or class) compared to another. The\nWeight-Median Sketch adopts the core data structure used in the Count-Sketch,\nbut, instead of sketching counts, it captures sketched gradient updates to the\nmodel parameters. We provide a theoretical analysis that establishes recovery\nguarantees for batch and online learning, and demonstrate empirical\nimprovements in memory-accuracy trade-offs over alternative memory-budgeted\nmethods, including count-based sketches and feature hashing.\n", "versions": [{"version": "v1", "created": "Tue, 7 Nov 2017 06:37:27 GMT"}, {"version": "v2", "created": "Fri, 6 Apr 2018 18:08:41 GMT"}], "update_date": "2018-04-10", "authors_parsed": [["Tai", "Kai Sheng", ""], ["Sharan", "Vatsal", ""], ["Bailis", "Peter", ""], ["Valiant", "Gregory", ""]]}, {"id": "1711.02309", "submitter": "Vatsal Sharan", "authors": "Vatsal Sharan, Sham Kakade, Percy Liang, Gregory Valiant", "title": "Learning Overcomplete HMMs", "comments": "Added acknowledgements", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study the problem of learning overcomplete HMMs---those that have many\nhidden states but a small output alphabet. Despite having significant practical\nimportance, such HMMs are poorly understood with no known positive or negative\nresults for efficient learning. In this paper, we present several new\nresults---both positive and negative---which help define the boundaries between\nthe tractable and intractable settings. Specifically, we show positive results\nfor a large subclass of HMMs whose transition matrices are sparse,\nwell-conditioned, and have small probability mass on short cycles. On the other\nhand, we show that learning is impossible given only a polynomial number of\nsamples for HMMs with a small output alphabet and whose transition matrices are\nrandom regular graphs with large degree. We also discuss these results in the\ncontext of learning HMMs which can capture long-term dependencies.\n", "versions": [{"version": "v1", "created": "Tue, 7 Nov 2017 06:55:03 GMT"}, {"version": "v2", "created": "Thu, 28 Jun 2018 01:49:33 GMT"}], "update_date": "2018-06-29", "authors_parsed": [["Sharan", "Vatsal", ""], ["Kakade", "Sham", ""], ["Liang", "Percy", ""], ["Valiant", "Gregory", ""]]}, {"id": "1711.02317", "submitter": "Lilian Besson", "authors": "Lilian Besson (IETR, SEQUEL), Emilie Kaufmann (CRIStAL, SEQUEL)", "title": "Multi-Player Bandits Revisited", "comments": null, "journal-ref": "Algorithmic Learning Theory, Apr 2018, Lanzarote, Spain. 2018,\n  http://www.cs.cornell.edu/conferences/alt2018/", "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Multi-player Multi-Armed Bandits (MAB) have been extensively studied in the\nliterature, motivated by applications to Cognitive Radio systems. Driven by\nsuch applications as well, we motivate the introduction of several levels of\nfeedback for multi-player MAB algorithms. Most existing work assume that\nsensing information is available to the algorithm. Under this assumption, we\nimprove the state-of-the-art lower bound for the regret of any decentralized\nalgorithms and introduce two algorithms, RandTopM and MCTopM, that are shown to\nempirically outperform existing algorithms. Moreover, we provide strong\ntheoretical guarantees for these algorithms, including a notion of asymptotic\noptimality in terms of the number of selections of bad arms. We then introduce\na promising heuristic, called Selfish, that can operate without sensing\ninformation, which is crucial for emerging applications to Internet of Things\nnetworks. We investigate the empirical performance of this algorithm and\nprovide some first theoretical elements for the understanding of its behavior.\n", "versions": [{"version": "v1", "created": "Tue, 7 Nov 2017 07:10:47 GMT"}, {"version": "v2", "created": "Wed, 7 Mar 2018 14:41:15 GMT"}, {"version": "v3", "created": "Tue, 13 Mar 2018 07:30:42 GMT"}], "update_date": "2019-04-30", "authors_parsed": [["Besson", "Lilian", "", "IETR, SEQUEL"], ["Kaufmann", "Emilie", "", "CRIStAL, SEQUEL"]]}, {"id": "1711.02326", "submitter": "Nan Rosemary Ke", "authors": "Nan Rosemary Ke, Anirudh Goyal, Olexa Bilaniuk, Jonathan Binas,\n  Laurent Charlin, Chris Pal, Yoshua Bengio", "title": "Sparse Attentive Backtracking: Long-Range Credit Assignment in Recurrent\n  Networks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.LG cs.NE stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A major drawback of backpropagation through time (BPTT) is the difficulty of\nlearning long-term dependencies, coming from having to propagate credit\ninformation backwards through every single step of the forward computation.\nThis makes BPTT both computationally impractical and biologically implausible.\nFor this reason, full backpropagation through time is rarely used on long\nsequences, and truncated backpropagation through time is used as a heuristic.\nHowever, this usually leads to biased estimates of the gradient in which longer\nterm dependencies are ignored. Addressing this issue, we propose an alternative\nalgorithm, Sparse Attentive Backtracking, which might also be related to\nprinciples used by brains to learn long-term dependencies. Sparse Attentive\nBacktracking learns an attention mechanism over the hidden states of the past\nand selectively backpropagates through paths with high attention weights. This\nallows the model to learn long term dependencies while only backtracking for a\nsmall number of time steps, not just from the recent past but also from\nattended relevant past states.\n", "versions": [{"version": "v1", "created": "Tue, 7 Nov 2017 07:52:12 GMT"}], "update_date": "2017-11-08", "authors_parsed": [["Ke", "Nan Rosemary", ""], ["Goyal", "Anirudh", ""], ["Bilaniuk", "Olexa", ""], ["Binas", "Jonathan", ""], ["Charlin", "Laurent", ""], ["Pal", "Chris", ""], ["Bengio", "Yoshua", ""]]}, {"id": "1711.02329", "submitter": "Reza Abbasi-Asl", "authors": "Reza Abbasi-Asl, Bin Yu", "title": "Interpreting Convolutional Neural Networks Through Compression", "comments": "Presented at NIPS 2017 Symposium on Interpretable Machine Learning", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Convolutional neural networks (CNNs) achieve state-of-the-art performance in\na wide variety of tasks in computer vision. However, interpreting CNNs still\nremains a challenge. This is mainly due to the large number of parameters in\nthese networks. Here, we investigate the role of compression and particularly\npruning filters in the interpretation of CNNs. We exploit our recently-proposed\ngreedy structural compression scheme that prunes filters in a trained CNN. In\nour compression, the filter importance index is defined as the classification\naccuracy reduction (CAR) of the network after pruning that filter. The filters\nare then iteratively pruned based on the CAR index. We demonstrate the\ninterpretability of CAR-compressed CNNs by showing that our algorithm prunes\nfilters with visually redundant pattern selectivity. Specifically, we show the\nimportance of shape-selective filters for object recognition, as opposed to\ncolor-selective filters. Out of top 20 CAR-pruned filters in AlexNet, 17 of\nthem in the first layer and 14 of them in the second layer are color-selective\nfilters. Finally, we introduce a variant of our CAR importance index that\nquantifies the importance of each image class to each CNN filter. We show that\nthe most and the least important class labels present a meaningful\ninterpretation of each filter that is consistent with the visualized pattern\nselectivity of that filter.\n", "versions": [{"version": "v1", "created": "Tue, 7 Nov 2017 08:10:52 GMT"}], "update_date": "2017-11-08", "authors_parsed": [["Abbasi-Asl", "Reza", ""], ["Yu", "Bin", ""]]}, {"id": "1711.02361", "submitter": "Kristiaan Pelckmans", "authors": "Kristiaan Pelckmans", "title": "FADO: A Deterministic Detection/Learning Algorithm", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper proposes and studies a detection technique for adversarial\nscenarios (dubbed deterministic detection). This technique provides an\nalternative detection methodology in case the usual stochastic methods are not\napplicable: this can be because the studied phenomenon does not follow a\nstochastic sampling scheme, samples are high-dimensional and subsequent\nmultiple-testing corrections render results overly conservative, sample sizes\nare too low for asymptotic results (as e.g. the central limit theorem) to kick\nin, or one cannot allow for the small probability of failure inherent to\nstochastic approaches. This paper instead designs a method based on insights\nfrom machine learning and online learning theory: this detection algorithm -\nnamed Online FAult Detection (FADO) - comes with theoretical guarantees of its\ndetection capabilities. A version of the margin is found to regulate the\ndetection performance of FADO. A precise expression is derived for bounding the\nperformance, and experimental results are presented assessing the influence of\ninvolved quantities. A case study of scene detection is used to illustrate the\napproach. The technology is closely related to the linear perceptron rule,\ninherits its computational attractiveness and flexibility towards various\nextensions.\n", "versions": [{"version": "v1", "created": "Tue, 7 Nov 2017 09:57:44 GMT"}], "update_date": "2017-11-08", "authors_parsed": [["Pelckmans", "Kristiaan", ""]]}, {"id": "1711.02368", "submitter": "Masato Asahara", "authors": "Masato Asahara and Ryohei Fujimaki", "title": "Distributed Bayesian Piecewise Sparse Linear Models", "comments": "Short version of this paper will be published in IEEE BigData 2017", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.DC cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The importance of interpretability of machine learning models has been\nincreasing due to emerging enterprise predictive analytics, threat of data\nprivacy, accountability of artificial intelligence in society, and so on.\nPiecewise linear models have been actively studied to achieve both accuracy and\ninterpretability. They often produce competitive accuracy against\nstate-of-the-art non-linear methods. In addition, their representations (i.e.,\nrule-based segmentation plus sparse linear formula) are often preferred by\ndomain experts. A disadvantage of such models, however, is high computational\ncost for simultaneous determinations of the number of \"pieces\" and cardinality\nof each linear predictor, which has restricted their applicability to\nmiddle-scale data sets. This paper proposes a distributed factorized asymptotic\nBayesian (FAB) inference of learning piece-wise sparse linear models on\ndistributed memory architectures. The distributed FAB inference solves the\nsimultaneous model selection issue without communicating $O(N)$ data where N is\nthe number of training samples and achieves linear scale-out against the number\nof CPU cores. Experimental results demonstrate that the distributed FAB\ninference achieves high prediction accuracy and performance scalability with\nboth synthetic and benchmark data.\n", "versions": [{"version": "v1", "created": "Tue, 7 Nov 2017 10:05:31 GMT"}], "update_date": "2017-11-08", "authors_parsed": [["Asahara", "Masato", ""], ["Fujimaki", "Ryohei", ""]]}, {"id": "1711.02391", "submitter": "Viivi Uurtio MSc", "authors": "Viivi Uurtio, Jo\\~ao M. Monteiro, Jaz Kandola, John Shawe-Taylor,\n  Delmiro Fernandez-Reyes, and Juho Rousu", "title": "A Tutorial on Canonical Correlation Methods", "comments": "33 pages", "journal-ref": "ACM Computing Surveys, Vol. 50, No. 6, Article 95. Publication\n  date: October 2017", "doi": "10.1145/3136624", "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Canonical correlation analysis is a family of multivariate statistical\nmethods for the analysis of paired sets of variables. Since its proposition,\ncanonical correlation analysis has for instance been extended to extract\nrelations between two sets of variables when the sample size is insufficient in\nrelation to the data dimensionality, when the relations have been considered to\nbe non-linear, and when the dimensionality is too large for human\ninterpretation. This tutorial explains the theory of canonical correlation\nanalysis including its regularised, kernel, and sparse variants. Additionally,\nthe deep and Bayesian CCA extensions are briefly reviewed. Together with the\nnumerical examples, this overview provides a coherent compendium on the\napplicability of the variants of canonical correlation analysis. By bringing\ntogether techniques for solving the optimisation problems, evaluating the\nstatistical significance and generalisability of the canonical correlation\nmodel, and interpreting the relations, we hope that this article can serve as a\nhands-on tool for applying canonical correlation methods in data analysis.\n", "versions": [{"version": "v1", "created": "Tue, 7 Nov 2017 11:01:00 GMT"}], "update_date": "2017-11-08", "authors_parsed": [["Uurtio", "Viivi", ""], ["Monteiro", "Jo\u00e3o M.", ""], ["Kandola", "Jaz", ""], ["Shawe-Taylor", "John", ""], ["Fernandez-Reyes", "Delmiro", ""], ["Rousu", "Juho", ""]]}, {"id": "1711.02421", "submitter": "Amichai Painsky", "authors": "Amichai Painsky and Naftali Tishby", "title": "Gaussian Lower Bound for the Information Bottleneck Limit", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The Information Bottleneck (IB) is a conceptual method for extracting the\nmost compact, yet informative, representation of a set of variables, with\nrespect to the target. It generalizes the notion of minimal sufficient\nstatistics from classical parametric statistics to a broader\ninformation-theoretic sense. The IB curve defines the optimal trade-off between\nrepresentation complexity and its predictive power. Specifically, it is\nachieved by minimizing the level of mutual information (MI) between the\nrepresentation and the original variables, subject to a minimal level of MI\nbetween the representation and the target. This problem is shown to be in\ngeneral NP hard. One important exception is the multivariate Gaussian case, for\nwhich the Gaussian IB (GIB) is known to obtain an analytical closed form\nsolution, similar to Canonical Correlation Analysis (CCA). In this work we\nintroduce a Gaussian lower bound to the IB curve; we find an embedding of the\ndata which maximizes its \"Gaussian part\", on which we apply the GIB. This\nembedding provides an efficient (and practical) representation of any arbitrary\ndata-set (in the IB sense), which in addition holds the favorable properties of\na Gaussian distribution. Importantly, we show that the optimal Gaussian\nembedding is bounded from above by non-linear CCA. This allows a fundamental\nlimit for our ability to Gaussianize arbitrary data-sets and solve complex\nproblems by linear methods.\n", "versions": [{"version": "v1", "created": "Tue, 7 Nov 2017 11:58:37 GMT"}], "update_date": "2017-11-08", "authors_parsed": [["Painsky", "Amichai", ""], ["Tishby", "Naftali", ""]]}, {"id": "1711.02448", "submitter": "Rui Ponte Costa", "authors": "Rui Ponte Costa, Yannis M. Assael, Brendan Shillingford, Nando de\n  Freitas and Tim P. Vogels", "title": "Cortical microcircuits as gated-recurrent neural networks", "comments": "To appear in Advances in Neural Information Processing Systems 30\n  (NIPS 2017). 13 pages, 2 figures (and 1 supp. figure)", "journal-ref": null, "doi": null, "report-no": null, "categories": "q-bio.NC cs.NE stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Cortical circuits exhibit intricate recurrent architectures that are\nremarkably similar across different brain areas. Such stereotyped structure\nsuggests the existence of common computational principles. However, such\nprinciples have remained largely elusive. Inspired by gated-memory networks,\nnamely long short-term memory networks (LSTMs), we introduce a recurrent neural\nnetwork in which information is gated through inhibitory cells that are\nsubtractive (subLSTM). We propose a natural mapping of subLSTMs onto known\ncanonical excitatory-inhibitory cortical microcircuits. Our empirical\nevaluation across sequential image classification and language modelling tasks\nshows that subLSTM units can achieve similar performance to LSTM units. These\nresults suggest that cortical circuits can be optimised to solve complex\ncontextual problems and proposes a novel view on their computational function.\nOverall our work provides a step towards unifying recurrent networks as used in\nmachine learning with their biological counterparts.\n", "versions": [{"version": "v1", "created": "Tue, 7 Nov 2017 13:03:35 GMT"}, {"version": "v2", "created": "Wed, 3 Jan 2018 17:29:28 GMT"}], "update_date": "2018-01-04", "authors_parsed": [["Costa", "Rui Ponte", ""], ["Assael", "Yannis M.", ""], ["Shillingford", "Brendan", ""], ["de Freitas", "Nando", ""], ["Vogels", "Tim P.", ""]]}, {"id": "1711.02475", "submitter": "Constantin Grigo", "authors": "Constantin Grigo and Phaedon-Stelios Koutsourelakis", "title": "Bayesian model and dimension reduction for uncertainty propagation:\n  applications in random media", "comments": "31 pages, 12 figures", "journal-ref": "SIAM/ASA Journal on Uncertainty Quantification 2019 7:1, 292-323", "doi": "10.1137/17M1155867", "report-no": null, "categories": "stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Well-established methods for the solution of stochastic partial differential\nequations (SPDEs) typically struggle in problems with high-dimensional\ninputs/outputs. Such difficulties are only amplified in large-scale\napplications where even a few tens of full-order model runs are impracticable.\nWhile dimensionality reduction can alleviate some of these issues, it is not\nknown which and how many features of the (high-dimensional) input are actually\npredictive of the (high-dimensional) output. In this paper, we advocate a\nBayesian formulation that is capable of performing simultaneous dimension and\nmodel-order reduction. It consists of a component that encodes the\nhigh-dimensional input into a low-dimensional set of feature functions by\nemploying sparsity-enforcing priors and a decoding component that makes use of\nthe solution of a coarse-grained model in order to reconstruct that of the\nfull-order model. Both components are represented with latent variables in a\nprobabilistic graphical model and are simultaneously trained using Stochastic\nVariational Inference methods. The model is capable of quantifying the\npredictive uncertainty due to the information loss that unavoidably takes place\nin any model-order/dimension reduction as well as the uncertainty arising from\nfinite-sized training datasets. We demonstrate its capabilities in the context\nof random media where fine-scale fluctuations can give rise to random inputs\nwith tens of thousands of variables. With a few tens of full-order model\nsimulations, the proposed model is capable of identifying salient physical\nfeatures and produce sharp predictions under different boundary conditions of\nthe full output which itself consists of thousands of components.\n", "versions": [{"version": "v1", "created": "Tue, 7 Nov 2017 14:19:24 GMT"}, {"version": "v2", "created": "Wed, 28 Mar 2018 15:41:05 GMT"}], "update_date": "2019-09-10", "authors_parsed": [["Grigo", "Constantin", ""], ["Koutsourelakis", "Phaedon-Stelios", ""]]}, {"id": "1711.02478", "submitter": "Shin Matsushima", "authors": "Taito Lee, Shin Matsushima, Kenji Yamanishi", "title": "Grafting for Combinatorial Boolean Model using Frequent Itemset Mining", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper introduces the combinatorial Boolean model (CBM), which is defined\nas the class of linear combinations of conjunctions of Boolean attributes. This\npaper addresses the issue of learning CBM from labeled data. CBM is of high\nknowledge interpretability but na\\\"{i}ve learning of it requires exponentially\nlarge computation time with respect to data dimension and sample size. To\novercome this computational difficulty, we propose an algorithm GRAB (GRAfting\nfor Boolean datasets), which efficiently learns CBM within the\n$L_1$-regularized loss minimization framework. The key idea of GRAB is to\nreduce the loss minimization problem to the weighted frequent itemset mining,\nin which frequent patterns are efficiently computable. We employ benchmark\ndatasets to empirically demonstrate that GRAB is effective in terms of\ncomputational efficiency, prediction accuracy and knowledge discovery.\n", "versions": [{"version": "v1", "created": "Tue, 7 Nov 2017 14:21:36 GMT"}, {"version": "v2", "created": "Mon, 13 Nov 2017 11:03:30 GMT"}], "update_date": "2017-11-15", "authors_parsed": [["Lee", "Taito", ""], ["Matsushima", "Shin", ""], ["Yamanishi", "Kenji", ""]]}, {"id": "1711.02515", "submitter": "Yatao A. Bian", "authors": "An Bian, Kfir Y. Levy, Andreas Krause, Joachim M. Buhmann", "title": "Continuous DR-submodular Maximization: Structure and Algorithms", "comments": "Published in NIPS 2017", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  DR-submodular continuous functions are important objectives with wide\nreal-world applications spanning MAP inference in determinantal point processes\n(DPPs), and mean-field inference for probabilistic submodular models, amongst\nothers. DR-submodularity captures a subclass of non-convex functions that\nenables both exact minimization and approximate maximization in polynomial\ntime.\n  In this work we study the problem of maximizing non-monotone DR-submodular\ncontinuous functions under general down-closed convex constraints. We start by\ninvestigating geometric properties that underlie such objectives, e.g., a\nstrong relation between (approximately) stationary points and global optimum is\nproved. These properties are then used to devise two optimization algorithms\nwith provable guarantees. Concretely, we first devise a \"two-phase\" algorithm\nwith $1/4$ approximation guarantee. This algorithm allows the use of existing\nmethods for finding (approximately) stationary points as a subroutine, thus,\nharnessing recent progress in non-convex optimization. Then we present a\nnon-monotone Frank-Wolfe variant with $1/e$ approximation guarantee and\nsublinear convergence rate. Finally, we extend our approach to a broader class\nof generalized DR-submodular continuous functions, which captures a wider\nspectrum of applications. Our theoretical findings are validated on synthetic\nand real-world problem instances.\n", "versions": [{"version": "v1", "created": "Sat, 4 Nov 2017 01:07:56 GMT"}, {"version": "v2", "created": "Fri, 24 Nov 2017 20:38:16 GMT"}, {"version": "v3", "created": "Sat, 16 Dec 2017 18:39:02 GMT"}, {"version": "v4", "created": "Fri, 24 May 2019 16:14:41 GMT"}], "update_date": "2019-05-27", "authors_parsed": [["Bian", "An", ""], ["Levy", "Kfir Y.", ""], ["Krause", "Andreas", ""], ["Buhmann", "Joachim M.", ""]]}, {"id": "1711.02545", "submitter": "Kwang-Sung Jun", "authors": "Kwang-Sung Jun, Francesco Orabona, Stephen Wright, Rebecca Willett", "title": "Online Learning for Changing Environments using Coin Betting", "comments": "submitted to a journal. arXiv admin note: substantial text overlap\n  with arXiv:1610.04578", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A key challenge in online learning is that classical algorithms can be slow\nto adapt to changing environments. Recent studies have proposed \"meta\"\nalgorithms that convert any online learning algorithm to one that is adaptive\nto changing environments, where the adaptivity is analyzed in a quantity called\nthe strongly-adaptive regret. This paper describes a new meta algorithm that\nhas a strongly-adaptive regret bound that is a factor of $\\sqrt{\\log(T)}$\nbetter than other algorithms with the same time complexity, where $T$ is the\ntime horizon. We also extend our algorithm to achieve a first-order (i.e.,\ndependent on the observed losses) strongly-adaptive regret bound for the first\ntime, to our knowledge. At its heart is a new parameter-free algorithm for the\nlearning with expert advice (LEA) problem in which experts sometimes do not\noutput advice for consecutive time steps (i.e., \\emph{sleeping} experts). This\nalgorithm is derived by a reduction from optimal algorithms for the so-called\ncoin betting problem. Empirical results show that our algorithm outperforms\nstate-of-the-art methods in both learning with expert advice and metric\nlearning scenarios.\n", "versions": [{"version": "v1", "created": "Mon, 6 Nov 2017 07:02:53 GMT"}], "update_date": "2017-11-08", "authors_parsed": [["Jun", "Kwang-Sung", ""], ["Orabona", "Francesco", ""], ["Wright", "Stephen", ""], ["Willett", "Rebecca", ""]]}, {"id": "1711.02598", "submitter": "Slobodan Mitrovi\\'c", "authors": "Slobodan Mitrovi\\'c, Ilija Bogunovic, Ashkan Norouzi-Fard, Jakub\n  Tarnawski, Volkan Cevher", "title": "Streaming Robust Submodular Maximization: A Partitioned Thresholding\n  Approach", "comments": "To appear in NIPS 2017", "journal-ref": "Proc. of 30th Advances in Neural Information Processing Systems\n  (NIPS) 2017, pages 4558-4567", "doi": null, "report-no": null, "categories": "cs.DS stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study the classical problem of maximizing a monotone submodular function\nsubject to a cardinality constraint k, with two additional twists: (i) elements\narrive in a streaming fashion, and (ii) m items from the algorithm's memory are\nremoved after the stream is finished. We develop a robust submodular algorithm\nSTAR-T. It is based on a novel partitioning structure and an exponentially\ndecreasing thresholding rule. STAR-T makes one pass over the data and retains a\nshort but robust summary. We show that after the removal of any m elements from\nthe obtained summary, a simple greedy algorithm STAR-T-GREEDY that runs on the\nremaining elements achieves a constant-factor approximation guarantee. In two\ndifferent data summarization tasks, we demonstrate that it matches or\noutperforms existing greedy and streaming methods, even if they are allowed the\nbenefit of knowing the removed subset in advance.\n", "versions": [{"version": "v1", "created": "Tue, 7 Nov 2017 16:37:25 GMT"}], "update_date": "2017-11-27", "authors_parsed": [["Mitrovi\u0107", "Slobodan", ""], ["Bogunovic", "Ilija", ""], ["Norouzi-Fard", "Ashkan", ""], ["Tarnawski", "Jakub", ""], ["Cevher", "Volkan", ""]]}, {"id": "1711.02613", "submitter": "Elliot J. Crowley", "authors": "Elliot J. Crowley, Gavin Gray, Amos Storkey", "title": "Moonshine: Distilling with Cheap Convolutions", "comments": "32nd Conference on Neural Information Processing Systems (NeurIPS\n  2018)", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.CV cs.LG", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Many engineers wish to deploy modern neural networks in memory-limited\nsettings; but the development of flexible methods for reducing memory use is in\nits infancy, and there is little knowledge of the resulting cost-benefit. We\npropose structural model distillation for memory reduction using a strategy\nthat produces a student architecture that is a simple transformation of the\nteacher architecture: no redesign is needed, and the same hyperparameters can\nbe used. Using attention transfer, we provide Pareto curves/tables for\ndistillation of residual networks with four benchmark datasets, indicating the\nmemory versus accuracy payoff. We show that substantial memory savings are\npossible with very little loss of accuracy, and confirm that distillation\nprovides student network performance that is better than training that student\narchitecture directly on data.\n", "versions": [{"version": "v1", "created": "Tue, 7 Nov 2017 17:21:06 GMT"}, {"version": "v2", "created": "Mon, 21 May 2018 11:43:02 GMT"}, {"version": "v3", "created": "Mon, 22 Oct 2018 16:47:40 GMT"}, {"version": "v4", "created": "Thu, 17 Jan 2019 12:26:19 GMT"}], "update_date": "2019-01-18", "authors_parsed": [["Crowley", "Elliot J.", ""], ["Gray", "Gavin", ""], ["Storkey", "Amos", ""]]}, {"id": "1711.02621", "submitter": "Oren Mangoubi", "authors": "Oren Mangoubi, Nisheeth K. Vishnoi", "title": "Convex Optimization with Unbounded Nonconvex Oracles using Simulated\n  Annealing", "comments": "To appear in COLT 2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.LG math.OC stat.CO stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider the problem of minimizing a convex objective function $F$ when\none can only evaluate its noisy approximation $\\hat{F}$. Unless one assumes\nsome structure on the noise, $\\hat{F}$ may be an arbitrary nonconvex function,\nmaking the task of minimizing $F$ intractable. To overcome this, prior work has\noften focused on the case when $F(x)-\\hat{F}(x)$ is uniformly-bounded. In this\npaper we study the more general case when the noise has magnitude $\\alpha F(x)\n+ \\beta$ for some $\\alpha, \\beta > 0$, and present a polynomial time algorithm\nthat finds an approximate minimizer of $F$ for this noise model. Previously,\nMarkov chains, such as the stochastic gradient Langevin dynamics, have been\nused to arrive at approximate solutions to these optimization problems.\nHowever, for the noise model considered in this paper, no single temperature\nallows such a Markov chain to both mix quickly and concentrate near the global\nminimizer. We bypass this by combining \"simulated annealing\" with the\nstochastic gradient Langevin dynamics, and gradually decreasing the temperature\nof the chain in order to approach the global minimizer. As a corollary one can\napproximately minimize a nonconvex function that is close to a convex function;\nhowever, the closeness can deteriorate as one moves away from the optimum.\n", "versions": [{"version": "v1", "created": "Tue, 7 Nov 2017 17:36:47 GMT"}, {"version": "v2", "created": "Mon, 18 Jun 2018 09:00:43 GMT"}], "update_date": "2018-06-19", "authors_parsed": [["Mangoubi", "Oren", ""], ["Vishnoi", "Nisheeth K.", ""]]}, {"id": "1711.02633", "submitter": "Taoli Cheng", "authors": "Taoli Cheng", "title": "Recursive Neural Networks in Quark/Gluon Tagging", "comments": "14 pages, 9 figures, 4 tables; matches to the version published in\n  Computing and Software for Big Science", "journal-ref": "Comput Softw Big Sci (2018) 2: 3", "doi": "10.1007/s41781-018-0007-y", "report-no": null, "categories": "hep-ph hep-ex stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Since the machine learning techniques are improving rapidly, it has been\nshown that the image recognition techniques in deep neural networks can be used\nto detect jet substructure. And it turns out that deep neural networks can\nmatch or outperform traditional approach of expert features. However, there are\ndisadvantages such as sparseness of jet images. Based on the natural tree-like\nstructure of jet sequential clustering, the recursive neural networks (RecNNs),\nwhich embed jet clustering history recursively as in natural language\nprocessing, have a better behavior when confronted with these problems. We thus\ntry to explore the performance of RecNNs in quark/gluon discrimination. The\nresults show that RecNNs work better than the baseline boosted decision tree\n(BDT) by a few percent in gluon rejection rate. However, extra implementation\nof particle flow identification only increases the performance slightly. We\nalso experimented on some relevant aspects which might influence the\nperformance of the networks. It shows that even taking only particle flow\nidentification as input feature without any extra information on momentum or\nangular position is already giving a fairly good result, which indicates that\nthe most of the information for quark/gluon discrimination is already included\nin the tree-structure itself. As a bonus, a rough up/down quark jets\ndiscrimination is also explored.\n", "versions": [{"version": "v1", "created": "Tue, 7 Nov 2017 17:54:43 GMT"}, {"version": "v2", "created": "Thu, 28 Jun 2018 17:14:47 GMT"}], "update_date": "2018-07-02", "authors_parsed": [["Cheng", "Taoli", ""]]}, {"id": "1711.02651", "submitter": "Andrej Risteski", "authors": "Sanjeev Arora, Andrej Risteski, Yi Zhang", "title": "Theoretical limitations of Encoder-Decoder GAN architectures", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Encoder-decoder GANs architectures (e.g., BiGAN and ALI) seek to add an\ninference mechanism to the GANs setup, consisting of a small encoder deep net\nthat maps data-points to their succinct encodings. The intuition is that being\nforced to train an encoder alongside the usual generator forces the system to\nlearn meaningful mappings from the code to the data-point and vice-versa, which\nshould improve the learning of the target distribution and ameliorate\nmode-collapse. It should also yield meaningful codes that are useful as\nfeatures for downstream tasks. The current paper shows rigorously that even on\nreal-life distributions of images, the encode-decoder GAN training objectives\n(a) cannot prevent mode collapse; i.e. the objective can be near-optimal even\nwhen the generated distribution has low and finite support (b) cannot prevent\nlearning meaningless codes for data -- essentially white noise. Thus if\nencoder-decoder GANs do indeed work then it must be due to reasons as yet not\nunderstood, since the training objective can be low even for meaningless\nsolutions.\n", "versions": [{"version": "v1", "created": "Tue, 7 Nov 2017 18:30:24 GMT"}], "update_date": "2017-11-08", "authors_parsed": [["Arora", "Sanjeev", ""], ["Risteski", "Andrej", ""], ["Zhang", "Yi", ""]]}, {"id": "1711.02653", "submitter": "David Klindt", "authors": "David A. Klindt, Alexander S. Ecker, Thomas Euler, Matthias Bethge", "title": "Neural system identification for large populations separating \"what\" and\n  \"where\"", "comments": "NIPS 2017", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG q-bio.NC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Neuroscientists classify neurons into different types that perform similar\ncomputations at different locations in the visual field. Traditional methods\nfor neural system identification do not capitalize on this separation of 'what'\nand 'where'. Learning deep convolutional feature spaces that are shared among\nmany neurons provides an exciting path forward, but the architectural design\nneeds to account for data limitations: While new experimental techniques enable\nrecordings from thousands of neurons, experimental time is limited so that one\ncan sample only a small fraction of each neuron's response space. Here, we show\nthat a major bottleneck for fitting convolutional neural networks (CNNs) to\nneural data is the estimation of the individual receptive field locations, a\nproblem that has been scratched only at the surface thus far. We propose a CNN\narchitecture with a sparse readout layer factorizing the spatial (where) and\nfeature (what) dimensions. Our network scales well to thousands of neurons and\nshort recordings and can be trained end-to-end. We evaluate this architecture\non ground-truth data to explore the challenges and limitations of CNN-based\nsystem identification. Moreover, we show that our network model outperforms\ncurrent state-of-the art system identification models of mouse primary visual\ncortex.\n", "versions": [{"version": "v1", "created": "Tue, 7 Nov 2017 18:33:02 GMT"}, {"version": "v2", "created": "Mon, 29 Jan 2018 12:56:18 GMT"}], "update_date": "2018-01-30", "authors_parsed": [["Klindt", "David A.", ""], ["Ecker", "Alexander S.", ""], ["Euler", "Thomas", ""], ["Bethge", "Matthias", ""]]}, {"id": "1711.02679", "submitter": "Volodymyr Kuleshov", "authors": "Volodymyr Kuleshov, Stefano Ermon", "title": "Neural Variational Inference and Learning in Undirected Graphical Models", "comments": "Appearing in Proceedings of the 31st Conference on Neural Information\n  Processing Systems (NIPS) 2017, Long Beach, CA, USA", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Many problems in machine learning are naturally expressed in the language of\nundirected graphical models. Here, we propose black-box learning and inference\nalgorithms for undirected models that optimize a variational approximation to\nthe log-likelihood of the model. Central to our approach is an upper bound on\nthe log-partition function parametrized by a function q that we express as a\nflexible neural network. Our bound makes it possible to track the partition\nfunction during learning, to speed-up sampling, and to train a broad class of\nhybrid directed/undirected models via a unified variational inference\nframework. We empirically demonstrate the effectiveness of our method on\nseveral popular generative modeling datasets.\n", "versions": [{"version": "v1", "created": "Tue, 7 Nov 2017 19:00:20 GMT"}, {"version": "v2", "created": "Thu, 16 Nov 2017 21:33:11 GMT"}], "update_date": "2017-11-20", "authors_parsed": [["Kuleshov", "Volodymyr", ""], ["Ermon", "Stefano", ""]]}, {"id": "1711.02712", "submitter": "Bart van Merri\\\"enboer", "authors": "Bart van Merri\\\"enboer, Alexander B. Wiltschko and Dan Moldovan", "title": "Tangent: Automatic Differentiation Using Source Code Transformation in\n  Python", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.MS stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Automatic differentiation (AD) is an essential primitive for machine learning\nprogramming systems. Tangent is a new library that performs AD using source\ncode transformation (SCT) in Python. It takes numeric functions written in a\nsyntactic subset of Python and NumPy as input, and generates new Python\nfunctions which calculate a derivative. This approach to automatic\ndifferentiation is different from existing packages popular in machine\nlearning, such as TensorFlow and Autograd. Advantages are that Tangent\ngenerates gradient code in Python which is readable by the user, easy to\nunderstand and debug, and has no runtime overhead. Tangent also introduces\nabstractions for easily injecting logic into the generated gradient code,\nfurther improving usability.\n", "versions": [{"version": "v1", "created": "Tue, 7 Nov 2017 20:15:24 GMT"}], "update_date": "2017-11-09", "authors_parsed": [["van Merri\u00ebnboer", "Bart", ""], ["Wiltschko", "Alexander B.", ""], ["Moldovan", "Dan", ""]]}, {"id": "1711.02771", "submitter": "Pengchuan Zhang", "authors": "Pengchuan Zhang, Qiang Liu, Dengyong Zhou, Tao Xu, Xiaodong He", "title": "On the Discrimination-Generalization Tradeoff in GANs", "comments": "ICLR 2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Generative adversarial training can be generally understood as minimizing\ncertain moment matching loss defined by a set of discriminator functions,\ntypically neural networks. The discriminator set should be large enough to be\nable to uniquely identify the true distribution (discriminative), and also be\nsmall enough to go beyond memorizing samples (generalizable). In this paper, we\nshow that a discriminator set is guaranteed to be discriminative whenever its\nlinear span is dense in the set of bounded continuous functions. This is a very\nmild condition satisfied even by neural networks with a single neuron. Further,\nwe develop generalization bounds between the learned distribution and true\ndistribution under different evaluation metrics. When evaluated with neural\ndistance, our bounds show that generalization is guaranteed as long as the\ndiscriminator set is small enough, regardless of the size of the generator or\nhypothesis set. When evaluated with KL divergence, our bound provides an\nexplanation on the counter-intuitive behaviors of testing likelihood in GAN\ntraining. Our analysis sheds lights on understanding the practical performance\nof GANs.\n", "versions": [{"version": "v1", "created": "Tue, 7 Nov 2017 23:45:20 GMT"}, {"version": "v2", "created": "Fri, 23 Feb 2018 20:25:46 GMT"}], "update_date": "2018-02-27", "authors_parsed": [["Zhang", "Pengchuan", ""], ["Liu", "Qiang", ""], ["Zhou", "Dengyong", ""], ["Xu", "Tao", ""], ["He", "Xiaodong", ""]]}, {"id": "1711.02782", "submitter": "Sharan Narang", "authors": "Sharan Narang, Eric Undersander, Gregory Diamos", "title": "Block-Sparse Recurrent Neural Networks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recurrent Neural Networks (RNNs) are used in state-of-the-art models in\ndomains such as speech recognition, machine translation, and language\nmodelling. Sparsity is a technique to reduce compute and memory requirements of\ndeep learning models. Sparse RNNs are easier to deploy on devices and high-end\nserver processors. Even though sparse operations need less compute and memory\nrelative to their dense counterparts, the speed-up observed by using sparse\noperations is less than expected on different hardware platforms. In order to\naddress this issue, we investigate two different approaches to induce block\nsparsity in RNNs: pruning blocks of weights in a layer and using group lasso\nregularization to create blocks of weights with zeros. Using these techniques,\nwe demonstrate that we can create block-sparse RNNs with sparsity ranging from\n80% to 90% with small loss in accuracy. This allows us to reduce the model size\nby roughly 10x. Additionally, we can prune a larger dense network to recover\nthis loss in accuracy while maintaining high block sparsity and reducing the\noverall parameter count. Our technique works with a variety of block sizes up\nto 32x32. Block-sparse RNNs eliminate overheads related to data storage and\nirregular memory accesses while increasing hardware efficiency compared to\nunstructured sparsity.\n", "versions": [{"version": "v1", "created": "Wed, 8 Nov 2017 00:57:54 GMT"}], "update_date": "2017-11-09", "authors_parsed": [["Narang", "Sharan", ""], ["Undersander", "Eric", ""], ["Diamos", "Gregory", ""]]}, {"id": "1711.02795", "submitter": "Ayaka Sakata", "authors": "Ayaka Sakata and Yingying Xu", "title": "Approximate message passing for nonconvex sparse regularization with\n  stability and asymptotic analysis", "comments": null, "journal-ref": null, "doi": "10.1088/1742-5468/aab051", "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We analyse a linear regression problem with nonconvex regularization called\nsmoothly clipped absolute deviation (SCAD) under an overcomplete Gaussian basis\nfor Gaussian random data. We propose an approximate message passing (AMP)\nalgorithm considering nonconvex regularization, namely SCAD-AMP, and\nanalytically show that the stability condition corresponds to the de\nAlmeida--Thouless condition in spin glass literature. Through asymptotic\nanalysis, we show the correspondence between the density evolution of SCAD-AMP\nand the replica symmetric solution. Numerical experiments confirm that for a\nsufficiently large system size, SCAD-AMP achieves the optimal performance\npredicted by the replica method. Through replica analysis, a phase transition\nbetween replica symmetric (RS) and replica symmetry breaking (RSB) region is\nfound in the parameter space of SCAD. The appearance of the RS region for a\nnonconvex penalty is a significant advantage that indicates the region of\nsmooth landscape of the optimization problem. Furthermore, we analytically show\nthat the statistical representation performance of the SCAD penalty is better\nthan that of L1-based methods, and the minimum representation error under RS\nassumption is obtained at the edge of the RS/RSB phase. The correspondence\nbetween the convergence of the existing coordinate descent algorithm and RS/RSB\ntransition is also indicated.\n", "versions": [{"version": "v1", "created": "Wed, 8 Nov 2017 01:36:55 GMT"}, {"version": "v2", "created": "Mon, 20 Nov 2017 11:10:04 GMT"}, {"version": "v3", "created": "Mon, 19 Feb 2018 01:43:55 GMT"}], "update_date": "2018-04-04", "authors_parsed": [["Sakata", "Ayaka", ""], ["Xu", "Yingying", ""]]}, {"id": "1711.02810", "submitter": "Biswarup Bhattacharya", "authors": "Biswarup Bhattacharya, Abhishek Sinha", "title": "Deep Fault Analysis and Subset Selection in Solar Power Grids", "comments": "Presented at NIPS 2017 Workshop on Machine Learning for the\n  Developing World", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI cs.CE stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Non-availability of reliable and sustainable electric power is a major\nproblem in the developing world. Renewable energy sources like solar are not\nvery lucrative in the current stage due to various uncertainties like weather,\nstorage, land use among others. There also exists various other issues like\nmis-commitment of power, absence of intelligent fault analysis, congestion,\netc. In this paper, we propose a novel deep learning-based system for\npredicting faults and selecting power generators optimally so as to reduce\ncosts and ensure higher reliability in solar power systems. The results are\nhighly encouraging and they suggest that the approaches proposed in this paper\nhave the potential to be applied successfully in the developing world.\n", "versions": [{"version": "v1", "created": "Wed, 8 Nov 2017 03:09:51 GMT"}], "update_date": "2017-11-13", "authors_parsed": [["Bhattacharya", "Biswarup", ""], ["Sinha", "Abhishek", ""]]}, {"id": "1711.02837", "submitter": "Qi Yan", "authors": "Qi Yan, Zhaofei Yu, Feng Chen and Jian K. Liu", "title": "Revealing structure components of the retina by deep learning networks", "comments": "Presented at NIPS 2017 Symposium on Interpretable Machine Learning", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.CV q-bio.NC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Deep convolutional neural networks (CNNs) have demonstrated impressive\nperformance on visual object classification tasks. In addition, it is a useful\nmodel for predication of neuronal responses recorded in visual system. However,\nthere is still no clear understanding of what CNNs learn in terms of visual\nneuronal circuits. Visualizing CNN's features to obtain possible connections to\nneuronscience underpinnings is not easy due to highly complex circuits from the\nretina to higher visual cortex. Here we address this issue by focusing on\nsingle retinal ganglion cells with a simple model and electrophysiological\nrecordings from salamanders. By training CNNs with white noise images to\npredicate neural responses, we found that convolutional filters learned in the\nend are resembling to biological components of the retinal circuit. Features\nrepresented by these filters tile the space of conventional receptive field of\nretinal ganglion cells. These results suggest that CNN could be used to reveal\nstructure components of neuronal circuits.\n", "versions": [{"version": "v1", "created": "Wed, 8 Nov 2017 05:35:27 GMT"}], "update_date": "2017-11-15", "authors_parsed": [["Yan", "Qi", ""], ["Yu", "Zhaofei", ""], ["Chen", "Feng", ""], ["Liu", "Jian K.", ""]]}, {"id": "1711.02838", "submitter": "Nilesh Tripuraneni", "authors": "Nilesh Tripuraneni, Mitchell Stern, Chi Jin, Jeffrey Regier, Michael\n  I. Jordan", "title": "Stochastic Cubic Regularization for Fast Nonconvex Optimization", "comments": "The first two authors contributed equally", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG math.OC stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper proposes a stochastic variant of a classic algorithm---the\ncubic-regularized Newton method [Nesterov and Polyak 2006]. The proposed\nalgorithm efficiently escapes saddle points and finds approximate local minima\nfor general smooth, nonconvex functions in only\n$\\mathcal{\\tilde{O}}(\\epsilon^{-3.5})$ stochastic gradient and stochastic\nHessian-vector product evaluations. The latter can be computed as efficiently\nas stochastic gradients. This improves upon the\n$\\mathcal{\\tilde{O}}(\\epsilon^{-4})$ rate of stochastic gradient descent. Our\nrate matches the best-known result for finding local minima without requiring\nany delicate acceleration or variance-reduction techniques.\n", "versions": [{"version": "v1", "created": "Wed, 8 Nov 2017 05:39:46 GMT"}, {"version": "v2", "created": "Tue, 5 Dec 2017 20:40:44 GMT"}], "update_date": "2017-12-07", "authors_parsed": [["Tripuraneni", "Nilesh", ""], ["Stern", "Mitchell", ""], ["Jin", "Chi", ""], ["Regier", "Jeffrey", ""], ["Jordan", "Michael I.", ""]]}, {"id": "1711.02846", "submitter": "Ekin Dogus Cubuk", "authors": "Ekin D. Cubuk, Barret Zoph, Samuel S. Schoenholz, Quoc V. Le", "title": "Intriguing Properties of Adversarial Examples", "comments": "17 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  It is becoming increasingly clear that many machine learning classifiers are\nvulnerable to adversarial examples. In attempting to explain the origin of\nadversarial examples, previous studies have typically focused on the fact that\nneural networks operate on high dimensional data, they overfit, or they are too\nlinear. Here we argue that the origin of adversarial examples is primarily due\nto an inherent uncertainty that neural networks have about their predictions.\nWe show that the functional form of this uncertainty is independent of\narchitecture, dataset, and training protocol; and depends only on the\nstatistics of the logit differences of the network, which do not change\nsignificantly during training. This leads to adversarial error having a\nuniversal scaling, as a power-law, with respect to the size of the adversarial\nperturbation. We show that this universality holds for a broad range of\ndatasets (MNIST, CIFAR10, ImageNet, and random data), models (including\nstate-of-the-art deep networks, linear models, adversarially trained networks,\nand networks trained on randomly shuffled labels), and attacks (FGSM, step\nl.l., PGD). Motivated by these results, we study the effects of reducing\nprediction entropy on adversarial robustness. Finally, we study the effect of\nnetwork architectures on adversarial sensitivity. To do this, we use neural\narchitecture search with reinforcement learning to find adversarially robust\narchitectures on CIFAR10. Our resulting architecture is more robust to white\n\\emph{and} black box attacks compared to previous attempts.\n", "versions": [{"version": "v1", "created": "Wed, 8 Nov 2017 06:54:49 GMT"}], "update_date": "2017-11-09", "authors_parsed": [["Cubuk", "Ekin D.", ""], ["Zoph", "Barret", ""], ["Schoenholz", "Samuel S.", ""], ["Le", "Quoc V.", ""]]}, {"id": "1711.02857", "submitter": "Jianqiao Wangni", "authors": "Jianqiao Wangni, Dahua Lin", "title": "Learning Sparse Visual Representations with Leaky Capped Norm\n  Regularizers", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI cs.CV math.NA stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Sparsity inducing regularization is an important part for learning\nover-complete visual representations. Despite the popularity of $\\ell_1$\nregularization, in this paper, we investigate the usage of non-convex\nregularizations in this problem. Our contribution consists of three parts.\nFirst, we propose the leaky capped norm regularization (LCNR), which allows\nmodel weights below a certain threshold to be regularized more strongly as\nopposed to those above, therefore imposes strong sparsity and only introduces\ncontrollable estimation bias. We propose a majorization-minimization algorithm\nto optimize the joint objective function. Second, our study over monocular 3D\nshape recovery and neural networks with LCNR outperforms $\\ell_1$ and other\nnon-convex regularizations, achieving state-of-the-art performance and faster\nconvergence. Third, we prove a theoretical global convergence speed on the 3D\nrecovery problem. To the best of our knowledge, this is the first convergence\nanalysis of the 3D recovery problem.\n", "versions": [{"version": "v1", "created": "Wed, 8 Nov 2017 07:54:41 GMT"}], "update_date": "2017-11-09", "authors_parsed": [["Wangni", "Jianqiao", ""], ["Lin", "Dahua", ""]]}, {"id": "1711.02876", "submitter": "Paulo Serra", "authors": "Paulo Serra and Michel Mandjes", "title": "Dimension Estimation Using Random Connection Models", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Information about intrinsic dimension is crucial to perform dimensionality\nreduction, compress information, design efficient algorithms, and do\nstatistical adaptation. In this paper we propose an estimator for the intrinsic\ndimension of a data set. The estimator is based on binary neighbourhood\ninformation about the observations in the form of two adjacency matrices, and\ndoes not require any explicit distance information. The underlying graph is\nmodelled according to a subset of a specific random connection model, sometimes\nreferred to as the Poisson blob model. Computationally the estimator scales\nlike n log n, and we specify its asymptotic distribution and rate of\nconvergence. A simulation study on both real and simulated data shows that our\napproach compares favourably with some competing methods from the literature,\nincluding approaches that rely on distance information.\n", "versions": [{"version": "v1", "created": "Wed, 8 Nov 2017 09:17:35 GMT"}], "update_date": "2017-11-09", "authors_parsed": [["Serra", "Paulo", ""], ["Mandjes", "Michel", ""]]}, {"id": "1711.02887", "submitter": "St\\'ephane Ga\\\"iffas", "authors": "Jaouad Mourtada, St\\'ephane Ga\\\"iffas and Erwan Scornet", "title": "Universal consistency and minimax rates for online Mondrian Forests", "comments": "NIPS 2017", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We establish the consistency of an algorithm of Mondrian Forests, a\nrandomized classification algorithm that can be implemented online. First, we\namend the original Mondrian Forest algorithm, that considers a fixed lifetime\nparameter. Indeed, the fact that this parameter is fixed hinders the\nstatistical consistency of the original procedure. Our modified Mondrian Forest\nalgorithm grows trees with increasing lifetime parameters $\\lambda_n$, and uses\nan alternative updating rule, allowing to work also in an online fashion.\nSecond, we provide a theoretical analysis establishing simple conditions for\nconsistency. Our theoretical analysis also exhibits a surprising fact: our\nalgorithm achieves the minimax rate (optimal rate) for the estimation of a\nLipschitz regression function, which is a strong extension of previous results\nto an arbitrary dimension.\n", "versions": [{"version": "v1", "created": "Wed, 8 Nov 2017 09:47:42 GMT"}], "update_date": "2017-11-09", "authors_parsed": [["Mourtada", "Jaouad", ""], ["Ga\u00efffas", "St\u00e9phane", ""], ["Scornet", "Erwan", ""]]}, {"id": "1711.02974", "submitter": "Michel  Barlaud", "authors": "Cyprien Gilet, Marie Deprez, Jean-Baptiste Caillau and Michel Barlaud", "title": "Clustering with feature selection using alternating minimization,\n  Application to computational biology", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper deals with unsupervised clustering with feature selection. The\nproblem is to estimate both labels and a sparse projection matrix of weights.\nTo address this combinatorial non-convex problem maintaining a strict control\non the sparsity of the matrix of weights, we propose an alternating\nminimization of the Frobenius norm criterion. We provide a new efficient\nalgorithm named K-sparse which alternates k-means with projection-gradient\nminimization. The projection-gradient step is a method of splitting type, with\nexact projection on the $\\ell^1$ ball to promote sparsity. The convergence of\nthe gradient-projection step is addressed, and a preliminary analysis of the\nalternating minimization is made. The Frobenius norm criterion converges as the\nnumber of iterates in Algorithm K-sparse goes to infinity. Experiments on\nSingle Cell RNA sequencing datasets show that our method significantly improves\nthe results of PCA k-means, spectral clustering, SIMLR, and Sparcl methods, and\nachieves a relevant selection of genes. The complexity of K-sparse is linear in\nthe number of samples (cells), so that the method scales up to large datasets.\n", "versions": [{"version": "v1", "created": "Wed, 8 Nov 2017 14:42:55 GMT"}, {"version": "v2", "created": "Tue, 5 Dec 2017 09:45:42 GMT"}, {"version": "v3", "created": "Mon, 29 Oct 2018 14:29:53 GMT"}, {"version": "v4", "created": "Fri, 24 May 2019 12:04:34 GMT"}], "update_date": "2019-05-27", "authors_parsed": [["Gilet", "Cyprien", ""], ["Deprez", "Marie", ""], ["Caillau", "Jean-Baptiste", ""], ["Barlaud", "Michel", ""]]}, {"id": "1711.02989", "submitter": "Jiri Hron", "authors": "Jiri Hron, Alexander G. de G. Matthews, Zoubin Ghahramani", "title": "Variational Gaussian Dropout is not Bayesian", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Gaussian multiplicative noise is commonly used as a stochastic regularisation\ntechnique in training of deterministic neural networks. A recent paper\nreinterpreted the technique as a specific algorithm for approximate inference\nin Bayesian neural networks; several extensions ensued. We show that the\nlog-uniform prior used in all the above publications does not generally induce\na proper posterior, and thus Bayesian inference in such models is ill-posed.\nIndependent of the log-uniform prior, the correlated weight noise approximation\nhas further issues leading to either infinite objective or high risk of\noverfitting. The above implies that the reported sparsity of obtained solutions\ncannot be explained by Bayesian or the related minimum description length\narguments. We thus study the objective from a non-Bayesian perspective, provide\nits previously unknown analytical form which allows exact gradient evaluation,\nand show that the later proposed additive reparametrisation introduces minima\nnot present in the original multiplicative parametrisation. Implications and\nfuture research directions are discussed.\n", "versions": [{"version": "v1", "created": "Wed, 8 Nov 2017 15:04:20 GMT"}], "update_date": "2017-11-09", "authors_parsed": [["Hron", "Jiri", ""], ["Matthews", "Alexander G. de G.", ""], ["Ghahramani", "Zoubin", ""]]}, {"id": "1711.03026", "submitter": "Biswarup Bhattacharya", "authors": "Biswarup Bhattacharya, Abhishek Sinha", "title": "Intelligent Fault Analysis in Electrical Power Grids", "comments": "In proceedings of the 29th IEEE International Conference on Tools\n  with Artificial Intelligence (ICTAI) 2017 (full paper); 6 pages; 13 figures", "journal-ref": null, "doi": "10.1109/ICTAI.2017.00151", "report-no": null, "categories": "cs.SY cs.AI stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Power grids are one of the most important components of infrastructure in\ntoday's world. Every nation is dependent on the security and stability of its\nown power grid to provide electricity to the households and industries. A\nmalfunction of even a small part of a power grid can cause loss of\nproductivity, revenue and in some cases even life. Thus, it is imperative to\ndesign a system which can detect the health of the power grid and take\nprotective measures accordingly even before a serious anomaly takes place. To\nachieve this objective, we have set out to create an artificially intelligent\nsystem which can analyze the grid information at any given time and determine\nthe health of the grid through the usage of sophisticated formal models and\nnovel machine learning techniques like recurrent neural networks. Our system\nsimulates grid conditions including stimuli like faults, generator output\nfluctuations, load fluctuations using Siemens PSS/E software and this data is\ntrained using various classifiers like SVM, LSTM and subsequently tested. The\nresults are excellent with our methods giving very high accuracy for the data.\nThis model can easily be scaled to handle larger and more complex grid\narchitectures.\n", "versions": [{"version": "v1", "created": "Wed, 8 Nov 2017 16:03:04 GMT"}], "update_date": "2017-11-13", "authors_parsed": [["Bhattacharya", "Biswarup", ""], ["Sinha", "Abhishek", ""]]}, {"id": "1711.03038", "submitter": "Kristjan Kalm", "authors": "Kristjan Kalm", "title": "Recency-weighted Markovian inference", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We describe a Markov latent state space (MLSS) model, where the latent state\ndistribution is a decaying mixture over multiple past states. We present a\nsimple sampling algorithm that allows to approximate such high-order MLSS with\nfixed time and memory costs.\n", "versions": [{"version": "v1", "created": "Wed, 8 Nov 2017 16:30:56 GMT"}], "update_date": "2017-11-09", "authors_parsed": [["Kalm", "Kristjan", ""]]}, {"id": "1711.03058", "submitter": "Michael Shvartsman", "authors": "Michael Shvartsman, Narayanan Sundaram, Mikio C. Aoi, Adam Charles,\n  Theodore C. Wilke, Jonathan D. Cohen", "title": "Matrix-normal models for fMRI analysis", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML q-bio.NC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Multivariate analysis of fMRI data has benefited substantially from advances\nin machine learning. Most recently, a range of probabilistic latent variable\nmodels applied to fMRI data have been successful in a variety of tasks,\nincluding identifying similarity patterns in neural data (Representational\nSimilarity Analysis and its empirical Bayes variant, RSA and BRSA; Intersubject\nFunctional Connectivity, ISFC), combining multi-subject datasets (Shared\nResponse Mapping; SRM), and mapping between brain and behavior (Joint\nModeling). Although these methods share some underpinnings, they have been\ndeveloped as distinct methods, with distinct algorithms and software tools. We\nshow how the matrix-variate normal (MN) formalism can unify some of these\nmethods into a single framework. In doing so, we gain the ability to reuse\nnoise modeling assumptions, algorithms, and code across models. Our primary\ntheoretical contribution shows how some of these methods can be written as\ninstantiations of the same model, allowing us to generalize them to flexibly\nmodeling structured noise covariances. Our formalism permits novel model\nvariants and improved estimation strategies: in contrast to SRM, the number of\nparameters for MN-SRM does not scale with the number of voxels or subjects; in\ncontrast to BRSA, the number of parameters for MN-RSA scales additively rather\nthan multiplicatively in the number of voxels. We empirically demonstrate\nadvantages of two new methods derived in the formalism: for MN-RSA, we show up\nto 10x improvement in runtime, up to 6x improvement in RMSE, and more\nconservative behavior under the null. For MN-SRM, our method grants a modest\nimprovement to out-of-sample reconstruction while relaxing an orthonormality\nconstraint of SRM. We also provide a software prototyping tool for MN models\nthat can flexibly reuse noise covariance assumptions and algorithms across\nmodels.\n", "versions": [{"version": "v1", "created": "Wed, 8 Nov 2017 17:22:51 GMT"}, {"version": "v2", "created": "Fri, 10 Nov 2017 04:20:54 GMT"}], "update_date": "2017-11-13", "authors_parsed": [["Shvartsman", "Michael", ""], ["Sundaram", "Narayanan", ""], ["Aoi", "Mikio C.", ""], ["Charles", "Adam", ""], ["Wilke", "Theodore C.", ""], ["Cohen", "Jonathan D.", ""]]}, {"id": "1711.03067", "submitter": "Ting Chen", "authors": "Ting Chen, Martin Renqiang Min and Yizhou Sun", "title": "Learning K-way D-dimensional Discrete Code For Compact Embedding\n  Representations", "comments": "NIPS'17 DISCML", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Embedding methods such as word embedding have become pillars for many\napplications containing discrete structures. Conventional embedding methods\ndirectly associate each symbol with a continuous embedding vector, which is\nequivalent to applying linear transformation based on \"one-hot\" encoding of the\ndiscrete symbols. Despite its simplicity, such approach yields number of\nparameters that grows linearly with the vocabulary size and can lead to\noverfitting. In this work we propose a much more compact K-way D-dimensional\ndiscrete encoding scheme to replace the \"one-hot\" encoding. In \"KD encoding\",\neach symbol is represented by a $D$-dimensional code, and each of its dimension\nhas a cardinality of $K$. The final symbol embedding vector can be generated by\ncomposing the code embedding vectors. To learn the semantically meaningful\ncode, we derive a relaxed discrete optimization technique based on stochastic\ngradient descent. By adopting the new coding system, the efficiency of\nparameterization can be significantly improved (from linear to logarithmic),\nand this can also mitigate the over-fitting problem. In our experiments with\nlanguage modeling, the number of embedding parameters can be reduced by 97\\%\nwhile achieving similar or better performance.\n", "versions": [{"version": "v1", "created": "Wed, 8 Nov 2017 17:46:55 GMT"}, {"version": "v2", "created": "Thu, 9 Nov 2017 06:12:54 GMT"}, {"version": "v3", "created": "Sun, 10 Dec 2017 22:00:30 GMT"}], "update_date": "2017-12-12", "authors_parsed": [["Chen", "Ting", ""], ["Min", "Martin Renqiang", ""], ["Sun", "Yizhou", ""]]}, {"id": "1711.03073", "submitter": "Anirbit Mukherjee", "authors": "Anirbit Mukherjee, Amitabh Basu", "title": "Lower bounds over Boolean inputs for deep neural networks with ReLU\n  gates", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CC cs.LG cs.NE stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Motivated by the resurgence of neural networks in being able to solve complex\nlearning tasks we undertake a study of high depth networks using ReLU gates\nwhich implement the function $x \\mapsto \\max\\{0,x\\}$. We try to understand the\nrole of depth in such neural networks by showing size lowerbounds against such\nnetwork architectures in parameter regimes hitherto unexplored. In particular\nwe show the following two main results about neural nets computing Boolean\nfunctions of input dimension $n$,\n  1. We use the method of random restrictions to show almost linear,\n$\\Omega(\\epsilon^{2(1-\\delta)}n^{1-\\delta})$, lower bound for completely weight\nunrestricted LTF-of-ReLU circuits to match the Andreev function on at least\n$\\frac{1}{2} +\\epsilon$ fraction of the inputs for $\\epsilon >\n\\sqrt{2\\frac{\\log^{\\frac {2}{2-\\delta}}(n)}{n}}$ for any $\\delta \\in (0,\\frac 1\n2)$\n  2. We use the method of sign-rank to show exponential in dimension lower\nbounds for ReLU circuits ending in a LTF gate and of depths upto $O(n^{\\xi})$\nwith $\\xi < \\frac{1}{8}$ with some restrictions on the weights in the bottom\nmost layer. All other weights in these circuits are kept unrestricted. This in\nturns also implies the same lowerbounds for LTF circuits with the same\narchitecture and the same weight restrictions on their bottom most layer.\n  Along the way we also show that there exists a $\\mathbb{R}^ n\\rightarrow\n\\mathbb{R}$ Sum-of-ReLU-of-ReLU function which Sum-of-ReLU neural nets can\nnever represent no matter how large they are allowed to be.\n", "versions": [{"version": "v1", "created": "Wed, 8 Nov 2017 18:02:47 GMT"}, {"version": "v2", "created": "Thu, 9 Nov 2017 02:35:25 GMT"}], "update_date": "2017-11-10", "authors_parsed": [["Mukherjee", "Anirbit", ""], ["Basu", "Amitabh", ""]]}, {"id": "1711.03149", "submitter": "Botond Szabo", "authors": "Botond Szabo and Harry van Zanten", "title": "An asymptotic analysis of distributed nonparametric methods", "comments": "29 pages, 4 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.ML stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We investigate and compare the fundamental performance of several distributed\nlearning methods that have been proposed recently. We do this in the context of\na distributed version of the classical signal-in-Gaussian-white-noise model,\nwhich serves as a benchmark model for studying performance in this setting. The\nresults show how the design and tuning of a distributed method can have great\nimpact on convergence rates and validity of uncertainty quantification.\nMoreover, we highlight the difficulty of designing nonparametric distributed\nprocedures that automatically adapt to smoothness.\n", "versions": [{"version": "v1", "created": "Wed, 8 Nov 2017 20:31:58 GMT"}], "update_date": "2017-11-10", "authors_parsed": [["Szabo", "Botond", ""], ["van Zanten", "Harry", ""]]}, {"id": "1711.03189", "submitter": "Weiyang Liu", "authors": "Weiyang Liu, Yan-Ming Zhang, Xingguo Li, Zhiding Yu, Bo Dai, Tuo Zhao,\n  Le Song", "title": "Deep Hyperspherical Learning", "comments": "NIPS 2017 (Spotlight)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Convolution as inner product has been the founding basis of convolutional\nneural networks (CNNs) and the key to end-to-end visual representation\nlearning. Benefiting from deeper architectures, recent CNNs have demonstrated\nincreasingly strong representation abilities. Despite such improvement, the\nincreased depth and larger parameter space have also led to challenges in\nproperly training a network. In light of such challenges, we propose\nhyperspherical convolution (SphereConv), a novel learning framework that gives\nangular representations on hyperspheres. We introduce SphereNet, deep\nhyperspherical convolution networks that are distinct from conventional inner\nproduct based convolutional networks. In particular, SphereNet adopts\nSphereConv as its basic convolution operator and is supervised by generalized\nangular softmax loss - a natural loss formulation under SphereConv. We show\nthat SphereNet can effectively encode discriminative representation and\nalleviate training difficulty, leading to easier optimization, faster\nconvergence and comparable (even better) classification accuracy over\nconvolutional counterparts. We also provide some theoretical insights for the\nadvantages of learning on hyperspheres. In addition, we introduce the learnable\nSphereConv, i.e., a natural improvement over prefixed SphereConv, and\nSphereNorm, i.e., hyperspherical learning as a normalization method.\nExperiments have verified our conclusions.\n", "versions": [{"version": "v1", "created": "Wed, 8 Nov 2017 22:21:21 GMT"}, {"version": "v2", "created": "Mon, 13 Nov 2017 15:15:19 GMT"}, {"version": "v3", "created": "Wed, 22 Nov 2017 18:18:04 GMT"}, {"version": "v4", "created": "Mon, 27 Nov 2017 20:48:17 GMT"}, {"version": "v5", "created": "Tue, 30 Jan 2018 16:00:14 GMT"}], "update_date": "2018-01-31", "authors_parsed": [["Liu", "Weiyang", ""], ["Zhang", "Yan-Ming", ""], ["Li", "Xingguo", ""], ["Yu", "Zhiding", ""], ["Dai", "Bo", ""], ["Zhao", "Tuo", ""], ["Song", "Le", ""]]}, {"id": "1711.03190", "submitter": "Jiaxuan Wang", "authors": "Jiaxuan Wang, Jeeheh Oh, Haozhu Wang, Jenna Wiens", "title": "Learning Credible Models", "comments": null, "journal-ref": "KDD '18 Proceedings of the 24th ACM SIGKDD International\n  Conference on Knowledge Discovery & Data Mining 2018", "doi": "10.1145/3219819.3220070", "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In many settings, it is important that a model be capable of providing\nreasons for its predictions (i.e., the model must be interpretable). However,\nthe model's reasoning may not conform with well-established knowledge. In such\ncases, while interpretable, the model lacks \\textit{credibility}. In this work,\nwe formally define credibility in the linear setting and focus on techniques\nfor learning models that are both accurate and credible. In particular, we\npropose a regularization penalty, expert yielded estimates (EYE), that\nincorporates expert knowledge about well-known relationships among covariates\nand the outcome of interest. We give both theoretical and empirical results\ncomparing our proposed method to several other regularization techniques.\nAcross a range of settings, experiments on both synthetic and real data show\nthat models learned using the EYE penalty are significantly more credible than\nthose learned using other penalties. Applied to a large-scale patient risk\nstratification task, our proposed technique results in a model whose top\nfeatures overlap significantly with known clinical risk factors, while still\nachieving good predictive performance.\n", "versions": [{"version": "v1", "created": "Wed, 8 Nov 2017 22:28:09 GMT"}, {"version": "v2", "created": "Tue, 14 Nov 2017 23:52:27 GMT"}, {"version": "v3", "created": "Thu, 7 Jun 2018 18:46:56 GMT"}], "update_date": "2019-02-19", "authors_parsed": [["Wang", "Jiaxuan", ""], ["Oh", "Jeeheh", ""], ["Wang", "Haozhu", ""], ["Wiens", "Jenna", ""]]}, {"id": "1711.03194", "submitter": "Alexander Korotin", "authors": "Alexander Korotin and Vladimir V'yugin and Evgeny Burnaev", "title": "Long-Term Online Smoothing Prediction Using Expert Advice", "comments": "22 pages, 1 figure", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  For the prediction with experts' advice setting, we construct forecasting\nalgorithms that suffer loss not much more than any expert in the pool. In\ncontrast to the standard approach, we investigate the case of long-term\nforecasting of time series and consider two scenarios. In the first one, at\neach step $t$ the learner has to combine the point forecasts of the experts\nissued for the time interval $[t+1, t+d]$ ahead. Our approach implies that at\neach time step experts issue point forecasts for arbitrary many steps ahead and\nthen the learner (algorithm) combines these forecasts and the forecasts made\nearlier into one vector forecast for steps $[t+1,t+d]$. By combining past and\nthe current long-term forecasts we obtain a smoothing mechanism that protects\nour algorithm from temporary trend changes, noise and outliers. In the second\nscenario, at each step $t$ experts issue a prediction function, and the learner\nhas to combine these functions into the single one, which will be used for\nlong-term time-series prediction. For each scenario, we develop an algorithm\nfor combining experts forecasts and prove $O(\\ln T)$ adversarial regret upper\nbound for both algorithms.\n", "versions": [{"version": "v1", "created": "Wed, 8 Nov 2017 22:35:56 GMT"}, {"version": "v2", "created": "Tue, 20 Feb 2018 07:19:02 GMT"}, {"version": "v3", "created": "Wed, 27 Feb 2019 09:58:44 GMT"}], "update_date": "2019-02-28", "authors_parsed": [["Korotin", "Alexander", ""], ["V'yugin", "Vladimir", ""], ["Burnaev", "Evgeny", ""]]}, {"id": "1711.03198", "submitter": "Fang Liu", "authors": "Fang Liu, Swapna Buccapatnam, Ness Shroff", "title": "Information Directed Sampling for Stochastic Bandits with Graph Feedback", "comments": "Accepted by AAAI 2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider stochastic multi-armed bandit problems with graph feedback, where\nthe decision maker is allowed to observe the neighboring actions of the chosen\naction. We allow the graph structure to vary with time and consider both\ndeterministic and Erd\\H{o}s-R\\'enyi random graph models. For such a graph\nfeedback model, we first present a novel analysis of Thompson sampling that\nleads to tighter performance bound than existing work. Next, we propose new\nInformation Directed Sampling based policies that are graph-aware in their\ndecision making. Under the deterministic graph case, we establish a Bayesian\nregret bound for the proposed policies that scales with the clique cover number\nof the graph instead of the number of actions. Under the random graph case, we\nprovide a Bayesian regret bound for the proposed policies that scales with the\nratio of the number of actions over the expected number of observations per\niteration. To the best of our knowledge, this is the first analytical result\nfor stochastic bandits with random graph feedback. Finally, using numerical\nevaluations, we demonstrate that our proposed IDS policies outperform existing\napproaches, including adaptions of upper confidence bound, $\\epsilon$-greedy\nand Exp3 algorithms.\n", "versions": [{"version": "v1", "created": "Wed, 8 Nov 2017 22:47:59 GMT"}], "update_date": "2017-11-10", "authors_parsed": [["Liu", "Fang", ""], ["Buccapatnam", "Swapna", ""], ["Shroff", "Ness", ""]]}, {"id": "1711.03280", "submitter": "Yuan Gong", "authors": "Yuan Gong, Christian Poellabauer", "title": "Crafting Adversarial Examples For Speech Paralinguistics Applications", "comments": "Published in DYnamic and Novel Advances in Machine Learning and\n  Intelligent Cyber Security (DYNAMICS) Workshop in conjunction with ACSAC'18,\n  San Juan, Puerto Rico, December 2018", "journal-ref": null, "doi": "10.1145/3306195.3306196", "report-no": null, "categories": "cs.LG cs.CR cs.SD eess.AS stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Computational paralinguistic analysis is increasingly being used in a wide\nrange of cyber applications, including security-sensitive applications such as\nspeaker verification, deceptive speech detection, and medical diagnostics.\nWhile state-of-the-art machine learning techniques, such as deep neural\nnetworks, can provide robust and accurate speech analysis, they are susceptible\nto adversarial attacks. In this work, we propose an end-to-end scheme to\ngenerate adversarial examples for computational paralinguistic applications by\nperturbing directly the raw waveform of an audio recording rather than specific\nacoustic features. Our experiments show that the proposed adversarial\nperturbation can lead to a significant performance drop of state-of-the-art\ndeep neural networks, while only minimally impairing the audio quality.\n", "versions": [{"version": "v1", "created": "Thu, 9 Nov 2017 07:41:53 GMT"}, {"version": "v2", "created": "Fri, 11 Jan 2019 06:57:33 GMT"}], "update_date": "2019-01-14", "authors_parsed": [["Gong", "Yuan", ""], ["Poellabauer", "Christian", ""]]}, {"id": "1711.03321", "submitter": "Alessandro Achille", "authors": "Alessandro Achille, Stefano Soatto", "title": "A Separation Principle for Control in the Age of Deep Learning", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We review the problem of defining and inferring a \"state\" for a control\nsystem based on complex, high-dimensional, highly uncertain measurement streams\nsuch as videos. Such a state, or representation, should contain all and only\nthe information needed for control, and discount nuisance variability in the\ndata. It should also have finite complexity, ideally modulated depending on\navailable resources. This representation is what we want to store in memory in\nlieu of the data, as it \"separates\" the control task from the measurement\nprocess. For the trivial case with no dynamics, a representation can be\ninferred by minimizing the Information Bottleneck Lagrangian in a function\nclass realized by deep neural networks. The resulting representation has much\nhigher dimension than the data, already in the millions, but it is smaller in\nthe sense of information content, retaining only what is needed for the task.\nThis process also yields representations that are invariant to nuisance factors\nand having maximally independent components. We extend these ideas to the\ndynamic case, where the representation is the posterior density of the task\nvariable given the measurements up to the current time, which is in general\nmuch simpler than the prediction density maintained by the classical Bayesian\nfilter. Again this can be finitely-parametrized using a deep neural network,\nand already some applications are beginning to emerge. No explicit assumption\nof Markovianity is needed; instead, complexity trades off approximation of an\noptimal representation, including the degree of Markovianity.\n", "versions": [{"version": "v1", "created": "Thu, 9 Nov 2017 11:10:24 GMT"}], "update_date": "2017-11-10", "authors_parsed": [["Achille", "Alessandro", ""], ["Soatto", "Stefano", ""]]}, {"id": "1711.03343", "submitter": "Kazuyuki Hara", "authors": "Kazuyuki Hara", "title": "Analysis of Dropout in Online Learning", "comments": "8 pages, 6 pages", "journal-ref": "IEICE Technical Report IBIS2017-61", "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Deep learning is the state-of-the-art in fields such as visual object\nrecognition and speech recognition. This learning uses a large number of layers\nand a huge number of units and connections. Therefore, overfitting is a serious\nproblem with it, and the dropout which is a kind of regularization tool is\nused. However, in online learning, the effect of dropout is not well known.\nThis paper presents our investigation on the effect of dropout in online\nlearning. We analyzed the effect of dropout on convergence speed near the\nsingular point. Our results indicated that dropout is effective in online\nlearning. Dropout tends to avoid the singular point for convergence speed near\nthat point.\n", "versions": [{"version": "v1", "created": "Thu, 9 Nov 2017 12:10:27 GMT"}], "update_date": "2017-11-10", "authors_parsed": [["Hara", "Kazuyuki", ""]]}, {"id": "1711.03346", "submitter": "Elizabeth Pei-Ting Chou", "authors": "Elizabeth P. Chou, Tzu-Wei Ko", "title": "Dimension Reduction of High-Dimensional Datasets Based on Stepwise SVM", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The current study proposes a dimension reduction method, stepwise support\nvector machine (SVM), to reduce the dimensions of large p small n datasets. The\nproposed method is compared with other dimension reduction methods, namely, the\nPearson product difference correlation coefficient (PCCs), recursive feature\nelimination based on random forest (RF-RFE), and principal component analysis\n(PCA), by using five gene expression datasets. Additionally, the prediction\nperformance of the variables selected by our method is evaluated. The study\nfound that stepwise SVM can effectively select the important variables and\nachieve good prediction performance. Moreover, the predictions of stepwise SVM\nfor reduced datasets was better than those for the unreduced datasets. The\nperformance of stepwise SVM was more stable than that of PCA and RF-RFE, but\nthe performance difference with respect to PCCs was minimal. It is necessary to\nreduce the dimensions of large p small n datasets. We believe that stepwise SVM\ncan effectively eliminate noise in data and improve the prediction accuracy in\nany large p small n dataset.\n", "versions": [{"version": "v1", "created": "Thu, 9 Nov 2017 12:16:24 GMT"}], "update_date": "2017-11-10", "authors_parsed": [["Chou", "Elizabeth P.", ""], ["Ko", "Tzu-Wei", ""]]}, {"id": "1711.03361", "submitter": "Tianchun Wang", "authors": "Tianchun Wang", "title": "Multi-Relevance Transfer Learning", "comments": "14 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Transfer learning aims to faciliate learning tasks in a label-scarce target\ndomain by leveraging knowledge from a related source domain with plenty of\nlabeled data. Often times we may have multiple domains with little or no\nlabeled data as targets waiting to be solved. Most existing efforts tackle\ntarget domains separately by modeling the `source-target' pairs without\nexploring the relatedness between them, which would cause loss of crucial\ninformation, thus failing to achieve optimal capability of knowledge transfer.\nIn this paper, we propose a novel and effective approach called Multi-Relevance\nTransfer Learning (MRTL) for this purpose, which can simultaneously transfer\ndifferent knowledge from the source and exploits the shared common latent\nfactors between target domains. Specifically, we formulate the problem as an\noptimization task based on a collective nonnegative matrix tri-factorization\nframework. The proposed approach achieves both source-target transfer and\ntarget-target leveraging by sharing multiple decomposed latent subspaces.\nFurther, an alternative minimization learning algorithm is developed with\nconvergence guarantee. Empirical study validates the performance and\neffectiveness of MRTL compared to the state-of-the-art methods.\n", "versions": [{"version": "v1", "created": "Thu, 9 Nov 2017 13:06:09 GMT"}], "update_date": "2017-11-10", "authors_parsed": [["Wang", "Tianchun", ""]]}, {"id": "1711.03404", "submitter": "Romain Couillet", "authors": "Xiaoyi Mai and Romain Couillet", "title": "A random matrix analysis and improvement of semi-supervised learning for\n  large dimensional data", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This article provides an original understanding of the behavior of a class of\ngraph-oriented semi-supervised learning algorithms in the limit of large and\nnumerous data. It is demonstrated that the intuition at the root of these\nmethods collapses in this limit and that, as a result, most of them become\ninconsistent. Corrective measures and a new data-driven parametrization scheme\nare proposed along with a theoretical analysis of the asymptotic performances\nof the resulting approach. A surprisingly close behavior between theoretical\nperformances on Gaussian mixture models and on real datasets is also\nillustrated throughout the article, thereby suggesting the importance of the\nproposed analysis for dealing with practical data. As a result, significant\nperformance gains are observed on practical data classification using the\nproposed parametrization.\n", "versions": [{"version": "v1", "created": "Thu, 9 Nov 2017 14:59:30 GMT"}], "update_date": "2017-11-10", "authors_parsed": [["Mai", "Xiaoyi", ""], ["Couillet", "Romain", ""]]}, {"id": "1711.03410", "submitter": "Pedram Gharani", "authors": "Brian Suffoletto, Pedram Gharani, Tammy Chung, Hassan Karimi", "title": "Using Phone Sensors and an Artificial Neural Network to Detect Gait\n  Changes During Drinking Episodes in the Natural Environment", "comments": null, "journal-ref": "Gait Posture 60 (2018) 116-12", "doi": "10.1016/j.gaitpost.2017.11.019", "report-no": null, "categories": "cs.CY cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Phone sensors could be useful in assessing changes in gait that occur with\nalcohol consumption. This study determined (1) feasibility of collecting\ngait-related data during drinking occasions in the natural environment, and (2)\nhow gait-related features measured by phone sensors relate to estimated blood\nalcohol concentration (eBAC). Ten young adult heavy drinkers were prompted to\ncomplete a 5-step gait task every hour from 8pm to 12am over four consecutive\nweekends. We collected 3-xis accelerometer, gyroscope, and magnetometer data\nfrom phone sensors, and computed 24 gait-related features using a sliding\nwindow technique. eBAC levels were calculated at each time point based on\nEcological Momentary Assessment (EMA) of alcohol use. We used an artificial\nneural network model to analyze associations between sensor features and eBACs\nin training (70% of the data) and validation and test (30% of the data)\ndatasets. We analyzed 128 data points where both eBAC and gait-related sensor\ndata was captured, either when not drinking (n=60), while eBAC was ascending\n(n=55) or eBAC was descending (n=13). 21 data points were captured at times\nwhen the eBAC was greater than the legal limit (0.08 mg/dl). Using a Bayesian\nregularized neural network, gait-related phone sensor features showed a high\ncorrelation with eBAC (Pearson's r > 0.9), and >95% of estimated eBAC would\nfall between -0.012 and +0.012 of actual eBAC. It is feasible to collect\ngait-related data from smartphone sensors during drinking occasions in the\nnatural environment. Sensor-based features can be used to infer gait changes\nassociated with elevated blood alcohol content.\n", "versions": [{"version": "v1", "created": "Thu, 9 Nov 2017 15:05:59 GMT"}, {"version": "v2", "created": "Mon, 13 Nov 2017 21:34:30 GMT"}], "update_date": "2017-12-01", "authors_parsed": [["Suffoletto", "Brian", ""], ["Gharani", "Pedram", ""], ["Chung", "Tammy", ""], ["Karimi", "Hassan", ""]]}, {"id": "1711.03431", "submitter": "Dennis Forster", "authors": "Dennis Forster, J\\\"org L\\\"ucke", "title": "Can clustering scale sublinearly with its clusters? A variational EM\n  acceleration of GMMs and $k$-means", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  One iteration of standard $k$-means (i.e., Lloyd's algorithm) or standard EM\nfor Gaussian mixture models (GMMs) scales linearly with the number of clusters\n$C$, data points $N$, and data dimensionality $D$. In this study, we explore\nwhether one iteration of $k$-means or EM for GMMs can scale sublinearly with\n$C$ at run-time, while improving the clustering objective remains effective.\nThe tool we apply for complexity reduction is variational EM, which is\ntypically used to make training of generative models with exponentially many\nhidden states tractable. Here, we apply novel theoretical results on truncated\nvariational EM to make tractable clustering algorithms more efficient. The\nbasic idea is to use a partial variational E-step which reduces the linear\ncomplexity of $\\mathcal{O}(NCD)$ required for a full E-step to a sublinear\ncomplexity. Our main observation is that the linear dependency on $C$ can be\nreduced to a dependency on a much smaller parameter $G$ which relates to\ncluster neighborhood relations. We focus on two versions of partial variational\nEM for clustering: variational GMM, scaling with $\\mathcal{O}(NG^2D)$, and\nvariational $k$-means, scaling with $\\mathcal{O}(NGD)$ per iteration. Empirical\nresults show that these algorithms still require comparable numbers of\niterations to improve the clustering objective to same values as $k$-means. For\ndata with many clusters, we consequently observe reductions of net\ncomputational demands between two and three orders of magnitude. More\ngenerally, our results provide substantial empirical evidence in favor of\nclustering to scale sublinearly with $C$.\n", "versions": [{"version": "v1", "created": "Thu, 9 Nov 2017 15:44:06 GMT"}, {"version": "v2", "created": "Tue, 17 Apr 2018 15:21:20 GMT"}], "update_date": "2018-04-18", "authors_parsed": [["Forster", "Dennis", ""], ["L\u00fccke", "J\u00f6rg", ""]]}, {"id": "1711.03439", "submitter": "Ahmet Alacaoglu", "authors": "Ahmet Alacaoglu, Quoc Tran-Dinh, Olivier Fercoq and Volkan Cevher", "title": "Smooth Primal-Dual Coordinate Descent Algorithms for Nonsmooth Convex\n  Optimization", "comments": "NIPS 2017", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.OC stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a new randomized coordinate descent method for a convex\noptimization template with broad applications. Our analysis relies on a novel\ncombination of four ideas applied to the primal-dual gap function: smoothing,\nacceleration, homotopy, and coordinate descent with non-uniform sampling. As a\nresult, our method features the first convergence rate guarantees among the\ncoordinate descent methods, that are the best-known under a variety of common\nstructure assumptions on the template. We provide numerical evidence to support\nthe theoretical results with a comparison to state-of-the-art algorithms.\n", "versions": [{"version": "v1", "created": "Thu, 9 Nov 2017 16:00:52 GMT"}], "update_date": "2017-11-10", "authors_parsed": [["Alacaoglu", "Ahmet", ""], ["Tran-Dinh", "Quoc", ""], ["Fercoq", "Olivier", ""], ["Cevher", "Volkan", ""]]}, {"id": "1711.03440", "submitter": "Zhao Song", "authors": "Kai Zhong, Zhao Song, Inderjit S. Dhillon", "title": "Learning Non-overlapping Convolutional Neural Networks with Multiple\n  Kernels", "comments": "arXiv admin note: text overlap with arXiv:1706.03175", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.DS stat.ML", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  In this paper, we consider parameter recovery for non-overlapping\nconvolutional neural networks (CNNs) with multiple kernels. We show that when\nthe inputs follow Gaussian distribution and the sample size is sufficiently\nlarge, the squared loss of such CNNs is $\\mathit{~locally~strongly~convex}$ in\na basin of attraction near the global optima for most popular activation\nfunctions, like ReLU, Leaky ReLU, Squared ReLU, Sigmoid and Tanh. The required\nsample complexity is proportional to the dimension of the input and polynomial\nin the number of kernels and a condition number of the parameters. We also show\nthat tensor methods are able to initialize the parameters to the local strong\nconvex region. Hence, for most smooth activations, gradient descent following\ntensor initialization is guaranteed to converge to the global optimal with time\nthat is linear in input dimension, logarithmic in precision and polynomial in\nother factors. To the best of our knowledge, this is the first work that\nprovides recovery guarantees for CNNs with multiple kernels under polynomial\nsample and computational complexities.\n", "versions": [{"version": "v1", "created": "Wed, 8 Nov 2017 14:45:31 GMT"}], "update_date": "2017-11-10", "authors_parsed": [["Zhong", "Kai", ""], ["Song", "Zhao", ""], ["Dhillon", "Inderjit S.", ""]]}, {"id": "1711.03481", "submitter": "David Eriksson", "authors": "Kun Dong, David Eriksson, Hannes Nickisch, David Bindel, Andrew Gordon\n  Wilson", "title": "Scalable Log Determinants for Gaussian Process Kernel Learning", "comments": "Appears at Advances in Neural Information Processing Systems 30\n  (NIPS), 2017", "journal-ref": "Advances in Neural Information Processing Systems 30 (NIPS), 2017", "doi": null, "report-no": null, "categories": "stat.ML cs.AI cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  For applications as varied as Bayesian neural networks, determinantal point\nprocesses, elliptical graphical models, and kernel learning for Gaussian\nprocesses (GPs), one must compute a log determinant of an $n \\times n$ positive\ndefinite matrix, and its derivatives - leading to prohibitive\n$\\mathcal{O}(n^3)$ computations. We propose novel $\\mathcal{O}(n)$ approaches\nto estimating these quantities from only fast matrix vector multiplications\n(MVMs). These stochastic approximations are based on Chebyshev, Lanczos, and\nsurrogate models, and converge quickly even for kernel matrices that have\nchallenging spectra. We leverage these approximations to develop a scalable\nGaussian process approach to kernel learning. We find that Lanczos is generally\nsuperior to Chebyshev for kernel learning, and that a surrogate approach can be\nhighly efficient and accurate with popular kernels.\n", "versions": [{"version": "v1", "created": "Thu, 9 Nov 2017 17:25:30 GMT"}], "update_date": "2017-11-10", "authors_parsed": [["Dong", "Kun", ""], ["Eriksson", "David", ""], ["Nickisch", "Hannes", ""], ["Bindel", "David", ""], ["Wilson", "Andrew Gordon", ""]]}, {"id": "1711.03512", "submitter": "Gerrit van den Burg", "authors": "Gerrit J. J. van den Burg, Alfred O. Hero", "title": "Fast Meta-Learning for Adaptive Hierarchical Classifier Design", "comments": "Code available at: https://github.com/HeroResearchGroup/SmartSVM", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI cs.IT math.IT stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a new splitting criterion for a meta-learning approach to\nmulticlass classifier design that adaptively merges the classes into a\ntree-structured hierarchy of increasingly difficult binary classification\nproblems. The classification tree is constructed from empirical estimates of\nthe Henze-Penrose bounds on the pairwise Bayes misclassification rates that\nrank the binary subproblems in terms of difficulty of classification. The\nproposed empirical estimates of the Bayes error rate are computed from the\nminimal spanning tree (MST) of the samples from each pair of classes. Moreover,\na meta-learning technique is presented for quantifying the one-vs-rest Bayes\nerror rate for each individual class from a single MST on the entire dataset.\nExtensive simulations on benchmark datasets show that the proposed hierarchical\nmethod can often be learned much faster than competing methods, while achieving\ncompetitive accuracy.\n", "versions": [{"version": "v1", "created": "Thu, 9 Nov 2017 18:22:32 GMT"}], "update_date": "2017-11-10", "authors_parsed": [["Burg", "Gerrit J. J. van den", ""], ["Hero", "Alfred O.", ""]]}, {"id": "1711.03539", "submitter": "Fang Liu", "authors": "Fang Liu, Joohyun Lee, Ness Shroff", "title": "A Change-Detection based Framework for Piecewise-stationary Multi-Armed\n  Bandit Problem", "comments": "accepted by AAAI 2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The multi-armed bandit problem has been extensively studied under the\nstationary assumption. However in reality, this assumption often does not hold\nbecause the distributions of rewards themselves may change over time. In this\npaper, we propose a change-detection (CD) based framework for multi-armed\nbandit problems under the piecewise-stationary setting, and study a class of\nchange-detection based UCB (Upper Confidence Bound) policies, CD-UCB, that\nactively detects change points and restarts the UCB indices. We then develop\nCUSUM-UCB and PHT-UCB, that belong to the CD-UCB class and use cumulative sum\n(CUSUM) and Page-Hinkley Test (PHT) to detect changes. We show that CUSUM-UCB\nobtains the best known regret upper bound under mild assumptions. We also\ndemonstrate the regret reduction of the CD-UCB policies over arbitrary\nBernoulli rewards and Yahoo! datasets of webpage click-through rates.\n", "versions": [{"version": "v1", "created": "Wed, 8 Nov 2017 22:48:20 GMT"}, {"version": "v2", "created": "Mon, 20 Nov 2017 21:25:15 GMT"}], "update_date": "2017-11-22", "authors_parsed": [["Liu", "Fang", ""], ["Lee", "Joohyun", ""], ["Shroff", "Ness", ""]]}, {"id": "1711.03543", "submitter": "Anush Sankaran", "authors": "Akshay Sethi, Anush Sankaran, Naveen Panwar, Shreya Khare, Senthil\n  Mani", "title": "DLPaper2Code: Auto-generation of Code from Deep Learning Research Papers", "comments": "AAAI2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  With an abundance of research papers in deep learning, reproducibility or\nadoption of the existing works becomes a challenge. This is due to the lack of\nopen source implementations provided by the authors. Further, re-implementing\nresearch papers in a different library is a daunting task. To address these\nchallenges, we propose a novel extensible approach, DLPaper2Code, to extract\nand understand deep learning design flow diagrams and tables available in a\nresearch paper and convert them to an abstract computational graph. The\nextracted computational graph is then converted into execution ready source\ncode in both Keras and Caffe, in real-time. An arXiv-like website is created\nwhere the automatically generated designs is made publicly available for 5,000\nresearch papers. The generated designs could be rated and edited using an\nintuitive drag-and-drop UI framework in a crowdsourced manner. To evaluate our\napproach, we create a simulated dataset with over 216,000 valid design\nvisualizations using a manually defined grammar. Experiments on the simulated\ndataset show that the proposed framework provide more than $93\\%$ accuracy in\nflow diagram content extraction.\n", "versions": [{"version": "v1", "created": "Thu, 9 Nov 2017 10:00:19 GMT"}], "update_date": "2017-11-13", "authors_parsed": [["Sethi", "Akshay", ""], ["Sankaran", "Anush", ""], ["Panwar", "Naveen", ""], ["Khare", "Shreya", ""], ["Mani", "Senthil", ""]]}, {"id": "1711.03560", "submitter": "Francisco Ruiz", "authors": "Francisco J. R. Ruiz, Susan Athey, David M. Blei", "title": "SHOPPER: A Probabilistic Model of Consumer Choice with Substitutes and\n  Complements", "comments": "Published at Annals of Applied Statistics. 27 pages, 4 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG econ.EM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We develop SHOPPER, a sequential probabilistic model of shopping data.\nSHOPPER uses interpretable components to model the forces that drive how a\ncustomer chooses products; in particular, we designed SHOPPER to capture how\nitems interact with other items. We develop an efficient posterior inference\nalgorithm to estimate these forces from large-scale data, and we analyze a\nlarge dataset from a major chain grocery store. We are interested in answering\ncounterfactual queries about changes in prices. We found that SHOPPER provides\naccurate predictions even under price interventions, and that it helps identify\ncomplementary and substitutable pairs of products.\n", "versions": [{"version": "v1", "created": "Thu, 9 Nov 2017 19:04:21 GMT"}, {"version": "v2", "created": "Mon, 2 Jul 2018 13:51:36 GMT"}, {"version": "v3", "created": "Sun, 9 Jun 2019 18:16:17 GMT"}], "update_date": "2019-06-11", "authors_parsed": [["Ruiz", "Francisco J. R.", ""], ["Athey", "Susan", ""], ["Blei", "David M.", ""]]}, {"id": "1711.03623", "submitter": "Ines Wilms", "authors": "Ines Wilms, Sumanta Basu, Jacob Bien, David S. Matteson", "title": "Interpretable Vector AutoRegressions with Exogenous Time Series", "comments": "Presented at NIPS 2017 Symposium on Interpretable Machine Learning", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The Vector AutoRegressive (VAR) model is fundamental to the study of\nmultivariate time series. Although VAR models are intensively investigated by\nmany researchers, practitioners often show more interest in analyzing VARX\nmodels that incorporate the impact of unmodeled exogenous variables (X) into\nthe VAR. However, since the parameter space grows quadratically with the number\nof time series, estimation quickly becomes challenging. While several proposals\nhave been made to sparsely estimate large VAR models, the estimation of large\nVARX models is under-explored. Moreover, typically these sparse proposals\ninvolve a lasso-type penalty and do not incorporate lag selection into the\nestimation procedure. As a consequence, the resulting models may be difficult\nto interpret. In this paper, we propose a lag-based hierarchically sparse\nestimator, called \"HVARX\", for large VARX models. We illustrate the usefulness\nof HVARX on a cross-category management marketing application. Our results show\nhow it provides a highly interpretable model, and improves out-of-sample\nforecast accuracy compared to a lasso-type approach.\n", "versions": [{"version": "v1", "created": "Thu, 9 Nov 2017 22:15:33 GMT"}], "update_date": "2017-11-13", "authors_parsed": [["Wilms", "Ines", ""], ["Basu", "Sumanta", ""], ["Bien", "Jacob", ""], ["Matteson", "David S.", ""]]}, {"id": "1711.03634", "submitter": "Niladri Chatterji", "authors": "Niladri S. Chatterji and Peter L. Bartlett", "title": "Alternating minimization for dictionary learning: Local Convergence\n  Guarantees", "comments": "Erratum: An earlier version of this paper appeared in NIPS 2017 which\n  had an erroneous claim about convergence guarantees with random\n  initialization. The main result -- Theorem 3 -- has been corrected by adding\n  an assumption about the initialization (Assumption B1)", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present theoretical guarantees for an alternating minimization algorithm\nfor the dictionary learning/sparse coding problem. The dictionary learning\nproblem is to factorize vector samples $y^{1},y^{2},\\ldots, y^{n}$ into an\nappropriate basis (dictionary) $A^*$ and sparse vectors $x^{1*},\\ldots,x^{n*}$.\nOur algorithm is a simple alternating minimization procedure that switches\nbetween $\\ell_1$ minimization and gradient descent in alternate steps.\nDictionary learning and specifically alternating minimization algorithms for\ndictionary learning are well studied both theoretically and empirically.\nHowever, in contrast to previous theoretical analyses for this problem, we\nreplace a condition on the operator norm (that is, the largest magnitude\nsingular value) of the true underlying dictionary $A^*$ with a condition on the\nmatrix infinity norm (that is, the largest magnitude term). Our guarantees are\nunder a reasonable generative model that allows for dictionaries with growing\noperator norms, and can handle an arbitrary level of overcompleteness, while\nhaving sparsity that is information theoretically optimal. We also establish\nupper bounds on the sample complexity of our algorithm.\n", "versions": [{"version": "v1", "created": "Thu, 9 Nov 2017 22:55:35 GMT"}, {"version": "v2", "created": "Wed, 22 Nov 2017 19:35:13 GMT"}, {"version": "v3", "created": "Tue, 30 Jan 2018 04:37:54 GMT"}, {"version": "v4", "created": "Tue, 30 Jul 2019 23:29:38 GMT"}], "update_date": "2019-08-01", "authors_parsed": [["Chatterji", "Niladri S.", ""], ["Bartlett", "Peter L.", ""]]}, {"id": "1711.03637", "submitter": "Shruti Kulkarni", "authors": "Shruti R. Kulkarni, John M. Alexiades, Bipin Rajendran", "title": "Learning and Real-time Classification of Hand-written Digits With\n  Spiking Neural Networks", "comments": "4 pages, 4 figures, 1 table, accepted at ICECS 2017", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.AI cs.LG cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We describe a novel spiking neural network (SNN) for automated, real-time\nhandwritten digit classification and its implementation on a GP-GPU platform.\nInformation processing within the network, from feature extraction to\nclassification is implemented by mimicking the basic aspects of neuronal spike\ninitiation and propagation in the brain. The feature extraction layer of the\nSNN uses fixed synaptic weight maps to extract the key features of the image\nand the classifier layer uses the recently developed NormAD approximate\ngradient descent based supervised learning algorithm for spiking neural\nnetworks to adjust the synaptic weights. On the standard MNIST database images\nof handwritten digits, our network achieves an accuracy of 99.80% on the\ntraining set and 98.06% on the test set, with nearly 7x fewer parameters\ncompared to the state-of-the-art spiking networks. We further use this network\nin a GPU based user-interface system demonstrating real-time SNN simulation to\ninfer digits written by different users. On a test set of 500 such images, this\nreal-time platform achieves an accuracy exceeding 97% while making a prediction\nwithin an SNN emulation time of less than 100ms.\n", "versions": [{"version": "v1", "created": "Thu, 9 Nov 2017 23:01:42 GMT"}], "update_date": "2017-11-13", "authors_parsed": [["Kulkarni", "Shruti R.", ""], ["Alexiades", "John M.", ""], ["Rajendran", "Bipin", ""]]}, {"id": "1711.03638", "submitter": "Thanh Nguyen", "authors": "Thanh V. Nguyen, Raymond K. W. Wong, and Chinmay Hegde", "title": "Provably Accurate Double-Sparse Coding", "comments": "40 pages. An abbreviated conference version appears at AAAI 2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Sparse coding is a crucial subroutine in algorithms for various signal\nprocessing, deep learning, and other machine learning applications. The central\ngoal is to learn an overcomplete dictionary that can sparsely represent a given\ninput dataset. However, a key challenge is that storage, transmission, and\nprocessing of the learned dictionary can be untenably high if the data\ndimension is high. In this paper, we consider the double-sparsity model\nintroduced by Rubinstein et al. (2010b) where the dictionary itself is the\nproduct of a fixed, known basis and a data-adaptive sparse component. First, we\nintroduce a simple algorithm for double-sparse coding that can be amenable to\nefficient implementation via neural architectures. Second, we theoretically\nanalyze its performance and demonstrate asymptotic sample complexity and\nrunning time benefits over existing (provable) approaches for sparse coding. To\nour knowledge, our work introduces the first computationally efficient\nalgorithm for double-sparse coding that enjoys rigorous statistical guarantees.\nFinally, we support our analysis via several numerical experiments on simulated\ndata, confirming that our method can indeed be useful in problem sizes\nencountered in practical applications.\n", "versions": [{"version": "v1", "created": "Thu, 9 Nov 2017 23:06:15 GMT"}, {"version": "v2", "created": "Tue, 12 Dec 2017 22:20:25 GMT"}], "update_date": "2017-12-14", "authors_parsed": [["Nguyen", "Thanh V.", ""], ["Wong", "Raymond K. W.", ""], ["Hegde", "Chinmay", ""]]}, {"id": "1711.03640", "submitter": "Anakha Vasanthakumari Babu", "authors": "Anakha V Babu, Bipin Rajendran", "title": "Stochastic Deep Learning in Memristive Networks", "comments": "4 pages, 5 figures, accepted at ICECS 2017", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.AI cs.LG cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study the performance of stochastically trained deep neural networks\n(DNNs) whose synaptic weights are implemented using emerging memristive devices\nthat exhibit limited dynamic range, resolution, and variability in their\nprogramming characteristics. We show that a key device parameter to optimize\nthe learning efficiency of DNNs is the variability in its programming\ncharacteristics. DNNs with such memristive synapses, even with dynamic range as\nlow as $15$ and only $32$ discrete levels, when trained based on stochastic\nupdates suffer less than $3\\%$ loss in accuracy compared to floating point\nsoftware baseline. We also study the performance of stochastic memristive DNNs\nwhen used as inference engines with noise corrupted data and find that if the\ndevice variability can be minimized, the relative degradation in performance\nfor the Stochastic DNN is better than that of the software baseline. Hence, our\nstudy presents a new optimization corner for memristive devices for building\nlarge noise-immune deep learning systems.\n", "versions": [{"version": "v1", "created": "Thu, 9 Nov 2017 23:09:36 GMT"}], "update_date": "2017-11-13", "authors_parsed": [["Babu", "Anakha V", ""], ["Rajendran", "Bipin", ""]]}, {"id": "1711.03654", "submitter": "Anthony Perez", "authors": "Anthony Perez, Christopher Yeh, George Azzari, Marshall Burke, David\n  Lobell, Stefano Ermon", "title": "Poverty Prediction with Public Landsat 7 Satellite Imagery and Machine\n  Learning", "comments": "Presented at NIPS 2017 Workshop on Machine Learning for the\n  Developing World", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Obtaining detailed and reliable data about local economic livelihoods in\ndeveloping countries is expensive, and data are consequently scarce. Previous\nwork has shown that it is possible to measure local-level economic livelihoods\nusing high-resolution satellite imagery. However, such imagery is relatively\nexpensive to acquire, often not updated frequently, and is mainly available for\nrecent years. We train CNN models on free and publicly available multispectral\ndaytime satellite images of the African continent from the Landsat 7 satellite,\nwhich has collected imagery with global coverage for almost two decades. We\nshow that despite these images' lower resolution, we can achieve accuracies\nthat exceed previous benchmarks.\n", "versions": [{"version": "v1", "created": "Fri, 10 Nov 2017 00:21:54 GMT"}], "update_date": "2017-11-13", "authors_parsed": [["Perez", "Anthony", ""], ["Yeh", "Christopher", ""], ["Azzari", "George", ""], ["Burke", "Marshall", ""], ["Lobell", "David", ""], ["Ermon", "Stefano", ""]]}, {"id": "1711.03656", "submitter": "Se Eun Oh", "authors": "Se Eun Oh, Saikrishna Sunkam, Nicholas Hopper", "title": "p-FP: Extraction, Classification, and Prediction of Website Fingerprints\n  with Deep Learning", "comments": "Under submission", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CR cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recent advances in learning Deep Neural Network (DNN) architectures have\nreceived a great deal of attention due to their ability to outperform\nstate-of-the-art classifiers across a wide range of applications, with little\nor no feature engineering. In this paper, we broadly study the applicability of\ndeep learning to website fingerprinting. We show that unsupervised DNNs can be\nused to extract low-dimensional feature vectors that improve the performance of\nstate-of-the-art website fingerprinting attacks. When used as classifiers, we\nshow that they can match or exceed performance of existing attacks across a\nrange of application scenarios, including fingerprinting Tor website traces,\nfingerprinting search engine queries over Tor, defeating fingerprinting\ndefenses, and fingerprinting TLS-encrypted websites. Finally, we show that DNNs\ncan be used to predict the fingerprintability of a website based on its\ncontents, achieving 99% accuracy on a data set of 4500 website downloads.\n", "versions": [{"version": "v1", "created": "Fri, 10 Nov 2017 00:56:20 GMT"}, {"version": "v2", "created": "Mon, 2 Apr 2018 15:48:04 GMT"}], "update_date": "2018-04-03", "authors_parsed": [["Oh", "Se Eun", ""], ["Sunkam", "Saikrishna", ""], ["Hopper", "Nicholas", ""]]}, {"id": "1711.03674", "submitter": "Krzysztof J. Geras", "authors": "Nan Wu, Krzysztof J. Geras, Yiqiu Shen, Jingyi Su, S. Gene Kim, Eric\n  Kim, Stacey Wolfson, Linda Moy, Kyunghyun Cho", "title": "Breast density classification with deep convolutional neural networks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Breast density classification is an essential part of breast cancer\nscreening. Although a lot of prior work considered this problem as a task for\nlearning algorithms, to our knowledge, all of them used small and not\nclinically realistic data both for training and evaluation of their models. In\nthis work, we explore the limits of this task with a data set coming from over\n200,000 breast cancer screening exams. We use this data to train and evaluate a\nstrong convolutional neural network classifier. In a reader study, we find that\nour model can perform this task comparably to a human expert.\n", "versions": [{"version": "v1", "created": "Fri, 10 Nov 2017 02:50:46 GMT"}], "update_date": "2017-11-13", "authors_parsed": [["Wu", "Nan", ""], ["Geras", "Krzysztof J.", ""], ["Shen", "Yiqiu", ""], ["Su", "Jingyi", ""], ["Kim", "S. Gene", ""], ["Kim", "Eric", ""], ["Wolfson", "Stacey", ""], ["Moy", "Linda", ""], ["Cho", "Kyunghyun", ""]]}, {"id": "1711.03689", "submitter": "Taku Kato", "authors": "Taku Kato, Takahiro Shinozaki", "title": "Reinforcement Learning of Speech Recognition System Based on Policy\n  Gradient and Hypothesis Selection", "comments": "5 pages, 6 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Speech recognition systems have achieved high recognition performance for\nseveral tasks. However, the performance of such systems is dependent on the\ntremendously costly development work of preparing vast amounts of task-matched\ntranscribed speech data for supervised training. The key problem here is the\ncost of transcribing speech data. The cost is repeatedly required to support\nnew languages and new tasks. Assuming broad network services for transcribing\nspeech data for many users, a system would become more self-sufficient and more\nuseful if it possessed the ability to learn from very light feedback from the\nusers without annoying them. In this paper, we propose a general reinforcement\nlearning framework for speech recognition systems based on the policy gradient\nmethod. As a particular instance of the framework, we also propose a hypothesis\nselection-based reinforcement learning method. The proposed framework provides\na new view for several existing training and adaptation methods. The\nexperimental results show that the proposed method improves the recognition\nperformance compared to unsupervised adaptation.\n", "versions": [{"version": "v1", "created": "Fri, 10 Nov 2017 04:42:44 GMT"}], "update_date": "2017-11-13", "authors_parsed": [["Kato", "Taku", ""], ["Shinozaki", "Takahiro", ""]]}, {"id": "1711.03712", "submitter": "Seongsik Park", "authors": "Seongsik Park, Seijoon Kim, Seil Lee, Ho Bae, Sungroh Yoon", "title": "Quantized Memory-Augmented Neural Networks", "comments": "AAAI 2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Memory-augmented neural networks (MANNs) refer to a class of neural network\nmodels equipped with external memory (such as neural Turing machines and memory\nnetworks). These neural networks outperform conventional recurrent neural\nnetworks (RNNs) in terms of learning long-term dependency, allowing them to\nsolve intriguing AI tasks that would otherwise be hard to address. This paper\nconcerns the problem of quantizing MANNs. Quantization is known to be effective\nwhen we deploy deep models on embedded systems with limited resources.\nFurthermore, quantization can substantially reduce the energy consumption of\nthe inference procedure. These benefits justify recent developments of\nquantized multi layer perceptrons, convolutional networks, and RNNs. However,\nno prior work has reported the successful quantization of MANNs. The in-depth\nanalysis presented here reveals various challenges that do not appear in the\nquantization of the other networks. Without addressing them properly, quantized\nMANNs would normally suffer from excessive quantization error which leads to\ndegraded performance. In this paper, we identify memory addressing\n(specifically, content-based addressing) as the main reason for the performance\ndegradation and propose a robust quantization method for MANNs to address the\nchallenge. In our experiments, we achieved a computation-energy gain of 22x\nwith 8-bit fixed-point and binary quantization compared to the floating-point\nimplementation. Measured on the bAbI dataset, the resulting model, named the\nquantized MANN (Q-MANN), improved the error rate by 46% and 30% with 8-bit\nfixed-point and binary quantization, respectively, compared to the MANN\nquantized using conventional techniques.\n", "versions": [{"version": "v1", "created": "Fri, 10 Nov 2017 06:54:45 GMT"}], "update_date": "2017-11-13", "authors_parsed": [["Park", "Seongsik", ""], ["Kim", "Seijoon", ""], ["Lee", "Seil", ""], ["Bae", "Ho", ""], ["Yoon", "Sungroh", ""]]}, {"id": "1711.03822", "submitter": "Nicol\\`o Navarin", "authors": "Nicol\\`o Navarin, Beatrice Vincenzi, Mirko Polato and Alessandro\n  Sperduti", "title": "LSTM Networks for Data-Aware Remaining Time Prediction of Business\n  Process Instances", "comments": "Article accepted for publication in 2017 IEEE Symposium on Deep\n  Learning (IEEE DL'17) @ SSCI", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Predicting the completion time of business process instances would be a very\nhelpful aid when managing processes under service level agreement constraints.\nThe ability to know in advance the trend of running process instances would\nallow business managers to react in time, in order to prevent delays or\nundesirable situations. However, making such accurate forecasts is not easy:\nmany factors may influence the required time to complete a process instance. In\nthis paper, we propose an approach based on deep Recurrent Neural Networks\n(specifically LSTMs) that is able to exploit arbitrary information associated\nto single events, in order to produce an as-accurate-as-possible prediction of\nthe completion time of running instances. Experiments on real-world datasets\nconfirm the quality of our proposal.\n", "versions": [{"version": "v1", "created": "Fri, 10 Nov 2017 13:58:43 GMT"}], "update_date": "2017-11-13", "authors_parsed": [["Navarin", "Nicol\u00f2", ""], ["Vincenzi", "Beatrice", ""], ["Polato", "Mirko", ""], ["Sperduti", "Alessandro", ""]]}, {"id": "1711.03845", "submitter": "Joachim van der Herten", "authors": "Nicolas Knudde, Joachim van der Herten, Tom Dhaene, Ivo Couckuyt", "title": "GPflowOpt: A Bayesian Optimization Library using TensorFlow", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A novel Python framework for Bayesian optimization known as GPflowOpt is\nintroduced. The package is based on the popular GPflow library for Gaussian\nprocesses, leveraging the benefits of TensorFlow including automatic\ndifferentiation, parallelization and GPU computations for Bayesian\noptimization. Design goals focus on a framework that is easy to extend with\ncustom acquisition functions and models. The framework is thoroughly tested and\nwell documented, and provides scalability. The current released version of\nGPflowOpt includes some standard single-objective acquisition functions, the\nstate-of-the-art max-value entropy search, as well as a Bayesian\nmulti-objective approach. Finally, it permits easy use of custom modeling\nstrategies implemented in GPflow.\n", "versions": [{"version": "v1", "created": "Fri, 10 Nov 2017 15:00:54 GMT"}], "update_date": "2017-11-13", "authors_parsed": [["Knudde", "Nicolas", ""], ["van der Herten", "Joachim", ""], ["Dhaene", "Tom", ""], ["Couckuyt", "Ivo", ""]]}, {"id": "1711.03846", "submitter": "Brett Israelsen", "authors": "Brett W Israelsen, Nisar R Ahmed", "title": "\"Dave...I can assure you...that it's going to be all right...\" -- A\n  definition, case for, and survey of algorithmic assurances in human-autonomy\n  trust relationships", "comments": "final version of accepted manuscript", "journal-ref": null, "doi": "10.1145/3267338", "report-no": null, "categories": "cs.CY cs.AI cs.HC cs.RO stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  People who design, use, and are affected by autonomous artificially\nintelligent agents want to be able to \\emph{trust} such agents -- that is, to\nknow that these agents will perform correctly, to understand the reasoning\nbehind their actions, and to know how to use them appropriately. Many\ntechniques have been devised to assess and influence human trust in\nartificially intelligent agents. However, these approaches are typically ad\nhoc, and have not been formally related to each other or to formal trust\nmodels. This paper presents a survey of \\emph{algorithmic assurances}, i.e.\nprogrammed components of agent operation that are expressly designed to\ncalibrate user trust in artificially intelligent agents. Algorithmic assurances\nare first formally defined and classified from the perspective of formally\nmodeled human-artificially intelligent agent trust relationships. Building on\nthese definitions, a synthesis of research across communities such as machine\nlearning, human-computer interaction, robotics, e-commerce, and others reveals\nthat assurance algorithms naturally fall along a spectrum in terms of their\nimpact on an agent's core functionality, with seven notable classes ranging\nfrom integral assurances (which impact an agent's core functionality) to\nsupplemental assurances (which have no direct effect on agent performance).\nCommon approaches within each of these classes are identified and discussed;\nbenefits and drawbacks of different approaches are also investigated.\n", "versions": [{"version": "v1", "created": "Wed, 8 Nov 2017 19:00:29 GMT"}, {"version": "v2", "created": "Tue, 14 Nov 2017 17:38:47 GMT"}, {"version": "v3", "created": "Wed, 4 Jul 2018 19:03:43 GMT"}, {"version": "v4", "created": "Tue, 28 Aug 2018 17:07:30 GMT"}], "update_date": "2019-02-06", "authors_parsed": [["Israelsen", "Brett W", ""], ["Ahmed", "Nisar R", ""]]}, {"id": "1711.03905", "submitter": "Jayaraman J. Thiagarajan", "authors": "Huan Song, Deepta Rajan, Jayaraman J. Thiagarajan and Andreas Spanias", "title": "Attend and Diagnose: Clinical Time Series Analysis using Attention\n  Models", "comments": "AAAI 2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  With widespread adoption of electronic health records, there is an increased\nemphasis for predictive models that can effectively deal with clinical\ntime-series data. Powered by Recurrent Neural Network (RNN) architectures with\nLong Short-Term Memory (LSTM) units, deep neural networks have achieved\nstate-of-the-art results in several clinical prediction tasks. Despite the\nsuccess of RNNs, its sequential nature prohibits parallelized computing, thus\nmaking it inefficient particularly when processing long sequences. Recently,\narchitectures which are based solely on attention mechanisms have shown\nremarkable success in transduction tasks in NLP, while being computationally\nsuperior. In this paper, for the first time, we utilize attention models for\nclinical time-series modeling, thereby dispensing recurrence entirely. We\ndevelop the \\textit{SAnD} (Simply Attend and Diagnose) architecture, which\nemploys a masked, self-attention mechanism, and uses positional encoding and\ndense interpolation strategies for incorporating temporal order. Furthermore,\nwe develop a multi-task variant of \\textit{SAnD} to jointly infer models with\nmultiple diagnosis tasks. Using the recent MIMIC-III benchmark datasets, we\ndemonstrate that the proposed approach achieves state-of-the-art performance in\nall tasks, outperforming LSTM models and classical baselines with\nhand-engineered features.\n", "versions": [{"version": "v1", "created": "Fri, 10 Nov 2017 16:26:14 GMT"}, {"version": "v2", "created": "Sun, 19 Nov 2017 21:19:12 GMT"}], "update_date": "2018-07-17", "authors_parsed": [["Song", "Huan", ""], ["Rajan", "Deepta", ""], ["Thiagarajan", "Jayaraman J.", ""], ["Spanias", "Andreas", ""]]}, {"id": "1711.03946", "submitter": "Robert Bamler", "authors": "Geng Ji, Robert Bamler, Erik B. Sudderth, Stephan Mandt", "title": "Bayesian Paragraph Vectors", "comments": "Presented at the NIPS 2017 workshop \"Advances in Approximate Bayesian\n  Inference\"", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Word2vec (Mikolov et al., 2013) has proven to be successful in natural\nlanguage processing by capturing the semantic relationships between different\nwords. Built on top of single-word embeddings, paragraph vectors (Le and\nMikolov, 2014) find fixed-length representations for pieces of text with\narbitrary lengths, such as documents, paragraphs, and sentences. In this work,\nwe propose a novel interpretation for neural-network-based paragraph vectors by\ndeveloping an unsupervised generative model whose maximum likelihood solution\ncorresponds to traditional paragraph vectors. This probabilistic formulation\nallows us to go beyond point estimates of parameters and to perform Bayesian\nposterior inference. We find that the entropy of paragraph vectors decreases\nwith the length of documents, and that information about posterior uncertainty\nimproves performance in supervised learning tasks such as sentiment analysis\nand paraphrase detection.\n", "versions": [{"version": "v1", "created": "Fri, 10 Nov 2017 18:09:15 GMT"}, {"version": "v2", "created": "Thu, 7 Dec 2017 20:37:31 GMT"}], "update_date": "2017-12-11", "authors_parsed": [["Ji", "Geng", ""], ["Bamler", "Robert", ""], ["Sudderth", "Erik B.", ""], ["Mandt", "Stephan", ""]]}, {"id": "1711.03947", "submitter": "Michael Smith", "authors": "Michael R. Smith, Joe B. Ingram, Christopher C. Lamb, Timothy J.\n  Draelos, Justin E. Doak, James B. Aimone, Conrad D. James", "title": "Dynamic Analysis of Executables to Detect and Characterize Malware", "comments": "9 pages, 6 Tables, 4 Figures", "journal-ref": null, "doi": null, "report-no": "SAND2018-11011 C", "categories": "cs.CR stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  It is needed to ensure the integrity of systems that process sensitive\ninformation and control many aspects of everyday life. We examine the use of\nmachine learning algorithms to detect malware using the system calls generated\nby executables-alleviating attempts at obfuscation as the behavior is monitored\nrather than the bytes of an executable. We examine several machine learning\ntechniques for detecting malware including random forests, deep learning\ntechniques, and liquid state machines. The experiments examine the effects of\nconcept drift on each algorithm to understand how well the algorithms\ngeneralize to novel malware samples by testing them on data that was collected\nafter the training data. The results suggest that each of the examined machine\nlearning algorithms is a viable solution to detect malware-achieving between\n90% and 95% class-averaged accuracy (CAA). In real-world scenarios, the\nperformance evaluation on an operational network may not match the performance\nachieved in training. Namely, the CAA may be about the same, but the values for\nprecision and recall over the malware can change significantly. We structure\nexperiments to highlight these caveats and offer insights into expected\nperformance in operational environments. In addition, we use the induced models\nto gain a better understanding about what differentiates the malware samples\nfrom the goodware, which can further be used as a forensics tool to understand\nwhat the malware (or goodware) was doing to provide directions for\ninvestigation and remediation.\n", "versions": [{"version": "v1", "created": "Fri, 10 Nov 2017 18:18:41 GMT"}, {"version": "v2", "created": "Fri, 28 Sep 2018 16:15:55 GMT"}], "update_date": "2018-10-01", "authors_parsed": [["Smith", "Michael R.", ""], ["Ingram", "Joe B.", ""], ["Lamb", "Christopher C.", ""], ["Draelos", "Timothy J.", ""], ["Doak", "Justin E.", ""], ["Aimone", "James B.", ""], ["James", "Conrad D.", ""]]}, {"id": "1711.03985", "submitter": "Mufti Mahmud", "authors": "Mufti Mahmud, M. Shamim Kaiser, Amir Hussain, Stefano Vassanelli", "title": "Applications of Deep Learning and Reinforcement Learning to Biological\n  Data", "comments": "33 pages, 5 figures, 1 table, survey paper, IEEE Trans. Neural Netw.\n  Learn. Syst., 2018", "journal-ref": null, "doi": "10.1109/TNNLS.2018.2790388", "report-no": null, "categories": "cs.LG stat.ML", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Rapid advances of hardware-based technologies during the past decades have\nopened up new possibilities for Life scientists to gather multimodal data in\nvarious application domains (e.g., Omics, Bioimaging, Medical Imaging, and\n[Brain/Body]-Machine Interfaces), thus generating novel opportunities for\ndevelopment of dedicated data intensive machine learning techniques. Overall,\nrecent research in Deep learning (DL), Reinforcement learning (RL), and their\ncombination (Deep RL) promise to revolutionize Artificial Intelligence. The\ngrowth in computational power accompanied by faster and increased data storage\nand declining computing costs have already allowed scientists in various fields\nto apply these techniques on datasets that were previously intractable for\ntheir size and complexity. This review article provides a comprehensive survey\non the application of DL, RL, and Deep RL techniques in mining Biological data.\nIn addition, we compare performances of DL techniques when applied to different\ndatasets across various application domains. Finally, we outline open issues in\nthis challenging research area and discuss future development perspectives.\n", "versions": [{"version": "v1", "created": "Fri, 10 Nov 2017 19:06:46 GMT"}, {"version": "v2", "created": "Sun, 7 Jan 2018 07:06:20 GMT"}], "update_date": "2018-01-09", "authors_parsed": [["Mahmud", "Mufti", ""], ["Kaiser", "M. Shamim", ""], ["Hussain", "Amir", ""], ["Vassanelli", "Stefano", ""]]}, {"id": "1711.04015", "submitter": "Kuan Liu", "authors": "Kuan Liu and Prem Natarajan", "title": "WMRB: Learning to Rank in a Scalable Batch Training Approach", "comments": "RecSys 2017 Poster Proceedings, August 27-31, Como, Italy", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.IR cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a new learning to rank algorithm, named Weighted Margin-Rank Batch\nloss (WMRB), to extend the popular Weighted Approximate-Rank Pairwise loss\n(WARP). WMRB uses a new rank estimator and an efficient batch training\nalgorithm. The approach allows more accurate item rank approximation and\nexplicit utilization of parallel computation to accelerate training. In three\nitem recommendation tasks, WMRB consistently outperforms WARP and other\nbaselines. Moreover, WMRB shows clear time efficiency advantages as data scale\nincreases.\n", "versions": [{"version": "v1", "created": "Fri, 10 Nov 2017 21:18:21 GMT"}], "update_date": "2017-11-15", "authors_parsed": [["Liu", "Kuan", ""], ["Natarajan", "Prem", ""]]}, {"id": "1711.04019", "submitter": "Kuan Liu", "authors": "Kuan Liu and Prem Natarajan", "title": "A Batch Learning Framework for Scalable Personalized Ranking", "comments": "AAAI 2018, Feb 2-7, New Orleans, USA", "journal-ref": "AAAI Conference on Artificial Intelligence 2018; Thirty-Second\n  AAAI Conference on Artificial Intelligence", "doi": null, "report-no": null, "categories": "stat.ML cs.IR cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In designing personalized ranking algorithms, it is desirable to encourage a\nhigh precision at the top of the ranked list. Existing methods either seek a\nsmooth convex surrogate for a non-smooth ranking metric or directly modify\nupdating procedures to encourage top accuracy. In this work we point out that\nthese methods do not scale well to a large-scale setting, and this is partly\ndue to the inaccurate pointwise or pairwise rank estimation. We propose a new\nframework for personalized ranking. It uses batch-based rank estimators and\nsmooth rank-sensitive loss functions. This new batch learning framework leads\nto more stable and accurate rank approximations compared to previous work.\nMoreover, it enables explicit use of parallel computation to speed up training.\nWe conduct empirical evaluation on three item recommendation tasks. Our method\nshows consistent accuracy improvements over state-of-the-art methods.\nAdditionally, we observe time efficiency advantages when data scale increases.\n", "versions": [{"version": "v1", "created": "Fri, 10 Nov 2017 21:25:30 GMT"}], "update_date": "2018-08-15", "authors_parsed": [["Liu", "Kuan", ""], ["Natarajan", "Prem", ""]]}, {"id": "1711.04043", "submitter": "Victor Garcia Satorras", "authors": "Victor Garcia, Joan Bruna", "title": "Few-Shot Learning with Graph Neural Networks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose to study the problem of few-shot learning with the prism of\ninference on a partially observed graphical model, constructed from a\ncollection of input images whose label can be either observed or not. By\nassimilating generic message-passing inference algorithms with their\nneural-network counterparts, we define a graph neural network architecture that\ngeneralizes several of the recently proposed few-shot learning models. Besides\nproviding improved numerical performance, our framework is easily extended to\nvariants of few-shot learning, such as semi-supervised or active learning,\ndemonstrating the ability of graph-based models to operate well on 'relational'\ntasks.\n", "versions": [{"version": "v1", "created": "Fri, 10 Nov 2017 23:32:47 GMT"}, {"version": "v2", "created": "Mon, 29 Jan 2018 12:13:06 GMT"}, {"version": "v3", "created": "Tue, 20 Feb 2018 16:52:36 GMT"}], "update_date": "2018-02-21", "authors_parsed": [["Garcia", "Victor", ""], ["Bruna", "Joan", ""]]}, {"id": "1711.04078", "submitter": "Baihan Lin", "authors": "Avinash Bukkittu, Baihan Lin, Trung Vu, Itsik Pe'er", "title": "Parkinson's Disease Digital Biomarker Discovery with Optimized\n  Transitions and Inferred Markov Emissions", "comments": "10th RECOMB/ISCB Conference on Regulatory & Systems Genomics with\n  DREAM Challenges", "journal-ref": null, "doi": null, "report-no": null, "categories": "q-bio.QM cs.AI stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We search for digital biomarkers from Parkinson's Disease by observing\napproximate repetitive patterns matching hypothesized step and stride periodic\ncycles. These observations were modeled as a cycle of hidden states with\nrandomness allowing deviation from a canonical pattern of transitions and\nemissions, under the hypothesis that the averaged features of hidden states\nwould serve to informatively characterize classes of patients/controls. We\npropose a Hidden Semi-Markov Model (HSMM), a latent-state model, emitting\n3D-acceleration vectors. Transitions and emissions are inferred from data. We\nfit separate models per unique device and training label. Hidden Markov Models\n(HMM) force geometric distributions of the duration spent at each state before\ntransition to a new state. Instead, our HSMM allows us to specify the\ndistribution of state duration. This modified version is more effective because\nwe are interested more in each state's duration than the sequence of distinct\nstates, allowing inclusion of these durations the feature vector.\n", "versions": [{"version": "v1", "created": "Sat, 11 Nov 2017 05:06:20 GMT"}], "update_date": "2017-11-15", "authors_parsed": [["Bukkittu", "Avinash", ""], ["Lin", "Baihan", ""], ["Vu", "Trung", ""], ["Pe'er", "Itsik", ""]]}, {"id": "1711.04094", "submitter": "Junliang Guo", "authors": "Junliang Guo, Linli Xu, Xunpeng Huang, Enhong Chen", "title": "Enhancing Network Embedding with Auxiliary Information: An Explicit\n  Matrix Factorization Perspective", "comments": "DASFAA 2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.SI cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recent advances in the field of network embedding have shown the\nlow-dimensional network representation is playing a critical role in network\nanalysis. However, most of the existing principles of network embedding do not\nincorporate auxiliary information such as content and labels of nodes flexibly.\nIn this paper, we take a matrix factorization perspective of network embedding,\nand incorporate structure, content and label information of the network\nsimultaneously. For structure, we validate that the matrix we construct\npreserves high-order proximities of the network. Label information can be\nfurther integrated into the matrix via the process of random walk sampling to\nenhance the quality of embedding in an unsupervised manner, i.e., without\nleveraging downstream classifiers. In addition, we generalize the Skip-Gram\nNegative Sampling model to integrate the content of the network in a matrix\nfactorization framework. As a consequence, network embedding can be learned in\na unified framework integrating network structure and node content as well as\nlabel information simultaneously. We demonstrate the efficacy of the proposed\nmodel with the tasks of semi-supervised node classification and link prediction\non a variety of real-world benchmark network datasets.\n", "versions": [{"version": "v1", "created": "Sat, 11 Nov 2017 08:07:23 GMT"}, {"version": "v2", "created": "Mon, 5 Mar 2018 03:39:23 GMT"}], "update_date": "2018-03-06", "authors_parsed": [["Guo", "Junliang", ""], ["Xu", "Linli", ""], ["Huang", "Xunpeng", ""], ["Chen", "Enhong", ""]]}, {"id": "1711.04126", "submitter": "Uiwon Hwang", "authors": "Uiwon Hwang, Sungwoon Choi, Han-Byoel Lee, Sungroh Yoon", "title": "Adversarial Training for Disease Prediction from Electronic Health\n  Records with Missing Data", "comments": "10 pages, 4 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Electronic health records (EHRs) have contributed to the computerization of\npatient records and can thus be used not only for efficient and systematic\nmedical services, but also for research on biomedical data science. However,\nthere are many missing values in EHRs when provided in matrix form, which is an\nimportant issue in many biomedical EHR applications. In this paper, we propose\na two-stage framework that includes missing data imputation and disease\nprediction to address the missing data problem in EHRs. We compared the disease\nprediction performance of generative adversarial networks (GANs) and\nconventional learning algorithms in combination with missing data prediction\nmethods. As a result, we obtained a level of accuracy of 0.9777, sensitivity of\n0.9521, specificity of 0.9925, area under the receiver operating characteristic\ncurve (AUC-ROC) of 0.9889, and F-score of 0.9688 with a stacked autoencoder as\nthe missing data prediction method and an auxiliary classifier GAN (AC-GAN) as\nthe disease prediction method. The comparison results show that a combination\nof a stacked autoencoder and an AC-GAN significantly outperforms other existing\napproaches. Our results suggest that the proposed framework is more robust for\ndisease prediction from EHRs with missing data.\n", "versions": [{"version": "v1", "created": "Sat, 11 Nov 2017 12:32:01 GMT"}, {"version": "v2", "created": "Thu, 18 Jan 2018 13:41:08 GMT"}, {"version": "v3", "created": "Tue, 1 May 2018 09:14:33 GMT"}, {"version": "v4", "created": "Tue, 22 May 2018 02:41:02 GMT"}], "update_date": "2018-05-23", "authors_parsed": [["Hwang", "Uiwon", ""], ["Choi", "Sungwoon", ""], ["Lee", "Han-Byoel", ""], ["Yoon", "Sungroh", ""]]}, {"id": "1711.04150", "submitter": "Supriya Pandhre", "authors": "Supriya Pandhre, Himangi Mittal, Manish Gupta, Vineeth N\n  Balasubramanian", "title": "STWalk: Learning Trajectory Representations in Temporal Graphs", "comments": "10 pages, 5 figures, 2 tables", "journal-ref": null, "doi": "10.1145/3152494.3152512", "report-no": null, "categories": "cs.SI cs.LG stat.ML", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Analyzing the temporal behavior of nodes in time-varying graphs is useful for\nmany applications such as targeted advertising, community evolution and outlier\ndetection. In this paper, we present a novel approach, STWalk, for learning\ntrajectory representations of nodes in temporal graphs. The proposed framework\nmakes use of structural properties of graphs at current and previous time-steps\nto learn effective node trajectory representations. STWalk performs random\nwalks on a graph at a given time step (called space-walk) as well as on graphs\nfrom past time-steps (called time-walk) to capture the spatio-temporal behavior\nof nodes. We propose two variants of STWalk to learn trajectory\nrepresentations. In one algorithm, we perform space-walk and time-walk as part\nof a single step. In the other variant, we perform space-walk and time-walk\nseparately and combine the learned representations to get the final trajectory\nembedding. Extensive experiments on three real-world temporal graph datasets\nvalidate the effectiveness of the learned representations when compared to\nthree baseline methods. We also show the goodness of the learned trajectory\nembeddings for change point detection, as well as demonstrate that arithmetic\noperations on these trajectory representations yield interesting and\ninterpretable results.\n", "versions": [{"version": "v1", "created": "Sat, 11 Nov 2017 15:19:27 GMT"}], "update_date": "2017-11-15", "authors_parsed": [["Pandhre", "Supriya", ""], ["Mittal", "Himangi", ""], ["Gupta", "Manish", ""], ["Balasubramanian", "Vineeth N", ""]]}, {"id": "1711.04162", "submitter": "Wenting Ye", "authors": "Wenting Ye, Xiang Liu, Haohan Wang and Eric P. Xing", "title": "A Sparse Graph-Structured Lasso Mixed Model for Genetic Association with\n  Confounding Correction", "comments": "Code available at https://github.com/YeWenting/sGLMM", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG q-bio.GN stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  While linear mixed model (LMM) has shown a competitive performance in\ncorrecting spurious associations raised by population stratification, family\nstructures, and cryptic relatedness, more challenges are still to be addressed\nregarding the complex structure of genotypic and phenotypic data. For example,\ngeneticists have discovered that some clusters of phenotypes are more\nco-expressed than others. Hence, a joint analysis that can utilize such\nrelatedness information in a heterogeneous data set is crucial for genetic\nmodeling.\n  We proposed the sparse graph-structured linear mixed model (sGLMM) that can\nincorporate the relatedness information from traits in a dataset with\nconfounding correction. Our method is capable of uncovering the genetic\nassociations of a large number of phenotypes together while considering the\nrelatedness of these phenotypes. Through extensive simulation experiments, we\nshow that the proposed model outperforms other existing approaches and can\nmodel correlation from both population structure and shared signals. Further,\nwe validate the effectiveness of sGLMM in the real-world genomic dataset on two\ndifferent species from plants and humans. In Arabidopsis thaliana data, sGLMM\nbehaves better than all other baseline models for 63.4% traits. We also discuss\nthe potential causal genetic variation of Human Alzheimer's disease discovered\nby our model and justify some of the most important genetic loci.\n", "versions": [{"version": "v1", "created": "Sat, 11 Nov 2017 16:01:53 GMT"}], "update_date": "2017-11-15", "authors_parsed": [["Ye", "Wenting", ""], ["Liu", "Xiang", ""], ["Wang", "Haohan", ""], ["Xing", "Eric P.", ""]]}, {"id": "1711.04168", "submitter": "Maksims Volkovs", "authors": "Chundi Liu, Shunan Zhao, Maksims Volkovs", "title": "Unsupervised Document Embedding With CNNs", "comments": "Major revision with additional experiments and model description", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a new model for unsupervised document embedding. Leading existing\napproaches either require complex inference or use recurrent neural networks\n(RNN) that are difficult to parallelize. We take a different route and develop\na convolutional neural network (CNN) embedding model. Our CNN architecture is\nfully parallelizable resulting in over 10x speedup in inference time over RNN\nmodels. Parallelizable architecture enables to train deeper models where each\nsuccessive layer has increasingly larger receptive field and models longer\nrange semantic structure within the document. We additionally propose a fully\nunsupervised learning algorithm to train this model based on stochastic forward\nprediction. Empirical results on two public benchmarks show that our approach\nproduces comparable to state-of-the-art accuracy at a fraction of computational\ncost.\n", "versions": [{"version": "v1", "created": "Sat, 11 Nov 2017 16:43:38 GMT"}, {"version": "v2", "created": "Wed, 15 Nov 2017 17:33:30 GMT"}, {"version": "v3", "created": "Tue, 20 Feb 2018 01:54:17 GMT"}], "update_date": "2018-02-21", "authors_parsed": [["Liu", "Chundi", ""], ["Zhao", "Shunan", ""], ["Volkovs", "Maksims", ""]]}, {"id": "1711.04178", "submitter": "Keaton Hamm", "authors": "Akram Aldroubi, Keaton Hamm, Ahmet Bugra Koku, and Ali Sekmen", "title": "CUR Decompositions, Similarity Matrices, and Subspace Clustering", "comments": "Approximately 30 pages. Current version contains improved algorithm\n  and numerical experiments from the previous version", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV math.NA stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A general framework for solving the subspace clustering problem using the CUR\ndecomposition is presented. The CUR decomposition provides a natural way to\nconstruct similarity matrices for data that come from a union of unknown\nsubspaces $\\mathscr{U}=\\underset{i=1}{\\overset{M}\\bigcup}S_i$. The similarity\nmatrices thus constructed give the exact clustering in the noise-free case.\nAdditionally, this decomposition gives rise to many distinct similarity\nmatrices from a given set of data, which allow enough flexibility to perform\naccurate clustering of noisy data. We also show that two known methods for\nsubspace clustering can be derived from the CUR decomposition. An algorithm\nbased on the theoretical construction of similarity matrices is presented, and\nexperiments on synthetic and real data are presented to test the method.\n  Additionally, an adaptation of our CUR based similarity matrices is utilized\nto provide a heuristic algorithm for subspace clustering; this algorithm yields\nthe best overall performance to date for clustering the Hopkins155 motion\nsegmentation dataset.\n", "versions": [{"version": "v1", "created": "Sat, 11 Nov 2017 18:34:34 GMT"}, {"version": "v2", "created": "Fri, 14 Sep 2018 21:14:03 GMT"}, {"version": "v3", "created": "Tue, 11 Dec 2018 20:53:22 GMT"}], "update_date": "2018-12-13", "authors_parsed": [["Aldroubi", "Akram", ""], ["Hamm", "Keaton", ""], ["Koku", "Ahmet Bugra", ""], ["Sekmen", "Ali", ""]]}, {"id": "1711.04248", "submitter": "Yunsung Kim", "authors": "Yunsung Kim", "title": "Linking Sequences of Events with Sparse or No Common Occurrence across\n  Data Sets", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.IR cs.SI stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Data of practical interest - such as personal records, transaction logs, and\nmedical histories - are sequential collections of events relevant to a\nparticular source entity. Recent studies have attempted to link sequences that\nrepresent a common entity across data sets to allow more comprehensive\nstatistical analyses and to identify potential privacy failures. Yet, current\napproaches remain tailored to their specific domains of application, and they\nfail when co-referent sequences in different data sets contain sparse or no\ncommon events, which occurs frequently in many cases.\n  To address this, we formalize the general problem of \"sequence linkage\" and\ndescribe \"LDA-Link,\" a generic solution that is applicable even when\nco-referent event sequences contain no common items at all. LDA-Link is built\nupon \"Split-Document\" model, a new mixed-membership probabilistic model for the\ngeneration of event sequence collections. It detects the latent similarity of\nsequences and thus achieves robustness particularly when co-referent sequences\nshare sparse or no event overlap. We apply LDA-Link in the context of social\nmedia profile reconciliation where users make no common posts across platforms,\ncomparing to the state-of-the-art generic solution to sequence linkage.\n", "versions": [{"version": "v1", "created": "Sun, 12 Nov 2017 07:46:28 GMT"}], "update_date": "2017-11-15", "authors_parsed": [["Kim", "Yunsung", ""]]}, {"id": "1711.04258", "submitter": "Zhao Kang", "authors": "Zhao Kang, Chong Peng, Qiang Cheng, Zenglin Xu", "title": "Unified Spectral Clustering with Optimal Graph", "comments": "Accepted by AAAI 2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI cs.CV cs.MM stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Spectral clustering has found extensive use in many areas. Most traditional\nspectral clustering algorithms work in three separate steps: similarity graph\nconstruction; continuous labels learning; discretizing the learned labels by\nk-means clustering. Such common practice has two potential flaws, which may\nlead to severe information loss and performance degradation. First, predefined\nsimilarity graph might not be optimal for subsequent clustering. It is\nwell-accepted that similarity graph highly affects the clustering results. To\nthis end, we propose to automatically learn similarity information from data\nand simultaneously consider the constraint that the similarity matrix has exact\nc connected components if there are c clusters. Second, the discrete solution\nmay deviate from the spectral solution since k-means method is well-known as\nsensitive to the initialization of cluster centers. In this work, we transform\nthe candidate solution into a new one that better approximates the discrete\none. Finally, those three subtasks are integrated into a unified framework,\nwith each subtask iteratively boosted by using the results of the others\ntowards an overall optimal solution. It is known that the performance of a\nkernel method is largely determined by the choice of kernels. To tackle this\npractical problem of how to select the most suitable kernel for a particular\ndata set, we further extend our model to incorporate multiple kernel learning\nability. Extensive experiments demonstrate the superiority of our proposed\nmethod as compared to existing clustering approaches.\n", "versions": [{"version": "v1", "created": "Sun, 12 Nov 2017 09:20:25 GMT"}], "update_date": "2017-11-15", "authors_parsed": [["Kang", "Zhao", ""], ["Peng", "Chong", ""], ["Cheng", "Qiang", ""], ["Xu", "Zenglin", ""]]}, {"id": "1711.04291", "submitter": "Valeriu Codreanu", "authors": "Valeriu Codreanu, Damian Podareanu, and Vikram Saletore", "title": "Scale out for large minibatch SGD: Residual network training on\n  ImageNet-1K with improved accuracy and reduced time to train", "comments": "10 pages, 4 figures, 13 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  For the past 5 years, the ILSVRC competition and the ImageNet dataset have\nattracted a lot of interest from the Computer Vision community, allowing for\nstate-of-the-art accuracy to grow tremendously. This should be credited to the\nuse of deep artificial neural network designs. As these became more complex,\nthe storage, bandwidth, and compute requirements increased. This means that\nwith a non-distributed approach, even when using the most high-density server\navailable, the training process may take weeks, making it prohibitive.\nFurthermore, as datasets grow, the representation learning potential of deep\nnetworks grows as well by using more complex models. This synchronicity\ntriggers a sharp increase in the computational requirements and motivates us to\nexplore the scaling behaviour on petaflop scale supercomputers. In this paper\nwe will describe the challenges and novel solutions needed in order to train\nResNet-50 in this large scale environment. We demonstrate above 90\\% scaling\nefficiency and a training time of 28 minutes using up to 104K x86 cores. This\nis supported by software tools from Intel's ecosystem. Moreover, we show that\nwith regular 90 - 120 epoch train runs we can achieve a top-1 accuracy as high\nas 77\\% for the unmodified ResNet-50 topology. We also introduce the novel\nCollapsed Ensemble (CE) technique that allows us to obtain a 77.5\\% top-1\naccuracy, similar to that of a ResNet-152, while training a unmodified\nResNet-50 topology for the same fixed training budget. All ResNet-50 models as\nwell as the scripts needed to replicate them will be posted shortly.\n", "versions": [{"version": "v1", "created": "Sun, 12 Nov 2017 13:26:31 GMT"}, {"version": "v2", "created": "Wed, 15 Nov 2017 19:47:04 GMT"}], "update_date": "2017-11-17", "authors_parsed": [["Codreanu", "Valeriu", ""], ["Podareanu", "Damian", ""], ["Saletore", "Vikram", ""]]}, {"id": "1711.04294", "submitter": "Joel Arrais", "authors": "Edgar D. Coelho, Igor N. Cruz, Andr\\'e Santiago, Jos\\'e Luis Oliveira,\n  Ant\\'onio Dourado and Joel P. Arrais", "title": "A Sequence-Based Mesh Classifier for the Prediction of Protein-Protein\n  Interactions", "comments": "17 pages, 2 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML q-bio.MN", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The worldwide surge of multiresistant microbial strains has propelled the\nsearch for alternative treatment options. The study of Protein-Protein\nInteractions (PPIs) has been a cornerstone in the clarification of complex\nphysiological and pathogenic processes, thus being a priority for the\nidentification of vital components and mechanisms in pathogens. Despite the\nadvances of laboratorial techniques, computational models allow the screening\nof protein interactions between entire proteomes in a fast and inexpensive\nmanner. Here, we present a supervised machine learning model for the prediction\nof PPIs based on the protein sequence. We cluster amino acids regarding their\nphysicochemical properties, and use the discrete cosine transform to represent\nprotein sequences. A mesh of classifiers was constructed to create\nhyper-specialised classifiers dedicated to the most relevant pairs of molecular\nfunction annotations from Gene Ontology. Based on an exhaustive evaluation that\nincludes datasets with different configurations, cross-validation and\nout-of-sampling validation, the obtained results outscore the state-of-the-art\nfor sequence-based methods. For the final mesh model using SVM with RBF, a\nconsistent average AUC of 0.84 was attained.\n", "versions": [{"version": "v1", "created": "Sun, 12 Nov 2017 13:36:59 GMT"}], "update_date": "2017-11-15", "authors_parsed": [["Coelho", "Edgar D.", ""], ["Cruz", "Igor N.", ""], ["Santiago", "Andr\u00e9", ""], ["Oliveira", "Jos\u00e9 Luis", ""], ["Dourado", "Ant\u00f3nio", ""], ["Arrais", "Joel P.", ""]]}, {"id": "1711.04297", "submitter": "Yuanhong Wang", "authors": "Yuanhong Wang, Yuyi Wang, Xingwu Liu, Juhua Pu", "title": "On the ERM Principle with Networked Data", "comments": "accepted by AAAI. arXiv admin note: substantial text overlap with\n  arXiv:math/0702683 by other authors", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.DS cs.SI stat.ML", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Networked data, in which every training example involves two objects and may\nshare some common objects with others, is used in many machine learning tasks\nsuch as learning to rank and link prediction. A challenge of learning from\nnetworked examples is that target values are not known for some pairs of\nobjects. In this case, neither the classical i.i.d.\\ assumption nor techniques\nbased on complete U-statistics can be used. Most existing theoretical results\nof this problem only deal with the classical empirical risk minimization (ERM)\nprinciple that always weights every example equally, but this strategy leads to\nunsatisfactory bounds. We consider general weighted ERM and show new universal\nrisk bounds for this problem. These new bounds naturally define an optimization\nproblem which leads to appropriate weights for networked examples. Though this\noptimization problem is not convex in general, we devise a new fully\npolynomial-time approximation scheme (FPTAS) to solve it.\n", "versions": [{"version": "v1", "created": "Sun, 12 Nov 2017 14:00:44 GMT"}, {"version": "v2", "created": "Wed, 22 Nov 2017 10:41:42 GMT"}], "update_date": "2017-11-23", "authors_parsed": [["Wang", "Yuanhong", ""], ["Wang", "Yuyi", ""], ["Liu", "Xingwu", ""], ["Pu", "Juhua", ""]]}, {"id": "1711.04308", "submitter": "Pengfei Zhang", "authors": "Pengfei Zhang and Ido Nevat and Gareth W. Peters and Wolfgang\n  Fruehwirt and Yongchao Huang and Ivonne Anders and Michael Osborne", "title": "Sensor Selection and Random Field Reconstruction for Robust and\n  Cost-effective Heterogeneous Weather Sensor Networks for the Developing World", "comments": "Presented at NIPS 2017 Workshop on Machine Learning for the\n  Developing World", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML eess.SP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We address the two fundamental problems of spatial field reconstruction and\nsensor selection in heterogeneous sensor networks: (i) how to efficiently\nperform spatial field reconstruction based on measurements obtained\nsimultaneously from networks with both high and low quality sensors; and (ii)\nhow to perform query based sensor set selection with predictive MSE performance\nguarantee. For the first problem, we developed a low complexity algorithm based\non the spatial best linear unbiased estimator (S-BLUE). Next, building on the\nS-BLUE, we address the second problem, and develop an efficient algorithm for\nquery based sensor set selection with performance guarantee. Our algorithm is\nbased on the Cross Entropy method which solves the combinatorial optimization\nproblem in an efficient manner.\n", "versions": [{"version": "v1", "created": "Sun, 12 Nov 2017 15:17:46 GMT"}, {"version": "v2", "created": "Wed, 15 Nov 2017 14:29:53 GMT"}, {"version": "v3", "created": "Thu, 23 Nov 2017 11:11:32 GMT"}], "update_date": "2017-11-27", "authors_parsed": [["Zhang", "Pengfei", ""], ["Nevat", "Ido", ""], ["Peters", "Gareth W.", ""], ["Fruehwirt", "Wolfgang", ""], ["Huang", "Yongchao", ""], ["Anders", "Ivonne", ""], ["Osborne", "Michael", ""]]}, {"id": "1711.04313", "submitter": "Randall Balestriero", "authors": "Randall Balestriero, Vincent Roger, Herve G. Glotin, Richard G.\n  Baraniuk", "title": "Semi-Supervised Learning via New Deep Network Inversion", "comments": "arXiv admin note: substantial text overlap with arXiv:1710.09302", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We exploit a recently derived inversion scheme for arbitrary deep neural\nnetworks to develop a new semi-supervised learning framework that applies to a\nwide range of systems and problems. The approach outperforms current\nstate-of-the-art methods on MNIST reaching $99.14\\%$ of test set accuracy while\nusing $5$ labeled examples per class. Experiments with one-dimensional signals\nhighlight the generality of the method. Importantly, our approach is simple,\nefficient, and requires no change in the deep network architecture.\n", "versions": [{"version": "v1", "created": "Sun, 12 Nov 2017 15:42:24 GMT"}], "update_date": "2017-11-15", "authors_parsed": [["Balestriero", "Randall", ""], ["Roger", "Vincent", ""], ["Glotin", "Herve G.", ""], ["Baraniuk", "Richard G.", ""]]}, {"id": "1711.04315", "submitter": "Shing Chan", "authors": "Shing Chan, Ahmed H. Elsheikh", "title": "A machine learning approach for efficient uncertainty quantification\n  using multiscale methods", "comments": "Journal of Computational Physics (2017)", "journal-ref": null, "doi": "10.1016/j.jcp.2017.10.034", "report-no": null, "categories": "cs.LG physics.comp-ph stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Several multiscale methods account for sub-grid scale features using coarse\nscale basis functions. For example, in the Multiscale Finite Volume method the\ncoarse scale basis functions are obtained by solving a set of local problems\nover dual-grid cells. We introduce a data-driven approach for the estimation of\nthese coarse scale basis functions. Specifically, we employ a neural network\npredictor fitted using a set of solution samples from which it learns to\ngenerate subsequent basis functions at a lower computational cost than solving\nthe local problems. The computational advantage of this approach is realized\nfor uncertainty quantification tasks where a large number of realizations has\nto be evaluated. We attribute the ability to learn these basis functions to the\nmodularity of the local problems and the redundancy of the permeability patches\nbetween samples. The proposed method is evaluated on elliptic problems yielding\nvery promising results.\n", "versions": [{"version": "v1", "created": "Sun, 12 Nov 2017 15:45:34 GMT"}], "update_date": "2017-11-15", "authors_parsed": [["Chan", "Shing", ""], ["Elsheikh", "Ahmed H.", ""]]}, {"id": "1711.04329", "submitter": "Shiyue Zhang", "authors": "Shiyue Zhang, Pengtao Xie, Dong Wang, Eric P. Xing", "title": "Medical Diagnosis From Laboratory Tests by Combining Generative and\n  Discriminative Learning", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A primary goal of computational phenotype research is to conduct medical\ndiagnosis. In hospital, physicians rely on massive clinical data to make\ndiagnosis decisions, among which laboratory tests are one of the most important\nresources. However, the longitudinal and incomplete nature of laboratory test\ndata casts a significant challenge on its interpretation and usage, which may\nresult in harmful decisions by both human physicians and automatic diagnosis\nsystems. In this work, we take advantage of deep generative models to deal with\nthe complex laboratory tests. Specifically, we propose an end-to-end\narchitecture that involves a deep generative variational recurrent neural\nnetworks (VRNN) to learn robust and generalizable features, and a\ndiscriminative neural network (NN) model to learn diagnosis decision making,\nand the two models are trained jointly. Our experiments are conducted on a\ndataset involving 46,252 patients, and the 50 most frequent tests are used to\npredict the 50 most common diagnoses. The results show that our model, VRNN+NN,\nsignificantly (p<0.001) outperforms other baseline models. Moreover, we\ndemonstrate that the representations learned by the joint training are more\ninformative than those learned by pure generative models. Finally, we find that\nour model offers a surprisingly good imputation for missing values.\n", "versions": [{"version": "v1", "created": "Sun, 12 Nov 2017 17:58:42 GMT"}, {"version": "v2", "created": "Thu, 16 Nov 2017 21:40:58 GMT"}], "update_date": "2017-11-20", "authors_parsed": [["Zhang", "Shiyue", ""], ["Xie", "Pengtao", ""], ["Wang", "Dong", ""], ["Xing", "Eric P.", ""]]}, {"id": "1711.04340", "submitter": "Antreas Antoniou Mr", "authors": "Antreas Antoniou, Amos Storkey and Harrison Edwards", "title": "Data Augmentation Generative Adversarial Networks", "comments": "10 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.CV cs.LG cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Effective training of neural networks requires much data. In the low-data\nregime, parameters are underdetermined, and learnt networks generalise poorly.\nData Augmentation alleviates this by using existing data more effectively.\nHowever standard data augmentation produces only limited plausible alternative\ndata. Given there is potential to generate a much broader set of augmentations,\nwe design and train a generative model to do data augmentation. The model,\nbased on image conditional Generative Adversarial Networks, takes data from a\nsource domain and learns to take any data item and generalise it to generate\nother within-class data items. As this generative process does not depend on\nthe classes themselves, it can be applied to novel unseen classes of data. We\nshow that a Data Augmentation Generative Adversarial Network (DAGAN) augments\nstandard vanilla classifiers well. We also show a DAGAN can enhance few-shot\nlearning systems such as Matching Networks. We demonstrate these approaches on\nOmniglot, on EMNIST having learnt the DAGAN on Omniglot, and VGG-Face data. In\nour experiments we can see over 13% increase in accuracy in the low-data regime\nexperiments in Omniglot (from 69% to 82%), EMNIST (73.9% to 76%) and VGG-Face\n(4.5% to 12%); in Matching Networks for Omniglot we observe an increase of 0.5%\n(from 96.9% to 97.4%) and an increase of 1.8% in EMNIST (from 59.5% to 61.3%).\n", "versions": [{"version": "v1", "created": "Sun, 12 Nov 2017 19:17:57 GMT"}, {"version": "v2", "created": "Thu, 8 Mar 2018 16:46:40 GMT"}, {"version": "v3", "created": "Wed, 21 Mar 2018 23:26:15 GMT"}], "update_date": "2018-03-23", "authors_parsed": [["Antoniou", "Antreas", ""], ["Storkey", "Amos", ""], ["Edwards", "Harrison", ""]]}, {"id": "1711.04345", "submitter": "Riashat Islam", "authors": "Bogdan Mazoure, Riashat Islam", "title": "Alpha-Divergences in Variational Dropout", "comments": "Bogdan Mazoure and Riashat Islam contributed equally", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We investigate the use of alternative divergences to Kullback-Leibler (KL) in\nvariational inference(VI), based on the Variational Dropout \\cite{kingma2015}.\nStochastic gradient variational Bayes (SGVB) \\cite{aevb} is a general framework\nfor estimating the evidence lower bound (ELBO) in Variational Bayes. In this\nwork, we extend the SGVB estimator with using Alpha-Divergences, which are\nalternative to divergences to VI' KL objective. The Gaussian dropout can be\nseen as a local reparametrization trick of the SGVB objective. We extend the\nVariational Dropout to use alpha divergences for variational inference. Our\nresults compare $\\alpha$-divergence variational dropout with standard\nvariational dropout with correlated and uncorrelated weight noise. We show that\nthe $\\alpha$-divergence with $\\alpha \\rightarrow 1$ (or KL divergence) is still\na good measure for use in variational inference, in spite of the efficient use\nof Alpha-divergences for Dropout VI \\cite{Li17}. $\\alpha \\rightarrow 1$ can\nyield the lowest training error, and optimizes a good lower bound for the\nevidence lower bound (ELBO) among all values of the parameter $\\alpha \\in\n[0,\\infty)$.\n", "versions": [{"version": "v1", "created": "Sun, 12 Nov 2017 19:38:09 GMT"}], "update_date": "2017-11-15", "authors_parsed": [["Mazoure", "Bogdan", ""], ["Islam", "Riashat", ""]]}, {"id": "1711.04366", "submitter": "Nicolas Papadakis", "authors": "Arnaud Dessein and Nicolas Papadakis and Charles-Alban Deledalle", "title": "Parameter Estimation in Finite Mixture Models by Regularized Optimal\n  Transport: A Unified Framework for Hard and Soft Clustering", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this short paper, we formulate parameter estimation for finite mixture\nmodels in the context of discrete optimal transportation with convex\nregularization. The proposed framework unifies hard and soft clustering methods\nfor general mixture models. It also generalizes the celebrated\n$k$\\nobreakdash-means and expectation-maximization algorithms in relation to\nassociated Bregman divergences when applied to exponential family mixture\nmodels.\n", "versions": [{"version": "v1", "created": "Sun, 12 Nov 2017 21:52:54 GMT"}], "update_date": "2017-11-15", "authors_parsed": [["Dessein", "Arnaud", ""], ["Papadakis", "Nicolas", ""], ["Deledalle", "Charles-Alban", ""]]}, {"id": "1711.04368", "submitter": "Jihun Hamm", "authors": "Jihun Hamm and Akshay Mehra", "title": "Machine vs Machine: Minimax-Optimal Defense Against Adversarial Examples", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recently, researchers have discovered that the state-of-the-art object\nclassifiers can be fooled easily by small perturbations in the input\nunnoticeable to human eyes. It is also known that an attacker can generate\nstrong adversarial examples if she knows the classifier parameters. Conversely,\na defender can robustify the classifier by retraining if she has access to the\nadversarial examples. We explain and formulate this adversarial example problem\nas a two-player continuous zero-sum game, and demonstrate the fallacy of\nevaluating a defense or an attack as a static problem. To find the best\nworst-case defense against whitebox attacks, we propose a continuous minimax\noptimization algorithm. We demonstrate the minimax defense with two types of\nattack classes -- gradient-based and neural network-based attacks. Experiments\nwith the MNIST and the CIFAR-10 datasets demonstrate that the defense found by\nnumerical minimax optimization is indeed more robust than non-minimax defenses.\nWe discuss directions for improving the result toward achieving robustness\nagainst multiple types of attack classes.\n", "versions": [{"version": "v1", "created": "Sun, 12 Nov 2017 22:07:36 GMT"}, {"version": "v2", "created": "Wed, 13 Dec 2017 05:19:56 GMT"}, {"version": "v3", "created": "Wed, 27 Jun 2018 03:45:47 GMT"}], "update_date": "2018-06-28", "authors_parsed": [["Hamm", "Jihun", ""], ["Mehra", "Akshay", ""]]}, {"id": "1711.04374", "submitter": "Lior Horesh", "authors": "Remi R. Lam and Lior Horesh and Haim Avron and Karen E. Willcox", "title": "Should You Derive, Or Let the Data Drive? An Optimization Framework for\n  Hybrid First-Principles Data-Driven Modeling", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML math.DS math.OC physics.data-an", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Mathematical models are used extensively for diverse tasks including\nanalysis, optimization, and decision making. Frequently, those models are\nprincipled but imperfect representations of reality. This is either due to\nincomplete physical description of the underlying phenomenon (simplified\ngoverning equations, defective boundary conditions, etc.), or due to numerical\napproximations (discretization, linearization, round-off error, etc.). Model\nmisspecification can lead to erroneous model predictions, and respectively\nsuboptimal decisions associated with the intended end-goal task. To mitigate\nthis effect, one can amend the available model using limited data produced by\nexperiments or higher fidelity models. A large body of research has focused on\nestimating explicit model parameters. This work takes a different perspective\nand targets the construction of a correction model operator with implicit\nattributes. We investigate the case where the end-goal is inversion and\nillustrate how appropriate choices of properties imposed upon the correction\nand corrected operator lead to improved end-goal insights.\n", "versions": [{"version": "v1", "created": "Sun, 12 Nov 2017 22:37:46 GMT"}], "update_date": "2017-11-15", "authors_parsed": [["Lam", "Remi R.", ""], ["Horesh", "Lior", ""], ["Avron", "Haim", ""], ["Willcox", "Karen E.", ""]]}, {"id": "1711.04416", "submitter": "Liu Liu", "authors": "Liu Liu, Ji Liu, and Dacheng Tao", "title": "Variance Reduced methods for Non-convex Composition Optimization", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML math.OC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper explores the non-convex composition optimization in the form\nincluding inner and outer finite-sum functions with a large number of component\nfunctions. This problem arises in some important applications such as nonlinear\nembedding and reinforcement learning. Although existing approaches such as\nstochastic gradient descent (SGD) and stochastic variance reduced gradient\n(SVRG) descent can be applied to solve this problem, their query complexity\ntends to be high, especially when the number of inner component functions is\nlarge. In this paper, we apply the variance-reduced technique to derive two\nvariance reduced algorithms that significantly improve the query complexity if\nthe number of inner component functions is large. To the best of our knowledge,\nthis is the first work that establishes the query complexity analysis for\nnon-convex stochastic composition. Experiments validate the proposed algorithms\nand theoretical analysis.\n", "versions": [{"version": "v1", "created": "Mon, 13 Nov 2017 04:21:41 GMT"}], "update_date": "2017-11-15", "authors_parsed": [["Liu", "Liu", ""], ["Liu", "Ji", ""], ["Tao", "Dacheng", ""]]}, {"id": "1711.04425", "submitter": "Jingwei Zhuo", "authors": "Jingwei Zhuo, Chang Liu, Jiaxin Shi, Jun Zhu, Ning Chen, Bo Zhang", "title": "Message Passing Stein Variational Gradient Descent", "comments": "To appear in the Proceedings of the 35th International Conference on\n  Machine Learning (ICML 2018)", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Stein variational gradient descent (SVGD) is a recently proposed\nparticle-based Bayesian inference method, which has attracted a lot of interest\ndue to its remarkable approximation ability and particle efficiency compared to\ntraditional variational inference and Markov Chain Monte Carlo methods.\nHowever, we observed that particles of SVGD tend to collapse to modes of the\ntarget distribution, and this particle degeneracy phenomenon becomes more\nsevere with higher dimensions. Our theoretical analysis finds out that there\nexists a negative correlation between the dimensionality and the repulsive\nforce of SVGD which should be blamed for this phenomenon. We propose Message\nPassing SVGD (MP-SVGD) to solve this problem. By leveraging the conditional\nindependence structure of probabilistic graphical models (PGMs), MP-SVGD\nconverts the original high-dimensional global inference problem into a set of\nlocal ones over the Markov blanket with lower dimensions. Experimental results\nshow its advantages of preventing vanishing repulsive force in high-dimensional\nspace over SVGD, and its particle efficiency and approximation flexibility over\nother inference methods on graphical models.\n", "versions": [{"version": "v1", "created": "Mon, 13 Nov 2017 05:39:25 GMT"}, {"version": "v2", "created": "Thu, 29 Mar 2018 16:56:17 GMT"}, {"version": "v3", "created": "Fri, 8 Jun 2018 04:37:25 GMT"}], "update_date": "2018-06-11", "authors_parsed": [["Zhuo", "Jingwei", ""], ["Liu", "Chang", ""], ["Shi", "Jiaxin", ""], ["Zhu", "Jun", ""], ["Chen", "Ning", ""], ["Zhang", "Bo", ""]]}, {"id": "1711.04454", "submitter": "Pierre Menard", "authors": "Aur\\'elien Garivier (IMT), Pierre M\\'enard (IMT), Laurent Rossi (IMT),\n  Pierre Menard (IMT)", "title": "Thresholding Bandit for Dose-ranging: The Impact of Monotonicity", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.ML stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We analyze the sample complexity of the thresholding bandit problem, with and\nwithout the assumption that the mean values of the arms are increasing. In each\ncase, we provide a lower bound valid for any risk $\\delta$ and any\n$\\delta$-correct algorithm; in addition, we propose an algorithm whose sample\ncomplexity is of the same order of magnitude for small risks. This work is\nmotivated by phase 1 clinical trials, a practically important setting where the\narm means are increasing by nature, and where no satisfactory solution is\navailable so far.\n", "versions": [{"version": "v1", "created": "Mon, 13 Nov 2017 07:36:01 GMT"}, {"version": "v2", "created": "Tue, 24 Jul 2018 08:38:39 GMT"}], "update_date": "2018-07-25", "authors_parsed": [["Garivier", "Aur\u00e9lien", "", "IMT"], ["M\u00e9nard", "Pierre", "", "IMT"], ["Rossi", "Laurent", "", "IMT"], ["Menard", "Pierre", "", "IMT"]]}, {"id": "1711.04460", "submitter": "Nicolas Keriven", "authors": "Nicolas Keriven (DMA), Antoine Deleforge (PANAMA), Antoine Liutkus\n  (ZENITH)", "title": "Blind Source Separation Using Mixtures of Alpha-Stable Distributions", "comments": "International Conference on Acoustics, Speech and Signal Processing\n  (ICASSP), Apr 2018, Calgary, Canada", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.SD eess.AS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a new blind source separation algorithm based on mixtures of\nalpha-stable distributions. Complex symmetric alpha-stable distributions have\nbeen recently showed to better model audio signals in the time-frequency domain\nthan classical Gaussian distributions thanks to their larger dynamic range.\nHowever, inference of these models is notoriously hard to perform because their\nprobability density functions do not have a closed-form expression in general.\nHere, we introduce a novel method for estimating mixture of alpha-stable\ndistributions based on characteristic function matching. We apply this to the\nblind estimation of binary masks in individual frequency bands from\nmultichannel convolutive audio mixes. We show that the proposed method yields\nbetter separation performance than Gaussian-based binary-masking methods.\n", "versions": [{"version": "v1", "created": "Mon, 13 Nov 2017 07:53:35 GMT"}, {"version": "v2", "created": "Wed, 15 Nov 2017 12:52:21 GMT"}, {"version": "v3", "created": "Mon, 12 Feb 2018 09:49:05 GMT"}], "update_date": "2018-02-13", "authors_parsed": [["Keriven", "Nicolas", "", "DMA"], ["Deleforge", "Antoine", "", "PANAMA"], ["Liutkus", "Antoine", "", "ZENITH"]]}, {"id": "1711.04489", "submitter": "Yang Yang", "authors": "Yang Yang, Marius Pesavento", "title": "A Parallel Best-Response Algorithm with Exact Line Search for Nonconvex\n  Sparsity-Regularized Rank Minimization", "comments": "Submitted to IEEE ICASSP 2017", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we propose a convergent parallel best-response algorithm with\nthe exact line search for the nondifferentiable nonconvex sparsity-regularized\nrank minimization problem. On the one hand, it exhibits a faster convergence\nthan subgradient algorithms and block coordinate descent algorithms. On the\nother hand, its convergence to a stationary point is guaranteed, while ADMM\nalgorithms only converge for convex problems. Furthermore, the exact line\nsearch procedure in the proposed algorithm is performed efficiently in\nclosed-form to avoid the meticulous choice of stepsizes, which is however a\ncommon bottleneck in subgradient algorithms and successive convex approximation\nalgorithms. Finally, the proposed algorithm is numerically tested.\n", "versions": [{"version": "v1", "created": "Mon, 13 Nov 2017 09:28:43 GMT"}], "update_date": "2017-11-15", "authors_parsed": [["Yang", "Yang", ""], ["Pesavento", "Marius", ""]]}, {"id": "1711.04528", "submitter": "Thomas Elsken", "authors": "Thomas Elsken, Jan-Hendrik Metzen, Frank Hutter", "title": "Simple And Efficient Architecture Search for Convolutional Neural\n  Networks", "comments": "Under review as a conference paper at ICLR 2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.AI cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Neural networks have recently had a lot of success for many tasks. However,\nneural network architectures that perform well are still typically designed\nmanually by experts in a cumbersome trial-and-error process. We propose a new\nmethod to automatically search for well-performing CNN architectures based on a\nsimple hill climbing procedure whose operators apply network morphisms,\nfollowed by short optimization runs by cosine annealing. Surprisingly, this\nsimple method yields competitive results, despite only requiring resources in\nthe same order of magnitude as training a single network. E.g., on CIFAR-10,\nour method designs and trains networks with an error rate below 6% in only 12\nhours on a single GPU; training for one day reduces this error further, to\nalmost 5%.\n", "versions": [{"version": "v1", "created": "Mon, 13 Nov 2017 11:23:36 GMT"}], "update_date": "2017-11-15", "authors_parsed": [["Elsken", "Thomas", ""], ["Metzen", "Jan-Hendrik", ""], ["Hutter", "Frank", ""]]}, {"id": "1711.04623", "submitter": "Zachary Kenton", "authors": "Stanis{\\l}aw Jastrz\\k{e}bski, Zachary Kenton, Devansh Arpit, Nicolas\n  Ballas, Asja Fischer, Yoshua Bengio, Amos Storkey", "title": "Three Factors Influencing Minima in SGD", "comments": "First two authors contributed equally. Short version accepted into\n  ICLR workshop. Accepted to Artificial Neural Networks and Machine Learning,\n  ICANN 2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI cs.CV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We investigate the dynamical and convergent properties of stochastic gradient\ndescent (SGD) applied to Deep Neural Networks (DNNs). Characterizing the\nrelation between learning rate, batch size and the properties of the final\nminima, such as width or generalization, remains an open question. In order to\ntackle this problem we investigate the previously proposed approximation of SGD\nby a stochastic differential equation (SDE). We theoretically argue that three\nfactors - learning rate, batch size and gradient covariance - influence the\nminima found by SGD. In particular we find that the ratio of learning rate to\nbatch size is a key determinant of SGD dynamics and of the width of the final\nminima, and that higher values of the ratio lead to wider minima and often\nbetter generalization. We confirm these findings experimentally. Further, we\ninclude experiments which show that learning rate schedules can be replaced\nwith batch size schedules and that the ratio of learning rate to batch size is\nan important factor influencing the memorization process.\n", "versions": [{"version": "v1", "created": "Mon, 13 Nov 2017 15:11:56 GMT"}, {"version": "v2", "created": "Fri, 15 Jun 2018 16:22:54 GMT"}, {"version": "v3", "created": "Thu, 13 Sep 2018 09:29:55 GMT"}], "update_date": "2018-09-14", "authors_parsed": [["Jastrz\u0119bski", "Stanis\u0142aw", ""], ["Kenton", "Zachary", ""], ["Arpit", "Devansh", ""], ["Ballas", "Nicolas", ""], ["Fischer", "Asja", ""], ["Bengio", "Yoshua", ""], ["Storkey", "Amos", ""]]}, {"id": "1711.04674", "submitter": "Sohan Seth", "authors": "Sohan Seth, Iain Murray, Christopher K. I. Williams", "title": "Model Criticism in Latent Space", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Model criticism is usually carried out by assessing if replicated data\ngenerated under the fitted model looks similar to the observed data, see e.g.\nGelman, Carlin, Stern, and Rubin [2004, p. 165]. This paper presents a method\nfor latent variable models by pulling back the data into the space of latent\nvariables, and carrying out model criticism in that space. Making use of a\nmodel's structure enables a more direct assessment of the assumptions made in\nthe prior and likelihood. We demonstrate the method with examples of model\ncriticism in latent space applied to factor analysis, linear dynamical systems\nand Gaussian processes.\n", "versions": [{"version": "v1", "created": "Mon, 13 Nov 2017 16:07:14 GMT"}, {"version": "v2", "created": "Mon, 2 Jul 2018 20:02:29 GMT"}], "update_date": "2018-07-04", "authors_parsed": [["Seth", "Sohan", ""], ["Murray", "Iain", ""], ["Williams", "Christopher K. I.", ""]]}, {"id": "1711.04679", "submitter": "Stephan Baier", "authors": "Stephan Baier, Sigurd Spieckermann and Volker Tresp", "title": "Attention-based Information Fusion using Multi-Encoder-Decoder Recurrent\n  Neural Networks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  With the rising number of interconnected devices and sensors, modeling\ndistributed sensor networks is of increasing interest. Recurrent neural\nnetworks (RNN) are considered particularly well suited for modeling sensory and\nstreaming data. When predicting future behavior, incorporating information from\nneighboring sensor stations is often beneficial. We propose a new RNN based\narchitecture for context specific information fusion across multiple spatially\ndistributed sensor stations. Hereby, latent representations of multiple local\nmodels, each modeling one sensor station, are jointed and weighted, according\nto their importance for the prediction. The particular importance is assessed\ndepending on the current context using a separate attention function. We\ndemonstrate the effectiveness of our model on three different real-world sensor\nnetwork datasets.\n", "versions": [{"version": "v1", "created": "Mon, 13 Nov 2017 16:17:45 GMT"}], "update_date": "2017-11-15", "authors_parsed": [["Baier", "Stephan", ""], ["Spieckermann", "Sigurd", ""], ["Tresp", "Volker", ""]]}, {"id": "1711.04683", "submitter": "Stephan Baier", "authors": "Stephan Baier and Volker Tresp", "title": "Tensor Decompositions for Modeling Inverse Dynamics", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.RO cs.SY stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Modeling inverse dynamics is crucial for accurate feedforward robot control.\nThe model computes the necessary joint torques, to perform a desired movement.\nThe highly non-linear inverse function of the dynamical system can be\napproximated using regression techniques. We propose as regression method a\ntensor decomposition model that exploits the inherent three-way interaction of\npositions x velocities x accelerations. Most work in tensor factorization has\naddressed the decomposition of dense tensors. In this paper, we build upon the\ndecomposition of sparse tensors, with only small amounts of nonzero entries.\nThe decomposition of sparse tensors has successfully been used in relational\nlearning, e.g., the modeling of large knowledge graphs. Recently, the approach\nhas been extended to multi-class classification with discrete input variables.\nRepresenting the data in high dimensional sparse tensors enables the\napproximation of complex highly non-linear functions. In this paper we show how\nthe decomposition of sparse tensors can be applied to regression problems.\nFurthermore, we extend the method to continuous inputs, by learning a mapping\nfrom the continuous inputs to the latent representations of the tensor\ndecomposition, using basis functions. We evaluate our proposed model on a\ndataset with trajectories from a seven degrees of freedom SARCOS robot arm. Our\nexperimental results show superior performance of the proposed functional\ntensor model, compared to challenging state-of-the art methods.\n", "versions": [{"version": "v1", "created": "Mon, 13 Nov 2017 16:26:51 GMT"}], "update_date": "2017-11-15", "authors_parsed": [["Baier", "Stephan", ""], ["Tresp", "Volker", ""]]}, {"id": "1711.04686", "submitter": "Brandon Reagen", "authors": "Brandon Reagen, Udit Gupta, Robert Adolf, Michael M. Mitzenmacher,\n  Alexander M. Rush, Gu-Yeon Wei, David Brooks", "title": "Weightless: Lossy Weight Encoding For Deep Neural Network Compression", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The large memory requirements of deep neural networks limit their deployment\nand adoption on many devices. Model compression methods effectively reduce the\nmemory requirements of these models, usually through applying transformations\nsuch as weight pruning or quantization. In this paper, we present a novel\nscheme for lossy weight encoding which complements conventional compression\ntechniques. The encoding is based on the Bloomier filter, a probabilistic data\nstructure that can save space at the cost of introducing random errors.\nLeveraging the ability of neural networks to tolerate these imperfections and\nby re-training around the errors, the proposed technique, Weightless, can\ncompress DNN weights by up to 496x with the same model accuracy. This results\nin up to a 1.51x improvement over the state-of-the-art.\n", "versions": [{"version": "v1", "created": "Mon, 13 Nov 2017 16:28:37 GMT"}], "update_date": "2017-11-15", "authors_parsed": [["Reagen", "Brandon", ""], ["Gupta", "Udit", ""], ["Adolf", "Robert", ""], ["Mitzenmacher", "Michael M.", ""], ["Rush", "Alexander M.", ""], ["Wei", "Gu-Yeon", ""], ["Brooks", "David", ""]]}, {"id": "1711.04712", "submitter": "George Linderman", "authors": "George C. Linderman, Gal Mishne, Yuval Kluger, Stefan Steinerberger", "title": "Randomized Near Neighbor Graphs, Giant Components, and Applications in\n  Data Science", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.CO cs.DM cs.DS math.PR stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  If we pick $n$ random points uniformly in $[0,1]^d$ and connect each point to\nits $k-$nearest neighbors, then it is well known that there exists a giant\nconnected component with high probability. We prove that in $[0,1]^d$ it\nsuffices to connect every point to $ c_{d,1} \\log{\\log{n}}$ points chosen\nrandomly among its $ c_{d,2} \\log{n}-$nearest neighbors to ensure a giant\ncomponent of size $n - o(n)$ with high probability. This construction yields a\nmuch sparser random graph with $\\sim n \\log\\log{n}$ instead of $\\sim n \\log{n}$\nedges that has comparable connectivity properties. This result has nontrivial\nimplications for problems in data science where an affinity matrix is\nconstructed: instead of picking the $k-$nearest neighbors, one can often pick\n$k' \\ll k$ random points out of the $k-$nearest neighbors without sacrificing\nefficiency. This can massively simplify and accelerate computation, we\nillustrate this with several numerical examples.\n", "versions": [{"version": "v1", "created": "Mon, 13 Nov 2017 17:22:00 GMT"}], "update_date": "2017-11-15", "authors_parsed": [["Linderman", "George C.", ""], ["Mishne", "Gal", ""], ["Kluger", "Yuval", ""], ["Steinerberger", "Stefan", ""]]}, {"id": "1711.04735", "submitter": "Jeffrey Pennington", "authors": "Jeffrey Pennington, Samuel S. Schoenholz, Surya Ganguli", "title": "Resurrecting the sigmoid in deep learning through dynamical isometry:\n  theory and practice", "comments": "13 pages, 6 figures. Appearing at the 31st Conference on Neural\n  Information Processing Systems (NIPS 2017)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  It is well known that the initialization of weights in deep neural networks\ncan have a dramatic impact on learning speed. For example, ensuring the mean\nsquared singular value of a network's input-output Jacobian is $O(1)$ is\nessential for avoiding the exponential vanishing or explosion of gradients. The\nstronger condition that all singular values of the Jacobian concentrate near\n$1$ is a property known as dynamical isometry. For deep linear networks,\ndynamical isometry can be achieved through orthogonal weight initialization and\nhas been shown to dramatically speed up learning; however, it has remained\nunclear how to extend these results to the nonlinear setting. We address this\nquestion by employing powerful tools from free probability theory to compute\nanalytically the entire singular value distribution of a deep network's\ninput-output Jacobian. We explore the dependence of the singular value\ndistribution on the depth of the network, the weight initialization, and the\nchoice of nonlinearity. Intriguingly, we find that ReLU networks are incapable\nof dynamical isometry. On the other hand, sigmoidal networks can achieve\nisometry, but only with orthogonal weight initialization. Moreover, we\ndemonstrate empirically that deep nonlinear networks achieving dynamical\nisometry learn orders of magnitude faster than networks that do not. Indeed, we\nshow that properly-initialized deep sigmoidal networks consistently outperform\ndeep ReLU networks. Overall, our analysis reveals that controlling the entire\ndistribution of Jacobian singular values is an important design consideration\nin deep learning.\n", "versions": [{"version": "v1", "created": "Mon, 13 Nov 2017 18:06:09 GMT"}], "update_date": "2017-11-15", "authors_parsed": [["Pennington", "Jeffrey", ""], ["Schoenholz", "Samuel S.", ""], ["Ganguli", "Surya", ""]]}, {"id": "1711.04755", "submitter": "Nan Rosemary Ke", "authors": "Anirudh Goyal, Nan Rosemary Ke, Alex Lamb, R Devon Hjelm, Chris Pal,\n  Joelle Pineau, Yoshua Bengio", "title": "ACtuAL: Actor-Critic Under Adversarial Learning", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Generative Adversarial Networks (GANs) are a powerful framework for deep\ngenerative modeling. Posed as a two-player minimax problem, GANs are typically\ntrained end-to-end on real-valued data and can be used to train a generator of\nhigh-dimensional and realistic images. However, a major limitation of GANs is\nthat training relies on passing gradients from the discriminator through the\ngenerator via back-propagation. This makes it fundamentally difficult to train\nGANs with discrete data, as generation in this case typically involves a\nnon-differentiable function. These difficulties extend to the reinforcement\nlearning setting when the action space is composed of discrete decisions. We\naddress these issues by reframing the GAN framework so that the generator is no\nlonger trained using gradients through the discriminator, but is instead\ntrained using a learned critic in the actor-critic framework with a Temporal\nDifference (TD) objective. This is a natural fit for sequence modeling and we\nuse it to achieve improvements on language modeling tasks over the standard\nTeacher-Forcing methods.\n", "versions": [{"version": "v1", "created": "Mon, 13 Nov 2017 18:49:06 GMT"}], "update_date": "2017-11-15", "authors_parsed": [["Goyal", "Anirudh", ""], ["Ke", "Nan Rosemary", ""], ["Lamb", "Alex", ""], ["Hjelm", "R Devon", ""], ["Pal", "Chris", ""], ["Pineau", "Joelle", ""], ["Bengio", "Yoshua", ""]]}, {"id": "1711.04810", "submitter": "Th\\'eophile Gaudin", "authors": "Philippe Schwaller and Theophile Gaudin, David Lanyi, Costas Bekas,\n  Teodoro Laino", "title": "\"Found in Translation\": Predicting Outcomes of Complex Organic Chemistry\n  Reactions using Neural Sequence-to-Sequence Models", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  There is an intuitive analogy of an organic chemist's understanding of a\ncompound and a language speaker's understanding of a word. Consequently, it is\npossible to introduce the basic concepts and analyze potential impacts of\nlinguistic analysis to the world of organic chemistry. In this work, we cast\nthe reaction prediction task as a translation problem by introducing a\ntemplate-free sequence-to-sequence model, trained end-to-end and fully\ndata-driven. We propose a novel way of tokenization, which is arbitrarily\nextensible with reaction information. With this approach, we demonstrate\nresults superior to the state-of-the-art solution by a significant margin on\nthe top-1 accuracy. Specifically, our approach achieves an accuracy of 80.1%\nwithout relying on auxiliary knowledge such as reaction templates. Also, 66.4%\naccuracy is reached on a larger and noisier dataset.\n", "versions": [{"version": "v1", "created": "Mon, 13 Nov 2017 19:38:14 GMT"}, {"version": "v2", "created": "Wed, 15 Nov 2017 08:06:57 GMT"}], "update_date": "2017-11-16", "authors_parsed": [["Schwaller", "Philippe", ""], ["Gaudin", "Theophile", ""], ["Lanyi", "David", ""], ["Bekas", "Costas", ""], ["Laino", "Teodoro", ""]]}, {"id": "1711.04817", "submitter": "Irina Gaynanova", "authors": "Irina Gaynanova and Tianying Wang", "title": "Sparse quadratic classification rules via linear dimension reduction", "comments": null, "journal-ref": "Journal of Multivariate Analysis 2019, Vol. 169, 278-299", "doi": "10.1016/j.jmva.2018.09.011", "report-no": null, "categories": "stat.ML stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider the problem of high-dimensional classification between the two\ngroups with unequal covariance matrices. Rather than estimating the full\nquadratic discriminant rule, we propose to perform simultaneous variable\nselection and linear dimension reduction on original data, with the subsequent\napplication of quadratic discriminant analysis on the reduced space. In\ncontrast to quadratic discriminant analysis, the proposed framework doesn't\nrequire estimation of precision matrices and scales linearly with the number of\nmeasurements, making it especially attractive for the use on high-dimensional\ndatasets. We support the methodology with theoretical guarantees on variable\nselection consistency, and empirical comparison with competing approaches. We\napply the method to gene expression data of breast cancer patients, and confirm\nthe crucial importance of ESR1 gene in differentiating estrogen receptor\nstatus.\n", "versions": [{"version": "v1", "created": "Mon, 13 Nov 2017 19:52:06 GMT"}, {"version": "v2", "created": "Tue, 16 Jan 2018 22:09:21 GMT"}], "update_date": "2021-04-01", "authors_parsed": [["Gaynanova", "Irina", ""], ["Wang", "Tianying", ""]]}, {"id": "1711.04837", "submitter": "Zachary Lipton", "authors": "John Alberg, Zachary C. Lipton", "title": "Improving Factor-Based Quantitative Investing by Forecasting Company\n  Fundamentals", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  On a periodic basis, publicly traded companies are required to report\nfundamentals: financial data such as revenue, operating income, debt, among\nothers. These data points provide some insight into the financial health of a\ncompany. Academic research has identified some factors, i.e. computed features\nof the reported data, that are known through retrospective analysis to\noutperform the market average. Two popular factors are the book value\nnormalized by market capitalization (book-to-market) and the operating income\nnormalized by the enterprise value (EBIT/EV). In this paper: we first show\nthrough simulation that if we could (clairvoyantly) select stocks using factors\ncalculated on future fundamentals (via oracle), then our portfolios would far\noutperform a standard factor approach. Motivated by this analysis, we train\ndeep neural networks to forecast future fundamentals based on a trailing\n5-years window. Quantitative analysis demonstrates a significant improvement in\nMSE over a naive strategy. Moreover, in retrospective analysis using an\nindustry-grade stock portfolio simulator (backtester), we show an improvement\nin compounded annual return to 17.1% (MLP) vs 14.4% for a standard factor\nmodel.\n", "versions": [{"version": "v1", "created": "Mon, 13 Nov 2017 20:30:02 GMT"}, {"version": "v2", "created": "Thu, 26 Apr 2018 03:19:32 GMT"}], "update_date": "2018-04-27", "authors_parsed": [["Alberg", "John", ""], ["Lipton", "Zachary C.", ""]]}, {"id": "1711.04845", "submitter": "John Thickstun", "authors": "John Thickstun, Zaid Harchaoui, Dean Foster, Sham M. Kakade", "title": "Invariances and Data Augmentation for Supervised Music Transcription", "comments": "6 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG cs.SD eess.AS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper explores a variety of models for frame-based music transcription,\nwith an emphasis on the methods needed to reach state-of-the-art on human\nrecordings. The translation-invariant network discussed in this paper, which\ncombines a traditional filterbank with a convolutional neural network, was the\ntop-performing model in the 2017 MIREX Multiple Fundamental Frequency\nEstimation evaluation. This class of models shares parameters in the\nlog-frequency domain, which exploits the frequency invariance of music to\nreduce the number of model parameters and avoid overfitting to the training\ndata. All models in this paper were trained with supervision by labeled data\nfrom the MusicNet dataset, augmented by random label-preserving pitch-shift\ntransformations.\n", "versions": [{"version": "v1", "created": "Mon, 13 Nov 2017 20:47:57 GMT"}], "update_date": "2017-11-15", "authors_parsed": [["Thickstun", "John", ""], ["Harchaoui", "Zaid", ""], ["Foster", "Dean", ""], ["Kakade", "Sham M.", ""]]}, {"id": "1711.04851", "submitter": "Aditya Balu", "authors": "Sambit Ghadai, Aditya Balu, Adarsh Krishnamurthy, Soumik Sarkar", "title": "Learning and Visualizing Localized Geometric Features Using 3D-CNN: An\n  Application to Manufacturability Analysis of Drilled Holes", "comments": "Presented at NIPS 2017 Symposium on Interpretable Machine Learning", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  3D Convolutional Neural Networks (3D-CNN) have been used for object\nrecognition based on the voxelized shape of an object. However, interpreting\nthe decision making process of these 3D-CNNs is still an infeasible task. In\nthis paper, we present a unique 3D-CNN based Gradient-weighted Class Activation\nMapping method (3D-GradCAM) for visual explanations of the distinct local\ngeometric features of interest within an object. To enable efficient learning\nof 3D geometries, we augment the voxel data with surface normals of the object\nboundary. We then train a 3D-CNN with this augmented data and identify the\nlocal features critical for decision-making using 3D GradCAM. An application of\nthis feature identification framework is to recognize difficult-to-manufacture\ndrilled hole features in a complex CAD geometry. The framework can be extended\nto identify difficult-to-manufacture features at multiple spatial scales\nleading to a real-time design for manufacturability decision support system.\n", "versions": [{"version": "v1", "created": "Mon, 13 Nov 2017 21:05:39 GMT"}, {"version": "v2", "created": "Thu, 16 Nov 2017 18:49:35 GMT"}, {"version": "v3", "created": "Wed, 28 Mar 2018 03:52:41 GMT"}], "update_date": "2018-03-29", "authors_parsed": [["Ghadai", "Sambit", ""], ["Balu", "Aditya", ""], ["Krishnamurthy", "Adarsh", ""], ["Sarkar", "Soumik", ""]]}, {"id": "1711.04855", "submitter": "Ruairidh Battleday", "authors": "Ruairidh M. Battleday, Joshua C. Peterson, Thomas L. Griffiths", "title": "Modeling Human Categorization of Natural Images Using Deep Feature\n  Representations", "comments": "13 pages, 7 figures, 6 tables. Preliminary work presented at CogSci\n  2017", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Over the last few decades, psychologists have developed sophisticated formal\nmodels of human categorization using simple artificial stimuli. In this paper,\nwe use modern machine learning methods to extend this work into the realm of\nnaturalistic stimuli, enabling human categorization to be studied over the\ncomplex visual domain in which it evolved and developed. We show that\nrepresentations derived from a convolutional neural network can be used to\nmodel behavior over a database of >300,000 human natural image classifications,\nand find that a group of models based on these representations perform well,\nnear the reliability of human judgments. Interestingly, this group includes\nboth exemplar and prototype models, contrasting with the dominance of exemplar\nmodels in previous work. We are able to improve the performance of the\nremaining models by preprocessing neural network representations to more\nclosely capture human similarity judgments.\n", "versions": [{"version": "v1", "created": "Mon, 13 Nov 2017 21:18:29 GMT"}], "update_date": "2017-11-15", "authors_parsed": [["Battleday", "Ruairidh M.", ""], ["Peterson", "Joshua C.", ""], ["Griffiths", "Thomas L.", ""]]}, {"id": "1711.04877", "submitter": "Andrew Holbrook", "authors": "Andrew Holbrook, Thomas Lumley, Daniel Gillen", "title": "Estimating prediction error for complex samples", "comments": "To appear in the Canadian Journal of Statistics", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  With a growing interest in using non-representative samples to train\nprediction models for numerous outcomes it is necessary to account for the\nsampling design that gives rise to the data in order to assess the generalized\npredictive utility of a proposed prediction rule. After learning a prediction\nrule based on a non-uniform sample, it is of interest to estimate the rule's\nerror rate when applied to unobserved members of the population. Efron (1986)\nproposed a general class of covariance penalty inflated prediction error\nestimators that assume the available training data are representative of the\ntarget population for which the prediction rule is to be applied. We extend\nEfron's estimator to the complex sample context by incorporating\nHorvitz-Thompson sampling weights and show that it is consistent for the true\ngeneralization error rate when applied to the underlying superpopulation. The\nresulting Horvitz-Thompson-Efron (HTE) estimator is equivalent to dAIC, a\nrecent extension of AIC to survey sampling data, but is more widely applicable.\nThe proposed methodology is assessed with simulations and is applied to models\npredicting renal function obtained from the large-scale NHANES survey.\n", "versions": [{"version": "v1", "created": "Mon, 13 Nov 2017 22:30:47 GMT"}, {"version": "v2", "created": "Mon, 12 Mar 2018 05:46:58 GMT"}, {"version": "v3", "created": "Sat, 14 Sep 2019 21:04:15 GMT"}], "update_date": "2019-09-17", "authors_parsed": [["Holbrook", "Andrew", ""], ["Lumley", "Thomas", ""], ["Gillen", "Daniel", ""]]}, {"id": "1711.04887", "submitter": "Mohsen Ghassemi", "authors": "Mohsen Ghassemi, Zahra Shakeri, Anand D. Sarwate, Waheed U. Bajwa", "title": "STARK: Structured Dictionary Learning Through Rank-one Tensor Recovery", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In recent years, a class of dictionaries have been proposed for\nmultidimensional (tensor) data representation that exploit the structure of\ntensor data by imposing a Kronecker structure on the dictionary underlying the\ndata. In this work, a novel algorithm called \"STARK\" is provided to learn\nKronecker structured dictionaries that can represent tensors of any order. By\nestablishing that the Kronecker product of any number of matrices can be\nrearranged to form a rank-1 tensor, we show that Kronecker structure can be\nenforced on the dictionary by solving a rank-1 tensor recovery problem. Because\nrank-1 tensor recovery is a challenging nonconvex problem, we resort to solving\na convex relaxation of this problem. Empirical experiments on synthetic and\nreal data show promising results for our proposed algorithm.\n", "versions": [{"version": "v1", "created": "Mon, 13 Nov 2017 23:17:51 GMT"}], "update_date": "2017-11-15", "authors_parsed": [["Ghassemi", "Mohsen", ""], ["Shakeri", "Zahra", ""], ["Sarwate", "Anand D.", ""], ["Bajwa", "Waheed U.", ""]]}, {"id": "1711.04894", "submitter": "Youssef  Mroueh", "authors": "Youssef Mroueh, Chun-Liang Li, Tom Sercu, Anant Raj, Yu Cheng", "title": "Sobolev GAN", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a new Integral Probability Metric (IPM) between distributions: the\nSobolev IPM. The Sobolev IPM compares the mean discrepancy of two distributions\nfor functions (critic) restricted to a Sobolev ball defined with respect to a\ndominant measure $\\mu$. We show that the Sobolev IPM compares two distributions\nin high dimensions based on weighted conditional Cumulative Distribution\nFunctions (CDF) of each coordinate on a leave one out basis. The Dominant\nmeasure $\\mu$ plays a crucial role as it defines the support on which\nconditional CDFs are compared. Sobolev IPM can be seen as an extension of the\none dimensional Von-Mises Cram\\'er statistics to high dimensional\ndistributions. We show how Sobolev IPM can be used to train Generative\nAdversarial Networks (GANs). We then exploit the intrinsic conditioning implied\nby Sobolev IPM in text generation. Finally we show that a variant of Sobolev\nGAN achieves competitive results in semi-supervised learning on CIFAR-10,\nthanks to the smoothness enforced on the critic by Sobolev GAN which relates to\nLaplacian regularization.\n", "versions": [{"version": "v1", "created": "Tue, 14 Nov 2017 00:41:09 GMT"}], "update_date": "2017-11-15", "authors_parsed": [["Mroueh", "Youssef", ""], ["Li", "Chun-Liang", ""], ["Sercu", "Tom", ""], ["Raj", "Anant", ""], ["Cheng", "Yu", ""]]}, {"id": "1711.04913", "submitter": "Asa Ben-Hur", "authors": "Amina Asif, Wajid Arshad Abbasi, Farzeen Munir, Asa Ben-Hur, and\n  Fayyaz ul Amir Afsar Minhas", "title": "pyLEMMINGS: Large Margin Multiple Instance Classification and Ranking\n  for Bioinformatics Applications", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Motivation: A major challenge in the development of machine learning based\nmethods in computational biology is that data may not be accurately labeled due\nto the time and resources required for experimentally annotating properties of\nproteins and DNA sequences. Standard supervised learning algorithms assume\naccurate instance-level labeling of training data. Multiple instance learning\nis a paradigm for handling such labeling ambiguities. However, the widely used\nlarge-margin classification methods for multiple instance learning are\nheuristic in nature with high computational requirements. In this paper, we\npresent stochastic sub-gradient optimization large margin algorithms for\nmultiple instance classification and ranking, and provide them in a software\nsuite called pyLEMMINGS.\n  Results: We have tested pyLEMMINGS on a number of bioinformatics problems as\nwell as benchmark datasets. pyLEMMINGS has successfully been able to identify\nfunctionally important segments of proteins: binding sites in Calmodulin\nbinding proteins, prion forming regions, and amyloid cores. pyLEMMINGS achieves\nstate-of-the-art performance in all these tasks, demonstrating the value of\nmultiple instance learning. Furthermore, our method has shown more than\n100-fold improvement in terms of running time as compared to heuristic\nsolutions with improved accuracy over benchmark datasets.\n  Availability and Implementation: pyLEMMINGS python package is available for\ndownload at: http://faculty.pieas.edu.pk/fayyaz/software.html#pylemmings.\n", "versions": [{"version": "v1", "created": "Tue, 14 Nov 2017 02:41:01 GMT"}], "update_date": "2017-11-15", "authors_parsed": [["Asif", "Amina", ""], ["Abbasi", "Wajid Arshad", ""], ["Munir", "Farzeen", ""], ["Ben-Hur", "Asa", ""], ["Minhas", "Fayyaz ul Amir Afsar", ""]]}, {"id": "1711.04934", "submitter": "Dong Xia", "authors": "Dong Xia and Ming Yuan and Cun-Hui Zhang", "title": "Statistically Optimal and Computationally Efficient Low Rank Tensor\n  Completion from Noisy Entries", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.IT math.IT math.ST stat.ME stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this article, we develop methods for estimating a low rank tensor from\nnoisy observations on a subset of its entries to achieve both statistical and\ncomputational efficiencies. There have been a lot of recent interests in this\nproblem of noisy tensor completion. Much of the attention has been focused on\nthe fundamental computational challenges often associated with problems\ninvolving higher order tensors, yet very little is known about their\nstatistical performance. To fill in this void, in this article, we characterize\nthe fundamental statistical limits of noisy tensor completion by establishing\nminimax optimal rates of convergence for estimating a $k$th order low rank\ntensor under the general $\\ell_p$ ($1\\le p\\le 2$) norm which suggest\nsignificant room for improvement over the existing approaches. Furthermore, we\npropose a polynomial-time computable estimating procedure based upon power\niteration and a second-order spectral initialization that achieves the optimal\nrates of convergence. Our method is fairly easy to implement and numerical\nexperiments are presented to further demonstrate the practical merits of our\nestimator.\n", "versions": [{"version": "v1", "created": "Tue, 14 Nov 2017 03:46:05 GMT"}, {"version": "v2", "created": "Mon, 19 Mar 2018 20:09:57 GMT"}], "update_date": "2018-03-21", "authors_parsed": [["Xia", "Dong", ""], ["Yuan", "Ming", ""], ["Zhang", "Cun-Hui", ""]]}, {"id": "1711.04952", "submitter": "Ilias Zadik", "authors": "David Gamarnik, Ilias Zadik", "title": "Sparse High-Dimensional Linear Regression. Algorithmic Barriers and a\n  Local Search Algorithm", "comments": "Added a result on the failure of the LASSO recovery mechanism in the\n  conjectured algorithmically hard regime $n<c n_{alg}$ and minor corrections", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST math.PR stat.ML stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider a sparse high dimensional regression model where the goal is to\nrecover a $k$-sparse unknown vector $\\beta^*$ from $n$ noisy linear\nobservations of the form $Y=X\\beta^*+W \\in \\mathbb{R}^n$ where $X \\in\n\\mathbb{R}^{n \\times p}$ has iid $N(0,1)$ entries and $W \\in \\mathbb{R}^n$ has\niid $N(0,\\sigma^2)$ entries. Under certain assumptions on the parameters, an\nintriguing assymptotic gap appears between the minimum value of $n$, call it\n$n^*$, for which the recovery is information theoretically possible, and the\nminimum value of $n$, call it $n_{\\mathrm{alg}}$, for which an efficient\nalgorithm is known to provably recover $\\beta^*$. In \\cite{gamarnikzadik} it\nwas conjectured that the gap is not artificial, in the sense that for sample\nsizes $n \\in [n^*,n_{\\mathrm{alg}}]$ the problem is algorithmically hard.\n  We support this conjecture in two ways. Firstly, we show that the optimal\nsolution of the LASSO provably fails to $\\ell_2$-stably recover the unknown\nvector $\\beta^*$ when $n \\in [n^*,c n_{\\mathrm{alg}}]$, for some sufficiently\nsmall constant $c>0$. Secondly, we establish that $n_{\\mathrm{alg}}$, up to a\nmultiplicative constant factor, is a phase transition point for the appearance\nof a certain Overlap Gap Property (OGP) over the space of $k$-sparse vectors.\nThe presence of such an Overlap Gap Property phase transition, which originates\nin statistical physics, is known to provide evidence of an algorithmic\nhardness. Finally we show that if $n>C n_{\\mathrm{alg}}$ for some large enough\nconstant $C>0$, a very simple algorithm based on a local search improvement\nrule is able both to $\\ell_2$-stably recover the unknown vector $\\beta^*$ and\nto infer correctly its support, adding it to the list of provably successful\nalgorithms for the high dimensional linear regression problem.\n", "versions": [{"version": "v1", "created": "Tue, 14 Nov 2017 05:20:20 GMT"}, {"version": "v2", "created": "Sun, 22 Sep 2019 17:03:34 GMT"}], "update_date": "2019-09-24", "authors_parsed": [["Gamarnik", "David", ""], ["Zadik", "Ilias", ""]]}, {"id": "1711.04955", "submitter": "Sen Na", "authors": "Sen Na, Mingyuan Ma, Mladen Kolar", "title": "Scalable Peaceman-Rachford Splitting Method with Proximal Terms", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Along with developing of Peaceman-Rachford Splittling Method (PRSM), many\nbatch algorithms based on it have been studied very deeply. But almost no\nalgorithm focused on the performance of stochastic version of PRSM. In this\npaper, we propose a new stochastic algorithm based on PRSM, prove its\nconvergence rate in ergodic sense, and test its performance on both artificial\nand real data. We show that our proposed algorithm, Stochastic Scalable PRSM\n(SS-PRSM), enjoys the $O(1/K)$ convergence rate, which is the same as those\nnewest stochastic algorithms that based on ADMM but faster than general\nStochastic ADMM (which is $O(1/\\sqrt{K})$). Our algorithm also owns wide\nflexibility, outperforms many state-of-the-art stochastic algorithms coming\nfrom ADMM, and has low memory cost in large-scale splitting optimization\nproblems.\n", "versions": [{"version": "v1", "created": "Tue, 14 Nov 2017 05:45:32 GMT"}, {"version": "v2", "created": "Sat, 10 Feb 2018 02:27:16 GMT"}], "update_date": "2018-02-13", "authors_parsed": [["Na", "Sen", ""], ["Ma", "Mingyuan", ""], ["Kolar", "Mladen", ""]]}, {"id": "1711.04965", "submitter": "Navid Ghadermarzy", "authors": "Navid Ghadermarzy, Yaniv Plan, \\\"Ozg\\\"ur Y{\\i}lmaz", "title": "Near-optimal sample complexity for convex tensor completion", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG math.OC stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We analyze low rank tensor completion (TC) using noisy measurements of a\nsubset of the tensor. Assuming a rank-$r$, order-$d$, $N \\times N \\times \\cdots\n\\times N$ tensor where $r=O(1)$, the best sampling complexity that was achieved\nis $O(N^{\\frac{d}{2}})$, which is obtained by solving a tensor nuclear-norm\nminimization problem. However, this bound is significantly larger than the\nnumber of free variables in a low rank tensor which is $O(dN)$. In this paper,\nwe show that by using an atomic-norm whose atoms are rank-$1$ sign tensors, one\ncan obtain a sample complexity of $O(dN)$. Moreover, we generalize the matrix\nmax-norm definition to tensors, which results in a max-quasi-norm (max-qnorm)\nwhose unit ball has small Rademacher complexity. We prove that solving a\nconstrained least squares estimation using either the convex atomic-norm or the\nnonconvex max-qnorm results in optimal sample complexity for the problem of\nlow-rank tensor completion. Furthermore, we show that these bounds are nearly\nminimax rate-optimal. We also provide promising numerical results for max-qnorm\nconstrained tensor completion, showing improved recovery results compared to\nmatricization and alternating least squares.\n", "versions": [{"version": "v1", "created": "Tue, 14 Nov 2017 06:20:05 GMT"}], "update_date": "2017-11-15", "authors_parsed": [["Ghadermarzy", "Navid", ""], ["Plan", "Yaniv", ""], ["Y\u0131lmaz", "\u00d6zg\u00fcr", ""]]}, {"id": "1711.04969", "submitter": "Can Karakus", "authors": "Can Karakus, Yifan Sun, Suhas Diggavi, Wotao Yin", "title": "Straggler Mitigation in Distributed Optimization Through Data Encoding", "comments": "appeared at NIPS 2017", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.DC cs.IT cs.LG math.IT", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Slow running or straggler tasks can significantly reduce computation speed in\ndistributed computation. Recently, coding-theory-inspired approaches have been\napplied to mitigate the effect of straggling, through embedding redundancy in\ncertain linear computational steps of the optimization algorithm, thus\ncompleting the computation without waiting for the stragglers. In this paper,\nwe propose an alternate approach where we embed the redundancy directly in the\ndata itself, and allow the computation to proceed completely oblivious to\nencoding. We propose several encoding schemes, and demonstrate that popular\nbatch algorithms, such as gradient descent and L-BFGS, applied in a\ncoding-oblivious manner, deterministically achieve sample path linear\nconvergence to an approximate solution of the original problem, using an\narbitrarily varying subset of the nodes at each iteration. Moreover, this\napproximation can be controlled by the amount of redundancy and the number of\nnodes used in each iteration. We provide experimental results demonstrating the\nadvantage of the approach over uncoded and data replication strategies.\n", "versions": [{"version": "v1", "created": "Tue, 14 Nov 2017 06:29:41 GMT"}, {"version": "v2", "created": "Mon, 22 Jan 2018 23:28:11 GMT"}], "update_date": "2018-01-24", "authors_parsed": [["Karakus", "Can", ""], ["Sun", "Yifan", ""], ["Diggavi", "Suhas", ""], ["Yin", "Wotao", ""]]}, {"id": "1711.04979", "submitter": "Jun Song", "authors": "Chenchao Zhao and Jun S. Song", "title": "Quantum transport senses community structure in networks", "comments": null, "journal-ref": "Phys. Rev. E 98, 022301 (2018)", "doi": "10.1103/PhysRevE.98.022301", "report-no": null, "categories": "quant-ph cond-mat.other cs.DS q-bio.QM stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Quantum time evolution exhibits rich physics, attributable to the interplay\nbetween the density and phase of a wave function. However, unlike classical\nheat diffusion, the wave nature of quantum mechanics has not yet been\nextensively explored in modern data analysis. We propose that the Laplace\ntransform of quantum transport (QT) can be used to construct an ensemble of\nmaps from a given complex network to a circle $S^1$, such that closely-related\nnodes on the network are grouped into sharply concentrated clusters on $S^1$.\nThe resulting QT clustering (QTC) algorithm is as powerful as the\nstate-of-the-art spectral clustering in discerning complex geometric patterns\nand more robust when clusters show strong density variations or heterogeneity\nin size. The observed phenomenon of QTC can be interpreted as a collective\nbehavior of the microscopic nodes that evolve as macroscopic cluster orbitals\nin an effective tight-binding model recapitulating the network. Python source\ncode implementing the algorithm and examples are available at\nhttps://github.com/jssong-lab/QTC.\n", "versions": [{"version": "v1", "created": "Tue, 14 Nov 2017 07:03:05 GMT"}, {"version": "v2", "created": "Fri, 12 Jan 2018 05:52:07 GMT"}], "update_date": "2018-08-08", "authors_parsed": [["Zhao", "Chenchao", ""], ["Song", "Jun S.", ""]]}, {"id": "1711.04992", "submitter": "Bogdan Kulynych", "authors": "Bogdan Kulynych, Carmela Troncoso", "title": "Feature importance scores and lossless feature pruning using Banzhaf\n  power indices", "comments": "Presented at NIPS 2017 Symposium on Interpretable Machine Learning", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Understanding the influence of features in machine learning is crucial to\ninterpreting models and selecting the best features for classification. In this\nwork we propose the use of principles from coalitional game theory to reason\nabout importance of features. In particular, we propose the use of the Banzhaf\npower index as a measure of influence of features on the outcome of a\nclassifier. We show that features having Banzhaf power index of zero can be\nlosslessly pruned without damage to classifier accuracy. Computing the power\nindices does not require having access to data samples. However, if samples are\navailable, the indices can be empirically estimated. We compute Banzhaf power\nindices for a neural network classifier on real-life data, and compare the\nresults with gradient-based feature saliency, and coefficients of a logistic\nregression model with $L_1$ regularization.\n", "versions": [{"version": "v1", "created": "Tue, 14 Nov 2017 08:24:01 GMT"}, {"version": "v2", "created": "Sun, 3 Dec 2017 05:40:39 GMT"}], "update_date": "2017-12-05", "authors_parsed": [["Kulynych", "Bogdan", ""], ["Troncoso", "Carmela", ""]]}, {"id": "1711.05068", "submitter": "Peng-Bo Zhang", "authors": "Peng-Bo Zhang and Zhi-Xin Yang", "title": "Robust Matrix Elastic Net based Canonical Correlation Analysis: An\n  Effective Algorithm for Multi-View Unsupervised Learning", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper presents a robust matrix elastic net based canonical correlation\nanalysis (RMEN-CCA) for multiple view unsupervised learning problems, which\nemphasizes the combination of CCA and the robust matrix elastic net (RMEN) used\nas coupled feature selection. The RMEN-CCA leverages the strength of the RMEN\nto distill naturally meaningful features without any prior assumption and to\nmeasure effectively correlations between different 'views'. We can further\nemploy directly the kernel trick to extend the RMEN-CCA to the kernel scenario\nwith theoretical guarantees, which takes advantage of the kernel trick for\nhighly complicated nonlinear feature learning. Rather than simply incorporating\nexisting regularization minimization terms into CCA, this paper provides a new\nlearning paradigm for CCA and is the first to derive a coupled feature\nselection based CCA algorithm that guarantees convergence. More significantly,\nfor CCA, the newly-derived RMEN-CCA bridges the gap between measurement of\nrelevance and coupled feature selection. Moreover, it is nontrivial to tackle\ndirectly the RMEN-CCA by previous optimization approaches derived from its\nsophisticated model architecture. Therefore, this paper further offers a bridge\nbetween a new optimization problem and an existing efficient iterative\napproach. As a consequence, the RMEN-CCA can overcome the limitation of CCA and\naddress large-scale and streaming data problems. Experimental results on four\npopular competing datasets illustrate that the RMEN-CCA performs more\neffectively and efficiently than do state-of-the-art approaches.\n", "versions": [{"version": "v1", "created": "Tue, 14 Nov 2017 12:00:53 GMT"}, {"version": "v2", "created": "Wed, 15 Nov 2017 07:38:17 GMT"}], "update_date": "2017-11-16", "authors_parsed": [["Zhang", "Peng-Bo", ""], ["Yang", "Zhi-Xin", ""]]}, {"id": "1711.05084", "submitter": "Gongze Cao", "authors": "Gongze Cao, Yezhou Yang, Jie Lei, Cheng Jin, Yang Liu, Mingli Song", "title": "TripletGAN: Training Generative Model with Triplet Loss", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  As an effective way of metric learning, triplet loss has been widely used in\nmany deep learning tasks, including face recognition and person-ReID, leading\nto many states of the arts. The main innovation of triplet loss is using\nfeature map to replace softmax in the classification task. Inspired by this\nconcept, we propose here a new adversarial modeling method by substituting the\nclassification loss of discriminator with triplet loss. Theoretical proof based\non IPM (Integral probability metric) demonstrates that such setting will help\nthe generator converge to the given distribution theoretically under some\nconditions. Moreover, since triplet loss requires the generator to maximize\ndistance within a class, we justify tripletGAN is also helpful to prevent mode\ncollapse through both theory and experiment.\n", "versions": [{"version": "v1", "created": "Tue, 14 Nov 2017 12:45:10 GMT"}], "update_date": "2017-11-15", "authors_parsed": [["Cao", "Gongze", ""], ["Yang", "Yezhou", ""], ["Lei", "Jie", ""], ["Jin", "Cheng", ""], ["Liu", "Yang", ""], ["Song", "Mingli", ""]]}, {"id": "1711.05090", "submitter": "Thomas Guyet", "authors": "Thomas Guyet (LACODAM), Yves Moinard (LACODAM), Ren\\'e Quiniou\n  (LACODAM), Torsten Schaub (LACODAM)", "title": "Efficiency Analysis of ASP Encodings for Sequential Pattern Mining Tasks", "comments": null, "journal-ref": "Bruno Pinaud; Fabrice Guillet; Bruno Cremilleux; Cyril de Runz.\n  Advances in Knowledge Discovery and Management, 7, Springer, pp.41--81, 2017,\n  978-3-319-65405-8", "doi": null, "report-no": null, "categories": "cs.AI cs.DB stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This article presents the use of Answer Set Programming (ASP) to mine\nsequential patterns. ASP is a high-level declarative logic programming paradigm\nfor high level encoding combinatorial and optimization problem solving as well\nas knowledge representation and reasoning. Thus, ASP is a good candidate for\nimplementing pattern mining with background knowledge, which has been a data\nmining issue for a long time. We propose encodings of the classical sequential\npattern mining tasks within two representations of embeddings (fill-gaps vs\nskip-gaps) and for various kinds of patterns: frequent, constrained and\ncondensed. We compare the computational performance of these encodings with\neach other to get a good insight into the efficiency of ASP encodings. The\nresults show that the fill-gaps strategy is better on real problems due to\nlower memory consumption. Finally, compared to a constraint programming\napproach (CPSM), another declarative programming paradigm, our proposal showed\ncomparable performance.\n", "versions": [{"version": "v1", "created": "Tue, 14 Nov 2017 14:09:05 GMT"}], "update_date": "2017-11-15", "authors_parsed": [["Guyet", "Thomas", "", "LACODAM"], ["Moinard", "Yves", "", "LACODAM"], ["Quiniou", "Ren\u00e9", "", "LACODAM"], ["Schaub", "Torsten", "", "LACODAM"]]}, {"id": "1711.05099", "submitter": "Maxwell Hutchinson", "authors": "Maxwell L. Hutchinson, Erin Antono, Brenna M. Gibbons, Sean Paradiso,\n  Julia Ling, Bryce Meredig", "title": "Overcoming data scarcity with transfer learning", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cond-mat.mtrl-sci stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Despite increasing focus on data publication and discovery in materials\nscience and related fields, the global view of materials data is highly sparse.\nThis sparsity encourages training models on the union of multiple datasets, but\nsimple unions can prove problematic as (ostensibly) equivalent properties may\nbe measured or computed differently depending on the data source. These hidden\ncontextual differences introduce irreducible errors into analyses,\nfundamentally limiting their accuracy. Transfer learning, where information\nfrom one dataset is used to inform a model on another, can be an effective tool\nfor bridging sparse data while preserving the contextual differences in the\nunderlying measurements. Here, we describe and compare three techniques for\ntransfer learning: multi-task, difference, and explicit latent variable\narchitectures. We show that difference architectures are most accurate in the\nmulti-fidelity case of mixed DFT and experimental band gaps, while multi-task\nmost improves classification performance of color with band gaps. For\nactivation energies of steps in NO reduction, the explicit latent variable\nmethod is not only the most accurate, but also enjoys cancellation of errors in\nfunctions that depend on multiple tasks. These results motivate the publication\nof high quality materials datasets that encode transferable information,\nindependent of industrial or academic interest in the particular labels, and\nencourage further development and application of transfer learning methods to\nmaterials informatics problems.\n", "versions": [{"version": "v1", "created": "Thu, 2 Nov 2017 12:54:51 GMT"}], "update_date": "2017-11-15", "authors_parsed": [["Hutchinson", "Maxwell L.", ""], ["Antono", "Erin", ""], ["Gibbons", "Brenna M.", ""], ["Paradiso", "Sean", ""], ["Ling", "Julia", ""], ["Meredig", "Bryce", ""]]}, {"id": "1711.05102", "submitter": "Qianqian Yang", "authors": "Qianqian Yang, Pablo Piantanida, Deniz G\\\"und\\\"uz", "title": "The Multi-layer Information Bottleneck Problem", "comments": "5 pages, 2 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.IT cs.LG math.IT", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The muti-layer information bottleneck (IB) problem, where information is\npropagated (or successively refined) from layer to layer, is considered. Based\non information forwarded by the preceding layer, each stage of the network is\nrequired to preserve a certain level of relevance with regards to a specific\nhidden variable, quantified by the mutual information. The hidden variables and\nthe source can be arbitrarily correlated. The optimal trade-off between rates\nof relevance and compression (or complexity) is obtained through a\nsingle-letter characterization, referred to as the rate-relevance region.\nConditions of successive refinabilty are given. Binary source with BSC hidden\nvariables and binary source with BSC/BEC mixed hidden variables are both proved\nto be successively refinable. We further extend our result to Guassian models.\nA counterexample of successive refinability is also provided.\n", "versions": [{"version": "v1", "created": "Tue, 14 Nov 2017 14:24:37 GMT"}], "update_date": "2017-11-15", "authors_parsed": [["Yang", "Qianqian", ""], ["Piantanida", "Pablo", ""], ["G\u00fcnd\u00fcz", "Deniz", ""]]}, {"id": "1711.05136", "submitter": "Guillaume Bellec", "authors": "Guillaume Bellec, David Kappel, Wolfgang Maass and Robert Legenstein", "title": "Deep Rewiring: Training very sparse deep networks", "comments": "Accepted for publication at ICLR 2018. 10 pages (12 with references,\n  24 with appendix), 4 Figures in the main text. Reviews are available at:\n  https://openreview.net/forum?id=BJ_wN01C- . This recent version contains\n  minor corrections in the appendix", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NE cs.AI cs.DC cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Neuromorphic hardware tends to pose limits on the connectivity of deep\nnetworks that one can run on them. But also generic hardware and software\nimplementations of deep learning run more efficiently for sparse networks.\nSeveral methods exist for pruning connections of a neural network after it was\ntrained without connectivity constraints. We present an algorithm, DEEP R, that\nenables us to train directly a sparsely connected neural network. DEEP R\nautomatically rewires the network during supervised training so that\nconnections are there where they are most needed for the task, while its total\nnumber is all the time strictly bounded. We demonstrate that DEEP R can be used\nto train very sparse feedforward and recurrent neural networks on standard\nbenchmark tasks with just a minor loss in performance. DEEP R is based on a\nrigorous theoretical foundation that views rewiring as stochastic sampling of\nnetwork configurations from a posterior.\n", "versions": [{"version": "v1", "created": "Tue, 14 Nov 2017 15:02:47 GMT"}, {"version": "v2", "created": "Fri, 15 Dec 2017 18:33:53 GMT"}, {"version": "v3", "created": "Fri, 2 Feb 2018 15:57:44 GMT"}, {"version": "v4", "created": "Mon, 5 Feb 2018 11:01:41 GMT"}, {"version": "v5", "created": "Tue, 7 Aug 2018 18:12:10 GMT"}], "update_date": "2018-08-09", "authors_parsed": [["Bellec", "Guillaume", ""], ["Kappel", "David", ""], ["Maass", "Wolfgang", ""], ["Legenstein", "Robert", ""]]}, {"id": "1711.05150", "submitter": "Yongjin Park", "authors": "Yongjin Park and Joel S. Bader", "title": "Fast and reliable inference algorithm for hierarchical stochastic block\n  models", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Network clustering reveals the organization of a network or corresponding\ncomplex system with elements represented as vertices and interactions as edges\nin a (directed, weighted) graph. Although the notion of clustering can be\nsomewhat loose, network clusters or groups are generally considered as nodes\nwith enriched interactions and edges sharing common patterns. Statistical\ninference often treats groups as latent variables, with observed networks\ngenerated from latent group structure, termed a stochastic block model.\nRegardless of the definitions, statistical inference can be either translated\nto modularity maximization, which is provably an NP-complete problem.\n  Here we present scalable and reliable algorithms that recover hierarchical\nstochastic block models fast and accurately. Our algorithm scales almost\nlinearly in number of edges, and inferred models were more accurate that other\nscalable methods.\n", "versions": [{"version": "v1", "created": "Tue, 14 Nov 2017 15:41:10 GMT"}], "update_date": "2017-11-15", "authors_parsed": [["Park", "Yongjin", ""], ["Bader", "Joel S.", ""]]}, {"id": "1711.05170", "submitter": "Diego Molla-Aliod", "authors": "Hamideh Hajiabadi, Diego Molla-Aliod, Reza Monsefi", "title": "On Extending Neural Networks with Loss Ensembles for Text Classification", "comments": "5 pages, 5 tables, 1 figure. Camera-ready submitted to The 2017\n  Australasian Language Technology Association Workshop (ALTA 2017)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Ensemble techniques are powerful approaches that combine several weak\nlearners to build a stronger one. As a meta learning framework, ensemble\ntechniques can easily be applied to many machine learning techniques. In this\npaper we propose a neural network extended with an ensemble loss function for\ntext classification. The weight of each weak loss function is tuned within the\ntraining phase through the gradient propagation optimization method of the\nneural network. The approach is evaluated on several text classification\ndatasets. We also evaluate its performance in various environments with several\ndegrees of label noise. Experimental results indicate an improvement of the\nresults and strong resilience against label noise in comparison with other\nmethods.\n", "versions": [{"version": "v1", "created": "Tue, 14 Nov 2017 16:19:34 GMT"}], "update_date": "2017-11-15", "authors_parsed": [["Hajiabadi", "Hamideh", ""], ["Molla-Aliod", "Diego", ""], ["Monsefi", "Reza", ""]]}, {"id": "1711.05174", "submitter": "Yining Wang", "authors": "Zeyuan Allen-Zhu and Yuanzhi Li and Aarti Singh and Yining Wang", "title": "Near-Optimal Discrete Optimization for Experimental Design: A Regret\n  Minimization Approach", "comments": "33 pages, 4 tables. A preliminary version of this paper titled\n  \"Near-Optimal Experimental Design via Regret Minimization\" with weaker\n  results appeared in the Proceedings of the 34th International Conference on\n  Machine Learning (ICML 2017), Sydney", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The experimental design problem concerns the selection of k points from a\npotentially large design pool of p-dimensional vectors, so as to maximize the\nstatistical efficiency regressed on the selected k design points. Statistical\nefficiency is measured by optimality criteria, including A(verage),\nD(eterminant), T(race), E(igen), V(ariance) and G-optimality. Except for the\nT-optimality, exact optimization is NP-hard.\n  We propose a polynomial-time regret minimization framework to achieve a\n$(1+\\varepsilon)$ approximation with only $O(p/\\varepsilon^2)$ design points,\nfor all the optimality criteria above.\n  In contrast, to the best of our knowledge, before our work, no\npolynomial-time algorithm achieves $(1+\\varepsilon)$ approximations for\nD/E/G-optimality, and the best poly-time algorithm achieving\n$(1+\\varepsilon)$-approximation for A/V-optimality requires $k =\n\\Omega(p^2/\\varepsilon)$ design points.\n", "versions": [{"version": "v1", "created": "Tue, 14 Nov 2017 16:21:57 GMT"}], "update_date": "2017-11-15", "authors_parsed": [["Allen-Zhu", "Zeyuan", ""], ["Li", "Yuanzhi", ""], ["Singh", "Aarti", ""], ["Wang", "Yining", ""]]}, {"id": "1711.05197", "submitter": "Daniel Heestermans Svendsen", "authors": "Daniel Heestermans Svendsen, Luca Martino, Manuel Campos-Taberner,\n  Francisco Javier Garc\\'ia-Haro and Gustau Camps-Valls", "title": "Joint Gaussian Processes for Biophysical Parameter Retrieval", "comments": "21 pages single column, Accepted for publication in IEEE Transactions\n  on Geoscience and Remote Sensing", "journal-ref": null, "doi": "10.1109/TGRS.2017.2767205", "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Solving inverse problems is central to geosciences and remote sensing.\nRadiative transfer models (RTMs) represent mathematically the physical laws\nwhich govern the phenomena in remote sensing applications (forward models). The\nnumerical inversion of the RTM equations is a challenging and computationally\ndemanding problem, and for this reason, often the application of a nonlinear\nstatistical regression is preferred. In general, regression models predict the\nbiophysical parameter of interest from the corresponding received radiance.\nHowever, this approach does not employ the physical information encoded in the\nRTMs. An alternative strategy, which attempts to include the physical\nknowledge, consists in learning a regression model trained using data simulated\nby an RTM code. In this work, we introduce a nonlinear nonparametric regression\nmodel which combines the benefits of the two aforementioned approaches. The\ninversion is performed taking into account jointly both real observations and\nRTM-simulated data. The proposed Joint Gaussian Process (JGP) provides a solid\nframework for exploiting the regularities between the two types of data. The\nJGP automatically detects the relative quality of the simulated and real data,\nand combines them accordingly. This occurs by learning an additional\nhyper-parameter w.r.t. a standard GP model, and fitting parameters through\nmaximizing the pseudo-likelihood of the real observations. The resulting scheme\nis both simple and robust, i.e., capable of adapting to different scenarios.\nThe advantages of the JGP method compared to benchmark strategies are shown\nconsidering RTM-simulated and real observations in different experiments.\nSpecifically, we consider leaf area index (LAI) retrieval from Landsat data\ncombined with simulated data generated by the PROSAIL model.\n", "versions": [{"version": "v1", "created": "Tue, 14 Nov 2017 17:03:51 GMT"}], "update_date": "2017-11-15", "authors_parsed": [["Svendsen", "Daniel Heestermans", ""], ["Martino", "Luca", ""], ["Campos-Taberner", "Manuel", ""], ["Garc\u00eda-Haro", "Francisco Javier", ""], ["Camps-Valls", "Gustau", ""]]}, {"id": "1711.05225", "submitter": "Pranav Rajpurkar", "authors": "Pranav Rajpurkar, Jeremy Irvin, Kaylie Zhu, Brandon Yang, Hershel\n  Mehta, Tony Duan, Daisy Ding, Aarti Bagul, Curtis Langlotz, Katie Shpanskaya,\n  Matthew P. Lungren, Andrew Y. Ng", "title": "CheXNet: Radiologist-Level Pneumonia Detection on Chest X-Rays with Deep\n  Learning", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We develop an algorithm that can detect pneumonia from chest X-rays at a\nlevel exceeding practicing radiologists. Our algorithm, CheXNet, is a 121-layer\nconvolutional neural network trained on ChestX-ray14, currently the largest\npublicly available chest X-ray dataset, containing over 100,000 frontal-view\nX-ray images with 14 diseases. Four practicing academic radiologists annotate a\ntest set, on which we compare the performance of CheXNet to that of\nradiologists. We find that CheXNet exceeds average radiologist performance on\nthe F1 metric. We extend CheXNet to detect all 14 diseases in ChestX-ray14 and\nachieve state of the art results on all 14 diseases.\n", "versions": [{"version": "v1", "created": "Tue, 14 Nov 2017 17:58:50 GMT"}, {"version": "v2", "created": "Sat, 25 Nov 2017 04:21:27 GMT"}, {"version": "v3", "created": "Mon, 25 Dec 2017 11:09:06 GMT"}], "update_date": "2017-12-27", "authors_parsed": [["Rajpurkar", "Pranav", ""], ["Irvin", "Jeremy", ""], ["Zhu", "Kaylie", ""], ["Yang", "Brandon", ""], ["Mehta", "Hershel", ""], ["Duan", "Tony", ""], ["Ding", "Daisy", ""], ["Bagul", "Aarti", ""], ["Langlotz", "Curtis", ""], ["Shpanskaya", "Katie", ""], ["Lungren", "Matthew P.", ""], ["Ng", "Andrew Y.", ""]]}, {"id": "1711.05233", "submitter": "Nazmus Saquib", "authors": "Manash Kumar Mandal, Pinku Deb Nath, Arpeeta Shams Mizan, Nazmus\n  Saquib", "title": "A visual search engine for Bangladeshi laws", "comments": "Presented at NIPS 2017 Workshop on Machine Learning for the\n  Developing World. Corresponding author: Nazmus Saquib", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC cs.CY stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Browsing and finding relevant information for Bangladeshi laws is a challenge\nfaced by all law students and researchers in Bangladesh, and by citizens who\nwant to learn about any legal procedure. Some law archives in Bangladesh are\ndigitized, but lack proper tools to organize the data meaningfully. We present\na text visualization tool that utilizes machine learning techniques to make the\nsearching of laws quicker and easier. Using Doc2Vec to layout law article\nnodes, link mining techniques to visualize relevant citation networks, and\nnamed entity recognition to quickly find relevant sections in long law\narticles, our tool provides a faster and better search experience to the users.\nQualitative feedback from law researchers, students, and government officials\nshow promise for visually intuitive search tools in the context of\ngovernmental, legal, and constitutional data in developing countries, where\ndigitized data does not necessarily pave the way towards an easy access to\ninformation.\n", "versions": [{"version": "v1", "created": "Tue, 14 Nov 2017 18:15:11 GMT"}], "update_date": "2017-11-15", "authors_parsed": [["Mandal", "Manash Kumar", ""], ["Nath", "Pinku Deb", ""], ["Mizan", "Arpeeta Shams", ""], ["Saquib", "Nazmus", ""]]}, {"id": "1711.05323", "submitter": "Ahmad Beirami", "authors": "Ahmad Beirami and Meisam Razaviyayn and Shahin Shahrampour and Vahid\n  Tarokh", "title": "On Optimal Generalizability in Parametric Learning", "comments": "Proc. of 2017 Advances in Neural Information Processing Systems (NIPS\n  2017)", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider the parametric learning problem, where the objective of the\nlearner is determined by a parametric loss function. Employing empirical risk\nminimization with possibly regularization, the inferred parameter vector will\nbe biased toward the training samples. Such bias is measured by the cross\nvalidation procedure in practice where the data set is partitioned into a\ntraining set used for training and a validation set, which is not used in\ntraining and is left to measure the out-of-sample performance. A classical\ncross validation strategy is the leave-one-out cross validation (LOOCV) where\none sample is left out for validation and training is done on the rest of the\nsamples that are presented to the learner, and this process is repeated on all\nof the samples. LOOCV is rarely used in practice due to the high computational\ncomplexity. In this paper, we first develop a computationally efficient\napproximate LOOCV (ALOOCV) and provide theoretical guarantees for its\nperformance. Then we use ALOOCV to provide an optimization algorithm for\nfinding the regularizer in the empirical risk minimization framework. In our\nnumerical experiments, we illustrate the accuracy and efficiency of ALOOCV as\nwell as our proposed framework for the optimization of the regularizer.\n", "versions": [{"version": "v1", "created": "Tue, 14 Nov 2017 21:37:03 GMT"}], "update_date": "2017-11-16", "authors_parsed": [["Beirami", "Ahmad", ""], ["Razaviyayn", "Meisam", ""], ["Shahrampour", "Shahin", ""], ["Tarokh", "Vahid", ""]]}, {"id": "1711.05355", "submitter": "Alistair Letcher", "authors": "Alistair Letcher, Jelena Tri\\v{s}ovi\\'c, Collin Cademartori, Xi Chen,\n  Jason Xu", "title": "Automatic Conflict Detection in Police Body-Worn Audio", "comments": "5 pages, 2 figures, 1 table", "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.AS cs.SD stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Automatic conflict detection has grown in relevance with the advent of\nbody-worn technology, but existing metrics such as turn-taking and overlap are\npoor indicators of conflict in police-public interactions. Moreover, standard\ntechniques to compute them fall short when applied to such diversified and\nnoisy contexts. We develop a pipeline catered to this task combining adaptive\nnoise removal, non-speech filtering and new measures of conflict based on the\nrepetition and intensity of phrases in speech. We demonstrate the effectiveness\nof our approach on body-worn audio data collected by the Los Angeles Police\nDepartment.\n", "versions": [{"version": "v1", "created": "Tue, 14 Nov 2017 23:28:05 GMT"}, {"version": "v2", "created": "Wed, 14 Feb 2018 09:04:24 GMT"}], "update_date": "2018-02-15", "authors_parsed": [["Letcher", "Alistair", ""], ["Tri\u0161ovi\u0107", "Jelena", ""], ["Cademartori", "Collin", ""], ["Chen", "Xi", ""], ["Xu", "Jason", ""]]}, {"id": "1711.05363", "submitter": "Michael Arbel", "authors": "Michael Arbel and Arthur Gretton", "title": "Kernel Conditional Exponential Family", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A nonparametric family of conditional distributions is introduced, which\ngeneralizes conditional exponential families using functional parameters in a\nsuitable RKHS. An algorithm is provided for learning the generalized natural\nparameter, and consistency of the estimator is established in the well\nspecified case. In experiments, the new method generally outperforms a\ncompeting approach with consistency guarantees, and is competitive with a deep\nconditional density model on datasets that exhibit abrupt transitions and\nheteroscedasticity.\n", "versions": [{"version": "v1", "created": "Wed, 15 Nov 2017 00:32:08 GMT"}, {"version": "v2", "created": "Sun, 8 Apr 2018 00:55:55 GMT"}], "update_date": "2018-04-10", "authors_parsed": [["Arbel", "Michael", ""], ["Gretton", "Arthur", ""]]}, {"id": "1711.05365", "submitter": "Sajid Ahmed", "authors": "Sajid Ahmed, Farshid Rayhan, Asif Mahbub, Md. Rafsan Jani, Swakkhar\n  Shatabda, Dewan Md. Farid and Chowdhury Mofizur Rahman", "title": "LIUBoost : Locality Informed Underboosting for Imbalanced Data\n  Classification", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The problem of class imbalance along with class-overlapping has become a\nmajor issue in the domain of supervised learning. Most supervised learning\nalgorithms assume equal cardinality of the classes under consideration while\noptimizing the cost function and this assumption does not hold true for\nimbalanced datasets which results in sub-optimal classification. Therefore,\nvarious approaches, such as undersampling, oversampling, cost-sensitive\nlearning and ensemble based methods have been proposed for dealing with\nimbalanced datasets. However, undersampling suffers from information loss,\noversampling suffers from increased runtime and potential overfitting while\ncost-sensitive methods suffer due to inadequately defined cost assignment\nschemes. In this paper, we propose a novel boosting based method called\nLIUBoost. LIUBoost uses under sampling for balancing the datasets in every\nboosting iteration like RUSBoost while incorporating a cost term for every\ninstance based on their hardness into the weight update formula minimizing the\ninformation loss introduced by undersampling. LIUBoost has been extensively\nevaluated on 18 imbalanced datasets and the results indicate significant\nimprovement over existing best performing method RUSBoost.\n", "versions": [{"version": "v1", "created": "Wed, 15 Nov 2017 00:44:41 GMT"}], "update_date": "2017-11-16", "authors_parsed": [["Ahmed", "Sajid", ""], ["Rayhan", "Farshid", ""], ["Mahbub", "Asif", ""], ["Jani", "Md. Rafsan", ""], ["Shatabda", "Swakkhar", ""], ["Farid", "Dewan Md.", ""], ["Rahman", "Chowdhury Mofizur", ""]]}, {"id": "1711.05374", "submitter": "Huan Song", "authors": "Huan Song, Jayaraman J. Thiagarajan, Prasanna Sattigeri, Andreas\n  Spanias", "title": "Optimizing Kernel Machines using Deep Learning", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Building highly non-linear and non-parametric models is central to several\nstate-of-the-art machine learning systems. Kernel methods form an important\nclass of techniques that induce a reproducing kernel Hilbert space (RKHS) for\ninferring non-linear models through the construction of similarity functions\nfrom data. These methods are particularly preferred in cases where the training\ndata sizes are limited and when prior knowledge of the data similarities is\navailable. Despite their usefulness, they are limited by the computational\ncomplexity and their inability to support end-to-end learning with a\ntask-specific objective. On the other hand, deep neural networks have become\nthe de facto solution for end-to-end inference in several learning paradigms.\nIn this article, we explore the idea of using deep architectures to perform\nkernel machine optimization, for both computational efficiency and end-to-end\ninferencing. To this end, we develop the DKMO (Deep Kernel Machine\nOptimization) framework, that creates an ensemble of dense embeddings using\nNystrom kernel approximations and utilizes deep learning to generate\ntask-specific representations through the fusion of the embeddings.\nIntuitively, the filters of the network are trained to fuse information from an\nensemble of linear subspaces in the RKHS. Furthermore, we introduce the kernel\ndropout regularization to enable improved training convergence. Finally, we\nextend this framework to the multiple kernel case, by coupling a global fusion\nlayer with pre-trained deep kernel machines for each of the constituent\nkernels. Using case studies with limited training data, and lack of explicit\nfeature sources, we demonstrate the effectiveness of our framework over\nconventional model inferencing techniques.\n", "versions": [{"version": "v1", "created": "Wed, 15 Nov 2017 01:30:58 GMT"}], "update_date": "2017-11-16", "authors_parsed": [["Song", "Huan", ""], ["Thiagarajan", "Jayaraman J.", ""], ["Sattigeri", "Prasanna", ""], ["Spanias", "Andreas", ""]]}, {"id": "1711.05376", "submitter": "Soheil Kolouri", "authors": "Soheil Kolouri, Gustavo K. Rohde, Heiko Hoffmann", "title": "Sliced Wasserstein Distance for Learning Gaussian Mixture Models", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Gaussian mixture models (GMM) are powerful parametric tools with many\napplications in machine learning and computer vision. Expectation maximization\n(EM) is the most popular algorithm for estimating the GMM parameters. However,\nEM guarantees only convergence to a stationary point of the log-likelihood\nfunction, which could be arbitrarily worse than the optimal solution. Inspired\nby the relationship between the negative log-likelihood function and the\nKullback-Leibler (KL) divergence, we propose an alternative formulation for\nestimating the GMM parameters using the sliced Wasserstein distance, which\ngives rise to a new algorithm. Specifically, we propose minimizing the\nsliced-Wasserstein distance between the mixture model and the data distribution\nwith respect to the GMM parameters. In contrast to the KL-divergence, the\nenergy landscape for the sliced-Wasserstein distance is more well-behaved and\ntherefore more suitable for a stochastic gradient descent scheme to obtain the\noptimal GMM parameters. We show that our formulation results in parameter\nestimates that are more robust to random initializations and demonstrate that\nit can estimate high-dimensional data distributions more faithfully than the EM\nalgorithm.\n", "versions": [{"version": "v1", "created": "Wed, 15 Nov 2017 01:33:01 GMT"}, {"version": "v2", "created": "Thu, 16 Nov 2017 02:05:11 GMT"}], "update_date": "2017-11-17", "authors_parsed": [["Kolouri", "Soheil", ""], ["Rohde", "Gustavo K.", ""], ["Hoffmann", "Heiko", ""]]}, {"id": "1711.05391", "submitter": "Tianpei Xie", "authors": "Tianpei Xie, Sijia Liu, Alfred O. Hero III", "title": "Semiblind subgraph reconstruction in Gaussian graphical models", "comments": "7 pages; 5 figures; 2017 5th IEEE Global Conference on Signal and\n  Information Processing", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Consider a social network where only a few nodes (agents) have meaningful\ninteractions in the sense that the conditional dependency graph over node\nattribute variables (behaviors) is sparse. A company that can only observe the\ninteractions between its own customers will generally not be able to accurately\nestimate its customers' dependency subgraph: it is blinded to any external\ninteractions of its customers and this blindness creates false edges in its\nsubgraph. In this paper we address the semiblind scenario where the company has\naccess to a noisy summary of the complementary subgraph connecting external\nagents, e.g., provided by a consolidator. The proposed framework applies to\nother applications as well, including field estimation from a network of awake\nand sleeping sensors and privacy-constrained information sharing over social\nsubnetworks. We propose a penalized likelihood approach in the context of a\ngraph signal obeying a Gaussian graphical models (GGM). We use a convex-concave\niterative optimization algorithm to maximize the penalized likelihood.\n", "versions": [{"version": "v1", "created": "Wed, 15 Nov 2017 03:04:51 GMT"}], "update_date": "2017-11-16", "authors_parsed": [["Xie", "Tianpei", ""], ["Liu", "Sijia", ""], ["Hero", "Alfred O.", "III"]]}, {"id": "1711.05401", "submitter": "Chandrahas Dewangan", "authors": "Srinivas Ravishankar, Chandrahas, Partha Pratim Talukdar", "title": "Revisiting Simple Neural Networks for Learning Representations of\n  Knowledge Graphs", "comments": "7 pages, submitted to and accepted in Automated Knowledge Base\n  Construction (AKBC) Workshop 2017, at NIPS 2017", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We address the problem of learning vector representations for entities and\nrelations in Knowledge Graphs (KGs) for Knowledge Base Completion (KBC). This\nproblem has received significant attention in the past few years and multiple\nmethods have been proposed. Most of the existing methods in the literature use\na predefined characteristic scoring function for evaluating the correctness of\nKG triples. These scoring functions distinguish correct triples (high score)\nfrom incorrect ones (low score). However, their performance vary across\ndifferent datasets. In this work, we demonstrate that a simple neural network\nbased score function can consistently achieve near start-of-the-art performance\non multiple datasets. We also quantitatively demonstrate biases in standard\nbenchmark datasets, and highlight the need to perform evaluation spanning\nvarious datasets.\n", "versions": [{"version": "v1", "created": "Wed, 15 Nov 2017 04:12:27 GMT"}, {"version": "v2", "created": "Fri, 17 Nov 2017 10:02:28 GMT"}, {"version": "v3", "created": "Mon, 8 Jan 2018 07:20:37 GMT"}], "update_date": "2018-01-09", "authors_parsed": [["Ravishankar", "Srinivas", ""], ["Chandrahas", "", ""], ["Talukdar", "Partha Pratim", ""]]}, {"id": "1711.05407", "submitter": "Rushil Anirudh", "authors": "Rushil Anirudh, Jayaraman J. Thiagarajan, Rahul Sridhar, Peer-Timo\n  Bremer", "title": "MARGIN: Uncovering Deep Neural Networks using Graph Signal Analysis", "comments": "Technical Report", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Interpretability has emerged as a crucial aspect of building trust in machine\nlearning systems, aimed at providing insights into the working of complex\nneural networks that are otherwise opaque to a user. There are a plethora of\nexisting solutions addressing various aspects of interpretability ranging from\nidentifying prototypical samples in a dataset to explaining image predictions\nor explaining mis-classifications. While all of these diverse techniques\naddress seemingly different aspects of interpretability, we hypothesize that a\nlarge family of interepretability tasks are variants of the same central\nproblem which is identifying \\emph{relative} change in a model's prediction.\nThis paper introduces MARGIN, a simple yet general approach to address a large\nset of interpretability tasks MARGIN exploits ideas rooted in graph signal\nanalysis to determine influential nodes in a graph, which are defined as those\nnodes that maximally describe a function defined on the graph. By carefully\ndefining task-specific graphs and functions, we demonstrate that MARGIN\noutperforms existing approaches in a number of disparate interpretability\nchallenges.\n", "versions": [{"version": "v1", "created": "Wed, 15 Nov 2017 04:52:38 GMT"}, {"version": "v2", "created": "Sat, 10 Feb 2018 15:58:09 GMT"}, {"version": "v3", "created": "Tue, 4 Dec 2018 04:36:36 GMT"}, {"version": "v4", "created": "Fri, 15 Jan 2021 20:40:23 GMT"}], "update_date": "2021-01-19", "authors_parsed": [["Anirudh", "Rushil", ""], ["Thiagarajan", "Jayaraman J.", ""], ["Sridhar", "Rahul", ""], ["Bremer", "Peer-Timo", ""]]}, {"id": "1711.05411", "submitter": "Anirudh Goyal", "authors": "Anirudh Goyal, Alessandro Sordoni, Marc-Alexandre C\\^ot\\'e, Nan\n  Rosemary Ke, Yoshua Bengio", "title": "Z-Forcing: Training Stochastic Recurrent Networks", "comments": "To appear in NIPS'17", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Many efforts have been devoted to training generative latent variable models\nwith autoregressive decoders, such as recurrent neural networks (RNN).\nStochastic recurrent models have been successful in capturing the variability\nobserved in natural sequential data such as speech. We unify successful ideas\nfrom recently proposed architectures into a stochastic recurrent model: each\nstep in the sequence is associated with a latent variable that is used to\ncondition the recurrent dynamics for future steps. Training is performed with\namortized variational inference where the approximate posterior is augmented\nwith a RNN that runs backward through the sequence. In addition to maximizing\nthe variational lower bound, we ease training of the latent variables by adding\nan auxiliary cost which forces them to reconstruct the state of the backward\nrecurrent network. This provides the latent variables with a task-independent\nobjective that enhances the performance of the overall model. We found this\nstrategy to perform better than alternative approaches such as KL annealing.\nAlthough being conceptually simple, our model achieves state-of-the-art results\non standard speech benchmarks such as TIMIT and Blizzard and competitive\nperformance on sequential MNIST. Finally, we apply our model to language\nmodeling on the IMDB dataset where the auxiliary cost helps in learning\ninterpretable latent variables. Source Code:\n\\url{https://github.com/anirudh9119/zforcing_nips17}\n", "versions": [{"version": "v1", "created": "Wed, 15 Nov 2017 05:16:49 GMT"}, {"version": "v2", "created": "Thu, 16 Nov 2017 05:10:54 GMT"}], "update_date": "2017-11-17", "authors_parsed": [["Goyal", "Anirudh", ""], ["Sordoni", "Alessandro", ""], ["C\u00f4t\u00e9", "Marc-Alexandre", ""], ["Ke", "Nan Rosemary", ""], ["Bengio", "Yoshua", ""]]}, {"id": "1711.05420", "submitter": "Tomoyuki Obuchi", "authors": "Tomoyuki Obuchi and Yoshiyuki Kabashima", "title": "Accelerating Cross-Validation in Multinomial Logistic Regression with\n  $\\ell_1$-Regularization", "comments": "30 pages, 9 figures. MATLAB and python codes implementing the formula\n  derived in the manuscript are distributed in\n  https://github.com/T-Obuchi/AcceleratedCVonMLR_matlab and\n  https://github.com/T-Obuchi/AcceleratedCVonMLR_python", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cond-mat.dis-nn", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We develop an approximate formula for evaluating a cross-validation estimator\nof predictive likelihood for multinomial logistic regression regularized by an\n$\\ell_1$-norm. This allows us to avoid repeated optimizations required for\nliterally conducting cross-validation; hence, the computational time can be\nsignificantly reduced. The formula is derived through a perturbative approach\nemploying the largeness of the data size and the model dimensionality. An\nextension to the elastic net regularization is also addressed. The usefulness\nof the approximate formula is demonstrated on simulated data and the ISOLET\ndataset from the UCI machine learning repository.\n", "versions": [{"version": "v1", "created": "Wed, 15 Nov 2017 06:02:30 GMT"}, {"version": "v2", "created": "Tue, 18 Sep 2018 15:49:22 GMT"}], "update_date": "2018-09-19", "authors_parsed": [["Obuchi", "Tomoyuki", ""], ["Kabashima", "Yoshiyuki", ""]]}, {"id": "1711.05424", "submitter": "Andrea Montanari", "authors": "Gerard Ben Arous, Song Mei, Andrea Montanari, Mihai Nica", "title": "The landscape of the spiked tensor model", "comments": "40 pages, 20 pdf figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST math.PR stat.ML stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider the problem of estimating a large rank-one tensor ${\\boldsymbol\nu}^{\\otimes k}\\in({\\mathbb R}^{n})^{\\otimes k}$, $k\\ge 3$ in Gaussian noise.\nEarlier work characterized a critical signal-to-noise ratio $\\lambda_{Bayes}=\nO(1)$ above which an ideal estimator achieves strictly positive correlation\nwith the unknown vector of interest. Remarkably no polynomial-time algorithm is\nknown that achieved this goal unless $\\lambda\\ge C n^{(k-2)/4}$ and even\npowerful semidefinite programming relaxations appear to fail for $1\\ll\n\\lambda\\ll n^{(k-2)/4}$.\n  In order to elucidate this behavior, we consider the maximum likelihood\nestimator, which requires maximizing a degree-$k$ homogeneous polynomial over\nthe unit sphere in $n$ dimensions. We compute the expected number of critical\npoints and local maxima of this objective function and show that it is\nexponential in the dimensions $n$, and give exact formulas for the exponential\ngrowth rate. We show that (for $\\lambda$ larger than a constant) critical\npoints are either very close to the unknown vector ${\\boldsymbol u}$, or are\nconfined in a band of width $\\Theta(\\lambda^{-1/(k-1)})$ around the maximum\ncircle that is orthogonal to ${\\boldsymbol u}$. For local maxima, this band\nshrinks to be of size $\\Theta(\\lambda^{-1/(k-2)})$. These `uninformative' local\nmaxima are likely to cause the failure of optimization algorithms.\n", "versions": [{"version": "v1", "created": "Wed, 15 Nov 2017 06:23:43 GMT"}, {"version": "v2", "created": "Thu, 25 Jan 2018 06:34:40 GMT"}], "update_date": "2018-01-26", "authors_parsed": [["Arous", "Gerard Ben", ""], ["Mei", "Song", ""], ["Montanari", "Andrea", ""], ["Nica", "Mihai", ""]]}, {"id": "1711.05448", "submitter": "Shankar Kumar", "authors": "Shankar Kumar, Michael Nirschl, Daniel Holtmann-Rice, Hank Liao,\n  Ananda Theertha Suresh, Felix Yu", "title": "Lattice Rescoring Strategies for Long Short Term Memory Language Models\n  in Speech Recognition", "comments": "Accepted at ASRU 2017", "journal-ref": "Proceedings of ASRU 2017", "doi": null, "report-no": null, "categories": "stat.ML cs.CL cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recurrent neural network (RNN) language models (LMs) and Long Short Term\nMemory (LSTM) LMs, a variant of RNN LMs, have been shown to outperform\ntraditional N-gram LMs on speech recognition tasks. However, these models are\ncomputationally more expensive than N-gram LMs for decoding, and thus,\nchallenging to integrate into speech recognizers. Recent research has proposed\nthe use of lattice-rescoring algorithms using RNNLMs and LSTMLMs as an\nefficient strategy to integrate these models into a speech recognition system.\nIn this paper, we evaluate existing lattice rescoring algorithms along with new\nvariants on a YouTube speech recognition task. Lattice rescoring using LSTMLMs\nreduces the word error rate (WER) for this task by 8\\% relative to the WER\nobtained using an N-gram LM.\n", "versions": [{"version": "v1", "created": "Wed, 15 Nov 2017 08:30:56 GMT"}], "update_date": "2017-11-16", "authors_parsed": [["Kumar", "Shankar", ""], ["Nirschl", "Michael", ""], ["Holtmann-Rice", "Daniel", ""], ["Liao", "Hank", ""], ["Suresh", "Ananda Theertha", ""], ["Yu", "Felix", ""]]}, {"id": "1711.05477", "submitter": "Brendon Colbert", "authors": "Brendon K. Colbert and Matthew M. Peet", "title": "A Convex Parametrization of a New Class of Universal Kernel Functions", "comments": "29 pages, 7 figures", "journal-ref": "Journal of Machine Learning Research 21.45 (2020): 1-29", "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The accuracy and complexity of kernel learning algorithms is determined by\nthe set of kernels over which it is able to optimize. An ideal set of kernels\nshould: admit a linear parameterization (tractability); be dense in the set of\nall kernels (accuracy); and every member should be universal so that the\nhypothesis space is infinite-dimensional (scalability). Currently, there is no\nclass of kernel that meets all three criteria - e.g. Gaussians are not\ntractable or accurate; polynomials are not scalable. We propose a new class\nthat meet all three criteria - the Tessellated Kernel (TK) class. Specifically,\nthe TK class: admits a linear parameterization using positive matrices; is\ndense in all kernels; and every element in the class is universal. This implies\nthat the use of TK kernels for learning the kernel can obviate the need for\nselecting candidate kernels in algorithms such as SimpleMKL and parameters such\nas the bandwidth. Numerical testing on soft margin Support Vector Machine (SVM)\nproblems show that algorithms using TK kernels outperform other kernel learning\nalgorithms and neural networks. Furthermore, our results show that when the\nratio of the number of training data to features is high, the improvement of TK\nover MKL increases significantly.\n", "versions": [{"version": "v1", "created": "Wed, 15 Nov 2017 09:44:18 GMT"}, {"version": "v2", "created": "Fri, 1 May 2020 00:36:23 GMT"}], "update_date": "2020-05-04", "authors_parsed": [["Colbert", "Brendon K.", ""], ["Peet", "Matthew M.", ""]]}, {"id": "1711.05482", "submitter": "Vivek Gupta", "authors": "Dhruv Mahajan, Vivek Gupta, S Sathiya Keerthi, Sellamanickam\n  Sundararajan, Shravan Narayanamurthy, Rahul Kidambi", "title": "Efficient Estimation of Generalization Error and Bias-Variance\n  Components of Ensembles", "comments": "12 Pages, 4 Figures, 12 Pages, Under Review in SDM 2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  For many applications, an ensemble of base classifiers is an effective\nsolution. The tuning of its parameters(number of classes, amount of data on\nwhich each classifier is to be trained on, etc.) requires G, the generalization\nerror of a given ensemble. The efficient estimation of G is the focus of this\npaper. The key idea is to approximate the variance of the class\nscores/probabilities of the base classifiers over the randomness imposed by the\ntraining subset by normal/beta distribution at each point x in the input\nfeature space. We estimate the parameters of the distribution using a small set\nof randomly chosen base classifiers and use those parameters to give efficient\nestimation schemes for G. We give empirical evidence for the quality of the\nvarious estimators. We also demonstrate their usefulness in making design\nchoices such as the number of classifiers in the ensemble and the size of a\nsubset of data used for training that is needed to achieve a certain value of\ngeneralization error. Our approach also has great potential for designing\ndistributed ensemble classifiers.\n", "versions": [{"version": "v1", "created": "Wed, 15 Nov 2017 10:03:01 GMT"}], "update_date": "2017-11-16", "authors_parsed": [["Mahajan", "Dhruv", ""], ["Gupta", "Vivek", ""], ["Keerthi", "S Sathiya", ""], ["Sundararajan", "Sellamanickam", ""], ["Narayanamurthy", "Shravan", ""], ["Kidambi", "Rahul", ""]]}, {"id": "1711.05551", "submitter": "Mathieu Lagrange", "authors": "Gr\\'egoire Lafay (1), Emmanouil Benetos (2), Mathieu Lagrange (3) ((1)\n  IRCCyN, (2) QMUL, (3) LS2N)", "title": "Sound Event Detection in Synthetic Audio: Analysis of the DCASE 2016\n  Task Results", "comments": null, "journal-ref": "IEEE Workshop on Applications of Signal Processing to Audio and\n  Acoustics (WASPAA 2017), Sep 2017, Mohonk, United States", "doi": null, "report-no": null, "categories": "eess.AS cs.SD stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  As part of the 2016 public evaluation challenge on Detection and\nClassification of Acoustic Scenes and Events (DCASE 2016), the second task\nfocused on evaluating sound event detection systems using synthetic mixtures of\noffice sounds. This task, which follows the `Event Detection - Office\nSynthetic' task of DCASE 2013, studies the behaviour of tested algorithms when\nfacing controlled levels of audio complexity with respect to background noise\nand polyphony/density, with the added benefit of a very accurate ground truth.\nThis paper presents the task formulation, evaluation metrics, submitted\nsystems, and provides a statistical analysis of the results achieved, with\nrespect to various aspects of the evaluation dataset.\n", "versions": [{"version": "v1", "created": "Wed, 15 Nov 2017 12:59:13 GMT"}], "update_date": "2017-11-16", "authors_parsed": [["Lafay", "Gr\u00e9goire", ""], ["Benetos", "Emmanouil", ""], ["Lagrange", "Mathieu", ""]]}, {"id": "1711.05560", "submitter": "Mohammad Emtiyaz Khan", "authors": "Mohammad Emtiyaz Khan, Wu Lin, Voot Tangkaratt, Zuozhu Liu, Didrik\n  Nielsen", "title": "Variational Adaptive-Newton Method for Explorative Learning", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present the Variational Adaptive Newton (VAN) method which is a black-box\noptimization method especially suitable for explorative-learning tasks such as\nactive learning and reinforcement learning. Similar to Bayesian methods, VAN\nestimates a distribution that can be used for exploration, but requires\ncomputations that are similar to continuous optimization methods. Our\ntheoretical contribution reveals that VAN is a second-order method that unifies\nexisting methods in distinct fields of continuous optimization, variational\ninference, and evolution strategies. Our experimental results show that VAN\nperforms well on a wide-variety of learning tasks. This work presents a\ngeneral-purpose explorative-learning method that has the potential to improve\nlearning in areas such as active learning and reinforcement learning.\n", "versions": [{"version": "v1", "created": "Wed, 15 Nov 2017 13:23:29 GMT"}], "update_date": "2017-11-16", "authors_parsed": [["Khan", "Mohammad Emtiyaz", ""], ["Lin", "Wu", ""], ["Tangkaratt", "Voot", ""], ["Liu", "Zuozhu", ""], ["Nielsen", "Didrik", ""]]}, {"id": "1711.05597", "submitter": "Cheng Zhang", "authors": "Cheng Zhang, Judith Butepage, Hedvig Kjellstrom, Stephan Mandt", "title": "Advances in Variational Inference", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Many modern unsupervised or semi-supervised machine learning algorithms rely\non Bayesian probabilistic models. These models are usually intractable and thus\nrequire approximate inference. Variational inference (VI) lets us approximate a\nhigh-dimensional Bayesian posterior with a simpler variational distribution by\nsolving an optimization problem. This approach has been successfully used in\nvarious models and large-scale applications. In this review, we give an\noverview of recent trends in variational inference. We first introduce standard\nmean field variational inference, then review recent advances focusing on the\nfollowing aspects: (a) scalable VI, which includes stochastic approximations,\n(b) generic VI, which extends the applicability of VI to a large class of\notherwise intractable models, such as non-conjugate models, (c) accurate VI,\nwhich includes variational models beyond the mean field approximation or with\natypical divergences, and (d) amortized VI, which implements the inference over\nlocal latent variables with inference networks. Finally, we provide a summary\nof promising future research directions.\n", "versions": [{"version": "v1", "created": "Wed, 15 Nov 2017 14:46:27 GMT"}, {"version": "v2", "created": "Wed, 1 Aug 2018 23:41:26 GMT"}, {"version": "v3", "created": "Tue, 23 Oct 2018 17:05:19 GMT"}], "update_date": "2018-10-24", "authors_parsed": [["Zhang", "Cheng", ""], ["Butepage", "Judith", ""], ["Kjellstrom", "Hedvig", ""], ["Mandt", "Stephan", ""]]}, {"id": "1711.05610", "submitter": "Vince Lyzinski", "authors": "Vince Lyzinski and Keith Levin and Carey E. Priebe", "title": "On consistent vertex nomination schemes", "comments": "32 pages, 4 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Given a vertex of interest in a network $G_1$, the vertex nomination problem\nseeks to find the corresponding vertex of interest (if it exists) in a second\nnetwork $G_2$. A vertex nomination scheme produces a list of the vertices in\n$G_2$, ranked according to how likely they are judged to be the corresponding\nvertex of interest in $G_2$. The vertex nomination problem and related\ninformation retrieval tasks have attracted much attention in the machine\nlearning literature, with numerous applications to social and biological\nnetworks. However, the current framework has often been confined to a\ncomparatively small class of network models, and the concept of statistically\nconsistent vertex nomination schemes has been only shallowly explored. In this\npaper, we extend the vertex nomination problem to a very general statistical\nmodel of graphs. Further, drawing inspiration from the long-established\nclassification framework in the pattern recognition literature, we provide\ndefinitions for the key notions of Bayes optimality and consistency in our\nextended vertex nomination framework, including a derivation of the Bayes\noptimal vertex nomination scheme. In addition, we prove that no universally\nconsistent vertex nomination schemes exist. Illustrative examples are provided\nthroughout.\n", "versions": [{"version": "v1", "created": "Wed, 15 Nov 2017 15:05:21 GMT"}, {"version": "v2", "created": "Wed, 6 Dec 2017 02:06:07 GMT"}, {"version": "v3", "created": "Tue, 29 May 2018 14:03:21 GMT"}, {"version": "v4", "created": "Mon, 10 Dec 2018 04:08:46 GMT"}], "update_date": "2018-12-11", "authors_parsed": [["Lyzinski", "Vince", ""], ["Levin", "Keith", ""], ["Priebe", "Carey E.", ""]]}, {"id": "1711.05615", "submitter": "Samir Bhatt Dr", "authors": "Jean-Francois Ton, Seth Flaxman, Dino Sejdinovic and Samir Bhatt", "title": "Spatial Mapping with Gaussian Processes and Nonstationary Fourier\n  Features", "comments": "under submission to Spatial Statistics Journal", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The use of covariance kernels is ubiquitous in the field of spatial\nstatistics. Kernels allow data to be mapped into high-dimensional feature\nspaces and can thus extend simple linear additive methods to nonlinear methods\nwith higher order interactions. However, until recently, there has been a\nstrong reliance on a limited class of stationary kernels such as the Matern or\nsquared exponential, limiting the expressiveness of these modelling approaches.\nRecent machine learning research has focused on spectral representations to\nmodel arbitrary stationary kernels and introduced more general representations\nthat include classes of nonstationary kernels. In this paper, we exploit the\nconnections between Fourier feature representations, Gaussian processes and\nneural networks to generalise previous approaches and develop a simple and\nefficient framework to learn arbitrarily complex nonstationary kernel functions\ndirectly from the data, while taking care to avoid overfitting using\nstate-of-the-art methods from deep learning. We highlight the very broad array\nof kernel classes that could be created within this framework. We apply this to\na time series dataset and a remote sensing problem involving land surface\ntemperature in Eastern Africa. We show that without increasing the\ncomputational or storage complexity, nonstationary kernels can be used to\nimprove generalisation performance and provide more interpretable results.\n", "versions": [{"version": "v1", "created": "Wed, 15 Nov 2017 15:07:11 GMT"}], "update_date": "2017-11-16", "authors_parsed": [["Ton", "Jean-Francois", ""], ["Flaxman", "Seth", ""], ["Sejdinovic", "Dino", ""], ["Bhatt", "Samir", ""]]}, {"id": "1711.05656", "submitter": "Anastasia Ushakova", "authors": "Anastasia Ushakova and Slava J. Mikhaylov", "title": "Learning to Predict with Highly Granular Temporal Data: Estimating\n  individual behavioral profiles with smart meter data", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Big spatio-temporal datasets, available through both open and administrative\ndata sources, offer significant potential for social science research. The\nmagnitude of the data allows for increased resolution and analysis at\nindividual level. While there are recent advances in forecasting techniques for\nhighly granular temporal data, little attention is given to segmenting the time\nseries and finding homogeneous patterns. In this paper, it is proposed to\nestimate behavioral profiles of individuals' activities over time using\nGaussian Process-based models. In particular, the aim is to investigate how\nindividuals or groups may be clustered according to the model parameters. Such\na Bayesian non-parametric method is then tested by looking at the\npredictability of the segments using a combination of models to fit different\nparts of the temporal profiles. Model validity is then tested on a set of\nholdout data. The dataset consists of half hourly energy consumption records\nfrom smart meters from more than 100,000 households in the UK and covers the\nperiod from 2015 to 2016. The methodological approach developed in the paper\nmay be easily applied to datasets of similar structure and granularity, for\nexample social media data, and may lead to improved accuracy in the prediction\nof social dynamics and behavior.\n", "versions": [{"version": "v1", "created": "Wed, 15 Nov 2017 16:36:14 GMT"}, {"version": "v2", "created": "Thu, 23 Nov 2017 19:09:15 GMT"}], "update_date": "2017-11-27", "authors_parsed": [["Ushakova", "Anastasia", ""], ["Mikhaylov", "Slava J.", ""]]}, {"id": "1711.05717", "submitter": "Samira Shabanian", "authors": "Samira Shabanian, Devansh Arpit, Adam Trischler, Yoshua Bengio", "title": "Variational Bi-LSTMs", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recurrent neural networks like long short-term memory (LSTM) are important\narchitectures for sequential prediction tasks. LSTMs (and RNNs in general)\nmodel sequences along the forward time direction. Bidirectional LSTMs\n(Bi-LSTMs) on the other hand model sequences along both forward and backward\ndirections and are generally known to perform better at such tasks because they\ncapture a richer representation of the data. In the training of Bi-LSTMs, the\nforward and backward paths are learned independently. We propose a variant of\nthe Bi-LSTM architecture, which we call Variational Bi-LSTM, that creates a\nchannel between the two paths (during training, but which may be omitted during\ninference); thus optimizing the two paths jointly. We arrive at this joint\nobjective for our model by minimizing a variational lower bound of the joint\nlikelihood of the data sequence. Our model acts as a regularizer and encourages\nthe two networks to inform each other in making their respective predictions\nusing distinct information. We perform ablation studies to better understand\nthe different components of our model and evaluate the method on various\nbenchmarks, showing state-of-the-art performance.\n", "versions": [{"version": "v1", "created": "Wed, 15 Nov 2017 18:30:05 GMT"}], "update_date": "2017-11-16", "authors_parsed": [["Shabanian", "Samira", ""], ["Arpit", "Devansh", ""], ["Trischler", "Adam", ""], ["Bengio", "Yoshua", ""]]}, {"id": "1711.05726", "submitter": "Aditya Modi", "authors": "Aditya Modi, Nan Jiang, Satinder Singh, Ambuj Tewari", "title": "Markov Decision Processes with Continuous Side Information", "comments": null, "journal-ref": "PMLR Volume 83: Algorithmic Learning Theory, 7-9 April 2018", "doi": null, "report-no": null, "categories": "stat.ML cs.AI cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider a reinforcement learning (RL) setting in which the agent\ninteracts with a sequence of episodic MDPs. At the start of each episode the\nagent has access to some side-information or context that determines the\ndynamics of the MDP for that episode. Our setting is motivated by applications\nin healthcare where baseline measurements of a patient at the start of a\ntreatment episode form the context that may provide information about how the\npatient might respond to treatment decisions. We propose algorithms for\nlearning in such Contextual Markov Decision Processes (CMDPs) under an\nassumption that the unobserved MDP parameters vary smoothly with the observed\ncontext. We also give lower and upper PAC bounds under the smoothness\nassumption. Because our lower bound has an exponential dependence on the\ndimension, we consider a tractable linear setting where the context is used to\ncreate linear combinations of a finite set of MDPs. For the linear setting, we\ngive a PAC learning algorithm based on KWIK learning techniques.\n", "versions": [{"version": "v1", "created": "Wed, 15 Nov 2017 18:49:16 GMT"}], "update_date": "2019-10-24", "authors_parsed": [["Modi", "Aditya", ""], ["Jiang", "Nan", ""], ["Singh", "Satinder", ""], ["Tewari", "Ambuj", ""]]}, {"id": "1711.05762", "submitter": "Yi Zhou", "authors": "Guanghui Lan and Yi Zhou", "title": "Random gradient extrapolation for distributed and stochastic\n  optimization", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.OC cs.CC cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we consider a class of finite-sum convex optimization problems\ndefined over a distributed multiagent network with $m$ agents connected to a\ncentral server. In particular, the objective function consists of the average\nof $m$ ($\\ge 1$) smooth components associated with each network agent together\nwith a strongly convex term. Our major contribution is to develop a new\nrandomized incremental gradient algorithm, namely random gradient extrapolation\nmethod (RGEM), which does not require any exact gradient evaluation even for\nthe initial point, but can achieve the optimal ${\\cal O}(\\log(1/\\epsilon))$\ncomplexity bound in terms of the total number of gradient evaluations of\ncomponent functions to solve the finite-sum problems. Furthermore, we\ndemonstrate that for stochastic finite-sum optimization problems, RGEM\nmaintains the optimal ${\\cal O}(1/\\epsilon)$ complexity (up to a certain\nlogarithmic factor) in terms of the number of stochastic gradient computations,\nbut attains an ${\\cal O}(\\log(1/\\epsilon))$ complexity in terms of\ncommunication rounds (each round involves only one agent). It is worth noting\nthat the former bound is independent of the number of agents $m$, while the\nlatter one only linearly depends on $m$ or even $\\sqrt m$ for ill-conditioned\nproblems. To the best of our knowledge, this is the first time that these\ncomplexity bounds have been obtained for distributed and stochastic\noptimization problems. Moreover, our algorithms were developed based on a novel\ndual perspective of Nesterov's accelerated gradient method.\n", "versions": [{"version": "v1", "created": "Wed, 15 Nov 2017 19:18:59 GMT"}], "update_date": "2017-11-17", "authors_parsed": [["Lan", "Guanghui", ""], ["Zhou", "Yi", ""]]}, {"id": "1711.05772", "submitter": "Jesse Engel", "authors": "Jesse Engel, Matthew Hoffman, Adam Roberts", "title": "Latent Constraints: Learning to Generate Conditionally from\n  Unconditional Generative Models", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.NE stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Deep generative neural networks have proven effective at both conditional and\nunconditional modeling of complex data distributions. Conditional generation\nenables interactive control, but creating new controls often requires expensive\nretraining. In this paper, we develop a method to condition generation without\nretraining the model. By post-hoc learning latent constraints, value functions\nthat identify regions in latent space that generate outputs with desired\nattributes, we can conditionally sample from these regions with gradient-based\noptimization or amortized actor functions. Combining attribute constraints with\na universal \"realism\" constraint, which enforces similarity to the data\ndistribution, we generate realistic conditional images from an unconditional\nvariational autoencoder. Further, using gradient-based optimization, we\ndemonstrate identity-preserving transformations that make the minimal\nadjustment in latent space to modify the attributes of an image. Finally, with\ndiscrete sequences of musical notes, we demonstrate zero-shot conditional\ngeneration, learning latent constraints in the absence of labeled data or a\ndifferentiable reward function. Code with dedicated cloud instance has been\nmade publicly available (https://goo.gl/STGMGx).\n", "versions": [{"version": "v1", "created": "Wed, 15 Nov 2017 19:45:10 GMT"}, {"version": "v2", "created": "Thu, 21 Dec 2017 23:50:53 GMT"}], "update_date": "2017-12-25", "authors_parsed": [["Engel", "Jesse", ""], ["Hoffman", "Matthew", ""], ["Roberts", "Adam", ""]]}, {"id": "1711.05792", "submitter": "Yukun Chen", "authors": "Yukun Chen, Jianbo Ye, and Jia Li", "title": "Aggregated Wasserstein Metric and State Registration for Hidden Markov\n  Models", "comments": "Our manuscript is based on our conference paper [arXiv:1608.01747]\n  published in 14th European Conference on Computer Vision (ECCV 2016,\n  spotlight). It has been significantly extended and is now in journal\n  submission", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a framework, named Aggregated Wasserstein, for computing a\ndissimilarity measure or distance between two Hidden Markov Models with state\nconditional distributions being Gaussian. For such HMMs, the marginal\ndistribution at any time position follows a Gaussian mixture distribution, a\nfact exploited to softly match, aka register, the states in two HMMs. We refer\nto such HMMs as Gaussian mixture model-HMM (GMM-HMM). The registration of\nstates is inspired by the intrinsic relationship of optimal transport and the\nWasserstein metric between distributions. Specifically, the components of the\nmarginal GMMs are matched by solving an optimal transport problem where the\ncost between components is the Wasserstein metric for Gaussian distributions.\nThe solution of the optimization problem is a fast approximation to the\nWasserstein metric between two GMMs. The new Aggregated Wasserstein distance is\na semi-metric and can be computed without generating Monte Carlo samples. It is\ninvariant to relabeling or permutation of states. The distance is defined\nmeaningfully even for two HMMs that are estimated from data of different\ndimensionality, a situation that can arise due to missing variables. This\ndistance quantifies the dissimilarity of GMM-HMMs by measuring both the\ndifference between the two marginal GMMs and that between the two transition\nmatrices. Our new distance is tested on tasks of retrieval, classification, and\nt-SNE visualization of time series. Experiments on both synthetic and real data\nhave demonstrated its advantages in terms of accuracy as well as efficiency in\ncomparison with existing distances based on the Kullback-Leibler divergence.\n", "versions": [{"version": "v1", "created": "Sun, 12 Nov 2017 22:43:22 GMT"}, {"version": "v2", "created": "Sun, 19 Nov 2017 20:19:50 GMT"}], "update_date": "2017-11-21", "authors_parsed": [["Chen", "Yukun", ""], ["Ye", "Jianbo", ""], ["Li", "Jia", ""]]}, {"id": "1711.05809", "submitter": "Huaiyang Zhong", "authors": "Huaiyang Zhong, Xiaocheng Li, David Lobell, Stefano Ermon and Margaret\n  L. Brandeau", "title": "Hierarchical Modeling of Seed Variety Yields and Decision Making for\n  Future Planting Plans", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Eradicating hunger and malnutrition is a key development goal of the 21st\ncentury. We address the problem of optimally identifying seed varieties to\nreliably increase crop yield within a risk-sensitive decision-making framework.\nSpecifically, we introduce a novel hierarchical machine learning mechanism for\npredicting crop yield (the yield of different seed varieties of the same crop).\nWe integrate this prediction mechanism with a weather forecasting model, and\npropose three different approaches for decision making under uncertainty to\nselect seed varieties for planting so as to balance yield maximization and\nrisk.We apply our model to the problem of soybean variety selection given in\nthe 2016 Syngenta Crop Challenge. Our prediction model achieves a median\nabsolute error of 3.74 bushels per acre and thus provides good estimates for\ninput into the decision models.Our decision models identify the selection of\nsoybean varieties that appropriately balance yield and risk as a function of\nthe farmer's risk aversion level. More generally, our models support farmers in\ndecision making about which seed varieties to plant.\n", "versions": [{"version": "v1", "created": "Wed, 15 Nov 2017 21:12:30 GMT"}], "update_date": "2017-11-17", "authors_parsed": [["Zhong", "Huaiyang", ""], ["Li", "Xiaocheng", ""], ["Lobell", "David", ""], ["Ermon", "Stefano", ""], ["Brandeau", "Margaret L.", ""]]}, {"id": "1711.05825", "submitter": "Richard Everitt", "authors": "Richard G. Everitt", "title": "Bootstrapped synthetic likelihood", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.CO cs.AI physics.data-an stat.ME stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Approximate Bayesian computation (ABC) and synthetic likelihood (SL)\ntechniques have enabled the use of Bayesian inference for models that may be\nsimulated, but for which the likelihood cannot be evaluated pointwise at values\nof an unknown parameter $\\theta$. The main idea in ABC and SL is to, for\ndifferent values of $\\theta$ (usually chosen using a Monte Carlo algorithm),\nbuild estimates of the likelihood based on simulations from the model\nconditional on $\\theta$. The quality of these estimates determines the\nefficiency of an ABC/SL algorithm. In standard ABC/SL, the only means to\nimprove an estimated likelihood at $\\theta$ is to simulate more times from the\nmodel conditional on $\\theta$, which is infeasible in cases where the simulator\nis computationally expensive. In this paper we describe how to use\nbootstrapping as a means for improving SL estimates whilst using fewer\nsimulations from the model, and also investigate its use in ABC. Further, we\ninvestigate the use of the bag of little bootstraps as a means for applying\nthis approach to large datasets, yielding Monte Carlo algorithms that\naccurately approximate posterior distributions whilst only simulating\nsubsamples of the full data. Examples of the approach applied to i.i.d.,\ntemporal and spatial data are given.\n", "versions": [{"version": "v1", "created": "Wed, 15 Nov 2017 22:13:48 GMT"}, {"version": "v2", "created": "Wed, 17 Jan 2018 23:16:04 GMT"}], "update_date": "2018-01-19", "authors_parsed": [["Everitt", "Richard G.", ""]]}, {"id": "1711.05828", "submitter": "Rhicheek Patra", "authors": "Rhicheek Patra, Egor Samosvat, Michael Roizner, Andrei Mishchenko", "title": "BoostJet: Towards Combining Statistical Aggregates with Neural\n  Embeddings for Recommendations", "comments": "9 pages, 9 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recommenders have become widely popular in recent years because of their\nbroader applicability in many e-commerce applications. These applications rely\non recommenders for generating advertisements for various offers or providing\ncontent recommendations. However, the quality of the generated recommendations\ndepends on user features (like demography, temporality), offer features (like\npopularity, price), and user-offer features (like implicit or explicit\nfeedback). Current state-of-the-art recommenders do not explore such diverse\nfeatures concurrently while generating the recommendations.\n  In this paper, we first introduce the notion of Trackers which enables us to\ncapture the above-mentioned features and thus incorporate users' online\nbehaviour through statistical aggregates of different features (demography,\ntemporality, popularity, price). We also show how to capture offer-to-offer\nrelations, based on their consumption sequence, leveraging neural embeddings\nfor offers in our Offer2Vec algorithm. We then introduce BoostJet, a novel\nrecommender which integrates the Trackers along with the neural embeddings\nusing MatrixNet, an efficient distributed implementation of gradient boosted\ndecision tree, to improve the recommendation quality significantly. We provide\nan in-depth evaluation of BoostJet on Yandex's dataset, collecting online\nbehaviour from tens of millions of online users, to demonstrate the\npracticality of BoostJet in terms of recommendation quality as well as\nscalability.\n", "versions": [{"version": "v1", "created": "Wed, 15 Nov 2017 22:25:49 GMT"}, {"version": "v2", "created": "Thu, 7 Dec 2017 21:16:53 GMT"}], "update_date": "2017-12-11", "authors_parsed": [["Patra", "Rhicheek", ""], ["Samosvat", "Egor", ""], ["Roizner", "Michael", ""], ["Mishchenko", "Andrei", ""]]}, {"id": "1711.05869", "submitter": "Franz J. Kir\\'aly", "authors": "Samuel Burkart and Franz J Kir\\'aly", "title": "Predictive Independence Testing, Predictive Conditional Independence\n  Testing, and Predictive Graphical Modelling", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG math.ST stat.ME stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Testing (conditional) independence of multivariate random variables is a task\ncentral to statistical inference and modelling in general - though\nunfortunately one for which to date there does not exist a practicable\nworkflow. State-of-art workflows suffer from the need for heuristic or\nsubjective manual choices, high computational complexity, or strong parametric\nassumptions.\n  We address these problems by establishing a theoretical link between\nmultivariate/conditional independence testing, and model comparison in the\nmultivariate predictive modelling aka supervised learning task. This link\nallows advances in the extensively studied supervised learning workflow to be\ndirectly transferred to independence testing workflows - including automated\ntuning of machine learning type which addresses the need for a heuristic\nchoice, the ability to quantitatively trade-off computational demand with\naccuracy, and the modern black-box philosophy for checking and interfacing.\n  As a practical implementation of this link between the two workflows, we\npresent a python package 'pcit', which implements our novel multivariate and\nconditional independence tests, interfacing the supervised learning API of the\nscikit-learn package. Theory and package also allow for straightforward\nindependence test based learning of graphical model structure.\n  We empirically show that our proposed predictive independence test outperform\nor are on par to current practice, and the derived graphical model structure\nlearning algorithms asymptotically recover the 'true' graph. This paper, and\nthe 'pcit' package accompanying it, thus provide powerful, scalable,\ngeneralizable, and easy-to-use methods for multivariate and conditional\nindependence testing, as well as for graphical model structure learning.\n", "versions": [{"version": "v1", "created": "Thu, 16 Nov 2017 00:37:34 GMT"}, {"version": "v2", "created": "Sat, 28 Apr 2018 20:32:52 GMT"}], "update_date": "2018-05-01", "authors_parsed": [["Burkart", "Samuel", ""], ["Kir\u00e1ly", "Franz J", ""]]}, {"id": "1711.05928", "submitter": "Datong-Paul Zhou", "authors": "Datong P. Zhou, Claire J. Tomlin", "title": "Budget-Constrained Multi-Armed Bandits with Multiple Plays", "comments": "20 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study the multi-armed bandit problem with multiple plays and a budget\nconstraint for both the stochastic and the adversarial setting. At each round,\nexactly $K$ out of $N$ possible arms have to be played (with $1\\leq K \\leq N$).\nIn addition to observing the individual rewards for each arm played, the player\nalso learns a vector of costs which has to be covered with an a-priori defined\nbudget $B$. The game ends when the sum of current costs associated with the\nplayed arms exceeds the remaining budget.\n  Firstly, we analyze this setting for the stochastic case, for which we assume\neach arm to have an underlying cost and reward distribution with support\n$[c_{\\min}, 1]$ and $[0, 1]$, respectively. We derive an Upper Confidence Bound\n(UCB) algorithm which achieves $O(NK^4 \\log B)$ regret.\n  Secondly, for the adversarial case in which the entire sequence of rewards\nand costs is fixed in advance, we derive an upper bound on the regret of order\n$O(\\sqrt{NB\\log(N/K)})$ utilizing an extension of the well-known\n$\\texttt{Exp3}$ algorithm. We also provide upper bounds that hold with high\nprobability and a lower bound of order $\\Omega((1 - K/N)^2 \\sqrt{NB/K})$.\n", "versions": [{"version": "v1", "created": "Thu, 16 Nov 2017 05:07:34 GMT"}], "update_date": "2017-11-17", "authors_parsed": [["Zhou", "Datong P.", ""], ["Tomlin", "Claire J.", ""]]}, {"id": "1711.05957", "submitter": "Qianqian Xu", "authors": "Qianqian Xu, Jiechao Xiong, Xi Chen, Qingming Huang, Yuan Yao", "title": "HodgeRank with Information Maximization for Crowdsourced Pairwise\n  Ranking Aggregation", "comments": "Accepted by AAAI2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recently, crowdsourcing has emerged as an effective paradigm for\nhuman-powered large scale problem solving in various domains. However, task\nrequester usually has a limited amount of budget, thus it is desirable to have\na policy to wisely allocate the budget to achieve better quality. In this\npaper, we study the principle of information maximization for active sampling\nstrategies in the framework of HodgeRank, an approach based on Hodge\nDecomposition of pairwise ranking data with multiple workers. The principle\nexhibits two scenarios of active sampling: Fisher information maximization that\nleads to unsupervised sampling based on a sequential maximization of graph\nalgebraic connectivity without considering labels; and Bayesian information\nmaximization that selects samples with the largest information gain from prior\nto posterior, which gives a supervised sampling involving the labels collected.\nExperiments show that the proposed methods boost the sampling efficiency as\ncompared to traditional sampling schemes and are thus valuable to practical\ncrowdsourcing experiments.\n", "versions": [{"version": "v1", "created": "Thu, 16 Nov 2017 06:58:23 GMT"}], "update_date": "2017-11-17", "authors_parsed": [["Xu", "Qianqian", ""], ["Xiong", "Jiechao", ""], ["Chen", "Xi", ""], ["Huang", "Qingming", ""], ["Yao", "Yuan", ""]]}, {"id": "1711.06047", "submitter": "Makoto Yamada", "authors": "Tanmoy Mukherjee, Makoto Yamada, Timothy M. Hospedales", "title": "Deep Matching Autoencoders", "comments": "10 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Increasingly many real world tasks involve data in multiple modalities or\nviews. This has motivated the development of many effective algorithms for\nlearning a common latent space to relate multiple domains. However, most\nexisting cross-view learning algorithms assume access to paired data for\ntraining. Their applicability is thus limited as the paired data assumption is\noften violated in practice: many tasks have only a small subset of data\navailable with pairing annotation, or even no paired data at all. In this paper\nwe introduce Deep Matching Autoencoders (DMAE), which learn a common latent\nspace and pairing from unpaired multi-modal data. Specifically we formulate\nthis as a cross-domain representation learning and object matching problem. We\nsimultaneously optimise parameters of representation learning auto-encoders and\nthe pairing of unpaired multi-modal data. This framework elegantly spans the\nfull regime from fully supervised, semi-supervised, and unsupervised (no paired\ndata) multi-modal learning. We show promising results in image captioning, and\non a new task that is uniquely enabled by our methodology: unsupervised\nclassifier learning.\n", "versions": [{"version": "v1", "created": "Thu, 16 Nov 2017 11:50:41 GMT"}], "update_date": "2017-11-17", "authors_parsed": [["Mukherjee", "Tanmoy", ""], ["Yamada", "Makoto", ""], ["Hospedales", "Timothy M.", ""]]}, {"id": "1711.06064", "submitter": "Kian Hsiang Low", "authors": "Ruofei Ouyang, Kian Hsiang Low", "title": "Gaussian Process Decentralized Data Fusion Meets Transfer Learning in\n  Large-Scale Distributed Cooperative Perception", "comments": "32nd AAAI Conference on Artificial Intelligence (AAAI 2018), Extended\n  version with proofs, 14 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG cs.MA cs.RO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper presents novel Gaussian process decentralized data fusion\nalgorithms exploiting the notion of agent-centric support sets for distributed\ncooperative perception of large-scale environmental phenomena. To overcome the\nlimitations of scale in existing works, our proposed algorithms allow every\nmobile sensing agent to choose a different support set and dynamically switch\nto another during execution for encapsulating its own data into a local summary\nthat, perhaps surprisingly, can still be assimilated with the other agents'\nlocal summaries (i.e., based on their current choices of support sets) into a\nglobally consistent summary to be used for predicting the phenomenon. To\nachieve this, we propose a novel transfer learning mechanism for a team of\nagents capable of sharing and transferring information encapsulated in a\nsummary based on a support set to that utilizing a different support set with\nsome loss that can be theoretically bounded and analyzed. To alleviate the\nissue of information loss accumulating over multiple instances of transfer\nlearning, we propose a new information sharing mechanism to be incorporated\ninto our algorithms in order to achieve memory-efficient lazy transfer\nlearning. Empirical evaluation on real-world datasets show that our algorithms\noutperform the state-of-the-art methods.\n", "versions": [{"version": "v1", "created": "Thu, 16 Nov 2017 12:41:33 GMT"}], "update_date": "2017-11-17", "authors_parsed": [["Ouyang", "Ruofei", ""], ["Low", "Kian Hsiang", ""]]}, {"id": "1711.06100", "submitter": "Rhicheek Patra", "authors": "Rachid Guerraoui, Erwan Le Merrer, Rhicheek Patra, Jean-Ronan\n  Vigouroux", "title": "Sequences, Items And Latent Links: Recommendation With Consumed Item\n  Packs", "comments": "12 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR cs.SI stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recommenders personalize the web content by typically using collaborative\nfiltering to relate users (or items) based on explicit feedback, e.g., ratings.\nThe difficulty of collecting this feedback has recently motivated to consider\nimplicit feedback (e.g., item consumption along with the corresponding time).\n  In this paper, we introduce the notion of consumed item pack (CIP) which\nenables to link users (or items) based on their implicit analogous consumption\nbehavior. Our proposal is generic, and we show that it captures three novel\nimplicit recommenders: a user-based (CIP-U), an item-based (CIP-I), and a word\nembedding-based (DEEPCIP), as well as a state-of-the-art technique using\nimplicit feedback (FISM). We show that our recommenders handle incremental\nupdates incorporating freshly consumed items. We demonstrate that all three\nrecommenders provide a recommendation quality that is competitive with\nstate-of-the-art ones, including one incorporating both explicit and implicit\nfeedback.\n", "versions": [{"version": "v1", "created": "Thu, 16 Nov 2017 14:11:53 GMT"}, {"version": "v2", "created": "Thu, 7 Dec 2017 21:16:41 GMT"}], "update_date": "2017-12-11", "authors_parsed": [["Guerraoui", "Rachid", ""], ["Merrer", "Erwan Le", ""], ["Patra", "Rhicheek", ""], ["Vigouroux", "Jean-Ronan", ""]]}, {"id": "1711.06104", "submitter": "Marco Ancona", "authors": "Marco Ancona, Enea Ceolini, Cengiz \\\"Oztireli and Markus Gross", "title": "Towards better understanding of gradient-based attribution methods for\n  Deep Neural Networks", "comments": "ICLR 2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Understanding the flow of information in Deep Neural Networks (DNNs) is a\nchallenging problem that has gain increasing attention over the last few years.\nWhile several methods have been proposed to explain network predictions, there\nhave been only a few attempts to compare them from a theoretical perspective.\nWhat is more, no exhaustive empirical comparison has been performed in the\npast. In this work, we analyze four gradient-based attribution methods and\nformally prove conditions of equivalence and approximation between them. By\nreformulating two of these methods, we construct a unified framework which\nenables a direct comparison, as well as an easier implementation. Finally, we\npropose a novel evaluation metric, called Sensitivity-n and test the\ngradient-based attribution methods alongside with a simple perturbation-based\nattribution method on several datasets in the domains of image and text\nclassification, using various network architectures.\n", "versions": [{"version": "v1", "created": "Thu, 16 Nov 2017 14:19:29 GMT"}, {"version": "v2", "created": "Tue, 6 Feb 2018 09:53:41 GMT"}, {"version": "v3", "created": "Mon, 12 Feb 2018 12:14:04 GMT"}, {"version": "v4", "created": "Wed, 7 Mar 2018 10:49:28 GMT"}], "update_date": "2018-03-08", "authors_parsed": [["Ancona", "Marco", ""], ["Ceolini", "Enea", ""], ["\u00d6ztireli", "Cengiz", ""], ["Gross", "Markus", ""]]}, {"id": "1711.06114", "submitter": "Werner Zellinger", "authors": "Werner Zellinger, Bernhard A. Moser, Thomas Grubinger, Edwin Lughofer,\n  Thomas Natschl\\\"ager, and Susanne Saminger-Platz", "title": "Robust Unsupervised Domain Adaptation for Neural Networks via Moment\n  Alignment", "comments": "Preliminary version of this work appeared in ICLR", "journal-ref": "Information Sciences 483: 174-191, May 2019", "doi": "10.1016/j.ins.2019.01.025", "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A novel approach for unsupervised domain adaptation for neural networks is\nproposed. It relies on metric-based regularization of the learning process. The\nmetric-based regularization aims at domain-invariant latent feature\nrepresentations by means of maximizing the similarity between domain-specific\nactivation distributions. The proposed metric results from modifying an\nintegral probability metric such that it becomes less translation-sensitive on\na polynomial function space. The metric has an intuitive interpretation in the\ndual space as the sum of differences of higher order central moments of the\ncorresponding activation distributions. Under appropriate assumptions on the\ninput distributions, error minimization is proven for the continuous case. As\ndemonstrated by an analysis of standard benchmark experiments for sentiment\nanalysis, object recognition and digit recognition, the outlined approach is\nrobust regarding parameter changes and achieves higher classification\naccuracies than comparable approaches. The source code is available at\nhttps://github.com/wzell/mann.\n", "versions": [{"version": "v1", "created": "Thu, 16 Nov 2017 14:45:05 GMT"}, {"version": "v2", "created": "Mon, 28 May 2018 16:40:41 GMT"}, {"version": "v3", "created": "Mon, 21 Jan 2019 11:40:16 GMT"}, {"version": "v4", "created": "Tue, 13 Aug 2019 06:40:25 GMT"}], "update_date": "2019-08-14", "authors_parsed": [["Zellinger", "Werner", ""], ["Moser", "Bernhard A.", ""], ["Grubinger", "Thomas", ""], ["Lughofer", "Edwin", ""], ["Natschl\u00e4ger", "Thomas", ""], ["Saminger-Platz", "Susanne", ""]]}, {"id": "1711.06178", "submitter": "Michael Hughes", "authors": "Mike Wu, Michael C. Hughes, Sonali Parbhoo, Maurizio Zazzi, Volker\n  Roth, and Finale Doshi-Velez", "title": "Beyond Sparsity: Tree Regularization of Deep Models for Interpretability", "comments": "To appear in AAAI 2018. Contains 9-page main paper and appendix with\n  supplementary material", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The lack of interpretability remains a key barrier to the adoption of deep\nmodels in many applications. In this work, we explicitly regularize deep models\nso human users might step through the process behind their predictions in\nlittle time. Specifically, we train deep time-series models so their\nclass-probability predictions have high accuracy while being closely modeled by\ndecision trees with few nodes. Using intuitive toy examples as well as medical\ntasks for treating sepsis and HIV, we demonstrate that this new tree\nregularization yields models that are easier for humans to simulate than\nsimpler L1 or L2 penalties without sacrificing predictive power.\n", "versions": [{"version": "v1", "created": "Thu, 16 Nov 2017 16:35:24 GMT"}], "update_date": "2017-11-17", "authors_parsed": [["Wu", "Mike", ""], ["Hughes", "Michael C.", ""], ["Parbhoo", "Sonali", ""], ["Zazzi", "Maurizio", ""], ["Roth", "Volker", ""], ["Doshi-Velez", "Finale", ""]]}, {"id": "1711.06195", "submitter": "Payel Das", "authors": "Tejas Dharamsi, Payel Das, Tejaswini Pedapati, Gregory Bramble, Vinod\n  Muthusamy, Horst Samulowitz, Kush R. Varshney, Yuvaraj Rajamanickam, John\n  Thomas, Justin Dauwels", "title": "Neurology-as-a-Service for the Developing World", "comments": "Presented at NIPS 2017 Workshop on Machine Learning for the\n  Developing World", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Electroencephalography (EEG) is an extensively-used and well-studied\ntechnique in the field of medical diagnostics and treatment for brain\ndisorders, including epilepsy, migraines, and tumors. The analysis and\ninterpretation of EEGs require physicians to have specialized training, which\nis not common even among most doctors in the developed world, let alone the\ndeveloping world where physician shortages plague society. This problem can be\naddressed by teleEEG that uses remote EEG analysis by experts or by local\ncomputer processing of EEGs. However, both of these options are prohibitively\nexpensive and the second option requires abundant computing resources and\ninfrastructure, which is another concern in developing countries where there\nare resource constraints on capital and computing infrastructure. In this work,\nwe present a cloud-based deep neural network approach to provide decision\nsupport for non-specialist physicians in EEG analysis and interpretation. Named\n`neurology-as-a-service,' the approach requires almost no manual intervention\nin feature engineering and in the selection of an optimal architecture and\nhyperparameters of the neural network. In this study, we deploy a pipeline that\nincludes moving EEG data to the cloud and getting optimal models for various\nclassification tasks. Our initial prototype has been tested only in developed\nworld environments to-date, but our intention is to test it in developing world\nenvironments in future work. We demonstrate the performance of our proposed\napproach using the BCI2000 EEG MMI dataset, on which our service attains 63.4%\naccuracy for the task of classifying real vs. imaginary activity performed by\nthe subject, which is significantly higher than what is obtained with a shallow\napproach such as support vector machines.\n", "versions": [{"version": "v1", "created": "Thu, 16 Nov 2017 16:58:53 GMT"}, {"version": "v2", "created": "Wed, 22 Nov 2017 01:11:52 GMT"}], "update_date": "2017-11-23", "authors_parsed": [["Dharamsi", "Tejas", ""], ["Das", "Payel", ""], ["Pedapati", "Tejaswini", ""], ["Bramble", "Gregory", ""], ["Muthusamy", "Vinod", ""], ["Samulowitz", "Horst", ""], ["Varshney", "Kush R.", ""], ["Rajamanickam", "Yuvaraj", ""], ["Thomas", "John", ""], ["Dauwels", "Justin", ""]]}, {"id": "1711.06221", "submitter": "Aditya Balu", "authors": "Aditya Balu, Thanh V. Nguyen, Apurva Kokate, Chinmay Hegde and Soumik\n  Sarkar", "title": "A Forward-Backward Approach for Visualizing Information Flow in Deep\n  Networks", "comments": "Presented at NIPS 2017 Symposium on Interpretable Machine Learning", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We introduce a new, systematic framework for visualizing information flow in\ndeep networks. Specifically, given any trained deep convolutional network model\nand a given test image, our method produces a compact support in the image\ndomain that corresponds to a (high-resolution) feature that contributes to the\ngiven explanation. Our method is both computationally efficient as well as\nnumerically robust. We present several preliminary numerical results that\nsupport the benefits of our framework over existing methods.\n", "versions": [{"version": "v1", "created": "Thu, 16 Nov 2017 18:00:24 GMT"}], "update_date": "2017-11-17", "authors_parsed": [["Balu", "Aditya", ""], ["Nguyen", "Thanh V.", ""], ["Kokate", "Apurva", ""], ["Hegde", "Chinmay", ""], ["Sarkar", "Soumik", ""]]}, {"id": "1711.06323", "submitter": "Jonathan Hersh", "authors": "Boris Babenko (1), Jonathan Hersh (2), David Newhouse (3), Anusha\n  Ramakrishnan (3), and Tom Swartz (1) ((1) Orbital Insight, (2) Chapman\n  University, (3) World Bank)", "title": "Poverty Mapping Using Convolutional Neural Networks Trained on High and\n  Medium Resolution Satellite Images, With an Application in Mexico", "comments": "4 pages, 2 figures, Presented at NIPS 2017 Workshop on Machine\n  Learning for the Developing World", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.CY stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Mapping the spatial distribution of poverty in developing countries remains\nan important and costly challenge. These \"poverty maps\" are key inputs for\npoverty targeting, public goods provision, political accountability, and impact\nevaluation, that are all the more important given the geographic dispersion of\nthe remaining bottom billion severely poor individuals. In this paper we train\nConvolutional Neural Networks (CNNs) to estimate poverty directly from high and\nmedium resolution satellite images. We use both Planet and Digital Globe\nimagery with spatial resolutions of 3-5 sq. m. and 50 sq. cm. respectively,\ncovering all 2 million sq. km. of Mexico. Benchmark poverty estimates come from\nthe 2014 MCS-ENIGH combined with the 2015 Intercensus and are used to estimate\npoverty rates for 2,456 Mexican municipalities. CNNs are trained using the 896\nmunicipalities in the 2014 MCS-ENIGH. We experiment with several architectures\n(GoogleNet, VGG) and use GoogleNet as a final architecture where weights are\nfine-tuned from ImageNet. We find that 1) the best models, which incorporate\nsatellite-estimated land use as a predictor, explain approximately 57% of the\nvariation in poverty in a validation sample of 10 percent of MCS-ENIGH\nmunicipalities; 2) Across all MCS-ENIGH municipalities explanatory power\nreduces to 44% in a CNN prediction and landcover model; 3) Predicted poverty\nfrom the CNN predictions alone explains 47% of the variation in poverty in the\nvalidation sample, and 37% over all MCS-ENIGH municipalities; 4) In urban areas\nwe see slight improvements from using Digital Globe versus Planet imagery,\nwhich explain 61% and 54% of poverty variation respectively. We conclude that\nCNNs can be trained end-to-end on satellite imagery to estimate poverty,\nalthough there is much work to be done to understand how the training process\ninfluences out of sample validation.\n", "versions": [{"version": "v1", "created": "Thu, 16 Nov 2017 21:27:33 GMT"}], "update_date": "2017-11-20", "authors_parsed": [["Babenko", "Boris", ""], ["Hersh", "Jonathan", ""], ["Newhouse", "David", ""], ["Ramakrishnan", "Anusha", ""], ["Swartz", "Tom", ""]]}, {"id": "1711.06346", "submitter": "Yunpeng Li", "authors": "Yunpeng Li, Davide Zilli, Henry Chan, Ivan Kiskin, Marianne Sinka,\n  Stephen Roberts and Kathy Willis", "title": "Mosquito detection with low-cost smartphones: data acquisition for\n  malaria research", "comments": "Presented at NIPS 2017 Workshop on Machine Learning for the\n  Developing World", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.CY", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Mosquitoes are a major vector for malaria, causing hundreds of thousands of\ndeaths in the developing world each year. Not only is the prevention of\nmosquito bites of paramount importance to the reduction of malaria transmission\ncases, but understanding in more forensic detail the interplay between malaria,\nmosquito vectors, vegetation, standing water and human populations is crucial\nto the deployment of more effective interventions. Typically the presence and\ndetection of malaria-vectoring mosquitoes is only quantified by hand-operated\ninsect traps or signified by the diagnosis of malaria. If we are to gather\ntimely, large-scale data to improve this situation, we need to automate the\nprocess of mosquito detection and classification as much as possible. In this\npaper, we present a candidate mobile sensing system that acts as both a\nportable early warning device and an automatic acoustic data acquisition\npipeline to help fuel scientific inquiry and policy. The machine learning\nalgorithm that powers the mobile system achieves excellent off-line\nmulti-species detection performance while remaining computationally efficient.\nFurther, we have conducted preliminary live mosquito detection tests using\nlow-cost mobile phones and achieved promising results. The deployment of this\nsystem for field usage in Southeast Asia and Africa is planned in the near\nfuture. In order to accelerate processing of field recordings and labelling of\ncollected data, we employ a citizen science platform in conjunction with\nautomated methods, the former implemented using the Zooniverse platform,\nallowing crowdsourcing on a grand scale.\n", "versions": [{"version": "v1", "created": "Thu, 16 Nov 2017 22:58:29 GMT"}, {"version": "v2", "created": "Tue, 21 Nov 2017 22:53:34 GMT"}, {"version": "v3", "created": "Wed, 6 Dec 2017 01:11:53 GMT"}], "update_date": "2017-12-07", "authors_parsed": [["Li", "Yunpeng", ""], ["Zilli", "Davide", ""], ["Chan", "Henry", ""], ["Kiskin", "Ivan", ""], ["Sinka", "Marianne", ""], ["Roberts", "Stephen", ""], ["Willis", "Kathy", ""]]}, {"id": "1711.06350", "submitter": "Mirco Musolesi", "authors": "Gatis Mikelsons and Matthew Smith and Abhinav Mehrotra and Mirco\n  Musolesi", "title": "Towards Deep Learning Models for Psychological State Prediction using\n  Smartphone Data: Challenges and Opportunities", "comments": "6 pages, 2 figures, In Proceedings of the NIPS Workshop on Machine\n  Learning for Healthcare 2017 (ML4H 2017). Colocated with NIPS 2017", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  There is an increasing interest in exploiting mobile sensing technologies and\nmachine learning techniques for mental health monitoring and intervention.\nResearchers have effectively used contextual information, such as mobility,\ncommunication and mobile phone usage patterns for quantifying individuals' mood\nand wellbeing. In this paper, we investigate the effectiveness of neural\nnetwork models for predicting users' level of stress by using the location\ninformation collected by smartphones. We characterize the mobility patterns of\nindividuals using the GPS metrics presented in the literature and employ these\nmetrics as input to the network. We evaluate our approach on the open-source\nStudentLife dataset. Moreover, we discuss the challenges and trade-offs\ninvolved in building machine learning models for digital mental health and\nhighlight potential future work in this direction.\n", "versions": [{"version": "v1", "created": "Thu, 16 Nov 2017 23:18:03 GMT"}], "update_date": "2017-11-20", "authors_parsed": [["Mikelsons", "Gatis", ""], ["Smith", "Matthew", ""], ["Mehrotra", "Abhinav", ""], ["Musolesi", "Mirco", ""]]}, {"id": "1711.06373", "submitter": "Zhe Li", "authors": "Zhe Li, Chong Wang, Mei Han, Yuan Xue, Wei Wei, Li-Jia Li, Li Fei-Fei", "title": "Thoracic Disease Identification and Localization with Limited\n  Supervision", "comments": "Conference on Computer Vision and Pattern Recognition 2018 (CVPR\n  2018). V1: CVPR submission; V2: +supplementary; V3: CVPR camera-ready; V4:\n  correction, update reference baseline results according to their latest post;\n  V5: minor correction; V6: Identification results using NIH data splits and\n  various image models", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Accurate identification and localization of abnormalities from radiology\nimages play an integral part in clinical diagnosis and treatment planning.\nBuilding a highly accurate prediction model for these tasks usually requires a\nlarge number of images manually annotated with labels and finding sites of\nabnormalities. In reality, however, such annotated data are expensive to\nacquire, especially the ones with location annotations. We need methods that\ncan work well with only a small amount of location annotations. To address this\nchallenge, we present a unified approach that simultaneously performs disease\nidentification and localization through the same underlying model for all\nimages. We demonstrate that our approach can effectively leverage both class\ninformation as well as limited location annotation, and significantly\noutperforms the comparative reference baseline in both classification and\nlocalization tasks.\n", "versions": [{"version": "v1", "created": "Fri, 17 Nov 2017 01:52:56 GMT"}, {"version": "v2", "created": "Wed, 22 Nov 2017 03:31:10 GMT"}, {"version": "v3", "created": "Tue, 27 Mar 2018 06:11:39 GMT"}, {"version": "v4", "created": "Wed, 28 Mar 2018 19:28:30 GMT"}, {"version": "v5", "created": "Tue, 24 Apr 2018 16:17:19 GMT"}, {"version": "v6", "created": "Wed, 20 Jun 2018 23:24:24 GMT"}], "update_date": "2018-06-22", "authors_parsed": [["Li", "Zhe", ""], ["Wang", "Chong", ""], ["Han", "Mei", ""], ["Xue", "Yuan", ""], ["Wei", "Wei", ""], ["Li", "Li-Jia", ""], ["Fei-Fei", "Li", ""]]}, {"id": "1711.06402", "submitter": "Anand Avati", "authors": "Anand Avati, Kenneth Jung, Stephanie Harman, Lance Downing, Andrew Ng\n  and Nigam H. Shah", "title": "Improving Palliative Care with Deep Learning", "comments": "IEEE International Conference on Bioinformatics and Biomedicine 2017", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CY cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Improving the quality of end-of-life care for hospitalized patients is a\npriority for healthcare organizations. Studies have shown that physicians tend\nto over-estimate prognoses, which in combination with treatment inertia results\nin a mismatch between patients wishes and actual care at the end of life. We\ndescribe a method to address this problem using Deep Learning and Electronic\nHealth Record (EHR) data, which is currently being piloted, with Institutional\nReview Board approval, at an academic medical center. The EHR data of admitted\npatients are automatically evaluated by an algorithm, which brings patients who\nare likely to benefit from palliative care services to the attention of the\nPalliative Care team. The algorithm is a Deep Neural Network trained on the EHR\ndata from previous years, to predict all-cause 3-12 month mortality of patients\nas a proxy for patients that could benefit from palliative care. Our\npredictions enable the Palliative Care team to take a proactive approach in\nreaching out to such patients, rather than relying on referrals from treating\nphysicians, or conduct time consuming chart reviews of all patients. We also\npresent a novel interpretation technique which we use to provide explanations\nof the model's predictions.\n", "versions": [{"version": "v1", "created": "Fri, 17 Nov 2017 04:46:17 GMT"}], "update_date": "2017-11-20", "authors_parsed": [["Avati", "Anand", ""], ["Jung", "Kenneth", ""], ["Harman", "Stephanie", ""], ["Downing", "Lance", ""], ["Ng", "Andrew", ""], ["Shah", "Nigam H.", ""]]}, {"id": "1711.06405", "submitter": "Charles Onu", "authors": "Charles C Onu, Innocent Udeogu, Eyenimi Ndiomu, Urbain Kengni, Doina\n  Precup, Guilherme M Sant'anna, Edward Alikor, Peace Opara", "title": "Ubenwa: Cry-based Diagnosis of Birth Asphyxia", "comments": "Presented at NIPS 2017 Workshop on Machine Learning for the\n  Developing World", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.CY", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Every year, 3 million newborns die within the first month of life. Birth\nasphyxia and other breathing-related conditions are a leading cause of\nmortality during the neonatal phase. Current diagnostic methods are too\nsophisticated in terms of equipment, required expertise, and general logistics.\nConsequently, early detection of asphyxia in newborns is very difficult in many\nparts of the world, especially in resource-poor settings. We are developing a\nmachine learning system, dubbed Ubenwa, which enables diagnosis of asphyxia\nthrough automated analysis of the infant cry. Deployed via smartphone and\nwearable technology, Ubenwa will drastically reduce the time, cost and skill\nrequired to make accurate and potentially life-saving diagnoses.\n", "versions": [{"version": "v1", "created": "Fri, 17 Nov 2017 04:51:43 GMT"}], "update_date": "2017-11-20", "authors_parsed": [["Onu", "Charles C", ""], ["Udeogu", "Innocent", ""], ["Ndiomu", "Eyenimi", ""], ["Kengni", "Urbain", ""], ["Precup", "Doina", ""], ["Sant'anna", "Guilherme M", ""], ["Alikor", "Edward", ""], ["Opara", "Peace", ""]]}, {"id": "1711.06424", "submitter": "Seong Jin Cho", "authors": "Seong Jin Cho, Sunghun Kang, Chang D. Yoo", "title": "A Resizable Mini-batch Gradient Descent based on a Multi-Armed Bandit", "comments": "8 pages, 5 figures, 5 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Determining the appropriate batch size for mini-batch gradient descent is\nalways time consuming as it often relies on grid search. This paper considers a\nresizable mini-batch gradient descent (RMGD) algorithm based on a multi-armed\nbandit for achieving best performance in grid search by selecting an\nappropriate batch size at each epoch with a probability defined as a function\nof its previous success/failure. This probability encourages exploration of\ndifferent batch size and then later exploitation of batch size with history of\nsuccess. At each epoch, the RMGD samples a batch size from its probability\ndistribution, then uses the selected batch size for mini-batch gradient\ndescent. After obtaining the validation loss at each epoch, the probability\ndistribution is updated to incorporate the effectiveness of the sampled batch\nsize. The RMGD essentially assists the learning process to explore the possible\ndomain of the batch size and exploit successful batch size. Experimental\nresults show that the RMGD achieves performance better than the best performing\nsingle batch size. Furthermore, it, obviously, attains this performance in a\nshorter amount of time than grid search. It is surprising that the RMGD\nachieves better performance than grid search.\n", "versions": [{"version": "v1", "created": "Fri, 17 Nov 2017 06:21:47 GMT"}, {"version": "v2", "created": "Sun, 3 Dec 2017 13:35:29 GMT"}, {"version": "v3", "created": "Tue, 27 Feb 2018 14:00:30 GMT"}], "update_date": "2018-02-28", "authors_parsed": [["Cho", "Seong Jin", ""], ["Kang", "Sunghun", ""], ["Yoo", "Chang D.", ""]]}, {"id": "1711.06428", "submitter": "Rajan Udwani", "authors": "Rajan Udwani", "title": "Multi-Objective Maximization of Monotone Submodular Functions with\n  Cardinality Constraint", "comments": "Most recent version fixes an error in the journal as well as\n  conference versions (INFORMS Journal on Optimization, Neurips 2018)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS math.OC stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider the problem of multi-objective maximization of monotone\nsubmodular functions subject to cardinality constraint, often formulated as\n$\\max_{|A|=k}\\min_{i\\in\\{1,\\dots,m\\}}f_i(A)$. While it is widely known that\ngreedy methods work well for a single objective, the problem becomes much\nharder with multiple objectives. In fact, Krause et al.\\ (2008) showed that\nwhen the number of objectives $m$ grows as the cardinality $k$ i.e.,\n$m=\\Omega(k)$, the problem is inapproximable (unless $P=NP$). On the other\nhand, when $m$ is constant Chekuri et al.\\ (2010) showed a randomized\n$(1-1/e)-\\epsilon$ approximation with runtime (number of queries to function\noracle) $n^{m/\\epsilon^3}$. %In fact, the result of Chekuri et al.\\ (2010) is\nfor the far more general case of matroid constant.\n  We focus on finding a fast and practical algorithm that has (asymptotic)\napproximation guarantees even when $m$ is super constant. We first modify the\nalgorithm of Chekuri et al.\\ (2010) to achieve a $(1-1/e)$ approximation for\n$m=o(\\frac{k}{\\log^3 k})$. This demonstrates a steep transition from constant\nfactor approximability to inapproximability around $m=\\Omega(k)$. Then using\nMultiplicative-Weight-Updates (MWU), we find a much faster\n$\\tilde{O}(n/\\delta^3)$ time asymptotic $(1-1/e)^2-\\delta$ approximation. While\nthe above results are all randomized, we also give a simple deterministic\n$(1-1/e)-\\epsilon$ approximation with runtime $kn^{m/\\epsilon^4}$. Finally, we\nrun synthetic experiments using Kronecker graphs and find that our MWU inspired\nheuristic outperforms existing heuristics.\n", "versions": [{"version": "v1", "created": "Fri, 17 Nov 2017 06:44:24 GMT"}, {"version": "v2", "created": "Fri, 2 Nov 2018 16:08:39 GMT"}, {"version": "v3", "created": "Mon, 24 Aug 2020 15:43:01 GMT"}, {"version": "v4", "created": "Sat, 1 May 2021 18:12:22 GMT"}], "update_date": "2021-05-04", "authors_parsed": [["Udwani", "Rajan", ""]]}, {"id": "1711.06431", "submitter": "Housam Khalifa Bashier Babiker", "authors": "Housam Khalifa Bashier Babiker and Randy Goebel", "title": "Using KL-divergence to focus Deep Visual Explanation", "comments": "Presented at NIPS 2017 Symposium on Interpretable Machine Learning", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.CV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a method for explaining the image classification predictions of\ndeep convolution neural networks, by highlighting the pixels in the image which\ninfluence the final class prediction. Our method requires the identification of\na heuristic method to select parameters hypothesized to be most relevant in\nthis prediction, and here we use Kullback-Leibler divergence to provide this\nfocus. Overall, our approach helps in understanding and interpreting deep\nnetwork predictions and we hope contributes to a foundation for such\nunderstanding of deep learning networks. In this brief paper, our experiments\nevaluate the performance of two popular networks in this context of\ninterpretability.\n", "versions": [{"version": "v1", "created": "Fri, 17 Nov 2017 06:53:17 GMT"}, {"version": "v2", "created": "Thu, 25 Jan 2018 06:18:18 GMT"}], "update_date": "2018-01-26", "authors_parsed": [["Babiker", "Housam Khalifa Bashier", ""], ["Goebel", "Randy", ""]]}, {"id": "1711.06446", "submitter": "Ke Ma", "authors": "Ke Ma, Jinshan Zeng, Jiechao Xiong, Qianqian Xu, Xiaochun Cao, Wei\n  Liu, Yuan Yao", "title": "Stochastic Non-convex Ordinal Embedding with Stabilized Barzilai-Borwein\n  Step Size", "comments": "11 pages, 3 figures, 2 tables, accepted by AAAI2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.IR cs.LG math.OC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Learning representation from relative similarity comparisons, often called\nordinal embedding, gains rising attention in recent years. Most of the existing\nmethods are batch methods designed mainly based on the convex optimization,\nsay, the projected gradient descent method. However, they are generally\ntime-consuming due to that the singular value decomposition (SVD) is commonly\nadopted during the update, especially when the data size is very large. To\novercome this challenge, we propose a stochastic algorithm called SVRG-SBB,\nwhich has the following features: (a) SVD-free via dropping convexity, with\ngood scalability by the use of stochastic algorithm, i.e., stochastic variance\nreduced gradient (SVRG), and (b) adaptive step size choice via introducing a\nnew stabilized Barzilai-Borwein (SBB) method as the original version for convex\nproblems might fail for the considered stochastic \\textit{non-convex}\noptimization problem. Moreover, we show that the proposed algorithm converges\nto a stationary point at a rate $\\mathcal{O}(\\frac{1}{T})$ in our setting,\nwhere $T$ is the number of total iterations. Numerous simulations and\nreal-world data experiments are conducted to show the effectiveness of the\nproposed algorithm via comparing with the state-of-the-art methods,\nparticularly, much lower computational cost with good prediction performance.\n", "versions": [{"version": "v1", "created": "Fri, 17 Nov 2017 08:01:07 GMT"}, {"version": "v2", "created": "Wed, 31 Jan 2018 02:47:26 GMT"}], "update_date": "2018-02-01", "authors_parsed": [["Ma", "Ke", ""], ["Zeng", "Jinshan", ""], ["Xiong", "Jiechao", ""], ["Xu", "Qianqian", ""], ["Cao", "Xiaochun", ""], ["Liu", "Wei", ""], ["Yao", "Yuan", ""]]}, {"id": "1711.06464", "submitter": "Jens Berg", "authors": "Jens Berg and Kaj Nystr\\\"om", "title": "A unified deep artificial neural network approach to partial\n  differential equations in complex geometries", "comments": "35 pages, 12 figures", "journal-ref": null, "doi": "10.1016/j.neucom.2018.06.056", "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper we use deep feedforward artificial neural networks to\napproximate solutions to partial differential equations in complex geometries.\nWe show how to modify the backpropagation algorithm to compute the partial\nderivatives of the network output with respect to the space variables which is\nneeded to approximate the differential operator. The method is based on an\nansatz for the solution which requires nothing but feedforward neural networks\nand an unconstrained gradient based optimization method such as gradient\ndescent or a quasi-Newton method.\n  We show an example where classical mesh based methods cannot be used and\nneural networks can be seen as an attractive alternative. Finally, we highlight\nthe benefits of deep compared to shallow neural networks and device some other\nconvergence enhancing techniques.\n", "versions": [{"version": "v1", "created": "Fri, 17 Nov 2017 09:29:52 GMT"}, {"version": "v2", "created": "Wed, 22 Aug 2018 07:28:28 GMT"}], "update_date": "2018-08-28", "authors_parsed": [["Berg", "Jens", ""], ["Nystr\u00f6m", "Kaj", ""]]}, {"id": "1711.06494", "submitter": "Marco Federici", "authors": "Marco Federici, Karen Ullrich, Max Welling", "title": "Improved Bayesian Compression", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Compression of Neural Networks (NN) has become a highly studied topic in\nrecent years. The main reason for this is the demand for industrial scale usage\nof NNs such as deploying them on mobile devices, storing them efficiently,\ntransmitting them via band-limited channels and most importantly doing\ninference at scale. In this work, we propose to join the Soft-Weight Sharing\nand Variational Dropout approaches that show strong results to define a new\nstate-of-the-art in terms of model compression.\n", "versions": [{"version": "v1", "created": "Fri, 17 Nov 2017 11:06:16 GMT"}, {"version": "v2", "created": "Thu, 7 Dec 2017 18:13:59 GMT"}], "update_date": "2017-12-08", "authors_parsed": [["Federici", "Marco", ""], ["Ullrich", "Karen", ""], ["Welling", "Max", ""]]}, {"id": "1711.06504", "submitter": "Luke Oakden-Rayner", "authors": "William Gale, Luke Oakden-Rayner, Gustavo Carneiro, Andrew P. Bradley,\n  Lyle J. Palmer", "title": "Detecting hip fractures with radiologist-level performance using deep\n  neural networks", "comments": "6 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We developed an automated deep learning system to detect hip fractures from\nfrontal pelvic x-rays, an important and common radiological task. Our system\nwas trained on a decade of clinical x-rays (~53,000 studies) and can be applied\nto clinical data, automatically excluding inappropriate and technically\nunsatisfactory studies. We demonstrate diagnostic performance equivalent to a\nhuman radiologist and an area under the ROC curve of 0.994. Translated to\nclinical practice, such a system has the potential to increase the efficiency\nof diagnosis, reduce the need for expensive additional testing, expand access\nto expert level medical image interpretation, and improve overall patient\noutcomes.\n", "versions": [{"version": "v1", "created": "Fri, 17 Nov 2017 11:56:07 GMT"}], "update_date": "2017-11-20", "authors_parsed": [["Gale", "William", ""], ["Oakden-Rayner", "Luke", ""], ["Carneiro", "Gustavo", ""], ["Bradley", "Andrew P.", ""], ["Palmer", "Lyle J.", ""]]}, {"id": "1711.06528", "submitter": "Xu Sun", "authors": "Xu Sun, Xuancheng Ren, Shuming Ma, Bingzhen Wei, Wei Li, Jingjing Xu,\n  Houfeng Wang, Yi Zhang", "title": "Training Simplification and Model Simplification for Deep Learning: A\n  Minimal Effort Back Propagation Method", "comments": "14 pages, 4 figures, 13 tables, accepted for publication in IEEE\n  TKDE; this article supersedes arXiv:1706.06197", "journal-ref": null, "doi": "10.1109/TKDE.2018.2883613", "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a simple yet effective technique to simplify the training and the\nresulting model of neural networks. In back propagation, only a small subset of\nthe full gradient is computed to update the model parameters. The gradient\nvectors are sparsified in such a way that only the top-k elements (in terms of\nmagnitude) are kept. As a result, only k rows or columns (depending on the\nlayout) of the weight matrix are modified, leading to a linear reduction in the\ncomputational cost. Based on the sparsified gradients, we further simplify the\nmodel by eliminating the rows or columns that are seldom updated, which will\nreduce the computational cost both in the training and decoding, and\npotentially accelerate decoding in real-world applications. Surprisingly,\nexperimental results demonstrate that most of time we only need to update fewer\nthan 5% of the weights at each back propagation pass. More interestingly, the\naccuracy of the resulting models is actually improved rather than degraded, and\na detailed analysis is given. The model simplification results show that we\ncould adaptively simplify the model which could often be reduced by around 9x,\nwithout any loss on accuracy or even with improved accuracy. The codes,\nincluding the extension, are available at https://github.com/lancopku/meSimp\n", "versions": [{"version": "v1", "created": "Fri, 17 Nov 2017 13:36:51 GMT"}, {"version": "v2", "created": "Tue, 12 Mar 2019 01:22:44 GMT"}], "update_date": "2019-03-13", "authors_parsed": [["Sun", "Xu", ""], ["Ren", "Xuancheng", ""], ["Ma", "Shuming", ""], ["Wei", "Bingzhen", ""], ["Li", "Wei", ""], ["Xu", "Jingjing", ""], ["Wang", "Houfeng", ""], ["Zhang", "Yi", ""]]}, {"id": "1711.06552", "submitter": "Isa Inuwa-Dutse", "authors": "Isa Inuwa-Dutse", "title": "Introduction to intelligent computing unit 1", "comments": "23 Pages and 10 figures document", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This brief note highlights some basic concepts required toward understanding\nthe evolution of machine learning and deep learning models. The note starts\nwith an overview of artificial intelligence and its relationship to biological\nneuron that ultimately led to the evolution of todays intelligent models.\n", "versions": [{"version": "v1", "created": "Wed, 15 Nov 2017 16:52:48 GMT"}], "update_date": "2017-11-20", "authors_parsed": [["Inuwa-Dutse", "Isa", ""]]}, {"id": "1711.06565", "submitter": "Andrew Lim", "authors": "Jun-Ya Gotoh, Michael Jong Kim, Andrew E.B. Lim", "title": "Calibration of Distributionally Robust Empirical Optimization Models", "comments": "51 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.SY econ.EM eess.SY q-fin.PM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study the out-of-sample properties of robust empirical optimization\nproblems with smooth $\\phi$-divergence penalties and smooth concave objective\nfunctions, and develop a theory for data-driven calibration of the non-negative\n\"robustness parameter\" $\\delta$ that controls the size of the deviations from\nthe nominal model. Building on the intuition that robust optimization reduces\nthe sensitivity of the expected reward to errors in the model by controlling\nthe spread of the reward distribution, we show that the first-order benefit of\n``little bit of robustness\" (i.e., $\\delta$ small, positive) is a significant\nreduction in the variance of the out-of-sample reward while the corresponding\nimpact on the mean is almost an order of magnitude smaller. One implication is\nthat substantial variance (sensitivity) reduction is possible at little cost if\nthe robustness parameter is properly calibrated. To this end, we introduce the\nnotion of a robust mean-variance frontier to select the robustness parameter\nand show that it can be approximated using resampling methods like the\nbootstrap. Our examples show that robust solutions resulting from \"open loop\"\ncalibration methods (e.g., selecting a $90\\%$ confidence level regardless of\nthe data and objective function) can be very conservative out-of-sample, while\nthose corresponding to the robustness parameter that optimizes an estimate of\nthe out-of-sample expected reward (e.g., via the bootstrap) with no regard for\nthe variance are often insufficiently robust.\n", "versions": [{"version": "v1", "created": "Fri, 17 Nov 2017 14:47:00 GMT"}, {"version": "v2", "created": "Tue, 19 May 2020 02:37:08 GMT"}], "update_date": "2020-05-20", "authors_parsed": [["Gotoh", "Jun-Ya", ""], ["Kim", "Michael Jong", ""], ["Lim", "Andrew E. B.", ""]]}, {"id": "1711.06583", "submitter": "Pawe{\\l} Liskowski", "authors": "Pawe{\\l} Liskowski, Wojciech Ja\\'skowski, Krzysztof Krawiec", "title": "Learning to Play Othello with Deep Neural Networks", "comments": null, "journal-ref": null, "doi": "10.1109/TG.2018.2799997", "report-no": null, "categories": "cs.AI cs.CV cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Achieving superhuman playing level by AlphaGo corroborated the capabilities\nof convolutional neural architectures (CNNs) for capturing complex spatial\npatterns. This result was to a great extent due to several analogies between Go\nboard states and 2D images CNNs have been designed for, in particular\ntranslational invariance and a relatively large board. In this paper, we verify\nwhether CNN-based move predictors prove effective for Othello, a game with\nsignificantly different characteristics, including a much smaller board size\nand complete lack of translational invariance. We compare several CNN\narchitectures and board encodings, augment them with state-of-the-art\nextensions, train on an extensive database of experts' moves, and examine them\nwith respect to move prediction accuracy and playing strength. The empirical\nevaluation confirms high capabilities of neural move predictors and suggests a\nstrong correlation between prediction accuracy and playing strength. The best\nCNNs not only surpass all other 1-ply Othello players proposed to date but\ndefeat (2-ply) Edax, the best open-source Othello player.\n", "versions": [{"version": "v1", "created": "Fri, 17 Nov 2017 15:14:20 GMT"}], "update_date": "2018-02-13", "authors_parsed": [["Liskowski", "Pawe\u0142", ""], ["Ja\u015bkowski", "Wojciech", ""], ["Krawiec", "Krzysztof", ""]]}, {"id": "1711.06598", "submitter": "Kathrin Grosse", "authors": "Kathrin Grosse, David Pfaff, Michael Thomas Smith, Michael Backes", "title": "How Wrong Am I? - Studying Adversarial Examples and their Impact on\n  Uncertainty in Gaussian Process Machine Learning Models", "comments": "Reasoning incomplete. Fixed issue in arXiv:1812.02606 (The\n  limitations of model uncertainty in adversarial settings)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CR cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Machine learning models are vulnerable to Adversarial Examples: minor\nperturbations to input samples intended to deliberately cause\nmisclassification. Current defenses against adversarial examples, especially\nfor Deep Neural Networks (DNN), are primarily derived from empirical\ndevelopments, and their security guarantees are often only justified\nretroactively. Many defenses therefore rely on hidden assumptions that are\nsubsequently subverted by increasingly elaborate attacks. This is not\nsurprising: deep learning notoriously lacks a comprehensive mathematical\nframework to provide meaningful guarantees.\n  In this paper, we leverage Gaussian Processes to investigate adversarial\nexamples in the framework of Bayesian inference. Across different models and\ndatasets, we find deviating levels of uncertainty reflect the perturbation\nintroduced to benign samples by state-of-the-art attacks, including novel\nwhite-box attacks on Gaussian Processes. Our experiments demonstrate that even\nunoptimized uncertainty thresholds already reject adversarial examples in many\nscenarios.\n  Comment: Thresholds can be broken in a modified attack, which was done in\narXiv:1812.02606 (The limitations of model uncertainty in adversarial\nsettings).\n", "versions": [{"version": "v1", "created": "Fri, 17 Nov 2017 15:46:44 GMT"}, {"version": "v2", "created": "Tue, 13 Feb 2018 09:06:33 GMT"}, {"version": "v3", "created": "Fri, 16 Feb 2018 09:37:19 GMT"}, {"version": "v4", "created": "Thu, 3 Jan 2019 12:29:59 GMT"}], "update_date": "2019-01-04", "authors_parsed": [["Grosse", "Kathrin", ""], ["Pfaff", "David", ""], ["Smith", "Michael Thomas", ""], ["Backes", "Michael", ""]]}, {"id": "1711.06642", "submitter": "Richard Samworth", "authors": "Thomas B. Berrett and Richard J. Samworth", "title": "Nonparametric independence testing via mutual information", "comments": "46 pages, 2 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME cs.IT math.IT math.ST stat.ML stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a test of independence of two multivariate random vectors, given a\nsample from the underlying population. Our approach, which we call MINT, is\nbased on the estimation of mutual information, whose decomposition into joint\nand marginal entropies facilitates the use of recently-developed efficient\nentropy estimators derived from nearest neighbour distances. The proposed\ncritical values, which may be obtained from simulation (in the case where one\nmarginal is known) or resampling, guarantee that the test has nominal size, and\nwe provide local power analyses, uniformly over classes of densities whose\nmutual information satisfies a lower bound. Our ideas may be extended to\nprovide a new goodness-of-fit tests of normal linear models based on assessing\nthe independence of our vector of covariates and an appropriately-defined\nnotion of an error vector. The theory is supported by numerical studies on both\nsimulated and real data.\n", "versions": [{"version": "v1", "created": "Fri, 17 Nov 2017 17:38:50 GMT"}], "update_date": "2017-11-20", "authors_parsed": [["Berrett", "Thomas B.", ""], ["Samworth", "Richard J.", ""]]}, {"id": "1711.06656", "submitter": "Palma London", "authors": "Palma London, Shai Vardi, Adam Wierman, Hanling Yi", "title": "A Parallelizable Acceleration Framework for Packing Linear Programs", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.OC cs.NA stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper presents an acceleration framework for packing linear programming\nproblems where the amount of data available is limited, i.e., where the number\nof constraints m is small compared to the variable dimension n. The framework\ncan be used as a black box to speed up linear programming solvers dramatically,\nby two orders of magnitude in our experiments. We present worst-case guarantees\non the quality of the solution and the speedup provided by the algorithm,\nshowing that the framework provides an approximately optimal solution while\nrunning the original solver on a much smaller problem. The framework can be\nused to accelerate exact solvers, approximate solvers, and parallel/distributed\nsolvers. Further, it can be used for both linear programs and integer linear\nprograms.\n", "versions": [{"version": "v1", "created": "Fri, 17 Nov 2017 18:17:01 GMT"}], "update_date": "2017-11-20", "authors_parsed": [["London", "Palma", ""], ["Vardi", "Shai", ""], ["Wierman", "Adam", ""], ["Yi", "Hanling", ""]]}, {"id": "1711.06664", "submitter": "David Madras", "authors": "David Madras, Toniann Pitassi, Richard Zemel", "title": "Predict Responsibly: Improving Fairness and Accuracy by Learning to\n  Defer", "comments": "Accepted as a conference paper at Neural Information Processing\n  Systems 2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In many machine learning applications, there are multiple decision-makers\ninvolved, both automated and human. The interaction between these agents often\ngoes unaddressed in algorithmic development. In this work, we explore a simple\nversion of this interaction with a two-stage framework containing an automated\nmodel and an external decision-maker. The model can choose to say \"Pass\", and\npass the decision downstream, as explored in rejection learning. We extend this\nconcept by proposing \"learning to defer\", which generalizes rejection learning\nby considering the effect of other agents in the decision-making process. We\npropose a learning algorithm which accounts for potential biases held by\nexternal decision-makers in a system. Experiments demonstrate that learning to\ndefer can make systems not only more accurate but also less biased. Even when\nworking with inconsistent or biased users, we show that deferring models still\ngreatly improve the accuracy and/or fairness of the entire system.\n", "versions": [{"version": "v1", "created": "Fri, 17 Nov 2017 18:43:04 GMT"}, {"version": "v2", "created": "Tue, 20 Feb 2018 16:34:56 GMT"}, {"version": "v3", "created": "Fri, 7 Sep 2018 00:48:55 GMT"}], "update_date": "2018-09-10", "authors_parsed": [["Madras", "David", ""], ["Pitassi", "Toniann", ""], ["Zemel", "Richard", ""]]}, {"id": "1711.06673", "submitter": "Zeyuan Allen-Zhu", "authors": "Zeyuan Allen-Zhu and Yuanzhi Li", "title": "Neon2: Finding Local Minima via First-Order Oracles", "comments": "version 2 and 3 improve writing", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.DS cs.NE math.OC stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a reduction for non-convex optimization that can (1) turn an\nstationary-point finding algorithm into an local-minimum finding one, and (2)\nreplace the Hessian-vector product computations with only gradient\ncomputations. It works both in the stochastic and the deterministic settings,\nwithout hurting the algorithm's performance.\n  As applications, our reduction turns Natasha2 into a first-order method\nwithout hurting its performance. It also converts SGD, GD, SCSG, and SVRG into\nalgorithms finding approximate local minima, outperforming some best known\nresults.\n", "versions": [{"version": "v1", "created": "Fri, 17 Nov 2017 18:59:01 GMT"}, {"version": "v2", "created": "Tue, 20 Feb 2018 06:34:18 GMT"}, {"version": "v3", "created": "Fri, 20 Apr 2018 07:58:25 GMT"}], "update_date": "2018-04-23", "authors_parsed": [["Allen-Zhu", "Zeyuan", ""], ["Li", "Yuanzhi", ""]]}, {"id": "1711.06705", "submitter": "Zhigang Yao", "authors": "Zhigang Yao and Zhenyue Zhang", "title": "Principal Boundary on Riemannian Manifolds", "comments": "31 pages,10 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider the classification problem and focus on nonlinear methods for\nclassification on manifolds. For multivariate datasets lying on an embedded\nnonlinear Riemannian manifold within the higher-dimensional ambient space, we\naim to acquire a classification boundary for the classes with labels, using the\nintrinsic metric on the manifolds. Motivated by finding an optimal boundary\nbetween the two classes, we invent a novel approach -- the principal boundary.\nFrom the perspective of classification, the principal boundary is defined as an\noptimal curve that moves in between the principal flows traced out from two\nclasses of data, and at any point on the boundary, it maximizes the margin\nbetween the two classes. We estimate the boundary in quality with its\ndirection, supervised by the two principal flows. We show that the principal\nboundary yields the usual decision boundary found by the support vector machine\nin the sense that locally, the two boundaries coincide. Some optimality and\nconvergence properties of the random principal boundary and its population\ncounterpart are also shown. We illustrate how to find, use and interpret the\nprincipal boundary with an application in real data.\n", "versions": [{"version": "v1", "created": "Sat, 21 Oct 2017 14:35:45 GMT"}, {"version": "v2", "created": "Sat, 30 Mar 2019 17:34:40 GMT"}], "update_date": "2019-04-02", "authors_parsed": [["Yao", "Zhigang", ""], ["Zhang", "Zhenyue", ""]]}, {"id": "1711.06711", "submitter": "Nicholas Marshall", "authors": "Nicholas F. Marshall, Ronald R. Coifman", "title": "Manifold learning with bi-stochastic kernels", "comments": "18 pages, 5 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML math.FA math.SP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper we answer the following question: what is the infinitesimal\ngenerator of the diffusion process defined by a kernel that is normalized such\nthat it is bi-stochastic with respect to a specified measure? More precisely,\nunder the assumption that data is sampled from a Riemannian manifold we\ndetermine how the resulting infinitesimal generator depends on the potentially\nnonuniform distribution of the sample points, and the specified measure for the\nbi-stochastic normalization. In a special case, we demonstrate a connection to\nthe heat kernel. We consider both the case where only a single data set is\ngiven, and the case where a data set and a reference set are given. The\nspectral theory of the constructed operators is studied, and Nystr\\\"om\nextension formulas for the gradients of the eigenfunctions are computed.\nApplications to discrete point sets and manifold learning are discussed.\n", "versions": [{"version": "v1", "created": "Fri, 17 Nov 2017 19:58:52 GMT"}, {"version": "v2", "created": "Tue, 27 Feb 2018 02:15:48 GMT"}], "update_date": "2018-02-28", "authors_parsed": [["Marshall", "Nicholas F.", ""], ["Coifman", "Ronald R.", ""]]}, {"id": "1711.06719", "submitter": "Alexander Terenin", "authors": "Alexander Terenin and Eric P. Xing", "title": "Techniques for proving Asynchronous Convergence results for Markov Chain\n  Monte Carlo methods", "comments": "Workshop on Advances in Approximate Bayesian Inference, 31st\n  Conference on Neural Information Processing Systems, 2017", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Markov Chain Monte Carlo (MCMC) methods such as Gibbs sampling are finding\nwidespread use in applied statistics and machine learning. These often lead to\ndifficult computational problems, which are increasingly being solved on\nparallel and distributed systems such as compute clusters. Recent work has\nproposed running iterative algorithms such as gradient descent and MCMC in\nparallel asynchronously for increased performance, with good empirical results\nin certain problems. Unfortunately, for MCMC this parallelization technique\nrequires new convergence theory, as it has been explicitly demonstrated to lead\nto divergence on some examples. Recent theory on Asynchronous Gibbs sampling\ndescribes why these algorithms can fail, and provides a way to alter them to\nmake them converge. In this article, we describe how to apply this theory in a\ngeneric setting, to understand the asynchronous behavior of any MCMC algorithm,\nincluding those implemented using parameter servers, and those not based on\nGibbs sampling.\n", "versions": [{"version": "v1", "created": "Fri, 17 Nov 2017 20:46:38 GMT"}, {"version": "v2", "created": "Fri, 24 Nov 2017 21:38:29 GMT"}, {"version": "v3", "created": "Tue, 28 Nov 2017 16:01:13 GMT"}, {"version": "v4", "created": "Thu, 30 Nov 2017 15:54:34 GMT"}, {"version": "v5", "created": "Sun, 3 Jun 2018 23:42:46 GMT"}], "update_date": "2018-06-05", "authors_parsed": [["Terenin", "Alexander", ""], ["Xing", "Eric P.", ""]]}, {"id": "1711.06756", "submitter": "Hesham Mostafa", "authors": "Hesham Mostafa, Vishwajith Ramesh, Gert Cauwenberghs", "title": "Deep supervised learning using local errors", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NE cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Error backpropagation is a highly effective mechanism for learning\nhigh-quality hierarchical features in deep networks. Updating the features or\nweights in one layer, however, requires waiting for the propagation of error\nsignals from higher layers. Learning using delayed and non-local errors makes\nit hard to reconcile backpropagation with the learning mechanisms observed in\nbiological neural networks as it requires the neurons to maintain a memory of\nthe input long enough until the higher-layer errors arrive. In this paper, we\npropose an alternative learning mechanism where errors are generated locally in\neach layer using fixed, random auxiliary classifiers. Lower layers could thus\nbe trained independently of higher layers and training could either proceed\nlayer by layer, or simultaneously in all layers using local error information.\nWe address biological plausibility concerns such as weight symmetry\nrequirements and show that the proposed learning mechanism based on fixed,\nbroad, and random tuning of each neuron to the classification categories\noutperforms the biologically-motivated feedback alignment learning technique on\nthe MNIST, CIFAR10, and SVHN datasets, approaching the performance of standard\nbackpropagation. Our approach highlights a potential biological mechanism for\nthe supervised, or task-dependent, learning of feature hierarchies. In\naddition, we show that it is well suited for learning deep networks in custom\nhardware where it can drastically reduce memory traffic and data communication\noverheads.\n", "versions": [{"version": "v1", "created": "Fri, 17 Nov 2017 22:48:02 GMT"}], "update_date": "2017-11-21", "authors_parsed": [["Mostafa", "Hesham", ""], ["Ramesh", "Vishwajith", ""], ["Cauwenberghs", "Gert", ""]]}, {"id": "1711.06771", "submitter": "Zachary Charles", "authors": "Zachary Charles, Dimitris Papailiopoulos, Jordan Ellenberg", "title": "Approximate Gradient Coding via Sparse Random Graphs", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.DC cs.IT cs.LG math.IT stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Distributed algorithms are often beset by the straggler effect, where the\nslowest compute nodes in the system dictate the overall running time.\nCoding-theoretic techniques have been recently proposed to mitigate stragglers\nvia algorithmic redundancy. Prior work in coded computation and gradient coding\nhas mainly focused on exact recovery of the desired output. However, slightly\ninexact solutions can be acceptable in applications that are robust to noise,\nsuch as model training via gradient-based algorithms. In this work, we present\ncomputationally simple gradient codes based on sparse graphs that guarantee\nfast and approximately accurate distributed computation. We demonstrate that\nsacrificing a small amount of accuracy can significantly increase algorithmic\nrobustness to stragglers.\n", "versions": [{"version": "v1", "created": "Fri, 17 Nov 2017 23:19:30 GMT"}], "update_date": "2017-11-21", "authors_parsed": [["Charles", "Zachary", ""], ["Papailiopoulos", "Dimitris", ""], ["Ellenberg", "Jordan", ""]]}, {"id": "1711.06786", "submitter": "Hong Xu", "authors": "Therese Anders and Hong Xu and Cheng Cheng and T. K. Satish Kumar", "title": "Measuring Territorial Control in Civil Wars Using Hidden Markov Models:\n  A Data Informatics-Based Approach", "comments": "Presented at NIPS 2017 Workshop on Machine Learning for the\n  Developing World", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP cs.CY physics.soc-ph stat.ML", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Territorial control is a key aspect shaping the dynamics of civil war.\nDespite its importance, we lack data on territorial control that are\nfine-grained enough to account for subnational spatio-temporal variation and\nthat cover a large set of conflicts. To resolve this issue, we propose a\ntheoretical model of the relationship between territorial control and tactical\nchoice in civil war and outline how Hidden Markov Models (HMMs) are suitable to\ncapture theoretical intuitions and estimate levels of territorial control. We\ndiscuss challenges of using HMMs in this application and mitigation strategies\nfor future work.\n", "versions": [{"version": "v1", "created": "Sat, 18 Nov 2017 01:35:59 GMT"}, {"version": "v2", "created": "Sat, 9 Dec 2017 08:18:55 GMT"}], "update_date": "2018-01-23", "authors_parsed": [["Anders", "Therese", ""], ["Xu", "Hong", ""], ["Cheng", "Cheng", ""], ["Kumar", "T. K. Satish", ""]]}, {"id": "1711.06788", "submitter": "Minmin Chen", "authors": "Minmin Chen", "title": "MinimalRNN: Toward More Interpretable and Trainable Recurrent Neural\n  Networks", "comments": "Presented at NIPS 2017 Symposium on Interpretable Machine Learning", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We introduce MinimalRNN, a new recurrent neural network architecture that\nachieves comparable performance as the popular gated RNNs with a simplified\nstructure. It employs minimal updates within RNN, which not only leads to\nefficient learning and testing but more importantly better interpretability and\ntrainability. We demonstrate that by endorsing the more restrictive update\nrule, MinimalRNN learns disentangled RNN states. We further examine the\nlearning dynamics of different RNN structures using input-output Jacobians, and\nshow that MinimalRNN is able to capture longer range dependencies than existing\nRNN architectures.\n", "versions": [{"version": "v1", "created": "Sat, 18 Nov 2017 01:42:04 GMT"}, {"version": "v2", "created": "Wed, 20 Jun 2018 02:19:13 GMT"}], "update_date": "2018-06-21", "authors_parsed": [["Chen", "Minmin", ""]]}, {"id": "1711.06793", "submitter": "Jos\\'e Marcio Luna", "authors": "Jos\\'e Marcio Luna, Eric Eaton, Lyle H. Ungar, Eric Diffenderfer,\n  Shane T. Jensen, Efstathios D. Gennatas, Mateo Wirth, Charles B. Simone II,\n  Timothy D. Solberg, Gilmer Valdes", "title": "Tree-Structured Boosting: Connections Between Gradient Boosted Stumps\n  and Full Decision Trees", "comments": "Presented at NIPS 2017 Symposium on Interpretable Machine Learning", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Additive models, such as produced by gradient boosting, and full interaction\nmodels, such as classification and regression trees (CART), are widely used\nalgorithms that have been investigated largely in isolation. We show that these\nmodels exist along a spectrum, revealing never-before-known connections between\nthese two approaches. This paper introduces a novel technique called\ntree-structured boosting for creating a single decision tree, and shows that\nthis method can produce models equivalent to CART or gradient boosted stumps at\nthe extremes by varying a single parameter. Although tree-structured boosting\nis designed primarily to provide both the model interpretability and predictive\nperformance needed for high-stake applications like medicine, it also can\nproduce decision trees represented by hybrid models between CART and boosted\nstumps that can outperform either of these approaches.\n", "versions": [{"version": "v1", "created": "Sat, 18 Nov 2017 02:05:44 GMT"}], "update_date": "2017-11-21", "authors_parsed": [["Luna", "Jos\u00e9 Marcio", ""], ["Eaton", "Eric", ""], ["Ungar", "Lyle H.", ""], ["Diffenderfer", "Eric", ""], ["Jensen", "Shane T.", ""], ["Gennatas", "Efstathios D.", ""], ["Wirth", "Mateo", ""], ["Simone", "Charles B.", "II"], ["Solberg", "Timothy D.", ""], ["Valdes", "Gilmer", ""]]}, {"id": "1711.06795", "submitter": "Bilal Alsallakh", "authors": "Medha Katehara, Emma Beauxis-Aussalet, Bilal Alsallakh", "title": "Prediction Scores as a Window into Classifier Behavior", "comments": "Presented at NIPS 2017 Symposium on Interpretable Machine Learning", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Most multi-class classifiers make their prediction for a test sample by\nscoring the classes and selecting the one with the highest score. Analyzing\nthese prediction scores is useful to understand the classifier behavior and to\nassess its reliability. We present an interactive visualization that\nfacilitates per-class analysis of these scores. Our system, called Classilist,\nenables relating these scores to the classification correctness and to the\nunderlying samples and their features. We illustrate how such analysis reveals\nvarying behavior of different classifiers. Classilist is available for use\nonline, along with source code, video tutorials, and plugins for R, RapidMiner,\nand KNIME at https://katehara.github.io/classilist-site/.\n", "versions": [{"version": "v1", "created": "Sat, 18 Nov 2017 02:07:52 GMT"}], "update_date": "2017-11-21", "authors_parsed": [["Katehara", "Medha", ""], ["Beauxis-Aussalet", "Emma", ""], ["Alsallakh", "Bilal", ""]]}, {"id": "1711.06798", "submitter": "Elad Eban", "authors": "Ariel Gordon, Elad Eban, Ofir Nachum, Bo Chen, Hao Wu, Tien-Ju Yang,\n  Edward Choi", "title": "MorphNet: Fast & Simple Resource-Constrained Structure Learning of Deep\n  Networks", "comments": "Added reproducibility and stability figures in the appendix, as well\n  minor typos and clarifications to the main text", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present MorphNet, an approach to automate the design of neural network\nstructures. MorphNet iteratively shrinks and expands a network, shrinking via a\nresource-weighted sparsifying regularizer on activations and expanding via a\nuniform multiplicative factor on all layers. In contrast to previous\napproaches, our method is scalable to large networks, adaptable to specific\nresource constraints (e.g. the number of floating-point operations per\ninference), and capable of increasing the network's performance. When applied\nto standard network architectures on a wide variety of datasets, our approach\ndiscovers novel structures in each domain, obtaining higher performance while\nrespecting the resource constraint.\n", "versions": [{"version": "v1", "created": "Sat, 18 Nov 2017 02:33:39 GMT"}, {"version": "v2", "created": "Thu, 30 Nov 2017 06:38:56 GMT"}, {"version": "v3", "created": "Tue, 17 Apr 2018 19:07:21 GMT"}], "update_date": "2018-04-19", "authors_parsed": [["Gordon", "Ariel", ""], ["Eban", "Elad", ""], ["Nachum", "Ofir", ""], ["Chen", "Bo", ""], ["Wu", "Hao", ""], ["Yang", "Tien-Ju", ""], ["Choi", "Edward", ""]]}, {"id": "1711.06813", "submitter": "Jerzy Wieczorek", "authors": "Varun Kshirsagar, Jerzy Wieczorek, Sharada Ramanathan, Rachel Wells", "title": "Household poverty classification in data-scarce environments: a machine\n  learning approach", "comments": "Presented at NIPS 2017 Workshop on Machine Learning for the\n  Developing World, 7 pages with 4 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We describe a method to identify poor households in data-scarce countries by\nleveraging information contained in nationally representative household\nsurveys. It employs standard statistical learning techniques---cross-validation\nand parameter regularization---which together reduce the extent to which the\nmodel is over-fitted to match the idiosyncracies of observed survey data. The\nautomated framework satisfies three important constraints of this development\nsetting: i) The prediction model uses at most ten questions, which limits the\ncosts of data collection; ii) No computation beyond simple arithmetic is needed\nto calculate the probability that a given household is poor, immediately after\ndata on the ten indicators is collected; and iii) One specification of the\nmodel (i.e. one scorecard) is used to predict poverty throughout a country that\nmay be characterized by significant sub-national differences. Using survey data\nfrom Zambia, the model's out-of-sample predictions distinguish poor households\nfrom non-poor households using information contained in ten questions.\n", "versions": [{"version": "v1", "created": "Sat, 18 Nov 2017 04:57:05 GMT"}], "update_date": "2017-11-21", "authors_parsed": [["Kshirsagar", "Varun", ""], ["Wieczorek", "Jerzy", ""], ["Ramanathan", "Sharada", ""], ["Wells", "Rachel", ""]]}, {"id": "1711.06821", "submitter": "Guillem Collell", "authors": "Guillem Collell, Luc Van Gool, Marie-Francine Moens", "title": "Acquiring Common Sense Spatial Knowledge through Implicit Spatial\n  Templates", "comments": "To appear at AAAI 2018 Conference", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.CL cs.CV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Spatial understanding is a fundamental problem with wide-reaching real-world\napplications. The representation of spatial knowledge is often modeled with\nspatial templates, i.e., regions of acceptability of two objects under an\nexplicit spatial relationship (e.g., \"on\", \"below\", etc.). In contrast with\nprior work that restricts spatial templates to explicit spatial prepositions\n(e.g., \"glass on table\"), here we extend this concept to implicit spatial\nlanguage, i.e., those relationships (generally actions) for which the spatial\narrangement of the objects is only implicitly implied (e.g., \"man riding\nhorse\"). In contrast with explicit relationships, predicting spatial\narrangements from implicit spatial language requires significant common sense\nspatial understanding. Here, we introduce the task of predicting spatial\ntemplates for two objects under a relationship, which can be seen as a spatial\nquestion-answering task with a (2D) continuous output (\"where is the man w.r.t.\na horse when the man is walking the horse?\"). We present two simple\nneural-based models that leverage annotated images and structured text to learn\nthis task. The good performance of these models reveals that spatial locations\nare to a large extent predictable from implicit spatial language. Crucially,\nthe models attain similar performance in a challenging generalized setting,\nwhere the object-relation-object combinations (e.g.,\"man walking dog\") have\nnever been seen before. Next, we go one step further by presenting the models\nwith unseen objects (e.g., \"dog\"). In this scenario, we show that leveraging\nword embeddings enables the models to output accurate spatial predictions,\nproving that the models acquire solid common sense spatial knowledge allowing\nfor such generalization.\n", "versions": [{"version": "v1", "created": "Sat, 18 Nov 2017 07:00:44 GMT"}, {"version": "v2", "created": "Tue, 21 Nov 2017 02:41:36 GMT"}, {"version": "v3", "created": "Fri, 6 Mar 2020 15:23:13 GMT"}], "update_date": "2020-03-09", "authors_parsed": [["Collell", "Guillem", ""], ["Van Gool", "Luc", ""], ["Moens", "Marie-Francine", ""]]}, {"id": "1711.06831", "submitter": "Lei Yang", "authors": "Lei Yang", "title": "Proximal Gradient Method with Extrapolation and Line Search for a Class\n  of Nonconvex and Nonsmooth Problems", "comments": "This version addresses some typos in previous version and adds more\n  comparisons", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.OC stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we consider a class of possibly nonconvex, nonsmooth and\nnon-Lipschitz optimization problems arising in many contemporary applications\nsuch as machine learning, variable selection and image processing. To solve\nthis class of problems, we propose a proximal gradient method with\nextrapolation and line search (PGels). This method is developed based on a\nspecial potential function and successfully incorporates both extrapolation and\nnon-monotone line search, which are two simple and efficient accelerating\ntechniques for the proximal gradient method. Thanks to the line search, this\nmethod allows more flexibilities in choosing the extrapolation parameters and\nupdates them adaptively at each iteration if a certain line search criterion is\nnot satisfied. Moreover, with proper choices of parameters, our PGels reduces\nto many existing algorithms. We also show that, under some mild conditions, our\nline search criterion is well defined and any cluster point of the sequence\ngenerated by PGels is a stationary point of our problem. In addition, by\nassuming the Kurdyka-${\\L}$ojasiewicz exponent of the objective in our problem,\nwe further analyze the local convergence rate of two special cases of PGels,\nincluding the widely used non-monotone proximal gradient method as one case.\nFinally, we conduct some numerical experiments for solving the $\\ell_1$\nregularized logistic regression problem and the $\\ell_{1\\text{-}2}$ regularized\nleast squares problem. Our numerical results illustrate the efficiency of PGels\nand show the potential advantage of combining two accelerating techniques.\n", "versions": [{"version": "v1", "created": "Sat, 18 Nov 2017 09:09:59 GMT"}, {"version": "v2", "created": "Tue, 21 Nov 2017 11:54:36 GMT"}, {"version": "v3", "created": "Mon, 10 Dec 2018 08:44:41 GMT"}], "update_date": "2018-12-11", "authors_parsed": [["Yang", "Lei", ""]]}, {"id": "1711.06839", "submitter": "Eli (Omid) David", "authors": "Eli David, Moshe Koppel, Nathan S. Netanyahu", "title": "Genetic Algorithms for Mentor-Assisted Evaluation Function Optimization", "comments": "Winner of Best Paper Award in GECCO 2008. arXiv admin note:\n  substantial text overlap with arXiv:1711.06840, arXiv:1711.06841", "journal-ref": "ACM Genetic and Evolutionary Computation Conference (GECCO), pages\n  1469-1475, Atlanta, GA, July 2008", "doi": "10.1145/1389095.1389382", "report-no": null, "categories": "cs.NE cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper we demonstrate how genetic algorithms can be used to reverse\nengineer an evaluation function's parameters for computer chess. Our results\nshow that using an appropriate mentor, we can evolve a program that is on par\nwith top tournament-playing chess programs, outperforming a two-time World\nComputer Chess Champion. This performance gain is achieved by evolving a\nprogram with a smaller number of parameters in its evaluation function to mimic\nthe behavior of a superior mentor which uses a more extensive evaluation\nfunction. In principle, our mentor-assisted approach could be used in a wide\nrange of problems for which appropriate mentors are available.\n", "versions": [{"version": "v1", "created": "Sat, 18 Nov 2017 10:12:02 GMT"}], "update_date": "2017-11-21", "authors_parsed": [["David", "Eli", ""], ["Koppel", "Moshe", ""], ["Netanyahu", "Nathan S.", ""]]}, {"id": "1711.06840", "submitter": "Eli (Omid) David", "authors": "Eli David, H. Jaap van den Herik, Moshe Koppel, Nathan S. Netanyahu", "title": "Simulating Human Grandmasters: Evolution and Coevolution of Evaluation\n  Functions", "comments": "arXiv admin note: substantial text overlap with arXiv:1711.06839,\n  arXiv:1711.06841", "journal-ref": "ACM Genetic and Evolutionary Computation Conference (GECCO), pages\n  1483-1489, Montreal, Canada, July 2009", "doi": "10.1145/1569901.1570100", "report-no": null, "categories": "cs.NE cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper demonstrates the use of genetic algorithms for evolving a\ngrandmaster-level evaluation function for a chess program. This is achieved by\ncombining supervised and unsupervised learning. In the supervised learning\nphase the organisms are evolved to mimic the behavior of human grandmasters,\nand in the unsupervised learning phase these evolved organisms are further\nimproved upon by means of coevolution.\n  While past attempts succeeded in creating a grandmaster-level program by\nmimicking the behavior of existing computer chess programs, this paper presents\nthe first successful attempt at evolving a state-of-the-art evaluation function\nby learning only from databases of games played by humans. Our results\ndemonstrate that the evolved program outperforms a two-time World Computer\nChess Champion.\n", "versions": [{"version": "v1", "created": "Sat, 18 Nov 2017 10:16:24 GMT"}], "update_date": "2017-11-21", "authors_parsed": [["David", "Eli", ""], ["Herik", "H. Jaap van den", ""], ["Koppel", "Moshe", ""], ["Netanyahu", "Nathan S.", ""]]}, {"id": "1711.06841", "submitter": "Eli (Omid) David", "authors": "Eli David, Moshe Koppel, Nathan S. Netanyahu", "title": "Expert-Driven Genetic Algorithms for Simulating Evaluation Functions", "comments": "arXiv admin note: substantial text overlap with arXiv:1711.06839,\n  arXiv:1711.06840", "journal-ref": "Genetic Programming and Evolvable Machines, Vol. 12, No. 1, pp.\n  5-22, March 2011", "doi": "10.1007/s10710-010-9103-4", "report-no": null, "categories": "cs.NE cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper we demonstrate how genetic algorithms can be used to reverse\nengineer an evaluation function's parameters for computer chess. Our results\nshow that using an appropriate expert (or mentor), we can evolve a program that\nis on par with top tournament-playing chess programs, outperforming a two-time\nWorld Computer Chess Champion. This performance gain is achieved by evolving a\nprogram that mimics the behavior of a superior expert. The resulting evaluation\nfunction of the evolved program consists of a much smaller number of parameters\nthan the expert's. The extended experimental results provided in this paper\ninclude a report of our successful participation in the 2008 World Computer\nChess Championship. In principle, our expert-driven approach could be used in a\nwide range of problems for which appropriate experts are available.\n", "versions": [{"version": "v1", "created": "Sat, 18 Nov 2017 10:22:49 GMT"}], "update_date": "2017-11-21", "authors_parsed": [["David", "Eli", ""], ["Koppel", "Moshe", ""], ["Netanyahu", "Nathan S.", ""]]}, {"id": "1711.06922", "submitter": "Mikhail Pavlov", "authors": "Mikhail Pavlov, Sergey Kolesnikov, Sergey M. Plis", "title": "Run, skeleton, run: skeletal model in a physics-based simulation", "comments": "Corrected typos and spelling", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we present our approach to solve a physics-based reinforcement\nlearning challenge \"Learning to Run\" with objective to train\nphysiologically-based human model to navigate a complex obstacle course as\nquickly as possible. The environment is computationally expensive, has a\nhigh-dimensional continuous action space and is stochastic. We benchmark state\nof the art policy-gradient methods and test several improvements, such as layer\nnormalization, parameter noise, action and state reflecting, to stabilize\ntraining and improve its sample-efficiency. We found that the Deep\nDeterministic Policy Gradient method is the most efficient method for this\nenvironment and the improvements we have introduced help to stabilize training.\nLearned models are able to generalize to new physical scenarios, e.g. different\nobstacle courses.\n", "versions": [{"version": "v1", "created": "Sat, 18 Nov 2017 20:18:16 GMT"}, {"version": "v2", "created": "Sun, 28 Jan 2018 09:29:07 GMT"}], "update_date": "2018-01-30", "authors_parsed": [["Pavlov", "Mikhail", ""], ["Kolesnikov", "Sergey", ""], ["Plis", "Sergey M.", ""]]}, {"id": "1711.06929", "submitter": "Cinzia Viroli", "authors": "Cinzia Viroli and Geoffrey J. McLachlan", "title": "Deep Gaussian Mixture Models", "comments": "19 pages, 4 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Deep learning is a hierarchical inference method formed by subsequent\nmultiple layers of learning able to more efficiently describe complex\nrelationships. In this work, Deep Gaussian Mixture Models are introduced and\ndiscussed. A Deep Gaussian Mixture model (DGMM) is a network of multiple layers\nof latent variables, where, at each layer, the variables follow a mixture of\nGaussian distributions. Thus, the deep mixture model consists of a set of\nnested mixtures of linear models, which globally provide a nonlinear model able\nto describe the data in a very flexible way. In order to avoid\noverparameterized solutions, dimension reduction by factor models can be\napplied at each layer of the architecture thus resulting in deep mixtures of\nfactor analysers.\n", "versions": [{"version": "v1", "created": "Sat, 18 Nov 2017 21:48:36 GMT"}], "update_date": "2017-11-21", "authors_parsed": [["Viroli", "Cinzia", ""], ["McLachlan", "Geoffrey J.", ""]]}, {"id": "1711.06940", "submitter": "Dennis Shen", "authors": "Muhammad Jehangir Amjad, Devavrat Shah, and Dennis Shen", "title": "Robust Synthetic Control", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "econ.EM stat.AP stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a robust generalization of the synthetic control method for\ncomparative case studies. Like the classical method, we present an algorithm to\nestimate the unobservable counterfactual of a treatment unit. A distinguishing\nfeature of our algorithm is that of de-noising the data matrix via singular\nvalue thresholding, which renders our approach robust in multiple facets: it\nautomatically identifies a good subset of donors, overcomes the challenges of\nmissing data, and continues to work well in settings where covariate\ninformation may not be provided. To begin, we establish the condition under\nwhich the fundamental assumption in synthetic control-like approaches holds,\ni.e. when the linear relationship between the treatment unit and the donor pool\nprevails in both the pre- and post-intervention periods. We provide the first\nfinite sample analysis for a broader class of models, the Latent Variable\nModel, in contrast to Factor Models previously considered in the literature.\nFurther, we show that our de-noising procedure accurately imputes missing\nentries, producing a consistent estimator of the underlying signal matrix\nprovided $p = \\Omega( T^{-1 + \\zeta})$ for some $\\zeta > 0$; here, $p$ is the\nfraction of observed data and $T$ is the time interval of interest. Under the\nsame setting, we prove that the mean-squared-error (MSE) in our prediction\nestimation scales as $O(\\sigma^2/p + 1/\\sqrt{T})$, where $\\sigma^2$ is the\nnoise variance. Using a data aggregation method, we show that the MSE can be\nmade as small as $O(T^{-1/2+\\gamma})$ for any $\\gamma \\in (0, 1/2)$, leading to\na consistent estimator. We also introduce a Bayesian framework to quantify the\nmodel uncertainty through posterior probabilities. Our experiments, using both\nreal-world and synthetic datasets, demonstrate that our robust generalization\nyields an improvement over the classical synthetic control method.\n", "versions": [{"version": "v1", "created": "Sat, 18 Nov 2017 23:22:34 GMT"}], "update_date": "2017-11-21", "authors_parsed": [["Amjad", "Muhammad Jehangir", ""], ["Shah", "Devavrat", ""], ["Shen", "Dennis", ""]]}, {"id": "1711.06959", "submitter": "Ziming Zhang", "authors": "Ziming Zhang, Yuanwei Wu, Guanghui Wang", "title": "BPGrad: Towards Global Optimality in Deep Learning via Branch and\n  Pruning", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Understanding the global optimality in deep learning (DL) has been attracting\nmore and more attention recently. Conventional DL solvers, however, have not\nbeen developed intentionally to seek for such global optimality. In this paper\nwe propose a novel approximation algorithm, BPGrad, towards optimizing deep\nmodels globally via branch and pruning. Our BPGrad algorithm is based on the\nassumption of Lipschitz continuity in DL, and as a result it can adaptively\ndetermine the step size for current gradient given the history of previous\nupdates, wherein theoretically no smaller steps can achieve the global\noptimality. We prove that, by repeating such branch-and-pruning procedure, we\ncan locate the global optimality within finite iterations. Empirically an\nefficient solver based on BPGrad for DL is proposed as well, and it outperforms\nconventional DL solvers such as Adagrad, Adadelta, RMSProp, and Adam in the\ntasks of object recognition, detection, and segmentation.\n", "versions": [{"version": "v1", "created": "Sun, 19 Nov 2017 02:44:31 GMT"}], "update_date": "2017-11-21", "authors_parsed": [["Zhang", "Ziming", ""], ["Wu", "Yuanwei", ""], ["Wang", "Guanghui", ""]]}, {"id": "1711.06969", "submitter": "Swami Sankaranarayanan", "authors": "Swami Sankaranarayanan, Yogesh Balaji, Arpit Jain, Ser Nam Lim, Rama\n  Chellappa", "title": "Learning from Synthetic Data: Addressing Domain Shift for Semantic\n  Segmentation", "comments": "Accepted as spotlight talk at CVPR 2018. Code available here:\n  https://github.com/swamiviv/LSD-seg", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Visual Domain Adaptation is a problem of immense importance in computer\nvision. Previous approaches showcase the inability of even deep neural networks\nto learn informative representations across domain shift. This problem is more\nsevere for tasks where acquiring hand labeled data is extremely hard and\ntedious. In this work, we focus on adapting the representations learned by\nsegmentation networks across synthetic and real domains. Contrary to previous\napproaches that use a simple adversarial objective or superpixel information to\naid the process, we propose an approach based on Generative Adversarial\nNetworks (GANs) that brings the embeddings closer in the learned feature space.\nTo showcase the generality and scalability of our approach, we show that we can\nachieve state of the art results on two challenging scenarios of synthetic to\nreal domain adaptation. Additional exploratory experiments show that our\napproach: (1) generalizes to unseen domains and (2) results in improved\nalignment of source and target distributions.\n", "versions": [{"version": "v1", "created": "Sun, 19 Nov 2017 05:25:24 GMT"}, {"version": "v2", "created": "Sun, 1 Apr 2018 21:48:18 GMT"}], "update_date": "2018-04-03", "authors_parsed": [["Sankaranarayanan", "Swami", ""], ["Balaji", "Yogesh", ""], ["Jain", "Arpit", ""], ["Lim", "Ser Nam", ""], ["Chellappa", "Rama", ""]]}, {"id": "1711.06989", "submitter": "George S. Eskander Ekladious PhD", "authors": "Shaunak D. Bopardikar and George S. Eskander Ekladious", "title": "Sequential Randomized Matrix Factorization for Gaussian Processes:\n  Efficient Predictions and Hyper-parameter Optimization", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper presents a sequential randomized lowrank matrix factorization\napproach for incrementally predicting values of an unknown function at test\npoints using the Gaussian Processes framework. It is well-known that in the\nGaussian processes framework, the computational bottlenecks are the inversion\nof the (regularized) kernel matrix and the computation of the hyper-parameters\ndefining the kernel. The main contributions of this paper are two-fold. First,\nwe formalize an approach to compute the inverse of the kernel matrix using\nrandomized matrix factorization algorithms in a streaming scenario, i.e., data\nis generated incrementally over time. The metrics of accuracy and computational\nefficiency of the proposed method are compared against a batch approach based\non use of randomized matrix factorization and an existing streaming approach\nbased on approximating the Gaussian process by a finite set of basis vectors.\nSecond, we extend the sequential factorization approach to a class of kernel\nfunctions for which the hyperparameters can be efficiently optimized. All\nresults are demonstrated on two publicly available datasets.\n", "versions": [{"version": "v1", "created": "Sun, 19 Nov 2017 09:38:33 GMT"}], "update_date": "2017-11-21", "authors_parsed": [["Bopardikar", "Shaunak D.", ""], ["Ekladious", "George S. Eskander", ""]]}, {"id": "1711.07005", "submitter": "Zhifeng Kong", "authors": "Zhifeng Kong", "title": "Convergence Analysis of the Dynamics of a Special Kind of Two-Layered\n  Neural Networks with $\\ell_1$ and $\\ell_2$ Regularization", "comments": "10 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we made an extension to the convergence analysis of the\ndynamics of two-layered bias-free networks with one $ReLU$ output. We took into\nconsideration two popular regularization terms: the $\\ell_1$ and $\\ell_2$ norm\nof the parameter vector $w$, and added it to the square loss function with\ncoefficient $\\lambda/2$. We proved that when $\\lambda$ is small, the weight\nvector $w$ converges to the optimal solution $\\hat{w}$ (with respect to the new\nloss function) with probability $\\geq (1-\\varepsilon)(1-A_d)/2$ under random\ninitiations in a sphere centered at the origin, where $\\varepsilon$ is a small\nvalue and $A_d$ is a constant. Numerical experiments including phase diagrams\nand repeated simulations verified our theory.\n", "versions": [{"version": "v1", "created": "Sun, 19 Nov 2017 11:54:45 GMT"}], "update_date": "2017-11-21", "authors_parsed": [["Kong", "Zhifeng", ""]]}, {"id": "1711.07033", "submitter": "Kian Hsiang Low", "authors": "Trong Nghia Hoang, Quang Minh Hoang, Ruofei Ouyang, Kian Hsiang Low", "title": "Decentralized High-Dimensional Bayesian Optimization with Factor Graphs", "comments": "32nd AAAI Conference on Artificial Intelligence (AAAI 2018), Extended\n  version with proofs, 13 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.DC cs.LG cs.MA", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper presents a novel decentralized high-dimensional Bayesian\noptimization (DEC-HBO) algorithm that, in contrast to existing HBO algorithms,\ncan exploit the interdependent effects of various input components on the\noutput of the unknown objective function f for boosting the BO performance and\nstill preserve scalability in the number of input dimensions without requiring\nprior knowledge or the existence of a low (effective) dimension of the input\nspace. To realize this, we propose a sparse yet rich factor graph\nrepresentation of f to be exploited for designing an acquisition function that\ncan be similarly represented by a sparse factor graph and hence be efficiently\noptimized in a decentralized manner using distributed message passing. Despite\nrichly characterizing the interdependent effects of the input components on the\noutput of f with a factor graph, DEC-HBO can still guarantee no-regret\nperformance asymptotically. Empirical evaluation on synthetic and real-world\nexperiments (e.g., sparse Gaussian process model with 1811 hyperparameters)\nshows that DEC-HBO outperforms the state-of-the-art HBO algorithms.\n", "versions": [{"version": "v1", "created": "Sun, 19 Nov 2017 15:45:53 GMT"}, {"version": "v2", "created": "Tue, 21 Nov 2017 10:08:08 GMT"}, {"version": "v3", "created": "Wed, 24 Jan 2018 18:56:10 GMT"}], "update_date": "2018-01-25", "authors_parsed": [["Hoang", "Trong Nghia", ""], ["Hoang", "Quang Minh", ""], ["Ouyang", "Ruofei", ""], ["Low", "Kian Hsiang", ""]]}, {"id": "1711.07042", "submitter": "Kieran Greer Dr", "authors": "Kieran Greer", "title": "An Improved Oscillating-Error Classifier with Branching", "comments": "This paper is now out of date. You should read 'An Improved Batch\n  Classifier with Bands and Dimensions', arXiv:1811.02617, instead", "journal-ref": "WSEAS Transactions on Computer Research, Vol. 6, pp. 49 - 54.\n  2018. E-ISSN: 2415-1521", "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper extends the earlier work on an oscillating error correction\ntechnique. Specifically, it extends the design to include further corrections,\nby adding new layers to the classifier through a branching method. This\ntechnique is still consistent with earlier work and also neural networks in\ngeneral. With this extended design, the classifier can now achieve the high\nlevels of accuracy reported previously.\n", "versions": [{"version": "v1", "created": "Sun, 19 Nov 2017 16:24:26 GMT"}, {"version": "v2", "created": "Fri, 18 Jan 2019 08:26:31 GMT"}, {"version": "v3", "created": "Thu, 3 Oct 2019 18:09:07 GMT"}], "update_date": "2020-05-20", "authors_parsed": [["Greer", "Kieran", ""]]}, {"id": "1711.07050", "submitter": "Jay Hennig", "authors": "Jay A. Hennig, Akash Umakantha, Ryan C. Williamson", "title": "A Classifying Variational Autoencoder with Application to Polyphonic\n  Music Generation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The variational autoencoder (VAE) is a popular probabilistic generative\nmodel. However, one shortcoming of VAEs is that the latent variables cannot be\ndiscrete, which makes it difficult to generate data from different modes of a\ndistribution. Here, we propose an extension of the VAE framework that\nincorporates a classifier to infer the discrete class of the modeled data. To\nmodel sequential data, we can combine our Classifying VAE with a recurrent\nneural network such as an LSTM. We apply this model to algorithmic music\ngeneration, where our model learns to generate musical sequences in different\nkeys. Most previous work in this area avoids modeling key by transposing data\ninto only one or two keys, as opposed to the 10+ different keys in the original\nmusic. We show that our Classifying VAE and Classifying VAE+LSTM models\noutperform the corresponding non-classifying models in generating musical\nsamples that stay in key. This benefit is especially apparent when trained on\nuntransposed music data in the original keys.\n", "versions": [{"version": "v1", "created": "Sun, 19 Nov 2017 16:48:48 GMT"}], "update_date": "2017-11-21", "authors_parsed": [["Hennig", "Jay A.", ""], ["Umakantha", "Akash", ""], ["Williamson", "Ryan C.", ""]]}, {"id": "1711.07076", "submitter": "Zachary Lipton", "authors": "Zachary C. Lipton, Alexandra Chouldechova, Julian McAuley", "title": "Does mitigating ML's impact disparity require treatment disparity?", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Following related work in law and policy, two notions of disparity have come\nto shape the study of fairness in algorithmic decision-making. Algorithms\nexhibit treatment disparity if they formally treat members of protected\nsubgroups differently; algorithms exhibit impact disparity when outcomes differ\nacross subgroups, even if the correlation arises unintentionally. Naturally, we\ncan achieve impact parity through purposeful treatment disparity. In one thread\nof technical work, papers aim to reconcile the two forms of parity proposing\ndisparate learning processes (DLPs). Here, the learning algorithm can see group\nmembership during training but produce a classifier that is group-blind at test\ntime. In this paper, we show theoretically that: (i) When other features\ncorrelate to group membership, DLPs will (indirectly) implement treatment\ndisparity, undermining the policy desiderata they are designed to address; (ii)\nWhen group membership is partly revealed by other features, DLPs induce\nwithin-class discrimination; and (iii) In general, DLPs provide a suboptimal\ntrade-off between accuracy and impact parity. Based on our technical analysis,\nwe argue that transparent treatment disparity is preferable to occluded methods\nfor achieving impact parity. Experimental results on several real-world\ndatasets highlight the practical consequences of applying DLPs vs. per-group\nthresholds.\n", "versions": [{"version": "v1", "created": "Sun, 19 Nov 2017 20:48:09 GMT"}, {"version": "v2", "created": "Wed, 28 Feb 2018 23:43:38 GMT"}, {"version": "v3", "created": "Fri, 11 Jan 2019 15:03:21 GMT"}], "update_date": "2019-01-14", "authors_parsed": [["Lipton", "Zachary C.", ""], ["Chouldechova", "Alexandra", ""], ["McAuley", "Julian", ""]]}, {"id": "1711.07077", "submitter": "Maria Dimakopoulou", "authors": "Maria Dimakopoulou, Zhengyuan Zhou, Susan Athey, Guido Imbens", "title": "Estimation Considerations in Contextual Bandits", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG econ.EM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Contextual bandit algorithms are sensitive to the estimation method of the\noutcome model as well as the exploration method used, particularly in the\npresence of rich heterogeneity or complex outcome models, which can lead to\ndifficult estimation problems along the path of learning. We study a\nconsideration for the exploration vs. exploitation framework that does not\narise in multi-armed bandits but is crucial in contextual bandits; the way\nexploration and exploitation is conducted in the present affects the bias and\nvariance in the potential outcome model estimation in subsequent stages of\nlearning. We develop parametric and non-parametric contextual bandits that\nintegrate balancing methods from the causal inference literature in their\nestimation to make it less prone to problems of estimation bias. We provide the\nfirst regret bound analyses for contextual bandits with balancing in the domain\nof linear contextual bandits that match the state of the art regret bounds. We\ndemonstrate the strong practical advantage of balanced contextual bandits on a\nlarge number of supervised learning datasets and on a synthetic example that\nsimulates model mis-specification and prejudice in the initial training data.\nAdditionally, we develop contextual bandits with simpler assignment policies by\nleveraging sparse model estimation methods from the econometrics literature and\ndemonstrate empirically that in the early stages they can improve the rate of\nlearning and decrease regret.\n", "versions": [{"version": "v1", "created": "Sun, 19 Nov 2017 20:49:47 GMT"}, {"version": "v2", "created": "Fri, 2 Mar 2018 00:57:42 GMT"}, {"version": "v3", "created": "Fri, 13 Jul 2018 11:14:23 GMT"}, {"version": "v4", "created": "Sun, 16 Dec 2018 07:50:33 GMT"}], "update_date": "2018-12-18", "authors_parsed": [["Dimakopoulou", "Maria", ""], ["Zhou", "Zhengyuan", ""], ["Athey", "Susan", ""], ["Imbens", "Guido", ""]]}, {"id": "1711.07099", "submitter": "Leonardo Rey Vega", "authors": "Mat\\'ias Vera, Leonardo Rey Vega, Pablo Piantanida", "title": "Compression-Based Regularization with an Application to Multi-Task\n  Learning", "comments": "13 pages, 7 figures. Submitted for publication", "journal-ref": null, "doi": "10.1109/JSTSP.2018.2846218", "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper investigates, from information theoretic grounds, a learning\nproblem based on the principle that any regularity in a given dataset can be\nexploited to extract compact features from data, i.e., using fewer bits than\nneeded to fully describe the data itself, in order to build meaningful\nrepresentations of a relevant content (multiple labels). We begin by\nintroducing the noisy lossy source coding paradigm with the log-loss fidelity\ncriterion which provides the fundamental tradeoffs between the\n\\emph{cross-entropy loss} (average risk) and the information rate of the\nfeatures (model complexity). Our approach allows an information theoretic\nformulation of the \\emph{multi-task learning} (MTL) problem which is a\nsupervised learning framework in which the prediction models for several\nrelated tasks are learned jointly from common representations to achieve better\ngeneralization performance. Then, we present an iterative algorithm for\ncomputing the optimal tradeoffs and its global convergence is proven provided\nthat some conditions hold. An important property of this algorithm is that it\nprovides a natural safeguard against overfitting, because it minimizes the\naverage risk taking into account a penalization induced by the model\ncomplexity. Remarkably, empirical results illustrate that there exists an\noptimal information rate minimizing the \\emph{excess risk} which depends on the\nnature and the amount of available training data. An application to\nhierarchical text categorization is also investigated, extending previous\nworks.\n", "versions": [{"version": "v1", "created": "Sun, 19 Nov 2017 23:07:18 GMT"}], "update_date": "2018-11-14", "authors_parsed": [["Vera", "Mat\u00edas", ""], ["Vega", "Leonardo Rey", ""], ["Piantanida", "Pablo", ""]]}, {"id": "1711.07104", "submitter": "Skyler Seto", "authors": "Skyler Seto, Sarah Tan, Giles Hooker, and Martin T. Wells", "title": "A Double Parametric Bootstrap Test for Topic Models", "comments": "Presented at NIPS 2017 Symposium on Interpretable Machine Learning", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Non-negative matrix factorization (NMF) is a technique for finding latent\nrepresentations of data. The method has been applied to corpora to construct\ntopic models. However, NMF has likelihood assumptions which are often violated\nby real document corpora. We present a double parametric bootstrap test for\nevaluating the fit of an NMF-based topic model based on the duality of the KL\ndivergence and Poisson maximum likelihood estimation. The test correctly\nidentifies whether a topic model based on an NMF approach yields reliable\nresults in simulated and real data.\n", "versions": [{"version": "v1", "created": "Sun, 19 Nov 2017 23:33:27 GMT"}, {"version": "v2", "created": "Tue, 21 Nov 2017 03:18:02 GMT"}], "update_date": "2017-11-22", "authors_parsed": [["Seto", "Skyler", ""], ["Tan", "Sarah", ""], ["Hooker", "Giles", ""], ["Wells", "Martin T.", ""]]}, {"id": "1711.07168", "submitter": "Dilin Wang", "authors": "Dilin Wang, Zhe Zeng, Qiang Liu", "title": "Stein Variational Message Passing for Continuous Graphical Models", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a novel distributed inference algorithm for continuous graphical\nmodels, by extending Stein variational gradient descent (SVGD) to leverage the\nMarkov dependency structure of the distribution of interest. Our approach\ncombines SVGD with a set of structured local kernel functions defined on the\nMarkov blanket of each node, which alleviates the curse of high dimensionality\nand simultaneously yields a distributed algorithm for decentralized inference\ntasks. We justify our method with theoretical analysis and show that the use of\nlocal kernels can be viewed as a new type of localized approximation that\nmatches the target distribution on the conditional distributions of each node\nover its Markov blanket. Our empirical results show that our method outperforms\na variety of baselines including standard MCMC and particle message passing\nmethods.\n", "versions": [{"version": "v1", "created": "Mon, 20 Nov 2017 06:25:16 GMT"}, {"version": "v2", "created": "Sun, 18 Feb 2018 05:32:02 GMT"}, {"version": "v3", "created": "Thu, 7 Jun 2018 22:20:13 GMT"}], "update_date": "2018-06-11", "authors_parsed": [["Wang", "Dilin", ""], ["Zeng", "Zhe", ""], ["Liu", "Qiang", ""]]}, {"id": "1711.07230", "submitter": "Mohamad Kazem Shirani Faradonbeh", "authors": "Mohamad Kazem Shirani Faradonbeh, Ambuj Tewari, George Michailidis", "title": "Optimism-Based Adaptive Regulation of Linear-Quadratic Systems", "comments": "28 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.SY math.OC stat.AP stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The main challenge for adaptive regulation of linear-quadratic systems is the\ntrade-off between identification and control. An adaptive policy needs to\naddress both the estimation of unknown dynamics parameters (exploration), as\nwell as the regulation of the underlying system (exploitation). To this end,\noptimism-based methods which bias the identification in favor of optimistic\napproximations of the true parameter are employed in the literature. A number\nof asymptotic results have been established, but their finite time counterparts\nare few, with important restrictions.\n  This study establishes results for the worst-case regret of optimism-based\nadaptive policies. The presented high probability upper bounds are optimal up\nto logarithmic factors. The non-asymptotic analysis of this work requires very\nmild assumptions; (i) stabilizability of the system's dynamics, and (ii)\nlimiting the degree of heaviness of the noise distribution. To establish such\nbounds, certain novel techniques are developed to comprehensively address the\nprobabilistic behavior of dependent random matrices with heavy-tailed\ndistributions.\n", "versions": [{"version": "v1", "created": "Mon, 20 Nov 2017 09:52:25 GMT"}, {"version": "v2", "created": "Sun, 29 Jul 2018 15:54:41 GMT"}, {"version": "v3", "created": "Fri, 29 Mar 2019 01:56:55 GMT"}], "update_date": "2019-04-01", "authors_parsed": [["Faradonbeh", "Mohamad Kazem Shirani", ""], ["Tewari", "Ambuj", ""], ["Michailidis", "George", ""]]}, {"id": "1711.07271", "submitter": "Micha\\\"el Fanuel", "authors": "Micha\\\"el Fanuel, Antoine Aspeel, Jean-Charles Delvenne, Johan A.K.\n  Suykens", "title": "Positive semi-definite embedding for dimensionality reduction and\n  out-of-sample extensions", "comments": "16 pages, 5 figures. Improved presentation", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In machine learning or statistics, it is often desirable to reduce the\ndimensionality of a sample of data points in a high dimensional space\n$\\mathbb{R}^d$. This paper introduces a dimensionality reduction method where\nthe embedding coordinates are the eigenvectors of a positive semi-definite\nkernel obtained as the solution of an infinite dimensional analogue of a\nsemi-definite program. This embedding is adaptive and non-linear. A main\nfeature of our approach is the existence of a non-linear out-of-sample\nextension formula of the embedding coordinates, called a projected Nystr\\\"om\napproximation. This extrapolation formula yields an extension of the kernel\nmatrix to a data-dependent Mercer kernel function. Our empirical results\nindicate that this embedding method is more robust with respect to the\ninfluence of outliers, compared with a spectral embedding method.\n", "versions": [{"version": "v1", "created": "Mon, 20 Nov 2017 12:04:37 GMT"}, {"version": "v2", "created": "Tue, 21 Nov 2017 10:02:09 GMT"}, {"version": "v3", "created": "Tue, 6 Oct 2020 13:15:06 GMT"}], "update_date": "2020-10-07", "authors_parsed": [["Fanuel", "Micha\u00ebl", ""], ["Aspeel", "Antoine", ""], ["Delvenne", "Jean-Charles", ""], ["Suykens", "Johan A. K.", ""]]}, {"id": "1711.07274", "submitter": "Chung-Cheng Chiu", "authors": "Chung-Cheng Chiu, Anshuman Tripathi, Katherine Chou, Chris Co, Navdeep\n  Jaitly, Diana Jaunzeikare, Anjuli Kannan, Patrick Nguyen, Hasim Sak, Ananth\n  Sankar, Justin Tansuwan, Nathan Wan, Yonghui Wu, Xuedong Zhang", "title": "Speech recognition for medical conversations", "comments": "Interspeech 2018 camera ready", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.SD eess.AS stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this work we explored building automatic speech recognition models for\ntranscribing doctor patient conversation. We collected a large scale dataset of\nclinical conversations ($14,000$ hr), designed the task to represent the real\nword scenario, and explored several alignment approaches to iteratively improve\ndata quality. We explored both CTC and LAS systems for building speech\nrecognition models. The LAS was more resilient to noisy data and CTC required\nmore data clean up. A detailed analysis is provided for understanding the\nperformance for clinical tasks. Our analysis showed the speech recognition\nmodels performed well on important medical utterances, while errors occurred in\ncausal conversations. Overall we believe the resulting models can provide\nreasonable quality in practice.\n", "versions": [{"version": "v1", "created": "Mon, 20 Nov 2017 12:07:22 GMT"}, {"version": "v2", "created": "Wed, 20 Jun 2018 17:54:30 GMT"}], "update_date": "2018-06-21", "authors_parsed": [["Chiu", "Chung-Cheng", ""], ["Tripathi", "Anshuman", ""], ["Chou", "Katherine", ""], ["Co", "Chris", ""], ["Jaitly", "Navdeep", ""], ["Jaunzeikare", "Diana", ""], ["Kannan", "Anjuli", ""], ["Nguyen", "Patrick", ""], ["Sak", "Hasim", ""], ["Sankar", "Ananth", ""], ["Tansuwan", "Justin", ""], ["Wan", "Nathan", ""], ["Wu", "Yonghui", ""], ["Zhang", "Xuedong", ""]]}, {"id": "1711.07287", "submitter": "Giuseppe Di Benedetto", "authors": "Giuseppe Di Benedetto, Fran\\c{c}ois Caron, Yee Whye Teh", "title": "Non-exchangeable random partition models for microclustering", "comments": "20 pages, 18 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Many popular random partition models, such as the Chinese restaurant process\nand its two-parameter extension, fall in the class of exchangeable random\npartitions, and have found wide applicability in model-based clustering,\npopulation genetics, ecology or network analysis. While the exchangeability\nassumption is sensible in many cases, it has some strong implications. In\nparticular, Kingman's representation theorem implies that the size of the\nclusters necessarily grows linearly with the sample size; this feature may be\nundesirable for some applications, as recently pointed out by Miller et al.\n(2015). We present here a flexible class of non-exchangeable random partition\nmodels which are able to generate partitions whose cluster sizes grow\nsublinearly with the sample size, and where the growth rate is controlled by\none parameter. Along with this result, we provide the asymptotic behaviour of\nthe number of clusters of a given size, and show that the model can exhibit a\npower-law behavior, controlled by another parameter. The construction is based\non completely random measures and a Poisson embedding of the random partition,\nand inference is performed using a Sequential Monte Carlo algorithm.\nAdditionally, we show how the model can also be directly used to generate\nsparse multigraphs with power-law degree distributions and degree sequences\nwith sublinear growth. Finally, experiments on real datasets emphasize the\nusefulness of the approach compared to a two-parameter Chinese restaurant\nprocess.\n", "versions": [{"version": "v1", "created": "Mon, 20 Nov 2017 12:45:36 GMT"}], "update_date": "2017-11-21", "authors_parsed": [["Di Benedetto", "Giuseppe", ""], ["Caron", "Fran\u00e7ois", ""], ["Teh", "Yee Whye", ""]]}, {"id": "1711.07354", "submitter": "Ziming Zhang", "authors": "Ziming Zhang and Matthew Brand", "title": "Convergent Block Coordinate Descent for Training Tikhonov Regularized\n  Deep Neural Networks", "comments": "NIPS 2017", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  By lifting the ReLU function into a higher dimensional space, we develop a\nsmooth multi-convex formulation for training feed-forward deep neural networks\n(DNNs). This allows us to develop a block coordinate descent (BCD) training\nalgorithm consisting of a sequence of numerically well-behaved convex\noptimizations. Using ideas from proximal point methods in convex analysis, we\nprove that this BCD algorithm will converge globally to a stationary point with\nR-linear convergence rate of order one. In experiments with the MNIST database,\nDNNs trained with this BCD algorithm consistently yielded better test-set error\nrates than identical DNN architectures trained via all the stochastic gradient\ndescent (SGD) variants in the Caffe toolbox.\n", "versions": [{"version": "v1", "created": "Mon, 20 Nov 2017 15:04:45 GMT"}], "update_date": "2017-11-21", "authors_parsed": [["Zhang", "Ziming", ""], ["Brand", "Matthew", ""]]}, {"id": "1711.07364", "submitter": "Jarom\\'ir Janisch", "authors": "Jarom\\'ir Janisch, Tom\\'a\\v{s} Pevn\\'y and Viliam Lis\\'y", "title": "Classification with Costly Features using Deep Reinforcement Learning", "comments": "AAAI 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study a classification problem where each feature can be acquired for a\ncost and the goal is to optimize a trade-off between the expected\nclassification error and the feature cost. We revisit a former approach that\nhas framed the problem as a sequential decision-making problem and solved it by\nQ-learning with a linear approximation, where individual actions are either\nrequests for feature values or terminate the episode by providing a\nclassification decision. On a set of eight problems, we demonstrate that by\nreplacing the linear approximation with neural networks the approach becomes\ncomparable to the state-of-the-art algorithms developed specifically for this\nproblem. The approach is flexible, as it can be improved with any new\nreinforcement learning enhancement, it allows inclusion of pre-trained\nhigh-performance classifier, and unlike prior art, its performance is robust\nacross all evaluated datasets.\n", "versions": [{"version": "v1", "created": "Mon, 20 Nov 2017 15:14:29 GMT"}, {"version": "v2", "created": "Mon, 12 Nov 2018 17:09:14 GMT"}], "update_date": "2018-11-13", "authors_parsed": [["Janisch", "Jarom\u00edr", ""], ["Pevn\u00fd", "Tom\u00e1\u0161", ""], ["Lis\u00fd", "Viliam", ""]]}, {"id": "1711.07414", "submitter": "Bernease Herman", "authors": "Bernease Herman", "title": "The Promise and Peril of Human Evaluation for Model Interpretability", "comments": "Presented at NIPS 2017 Symposium on Interpretable Machine Learning.\n  I'm not happy with the writing and presentation of these ideas and hope to\n  submit an updated and extended version in 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Transparency, user trust, and human comprehension are popular ethical\nmotivations for interpretable machine learning. In support of these goals,\nresearchers evaluate model explanation performance using humans and real world\napplications. This alone presents a challenge in many areas of artificial\nintelligence. In this position paper, we propose a distinction between\ndescriptive and persuasive explanations. We discuss reasoning suggesting that\nfunctional interpretability may be correlated with cognitive function and user\npreferences. If this is indeed the case, evaluation and optimization using\nfunctional metrics could perpetuate implicit cognitive bias in explanations\nthat threaten transparency. Finally, we propose two potential research\ndirections to disambiguate cognitive function and explanation models, retaining\ncontrol over the tradeoff between accuracy and interpretability.\n", "versions": [{"version": "v1", "created": "Mon, 20 Nov 2017 17:05:11 GMT"}, {"version": "v2", "created": "Wed, 30 Oct 2019 13:01:44 GMT"}], "update_date": "2019-10-31", "authors_parsed": [["Herman", "Bernease", ""]]}, {"id": "1711.07425", "submitter": "Kevin Feigelis", "authors": "Kevin T. Feigelis, Blue Sheffer, Daniel L. K. Yamins", "title": "Modular Continual Learning in a Unified Visual Environment", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI q-bio.NC stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A core aspect of human intelligence is the ability to learn new tasks quickly\nand switch between them flexibly. Here, we describe a modular continual\nreinforcement learning paradigm inspired by these abilities. We first introduce\na visual interaction environment that allows many types of tasks to be unified\nin a single framework. We then describe a reward map prediction scheme that\nlearns new tasks robustly in the very large state and action spaces required by\nsuch an environment. We investigate how properties of module architecture\ninfluence efficiency of task learning, showing that a module motif\nincorporating specific design principles (e.g. early bottlenecks, low-order\npolynomial nonlinearities, and symmetry) significantly outperforms more\nstandard neural network motifs, needing fewer training examples and fewer\nneurons to achieve high levels of performance. Finally, we present a\nmeta-controller architecture for task switching based on a dynamic neural\nvoting scheme, which allows new modules to use information learned from\npreviously-seen tasks to substantially improve their own learning efficiency.\n", "versions": [{"version": "v1", "created": "Mon, 20 Nov 2017 17:31:12 GMT"}, {"version": "v2", "created": "Tue, 12 Dec 2017 04:31:00 GMT"}], "update_date": "2017-12-13", "authors_parsed": [["Feigelis", "Kevin T.", ""], ["Sheffer", "Blue", ""], ["Yamins", "Daniel L. K.", ""]]}, {"id": "1711.07433", "submitter": "Taewan Kim", "authors": "Taewan Kim, Joydeep Ghosh", "title": "Relaxed Oracles for Semi-Supervised Clustering", "comments": "NIPS 2017 Workshop: Learning with Limited Labeled Data (LLD 2017)", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Pairwise \"same-cluster\" queries are one of the most widely used forms of\nsupervision in semi-supervised clustering. However, it is impractical to ask\nhuman oracles to answer every query correctly. In this paper, we study the\ninfluence of allowing \"not-sure\" answers from a weak oracle and propose an\neffective algorithm to handle such uncertainties in query responses. Two\nrealistic weak oracle models are considered where ambiguity in answering\ndepends on the distance between two points. We show that a small query\ncomplexity is adequate for effective clustering with high probability by\nproviding better pairs to the weak oracle. Experimental results on synthetic\nand real data show the effectiveness of our approach in overcoming supervision\nuncertainties and yielding high quality clusters.\n", "versions": [{"version": "v1", "created": "Mon, 20 Nov 2017 17:40:50 GMT"}], "update_date": "2017-11-21", "authors_parsed": [["Kim", "Taewan", ""], ["Ghosh", "Joydeep", ""]]}, {"id": "1711.07441", "submitter": "Kejun Huang", "authors": "Kejun Huang, Xiao Fu, Nicholas D. Sidiropoulos", "title": "On Convergence of Epanechnikov Mean Shift", "comments": "AAAI 2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG math.OC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Epanechnikov Mean Shift is a simple yet empirically very effective algorithm\nfor clustering. It localizes the centroids of data clusters via estimating\nmodes of the probability distribution that generates the data points, using the\n`optimal' Epanechnikov kernel density estimator. However, since the procedure\ninvolves non-smooth kernel density functions, the convergence behavior of\nEpanechnikov mean shift lacks theoretical support as of this writing---most of\nthe existing analyses are based on smooth functions and thus cannot be applied\nto Epanechnikov Mean Shift. In this work, we first show that the original\nEpanechnikov Mean Shift may indeed terminate at a non-critical point, due to\nthe non-smoothness nature. Based on our analysis, we propose a simple remedy to\nfix it. The modified Epanechnikov Mean Shift is guaranteed to terminate at a\nlocal maximum of the estimated density, which corresponds to a cluster\ncentroid, within a finite number of iterations. We also propose a way to avoid\nrunning the Mean Shift iterates from every data point, while maintaining good\nclustering accuracies under non-overlapping spherical Gaussian mixture models.\nThis further pushes Epanechnikov Mean Shift to handle very large and\nhigh-dimensional data sets. Experiments show surprisingly good performance\ncompared to the Lloyd's K-means algorithm and the EM algorithm.\n", "versions": [{"version": "v1", "created": "Mon, 20 Nov 2017 17:51:01 GMT"}], "update_date": "2017-11-21", "authors_parsed": [["Huang", "Kejun", ""], ["Fu", "Xiao", ""], ["Sidiropoulos", "Nicholas D.", ""]]}, {"id": "1711.07446", "submitter": "Avgoustinos Vouros", "authors": "Avgoustinos Vouros, Tiago V. Gehring, Kinga Szydlowska, Artur Janusz,\n  Mike Croucher, Katarzyna Lukasiuk, Witold Konopka, Carmen Sandi, Zehai Tu,\n  Eleni Vasilaki", "title": "A generalised framework for detailed classification of swimming paths\n  inside the Morris Water Maze", "comments": null, "journal-ref": "Scientific Reports volume 8, Article number: 15089 (2018)", "doi": "10.1038/s41598-018-33456-1", "report-no": null, "categories": "q-bio.QM cs.AI stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The Morris Water Maze is commonly used in behavioural neuroscience for the\nstudy of spatial learning with rodents. Over the years, various methods of\nanalysing rodent data collected in this task have been proposed. These methods\nspan from classical performance measurements (e.g. escape latency, rodent\nspeed, quadrant preference) to more sophisticated methods of categorisation\nwhich classify the animal swimming path into behavioural classes known as\nstrategies. Classification techniques provide additional insight in relation to\nthe actual animal behaviours but still only a limited amount of studies utilise\nthem mainly because they highly depend on machine learning knowledge. We have\npreviously demonstrated that the animals implement various strategies and by\nclassifying whole trajectories can lead to the loss of important information.\nIn this work, we developed a generalised and robust classification methodology\nwhich implements majority voting to boost the classification performance and\nsuccessfully nullify the need of manual tuning. Based on this framework, we\nbuilt a complete software, capable of performing the full analysis described in\nthis paper. The software provides an easy to use graphical user interface (GUI)\nthrough which users can enter their trajectory data, segment and label them and\nfinally generate reports and figures of the results.\n", "versions": [{"version": "v1", "created": "Mon, 20 Nov 2017 18:12:19 GMT"}, {"version": "v2", "created": "Mon, 18 Dec 2017 18:25:17 GMT"}], "update_date": "2018-10-11", "authors_parsed": [["Vouros", "Avgoustinos", ""], ["Gehring", "Tiago V.", ""], ["Szydlowska", "Kinga", ""], ["Janusz", "Artur", ""], ["Croucher", "Mike", ""], ["Lukasiuk", "Katarzyna", ""], ["Konopka", "Witold", ""], ["Sandi", "Carmen", ""], ["Tu", "Zehai", ""], ["Vasilaki", "Eleni", ""]]}, {"id": "1711.07461", "submitter": "Ayush Jaiswal", "authors": "Ayush Jaiswal, Wael AbdAlmageed, Yue Wu, Premkumar Natarajan", "title": "Bidirectional Conditional Generative Adversarial Networks", "comments": "To appear in Proceedings of ACCV 2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Conditional Generative Adversarial Networks (cGANs) are generative models\nthat can produce data samples ($x$) conditioned on both latent variables ($z$)\nand known auxiliary information ($c$). We propose the Bidirectional cGAN\n(BiCoGAN), which effectively disentangles $z$ and $c$ in the generation process\nand provides an encoder that learns inverse mappings from $x$ to both $z$ and\n$c$, trained jointly with the generator and the discriminator. We present\ncrucial techniques for training BiCoGANs, which involve an extrinsic factor\nloss along with an associated dynamically-tuned importance weight. As compared\nto other encoder-based cGANs, BiCoGANs encode $c$ more accurately, and utilize\n$z$ and $c$ more effectively and in a more disentangled way to generate\nsamples.\n", "versions": [{"version": "v1", "created": "Mon, 20 Nov 2017 18:54:05 GMT"}, {"version": "v2", "created": "Thu, 15 Mar 2018 01:23:01 GMT"}, {"version": "v3", "created": "Sat, 27 Oct 2018 07:42:44 GMT"}, {"version": "v4", "created": "Sat, 3 Nov 2018 23:03:07 GMT"}], "update_date": "2018-11-06", "authors_parsed": [["Jaiswal", "Ayush", ""], ["AbdAlmageed", "Wael", ""], ["Wu", "Yue", ""], ["Natarajan", "Premkumar", ""]]}, {"id": "1711.07468", "submitter": "Daniel George", "authors": "Daniel George, Hongyu Shen, E. A. Huerta", "title": "Glitch Classification and Clustering for LIGO with Deep Transfer\n  Learning", "comments": "Camera-ready (final) paper accepted to NIPS 2017 conference workshop\n  on Deep Learning for Physical Sciences. Extended article: arXiv:1706.07446", "journal-ref": "Phys. Rev. D 97, 101501 (2018)", "doi": "10.1103/PhysRevD.97.101501", "report-no": null, "categories": "astro-ph.IM cs.LG gr-qc stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The detection of gravitational waves with LIGO and Virgo requires a detailed\nunderstanding of the response of these instruments in the presence of\nenvironmental and instrumental noise. Of particular interest is the study of\nanomalous non-Gaussian noise transients known as glitches, since their high\noccurrence rate in LIGO/Virgo data can obscure or even mimic true gravitational\nwave signals. Therefore, successfully identifying and excising glitches is of\nutmost importance to detect and characterize gravitational waves. In this\narticle, we present the first application of Deep Learning combined with\nTransfer Learning for glitch classification, using real data from LIGO's first\ndiscovery campaign labeled by Gravity Spy, showing that knowledge from\npre-trained models for real-world object recognition can be transferred for\nclassifying spectrograms of glitches. We demonstrate that this method enables\nthe optimal use of very deep convolutional neural networks for glitch\nclassification given small unbalanced training datasets, significantly reduces\nthe training time, and achieves state-of-the-art accuracy above 98.8%. Once\ntrained via transfer learning, we show that the networks can be truncated and\nused as feature extractors for unsupervised clustering to automatically group\ntogether new classes of glitches and anomalies. This novel capability is of\ncritical importance to identify and remove new types of glitches which will\noccur as the LIGO/Virgo detectors gradually attain design sensitivity.\n", "versions": [{"version": "v1", "created": "Mon, 20 Nov 2017 18:58:28 GMT"}, {"version": "v2", "created": "Mon, 11 Dec 2017 18:11:34 GMT"}], "update_date": "2018-05-30", "authors_parsed": [["George", "Daniel", ""], ["Shen", "Hongyu", ""], ["Huerta", "E. A.", ""]]}, {"id": "1711.07476", "submitter": "Saki Shinoda", "authors": "Saki Shinoda, Daniel E. Worrall, Gabriel J. Brostow", "title": "Virtual Adversarial Ladder Networks For Semi-supervised Learning", "comments": "Camera-ready version for NIPS 2017 workshop Learning with Limited\n  Labeled Data", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Semi-supervised learning (SSL) partially circumvents the high cost of\nlabeling data by augmenting a small labeled dataset with a large and relatively\ncheap unlabeled dataset drawn from the same distribution. This paper offers a\nnovel interpretation of two deep learning-based SSL approaches, ladder networks\nand virtual adversarial training (VAT), as applying distributional smoothing to\ntheir respective latent spaces. We propose a class of models that fuse these\napproaches. We achieve near-supervised accuracy with high consistency on the\nMNIST dataset using just 5 labels per class: our best model, ladder with\nlayer-wise virtual adversarial noise (LVAN-LW), achieves 1.42% +/- 0.12 average\nerror rate on the MNIST test set, in comparison with 1.62% +/- 0.65 reported\nfor the ladder network. On adversarial examples generated with L2-normalized\nfast gradient method, LVAN-LW trained with 5 examples per class achieves\naverage error rate 2.4% +/- 0.3 compared to 68.6% +/- 6.5 for the ladder\nnetwork and 9.9% +/- 7.5 for VAT.\n", "versions": [{"version": "v1", "created": "Mon, 20 Nov 2017 11:10:40 GMT"}, {"version": "v2", "created": "Tue, 12 Dec 2017 11:23:01 GMT"}], "update_date": "2017-12-13", "authors_parsed": [["Shinoda", "Saki", ""], ["Worrall", "Daniel E.", ""], ["Brostow", "Gabriel J.", ""]]}, {"id": "1711.07479", "submitter": "Gino Brunner", "authors": "Gino Brunner, Oliver Richter, Yuyi Wang, Roger Wattenhofer", "title": "Teaching a Machine to Read Maps with Deep Reinforcement Learning", "comments": "Paper accepted at 32nd AAAI Conference on Artificial Intelligence,\n  AAAI 2018, New Orleans, Louisiana, USA", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.RO cs.AI cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The ability to use a 2D map to navigate a complex 3D environment is quite\nremarkable, and even difficult for many humans. Localization and navigation is\nalso an important problem in domains such as robotics, and has recently become\na focus of the deep reinforcement learning community. In this paper we teach a\nreinforcement learning agent to read a map in order to find the shortest way\nout of a random maze it has never seen before. Our system combines several\nstate-of-the-art methods such as A3C and incorporates novel elements such as a\nrecurrent localization cell. Our agent learns to localize itself based on 3D\nfirst person images and an approximate orientation angle. The agent generalizes\nwell to bigger mazes, showing that it learned useful localization and\nnavigation capabilities.\n", "versions": [{"version": "v1", "created": "Mon, 20 Nov 2017 16:45:58 GMT"}], "update_date": "2017-11-22", "authors_parsed": [["Brunner", "Gino", ""], ["Richter", "Oliver", ""], ["Wang", "Yuyi", ""], ["Wattenhofer", "Roger", ""]]}, {"id": "1711.07511", "submitter": "Matthew Norton", "authors": "Matthew Norton and Akiko Takeda and Alexander Mafusalov", "title": "Optimistic Robust Optimization With Applications To Machine Learning", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG math.OC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Robust Optimization has traditionally taken a pessimistic, or worst-case\nviewpoint of uncertainty which is motivated by a desire to find sets of optimal\npolicies that maintain feasibility under a variety of operating conditions. In\nthis paper, we explore an optimistic, or best-case view of uncertainty and show\nthat it can be a fruitful approach. We show that these techniques can be used\nto address a wide variety of problems. First, we apply our methods in the\ncontext of robust linear programming, providing a method for reducing\nconservatism in intuitive ways that encode economically realistic modeling\nassumptions. Second, we look at problems in machine learning and find that this\napproach is strongly connected to the existing literature. Specifically, we\nprovide a new interpretation for popular sparsity inducing non-convex\nregularization schemes. Additionally, we show that successful approaches for\ndealing with outliers and noise can be interpreted as optimistic robust\noptimization problems. Although many of the problems resulting from our\napproach are non-convex, we find that DCA or DCA-like optimization approaches\ncan be intuitive and efficient.\n", "versions": [{"version": "v1", "created": "Mon, 20 Nov 2017 19:39:48 GMT"}], "update_date": "2017-11-22", "authors_parsed": [["Norton", "Matthew", ""], ["Takeda", "Akiko", ""], ["Mafusalov", "Alexander", ""]]}, {"id": "1711.07527", "submitter": "Christoph Kurz", "authors": "Christoph Kurz, Laura Hatfield", "title": "Subgroup Identification and Interpretation with Bayesian Nonparametric\n  Models in Health Care Claims Data", "comments": "NIPS symposium Interpretable Machine Learning 2017", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Inpatient care is a large share of total health care spending, making\nanalysis of inpatient utilization patterns an important part of understanding\nwhat drives health care spending growth. Common features of inpatient\nutilization measures include zero inflation, over-dispersion, and skewness, all\nof which complicate statistical modeling. Mixture modeling is a popular\napproach that can accommodate these features of health care utilization data.\nIn this work, we add a nonparametric clustering component to such models. Our\nfully Bayesian model framework allows for an unknown number of mixing\ncomponents, so that the data determine the number of mixture components. When\nwe apply the modeling framework to data on hospital lengths of stay for\npatients with lung cancer, we find distinct subgroups of patients with\ndifferences in means and variances of hospital days, health and treatment\ncovariates, and relationships between covariates and length of stay.\n", "versions": [{"version": "v1", "created": "Mon, 20 Nov 2017 20:22:36 GMT"}], "update_date": "2017-11-22", "authors_parsed": [["Kurz", "Christoph", ""], ["Hatfield", "Laura", ""]]}, {"id": "1711.07553", "submitter": "Xavier Bresson", "authors": "Xavier Bresson and Thomas Laurent", "title": "Residual Gated Graph ConvNets", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Graph-structured data such as social networks, functional brain networks,\ngene regulatory networks, communications networks have brought the interest in\ngeneralizing deep learning techniques to graph domains. In this paper, we are\ninterested to design neural networks for graphs with variable length in order\nto solve learning problems such as vertex classification, graph classification,\ngraph regression, and graph generative tasks. Most existing works have focused\non recurrent neural networks (RNNs) to learn meaningful representations of\ngraphs, and more recently new convolutional neural networks (ConvNets) have\nbeen introduced. In this work, we want to compare rigorously these two\nfundamental families of architectures to solve graph learning tasks. We review\nexisting graph RNN and ConvNet architectures, and propose natural extension of\nLSTM and ConvNet to graphs with arbitrary size. Then, we design a set of\nanalytically controlled experiments on two basic graph problems, i.e. subgraph\nmatching and graph clustering, to test the different architectures. Numerical\nresults show that the proposed graph ConvNets are 3-17% more accurate and\n1.5-4x faster than graph RNNs. Graph ConvNets are also 36% more accurate than\nvariational (non-learning) techniques. Finally, the most effective graph\nConvNet architecture uses gated edges and residuality. Residuality plays an\nessential role to learn multi-layer architectures as they provide a 10% gain of\nperformance.\n", "versions": [{"version": "v1", "created": "Mon, 20 Nov 2017 21:28:40 GMT"}, {"version": "v2", "created": "Tue, 24 Apr 2018 08:19:32 GMT"}], "update_date": "2018-04-25", "authors_parsed": [["Bresson", "Xavier", ""], ["Laurent", "Thomas", ""]]}, {"id": "1711.07561", "submitter": "Namjoon Suh", "authors": "Namjoon Suh", "title": "Review on Parameter Estimation in HMRF", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This is a technical report which explores the estimation methodologies on\nhyper-parameters in Markov Random Field and Gaussian Hidden Markov Random\nField. In first section, we briefly investigate a theoretical framework on\nMetropolis-Hastings algorithm. Next, by using MH algorithm, we simulate the\ndata from Ising model, and study on how hyper-parameter estimation in Ising\nmodel is enabled through MCMC algorithm using pseudo-likelihood approximation.\nFollowing section deals with an issue on parameters estimation process of\nGaussian Hidden Markov Random Field using MAP estimation and EM algorithm, and\nalso discusses problems, found through several experiments. In following\nsection, we expand this idea on estimating parameters in Gaussian Hidden Markov\nSpatial-Temporal Random Field, and display results on two performed\nexperiments.\n", "versions": [{"version": "v1", "created": "Mon, 20 Nov 2017 21:50:51 GMT"}], "update_date": "2017-11-22", "authors_parsed": [["Suh", "Namjoon", ""]]}, {"id": "1711.07575", "submitter": "Ronak Mehta", "authors": "Ronak Mehta, Hyunwoo J. Kim, Shulei Wang, Sterling C. Johnson, Ming\n  Yuan, Vikas Singh", "title": "Finding Differentially Covarying Needles in a Temporally Evolving\n  Haystack: A Scan Statistics Perspective", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Recent results in coupled or temporal graphical models offer schemes for\nestimating the relationship structure between features when the data come from\nrelated (but distinct) longitudinal sources. A novel application of these ideas\nis for analyzing group-level differences, i.e., in identifying if trends of\nestimated objects (e.g., covariance or precision matrices) are different across\ndisparate conditions (e.g., gender or disease). Often, poor effect sizes make\ndetecting the differential signal over the full set of features difficult: for\nexample, dependencies between only a subset of features may manifest\ndifferently across groups. In this work, we first give a parametric model for\nestimating trends in the space of SPD matrices as a function of one or more\ncovariates. We then generalize scan statistics to graph structures, to search\nover distinct subsets of features (graph partitions) whose temporal dependency\nstructure may show statistically significant group-wise differences. We\ntheoretically analyze the Family Wise Error Rate (FWER) and bounds on Type 1\nand Type 2 error. On a cohort of individuals with risk factors for Alzheimer's\ndisease (but otherwise cognitively healthy), we find scientifically interesting\ngroup differences where the default analysis, i.e., models estimated on the\nfull graph, do not survive reasonable significance thresholds.\n", "versions": [{"version": "v1", "created": "Mon, 20 Nov 2017 23:14:20 GMT"}], "update_date": "2017-11-22", "authors_parsed": [["Mehta", "Ronak", ""], ["Kim", "Hyunwoo J.", ""], ["Wang", "Shulei", ""], ["Johnson", "Sterling C.", ""], ["Yuan", "Ming", ""], ["Singh", "Vikas", ""]]}, {"id": "1711.07592", "submitter": "Jean Feng", "authors": "Jean Feng, Noah Simon", "title": "Sparse-Input Neural Networks for High-dimensional Nonparametric\n  Regression and Classification", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Neural networks are usually not the tool of choice for nonparametric\nhigh-dimensional problems where the number of input features is much larger\nthan the number of observations. Though neural networks can approximate complex\nmultivariate functions, they generally require a large number of training\nobservations to obtain reasonable fits, unless one can learn the appropriate\nnetwork structure. In this manuscript, we show that neural networks can be\napplied successfully to high-dimensional settings if the true function falls in\na low dimensional subspace, and proper regularization is used. We propose\nfitting a neural network with a sparse group lasso penalty on the first-layer\ninput weights. This results in a neural net that only uses a small subset of\nthe original features. In addition, we characterize the statistical convergence\nof the penalized empirical risk minimizer to the optimal neural network: we\nshow that the excess risk of this penalized estimator only grows with the\nlogarithm of the number of input features; and we show that the weights of\nirrelevant features converge to zero. Via simulation studies and data analyses,\nwe show that these sparse-input neural networks outperform existing\nnonparametric high-dimensional estimation methods when the data has complex\nhigher-order interactions.\n", "versions": [{"version": "v1", "created": "Tue, 21 Nov 2017 01:11:00 GMT"}, {"version": "v2", "created": "Fri, 21 Jun 2019 18:23:48 GMT"}], "update_date": "2019-06-25", "authors_parsed": [["Feng", "Jean", ""], ["Simon", "Noah", ""]]}, {"id": "1711.07655", "submitter": "Eli (Omid) David", "authors": "Eli David, Iddo Greental", "title": "Genetic Algorithms for Evolving Deep Neural Networks", "comments": null, "journal-ref": "ACM Genetic and Evolutionary Computation Conference (GECCO), pages\n  1451-1452, Vancouver, Canada, July 2014", "doi": "10.1145/2598394.2602287", "report-no": null, "categories": "cs.NE cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In recent years, deep learning methods applying unsupervised learning to\ntrain deep layers of neural networks have achieved remarkable results in\nnumerous fields. In the past, many genetic algorithms based methods have been\nsuccessfully applied to training neural networks. In this paper, we extend\nprevious work and propose a GA-assisted method for deep learning. Our\nexperimental results indicate that this GA-assisted approach improves the\nperformance of a deep autoencoder, producing a sparser neural network.\n", "versions": [{"version": "v1", "created": "Tue, 21 Nov 2017 07:23:32 GMT"}], "update_date": "2017-11-22", "authors_parsed": [["David", "Eli", ""], ["Greental", "Iddo", ""]]}, {"id": "1711.07673", "submitter": "Disi Ji", "authors": "Disi Ji, Eric Nalisnick, Padhraic Smyth", "title": "Mondrian Processes for Flow Cytometry Analysis", "comments": "7 pages, 4 figures, NIPS workshop ML4H: Machine Learning for Health\n  2017, Long Beach, CA, USA", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML q-bio.QM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Analysis of flow cytometry data is an essential tool for clinical diagnosis\nof hematological and immunological conditions. Current clinical workflows rely\non a manual process called gating to classify cells into their canonical types.\nThis dependence on human annotation limits the rate, reproducibility, and\ncomplexity of flow cytometry analysis. In this paper, we propose using Mondrian\nprocesses to perform automated gating by incorporating prior information of the\nkind used by gating technicians. The method segments cells into types via\nBayesian nonparametric trees. Examining the posterior over trees allows for\ninterpretable visualizations and uncertainty quantification - two vital\nqualities for implementation in clinical practice.\n", "versions": [{"version": "v1", "created": "Tue, 21 Nov 2017 08:16:37 GMT"}, {"version": "v2", "created": "Tue, 28 Nov 2017 11:22:40 GMT"}], "update_date": "2017-11-29", "authors_parsed": [["Ji", "Disi", ""], ["Nalisnick", "Eric", ""], ["Smyth", "Padhraic", ""]]}, {"id": "1711.07676", "submitter": "Ashley Edwards", "authors": "Ashley D. Edwards, Charles L. Isbell Jr", "title": "Transferring Agent Behaviors from Videos via Motion GANs", "comments": "Deep Reinforcement Learning Symposium, NIPS 2017", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A major bottleneck for developing general reinforcement learning agents is\ndetermining rewards that will yield desirable behaviors under various\ncircumstances. We introduce a general mechanism for automatically specifying\nmeaningful behaviors from raw pixels. In particular, we train a generative\nadversarial network to produce short sub-goals represented through motion\ntemplates. We demonstrate that this approach generates visually meaningful\nbehaviors in unknown environments with novel agents and describe how these\nmotions can be used to train reinforcement learning agents.\n", "versions": [{"version": "v1", "created": "Tue, 21 Nov 2017 08:51:31 GMT"}], "update_date": "2017-11-22", "authors_parsed": [["Edwards", "Ashley D.", ""], ["Isbell", "Charles L.", "Jr"]]}, {"id": "1711.07682", "submitter": "Gino Brunner", "authors": "Gino Brunner, Yuyi Wang, Roger Wattenhofer, Jonas Wiesendanger", "title": "JamBot: Music Theory Aware Chord Based Generation of Polyphonic Music\n  with LSTMs", "comments": "Paper presented at the 29th International Conference on Tools with\n  Artificial Intelligence, ICTAI 2017, Boston, MA, USA", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.SD cs.AI cs.IT cs.LG eess.AS math.IT stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a novel approach for the generation of polyphonic music based on\nLSTMs. We generate music in two steps. First, a chord LSTM predicts a chord\nprogression based on a chord embedding. A second LSTM then generates polyphonic\nmusic from the predicted chord progression. The generated music sounds pleasing\nand harmonic, with only few dissonant notes. It has clear long-term structure\nthat is similar to what a musician would play during a jam session. We show\nthat our approach is sensible from a music theory perspective by evaluating the\nlearned chord embeddings. Surprisingly, our simple model managed to extract the\ncircle of fifths, an important tool in music theory, from the dataset.\n", "versions": [{"version": "v1", "created": "Tue, 21 Nov 2017 09:19:16 GMT"}], "update_date": "2017-11-22", "authors_parsed": [["Brunner", "Gino", ""], ["Wang", "Yuyi", ""], ["Wattenhofer", "Roger", ""], ["Wiesendanger", "Jonas", ""]]}, {"id": "1711.07693", "submitter": "Wataru Kumagai", "authors": "Wataru Kumagai", "title": "Regret Analysis for Continuous Dueling Bandit", "comments": "14 pages. This paper was accepted at NIPS 2017 as a spotlight\n  presentation", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The dueling bandit is a learning framework wherein the feedback information\nin the learning process is restricted to a noisy comparison between a pair of\nactions. In this research, we address a dueling bandit problem based on a cost\nfunction over a continuous space. We propose a stochastic mirror descent\nalgorithm and show that the algorithm achieves an $O(\\sqrt{T\\log T})$-regret\nbound under strong convexity and smoothness assumptions for the cost function.\nSubsequently, we clarify the equivalence between regret minimization in dueling\nbandit and convex optimization for the cost function. Moreover, when\nconsidering a lower bound in convex optimization, our algorithm is shown to\nachieve the optimal convergence rate in convex optimization and the optimal\nregret in dueling bandit except for a logarithmic factor.\n", "versions": [{"version": "v1", "created": "Tue, 21 Nov 2017 09:58:00 GMT"}, {"version": "v2", "created": "Tue, 12 Dec 2017 07:32:36 GMT"}], "update_date": "2017-12-13", "authors_parsed": [["Kumagai", "Wataru", ""]]}, {"id": "1711.07732", "submitter": "Zuozhu Liu", "authors": "Zuozhu Liu, Tony Q.S. Quek, Shaowei Lin", "title": "Variational Probability Flow for Biologically Plausible Training of Deep\n  Neural Networks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.NE stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The quest for biologically plausible deep learning is driven, not just by the\ndesire to explain experimentally-observed properties of biological neural\nnetworks, but also by the hope of discovering more efficient methods for\ntraining artificial networks. In this paper, we propose a new algorithm named\nVariational Probably Flow (VPF), an extension of minimum probability flow for\ntraining binary Deep Boltzmann Machines (DBMs). We show that weight updates in\nVPF are local, depending only on the states and firing rates of the adjacent\nneurons. Unlike contrastive divergence, there is no need for Gibbs\nconfabulations; and unlike backpropagation, alternating feedforward and\nfeedback phases are not required. Moreover, the learning algorithm is effective\nfor training DBMs with intra-layer connections between the hidden nodes.\nExperiments with MNIST and Fashion MNIST demonstrate that VPF learns reasonable\nfeatures quickly, reconstructs corrupted images more accurately, and generates\nsamples with a high estimated log-likelihood. Lastly, we note that,\ninterestingly, if an asymmetric version of VPF exists, the weight updates\ndirectly explain experimental results in Spike-Timing-Dependent Plasticity\n(STDP).\n", "versions": [{"version": "v1", "created": "Tue, 21 Nov 2017 11:49:05 GMT"}], "update_date": "2017-11-22", "authors_parsed": [["Liu", "Zuozhu", ""], ["Quek", "Tony Q. S.", ""], ["Lin", "Shaowei", ""]]}, {"id": "1711.07792", "submitter": "Kay Gregor Hartmann", "authors": "Kay Gregor Hartmann, Robin Tibor Schirrmeister, Tonio Ball", "title": "Hierarchical internal representation of spectral features in deep\n  convolutional networks trained for EEG decoding", "comments": "6 pages, 7 figures, The 6th International Winter Conference on\n  Brain-Computer Interface", "journal-ref": null, "doi": "10.1109/IWW-BCI.2018.8311493", "report-no": null, "categories": "cs.LG q-bio.NC stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recently, there is increasing interest and research on the interpretability\nof machine learning models, for example how they transform and internally\nrepresent EEG signals in Brain-Computer Interface (BCI) applications. This can\nhelp to understand the limits of the model and how it may be improved, in\naddition to possibly provide insight about the data itself. Schirrmeister et\nal. (2017) have recently reported promising results for EEG decoding with deep\nconvolutional neural networks (ConvNets) trained in an end-to-end manner and,\nwith a causal visualization approach, showed that they learn to use spectral\namplitude changes in the input. In this study, we investigate how ConvNets\nrepresent spectral features through the sequence of intermediate stages of the\nnetwork. We show higher sensitivity to EEG phase features at earlier stages and\nhigher sensitivity to EEG amplitude features at later stages. Intriguingly, we\nobserved a specialization of individual stages of the network to the classical\nEEG frequency bands alpha, beta, and high gamma. Furthermore, we find first\nevidence that particularly in the last convolutional layer, the network learns\nto detect more complex oscillatory patterns beyond spectral phase and\namplitude, reminiscent of the representation of complex visual features in\nlater layers of ConvNets in computer vision tasks. Our findings thus provide\ninsights into how ConvNets hierarchically represent spectral EEG features in\ntheir intermediate layers and suggest that ConvNets can exploit and might help\nto better understand the compositional structure of EEG time series.\n", "versions": [{"version": "v1", "created": "Tue, 21 Nov 2017 14:05:25 GMT"}, {"version": "v2", "created": "Thu, 23 Nov 2017 18:12:03 GMT"}, {"version": "v3", "created": "Fri, 15 Dec 2017 16:29:12 GMT"}], "update_date": "2018-03-16", "authors_parsed": [["Hartmann", "Kay Gregor", ""], ["Schirrmeister", "Robin Tibor", ""], ["Ball", "Tonio", ""]]}, {"id": "1711.07812", "submitter": "Caifa Zhou", "authors": "Caifa Zhou, Andreas Wieser", "title": "Jaccard analysis and LASSO-based feature selection for location\n  fingerprinting with limited computational complexity", "comments": "16 pages, 4 figures, and 2 tables. Accepted to publish on LBS 2018,\n  Zurich", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose an approach to reduce both computational complexity and data\nstorage requirements for the online positioning stage of a fingerprinting-based\nindoor positioning system (FIPS) by introducing segmentation of the region of\ninterest (RoI) into sub-regions, sub-region selection using a modified Jaccard\nindex, and feature selection based on randomized least absolute shrinkage and\nselection operator (LASSO). We implement these steps into a Bayesian framework\nof position estimation using the maximum a posteriori (MAP) principle. An\nadditional benefit of these steps is that the time for estimating the position,\nand the required data storage are virtually independent of the size of the RoI\nand of the total number of available features within the RoI. Thus the proposed\nsteps facilitate application of FIPS to large areas. Results of an experimental\nanalysis using real data collected in an office building using a Nexus 6P smart\nphone as user device and a total station for providing position ground truth\ncorroborate the expected performance of the proposed approach. The positioning\naccuracy obtained by only processing 10 automatically identified features\ninstead of all available ones and limiting position estimation to 10\nautomatically identified sub-regions instead of the entire RoI is equivalent to\nprocessing all available data. In the chosen example, 50% of the errors are\nless than 1.8 m and 90% are less than 5 m. However, the computation time using\nthe automatically identified subset of data is only about 1% of that required\nfor processing the entire data set.\n", "versions": [{"version": "v1", "created": "Tue, 21 Nov 2017 14:42:37 GMT"}], "update_date": "2017-11-22", "authors_parsed": [["Zhou", "Caifa", ""], ["Wieser", "Andreas", ""]]}, {"id": "1711.07814", "submitter": "Val Andrei Fajardo", "authors": "Val Andrei Fajardo and Jiaxi Liang", "title": "On the EM-Tau algorithm: a new EM-style algorithm with partial E-steps", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The EM algorithm is one of many important tools in the field of statistics.\nWhile often used for imputing missing data, its widespread applications include\nother common statistical tasks, such as clustering. In clustering, the EM\nalgorithm assumes a parametric distribution for the clusters, whose parameters\nare estimated through a novel iterative procedure that is based on the theory\nof maximum likelihood. However, one major drawback of the EM algorithm, that\nrenders it impractical especially when working with large datasets, is that it\noften requires several passes of the data before convergence. In this paper, we\nintroduce a new EM-style algorithm that implements a novel policy for\nperforming partial E-steps. We call the new algorithm the EM-Tau algorithm,\nwhich can approximate the traditional EM algorithm with high accuracy but with\nonly a fraction of the running time.\n", "versions": [{"version": "v1", "created": "Tue, 21 Nov 2017 14:46:34 GMT"}], "update_date": "2017-11-22", "authors_parsed": [["Fajardo", "Val Andrei", ""], ["Liang", "Jiaxi", ""]]}, {"id": "1711.07831", "submitter": "Abien Fred Agarap", "authors": "Abien Fred Agarap", "title": "On Breast Cancer Detection: An Application of Machine Learning\n  Algorithms on the Wisconsin Diagnostic Dataset", "comments": "5 pages, 5 figures, 2 tables, presented at the International\n  Conference on Machine Learning and Soft Computing (ICMLSC) 2018 in Phu Quoc\n  Island, Viet Nam", "journal-ref": null, "doi": "10.1145/3184066.3184080", "report-no": null, "categories": "cs.LG stat.ML", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  This paper presents a comparison of six machine learning (ML) algorithms:\nGRU-SVM (Agarap, 2017), Linear Regression, Multilayer Perceptron (MLP), Nearest\nNeighbor (NN) search, Softmax Regression, and Support Vector Machine (SVM) on\nthe Wisconsin Diagnostic Breast Cancer (WDBC) dataset (Wolberg, Street, &\nMangasarian, 1992) by measuring their classification test accuracy and their\nsensitivity and specificity values. The said dataset consists of features which\nwere computed from digitized images of FNA tests on a breast mass (Wolberg,\nStreet, & Mangasarian, 1992). For the implementation of the ML algorithms, the\ndataset was partitioned in the following fashion: 70% for training phase, and\n30% for the testing phase. The hyper-parameters used for all the classifiers\nwere manually assigned. Results show that all the presented ML algorithms\nperformed well (all exceeded 90% test accuracy) on the classification task. The\nMLP algorithm stands out among the implemented algorithms with a test accuracy\nof ~99.04%.\n", "versions": [{"version": "v1", "created": "Mon, 20 Nov 2017 06:33:34 GMT"}, {"version": "v2", "created": "Sun, 28 Jan 2018 01:30:05 GMT"}, {"version": "v3", "created": "Tue, 6 Mar 2018 13:47:58 GMT"}, {"version": "v4", "created": "Thu, 7 Feb 2019 06:30:57 GMT"}], "update_date": "2019-02-08", "authors_parsed": [["Agarap", "Abien Fred", ""]]}, {"id": "1711.07839", "submitter": "Thomas Blaschke", "authors": "Thomas Blaschke, Marcus Olivecrona, Ola Engkvist, J\\\"urgen Bajorath,\n  Hongming Chen", "title": "Application of generative autoencoder in de novo molecular design", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A major challenge in computational chemistry is the generation of novel\nmolecular structures with desirable pharmacological and physiochemical\nproperties. In this work, we investigate the potential use of autoencoder, a\ndeep learning methodology, for de novo molecular design. Various generative\nautoencoders were used to map molecule structures into a continuous latent\nspace and vice versa and their performance as structure generator was assessed.\nOur results show that the latent space preserves chemical similarity principle\nand thus can be used for the generation of analogue structures. Furthermore,\nthe latent space created by autoencoders were searched systematically to\ngenerate novel compounds with predicted activity against dopamine receptor type\n2 and compounds similar to known active compounds not included in the training\nset were identified.\n", "versions": [{"version": "v1", "created": "Tue, 21 Nov 2017 15:19:36 GMT"}], "update_date": "2017-11-22", "authors_parsed": [["Blaschke", "Thomas", ""], ["Olivecrona", "Marcus", ""], ["Engkvist", "Ola", ""], ["Bajorath", "J\u00fcrgen", ""], ["Chen", "Hongming", ""]]}, {"id": "1711.07871", "submitter": "Ya Ju Fan", "authors": "Ya Ju Fan", "title": "Autoencoder Node Saliency: Selecting Relevant Latent Representations", "comments": null, "journal-ref": "Pattern Recognition, Volume 88, 2019, Pages 643-653", "doi": "10.1016/j.patcog.2018.12.015", "report-no": "ISSN 0031-3203", "categories": "cs.CV cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The autoencoder is an artificial neural network model that learns hidden\nrepresentations of unlabeled data. With a linear transfer function it is\nsimilar to the principal component analysis (PCA). While both methods use\nweight vectors for linear transformations, the autoencoder does not come with\nany indication similar to the eigenvalues in PCA that are paired with the\neigenvectors. We propose a novel supervised node saliency (SNS) method that\nranks the hidden nodes by comparing class distributions of latent\nrepresentations against a fixed reference distribution. The latent\nrepresentations of a hidden node can be described using a one-dimensional\nhistogram. We apply normalized entropy difference (NED) to measure the\n\"interestingness\" of the histograms, and conclude a property for NED values to\nidentify a good classifying node. By applying our methods to real data sets, we\ndemonstrate the ability of SNS to explain what the trained autoencoders have\nlearned.\n", "versions": [{"version": "v1", "created": "Tue, 21 Nov 2017 16:17:14 GMT"}, {"version": "v2", "created": "Thu, 8 Mar 2018 02:09:31 GMT"}], "update_date": "2019-02-04", "authors_parsed": [["Fan", "Ya Ju", ""]]}, {"id": "1711.07878", "submitter": "Jingguang Zhou", "authors": "Jingguang Zhou, Zili Huang", "title": "Recover Missing Sensor Data with Iterative Imputing Network", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Sensor data has been playing an important role in machine learning tasks,\ncomplementary to the human-annotated data that is usually rather costly.\nHowever, due to systematic or accidental mis-operations, sensor data comes very\noften with a variety of missing values, resulting in considerable difficulties\nin the follow-up analysis and visualization. Previous work imputes the missing\nvalues by interpolating in the observational feature space, without consulting\nany latent (hidden) dynamics. In contrast, our model captures the latent\ncomplex temporal dynamics by summarizing each observation's context with a\nnovel Iterative Imputing Network, thus significantly outperforms previous work\non the benchmark Beijing air quality and meteorological dataset. Our model also\nyields consistent superiority over other methods in cases of different missing\nrates.\n", "versions": [{"version": "v1", "created": "Mon, 20 Nov 2017 15:58:02 GMT"}], "update_date": "2017-11-22", "authors_parsed": [["Zhou", "Jingguang", ""], ["Huang", "Zili", ""]]}, {"id": "1711.07886", "submitter": "Fayyaz Minhas", "authors": "Abdul Hannan Basit, Wajid Arshad Abbasi, Amina Asif, and Fayyaz Ul\n  Amir Afsar Minhas", "title": "Training large margin host-pathogen protein-protein interaction\n  predictors", "comments": "12 pages", "journal-ref": "Journal of Bioinformatics and Computational Biology 2018", "doi": "10.1142/S0219720018500142", "report-no": "Vol. 16, No. 04 1850014", "categories": "cs.LG q-bio.QM stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Detection of protein-protein interactions (PPIs) plays a vital role in\nmolecular biology. Particularly, infections are caused by the interactions of\nhost and pathogen proteins. It is important to identify host-pathogen\ninteractions (HPIs) to discover new drugs to counter infectious diseases.\nConventional wet lab PPI prediction techniques have limitations in terms of\nlarge scale application and budget. Hence, computational approaches are\ndeveloped to predict PPIs. This study aims to develop large margin machine\nlearning models to predict interspecies PPIs with a special interest in\nhost-pathogen protein interactions (HPIs). Especially, we focus on seeking\nanswers to three queries that arise while developing an HPI predictor. 1) How\nshould we select negative samples? 2) What should be the size of negative\nsamples as compared to the positive samples? 3) What type of margin violation\npenalty should be used to train the predictor? We compare two available methods\nfor negative sampling. Moreover, we propose a new method of assigning weights\nto each training example in weighted SVM depending on the distance of the\nnegative examples from the positive examples. We have also developed a web\nserver for our HPI predictor called HoPItor (Host Pathogen Interaction\npredicTOR) that can predict interactions between human and viral proteins. This\nwebserver can be accessed at the URL:\nhttp://faculty.pieas.edu.pk/fayyaz/software.html#HoPItor.\n", "versions": [{"version": "v1", "created": "Tue, 21 Nov 2017 16:31:43 GMT"}], "update_date": "2019-03-26", "authors_parsed": [["Basit", "Abdul Hannan", ""], ["Abbasi", "Wajid Arshad", ""], ["Asif", "Amina", ""], ["Minhas", "Fayyaz Ul Amir Afsar", ""]]}, {"id": "1711.07894", "submitter": "Kun ho Kim", "authors": "Yanan Sui, Kun ho Kim, Joel W. Burdick", "title": "Quantifying Performance of Bipedal Standing with Multi-channel EMG", "comments": null, "journal-ref": "IROS 2017", "doi": null, "report-no": null, "categories": "stat.ML cs.AI q-bio.NC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Spinal cord stimulation has enabled humans with motor complete spinal cord\ninjury (SCI) to independently stand and recover some lost autonomic function.\nQuantifying the quality of bipedal standing under spinal stimulation is\nimportant for spinal rehabilitation therapies and for new strategies that seek\nto combine spinal stimulation and rehabilitative robots (such as exoskeletons)\nin real time feedback. To study the potential for automated electromyography\n(EMG) analysis in SCI, we evaluated the standing quality of paralyzed patients\nundergoing electrical spinal cord stimulation using both video and\nmulti-channel surface EMG recordings during spinal stimulation therapy\nsessions. The quality of standing under different stimulation settings was\nquantified manually by experienced clinicians. By correlating features of the\nrecorded EMG activity with the expert evaluations, we show that multi-channel\nEMG recording can provide accurate, fast, and robust estimation for the quality\nof bipedal standing in spinally stimulated SCI patients. Moreover, our analysis\nshows that the total number of EMG channels needed to effectively predict\nstanding quality can be reduced while maintaining high estimation accuracy,\nwhich provides more flexibility for rehabilitation robotic systems to\nincorporate EMG recordings.\n", "versions": [{"version": "v1", "created": "Tue, 21 Nov 2017 16:40:26 GMT"}], "update_date": "2017-11-22", "authors_parsed": [["Sui", "Yanan", ""], ["Kim", "Kun ho", ""], ["Burdick", "Joel W.", ""]]}, {"id": "1711.07910", "submitter": "Clayton Scott", "authors": "Gilles Blanchard, Aniket Anand Deshmukh, Urun Dogan, Gyemin Lee,\n  Clayton Scott", "title": "Domain Generalization by Marginal Transfer Learning", "comments": "Accepted to Journal of Machine Learning Research", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In the problem of domain generalization (DG), there are labeled training data\nsets from several related prediction problems, and the goal is to make accurate\npredictions on future unlabeled data sets that are not known to the learner.\nThis problem arises in several applications where data distributions fluctuate\nbecause of environmental, technical, or other sources of variation. We\nintroduce a formal framework for DG, and argue that it can be viewed as a kind\nof supervised learning problem by augmenting the original feature space with\nthe marginal distribution of feature vectors. While our framework has several\nconnections to conventional analysis of supervised learning algorithms, several\nunique aspects of DG require new methods of analysis.\n  This work lays the learning theoretic foundations of domain generalization,\nbuilding on our earlier conference paper where the problem of DG was introduced\n(Blanchard et al., 2011). We present two formal models of data generation,\ncorresponding notions of risk, and distribution-free generalization error\nanalysis. By focusing our attention on kernel methods, we also provide more\nquantitative results and a universally consistent algorithm. An efficient\nimplementation is provided for this algorithm, which is experimentally compared\nto a pooling strategy on one synthetic and three real-world data sets.\n", "versions": [{"version": "v1", "created": "Tue, 21 Nov 2017 16:59:43 GMT"}, {"version": "v2", "created": "Fri, 17 Apr 2020 21:55:36 GMT"}, {"version": "v3", "created": "Wed, 6 Jan 2021 23:07:06 GMT"}], "update_date": "2021-01-08", "authors_parsed": [["Blanchard", "Gilles", ""], ["Deshmukh", "Aniket Anand", ""], ["Dogan", "Urun", ""], ["Lee", "Gyemin", ""], ["Scott", "Clayton", ""]]}, {"id": "1711.07925", "submitter": "Kejun Huang", "authors": "Kejun Huang, Nicholas D. Sidiropoulos", "title": "Kullback-Leibler Principal Component for Tensors is not NP-hard", "comments": "Asilomar 2017", "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.SP cs.IT math.IT math.OC math.PR stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study the problem of nonnegative rank-one approximation of a nonnegative\ntensor, and show that the globally optimal solution that minimizes the\ngeneralized Kullback-Leibler divergence can be efficiently obtained, i.e., it\nis not NP-hard. This result works for arbitrary nonnegative tensors with an\narbitrary number of modes (including two, i.e., matrices). We derive a\nclosed-form expression for the KL principal component, which is easy to compute\nand has an intuitive probabilistic interpretation. For generalized KL\napproximation with higher ranks, the problem is for the first time shown to be\nequivalent to multinomial latent variable modeling, and an iterative algorithm\nis derived that resembles the expectation-maximization algorithm. On the Iris\ndataset, we showcase how the derived results help us learn the model in an\n\\emph{unsupervised} manner, and obtain strikingly close performance to that\nfrom supervised methods.\n", "versions": [{"version": "v1", "created": "Tue, 21 Nov 2017 17:17:03 GMT"}], "update_date": "2017-11-22", "authors_parsed": [["Huang", "Kejun", ""], ["Sidiropoulos", "Nicholas D.", ""]]}, {"id": "1711.07970", "submitter": "Arthur Pajot", "authors": "Emmanuel de Bezenac, Arthur Pajot, Patrick Gallinari", "title": "Deep Learning for Physical Processes: Incorporating Prior Scientific\n  Knowledge", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider the use of Deep Learning methods for modeling complex phenomena\nlike those occurring in natural physical processes. With the large amount of\ndata gathered on these phenomena the data intensive paradigm could begin to\nchallenge more traditional approaches elaborated over the years in fields like\nmaths or physics. However, despite considerable successes in a variety of\napplication domains, the machine learning field is not yet ready to handle the\nlevel of complexity required by such problems. Using an example application,\nnamely Sea Surface Temperature Prediction, we show how general background\nknowledge gained from physics could be used as a guideline for designing\nefficient Deep Learning models. In order to motivate the approach and to assess\nits generality we demonstrate a formal link between the solution of a class of\ndifferential equations underlying a large family of physical phenomena and the\nproposed model. Experiments and comparison with series of baselines including a\nstate of the art numerical approach is then provided.\n", "versions": [{"version": "v1", "created": "Tue, 21 Nov 2017 18:49:47 GMT"}, {"version": "v2", "created": "Tue, 9 Jan 2018 16:43:39 GMT"}], "update_date": "2018-01-10", "authors_parsed": [["de Bezenac", "Emmanuel", ""], ["Pajot", "Arthur", ""], ["Gallinari", "Patrick", ""]]}, {"id": "1711.08001", "submitter": "Xi Wu", "authors": "Xi Wu, Uyeong Jang, Jiefeng Chen, Lingjiao Chen, Somesh Jha", "title": "Reinforcing Adversarial Robustness using Model Confidence Induced by\n  Adversarial Training", "comments": "To appear in ICML 2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CR stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper we study leveraging confidence information induced by\nadversarial training to reinforce adversarial robustness of a given\nadversarially trained model. A natural measure of confidence is $\\|F({\\bf\nx})\\|_\\infty$ (i.e. how confident $F$ is about its prediction?). We start by\nanalyzing an adversarial training formulation proposed by Madry et al.. We\ndemonstrate that, under a variety of instantiations, an only somewhat good\nsolution to their objective induces confidence to be a discriminator, which can\ndistinguish between right and wrong model predictions in a neighborhood of a\npoint sampled from the underlying distribution. Based on this, we propose\nHighly Confident Near Neighbor (${\\tt HCNN}$), a framework that combines\nconfidence information and nearest neighbor search, to reinforce adversarial\nrobustness of a base model. We give algorithms in this framework and perform a\ndetailed empirical study. We report encouraging experimental results that\nsupport our analysis, and also discuss problems we observed with existing\nadversarial training.\n", "versions": [{"version": "v1", "created": "Tue, 21 Nov 2017 19:15:05 GMT"}, {"version": "v2", "created": "Mon, 1 Jan 2018 20:12:55 GMT"}, {"version": "v3", "created": "Fri, 8 Jun 2018 13:46:51 GMT"}], "update_date": "2018-10-16", "authors_parsed": [["Wu", "Xi", ""], ["Jang", "Uyeong", ""], ["Chen", "Jiefeng", ""], ["Chen", "Lingjiao", ""], ["Jha", "Somesh", ""]]}, {"id": "1711.08014", "submitter": "Abhishek Kumar", "authors": "Hang Shao, Abhishek Kumar, P. Thomas Fletcher", "title": "The Riemannian Geometry of Deep Generative Models", "comments": "9 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Deep generative models learn a mapping from a low dimensional latent space to\na high-dimensional data space. Under certain regularity conditions, these\nmodels parameterize nonlinear manifolds in the data space. In this paper, we\ninvestigate the Riemannian geometry of these generated manifolds. First, we\ndevelop efficient algorithms for computing geodesic curves, which provide an\nintrinsic notion of distance between points on the manifold. Second, we develop\nan algorithm for parallel translation of a tangent vector along a path on the\nmanifold. We show how parallel translation can be used to generate analogies,\ni.e., to transport a change in one data point into a semantically similar\nchange of another data point. Our experiments on real image data show that the\nmanifolds learned by deep generative models, while nonlinear, are surprisingly\nclose to zero curvature. The practical implication is that linear paths in the\nlatent space closely approximate geodesics on the generated manifold. However,\nfurther investigation into this phenomenon is warranted, to identify if there\nare other architectures or datasets where curvature plays a more prominent\nrole. We believe that exploring the Riemannian geometry of deep generative\nmodels, using the tools developed in this paper, will be an important step in\nunderstanding the high-dimensional, nonlinear spaces these models learn.\n", "versions": [{"version": "v1", "created": "Tue, 21 Nov 2017 19:59:24 GMT"}], "update_date": "2017-11-23", "authors_parsed": [["Shao", "Hang", ""], ["Kumar", "Abhishek", ""], ["Fletcher", "P. Thomas", ""]]}, {"id": "1711.08018", "submitter": "Akshay Krishnamurthy", "authors": "Tongyi Cao, Akshay Krishnamurthy", "title": "Disagreement-Based Combinatorial Pure Exploration: Sample Complexity\n  Bounds and an Efficient Algorithm", "comments": null, "journal-ref": "Conference on Learning Theory, 2019", "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We design new algorithms for the combinatorial pure exploration problem in\nthe multi-arm bandit framework. In this problem, we are given $K$ distributions\nand a collection of subsets $\\mathcal{V} \\subset 2^{[K]}$ of these\ndistributions, and we would like to find the subset $v \\in \\mathcal{V}$ that\nhas largest mean, while collecting, in a sequential fashion, as few samples\nfrom the distributions as possible. In both the fixed budget and fixed\nconfidence settings, our algorithms achieve new sample-complexity bounds that\nprovide polynomial improvements on previous results in some settings. Via an\ninformation-theoretic lower bound, we show that no approach based on uniform\nsampling can improve on ours in any regime, yielding the first interactive\nalgorithms for this problem with this basic property. Computationally, we show\nhow to efficiently implement our fixed confidence algorithm whenever\n$\\mathcal{V}$ supports efficient linear optimization. Our results involve\nprecise concentration-of-measure arguments and a new algorithm for linear\nprogramming with exponentially many constraints.\n", "versions": [{"version": "v1", "created": "Tue, 21 Nov 2017 20:16:35 GMT"}, {"version": "v2", "created": "Thu, 30 Nov 2017 12:44:57 GMT"}, {"version": "v3", "created": "Mon, 4 Feb 2019 15:04:12 GMT"}, {"version": "v4", "created": "Tue, 28 May 2019 12:59:42 GMT"}], "update_date": "2019-05-29", "authors_parsed": [["Cao", "Tongyi", ""], ["Krishnamurthy", "Akshay", ""]]}, {"id": "1711.08037", "submitter": "Zachary Lipton", "authors": "Zachary C. Lipton", "title": "The Doctor Just Won't Accept That!", "comments": "Presented at NIPS 2017 Interpretable ML Symposium", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Calls to arms to build interpretable models express a well-founded discomfort\nwith machine learning. Should a software agent that does not even know what a\nloan is decide who qualifies for one? Indeed, we ought to be cautious about\ninjecting machine learning (or anything else, for that matter) into\napplications where there may be a significant risk of causing social harm.\nHowever, claims that stakeholders \"just won't accept that!\" do not provide a\nsufficient foundation for a proposed field of study. For the field of\ninterpretable machine learning to advance, we must ask the following questions:\nWhat precisely won't various stakeholders accept? What do they want? Are these\ndesiderata reasonable? Are they feasible? In order to answer these questions,\nwe'll have to give real-world problems and their respective stakeholders\ngreater consideration.\n", "versions": [{"version": "v1", "created": "Mon, 20 Nov 2017 04:19:49 GMT"}, {"version": "v2", "created": "Fri, 24 Nov 2017 07:02:24 GMT"}], "update_date": "2017-11-27", "authors_parsed": [["Lipton", "Zachary C.", ""]]}, {"id": "1711.08042", "submitter": "Fabian Offert", "authors": "Fabian Offert", "title": "\"I know it when I see it\". Visualization and Intuitive Interpretability", "comments": "Presented at NIPS 2017 Symposium on Interpretable Machine Learning", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Most research on the interpretability of machine learning systems focuses on\nthe development of a more rigorous notion of interpretability. I suggest that a\nbetter understanding of the deficiencies of the intuitive notion of\ninterpretability is needed as well. I show that visualization enables but also\nimpedes intuitive interpretability, as it presupposes two levels of technical\npre-interpretation: dimensionality reduction and regularization. Furthermore, I\nargue that the use of positive concepts to emulate the distributed semantic\nstructure of machine learning models introduces a significant human bias into\nthe model. As a consequence, I suggest that, if intuitive interpretability is\nneeded, singular representations of internal model states should be avoided.\n", "versions": [{"version": "v1", "created": "Mon, 20 Nov 2017 05:25:06 GMT"}, {"version": "v2", "created": "Wed, 6 Dec 2017 23:46:01 GMT"}], "update_date": "2017-12-08", "authors_parsed": [["Offert", "Fabian", ""]]}, {"id": "1711.08054", "submitter": "Ming Hou", "authors": "Ming Hou, Brahim Chaib-draa, Chao Li, Qibin Zhao", "title": "Generative Adversarial Positive-Unlabelled Learning", "comments": "8 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this work, we consider the task of classifying binary positive-unlabeled\n(PU) data. The existing discriminative learning based PU models attempt to seek\nan optimal reweighting strategy for U data, so that a decent decision boundary\ncan be found. However, given limited P data, the conventional PU models tend to\nsuffer from overfitting when adapted to very flexible deep neural networks. In\ncontrast, we are the first to innovate a totally new paradigm to attack the\nbinary PU task, from perspective of generative learning by leveraging the\npowerful generative adversarial networks (GAN). Our generative\npositive-unlabeled (GenPU) framework incorporates an array of discriminators\nand generators that are endowed with different roles in simultaneously\nproducing positive and negative realistic samples. We provide theoretical\nanalysis to justify that, at equilibrium, GenPU is capable of recovering both\npositive and negative data distributions. Moreover, we show GenPU is\ngeneralizable and closely related to the semi-supervised classification. Given\nrather limited P data, experiments on both synthetic and real-world dataset\ndemonstrate the effectiveness of our proposed framework. With infinite\nrealistic and diverse sample streams generated from GenPU, a very flexible\nclassifier can then be trained using deep neural networks.\n", "versions": [{"version": "v1", "created": "Tue, 21 Nov 2017 21:40:24 GMT"}, {"version": "v2", "created": "Wed, 4 Apr 2018 08:18:27 GMT"}], "update_date": "2018-04-05", "authors_parsed": [["Hou", "Ming", ""], ["Chaib-draa", "Brahim", ""], ["Li", "Chao", ""], ["Zhao", "Qibin", ""]]}, {"id": "1711.08063", "submitter": "Shatrunjai Singh", "authors": "Shatrunjai P. Singh, Candi L. LaSarge, Amen An, John J. McAuliffe and\n  Steve C. Danzer", "title": "Clonal analysis of newborn hippocampal dentate granule cell\n  proliferation and development in temporal lobe epilepsy", "comments": "44 pages, 6 figures", "journal-ref": "eNeuro. 2015;2(6):ENEURO.0087-15.2015.\n  doi:10.1523/ENEURO.0087-15.2015", "doi": null, "report-no": null, "categories": "stat.ML q-bio.NC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Hippocampal dentate granule cells are among the few neuronal cell types\ngenerated throughout adult life in mammals. In the normal brain, new granule\ncells are generated from progenitors in the subgranular zone and integrate in a\ntypical fashion. During the development of epilepsy, granule cell integration\nis profoundly altered. The new cells migrate to ectopic locations and develop\nmisoriented basal dendrites. Although it has been established that these\nabnormal cells are newly generated, it is not known whether they arise\nubiquitously throughout the progenitor cell pool or are derived from a smaller\nnumber of bad actor progenitors. To explore this question, we conducted a\nclonal analysis study in mice expressing the Brainbow fluorescent protein\nreporter construct in dentate granule cell progenitors. Mice were examined 2\nmonths after pilocarpine-induced status epilepticus, a treatment that leads to\nthe development of epilepsy. Brain sections were rendered translucent so that\nentire hippocampi could be reconstructed and all fluorescently labeled cells\nidentified. Our findings reveal that a small number of progenitors produce the\nmajority of ectopic cells following status epilepticus, indicating that either\nthe affected progenitors or their local microenvironments have become\npathological. By contrast, granule cells with basal dendrites were equally\ndistributed among clonal groups. This indicates that these progenitors can\nproduce normal cells and suggests that global factors sporadically disrupt the\ndendritic development of some new cells. Together, these findings strongly\npredict that distinct mechanisms regulate different aspects\n", "versions": [{"version": "v1", "created": "Tue, 21 Nov 2017 22:00:01 GMT"}], "update_date": "2017-11-23", "authors_parsed": [["Singh", "Shatrunjai P.", ""], ["LaSarge", "Candi L.", ""], ["An", "Amen", ""], ["McAuliffe", "John J.", ""], ["Danzer", "Steve C.", ""]]}, {"id": "1711.08095", "submitter": "Dongjin Choi", "authors": "Dongjin Choi, Lee Sael", "title": "SNeCT: Scalable network constrained Tucker decomposition for integrative\n  multi-platform data analysis", "comments": "8 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG q-bio.QM stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Motivation: How do we integratively analyze large-scale multi-platform\ngenomic data that are high dimensional and sparse? Furthermore, how can we\nincorporate prior knowledge, such as the association between genes, in the\nanalysis systematically? Method: To solve this problem, we propose a Scalable\nNetwork Constrained Tucker decomposition method we call SNeCT. SNeCT adopts\nparallel stochastic gradient descent approach on the proposed parallelizable\nnetwork constrained optimization function. SNeCT decomposition is applied to\ntensor constructed from large scale multi-platform multi-cohort cancer data,\nPanCan12, constrained on a network built from PathwayCommons database. Results:\nThe decomposed factor matrices are applied to stratify cancers, to search for\ntop-k similar patients, and to illustrate how the matrices can be used for\npersonalized interpretation. In the stratification test, combined twelve-cohort\ndata is clustered to form thirteen subclasses. The thirteen subclasses have a\nhigh correlation to tissue of origin in addition to other interesting\nobservations, such as clear separation of OV cancers to two groups, and high\nclinical correlation within subclusters formed in cohorts BRCA and UCEC. In the\ntop-k search, a new patient's genomic profile is generated and searched against\nexisting patients based on the factor matrices. The similarity of the top-k\npatient to the query is high for 23 clinical features, including\nestrogen/progesterone receptor statuses of BRCA patients with average precision\nvalue ranges from 0.72 to 0.86 and from 0.68 to 0.86, respectively. We also\nprovide an illustration of how the factor matrices can be used for\ninterpretable personalized analysis of each patient.\n", "versions": [{"version": "v1", "created": "Wed, 22 Nov 2017 01:03:49 GMT"}, {"version": "v2", "created": "Sun, 26 Nov 2017 11:02:13 GMT"}], "update_date": "2017-11-28", "authors_parsed": [["Choi", "Dongjin", ""], ["Sael", "Lee", ""]]}, {"id": "1711.08132", "submitter": "Liang Pang", "authors": "Liang Pang, Yanyan Lan, Jun Xu, Jiafeng Guo, Xueqi Cheng", "title": "Locally Smoothed Neural Networks", "comments": "In Proceedings of 9th Asian Conference on Machine Learning (ACML2017)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Convolutional Neural Networks (CNN) and the locally connected layer are\nlimited in capturing the importance and relations of different local receptive\nfields, which are often crucial for tasks such as face verification, visual\nquestion answering, and word sequence prediction. To tackle the issue, we\npropose a novel locally smoothed neural network (LSNN) in this paper. The main\nidea is to represent the weight matrix of the locally connected layer as the\nproduct of the kernel and the smoother, where the kernel is shared over\ndifferent local receptive fields, and the smoother is for determining the\nimportance and relations of different local receptive fields. Specifically, a\nmulti-variate Gaussian function is utilized to generate the smoother, for\nmodeling the location relations among different local receptive fields.\nFurthermore, the content information can also be leveraged by setting the mean\nand precision of the Gaussian function according to the content. Experiments on\nsome variant of MNIST clearly show our advantages over CNN and locally\nconnected layer.\n", "versions": [{"version": "v1", "created": "Wed, 22 Nov 2017 05:05:32 GMT"}], "update_date": "2017-11-23", "authors_parsed": [["Pang", "Liang", ""], ["Lan", "Yanyan", ""], ["Xu", "Jun", ""], ["Guo", "Jiafeng", ""], ["Cheng", "Xueqi", ""]]}, {"id": "1711.08160", "submitter": "Alexander Tank", "authors": "Alex Tank, Ian Cover, Nicholas J. Foti, Ali Shojaie, Emily B. Fox", "title": "An Interpretable and Sparse Neural Network Model for Nonlinear Granger\n  Causality Discovery", "comments": "Accepted to the NIPS Time Series Workshop 2017", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  While most classical approaches to Granger causality detection repose upon\nlinear time series assumptions, many interactions in neuroscience and economics\napplications are nonlinear. We develop an approach to nonlinear Granger\ncausality detection using multilayer perceptrons where the input to the network\nis the past time lags of all series and the output is the future value of a\nsingle series. A sufficient condition for Granger non-causality in this setting\nis that all of the outgoing weights of the input data, the past lags of a\nseries, to the first hidden layer are zero. For estimation, we utilize a group\nlasso penalty to shrink groups of input weights to zero. We also propose a\nhierarchical penalty for simultaneous Granger causality and lag estimation. We\nvalidate our approach on simulated data from both a sparse linear\nautoregressive model and the sparse and nonlinear Lorenz-96 model.\n", "versions": [{"version": "v1", "created": "Wed, 22 Nov 2017 07:44:20 GMT"}, {"version": "v2", "created": "Mon, 25 Jun 2018 17:47:45 GMT"}], "update_date": "2018-06-26", "authors_parsed": [["Tank", "Alex", ""], ["Cover", "Ian", ""], ["Foti", "Nicholas J.", ""], ["Shojaie", "Ali", ""], ["Fox", "Emily B.", ""]]}, {"id": "1711.08171", "submitter": "Shota Saito", "authors": "Shota Saito, Danilo P Mandic, Hideyuki Suzuki", "title": "Hypergraph $p$-Laplacian: A Differential Geometry View", "comments": "Extended version of our AAAI-18 paper", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The graph Laplacian plays key roles in information processing of relational\ndata, and has analogies with the Laplacian in differential geometry. In this\npaper, we generalize the analogy between graph Laplacian and differential\ngeometry to the hypergraph setting, and propose a novel hypergraph\n$p$-Laplacian. Unlike the existing two-node graph Laplacians, this\ngeneralization makes it possible to analyze hypergraphs, where the edges are\nallowed to connect any number of nodes. Moreover, we propose a semi-supervised\nlearning method based on the proposed hypergraph $p$-Laplacian, and formalize\nthem as the analogue to the Dirichlet problem, which often appears in physics.\nWe further explore theoretical connections to normalized hypergraph cut on a\nhypergraph, and propose normalized cut corresponding to hypergraph\n$p$-Laplacian. The proposed $p$-Laplacian is shown to outperform standard\nhypergraph Laplacians in the experiment on a hypergraph semi-supervised\nlearning and normalized cut setting.\n", "versions": [{"version": "v1", "created": "Wed, 22 Nov 2017 08:12:23 GMT"}], "update_date": "2017-11-23", "authors_parsed": [["Saito", "Shota", ""], ["Mandic", "Danilo P", ""], ["Suzuki", "Hideyuki", ""]]}, {"id": "1711.08172", "submitter": "Yuejiao Sun", "authors": "Yifan Chen, Yuejiao Sun, Wotao Yin", "title": "Run-and-Inspect Method for Nonconvex Optimization and Global Optimality\n  Bounds for R-Local Minimizers", "comments": null, "journal-ref": null, "doi": null, "report-no": "UCLA CAM 17-67", "categories": "math.OC cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Many optimization algorithms converge to stationary points. When the\nunderlying problem is nonconvex, they may get trapped at local minimizers and\noccasionally stagnate near saddle points. We propose the Run-and-Inspect\nMethod, which adds an \"inspect\" phase to existing algorithms that helps escape\nfrom non-global stationary points. The inspection samples a set of points in a\nradius $R$ around the current point. When a sample point yields a sufficient\ndecrease in the objective, we move there and resume an existing algorithm. If\nno sufficient decrease is found, the current point is called an approximate\n$R$-local minimizer. We show that an $R$-local minimizer is globally optimal,\nup to a specific error depending on $R$, if the objective function can be\nimplicitly decomposed into a smooth convex function plus a restricted function\nthat is possibly nonconvex, nonsmooth. For high-dimensional problems, we\nintroduce blockwise inspections to overcome the curse of dimensionality while\nstill maintaining optimality bounds up to a factor equal to the number of\nblocks. Our method performs well on a set of artificial and realistic nonconvex\nproblems by coupling with gradient descent, coordinate descent, EM, and\nprox-linear algorithms.\n", "versions": [{"version": "v1", "created": "Wed, 22 Nov 2017 08:15:03 GMT"}, {"version": "v2", "created": "Fri, 29 Jun 2018 07:03:25 GMT"}], "update_date": "2018-07-02", "authors_parsed": [["Chen", "Yifan", ""], ["Sun", "Yuejiao", ""], ["Yin", "Wotao", ""]]}, {"id": "1711.08208", "submitter": "Sebasti\\'an Casta\\~no-Candamil", "authors": "Sebastian Casta\\~no-Candamil and Andreas Meinel and Michael Tangermann", "title": "Post-hoc labeling of arbitrary EEG recordings for data-efficient\n  evaluation of neural decoding methods", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.AP stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Many cognitive, sensory and motor processes have correlates in oscillatory\nneural sources, which are embedded as a subspace into the recorded brain\nsignals. Decoding such processes from noisy\nmagnetoencephalogram/electroencephalogram (M/EEG) signals usually requires the\nuse of data-driven analysis methods. The objective evaluation of such decoding\nalgorithms on experimental raw signals, however, is a challenge: the amount of\navailable M/EEG data typically is limited, labels can be unreliable, and raw\nsignals often are contaminated with artifacts. The latter is specifically\nproblematic, if the artifacts stem from behavioral confounds of the oscillatory\nneural processes of interest.\n  To overcome some of these problems, simulation frameworks have been\nintroduced for benchmarking decoding methods. Generating artificial brain\nsignals, however, most simulation frameworks make strong and partially\nunrealistic assumptions about brain activity, which limits the generalization\nof obtained results to real-world conditions.\n  In the present contribution, we thrive to remove many shortcomings of current\nsimulation frameworks and propose a versatile alternative, that allows for\nobjective evaluation and benchmarking of novel data-driven decoding methods for\nneural signals. Its central idea is to utilize post-hoc labelings of arbitrary\nM/EEG recordings. This strategy makes it paradigm-agnostic and allows to\ngenerate comparatively large datasets with noiseless labels. Source code and\ndata of the novel simulation approach are made available for facilitating its\nadoption.\n", "versions": [{"version": "v1", "created": "Wed, 22 Nov 2017 10:14:51 GMT"}], "update_date": "2017-11-23", "authors_parsed": [["Casta\u00f1o-Candamil", "Sebastian", ""], ["Meinel", "Andreas", ""], ["Tangermann", "Michael", ""]]}, {"id": "1711.08244", "submitter": "Ambrish Rawat", "authors": "Ambrish Rawat, Martin Wistuba, Maria-Irina Nicolae", "title": "Adversarial Phenomenon in the Eyes of Bayesian Deep Learning", "comments": "13 pages, 7 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Deep Learning models are vulnerable to adversarial examples, i.e.\\ images\nobtained via deliberate imperceptible perturbations, such that the model\nmisclassifies them with high confidence. However, class confidence by itself is\nan incomplete picture of uncertainty. We therefore use principled Bayesian\nmethods to capture model uncertainty in prediction for observing adversarial\nmisclassification. We provide an extensive study with different Bayesian neural\nnetworks attacked in both white-box and black-box setups. The behaviour of the\nnetworks for noise, attacks and clean test data is compared. We observe that\nBayesian neural networks are uncertain in their predictions for adversarial\nperturbations, a behaviour similar to the one observed for random Gaussian\nperturbations. Thus, we conclude that Bayesian neural networks can be\nconsidered for detecting adversarial examples.\n", "versions": [{"version": "v1", "created": "Wed, 22 Nov 2017 12:02:53 GMT"}], "update_date": "2017-11-23", "authors_parsed": [["Rawat", "Ambrish", ""], ["Wistuba", "Martin", ""], ["Nicolae", "Maria-Irina", ""]]}, {"id": "1711.08247", "submitter": "Stefano Teso", "authors": "Paolo Dragone, Stefano Teso, Mohit Kumar, Andrea Passerini", "title": "Decomposition Strategies for Constructive Preference Elicitation", "comments": "Accepted at the Thirty-Second AAAI Conference on Artificial\n  Intelligence (AAAI-18)", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We tackle the problem of constructive preference elicitation, that is the\nproblem of learning user preferences over very large decision problems,\ninvolving a combinatorial space of possible outcomes. In this setting, the\nsuggested configuration is synthesized on-the-fly by solving a constrained\noptimization problem, while the preferences are learned itera tively by\ninteracting with the user. Previous work has shown that Coactive Learning is a\nsuitable method for learning user preferences in constructive scenarios. In\nCoactive Learning the user provides feedback to the algorithm in the form of an\nimprovement to a suggested configuration. When the problem involves many\ndecision variables and constraints, this type of interaction poses a\nsignificant cognitive burden on the user. We propose a decomposition technique\nfor large preference-based decision problems relying exclusively on inference\nand feedback over partial configurations. This has the clear advantage of\ndrastically reducing the user cognitive load. Additionally, part-wise inference\ncan be (up to exponentially) less computationally demanding than inference over\nfull configurations. We discuss the theoretical implications of working with\nparts and present promising empirical results on one synthetic and two\nrealistic constructive problems.\n", "versions": [{"version": "v1", "created": "Wed, 22 Nov 2017 12:16:40 GMT"}, {"version": "v2", "created": "Sun, 6 May 2018 11:15:50 GMT"}], "update_date": "2018-05-08", "authors_parsed": [["Dragone", "Paolo", ""], ["Teso", "Stefano", ""], ["Kumar", "Mohit", ""], ["Passerini", "Andrea", ""]]}, {"id": "1711.08267", "submitter": "Hongwei Wang", "authors": "Hongwei Wang, Jia Wang, Jialin Wang, Miao Zhao, Weinan Zhang, Fuzheng\n  Zhang, Xing Xie, Minyi Guo", "title": "GraphGAN: Graph Representation Learning with Generative Adversarial Nets", "comments": "The 32nd AAAI Conference on Artificial Intelligence (AAAI 2018), 8\n  pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The goal of graph representation learning is to embed each vertex in a graph\ninto a low-dimensional vector space. Existing graph representation learning\nmethods can be classified into two categories: generative models that learn the\nunderlying connectivity distribution in the graph, and discriminative models\nthat predict the probability of edge existence between a pair of vertices. In\nthis paper, we propose GraphGAN, an innovative graph representation learning\nframework unifying above two classes of methods, in which the generative model\nand discriminative model play a game-theoretical minimax game. Specifically,\nfor a given vertex, the generative model tries to fit its underlying true\nconnectivity distribution over all other vertices and produces \"fake\" samples\nto fool the discriminative model, while the discriminative model tries to\ndetect whether the sampled vertex is from ground truth or generated by the\ngenerative model. With the competition between these two models, both of them\ncan alternately and iteratively boost their performance. Moreover, when\nconsidering the implementation of generative model, we propose a novel graph\nsoftmax to overcome the limitations of traditional softmax function, which can\nbe proven satisfying desirable properties of normalization, graph structure\nawareness, and computational efficiency. Through extensive experiments on\nreal-world datasets, we demonstrate that GraphGAN achieves substantial gains in\na variety of applications, including link prediction, node classification, and\nrecommendation, over state-of-the-art baselines.\n", "versions": [{"version": "v1", "created": "Wed, 22 Nov 2017 13:20:17 GMT"}], "update_date": "2017-11-23", "authors_parsed": [["Wang", "Hongwei", ""], ["Wang", "Jia", ""], ["Wang", "Jialin", ""], ["Zhao", "Miao", ""], ["Zhang", "Weinan", ""], ["Zhang", "Fuzheng", ""], ["Xie", "Xing", ""], ["Guo", "Minyi", ""]]}, {"id": "1711.08277", "submitter": "Boyang Deng", "authors": "Boyang Deng, Qing Liu, Siyuan Qiao, Alan Yuille", "title": "Few-shot Learning by Exploiting Visual Concepts within CNNs", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Convolutional neural networks (CNNs) are one of the driving forces for the\nadvancement of computer vision. Despite their promising performances on many\ntasks, CNNs still face major obstacles on the road to achieving ideal machine\nintelligence. One is that CNNs are complex and hard to interpret. Another is\nthat standard CNNs require large amounts of annotated data, which is sometimes\nhard to obtain, and it is desirable to learn to recognize objects from few\nexamples. In this work, we address these limitations of CNNs by developing\nnovel, flexible, and interpretable models for few-shot learning. Our models are\nbased on the idea of encoding objects in terms of visual concepts (VCs), which\nare interpretable visual cues represented by the feature vectors within CNNs.\nWe first adapt the learning of VCs to the few-shot setting, and then uncover\ntwo key properties of feature encoding using VCs, which we call category\nsensitivity and spatial pattern. Motivated by these properties, we present two\nintuitive models for the problem of few-shot learning. Experiments show that\nour models achieve competitive performances, while being more flexible and\ninterpretable than alternative state-of-the-art few-shot learning methods. We\nconclude that using VCs helps expose the natural capability of CNNs for\nfew-shot learning.\n", "versions": [{"version": "v1", "created": "Wed, 22 Nov 2017 13:44:44 GMT"}, {"version": "v2", "created": "Tue, 16 Jan 2018 13:09:51 GMT"}, {"version": "v3", "created": "Tue, 13 Feb 2018 12:30:09 GMT"}], "update_date": "2018-02-14", "authors_parsed": [["Deng", "Boyang", ""], ["Liu", "Qing", ""], ["Qiao", "Siyuan", ""], ["Yuille", "Alan", ""]]}, {"id": "1711.08325", "submitter": "Elham Taghizadeh", "authors": "Elham Taghizadeh", "title": "Utilizing artificial neural networks to predict demand for\n  weather-sensitive products at retail stores", "comments": null, "journal-ref": "Proceedings of the International Annual Conference of the American\n  Society for Engineering Management 2017", "doi": null, "report-no": "2010278851", "categories": "cs.LG stat.ML", "license": "http://creativecommons.org/publicdomain/zero/1.0/", "abstract": "  One key requirement for effective supply chain management is the quality of\nits inventory management. Various inventory management methods are typically\nemployed for different types of products based on their demand patterns,\nproduct attributes, and supply network. In this paper, our goal is to develop\nrobust demand prediction methods for weather sensitive products at retail\nstores. We employ historical datasets from Walmart, whose customers and markets\nare often exposed to extreme weather events which can have a huge impact on\nsales regarding the affected stores and products. We want to accurately predict\nthe sales of 111 potentially weather-sensitive products around the time of\nmajor weather events at 45 of Walmart retails locations in the U.S.\nIntuitively, we may expect an uptick in the sales of umbrellas before a big\nthunderstorm, but it is difficult for replenishment managers to predict the\nlevel of inventory needed to avoid being out-of-stock or overstock during and\nafter that storm. While they rely on a variety of vendor tools to predict sales\naround extreme weather events, they mostly employ a time-consuming process that\nlacks a systematic measure of effectiveness. We employ all the methods critical\nto any analytics project and start with data exploration. Critical features are\nextracted from the raw historical dataset for demand forecasting accuracy and\nrobustness. In particular, we employ Artificial Neural Network for forecasting\ndemand for each product sold around the time of major weather events. Finally,\nwe evaluate our model to evaluate their accuracy and robustness.\n", "versions": [{"version": "v1", "created": "Mon, 20 Nov 2017 15:58:58 GMT"}], "update_date": "2020-11-17", "authors_parsed": [["Taghizadeh", "Elham", ""]]}, {"id": "1711.08330", "submitter": "Oleg Ivanov", "authors": "Oleg Ivanov, Sergey Bartunov", "title": "Adaptive Cardinality Estimation", "comments": "12 pages, 11 figures, 1 table", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper we address cardinality estimation problem which is an important\nsubproblem in query optimization. Query optimization is a part of every\nrelational DBMS responsible for finding the best way of the execution for the\ngiven query. These ways are called plans. The execution time of different plans\nmay differ by several orders, so query optimizer has a great influence on the\nwhole DBMS performance. We consider cost-based query optimization approach as\nthe most popular one. It was observed that cost-based optimization quality\ndepends much on cardinality estimation quality. Cardinality of the plan node is\nthe number of tuples returned by it.\n  In the paper we propose a novel cardinality estimation approach with the use\nof machine learning methods. The main point of the approach is using query\nexecution statistics of the previously executed queries to improve cardinality\nestimations. We called this approach adaptive cardinality estimation to reflect\nthis point. The approach is general, flexible, and easy to implement. The\nexperimental evaluation shows that this approach significantly increases the\nquality of cardinality estimation, and therefore increases the DBMS performance\nfor some queries by several times or even by several dozens of times.\n", "versions": [{"version": "v1", "created": "Wed, 22 Nov 2017 15:20:19 GMT"}], "update_date": "2017-11-23", "authors_parsed": [["Ivanov", "Oleg", ""], ["Bartunov", "Sergey", ""]]}, {"id": "1711.08331", "submitter": "Adish Singla", "authors": "Christoph Hirnschall, Adish Singla, Sebastian Tschiatschek, Andreas\n  Krause", "title": "Learning User Preferences to Incentivize Exploration in the Sharing\n  Economy", "comments": "Longer version of AAAI'18 paper. arXiv admin note: text overlap with\n  arXiv:1702.02849", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.SI stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study platforms in the sharing economy and discuss the need for\nincentivizing users to explore options that otherwise would not be chosen. For\ninstance, rental platforms such as Airbnb typically rely on customer reviews to\nprovide users with relevant information about different options. Yet, often a\nlarge fraction of options does not have any reviews available. Such options are\nfrequently neglected as viable choices, and in turn are unlikely to be\nevaluated, creating a vicious cycle. Platforms can engage users to deviate from\ntheir preferred choice by offering monetary incentives for choosing a different\noption instead. To efficiently learn the optimal incentives to offer, we\nconsider structural information in user preferences and introduce a novel\nalgorithm - Coordinated Online Learning (CoOL) - for learning with structural\ninformation modeled as convex constraints. We provide formal guarantees on the\nperformance of our algorithm and test the viability of our approach in a user\nstudy with data of apartments on Airbnb. Our findings suggest that our approach\nis well-suited to learn appropriate incentives and increase exploration on the\ninvestigated platform.\n", "versions": [{"version": "v1", "created": "Fri, 17 Nov 2017 22:19:19 GMT"}, {"version": "v2", "created": "Fri, 24 Nov 2017 16:03:55 GMT"}], "update_date": "2017-11-27", "authors_parsed": [["Hirnschall", "Christoph", ""], ["Singla", "Adish", ""], ["Tschiatschek", "Sebastian", ""], ["Krause", "Andreas", ""]]}, {"id": "1711.08336", "submitter": "Eli (Omid) David", "authors": "Eli David, Nathan S. Netanyahu", "title": "DeepSign: Deep Learning for Automatic Malware Signature Generation and\n  Classification", "comments": null, "journal-ref": "International Joint Conference on Neural Networks (IJCNN), pages\n  1-8, Killarney, Ireland, July 2015", "doi": "10.1109/IJCNN.2015.7280815", "report-no": null, "categories": "cs.CR cs.LG cs.NE stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper presents a novel deep learning based method for automatic malware\nsignature generation and classification. The method uses a deep belief network\n(DBN), implemented with a deep stack of denoising autoencoders, generating an\ninvariant compact representation of the malware behavior. While conventional\nsignature and token based methods for malware detection do not detect a\nmajority of new variants for existing malware, the results presented in this\npaper show that signatures generated by the DBN allow for an accurate\nclassification of new malware variants. Using a dataset containing hundreds of\nvariants for several major malware families, our method achieves 98.6%\nclassification accuracy using the signatures generated by the DBN. The\npresented method is completely agnostic to the type of malware behavior that is\nlogged (e.g., API calls and their parameters, registry entries, websites and\nports accessed, etc.), and can use any raw input from a sandbox to successfully\ntrain the deep neural network which is used to generate malware signatures.\n", "versions": [{"version": "v1", "created": "Tue, 21 Nov 2017 07:22:58 GMT"}, {"version": "v2", "created": "Thu, 23 Nov 2017 16:27:18 GMT"}], "update_date": "2017-11-27", "authors_parsed": [["David", "Eli", ""], ["Netanyahu", "Nathan S.", ""]]}, {"id": "1711.08337", "submitter": "Eli (Omid) David", "authors": "Eli David, H. Jaap van den Herik, Moshe Koppel, Nathan S. Netanyahu", "title": "Genetic Algorithms for Evolving Computer Chess Programs", "comments": "Winner of Gold Award in 11th Annual \"Humies\" Awards for\n  Human-Competitive Results. arXiv admin note: substantial text overlap with\n  arXiv:1711.06840, arXiv:1711.06841, arXiv:1711.06839", "journal-ref": "IEEE Transactions on Evolutionary Computation, Vol. 18, No. 5, pp.\n  779-789, September 2014", "doi": "10.1109/TEVC.2013.2285111", "report-no": null, "categories": "cs.NE cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper demonstrates the use of genetic algorithms for evolving: 1) a\ngrandmaster-level evaluation function, and 2) a search mechanism for a chess\nprogram, the parameter values of which are initialized randomly. The evaluation\nfunction of the program is evolved by learning from databases of (human)\ngrandmaster games. At first, the organisms are evolved to mimic the behavior of\nhuman grandmasters, and then these organisms are further improved upon by means\nof coevolution. The search mechanism is evolved by learning from tactical test\nsuites. Our results show that the evolved program outperforms a two-time world\ncomputer chess champion and is at par with the other leading computer chess\nprograms.\n", "versions": [{"version": "v1", "created": "Tue, 21 Nov 2017 07:24:24 GMT"}], "update_date": "2017-11-23", "authors_parsed": [["David", "Eli", ""], ["Herik", "H. Jaap van den", ""], ["Koppel", "Moshe", ""], ["Netanyahu", "Nathan S.", ""]]}, {"id": "1711.08352", "submitter": "Guoqing Zheng", "authors": "Guoqing Zheng, Yiming Yang, Jaime Carbonell", "title": "Asymmetric Variational Autoencoders", "comments": "ICML 2018 Workshop on Theoretical Foundations and Applications of\n  Deep Generative Models", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Variational inference for latent variable models is prevalent in various\nmachine learning problems, typically solved by maximizing the Evidence Lower\nBound (ELBO) of the true data likelihood with respect to a variational\ndistribution. However, freely enriching the family of variational distribution\nis challenging since the ELBO requires variational likelihood evaluations of\nthe latent variables. In this paper, we propose a novel framework to enrich the\nvariational family by incorporating auxiliary variables to the variational\nfamily. The resulting inference network doesn't require density evaluations for\nthe auxiliary variables and thus complex implicit densities over the auxiliary\nvariables can be constructed by neural networks. It can be shown that the\nactual variational posterior of the proposed approach is essentially modeling a\nrich probabilistic mixture of simple variational posterior indexed by auxiliary\nvariables, thus a flexible inference model can be built. Empirical evaluations\non several density estimation tasks demonstrates the effectiveness of the\nproposed method.\n", "versions": [{"version": "v1", "created": "Mon, 20 Nov 2017 21:27:32 GMT"}, {"version": "v2", "created": "Mon, 9 Jul 2018 18:44:43 GMT"}], "update_date": "2018-07-11", "authors_parsed": [["Zheng", "Guoqing", ""], ["Yang", "Yiming", ""], ["Carbonell", "Jaime", ""]]}, {"id": "1711.08359", "submitter": "Wolfgang Fr\\\"uhwirt", "authors": "Wolfgang Fruehwirt, Matthias Gerstgrasser, Pengfei Zhang, Leonard\n  Weydemann, Markus Waser, Reinhold Schmidt, Thomas Benke, Peter Dal-Bianco,\n  Gerhard Ransmayr, Dieter Grossegger, Heinrich Garn, Gareth W. Peters, Stephen\n  Roberts, Georg Dorffner", "title": "Riemannian tangent space mapping and elastic net regularization for\n  cost-effective EEG markers of brain atrophy in Alzheimer's disease", "comments": "Presented at NIPS 2017 Workshop on Machine Learning for Health", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML eess.SP q-bio.NC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The diagnosis of Alzheimer's disease (AD) in routine clinical practice is\nmost commonly based on subjective clinical interpretations. Quantitative\nelectroencephalography (QEEG) measures have been shown to reflect\nneurodegenerative processes in AD and might qualify as affordable and thereby\nwidely available markers to facilitate the objectivization of AD assessment.\nHere, we present a novel framework combining Riemannian tangent space mapping\nand elastic net regression for the development of brain atrophy markers. While\nmost AD QEEG studies are based on small sample sizes and psychological test\nscores as outcome measures, here we train and test our models using data of one\nof the largest prospective EEG AD trials ever conducted, including MRI\nbiomarkers of brain atrophy.\n", "versions": [{"version": "v1", "created": "Wed, 22 Nov 2017 16:00:04 GMT"}], "update_date": "2017-11-24", "authors_parsed": [["Fruehwirt", "Wolfgang", ""], ["Gerstgrasser", "Matthias", ""], ["Zhang", "Pengfei", ""], ["Weydemann", "Leonard", ""], ["Waser", "Markus", ""], ["Schmidt", "Reinhold", ""], ["Benke", "Thomas", ""], ["Dal-Bianco", "Peter", ""], ["Ransmayr", "Gerhard", ""], ["Grossegger", "Dieter", ""], ["Garn", "Heinrich", ""], ["Peters", "Gareth W.", ""], ["Roberts", "Stephen", ""], ["Dorffner", "Georg", ""]]}, {"id": "1711.08364", "submitter": "Qiang Qiu", "authors": "Qiang Qiu, Jose Lezama, Alex Bronstein, Guillermo Sapiro", "title": "ForestHash: Semantic Hashing With Shallow Random Forests and Tiny\n  Convolutional Networks", "comments": "Accepted to ECCV 2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Hash codes are efficient data representations for coping with the ever\ngrowing amounts of data. In this paper, we introduce a random forest semantic\nhashing scheme that embeds tiny convolutional neural networks (CNN) into\nshallow random forests, with near-optimal information-theoretic code\naggregation among trees. We start with a simple hashing scheme, where random\ntrees in a forest act as hashing functions by setting `1' for the visited tree\nleaf, and `0' for the rest. We show that traditional random forests fail to\ngenerate hashes that preserve the underlying similarity between the trees,\nrendering the random forests approach to hashing challenging. To address this,\nwe propose to first randomly group arriving classes at each tree split node\ninto two groups, obtaining a significantly simplified two-class classification\nproblem, which can be handled using a light-weight CNN weak learner. Such\nrandom class grouping scheme enables code uniqueness by enforcing each class to\nshare its code with different classes in different trees. A non-conventional\nlow-rank loss is further adopted for the CNN weak learners to encourage code\nconsistency by minimizing intra-class variations and maximizing inter-class\ndistance for the two random class groups. Finally, we introduce an\ninformation-theoretic approach for aggregating codes of individual trees into a\nsingle hash code, producing a near-optimal unique hash for each class. The\nproposed approach significantly outperforms state-of-the-art hashing methods\nfor image retrieval tasks on large-scale public datasets, while performing at\nthe level of other state-of-the-art image classification techniques while\nutilizing a more compact and efficient scalable representation. This work\nproposes a principled and robust procedure to train and deploy in parallel an\nensemble of light-weight CNNs, instead of simply going deeper.\n", "versions": [{"version": "v1", "created": "Wed, 22 Nov 2017 16:16:42 GMT"}, {"version": "v2", "created": "Sat, 28 Jul 2018 00:06:37 GMT"}], "update_date": "2018-07-31", "authors_parsed": [["Qiu", "Qiang", ""], ["Lezama", "Jose", ""], ["Bronstein", "Alex", ""], ["Sapiro", "Guillermo", ""]]}, {"id": "1711.08374", "submitter": "Guillaume Revillon", "authors": "G. Revillon, A. Djafari and C. Enderli", "title": "Variational Bayesian Inference For A Scale Mixture Of Normal\n  Distributions Handling Missing Data", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, a scale mixture of Normal distributions model is developed for\nclassification and clustering of data having outliers and missing values. The\nclassification method, based on a mixture model, focuses on the introduction of\nlatent variables that gives us the possibility to handle sensitivity of model\nto outliers and to allow a less restrictive modelling of missing data.\nInference is processed through a Variational Bayesian Approximation and a\nBayesian treatment is adopted for model learning, supervised classification and\nclustering.\n", "versions": [{"version": "v1", "created": "Wed, 22 Nov 2017 16:28:42 GMT"}], "update_date": "2017-11-23", "authors_parsed": [["Revillon", "G.", ""], ["Djafari", "A.", ""], ["Enderli", "C.", ""]]}, {"id": "1711.08392", "submitter": "Alexander Tank", "authors": "Alex Tank, Emily B. Fox, Ali Shojaie", "title": "An Efficient ADMM Algorithm for Structural Break Detection in\n  Multivariate Time Series", "comments": "Accepted to the NIPS Time Series Workshop 2017", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present an efficient alternating direction method of multipliers (ADMM)\nalgorithm for segmenting a multivariate non-stationary time series with\nstructural breaks into stationary regions. We draw from recent work where the\nseries is assumed to follow a vector autoregressive model within segments and a\nconvex estimation procedure may be formulated using group fused lasso\npenalties. Our ADMM approach first splits the convex problem into a global\nquadratic program and a simple group lasso proximal update. We show that the\nglobal problem may be parallelized over rows of the time dependent transition\nmatrices and furthermore that each subproblem may be rewritten in a form\nidentical to the log-likelihood of a Gaussian state space model. Consequently,\nwe develop a Kalman smoothing algorithm to solve the global update in time\nlinear in the length of the series.\n", "versions": [{"version": "v1", "created": "Wed, 22 Nov 2017 17:00:39 GMT"}, {"version": "v2", "created": "Mon, 25 Jun 2018 17:34:03 GMT"}], "update_date": "2018-06-26", "authors_parsed": [["Tank", "Alex", ""], ["Fox", "Emily B.", ""], ["Shojaie", "Ali", ""]]}, {"id": "1711.08413", "submitter": "Subhadip Dey", "authors": "Subhadip Dey, Sawon Pratiher, Saon Banerjee, Chanchal Kumar Mukherjee", "title": "SolarisNet: A Deep Regression Network for Solar Radiation Prediction", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV stat.AP stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Effective utilization of photovoltaic (PV) plants requires weather\nvariability robust global solar radiation (GSR) forecasting models. Random\nweather turbulence phenomena coupled with assumptions of clear sky model as\nsuggested by Hottel pose significant challenges to parametric & non-parametric\nmodels in GSR conversion rate estimation. Also, a decent GSR estimate requires\ncostly high-tech radiometer and expert dependent instrument handling and\nmeasurements, which are subjective. As such, a computer aided monitoring (CAM)\nsystem to evaluate PV plant operation feasibility by employing smart grid past\ndata analytics and deep learning is developed. Our algorithm, SolarisNet is a\n6-layer deep neural network trained on data collected at two weather stations\nlocated near Kalyani metrological site, West Bengal, India. The daily GSR\nprediction performance using SolarisNet outperforms the existing state of art\nand its efficacy in inferring past GSR data insights to comprehend daily and\nseasonal GSR variability along with its competence for short term forecasting\nis discussed.\n", "versions": [{"version": "v1", "created": "Wed, 22 Nov 2017 17:41:40 GMT"}, {"version": "v2", "created": "Sun, 10 Dec 2017 18:56:29 GMT"}], "update_date": "2017-12-12", "authors_parsed": [["Dey", "Subhadip", ""], ["Pratiher", "Sawon", ""], ["Banerjee", "Saon", ""], ["Mukherjee", "Chanchal Kumar", ""]]}, {"id": "1711.08421", "submitter": "Ryan Urbanowicz", "authors": "Ryan J. Urbanowicz, Melissa Meeker, William LaCava, Randal S. Olson,\n  Jason H. Moore", "title": "Relief-Based Feature Selection: Introduction and Review", "comments": "Submitted revisions for publication based on reviews by the Journal\n  of Biomedical Informatics", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.LG stat.ML", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Feature selection plays a critical role in biomedical data mining, driven by\nincreasing feature dimensionality in target problems and growing interest in\nadvanced but computationally expensive methodologies able to model complex\nassociations. Specifically, there is a need for feature selection methods that\nare computationally efficient, yet sensitive to complex patterns of\nassociation, e.g. interactions, so that informative features are not mistakenly\neliminated prior to downstream modeling. This paper focuses on Relief-based\nalgorithms (RBAs), a unique family of filter-style feature selection algorithms\nthat have gained appeal by striking an effective balance between these\nobjectives while flexibly adapting to various data characteristics, e.g.\nclassification vs. regression. First, this work broadly examines types of\nfeature selection and defines RBAs within that context. Next, we introduce the\noriginal Relief algorithm and associated concepts, emphasizing the intuition\nbehind how it works, how feature weights generated by the algorithm can be\ninterpreted, and why it is sensitive to feature interactions without evaluating\ncombinations of features. Lastly, we include an expansive review of RBA\nmethodological research beyond Relief and its popular descendant, ReliefF. In\nparticular, we characterize branches of RBA research, and provide comparative\nsummaries of RBA algorithms including contributions, strategies, functionality,\ntime complexity, adaptation to key data characteristics, and software\navailability.\n", "versions": [{"version": "v1", "created": "Wed, 22 Nov 2017 18:06:25 GMT"}, {"version": "v2", "created": "Mon, 2 Apr 2018 20:46:42 GMT"}], "update_date": "2018-04-04", "authors_parsed": [["Urbanowicz", "Ryan J.", ""], ["Meeker", "Melissa", ""], ["LaCava", "William", ""], ["Olson", "Randal S.", ""], ["Moore", "Jason H.", ""]]}, {"id": "1711.08426", "submitter": "Naman Agarwal", "authors": "Naman Agarwal, Sham Kakade, Rahul Kidambi, Yin Tat Lee, Praneeth\n  Netrapalli, Aaron Sidford", "title": "Leverage Score Sampling for Faster Accelerated Regression and ERM", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG math.OC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Given a matrix $\\mathbf{A}\\in\\mathbb{R}^{n\\times d}$ and a vector $b\n\\in\\mathbb{R}^{d}$, we show how to compute an $\\epsilon$-approximate solution\nto the regression problem $ \\min_{x\\in\\mathbb{R}^{d}}\\frac{1}{2} \\|\\mathbf{A} x\n- b\\|_{2}^{2} $ in time $ \\tilde{O} ((n+\\sqrt{d\\cdot\\kappa_{\\text{sum}}})\\cdot\ns\\cdot\\log\\epsilon^{-1}) $ where\n$\\kappa_{\\text{sum}}=\\mathrm{tr}\\left(\\mathbf{A}^{\\top}\\mathbf{A}\\right)/\\lambda_{\\min}(\\mathbf{A}^{T}\\mathbf{A})$\nand $s$ is the maximum number of non-zero entries in a row of $\\mathbf{A}$. Our\nalgorithm improves upon the previous best running time of $ \\tilde{O}\n((n+\\sqrt{n \\cdot\\kappa_{\\text{sum}}})\\cdot s\\cdot\\log\\epsilon^{-1})$.\n  We achieve our result through a careful combination of leverage score\nsampling techniques, proximal point methods, and accelerated coordinate\ndescent. Our method not only matches the performance of previous methods, but\nfurther improves whenever leverage scores of rows are small (up to\npolylogarithmic factors). We also provide a non-linear generalization of these\nresults that improves the running time for solving a broader class of ERM\nproblems.\n", "versions": [{"version": "v1", "created": "Wed, 22 Nov 2017 18:18:22 GMT"}], "update_date": "2017-11-23", "authors_parsed": [["Agarwal", "Naman", ""], ["Kakade", "Sham", ""], ["Kidambi", "Rahul", ""], ["Lee", "Yin Tat", ""], ["Netrapalli", "Praneeth", ""], ["Sidford", "Aaron", ""]]}, {"id": "1711.08442", "submitter": "Mayank Kakodkar", "authors": "Pedro H. P. Savarese, Mayank Kakodkar, Bruno Ribeiro", "title": "From Monte Carlo to Las Vegas: Improving Restricted Boltzmann Machine\n  Training Through Stopping Sets", "comments": "AAAI2018, 10 Pages", "journal-ref": "Proceedings of the Thirty-Second {AAAI} Conference on Artificial\n  Intelligence, New Orleans, Louisiana, USA, February 2-7, 2018", "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a Las Vegas transformation of Markov Chain Monte Carlo (MCMC)\nestimators of Restricted Boltzmann Machines (RBMs). We denote our approach\nMarkov Chain Las Vegas (MCLV). MCLV gives statistical guarantees in exchange\nfor random running times. MCLV uses a stopping set built from the training data\nand has maximum number of Markov chain steps K (referred as MCLV-K). We present\na MCLV-K gradient estimator (LVS-K) for RBMs and explore the correspondence and\ndifferences between LVS-K and Contrastive Divergence (CD-K), with LVS-K\nsignificantly outperforming CD-K training RBMs over the MNIST dataset,\nindicating MCLV to be a promising direction in learning generative models.\n", "versions": [{"version": "v1", "created": "Wed, 22 Nov 2017 18:38:22 GMT"}], "update_date": "2018-05-31", "authors_parsed": [["Savarese", "Pedro H. P.", ""], ["Kakodkar", "Mayank", ""], ["Ribeiro", "Bruno", ""]]}, {"id": "1711.08451", "submitter": "Xin Zhou", "authors": "Xin Zhou and Michael R. Kosorok", "title": "Causal nearest neighbor rules for optimal treatment regimes", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The estimation of optimal treatment regimes is of considerable interest to\nprecision medicine. In this work, we propose a causal $k$-nearest neighbor\nmethod to estimate the optimal treatment regime. The method roots in the\nframework of causal inference, and estimates the causal treatment effects\nwithin the nearest neighborhood. Although the method is simple, it possesses\nnice theoretical properties. We show that the causal $k$-nearest neighbor\nregime is universally consistent. That is, the causal $k$-nearest neighbor\nregime will eventually learn the optimal treatment regime as the sample size\nincreases. We also establish its convergence rate. However, the causal\n$k$-nearest neighbor regime may suffer from the curse of dimensionality, i.e.\nperformance deteriorates as dimensionality increases. To alleviate this\nproblem, we develop an adaptive causal $k$-nearest neighbor method to perform\nmetric selection and variable selection simultaneously. The performance of the\nproposed methods is illustrated in simulation studies and in an analysis of a\nchronic depression clinical trial.\n", "versions": [{"version": "v1", "created": "Wed, 22 Nov 2017 18:54:59 GMT"}], "update_date": "2017-11-23", "authors_parsed": [["Zhou", "Xin", ""], ["Kosorok", "Michael R.", ""]]}, {"id": "1711.08513", "submitter": "Michael P. Kim", "authors": "\\'Ursula H\\'ebert-Johnson, Michael P. Kim, Omer Reingold, Guy N.\n  Rothblum", "title": "Calibration for the (Computationally-Identifiable) Masses", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.DS stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  As algorithms increasingly inform and influence decisions made about\nindividuals, it becomes increasingly important to address concerns that these\nalgorithms might be discriminatory. The output of an algorithm can be\ndiscriminatory for many reasons, most notably: (1) the data used to train the\nalgorithm might be biased (in various ways) to favor certain populations over\nothers; (2) the analysis of this training data might inadvertently or\nmaliciously introduce biases that are not borne out in the data. This work\nfocuses on the latter concern.\n  We develop and study multicalbration -- a new measure of algorithmic fairness\nthat aims to mitigate concerns about discrimination that is introduced in the\nprocess of learning a predictor from data. Multicalibration guarantees accurate\n(calibrated) predictions for every subpopulation that can be identified within\na specified class of computations. We think of the class as being quite rich;\nin particular, it can contain many overlapping subgroups of a protected group.\n  We show that in many settings this strong notion of protection from\ndiscrimination is both attainable and aligned with the goal of obtaining\naccurate predictions. Along the way, we present new algorithms for learning a\nmulticalibrated predictor, study the computational complexity of this task, and\ndraw new connections to computational learning models such as agnostic\nlearning.\n", "versions": [{"version": "v1", "created": "Wed, 22 Nov 2017 21:47:55 GMT"}, {"version": "v2", "created": "Fri, 16 Mar 2018 17:50:06 GMT"}], "update_date": "2018-03-19", "authors_parsed": [["H\u00e9bert-Johnson", "\u00darsula", ""], ["Kim", "Michael P.", ""], ["Reingold", "Omer", ""], ["Rothblum", "Guy N.", ""]]}, {"id": "1711.08534", "submitter": "William Wang", "authors": "William Wang, Angelina Wang, Aviv Tamar, Xi Chen, Pieter Abbeel", "title": "Safer Classification by Synthesis", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The discriminative approach to classification using deep neural networks has\nbecome the de-facto standard in various fields. Complementing recent\nreservations about safety against adversarial examples, we show that\nconventional discriminative methods can easily be fooled to provide incorrect\nlabels with very high confidence to out of distribution examples. We posit that\na generative approach is the natural remedy for this problem, and propose a\nmethod for classification using generative models. At training time, we learn a\ngenerative model for each class, while at test time, given an example to\nclassify, we query each generator for its most similar generation, and select\nthe class corresponding to the most similar one. Our approach is general and\ncan be used with expressive models such as GANs and VAEs. At test time, our\nmethod accurately \"knows when it does not know,\" and provides resilience to out\nof distribution examples while maintaining competitive performance for standard\nexamples.\n", "versions": [{"version": "v1", "created": "Wed, 22 Nov 2017 23:32:20 GMT"}, {"version": "v2", "created": "Mon, 23 Jul 2018 23:47:59 GMT"}], "update_date": "2018-07-25", "authors_parsed": [["Wang", "William", ""], ["Wang", "Angelina", ""], ["Tamar", "Aviv", ""], ["Chen", "Xi", ""], ["Abbeel", "Pieter", ""]]}, {"id": "1711.08536", "submitter": "Shreya Shankar", "authors": "Shreya Shankar, Yoni Halpern, Eric Breck, James Atwood, Jimbo Wilson,\n  D. Sculley", "title": "No Classification without Representation: Assessing Geodiversity Issues\n  in Open Data Sets for the Developing World", "comments": "Presented at NIPS 2017 Workshop on Machine Learning for the\n  Developing World", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Modern machine learning systems such as image classifiers rely heavily on\nlarge scale data sets for training. Such data sets are costly to create, thus\nin practice a small number of freely available, open source data sets are\nwidely used. We suggest that examining the geo-diversity of open data sets is\ncritical before adopting a data set for use cases in the developing world. We\nanalyze two large, publicly available image data sets to assess geo-diversity\nand find that these data sets appear to exhibit an observable amerocentric and\neurocentric representation bias. Further, we analyze classifiers trained on\nthese data sets to assess the impact of these training distributions and find\nstrong differences in the relative performance on images from different\nlocales. These results emphasize the need to ensure geo-representation when\nconstructing data sets for use in the developing world.\n", "versions": [{"version": "v1", "created": "Wed, 22 Nov 2017 23:56:37 GMT"}], "update_date": "2017-11-27", "authors_parsed": [["Shankar", "Shreya", ""], ["Halpern", "Yoni", ""], ["Breck", "Eric", ""], ["Atwood", "James", ""], ["Wilson", "Jimbo", ""], ["Sculley", "D.", ""]]}, {"id": "1711.08576", "submitter": "Carlos Xavier Hernandez", "authors": "Carlos X. Hern\\'andez, Hannah K. Wayment-Steele, Mohammad M. Sultan,\n  Brooke E. Husic and Vijay S. Pande", "title": "Variational Encoding of Complex Dynamics", "comments": "Fixed typos and added references", "journal-ref": "Phys. Rev. E 97, 062412 (2018)", "doi": "10.1103/PhysRevE.97.062412", "report-no": null, "categories": "stat.ML physics.bio-ph physics.chem-ph physics.comp-ph q-bio.BM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Often the analysis of time-dependent chemical and biophysical systems\nproduces high-dimensional time-series data for which it can be difficult to\ninterpret which individual features are most salient. While recent work from\nour group and others has demonstrated the utility of time-lagged co-variate\nmodels to study such systems, linearity assumptions can limit the compression\nof inherently nonlinear dynamics into just a few characteristic components.\nRecent work in the field of deep learning has led to the development of\nvariational autoencoders (VAE), which are able to compress complex datasets\ninto simpler manifolds. We present the use of a time-lagged VAE, or variational\ndynamics encoder (VDE), to reduce complex, nonlinear processes to a single\nembedding with high fidelity to the underlying dynamics. We demonstrate how the\nVDE is able to capture nontrivial dynamics in a variety of examples, including\nBrownian dynamics and atomistic protein folding. Additionally, we demonstrate a\nmethod for analyzing the VDE model, inspired by saliency mapping, to determine\nwhat features are selected by the VDE model to describe dynamics. The VDE\npresents an important step in applying techniques from deep learning to more\naccurately model and interpret complex biophysics.\n", "versions": [{"version": "v1", "created": "Thu, 23 Nov 2017 05:14:47 GMT"}, {"version": "v2", "created": "Sat, 2 Dec 2017 00:19:26 GMT"}], "update_date": "2018-06-27", "authors_parsed": [["Hern\u00e1ndez", "Carlos X.", ""], ["Wayment-Steele", "Hannah K.", ""], ["Sultan", "Mohammad M.", ""], ["Husic", "Brooke E.", ""], ["Pande", "Vijay S.", ""]]}, {"id": "1711.08598", "submitter": "Maxime Voisin", "authors": "Maxime Voisin, Daniel Ritchie", "title": "An Improved Training Procedure for Neural Autoregressive Data Completion", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Neural autoregressive models are explicit density estimators that achieve\nstate-of-the-art likelihoods for generative modeling. The D-dimensional data\ndistribution is factorized into an autoregressive product of one-dimensional\nconditional distributions according to the chain rule. Data completion is a\nmore involved task than data generation: the model must infer missing variables\nfor any partially observed input vector. Previous work introduced an\norder-agnostic training procedure for data completion with autoregressive\nmodels. Missing variables in any partially observed input vector can be imputed\nefficiently by choosing an ordering where observed dimensions precede\nunobserved ones and by computing the autoregressive product in this order. In\nthis paper, we provide evidence that the order-agnostic (OA) training procedure\nis suboptimal for data completion. We propose an alternative procedure (OA++)\nthat reaches better performance in fewer computations. It can handle all data\ncompletion queries while training fewer one-dimensional conditional\ndistributions than the OA procedure. In addition, these one-dimensional\nconditional distributions are trained proportionally to their expected usage at\ninference time, reducing overfitting. Finally, our OA++ procedure can exploit\nprior knowledge about the distribution of inference completion queries, as\nopposed to OA. We support these claims with quantitative experiments on\nstandard datasets used to evaluate autoregressive generative models.\n", "versions": [{"version": "v1", "created": "Thu, 23 Nov 2017 07:41:50 GMT"}], "update_date": "2017-11-27", "authors_parsed": [["Voisin", "Maxime", ""], ["Ritchie", "Daniel", ""]]}, {"id": "1711.08621", "submitter": "Carolin Lawrence", "authors": "Carolin Lawrence, Pratik Gajane, Stefan Riezler", "title": "Counterfactual Learning for Machine Translation: Degeneracies and\n  Solutions", "comments": "Workshop \"From 'What If?' To 'What Next?'\" at the 31st Conference on\n  Neural Information Processing Systems (NIPS 2017), Long Beach, CA, USA", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.CL cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Counterfactual learning is a natural scenario to improve web-based machine\ntranslation services by offline learning from feedback logged during user\ninteractions. In order to avoid the risk of showing inferior translations to\nusers, in such scenarios mostly exploration-free deterministic logging policies\nare in place. We analyze possible degeneracies of inverse and reweighted\npropensity scoring estimators, in stochastic and deterministic settings, and\nrelate them to recently proposed techniques for counterfactual learning under\ndeterministic logging.\n", "versions": [{"version": "v1", "created": "Thu, 23 Nov 2017 08:54:05 GMT"}, {"version": "v2", "created": "Mon, 27 Nov 2017 13:25:49 GMT"}, {"version": "v3", "created": "Thu, 14 Dec 2017 13:47:21 GMT"}], "update_date": "2017-12-15", "authors_parsed": [["Lawrence", "Carolin", ""], ["Gajane", "Pratik", ""], ["Riezler", "Stefan", ""]]}, {"id": "1711.08646", "submitter": "Djork-Arn\\'e Clevert", "authors": "Robin Winter, Djork-Arn\\'e Clevert", "title": "IVE-GAN: Invariant Encoding Generative Adversarial Networks", "comments": "under review at ICLR2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Generative adversarial networks (GANs) are a powerful framework for\ngenerative tasks. However, they are difficult to train and tend to miss modes\nof the true data generation process. Although GANs can learn a rich\nrepresentation of the covered modes of the data in their latent space, the\nframework misses an inverse mapping from data to this latent space. We propose\nInvariant Encoding Generative Adversarial Networks (IVE-GANs), a novel GAN\nframework that introduces such a mapping for individual samples from the data\nby utilizing features in the data which are invariant to certain\ntransformations. Since the model maps individual samples to the latent space,\nit naturally encourages the generator to cover all modes. We demonstrate the\neffectiveness of our approach in terms of generative performance and learning\nrich representations on several datasets including common benchmark image\ngeneration tasks.\n", "versions": [{"version": "v1", "created": "Thu, 23 Nov 2017 10:36:52 GMT"}], "update_date": "2017-11-27", "authors_parsed": [["Winter", "Robin", ""], ["Clevert", "Djork-Arn\u00e9", ""]]}, {"id": "1711.08677", "submitter": "Badong Chen", "authors": "Wentao Ma, Dongqiao Zheng, Yuanhao Li, Zhiyu Zhang, Badong Chen", "title": "Bias-Compensated Normalized Maximum Correntropy Criterion Algorithm for\n  System Identification with Noisy Input", "comments": "14 pages, 4 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML eess.SP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper proposed a bias-compensated normalized maximum correntropy\ncriterion (BCNMCC) algorithm charactered by its low steady-state misalignment\nfor system identification with noisy input in an impulsive output noise\nenvironment. The normalized maximum correntropy criterion (NMCC) is derived\nfrom a correntropy based cost function, which is rather robust with respect to\nimpulsive noises. To deal with the noisy input, we introduce a bias-compensated\nvector (BCV) to the NMCC algorithm, and then an unbiasedness criterion and some\nreasonable assumptions are used to compute the BCV. Taking advantage of the\nBCV, the bias caused by the input noise can be effectively suppressed. System\nidentification simulation results demonstrate that the proposed BCNMCC\nalgorithm can outperform other related algorithms with noisy input especially\nin an impulsive output noise environment.\n", "versions": [{"version": "v1", "created": "Thu, 23 Nov 2017 13:02:07 GMT"}], "update_date": "2017-11-27", "authors_parsed": [["Ma", "Wentao", ""], ["Zheng", "Dongqiao", ""], ["Li", "Yuanhao", ""], ["Zhang", "Zhiyu", ""], ["Chen", "Badong", ""]]}, {"id": "1711.08682", "submitter": "Chunyan Bai", "authors": "Haoye Cai, Chunyan Bai, Yu-Wing Tai, Chi-Keung Tang", "title": "Deep Video Generation, Prediction and Completion of Human Action\n  Sequences", "comments": "Under review for CVPR 2018. Haoye and Chunyan have equal contribution", "journal-ref": null, "doi": "10.1007/978-3-030-01216-8_23", "report-no": null, "categories": "cs.CV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Current deep learning results on video generation are limited while there are\nonly a few first results on video prediction and no relevant significant\nresults on video completion. This is due to the severe ill-posedness inherent\nin these three problems. In this paper, we focus on human action videos, and\npropose a general, two-stage deep framework to generate human action videos\nwith no constraints or arbitrary number of constraints, which uniformly address\nthe three problems: video generation given no input frames, video prediction\ngiven the first few frames, and video completion given the first and last\nframes. To make the problem tractable, in the first stage we train a deep\ngenerative model that generates a human pose sequence from random noise. In the\nsecond stage, a skeleton-to-image network is trained, which is used to generate\na human action video given the complete human pose sequence generated in the\nfirst stage. By introducing the two-stage strategy, we sidestep the original\nill-posed problems while producing for the first time high-quality video\ngeneration/prediction/completion results of much longer duration. We present\nquantitative and qualitative evaluation to show that our two-stage approach\noutperforms state-of-the-art methods in video generation, prediction and video\ncompletion. Our video result demonstration can be viewed at\nhttps://iamacewhite.github.io/supp/index.html\n", "versions": [{"version": "v1", "created": "Thu, 23 Nov 2017 13:10:34 GMT"}, {"version": "v2", "created": "Mon, 27 Nov 2017 04:48:27 GMT"}, {"version": "v3", "created": "Fri, 8 Dec 2017 12:17:15 GMT"}], "update_date": "2018-12-24", "authors_parsed": [["Cai", "Haoye", ""], ["Bai", "Chunyan", ""], ["Tai", "Yu-Wing", ""], ["Tang", "Chi-Keung", ""]]}, {"id": "1711.08716", "submitter": "Alexandre B\\^one", "authors": "Alexandre B\\^one, Maxime Louis, Alexandre Routier, Jorge Samper,\n  Michael Bacci, Benjamin Charlier, Olivier Colliot, Stanley Durrleman", "title": "Prediction of the progression of subcortical brain structures in\n  Alzheimer's disease from baseline", "comments": null, "journal-ref": null, "doi": "10.1007/978-3-319-67675-3_10", "report-no": null, "categories": "cs.CV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a method to predict the subject-specific longitudinal progression\nof brain structures extracted from baseline MRI, and evaluate its performance\non Alzheimer's disease data. The disease progression is modeled as a trajectory\non a group of diffeomorphisms in the context of large deformation diffeomorphic\nmetric mapping (LDDMM). We first exhibit the limited predictive abilities of\ngeodesic regression extrapolation on this group. Building on the recent concept\nof parallel curves in shape manifolds, we then introduce a second predictive\nprotocol which personalizes previously learned trajectories to new subjects,\nand investigate the relative performances of two parallel shifting paradigms.\nThis design only requires the baseline imaging data. Finally, coefficients\nencoding the disease dynamics are obtained from longitudinal cognitive\nmeasurements for each subject, and exploited to refine our methodology which is\ndemonstrated to successfully predict the follow-up visits.\n", "versions": [{"version": "v1", "created": "Thu, 23 Nov 2017 14:47:42 GMT"}], "update_date": "2017-11-27", "authors_parsed": [["B\u00f4ne", "Alexandre", ""], ["Louis", "Maxime", ""], ["Routier", "Alexandre", ""], ["Samper", "Jorge", ""], ["Bacci", "Michael", ""], ["Charlier", "Benjamin", ""], ["Colliot", "Olivier", ""], ["Durrleman", "Stanley", ""]]}, {"id": "1711.08725", "submitter": "Alexandre B\\^one", "authors": "Maxime Louis, Alexandre B\\^one, Benjamin Charlier, Stanley Durrleman", "title": "Parallel transport in shape analysis: a scalable numerical scheme", "comments": null, "journal-ref": null, "doi": "10.1007/978-3-319-68445-1_4", "report-no": null, "categories": "cs.CV math.DG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The analysis of manifold-valued data requires efficient tools from Riemannian\ngeometry to cope with the computational complexity at stake. This complexity\narises from the always-increasing dimension of the data, and the absence of\nclosed-form expressions to basic operations such as the Riemannian logarithm.\nIn this paper, we adapt a generic numerical scheme recently introduced for\ncomputing parallel transport along geodesics in a Riemannian manifold to\nfinite-dimensional manifolds of diffeomorphisms. We provide a qualitative and\nquantitative analysis of its behavior on high-dimensional manifolds, and\ninvestigate an application with the prediction of brain structures progression.\n", "versions": [{"version": "v1", "created": "Thu, 23 Nov 2017 14:57:32 GMT"}], "update_date": "2017-11-27", "authors_parsed": [["Louis", "Maxime", ""], ["B\u00f4ne", "Alexandre", ""], ["Charlier", "Benjamin", ""], ["Durrleman", "Stanley", ""]]}, {"id": "1711.08762", "submitter": "Eli (Omid) David", "authors": "Dror Sholomon, Eli David, Nathan S. Netanyahu", "title": "DNN-Buddies: A Deep Neural Network-Based Estimation Metric for the\n  Jigsaw Puzzle Problem", "comments": null, "journal-ref": "International Conference on Artificial Neural Networks (ICANN),\n  Springer LNCS, Vol. 9887, pp. 170-178, Barcelona, Spain, September 2016", "doi": "10.1007/978-3-319-44781-0_21", "report-no": null, "categories": "cs.CV cs.LG cs.NE stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper introduces the first deep neural network-based estimation metric\nfor the jigsaw puzzle problem. Given two puzzle piece edges, the neural network\npredicts whether or not they should be adjacent in the correct assembly of the\npuzzle, using nothing but the pixels of each piece. The proposed metric\nexhibits an extremely high precision even though no manual feature extraction\nis performed. When incorporated into an existing puzzle solver, the solution's\naccuracy increases significantly, achieving thereby a new state-of-the-art\nstandard.\n", "versions": [{"version": "v1", "created": "Thu, 23 Nov 2017 16:32:57 GMT"}], "update_date": "2017-11-27", "authors_parsed": [["Sholomon", "Dror", ""], ["David", "Eli", ""], ["Netanyahu", "Nathan S.", ""]]}, {"id": "1711.08763", "submitter": "Eli (Omid) David", "authors": "Eli David, Nathan S. Netanyahu", "title": "DeepPainter: Painter Classification Using Deep Convolutional\n  Autoencoders", "comments": null, "journal-ref": "International Conference on Artificial Neural Networks (ICANN),\n  Springer LNCS, Vol. 9887, pp. 20-28, Barcelona, Spain, September 2016", "doi": "10.1007/978-3-319-44781-0_3", "report-no": null, "categories": "cs.CV cs.LG cs.NE stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper we describe the problem of painter classification, and propose\na novel approach based on deep convolutional autoencoder neural networks. While\nprevious approaches relied on image processing and manual feature extraction\nfrom paintings, our approach operates on the raw pixel level, without any\npreprocessing or manual feature extraction. We first train a deep convolutional\nautoencoder on a dataset of paintings, and subsequently use it to initialize a\nsupervised convolutional neural network for the classification phase.\n  The proposed approach substantially outperforms previous methods, improving\nthe previous state-of-the-art for the 3-painter classification problem from\n90.44% accuracy (previous state-of-the-art) to 96.52% accuracy, i.e., a 63%\nreduction in error rate.\n", "versions": [{"version": "v1", "created": "Thu, 23 Nov 2017 16:36:28 GMT"}], "update_date": "2017-11-27", "authors_parsed": [["David", "Eli", ""], ["Netanyahu", "Nathan S.", ""]]}, {"id": "1711.08770", "submitter": "Pengtao Xie", "authors": "Pengtao Xie, Jun Zhu, Eric P. Xing", "title": "Diversity-Promoting Bayesian Learning of Latent Variable Models", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  To address three important issues involved in latent variable models (LVMs),\nincluding capturing infrequent patterns, achieving small-sized but expressive\nmodels and alleviating overfitting, several studies have been devoted to\n\"diversifying\" LVMs, which aim at encouraging the components in LVMs to be\ndiverse. Most existing studies fall into a frequentist-style regularization\nframework, where the components are learned via point estimation. In this\npaper, we investigate how to \"diversify\" LVMs in the paradigm of Bayesian\nlearning. We propose two approaches that have complementary advantages. One is\nto define a diversity-promoting mutual angular prior which assigns larger\ndensity to components with larger mutual angles and use this prior to affect\nthe posterior via Bayes' rule. We develop two efficient approximate posterior\ninference algorithms based on variational inference and MCMC sampling. The\nother approach is to impose diversity-promoting regularization directly over\nthe post-data distribution of components. We also extend our approach to\n\"diversify\" Bayesian nonparametric models where the number of components is\ninfinite. A sampling algorithm based on slice sampling and Hamiltonian Monte\nCarlo is developed. We apply these methods to \"diversify\" Bayesian mixture of\nexperts model and infinite latent feature model. Experiments on various\ndatasets demonstrate the effectiveness and efficiency of our methods.\n", "versions": [{"version": "v1", "created": "Thu, 23 Nov 2017 16:44:36 GMT"}], "update_date": "2017-11-27", "authors_parsed": [["Xie", "Pengtao", ""], ["Zhu", "Jun", ""], ["Xing", "Eric P.", ""]]}, {"id": "1711.08797", "submitter": "S{\\o}ren Dahlgaard", "authors": "S{\\o}ren Dahlgaard, Mathias B{\\ae}k Tejs Knudsen, Mikkel Thorup", "title": "Practical Hash Functions for Similarity Estimation and Dimensionality\n  Reduction", "comments": "Preliminary version of this paper will appear at NIPS 2017", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.DS cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Hashing is a basic tool for dimensionality reduction employed in several\naspects of machine learning. However, the perfomance analysis is often carried\nout under the abstract assumption that a truly random unit cost hash function\nis used, without concern for which concrete hash function is employed. The\nconcrete hash function may work fine on sufficiently random input. The question\nis if it can be trusted in the real world when faced with more structured\ninput.\n  In this paper we focus on two prominent applications of hashing, namely\nsimilarity estimation with the one permutation hashing (OPH) scheme of Li et\nal. [NIPS'12] and feature hashing (FH) of Weinberger et al. [ICML'09], both of\nwhich have found numerous applications, i.e. in approximate near-neighbour\nsearch with LSH and large-scale classification with SVM.\n  We consider mixed tabulation hashing of Dahlgaard et al.[FOCS'15] which was\nproved to perform like a truly random hash function in many applications,\nincluding OPH. Here we first show improved concentration bounds for FH with\ntruly random hashing and then argue that mixed tabulation performs similar for\nsparse input. Our main contribution, however, is an experimental comparison of\ndifferent hashing schemes when used inside FH, OPH, and LSH.\n  We find that mixed tabulation hashing is almost as fast as the\nmultiply-mod-prime scheme ax+b mod p. Mutiply-mod-prime is guaranteed to work\nwell on sufficiently random data, but we demonstrate that in the above\napplications, it can lead to bias and poor concentration on both real-world and\nsynthetic data. We also compare with the popular MurmurHash3, which has no\nproven guarantees. Mixed tabulation and MurmurHash3 both perform similar to\ntruly random hashing in our experiments. However, mixed tabulation is 40%\nfaster than MurmurHash3, and it has the proven guarantee of good performance on\nall possible input.\n", "versions": [{"version": "v1", "created": "Thu, 23 Nov 2017 18:27:37 GMT"}], "update_date": "2017-11-27", "authors_parsed": [["Dahlgaard", "S\u00f8ren", ""], ["Knudsen", "Mathias B\u00e6k Tejs", ""], ["Thorup", "Mikkel", ""]]}, {"id": "1711.08824", "submitter": "Jiantao Jiao", "authors": "Jiantao Jiao and Weihao Gao and Yanjun Han", "title": "The Nearest Neighbor Information Estimator is Adaptively Near Minimax\n  Rate-Optimal", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.IT math.IT", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We analyze the Kozachenko--Leonenko (KL) nearest neighbor estimator for the\ndifferential entropy. We obtain the first uniform upper bound on its\nperformance over H\\\"older balls on a torus without assuming any conditions on\nhow close the density could be from zero. Accompanying a new minimax lower\nbound over the H\\\"older ball, we show that the KL estimator is achieving the\nminimax rates up to logarithmic factors without cognizance of the smoothness\nparameter $s$ of the H\\\"older ball for $s\\in (0,2]$ and arbitrary dimension\n$d$, rendering it the first estimator that provably satisfies this property.\n", "versions": [{"version": "v1", "created": "Thu, 23 Nov 2017 20:20:28 GMT"}, {"version": "v2", "created": "Thu, 22 Feb 2018 02:45:22 GMT"}, {"version": "v3", "created": "Wed, 12 Sep 2018 05:03:09 GMT"}], "update_date": "2018-09-13", "authors_parsed": [["Jiao", "Jiantao", ""], ["Gao", "Weihao", ""], ["Han", "Yanjun", ""]]}, {"id": "1711.08833", "submitter": "Bao Wang", "authors": "Bao Wang, Penghang Yin, Andrea L. Bertozzi, P. Jeffrey Brantingham,\n  Stanley J. Osher and Jack Xin", "title": "Deep Learning for Real-Time Crime Forecasting and its Ternarization", "comments": "14 pages, 7 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG math.NA stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Real-time crime forecasting is important. However, accurate prediction of\nwhen and where the next crime will happen is difficult. No known physical model\nprovides a reasonable approximation to such a complex system. Historical crime\ndata are sparse in both space and time and the signal of interests is weak. In\nthis work, we first present a proper representation of crime data. We then\nadapt the spatial temporal residual network on the well represented data to\npredict the distribution of crime in Los Angeles at the scale of hours in\nneighborhood-sized parcels. These experiments as well as comparisons with\nseveral existing approaches to prediction demonstrate the superiority of the\nproposed model in terms of accuracy. Finally, we present a ternarization\ntechnique to address the resource consumption issue for its deployment in real\nworld. This work is an extension of our short conference proceeding paper [Wang\net al, Arxiv 1707.03340].\n", "versions": [{"version": "v1", "created": "Thu, 23 Nov 2017 21:39:40 GMT"}], "update_date": "2017-11-27", "authors_parsed": [["Wang", "Bao", ""], ["Yin", "Penghang", ""], ["Bertozzi", "Andrea L.", ""], ["Brantingham", "P. Jeffrey", ""], ["Osher", "Stanley J.", ""], ["Xin", "Jack", ""]]}, {"id": "1711.08856", "submitter": "Alessandro Achille", "authors": "Alessandro Achille, Matteo Rovere, Stefano Soatto", "title": "Critical Learning Periods in Deep Neural Networks", "comments": null, "journal-ref": null, "doi": null, "report-no": "UCLA-TR-170017", "categories": "cs.LG q-bio.NC stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Similar to humans and animals, deep artificial neural networks exhibit\ncritical periods during which a temporary stimulus deficit can impair the\ndevelopment of a skill. The extent of the impairment depends on the onset and\nlength of the deficit window, as in animal models, and on the size of the\nneural network. Deficits that do not affect low-level statistics, such as\nvertical flipping of the images, have no lasting effect on performance and can\nbe overcome with further training. To better understand this phenomenon, we use\nthe Fisher Information of the weights to measure the effective connectivity\nbetween layers of a network during training. Counterintuitively, information\nrises rapidly in the early phases of training, and then decreases, preventing\nredistribution of information resources in a phenomenon we refer to as a loss\nof \"Information Plasticity\". Our analysis suggests that the first few epochs\nare critical for the creation of strong connections that are optimal relative\nto the input data distribution. Once such strong connections are created, they\ndo not appear to change during additional training. These findings suggest that\nthe initial learning transient, under-scrutinized compared to asymptotic\nbehavior, plays a key role in determining the outcome of the training process.\nOur findings, combined with recent theoretical results in the literature, also\nsuggest that forgetting (decrease of information in the weights) is critical to\nachieving invariance and disentanglement in representation learning. Finally,\ncritical periods are not restricted to biological systems, but can emerge\nnaturally in learning systems, whether biological or artificial, due to\nfundamental constrains arising from learning dynamics and information\nprocessing.\n", "versions": [{"version": "v1", "created": "Fri, 24 Nov 2017 01:58:54 GMT"}, {"version": "v2", "created": "Tue, 15 May 2018 13:52:02 GMT"}, {"version": "v3", "created": "Mon, 25 Feb 2019 11:08:56 GMT"}], "update_date": "2019-02-26", "authors_parsed": [["Achille", "Alessandro", ""], ["Rovere", "Matteo", ""], ["Soatto", "Stefano", ""]]}, {"id": "1711.08861", "submitter": "Krithika Manohar", "authors": "Krithika Manohar, Thomas Hogan, Jim Buttrick, Ashis G. Banerjee, J.\n  Nathan Kutz and Steven L. Brunton", "title": "Predicting shim gaps in aircraft assembly with machine learning and\n  sparse sensing", "comments": "13 pages, 8 figures", "journal-ref": "J. Manuf. Syst. 48 (2018) 87-95", "doi": "10.1016/j.jmsy.2018.01.011", "report-no": null, "categories": "stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A modern aircraft may require on the order of thousands of custom shims to\nfill gaps between structural components in the airframe that arise due to\nmanufacturing tolerances adding up across large structures. These shims are\nnecessary to eliminate gaps, maintain structural performance, and minimize\npull-down forces required to bring the aircraft into engineering nominal\nconfiguration for peak aerodynamic efficiency. Gap filling is a time-consuming\nprocess, involving either expensive by-hand inspection or computations on vast\nquantities of measurement data from increasingly sophisticated metrology\nequipment. Either case amounts to significant delays in production, with much\nof the time spent in the critical path of aircraft assembly. This work presents\nan alternative strategy for predictive shimming, based on machine learning and\nsparse sensing to first learn gap distributions from historical data, and then\ndesign optimized sparse sensing strategies to streamline data collection and\nprocessing. This new approach is based on the assumption that patterns exist in\nshim distributions across aircraft, which may be mined and used to reduce the\nburden of data collection and processing in future aircraft. Specifically,\nrobust principal component analysis is used to extract low-dimensional patterns\nin the gap measurements while rejecting outliers. Next, optimized sparse\nsensors are obtained that are most informative about the dimensions of a new\naircraft in these low-dimensional principal components. We demonstrate the\nsuccess of the proposed approach, called PIXel Identification Despite\nUncertainty in Sensor Technology (PIXI-DUST), on historical production data\nfrom 54 representative Boeing commercial aircraft. Our algorithm successfully\npredicts $99\\%$ of shim gaps within the desired measurement tolerance using\n$3\\%$ of the laser scan points typically required; all results are\ncross-validated.\n", "versions": [{"version": "v1", "created": "Fri, 24 Nov 2017 02:50:02 GMT"}], "update_date": "2020-05-18", "authors_parsed": [["Manohar", "Krithika", ""], ["Hogan", "Thomas", ""], ["Buttrick", "Jim", ""], ["Banerjee", "Ashis G.", ""], ["Kutz", "J. Nathan", ""], ["Brunton", "Steven L.", ""]]}, {"id": "1711.08870", "submitter": "Namkyu Jung", "authors": "Namkyu Jung, Hyeong In Choi", "title": "Continuous Semantic Topic Embedding Model Using Variational Autoencoder", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.CL cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper proposes the continuous semantic topic embedding model (CSTEM)\nwhich finds latent topic variables in documents using continuous semantic\ndistance function between the topics and the words by means of the variational\nautoencoder(VAE). The semantic distance could be represented by any symmetric\nbell-shaped geometric distance function on the Euclidean space, for which the\nMahalanobis distance is used in this paper. In order for the semantic distance\nto perform more properly, we newly introduce an additional model parameter for\neach word to take out the global factor from this distance indicating how\nlikely it occurs regardless of its topic. It certainly improves the problem\nthat the Gaussian distribution which is used in previous topic model with\ncontinuous word embedding could not explain the semantic relation correctly and\nhelps to obtain the higher topic coherence. Through the experiments with the\ndataset of 20 Newsgroup, NIPS papers and CNN/Dailymail corpus, the performance\nof the recent state-of-the-art models is accomplished by our model as well as\ngenerating topic embedding vectors which makes possible to observe where the\ntopic vectors are embedded with the word vectors in the real Euclidean space\nand how the topics are related each other semantically.\n", "versions": [{"version": "v1", "created": "Fri, 24 Nov 2017 05:37:35 GMT"}], "update_date": "2017-11-27", "authors_parsed": [["Jung", "Namkyu", ""], ["Choi", "Hyeong In", ""]]}, {"id": "1711.08911", "submitter": "Guillaume Dehaene P.", "authors": "Guillaume P. Dehaene", "title": "Computing the quality of the Laplace approximation", "comments": "Advances in Approximate Bayesian Inference NIPS 2017 Workshop", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Bayesian inference requires approximation methods to become computable, but\nfor most of them it is impossible to quantify how close the approximation is to\nthe true posterior. In this work, we present a theorem upper-bounding the KL\ndivergence between a log-concave target density\n$f\\left(\\boldsymbol{\\theta}\\right)$ and its Laplace approximation\n$g\\left(\\boldsymbol{\\theta}\\right)$. The bound we present is computable: on the\nclassical logistic regression model, we find our bound to be almost exact as\nlong as the dimensionality of the parameter space is high.\n  The approach we followed in this work can be extended to other Gaussian\napproximations, as we will do in an extended version of this work, to be\nsubmitted to the Annals of Statistics. It will then become a critical tool for\ncharacterizing whether, for a given problem, a given Gaussian approximation is\nsuitable, or whether a more precise alternative method should be used instead.\n", "versions": [{"version": "v1", "created": "Fri, 24 Nov 2017 10:10:15 GMT"}], "update_date": "2017-11-27", "authors_parsed": [["Dehaene", "Guillaume P.", ""]]}, {"id": "1711.08921", "submitter": "Pascal Kerschke", "authors": "Pascal Kerschke and Heike Trautmann", "title": "Automated Algorithm Selection on Continuous Black-Box Problems By\n  Combining Exploratory Landscape Analysis and Machine Learning", "comments": "This is the author's final version, and the article has been accepted\n  for publication in Evolutionary Computation", "journal-ref": null, "doi": "10.1162/evco_a_00236", "report-no": null, "categories": "stat.ML cs.AI cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we build upon previous work on designing informative and\nefficient Exploratory Landscape Analysis features for characterizing problems'\nlandscapes and show their effectiveness in automatically constructing algorithm\nselection models in continuous black-box optimization problems. Focussing on\nalgorithm performance results of the COCO platform of several years, we\nconstruct a representative set of high-performing complementary solvers and\npresent an algorithm selection model that - compared to the portfolio's single\nbest solver - on average requires less than half of the resources for solving a\ngiven problem. Therefore, there is a huge gain in efficiency compared to\nclassical ensemble methods combined with an increased insight into problem\ncharacteristics and algorithm properties by using informative features. Acting\non the assumption that the function set of the Black-Box Optimization Benchmark\nis representative enough for practical applications the model allows for\nselecting the best suited optimization algorithm within the considered set for\nunseen problems prior to the optimization itself based on a small sample of\nfunction evaluations. Note that such a sample can even be reused for the\ninitial population of an evolutionary (optimization) algorithm so that even the\nfeature costs become negligible.\n", "versions": [{"version": "v1", "created": "Fri, 24 Nov 2017 10:35:02 GMT"}, {"version": "v2", "created": "Mon, 19 Nov 2018 18:37:29 GMT"}, {"version": "v3", "created": "Thu, 29 Nov 2018 07:05:21 GMT"}], "update_date": "2018-11-30", "authors_parsed": [["Kerschke", "Pascal", ""], ["Trautmann", "Heike", ""]]}, {"id": "1711.08936", "submitter": "Olivier Goudet Dr", "authors": "Olivier Goudet, Diviyan Kalainathan, Philippe Caillou, Isabelle Guyon,\n  David Lopez-Paz, Mich\\`ele Sebag", "title": "Causal Generative Neural Networks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present Causal Generative Neural Networks (CGNNs) to learn functional\ncausal models from observational data. CGNNs leverage conditional\nindependencies and distributional asymmetries to discover bivariate and\nmultivariate causal structures. CGNNs make no assumption regarding the lack of\nconfounders, and learn a differentiable generative model of the data by using\nbackpropagation. Extensive experiments show their good performances\ncomparatively to the state of the art in observational causal discovery on both\nsimulated and real data, with respect to cause-effect inference, v-structure\nidentification, and multivariate causal discovery.\n", "versions": [{"version": "v1", "created": "Fri, 24 Nov 2017 12:06:54 GMT"}, {"version": "v2", "created": "Mon, 5 Feb 2018 13:06:35 GMT"}], "update_date": "2018-02-06", "authors_parsed": [["Goudet", "Olivier", ""], ["Kalainathan", "Diviyan", ""], ["Caillou", "Philippe", ""], ["Guyon", "Isabelle", ""], ["Lopez-Paz", "David", ""], ["Sebag", "Mich\u00e8le", ""]]}, {"id": "1711.08992", "submitter": "Kalin Stefanov", "authors": "Kalin Stefanov, Jonas Beskow and Giampiero Salvi", "title": "Self-Supervised Vision-Based Detection of the Active Speaker as Support\n  for Socially-Aware Language Acquisition", "comments": "10 pages, IEEE Transactions on Cognitive and Developmental Systems", "journal-ref": null, "doi": "10.1109/TCDS.2019.2927941", "report-no": null, "categories": "cs.CV cs.CL cs.HC cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper presents a self-supervised method for visual detection of the\nactive speaker in a multi-person spoken interaction scenario. Active speaker\ndetection is a fundamental prerequisite for any artificial cognitive system\nattempting to acquire language in social settings. The proposed method is\nintended to complement the acoustic detection of the active speaker, thus\nimproving the system robustness in noisy conditions. The method can detect an\narbitrary number of possibly overlapping active speakers based exclusively on\nvisual information about their face. Furthermore, the method does not rely on\nexternal annotations, thus complying with cognitive development. Instead, the\nmethod uses information from the auditory modality to support learning in the\nvisual domain. This paper reports an extensive evaluation of the proposed\nmethod using a large multi-person face-to-face interaction dataset. The results\nshow good performance in a speaker dependent setting. However, in a speaker\nindependent setting the proposed method yields a significantly lower\nperformance. We believe that the proposed method represents an essential\ncomponent of any artificial cognitive system or robotic platform engaging in\nsocial interactions.\n", "versions": [{"version": "v1", "created": "Fri, 24 Nov 2017 14:45:06 GMT"}, {"version": "v2", "created": "Thu, 18 Jul 2019 17:55:38 GMT"}], "update_date": "2019-07-19", "authors_parsed": [["Stefanov", "Kalin", ""], ["Beskow", "Jonas", ""], ["Salvi", "Giampiero", ""]]}, {"id": "1711.09059", "submitter": "Wojciech Fedorko", "authors": "Shannon Egan, Wojciech Fedorko, Alison Lister, Jannicke Pearkes, Colin\n  Gay", "title": "Long Short-Term Memory (LSTM) networks with jet constituents for boosted\n  top tagging at the LHC", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "hep-ex cs.LG hep-ph stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Multivariate techniques based on engineered features have found wide adoption\nin the identification of jets resulting from hadronic top decays at the Large\nHadron Collider (LHC). Recent Deep Learning developments in this area include\nthe treatment of the calorimeter activation as an image or supplying a list of\njet constituent momenta to a fully connected network. This latter approach\nlends itself well to the use of Recurrent Neural Networks. In this work the\napplicability of architectures incorporating Long Short-Term Memory (LSTM)\nnetworks is explored. Several network architectures, methods of ordering of jet\nconstituents, and input pre-processing are studied. The best performing LSTM\nnetwork achieves a background rejection of 100 for 50% signal efficiency. This\nrepresents more than a factor of two improvement over a fully connected Deep\nNeural Network (DNN) trained on similar types of inputs.\n", "versions": [{"version": "v1", "created": "Fri, 24 Nov 2017 17:48:21 GMT"}], "update_date": "2017-11-27", "authors_parsed": [["Egan", "Shannon", ""], ["Fedorko", "Wojciech", ""], ["Lister", "Alison", ""], ["Pearkes", "Jannicke", ""], ["Gay", "Colin", ""]]}, {"id": "1711.09090", "submitter": "Russell Tsuchida B.E.", "authors": "Russell Tsuchida, Farbod Roosta-Khorasani, Marcus Gallagher", "title": "Invariance of Weight Distributions in Rectified MLPs", "comments": "ICML 2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  An interesting approach to analyzing neural networks that has received\nrenewed attention is to examine the equivalent kernel of the neural network.\nThis is based on the fact that a fully connected feedforward network with one\nhidden layer, a certain weight distribution, an activation function, and an\ninfinite number of neurons can be viewed as a mapping into a Hilbert space. We\nderive the equivalent kernels of MLPs with ReLU or Leaky ReLU activations for\nall rotationally-invariant weight distributions, generalizing a previous result\nthat required Gaussian weight distributions. Additionally, the Central Limit\nTheorem is used to show that for certain activation functions, kernels\ncorresponding to layers with weight distributions having $0$ mean and finite\nabsolute third moment are asymptotically universal, and are well approximated\nby the kernel corresponding to layers with spherical Gaussian weights. In deep\nnetworks, as depth increases the equivalent kernel approaches a pathological\nfixed point, which can be used to argue why training randomly initialized\nnetworks can be difficult. Our results also have implications for weight\ninitialization.\n", "versions": [{"version": "v1", "created": "Fri, 24 Nov 2017 05:27:19 GMT"}, {"version": "v2", "created": "Fri, 2 Feb 2018 07:04:51 GMT"}, {"version": "v3", "created": "Fri, 1 Jun 2018 00:11:34 GMT"}], "update_date": "2018-06-04", "authors_parsed": [["Tsuchida", "Russell", ""], ["Roosta-Khorasani", "Farbod", ""], ["Gallagher", "Marcus", ""]]}, {"id": "1711.09091", "submitter": "Xiao Dong", "authors": "Xiao Dong, Jiasong Wu, Ling Zhou", "title": "Demystifying AlphaGo Zero as AlphaGo GAN", "comments": "3 pages, 1 figure", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The astonishing success of AlphaGo Zero\\cite{Silver_AlphaGo} invokes a\nworldwide discussion of the future of our human society with a mixed mood of\nhope, anxiousness, excitement and fear. We try to dymystify AlphaGo Zero by a\nqualitative analysis to indicate that AlphaGo Zero can be understood as a\nspecially structured GAN system which is expected to possess an inherent good\nconvergence property. Thus we deduct the success of AlphaGo Zero may not be a\nsign of a new generation of AI.\n", "versions": [{"version": "v1", "created": "Fri, 24 Nov 2017 08:11:11 GMT"}], "update_date": "2017-11-28", "authors_parsed": [["Dong", "Xiao", ""], ["Wu", "Jiasong", ""], ["Zhou", "Ling", ""]]}, {"id": "1711.09131", "submitter": "Salar Fattahi", "authors": "Salar Fattahi, Richard Y. Zhang, Somayeh Sojoudi", "title": "Sparse Inverse Covariance Estimation for Chordal Structures", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we consider the Graphical Lasso (GL), a popular optimization\nproblem for learning the sparse representations of high-dimensional datasets,\nwhich is well-known to be computationally expensive for large-scale problems.\nRecently, we have shown that the sparsity pattern of the optimal solution of GL\nis equivalent to the one obtained from simply thresholding the sample\ncovariance matrix, for sparse graphs under different conditions. We have also\nderived a closed-form solution that is optimal when the thresholded sample\ncovariance matrix has an acyclic structure. As a major generalization of the\nprevious result, in this paper we derive a closed-form solution for the GL for\ngraphs with chordal structures. We show that the GL and thresholding\nequivalence conditions can significantly be simplified and are expected to hold\nfor high-dimensional problems if the thresholded sample covariance matrix has a\nchordal structure. We then show that the GL and thresholding equivalence is\nenough to reduce the GL to a maximum determinant matrix completion problem and\ndrive a recursive closed-form solution for the GL when the thresholded sample\ncovariance matrix has a chordal structure. For large-scale problems with up to\n450 million variables, the proposed method can solve the GL problem in less\nthan 2 minutes, while the state-of-the-art methods converge in more than 2\nhours.\n", "versions": [{"version": "v1", "created": "Fri, 24 Nov 2017 20:45:26 GMT"}], "update_date": "2017-11-28", "authors_parsed": [["Fattahi", "Salar", ""], ["Zhang", "Richard Y.", ""], ["Sojoudi", "Somayeh", ""]]}, {"id": "1711.09158", "submitter": "Noah Giansiracusa", "authors": "Noah Giansiracusa, Robert Giansiracusa, Chul Moon", "title": "Persistent homology machine learning for fingerprint classification", "comments": "15 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML math.AT", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The fingerprint classification problem is to sort fingerprints into\npre-determined groups, such as arch, loop, and whorl. It was asserted in the\nliterature that minutiae points, which are commonly used for fingerprint\nmatching, are not useful for classification. We show that, to the contrary,\nnear state-of-the-art classification accuracy rates can be achieved when\napplying topological data analysis (TDA) to 3-dimensional point clouds of\noriented minutiae points. We also apply TDA to fingerprint ink-roll images,\nwhich yields a lower accuracy rate but still shows promise, particularly since\nthe only preprocessing is cropping; moreover, combining the two approaches\noutperforms each one individually. These methods use supervised learning\napplied to persistent homology and allow us to explore feature selection on\nbarcodes, an important topic at the interface between TDA and machine learning.\nWe test our classification algorithms on the NIST fingerprint database SD-27.\n", "versions": [{"version": "v1", "created": "Fri, 24 Nov 2017 22:27:16 GMT"}], "update_date": "2017-11-28", "authors_parsed": [["Giansiracusa", "Noah", ""], ["Giansiracusa", "Robert", ""], ["Moon", "Chul", ""]]}, {"id": "1711.09159", "submitter": "Momchil Peychev", "authors": "Momchil Peychev, Petar Veli\\v{c}kovi\\'c, Pietro Li\\`o", "title": "Quantifying the Effects of Enforcing Disentanglement on Variational\n  Autoencoders", "comments": "Accepted to the Workshop on Learning Disentangled Representations at\n  the 31st Annual Conference on Neural Information Processing Systems (NIPS\n  2017), 5 pages, 2 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The notion of disentangled autoencoders was proposed as an extension to the\nvariational autoencoder by introducing a disentanglement parameter $\\beta$,\ncontrolling the learning pressure put on the possible underlying latent\nrepresentations. For certain values of $\\beta$ this kind of autoencoders is\ncapable of encoding independent input generative factors in separate elements\nof the code, leading to a more interpretable and predictable model behaviour.\nIn this paper we quantify the effects of the parameter $\\beta$ on the model\nperformance and disentanglement. After training multiple models with the same\nvalue of $\\beta$, we establish the existence of consistent variance in one of\nthe disentanglement measures, proposed in literature. The negative consequences\nof the disentanglement to the autoencoder's discriminative ability are also\nasserted while varying the amount of examples available during training.\n", "versions": [{"version": "v1", "created": "Fri, 24 Nov 2017 22:28:48 GMT"}], "update_date": "2017-11-28", "authors_parsed": [["Peychev", "Momchil", ""], ["Veli\u010dkovi\u0107", "Petar", ""], ["Li\u00f2", "Pietro", ""]]}, {"id": "1711.09163", "submitter": "Ershad Banijamali Mr.", "authors": "Ershad Banijamali, Amir-Hossein Karimi, Alexander Wong, Ali Ghodsi", "title": "JADE: Joint Autoencoders for Dis-Entanglement", "comments": "5 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The problem of feature disentanglement has been explored in the literature,\nfor the purpose of image and video processing and text analysis.\nState-of-the-art methods for disentangling feature representations rely on the\npresence of many labeled samples. In this work, we present a novel method for\ndisentangling factors of variation in data-scarce regimes. Specifically, we\nexplore the application of feature disentangling for the problem of supervised\nclassification in a setting where few labeled samples exist, and there are no\nunlabeled samples for use in unsupervised training. Instead, a similar datasets\nexists which shares at least one direction of variation with the\nsample-constrained datasets. We train our model end-to-end using the framework\nof variational autoencoders and are able to experimentally demonstrate that\nusing an auxiliary dataset with similar variation factors contribute positively\nto classification performance, yielding competitive results with the\nstate-of-the-art in unsupervised learning.\n", "versions": [{"version": "v1", "created": "Fri, 24 Nov 2017 22:58:10 GMT"}], "update_date": "2017-11-28", "authors_parsed": [["Banijamali", "Ershad", ""], ["Karimi", "Amir-Hossein", ""], ["Wong", "Alexander", ""], ["Ghodsi", "Ali", ""]]}, {"id": "1711.09176", "submitter": "Jonathan Schneider", "authors": "Mark Braverman, Jieming Mao, Jon Schneider, S. Matthew Weinberg", "title": "Selling to a No-Regret Buyer", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.GT cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider the problem of a single seller repeatedly selling a single item\nto a single buyer (specifically, the buyer has a value drawn fresh from known\ndistribution $D$ in every round). Prior work assumes that the buyer is fully\nrational and will perfectly reason about how their bids today affect the\nseller's decisions tomorrow. In this work we initiate a different direction:\nthe buyer simply runs a no-regret learning algorithm over possible bids. We\nprovide a fairly complete characterization of optimal auctions for the seller\nin this domain. Specifically:\n  - If the buyer bids according to EXP3 (or any \"mean-based\" learning\nalgorithm), then the seller can extract expected revenue arbitrarily close to\nthe expected welfare. This auction is independent of the buyer's valuation $D$,\nbut somewhat unnatural as it is sometimes in the buyer's interest to overbid. -\nThere exists a learning algorithm $\\mathcal{A}$ such that if the buyer bids\naccording to $\\mathcal{A}$ then the optimal strategy for the seller is simply\nto post the Myerson reserve for $D$ every round. - If the buyer bids according\nto EXP3 (or any \"mean-based\" learning algorithm), but the seller is restricted\nto \"natural\" auction formats where overbidding is dominated (e.g. Generalized\nFirst-Price or Generalized Second-Price), then the optimal strategy for the\nseller is a pay-your-bid format with decreasing reserves over time. Moreover,\nthe seller's optimal achievable revenue is characterized by a linear program,\nand can be unboundedly better than the best truthful auction yet simultaneously\nunboundedly worse than the expected welfare.\n", "versions": [{"version": "v1", "created": "Sat, 25 Nov 2017 01:35:45 GMT"}], "update_date": "2017-11-28", "authors_parsed": [["Braverman", "Mark", ""], ["Mao", "Jieming", ""], ["Schneider", "Jon", ""], ["Weinberg", "S. Matthew", ""]]}, {"id": "1711.09195", "submitter": "Vincent Zhao", "authors": "Vincent Zhao, Steven W. Zucker", "title": "Feature Selection Facilitates Learning Mixtures of Discrete Product\n  Distributions", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Feature selection can facilitate the learning of mixtures of discrete random\nvariables as they arise, e.g. in crowdsourcing tasks. Intuitively, not all\nworkers are equally reliable but, if the less reliable ones could be\neliminated, then learning should be more robust. By analogy with Gaussian\nmixture models, we seek a low-order statistical approach, and here introduce an\nalgorithm based on the (pairwise) mutual information. This induces an order\nover workers that is well structured for the `one coin' model. More generally,\nit is justified by a goodness-of-fit measure and is validated empirically.\nImprovement in real data sets can be substantial.\n", "versions": [{"version": "v1", "created": "Sat, 25 Nov 2017 05:34:48 GMT"}], "update_date": "2017-11-28", "authors_parsed": [["Zhao", "Vincent", ""], ["Zucker", "Steven W.", ""]]}, {"id": "1711.09219", "submitter": "Shuai Zhang", "authors": "Shuai Zhang, Jianxin Li, Pengtao Xie, Yingchun Zhang, Minglai Shao,\n  Haoyi Zhou, Mengyi Yan", "title": "Stacked Kernel Network", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Kernel methods are powerful tools to capture nonlinear patterns behind data.\nThey implicitly learn high (even infinite) dimensional nonlinear features in\nthe Reproducing Kernel Hilbert Space (RKHS) while making the computation\ntractable by leveraging the kernel trick. Classic kernel methods learn a single\nlayer of nonlinear features, whose representational power may be limited.\nMotivated by recent success of deep neural networks (DNNs) that learn\nmulti-layer hierarchical representations, we propose a Stacked Kernel Network\n(SKN) that learns a hierarchy of RKHS-based nonlinear features. SKN interleaves\nseveral layers of nonlinear transformations (from a linear space to a RKHS) and\nlinear transformations (from a RKHS to a linear space). Similar to DNNs, a SKN\nis composed of multiple layers of hidden units, but each parameterized by a\nRKHS function rather than a finite-dimensional vector. We propose three ways to\nrepresent the RKHS functions in SKN: (1)nonparametric representation,\n(2)parametric representation and (3)random Fourier feature representation.\nFurthermore, we expand SKN into CNN architecture called Stacked Kernel\nConvolutional Network (SKCN). SKCN learning a hierarchy of RKHS-based nonlinear\nfeatures by convolutional operation with each filter also parameterized by a\nRKHS function rather than a finite-dimensional matrix in CNN, which is suitable\nfor image inputs. Experiments on various datasets demonstrate the effectiveness\nof SKN and SKCN, which outperform the competitive methods.\n", "versions": [{"version": "v1", "created": "Sat, 25 Nov 2017 09:01:40 GMT"}], "update_date": "2017-11-28", "authors_parsed": [["Zhang", "Shuai", ""], ["Li", "Jianxin", ""], ["Xie", "Pengtao", ""], ["Zhang", "Yingchun", ""], ["Shao", "Minglai", ""], ["Zhou", "Haoyi", ""], ["Yan", "Mengyi", ""]]}, {"id": "1711.09223", "submitter": "Pranav Rajpurkar", "authors": "Pranav Rajpurkar, Vinaya Polamreddi, Anusha Balakrishnan", "title": "Malaria Likelihood Prediction By Effectively Surveying Households Using\n  Deep Reinforcement Learning", "comments": "Accepted at NIPS 2017 Workshop on Machine Learning for Health (NIPS\n  2017 ML4H)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We build a deep reinforcement learning (RL) agent that can predict the\nlikelihood of an individual testing positive for malaria by asking questions\nabout their household. The RL agent learns to determine which survey question\nto ask next and when to stop to make a prediction about their likelihood of\nmalaria based on their responses hitherto. The agent incurs a small penalty for\neach question asked, and a large reward/penalty for making the correct/wrong\nprediction; it thus has to learn to balance the length of the survey with the\naccuracy of its final predictions. Our RL agent is a Deep Q-network that learns\na policy directly from the responses to the questions, with an action defined\nfor each possible survey question and for each possible prediction class. We\nfocus on Kenya, where malaria is a massive health burden, and train the RL\nagent on a dataset of 6481 households from the Kenya Malaria Indicator Survey\n2015. To investigate the importance of having survey questions be adaptive to\nresponses, we compare our RL agent to a supervised learning (SL) baseline that\nfixes its set of survey questions a priori. We evaluate on prediction accuracy\nand on the number of survey questions asked on a holdout set and find that the\nRL agent is able to predict with 80% accuracy, using only 2.5 questions on\naverage. In addition, the RL agent learns to survey adaptively to responses and\nis able to match the SL baseline in prediction accuracy while significantly\nreducing survey length.\n", "versions": [{"version": "v1", "created": "Sat, 25 Nov 2017 09:33:05 GMT"}], "update_date": "2017-11-28", "authors_parsed": [["Rajpurkar", "Pranav", ""], ["Polamreddi", "Vinaya", ""], ["Balakrishnan", "Anusha", ""]]}, {"id": "1711.09268", "submitter": "Daniel L\\'evy", "authors": "Daniel Levy, Matthew D. Hoffman, Jascha Sohl-Dickstein", "title": "Generalizing Hamiltonian Monte Carlo with Neural Networks", "comments": "ICLR 2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.AI cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a general-purpose method to train Markov chain Monte Carlo\nkernels, parameterized by deep neural networks, that converge and mix quickly\nto their target distribution. Our method generalizes Hamiltonian Monte Carlo\nand is trained to maximize expected squared jumped distance, a proxy for mixing\nspeed. We demonstrate large empirical gains on a collection of simple but\nchallenging distributions, for instance achieving a 106x improvement in\neffective sample size in one case, and mixing when standard HMC makes no\nmeasurable progress in a second. Finally, we show quantitative and qualitative\ngains on a real-world task: latent-variable generative modeling. We release an\nopen source TensorFlow implementation of the algorithm.\n", "versions": [{"version": "v1", "created": "Sat, 25 Nov 2017 18:08:02 GMT"}, {"version": "v2", "created": "Fri, 12 Jan 2018 06:55:12 GMT"}, {"version": "v3", "created": "Fri, 2 Mar 2018 21:05:40 GMT"}], "update_date": "2018-03-06", "authors_parsed": [["Levy", "Daniel", ""], ["Hoffman", "Matthew D.", ""], ["Sohl-Dickstein", "Jascha", ""]]}, {"id": "1711.09279", "submitter": "Ritvik Shrivastava", "authors": "Anand Gupta, Hardeo Thakur, Ritvik Shrivastava, Pulkit Kumar, Sreyashi\n  Nag", "title": "A Big Data Analysis Framework Using Apache Spark and Deep Learning", "comments": "To be published in IEEE ICDM 2017 (International Conference on Data\n  Mining) Workshop on Data Science and Big Data Analytics (DSBDA)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  With the spreading prevalence of Big Data, many advances have recently been\nmade in this field. Frameworks such as Apache Hadoop and Apache Spark have\ngained a lot of traction over the past decades and have become massively\npopular, especially in industries. It is becoming increasingly evident that\neffective big data analysis is key to solving artificial intelligence problems.\nThus, a multi-algorithm library was implemented in the Spark framework, called\nMLlib. While this library supports multiple machine learning algorithms, there\nis still scope to use the Spark setup efficiently for highly time-intensive and\ncomputationally expensive procedures like deep learning. In this paper, we\npropose a novel framework that combines the distributive computational\nabilities of Apache Spark and the advanced machine learning architecture of a\ndeep multi-layer perceptron (MLP), using the popular concept of Cascade\nLearning. We conduct empirical analysis of our framework on two real world\ndatasets. The results are encouraging and corroborate our proposed framework,\nin turn proving that it is an improvement over traditional big data analysis\nmethods that use either Spark or Deep learning as individual elements.\n", "versions": [{"version": "v1", "created": "Sat, 25 Nov 2017 20:11:41 GMT"}], "update_date": "2017-11-28", "authors_parsed": [["Gupta", "Anand", ""], ["Thakur", "Hardeo", ""], ["Shrivastava", "Ritvik", ""], ["Kumar", "Pulkit", ""], ["Nag", "Sreyashi", ""]]}, {"id": "1711.09294", "submitter": "Andrea Locatelli", "authors": "Andrea Locatelli, Alexandra Carpentier, Samory Kpotufe", "title": "An Adaptive Strategy for Active Learning with Smooth Decision Boundary", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present the first adaptive strategy for active learning in the setting of\nclassification with smooth decision boundary. The problem of adaptivity (to\nunknown distributional parameters) has remained opened since the seminal work\nof Castro and Nowak (2007), which first established (active learning) rates for\nthis setting. While some recent advances on this problem establish adaptive\nrates in the case of univariate data, adaptivity in the more practical setting\nof multivariate data has so far remained elusive. Combining insights from\nvarious recent works, we show that, for the multivariate case, a careful\nreduction to univariate-adaptive strategies yield near-optimal rates without\nprior knowledge of distributional parameters.\n", "versions": [{"version": "v1", "created": "Sat, 25 Nov 2017 21:23:46 GMT"}], "update_date": "2017-11-28", "authors_parsed": [["Locatelli", "Andrea", ""], ["Carpentier", "Alexandra", ""], ["Kpotufe", "Samory", ""]]}, {"id": "1711.09300", "submitter": "Pengtao Xie", "authors": "Pengtao Xie, Hongbao Zhang, Eric P. Xing", "title": "Learning Less-Overlapping Representations", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In representation learning (RL), how to make the learned representations easy\nto interpret and less overfitted to training data are two important but\nchallenging issues. To address these problems, we study a new type of\nregulariza- tion approach that encourages the supports of weight vectors in RL\nmodels to have small overlap, by simultaneously promoting near-orthogonality\namong vectors and sparsity of each vector. We apply the proposed regularizer to\ntwo models: neural networks (NNs) and sparse coding (SC), and develop an\nefficient ADMM-based algorithm for regu- larized SC. Experiments on various\ndatasets demonstrate that weight vectors learned under our regularizer are more\ninterpretable and have better generalization performance.\n", "versions": [{"version": "v1", "created": "Sat, 25 Nov 2017 21:52:14 GMT"}], "update_date": "2017-11-28", "authors_parsed": [["Xie", "Pengtao", ""], ["Zhang", "Hongbao", ""], ["Xing", "Eric P.", ""]]}, {"id": "1711.09306", "submitter": "Vassilis N. Ioannidis", "authors": "Vassilis N. Ioannidis, Daniel Romero, Georgios B. Giannakis", "title": "Inference of Spatio-Temporal Functions over Graphs via Multi-Kernel\n  Kriged Kalman Filtering", "comments": "Submitted to IEEE Transactions on Signal processing, Nov. 2017", "journal-ref": null, "doi": "10.1109/TSP.2018.2827328", "report-no": null, "categories": "cs.LG eess.SP stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Inference of space-time varying signals on graphs emerges naturally in a\nplethora of network science related applications. A frequently encountered\nchallenge pertains to reconstructing such dynamic processes, given their values\nover a subset of vertices and time instants. The present paper develops a\ngraph-aware kernel-based kriged Kalman filter that accounts for the\nspatio-temporal variations, and offers efficient online reconstruction, even\nfor dynamically evolving network topologies. The kernel-based learning\nframework bypasses the need for statistical information by capitalizing on the\nsmoothness that graph signals exhibit with respect to the underlying graph. To\naddress the challenge of selecting the appropriate kernel, the proposed filter\nis combined with a multi-kernel selection module. Such a data-driven method\nselects a kernel attuned to the signal dynamics on-the-fly within the linear\nspan of a pre-selected dictionary. The novel multi-kernel learning algorithm\nexploits the eigenstructure of Laplacian kernel matrices to reduce\ncomputational complexity. Numerical tests with synthetic and real data\ndemonstrate the superior reconstruction performance of the novel approach\nrelative to state-of-the-art alternatives.\n", "versions": [{"version": "v1", "created": "Sat, 25 Nov 2017 23:25:49 GMT"}], "update_date": "2018-09-25", "authors_parsed": [["Ioannidis", "Vassilis N.", ""], ["Romero", "Daniel", ""], ["Giannakis", "Georgios B.", ""]]}, {"id": "1711.09325", "submitter": "Kimin Lee", "authors": "Kimin Lee, Honglak Lee, Kibok Lee, Jinwoo Shin", "title": "Training Confidence-calibrated Classifiers for Detecting\n  Out-of-Distribution Samples", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The problem of detecting whether a test sample is from in-distribution (i.e.,\ntraining distribution by a classifier) or out-of-distribution sufficiently\ndifferent from it arises in many real-world machine learning applications.\nHowever, the state-of-art deep neural networks are known to be highly\noverconfident in their predictions, i.e., do not distinguish in- and\nout-of-distributions. Recently, to handle this issue, several threshold-based\ndetectors have been proposed given pre-trained neural classifiers. However, the\nperformance of prior works highly depends on how to train the classifiers since\nthey only focus on improving inference procedures. In this paper, we develop a\nnovel training method for classifiers so that such inference algorithms can\nwork better. In particular, we suggest two additional terms added to the\noriginal loss (e.g., cross entropy). The first one forces samples from\nout-of-distribution less confident by the classifier and the second one is for\n(implicitly) generating most effective training samples for the first one. In\nessence, our method jointly trains both classification and generative neural\nnetworks for out-of-distribution. We demonstrate its effectiveness using deep\nconvolutional neural networks on various popular image datasets.\n", "versions": [{"version": "v1", "created": "Sun, 26 Nov 2017 02:50:39 GMT"}, {"version": "v2", "created": "Fri, 1 Dec 2017 10:03:12 GMT"}, {"version": "v3", "created": "Fri, 23 Feb 2018 19:42:15 GMT"}], "update_date": "2018-02-27", "authors_parsed": [["Lee", "Kimin", ""], ["Lee", "Honglak", ""], ["Lee", "Kibok", ""], ["Shin", "Jinwoo", ""]]}, {"id": "1711.09482", "submitter": "Housam Khalifa Bashier Babiker", "authors": "Housam Khalifa Bashier Babiker and Randy Goebel", "title": "An Introduction to Deep Visual Explanation", "comments": "Accepted at NIPS 2017 - Workshop Interpreting, Explaining and\n  Visualizing Deep Learning", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The practical impact of deep learning on complex supervised learning problems\nhas been significant, so much so that almost every Artificial Intelligence\nproblem, or at least a portion thereof, has been somehow recast as a deep\nlearning problem. The applications appeal is significant, but this appeal is\nincreasingly challenged by what some call the challenge of explainability, or\nmore generally the more traditional challenge of debuggability: if the outcomes\nof a deep learning process produce unexpected results (e.g., less than expected\nperformance of a classifier), then there is little available in the way of\ntheories or tools to help investigate the potential causes of such unexpected\nbehavior, especially when this behavior could impact people's lives. We\ndescribe a preliminary framework to help address this issue, which we call\n\"deep visual explanation\" (DVE). \"Deep,\" because it is the development and\nperformance of deep neural network models that we want to understand. \"Visual,\"\nbecause we believe that the most rapid insight into a complex multi-dimensional\nmodel is provided by appropriate visualization techniques, and \"Explanation,\"\nbecause in the spectrum from instrumentation by inserting print statements to\nthe abductive inference of explanatory hypotheses, we believe that the key to\nunderstanding deep learning relies on the identification and exposure of\nhypotheses about the performance behavior of a learned deep model. In the\nexposition of our preliminary framework, we use relatively straightforward\nimage classification examples and a variety of choices on initial configuration\nof a deep model building scenario. By careful but not complicated\ninstrumentation, we expose classification outcomes of deep models using\nvisualization, and also show initial results for one potential application of\ninterpretability.\n", "versions": [{"version": "v1", "created": "Sun, 26 Nov 2017 22:54:18 GMT"}, {"version": "v2", "created": "Thu, 15 Mar 2018 19:18:33 GMT"}], "update_date": "2018-03-19", "authors_parsed": [["Babiker", "Housam Khalifa Bashier", ""], ["Goebel", "Randy", ""]]}, {"id": "1711.09492", "submitter": "Praneeth Narayanamurthy", "authors": "Namrata Vaswani, Thierry Bouwmans, Sajid Javed, and Praneeth\n  Narayanamurthy", "title": "Robust Subspace Learning: Robust PCA, Robust Subspace Tracking, and\n  Robust Subspace Recovery", "comments": "To appear, IEEE Signal Processing Magazine, July 2018", "journal-ref": "IEEE Signal Processing Magazine (Volume: 35, Issue: 4, July 2018)", "doi": "10.1109/MSP.2018.2826566", "report-no": null, "categories": "cs.IT cs.CV math.IT stat.ME stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  PCA is one of the most widely used dimension reduction techniques. A related\neasier problem is \"subspace learning\" or \"subspace estimation\". Given\nrelatively clean data, both are easily solved via singular value decomposition\n(SVD). The problem of subspace learning or PCA in the presence of outliers is\ncalled robust subspace learning or robust PCA (RPCA). For long data sequences,\nif one tries to use a single lower dimensional subspace to represent the data,\nthe required subspace dimension may end up being quite large. For such data, a\nbetter model is to assume that it lies in a low-dimensional subspace that can\nchange over time, albeit gradually. The problem of tracking such data (and the\nsubspaces) while being robust to outliers is called robust subspace tracking\n(RST). This article provides a magazine-style overview of the entire field of\nrobust subspace learning and tracking. In particular solutions for three\nproblems are discussed in detail: RPCA via sparse+low-rank matrix decomposition\n(S+LR), RST via S+LR, and \"robust subspace recovery (RSR)\". RSR assumes that an\nentire data vector is either an outlier or an inlier. The S+LR formulation\ninstead assumes that outliers occur on only a few data vector indices and hence\nare well modeled as sparse corruptions.\n", "versions": [{"version": "v1", "created": "Sun, 26 Nov 2017 23:52:53 GMT"}, {"version": "v2", "created": "Thu, 1 Mar 2018 23:33:54 GMT"}, {"version": "v3", "created": "Fri, 25 May 2018 21:49:47 GMT"}, {"version": "v4", "created": "Thu, 5 Jul 2018 22:46:31 GMT"}], "update_date": "2020-06-25", "authors_parsed": [["Vaswani", "Namrata", ""], ["Bouwmans", "Thierry", ""], ["Javed", "Sajid", ""], ["Narayanamurthy", "Praneeth", ""]]}, {"id": "1711.09511", "submitter": "Yafeng Liu", "authors": "Yafeng Liu, Shimin Feng, Zhikai Zhao and Enjie Ding", "title": "Highly Efficient Human Action Recognition with Quantum Genetic Algorithm\n  Optimized Support Vector Machine", "comments": "8 pages, 6 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper we propose the use of quantum genetic algorithm to optimize the\nsupport vector machine (SVM) for human action recognition. The Microsoft Kinect\nsensor can be used for skeleton tracking, which provides the joints' position\ndata. However, how to extract the motion features for representing the dynamics\nof a human skeleton is still a challenge due to the complexity of human motion.\nWe present a highly efficient features extraction method for action\nclassification, that is, using the joint angles to represent a human skeleton\nand calculating the variance of each angle during an action time window. Using\nthe proposed representation, we compared the human action classification\naccuracy of two approaches, including the optimized SVM based on quantum\ngenetic algorithm and the conventional SVM with grid search. Experimental\nresults on the MSR-12 dataset show that the conventional SVM achieved an\naccuracy of $ 93.85\\% $. The proposed approach outperforms the conventional\nmethod with an accuracy of $ 96.15\\% $.\n", "versions": [{"version": "v1", "created": "Mon, 27 Nov 2017 02:39:29 GMT"}, {"version": "v2", "created": "Fri, 15 Dec 2017 03:05:55 GMT"}], "update_date": "2017-12-18", "authors_parsed": [["Liu", "Yafeng", ""], ["Feng", "Shimin", ""], ["Zhao", "Zhikai", ""], ["Ding", "Enjie", ""]]}, {"id": "1711.09514", "submitter": "Yazhen Wang", "authors": "Yazhen Wang", "title": "Asymptotic Analysis via Stochastic Differential Equations of Gradient\n  Descent Algorithms in Statistical and Computational Paradigms", "comments": "102 pages 2 figure2", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper investigates asymptotic behaviors of gradient descent algorithms\n(particularly accelerated gradient descent and stochastic gradient descent) in\nthe context of stochastic optimization arising in statistics and machine\nlearning where objective functions are estimated from available data. We show\nthat these algorithms can be computationally modeled by continuous-time\nordinary or stochastic differential equations. We establish gradient flow\ncentral limit theorems to describe the limiting dynamic behaviors of these\ncomputational algorithms and the large-sample performances of the related\nstatistical procedures, as the number of algorithm iterations and data size\nboth go to infinity, where the gradient flow central limit theorems are\ngoverned by some linear ordinary or stochastic differential equations like\ntime-dependent Ornstein-Uhlenbeck processes. We illustrate that our study can\nprovide a novel unified framework for a joint computational and statistical\nasymptotic analysis, where the computational asymptotic analysis studies\ndynamic behaviors of these algorithms with the time (or the number of\niterations in the algorithms), the statistical asymptotic analysis investigates\nlarge sample behaviors of the statistical procedures (like estimators and\nclassifiers) that the algorithms are applied to compute, and in fact the\nstatistical procedures are equal to the limits of the random sequences\ngenerated from these iterative algorithms as the number of iterations goes to\ninfinity. The joint analysis results based on the obtained gradient flow\ncentral limit theorems can identify four factors - learning rate, batch size,\ngradient covariance, and Hessian - to derive new theory regarding the local\nminima found by stochastic gradient descent for solving non-convex optimization\nproblems.\n", "versions": [{"version": "v1", "created": "Mon, 27 Nov 2017 02:52:29 GMT"}, {"version": "v2", "created": "Fri, 12 Jan 2018 19:24:34 GMT"}, {"version": "v3", "created": "Wed, 14 Mar 2018 04:35:33 GMT"}, {"version": "v4", "created": "Sun, 9 Dec 2018 04:58:42 GMT"}, {"version": "v5", "created": "Tue, 12 Nov 2019 02:44:20 GMT"}], "update_date": "2019-11-13", "authors_parsed": [["Wang", "Yazhen", ""]]}, {"id": "1711.09522", "submitter": "William Herlands", "authors": "Maria De-Arteaga, William Herlands", "title": "Proceedings of NIPS 2017 Workshop on Machine Learning for the Developing\n  World", "comments": "15 papers", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This is the Proceedings of NIPS 2017 Workshop on Machine Learning for the\nDeveloping World, held in Long Beach, California, USA on December 8, 2017\n", "versions": [{"version": "v1", "created": "Mon, 27 Nov 2017 03:27:17 GMT"}, {"version": "v2", "created": "Tue, 12 Dec 2017 14:09:38 GMT"}], "update_date": "2017-12-13", "authors_parsed": [["De-Arteaga", "Maria", ""], ["Herlands", "William", ""]]}, {"id": "1711.09534", "submitter": "Ziang Xie", "authors": "Ziang Xie", "title": "Neural Text Generation: A Practical Guide", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Deep learning methods have recently achieved great empirical success on\nmachine translation, dialogue response generation, summarization, and other\ntext generation tasks. At a high level, the technique has been to train\nend-to-end neural network models consisting of an encoder model to produce a\nhidden representation of the source text, followed by a decoder model to\ngenerate the target. While such models have significantly fewer pieces than\nearlier systems, significant tuning is still required to achieve good\nperformance. For text generation models in particular, the decoder can behave\nin undesired ways, such as by generating truncated or repetitive outputs,\noutputting bland and generic responses, or in some cases producing\nungrammatical gibberish. This paper is intended as a practical guide for\nresolving such undesired behavior in text generation models, with the aim of\nhelping enable real-world applications.\n", "versions": [{"version": "v1", "created": "Mon, 27 Nov 2017 04:50:15 GMT"}], "update_date": "2017-11-28", "authors_parsed": [["Xie", "Ziang", ""]]}, {"id": "1711.09535", "submitter": "Dacheng Tao", "authors": "Xiyu Yu, Tongliang Liu, Mingming Gong, and Dacheng Tao", "title": "Learning with Biased Complementary Labels", "comments": "ECCV 2018 Oral", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we study the classification problem in which we have access to\neasily obtainable surrogate for true labels, namely complementary labels, which\nspecify classes that observations do \\textbf{not} belong to. Let $Y$ and\n$\\bar{Y}$ be the true and complementary labels, respectively. We first model\nthe annotation of complementary labels via transition probabilities\n$P(\\bar{Y}=i|Y=j), i\\neq j\\in\\{1,\\cdots,c\\}$, where $c$ is the number of\nclasses. Previous methods implicitly assume that $P(\\bar{Y}=i|Y=j), \\forall\ni\\neq j$, are identical, which is not true in practice because humans are\nbiased toward their own experience. For example, as shown in Figure 1, if an\nannotator is more familiar with monkeys than prairie dogs when providing\ncomplementary labels for meerkats, she is more likely to employ \"monkey\" as a\ncomplementary label. We therefore reason that the transition probabilities will\nbe different. In this paper, we propose a framework that contributes three main\ninnovations to learning with \\textbf{biased} complementary labels: (1) It\nestimates transition probabilities with no bias. (2) It provides a general\nmethod to modify traditional loss functions and extends standard deep neural\nnetwork classifiers to learn with biased complementary labels. (3) It\ntheoretically ensures that the classifier learned with complementary labels\nconverges to the optimal one learned with true labels. Comprehensive\nexperiments on several benchmark datasets validate the superiority of our\nmethod to current state-of-the-art methods.\n", "versions": [{"version": "v1", "created": "Mon, 27 Nov 2017 04:52:05 GMT"}, {"version": "v2", "created": "Sun, 3 Dec 2017 23:27:18 GMT"}, {"version": "v3", "created": "Wed, 8 Aug 2018 01:40:46 GMT"}], "update_date": "2018-08-09", "authors_parsed": [["Yu", "Xiyu", ""], ["Liu", "Tongliang", ""], ["Gong", "Mingming", ""], ["Tao", "Dacheng", ""]]}, {"id": "1711.09545", "submitter": "Matthew Dixon", "authors": "Matthew Dixon, Diego Klabjan and Lan Wei", "title": "OSTSC: Over Sampling for Time Series Classification in R", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.CO stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The OSTSC package is a powerful oversampling approach for classifying\nunivariant, but multinomial time series data in R. This article provides a\nbrief overview of the oversampling methodology implemented by the package. A\ntutorial of the OSTSC package is provided. We begin by providing three test\ncases for the user to quickly validate the functionality in the package. To\ndemonstrate the performance impact of OSTSC, we then provide two medium size\nimbalanced time series datasets. Each example applies a TensorFlow\nimplementation of a Long Short-Term Memory (LSTM) classifier - a type of a\nRecurrent Neural Network (RNN) classifier - to imbalanced time series. The\nclassifier performance is compared with and without oversampling. Finally,\nlarger versions of these two datasets are evaluated to demonstrate the\nscalability of the package. The examples demonstrate that the OSTSC package\nimproves the performance of RNN classifiers applied to highly imbalanced time\nseries data. In particular, OSTSC is observed to increase the AUC of LSTM from\n0.543 to 0.784 on a high frequency trading dataset consisting of 30,000 time\nseries observations.\n", "versions": [{"version": "v1", "created": "Mon, 27 Nov 2017 05:43:48 GMT"}], "update_date": "2017-11-28", "authors_parsed": [["Dixon", "Matthew", ""], ["Klabjan", "Diego", ""], ["Wei", "Lan", ""]]}, {"id": "1711.09601", "submitter": "Rahaf Aljundi", "authors": "Rahaf Aljundi, Francesca Babiloni, Mohamed Elhoseiny, Marcus Rohrbach,\n  Tinne Tuytelaars", "title": "Memory Aware Synapses: Learning what (not) to forget", "comments": "ECCV 2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Humans can learn in a continuous manner. Old rarely utilized knowledge can be\noverwritten by new incoming information while important, frequently used\nknowledge is prevented from being erased. In artificial learning systems,\nlifelong learning so far has focused mainly on accumulating knowledge over\ntasks and overcoming catastrophic forgetting. In this paper, we argue that,\ngiven the limited model capacity and the unlimited new information to be\nlearned, knowledge has to be preserved or erased selectively. Inspired by\nneuroplasticity, we propose a novel approach for lifelong learning, coined\nMemory Aware Synapses (MAS). It computes the importance of the parameters of a\nneural network in an unsupervised and online manner. Given a new sample which\nis fed to the network, MAS accumulates an importance measure for each parameter\nof the network, based on how sensitive the predicted output function is to a\nchange in this parameter. When learning a new task, changes to important\nparameters can then be penalized, effectively preventing important knowledge\nrelated to previous tasks from being overwritten. Further, we show an\ninteresting connection between a local version of our method and Hebb's\nrule,which is a model for the learning process in the brain. We test our method\non a sequence of object recognition tasks and on the challenging problem of\nlearning an embedding for predicting $<$subject, predicate, object$>$ triplets.\nWe show state-of-the-art performance and, for the first time, the ability to\nadapt the importance of the parameters based on unlabeled data towards what the\nnetwork needs (not) to forget, which may vary depending on test conditions.\n", "versions": [{"version": "v1", "created": "Mon, 27 Nov 2017 09:48:44 GMT"}, {"version": "v2", "created": "Fri, 23 Mar 2018 10:46:56 GMT"}, {"version": "v3", "created": "Thu, 4 Oct 2018 16:41:42 GMT"}, {"version": "v4", "created": "Fri, 5 Oct 2018 08:40:30 GMT"}], "update_date": "2018-10-08", "authors_parsed": [["Aljundi", "Rahaf", ""], ["Babiloni", "Francesca", ""], ["Elhoseiny", "Mohamed", ""], ["Rohrbach", "Marcus", ""], ["Tuytelaars", "Tinne", ""]]}, {"id": "1711.09649", "submitter": "Olivier Bachem", "authors": "Olivier Bachem and Mario Lucic and Silvio Lattanzi", "title": "One-Shot Coresets: The Case of k-Clustering", "comments": "To Appear In AISTATS 2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Scaling clustering algorithms to massive data sets is a challenging task.\nRecently, several successful approaches based on data summarization methods,\nsuch as coresets and sketches, were proposed. While these techniques provide\nprovably good and small summaries, they are inherently problem dependent - the\npractitioner has to commit to a fixed clustering objective before even\nexploring the data. However, can one construct small data summaries for a wide\nrange of clustering problems simultaneously? In this work, we affirmatively\nanswer this question by proposing an efficient algorithm that constructs such\none-shot summaries for k-clustering problems while retaining strong theoretical\nguarantees.\n", "versions": [{"version": "v1", "created": "Mon, 27 Nov 2017 12:33:20 GMT"}, {"version": "v2", "created": "Tue, 28 Nov 2017 15:31:27 GMT"}, {"version": "v3", "created": "Tue, 20 Feb 2018 15:22:51 GMT"}], "update_date": "2018-02-21", "authors_parsed": [["Bachem", "Olivier", ""], ["Lucic", "Mario", ""], ["Lattanzi", "Silvio", ""]]}, {"id": "1711.09663", "submitter": "Eli (Omid) David", "authors": "Ido Cohen, Eli David, Nathan S. Netanyahu, Noa Liscovitch, Gal Chechik", "title": "DeepBrain: Functional Representation of Neural In-Situ Hybridization\n  Images for Gene Ontology Classification Using Deep Convolutional Autoencoders", "comments": null, "journal-ref": "International Conference on Artificial Neural Networks (ICANN),\n  Springer LNCS, Vol. 10614, pp. 287-296, Alghero, Italy, September, 2017", "doi": "10.1007/978-3-319-68612-7_33", "report-no": null, "categories": "cs.CV cs.LG cs.NE stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper presents a novel deep learning-based method for learning a\nfunctional representation of mammalian neural images. The method uses a deep\nconvolutional denoising autoencoder (CDAE) for generating an invariant, compact\nrepresentation of in situ hybridization (ISH) images. While most existing\nmethods for bio-imaging analysis were not developed to handle images with\nhighly complex anatomical structures, the results presented in this paper show\nthat functional representation extracted by CDAE can help learn features of\nfunctional gene ontology categories for their classification in a highly\naccurate manner. Using this CDAE representation, our method outperforms the\nprevious state-of-the-art classification rate, by improving the average AUC\nfrom 0.92 to 0.98, i.e., achieving 75% reduction in error. The method operates\non input images that were downsampled significantly with respect to the\noriginal ones to make it computationally feasible.\n", "versions": [{"version": "v1", "created": "Mon, 27 Nov 2017 13:00:03 GMT"}], "update_date": "2017-11-28", "authors_parsed": [["Cohen", "Ido", ""], ["David", "Eli", ""], ["Netanyahu", "Nathan S.", ""], ["Liscovitch", "Noa", ""], ["Chechik", "Gal", ""]]}, {"id": "1711.09666", "submitter": "Eli (Omid) David", "authors": "Ishai Rosenberg, Guillaume Sicard, Eli David", "title": "DeepAPT: Nation-State APT Attribution Using End-to-End Deep Neural\n  Networks", "comments": null, "journal-ref": "International Conference on Artificial Neural Networks (ICANN),\n  Springer LNCS, Vol. 10614, pp. 91-99, Alghero, Italy, September, 2017", "doi": "10.1007/978-3-319-68612-7_11", "report-no": null, "categories": "cs.CR cs.LG cs.NE stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In recent years numerous advanced malware, aka advanced persistent threats\n(APT) are allegedly developed by nation-states. The task of attributing an APT\nto a specific nation-state is extremely challenging for several reasons. Each\nnation-state has usually more than a single cyber unit that develops such\nadvanced malware, rendering traditional authorship attribution algorithms\nuseless. Furthermore, those APTs use state-of-the-art evasion techniques,\nmaking feature extraction challenging. Finally, the dataset of such available\nAPTs is extremely small.\n  In this paper we describe how deep neural networks (DNN) could be\nsuccessfully employed for nation-state APT attribution. We use sandbox reports\n(recording the behavior of the APT when run dynamically) as raw input for the\nneural network, allowing the DNN to learn high level feature abstractions of\nthe APTs itself. Using a test set of 1,000 Chinese and Russian developed APTs,\nwe achieved an accuracy rate of 94.6%.\n", "versions": [{"version": "v1", "created": "Mon, 27 Nov 2017 13:04:46 GMT"}], "update_date": "2017-11-28", "authors_parsed": [["Rosenberg", "Ishai", ""], ["Sicard", "Guillaume", ""], ["David", "Eli", ""]]}, {"id": "1711.09667", "submitter": "Eli (Omid) David", "authors": "Eli David, Nathan S. Netanyahu, Lior Wolf", "title": "DeepChess: End-to-End Deep Neural Network for Automatic Learning in\n  Chess", "comments": "Winner of Best Paper Award in ICANN 2016", "journal-ref": "International Conference on Artificial Neural Networks (ICANN),\n  Springer LNCS, Vol. 9887, pp. 88-96, Barcelona, Spain, 2016", "doi": "10.1007/978-3-319-44781-0_11", "report-no": null, "categories": "cs.NE cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present an end-to-end learning method for chess, relying on deep neural\nnetworks. Without any a priori knowledge, in particular without any knowledge\nregarding the rules of chess, a deep neural network is trained using a\ncombination of unsupervised pretraining and supervised training. The\nunsupervised training extracts high level features from a given position, and\nthe supervised training learns to compare two chess positions and select the\nmore favorable one. The training relies entirely on datasets of several million\nchess games, and no further domain specific knowledge is incorporated.\n  The experiments show that the resulting neural network (referred to as\nDeepChess) is on a par with state-of-the-art chess playing programs, which have\nbeen developed through many years of manual feature selection and tuning.\nDeepChess is the first end-to-end machine learning-based method that results in\na grandmaster-level chess playing performance.\n", "versions": [{"version": "v1", "created": "Mon, 27 Nov 2017 13:04:57 GMT"}], "update_date": "2017-11-28", "authors_parsed": [["David", "Eli", ""], ["Netanyahu", "Nathan S.", ""], ["Wolf", "Lior", ""]]}, {"id": "1711.09681", "submitter": "YoungJoon Yoo", "authors": "YoungJoon Yoo, Seonguk Park, Junyoung Choi, Sangdoo Yun, Nojun Kwak", "title": "Butterfly Effect: Bidirectional Control of Classification Performance by\n  Small Additive Perturbation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper proposes a new algorithm for controlling classification results by\ngenerating a small additive perturbation without changing the classifier\nnetwork. Our work is inspired by existing works generating adversarial\nperturbation that worsens classification performance. In contrast to the\nexisting methods, our work aims to generate perturbations that can enhance\noverall classification performance. To solve this performance enhancement\nproblem, we newly propose a perturbation generation network (PGN) influenced by\nthe adversarial learning strategy. In our problem, the information in a large\nexternal dataset is summarized by a small additive perturbation, which helps to\nimprove the performance of the classifier trained with the target dataset. In\naddition to this performance enhancement problem, we show that the proposed PGN\ncan be adopted to solve the classical adversarial problem without utilizing the\ninformation on the target classifier. The mentioned characteristics of our\nmethod are verified through extensive experiments on publicly available visual\ndatasets.\n", "versions": [{"version": "v1", "created": "Mon, 27 Nov 2017 13:32:45 GMT"}, {"version": "v2", "created": "Mon, 4 Dec 2017 05:39:10 GMT"}], "update_date": "2017-12-05", "authors_parsed": [["Yoo", "YoungJoon", ""], ["Park", "Seonguk", ""], ["Choi", "Junyoung", ""], ["Yun", "Sangdoo", ""], ["Kwak", "Nojun", ""]]}, {"id": "1711.09714", "submitter": "Giampiero Salvi", "authors": "Giampiero Salvi, Luis Montesano, Alexandre Bernardino, Jos\\'e\n  Santos-Victor", "title": "Language Bootstrapping: Learning Word Meanings From Perception-Action\n  Association", "comments": "code available at\n  https://github.com/giampierosalvi/AffordancesAndSpeech", "journal-ref": "in IEEE Transactions on Systems, Man, and Cybernetics, Part B\n  (Cybernetics), Volume: 42 Issue: 3, year 2012, pages 660-671", "doi": "10.1109/TSMCB.2011.2172420", "report-no": null, "categories": "cs.RO cs.CL cs.HC stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We address the problem of bootstrapping language acquisition for an\nartificial system similarly to what is observed in experiments with human\ninfants. Our method works by associating meanings to words in manipulation\ntasks, as a robot interacts with objects and listens to verbal descriptions of\nthe interactions. The model is based on an affordance network, i.e., a mapping\nbetween robot actions, robot perceptions, and the perceived effects of these\nactions upon objects. We extend the affordance model to incorporate spoken\nwords, which allows us to ground the verbal symbols to the execution of actions\nand the perception of the environment. The model takes verbal descriptions of a\ntask as the input and uses temporal co-occurrence to create links between\nspeech utterances and the involved objects, actions, and effects. We show that\nthe robot is able form useful word-to-meaning associations, even without\nconsidering grammatical structure in the learning process and in the presence\nof recognition errors. These word-to-meaning associations are embedded in the\nrobot's own understanding of its actions. Thus, they can be directly used to\ninstruct the robot to perform tasks and also allow to incorporate context in\nthe speech recognition task. We believe that the encouraging results with our\napproach may afford robots with a capacity to acquire language descriptors in\ntheir operation's environment as well as to shed some light as to how this\nchallenging process develops with human infants.\n", "versions": [{"version": "v1", "created": "Mon, 27 Nov 2017 14:42:26 GMT"}], "update_date": "2017-11-28", "authors_parsed": [["Salvi", "Giampiero", ""], ["Montesano", "Luis", ""], ["Bernardino", "Alexandre", ""], ["Santos-Victor", "Jos\u00e9", ""]]}, {"id": "1711.09715", "submitter": "ANtoine Marot", "authors": "Antoine Marot, Sami Tazi, Benjamin Donnot (LRI, TAU), Patrick\n  Panciatici", "title": "Guided Machine Learning for power grid segmentation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The segmentation of large scale power grids into zones is crucial for control\nroom operators when managing the grid complexity near real time. In this paper\nwe propose a new method in two steps which is able to automatically do this\nsegmentation, while taking into account the real time context, in order to help\nthem handle shifting dynamics. Our method relies on a \"guided\" machine learning\napproach. As a first step, we define and compute a task specific \"Influence\nGraph\" in a guided manner. We indeed simulate on a grid state chosen\ninterventions, representative of our task of interest (managing active power\nflows in our case). For visualization and interpretation, we then build a\nhigher representation of the grid relevant to this task by applying the graph\ncommunity detection algorithm \\textit{Infomap} on this Influence Graph. To\nillustrate our method and demonstrate its practical interest, we apply it on\ncommonly used systems, the IEEE-14 and IEEE-118. We show promising and original\ninterpretable results, especially on the previously well studied RTS-96 system\nfor grid segmentation. We eventually share initial investigation and results on\na large-scale system, the French power grid, whose segmentation had a\nsurprising resemblance with RTE's historical partitioning.\n", "versions": [{"version": "v1", "created": "Mon, 13 Nov 2017 10:44:01 GMT"}, {"version": "v2", "created": "Wed, 29 Nov 2017 13:34:11 GMT"}, {"version": "v3", "created": "Fri, 30 Mar 2018 10:44:58 GMT"}], "update_date": "2018-04-02", "authors_parsed": [["Marot", "Antoine", "", "LRI, TAU"], ["Tazi", "Sami", "", "LRI, TAU"], ["Donnot", "Benjamin", "", "LRI, TAU"], ["Panciatici", "Patrick", ""]]}, {"id": "1711.09728", "submitter": "Nazmus Saquib", "authors": "Md. Naimul Hoque, Rawshan E Fatima, Manash Kumar Mandal, Nazmus Saquib", "title": "Evaluating gender portrayal in Bangladeshi TV", "comments": "Presented at NIPS 2017 Workshop on Machine Learning for the\n  Developing World. Corresponding author: Nazmus Saquib", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CY stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Computer Vision and machine learning methods were previously used to reveal\nscreen presence of genders in TV and movies. In this work, using head pose,\ngender detection, and skin color estimation techniques, we demonstrate that the\ngender disparity in TV in a South Asian country such as Bangladesh exhibits\nunique characteristics and is sometimes counter-intuitive to popular\nperception. We demonstrate a noticeable discrepancy in female screen presence\nin Bangladeshi TV advertisements and political talk shows. Further, contrary to\npopular hypotheses, we demonstrate that lighter-toned skin colors are less\nprevalent than darker complexions, and additionally, quantifiable body language\nmarkers do not provide conclusive insights about gender dynamics. Overall,\nthese gender portrayal parameters reveal the different layers of onscreen\ngender politics and can help direct incentives to address existing disparities\nin a nuanced and targeted manner.\n", "versions": [{"version": "v1", "created": "Tue, 14 Nov 2017 17:54:13 GMT"}], "update_date": "2017-11-29", "authors_parsed": [["Hoque", "Md. Naimul", ""], ["Fatima", "Rawshan E", ""], ["Mandal", "Manash Kumar", ""], ["Saquib", "Nazmus", ""]]}, {"id": "1711.09783", "submitter": "Bharath Bhushan Damodaran", "authors": "Bharath Bhushan Damodaran, Nicolas Courty, Philippe-Henri Gosselin", "title": "Data Dependent Kernel Approximation using Pseudo Random Fourier Features", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Kernel methods are powerful and flexible approach to solve many problems in\nmachine learning. Due to the pairwise evaluations in kernel methods, the\ncomplexity of kernel computation grows as the data size increases; thus the\napplicability of kernel methods is limited for large scale datasets. Random\nFourier Features (RFF) has been proposed to scale the kernel method for solving\nlarge scale datasets by approximating kernel function using randomized Fourier\nfeatures. While this method proved very popular, still it exists shortcomings\nto be effectively used. As RFF samples the randomized features from a\ndistribution independent of training data, it requires sufficient large number\nof feature expansions to have similar performances to kernelized classifiers,\nand this is proportional to the number samples in the dataset. Thus, reducing\nthe number of feature dimensions is necessary to effectively scale to large\ndatasets. In this paper, we propose a kernel approximation method in a data\ndependent way, coined as Pseudo Random Fourier Features (PRFF) for reducing the\nnumber of feature dimensions and also to improve the prediction performance.\nThe proposed approach is evaluated on classification and regression problems\nand compared with the RFF, orthogonal random features and Nystr{\\\"o}m approach\n", "versions": [{"version": "v1", "created": "Mon, 27 Nov 2017 15:48:31 GMT"}], "update_date": "2017-11-28", "authors_parsed": [["Damodaran", "Bharath Bhushan", ""], ["Courty", "Nicolas", ""], ["Gosselin", "Philippe-Henri", ""]]}, {"id": "1711.09784", "submitter": "Nicholas Frosst", "authors": "Nicholas Frosst, Geoffrey Hinton", "title": "Distilling a Neural Network Into a Soft Decision Tree", "comments": "presented at the CEX workshop at AI*IA 2017 conference", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Deep neural networks have proved to be a very effective way to perform\nclassification tasks. They excel when the input data is high dimensional, the\nrelationship between the input and the output is complicated, and the number of\nlabeled training examples is large. But it is hard to explain why a learned\nnetwork makes a particular classification decision on a particular test case.\nThis is due to their reliance on distributed hierarchical representations. If\nwe could take the knowledge acquired by the neural net and express the same\nknowledge in a model that relies on hierarchical decisions instead, explaining\na particular decision would be much easier. We describe a way of using a\ntrained neural net to create a type of soft decision tree that generalizes\nbetter than one learned directly from the training data.\n", "versions": [{"version": "v1", "created": "Mon, 27 Nov 2017 15:50:50 GMT"}], "update_date": "2017-11-28", "authors_parsed": [["Frosst", "Nicholas", ""], ["Hinton", "Geoffrey", ""]]}, {"id": "1711.09876", "submitter": "James Aimone", "authors": "James B. Aimone and William M. Severa", "title": "Context-modulation of hippocampal dynamics and deep convolutional\n  networks", "comments": "4 pages; short paper accepted to 2017 NIPS Cognitively Informed AI\n  Workshop", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG q-bio.NC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Complex architectures of biological neural circuits, such as parallel\nprocessing pathways, has been behaviorally implicated in many cognitive\nstudies. However, the theoretical consequences of circuit complexity on neural\ncomputation have only been explored in limited cases. Here, we introduce a\nmechanism by which direct and indirect pathways from cortex to the CA3 region\nof the hippocampus can balance both contextual gating of memory formation and\ndriving network activity. We implement this concept in a deep artificial neural\nnetwork by enabling a context-sensitive bias. The motivation for this is to\nimprove performance of a size-constrained network. Using direct knowledge of\nthe superclass information in the CIFAR-100 and Fashion-MNIST datasets, we show\na dramatic increase in performance without an increase in network size.\n", "versions": [{"version": "v1", "created": "Mon, 27 Nov 2017 18:47:21 GMT"}], "update_date": "2017-11-28", "authors_parsed": [["Aimone", "James B.", ""], ["Severa", "William M.", ""]]}, {"id": "1711.09889", "submitter": "William Herlands", "authors": "Andrew Gordon Wilson, Jason Yosinski, Patrice Simard, Rich Caruana,\n  William Herlands", "title": "Proceedings of NIPS 2017 Symposium on Interpretable Machine Learning", "comments": "25 papers", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This is the Proceedings of NIPS 2017 Symposium on Interpretable Machine\nLearning, held in Long Beach, California, USA on December 7, 2017\n", "versions": [{"version": "v1", "created": "Mon, 27 Nov 2017 03:33:55 GMT"}, {"version": "v2", "created": "Thu, 30 Nov 2017 15:12:48 GMT"}, {"version": "v3", "created": "Tue, 12 Dec 2017 02:07:47 GMT"}], "update_date": "2017-12-13", "authors_parsed": [["Wilson", "Andrew Gordon", ""], ["Yosinski", "Jason", ""], ["Simard", "Patrice", ""], ["Caruana", "Rich", ""], ["Herlands", "William", ""]]}, {"id": "1711.09918", "submitter": "Behzad Tabibian", "authors": "Jooyeon Kim, Behzad Tabibian, Alice Oh, Bernhard Schoelkopf, Manuel\n  Gomez-Rodriguez", "title": "Leveraging the Crowd to Detect and Reduce the Spread of Fake News and\n  Misinformation", "comments": "To appear at the 11th ACM International Conference on Web Search and\n  Data Mining (WSDM 2018)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.SI cs.HC stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Online social networking sites are experimenting with the following\ncrowd-powered procedure to reduce the spread of fake news and misinformation:\nwhenever a user is exposed to a story through her feed, she can flag the story\nas misinformation and, if the story receives enough flags, it is sent to a\ntrusted third party for fact checking. If this party identifies the story as\nmisinformation, it is marked as disputed. However, given the uncertain number\nof exposures, the high cost of fact checking, and the trade-off between flags\nand exposures, the above mentioned procedure requires careful reasoning and\nsmart algorithms which, to the best of our knowledge, do not exist to date.\n  In this paper, we first introduce a flexible representation of the above\nprocedure using the framework of marked temporal point processes. Then, we\ndevelop a scalable online algorithm, Curb, to select which stories to send for\nfact checking and when to do so to efficiently reduce the spread of\nmisinformation with provable guarantees. In doing so, we need to solve a novel\nstochastic optimal control problem for stochastic differential equations with\njumps, which is of independent interest. Experiments on two real-world datasets\ngathered from Twitter and Weibo show that our algorithm may be able to\neffectively reduce the spread of fake news and misinformation.\n", "versions": [{"version": "v1", "created": "Mon, 27 Nov 2017 19:00:08 GMT"}], "update_date": "2017-11-29", "authors_parsed": [["Kim", "Jooyeon", ""], ["Tabibian", "Behzad", ""], ["Oh", "Alice", ""], ["Schoelkopf", "Bernhard", ""], ["Gomez-Rodriguez", "Manuel", ""]]}, {"id": "1711.09974", "submitter": "Bart Paul Gerard Van Parys", "authors": "Dimitris Bertsimas, Bart Van Parys", "title": "Bootstrap Robust Prescriptive Analytics", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.OC math.PR stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We address the problem of prescribing an optimal decision in a framework\nwhere the cost function depends on uncertain problem parameters that need to be\nlearned from data. Earlier work proposed prescriptive formulations based on\nsupervised machine learning methods. These prescriptive methods can factor in\ncontextual information on a potentially large number of covariates to take\ncontext specific actions which are superior to any static decision. When\nworking with noisy or corrupt data, however, such nominal prescriptive methods\ncan be prone to adverse overfitting phenomena and fail to generalize on\nout-of-sample data. In this paper we combine ideas from robust optimization and\nthe statistical bootstrap to propose novel prescriptive methods which safeguard\nagainst overfitting. We show indeed that a particular entropic robust\ncounterpart to such nominal formulations guarantees good performance on\nsynthetic bootstrap data. As bootstrap data is often a sensible proxy to actual\nout-of-sample data, our robust counterpart can be interpreted to directly\nencourage good out-of-sample performance. The associated robust prescriptive\nmethods furthermore reduce to convenient tractable convex optimization problems\nin the context of local learning methods such as nearest neighbors and\nNadaraya-Watson learning. We illustrate our data-driven decision-making\nframework and our novel robustness notion on a small newsvendor problem.\n", "versions": [{"version": "v1", "created": "Mon, 27 Nov 2017 20:29:42 GMT"}, {"version": "v2", "created": "Tue, 8 Jun 2021 14:20:46 GMT"}], "update_date": "2021-06-09", "authors_parsed": [["Bertsimas", "Dimitris", ""], ["Van Parys", "Bart", ""]]}, {"id": "1711.10056", "submitter": "Thomas Gebhart", "authors": "Thomas Gebhart and Paul Schrater", "title": "Adversary Detection in Neural Networks via Persistent Homology", "comments": "16 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We outline a detection method for adversarial inputs to deep neural networks.\nBy viewing neural network computations as graphs upon which information flows\nfrom input space to out- put distribution, we compare the differences in graphs\ninduced by different inputs. Specifically, by applying persistent homology to\nthese induced graphs, we observe that the structure of the most persistent\nsubgraphs which generate the first homology group differ between adversarial\nand unperturbed inputs. Based on this observation, we build a detection\nalgorithm that depends only on the topological information extracted during\ntraining. We test our algorithm on MNIST and achieve 98% detection adversary\naccuracy with F1-score 0.98.\n", "versions": [{"version": "v1", "created": "Tue, 28 Nov 2017 00:08:10 GMT"}], "update_date": "2017-12-01", "authors_parsed": [["Gebhart", "Thomas", ""], ["Schrater", "Paul", ""]]}, {"id": "1711.10057", "submitter": "Harish S. Bhat", "authors": "Harish S. Bhat and Sidra J. Goldman-Mellor", "title": "Predicting Adolescent Suicide Attempts with Neural Networks", "comments": "Accepted poster at NIPS 2017 Workshop on Machine Learning for Health\n  (https://ml4health.github.io/2017/)", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Though suicide is a major public health problem in the US, machine learning\nmethods are not commonly used to predict an individual's risk of\nattempting/committing suicide. In the present work, starting with an anonymized\ncollection of electronic health records for 522,056 unique, California-resident\nadolescents, we develop neural network models to predict suicide attempts. We\nframe the problem as a binary classification problem in which we use a\npatient's data from 2006-2009 to predict either the presence (1) or absence (0)\nof a suicide attempt in 2010. After addressing issues such as severely\nimbalanced classes and the variable length of a patient's history, we build\nneural networks with depths varying from two to eight hidden layers. For test\nset observations where we have at least five ED/hospital visits' worth of data\non a patient, our depth-4 model achieves a sensitivity of 0.703, specificity of\n0.980, and AUC of 0.958.\n", "versions": [{"version": "v1", "created": "Tue, 28 Nov 2017 00:08:33 GMT"}, {"version": "v2", "created": "Fri, 1 Dec 2017 08:26:34 GMT"}], "update_date": "2017-12-04", "authors_parsed": [["Bhat", "Harish S.", ""], ["Goldman-Mellor", "Sidra J.", ""]]}, {"id": "1711.10058", "submitter": "Anqi Wu", "authors": "Anqi Wu, Oluwasanmi Koyejo, Jonathan W. Pillow", "title": "Dependent relevance determination for smooth and structured sparse\n  regression", "comments": "42 pages, 15 figures, submitted to JMLR", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In many problem settings, parameter vectors are not merely sparse but\ndependent in such a way that non-zero coefficients tend to cluster together. We\nrefer to this form of dependency as \"region sparsity.\" Classical sparse\nregression methods, such as the lasso and automatic relevance determination\n(ARD), which model parameters as independent a priori, and therefore do not\nexploit such dependencies. Here we introduce a hierarchical model for smooth,\nregion-sparse weight vectors and tensors in a linear regression setting. Our\napproach represents a hierarchical extension of the relevance determination\nframework, where we add a transformed Gaussian process to model the\ndependencies between the prior variances of regression weights. We combine this\nwith a structured model of the prior variances of Fourier coefficients, which\neliminates unnecessary high frequencies. The resulting prior encourages weights\nto be region-sparse in two different bases simultaneously. We develop Laplace\napproximation and Monte Carlo Markov Chain (MCMC) sampling to provide efficient\ninference for the posterior. Furthermore, a two-stage convex relaxation of the\nLaplace approximation approach is also provided to relax the inevitable\nnon-convexity during the optimization. We finally show substantial improvements\nover comparable methods for both simulated and real datasets from brain\nimaging.\n", "versions": [{"version": "v1", "created": "Tue, 28 Nov 2017 00:11:10 GMT"}, {"version": "v2", "created": "Tue, 5 Dec 2017 00:18:55 GMT"}, {"version": "v3", "created": "Thu, 24 Jan 2019 23:13:54 GMT"}], "update_date": "2019-01-28", "authors_parsed": [["Wu", "Anqi", ""], ["Koyejo", "Oluwasanmi", ""], ["Pillow", "Jonathan W.", ""]]}, {"id": "1711.10105", "submitter": "Qingquan Song", "authors": "Qingquan Song, Hancheng Ge, James Caverlee, Xia Hu", "title": "Tensor Completion Algorithms in Big Data Analytics", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.AI cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Tensor completion is a problem of filling the missing or unobserved entries\nof partially observed tensors. Due to the multidimensional character of tensors\nin describing complex datasets, tensor completion algorithms and their\napplications have received wide attention and achievement in areas like data\nmining, computer vision, signal processing, and neuroscience. In this survey,\nwe provide a modern overview of recent advances in tensor completion algorithms\nfrom the perspective of big data analytics characterized by diverse variety,\nlarge volume, and high velocity. We characterize these advances from four\nperspectives: general tensor completion algorithms, tensor completion with\nauxiliary information (variety), scalable tensor completion algorithms\n(volume), and dynamic tensor completion algorithms (velocity). Further, we\nidentify several tensor completion applications on real-world data-driven\nproblems and present some common experimental frameworks popularized in the\nliterature. Our goal is to summarize these popular methods and introduce them\nto researchers and practitioners for promoting future research and\napplications. We conclude with a discussion of key challenges and promising\nresearch directions in this community for future exploration.\n", "versions": [{"version": "v1", "created": "Tue, 28 Nov 2017 03:44:29 GMT"}, {"version": "v2", "created": "Thu, 3 May 2018 02:26:17 GMT"}], "update_date": "2018-05-04", "authors_parsed": [["Song", "Qingquan", ""], ["Ge", "Hancheng", ""], ["Caverlee", "James", ""], ["Hu", "Xia", ""]]}, {"id": "1711.10127", "submitter": "Ching-An Cheng", "authors": "Ching-An Cheng, Byron Boots", "title": "Variational Inference for Gaussian Process Models with Linear Complexity", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Large-scale Gaussian process inference has long faced practical challenges\ndue to time and space complexity that is superlinear in dataset size. While\nsparse variational Gaussian process models are capable of learning from\nlarge-scale data, standard strategies for sparsifying the model can prevent the\napproximation of complex functions. In this work, we propose a novel\nvariational Gaussian process model that decouples the representation of mean\nand covariance functions in reproducing kernel Hilbert space. We show that this\nnew parametrization generalizes previous models. Furthermore, it yields a\nvariational inference problem that can be solved by stochastic gradient ascent\nwith time and space complexity that is only linear in the number of mean\nfunction parameters, regardless of the choice of kernels, likelihoods, and\ninducing points. This strategy makes the adoption of large-scale expressive\nGaussian process models possible. We run several experiments on regression\ntasks and show that this decoupled approach greatly outperforms previous sparse\nvariational Gaussian process inference procedures.\n", "versions": [{"version": "v1", "created": "Tue, 28 Nov 2017 05:29:32 GMT"}, {"version": "v2", "created": "Mon, 22 Jan 2018 18:41:36 GMT"}], "update_date": "2018-01-23", "authors_parsed": [["Cheng", "Ching-An", ""], ["Boots", "Byron", ""]]}, {"id": "1711.10131", "submitter": "Shan Suthaharan", "authors": "Shan Suthaharan", "title": "A fatal point concept and a low-sensitivity quantitative measure for\n  traffic safety analytics", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV stat.AP stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The variability of the clusters generated by clustering techniques in the\ndomain of latitude and longitude variables of fatal crash data are\nsignificantly unpredictable. This unpredictability, caused by the randomness of\nfatal crash incidents, reduces the accuracy of crash frequency (i.e., counts of\nfatal crashes per cluster) which is used to measure traffic safety in practice.\nIn this paper, a quantitative measure of traffic safety that is not\nsignificantly affected by the aforementioned variability is proposed. It\nintroduces a fatal point -- a segment with the highest frequency of fatality --\nconcept based on cluster characteristics and detects them by imposing rounding\nerrors to the hundredth decimal place of the longitude. The frequencies of the\ncluster and the cluster's fatal point are combined to construct a low-sensitive\nquantitative measure of traffic safety for the cluster. The performance of the\nproposed measure of traffic safety is then studied by varying the parameter k\nof k-means clustering with the expectation that other clustering techniques can\nbe adopted in a similar fashion. The 2015 North Carolina fatal crash dataset of\nFatality Analysis Reporting System (FARS) is used to evaluate the proposed\nfatal point concept and perform experimental analysis to determine the\neffectiveness of the proposed measure. The empirical study shows that the\naverage traffic safety, measured by the proposed quantitative measure over\nseveral clusters, is not significantly affected by the variability, compared to\nthat of the standard crash frequency.\n", "versions": [{"version": "v1", "created": "Tue, 28 Nov 2017 05:37:37 GMT"}], "update_date": "2017-11-29", "authors_parsed": [["Suthaharan", "Shan", ""]]}, {"id": "1711.10157", "submitter": "Utako Yamamoto", "authors": "Utako Yamamoto, Megumi Nakao, Masayuki Ohzeki and Tetsuya Matsuda", "title": "Deformation estimation of an elastic object by partial observation using\n  a neural network", "comments": "12 pages, 12 figures, 1 table", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Deformation estimation of elastic object assuming an internal organ is\nimportant for the computer navigation of surgery. The aim of this study is to\nestimate the deformation of an entire three-dimensional elastic object using\ndisplacement information of very few observation points. A learning approach\nwith a neural network was introduced to estimate the entire deformation of an\nobject. We applied our method to two elastic objects; a rectangular\nparallelepiped model, and a human liver model reconstructed from computed\ntomography data. The average estimation error for the human liver model was\n0.041 mm when the object was deformed up to 66.4 mm, from only around 3 %\nobservations. These results indicate that the deformation of an entire elastic\nobject can be estimated with an acceptable level of error from limited\nobservations by applying a trained neural network to a new deformation.\n", "versions": [{"version": "v1", "created": "Tue, 28 Nov 2017 07:28:48 GMT"}], "update_date": "2017-11-29", "authors_parsed": [["Yamamoto", "Utako", ""], ["Nakao", "Megumi", ""], ["Ohzeki", "Masayuki", ""], ["Matsuda", "Tetsuya", ""]]}, {"id": "1711.10160", "submitter": "Alexander Ratner", "authors": "Alexander Ratner, Stephen H. Bach, Henry Ehrenberg, Jason Fries, Sen\n  Wu, Christopher R\\'e", "title": "Snorkel: Rapid Training Data Creation with Weak Supervision", "comments": null, "journal-ref": "Proceedings of the VLDB Endowment, 11(3), 269-282, 2017", "doi": "10.14778/3157794.3157797", "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Labeling training data is increasingly the largest bottleneck in deploying\nmachine learning systems. We present Snorkel, a first-of-its-kind system that\nenables users to train state-of-the-art models without hand labeling any\ntraining data. Instead, users write labeling functions that express arbitrary\nheuristics, which can have unknown accuracies and correlations. Snorkel\ndenoises their outputs without access to ground truth by incorporating the\nfirst end-to-end implementation of our recently proposed machine learning\nparadigm, data programming. We present a flexible interface layer for writing\nlabeling functions based on our experience over the past year collaborating\nwith companies, agencies, and research labs. In a user study, subject matter\nexperts build models 2.8x faster and increase predictive performance an average\n45.5% versus seven hours of hand labeling. We study the modeling tradeoffs in\nthis new setting and propose an optimizer for automating tradeoff decisions\nthat gives up to 1.8x speedup per pipeline execution. In two collaborations,\nwith the U.S. Department of Veterans Affairs and the U.S. Food and Drug\nAdministration, and on four open-source text and image data sets representative\nof other deployments, Snorkel provides 132% average improvements to predictive\nperformance over prior heuristic approaches and comes within an average 3.60%\nof the predictive performance of large hand-curated training sets.\n", "versions": [{"version": "v1", "created": "Tue, 28 Nov 2017 07:48:05 GMT"}], "update_date": "2017-11-29", "authors_parsed": [["Ratner", "Alexander", ""], ["Bach", "Stephen H.", ""], ["Ehrenberg", "Henry", ""], ["Fries", "Jason", ""], ["Wu", "Sen", ""], ["R\u00e9", "Christopher", ""]]}, {"id": "1711.10162", "submitter": "Vincent Zheng", "authors": "Jia Wang, Vincent W. Zheng, Zemin Liu, Kevin Chen-Chuan Chang", "title": "Topological Recurrent Neural Network for Diffusion Prediction", "comments": "In Proc. of The IEEE International Conference on Data Mining (ICDM\n  '17), New Orleans, Louisiana, USA, 2017", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we study the problem of using representation learning to\nassist information diffusion prediction on graphs. In particular, we aim at\nestimating the probability of an inactive node to be activated next in a\ncascade. Despite the success of recent deep learning methods for diffusion, we\nfind that they often underexplore the cascade structure. We consider a cascade\nas not merely a sequence of nodes ordered by their activation time stamps;\ninstead, it has a richer structure indicating the diffusion process over the\ndata graph. As a result, we introduce a new data model, namely diffusion\ntopologies, to fully describe the cascade structure. We find it challenging to\nmodel diffusion topologies, which are dynamic directed acyclic graphs (DAGs),\nwith the existing neural networks. Therefore, we propose a novel topological\nrecurrent neural network, namely Topo-LSTM, for modeling dynamic DAGs. We\ncustomize Topo-LSTM for the diffusion prediction task, and show it improves the\nstate-of-the-art baselines, by 20.1%--56.6% (MAP) relatively, across multiple\nreal-world data sets. Our code and data sets are available online at\nhttps://github.com/vwz/topolstm.\n", "versions": [{"version": "v1", "created": "Tue, 28 Nov 2017 07:53:51 GMT"}, {"version": "v2", "created": "Wed, 29 Nov 2017 03:20:41 GMT"}], "update_date": "2017-12-01", "authors_parsed": [["Wang", "Jia", ""], ["Zheng", "Vincent W.", ""], ["Liu", "Zemin", ""], ["Chang", "Kevin Chen-Chuan", ""]]}, {"id": "1711.10166", "submitter": "Tomas Kliegr", "authors": "Tomas Kliegr", "title": "QCBA: Postoptimization of Quantitative Attributes in Classifiers based\n  on Association Rules", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.AI cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The need to prediscretize numeric attributes before they can be used in\nassociation rule learning is a source of inefficiencies in the resulting\nclassifier. This paper describes several new rule tuning steps aiming to\nrecover information lost in the discretization of numeric (quantitative)\nattributes, and a new rule pruning strategy, which further reduces the size of\nthe classification models. We demonstrate the effectiveness of the proposed\nmethods on postoptimization of models generated by three state-of-the-art\nassociation rule classification algorithms: Classification based on\nAssociations (Liu, 1998), Interpretable Decision Sets (Lakkaraju et al, 2016),\nand Scalable Bayesian Rule Lists (Yang, 2017). Benchmarks on 22 datasets from\nthe UCI repository show that the postoptimized models are consistently smaller\n-- typically by about 50% -- and have better classification performance on most\ndatasets.\n", "versions": [{"version": "v1", "created": "Tue, 28 Nov 2017 08:09:14 GMT"}, {"version": "v2", "created": "Fri, 18 Oct 2019 12:22:17 GMT"}], "update_date": "2019-10-21", "authors_parsed": [["Kliegr", "Tomas", ""]]}, {"id": "1711.10168", "submitter": "Kenta Oono", "authors": "Hai Nguyen, Shin-ichi Maeda, Kenta Oono", "title": "Semi-supervised learning of hierarchical representations of molecules\n  using neural message passing", "comments": "8 pages, 2 figures. Appeared as a poster presentation in workshop on\n  Machine Learning for Molecules and Materials in NIPS 2017", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  With the rapid increase of compound databases available in medicinal and\nmaterial science, there is a growing need for learning representations of\nmolecules in a semi-supervised manner. In this paper, we propose an\nunsupervised hierarchical feature extraction algorithm for molecules (or more\ngenerally, graph-structured objects with fixed number of types of nodes and\nedges), which is applicable to both unsupervised and semi-supervised tasks. Our\nmethod extends recently proposed Paragraph Vector algorithm and incorporates\nneural message passing to obtain hierarchical representations of subgraphs. We\napplied our method to an unsupervised task and demonstrated that it outperforms\nexisting proposed methods in several benchmark datasets. We also experimentally\nshowed that semi-supervised tasks enhanced predictive performance compared with\nsupervised ones with labeled molecules only.\n", "versions": [{"version": "v1", "created": "Tue, 28 Nov 2017 08:13:17 GMT"}, {"version": "v2", "created": "Wed, 29 Nov 2017 02:17:17 GMT"}], "update_date": "2017-11-30", "authors_parsed": [["Nguyen", "Hai", ""], ["Maeda", "Shin-ichi", ""], ["Oono", "Kenta", ""]]}, {"id": "1711.10173", "submitter": "Takayuki Osa", "authors": "Takayuki Osa and Masashi Sugiyama", "title": "Hierarchical Policy Search via Return-Weighted Density Estimation", "comments": "The 32nd AAAI Conference on Artificial Intelligence (AAAI 2018), 9\n  pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Learning an optimal policy from a multi-modal reward function is a\nchallenging problem in reinforcement learning (RL). Hierarchical RL (HRL)\ntackles this problem by learning a hierarchical policy, where multiple option\npolicies are in charge of different strategies corresponding to modes of a\nreward function and a gating policy selects the best option for a given\ncontext. Although HRL has been demonstrated to be promising, current\nstate-of-the-art methods cannot still perform well in complex real-world\nproblems due to the difficulty of identifying modes of the reward function. In\nthis paper, we propose a novel method called hierarchical policy search via\nreturn-weighted density estimation (HPSDE), which can efficiently identify the\nmodes through density estimation with return-weighted importance sampling. Our\nproposed method finds option policies corresponding to the modes of the return\nfunction and automatically determines the number and the location of option\npolicies, which significantly reduces the burden of hyper-parameters tuning.\nThrough experiments, we demonstrate that the proposed HPSDE successfully learns\noption policies corresponding to modes of the return function and that it can\nbe successfully applied to a challenging motion planning problem of a redundant\nrobotic manipulator.\n", "versions": [{"version": "v1", "created": "Tue, 28 Nov 2017 08:30:11 GMT"}, {"version": "v2", "created": "Thu, 30 Nov 2017 08:43:26 GMT"}], "update_date": "2017-12-01", "authors_parsed": [["Osa", "Takayuki", ""], ["Sugiyama", "Masashi", ""]]}, {"id": "1711.10204", "submitter": "Kevin O'Regan", "authors": "Guglielmo Montone, J. Kevin O'Regan, Alexander V. Terekhov", "title": "Block Neural Network Avoids Catastrophic Forgetting When Learning\n  Multiple Task", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NE cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In the present work we propose a Deep Feed Forward network architecture which\ncan be trained according to a sequential learning paradigm, where tasks of\nincreasing difficulty are learned sequentially, yet avoiding catastrophic\nforgetting. The proposed architecture can re-use the features learned on\nprevious tasks in a new task when the old tasks and the new one are related.\nThe architecture needs fewer computational resources (neurons and connections)\nand less data for learning the new task than a network trained from scratch\n", "versions": [{"version": "v1", "created": "Tue, 28 Nov 2017 09:47:51 GMT"}], "update_date": "2017-11-29", "authors_parsed": [["Montone", "Guglielmo", ""], ["O'Regan", "J. Kevin", ""], ["Terekhov", "Alexander V.", ""]]}, {"id": "1711.10207", "submitter": "Mohsen Ahmadi Fahandar", "authors": "Mohsen Ahmadi Fahandar, Eyke H\\\"ullermeier", "title": "Learning to Rank based on Analogical Reasoning", "comments": "Thirty-Second AAAI Conference on Artificial Intelligence (AAAI-18), 8\n  pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.AI cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Object ranking or \"learning to rank\" is an important problem in the realm of\npreference learning. On the basis of training data in the form of a set of\nrankings of objects represented as feature vectors, the goal is to learn a\nranking function that predicts a linear order of any new set of objects. In\nthis paper, we propose a new approach to object ranking based on principles of\nanalogical reasoning. More specifically, our inference pattern is formalized in\nterms of so-called analogical proportions and can be summarized as follows:\nGiven objects $A,B,C,D$, if object $A$ is known to be preferred to $B$, and $C$\nrelates to $D$ as $A$ relates to $B$, then $C$ is (supposedly) preferred to\n$D$. Our method applies this pattern as a main building block and combines it\nwith ideas and techniques from instance-based learning and rank aggregation.\nBased on first experimental results for data sets from various domains (sports,\neducation, tourism, etc.), we conclude that our approach is highly competitive.\nIt appears to be specifically interesting in situations in which the objects\nare coming from different subdomains, and which hence require a kind of\nknowledge transfer.\n", "versions": [{"version": "v1", "created": "Tue, 28 Nov 2017 09:51:18 GMT"}], "update_date": "2017-12-05", "authors_parsed": [["Fahandar", "Mohsen Ahmadi", ""], ["H\u00fcllermeier", "Eyke", ""]]}, {"id": "1711.10271", "submitter": "Marius Paraschiv", "authors": "Marius Paraschiv, Lasse Borgholt, Tycho Max Sylvester Tax, Marco Singh\n  and Lars Maal{\\o}e", "title": "Exploiting Nontrivial Connectivity for Automatic Speech Recognition", "comments": "Accepted at the ML4Audio workshop at the NIPS 2017", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.SD eess.AS stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Nontrivial connectivity has allowed the training of very deep networks by\naddressing the problem of vanishing gradients and offering a more efficient\nmethod of reusing parameters. In this paper we make a comparison between\nresidual networks, densely-connected networks and highway networks on an image\nclassification task. Next, we show that these methodologies can easily be\ndeployed into automatic speech recognition and provide significant improvements\nto existing models.\n", "versions": [{"version": "v1", "created": "Tue, 28 Nov 2017 13:13:41 GMT"}], "update_date": "2017-11-29", "authors_parsed": [["Paraschiv", "Marius", ""], ["Borgholt", "Lasse", ""], ["Tax", "Tycho Max Sylvester", ""], ["Singh", "Marco", ""], ["Maal\u00f8e", "Lars", ""]]}, {"id": "1711.10282", "submitter": "Yuji Tokozume", "authors": "Yuji Tokozume, Yoshitaka Ushiku, Tatsuya Harada", "title": "Learning from Between-class Examples for Deep Sound Recognition", "comments": "13 pages, 6 figures, published as a conference paper at ICLR 2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.SD eess.AS stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Deep learning methods have achieved high performance in sound recognition\ntasks. Deciding how to feed the training data is important for further\nperformance improvement. We propose a novel learning method for deep sound\nrecognition: Between-Class learning (BC learning). Our strategy is to learn a\ndiscriminative feature space by recognizing the between-class sounds as\nbetween-class sounds. We generate between-class sounds by mixing two sounds\nbelonging to different classes with a random ratio. We then input the mixed\nsound to the model and train the model to output the mixing ratio. The\nadvantages of BC learning are not limited only to the increase in variation of\nthe training data; BC learning leads to an enlargement of Fisher's criterion in\nthe feature space and a regularization of the positional relationship among the\nfeature distributions of the classes. The experimental results show that BC\nlearning improves the performance on various sound recognition networks,\ndatasets, and data augmentation schemes, in which BC learning proves to be\nalways beneficial. Furthermore, we construct a new deep sound recognition\nnetwork (EnvNet-v2) and train it with BC learning. As a result, we achieved a\nperformance surpasses the human level.\n", "versions": [{"version": "v1", "created": "Tue, 28 Nov 2017 13:29:45 GMT"}, {"version": "v2", "created": "Wed, 28 Feb 2018 12:41:50 GMT"}], "update_date": "2018-03-01", "authors_parsed": [["Tokozume", "Yuji", ""], ["Ushiku", "Yoshitaka", ""], ["Harada", "Tatsuya", ""]]}, {"id": "1711.10284", "submitter": "Yuji Tokozume", "authors": "Yuji Tokozume, Yoshitaka Ushiku, Tatsuya Harada", "title": "Between-class Learning for Image Classification", "comments": "11 pages, 8 figures, published as a conference paper at CVPR 2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we propose a novel learning method for image classification\ncalled Between-Class learning (BC learning). We generate between-class images\nby mixing two images belonging to different classes with a random ratio. We\nthen input the mixed image to the model and train the model to output the\nmixing ratio. BC learning has the ability to impose constraints on the shape of\nthe feature distributions, and thus the generalization ability is improved. BC\nlearning is originally a method developed for sounds, which can be digitally\nmixed. Mixing two image data does not appear to make sense; however, we argue\nthat because convolutional neural networks have an aspect of treating input\ndata as waveforms, what works on sounds must also work on images. First, we\npropose a simple mixing method using internal divisions, which surprisingly\nproves to significantly improve performance. Second, we propose a mixing method\nthat treats the images as waveforms, which leads to a further improvement in\nperformance. As a result, we achieved 19.4% and 2.26% top-1 errors on\nImageNet-1K and CIFAR-10, respectively.\n", "versions": [{"version": "v1", "created": "Tue, 28 Nov 2017 13:31:14 GMT"}, {"version": "v2", "created": "Sun, 8 Apr 2018 08:50:09 GMT"}], "update_date": "2018-04-10", "authors_parsed": [["Tokozume", "Yuji", ""], ["Ushiku", "Yoshitaka", ""], ["Harada", "Tatsuya", ""]]}, {"id": "1711.10327", "submitter": "Danijar Hafner", "authors": "Danijar Hafner, Alexander Immer, Willi Raschkowski, Fabian Windheuser", "title": "Generative Interest Estimation for Document Recommendations", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR cs.CL cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Learning distributed representations of documents has pushed the\nstate-of-the-art in several natural language processing tasks and was\nsuccessfully applied to the field of recommender systems recently. In this\npaper, we propose a novel content-based recommender system based on learned\nrepresentations and a generative model of user interest. Our method works as\nfollows: First, we learn representations on a corpus of text documents. Then,\nwe capture a user's interest as a generative model in the space of the document\nrepresentations. In particular, we model the distribution of interest for each\nuser as a Gaussian mixture model (GMM). Recommendations can be obtained\ndirectly by sampling from a user's generative model. Using Latent semantic\nanalysis (LSA) as comparison, we compute and explore document representations\non the Delicious bookmarks dataset, a standard benchmark for recommender\nsystems. We then perform density estimation in both spaces and show that\nlearned representations outperform LSA in terms of predictive performance.\n", "versions": [{"version": "v1", "created": "Tue, 28 Nov 2017 15:00:24 GMT"}], "update_date": "2017-11-29", "authors_parsed": [["Hafner", "Danijar", ""], ["Immer", "Alexander", ""], ["Raschkowski", "Willi", ""], ["Windheuser", "Fabian", ""]]}, {"id": "1711.10337", "submitter": "Mario Lucic", "authors": "Mario Lucic, Karol Kurach, Marcin Michalski, Sylvain Gelly, Olivier\n  Bousquet", "title": "Are GANs Created Equal? A Large-Scale Study", "comments": "NIPS'18: Added a section on the limitations of the study and\n  additional empirical results", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Generative adversarial networks (GAN) are a powerful subclass of generative\nmodels. Despite a very rich research activity leading to numerous interesting\nGAN algorithms, it is still very hard to assess which algorithm(s) perform\nbetter than others. We conduct a neutral, multi-faceted large-scale empirical\nstudy on state-of-the art models and evaluation measures. We find that most\nmodels can reach similar scores with enough hyperparameter optimization and\nrandom restarts. This suggests that improvements can arise from a higher\ncomputational budget and tuning more than fundamental algorithmic changes. To\novercome some limitations of the current metrics, we also propose several data\nsets on which precision and recall can be computed. Our experimental results\nsuggest that future GAN research should be based on more systematic and\nobjective evaluation procedures. Finally, we did not find evidence that any of\nthe tested algorithms consistently outperforms the non-saturating GAN\nintroduced in \\cite{goodfellow2014generative}.\n", "versions": [{"version": "v1", "created": "Tue, 28 Nov 2017 15:19:53 GMT"}, {"version": "v2", "created": "Thu, 30 Nov 2017 17:09:16 GMT"}, {"version": "v3", "created": "Tue, 20 Mar 2018 09:18:08 GMT"}, {"version": "v4", "created": "Mon, 29 Oct 2018 15:34:15 GMT"}], "update_date": "2018-10-30", "authors_parsed": [["Lucic", "Mario", ""], ["Kurach", "Karol", ""], ["Michalski", "Marcin", ""], ["Gelly", "Sylvain", ""], ["Bousquet", "Olivier", ""]]}, {"id": "1711.10353", "submitter": "Vassilis N. Ioannidis", "authors": "Vassilis N. Ioannidis, Meng Ma, Athanasios N. Nikolakopoulos, Georgios\n  B. Giannakis, and Daniel Romero", "title": "Kernel-based Inference of Functions over Graphs", "comments": "To be published as a chapter in `Adaptive Learning Methods for\n  Nonlinear System Modeling', Elsevier Publishing, Eds. D. Comminiello and J.C.\n  Principe (2018). This chapter surveys recent work on kernel-based inference\n  of functions over graphs including arXiv:1612.03615 and arXiv:1605.07174 and\n  arXiv:1711.09306", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG eess.SP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The study of networks has witnessed an explosive growth over the past decades\nwith several ground-breaking methods introduced. A particularly interesting --\nand prevalent in several fields of study -- problem is that of inferring a\nfunction defined over the nodes of a network. This work presents a versatile\nkernel-based framework for tackling this inference problem that naturally\nsubsumes and generalizes the reconstruction approaches put forth recently by\nthe signal processing on graphs community. Both the static and the dynamic\nsettings are considered along with effective modeling approaches for addressing\nreal-world problems. The herein analytical discussion is complemented by a set\nof numerical examples, which showcase the effectiveness of the presented\ntechniques, as well as their merits related to state-of-the-art methods.\n", "versions": [{"version": "v1", "created": "Tue, 28 Nov 2017 15:43:04 GMT"}, {"version": "v2", "created": "Tue, 10 Apr 2018 23:54:38 GMT"}], "update_date": "2018-09-25", "authors_parsed": [["Ioannidis", "Vassilis N.", ""], ["Ma", "Meng", ""], ["Nikolakopoulos", "Athanasios N.", ""], ["Giannakis", "Georgios B.", ""], ["Romero", "Daniel", ""]]}, {"id": "1711.10388", "submitter": "Rushil Anirudh", "authors": "Rushil Anirudh, Hyojin Kim, Jayaraman J. Thiagarajan, K. Aditya Mohan,\n  Kyle Champley, Timo Bremer", "title": "Lose The Views: Limited Angle CT Reconstruction via Implicit Sinogram\n  Completion", "comments": "Spotlight presentation at CVPR 2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Computed Tomography (CT) reconstruction is a fundamental component to a wide\nvariety of applications ranging from security, to healthcare. The classical\ntechniques require measuring projections, called sinograms, from a full\n180$^\\circ$ view of the object. This is impractical in a limited angle\nscenario, when the viewing angle is less than 180$^\\circ$, which can occur due\nto different factors including restrictions on scanning time, limited\nflexibility of scanner rotation, etc. The sinograms obtained as a result, cause\nexisting techniques to produce highly artifact-laden reconstructions. In this\npaper, we propose to address this problem through implicit sinogram completion,\non a challenging real world dataset containing scans of common checked-in\nluggage. We propose a system, consisting of 1D and 2D convolutional neural\nnetworks, that operates on a limited angle sinogram to directly produce the\nbest estimate of a reconstruction. Next, we use the x-ray transform on this\nreconstruction to obtain a \"completed\" sinogram, as if it came from a full\n180$^\\circ$ measurement. We feed this to standard analytical and iterative\nreconstruction techniques to obtain the final reconstruction. We show with\nextensive experimentation that this combined strategy outperforms many\ncompetitive baselines. We also propose a measure of confidence for the\nreconstruction that enables a practitioner to gauge the reliability of a\nprediction made by our network. We show that this measure is a strong indicator\nof quality as measured by the PSNR, while not requiring ground truth at test\ntime. Finally, using a segmentation experiment, we show that our reconstruction\npreserves the 3D structure of objects effectively.\n", "versions": [{"version": "v1", "created": "Tue, 28 Nov 2017 16:37:14 GMT"}, {"version": "v2", "created": "Wed, 14 Mar 2018 16:23:30 GMT"}, {"version": "v3", "created": "Wed, 11 Jul 2018 17:20:08 GMT"}], "update_date": "2018-07-12", "authors_parsed": [["Anirudh", "Rushil", ""], ["Kim", "Hyojin", ""], ["Thiagarajan", "Jayaraman J.", ""], ["Mohan", "K. Aditya", ""], ["Champley", "Kyle", ""], ["Bremer", "Timo", ""]]}, {"id": "1711.10396", "submitter": "Pouyan Ahmadi", "authors": "Khondkar Islam, Pouyan Ahmadi, Salman Yousaf", "title": "Assessment Formats and Student Learning Performance: What is the\n  Relation?", "comments": "Proceedings of The 7th Research in Engineering Education Symposium\n  (REES 2017)", "journal-ref": null, "doi": null, "report-no": null, "categories": "physics.ed-ph cs.CY stat.AP stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Although compelling assessments have been examined in recent years, more\nstudies are required to yield a better understanding of the several methods\nwhere assessment techniques significantly affect student learning process. Most\nof the educational research in this area does not consider demographics data,\ndiffering methodologies, and notable sample size. To address these drawbacks,\nthe objective of our study is to analyse student learning outcomes of multiple\nassessment formats for a web-facilitated in-class section with an asynchronous\nonline class of a core data communications course in the Undergraduate IT\nprogram of the Information Sciences and Technology (IST) Department at George\nMason University (GMU). In this study, students were evaluated based on course\nassessments such as home and lab assignments, skill-based assessments, and\ntraditional midterm and final exams across all four sections of the course. All\nsections have equivalent content, assessments, and teaching methodologies.\nStudent demographics such as exam type and location preferences are considered\nin our study to determine whether they have any impact on their learning\napproach. Large amount of data from the learning management system (LMS),\nBlackboard (BB) Learn, had to be examined to compare the results of several\nassessment outcomes for all students within their respective section and\namongst students of other sections. To investigate the effect of dissimilar\nassessment formats on student performance, we had to correlate individual\nquestion formats with the overall course grade. The results show that\ncollective assessment formats allow students to be effective in demonstrating\ntheir knowledge.\n", "versions": [{"version": "v1", "created": "Wed, 15 Nov 2017 18:35:58 GMT"}], "update_date": "2017-12-15", "authors_parsed": [["Islam", "Khondkar", ""], ["Ahmadi", "Pouyan", ""], ["Yousaf", "Salman", ""]]}, {"id": "1711.10411", "submitter": "Yang Feng", "authors": "Yang Feng, Yichao Wu, Leonard Stefanski", "title": "Nonparametric Independence Screening via Favored Smoothing Bandwidth", "comments": "22 pages", "journal-ref": "Journal of Statistical Planning and Inference Volume 197, December\n  2018, Pages 1-14", "doi": "10.1016/j.jspi.2017.11.006", "report-no": null, "categories": "stat.ME stat.AP stat.CO stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a flexible nonparametric regression method for\nultrahigh-dimensional data. As a first step, we propose a fast screening method\nbased on the favored smoothing bandwidth of the marginal local constant\nregression. Then, an iterative procedure is developed to recover both the\nimportant covariates and the regression function. Theoretically, we prove that\nthe favored smoothing bandwidth based screening possesses the model selection\nconsistency property. Simulation studies as well as real data analysis show the\ncompetitive performance of the new procedure.\n", "versions": [{"version": "v1", "created": "Tue, 28 Nov 2017 17:11:20 GMT"}], "update_date": "2018-07-30", "authors_parsed": [["Feng", "Yang", ""], ["Wu", "Yichao", ""], ["Stefanski", "Leonard", ""]]}, {"id": "1711.10456", "submitter": "Chi Jin", "authors": "Chi Jin, Praneeth Netrapalli, Michael I. Jordan", "title": "Accelerated Gradient Descent Escapes Saddle Points Faster than Gradient\n  Descent", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG math.OC stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Nesterov's accelerated gradient descent (AGD), an instance of the general\nfamily of \"momentum methods\", provably achieves faster convergence rate than\ngradient descent (GD) in the convex setting. However, whether these methods are\nsuperior to GD in the nonconvex setting remains open. This paper studies a\nsimple variant of AGD, and shows that it escapes saddle points and finds a\nsecond-order stationary point in $\\tilde{O}(1/\\epsilon^{7/4})$ iterations,\nfaster than the $\\tilde{O}(1/\\epsilon^{2})$ iterations required by GD. To the\nbest of our knowledge, this is the first Hessian-free algorithm to find a\nsecond-order stationary point faster than GD, and also the first single-loop\nalgorithm with a faster rate than GD even in the setting of finding a\nfirst-order stationary point. Our analysis is based on two key ideas: (1) the\nuse of a simple Hamiltonian function, inspired by a continuous-time\nperspective, which AGD monotonically decreases per step even for nonconvex\nfunctions, and (2) a novel framework called improve or localize, which is\nuseful for tracking the long-term behavior of gradient-based optimization\nalgorithms. We believe that these techniques may deepen our understanding of\nboth acceleration algorithms and nonconvex optimization.\n", "versions": [{"version": "v1", "created": "Tue, 28 Nov 2017 18:38:35 GMT"}], "update_date": "2017-11-29", "authors_parsed": [["Jin", "Chi", ""], ["Netrapalli", "Praneeth", ""], ["Jordan", "Michael I.", ""]]}, {"id": "1711.10462", "submitter": "Francis Dutil", "authors": "Francis Dutil, Caglar Gulcehre, Adam Trischler, Yoshua Bengio", "title": "Plan, Attend, Generate: Planning for Sequence-to-Sequence Models", "comments": "NIPS 2017", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We investigate the integration of a planning mechanism into\nsequence-to-sequence models using attention. We develop a model which can plan\nahead in the future when it computes its alignments between input and output\nsequences, constructing a matrix of proposed future alignments and a commitment\nvector that governs whether to follow or recompute the plan. This mechanism is\ninspired by the recently proposed strategic attentive reader and writer (STRAW)\nmodel for Reinforcement Learning. Our proposed model is end-to-end trainable\nusing primarily differentiable operations. We show that it outperforms a strong\nbaseline on character-level translation tasks from WMT'15, the algorithmic task\nof finding Eulerian circuits of graphs, and question generation from the text.\nOur analysis demonstrates that the model computes qualitatively intuitive\nalignments, converges faster than the baselines, and achieves superior\nperformance with fewer parameters.\n", "versions": [{"version": "v1", "created": "Tue, 28 Nov 2017 18:50:05 GMT"}], "update_date": "2017-11-29", "authors_parsed": [["Dutil", "Francis", ""], ["Gulcehre", "Caglar", ""], ["Trischler", "Adam", ""], ["Bengio", "Yoshua", ""]]}, {"id": "1711.10467", "submitter": "Cong Ma", "authors": "Cong Ma, Kaizheng Wang, Yuejie Chi, Yuxin Chen", "title": "Implicit Regularization in Nonconvex Statistical Estimation: Gradient\n  Descent Converges Linearly for Phase Retrieval, Matrix Completion, and Blind\n  Deconvolution", "comments": "accepted to Foundations of Computational Mathematics (FOCM)", "journal-ref": "Foundations of Computational Mathematics, vol. 20, no. 3, pp.\n  451-632, June 2020", "doi": "10.1007/s10208-019-09429-9", "report-no": null, "categories": "cs.LG cs.IT math.IT math.OC math.ST stat.ML stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recent years have seen a flurry of activities in designing provably efficient\nnonconvex procedures for solving statistical estimation problems. Due to the\nhighly nonconvex nature of the empirical loss, state-of-the-art procedures\noften require proper regularization (e.g. trimming, regularized cost,\nprojection) in order to guarantee fast convergence. For vanilla procedures such\nas gradient descent, however, prior theory either recommends highly\nconservative learning rates to avoid overshooting, or completely lacks\nperformance guarantees.\n  This paper uncovers a striking phenomenon in nonconvex optimization: even in\nthe absence of explicit regularization, gradient descent enforces proper\nregularization implicitly under various statistical models. In fact, gradient\ndescent follows a trajectory staying within a basin that enjoys nice geometry,\nconsisting of points incoherent with the sampling mechanism. This \"implicit\nregularization\" feature allows gradient descent to proceed in a far more\naggressive fashion without overshooting, which in turn results in substantial\ncomputational savings. Focusing on three fundamental statistical estimation\nproblems, i.e. phase retrieval, low-rank matrix completion, and blind\ndeconvolution, we establish that gradient descent achieves near-optimal\nstatistical and computational guarantees without explicit regularization. In\nparticular, by marrying statistical modeling with generic optimization theory,\nwe develop a general recipe for analyzing the trajectories of iterative\nalgorithms via a leave-one-out perturbation argument. As a byproduct, for noisy\nmatrix completion, we demonstrate that gradient descent achieves near-optimal\nerror control --- measured entrywise and by the spectral norm --- which might\nbe of independent interest.\n", "versions": [{"version": "v1", "created": "Tue, 28 Nov 2017 18:53:38 GMT"}, {"version": "v2", "created": "Thu, 14 Dec 2017 10:47:03 GMT"}, {"version": "v3", "created": "Tue, 30 Jul 2019 02:14:54 GMT"}], "update_date": "2020-06-09", "authors_parsed": [["Ma", "Cong", ""], ["Wang", "Kaizheng", ""], ["Chi", "Yuejie", ""], ["Chen", "Yuxin", ""]]}, {"id": "1711.10558", "submitter": "Biswarup Bhattacharya", "authors": "Biswarup Bhattacharya, Iftikhar Burhanuddin, Abhilasha Sancheti,\n  Kushal Satya", "title": "Intent-Aware Contextual Recommendation System", "comments": "Presented at the 5th International Workshop on Data Science and Big\n  Data Analytics (DSBDA), 17th IEEE International Conference on Data Mining\n  (ICDM) 2017; 8 pages; 4 figures; Due to the limitation \"The abstract field\n  cannot be longer than 1,920 characters,\" the abstract appearing here is\n  slightly shorter than the one in the PDF file", "journal-ref": null, "doi": "10.1109/ICDMW.2017.8", "report-no": null, "categories": "cs.IR cs.AI cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recommender systems take inputs from user history, use an internal ranking\nalgorithm to generate results and possibly optimize this ranking based on\nfeedback. However, often the recommender system is unaware of the actual intent\nof the user and simply provides recommendations dynamically without properly\nunderstanding the thought process of the user. An intelligent recommender\nsystem is not only useful for the user but also for businesses which want to\nlearn the tendencies of their users. Finding out tendencies or intents of a\nuser is a difficult problem to solve.\n  Keeping this in mind, we sought out to create an intelligent system which\nwill keep track of the user's activity on a web-application as well as\ndetermine the intent of the user in each session. We devised a way to encode\nthe user's activity through the sessions. Then, we have represented the\ninformation seen by the user in a high dimensional format which is reduced to\nlower dimensions using tensor factorization techniques. The aspect of intent\nawareness (or scoring) is dealt with at this stage. Finally, combining the user\nactivity data with the contextual information gives the recommendation score.\nThe final recommendations are then ranked using filtering and collaborative\nrecommendation techniques to show the top-k recommendations to the user. A\nprovision for feedback is also envisioned in the current system which informs\nthe model to update the various weights in the recommender system. Our overall\nmodel aims to combine both frequency-based and context-based recommendation\nsystems and quantify the intent of a user to provide better recommendations.\n  We ran experiments on real-world timestamped user activity data, in the\nsetting of recommending reports to the users of a business analytics tool and\nthe results are better than the baselines. We also tuned certain aspects of our\nmodel to arrive at optimized results.\n", "versions": [{"version": "v1", "created": "Tue, 28 Nov 2017 21:58:52 GMT"}], "update_date": "2017-11-30", "authors_parsed": [["Bhattacharya", "Biswarup", ""], ["Burhanuddin", "Iftikhar", ""], ["Sancheti", "Abhilasha", ""], ["Satya", "Kushal", ""]]}, {"id": "1711.10561", "submitter": "Maziar Raissi", "authors": "Maziar Raissi, Paris Perdikaris, and George Em Karniadakis", "title": "Physics Informed Deep Learning (Part I): Data-driven Solutions of\n  Nonlinear Partial Differential Equations", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.LG cs.NA math.DS stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We introduce physics informed neural networks -- neural networks that are\ntrained to solve supervised learning tasks while respecting any given law of\nphysics described by general nonlinear partial differential equations. In this\ntwo part treatise, we present our developments in the context of solving two\nmain classes of problems: data-driven solution and data-driven discovery of\npartial differential equations. Depending on the nature and arrangement of the\navailable data, we devise two distinct classes of algorithms, namely continuous\ntime and discrete time models. The resulting neural networks form a new class\nof data-efficient universal function approximators that naturally encode any\nunderlying physical laws as prior information. In this first part, we\ndemonstrate how these networks can be used to infer solutions to partial\ndifferential equations, and obtain physics-informed surrogate models that are\nfully differentiable with respect to all input coordinates and free parameters.\n", "versions": [{"version": "v1", "created": "Tue, 28 Nov 2017 21:21:59 GMT"}], "update_date": "2017-11-30", "authors_parsed": [["Raissi", "Maziar", ""], ["Perdikaris", "Paris", ""], ["Karniadakis", "George Em", ""]]}, {"id": "1711.10566", "submitter": "Maziar Raissi", "authors": "Maziar Raissi, Paris Perdikaris, and George Em Karniadakis", "title": "Physics Informed Deep Learning (Part II): Data-driven Discovery of\n  Nonlinear Partial Differential Equations", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.LG math.AP math.NA stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We introduce physics informed neural networks -- neural networks that are\ntrained to solve supervised learning tasks while respecting any given law of\nphysics described by general nonlinear partial differential equations. In this\nsecond part of our two-part treatise, we focus on the problem of data-driven\ndiscovery of partial differential equations. Depending on whether the available\ndata is scattered in space-time or arranged in fixed temporal snapshots, we\nintroduce two main classes of algorithms, namely continuous time and discrete\ntime models. The effectiveness of our approach is demonstrated using a wide\nrange of benchmark problems in mathematical physics, including conservation\nlaws, incompressible fluid flow, and the propagation of nonlinear shallow-water\nwaves.\n", "versions": [{"version": "v1", "created": "Tue, 28 Nov 2017 21:29:35 GMT"}], "update_date": "2017-11-30", "authors_parsed": [["Raissi", "Maziar", ""], ["Perdikaris", "Paris", ""], ["Karniadakis", "George Em", ""]]}, {"id": "1711.10581", "submitter": "Daniel Luckett", "authors": "Daniel J. Luckett, Eric B. Laber, Michael R. Kosorok", "title": "Estimation and Optimization of Composite Outcomes", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  There is tremendous interest in precision medicine as a means to improve\npatient outcomes by tailoring treatment to individual characteristics. An\nindividualized treatment rule formalizes precision medicine as a map from\npatient information to a recommended treatment. A treatment rule is defined to\nbe optimal if it maximizes the mean of a scalar outcome in a population of\ninterest, e.g., symptom reduction. However, clinical and intervention\nscientists often must balance multiple and possibly competing outcomes, e.g.,\nsymptom reduction and the risk of an adverse event. One approach to precision\nmedicine in this setting is to elicit a composite outcome which balances all\ncompeting outcomes; unfortunately, eliciting a composite outcome directly from\npatients is difficult without a high-quality instrument, and an expert-derived\ncomposite outcome may not account for heterogeneity in patient preferences. We\npropose a new paradigm for the study of precision medicine using observational\ndata that relies solely on the assumption that clinicians are approximately\n(i.e., imperfectly) making decisions to maximize individual patient utility.\nEstimated composite outcomes are subsequently used to construct an estimator of\nan individualized treatment rule which maximizes the mean of patient-specific\ncomposite outcomes. The estimated composite outcomes and estimated optimal\nindividualized treatment rule provide new insights into patient preference\nheterogeneity, clinician behavior, and the value of precision medicine in a\ngiven domain. We derive inference procedures for the proposed estimators under\nmild conditions and demonstrate their finite sample performance through a suite\nof simulation experiments and an illustrative application to data from a study\nof bipolar depression.\n", "versions": [{"version": "v1", "created": "Tue, 28 Nov 2017 22:00:59 GMT"}, {"version": "v2", "created": "Tue, 13 Feb 2018 01:42:16 GMT"}, {"version": "v3", "created": "Wed, 23 Jan 2019 20:32:12 GMT"}, {"version": "v4", "created": "Tue, 26 May 2020 22:14:44 GMT"}], "update_date": "2020-05-28", "authors_parsed": [["Luckett", "Daniel J.", ""], ["Laber", "Eric B.", ""], ["Kosorok", "Michael R.", ""]]}, {"id": "1711.10589", "submitter": "Ninghao Liu", "authors": "Ninghao Liu, Donghwa Shin, Xia Hu", "title": "Contextual Outlier Interpretation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Outlier detection plays an essential role in many data-driven applications to\nidentify isolated instances that are different from the majority. While many\nstatistical learning and data mining techniques have been used for developing\nmore effective outlier detection algorithms, the interpretation of detected\noutliers does not receive much attention. Interpretation is becoming\nincreasingly important to help people trust and evaluate the developed models\nthrough providing intrinsic reasons why the certain outliers are chosen. It is\ndifficult, if not impossible, to simply apply feature selection for explaining\noutliers due to the distinct characteristics of various detection models,\ncomplicated structures of data in certain applications, and imbalanced\ndistribution of outliers and normal instances. In addition, the role of\ncontrastive contexts where outliers locate, as well as the relation between\noutliers and contexts, are usually overlooked in interpretation. To tackle the\nissues above, in this paper, we propose a novel Contextual Outlier\nINterpretation (COIN) method to explain the abnormality of existing outliers\nspotted by detectors. The interpretability for an outlier is achieved from\nthree aspects: outlierness score, attributes that contribute to the\nabnormality, and contextual description of its neighborhoods. Experimental\nresults on various types of datasets demonstrate the flexibility and\neffectiveness of the proposed framework compared with existing interpretation\napproaches.\n", "versions": [{"version": "v1", "created": "Tue, 28 Nov 2017 22:17:56 GMT"}, {"version": "v2", "created": "Thu, 19 Apr 2018 21:37:27 GMT"}, {"version": "v3", "created": "Fri, 4 May 2018 23:25:18 GMT"}], "update_date": "2018-05-08", "authors_parsed": [["Liu", "Ninghao", ""], ["Shin", "Donghwa", ""], ["Hu", "Xia", ""]]}, {"id": "1711.10604", "submitter": "Dustin Tran", "authors": "Joshua V. Dillon, Ian Langmore, Dustin Tran, Eugene Brevdo, Srinivas\n  Vasudevan, Dave Moore, Brian Patton, Alex Alemi, Matt Hoffman, Rif A. Saurous", "title": "TensorFlow Distributions", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI cs.PL stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The TensorFlow Distributions library implements a vision of probability\ntheory adapted to the modern deep-learning paradigm of end-to-end\ndifferentiable computation. Building on two basic abstractions, it offers\nflexible building blocks for probabilistic computation. Distributions provide\nfast, numerically stable methods for generating samples and computing\nstatistics, e.g., log density. Bijectors provide composable volume-tracking\ntransformations with automatic caching. Together these enable modular\nconstruction of high dimensional distributions and transformations not possible\nwith previous libraries (e.g., pixelCNNs, autoregressive flows, and reversible\nresidual networks). They are the workhorse behind deep probabilistic\nprogramming systems like Edward and empower fast black-box inference in\nprobabilistic models built on deep-network components. TensorFlow Distributions\nhas proven an important part of the TensorFlow toolkit within Google and in the\nbroader deep learning community.\n", "versions": [{"version": "v1", "created": "Tue, 28 Nov 2017 23:05:15 GMT"}], "update_date": "2017-12-04", "authors_parsed": [["Dillon", "Joshua V.", ""], ["Langmore", "Ian", ""], ["Tran", "Dustin", ""], ["Brevdo", "Eugene", ""], ["Vasudevan", "Srinivas", ""], ["Moore", "Dave", ""], ["Patton", "Brian", ""], ["Alemi", "Alex", ""], ["Hoffman", "Matt", ""], ["Saurous", "Rif A.", ""]]}, {"id": "1711.10635", "submitter": "Shuxiao Chen", "authors": "Shuxiao Chen, Jacob Bien", "title": "Valid Inference Corrected for Outlier Removal", "comments": "21 pages, 6 figures, 2 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME math.ST stat.CO stat.ML stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Ordinary least square (OLS) estimation of a linear regression model is\nwell-known to be highly sensitive to outliers. It is common practice to (1)\nidentify and remove outliers by looking at the data and (2) to fit OLS and form\nconfidence intervals and p-values on the remaining data as if this were the\noriginal data collected. This standard \"detect-and-forget\" approach has been\nshown to be problematic, and in this paper we highlight the fact that it can\nlead to invalid inference and show how recently developed tools in selective\ninference can be used to properly account for outlier detection and removal.\nOur inferential procedures apply to a general class of outlier removal\nprocedures that includes several of the most commonly used approaches. We\nconduct simulations to corroborate the theoretical results, and we apply our\nmethod to three real data sets to illustrate how our inferential results can\ndiffer from the traditional detect-and-forget strategy. A companion R package,\noutference, implements these new procedures with an interface that matches the\nfunctions commonly used for inference with lm in R.\n", "versions": [{"version": "v1", "created": "Wed, 29 Nov 2017 01:18:56 GMT"}, {"version": "v2", "created": "Thu, 30 Nov 2017 01:13:13 GMT"}, {"version": "v3", "created": "Sat, 10 Aug 2019 04:41:56 GMT"}], "update_date": "2019-08-13", "authors_parsed": [["Chen", "Shuxiao", ""], ["Bien", "Jacob", ""]]}, {"id": "1711.10663", "submitter": "Erin Craig", "authors": "Erin Craig, Carlos Arias and David Gillman", "title": "Predicting readmission risk from doctors' notes", "comments": "Accepted poster at NIPS 2017 Workshop on Machine Learning for Health\n  (https://ml4health.github.io/2017/)", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We develop a model using deep learning techniques and natural language\nprocessing on unstructured text from medical records to predict hospital-wide\n$30$-day unplanned readmission, with c-statistic $.70$. Our model is\nconstructed to allow physicians to interpret the significant features for\nprediction.\n", "versions": [{"version": "v1", "created": "Wed, 29 Nov 2017 03:32:36 GMT"}, {"version": "v2", "created": "Wed, 20 Dec 2017 17:19:26 GMT"}], "update_date": "2017-12-21", "authors_parsed": [["Craig", "Erin", ""], ["Arias", "Carlos", ""], ["Gillman", "David", ""]]}, {"id": "1711.10678", "submitter": "Zhenliang He", "authors": "Zhenliang He, Wangmeng Zuo, Meina Kan, Shiguang Shan and Xilin Chen", "title": "AttGAN: Facial Attribute Editing by Only Changing What You Want", "comments": "Submitted to IEEE Transactions on Image Processing, Code:\n  https://github.com/LynnHo/AttGAN-Tensorflow", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Facial attribute editing aims to manipulate single or multiple attributes of\na face image, i.e., to generate a new face with desired attributes while\npreserving other details. Recently, generative adversarial net (GAN) and\nencoder-decoder architecture are usually incorporated to handle this task with\npromising results. Based on the encoder-decoder architecture, facial attribute\nediting is achieved by decoding the latent representation of the given face\nconditioned on the desired attributes. Some existing methods attempt to\nestablish an attribute-independent latent representation for further attribute\nediting. However, such attribute-independent constraint on the latent\nrepresentation is excessive because it restricts the capacity of the latent\nrepresentation and may result in information loss, leading to over-smooth and\ndistorted generation. Instead of imposing constraints on the latent\nrepresentation, in this work we apply an attribute classification constraint to\nthe generated image to just guarantee the correct change of desired attributes,\ni.e., to \"change what you want\". Meanwhile, the reconstruction learning is\nintroduced to preserve attribute-excluding details, in other words, to \"only\nchange what you want\". Besides, the adversarial learning is employed for\nvisually realistic editing. These three components cooperate with each other\nforming an effective framework for high quality facial attribute editing,\nreferred as AttGAN. Furthermore, our method is also directly applicable for\nattribute intensity control and can be naturally extended for attribute style\nmanipulation. Experiments on CelebA dataset show that our method outperforms\nthe state-of-the-arts on realistic attribute editing with facial details well\npreserved.\n", "versions": [{"version": "v1", "created": "Wed, 29 Nov 2017 04:50:31 GMT"}, {"version": "v2", "created": "Wed, 6 Dec 2017 04:28:30 GMT"}, {"version": "v3", "created": "Wed, 25 Jul 2018 10:08:00 GMT"}], "update_date": "2018-07-26", "authors_parsed": [["He", "Zhenliang", ""], ["Zuo", "Wangmeng", ""], ["Kan", "Meina", ""], ["Shan", "Shiguang", ""], ["Chen", "Xilin", ""]]}, {"id": "1711.10733", "submitter": "Florian Bernard", "authors": "Florian Bernard, Christian Theobalt, Michael Moeller", "title": "DS*: Tighter Lifting-Free Convex Relaxations for Quadratic Matching\n  Problems", "comments": "Published at CVPR 2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.OC cs.CV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this work we study convex relaxations of quadratic optimisation problems\nover permutation matrices. While existing semidefinite programming approaches\ncan achieve remarkably tight relaxations, they have the strong disadvantage\nthat they lift the original $n {\\times} n$-dimensional variable to an $n^2\n{\\times} n^2$-dimensional variable, which limits their practical applicability.\nIn contrast, here we present a lifting-free convex relaxation that is provably\nat least as tight as existing (lifting-free) convex relaxations. We demonstrate\nexperimentally that our approach is superior to existing convex and non-convex\nmethods for various problems, including image arrangement and multi-graph\nmatching.\n", "versions": [{"version": "v1", "created": "Wed, 29 Nov 2017 09:04:27 GMT"}, {"version": "v2", "created": "Tue, 31 Jul 2018 07:22:55 GMT"}], "update_date": "2018-08-01", "authors_parsed": [["Bernard", "Florian", ""], ["Theobalt", "Christian", ""], ["Moeller", "Michael", ""]]}, {"id": "1711.10755", "submitter": "Rui Feng", "authors": "Rui Feng, Yang Yang, Wenjie Hu, Fei Wu, Yueting Zhuang", "title": "Representation Learning for Scale-free Networks", "comments": "8 figures; accepted by AAAI 2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.SI cs.AI stat.AP stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Network embedding aims to learn the low-dimensional representations of\nvertexes in a network, while structure and inherent properties of the network\nis preserved. Existing network embedding works primarily focus on preserving\nthe microscopic structure, such as the first- and second-order proximity of\nvertexes, while the macroscopic scale-free property is largely ignored.\nScale-free property depicts the fact that vertex degrees follow a heavy-tailed\ndistribution (i.e., only a few vertexes have high degrees) and is a critical\nproperty of real-world networks, such as social networks. In this paper, we\nstudy the problem of learning representations for scale-free networks. We first\ntheoretically analyze the difficulty of embedding and reconstructing a\nscale-free network in the Euclidean space, by converting our problem to the\nsphere packing problem. Then, we propose the \"degree penalty\" principle for\ndesigning scale-free property preserving network embedding algorithm: punishing\nthe proximity between high-degree vertexes. We introduce two implementations of\nour principle by utilizing the spectral techniques and a skip-gram model\nrespectively. Extensive experiments on six datasets show that our algorithms\nare able to not only reconstruct heavy-tailed distributed degree distribution,\nbut also outperform state-of-the-art embedding models in various network mining\ntasks, such as vertex classification and link prediction.\n", "versions": [{"version": "v1", "created": "Wed, 29 Nov 2017 10:15:17 GMT"}], "update_date": "2017-11-30", "authors_parsed": [["Feng", "Rui", ""], ["Yang", "Yang", ""], ["Hu", "Wenjie", ""], ["Wu", "Fei", ""], ["Zhuang", "Yueting", ""]]}, {"id": "1711.10781", "submitter": "Stephan Rabanser", "authors": "Stephan Rabanser, Oleksandr Shchur, Stephan G\\\"unnemann", "title": "Introduction to Tensor Decompositions and their Applications in Machine\n  Learning", "comments": "13 pages, 12 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Tensors are multidimensional arrays of numerical values and therefore\ngeneralize matrices to multiple dimensions. While tensors first emerged in the\npsychometrics community in the $20^{\\text{th}}$ century, they have since then\nspread to numerous other disciplines, including machine learning. Tensors and\ntheir decompositions are especially beneficial in unsupervised learning\nsettings, but are gaining popularity in other sub-disciplines like temporal and\nmulti-relational data analysis, too.\n  The scope of this paper is to give a broad overview of tensors, their\ndecompositions, and how they are used in machine learning. As part of this, we\nare going to introduce basic tensor concepts, discuss why tensors can be\nconsidered more rigid than matrices with respect to the uniqueness of their\ndecomposition, explain the most important factorization algorithms and their\nproperties, provide concrete examples of tensor decomposition applications in\nmachine learning, conduct a case study on tensor-based estimation of mixture\nmodels, talk about the current state of research, and provide references to\navailable software libraries.\n", "versions": [{"version": "v1", "created": "Wed, 29 Nov 2017 11:29:09 GMT"}], "update_date": "2017-11-30", "authors_parsed": [["Rabanser", "Stephan", ""], ["Shchur", "Oleksandr", ""], ["G\u00fcnnemann", "Stephan", ""]]}, {"id": "1711.10789", "submitter": "Thomas Moerland", "authors": "Thomas M. Moerland, Joost Broekens and Catholijn M. Jonker", "title": "Efficient exploration with Double Uncertain Value Networks", "comments": "Deep Reinforcement Learning Symposium @ Conference on Neural\n  Information Processing Systems (NIPS) 2017", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper studies directed exploration for reinforcement learning agents by\ntracking uncertainty about the value of each available action. We identify two\nsources of uncertainty that are relevant for exploration. The first originates\nfrom limited data (parametric uncertainty), while the second originates from\nthe distribution of the returns (return uncertainty). We identify methods to\nlearn these distributions with deep neural networks, where we estimate\nparametric uncertainty with Bayesian drop-out, while return uncertainty is\npropagated through the Bellman equation as a Gaussian distribution. Then, we\nidentify that both can be jointly estimated in one network, which we call the\nDouble Uncertain Value Network. The policy is directly derived from the learned\ndistributions based on Thompson sampling. Experimental results show that both\ntypes of uncertainty may vastly improve learning in domains with a strong\nexploration challenge.\n", "versions": [{"version": "v1", "created": "Wed, 29 Nov 2017 11:41:41 GMT"}], "update_date": "2017-11-30", "authors_parsed": [["Moerland", "Thomas M.", ""], ["Broekens", "Joost", ""], ["Jonker", "Catholijn M.", ""]]}, {"id": "1711.10856", "submitter": "Rinu Boney", "authors": "Rinu Boney and Alexander Ilin", "title": "Semi-Supervised and Active Few-Shot Learning with Prototypical Networks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider the problem of semi-supervised few-shot classification where a\nclassifier needs to adapt to new tasks using a few labeled examples and\n(potentially many) unlabeled examples. We propose a clustering approach to the\nproblem. The features extracted with Prototypical Networks are clustered using\n$K$-means with the few labeled examples guiding the clustering process. We note\nthat in many real-world applications the adaptation performance can be\nsignificantly improved by requesting the few labels through user feedback. We\ndemonstrate good performance of the active adaptation strategy using image\ndata.\n", "versions": [{"version": "v1", "created": "Wed, 29 Nov 2017 14:02:42 GMT"}, {"version": "v2", "created": "Wed, 25 Apr 2018 11:56:17 GMT"}], "update_date": "2018-04-26", "authors_parsed": [["Boney", "Rinu", ""], ["Ilin", "Alexander", ""]]}, {"id": "1711.10873", "submitter": "Pierre Ablin", "authors": "Pierre Ablin, Jean-Fran\\c{c}ois Cardoso and Alexandre Gramfort", "title": "Faster ICA under orthogonal constraint", "comments": "11 pages, 1 figure", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Independent Component Analysis (ICA) is a technique for unsupervised\nexploration of multi-channel data widely used in observational sciences. In its\nclassical form, ICA relies on modeling the data as a linear mixture of\nnon-Gaussian independent sources. The problem can be seen as a likelihood\nmaximization problem. We introduce Picard-O, a preconditioned L-BFGS strategy\nover the set of orthogonal matrices, which can quickly separate both super- and\nsub-Gaussian signals. It returns the same set of sources as the widely used\nFastICA algorithm. Through numerical experiments, we show that our method is\nfaster and more robust than FastICA on real data.\n", "versions": [{"version": "v1", "created": "Wed, 29 Nov 2017 14:25:25 GMT"}], "update_date": "2017-11-30", "authors_parsed": [["Ablin", "Pierre", ""], ["Cardoso", "Jean-Fran\u00e7ois", ""], ["Gramfort", "Alexandre", ""]]}, {"id": "1711.10907", "submitter": "Olexandr Isayev", "authors": "Mariya Popova, Olexandr Isayev, Alexander Tropsha", "title": "Deep Reinforcement Learning for De-Novo Drug Design", "comments": null, "journal-ref": "Science Advances, 2018, vol. 4, no. 7, eaap7885", "doi": "10.1126/sciadv.aap7885", "report-no": null, "categories": "cs.AI cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a novel computational strategy for de novo design of molecules\nwith desired properties termed ReLeaSE (Reinforcement Learning for Structural\nEvolution). Based on deep and reinforcement learning approaches, ReLeaSE\nintegrates two deep neural networks - generative and predictive - that are\ntrained separately but employed jointly to generate novel targeted chemical\nlibraries. ReLeaSE employs simple representation of molecules by their SMILES\nstrings only. Generative models are trained with stack-augmented memory network\nto produce chemically feasible SMILES strings, and predictive models are\nderived to forecast the desired properties of the de novo generated compounds.\nIn the first phase of the method, generative and predictive models are trained\nseparately with a supervised learning algorithm. In the second phase, both\nmodels are trained jointly with the reinforcement learning approach to bias the\ngeneration of new chemical structures towards those with the desired physical\nand/or biological properties. In the proof-of-concept study, we have employed\nthe ReLeaSE method to design chemical libraries with a bias toward structural\ncomplexity or biased toward compounds with either maximal, minimal, or specific\nrange of physical properties such as melting point or hydrophobicity, as well\nas to develop novel putative inhibitors of JAK2. The approach proposed herein\ncan find a general use for generating targeted chemical libraries of novel\ncompounds optimized for either a single desired property or multiple\nproperties.\n", "versions": [{"version": "v1", "created": "Wed, 29 Nov 2017 15:10:49 GMT"}, {"version": "v2", "created": "Thu, 31 May 2018 17:13:56 GMT"}], "update_date": "2018-07-30", "authors_parsed": [["Popova", "Mariya", ""], ["Isayev", "Olexandr", ""], ["Tropsha", "Alexander", ""]]}, {"id": "1711.10915", "submitter": "Marcus Klasson", "authors": "Marcus Klasson, Kun Zhang, Bo C. Bertilson, Cheng Zhang, Hedvig\n  Kjellstr\\\"om", "title": "Causality Refined Diagnostic Prediction", "comments": "NIPS 2017 Workshop on Machine Learning for Health (ML4H)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Applying machine learning in the health care domain has shown promising\nresults in recent years. Interpretable outputs from learning algorithms are\ndesirable for decision making by health care personnel. In this work, we\nexplore the possibility of utilizing causal relationships to refine diagnostic\nprediction. We focus on the task of diagnostic prediction using discomfort\ndrawings, and explore two ways to employ causal identification to improve the\ndiagnostic results. Firstly, we use causal identification to infer the causal\nrelationships among diagnostic labels which, by itself, provides interpretable\nresults to aid the decision making and training of health care personnel.\nSecondly, we suggest a post-processing approach where the inferred causal\nrelationships are used to refine the prediction accuracy of a multi-view\nprobabilistic model. Experimental results show firstly that causal\nidentification is capable of detecting the causal relationships among\ndiagnostic labels correctly, and secondly that there is potential for improving\npain diagnostics prediction accuracy using the causal relationships.\n", "versions": [{"version": "v1", "created": "Wed, 29 Nov 2017 15:28:20 GMT"}], "update_date": "2017-11-30", "authors_parsed": [["Klasson", "Marcus", ""], ["Zhang", "Kun", ""], ["Bertilson", "Bo C.", ""], ["Zhang", "Cheng", ""], ["Kjellstr\u00f6m", "Hedvig", ""]]}, {"id": "1711.10925", "submitter": "Dmitry Ulyanov", "authors": "Dmitry Ulyanov, Andrea Vedaldi, Victor Lempitsky", "title": "Deep Image Prior", "comments": null, "journal-ref": null, "doi": "10.1007/s11263-020-01303-4", "report-no": null, "categories": "cs.CV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Deep convolutional networks have become a popular tool for image generation\nand restoration. Generally, their excellent performance is imputed to their\nability to learn realistic image priors from a large number of example images.\nIn this paper, we show that, on the contrary, the structure of a generator\nnetwork is sufficient to capture a great deal of low-level image statistics\nprior to any learning. In order to do so, we show that a randomly-initialized\nneural network can be used as a handcrafted prior with excellent results in\nstandard inverse problems such as denoising, super-resolution, and inpainting.\nFurthermore, the same prior can be used to invert deep neural representations\nto diagnose them, and to restore images based on flash-no flash input pairs.\n  Apart from its diverse applications, our approach highlights the inductive\nbias captured by standard generator network architectures. It also bridges the\ngap between two very popular families of image restoration methods:\nlearning-based methods using deep convolutional networks and learning-free\nmethods based on handcrafted image priors such as self-similarity. Code and\nsupplementary material are available at\nhttps://dmitryulyanov.github.io/deep_image_prior .\n", "versions": [{"version": "v1", "created": "Wed, 29 Nov 2017 15:50:05 GMT"}, {"version": "v2", "created": "Thu, 30 Nov 2017 10:14:39 GMT"}, {"version": "v3", "created": "Thu, 5 Apr 2018 14:37:57 GMT"}, {"version": "v4", "created": "Sun, 17 May 2020 10:57:04 GMT"}], "update_date": "2020-05-19", "authors_parsed": [["Ulyanov", "Dmitry", ""], ["Vedaldi", "Andrea", ""], ["Lempitsky", "Victor", ""]]}, {"id": "1711.10927", "submitter": "Changyou Chen", "authors": "Changyou Chen and Ruiyi Zhang", "title": "Particle Optimization in Stochastic Gradient MCMC", "comments": "Technical report on performing SG-MCMC with particle optimization", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Stochastic gradient Markov chain Monte Carlo (SG-MCMC) has been increasingly\npopular in Bayesian learning due to its ability to deal with large data. A\nstandard SG-MCMC algorithm simulates samples from a discretized-time Markov\nchain to approximate a target distribution. However, the samples are typically\nhighly correlated due to the sequential generation process, an undesired\nproperty in SG-MCMC. In contrary, Stein variational gradient descent (SVGD)\ndirectly optimizes a set of particles, and it is able to approximate a target\ndistribution with much fewer samples. In this paper, we propose a novel method\nto directly optimize particles (or samples) in SG-MCMC from scratch.\nSpecifically, we propose efficient methods to solve the corresponding\nFokker-Planck equation on the space of probability distributions, whose\nsolution (i.e., a distribution) is approximated by particles. Through our\nframework, we are able to show connections of SG-MCMC to SVGD, as well as the\nseemly unrelated generative-adversarial-net framework. Under certain\nrelaxations, particle optimization in SG-MCMC can be interpreted as an\nextension of standard SVGD with momentum.\n", "versions": [{"version": "v1", "created": "Wed, 29 Nov 2017 15:54:29 GMT"}], "update_date": "2017-11-30", "authors_parsed": [["Chen", "Changyou", ""], ["Zhang", "Ruiyi", ""]]}, {"id": "1711.10934", "submitter": "Soroush Saryazdi", "authors": "Soroush Saryazdi, Bahareh Nikpour and Hossein Nezamabadi-pour", "title": "NPC: Neighbors Progressive Competition Algorithm for Classification of\n  Imbalanced Data Sets", "comments": "6 Pages. Accepted Signal Processing and Intelligent Systems (ICSPIS),\n  International Conference of. IEEE, 2017", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Learning from many real-world datasets is limited by a problem called the\nclass imbalance problem. A dataset is imbalanced when one class (the majority\nclass) has significantly more samples than the other class (the minority\nclass). Such datasets cause typical machine learning algorithms to perform\npoorly on the classification task. To overcome this issue, this paper proposes\na new approach Neighbors Progressive Competition (NPC) for classification of\nimbalanced datasets. Whilst the proposed algorithm is inspired by weighted\nk-Nearest Neighbor (k-NN) algorithms, it has major differences from them.\nUnlike k- NN, NPC does not limit its decision criteria to a preset number of\nnearest neighbors. In contrast, NPC considers progressively more neighbors of\nthe query sample in its decision making until the sum of grades for one class\nis much higher than the other classes. Furthermore, NPC uses a novel method for\ngrading the training samples to compensate for the imbalance issue. The grades\nare calculated using both local and global information. In brief, the\ncontribution of this paper is an entirely new classifier for handling the\nimbalance issue effectively without any manually-set parameters or any need for\nexpert knowledge. Experimental results compare the proposed approach with five\nrepresentative algorithms applied to fifteen imbalanced datasets and illustrate\nthis algorithms effectiveness.\n", "versions": [{"version": "v1", "created": "Wed, 29 Nov 2017 16:15:59 GMT"}], "update_date": "2017-11-30", "authors_parsed": [["Saryazdi", "Soroush", ""], ["Nikpour", "Bahareh", ""], ["Nezamabadi-pour", "Hossein", ""]]}, {"id": "1711.10937", "submitter": "Maxime Taillardat", "authors": "Maxime Taillardat (1,2,3), Anne-Laure Foug\\`eres (3), Philippe Naveau\n  (2), Olivier Mestre (1) ((1) CNRM, (2) LSCE, (3) ICJ)", "title": "Forest-based methods and ensemble model output statistics for rainfall\n  ensemble forecasting", "comments": null, "journal-ref": null, "doi": "10.1175/WAF-D-18-0149.1", "report-no": null, "categories": "stat.ML math.ST stat.AP stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Rainfall ensemble forecasts have to be skillful for both low precipitation\nand extreme events. We present statistical post-processing methods based on\nQuantile Regression Forests (QRF) and Gradient Forests (GF) with a parametric\nextension for heavy-tailed distributions. Our goal is to improve ensemble\nquality for all types of precipitation events, heavy-tailed included, subject\nto a good overall performance. Our hybrid proposed methods are applied to daily\n51-h forecasts of 6-h accumulated precipitation from 2012 to 2015 over France\nusing the M{\\'e}t{\\'e}o-France ensemble prediction system called PEARP. They\nprovide calibrated pre-dictive distributions and compete favourably with\nstate-of-the-art methods like Analogs method or Ensemble Model Output\nStatistics. In particular, hybrid forest-based procedures appear to bring an\nadded value to the forecast of heavy rainfall.\n", "versions": [{"version": "v1", "created": "Wed, 29 Nov 2017 16:17:17 GMT"}], "update_date": "2019-06-07", "authors_parsed": [["Taillardat", "Maxime", "", "CNRM", "LSCE", "ICJ"], ["Foug\u00e8res", "Anne-Laure", "", "ICJ"], ["Naveau", "Philippe", "", "LSCE"], ["Mestre", "Olivier", "", "CNRM"]]}, {"id": "1711.10938", "submitter": "Fulton Wang", "authors": "Fulton Wang, Cynthia Rudin", "title": "Extreme Dimension Reduction for Handling Covariate Shift", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In the covariate shift learning scenario, the training and test covariate\ndistributions differ, so that a predictor's average loss over the training and\ntest distributions also differ. In this work, we explore the potential of\nextreme dimension reduction, i.e. to very low dimensions, in improving the\nperformance of importance weighting methods for handling covariate shift, which\nfail in high dimensions due to potentially high train/test covariate divergence\nand the inability to accurately estimate the requisite density ratios. We first\nformulate and solve a problem optimizing over linear subspaces a combination of\ntheir predictive utility and train/test divergence within. Applying it to\nsimulated and real data, we show extreme dimension reduction helps sometimes\nbut not always, due to a bias introduced by dimension reduction.\n", "versions": [{"version": "v1", "created": "Wed, 29 Nov 2017 16:20:06 GMT"}, {"version": "v2", "created": "Mon, 12 Mar 2018 15:44:49 GMT"}], "update_date": "2018-03-13", "authors_parsed": [["Wang", "Fulton", ""], ["Rudin", "Cynthia", ""]]}, {"id": "1711.11023", "submitter": "Pawe{\\l} Budzianowski", "authors": "I\\~nigo Casanueva, Pawe{\\l} Budzianowski, Pei-Hao Su, Nikola\n  Mrk\\v{s}i\\'c, Tsung-Hsien Wen, Stefan Ultes, Lina Rojas-Barahona, Steve\n  Young, Milica Ga\\v{s}i\\'c", "title": "A Benchmarking Environment for Reinforcement Learning Based Task\n  Oriented Dialogue Management", "comments": "Accepted at the Deep Reinforcement Learning Symposium, 31st\n  Conference on Neural Information Processing Systems (NIPS 2017) Paper updated\n  with minor changes", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.CL cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Dialogue assistants are rapidly becoming an indispensable daily aid. To avoid\nthe significant effort needed to hand-craft the required dialogue flow, the\nDialogue Management (DM) module can be cast as a continuous Markov Decision\nProcess (MDP) and trained through Reinforcement Learning (RL). Several RL\nmodels have been investigated over recent years. However, the lack of a common\nbenchmarking framework makes it difficult to perform a fair comparison between\ndifferent models and their capability to generalise to different environments.\nTherefore, this paper proposes a set of challenging simulated environments for\ndialogue model development and evaluation. To provide some baselines, we\ninvestigate a number of representative parametric algorithms, namely deep\nreinforcement learning algorithms - DQN, A2C and Natural Actor-Critic and\ncompare them to a non-parametric model, GP-SARSA. Both the environments and\npolicy models are implemented using the publicly available PyDial toolkit and\nreleased on-line, in order to establish a testbed framework for further\nexperiments and to facilitate experimental reproducibility.\n", "versions": [{"version": "v1", "created": "Wed, 29 Nov 2017 18:51:14 GMT"}, {"version": "v2", "created": "Fri, 6 Apr 2018 10:50:44 GMT"}], "update_date": "2018-04-09", "authors_parsed": [["Casanueva", "I\u00f1igo", ""], ["Budzianowski", "Pawe\u0142", ""], ["Su", "Pei-Hao", ""], ["Mrk\u0161i\u0107", "Nikola", ""], ["Wen", "Tsung-Hsien", ""], ["Ultes", "Stefan", ""], ["Rojas-Barahona", "Lina", ""], ["Young", "Steve", ""], ["Ga\u0161i\u0107", "Milica", ""]]}, {"id": "1711.11034", "submitter": "Lingfei Wang", "authors": "Lingfei Wang and Tom Michoel", "title": "Wisdom of the crowd from unsupervised dimension reduction", "comments": "12 pages, 4 figures. Supplementary in sup folder of source files. 5\n  sup figures, 2 sup tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML q-bio.QM stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Wisdom of the crowd, the collective intelligence derived from responses of\nmultiple human or machine individuals to the same questions, can be more\naccurate than each individual, and improve social decision-making and\nprediction accuracy. This can also integrate multiple programs or datasets,\neach as an individual, for the same predictive questions. Crowd wisdom\nestimates each individual's independent error level arising from their limited\nknowledge, and finds the crowd consensus that minimizes the overall error.\nHowever, previous studies have merely built isolated, problem-specific models\nwith limited generalizability, and mainly for binary (yes/no) responses. Here\nwe show with simulation and real-world data that the crowd wisdom problem is\nanalogous to one-dimensional unsupervised dimension reduction in machine\nlearning. This provides a natural class of crowd wisdom solutions, such as\nprincipal component analysis and Isomap, which can handle binary and also\ncontinuous responses, like confidence levels, and consequently can be more\naccurate than existing solutions. They can even outperform\nsupervised-learning-based collective intelligence that is calibrated on\nhistorical performance of individuals, e.g. penalized linear regression and\nrandom forest. This study unifies crowd wisdom and unsupervised dimension\nreduction, and thereupon introduces a broad range of highly-performing and\nwidely-applicable crowd wisdom methods. As the costs for data acquisition and\nprocessing rapidly decrease, this study will promote and guide crowd wisdom\napplications in the social and natural sciences, including data fusion,\nmeta-analysis, crowd-sourcing, and committee decision making.\n", "versions": [{"version": "v1", "created": "Tue, 28 Nov 2017 21:40:49 GMT"}], "update_date": "2017-12-01", "authors_parsed": [["Wang", "Lingfei", ""], ["Michoel", "Tom", ""]]}, {"id": "1711.11053", "submitter": "Ruofeng Wen", "authors": "Ruofeng Wen, Kari Torkkola, Balakrishnan Narayanaswamy, Dhruv Madeka", "title": "A Multi-Horizon Quantile Recurrent Forecaster", "comments": "Published @ 31st Conference on Neural Information Processing Systems\n  (NIPS 2017), Time Series Workshop. Long Beach, CA, USA", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a framework for general probabilistic multi-step time series\nregression. Specifically, we exploit the expressiveness and temporal nature of\nSequence-to-Sequence Neural Networks (e.g. recurrent and convolutional\nstructures), the nonparametric nature of Quantile Regression and the efficiency\nof Direct Multi-Horizon Forecasting. A new training scheme,\n*forking-sequences*, is designed for sequential nets to boost stability and\nperformance. We show that the approach accommodates both temporal and static\ncovariates, learning across multiple related series, shifting seasonality,\nfuture planned event spikes and cold-starts in real life large-scale\nforecasting. The performance of the framework is demonstrated in an application\nto predict the future demand of items sold on Amazon.com, and in a public\nprobabilistic forecasting competition to predict electricity price and load.\n", "versions": [{"version": "v1", "created": "Wed, 29 Nov 2017 19:01:32 GMT"}, {"version": "v2", "created": "Thu, 28 Jun 2018 17:54:39 GMT"}], "update_date": "2018-06-29", "authors_parsed": [["Wen", "Ruofeng", ""], ["Torkkola", "Kari", ""], ["Narayanaswamy", "Balakrishnan", ""], ["Madeka", "Dhruv", ""]]}, {"id": "1711.11057", "submitter": "Yen-Chi Chen", "authors": "Yen-Chi Chen, Y. Samuel Wang, Elena A. Erosheva", "title": "On the use of bootstrap with variational inference: Theory,\n  interpretation, and a two-sample test example", "comments": "Accepted to the Annals of Applied Statistics; 34 pages, 8 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.AP stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Variational inference is a general approach for approximating complex density\nfunctions, such as those arising in latent variable models, popular in machine\nlearning. It has been applied to approximate the maximum likelihood estimator\nand to carry out Bayesian inference, however, quantification of uncertainty\nwith variational inference remains challenging from both theoretical and\npractical perspectives. This paper is concerned with developing uncertainty\nmeasures for variational inference by using bootstrap procedures. We first\ndevelop two general bootstrap approaches for assessing the uncertainty of a\nvariational estimate and the study the underlying bootstrap theory in both\nfixed- and increasing-dimension settings. We then use the bootstrap approach\nand our theoretical results in the context of mixed membership modeling with\nmultivariate binary data on functional disability from the National Long Term\nCare Survey. We carry out a two-sample approach to test for changes in the\nrepeated measures of functional disability for the subset of individuals\npresent in 1989 and 1994 waves.\n", "versions": [{"version": "v1", "created": "Wed, 29 Nov 2017 19:08:07 GMT"}, {"version": "v2", "created": "Wed, 18 Apr 2018 03:35:00 GMT"}], "update_date": "2018-04-19", "authors_parsed": [["Chen", "Yen-Chi", ""], ["Wang", "Y. Samuel", ""], ["Erosheva", "Elena A.", ""]]}, {"id": "1711.11059", "submitter": "Sebastian Urban", "authors": "Sebastian Urban, Marcus Basalla, Patrick van der Smagt", "title": "Gaussian Process Neurons Learn Stochastic Activation Functions", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose stochastic, non-parametric activation functions that are fully\nlearnable and individual to each neuron. Complexity and the risk of overfitting\nare controlled by placing a Gaussian process prior over these functions. The\nresult is the Gaussian process neuron, a probabilistic unit that can be used as\nthe basic building block for probabilistic graphical models that resemble the\nstructure of neural networks. The proposed model can intrinsically handle\nuncertainties in its inputs and self-estimate the confidence of its\npredictions. Using variational Bayesian inference and the central limit\ntheorem, a fully deterministic loss function is derived, allowing it to be\ntrained as efficiently as a conventional neural network using mini-batch\ngradient descent. The posterior distribution of activation functions is\ninferred from the training data alongside the weights of the network.\n  The proposed model favorably compares to deep Gaussian processes, both in\nmodel complexity and efficiency of inference. It can be directly applied to\nrecurrent or convolutional network structures, allowing its use in audio and\nimage processing tasks.\n  As an preliminary empirical evaluation we present experiments on regression\nand classification tasks, in which our model achieves performance comparable to\nor better than a Dropout regularized neural network with a fixed activation\nfunction. Experiments are ongoing and results will be added as they become\navailable.\n", "versions": [{"version": "v1", "created": "Wed, 29 Nov 2017 19:09:14 GMT"}], "update_date": "2017-12-01", "authors_parsed": [["Urban", "Sebastian", ""], ["Basalla", "Marcus", ""], ["van der Smagt", "Patrick", ""]]}, {"id": "1711.11139", "submitter": "Vinay Jethava", "authors": "Vinay Jethava and Devdatt Dubhashi", "title": "Easy High-Dimensional Likelihood-Free Inference", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We introduce a framework using Generative Adversarial Networks (GANs) for\nlikelihood--free inference (LFI) and Approximate Bayesian Computation (ABC)\nwhere we replace the black-box simulator model with an approximator network and\ngenerate a rich set of summary features in a data driven fashion. On benchmark\ndata sets, our approach improves on others with respect to scalability, ability\nto handle high dimensional data and complex probability distributions.\n", "versions": [{"version": "v1", "created": "Wed, 29 Nov 2017 22:43:20 GMT"}, {"version": "v2", "created": "Thu, 23 Aug 2018 14:27:53 GMT"}], "update_date": "2018-08-24", "authors_parsed": [["Jethava", "Vinay", ""], ["Dubhashi", "Devdatt", ""]]}, {"id": "1711.11157", "submitter": "Yitao Liang", "authors": "Jingyi Xu, Zilu Zhang, Tal Friedman, Yitao Liang, Guy Van den Broeck", "title": "A Semantic Loss Function for Deep Learning with Symbolic Knowledge", "comments": "This version appears in the Proceedings of the 35th International\n  Conference on Machine Learning (ICML 2018)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.LG cs.LO stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper develops a novel methodology for using symbolic knowledge in deep\nlearning. From first principles, we derive a semantic loss function that\nbridges between neural output vectors and logical constraints. This loss\nfunction captures how close the neural network is to satisfying the constraints\non its output. An experimental evaluation shows that it effectively guides the\nlearner to achieve (near-)state-of-the-art results on semi-supervised\nmulti-class classification. Moreover, it significantly increases the ability of\nthe neural network to predict structured objects, such as rankings and paths.\nThese discrete concepts are tremendously difficult to learn, and benefit from a\ntight integration of deep learning and symbolic reasoning methods.\n", "versions": [{"version": "v1", "created": "Wed, 29 Nov 2017 23:49:55 GMT"}, {"version": "v2", "created": "Fri, 8 Jun 2018 00:05:58 GMT"}], "update_date": "2018-06-11", "authors_parsed": [["Xu", "Jingyi", ""], ["Zhang", "Zilu", ""], ["Friedman", "Tal", ""], ["Liang", "Yitao", ""], ["Broeck", "Guy Van den", ""]]}, {"id": "1711.11179", "submitter": "Manzil Zaheer", "authors": "Xun Zheng, Manzil Zaheer, Amr Ahmed, Yuan Wang, Eric P Xing, Alexander\n  J Smola", "title": "State Space LSTM Models with Particle MCMC Inference", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Long Short-Term Memory (LSTM) is one of the most powerful sequence models.\nDespite the strong performance, however, it lacks the nice interpretability as\nin state space models. In this paper, we present a way to combine the best of\nboth worlds by introducing State Space LSTM (SSL) models that generalizes the\nearlier work \\cite{zaheer2017latent} of combining topic models with LSTM.\nHowever, unlike \\cite{zaheer2017latent}, we do not make any factorization\nassumptions in our inference algorithm. We present an efficient sampler based\non sequential Monte Carlo (SMC) method that draws from the joint posterior\ndirectly. Experimental results confirms the superiority and stability of this\nSMC inference algorithm on a variety of domains.\n", "versions": [{"version": "v1", "created": "Thu, 30 Nov 2017 01:42:26 GMT"}], "update_date": "2017-12-04", "authors_parsed": [["Zheng", "Xun", ""], ["Zaheer", "Manzil", ""], ["Ahmed", "Amr", ""], ["Wang", "Yuan", ""], ["Xing", "Eric P", ""], ["Smola", "Alexander J", ""]]}, {"id": "1711.11189", "submitter": "Chao Gao", "authors": "Chao Gao", "title": "Phase Transitions in Approximate Ranking", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.ML stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study the problem of approximate ranking from observations of pairwise\ninteractions. The goal is to estimate the underlying ranks of $n$ objects from\ndata through interactions of comparison or collaboration. Under a general\nframework of approximate ranking models, we characterize the exact optimal\nstatistical error rates of estimating the underlying ranks. We discover\nimportant phase transition boundaries of the optimal error rates. Depending on\nthe value of the signal-to-noise ratio (SNR) parameter, the optimal rate, as a\nfunction of SNR, is either trivial, polynomial, exponential or zero. The four\ncorresponding regimes thus have completely different error behaviors. To the\nbest of our knowledge, this phenomenon, especially the phase transition between\nthe polynomial and the exponential rates, has not been discovered before.\n", "versions": [{"version": "v1", "created": "Thu, 30 Nov 2017 02:03:43 GMT"}, {"version": "v2", "created": "Tue, 25 Jun 2019 14:36:19 GMT"}], "update_date": "2019-06-26", "authors_parsed": [["Gao", "Chao", ""]]}, {"id": "1711.11200", "submitter": "Hyunwoo Lee", "authors": "Hyunwoo Lee, Jooyoung Kim, Dojun Yang and Joon-Ho Kim", "title": "Embedded Real-Time Fall Detection Using Deep Learning For Elderly Care", "comments": "5 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.AI cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper proposes a real-time embedded fall detection system using a\nDVS(Dynamic Vision Sensor) that has never been used for traditional fall\ndetection, a dataset for fall detection using that, and a DVS-TN(DVS-Temporal\nNetwork). The first contribution is building a DVS Falls Dataset, which made\nour network to recognize a much greater variety of falls than the existing\ndatasets that existed before and solved privacy issues using the DVS. Secondly,\nwe introduce the DVS-TN : optimized deep learning network to detect falls using\nDVS. Finally, we implemented a fall detection system which can run on\nlow-computing H/W with real-time, and tested on DVS Falls Dataset that takes\ninto account various falls situations. Our approach achieved 95.5% on the\nF1-score and operates at 31.25 FPS on NVIDIA Jetson TX1 board.\n", "versions": [{"version": "v1", "created": "Thu, 30 Nov 2017 03:07:14 GMT"}], "update_date": "2017-12-01", "authors_parsed": [["Lee", "Hyunwoo", ""], ["Kim", "Jooyoung", ""], ["Yang", "Dojun", ""], ["Kim", "Joon-Ho", ""]]}, {"id": "1711.11216", "submitter": "Chang Liu", "authors": "Chang Liu, Jun Zhu", "title": "Riemannian Stein Variational Gradient Descent for Bayesian Inference", "comments": "12 pages, 2 figures, AAAI-18", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We develop Riemannian Stein Variational Gradient Descent (RSVGD), a Bayesian\ninference method that generalizes Stein Variational Gradient Descent (SVGD) to\nRiemann manifold. The benefits are two-folds: (i) for inference tasks in\nEuclidean spaces, RSVGD has the advantage over SVGD of utilizing information\ngeometry, and (ii) for inference tasks on Riemann manifolds, RSVGD brings the\nunique advantages of SVGD to the Riemannian world. To appropriately transfer to\nRiemann manifolds, we conceive novel and non-trivial techniques for RSVGD,\nwhich are required by the intrinsically different characteristics of general\nRiemann manifolds from Euclidean spaces. We also discover Riemannian Stein's\nIdentity and Riemannian Kernelized Stein Discrepancy. Experimental results show\nthe advantages over SVGD of exploring distribution geometry and the advantages\nof particle-efficiency, iteration-effectiveness and approximation flexibility\nover other inference methods on Riemann manifolds.\n", "versions": [{"version": "v1", "created": "Thu, 30 Nov 2017 04:11:46 GMT"}], "update_date": "2017-12-01", "authors_parsed": [["Liu", "Chang", ""], ["Zhu", "Jun", ""]]}, {"id": "1711.11225", "submitter": "Yunhao Tang", "authors": "Yunhao Tang and Alp Kucukelbir", "title": "Variational Deep Q Network", "comments": "12 pages, 5 figures, Second workshop on Bayesian Deep Learning (NIPS\n  2017)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a framework that directly tackles the probability distribution of\nthe value function parameters in Deep Q Network (DQN), with powerful\nvariational inference subroutines to approximate the posterior of the\nparameters. We will establish the equivalence between our proposed surrogate\nobjective and variational inference loss. Our new algorithm achieves efficient\nexploration and performs well on large scale chain Markov Decision Process\n(MDP).\n", "versions": [{"version": "v1", "created": "Thu, 30 Nov 2017 04:52:09 GMT"}], "update_date": "2017-12-01", "authors_parsed": [["Tang", "Yunhao", ""], ["Kucukelbir", "Alp", ""]]}, {"id": "1711.11279", "submitter": "Been Kim", "authors": "Been Kim, Martin Wattenberg, Justin Gilmer, Carrie Cai, James Wexler,\n  Fernanda Viegas, Rory Sayres", "title": "Interpretability Beyond Feature Attribution: Quantitative Testing with\n  Concept Activation Vectors (TCAV)", "comments": null, "journal-ref": "ICML 2018", "doi": null, "report-no": null, "categories": "stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The interpretation of deep learning models is a challenge due to their size,\ncomplexity, and often opaque internal state. In addition, many systems, such as\nimage classifiers, operate on low-level features rather than high-level\nconcepts. To address these challenges, we introduce Concept Activation Vectors\n(CAVs), which provide an interpretation of a neural net's internal state in\nterms of human-friendly concepts. The key idea is to view the high-dimensional\ninternal state of a neural net as an aid, not an obstacle. We show how to use\nCAVs as part of a technique, Testing with CAVs (TCAV), that uses directional\nderivatives to quantify the degree to which a user-defined concept is important\nto a classification result--for example, how sensitive a prediction of \"zebra\"\nis to the presence of stripes. Using the domain of image classification as a\ntesting ground, we describe how CAVs may be used to explore hypotheses and\ngenerate insights for a standard image classification network as well as a\nmedical application.\n", "versions": [{"version": "v1", "created": "Thu, 30 Nov 2017 09:26:12 GMT"}, {"version": "v2", "created": "Sat, 16 Dec 2017 05:49:09 GMT"}, {"version": "v3", "created": "Sun, 11 Mar 2018 06:05:19 GMT"}, {"version": "v4", "created": "Wed, 11 Apr 2018 04:47:35 GMT"}, {"version": "v5", "created": "Thu, 7 Jun 2018 04:33:27 GMT"}], "update_date": "2019-04-05", "authors_parsed": [["Kim", "Been", ""], ["Wattenberg", "Martin", ""], ["Gilmer", "Justin", ""], ["Cai", "Carrie", ""], ["Wexler", "James", ""], ["Viegas", "Fernanda", ""], ["Sayres", "Rory", ""]]}, {"id": "1711.11293", "submitter": "Takuhiro Kaneko", "authors": "Takuhiro Kaneko, Hirokazu Kameoka", "title": "Parallel-Data-Free Voice Conversion Using Cycle-Consistent Adversarial\n  Networks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.SD eess.AS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a parallel-data-free voice-conversion (VC) method that can learn a\nmapping from source to target speech without relying on parallel data. The\nproposed method is general purpose, high quality, and parallel-data free and\nworks without any extra data, modules, or alignment procedure. It also avoids\nover-smoothing, which occurs in many conventional statistical model-based VC\nmethods. Our method, called CycleGAN-VC, uses a cycle-consistent adversarial\nnetwork (CycleGAN) with gated convolutional neural networks (CNNs) and an\nidentity-mapping loss. A CycleGAN learns forward and inverse mappings\nsimultaneously using adversarial and cycle-consistency losses. This makes it\npossible to find an optimal pseudo pair from unpaired data. Furthermore, the\nadversarial loss contributes to reducing over-smoothing of the converted\nfeature sequence. We configure a CycleGAN with gated CNNs and train it with an\nidentity-mapping loss. This allows the mapping function to capture sequential\nand hierarchical structures while preserving linguistic information. We\nevaluated our method on a parallel-data-free VC task. An objective evaluation\nshowed that the converted feature sequence was near natural in terms of global\nvariance and modulation spectra. A subjective evaluation showed that the\nquality of the converted speech was comparable to that obtained with a Gaussian\nmixture model-based method under advantageous conditions with parallel and\ntwice the amount of data.\n", "versions": [{"version": "v1", "created": "Thu, 30 Nov 2017 09:57:19 GMT"}, {"version": "v2", "created": "Wed, 20 Dec 2017 14:27:33 GMT"}], "update_date": "2017-12-21", "authors_parsed": [["Kaneko", "Takuhiro", ""], ["Kameoka", "Hirokazu", ""]]}, {"id": "1711.11294", "submitter": "Wei Pan", "authors": "Xiaofan Lin, Cong Zhao, Wei Pan", "title": "Towards Accurate Binary Convolutional Neural Network", "comments": null, "journal-ref": "NIPS 2017", "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  We introduce a novel scheme to train binary convolutional neural networks\n(CNNs) -- CNNs with weights and activations constrained to {-1,+1} at run-time.\nIt has been known that using binary weights and activations drastically reduce\nmemory size and accesses, and can replace arithmetic operations with more\nefficient bitwise operations, leading to much faster test-time inference and\nlower power consumption. However, previous works on binarizing CNNs usually\nresult in severe prediction accuracy degradation. In this paper, we address\nthis issue with two major innovations: (1) approximating full-precision weights\nwith the linear combination of multiple binary weight bases; (2) employing\nmultiple binary activations to alleviate information loss. The implementation\nof the resulting binary CNN, denoted as ABC-Net, is shown to achieve much\ncloser performance to its full-precision counterpart, and even reach the\ncomparable prediction accuracy on ImageNet and forest trail datasets, given\nadequate binary weight bases and activations.\n", "versions": [{"version": "v1", "created": "Thu, 30 Nov 2017 09:58:14 GMT"}], "update_date": "2017-12-01", "authors_parsed": [["Lin", "Xiaofan", ""], ["Zhao", "Cong", ""], ["Pan", "Wei", ""]]}, {"id": "1711.11383", "submitter": "Mostafa Dehghani", "authors": "Mostafa Dehghani, Aliaksei Severyn, Sascha Rothe, Jaap Kamps", "title": "Learning to Learn from Weak Supervision by Full Supervision", "comments": "Accepted at NIPS Workshop on Meta-Learning (MetaLearn 2017), Long\n  Beach, CA, USA", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.AI cs.CL cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we propose a method for training neural networks when we have\na large set of data with weak labels and a small amount of data with true\nlabels. In our proposed model, we train two neural networks: a target network,\nthe learner and a confidence network, the meta-learner. The target network is\noptimized to perform a given task and is trained using a large set of unlabeled\ndata that are weakly annotated. We propose to control the magnitude of the\ngradient updates to the target network using the scores provided by the second\nconfidence network, which is trained on a small amount of supervised data. Thus\nwe avoid that the weight updates computed from noisy labels harm the quality of\nthe target network model.\n", "versions": [{"version": "v1", "created": "Thu, 30 Nov 2017 13:32:45 GMT"}], "update_date": "2017-12-01", "authors_parsed": [["Dehghani", "Mostafa", ""], ["Severyn", "Aliaksei", ""], ["Rothe", "Sascha", ""], ["Kamps", "Jaap", ""]]}, {"id": "1711.11386", "submitter": "Kerem Can Tezcan", "authors": "Kerem C. Tezcan, Christian F. Baumgartner, Roger Luechinger, Klaas P.\n  Pruessmann, Ender Konukoglu", "title": "MR image reconstruction using deep density priors", "comments": "Published in IEEE TMI. Main text and supplementary material, 19 pages\n  total", "journal-ref": "IEEE Transactions on Medical Imaging, December 2018", "doi": "10.1109/TMI.2018.2887072", "report-no": null, "categories": "cs.CV eess.IV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Algorithms for Magnetic Resonance (MR) image reconstruction from undersampled\nmeasurements exploit prior information to compensate for missing k-space data.\nDeep learning (DL) provides a powerful framework for extracting such\ninformation from existing image datasets, through learning, and then using it\nfor reconstruction. Leveraging this, recent methods employed DL to learn\nmappings from undersampled to fully sampled images using paired datasets,\nincluding undersampled and corresponding fully sampled images, integrating\nprior knowledge implicitly. In this article, we propose an alternative approach\nthat learns the probability distribution of fully sampled MR images using\nunsupervised DL, specifically Variational Autoencoders (VAE), and use this as\nan explicit prior term in reconstruction, completely decoupling the encoding\noperation from the prior. The resulting reconstruction algorithm enjoys a\npowerful image prior to compensate for missing k-space data without requiring\npaired datasets for training nor being prone to associated sensitivities, such\nas deviations in undersampling patterns used in training and test time or coil\nsettings. We evaluated the proposed method with T1 weighted images from a\npublicly available dataset, multi-coil complex images acquired from healthy\nvolunteers (N=8) and images with white matter lesions. The proposed algorithm,\nusing the VAE prior, produced visually high quality reconstructions and\nachieved low RMSE values, outperforming most of the alternative methods on the\nsame dataset. On multi-coil complex data, the algorithm yielded accurate\nmagnitude and phase reconstruction results. In the experiments on images with\nwhite matter lesions, the method faithfully reconstructed the lesions.\n  Keywords: Reconstruction, MRI, prior probability, machine learning, deep\nlearning, unsupervised learning, density estimation\n", "versions": [{"version": "v1", "created": "Thu, 30 Nov 2017 13:36:58 GMT"}, {"version": "v2", "created": "Fri, 1 Dec 2017 10:34:44 GMT"}, {"version": "v3", "created": "Wed, 17 Jan 2018 13:42:41 GMT"}, {"version": "v4", "created": "Wed, 19 Dec 2018 18:00:04 GMT"}], "update_date": "2018-12-20", "authors_parsed": [["Tezcan", "Kerem C.", ""], ["Baumgartner", "Christian F.", ""], ["Luechinger", "Roger", ""], ["Pruessmann", "Klaas P.", ""], ["Konukoglu", "Ender", ""]]}, {"id": "1711.11394", "submitter": "Burim Ramosaj", "authors": "Burim Ramosaj and Markus Pauly", "title": "Who wins the Miss Contest for Imputation Methods? Our Vote for Miss\n  BooPF", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Missing data is an expected issue when large amounts of data is collected,\nand several imputation techniques have been proposed to tackle this problem.\nBeneath classical approaches such as MICE, the application of Machine Learning\ntechniques is tempting. Here, the recently proposed missForest imputation\nmethod has shown high imputation accuracy under the Missing (Completely) at\nRandom scheme with various missing rates. In its core, it is based on a random\nforest for classification and regression, respectively. In this paper we study\nwhether this approach can even be enhanced by other methods such as the\nstochastic gradient tree boosting method, the C5.0 algorithm or modified random\nforest procedures. In particular, other resampling strategies within the random\nforest protocol are suggested. In an extensive simulation study, we analyze\ntheir performances for continuous, categorical as well as mixed-type data.\nTherein, MissBooPF, a combination of the stochastic gradient tree boosting\nmethod together with the parametrically bootstrapped random forest method,\nappeared to be promising. Finally, an empirical analysis focusing on credit\ninformation and Facebook data is conducted.\n", "versions": [{"version": "v1", "created": "Thu, 30 Nov 2017 13:51:11 GMT"}], "update_date": "2017-12-01", "authors_parsed": [["Ramosaj", "Burim", ""], ["Pauly", "Markus", ""]]}, {"id": "1711.11408", "submitter": "Daniel Chicharro", "authors": "Daniel Chicharro, Giuseppe Pica, Stefano Panzeri", "title": "The identity of information: how deterministic dependencies constrain\n  information synergy and redundancy", "comments": null, "journal-ref": null, "doi": "10.3390/e20030169", "report-no": null, "categories": "q-bio.NC stat.ML", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Understanding how different information sources together transmit information\nis crucial in many domains. For example, understanding the neural code requires\ncharacterizing how different neurons contribute unique, redundant, or\nsynergistic pieces of information about sensory or behavioral variables.\nWilliams and Beer (2010) proposed a partial information decomposition (PID)\nwhich separates the mutual information that a set of sources contains about a\nset of targets into nonnegative terms interpretable as these pieces.\nQuantifying redundancy requires assigning an identity to different information\npieces, to assess when information is common across sources. Harder et al.\n(2013) proposed an identity axiom stating that there cannot be redundancy\nbetween two independent sources about a copy of themselves. However,\nBertschinger et al. (2012) showed that with a deterministically related\nsources-target copy this axiom is incompatible with ensuring PID nonnegativity.\nHere we study systematically the effect of deterministic target-sources\ndependencies. We introduce two synergy stochasticity axioms that generalize the\nidentity axiom, and we derive general expressions separating stochastic and\ndeterministic PID components. Our analysis identifies how negative terms can\noriginate from deterministic dependencies and shows how different assumptions\non information identity, implicit in the stochasticity and identity axioms,\ndetermine the PID structure. The implications for studying neural coding are\ndiscussed.\n", "versions": [{"version": "v1", "created": "Mon, 13 Nov 2017 12:27:42 GMT"}], "update_date": "2018-04-04", "authors_parsed": [["Chicharro", "Daniel", ""], ["Pica", "Giuseppe", ""], ["Panzeri", "Stefano", ""]]}, {"id": "1711.11423", "submitter": "Remi Flamary", "authors": "Ibrahim El Khalil Harrane, R\\'emi Flamary, C\\'edric Richard", "title": "On reducing the communication cost of the diffusion LMS algorithm", "comments": null, "journal-ref": null, "doi": "10.1109/TSIPN.2018.2863218", "report-no": null, "categories": "stat.ML cs.LG math.OC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The rise of digital and mobile communications has recently made the world\nmore connected and networked, resulting in an unprecedented volume of data\nflowing between sources, data centers, or processes. While these data may be\nprocessed in a centralized manner, it is often more suitable to consider\ndistributed strategies such as diffusion as they are scalable and can handle\nlarge amounts of data by distributing tasks over networked agents. Although it\nis relatively simple to implement diffusion strategies over a cluster, it\nappears to be challenging to deploy them in an ad-hoc network with limited\nenergy budget for communication. In this paper, we introduce a diffusion LMS\nstrategy that significantly reduces communication costs without compromising\nthe performance. Then, we analyze the proposed algorithm in the mean and\nmean-square sense. Next, we conduct numerical experiments to confirm the\ntheoretical findings. Finally, we perform large scale simulations to test the\nalgorithm efficiency in a scenario where energy is limited.\n", "versions": [{"version": "v1", "created": "Thu, 30 Nov 2017 14:31:11 GMT"}, {"version": "v2", "created": "Mon, 23 Jul 2018 06:32:31 GMT"}], "update_date": "2018-09-21", "authors_parsed": [["Harrane", "Ibrahim El Khalil", ""], ["Flamary", "R\u00e9mi", ""], ["Richard", "C\u00e9dric", ""]]}, {"id": "1711.11443", "submitter": "Pierre Stock", "authors": "Pierre Stock and Moustapha Cisse", "title": "ConvNets and ImageNet Beyond Accuracy: Understanding Mistakes and\n  Uncovering Biases", "comments": "ECCV 2018 camera-ready", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI cs.CV cs.CY stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  ConvNets and Imagenet have driven the recent success of deep learning for\nimage classification. However, the marked slowdown in performance improvement\ncombined with the lack of robustness of neural networks to adversarial examples\nand their tendency to exhibit undesirable biases question the reliability of\nthese methods. This work investigates these questions from the perspective of\nthe end-user by using human subject studies and explanations. The contribution\nof this study is threefold. We first experimentally demonstrate that the\naccuracy and robustness of ConvNets measured on Imagenet are vastly\nunderestimated. Next, we show that explanations can mitigate the impact of\nmisclassified adversarial examples from the perspective of the end-user. We\nfinally introduce a novel tool for uncovering the undesirable biases learned by\na model. These contributions also show that explanations are a valuable tool\nboth for improving our understanding of ConvNets' predictions and for designing\nmore reliable models.\n", "versions": [{"version": "v1", "created": "Thu, 30 Nov 2017 14:50:55 GMT"}, {"version": "v2", "created": "Fri, 20 Jul 2018 16:57:30 GMT"}], "update_date": "2018-07-23", "authors_parsed": [["Stock", "Pierre", ""], ["Cisse", "Moustapha", ""]]}, {"id": "1711.11486", "submitter": "Pawe{\\l} Budzianowski", "authors": "Christopher Tegho, Pawe{\\l} Budzianowski, Milica Ga\\v{s}i\\'c", "title": "Uncertainty Estimates for Efficient Neural Network-based Dialogue Policy\n  Optimisation", "comments": "Accepted at the Bayesian Deep Learning Workshop, 31st Conference on\n  Neural Information Processing Systems (NIPS 2017)", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.CL cs.LG cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In statistical dialogue management, the dialogue manager learns a policy that\nmaps a belief state to an action for the system to perform. Efficient\nexploration is key to successful policy optimisation. Current deep\nreinforcement learning methods are very promising but rely on epsilon-greedy\nexploration, thus subjecting the user to a random choice of action during\nlearning. Alternative approaches such as Gaussian Process SARSA (GPSARSA)\nestimate uncertainties and are sample efficient, leading to better user\nexperience, but on the expense of a greater computational complexity. This\npaper examines approaches to extract uncertainty estimates from deep Q-networks\n(DQN) in the context of dialogue management. We perform an extensive benchmark\nof deep Bayesian methods to extract uncertainty estimates, namely\nBayes-By-Backprop, dropout, its concrete variation, bootstrapped ensemble and\nalpha-divergences, combining it with DQN algorithm.\n", "versions": [{"version": "v1", "created": "Thu, 30 Nov 2017 16:09:02 GMT"}], "update_date": "2017-12-04", "authors_parsed": [["Tegho", "Christopher", ""], ["Budzianowski", "Pawe\u0142", ""], ["Ga\u0161i\u0107", "Milica", ""]]}, {"id": "1711.11510", "submitter": "Francisco J. Valverde-Albacete Dr.", "authors": "Francisco J. Valverde-Albacete and Carmen Pel\\'aez-Moreno", "title": "Assessing Information Transmission in Data Transformations with the\n  Channel Multivariate Entropy Triangle", "comments": "21 pages, 7 figures and 1 table", "journal-ref": "Entropy 2018, 20(7), 498", "doi": "10.3390/e20070498", "report-no": null, "categories": "cs.IT math.IT stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Data transformation, e.g. feature transformation and selection, is an\nintegral part of any machine learning procedure. In this paper we introduce an\ninformation-theoretic model and tools to assess the quality of data\ntransformations in machine learning tasks. In an unsupervised fashion, we\nanalyze the transfer of information of the transformation of a discrete,\nmultivariate source of information X into a discrete, multivariate sink of\ninformation Y related by a distribution PXY . The first contribution is a\ndecomposition of the maximal potential entropy of (X, Y) that we call a balance\nequation, into its a) non-transferable, b) transferable but not transferred and\nc) transferred parts. Such balance equations can be represented in (de Finetti)\nentropy diagrams, our second set of contributions. The most important of these,\nthe aggregate Channel Multivariate Entropy Triangle is a visual exploratory\ntool to assess the effectiveness of multivariate data transformations in\ntransferring information from input to output variables. We also show how these\ndecomposition and balance equation also apply to the entropies of X and Y\nrespectively and generate entropy triangles for them. As an example, we present\nthe application of these tools to the assessment of information transfer\nefficiency for PCA and ICA as unsupervised feature transformation and selection\nprocedures in supervised classification tasks.\n", "versions": [{"version": "v1", "created": "Thu, 30 Nov 2017 16:59:23 GMT"}, {"version": "v2", "created": "Wed, 10 Oct 2018 08:08:55 GMT"}], "update_date": "2018-10-11", "authors_parsed": [["Valverde-Albacete", "Francisco J.", ""], ["Pel\u00e1ez-Moreno", "Carmen", ""]]}, {"id": "1711.11511", "submitter": "Rui Luo", "authors": "Rui Luo, Jianhong Wang, Yaodong Yang, Zhanxing Zhu, and Jun Wang", "title": "Thermostat-assisted continuously-tempered Hamiltonian Monte Carlo for\n  Bayesian learning", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a new sampling method, the thermostat-assisted\ncontinuously-tempered Hamiltonian Monte Carlo, for Bayesian learning on large\ndatasets and multimodal distributions. It simulates the Nos\\'e-Hoover dynamics\nof a continuously-tempered Hamiltonian system built on the distribution of\ninterest. A significant advantage of this method is that it is not only able to\nefficiently draw representative i.i.d. samples when the distribution contains\nmultiple isolated modes, but capable of adaptively neutralising the noise\narising from mini-batches and maintaining accurate sampling. While the\nproperties of this method have been studied using synthetic distributions,\nexperiments on three real datasets also demonstrated the gain of performance\nover several strong baselines with various types of neural networks plunged in.\n", "versions": [{"version": "v1", "created": "Thu, 30 Nov 2017 16:59:42 GMT"}, {"version": "v2", "created": "Fri, 1 Dec 2017 22:19:15 GMT"}, {"version": "v3", "created": "Mon, 28 May 2018 16:54:47 GMT"}, {"version": "v4", "created": "Wed, 12 Dec 2018 18:12:15 GMT"}, {"version": "v5", "created": "Mon, 28 Jan 2019 17:31:11 GMT"}], "update_date": "2019-01-29", "authors_parsed": [["Luo", "Rui", ""], ["Wang", "Jianhong", ""], ["Yang", "Yaodong", ""], ["Zhu", "Zhanxing", ""], ["Wang", "Jun", ""]]}, {"id": "1711.11527", "submitter": "Kshiteej Sheth Jitesh", "authors": "Kshiteej Sheth, Dinesh Garg and Anirban Dasgupta", "title": "Improved Linear Embeddings via Lagrange Duality", "comments": "20 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Near isometric orthogonal embeddings to lower dimensions are a fundamental\ntool in data science and machine learning. In this paper, we present the\nconstruction of such embeddings that minimizes the maximum distortion for a\ngiven set of points. We formulate the problem as a non convex constrained\noptimization problem. We first construct a primal relaxation and then use the\ntheory of Lagrange duality to create dual relaxation. We also suggest a\npolynomial time algorithm based on the theory of convex optimization to solve\nthe dual relaxation provably. We provide a theoretical upper bound on the\napproximation guarantees for our algorithm, which depends only on the spectral\nproperties of the dataset. We experimentally demonstrate the superiority of our\nalgorithm compared to baselines in terms of the scalability and the ability to\nachieve lower distortion.\n", "versions": [{"version": "v1", "created": "Thu, 30 Nov 2017 17:31:56 GMT"}, {"version": "v2", "created": "Thu, 14 Dec 2017 14:37:02 GMT"}], "update_date": "2017-12-15", "authors_parsed": [["Sheth", "Kshiteej", ""], ["Garg", "Dinesh", ""], ["Dasgupta", "Anirban", ""]]}, {"id": "1711.11542", "submitter": "Alexander Ororbia II", "authors": "Alexander G. Ororbia II, Patrick Haffner, David Reitter, and C. Lee\n  Giles", "title": "Learning to Adapt by Minimizing Discrepancy", "comments": "Note: Additional experiments in support of this paper are still\n  running (updates will be made as they are completed)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We explore whether useful temporal neural generative models can be learned\nfrom sequential data without back-propagation through time. We investigate the\nviability of a more neurocognitively-grounded approach in the context of\nunsupervised generative modeling of sequences. Specifically, we build on the\nconcept of predictive coding, which has gained influence in cognitive science,\nin a neural framework. To do so we develop a novel architecture, the Temporal\nNeural Coding Network, and its learning algorithm, Discrepancy Reduction. The\nunderlying directed generative model is fully recurrent, meaning that it\nemploys structural feedback connections and temporal feedback connections,\nyielding information propagation cycles that create local learning signals.\nThis facilitates a unified bottom-up and top-down approach for information\ntransfer inside the architecture. Our proposed algorithm shows promise on the\nbouncing balls generative modeling problem. Further experiments could be\nconducted to explore the strengths and weaknesses of our approach.\n", "versions": [{"version": "v1", "created": "Thu, 30 Nov 2017 18:03:47 GMT"}], "update_date": "2017-12-01", "authors_parsed": [["Ororbia", "Alexander G.", "II"], ["Haffner", "Patrick", ""], ["Reitter", "David", ""], ["Giles", "C. Lee", ""]]}, {"id": "1711.11561", "submitter": "Jason Jo", "authors": "Jason Jo, Yoshua Bengio", "title": "Measuring the tendency of CNNs to Learn Surface Statistical Regularities", "comments": "Submitted", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Deep CNNs are known to exhibit the following peculiarity: on the one hand\nthey generalize extremely well to a test set, while on the other hand they are\nextremely sensitive to so-called adversarial perturbations. The extreme\nsensitivity of high performance CNNs to adversarial examples casts serious\ndoubt that these networks are learning high level abstractions in the dataset.\nWe are concerned with the following question: How can a deep CNN that does not\nlearn any high level semantics of the dataset manage to generalize so well? The\ngoal of this article is to measure the tendency of CNNs to learn surface\nstatistical regularities of the dataset. To this end, we use Fourier filtering\nto construct datasets which share the exact same high level abstractions but\nexhibit qualitatively different surface statistical regularities. For the SVHN\nand CIFAR-10 datasets, we present two Fourier filtered variants: a low\nfrequency variant and a randomly filtered variant. Each of the Fourier\nfiltering schemes is tuned to preserve the recognizability of the objects. Our\nmain finding is that CNNs exhibit a tendency to latch onto the Fourier image\nstatistics of the training dataset, sometimes exhibiting up to a 28%\ngeneralization gap across the various test sets. Moreover, we observe that\nsignificantly increasing the depth of a network has a very marginal impact on\nclosing the aforementioned generalization gap. Thus we provide quantitative\nevidence supporting the hypothesis that deep CNNs tend to learn surface\nstatistical regularities in the dataset rather than higher-level abstract\nconcepts.\n", "versions": [{"version": "v1", "created": "Thu, 30 Nov 2017 18:30:49 GMT"}], "update_date": "2017-12-01", "authors_parsed": [["Jo", "Jason", ""], ["Bengio", "Yoshua", ""]]}, {"id": "1711.11581", "submitter": "David Steurer", "authors": "Pravesh K. Kothari and David Steurer", "title": "Outlier-robust moment-estimation via sum-of-squares", "comments": "Fix references for robust mean estimation without exploiting\n  higher-order moments", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We develop efficient algorithms for estimating low-degree moments of unknown\ndistributions in the presence of adversarial outliers. The guarantees of our\nalgorithms improve in many cases significantly over the best previous ones,\nobtained in recent works of Diakonikolas et al, Lai et al, and Charikar et al.\nWe also show that the guarantees of our algorithms match information-theoretic\nlower-bounds for the class of distributions we consider. These improved\nguarantees allow us to give improved algorithms for independent component\nanalysis and learning mixtures of Gaussians in the presence of outliers.\n  Our algorithms are based on a standard sum-of-squares relaxation of the\nfollowing conceptually-simple optimization problem: Among all distributions\nwhose moments are bounded in the same way as for the unknown distribution, find\nthe one that is closest in statistical distance to the empirical distribution\nof the adversarially-corrupted sample.\n", "versions": [{"version": "v1", "created": "Thu, 30 Nov 2017 18:54:33 GMT"}, {"version": "v2", "created": "Sat, 23 Dec 2017 22:24:18 GMT"}], "update_date": "2017-12-27", "authors_parsed": [["Kothari", "Pravesh K.", ""], ["Steurer", "David", ""]]}, {"id": "1711.11586", "submitter": "Richard Zhang", "authors": "Jun-Yan Zhu, Richard Zhang, Deepak Pathak, Trevor Darrell, Alexei A.\n  Efros, Oliver Wang, Eli Shechtman", "title": "Toward Multimodal Image-to-Image Translation", "comments": "NIPS 2017 Final paper. v4 updated acknowledgment. Website:\n  https://junyanz.github.io/BicycleGAN/", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.GR stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Many image-to-image translation problems are ambiguous, as a single input\nimage may correspond to multiple possible outputs. In this work, we aim to\nmodel a \\emph{distribution} of possible outputs in a conditional generative\nmodeling setting. The ambiguity of the mapping is distilled in a\nlow-dimensional latent vector, which can be randomly sampled at test time. A\ngenerator learns to map the given input, combined with this latent code, to the\noutput. We explicitly encourage the connection between output and the latent\ncode to be invertible. This helps prevent a many-to-one mapping from the latent\ncode to the output during training, also known as the problem of mode collapse,\nand produces more diverse results. We explore several variants of this approach\nby employing different training objectives, network architectures, and methods\nof injecting the latent code. Our proposed method encourages bijective\nconsistency between the latent encoding and output modes. We present a\nsystematic comparison of our method and other variants on both perceptual\nrealism and diversity.\n", "versions": [{"version": "v1", "created": "Thu, 30 Nov 2017 18:59:01 GMT"}, {"version": "v2", "created": "Thu, 1 Feb 2018 02:56:04 GMT"}, {"version": "v3", "created": "Wed, 2 May 2018 23:37:28 GMT"}, {"version": "v4", "created": "Wed, 24 Oct 2018 00:29:43 GMT"}], "update_date": "2018-10-25", "authors_parsed": [["Zhu", "Jun-Yan", ""], ["Zhang", "Richard", ""], ["Pathak", "Deepak", ""], ["Darrell", "Trevor", ""], ["Efros", "Alexei A.", ""], ["Wang", "Oliver", ""], ["Shechtman", "Eli", ""]]}]