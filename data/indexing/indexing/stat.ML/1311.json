[{"id": "1311.0035", "submitter": "Ali Mousavi", "authors": "Ali Mousavi, Arian Maleki, Richard G. Baraniuk", "title": "Parameterless Optimal Approximate Message Passing", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IT math.IT math.ST stat.ML stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Iterative thresholding algorithms are well-suited for high-dimensional\nproblems in sparse recovery and compressive sensing. The performance of this\nclass of algorithms depends heavily on the tuning of certain threshold\nparameters. In particular, both the final reconstruction error and the\nconvergence rate of the algorithm crucially rely on how the threshold parameter\nis set at each step of the algorithm. In this paper, we propose a\nparameter-free approximate message passing (AMP) algorithm that sets the\nthreshold parameter at each iteration in a fully automatic way without either\nhaving an information about the signal to be reconstructed or needing any\ntuning from the user. We show that the proposed method attains both the minimum\nreconstruction error and the highest convergence rate. Our method is based on\napplying the Stein unbiased risk estimate (SURE) along with a modified gradient\ndescent to find the optimal threshold in each iteration. Motivated by the\nconnections between AMP and LASSO, it could be employed to find the solution of\nthe LASSO for the optimal regularization parameter. To the best of our\nknowledge, this is the first work concerning parameter tuning that obtains the\nfastest convergence rate with theoretical guarantees.\n", "versions": [{"version": "v1", "created": "Thu, 31 Oct 2013 21:03:00 GMT"}], "update_date": "2013-11-04", "authors_parsed": [["Mousavi", "Ali", ""], ["Maleki", "Arian", ""], ["Baraniuk", "Richard G.", ""]]}, {"id": "1311.0053", "submitter": "Will Landecker", "authors": "Will Landecker and Rick Chartrand and Simon DeDeo", "title": "Robust Compressed Sensing and Sparse Coding with the Difference Map", "comments": "8 pages; Revised comparison to DM-ECME algorithm in Section 2.1", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV physics.data-an stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In compressed sensing, we wish to reconstruct a sparse signal $x$ from\nobserved data $y$. In sparse coding, on the other hand, we wish to find a\nrepresentation of an observed signal $y$ as a sparse linear combination, with\ncoefficients $x$, of elements from an overcomplete dictionary. While many\nalgorithms are competitive at both problems when $x$ is very sparse, it can be\nchallenging to recover $x$ when it is less sparse. We present the Difference\nMap, which excels at sparse recovery when sparseness is lower and noise is\nhigher. The Difference Map out-performs the state of the art with\nreconstruction from random measurements and natural image reconstruction via\nsparse coding.\n", "versions": [{"version": "v1", "created": "Thu, 31 Oct 2013 22:33:36 GMT"}, {"version": "v2", "created": "Thu, 21 Nov 2013 00:27:39 GMT"}], "update_date": "2013-11-25", "authors_parsed": [["Landecker", "Will", ""], ["Chartrand", "Rick", ""], ["DeDeo", "Simon", ""]]}, {"id": "1311.0072", "submitter": "Arash Ali Amini", "authors": "Arash A. Amini and XuanLong Nguyen", "title": "Bayesian inference as iterated random functions with applications to\n  sequential inference in graphical models", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML math.ST stat.ME stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a general formalism of iterated random functions with semigroup\nproperty, under which exact and approximate Bayesian posterior updates can be\nviewed as specific instances. A convergence theory for iterated random\nfunctions is presented. As an application of the general theory we analyze\nconvergence behaviors of exact and approximate message-passing algorithms that\narise in a sequential change point detection problem formulated via a latent\nvariable directed graphical model. The sequential inference algorithm and its\nsupporting theory are illustrated by simulated examples.\n", "versions": [{"version": "v1", "created": "Fri, 1 Nov 2013 02:17:06 GMT"}], "update_date": "2013-11-05", "authors_parsed": [["Amini", "Arash A.", ""], ["Nguyen", "XuanLong", ""]]}, {"id": "1311.0219", "submitter": "Fang Han", "authors": "Huitong Qiu, Fang Han, Han Liu, Brian Caffo", "title": "Joint Estimation of Multiple Graphical Models from High Dimensional Time\n  Series", "comments": "40 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this manuscript we consider the problem of jointly estimating multiple\ngraphical models in high dimensions. We assume that the data are collected from\nn subjects, each of which consists of T possibly dependent observations. The\ngraphical models of subjects vary, but are assumed to change smoothly\ncorresponding to a measure of closeness between subjects. We propose a kernel\nbased method for jointly estimating all graphical models. Theoretically, under\na double asymptotic framework, where both (T,n) and the dimension d can\nincrease, we provide the explicit rate of convergence in parameter estimation.\nIt characterizes the strength one can borrow across different individuals and\nimpact of data dependence on parameter estimation. Empirically, experiments on\nboth synthetic and real resting state functional magnetic resonance imaging\n(rs-fMRI) data illustrate the effectiveness of the proposed method.\n", "versions": [{"version": "v1", "created": "Fri, 1 Nov 2013 16:32:17 GMT"}, {"version": "v2", "created": "Wed, 8 Oct 2014 16:28:51 GMT"}], "update_date": "2014-10-09", "authors_parsed": [["Qiu", "Huitong", ""], ["Han", "Fang", ""], ["Liu", "Han", ""], ["Caffo", "Brian", ""]]}, {"id": "1311.0222", "submitter": "Julien Audiffren", "authors": "Julien Audiffren (LIF), Hachem Kadri (LIF)", "title": "Online Learning with Multiple Operator-valued Kernels", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider the problem of learning a vector-valued function f in an online\nlearning setting. The function f is assumed to lie in a reproducing Hilbert\nspace of operator-valued kernels. We describe two online algorithms for\nlearning f while taking into account the output structure. A first contribution\nis an algorithm, ONORMA, that extends the standard kernel-based online learning\nalgorithm NORMA from scalar-valued to operator-valued setting. We report a\ncumulative error bound that holds both for classification and regression. We\nthen define a second algorithm, MONORMA, which addresses the limitation of\npre-defining the output structure in ONORMA by learning sequentially a linear\ncombination of operator-valued kernels. Our experiments show that the proposed\nalgorithms achieve good performance results with low computational cost.\n", "versions": [{"version": "v1", "created": "Fri, 1 Nov 2013 16:51:02 GMT"}, {"version": "v2", "created": "Tue, 5 Nov 2013 17:53:10 GMT"}], "update_date": "2013-11-06", "authors_parsed": [["Audiffren", "Julien", "", "LIF"], ["Kadri", "Hachem", "", "LIF"]]}, {"id": "1311.0317", "submitter": "Paul McNicholas", "authors": "Brian C. Franczak, Paul D. McNicholas, Ryan P. Browne and Paula M.\n  Murray", "title": "Parsimonious Shifted Asymmetric Laplace Mixtures", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.CO stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A family of parsimonious shifted asymmetric Laplace mixture models is\nintroduced. We extend the mixture of factor analyzers model to the shifted\nasymmetric Laplace distribution. Imposing constraints on the constitute parts\nof the resulting decomposed component scale matrices leads to a family of\nparsimonious models. An explicit two-stage parameter estimation procedure is\ndescribed, and the Bayesian information criterion and the integrated completed\nlikelihood are compared for model selection. This novel family of models is\napplied to real data, where it is compared to its Gaussian analogue within\nclustering and classification paradigms.\n", "versions": [{"version": "v1", "created": "Fri, 1 Nov 2013 22:14:06 GMT"}], "update_date": "2013-11-05", "authors_parsed": [["Franczak", "Brian C.", ""], ["McNicholas", "Paul D.", ""], ["Browne", "Ryan P.", ""], ["Murray", "Paula M.", ""]]}, {"id": "1311.0360", "submitter": "Antoni Chan", "authors": "Antoni B. Chan", "title": "Multivariate Generalized Gaussian Process Models", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a family of multivariate Gaussian process models for correlated\noutputs, based on assuming that the likelihood function takes the generic form\nof the multivariate exponential family distribution (EFD). We denote this model\nas a multivariate generalized Gaussian process model, and derive Taylor and\nLaplace algorithms for approximate inference on the generic model. By\ninstantiating the EFD with specific parameter functions, we obtain two novel GP\nmodels (and corresponding inference algorithms) for correlated outputs: 1) a\nVon-Mises GP for angle regression; and 2) a Dirichlet GP for regressing on the\nmultinomial simplex.\n", "versions": [{"version": "v1", "created": "Sat, 2 Nov 2013 09:09:35 GMT"}], "update_date": "2013-11-05", "authors_parsed": [["Chan", "Antoni B.", ""]]}, {"id": "1311.0396", "submitter": "Biao Luo", "authors": "Biao Luo, Huai-Ning Wu, Tingwen Huang, Derong Liu", "title": "Data-based approximate policy iteration for nonlinear continuous-time\n  optimal control design", "comments": "22 pages, 21 figures, submitted for Peer Review", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.SY math.OC stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper addresses the model-free nonlinear optimal problem with\ngeneralized cost functional, and a data-based reinforcement learning technique\nis developed. It is known that the nonlinear optimal control problem relies on\nthe solution of the Hamilton-Jacobi-Bellman (HJB) equation, which is a\nnonlinear partial differential equation that is generally impossible to be\nsolved analytically. Even worse, most of practical systems are too complicated\nto establish their accurate mathematical model. To overcome these difficulties,\nwe propose a data-based approximate policy iteration (API) method by using real\nsystem data rather than system model. Firstly, a model-free policy iteration\nalgorithm is derived for constrained optimal control problem and its\nconvergence is proved, which can learn the solution of HJB equation and optimal\ncontrol policy without requiring any knowledge of system mathematical model.\nThe implementation of the algorithm is based on the thought of actor-critic\nstructure, where actor and critic neural networks (NNs) are employed to\napproximate the control policy and cost function, respectively. To update the\nweights of actor and critic NNs, a least-square approach is developed based on\nthe method of weighted residuals. The whole data-based API method includes two\nparts, where the first part is implemented online to collect real system\ninformation, and the second part is conducting offline policy iteration to\nlearn the solution of HJB equation and the control policy. Then, the data-based\nAPI algorithm is simplified for solving unconstrained optimal control problem\nof nonlinear and linear systems. Finally, we test the efficiency of the\ndata-based API control design method on a simple nonlinear system, and further\napply it to a rotational/translational actuator system. The simulation results\ndemonstrate the effectiveness of the proposed method.\n", "versions": [{"version": "v1", "created": "Sat, 2 Nov 2013 17:37:47 GMT"}], "update_date": "2013-11-20", "authors_parsed": [["Luo", "Biao", ""], ["Wu", "Huai-Ning", ""], ["Huang", "Tingwen", ""], ["Liu", "Derong", ""]]}, {"id": "1311.0466", "submitter": "Aditya Gopalan", "authors": "Aditya Gopalan, Shie Mannor and Yishay Mansour", "title": "Thompson Sampling for Complex Bandit Problems", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider stochastic multi-armed bandit problems with complex actions over\na set of basic arms, where the decision maker plays a complex action rather\nthan a basic arm in each round. The reward of the complex action is some\nfunction of the basic arms' rewards, and the feedback observed may not\nnecessarily be the reward per-arm. For instance, when the complex actions are\nsubsets of the arms, we may only observe the maximum reward over the chosen\nsubset. Thus, feedback across complex actions may be coupled due to the nature\nof the reward function. We prove a frequentist regret bound for Thompson\nsampling in a very general setting involving parameter, action and observation\nspaces and a likelihood function over them. The bound holds for\ndiscretely-supported priors over the parameter space and without additional\nstructural properties such as closed-form posteriors, conjugate prior structure\nor independence across arms. The regret bound scales logarithmically with time\nbut, more importantly, with an improved constant that non-trivially captures\nthe coupling across complex actions due to the structure of the rewards. As\napplications, we derive improved regret bounds for classes of complex bandit\nproblems involving selecting subsets of arms, including the first nontrivial\nregret bounds for nonlinear MAX reward feedback from subsets.\n", "versions": [{"version": "v1", "created": "Sun, 3 Nov 2013 13:51:55 GMT"}], "update_date": "2013-11-05", "authors_parsed": [["Gopalan", "Aditya", ""], ["Mannor", "Shie", ""], ["Mansour", "Yishay", ""]]}, {"id": "1311.0468", "submitter": "Aditya Gopalan", "authors": "Aditya Gopalan", "title": "Thompson Sampling for Online Learning with Linear Experts", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this note, we present a version of the Thompson sampling algorithm for the\nproblem of online linear generalization with full information (i.e., the\nexperts setting), studied by Kalai and Vempala, 2005. The algorithm uses a\nGaussian prior and time-varying Gaussian likelihoods, and we show that it\nessentially reduces to Kalai and Vempala's Follow-the-Perturbed-Leader\nstrategy, with exponentially distributed noise replaced by Gaussian noise. This\nimplies sqrt(T) regret bounds for Thompson sampling (with time-varying\nlikelihood) for online learning with full information.\n", "versions": [{"version": "v1", "created": "Sun, 3 Nov 2013 14:18:56 GMT"}], "update_date": "2013-11-05", "authors_parsed": [["Gopalan", "Aditya", ""]]}, {"id": "1311.0622", "submitter": "Taiji Suzuki", "authors": "Taiji Suzuki", "title": "Stochastic Dual Coordinate Ascent with Alternating Direction Multiplier\n  Method", "comments": "26pages, 12 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a new stochastic dual coordinate ascent technique that can be\napplied to a wide range of regularized learning problems. Our method is based\non Alternating Direction Multiplier Method (ADMM) to deal with complex\nregularization functions such as structured regularizations. Although the\noriginal ADMM is a batch method, the proposed method offers a stochastic update\nrule where each iteration requires only one or few sample observations.\nMoreover, our method can naturally afford mini-batch update and it gives speed\nup of convergence. We show that, under mild assumptions, our method converges\nexponentially. The numerical experiments show that our method actually performs\nefficiently.\n", "versions": [{"version": "v1", "created": "Mon, 4 Nov 2013 09:31:51 GMT"}], "update_date": "2013-11-05", "authors_parsed": [["Suzuki", "Taiji", ""]]}, {"id": "1311.0686", "submitter": "Johan Dahlin Mr.", "authors": "Johan Dahlin and Fredrik Lindsten and Thomas B. Sch\\\"on", "title": "Particle Metropolis-Hastings using gradient and Hessian information", "comments": "27 pages, 5 figures, 2 tables. The final publication is available at\n  Springer via: http://dx.doi.org/10.1007/s11222-014-9510-0", "journal-ref": "Statistics and Computing, Volume 25, Issue 1, pp 81-92, 2015", "doi": "10.1007/s11222-014-9510-0", "report-no": null, "categories": "stat.CO stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Particle Metropolis-Hastings (PMH) allows for Bayesian parameter inference in\nnonlinear state space models by combining Markov chain Monte Carlo (MCMC) and\nparticle filtering. The latter is used to estimate the intractable likelihood.\nIn its original formulation, PMH makes use of a marginal MCMC proposal for the\nparameters, typically a Gaussian random walk. However, this can lead to a poor\nexploration of the parameter space and an inefficient use of the generated\nparticles.\n  We propose a number of alternative versions of PMH that incorporate gradient\nand Hessian information about the posterior into the proposal. This information\nis more or less obtained as a byproduct of the likelihood estimation. Indeed,\nwe show how to estimate the required information using a fixed-lag particle\nsmoother, with a computational cost growing linearly in the number of\nparticles. We conclude that the proposed methods can: (i) decrease the length\nof the burn-in phase, (ii) increase the mixing of the Markov chain at the\nstationary phase, and (iii) make the proposal distribution scale invariant\nwhich simplifies tuning.\n", "versions": [{"version": "v1", "created": "Mon, 4 Nov 2013 13:10:31 GMT"}, {"version": "v2", "created": "Mon, 31 Mar 2014 08:27:32 GMT"}, {"version": "v3", "created": "Mon, 16 Jun 2014 12:07:03 GMT"}, {"version": "v4", "created": "Thu, 18 Sep 2014 22:28:55 GMT"}], "update_date": "2016-04-01", "authors_parsed": [["Dahlin", "Johan", ""], ["Lindsten", "Fredrik", ""], ["Sch\u00f6n", "Thomas B.", ""]]}, {"id": "1311.0689", "submitter": "Johan Dahlin Mr.", "authors": "Johan Dahlin and Fredrik Lindsten", "title": "Particle filter-based Gaussian process optimisation for parameter\n  inference", "comments": "Accepted for publication in proceedings of the 19th World Congress of\n  the International Federation of Automatic Control (IFAC), Cape Town, South\n  Africa, August 2014. 6 pages, 4 figures", "journal-ref": null, "doi": "10.3182/20140824-6-ZA-1003.00278", "report-no": null, "categories": "stat.CO stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a novel method for maximum likelihood-based parameter inference in\nnonlinear and/or non-Gaussian state space models. The method is an iterative\nprocedure with three steps. At each iteration a particle filter is used to\nestimate the value of the log-likelihood function at the current parameter\niterate. Using these log-likelihood estimates, a surrogate objective function\nis created by utilizing a Gaussian process model. Finally, we use a heuristic\nprocedure to obtain a revised parameter iterate, providing an automatic\ntrade-off between exploration and exploitation of the surrogate model. The\nmethod is profiled on two state space models with good performance both\nconsidering accuracy and computational cost.\n", "versions": [{"version": "v1", "created": "Mon, 4 Nov 2013 13:14:27 GMT"}, {"version": "v2", "created": "Mon, 31 Mar 2014 07:12:58 GMT"}], "update_date": "2016-03-22", "authors_parsed": [["Dahlin", "Johan", ""], ["Lindsten", "Fredrik", ""]]}, {"id": "1311.0701", "submitter": "Justin Bayer", "authors": "Justin Bayer, Christian Osendorfer, Daniela Korhammer, Nutan Chen,\n  Sebastian Urban, Patrick van der Smagt", "title": "On Fast Dropout and its Applicability to Recurrent Networks", "comments": "The experiments for the Penn Treebank corpus were erroneous and have\n  been stripped from this version", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recurrent Neural Networks (RNNs) are rich models for the processing of\nsequential data. Recent work on advancing the state of the art has been focused\non the optimization or modelling of RNNs, mostly motivated by adressing the\nproblems of the vanishing and exploding gradients. The control of overfitting\nhas seen considerably less attention. This paper contributes to that by\nanalyzing fast dropout, a recent regularization method for generalized linear\nmodels and neural networks from a back-propagation inspired perspective. We\nshow that fast dropout implements a quadratic form of an adaptive,\nper-parameter regularizer, which rewards large weights in the light of\nunderfitting, penalizes them for overconfident predictions and vanishes at\nminima of an unregularized training loss. The derivatives of that regularizer\nare exclusively based on the training error signal. One consequence of this is\nthe absense of a global weight attractor, which is particularly appealing for\nRNNs, since the dynamics are not biased towards a certain regime. We positively\ntest the hypothesis that this improves the performance of RNNs on four musical\ndata sets.\n", "versions": [{"version": "v1", "created": "Mon, 4 Nov 2013 13:56:23 GMT"}, {"version": "v2", "created": "Mon, 11 Nov 2013 09:53:52 GMT"}, {"version": "v3", "created": "Wed, 18 Dec 2013 15:28:07 GMT"}, {"version": "v4", "created": "Wed, 1 Jan 2014 20:49:56 GMT"}, {"version": "v5", "created": "Sun, 16 Feb 2014 10:21:01 GMT"}, {"version": "v6", "created": "Mon, 3 Mar 2014 14:48:37 GMT"}, {"version": "v7", "created": "Wed, 5 Mar 2014 19:32:29 GMT"}], "update_date": "2014-03-06", "authors_parsed": [["Bayer", "Justin", ""], ["Osendorfer", "Christian", ""], ["Korhammer", "Daniela", ""], ["Chen", "Nutan", ""], ["Urban", "Sebastian", ""], ["van der Smagt", "Patrick", ""]]}, {"id": "1311.0707", "submitter": "Niko Br\\\"ummer", "authors": "Niko Br\\\"ummer and Daniel Garcia-Romero", "title": "Generative Modelling for Unsupervised Score Calibration", "comments": "Accepted for ICASSP 2014", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Score calibration enables automatic speaker recognizers to make\ncost-effective accept / reject decisions. Traditional calibration requires\nsupervised data, which is an expensive resource. We propose a 2-component GMM\nfor unsupervised calibration and demonstrate good performance relative to a\nsupervised baseline on NIST SRE'10 and SRE'12. A Bayesian analysis demonstrates\nthat the uncertainty associated with the unsupervised calibration parameter\nestimates is surprisingly small.\n", "versions": [{"version": "v1", "created": "Mon, 4 Nov 2013 14:13:27 GMT"}, {"version": "v2", "created": "Tue, 5 Nov 2013 07:15:25 GMT"}, {"version": "v3", "created": "Fri, 14 Feb 2014 15:15:43 GMT"}], "update_date": "2014-02-17", "authors_parsed": [["Br\u00fcmmer", "Niko", ""], ["Garcia-Romero", "Daniel", ""]]}, {"id": "1311.0811", "submitter": "Anders Bredahl Kock", "authors": "Anders Bredahl Kock and Laurent A.F. Callot", "title": "Oracle Inequalities for High Dimensional Vector Autoregressions", "comments": "39 pages. This is a revised version of an article not previously\n  circulated on arXiv", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.ML stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper establishes non-asymptotic oracle inequalities for the prediction\nerror and estimation accuracy of the LASSO in stationary vector autoregressive\nmodels. These inequalities are used to establish consistency of the LASSO even\nwhen the number of parameters is of a much larger order of magnitude than the\nsample size. We also give conditions under which no relevant variables are\nexcluded.\n  Next, non-asymptotic probabilities are given for the Adaptive LASSO to select\nthe correct sparsity pattern. We then give conditions under which the Adaptive\nLASSO reveals the correct sparsity pattern asymptotically. We establish that\nthe estimates of the non-zero coefficients are asymptotically equivalent to the\noracle assisted least squares estimator. This is used to show that the rate of\nconvergence of the estimates of the non-zero coefficients is identical to the\none of least squares only including the relevant covariates.\n", "versions": [{"version": "v1", "created": "Mon, 4 Nov 2013 18:47:25 GMT"}, {"version": "v2", "created": "Thu, 15 May 2014 09:41:21 GMT"}], "update_date": "2014-05-16", "authors_parsed": [["Kock", "Anders Bredahl", ""], ["Callot", "Laurent A. F.", ""]]}, {"id": "1311.0830", "submitter": "Samet Oymak", "authors": "Samet Oymak, Christos Thrampoulidis, Babak Hassibi", "title": "The Squared-Error of Generalized LASSO: A Precise Analysis", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IT math.IT math.OC stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider the problem of estimating an unknown signal $x_0$ from noisy\nlinear observations $y = Ax_0 + z\\in R^m$. In many practical instances, $x_0$\nhas a certain structure that can be captured by a structure inducing convex\nfunction $f(\\cdot)$. For example, $\\ell_1$ norm can be used to encourage a\nsparse solution. To estimate $x_0$ with the aid of $f(\\cdot)$, we consider the\nwell-known LASSO method and provide sharp characterization of its performance.\nWe assume the entries of the measurement matrix $A$ and the noise vector $z$\nhave zero-mean normal distributions with variances $1$ and $\\sigma^2$\nrespectively. For the LASSO estimator $x^*$, we attempt to calculate the\nNormalized Square Error (NSE) defined as $\\frac{\\|x^*-x_0\\|_2^2}{\\sigma^2}$ as\na function of the noise level $\\sigma$, the number of observations $m$ and the\nstructure of the signal. We show that, the structure of the signal $x_0$ and\nchoice of the function $f(\\cdot)$ enter the error formulae through the summary\nparameters $D(cone)$ and $D(\\lambda)$, which are defined as the Gaussian\nsquared-distances to the subdifferential cone and to the $\\lambda$-scaled\nsubdifferential, respectively. The first LASSO estimator assumes a-priori\nknowledge of $f(x_0)$ and is given by $\\arg\\min_{x}\\{{\\|y-Ax\\|_2}~\\text{subject\nto}~f(x)\\leq f(x_0)\\}$. We prove that its worst case NSE is achieved when\n$\\sigma\\rightarrow 0$ and concentrates around $\\frac{D(cone)}{m-D(cone)}$.\nSecondly, we consider $\\arg\\min_{x}\\{\\|y-Ax\\|_2+\\lambda f(x)\\}$, for some\n$\\lambda\\geq 0$. This time the NSE formula depends on the choice of $\\lambda$\nand is given by $\\frac{D(\\lambda)}{m-D(\\lambda)}$. We then establish a mapping\nbetween this and the third estimator $\\arg\\min_{x}\\{\\frac{1}{2}\\|y-Ax\\|_2^2+\n\\lambda f(x)\\}$. Finally, for a number of important structured signal classes,\nwe translate our abstract formulae to closed-form upper bounds on the NSE.\n", "versions": [{"version": "v1", "created": "Mon, 4 Nov 2013 20:06:44 GMT"}, {"version": "v2", "created": "Wed, 6 Nov 2013 03:33:46 GMT"}], "update_date": "2013-11-07", "authors_parsed": [["Oymak", "Samet", ""], ["Thrampoulidis", "Christos", ""], ["Hassibi", "Babak", ""]]}, {"id": "1311.1033", "submitter": "Mikkel Schmidt", "authors": "Mikkel N. Schmidt, Tue Herlau, and Morten M{\\o}rup", "title": "Nonparametric Bayesian models of hierarchical structure in complex\n  networks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Analyzing and understanding the structure of complex relational data is\nimportant in many applications including analysis of the connectivity in the\nhuman brain. Such networks can have prominent patterns on different scales,\ncalling for a hierarchically structured model. We propose two non-parametric\nBayesian hierarchical network models based on Gibbs fragmentation tree priors,\nand demonstrate their ability to capture nested patterns in simulated networks.\nOn real networks we demonstrate detection of hierarchical structure and show\npredictive performance on par with the state of the art. We envision that our\nmethods can be employed in exploratory analysis of large scale complex networks\nfor example to model human brain connectivity.\n", "versions": [{"version": "v1", "created": "Tue, 5 Nov 2013 12:39:04 GMT"}, {"version": "v2", "created": "Thu, 21 Nov 2013 10:51:26 GMT"}], "update_date": "2013-11-22", "authors_parsed": [["Schmidt", "Mikkel N.", ""], ["Herlau", "Tue", ""], ["M\u00f8rup", "Morten", ""]]}, {"id": "1311.1040", "submitter": "Xiao-Feng Gong", "authors": "Xiao-Feng Gong, Cheng-Yuan Wang, Ya-Na Hao, and Qiu-Hua Lin", "title": "Combined Independent Component Analysis and Canonical Polyadic\n  Decomposition via Joint Diagonalization", "comments": "IEEE China Summit & International Conference on Signal and\n  Information Processing. IEEE, 2014:804 - 808", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recently, there has been a trend to combine independent component analysis\nand canonical polyadic decomposition (ICA-CPD) for an enhanced robustness for\nthe computation of CPD, and ICA-CPD could be further converted into CPD of a\n5th-order partially symmetric tensor, by calculating the eigenmatrices of the\n4th-order cumulant slices of a trilinear mixture. In this study, we propose a\nnew 5th-order CPD algorithm constrained with partial symmetry based on joint\ndiagonalization. As the main steps involved in the proposed algorithm undergo\nno updating iterations for the loading matrices, it is much faster than the\nexisting algorithm based on alternating least squares and enhanced line search,\nwith competent performances. Simulation results are provided to demonstrate the\nperformance of the proposed algorithm.\n", "versions": [{"version": "v1", "created": "Tue, 5 Nov 2013 13:07:46 GMT"}, {"version": "v2", "created": "Wed, 28 Dec 2016 02:09:17 GMT"}], "update_date": "2016-12-30", "authors_parsed": [["Gong", "Xiao-Feng", ""], ["Wang", "Cheng-Yuan", ""], ["Hao", "Ya-Na", ""], ["Lin", "Qiu-Hua", ""]]}, {"id": "1311.1189", "submitter": "Michalis Titsias", "authors": "Michalis K. Titsias, Christopher Yau, Christopher C. Holmes", "title": "Statistical Inference in Hidden Markov Models using $k$-segment\n  Constraints", "comments": "37 pages", "journal-ref": null, "doi": "10.1080/01621459.2014.998762", "report-no": null, "categories": "stat.ME cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Hidden Markov models (HMMs) are one of the most widely used statistical\nmethods for analyzing sequence data. However, the reporting of output from HMMs\nhas largely been restricted to the presentation of the most-probable (MAP)\nhidden state sequence, found via the Viterbi algorithm, or the sequence of most\nprobable marginals using the forward-backward (F-B) algorithm. In this article,\nwe expand the amount of information we could obtain from the posterior\ndistribution of an HMM by introducing linear-time dynamic programming\nalgorithms that, we collectively call $k$-segment algorithms, that allow us to\ni) find MAP sequences, ii) compute posterior probabilities and iii) simulate\nsample paths conditional on a user specified number of segments, i.e.\ncontiguous runs in a hidden state, possibly of a particular type. We illustrate\nthe utility of these methods using simulated and real examples and highlight\nthe application of prospective and retrospective use of these methods for\nfitting HMMs or exploring existing model fits.\n", "versions": [{"version": "v1", "created": "Tue, 5 Nov 2013 20:41:17 GMT"}], "update_date": "2015-05-01", "authors_parsed": [["Titsias", "Michalis K.", ""], ["Yau", "Christopher", ""], ["Holmes", "Christopher C.", ""]]}, {"id": "1311.1354", "submitter": "Jan Melchior", "authors": "Jan Melchior, Asja Fischer, Laurenz Wiskott", "title": "How to Center Binary Deep Boltzmann Machines", "comments": "Author list in meta data corrected - 57 pages, 17 figures, 13 tables", "journal-ref": "Journal of Machine Learning Research, 17(99), 2016, 1:61", "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This work analyzes centered binary Restricted Boltzmann Machines (RBMs) and\nbinary Deep Boltzmann Machines (DBMs), where centering is done by subtracting\noffset values from visible and hidden variables. We show analytically that (i)\ncentering results in a different but equivalent parameterization for artificial\nneural networks in general, (ii) the expected performance of centered binary\nRBMs/DBMs is invariant under simultaneous flip of data and offsets, for any\noffset value in the range of zero to one, (iii) centering can be reformulated\nas a different update rule for normal binary RBMs/DBMs, and (iv) using the\nenhanced gradient is equivalent to setting the offset values to the average\nover model and data mean. Furthermore, numerical simulations suggest that (i)\noptimal generative performance is achieved by subtracting mean values from\nvisible as well as hidden variables, (ii) centered RBMs/DBMs reach\nsignificantly higher log-likelihood values than normal binary RBMs/DBMs, (iii)\ncentering variants whose offsets depend on the model mean, like the enhanced\ngradient, suffer from severe divergence problems, (iv) learning is stabilized\nif an exponentially moving average over the batch means is used for the offset\nvalues instead of the current batch mean, which also prevents the enhanced\ngradient from diverging, (v) centered RBMs/DBMs reach higher LL values than\nnormal RBMs/DBMs while having a smaller norm of the weight matrix, (vi)\ncentering leads to an update direction that is closer to the natural gradient\nand that the natural gradient is extremly efficient for training RBMs, (vii)\ncentering dispense the need for greedy layer-wise pre-training of DBMs, (viii)\nfurthermore we show that pre-training often even worsen the results\nindependently whether centering is used or not, and (ix) centering is also\nbeneficial for auto encoders.\n", "versions": [{"version": "v1", "created": "Wed, 6 Nov 2013 11:25:42 GMT"}, {"version": "v2", "created": "Wed, 15 Jul 2015 08:31:27 GMT"}, {"version": "v3", "created": "Thu, 16 Jul 2015 08:37:23 GMT"}], "update_date": "2017-02-08", "authors_parsed": [["Melchior", "Jan", ""], ["Fischer", "Asja", ""], ["Wiskott", "Laurenz", ""]]}, {"id": "1311.1644", "submitter": "Matan Gavish", "authors": "Moshe Dubiner, Matan Gavish and Yoram Singer", "title": "The Maximum Entropy Relaxation Path", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG math.OC stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The relaxed maximum entropy problem is concerned with finding a probability\ndistribution on a finite set that minimizes the relative entropy to a given\nprior distribution, while satisfying relaxed max-norm constraints with respect\nto a third observed multinomial distribution. We study the entire relaxation\npath for this problem in detail. We show existence and a geometric description\nof the relaxation path. Specifically, we show that the maximum entropy\nrelaxation path admits a planar geometric description as an increasing,\npiecewise linear function in the inverse relaxation parameter. We derive fast\nalgorithms for tracking the path. In various realistic settings, our algorithms\nrequire $O(n\\log(n))$ operations for probability distributions on $n$ points,\nmaking it possible to handle large problems. Once the path has been recovered,\nwe show that given a validation set, the family of admissible models is reduced\nfrom an infinite family to a small, discrete set. We demonstrate the merits of\nour approach in experiments with synthetic data and discuss its potential for\nthe estimation of compact n-gram language models.\n", "versions": [{"version": "v1", "created": "Thu, 7 Nov 2013 11:33:14 GMT"}], "update_date": "2013-11-08", "authors_parsed": [["Dubiner", "Moshe", ""], ["Gavish", "Matan", ""], ["Singer", "Yoram", ""]]}, {"id": "1311.1704", "submitter": "Prem Gopalan", "authors": "Prem Gopalan, Jake M. Hofman, David M. Blei", "title": "Scalable Recommendation with Poisson Factorization", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR cs.AI cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We develop a Bayesian Poisson matrix factorization model for forming\nrecommendations from sparse user behavior data. These data are large user/item\nmatrices where each user has provided feedback on only a small subset of items,\neither explicitly (e.g., through star ratings) or implicitly (e.g., through\nviews or purchases). In contrast to traditional matrix factorization\napproaches, Poisson factorization implicitly models each user's limited\nattention to consume items. Moreover, because of the mathematical form of the\nPoisson likelihood, the model needs only to explicitly consider the observed\nentries in the matrix, leading to both scalable computation and good predictive\nperformance. We develop a variational inference algorithm for approximate\nposterior inference that scales up to massive data sets. This is an efficient\nalgorithm that iterates over the observed entries and adjusts an approximate\nposterior over the user/item representations. We apply our method to large\nreal-world user data containing users rating movies, users listening to songs,\nand users reading scientific papers. In all these settings, Bayesian Poisson\nfactorization outperforms state-of-the-art matrix factorization methods.\n", "versions": [{"version": "v1", "created": "Thu, 7 Nov 2013 14:58:40 GMT"}, {"version": "v2", "created": "Tue, 12 Nov 2013 17:23:05 GMT"}, {"version": "v3", "created": "Tue, 20 May 2014 19:19:30 GMT"}], "update_date": "2014-05-21", "authors_parsed": [["Gopalan", "Prem", ""], ["Hofman", "Jake M.", ""], ["Blei", "David M.", ""]]}, {"id": "1311.1731", "submitter": "Edoardo Airoldi", "authors": "Edoardo M Airoldi, Thiago B Costa, Stanley H Chan", "title": "Stochastic blockmodel approximation of a graphon: Theory and consistent\n  estimation", "comments": "20 pages, 4 figures, 2 algorithms. Neural Information Processing\n  Systems (NIPS), 2013", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME cs.LG cs.SI physics.data-an stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Non-parametric approaches for analyzing network data based on exchangeable\ngraph models (ExGM) have recently gained interest. The key object that defines\nan ExGM is often referred to as a graphon. This non-parametric perspective on\nnetwork modeling poses challenging questions on how to make inference on the\ngraphon underlying observed network data. In this paper, we propose a\ncomputationally efficient procedure to estimate a graphon from a set of\nobserved networks generated from it. This procedure is based on a stochastic\nblockmodel approximation (SBA) of the graphon. We show that, by approximating\nthe graphon with a stochastic block model, the graphon can be consistently\nestimated, that is, the estimation error vanishes as the size of the graph\napproaches infinity.\n", "versions": [{"version": "v1", "created": "Thu, 7 Nov 2013 16:20:02 GMT"}, {"version": "v2", "created": "Fri, 8 Nov 2013 04:09:51 GMT"}], "update_date": "2013-11-14", "authors_parsed": [["Airoldi", "Edoardo M", ""], ["Costa", "Thiago B", ""], ["Chan", "Stanley H", ""]]}, {"id": "1311.1756", "submitter": "Quoc Tran-Dinh", "authors": "Quoc Tran Dinh, Anastasios Kyrillidis, and Volkan Cevher", "title": "An Inexact Proximal Path-Following Algorithm for Constrained Convex\n  Minimization", "comments": "26 pages, 4 figures, and 4 tables (submitted), LIONS-EPFL Report 2013\n  - This is the substitution for the old version", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.OC stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Many scientific and engineering applications feature nonsmooth convex\nminimization problems over convex sets. In this paper, we address an important\ninstance of this broad class where we assume that the nonsmooth objective is\nequipped with a tractable proximity operator and that the convex constraint set\naffords a self-concordant barrier. We provide a new joint treatment of proximal\nand self-concordant barrier concepts and illustrate that such problems can be\nefficiently solved, without the need of lifting the problem dimensions, as in\ndisciplined convex optimization approach. We propose an inexact path-following\nalgorithmic framework and theoretically characterize the worst-case analytical\ncomplexity of this framework when the proximal subproblems are solved\ninexactly. To show the merits of our framework, we apply its instances to both\nsynthetic and real-world applications, where it shows advantages over standard\ninterior point methods. As a by-product, we describe how our framework can\nobtain points on the Pareto frontier of regularized problems with\nself-concordant objectives in a tuning free fashion.\n", "versions": [{"version": "v1", "created": "Thu, 7 Nov 2013 17:25:08 GMT"}, {"version": "v2", "created": "Fri, 20 Jun 2014 14:40:00 GMT"}], "update_date": "2014-06-23", "authors_parsed": [["Dinh", "Quoc Tran", ""], ["Kyrillidis", "Anastasios", ""], ["Cevher", "Volkan", ""]]}, {"id": "1311.1780", "submitter": "KyungHyun Cho", "authors": "Caglar Gulcehre, Kyunghyun Cho, Razvan Pascanu and Yoshua Bengio", "title": "Learned-Norm Pooling for Deep Feedforward and Recurrent Neural Networks", "comments": "ECML/PKDD 2014", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NE cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper we propose and investigate a novel nonlinear unit, called $L_p$\nunit, for deep neural networks. The proposed $L_p$ unit receives signals from\nseveral projections of a subset of units in the layer below and computes a\nnormalized $L_p$ norm. We notice two interesting interpretations of the $L_p$\nunit. First, the proposed unit can be understood as a generalization of a\nnumber of conventional pooling operators such as average, root-mean-square and\nmax pooling widely used in, for instance, convolutional neural networks (CNN),\nHMAX models and neocognitrons. Furthermore, the $L_p$ unit is, to a certain\ndegree, similar to the recently proposed maxout unit (Goodfellow et al., 2013)\nwhich achieved the state-of-the-art object recognition results on a number of\nbenchmark datasets. Secondly, we provide a geometrical interpretation of the\nactivation function based on which we argue that the $L_p$ unit is more\nefficient at representing complex, nonlinear separating boundaries. Each $L_p$\nunit defines a superelliptic boundary, with its exact shape defined by the\norder $p$. We claim that this makes it possible to model arbitrarily shaped,\ncurved boundaries more efficiently by combining a few $L_p$ units of different\norders. This insight justifies the need for learning different orders for each\nunit in the model. We empirically evaluate the proposed $L_p$ units on a number\nof datasets and show that multilayer perceptrons (MLP) consisting of the $L_p$\nunits achieve the state-of-the-art results on a number of benchmark datasets.\nFurthermore, we evaluate the proposed $L_p$ unit on the recently proposed deep\nrecurrent neural networks (RNN).\n", "versions": [{"version": "v1", "created": "Thu, 7 Nov 2013 18:30:37 GMT"}, {"version": "v2", "created": "Mon, 11 Nov 2013 03:32:43 GMT"}, {"version": "v3", "created": "Tue, 12 Nov 2013 18:32:42 GMT"}, {"version": "v4", "created": "Wed, 29 Jan 2014 22:55:24 GMT"}, {"version": "v5", "created": "Sat, 1 Feb 2014 18:17:38 GMT"}, {"version": "v6", "created": "Fri, 7 Feb 2014 18:55:42 GMT"}, {"version": "v7", "created": "Tue, 2 Sep 2014 00:53:40 GMT"}], "update_date": "2014-09-03", "authors_parsed": [["Gulcehre", "Caglar", ""], ["Cho", "Kyunghyun", ""], ["Pascanu", "Razvan", ""], ["Bengio", "Yoshua", ""]]}, {"id": "1311.1903", "submitter": "Matus Telgarsky", "authors": "Matus Telgarsky, Sanjoy Dasgupta", "title": "Moment-based Uniform Deviation Bounds for $k$-means and Friends", "comments": "To appear, NIPS 2013", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Suppose $k$ centers are fit to $m$ points by heuristically minimizing the\n$k$-means cost; what is the corresponding fit over the source distribution?\nThis question is resolved here for distributions with $p\\geq 4$ bounded\nmoments; in particular, the difference between the sample cost and distribution\ncost decays with $m$ and $p$ as $m^{\\min\\{-1/4, -1/2+2/p\\}}$. The essential\ntechnical contribution is a mechanism to uniformly control deviations in the\nface of unbounded parameter sets, cost functions, and source distributions. To\nfurther demonstrate this mechanism, a soft clustering variant of $k$-means cost\nis also considered, namely the log likelihood of a Gaussian mixture, subject to\nthe constraint that all covariance matrices have bounded spectrum. Lastly, a\nrate with refined constants is provided for $k$-means instances possessing some\ncluster structure.\n", "versions": [{"version": "v1", "created": "Fri, 8 Nov 2013 08:44:11 GMT"}], "update_date": "2013-11-11", "authors_parsed": [["Telgarsky", "Matus", ""], ["Dasgupta", "Sanjoy", ""]]}, {"id": "1311.1911", "submitter": "Gina Gruenhage", "authors": "Gina Gruenhage, Manfred Opper and Simon Barthelme", "title": "Visualizing the Effects of a Changing Distance on Data Using Continuous\n  Embeddings", "comments": "This is manuscript is accepted for publication in 'Computational\n  Statistics and Data Analysis'", "journal-ref": null, "doi": "10.1016/j.csda.2016.06.006", "report-no": null, "categories": "stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Most Machine Learning (ML) methods, from clustering to classification, rely\non a distance function to describe relationships between datapoints. For\ncomplex datasets it is hard to avoid making some arbitrary choices when\ndefining a distance function. To compare images, one must choose a spatial\nscale, for signals, a temporal scale. The right scale is hard to pin down and\nit is preferable when results do not depend too tightly on the exact value one\npicked. Topological data analysis seeks to address this issue by focusing on\nthe notion of neighbourhood instead of distance. It is shown that in some cases\na simpler solution is available. It can be checked how strongly distance\nrelationships depend on a hyperparameter using dimensionality reduction. A\nvariant of dynamical multi-dimensional scaling (MDS) is formulated, which\nembeds datapoints as curves. The resulting algorithm is based on the\nConcave-Convex Procedure (CCCP) and provides a simple and efficient way of\nvisualizing changes and invariances in distance patterns as a hyperparameter is\nvaried. A variant to analyze the dependence on multiple hyperparameters is also\npresented. A cMDS algorithm that is straightforward to implement, use and\nextend is provided. To illustrate the possibilities of cMDS, cMDS is applied to\nseveral real-world data sets.\n", "versions": [{"version": "v1", "created": "Fri, 8 Nov 2013 09:33:06 GMT"}, {"version": "v2", "created": "Tue, 20 Jan 2015 14:09:35 GMT"}, {"version": "v3", "created": "Fri, 1 Jul 2016 08:32:26 GMT"}], "update_date": "2016-07-04", "authors_parsed": [["Gruenhage", "Gina", ""], ["Opper", "Manfred", ""], ["Barthelme", "Simon", ""]]}, {"id": "1311.2079", "submitter": "Myunghwan Kim", "authors": "Myunghwan Kim and Jure Leskovec", "title": "Nonparametric Multi-group Membership Model for Dynamic Networks", "comments": "In Advances in Neural Information Processing Systems 25 (2013)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.SI physics.soc-ph stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Relational data-like graphs, networks, and matrices-is often dynamic, where\nthe relational structure evolves over time. A fundamental problem in the\nanalysis of time-varying network data is to extract a summary of the common\nstructure and the dynamics of the underlying relations between the entities.\nHere we build on the intuition that changes in the network structure are driven\nby the dynamics at the level of groups of nodes. We propose a nonparametric\nmulti-group membership model for dynamic networks. Our model contains three\nmain components: We model the birth and death of individual groups with respect\nto the dynamics of the network structure via a distance dependent Indian Buffet\nProcess. We capture the evolution of individual node group memberships via a\nFactorial Hidden Markov model. And, we explain the dynamics of the network\nstructure by explicitly modeling the connectivity structure of groups. We\ndemonstrate our model's capability of identifying the dynamics of latent groups\nin a number of different types of network data. Experimental results show that\nour model provides improved predictive performance over existing dynamic\nnetwork models on future network forecasting and missing link prediction.\n", "versions": [{"version": "v1", "created": "Fri, 8 Nov 2013 21:00:51 GMT"}], "update_date": "2013-11-12", "authors_parsed": [["Kim", "Myunghwan", ""], ["Leskovec", "Jure", ""]]}, {"id": "1311.2150", "submitter": "Jun Fang", "authors": "Jun Fang, Yanning Shen, Hongbin Li (IEEE), and Pu Wang", "title": "Pattern-Coupled Sparse Bayesian Learning for Recovery of Block-Sparse\n  Signals", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IT cs.LG math.IT stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider the problem of recovering block-sparse signals whose structures\nare unknown \\emph{a priori}. Block-sparse signals with nonzero coefficients\noccurring in clusters arise naturally in many practical scenarios. However, the\nknowledge of the block structure is usually unavailable in practice. In this\npaper, we develop a new sparse Bayesian learning method for recovery of\nblock-sparse signals with unknown cluster patterns. Specifically, a\npattern-coupled hierarchical Gaussian prior model is introduced to characterize\nthe statistical dependencies among coefficients, in which a set of\nhyperparameters are employed to control the sparsity of signal coefficients.\nUnlike the conventional sparse Bayesian learning framework in which each\nindividual hyperparameter is associated independently with each coefficient, in\nthis paper, the prior for each coefficient not only involves its own\nhyperparameter, but also the hyperparameters of its immediate neighbors. In\ndoing this way, the sparsity patterns of neighboring coefficients are related\nto each other and the hierarchical model has the potential to encourage\nstructured-sparse solutions. The hyperparameters, along with the sparse signal,\nare learned by maximizing their posterior probability via an\nexpectation-maximization (EM) algorithm. Numerical results show that the\nproposed algorithm presents uniform superiority over other existing methods in\na series of experiments.\n", "versions": [{"version": "v1", "created": "Sat, 9 Nov 2013 08:28:27 GMT"}], "update_date": "2013-11-12", "authors_parsed": [["Fang", "Jun", "", "IEEE"], ["Shen", "Yanning", "", "IEEE"], ["Li", "Hongbin", "", "IEEE"], ["Wang", "Pu", ""]]}, {"id": "1311.2234", "submitter": "Junier Oliva", "authors": "Junier B. Oliva, Barnabas Poczos, Timothy Verstynen, Aarti Singh, Jeff\n  Schneider, Fang-Cheng Yeh, Wen-Yih Tseng", "title": "FuSSO: Functional Shrinkage and Selection Operator", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present the FuSSO, a functional analogue to the LASSO, that efficiently\nfinds a sparse set of functional input covariates to regress a real-valued\nresponse against. The FuSSO does so in a semi-parametric fashion, making no\nparametric assumptions about the nature of input functional covariates and\nassuming a linear form to the mapping of functional covariates to the response.\nWe provide a statistical backing for use of the FuSSO via proof of asymptotic\nsparsistency under various conditions. Furthermore, we observe good results on\nboth synthetic and real-world data.\n", "versions": [{"version": "v1", "created": "Sun, 10 Nov 2013 00:44:01 GMT"}, {"version": "v2", "created": "Sun, 9 Mar 2014 02:30:26 GMT"}], "update_date": "2014-03-11", "authors_parsed": [["Oliva", "Junier B.", ""], ["Poczos", "Barnabas", ""], ["Verstynen", "Timothy", ""], ["Singh", "Aarti", ""], ["Schneider", "Jeff", ""], ["Yeh", "Fang-Cheng", ""], ["Tseng", "Wen-Yih", ""]]}, {"id": "1311.2236", "submitter": "Junier Oliva", "authors": "Junier B. Oliva, Willie Neiswanger, Barnabas Poczos, Jeff Schneider,\n  Eric Xing", "title": "Fast Distribution To Real Regression", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study the problem of distribution to real-value regression, where one aims\nto regress a mapping $f$ that takes in a distribution input covariate $P\\in\n\\mathcal{I}$ (for a non-parametric family of distributions $\\mathcal{I}$) and\noutputs a real-valued response $Y=f(P) + \\epsilon$. This setting was recently\nstudied, and a \"Kernel-Kernel\" estimator was introduced and shown to have a\npolynomial rate of convergence. However, evaluating a new prediction with the\nKernel-Kernel estimator scales as $\\Omega(N)$. This causes the difficult\nsituation where a large amount of data may be necessary for a low estimation\nrisk, but the computation cost of estimation becomes infeasible when the\ndata-set is too large. To this end, we propose the Double-Basis estimator,\nwhich looks to alleviate this big data problem in two ways: first, the\nDouble-Basis estimator is shown to have a computation complexity that is\nindependent of the number of of instances $N$ when evaluating new predictions\nafter training; secondly, the Double-Basis estimator is shown to have a fast\nrate of convergence for a general class of mappings $f\\in\\mathcal{F}$.\n", "versions": [{"version": "v1", "created": "Sun, 10 Nov 2013 01:17:19 GMT"}, {"version": "v2", "created": "Sun, 9 Mar 2014 03:41:35 GMT"}], "update_date": "2014-03-11", "authors_parsed": [["Oliva", "Junier B.", ""], ["Neiswanger", "Willie", ""], ["Poczos", "Barnabas", ""], ["Schneider", "Jeff", ""], ["Xing", "Eric", ""]]}, {"id": "1311.2241", "submitter": "Ying Liu", "authors": "Ying Liu and Alan S. Willsky", "title": "Learning Gaussian Graphical Models with Observed or Latent FVSs", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Gaussian Graphical Models (GGMs) or Gauss Markov random fields are widely\nused in many applications, and the trade-off between the modeling capacity and\nthe efficiency of learning and inference has been an important research\nproblem. In this paper, we study the family of GGMs with small feedback vertex\nsets (FVSs), where an FVS is a set of nodes whose removal breaks all the\ncycles. Exact inference such as computing the marginal distributions and the\npartition function has complexity $O(k^{2}n)$ using message-passing algorithms,\nwhere k is the size of the FVS, and n is the total number of nodes. We propose\nefficient structure learning algorithms for two cases: 1) All nodes are\nobserved, which is useful in modeling social or flight networks where the FVS\nnodes often correspond to a small number of high-degree nodes, or hubs, while\nthe rest of the networks is modeled by a tree. Regardless of the maximum\ndegree, without knowing the full graph structure, we can exactly compute the\nmaximum likelihood estimate in $O(kn^2+n^2\\log n)$ if the FVS is known or in\npolynomial time if the FVS is unknown but has bounded size. 2) The FVS nodes\nare latent variables, where structure learning is equivalent to decomposing a\ninverse covariance matrix (exactly or approximately) into the sum of a\ntree-structured matrix and a low-rank matrix. By incorporating efficient\ninference into the learning steps, we can obtain a learning algorithm using\nalternating low-rank correction with complexity $O(kn^{2}+n^{2}\\log n)$ per\niteration. We also perform experiments using both synthetic data as well as\nreal data of flight delays to demonstrate the modeling capacity with FVSs of\nvarious sizes.\n", "versions": [{"version": "v1", "created": "Sun, 10 Nov 2013 02:39:48 GMT"}], "update_date": "2013-11-12", "authors_parsed": [["Liu", "Ying", ""], ["Willsky", "Alan S.", ""]]}, {"id": "1311.2483", "submitter": "Sebastien Da Veiga", "authors": "S\\'ebastien Da Veiga (IFPEN, - M\\'ethodes d'Analyse Stochastique des\n  Codes et Traitements Num\\'eriques)", "title": "Global Sensitivity Analysis with Dependence Measures", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST cs.LG stat.ML stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Global sensitivity analysis with variance-based measures suffers from several\ntheoretical and practical limitations, since they focus only on the variance of\nthe output and handle multivariate variables in a limited way. In this paper,\nwe introduce a new class of sensitivity indices based on dependence measures\nwhich overcomes these insufficiencies. Our approach originates from the idea to\ncompare the output distribution with its conditional counterpart when one of\nthe input variables is fixed. We establish that this comparison yields\npreviously proposed indices when it is performed with Csiszar f-divergences, as\nwell as sensitivity indices which are well-known dependence measures between\nrandom variables. This leads us to investigate completely new sensitivity\nindices based on recent state-of-the-art dependence measures, such as distance\ncorrelation and the Hilbert-Schmidt independence criterion. We also emphasize\nthe potential of feature selection techniques relying on such dependence\nmeasures as alternatives to screening in high dimension.\n", "versions": [{"version": "v1", "created": "Mon, 11 Nov 2013 16:30:06 GMT"}], "update_date": "2013-11-12", "authors_parsed": [["Da Veiga", "S\u00e9bastien", "", "IFPEN, - M\u00e9thodes d'Analyse Stochastique des\n  Codes et Traitements Num\u00e9riques"]]}, {"id": "1311.2503", "submitter": "Stefan Richthofer", "authors": "Stefan Richthofer, Laurenz Wiskott", "title": "Predictable Feature Analysis", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://creativecommons.org/licenses/by-nc-sa/3.0/", "abstract": "  Every organism in an environment, whether biological, robotic or virtual,\nmust be able to predict certain aspects of its environment in order to survive\nor perform whatever task is intended. It needs a model that is capable of\nestimating the consequences of possible actions, so that planning, control, and\ndecision-making become feasible. For scientific purposes, such models are\nusually created in a problem specific manner using differential equations and\nother techniques from control- and system-theory. In contrast to that, we aim\nfor an unsupervised approach that builds up the desired model in a\nself-organized fashion. Inspired by Slow Feature Analysis (SFA), our approach\nis to extract sub-signals from the input, that behave as predictable as\npossible. These \"predictable features\" are highly relevant for modeling,\nbecause predictability is a desired property of the needed\nconsequence-estimating model by definition. In our approach, we measure\npredictability with respect to a certain prediction model. We focus here on the\nsolution of the arising optimization problem and present a tractable algorithm\nbased on algebraic methods which we call Predictable Feature Analysis (PFA). We\nprove that the algorithm finds the globally optimal signal, if this signal can\nbe predicted with low error. To deal with cases where the optimal signal has a\nsignificant prediction error, we provide a robust, heuristically motivated\nvariant of the algorithm and verify it empirically. Additionally, we give\nformal criteria a prediction-model must meet to be suitable for measuring\npredictability in the PFA setting and also provide a suitable default-model\nalong with a formal proof that it meets these criteria.\n", "versions": [{"version": "v1", "created": "Mon, 11 Nov 2013 17:05:22 GMT"}], "update_date": "2013-11-12", "authors_parsed": [["Richthofer", "Stefan", ""], ["Wiskott", "Laurenz", ""]]}, {"id": "1311.2520", "submitter": "Tue Herlau", "authors": "Tue Herlau, Mikkel N. Schmidt, Morten M{\\o}rup", "title": "The Infinite Degree Corrected Stochastic Block Model", "comments": "Originally presented at the Complex Networks workshop NIPS 2013", "journal-ref": "Phys. Rev. E 90, 032819 (2014)", "doi": "10.1103/PhysRevE.90.032819", "report-no": null, "categories": "stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In Stochastic blockmodels, which are among the most prominent statistical\nmodels for cluster analysis of complex networks, clusters are defined as groups\nof nodes with statistically similar link probabilities within and between\ngroups. A recent extension by Karrer and Newman incorporates a node degree\ncorrection to model degree heterogeneity within each group. Although this\ndemonstrably leads to better performance on several networks it is not obvious\nwhether modelling node degree is always appropriate or necessary. We formulate\nthe degree corrected stochastic blockmodel as a non-parametric Bayesian model,\nincorporating a parameter to control the amount of degree correction which can\nthen be inferred from data. Additionally, our formulation yields principled\nways of inferring the number of groups as well as predicting missing links in\nthe network which can be used to quantify the model's predictive performance.\nOn synthetic data we demonstrate that including the degree correction yields\nbetter performance both on recovering the true group structure and predicting\nmissing links when degree heterogeneity is present, whereas performance is on\npar for data with no degree heterogeneity within clusters. On seven real\nnetworks (with no ground truth group structure available) we show that\npredictive performance is about equal whether or not degree correction is\nincluded; however, for some networks significantly fewer clusters are\ndiscovered when correcting for degree indicating that the data can be more\ncompactly explained by clusters of heterogenous degree nodes.\n", "versions": [{"version": "v1", "created": "Mon, 11 Nov 2013 18:37:35 GMT"}, {"version": "v2", "created": "Wed, 13 Nov 2013 13:27:02 GMT"}, {"version": "v3", "created": "Fri, 30 May 2014 19:02:45 GMT"}], "update_date": "2014-10-08", "authors_parsed": [["Herlau", "Tue", ""], ["Schmidt", "Mikkel N.", ""], ["M\u00f8rup", "Morten", ""]]}, {"id": "1311.2542", "submitter": "Jelani Nelson", "authors": "Jean Bourgain, Sjoerd Dirksen, Jelani Nelson", "title": "Toward a unified theory of sparse dimensionality reduction in Euclidean\n  space", "comments": null, "journal-ref": "Geometric and Functional Analysis 25 (2015), no. 4, 1009-1088", "doi": null, "report-no": null, "categories": "cs.DS cs.CG cs.IT math.IT math.PR stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Let $\\Phi\\in\\mathbb{R}^{m\\times n}$ be a sparse Johnson-Lindenstrauss\ntransform [KN14] with $s$ non-zeroes per column. For a subset $T$ of the unit\nsphere, $\\varepsilon\\in(0,1/2)$ given, we study settings for $m,s$ required to\nensure $$ \\mathop{\\mathbb{E}}_\\Phi \\sup_{x\\in T} \\left|\\|\\Phi x\\|_2^2 - 1\n\\right| < \\varepsilon , $$ i.e. so that $\\Phi$ preserves the norm of every\n$x\\in T$ simultaneously and multiplicatively up to $1+\\varepsilon$. We\nintroduce a new complexity parameter, which depends on the geometry of $T$, and\nshow that it suffices to choose $s$ and $m$ such that this parameter is small.\nOur result is a sparse analog of Gordon's theorem, which was concerned with a\ndense $\\Phi$ having i.i.d. Gaussian entries. We qualitatively unify several\nresults related to the Johnson-Lindenstrauss lemma, subspace embeddings, and\nFourier-based restricted isometries. Our work also implies new results in using\nthe sparse Johnson-Lindenstrauss transform in numerical linear algebra,\nclassical and model-based compressed sensing, manifold learning, and\nconstrained least squares problems such as the Lasso.\n", "versions": [{"version": "v1", "created": "Mon, 11 Nov 2013 19:30:30 GMT"}, {"version": "v2", "created": "Thu, 3 Apr 2014 19:17:52 GMT"}, {"version": "v3", "created": "Fri, 19 Dec 2014 14:45:27 GMT"}, {"version": "v4", "created": "Tue, 25 Aug 2015 21:08:59 GMT"}], "update_date": "2015-08-27", "authors_parsed": [["Bourgain", "Jean", ""], ["Dirksen", "Sjoerd", ""], ["Nelson", "Jelani", ""]]}, {"id": "1311.2547", "submitter": "Yuekai Sun", "authors": "Yuekai Sun, Stratis Ioannidis, Andrea Montanari", "title": "Learning Mixtures of Linear Classifiers", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider a discriminative learning (regression) problem, whereby the\nregression function is a convex combination of k linear classifiers. Existing\napproaches are based on the EM algorithm, or similar techniques, without\nprovable guarantees. We develop a simple method based on spectral techniques\nand a `mirroring' trick, that discovers the subspace spanned by the\nclassifiers' parameter vectors. Under a probabilistic assumption on the feature\nvector distribution, we prove that this approach has nearly optimal statistical\nefficiency.\n", "versions": [{"version": "v1", "created": "Mon, 11 Nov 2013 19:50:51 GMT"}, {"version": "v2", "created": "Fri, 11 Apr 2014 18:52:34 GMT"}, {"version": "v3", "created": "Mon, 14 Jul 2014 18:26:20 GMT"}, {"version": "v4", "created": "Wed, 30 Jul 2014 23:40:04 GMT"}], "update_date": "2014-08-01", "authors_parsed": [["Sun", "Yuekai", ""], ["Ioannidis", "Stratis", ""], ["Montanari", "Andrea", ""]]}, {"id": "1311.2645", "submitter": "Ivan Fernandez-Val", "authors": "Alexandre Belloni and Victor Chernozhukov and Ivan Fern\\'andez-Val and\n  Christian Hansen", "title": "Program Evaluation and Causal Inference with High-Dimensional Data", "comments": "118 pages, 3 tables, 11 figures, includes supplementary appendix.\n  This version corrects some typos in Example 2 of the published version", "journal-ref": "Econometrica, Vol. 85, No. 1 (January, 2017), 233-298", "doi": null, "report-no": null, "categories": "math.ST econ.EM stat.ME stat.ML stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we provide efficient estimators and honest confidence bands\nfor a variety of treatment effects including local average (LATE) and local\nquantile treatment effects (LQTE) in data-rich environments. We can handle very\nmany control variables, endogenous receipt of treatment, heterogeneous\ntreatment effects, and function-valued outcomes. Our framework covers the\nspecial case of exogenous receipt of treatment, either conditional on controls\nor unconditionally as in randomized control trials. In the latter case, our\napproach produces efficient estimators and honest bands for (functional)\naverage treatment effects (ATE) and quantile treatment effects (QTE). To make\ninformative inference possible, we assume that key reduced form predictive\nrelationships are approximately sparse. This assumption allows the use of\nregularization and selection methods to estimate those relations, and we\nprovide methods for post-regularization and post-selection inference that are\nuniformly valid (honest) across a wide-range of models. We show that a key\ningredient enabling honest inference is the use of orthogonal or doubly robust\nmoment conditions in estimating certain reduced form functional parameters. We\nillustrate the use of the proposed methods with an application to estimating\nthe effect of 401(k) eligibility and participation on accumulated assets.\n", "versions": [{"version": "v1", "created": "Mon, 11 Nov 2013 23:36:44 GMT"}, {"version": "v2", "created": "Mon, 30 Dec 2013 19:24:31 GMT"}, {"version": "v3", "created": "Thu, 5 Jun 2014 20:21:24 GMT"}, {"version": "v4", "created": "Sat, 9 Aug 2014 17:53:36 GMT"}, {"version": "v5", "created": "Mon, 21 Sep 2015 02:26:55 GMT"}, {"version": "v6", "created": "Fri, 18 Mar 2016 02:33:33 GMT"}, {"version": "v7", "created": "Wed, 21 Dec 2016 18:57:37 GMT"}, {"version": "v8", "created": "Fri, 5 Jan 2018 07:13:54 GMT"}], "update_date": "2018-01-08", "authors_parsed": [["Belloni", "Alexandre", ""], ["Chernozhukov", "Victor", ""], ["Fern\u00e1ndez-Val", "Ivan", ""], ["Hansen", "Christian", ""]]}, {"id": "1311.2663", "submitter": "Shandian Zhe", "authors": "Shandian Zhe and Yuan Qi and Youngja Park and Ian Molloy and Suresh\n  Chari", "title": "DinTucker: Scaling up Gaussian process models on multidimensional arrays\n  with billions of elements", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.DC stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Infinite Tucker Decomposition (InfTucker) and random function prior models,\nas nonparametric Bayesian models on infinite exchangeable arrays, are more\npowerful models than widely-used multilinear factorization methods including\nTucker and PARAFAC decomposition, (partly) due to their capability of modeling\nnonlinear relationships between array elements. Despite their great predictive\nperformance and sound theoretical foundations, they cannot handle massive data\ndue to a prohibitively high training time. To overcome this limitation, we\npresent Distributed Infinite Tucker (DINTUCKER), a large-scale nonlinear tensor\ndecomposition algorithm on MAPREDUCE. While maintaining the predictive accuracy\nof InfTucker, it is scalable on massive data. DINTUCKER is based on a new\nhierarchical Bayesian model that enables local training of InfTucker on\nsubarrays and information integration from all local training results. We use\ndistributed stochastic gradient descent, coupled with variational inference, to\ntrain this model. We apply DINTUCKER to multidimensional arrays with billions\nof elements from applications in the \"Read the Web\" project (Carlson et al.,\n2010) and in information security and compare it with the state-of-the-art\nlarge-scale tensor decomposition method, GigaTensor. On both datasets,\nDINTUCKER achieves significantly higher prediction accuracy with less\ncomputational time.\n", "versions": [{"version": "v1", "created": "Tue, 12 Nov 2013 02:36:03 GMT"}, {"version": "v2", "created": "Wed, 13 Nov 2013 23:50:57 GMT"}, {"version": "v3", "created": "Sun, 15 Dec 2013 13:56:18 GMT"}, {"version": "v4", "created": "Thu, 23 Jan 2014 05:49:44 GMT"}, {"version": "v5", "created": "Sat, 1 Feb 2014 14:35:04 GMT"}], "update_date": "2014-02-04", "authors_parsed": [["Zhe", "Shandian", ""], ["Qi", "Yuan", ""], ["Park", "Youngja", ""], ["Molloy", "Ian", ""], ["Chari", "Suresh", ""]]}, {"id": "1311.2694", "submitter": "Purnamrita Sarkar", "authors": "Peter J. Bickel, Purnamrita Sarkar", "title": "Hypothesis Testing for Automated Community Detection in Networks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG cs.SI math.ST physics.soc-ph stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Community detection in networks is a key exploratory tool with applications\nin a diverse set of areas, ranging from finding communities in social and\nbiological networks to identifying link farms in the World Wide Web. The\nproblem of finding communities or clusters in a network has received much\nattention from statistics, physics and computer science. However, most\nclustering algorithms assume knowledge of the number of clusters k. In this\npaper we propose to automatically determine k in a graph generated from a\nStochastic Blockmodel. Our main contribution is twofold; first, we\ntheoretically establish the limiting distribution of the principal eigenvalue\nof the suitably centered and scaled adjacency matrix, and use that distribution\nfor our hypothesis test. Secondly, we use this test to design a recursive\nbipartitioning algorithm. Using quantifiable classification tasks on real world\nnetworks with ground truth, we show that our algorithm outperforms existing\nprobabilistic models for learning overlapping clusters, and on unlabeled\nnetworks, we show that we uncover nested community structure.\n", "versions": [{"version": "v1", "created": "Tue, 12 Nov 2013 07:00:13 GMT"}, {"version": "v2", "created": "Wed, 20 Nov 2013 05:40:00 GMT"}], "update_date": "2017-09-05", "authors_parsed": [["Bickel", "Peter J.", ""], ["Sarkar", "Purnamrita", ""]]}, {"id": "1311.2791", "submitter": "Shachar Kaufman", "authors": "Shachar Kaufman and Saharon Rosset", "title": "When Does More Regularization Imply Fewer Degrees of Freedom? Sufficient\n  Conditions and Counter Examples from Lasso and Ridge Regression", "comments": "Main text: 15 pages, 2 figures; Supplementary material is included at\n  the end of the main text: 9 pages, 7 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.ML stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Regularization aims to improve prediction performance of a given statistical\nmodeling approach by moving to a second approach which achieves worse training\nerror but is expected to have fewer degrees of freedom, i.e., better agreement\nbetween training and prediction error. We show here, however, that this\nexpected behavior does not hold in general. In fact, counter examples are given\nthat show regularization can increase the degrees of freedom in simple\nsituations, including lasso and ridge regression, which are the most common\nregularization approaches in use. In such situations, the regularization\nincreases both training error and degrees of freedom, and is thus inherently\nwithout merit. On the other hand, two important regularization scenarios are\ndescribed where the expected reduction in degrees of freedom is indeed\nguaranteed: (a) all symmetric linear smoothers, and (b) linear regression\nversus convex constrained linear regression (as in the constrained variant of\nridge regression and lasso).\n", "versions": [{"version": "v1", "created": "Tue, 12 Nov 2013 14:29:32 GMT"}], "update_date": "2013-11-13", "authors_parsed": [["Kaufman", "Shachar", ""], ["Rosset", "Saharon", ""]]}, {"id": "1311.2838", "submitter": "Christoph H. Lampert", "authors": "Anastasia Pentina and Christoph H. Lampert", "title": "A PAC-Bayesian bound for Lifelong Learning", "comments": "to appear at ICML 2014", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Transfer learning has received a lot of attention in the machine learning\ncommunity over the last years, and several effective algorithms have been\ndeveloped. However, relatively little is known about their theoretical\nproperties, especially in the setting of lifelong learning, where the goal is\nto transfer information to tasks for which no data have been observed so far.\nIn this work we study lifelong learning from a theoretical perspective. Our\nmain result is a PAC-Bayesian generalization bound that offers a unified view\non existing paradigms for transfer learning, such as the transfer of parameters\nor the transfer of low-dimensional representations. We also use the bound to\nderive two principled lifelong learning algorithms, and we show that these\nyield results comparable with existing methods.\n", "versions": [{"version": "v1", "created": "Tue, 12 Nov 2013 17:05:04 GMT"}, {"version": "v2", "created": "Sat, 10 May 2014 10:45:51 GMT"}], "update_date": "2014-05-13", "authors_parsed": [["Pentina", "Anastasia", ""], ["Lampert", "Christoph H.", ""]]}, {"id": "1311.2889", "submitter": "Adwaitvedant Mathkar", "authors": "Vivek S. Borkar and Adwaitvedant S. Mathkar", "title": "Reinforcement Learning for Matrix Computations: PageRank as an Example", "comments": "12 pages, 6 figures, invited lecture at ICDIT (International\n  Conference on Distributed Computing and Internet Technologies), 2014, will be\n  published in Lecture notes in Computer Science along with the conference\n  proceedings", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.SI stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Reinforcement learning has gained wide popularity as a technique for\nsimulation-driven approximate dynamic programming. A less known aspect is that\nthe very reasons that make it effective in dynamic programming can also be\nleveraged for using it for distributed schemes for certain matrix computations\ninvolving non-negative matrices. In this spirit, we propose a reinforcement\nlearning algorithm for PageRank computation that is fashioned after analogous\nschemes for approximate dynamic programming. The algorithm has the advantage of\nease of distributed implementation and more importantly, of being model-free,\ni.e., not dependent on any specific assumptions about the transition\nprobabilities in the random web-surfer model. We analyze its convergence and\nfinite time behavior and present some supporting numerical experiments.\n", "versions": [{"version": "v1", "created": "Fri, 1 Nov 2013 14:24:32 GMT"}], "update_date": "2013-11-13", "authors_parsed": [["Borkar", "Vivek S.", ""], ["Mathkar", "Adwaitvedant S.", ""]]}, {"id": "1311.2891", "submitter": "Joseph Anderson", "authors": "Joseph Anderson, Mikhail Belkin, Navin Goyal, Luis Rademacher, James\n  Voss", "title": "The More, the Merrier: the Blessing of Dimensionality for Learning Large\n  Gaussian Mixtures", "comments": "29 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.DS stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper we show that very large mixtures of Gaussians are efficiently\nlearnable in high dimension. More precisely, we prove that a mixture with known\nidentical covariance matrices whose number of components is a polynomial of any\nfixed degree in the dimension n is polynomially learnable as long as a certain\nnon-degeneracy condition on the means is satisfied. It turns out that this\ncondition is generic in the sense of smoothed complexity, as soon as the\ndimensionality of the space is high enough. Moreover, we prove that no such\ncondition can possibly exist in low dimension and the problem of learning the\nparameters is generically hard. In contrast, much of the existing work on\nGaussian Mixtures relies on low-dimensional projections and thus hits an\nartificial barrier. Our main result on mixture recovery relies on a new\n\"Poissonization\"-based technique, which transforms a mixture of Gaussians to a\nlinear map of a product distribution. The problem of learning this map can be\nefficiently solved using some recent results on tensor decompositions and\nIndependent Component Analysis (ICA), thus giving an algorithm for recovering\nthe mixture. In addition, we combine our low-dimensional hardness results for\nGaussian mixtures with Poissonization to show how to embed difficult instances\nof low-dimensional Gaussian mixtures into the ICA setting, thus establishing\nexponential information-theoretic lower bounds for underdetermined ICA in low\ndimension. To the best of our knowledge, this is the first such result in the\nliterature. In addition to contributing to the problem of Gaussian mixture\nlearning, we believe that this work is among the first steps toward better\nunderstanding the rare phenomenon of the \"blessing of dimensionality\" in the\ncomputational aspects of statistical inference.\n", "versions": [{"version": "v1", "created": "Tue, 12 Nov 2013 19:21:03 GMT"}, {"version": "v2", "created": "Mon, 17 Feb 2014 20:32:45 GMT"}, {"version": "v3", "created": "Tue, 18 Feb 2014 03:34:38 GMT"}], "update_date": "2014-02-19", "authors_parsed": [["Anderson", "Joseph", ""], ["Belkin", "Mikhail", ""], ["Goyal", "Navin", ""], ["Rademacher", "Luis", ""], ["Voss", "James", ""]]}, {"id": "1311.2971", "submitter": "Raja Hafiz Affandi", "authors": "Raja Hafiz Affandi, Emily B. Fox, Ben Taskar", "title": "Approximate Inference in Continuous Determinantal Point Processes", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Determinantal point processes (DPPs) are random point processes well-suited\nfor modeling repulsion. In machine learning, the focus of DPP-based models has\nbeen on diverse subset selection from a discrete and finite base set. This\ndiscrete setting admits an efficient sampling algorithm based on the\neigendecomposition of the defining kernel matrix. Recently, there has been\ngrowing interest in using DPPs defined on continuous spaces. While the\ndiscrete-DPP sampler extends formally to the continuous case, computationally,\nthe steps required are not tractable in general. In this paper, we present two\nefficient DPP sampling schemes that apply to a wide range of kernel functions:\none based on low rank approximations via Nystrom and random Fourier feature\ntechniques and another based on Gibbs sampling. We demonstrate the utility of\ncontinuous DPPs in repulsive mixture modeling and synthesizing human poses\nspanning activity spaces.\n", "versions": [{"version": "v1", "created": "Tue, 12 Nov 2013 22:15:26 GMT"}], "update_date": "2013-11-14", "authors_parsed": [["Affandi", "Raja Hafiz", ""], ["Fox", "Emily B.", ""], ["Taskar", "Ben", ""]]}, {"id": "1311.2972", "submitter": "Sewoong Oh", "authors": "Prateek Jain and Sewoong Oh", "title": "Learning Mixtures of Discrete Product Distributions using Spectral\n  Decompositions", "comments": "30 pages no figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.CC cs.IT cs.LG math.IT", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study the problem of learning a distribution from samples, when the\nunderlying distribution is a mixture of product distributions over discrete\ndomains. This problem is motivated by several practical applications such as\ncrowd-sourcing, recommendation systems, and learning Boolean functions. The\nexisting solutions either heavily rely on the fact that the number of\ncomponents in the mixtures is finite or have sample/time complexity that is\nexponential in the number of components. In this paper, we introduce a\npolynomial time/sample complexity method for learning a mixture of $r$ discrete\nproduct distributions over $\\{1, 2, \\dots, \\ell\\}^n$, for general $\\ell$ and\n$r$. We show that our approach is statistically consistent and further provide\nfinite sample guarantees.\n  We use techniques from the recent work on tensor decompositions for\nhigher-order moment matching. A crucial step in these moment matching methods\nis to construct a certain matrix and a certain tensor with low-rank spectral\ndecompositions. These tensors are typically estimated directly from the\nsamples. The main challenge in learning mixtures of discrete product\ndistributions is that these low-rank tensors cannot be obtained directly from\nthe sample moments. Instead, we reduce the tensor estimation problem to: $a$)\nestimating a low-rank matrix using only off-diagonal block elements; and $b$)\nestimating a tensor using a small number of linear measurements. Leveraging on\nrecent developments in matrix completion, we give an alternating minimization\nbased method to estimate the low-rank matrix, and formulate the tensor\ncompletion problem as a least-squares problem.\n", "versions": [{"version": "v1", "created": "Tue, 12 Nov 2013 22:15:35 GMT"}, {"version": "v2", "created": "Sat, 17 May 2014 19:38:34 GMT"}], "update_date": "2014-05-20", "authors_parsed": [["Jain", "Prateek", ""], ["Oh", "Sewoong", ""]]}, {"id": "1311.3001", "submitter": "Kevin H. Knuth", "authors": "Kevin H. Knuth", "title": "Informed Source Separation: A Bayesian Tutorial", "comments": "8 pages Knuth K.H. 2005. Informed source separation: A Bayesian\n  tutorial. (Invited paper) B. Sankur, E. Cetin, M. Tekalp, E. Kuruoglu (eds.),\n  Proceedings of the 13th European Signal Processing Conference (EUSIPCO 2005),\n  Antalya, Turkey", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Source separation problems are ubiquitous in the physical sciences; any\nsituation where signals are superimposed calls for source separation to\nestimate the original signals. In this tutorial I will discuss the Bayesian\napproach to the source separation problem. This approach has a specific\nadvantage in that it requires the designer to explicitly describe the signal\nmodel in addition to any other information or assumptions that go into the\nproblem description. This leads naturally to the idea of informed source\nseparation, where the algorithm design incorporates relevant information about\nthe specific problem. This approach promises to enable researchers to design\ntheir own high-quality algorithms that are specifically tailored to the problem\nat hand.\n", "versions": [{"version": "v1", "created": "Wed, 13 Nov 2013 02:23:34 GMT"}], "update_date": "2013-11-14", "authors_parsed": [["Knuth", "Kevin H.", ""]]}, {"id": "1311.3211", "submitter": "Mihai Alexandru Petrovici", "authors": "Mihai A. Petrovici, Johannes Bill, Ilja Bytschok, Johannes Schemmel,\n  Karlheinz Meier", "title": "Stochastic inference with deterministic spiking neurons", "comments": "6 pages, 4 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "q-bio.NC cond-mat.dis-nn cs.NE physics.bio-ph stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The seemingly stochastic transient dynamics of neocortical circuits observed\nin vivo have been hypothesized to represent a signature of ongoing stochastic\ninference. In vitro neurons, on the other hand, exhibit a highly deterministic\nresponse to various types of stimulation. We show that an ensemble of\ndeterministic leaky integrate-and-fire neurons embedded in a spiking noisy\nenvironment can attain the correct firing statistics in order to sample from a\nwell-defined target distribution. We provide an analytical derivation of the\nactivation function on the single cell level; for recurrent networks, we\nexamine convergence towards stationarity in computer simulations and\ndemonstrate sample-based Bayesian inference in a mixed graphical model. This\nestablishes a rigorous link between deterministic neuron models and functional\nstochastic dynamics on the network level.\n", "versions": [{"version": "v1", "created": "Wed, 13 Nov 2013 17:04:41 GMT"}], "update_date": "2017-03-14", "authors_parsed": [["Petrovici", "Mihai A.", ""], ["Bill", "Johannes", ""], ["Bytschok", "Ilja", ""], ["Schemmel", "Johannes", ""], ["Meier", "Karlheinz", ""]]}, {"id": "1311.3257", "submitter": "Alexander Jung", "authors": "Alexander Jung and Reinhard Heckel and Helmut B\\\"olcskei and Franz\n  Hlawatsch", "title": "Compressive Nonparametric Graphical Model Selection For Time Series", "comments": "to appear in Proc. IEEE ICASSP 2014", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a method for inferring the conditional indepen- dence graph (CIG)\nof a high-dimensional discrete-time Gaus- sian vector random process from\nfinite-length observations. Our approach does not rely on a parametric model\n(such as, e.g., an autoregressive model) for the vector random process; rather,\nit only assumes certain spectral smoothness proper- ties. The proposed\ninference scheme is compressive in that it works for sample sizes that are\n(much) smaller than the number of scalar process components. We provide\nanalytical conditions for our method to correctly identify the CIG with high\nprobability.\n", "versions": [{"version": "v1", "created": "Wed, 13 Nov 2013 19:12:55 GMT"}, {"version": "v2", "created": "Sat, 8 Mar 2014 16:49:40 GMT"}], "update_date": "2014-03-11", "authors_parsed": [["Jung", "Alexander", ""], ["Heckel", "Reinhard", ""], ["B\u00f6lcskei", "Helmut", ""], ["Hlawatsch", "Franz", ""]]}, {"id": "1311.3287", "submitter": "Le Song", "authors": "Le Song, Animashree Anandkumar, Bo Dai, Bo Xie", "title": "Nonparametric Estimation of Multi-View Latent Variable Models", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Spectral methods have greatly advanced the estimation of latent variable\nmodels, generating a sequence of novel and efficient algorithms with strong\ntheoretical guarantees. However, current spectral algorithms are largely\nrestricted to mixtures of discrete or Gaussian distributions. In this paper, we\npropose a kernel method for learning multi-view latent variable models,\nallowing each mixture component to be nonparametric. The key idea of the method\nis to embed the joint distribution of a multi-view latent variable into a\nreproducing kernel Hilbert space, and then the latent parameters are recovered\nusing a robust tensor power method. We establish that the sample complexity for\nthe proposed method is quadratic in the number of latent components and is a\nlow order polynomial in the other relevant parameters. Thus, our non-parametric\ntensor approach to learning latent variable models enjoys good sample and\ncomputational efficiencies. Moreover, the non-parametric tensor power method\ncompares favorably to EM algorithm and other existing spectral algorithms in\nour experiments.\n", "versions": [{"version": "v1", "created": "Wed, 13 Nov 2013 20:42:21 GMT"}, {"version": "v2", "created": "Sun, 8 Dec 2013 01:58:58 GMT"}], "update_date": "2013-12-10", "authors_parsed": [["Song", "Le", ""], ["Anandkumar", "Animashree", ""], ["Dai", "Bo", ""], ["Xie", "Bo", ""]]}, {"id": "1311.3315", "submitter": "Behnam Neyshabur", "authors": "Behnam Neyshabur, Rina Panigrahy", "title": "Sparse Matrix Factorization", "comments": "20 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We investigate the problem of factorizing a matrix into several sparse\nmatrices and propose an algorithm for this under randomness and sparsity\nassumptions. This problem can be viewed as a simplification of the deep\nlearning problem where finding a factorization corresponds to finding edges in\ndifferent layers and values of hidden units. We prove that under certain\nassumptions for a sparse linear deep network with $n$ nodes in each layer, our\nalgorithm is able to recover the structure of the network and values of top\nlayer hidden units for depths up to $\\tilde O(n^{1/6})$. We further discuss the\nrelation among sparse matrix factorization, deep learning, sparse recovery and\ndictionary learning.\n", "versions": [{"version": "v1", "created": "Wed, 13 Nov 2013 21:33:05 GMT"}, {"version": "v2", "created": "Fri, 29 Nov 2013 23:53:26 GMT"}, {"version": "v3", "created": "Tue, 13 May 2014 14:24:33 GMT"}], "update_date": "2014-05-14", "authors_parsed": [["Neyshabur", "Behnam", ""], ["Panigrahy", "Rina", ""]]}, {"id": "1311.3368", "submitter": "Sameer Singh", "authors": "Sameer Singh and Sebastian Riedel and Andrew McCallum", "title": "Anytime Belief Propagation Using Sparse Domains", "comments": "NIPS 2013 Workshop on Resource-Efficient Machine Learning", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.AI cs.LG", "license": "http://creativecommons.org/licenses/by/3.0/", "abstract": "  Belief Propagation has been widely used for marginal inference, however it is\nslow on problems with large-domain variables and high-order factors. Previous\nwork provides useful approximations to facilitate inference on such models, but\nlacks important anytime properties such as: 1) providing accurate and\nconsistent marginals when stopped early, 2) improving the approximation when\nrun longer, and 3) converging to the fixed point of BP. To this end, we propose\na message passing algorithm that works on sparse (partially instantiated)\ndomains, and converges to consistent marginals using dynamic message\nscheduling. The algorithm grows the sparse domains incrementally, selecting the\nnext value to add using prioritization schemes based on the gradients of the\nmarginal inference objective. Our experiments demonstrate local anytime\nconsistency and fast convergence, providing significant speedups over BP to\nobtain low-error marginals: up to 25 times on grid models, and up to 6 times on\na real-world natural language processing task.\n", "versions": [{"version": "v1", "created": "Thu, 14 Nov 2013 02:39:45 GMT"}], "update_date": "2013-11-15", "authors_parsed": [["Singh", "Sameer", ""], ["Riedel", "Sebastian", ""], ["McCallum", "Andrew", ""]]}, {"id": "1311.3492", "submitter": "Po-Ling Loh", "authors": "Po-Ling Loh and Peter B\\\"uhlmann", "title": "High-dimensional learning of linear causal networks via inverse\n  covariance estimation", "comments": "41 pages, 7 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We establish a new framework for statistical estimation of directed acyclic\ngraphs (DAGs) when data are generated from a linear, possibly non-Gaussian\nstructural equation model. Our framework consists of two parts: (1) inferring\nthe moralized graph from the support of the inverse covariance matrix; and (2)\nselecting the best-scoring graph amongst DAGs that are consistent with the\nmoralized graph. We show that when the error variances are known or estimated\nto close enough precision, the true DAG is the unique minimizer of the score\ncomputed using the reweighted squared l_2-loss. Our population-level results\nhave implications for the identifiability of linear SEMs when the error\ncovariances are specified up to a constant multiple. On the statistical side,\nwe establish rigorous conditions for high-dimensional consistency of our\ntwo-part algorithm, defined in terms of a \"gap\" between the true DAG and the\nnext best candidate. Finally, we demonstrate that dynamic programming may be\nused to select the optimal DAG in linear time when the treewidth of the\nmoralized graph is bounded.\n", "versions": [{"version": "v1", "created": "Thu, 14 Nov 2013 13:08:51 GMT"}], "update_date": "2013-11-15", "authors_parsed": [["Loh", "Po-Ling", ""], ["B\u00fchlmann", "Peter", ""]]}, {"id": "1311.3494", "submitter": "Ohad Shamir", "authors": "Ohad Shamir", "title": "Fundamental Limits of Online and Distributed Algorithms for Statistical\n  Learning and Estimation", "comments": "Full version of NIPS 2014 paper", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Many machine learning approaches are characterized by information constraints\non how they interact with the training data. These include memory and\nsequential access constraints (e.g. fast first-order methods to solve\nstochastic optimization problems); communication constraints (e.g. distributed\nlearning); partial access to the underlying data (e.g. missing features and\nmulti-armed bandits) and more. However, currently we have little understanding\nhow such information constraints fundamentally affect our performance,\nindependent of the learning problem semantics. For example, are there learning\nproblems where any algorithm which has small memory footprint (or can use any\nbounded number of bits from each example, or has certain communication\nconstraints) will perform worse than what is possible without such constraints?\nIn this paper, we describe how a single set of results implies positive answers\nto the above, for several different settings.\n", "versions": [{"version": "v1", "created": "Thu, 14 Nov 2013 13:21:15 GMT"}, {"version": "v2", "created": "Wed, 5 Feb 2014 14:55:06 GMT"}, {"version": "v3", "created": "Thu, 6 Feb 2014 06:23:28 GMT"}, {"version": "v4", "created": "Tue, 6 May 2014 10:56:31 GMT"}, {"version": "v5", "created": "Wed, 21 May 2014 18:35:13 GMT"}, {"version": "v6", "created": "Tue, 28 Oct 2014 13:25:09 GMT"}], "update_date": "2014-10-29", "authors_parsed": [["Shamir", "Ohad", ""]]}, {"id": "1311.3576", "submitter": "Javier  Goz\\'alez", "authors": "Javier Gonz\\'alez, Ivan Vuja\\v{c}i\\'c, Ernst Wit", "title": "Reproducing kernel Hilbert space based estimation of systems of ordinary\n  differential equations", "comments": "16 pages, 6 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Non-linear systems of differential equations have attracted the interest in\nfields like system biology, ecology or biochemistry, due to their flexibility\nand their ability to describe dynamical systems. Despite the importance of such\nmodels in many branches of science they have not been the focus of systematic\nstatistical analysis until recently. In this work we propose a general approach\nto estimate the parameters of systems of differential equations measured with\nnoise. Our methodology is based on the maximization of the penalized likelihood\nwhere the system of differential equations is used as a penalty. To do so, we\nuse a Reproducing Kernel Hilbert Space approach that allows to formulate the\nestimation problem as an unconstrained numeric maximization problem easy to\nsolve. The proposed method is tested with synthetically simulated data and it\nis used to estimate the unobserved transcription factor CdaR in Steptomyes\ncoelicolor using gene expression data of the genes it regulates.\n", "versions": [{"version": "v1", "created": "Thu, 14 Nov 2013 16:56:34 GMT"}, {"version": "v2", "created": "Thu, 8 May 2014 15:56:54 GMT"}], "update_date": "2014-05-09", "authors_parsed": [["Gonz\u00e1lez", "Javier", ""], ["Vuja\u010di\u0107", "Ivan", ""], ["Wit", "Ernst", ""]]}, {"id": "1311.3651", "submitter": "Aravindan Vijayaraghavan", "authors": "Aditya Bhaskara, Moses Charikar, Ankur Moitra and Aravindan\n  Vijayaraghavan", "title": "Smoothed Analysis of Tensor Decompositions", "comments": "32 pages (including appendix)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Low rank tensor decompositions are a powerful tool for learning generative\nmodels, and uniqueness results give them a significant advantage over matrix\ndecomposition methods. However, tensors pose significant algorithmic challenges\nand tensors analogs of much of the matrix algebra toolkit are unlikely to exist\nbecause of hardness results. Efficient decomposition in the overcomplete case\n(where rank exceeds dimension) is particularly challenging. We introduce a\nsmoothed analysis model for studying these questions and develop an efficient\nalgorithm for tensor decomposition in the highly overcomplete case (rank\npolynomial in the dimension). In this setting, we show that our algorithm is\nrobust to inverse polynomial error -- a crucial property for applications in\nlearning since we are only allowed a polynomial number of samples. While\nalgorithms are known for exact tensor decomposition in some overcomplete\nsettings, our main contribution is in analyzing their stability in the\nframework of smoothed analysis.\n  Our main technical contribution is to show that tensor products of perturbed\nvectors are linearly independent in a robust sense (i.e. the associated matrix\nhas singular values that are at least an inverse polynomial). This key result\npaves the way for applying tensor methods to learning problems in the smoothed\nsetting. In particular, we use it to obtain results for learning multi-view\nmodels and mixtures of axis-aligned Gaussians where there are many more\n\"components\" than dimensions. The assumption here is that the model is not\nadversarially chosen, formalized by a perturbation of model parameters. We\nbelieve this an appealing way to analyze realistic instances of learning\nproblems, since this framework allows us to overcome many of the usual\nlimitations of using tensor methods.\n", "versions": [{"version": "v1", "created": "Thu, 14 Nov 2013 20:49:55 GMT"}, {"version": "v2", "created": "Sun, 17 Nov 2013 15:28:47 GMT"}, {"version": "v3", "created": "Fri, 3 Jan 2014 21:09:31 GMT"}, {"version": "v4", "created": "Mon, 20 Jan 2014 06:19:39 GMT"}], "update_date": "2014-01-21", "authors_parsed": [["Bhaskara", "Aditya", ""], ["Charikar", "Moses", ""], ["Moitra", "Ankur", ""], ["Vijayaraghavan", "Aravindan", ""]]}, {"id": "1311.3709", "submitter": "Noah Simon", "authors": "Noah Simon, Richard Simon", "title": "On Estimating Many Means, Selection Bias, and the Bootstrap", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  With recent advances in high throughput technology, researchers often find\nthemselves running a large number of hypothesis tests (thousands+) and esti-\nmating a large number of effect-sizes. Generally there is particular interest\nin those effects estimated to be most extreme. Unfortunately naive estimates of\nthese effect-sizes (even after potentially accounting for multiplicity in a\ntesting procedure) can be severely biased. In this manuscript we explore this\nbias from a frequentist perspective: we give a formal definition, and show that\nan oracle estimator using this bias dominates the naive maximum likelihood\nestimate. We give a resampling estimator to approximate this oracle, and show\nthat it works well on simulated data. We also connect this to ideas in\nempirical Bayes.\n", "versions": [{"version": "v1", "created": "Fri, 15 Nov 2013 01:33:48 GMT"}], "update_date": "2013-11-18", "authors_parsed": [["Simon", "Noah", ""], ["Simon", "Richard", ""]]}, {"id": "1311.3755", "submitter": "Gaurav Thakur", "authors": "Gaurav Thakur", "title": "Deterministic Bayesian Information Fusion and the Analysis of its\n  Performance", "comments": null, "journal-ref": "Information and Inference 3(4):345-366 (2014)", "doi": "10.1093/imaiai/iau009", "report-no": null, "categories": "math.ST stat.ML stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper develops a mathematical and computational framework for analyzing\nthe expected performance of Bayesian data fusion, or joint statistical\ninference, within a sensor network. We use variational techniques to obtain the\nposterior expectation as the optimal fusion rule under a deterministic\nconstraint and a quadratic cost, and study the smoothness and other properties\nof its classification performance. For a certain class of fusion problems, we\nprove that this fusion rule is also optimal in a much wider sense and satisfies\nstrong asymptotic convergence results. We show how these results apply to a\nvariety of examples with Gaussian, exponential and other statistics, and\ndiscuss computational methods for determining the fusion system's performance\nin more general, large-scale problems. These results are motivated by studying\nthe performance of fusing multi-modal radar and acoustic sensors for detecting\nexplosive substances, but have broad applicability to other Bayesian decision\nproblems.\n", "versions": [{"version": "v1", "created": "Fri, 15 Nov 2013 07:46:54 GMT"}, {"version": "v2", "created": "Wed, 2 Apr 2014 01:26:16 GMT"}, {"version": "v3", "created": "Sat, 4 Oct 2014 17:30:11 GMT"}, {"version": "v4", "created": "Sun, 2 Nov 2014 13:51:59 GMT"}], "update_date": "2016-02-23", "authors_parsed": [["Thakur", "Gaurav", ""]]}, {"id": "1311.3859", "submitter": "Yannick Schwartz", "authors": "Yannick Schwartz (INRIA Saclay - Ile de France, NEUROSPIN), Bertrand\n  Thirion (INRIA Saclay - Ile de France, NEUROSPIN), Ga\\\"el Varoquaux (INRIA\n  Saclay - Ile de France, LNAO)", "title": "Mapping cognitive ontologies to and from the brain", "comments": "NIPS (Neural Information Processing Systems), United States (2013)", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG q-bio.NC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Imaging neuroscience links brain activation maps to behavior and cognition\nvia correlational studies. Due to the nature of the individual experiments,\nbased on eliciting neural response from a small number of stimuli, this link is\nincomplete, and unidirectional from the causal point of view. To come to\nconclusions on the function implied by the activation of brain regions, it is\nnecessary to combine a wide exploration of the various brain functions and some\ninversion of the statistical inference. Here we introduce a methodology for\naccumulating knowledge towards a bidirectional link between observed brain\nactivity and the corresponding function. We rely on a large corpus of imaging\nstudies and a predictive engine. Technically, the challenges are to find\ncommonality between the studies without denaturing the richness of the corpus.\nThe key elements that we contribute are labeling the tasks performed with a\ncognitive ontology, and modeling the long tail of rare paradigms in the corpus.\nTo our knowledge, our approach is the first demonstration of predicting the\ncognitive content of completely new brain images. To that end, we propose a\nmethod that predicts the experimental paradigms across different studies.\n", "versions": [{"version": "v1", "created": "Fri, 15 Nov 2013 14:19:31 GMT"}, {"version": "v2", "created": "Wed, 20 Nov 2013 12:26:50 GMT"}], "update_date": "2013-11-21", "authors_parsed": [["Schwartz", "Yannick", "", "INRIA Saclay - Ile de France, NEUROSPIN"], ["Thirion", "Bertrand", "", "INRIA Saclay - Ile de France, NEUROSPIN"], ["Varoquaux", "Ga\u00ebl", "", "INRIA\n  Saclay - Ile de France, LNAO"]]}, {"id": "1311.3995", "submitter": "Zhilin Zhang", "authors": "Zhilin Zhang, Bhaskar D. Rao, Tzyy-Ping Jung", "title": "Compressed Sensing for Energy-Efficient Wireless Telemonitoring:\n  Challenges and Opportunities", "comments": "Invited paper for 2013 Asilomar Conference on Signals, Systems &\n  Computers (Asilomar 2013)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IT math.IT stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  As a lossy compression framework, compressed sensing has drawn much attention\nin wireless telemonitoring of biosignals due to its ability to reduce energy\nconsumption and make possible the design of low-power devices. However, the\nnon-sparseness of biosignals presents a major challenge to compressed sensing.\nThis study proposes and evaluates a spatio-temporal sparse Bayesian learning\nalgorithm, which has the desired ability to recover such non-sparse biosignals.\nIt exploits both temporal correlation in each individual biosignal and\ninter-channel correlation among biosignals from different channels. The\nproposed algorithm was used for compressed sensing of multichannel\nelectroencephalographic (EEG) signals for estimating vehicle drivers'\ndrowsiness. Results showed that the drowsiness estimation was almost unaffected\neven if raw EEG signals (containing various artifacts) were compressed by 90%.\n", "versions": [{"version": "v1", "created": "Fri, 15 Nov 2013 22:55:24 GMT"}, {"version": "v2", "created": "Mon, 21 Apr 2014 04:45:55 GMT"}], "update_date": "2014-04-22", "authors_parsed": [["Zhang", "Zhilin", ""], ["Rao", "Bhaskar D.", ""], ["Jung", "Tzyy-Ping", ""]]}, {"id": "1311.4025", "submitter": "Joan Bruna", "authors": "Joan Bruna, Arthur Szlam, Yann LeCun", "title": "Signal Recovery from Pooling Representations", "comments": "17 pages, 3 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this work we compute lower Lipschitz bounds of $\\ell_p$ pooling operators\nfor $p=1, 2, \\infty$ as well as $\\ell_p$ pooling operators preceded by\nhalf-rectification layers. These give sufficient conditions for the design of\ninvertible neural network layers. Numerical experiments on MNIST and image\npatches confirm that pooling layers can be inverted with phase recovery\nalgorithms. Moreover, the regularity of the inverse pooling, controlled by the\nlower Lipschitz constant, is empirically verified with a nearest neighbor\nregression.\n", "versions": [{"version": "v1", "created": "Sat, 16 Nov 2013 06:53:44 GMT"}, {"version": "v2", "created": "Mon, 3 Feb 2014 17:25:10 GMT"}, {"version": "v3", "created": "Thu, 27 Feb 2014 22:36:08 GMT"}], "update_date": "2014-03-03", "authors_parsed": [["Bruna", "Joan", ""], ["Szlam", "Arthur", ""], ["LeCun", "Yann", ""]]}, {"id": "1311.4150", "submitter": "Jian-Feng Yan", "authors": "Jian-Feng Yan, Jia Zeng, Zhi-Qiang Liu, Yang Gao", "title": "Towards Big Topic Modeling", "comments": "14 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.DC cs.IR stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  To solve the big topic modeling problem, we need to reduce both time and\nspace complexities of batch latent Dirichlet allocation (LDA) algorithms.\nAlthough parallel LDA algorithms on the multi-processor architecture have low\ntime and space complexities, their communication costs among processors often\nscale linearly with the vocabulary size and the number of topics, leading to a\nserious scalability problem. To reduce the communication complexity among\nprocessors for a better scalability, we propose a novel communication-efficient\nparallel topic modeling architecture based on power law, which consumes orders\nof magnitude less communication time when the number of topics is large. We\ncombine the proposed communication-efficient parallel architecture with the\nonline belief propagation (OBP) algorithm referred to as POBP for big topic\nmodeling tasks. Extensive empirical results confirm that POBP has the following\nadvantages to solve the big topic modeling problem: 1) high accuracy, 2)\ncommunication-efficient, 3) fast speed, and 4) constant memory usage when\ncompared with recent state-of-the-art parallel LDA algorithms on the\nmulti-processor architecture.\n", "versions": [{"version": "v1", "created": "Sun, 17 Nov 2013 11:52:42 GMT"}], "update_date": "2013-11-19", "authors_parsed": [["Yan", "Jian-Feng", ""], ["Zeng", "Jia", ""], ["Liu", "Zhi-Qiang", ""], ["Gao", "Yang", ""]]}, {"id": "1311.4291", "submitter": "Lei Yang", "authors": "Min Zhang, Lei Yang and Zheng-Hai Huang", "title": "Minimum $n$-Rank Approximation via Iterative Hard Thresholding", "comments": "Iterative hard thresholding; low-$n$-rank tensor recovery; tensor\n  completion; compressed sensing", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.OC stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The problem of recovering a low $n$-rank tensor is an extension of sparse\nrecovery problem from the low dimensional space (matrix space) to the high\ndimensional space (tensor space) and has many applications in computer vision\nand graphics such as image inpainting and video inpainting. In this paper, we\nconsider a new tensor recovery model, named as minimum $n$-rank approximation\n(MnRA), and propose an appropriate iterative hard thresholding algorithm with\ngiving the upper bound of the $n$-rank in advance. The convergence analysis of\nthe proposed algorithm is also presented. Particularly, we show that for the\nnoiseless case, the linear convergence with rate $\\frac{1}{2}$ can be obtained\nfor the proposed algorithm under proper conditions. Additionally, combining an\neffective heuristic for determining $n$-rank, we can also apply the proposed\nalgorithm to solve MnRA when $n$-rank is unknown in advance. Some preliminary\nnumerical results on randomly generated and real low $n$-rank tensor completion\nproblems are reported, which show the efficiency of the proposed algorithms.\n", "versions": [{"version": "v1", "created": "Mon, 18 Nov 2013 08:33:00 GMT"}, {"version": "v2", "created": "Tue, 8 Apr 2014 13:48:43 GMT"}], "update_date": "2014-04-09", "authors_parsed": [["Zhang", "Min", ""], ["Yang", "Lei", ""], ["Huang", "Zheng-Hai", ""]]}, {"id": "1311.4468", "submitter": "Jan-Peter Calliess", "authors": "Jan-Peter Calliess, Antonis Papachristodoulou and Stephen J. Roberts", "title": "Stochastic processes and feedback-linearisation for online\n  identification and Bayesian adaptive control of fully-actuated mechanical\n  systems", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.SY physics.data-an stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This work proposes a new method for simultaneous probabilistic identification\nand control of an observable, fully-actuated mechanical system. Identification\nis achieved by conditioning stochastic process priors on observations of\nconfigurations and noisy estimates of configuration derivatives. In contrast to\nprevious work that has used stochastic processes for identification, we\nleverage the structural knowledge afforded by Lagrangian mechanics and learn\nthe drift and control input matrix functions of the control-affine system\nseparately. We utilise feedback-linearisation to reduce, in expectation, the\nuncertain nonlinear control problem to one that is easy to regulate in a\ndesired manner. Thereby, our method combines the flexibility of nonparametric\nBayesian learning with epistemological guarantees on the expected closed-loop\ntrajectory. We illustrate our method in the context of torque-actuated pendula\nwhere the dynamics are learned with a combination of normal and log-normal\nprocesses.\n", "versions": [{"version": "v1", "created": "Mon, 18 Nov 2013 17:31:48 GMT"}, {"version": "v2", "created": "Tue, 26 Nov 2013 23:43:11 GMT"}, {"version": "v3", "created": "Tue, 1 Apr 2014 15:52:02 GMT"}], "update_date": "2014-04-02", "authors_parsed": [["Calliess", "Jan-Peter", ""], ["Papachristodoulou", "Antonis", ""], ["Roberts", "Stephen J.", ""]]}, {"id": "1311.4472", "submitter": "Rob Tibshirani", "authors": "Nadine Hussami and Robert Tibshirani", "title": "A Component Lasso", "comments": "18 pages, 4 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a new sparse regression method called the component lasso, based\non a simple idea. The method uses the connected-components structure of the\nsample covariance matrix to split the problem into smaller ones. It then solves\nthe subproblems separately, obtaining a coefficient vector for each one. Then,\nit uses non-negative least squares to recombine the different vectors into a\nsingle solution. This step is useful in selecting and reweighting components\nthat are correlated with the response. Simulated and real data examples show\nthat the component lasso can outperform standard regression methods such as the\nlasso and elastic net, achieving a lower mean squared error as well as better\nsupport recovery.\n", "versions": [{"version": "v1", "created": "Mon, 18 Nov 2013 17:56:28 GMT"}, {"version": "v2", "created": "Fri, 6 Dec 2013 22:02:14 GMT"}], "update_date": "2013-12-10", "authors_parsed": [["Hussami", "Nadine", ""], ["Tibshirani", "Robert", ""]]}, {"id": "1311.4555", "submitter": "Stefan Wager", "authors": "Stefan Wager, Trevor Hastie, and Bradley Efron", "title": "Confidence Intervals for Random Forests: The Jackknife and the\n  Infinitesimal Jackknife", "comments": "To appear in Journal of Machine Learning Research (JMLR)", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML stat.CO stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study the variability of predictions made by bagged learners and random\nforests, and show how to estimate standard errors for these methods. Our work\nbuilds on variance estimates for bagging proposed by Efron (1992, 2012) that\nare based on the jackknife and the infinitesimal jackknife (IJ). In practice,\nbagged predictors are computed using a finite number B of bootstrap replicates,\nand working with a large B can be computationally expensive. Direct\napplications of jackknife and IJ estimators to bagging require B on the order\nof n^{1.5} bootstrap replicates to converge, where n is the size of the\ntraining set. We propose improved versions that only require B on the order of\nn replicates. Moreover, we show that the IJ estimator requires 1.7 times less\nbootstrap replicates than the jackknife to achieve a given accuracy. Finally,\nwe study the sampling distributions of the jackknife and IJ variance estimates\nthemselves. We illustrate our findings with multiple experiments and simulation\nstudies.\n", "versions": [{"version": "v1", "created": "Mon, 18 Nov 2013 21:03:31 GMT"}, {"version": "v2", "created": "Sat, 29 Mar 2014 00:06:16 GMT"}], "update_date": "2014-04-01", "authors_parsed": [["Wager", "Stefan", ""], ["Hastie", "Trevor", ""], ["Efron", "Bradley", ""]]}, {"id": "1311.4643", "submitter": "Zohar Karnin", "authors": "Dimitris Achlioptas, Zohar Karnin, Edo Liberty", "title": "Near-Optimal Entrywise Sampling for Data Matrices", "comments": "14 pages, to appear in NIPS' 13", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.IT cs.NA math.IT stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider the problem of selecting non-zero entries of a matrix $A$ in\norder to produce a sparse sketch of it, $B$, that minimizes $\\|A-B\\|_2$. For\nlarge $m \\times n$ matrices, such that $n \\gg m$ (for example, representing $n$\nobservations over $m$ attributes) we give sampling distributions that exhibit\nfour important properties. First, they have closed forms computable from\nminimal information regarding $A$. Second, they allow sketching of matrices\nwhose non-zeros are presented to the algorithm in arbitrary order as a stream,\nwith $O(1)$ computation per non-zero. Third, the resulting sketch matrices are\nnot only sparse, but their non-zero entries are highly compressible. Lastly,\nand most importantly, under mild assumptions, our distributions are provably\ncompetitive with the optimal offline distribution. Note that the probabilities\nin the optimal offline distribution may be complex functions of all the entries\nin the matrix. Therefore, regardless of computational complexity, the optimal\ndistribution might be impossible to compute in the streaming model.\n", "versions": [{"version": "v1", "created": "Tue, 19 Nov 2013 08:00:50 GMT"}], "update_date": "2013-11-20", "authors_parsed": [["Achlioptas", "Dimitris", ""], ["Karnin", "Zohar", ""], ["Liberty", "Edo", ""]]}, {"id": "1311.4669", "submitter": "Daniele Durante", "authors": "Daniele Durante, David B. Dunson", "title": "Nonparametric Bayes dynamic modeling of relational data", "comments": null, "journal-ref": "Biometrika (2014). 101, 883-898", "doi": "10.1093/biomet/asu040", "report-no": null, "categories": "stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Symmetric binary matrices representing relations among entities are commonly\ncollected in many areas. Our focus is on dynamically evolving binary relational\nmatrices, with interest being in inference on the relationship structure and\nprediction. We propose a nonparametric Bayesian dynamic model, which reduces\ndimensionality in characterizing the binary matrix through a lower-dimensional\nlatent space representation, with the latent coordinates evolving in continuous\ntime via Gaussian processes. By using a logistic mapping function from the\nprobability matrix space to the latent relational space, we obtain a flexible\nand computational tractable formulation. Employing P\\`olya-Gamma data\naugmentation, an efficient Gibbs sampler is developed for posterior\ncomputation, with the dimension of the latent space automatically inferred. We\nprovide some theoretical results on flexibility of the model, and illustrate\nperformance via simulation experiments. We also consider an application to\nco-movements in world financial markets.\n", "versions": [{"version": "v1", "created": "Tue, 19 Nov 2013 09:39:26 GMT"}], "update_date": "2018-09-11", "authors_parsed": [["Durante", "Daniele", ""], ["Dunson", "David B.", ""]]}, {"id": "1311.4780", "submitter": "Willie Neiswanger", "authors": "Willie Neiswanger, Chong Wang, Eric Xing", "title": "Asymptotically Exact, Embarrassingly Parallel MCMC", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.DC cs.LG stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Communication costs, resulting from synchronization requirements during\nlearning, can greatly slow down many parallel machine learning algorithms. In\nthis paper, we present a parallel Markov chain Monte Carlo (MCMC) algorithm in\nwhich subsets of data are processed independently, with very little\ncommunication. First, we arbitrarily partition data onto multiple machines.\nThen, on each machine, any classical MCMC method (e.g., Gibbs sampling) may be\nused to draw samples from a posterior distribution given the data subset.\nFinally, the samples from each machine are combined to form samples from the\nfull posterior. This embarrassingly parallel algorithm allows each machine to\nact independently on a subset of the data (without communication) until the\nfinal combination stage. We prove that our algorithm generates asymptotically\nexact samples and empirically demonstrate its ability to parallelize burn-in\nand sampling in several models.\n", "versions": [{"version": "v1", "created": "Tue, 19 Nov 2013 15:23:04 GMT"}, {"version": "v2", "created": "Fri, 21 Mar 2014 04:25:50 GMT"}], "update_date": "2014-03-24", "authors_parsed": [["Neiswanger", "Willie", ""], ["Wang", "Chong", ""], ["Xing", "Eric", ""]]}, {"id": "1311.4803", "submitter": "Lijun Zhang", "authors": "Lijun Zhang and Mehrdad Mahdavi and Rong Jin", "title": "Beating the Minimax Rate of Active Learning with Prior Knowledge", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Active learning refers to the learning protocol where the learner is allowed\nto choose a subset of instances for labeling. Previous studies have shown that,\ncompared with passive learning, active learning is able to reduce the label\ncomplexity exponentially if the data are linearly separable or satisfy the\nTsybakov noise condition with parameter $\\kappa=1$. In this paper, we propose a\nnovel active learning algorithm using a convex surrogate loss, with the goal to\nbroaden the cases for which active learning achieves an exponential\nimprovement. We make use of a convex loss not only because it reduces the\ncomputational cost, but more importantly because it leads to a tight bound for\nthe empirical process (i.e., the difference between the empirical estimation\nand the expectation) when the current solution is close to the optimal one.\nUnder the assumption that the norm of the optimal classifier that minimizes the\nconvex risk is available, our analysis shows that the introduction of the\nconvex surrogate loss yields an exponential reduction in the label complexity\neven when the parameter $\\kappa$ of the Tsybakov noise is larger than $1$. To\nthe best of our knowledge, this is the first work that improves the minimax\nrate of active learning by utilizing certain priori knowledge.\n", "versions": [{"version": "v1", "created": "Tue, 19 Nov 2013 16:56:55 GMT"}, {"version": "v2", "created": "Thu, 6 Feb 2014 20:07:49 GMT"}], "update_date": "2014-02-07", "authors_parsed": [["Zhang", "Lijun", ""], ["Mahdavi", "Mehrdad", ""], ["Jin", "Rong", ""]]}, {"id": "1311.4825", "submitter": "Emile Contal", "authors": "Emile Contal, Vianney Perchet, Nicolas Vayatis", "title": "Gaussian Process Optimization with Mutual Information", "comments": "Proceedings of The 31st International Conference on Machine Learning\n  (ICML 2014)", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we analyze a generic algorithm scheme for sequential global\noptimization using Gaussian processes. The upper bounds we derive on the\ncumulative regret for this generic algorithm improve by an exponential factor\nthe previously known bounds for algorithms like GP-UCB. We also introduce the\nnovel Gaussian Process Mutual Information algorithm (GP-MI), which\nsignificantly improves further these upper bounds for the cumulative regret. We\nconfirm the efficiency of this algorithm on synthetic and real tasks against\nthe natural competitor, GP-UCB, and also the Expected Improvement heuristic.\n", "versions": [{"version": "v1", "created": "Tue, 19 Nov 2013 18:29:19 GMT"}, {"version": "v2", "created": "Tue, 20 May 2014 18:25:19 GMT"}, {"version": "v3", "created": "Mon, 8 Jun 2015 13:27:19 GMT"}], "update_date": "2015-06-09", "authors_parsed": [["Contal", "Emile", ""], ["Perchet", "Vianney", ""], ["Vayatis", "Nicolas", ""]]}, {"id": "1311.4833", "submitter": "Emilie Morvant", "authors": "Emilie Morvant (IST Austria)", "title": "Domain Adaptation of Majority Votes via Perturbed Variation-based Label\n  Transfer", "comments": null, "journal-ref": "Pattern Recognition Letters 2015", "doi": "10.1016/j.patrec.2014.08.013", "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We tackle the PAC-Bayesian Domain Adaptation (DA) problem. This arrives when\none desires to learn, from a source distribution, a good weighted majority vote\n(over a set of classifiers) on a different target distribution. In this\ncontext, the disagreement between classifiers is known crucial to control. In\nnon-DA supervised setting, a theoretical bound - the C-bound - involves this\ndisagreement and leads to a majority vote learning algorithm: MinCq. In this\nwork, we extend MinCq to DA by taking advantage of an elegant divergence\nbetween distribution called the Perturbed Varation (PV). Firstly, justified by\na new formulation of the C-bound, we provide to MinCq a target sample labeled\nthanks to a PV-based self-labeling focused on regions where the source and\ntarget marginal distributions are closer. Secondly, we propose an original\nprocess for tuning the hyperparameters. Our framework shows very promising\nresults on a toy problem.\n", "versions": [{"version": "v1", "created": "Tue, 19 Nov 2013 18:46:59 GMT"}], "update_date": "2016-06-24", "authors_parsed": [["Morvant", "Emilie", "", "IST Austria"]]}, {"id": "1311.4924", "submitter": "Yipeng Liu Prof.", "authors": "Yipeng Liu", "title": "Robust Compressed Sensing Under Matrix Uncertainties", "comments": "17 pages, 8 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IT cs.CV math.IT math.RT stat.AP stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Compressed sensing (CS) shows that a signal having a sparse or compressible\nrepresentation can be recovered from a small set of linear measurements. In\nclassical CS theory, the sampling matrix and representation matrix are assumed\nto be known exactly in advance. However, uncertainties exist due to sampling\ndistortion, finite grids of the parameter space of dictionary, etc. In this\npaper, we take a generalized sparse signal model, which simultaneously\nconsiders the sampling and representation matrix uncertainties. Based on the\nnew signal model, a new optimization model for robust sparse signal\nreconstruction is proposed. This optimization model can be deduced with\nstochastic robust approximation analysis. Both convex relaxation and greedy\nalgorithms are used to solve the optimization problem. For the convex\nrelaxation method, a sufficient condition for recovery by convex relaxation is\ngiven; For the greedy algorithm, it is realized by the introduction of a\npre-processing of the sensing matrix and the measurements. In numerical\nexperiments, both simulated data and real-life ECG data based results show that\nthe proposed method has a better performance than the current methods.\n", "versions": [{"version": "v1", "created": "Wed, 20 Nov 2013 00:20:50 GMT"}, {"version": "v2", "created": "Tue, 18 Mar 2014 10:35:33 GMT"}, {"version": "v3", "created": "Wed, 19 Mar 2014 17:17:46 GMT"}, {"version": "v4", "created": "Thu, 2 Jul 2015 15:23:32 GMT"}], "update_date": "2015-07-03", "authors_parsed": [["Liu", "Yipeng", ""]]}, {"id": "1311.5179", "submitter": "Yash Deshpande", "authors": "Yash Deshpande and Andrea Montanari", "title": "Sparse PCA via Covariance Thresholding", "comments": "40 pages, 3 figures, preprint", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.ML stat.TH", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  In sparse principal component analysis we are given noisy observations of a\nlow-rank matrix of dimension $n\\times p$ and seek to reconstruct it under\nadditional sparsity assumptions. In particular, we assume here each of the\nprincipal components $\\mathbf{v}_1,\\dots,\\mathbf{v}_r$ has at most $s_0$\nnon-zero entries. We are particularly interested in the high dimensional regime\nwherein $p$ is comparable to, or even much larger than $n$. In an influential\npaper, \\cite{johnstone2004sparse} introduced a simple algorithm that estimates\nthe support of the principal vectors $\\mathbf{v}_1,\\dots,\\mathbf{v}_r$ by the\nlargest entries in the diagonal of the empirical covariance. This method can be\nshown to identify the correct support with high probability if $s_0\\le\nK_1\\sqrt{n/\\log p}$, and to fail with high probability if $s_0\\ge K_2\n\\sqrt{n/\\log p}$ for two constants $0<K_1,K_2<\\infty$. Despite a considerable\namount of work over the last ten years, no practical algorithm exists with\nprovably better support recovery guarantees.\n  Here we analyze a covariance thresholding algorithm that was recently\nproposed by \\cite{KrauthgamerSPCA}. On the basis of numerical simulations (for\nthe rank-one case), these authors conjectured that covariance thresholding\ncorrectly recover the support with high probability for $s_0\\le K\\sqrt{n}$\n(assuming $n$ of the same order as $p$). We prove this conjecture, and in fact\nestablish a more general guarantee including higher-rank as well as $n$ much\nsmaller than $p$. Recent lower bounds \\cite{berthet2013computational,\nma2015sum} suggest that no polynomial time algorithm can do significantly\nbetter. The key technical component of our analysis develops new bounds on the\nnorm of kernel random matrices, in regimes that were not considered before.\n", "versions": [{"version": "v1", "created": "Wed, 20 Nov 2013 19:21:02 GMT"}, {"version": "v2", "created": "Tue, 28 Jan 2014 00:10:09 GMT"}, {"version": "v3", "created": "Tue, 28 Oct 2014 05:39:21 GMT"}, {"version": "v4", "created": "Tue, 4 Nov 2014 02:43:38 GMT"}, {"version": "v5", "created": "Mon, 25 Apr 2016 22:43:29 GMT"}], "update_date": "2016-04-27", "authors_parsed": [["Deshpande", "Yash", ""], ["Montanari", "Andrea", ""]]}, {"id": "1311.5406", "submitter": "Gustavo Camps-Valls", "authors": "Jos\\'e Luis Rojo-\\'Alvarez, Manel Mart\\'inez-Ram\\'on, Jordi\n  Mu\\~noz-Mar\\'i and Gustavo Camps-Valls", "title": "A Unified SVM Framework for Signal Estimation", "comments": "22 pages, 13 figures. Digital Signal Processing, 2014", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper presents a unified framework to tackle estimation problems in\nDigital Signal Processing (DSP) using Support Vector Machines (SVMs). The use\nof SVMs in estimation problems has been traditionally limited to its mere use\nas a black-box model. Noting such limitations in the literature, we take\nadvantage of several properties of Mercer's kernels and functional analysis to\ndevelop a family of SVM methods for estimation in DSP. Three types of signal\nmodel equations are analyzed. First, when a specific time-signal structure is\nassumed to model the underlying system that generated the data, the linear\nsignal model (so called Primal Signal Model formulation) is first stated and\nanalyzed. Then, non-linear versions of the signal structure can be readily\ndeveloped by following two different approaches. On the one hand, the signal\nmodel equation is written in reproducing kernel Hilbert spaces (RKHS) using the\nwell-known RKHS Signal Model formulation, and Mercer's kernels are readily used\nin SVM non-linear algorithms. On the other hand, in the alternative and not so\ncommon Dual Signal Model formulation, a signal expansion is made by using an\nauxiliary signal model equation given by a non-linear regression of each time\ninstant in the observed time series. These building blocks can be used to\ngenerate different novel SVM-based methods for problems of signal estimation,\nand we deal with several of the most important ones in DSP. We illustrate the\nusefulness of this methodology by defining SVM algorithms for linear and\nnon-linear system identification, spectral analysis, nonuniform interpolation,\nsparse deconvolution, and array processing. The performance of the developed\nSVM methods is compared to standard approaches in all these settings. The\nexperimental results illustrate the generality, simplicity, and capabilities of\nthe proposed SVM framework for DSP.\n", "versions": [{"version": "v1", "created": "Thu, 21 Nov 2013 13:55:22 GMT"}], "update_date": "2013-11-22", "authors_parsed": [["Rojo-\u00c1lvarez", "Jos\u00e9 Luis", ""], ["Mart\u00ednez-Ram\u00f3n", "Manel", ""], ["Mu\u00f1oz-Mar\u00ed", "Jordi", ""], ["Camps-Valls", "Gustavo", ""]]}, {"id": "1311.5422", "submitter": "Nikhil Rao", "authors": "Nikhil Rao, Christopher Cox, Robert Nowak, Timothy Rogers", "title": "Sparse Overlapping Sets Lasso for Multitask Learning and its Application\n  to fMRI Analysis", "comments": "To appear in Advances in Neural Information Processing Systems, 2013", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Multitask learning can be effective when features useful in one task are also\nuseful for other tasks, and the group lasso is a standard method for selecting\na common subset of features. In this paper, we are interested in a less\nrestrictive form of multitask learning, wherein (1) the available features can\nbe organized into subsets according to a notion of similarity and (2) features\nuseful in one task are similar, but not necessarily identical, to the features\nbest suited for other tasks. The main contribution of this paper is a new\nprocedure called Sparse Overlapping Sets (SOS) lasso, a convex optimization\nthat automatically selects similar features for related learning tasks. Error\nbounds are derived for SOSlasso and its consistency is established for squared\nerror loss. In particular, SOSlasso is motivated by multi- subject fMRI studies\nin which functional activity is classified using brain voxels as features.\nExperiments with real and synthetic data demonstrate the advantages of SOSlasso\ncompared to the lasso and group lasso.\n", "versions": [{"version": "v1", "created": "Wed, 20 Nov 2013 16:45:51 GMT"}, {"version": "v2", "created": "Fri, 22 Nov 2013 04:49:54 GMT"}], "update_date": "2013-11-25", "authors_parsed": [["Rao", "Nikhil", ""], ["Cox", "Christopher", ""], ["Nowak", "Robert", ""], ["Rogers", "Timothy", ""]]}, {"id": "1311.5479", "submitter": "Xiaotong Yuan", "authors": "Xiao-Tong Yuan, Ping Li, Tong Zhang", "title": "Learning Pairwise Graphical Models with Nonlinear Sufficient Statistics", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We investigate a generic problem of learning pairwise exponential family\ngraphical models with pairwise sufficient statistics defined by a global\nmapping function, e.g., Mercer kernels. This subclass of pairwise graphical\nmodels allow us to flexibly capture complex interactions among variables beyond\npairwise product. We propose two $\\ell_1$-norm penalized maximum likelihood\nestimators to learn the model parameters from i.i.d. samples. The first one is\na joint estimator which estimates all the parameters simultaneously. The second\none is a node-wise conditional estimator which estimates the parameters\nindividually for each node. For both estimators, we show that under proper\nconditions the extra flexibility gained in our model comes at almost no cost of\nstatistical and computational efficiency. We demonstrate the advantages of our\nmodel over state-of-the-art methods on synthetic and real datasets.\n", "versions": [{"version": "v1", "created": "Thu, 21 Nov 2013 16:59:52 GMT"}, {"version": "v2", "created": "Fri, 22 Nov 2013 08:04:25 GMT"}], "update_date": "2013-11-25", "authors_parsed": [["Yuan", "Xiao-Tong", ""], ["Li", "Ping", ""], ["Zhang", "Tong", ""]]}, {"id": "1311.5552", "submitter": "Steven Smith", "authors": "Steven T. Smith, Edward K. Kao, Kenneth D. Senne, Garrett Bernstein,\n  and Scott Philips", "title": "Bayesian Discovery of Threat Networks", "comments": "IEEE Trans. Signal Process., major revision of\n  arxiv.org/abs/1303.5613. arXiv admin note: substantial text overlap with\n  arXiv:1303.5613", "journal-ref": "IEEE Trans. Signal Process., vol. 62, no. 20, pp. 5324-5338,\n  October 2014", "doi": "10.1109/TSP.2014.2336613", "report-no": null, "categories": "cs.SI cs.LG math.ST physics.soc-ph stat.ML stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A novel unified Bayesian framework for network detection is developed, under\nwhich a detection algorithm is derived based on random walks on graphs. The\nalgorithm detects threat networks using partial observations of their activity,\nand is proved to be optimum in the Neyman-Pearson sense. The algorithm is\ndefined by a graph, at least one observation, and a diffusion model for threat.\nA link to well-known spectral detection methods is provided, and the\nequivalence of the random walk and harmonic solutions to the Bayesian\nformulation is proven. A general diffusion model is introduced that utilizes\nspatio-temporal relationships between vertices, and is used for a specific\nspace-time formulation that leads to significant performance improvements on\ncoordinated covert networks. This performance is demonstrated using a new\nhybrid mixed-membership blockmodel introduced to simulate random covert\nnetworks with realistic properties.\n", "versions": [{"version": "v1", "created": "Thu, 21 Nov 2013 20:43:44 GMT"}, {"version": "v2", "created": "Thu, 20 Mar 2014 20:07:08 GMT"}, {"version": "v3", "created": "Mon, 8 Sep 2014 17:14:10 GMT"}], "update_date": "2014-09-09", "authors_parsed": [["Smith", "Steven T.", ""], ["Kao", "Edward K.", ""], ["Senne", "Kenneth D.", ""], ["Bernstein", "Garrett", ""], ["Philips", "Scott", ""]]}, {"id": "1311.5599", "submitter": "Swayambhoo Jain", "authors": "Swayambhoo Jain, Akshay Soni, and Jarvis Haupt", "title": "Compressive Measurement Designs for Estimating Structured Signals in\n  Structured Clutter: A Bayesian Experimental Design Approach", "comments": "5 pages, 4 figures. Accepted for publication at The Asilomar\n  Conference on Signals, Systems, and Computers 2013", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This work considers an estimation task in compressive sensing, where the goal\nis to estimate an unknown signal from compressive measurements that are\ncorrupted by additive pre-measurement noise (interference, or clutter) as well\nas post-measurement noise, in the specific setting where some (perhaps limited)\nprior knowledge on the signal, interference, and noise is available. The\nspecific aim here is to devise a strategy for incorporating this prior\ninformation into the design of an appropriate compressive measurement strategy.\nHere, the prior information is interpreted as statistics of a prior\ndistribution on the relevant quantities, and an approach based on Bayesian\nExperimental Design is proposed. Experimental results on synthetic data\ndemonstrate that the proposed approach outperforms traditional random\ncompressive measurement designs, which are agnostic to the prior information,\nas well as several other knowledge-enhanced sensing matrix designs based on\nmore heuristic notions.\n", "versions": [{"version": "v1", "created": "Thu, 21 Nov 2013 22:16:00 GMT"}], "update_date": "2013-11-25", "authors_parsed": [["Jain", "Swayambhoo", ""], ["Soni", "Akshay", ""], ["Haupt", "Jarvis", ""]]}, {"id": "1311.5750", "submitter": "Xiaotong Yuan", "authors": "Xiao-Tong Yuan, Ping Li, Tong Zhang", "title": "Gradient Hard Thresholding Pursuit for Sparsity-Constrained Optimization", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.NA stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Hard Thresholding Pursuit (HTP) is an iterative greedy selection procedure\nfor finding sparse solutions of underdetermined linear systems. This method has\nbeen shown to have strong theoretical guarantee and impressive numerical\nperformance. In this paper, we generalize HTP from compressive sensing to a\ngeneric problem setup of sparsity-constrained convex optimization. The proposed\nalgorithm iterates between a standard gradient descent step and a hard\nthresholding step with or without debiasing. We prove that our method enjoys\nthe strong guarantees analogous to HTP in terms of rate of convergence and\nparameter estimation accuracy. Numerical evidences show that our method is\nsuperior to the state-of-the-art greedy selection methods in sparse logistic\nregression and sparse precision matrix estimation tasks.\n", "versions": [{"version": "v1", "created": "Fri, 22 Nov 2013 13:52:07 GMT"}, {"version": "v2", "created": "Mon, 25 Nov 2013 04:19:39 GMT"}], "update_date": "2013-11-26", "authors_parsed": [["Yuan", "Xiao-Tong", ""], ["Li", "Ping", ""], ["Zhang", "Tong", ""]]}, {"id": "1311.5871", "submitter": "Fabien Lauer", "authors": "Fabien Lauer (LORIA), Henrik Ohlsson", "title": "Finding sparse solutions of systems of polynomial equations via\n  group-sparsity optimization", "comments": "Journal of Global Optimization (2014) to appear", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IT cs.LG math.IT math.OC stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The paper deals with the problem of finding sparse solutions to systems of\npolynomial equations possibly perturbed by noise. In particular, we show how\nthese solutions can be recovered from group-sparse solutions of a derived\nsystem of linear equations. Then, two approaches are considered to find these\ngroup-sparse solutions. The first one is based on a convex relaxation resulting\nin a second-order cone programming formulation which can benefit from efficient\nreweighting techniques for sparsity enhancement. For this approach, sufficient\nconditions for the exact recovery of the sparsest solution to the polynomial\nsystem are derived in the noiseless setting, while stable recovery results are\nobtained for the noisy case. Though lacking a similar analysis, the second\napproach provides a more computationally efficient algorithm based on a greedy\nstrategy adding the groups one-by-one. With respect to previous work, the\nproposed methods recover the sparsest solution in a very short computing time\nwhile remaining at least as accurate in terms of the probability of success.\nThis probability is empirically analyzed to emphasize the relationship between\nthe ability of the methods to solve the polynomial system and the sparsity of\nthe solution.\n", "versions": [{"version": "v1", "created": "Fri, 22 Nov 2013 20:29:38 GMT"}, {"version": "v2", "created": "Wed, 16 Jul 2014 15:47:44 GMT"}], "update_date": "2014-07-17", "authors_parsed": [["Lauer", "Fabien", "", "LORIA"], ["Ohlsson", "Henrik", ""]]}, {"id": "1311.5954", "submitter": "Li Chen", "authors": "Li Chen, Cencheng Shen, Joshua Vogelstein, and Carey Priebe", "title": "Robust Vertex Classification", "comments": "18 pages, 13 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  For random graphs distributed according to stochastic blockmodels, a special\ncase of latent position graphs, adjacency spectral embedding followed by\nappropriate vertex classification is asymptotically Bayes optimal; but this\napproach requires knowledge of and critically depends on the model dimension.\nIn this paper, we propose a sparse representation vertex classifier which does\nnot require information about the model dimension. This classifier represents a\ntest vertex as a sparse combination of the vertices in the training set and\nuses the recovered coefficients to classify the test vertex. We prove\nconsistency of our proposed classifier for stochastic blockmodels, and\ndemonstrate that the sparse representation classifier can predict vertex labels\nwith higher accuracy than adjacency spectral embedding approaches via both\nsimulation studies and real data experiments. Our results demonstrate the\nrobustness and effectiveness of our proposed vertex classifier when the model\ndimension is unknown.\n", "versions": [{"version": "v1", "created": "Sat, 23 Nov 2013 03:53:31 GMT"}, {"version": "v2", "created": "Wed, 22 Apr 2015 18:57:58 GMT"}], "update_date": "2015-04-23", "authors_parsed": [["Chen", "Li", ""], ["Shen", "Cencheng", ""], ["Vogelstein", "Joshua", ""], ["Priebe", "Carey", ""]]}, {"id": "1311.6107", "submitter": "Biao Luo", "authors": "Biao Luo, Huai-Ning Wu, Tingwen Huang", "title": "Off-policy reinforcement learning for $ H_\\infty $ control design", "comments": "Accepted by IEEE Transactions on Cybernetics. IEEE Transactions on\n  Cybernetics, Online Available, 2014", "journal-ref": null, "doi": "10.1109/TCYB.2014.2319577", "report-no": null, "categories": "cs.SY cs.LG math.OC stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The $H_\\infty$ control design problem is considered for nonlinear systems\nwith unknown internal system model. It is known that the nonlinear $ H_\\infty $\ncontrol problem can be transformed into solving the so-called\nHamilton-Jacobi-Isaacs (HJI) equation, which is a nonlinear partial\ndifferential equation that is generally impossible to be solved analytically.\nEven worse, model-based approaches cannot be used for approximately solving HJI\nequation, when the accurate system model is unavailable or costly to obtain in\npractice. To overcome these difficulties, an off-policy reinforcement leaning\n(RL) method is introduced to learn the solution of HJI equation from real\nsystem data instead of mathematical system model, and its convergence is\nproved. In the off-policy RL method, the system data can be generated with\narbitrary policies rather than the evaluating policy, which is extremely\nimportant and promising for practical systems. For implementation purpose, a\nneural network (NN) based actor-critic structure is employed and a least-square\nNN weight update algorithm is derived based on the method of weighted\nresiduals. Finally, the developed NN-based off-policy RL method is tested on a\nlinear F16 aircraft plant, and further applied to a rotational/translational\nactuator system.\n", "versions": [{"version": "v1", "created": "Sun, 24 Nov 2013 11:26:07 GMT"}, {"version": "v2", "created": "Sun, 19 Jan 2014 16:06:31 GMT"}, {"version": "v3", "created": "Sun, 11 May 2014 07:33:16 GMT"}], "update_date": "2014-05-13", "authors_parsed": [["Luo", "Biao", ""], ["Wu", "Huai-Ning", ""], ["Huang", "Tingwen", ""]]}, {"id": "1311.6182", "submitter": "Zhiwei Qin", "authors": "Donald Goldfarb, Zhiwei Qin", "title": "Robust Low-rank Tensor Recovery: Models and Algorithms", "comments": "appearing in SIAM Journal on Matrix Analysis and Applications", "journal-ref": null, "doi": "10.1137/130905010", "report-no": null, "categories": "stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Robust tensor recovery plays an instrumental role in robustifying tensor\ndecompositions for multilinear data analysis against outliers, gross\ncorruptions and missing values and has a diverse array of applications. In this\npaper, we study the problem of robust low-rank tensor recovery in a convex\noptimization framework, drawing upon recent advances in robust Principal\nComponent Analysis and tensor completion. We propose tailored optimization\nalgorithms with global convergence guarantees for solving both the constrained\nand the Lagrangian formulations of the problem. These algorithms are based on\nthe highly efficient alternating direction augmented Lagrangian and accelerated\nproximal gradient methods. We also propose a nonconvex model that can often\nimprove the recovery results from the convex models. We investigate the\nempirical recoverability properties of the convex and nonconvex formulations\nand compare the computational performance of the algorithms on simulated data.\nWe demonstrate through a number of real applications the practical\neffectiveness of this convex optimization framework for robust low-rank tensor\nrecovery.\n", "versions": [{"version": "v1", "created": "Sun, 24 Nov 2013 22:41:20 GMT"}], "update_date": "2014-08-26", "authors_parsed": [["Goldfarb", "Donald", ""], ["Qin", "Zhiwei", ""]]}, {"id": "1311.6186", "submitter": "Zhao Ren", "authors": "Mengjie Chen, Chao Gao, Zhao Ren, Harrison H. Zhou", "title": "Sparse CCA via Precision Adjusted Iterative Thresholding", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.ME stat.ML stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Sparse Canonical Correlation Analysis (CCA) has received considerable\nattention in high-dimensional data analysis to study the relationship between\ntwo sets of random variables. However, there has been remarkably little\ntheoretical statistical foundation on sparse CCA in high-dimensional settings\ndespite active methodological and applied research activities. In this paper,\nwe introduce an elementary sufficient and necessary characterization such that\nthe solution of CCA is indeed sparse, propose a computationally efficient\nprocedure, called CAPIT, to estimate the canonical directions, and show that\nthe procedure is rate-optimal under various assumptions on nuisance parameters.\nThe procedure is applied to a breast cancer dataset from The Cancer Genome\nAtlas project. We identify methylation probes that are associated with genes,\nwhich have been previously characterized as prognosis signatures of the\nmetastasis of breast cancer.\n", "versions": [{"version": "v1", "created": "Sun, 24 Nov 2013 23:50:20 GMT"}], "update_date": "2013-11-26", "authors_parsed": [["Chen", "Mengjie", ""], ["Gao", "Chao", ""], ["Ren", "Zhao", ""], ["Zhou", "Harrison H.", ""]]}, {"id": "1311.6238", "submitter": "Jason D. Lee", "authors": "Jason D. Lee, Dennis L. Sun, Yuekai Sun, Jonathan E. Taylor", "title": "Exact post-selection inference, with application to the lasso", "comments": "Published at http://dx.doi.org/10.1214/15-AOS1371 in the Annals of\n  Statistics (http://www.imstat.org/aos/) by the Institute of Mathematical\n  Statistics (http://www.imstat.org)", "journal-ref": "Annals of Statistics 2016, Vol. 44, No. 3, 907-927", "doi": "10.1214/15-AOS1371", "report-no": "IMS-AOS-AOS1371", "categories": "math.ST stat.ME stat.ML stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We develop a general approach to valid inference after model selection. At\nthe core of our framework is a result that characterizes the distribution of a\npost-selection estimator conditioned on the selection event. We specialize the\napproach to model selection by the lasso to form valid confidence intervals for\nthe selected coefficients and test whether all relevant variables have been\nincluded in the model.\n", "versions": [{"version": "v1", "created": "Mon, 25 Nov 2013 09:01:08 GMT"}, {"version": "v2", "created": "Wed, 5 Feb 2014 20:15:48 GMT"}, {"version": "v3", "created": "Thu, 13 Feb 2014 00:56:16 GMT"}, {"version": "v4", "created": "Fri, 14 Feb 2014 03:06:38 GMT"}, {"version": "v5", "created": "Wed, 14 Jan 2015 02:52:11 GMT"}, {"version": "v6", "created": "Mon, 10 Aug 2015 23:57:55 GMT"}, {"version": "v7", "created": "Tue, 16 Feb 2016 09:02:58 GMT"}, {"version": "v8", "created": "Tue, 3 May 2016 07:48:17 GMT"}], "update_date": "2016-05-04", "authors_parsed": [["Lee", "Jason D.", ""], ["Sun", "Dennis L.", ""], ["Sun", "Yuekai", ""], ["Taylor", "Jonathan E.", ""]]}, {"id": "1311.6334", "submitter": "Charanpal Dhanjal", "authors": "Charanpal Dhanjal (LTCI), St\\'ephan Cl\\'emen\\c{c}on (LTCI)", "title": "Learning Reputation in an Authorship Network", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.SI cs.IR cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The problem of searching for experts in a given academic field is hugely\nimportant in both industry and academia. We study exactly this issue with\nrespect to a database of authors and their publications. The idea is to use\nLatent Semantic Indexing (LSI) and Latent Dirichlet Allocation (LDA) to perform\ntopic modelling in order to find authors who have worked in a query field. We\nthen construct a coauthorship graph and motivate the use of influence\nmaximisation and a variety of graph centrality measures to obtain a ranked list\nof experts. The ranked lists are further improved using a Markov Chain-based\nrank aggregation approach. The complete method is readily scalable to large\ndatasets. To demonstrate the efficacy of the approach we report on an extensive\nset of computational simulations using the Arnetminer dataset. An improvement\nin mean average precision is demonstrated over the baseline case of simply\nusing the order of authors found by the topic models.\n", "versions": [{"version": "v1", "created": "Mon, 25 Nov 2013 15:25:28 GMT"}], "update_date": "2013-11-26", "authors_parsed": [["Dhanjal", "Charanpal", "", "LTCI"], ["Cl\u00e9men\u00e7on", "St\u00e9phan", "", "LTCI"]]}, {"id": "1311.6359", "submitter": "Christopher Nowzohour", "authors": "Christopher Nowzohour and Peter B\\\"uhlmann", "title": "Score-based Causal Learning in Additive Noise Models", "comments": null, "journal-ref": "Statistics Vol. 50, Iss. 3, 2016", "doi": "10.1080/02331888.2015.1060237", "report-no": null, "categories": "stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Given data sampled from a number of variables, one is often interested in the\nunderlying causal relationships in the form of a directed acyclic graph. In the\ngeneral case, without interventions on some of the variables it is only\npossible to identify the graph up to its Markov equivalence class. However, in\nsome situations one can find the true causal graph just from observational\ndata, for example in structural equation models with additive noise and\nnonlinear edge functions. Most current methods for achieving this rely on\nnonparametric independence tests. One of the problems there is that the null\nhypothesis is independence, which is what one would like to get evidence for.\nWe take a different approach in our work by using a penalized likelihood as a\nscore for model selection. This is practically feasible in many settings and\nhas the advantage of yielding a natural ranking of the candidate models. When\nmaking smoothness assumptions on the probability density space, we prove\nconsistency of the penalized maximum likelihood estimator. We also present\nempirical results for simulated scenarios and real two-dimensional data sets\n(cause-effect pairs) where we obtain similar results as other state-of-the-art\nmethods.\n", "versions": [{"version": "v1", "created": "Mon, 25 Nov 2013 16:42:57 GMT"}, {"version": "v2", "created": "Mon, 14 Apr 2014 15:16:32 GMT"}, {"version": "v3", "created": "Mon, 13 Jul 2015 08:19:16 GMT"}], "update_date": "2017-12-05", "authors_parsed": [["Nowzohour", "Christopher", ""], ["B\u00fchlmann", "Peter", ""]]}, {"id": "1311.6371", "submitter": "Antoni Chan", "authors": "Lifeng Shang and Antoni B. Chan", "title": "On Approximate Inference for Generalized Gaussian Process Models", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A generalized Gaussian process model (GGPM) is a unifying framework that\nencompasses many existing Gaussian process (GP) models, such as GP regression,\nclassification, and counting. In the GGPM framework, the observation likelihood\nof the GP model is itself parameterized using the exponential family\ndistribution (EFD). In this paper, we consider efficient algorithms for\napproximate inference on GGPMs using the general form of the EFD. A particular\nGP model and its associated inference algorithms can then be formed by changing\nthe parameters of the EFD, thus greatly simplifying its creation for\ntask-specific output domains. We demonstrate the efficacy of this framework by\ncreating several new GP models for regressing to non-negative reals and to real\nintervals. We also consider a closed-form Taylor approximation for efficient\ninference on GGPMs, and elaborate on its connections with other model-specific\nheuristic closed-form approximations. Finally, we present a comprehensive set\nof experiments to compare approximate inference algorithms on a wide variety of\nGGPMs.\n", "versions": [{"version": "v1", "created": "Mon, 25 Nov 2013 17:22:22 GMT"}, {"version": "v2", "created": "Tue, 26 Nov 2013 04:24:02 GMT"}, {"version": "v3", "created": "Wed, 27 Nov 2013 07:43:48 GMT"}], "update_date": "2013-11-28", "authors_parsed": [["Shang", "Lifeng", ""], ["Chan", "Antoni B.", ""]]}, {"id": "1311.6392", "submitter": "Nuri Denizcan Vanli", "authors": "N. Denizcan Vanli and Suleyman S. Kozat", "title": "A Comprehensive Approach to Universal Piecewise Nonlinear Regression\n  Based on Trees", "comments": "Submitted to IEEE Transactions on Signal Processing", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we investigate adaptive nonlinear regression and introduce\ntree based piecewise linear regression algorithms that are highly efficient and\nprovide significantly improved performance with guaranteed upper bounds in an\nindividual sequence manner. We use a tree notion in order to partition the\nspace of regressors in a nested structure. The introduced algorithms adapt not\nonly their regression functions but also the complete tree structure while\nachieving the performance of the \"best\" linear mixture of a doubly exponential\nnumber of partitions, with a computational complexity only polynomial in the\nnumber of nodes of the tree. While constructing these algorithms, we also avoid\nusing any artificial \"weighting\" of models (with highly data dependent\nparameters) and, instead, directly minimize the final regression error, which\nis the ultimate performance goal. The introduced methods are generic such that\nthey can readily incorporate different tree construction methods such as random\ntrees in their framework and can use different regressor or partitioning\nfunctions as demonstrated in the paper.\n", "versions": [{"version": "v1", "created": "Mon, 25 Nov 2013 18:31:40 GMT"}, {"version": "v2", "created": "Fri, 27 Dec 2013 13:21:40 GMT"}], "update_date": "2013-12-30", "authors_parsed": [["Vanli", "N. Denizcan", ""], ["Kozat", "Suleyman S.", ""]]}, {"id": "1311.6425", "submitter": "Marcelo Fiori", "authors": "Marcelo Fiori, Pablo Sprechmann, Joshua Vogelstein, Pablo Mus\\'e,\n  Guillermo Sapiro", "title": "Robust Multimodal Graph Matching: Sparse Coding Meets Graph Matching", "comments": "NIPS 2013", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.OC cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Graph matching is a challenging problem with very important applications in a\nwide range of fields, from image and video analysis to biological and\nbiomedical problems. We propose a robust graph matching algorithm inspired in\nsparsity-related techniques. We cast the problem, resembling group or\ncollaborative sparsity formulations, as a non-smooth convex optimization\nproblem that can be efficiently solved using augmented Lagrangian techniques.\nThe method can deal with weighted or unweighted graphs, as well as multimodal\ndata, where different graphs represent different types of data. The proposed\napproach is also naturally integrated with collaborative graph inference\ntechniques, solving general network inference problems where the observed\nvariables, possibly coming from different modalities, are not in\ncorrespondence. The algorithm is tested and compared with state-of-the-art\ngraph matching techniques in both synthetic and real graphs. We also present\nresults on multimodal graphs and applications to collaborative inference of\nbrain connectivity from alignment-free functional magnetic resonance imaging\n(fMRI) data. The code is publicly available.\n", "versions": [{"version": "v1", "created": "Mon, 25 Nov 2013 19:57:49 GMT"}], "update_date": "2013-11-26", "authors_parsed": [["Fiori", "Marcelo", ""], ["Sprechmann", "Pablo", ""], ["Vogelstein", "Joshua", ""], ["Mus\u00e9", "Pablo", ""], ["Sapiro", "Guillermo", ""]]}, {"id": "1311.6510", "submitter": "Agata Lapedriza", "authors": "Agata Lapedriza and Hamed Pirsiavash and Zoya Bylinskii and Antonio\n  Torralba", "title": "Are all training examples equally valuable?", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  When learning a new concept, not all training examples may prove equally\nuseful for training: some may have higher or lower training value than others.\nThe goal of this paper is to bring to the attention of the vision community the\nfollowing considerations: (1) some examples are better than others for training\ndetectors or classifiers, and (2) in the presence of better examples, some\nexamples may negatively impact performance and removing them may be beneficial.\nIn this paper, we propose an approach for measuring the training value of an\nexample, and use it for ranking and greedily sorting examples. We test our\nmethods on different vision tasks, models, datasets and classifiers. Our\nexperiments show that the performance of current state-of-the-art detectors and\nclassifiers can be improved when training on a subset, rather than the whole\ntraining set.\n", "versions": [{"version": "v1", "created": "Mon, 25 Nov 2013 22:59:24 GMT"}], "update_date": "2013-11-27", "authors_parsed": [["Lapedriza", "Agata", ""], ["Pirsiavash", "Hamed", ""], ["Bylinskii", "Zoya", ""], ["Torralba", "Antonio", ""]]}, {"id": "1311.6529", "submitter": "Noah Simon", "authors": "Noah Simon, Jerome Friedman, Trevor Hastie", "title": "A Blockwise Descent Algorithm for Group-penalized Multiresponse and\n  Multinomial Regression", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.CO stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper we purpose a blockwise descent algorithm for group-penalized\nmultiresponse regression. Using a quasi-newton framework we extend this to\ngroup-penalized multinomial regression. We give a publicly available\nimplementation for these in R, and compare the speed of this algorithm to a\ncompeting algorithm --- we show that our implementation is an order of\nmagnitude faster than its competitor, and can solve gene-expression-sized\nproblems in real time.\n", "versions": [{"version": "v1", "created": "Tue, 26 Nov 2013 00:57:30 GMT"}], "update_date": "2013-11-27", "authors_parsed": [["Simon", "Noah", ""], ["Friedman", "Jerome", ""], ["Hastie", "Trevor", ""]]}, {"id": "1311.6530", "submitter": "Cristina Tortora Dr", "authors": "Cristina Tortora, Paul D. McNicholas and Ryan P. Browne", "title": "A Mixture of Generalized Hyperbolic Factor Analyzers", "comments": null, "journal-ref": null, "doi": "10.1007/s11634-015-0204-z", "report-no": null, "categories": "stat.ME stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Model-based clustering imposes a finite mixture modelling structure on data\nfor clustering. Finite mixture models assume that the population is a convex\ncombination of a finite number of densities, the distribution within each\npopulation is a basic assumption of each particular model. Among all\ndistributions that have been tried, the generalized hyperbolic distribution has\nthe advantage that is a generalization of several other methods, such as the\nGaussian distribution, the skew t-distribution, etc. With specific parameters,\nit can represent either a symmetric or a skewed distribution. While its\ninherent flexibility is an advantage in many ways, it means the estimation of\nmore parameters than its special and limiting cases. The aim of this work is to\npropose a mixture of generalized hyperbolic factor analyzers to introduce\nparsimony and extend the method to high dimensional data. This work can be seen\nas an extension of the mixture of factor analyzers model to generalized\nhyperbolic mixtures. The performance of our generalized hyperbolic factor\nanalyzers is illustrated on real data, where it performs favourably compared to\nits Gaussian analogue.\n", "versions": [{"version": "v1", "created": "Tue, 26 Nov 2013 00:59:26 GMT"}, {"version": "v2", "created": "Mon, 15 Dec 2014 15:03:49 GMT"}, {"version": "v3", "created": "Fri, 22 May 2015 14:19:46 GMT"}], "update_date": "2017-10-09", "authors_parsed": [["Tortora", "Cristina", ""], ["McNicholas", "Paul D.", ""], ["Browne", "Ryan P.", ""]]}, {"id": "1311.6547", "submitter": "Xiaocheng  Tang", "authors": "Katya Scheinberg and Xiaocheng Tang", "title": "Practical Inexact Proximal Quasi-Newton Method with Global Complexity\n  Analysis", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG math.OC stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recently several methods were proposed for sparse optimization which make\ncareful use of second-order information [10, 28, 16, 3] to improve local\nconvergence rates. These methods construct a composite quadratic approximation\nusing Hessian information, optimize this approximation using a first-order\nmethod, such as coordinate descent and employ a line search to ensure\nsufficient descent. Here we propose a general framework, which includes\nslightly modified versions of existing algorithms and also a new algorithm,\nwhich uses limited memory BFGS Hessian approximations, and provide a novel\nglobal convergence rate analysis, which covers methods that solve subproblems\nvia coordinate descent.\n", "versions": [{"version": "v1", "created": "Tue, 26 Nov 2013 03:36:21 GMT"}, {"version": "v2", "created": "Tue, 31 Dec 2013 23:51:19 GMT"}, {"version": "v3", "created": "Mon, 17 Mar 2014 21:49:04 GMT"}, {"version": "v4", "created": "Tue, 14 Jul 2015 16:07:49 GMT"}], "update_date": "2015-07-15", "authors_parsed": [["Scheinberg", "Katya", ""], ["Tang", "Xiaocheng", ""]]}, {"id": "1311.6594", "submitter": "\\'Angela Fern\\'andez Pascual", "authors": "\\'Angela Fern\\'andez, Neta Rabin, Dalia Fishelov, Jos\\'e R. Dorronsoro", "title": "Auto-adaptative Laplacian Pyramids for High-dimensional Data Analysis", "comments": "11 pages, 9 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Non-linear dimensionality reduction techniques such as manifold learning\nalgorithms have become a common way for processing and analyzing\nhigh-dimensional patterns that often have attached a target that corresponds to\nthe value of an unknown function. Their application to new points consists in\ntwo steps: first, embedding the new data point into the low dimensional space\nand then, estimating the function value on the test point from its neighbors in\nthe embedded space.\n  However, finding the low dimension representation of a test point, while easy\nfor simple but often not powerful enough procedures such as PCA, can be much\nmore complicated for methods that rely on some kind of eigenanalysis, such as\nSpectral Clustering (SC) or Diffusion Maps (DM). Similarly, when a target\nfunction is to be evaluated, averaging methods like nearest neighbors may give\nunstable results if the function is noisy. Thus, the smoothing of the target\nfunction with respect to the intrinsic, low-dimensional representation that\ndescribes the geometric structure of the examined data is a challenging task.\n  In this paper we propose Auto-adaptive Laplacian Pyramids (ALP), an extension\nof the standard Laplacian Pyramids model that incorporates a modified LOOCV\nprocedure that avoids the large cost of the standard one and offers the\nfollowing advantages: (i) it selects automatically the optimal function\nresolution (stopping time) adapted to the data and its noise, (ii) it is easy\nto apply as it does not require parameterization, (iii) it does not overfit the\ntraining set and (iv) it adds no extra cost compared to other classical\ninterpolation methods. We illustrate numerically ALP's behavior on a synthetic\nproblem and apply it to the computation of the DM projection of new patterns\nand to the extension to them of target function values on a radiation\nforecasting problem over very high dimensional patterns.\n", "versions": [{"version": "v1", "created": "Tue, 26 Nov 2013 09:03:10 GMT"}, {"version": "v2", "created": "Tue, 20 May 2014 10:17:31 GMT"}], "update_date": "2014-05-21", "authors_parsed": [["Fern\u00e1ndez", "\u00c1ngela", ""], ["Rabin", "Neta", ""], ["Fishelov", "Dalia", ""], ["Dorronsoro", "Jos\u00e9 R.", ""]]}, {"id": "1311.6834", "submitter": "Jim Jing-Yan Wang", "authors": "Jim Jing-Yan Wang and Xin Gao", "title": "Semi-Supervised Sparse Coding", "comments": null, "journal-ref": null, "doi": "10.1109/IJCNN.2014.6889449", "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Sparse coding approximates the data sample as a sparse linear combination of\nsome basic codewords and uses the sparse codes as new presentations. In this\npaper, we investigate learning discriminative sparse codes by sparse coding in\na semi-supervised manner, where only a few training samples are labeled. By\nusing the manifold structure spanned by the data set of both labeled and\nunlabeled samples and the constraints provided by the labels of the labeled\nsamples, we learn the variable class labels for all the samples. Furthermore,\nto improve the discriminative ability of the learned sparse codes, we assume\nthat the class labels could be predicted from the sparse codes directly using a\nlinear classifier. By solving the codebook, sparse codes, class labels and\nclassifier parameters simultaneously in a unified objective function, we\ndevelop a semi-supervised sparse coding algorithm. Experiments on two\nreal-world pattern recognition problems demonstrate the advantage of the\nproposed methods over supervised sparse coding methods on partially labeled\ndata sets.\n", "versions": [{"version": "v1", "created": "Tue, 26 Nov 2013 22:13:37 GMT"}, {"version": "v2", "created": "Fri, 16 Jan 2015 18:37:00 GMT"}], "update_date": "2015-01-19", "authors_parsed": [["Wang", "Jim Jing-Yan", ""], ["Gao", "Xin", ""]]}, {"id": "1311.6976", "submitter": "Bjarne {\\O}rum Fruergaard", "authors": "Bjarne {\\O}rum Fruergaard, Toke Jansen Hansen, Lars Kai Hansen", "title": "Dimensionality reduction for click-through rate prediction: Dense versus\n  sparse representation", "comments": "Presented at the Probabilistic Models for Big Data workshop at NIPS\n  2013", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG stat.AP stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In online advertising, display ads are increasingly being placed based on\nreal-time auctions where the advertiser who wins gets to serve the ad. This is\ncalled real-time bidding (RTB). In RTB, auctions have very tight time\nconstraints on the order of 100ms. Therefore mechanisms for bidding\nintelligently such as clickthrough rate prediction need to be sufficiently\nfast. In this work, we propose to use dimensionality reduction of the\nuser-website interaction graph in order to produce simplified features of users\nand websites that can be used as predictors of clickthrough rate. We\ndemonstrate that the Infinite Relational Model (IRM) as a dimensionality\nreduction offers comparable predictive performance to conventional\ndimensionality reduction schemes, while achieving the most economical usage of\nfeatures and fastest computations at run-time. For applications such as\nreal-time bidding, where fast database I/O and few computations are key to\nsuccess, we thus recommend using IRM based features as predictors to exploit\nthe recommender effects from bipartite graphs.\n", "versions": [{"version": "v1", "created": "Wed, 27 Nov 2013 14:19:21 GMT"}, {"version": "v2", "created": "Tue, 13 May 2014 09:21:28 GMT"}], "update_date": "2014-05-14", "authors_parsed": [["Fruergaard", "Bjarne \u00d8rum", ""], ["Hansen", "Toke Jansen", ""], ["Hansen", "Lars Kai", ""]]}, {"id": "1311.7071", "submitter": "Zitao Liu", "authors": "Zitao Liu and Milos Hauskrecht", "title": "Sparse Linear Dynamical System with Its Application in Multivariate\n  Clinical Time Series", "comments": "Appear in Neural Information Processing Systems(NIPS) Workshop on\n  Machine Learning for Clinical Data Analysis and Healthcare 2013", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.LG stat.ML", "license": "http://creativecommons.org/licenses/by/3.0/", "abstract": "  Linear Dynamical System (LDS) is an elegant mathematical framework for\nmodeling and learning multivariate time series. However, in general, it is\ndifficult to set the dimension of its hidden state space. A small number of\nhidden states may not be able to model the complexities of a time series, while\na large number of hidden states can lead to overfitting. In this paper, we\nstudy methods that impose an $\\ell_1$ regularization on the transition matrix\nof an LDS model to alleviate the problem of choosing the optimal number of\nhidden states. We incorporate a generalized gradient descent method into the\nMaximum a Posteriori (MAP) framework and use Expectation Maximization (EM) to\niteratively achieve sparsity on the transition matrix of an LDS model. We show\nthat our Sparse Linear Dynamical System (SLDS) improves the predictive\nperformance when compared to ordinary LDS on a multivariate clinical time\nseries dataset.\n", "versions": [{"version": "v1", "created": "Wed, 27 Nov 2013 18:58:07 GMT"}, {"version": "v2", "created": "Tue, 3 Dec 2013 20:08:28 GMT"}], "update_date": "2013-12-04", "authors_parsed": [["Liu", "Zitao", ""], ["Hauskrecht", "Milos", ""]]}, {"id": "1311.7080", "submitter": "Jim Jing-Yan Wang", "authors": "Jim Jing-Yan Wang", "title": "Cross-Domain Sparse Coding", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Sparse coding has shown its power as an effective data representation method.\nHowever, up to now, all the sparse coding approaches are limited within the\nsingle domain learning problem. In this paper, we extend the sparse coding to\ncross domain learning problem, which tries to learn from a source domain to a\ntarget domain with significant different distribution. We impose the Maximum\nMean Discrepancy (MMD) criterion to reduce the cross-domain distribution\ndifference of sparse codes, and also regularize the sparse codes by the class\nlabels of the samples from both domains to increase the discriminative ability.\nThe encouraging experiment results of the proposed cross-domain sparse coding\nalgorithm on two challenging tasks --- image classification of photograph and\noil painting domains, and multiple user spam detection --- show the advantage\nof the proposed method over other cross-domain data representation methods.\n", "versions": [{"version": "v1", "created": "Wed, 27 Nov 2013 19:27:06 GMT"}], "update_date": "2013-11-28", "authors_parsed": [["Wang", "Jim Jing-Yan", ""]]}, {"id": "1311.7184", "submitter": "Jason Lee", "authors": "Jason D Lee, Ran Gilad-Bachrach, and Rich Caruana", "title": "Using Multiple Samples to Learn Mixture Models", "comments": "Published in Neural Information Processing Systems (NIPS) 2013", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In the mixture models problem it is assumed that there are $K$ distributions\n$\\theta_{1},\\ldots,\\theta_{K}$ and one gets to observe a sample from a mixture\nof these distributions with unknown coefficients. The goal is to associate\ninstances with their generating distributions, or to identify the parameters of\nthe hidden distributions. In this work we make the assumption that we have\naccess to several samples drawn from the same $K$ underlying distributions, but\nwith different mixing weights. As with topic modeling, having multiple samples\nis often a reasonable assumption. Instead of pooling the data into one sample,\nwe prove that it is possible to use the differences between the samples to\nbetter recover the underlying structure. We present algorithms that recover the\nunderlying structure under milder assumptions than the current state of art\nwhen either the dimensionality or the separation is high. The methods, when\napplied to topic modeling, allow generalization to words not present in the\ntraining data.\n", "versions": [{"version": "v1", "created": "Thu, 28 Nov 2013 01:36:49 GMT"}], "update_date": "2013-12-02", "authors_parsed": [["Lee", "Jason D", ""], ["Gilad-Bachrach", "Ran", ""], ["Caruana", "Rich", ""]]}, {"id": "1311.7198", "submitter": "Karthik Mohan", "authors": "Karthik Mohan", "title": "ADMM Algorithm for Graphical Lasso with an $\\ell_{\\infty}$ Element-wise\n  Norm Constraint", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG math.OC stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider the problem of Graphical lasso with an additional $\\ell_{\\infty}$\nelement-wise norm constraint on the precision matrix. This problem has\napplications in high-dimensional covariance decomposition such as in\n\\citep{Janzamin-12}. We propose an ADMM algorithm to solve this problem. We\nalso use a continuation strategy on the penalty parameter to have a fast\nimplemenation of the algorithm.\n", "versions": [{"version": "v1", "created": "Thu, 28 Nov 2013 03:59:31 GMT"}], "update_date": "2013-12-02", "authors_parsed": [["Mohan", "Karthik", ""]]}, {"id": "1311.7320", "submitter": "Maurizio Filippone", "authors": "Maurizio Filippone", "title": "Bayesian Inference for Gaussian Process Classifiers with Annealing and\n  Pseudo-Marginal MCMC", "comments": "6 pages, 2 figures, 1 table - to appear in ICPR 2014", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Kernel methods have revolutionized the fields of pattern recognition and\nmachine learning. Their success, however, critically depends on the choice of\nkernel parameters. Using Gaussian process (GP) classification as a working\nexample, this paper focuses on Bayesian inference of covariance (kernel)\nparameters using Markov chain Monte Carlo (MCMC) methods. The motivation is\nthat, compared to standard optimization of kernel parameters, they have been\nsystematically demonstrated to be superior in quantifying uncertainty in\npredictions. Recently, the Pseudo-Marginal MCMC approach has been proposed as a\npractical inference tool for GP models. In particular, it amounts in replacing\nthe analytically intractable marginal likelihood by an unbiased estimate\nobtainable by approximate methods and importance sampling. After discussing the\npotential drawbacks in employing importance sampling, this paper proposes the\napplication of annealed importance sampling. The results empirically\ndemonstrate that compared to importance sampling, annealed importance sampling\ncan reduce the variance of the estimate of the marginal likelihood\nexponentially in the number of data at a computational cost that scales only\npolynomially. The results on real data demonstrate that employing annealed\nimportance sampling in the Pseudo-Marginal MCMC approach represents a step\nforward in the development of fully automated exact inference engines for GP\nmodels.\n", "versions": [{"version": "v1", "created": "Thu, 28 Nov 2013 14:03:17 GMT"}, {"version": "v2", "created": "Mon, 26 May 2014 14:59:29 GMT"}], "update_date": "2014-05-27", "authors_parsed": [["Filippone", "Maurizio", ""]]}, {"id": "1311.7650", "submitter": "Mikhail Langovoy", "authors": "Mikhail Langovoy and Michael Habeck and Bernhard Schoelkopf", "title": "Adaptive nonparametric detection in cryo-electron microscopy", "comments": "Proceedings of the 58-th World Statistical Congress (2011)", "journal-ref": "Proceedings of the 58-th World Statistical Congress (2011),\n  Session: High Dimensional Data, pp. 4456 - 4461", "doi": null, "report-no": null, "categories": "stat.AP stat.ME stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Cryo-electron microscopy (cryo-EM) is an emerging experimental method to\ncharacterize the structure of large biomolecular assemblies. Single particle\ncryo-EM records 2D images (so-called micrographs) of projections of the\nthree-dimensional particle, which need to be processed to obtain the\nthree-dimensional reconstruction. A crucial step in the reconstruction process\nis particle picking which involves detection of particles in noisy 2D\nmicrographs with low signal-to-noise ratios of typically 1:10 or even lower.\nTypically, each picture contains a large number of particles, and particles\nhave unknown irregular and nonconvex shapes.\n", "versions": [{"version": "v1", "created": "Fri, 29 Nov 2013 18:05:50 GMT"}], "update_date": "2013-12-02", "authors_parsed": [["Langovoy", "Mikhail", ""], ["Habeck", "Michael", ""], ["Schoelkopf", "Bernhard", ""]]}, {"id": "1311.7656", "submitter": "Mikhail Langovoy", "authors": "Mikhail Langovoy and Suvrit Sra", "title": "Statistical estimation for optimization problems on graphs", "comments": "Paper for the NIPS Workshop on Discrete Optimization for Machine\n  Learning (DISCML) (2011): Uncertainty, Generalization and Feedback", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.DM math.OC stat.CO stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Large graphs abound in machine learning, data mining, and several related\nareas. A useful step towards analyzing such graphs is that of obtaining certain\nsummary statistics - e.g., or the expected length of a shortest path between\ntwo nodes, or the expected weight of a minimum spanning tree of the graph, etc.\nThese statistics provide insight into the structure of a graph, and they can\nhelp predict global properties of a graph. Motivated thus, we propose to study\nstatistical properties of structured subgraphs (of a given graph), in\nparticular, to estimate the expected objective function value of a\ncombinatorial optimization problem over these subgraphs. The general task is\nvery difficult, if not unsolvable; so for concreteness we describe a more\nspecific statistical estimation problem based on spanning trees. We hope that\nour position paper encourages others to also study other types of graphical\nstructures for which one can prove nontrivial statistical estimates.\n", "versions": [{"version": "v1", "created": "Fri, 29 Nov 2013 18:21:13 GMT"}], "update_date": "2013-12-02", "authors_parsed": [["Langovoy", "Mikhail", ""], ["Sra", "Suvrit", ""]]}]