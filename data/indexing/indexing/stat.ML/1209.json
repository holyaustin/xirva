[{"id": "1209.0001", "submitter": "Mehrdad Mahdavi", "authors": "Mehrdad Mahdavi, Tianbao Yang, Rong Jin", "title": "An Improved Bound for the Nystrom Method for Large Eigengap", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.NA stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We develop an improved bound for the approximation error of the Nystr\\\"{o}m\nmethod under the assumption that there is a large eigengap in the spectrum of\nkernel matrix. This is based on the empirical observation that the eigengap has\na significant impact on the approximation error of the Nystr\\\"{o}m method. Our\napproach is based on the concentration inequality of integral operator and the\ntheory of matrix perturbation. Our analysis shows that when there is a large\neigengap, we can improve the approximation error of the Nystr\\\"{o}m method from\n$O(N/m^{1/4})$ to $O(N/m^{1/2})$ when measured in Frobenius norm, where $N$ is\nthe size of the kernel matrix, and $m$ is the number of sampled columns.\n", "versions": [{"version": "v1", "created": "Thu, 30 Aug 2012 20:40:06 GMT"}], "update_date": "2012-09-04", "authors_parsed": [["Mahdavi", "Mehrdad", ""], ["Yang", "Tianbao", ""], ["Jin", "Rong", ""]]}, {"id": "1209.0016", "submitter": "Ery Arias-Castro", "authors": "Ery Arias-Castro and Bruno Pelletier", "title": "On the convergence of maximum variance unfolding", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Maximum Variance Unfolding is one of the main methods for (nonlinear)\ndimensionality reduction. We study its large sample limit, providing specific\nrates of convergence under standard assumptions. We find that it is consistent\nwhen the underlying submanifold is isometric to a convex subset, and we provide\nsome simple examples where it fails to be consistent.\n", "versions": [{"version": "v1", "created": "Fri, 31 Aug 2012 20:58:15 GMT"}, {"version": "v2", "created": "Tue, 19 Mar 2013 23:28:49 GMT"}], "update_date": "2013-03-21", "authors_parsed": [["Arias-Castro", "Ery", ""], ["Pelletier", "Bruno", ""]]}, {"id": "1209.0029", "submitter": "Stephen Purpura", "authors": "Stephen Purpura, Dustin Hillard, Mark Hubenthal, Jim Walsh, Scott\n  Golder, Scott Smith", "title": "Statistically adaptive learning for a general class of cost functions\n  (SA L-BFGS)", "comments": "7 pages, 2 tables", "journal-ref": null, "doi": null, "report-no": "version 0.05", "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a system that enables rapid model experimentation for tera-scale\nmachine learning with trillions of non-zero features, billions of training\nexamples, and millions of parameters. Our contribution to the literature is a\nnew method (SA L-BFGS) for changing batch L-BFGS to perform in near real-time\nby using statistical tools to balance the contributions of previous weights,\nold training examples, and new training examples to achieve fast convergence\nwith few iterations. The result is, to our knowledge, the most scalable and\nflexible linear learning system reported in the literature, beating standard\npractice with the current best system (Vowpal Wabbit and AllReduce). Using the\nKDD Cup 2012 data set from Tencent, Inc. we provide experimental results to\nverify the performance of this method.\n", "versions": [{"version": "v1", "created": "Fri, 31 Aug 2012 22:50:00 GMT"}, {"version": "v2", "created": "Tue, 4 Sep 2012 02:15:07 GMT"}, {"version": "v3", "created": "Wed, 5 Sep 2012 20:21:29 GMT"}], "update_date": "2012-09-07", "authors_parsed": [["Purpura", "Stephen", ""], ["Hillard", "Dustin", ""], ["Hubenthal", "Mark", ""], ["Walsh", "Jim", ""], ["Golder", "Scott", ""], ["Smith", "Scott", ""]]}, {"id": "1209.0125", "submitter": "Fionn Murtagh", "authors": "Fionn Murtagh and Michael J. Kurtz", "title": "A History of Cluster Analysis Using the Classification Society's\n  Bibliography Over Four Decades", "comments": "23 pages, 9 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DL cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The Classification Literature Automated Search Service, an annual\nbibliography based on citation of one or more of a set of around 80 book or\njournal publications, ran from 1972 to 2012. We analyze here the years 1994 to\n2011. The Classification Society's Service, as it was termed, has been produced\nby the Classification Society. In earlier decades it was distributed as a\ndiskette or CD with the Journal of Classification. Among our findings are the\nfollowing: an enormous increase in scholarly production post approximately\n2000; a very major increase in quantity, coupled with work in different\ndisciplines, from approximately 2004; and a major shift also from cluster\nanalysis in earlier times having mathematics and psychology as disciplines of\nthe journals published in, and affiliations of authors, contrasted with, in\nmore recent times, a \"centre of gravity\" in management and engineering.\n", "versions": [{"version": "v1", "created": "Sat, 1 Sep 2012 19:27:19 GMT"}, {"version": "v2", "created": "Fri, 16 Aug 2013 23:52:49 GMT"}], "update_date": "2013-08-20", "authors_parsed": [["Murtagh", "Fionn", ""], ["Kurtz", "Michael J.", ""]]}, {"id": "1209.0367", "submitter": "Youngser Park", "authors": "Donniell E. Fishkind, Sancar Adali, Heather G. Patsolic, Lingyao Meng,\n  Digvijay Singh, Vince Lyzinski, Carey E. Priebe", "title": "Seeded Graph Matching", "comments": "24 pages, 10 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Given two graphs, the graph matching problem is to align the two vertex sets\nso as to minimize the number of adjacency disagreements between the two graphs.\nThe seeded graph matching problem is the graph matching problem when we are\nfirst given a partial alignment that we are tasked with completing. In this\npaper, we modify the state-of-the-art approximate graph matching algorithm\n\"FAQ\" of Vogelstein et al. (2015) to make it a fast approximate seeded graph\nmatching algorithm, adapt its applicability to include graphs with differently\nsized vertex sets, and extend the algorithm so as to provide, for each\nindividual vertex, a nomination list of likely matches. We demonstrate the\neffectiveness of our algorithm via simulation and real data experiments;\nindeed, knowledge of even a few seeds can be extremely effective when our\nseeded graph matching algorithm is used to recover a naturally existing\nalignment that is only partially observed.\n", "versions": [{"version": "v1", "created": "Mon, 3 Sep 2012 14:45:53 GMT"}, {"version": "v2", "created": "Fri, 28 Apr 2017 18:03:16 GMT"}, {"version": "v3", "created": "Fri, 6 Apr 2018 21:24:05 GMT"}, {"version": "v4", "created": "Tue, 10 Apr 2018 18:10:17 GMT"}], "update_date": "2018-04-12", "authors_parsed": [["Fishkind", "Donniell E.", ""], ["Adali", "Sancar", ""], ["Patsolic", "Heather G.", ""], ["Meng", "Lingyao", ""], ["Singh", "Digvijay", ""], ["Lyzinski", "Vince", ""], ["Priebe", "Carey E.", ""]]}, {"id": "1209.0368", "submitter": "Silvia Villa", "authors": "Silvia Villa, Lorenzo Rosasco, Sofia Mosci, Alessandro Verri", "title": "Proximal methods for the latent group lasso penalty", "comments": "4 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.OC cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider a regularized least squares problem, with regularization by\nstructured sparsity-inducing norms, which extend the usual $\\ell_1$ and the\ngroup lasso penalty, by allowing the subsets to overlap. Such regularizations\nlead to nonsmooth problems that are difficult to optimize, and we propose in\nthis paper a suitable version of an accelerated proximal method to solve them.\nWe prove convergence of a nested procedure, obtained composing an accelerated\nproximal method with an inner algorithm for computing the proximity operator.\nBy exploiting the geometrical properties of the penalty, we devise a new active\nset strategy, thanks to which the inner iteration is relatively fast, thus\nguaranteeing good computational performances of the overall algorithm. Our\napproach allows to deal with high dimensional problems without pre-processing\nfor dimensionality reduction, leading to better computational and prediction\nperformances with respect to the state-of-the art methods, as shown empirically\nboth on toy and real data.\n", "versions": [{"version": "v1", "created": "Mon, 3 Sep 2012 14:46:14 GMT"}], "update_date": "2012-09-04", "authors_parsed": [["Villa", "Silvia", ""], ["Rosasco", "Lorenzo", ""], ["Mosci", "Sofia", ""], ["Verri", "Alessandro", ""]]}, {"id": "1209.0521", "submitter": "Olivier Delalleau", "authors": "Olivier Delalleau and Aaron Courville and Yoshua Bengio", "title": "Efficient EM Training of Gaussian Mixtures with Missing Data", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In data-mining applications, we are frequently faced with a large fraction of\nmissing entries in the data matrix, which is problematic for most discriminant\nmachine learning algorithms. A solution that we explore in this paper is the\nuse of a generative model (a mixture of Gaussians) to compute the conditional\nexpectation of the missing variables given the observed variables. Since\ntraining a Gaussian mixture with many different patterns of missing values can\nbe computationally very expensive, we introduce a spanning-tree based algorithm\nthat significantly speeds up training in these conditions. We also observe that\ngood results can be obtained by using the generative model to fill-in the\nmissing values for a separate discriminant learning algorithm.\n", "versions": [{"version": "v1", "created": "Tue, 4 Sep 2012 03:15:53 GMT"}, {"version": "v2", "created": "Mon, 8 Jan 2018 15:50:42 GMT"}], "update_date": "2018-01-09", "authors_parsed": [["Delalleau", "Olivier", ""], ["Courville", "Aaron", ""], ["Bengio", "Yoshua", ""]]}, {"id": "1209.0738", "submitter": "Bernardino Romera Paredes", "authors": "Andreas Maurer, Massimiliano Pontil, Bernardino Romera-Paredes", "title": "Sparse coding for multitask and transfer learning", "comments": "International Conference on Machine Learning 2013", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We investigate the use of sparse coding and dictionary learning in the\ncontext of multitask and transfer learning. The central assumption of our\nlearning method is that the tasks parameters are well approximated by sparse\nlinear combinations of the atoms of a dictionary on a high or infinite\ndimensional space. This assumption, together with the large quantity of\navailable data in the multitask and transfer learning settings, allows a\nprincipled choice of the dictionary. We provide bounds on the generalization\nerror of this approach, for both settings. Numerical experiments on one\nsynthetic and two real datasets show the advantage of our method over single\ntask learning, a previous method based on orthogonal and dense representation\nof the tasks and a related method learning task grouping.\n", "versions": [{"version": "v1", "created": "Tue, 4 Sep 2012 19:06:51 GMT"}, {"version": "v2", "created": "Sat, 23 Mar 2013 19:35:27 GMT"}, {"version": "v3", "created": "Mon, 16 Jun 2014 15:06:48 GMT"}], "update_date": "2014-06-17", "authors_parsed": [["Maurer", "Andreas", ""], ["Pontil", "Massimiliano", ""], ["Romera-Paredes", "Bernardino", ""]]}, {"id": "1209.0833", "submitter": "Emily Fox", "authors": "Emily B. Fox and David B. Dunson", "title": "Multiresolution Gaussian Processes", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a multiresolution Gaussian process to capture long-range,\nnon-Markovian dependencies while allowing for abrupt changes. The\nmultiresolution GP hierarchically couples a collection of smooth GPs, each\ndefined over an element of a random nested partition. Long-range dependencies\nare captured by the top-level GP while the partition points define the abrupt\nchanges. Due to the inherent conjugacy of the GPs, one can analytically\nmarginalize the GPs and compute the conditional likelihood of the observations\ngiven the partition tree. This property allows for efficient inference of the\npartition itself, for which we employ graph-theoretic techniques. We apply the\nmultiresolution GP to the analysis of Magnetoencephalography (MEG) recordings\nof brain activity.\n", "versions": [{"version": "v1", "created": "Wed, 5 Sep 2012 00:34:41 GMT"}], "update_date": "2012-09-06", "authors_parsed": [["Fox", "Emily B.", ""], ["Dunson", "David B.", ""]]}, {"id": "1209.1064", "submitter": "Aleksandar Dogand\\v{z}i\\'c", "authors": "Zhao Song and Aleksandar Dogandzic", "title": "A Max-Product EM Algorithm for Reconstructing Markov-tree Sparse Signals\n  from Compressive Samples", "comments": "To appear in IEEE Transactions on Signal Processing", "journal-ref": null, "doi": "10.1109/TSP.2013.2277833", "report-no": null, "categories": "stat.ML cs.IT math.IT", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a Bayesian expectation-maximization (EM) algorithm for\nreconstructing Markov-tree sparse signals via belief propagation. The\nmeasurements follow an underdetermined linear model where the\nregression-coefficient vector is the sum of an unknown approximately sparse\nsignal and a zero-mean white Gaussian noise with an unknown variance. The\nsignal is composed of large- and small-magnitude components identified by\nbinary state variables whose probabilistic dependence structure is described by\na Markov tree. Gaussian priors are assigned to the signal coefficients given\ntheir state variables and the Jeffreys' noninformative prior is assigned to the\nnoise variance. Our signal reconstruction scheme is based on an EM iteration\nthat aims at maximizing the posterior distribution of the signal and its state\nvariables given the noise variance. We construct the missing data for the EM\niteration so that the complete-data posterior distribution corresponds to a\nhidden Markov tree (HMT) probabilistic graphical model that contains no loops\nand implement its maximization (M) step via a max-product algorithm. This EM\nalgorithm estimates the vector of state variables as well as solves iteratively\na linear system of equations to obtain the corresponding signal estimate. We\nselect the noise variance so that the corresponding estimated signal and state\nvariables obtained upon convergence of the EM iteration have the largest\nmarginal posterior distribution. We compare the proposed and existing\nstate-of-the-art reconstruction methods via signal and image reconstruction\nexperiments.\n", "versions": [{"version": "v1", "created": "Wed, 5 Sep 2012 18:06:29 GMT"}, {"version": "v2", "created": "Fri, 7 Sep 2012 19:25:34 GMT"}, {"version": "v3", "created": "Wed, 10 Oct 2012 19:05:15 GMT"}, {"version": "v4", "created": "Sun, 25 Aug 2013 17:51:55 GMT"}], "update_date": "2013-08-27", "authors_parsed": [["Song", "Zhao", ""], ["Dogandzic", "Aleksandar", ""]]}, {"id": "1209.1077", "submitter": "Guillermo Diez-Canas", "authors": "Guillermo D. Canas and Lorenzo Rosasco", "title": "Learning Probability Measures with respect to Optimal Transport Metrics", "comments": "13 pages, 2 figures. Advances in Neural Information Processing\n  Systems, NIPS 2012", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study the problem of estimating, in the sense of optimal transport\nmetrics, a measure which is assumed supported on a manifold embedded in a\nHilbert space. By establishing a precise connection between optimal transport\nmetrics, optimal quantization, and learning theory, we derive new probabilistic\nbounds for the performance of a classic algorithm in unsupervised learning\n(k-means), when used to produce a probability measure derived from the data. In\nthe course of the analysis, we arrive at new lower bounds, as well as\nprobabilistic upper bounds on the convergence rate of the empirical law of\nlarge numbers, which, unlike existing bounds, are applicable to a wide class of\nmeasures.\n", "versions": [{"version": "v1", "created": "Wed, 5 Sep 2012 19:10:09 GMT"}], "update_date": "2012-09-06", "authors_parsed": [["Canas", "Guillermo D.", ""], ["Rosasco", "Lorenzo", ""]]}, {"id": "1209.1086", "submitter": "Aur\\'elien Bellet", "authors": "Aur\\'elien Bellet and Amaury Habrard", "title": "Robustness and Generalization for Metric Learning", "comments": "16 pages, to appear in Neurocomputing", "journal-ref": "Neurocomputing,151(1):259-267, 2015", "doi": "10.1016/j.neucom.2014.09.044", "report-no": null, "categories": "cs.LG cs.AI stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Metric learning has attracted a lot of interest over the last decade, but the\ngeneralization ability of such methods has not been thoroughly studied. In this\npaper, we introduce an adaptation of the notion of algorithmic robustness\n(previously introduced by Xu and Mannor) that can be used to derive\ngeneralization bounds for metric learning. We further show that a weak notion\nof robustness is in fact a necessary and sufficient condition for a metric\nlearning algorithm to generalize. To illustrate the applicability of the\nproposed framework, we derive generalization results for a large family of\nexisting metric learning algorithms, including some sparse formulations that\nare not covered by previous results.\n", "versions": [{"version": "v1", "created": "Wed, 5 Sep 2012 19:48:59 GMT"}, {"version": "v2", "created": "Wed, 10 Jul 2013 20:25:54 GMT"}, {"version": "v3", "created": "Mon, 29 Sep 2014 09:27:31 GMT"}], "update_date": "2019-01-25", "authors_parsed": [["Bellet", "Aur\u00e9lien", ""], ["Habrard", "Amaury", ""]]}, {"id": "1209.1119", "submitter": "Mingyuan Zhou", "authors": "Mingyuan Zhou and Lawrence Carin", "title": "Augment-and-Conquer Negative Binomial Processes", "comments": "Neural Information Processing Systems, NIPS 2012", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  By developing data augmentation methods unique to the negative binomial (NB)\ndistribution, we unite seemingly disjoint count and mixture models under the NB\nprocess framework. We develop fundamental properties of the models and derive\nefficient Gibbs sampling inference. We show that the gamma-NB process can be\nreduced to the hierarchical Dirichlet process with normalization, highlighting\nits unique theoretical, structural and computational advantages. A variety of\nNB processes with distinct sharing mechanisms are constructed and applied to\ntopic modeling, with connections to existing algorithms, showing the importance\nof inferring both the NB dispersion and probability parameters.\n", "versions": [{"version": "v1", "created": "Wed, 5 Sep 2012 21:06:32 GMT"}, {"version": "v2", "created": "Fri, 15 Feb 2013 16:30:49 GMT"}], "update_date": "2013-02-18", "authors_parsed": [["Zhou", "Mingyuan", ""], ["Carin", "Lawrence", ""]]}, {"id": "1209.1121", "submitter": "Guillermo Diez-Canas", "authors": "Guillermo D. Canas and Tomaso Poggio and Lorenzo Rosasco", "title": "Learning Manifolds with K-Means and K-Flats", "comments": "19 pages, 2 figures; Advances in Neural Information Processing\n  Systems, NIPS 2012", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study the problem of estimating a manifold from random samples. In\nparticular, we consider piecewise constant and piecewise linear estimators\ninduced by k-means and k-flats, and analyze their performance. We extend\nprevious results for k-means in two separate directions. First, we provide new\nresults for k-means reconstruction on manifolds and, secondly, we prove\nreconstruction bounds for higher-order approximation (k-flats), for which no\nknown results were previously available. While the results for k-means are\nnovel, some of the technical tools are well-established in the literature. In\nthe case of k-flats, both the results and the mathematical tools are new.\n", "versions": [{"version": "v1", "created": "Wed, 5 Sep 2012 21:18:03 GMT"}, {"version": "v2", "created": "Fri, 7 Sep 2012 17:11:23 GMT"}, {"version": "v3", "created": "Tue, 11 Sep 2012 16:00:03 GMT"}, {"version": "v4", "created": "Tue, 19 Feb 2013 17:53:17 GMT"}], "update_date": "2015-03-20", "authors_parsed": [["Canas", "Guillermo D.", ""], ["Poggio", "Tomaso", ""], ["Rosasco", "Lorenzo", ""]]}, {"id": "1209.1145", "submitter": "Sinead Williamson", "authors": "Sinead Williamson, Zoubin Ghahramani, Steven N. MacEachern, Eric P.\n  Xing", "title": "Restricting exchangeable nonparametric distributions", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Distributions over exchangeable matrices with infinitely many columns, such\nas the Indian buffet process, are useful in constructing nonparametric latent\nvariable models. However, the distribution implied by such models over the\nnumber of features exhibited by each data point may be poorly- suited for many\nmodeling tasks. In this paper, we propose a class of exchangeable nonparametric\npriors obtained by restricting the domain of existing models. Such models allow\nus to specify the distribution over the number of features per data point, and\ncan achieve better performance on data sets where the number of features is not\nwell-modeled by the original distribution.\n", "versions": [{"version": "v1", "created": "Wed, 5 Sep 2012 23:33:28 GMT"}], "update_date": "2012-09-07", "authors_parsed": [["Williamson", "Sinead", ""], ["Ghahramani", "Zoubin", ""], ["MacEachern", "Steven N.", ""], ["Xing", "Eric P.", ""]]}, {"id": "1209.1171", "submitter": "Qi Ye", "authors": "Gregory E. Fasshauer and Fred J. Hickernell and Qi Ye", "title": "Solving Support Vector Machines in Reproducing Kernel Banach Spaces with\n  Positive Definite Functions", "comments": "26 pages", "journal-ref": "Appl. Comput. Harmon. Anal., 38:115-139, 2015", "doi": "10.1016/j.acha.2014.03.007", "report-no": null, "categories": "stat.ML math.NA math.OC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper we solve support vector machines in reproducing kernel Banach\nspaces with reproducing kernels defined on nonsymmetric domains instead of the\ntraditional methods in reproducing kernel Hilbert spaces. Using the\northogonality of semi-inner-products, we can obtain the explicit\nrepresentations of the dual (normalized-duality-mapping) elements of support\nvector machine solutions. In addition, we can introduce the reproduction\nproperty in a generalized native space by Fourier transform techniques such\nthat it becomes a reproducing kernel Banach space, which can be even embedded\ninto Sobolev spaces, and its reproducing kernel is set up by the related\npositive definite function. The representations of the optimal solutions of\nsupport vector machines (regularized empirical risks) in these reproducing\nkernel Banach spaces are formulated explicitly in terms of positive definite\nfunctions, and their finite numbers of coefficients can be computed by fixed\npoint iteration. We also give some typical examples of reproducing kernel\nBanach spaces induced by Mat\\'ern functions (Sobolev splines) so that their\nsupport vector machine solutions are well computable as the classical\nalgorithms. Moreover, each of their reproducing bases includes information from\nmultiple training data points. The concept of reproducing kernel Banach spaces\noffers us a new numerical tool for solving support vector machines.\n", "versions": [{"version": "v1", "created": "Thu, 6 Sep 2012 03:47:04 GMT"}, {"version": "v2", "created": "Tue, 18 Dec 2012 04:50:55 GMT"}, {"version": "v3", "created": "Tue, 23 Apr 2013 23:40:04 GMT"}], "update_date": "2015-01-16", "authors_parsed": [["Fasshauer", "Gregory E.", ""], ["Hickernell", "Fred J.", ""], ["Ye", "Qi", ""]]}, {"id": "1209.1360", "submitter": "Youssef  Mroueh", "authors": "Youssef Mroueh, Tomaso Poggio, Lorenzo Rosasco, Jean-Jacques Slotine", "title": "Multiclass Learning with Simplex Coding", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper we discuss a novel framework for multiclass learning, defined\nby a suitable coding/decoding strategy, namely the simplex coding, that allows\nto generalize to multiple classes a relaxation approach commonly used in binary\nclassification. In this framework, a relaxation error analysis can be developed\navoiding constraints on the considered hypotheses class. Moreover, we show that\nin this setting it is possible to derive the first provably consistent\nregularized method with training/tuning complexity which is independent to the\nnumber of classes. Tools from convex analysis are introduced that can be used\nbeyond the scope of this paper.\n", "versions": [{"version": "v1", "created": "Thu, 6 Sep 2012 18:22:25 GMT"}, {"version": "v2", "created": "Fri, 14 Sep 2012 14:14:53 GMT"}], "update_date": "2015-03-20", "authors_parsed": [["Mroueh", "Youssef", ""], ["Poggio", "Tomaso", ""], ["Rosasco", "Lorenzo", ""], ["Slotine", "Jean-Jacques", ""]]}, {"id": "1209.1380", "submitter": "Matthew Malloy", "authors": "Matthew L. Malloy, Gongguo Tang, Robert D. Nowak", "title": "The Sample Complexity of Search over Multiple Populations", "comments": "To appear, IEEE Transactions on Information Theory", "journal-ref": null, "doi": "10.1109/TIT.2013.2258071", "report-no": null, "categories": "cs.IT math.IT stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper studies the sample complexity of searching over multiple\npopulations. We consider a large number of populations, each corresponding to\neither distribution P0 or P1. The goal of the search problem studied here is to\nfind one population corresponding to distribution P1 with as few samples as\npossible. The main contribution is to quantify the number of samples needed to\ncorrectly find one such population. We consider two general approaches:\nnon-adaptive sampling methods, which sample each population a predetermined\nnumber of times until a population following P1 is found, and adaptive sampling\nmethods, which employ sequential sampling schemes for each population. We first\nderive a lower bound on the number of samples required by any sampling scheme.\nWe then consider an adaptive procedure consisting of a series of sequential\nprobability ratio tests, and show it comes within a constant factor of the\nlower bound. We give explicit expressions for this constant when samples of the\npopulations follow Gaussian and Bernoulli distributions. An alternative\nadaptive scheme is discussed which does not require full knowledge of P1, and\ncomes within a constant factor of the optimal scheme. For comparison, a lower\nbound on the sampling requirements of any non-adaptive scheme is presented.\n", "versions": [{"version": "v1", "created": "Thu, 6 Sep 2012 19:38:22 GMT"}, {"version": "v2", "created": "Wed, 1 May 2013 18:50:36 GMT"}], "update_date": "2016-11-17", "authors_parsed": [["Malloy", "Matthew L.", ""], ["Tang", "Gongguo", ""], ["Nowak", "Robert D.", ""]]}, {"id": "1209.1450", "submitter": "Yannick Schwartz", "authors": "Yannick Schwartz (INRIA Saclay - Ile de France, LNAO), Ga\\\"el\n  Varoquaux (INRIA Saclay - Ile de France, LNAO), Bertrand Thirion (INRIA\n  Saclay - Ile de France, LNAO)", "title": "On spatial selectivity and prediction across conditions with fMRI", "comments": "PRNI 2012 : 2nd International Workshop on Pattern Recognition in\n  NeuroImaging, London : United Kingdom (2012)", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Researchers in functional neuroimaging mostly use activation coordinates to\nformulate their hypotheses. Instead, we propose to use the full statistical\nimages to define regions of interest (ROIs). This paper presents two machine\nlearning approaches, transfer learning and selection transfer, that are\ncompared upon their ability to identify the common patterns between brain\nactivation maps related to two functional tasks. We provide some preliminary\nquantification of these similarities, and show that selection transfer makes it\npossible to set a spatial scale yielding ROIs that are more specific to the\ncontext of interest than with transfer learning. In particular, selection\ntransfer outlines well known regions such as the Visual Word Form Area when\ndiscriminating between different visual tasks.\n", "versions": [{"version": "v1", "created": "Fri, 7 Sep 2012 06:28:42 GMT"}], "update_date": "2012-09-10", "authors_parsed": [["Schwartz", "Yannick", "", "INRIA Saclay - Ile de France, LNAO"], ["Varoquaux", "Ga\u00ebl", "", "INRIA Saclay - Ile de France, LNAO"], ["Thirion", "Bertrand", "", "INRIA\n  Saclay - Ile de France, LNAO"]]}, {"id": "1209.1557", "submitter": "Sohail Bahmani", "authors": "Sohail Bahmani, Petros T. Boufounos, and Bhiksha Raj", "title": "Learning Model-Based Sparsity via Projected Gradient Descent", "comments": null, "journal-ref": "IEEE Transactions on Information Theory 62(4):2092--2099, 2016", "doi": "10.1109/TIT.2016.2515078", "report-no": null, "categories": "stat.ML cs.LG math.OC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Several convex formulation methods have been proposed previously for\nstatistical estimation with structured sparsity as the prior. These methods\noften require a carefully tuned regularization parameter, often a cumbersome or\nheuristic exercise. Furthermore, the estimate that these methods produce might\nnot belong to the desired sparsity model, albeit accurately approximating the\ntrue parameter. Therefore, greedy-type algorithms could often be more desirable\nin estimating structured-sparse parameters. So far, these greedy methods have\nmostly focused on linear statistical models. In this paper we study the\nprojected gradient descent with non-convex structured-sparse parameter model as\nthe constraint set. Should the cost function have a Stable Model-Restricted\nHessian the algorithm produces an approximation for the desired minimizer. As\nan example we elaborate on application of the main results to estimation in\nGeneralized Linear Model.\n", "versions": [{"version": "v1", "created": "Fri, 7 Sep 2012 14:46:49 GMT"}, {"version": "v2", "created": "Fri, 9 Nov 2012 22:10:01 GMT"}, {"version": "v3", "created": "Mon, 30 Dec 2013 20:08:10 GMT"}, {"version": "v4", "created": "Wed, 27 Jan 2016 13:14:52 GMT"}], "update_date": "2016-03-23", "authors_parsed": [["Bahmani", "Sohail", ""], ["Boufounos", "Petros T.", ""], ["Raj", "Bhiksha", ""]]}, {"id": "1209.1688", "submitter": "Sewoong Oh", "authors": "Sahand Negahban, Sewoong Oh, Devavrat Shah", "title": "Rank Centrality: Ranking from Pair-wise Comparisons", "comments": "45 pages, 3 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The question of aggregating pair-wise comparisons to obtain a global ranking\nover a collection of objects has been of interest for a very long time: be it\nranking of online gamers (e.g. MSR's TrueSkill system) and chess players,\naggregating social opinions, or deciding which product to sell based on\ntransactions. In most settings, in addition to obtaining a ranking, finding\n`scores' for each object (e.g. player's rating) is of interest for\nunderstanding the intensity of the preferences.\n  In this paper, we propose Rank Centrality, an iterative rank aggregation\nalgorithm for discovering scores for objects (or items) from pair-wise\ncomparisons. The algorithm has a natural random walk interpretation over the\ngraph of objects with an edge present between a pair of objects if they are\ncompared; the score, which we call Rank Centrality, of an object turns out to\nbe its stationary probability under this random walk. To study the efficacy of\nthe algorithm, we consider the popular Bradley-Terry-Luce (BTL) model\n(equivalent to the Multinomial Logit (MNL) for pair-wise comparisons) in which\neach object has an associated score which determines the probabilistic outcomes\nof pair-wise comparisons between objects. In terms of the pair-wise marginal\nprobabilities, which is the main subject of this paper, the MNL model and the\nBTL model are identical. We bound the finite sample error rates between the\nscores assumed by the BTL model and those estimated by our algorithm. In\nparticular, the number of samples required to learn the score well with high\nprobability depends on the structure of the comparison graph. When the\nLaplacian of the comparison graph has a strictly positive spectral gap, e.g.\neach item is compared to a subset of randomly chosen items, this leads to\ndependence on the number of samples that is nearly order-optimal.\n", "versions": [{"version": "v1", "created": "Sat, 8 Sep 2012 04:42:18 GMT"}, {"version": "v2", "created": "Fri, 3 Jan 2014 06:28:50 GMT"}, {"version": "v3", "created": "Thu, 18 Jun 2015 21:42:49 GMT"}, {"version": "v4", "created": "Thu, 12 Nov 2015 17:51:33 GMT"}], "update_date": "2015-11-13", "authors_parsed": [["Negahban", "Sahand", ""], ["Oh", "Sewoong", ""], ["Shah", "Devavrat", ""]]}, {"id": "1209.1727", "submitter": "Sebastien Bubeck", "authors": "S\\'ebastien Bubeck, Nicol\\`o Cesa-Bianchi and G\\'abor Lugosi", "title": "Bandits with heavy tail", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The stochastic multi-armed bandit problem is well understood when the reward\ndistributions are sub-Gaussian. In this paper we examine the bandit problem\nunder the weaker assumption that the distributions have moments of order\n1+\\epsilon, for some $\\epsilon \\in (0,1]$. Surprisingly, moments of order 2\n(i.e., finite variance) are sufficient to obtain regret bounds of the same\norder as under sub-Gaussian reward distributions. In order to achieve such\nregret, we define sampling strategies based on refined estimators of the mean\nsuch as the truncated empirical mean, Catoni's M-estimator, and the\nmedian-of-means estimator. We also derive matching lower bounds that also show\nthat the best achievable regret deteriorates when \\epsilon <1.\n", "versions": [{"version": "v1", "created": "Sat, 8 Sep 2012 15:22:07 GMT"}], "update_date": "2012-09-11", "authors_parsed": [["Bubeck", "S\u00e9bastien", ""], ["Cesa-Bianchi", "Nicol\u00f2", ""], ["Lugosi", "G\u00e1bor", ""]]}, {"id": "1209.1873", "submitter": "Tong Zhang", "authors": "Shai Shalev-Shwartz and Tong Zhang", "title": "Stochastic Dual Coordinate Ascent Methods for Regularized Loss\n  Minimization", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG math.OC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Stochastic Gradient Descent (SGD) has become popular for solving large scale\nsupervised machine learning optimization problems such as SVM, due to their\nstrong theoretical guarantees. While the closely related Dual Coordinate Ascent\n(DCA) method has been implemented in various software packages, it has so far\nlacked good convergence analysis. This paper presents a new analysis of\nStochastic Dual Coordinate Ascent (SDCA) showing that this class of methods\nenjoy strong theoretical guarantees that are comparable or better than SGD.\nThis analysis justifies the effectiveness of SDCA for practical applications.\n", "versions": [{"version": "v1", "created": "Mon, 10 Sep 2012 03:25:29 GMT"}, {"version": "v2", "created": "Wed, 30 Jan 2013 15:30:25 GMT"}], "update_date": "2015-03-20", "authors_parsed": [["Shalev-Shwartz", "Shai", ""], ["Zhang", "Tong", ""]]}, {"id": "1209.1996", "submitter": "Alexander  Lorbert", "authors": "Alexander Lorbert and David M. Blei and Robert E. Schapire and Peter\n  J. Ramadge", "title": "A Bayesian Boosting Model", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We offer a novel view of AdaBoost in a statistical setting. We propose a\nBayesian model for binary classification in which label noise is modeled\nhierarchically. Using variational inference to optimize a dynamic evidence\nlower bound, we derive a new boosting-like algorithm called VIBoost. We show\nits close connections to AdaBoost and give experimental results from four\ndatasets.\n", "versions": [{"version": "v1", "created": "Mon, 10 Sep 2012 13:57:37 GMT"}], "update_date": "2012-09-11", "authors_parsed": [["Lorbert", "Alexander", ""], ["Blei", "David M.", ""], ["Schapire", "Robert E.", ""], ["Ramadge", "Peter J.", ""]]}, {"id": "1209.2139", "submitter": "Sen Yang", "authors": "Sen Yang, Zhaosong Lu, Xiaotong Shen, Peter Wonka, Jieping Ye", "title": "Fused Multiple Graphical Lasso", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we consider the problem of estimating multiple graphical\nmodels simultaneously using the fused lasso penalty, which encourages adjacent\ngraphs to share similar structures. A motivating example is the analysis of\nbrain networks of Alzheimer's disease using neuroimaging data. Specifically, we\nmay wish to estimate a brain network for the normal controls (NC), a brain\nnetwork for the patients with mild cognitive impairment (MCI), and a brain\nnetwork for Alzheimer's patients (AD). We expect the two brain networks for NC\nand MCI to share common structures but not to be identical to each other;\nsimilarly for the two brain networks for MCI and AD. The proposed formulation\ncan be solved using a second-order method. Our key technical contribution is to\nestablish the necessary and sufficient condition for the graphs to be\ndecomposable. Based on this key property, a simple screening rule is presented,\nwhich decomposes the large graphs into small subgraphs and allows an efficient\nestimation of multiple independent (small) subgraphs, dramatically reducing the\ncomputational cost. We perform experiments on both synthetic and real data; our\nresults demonstrate the effectiveness and efficiency of the proposed approach.\n", "versions": [{"version": "v1", "created": "Mon, 10 Sep 2012 20:13:42 GMT"}, {"version": "v2", "created": "Tue, 31 Dec 2013 03:19:45 GMT"}], "update_date": "2014-01-03", "authors_parsed": [["Yang", "Sen", ""], ["Lu", "Zhaosong", ""], ["Shen", "Xiaotong", ""], ["Wonka", "Peter", ""], ["Ye", "Jieping", ""]]}, {"id": "1209.2160", "submitter": "Patrick Breheny", "authors": "Patrick Breheny, Jian Huang", "title": "Group descent algorithms for nonconvex penalized linear and logistic\n  regression models with grouped predictors", "comments": null, "journal-ref": "Statistics and Computing, 25: 173-187 (2015)", "doi": "10.1007/s11222-013-9424-2", "report-no": null, "categories": "stat.CO stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Penalized regression is an attractive framework for variable selection\nproblems. Often, variables possess a grouping structure, and the relevant\nselection problem is that of selecting groups, not individual variables. The\ngroup lasso has been proposed as a way of extending the ideas of the lasso to\nthe problem of group selection. Nonconvex penalties such as SCAD and MCP have\nbeen proposed and shown to have several advantages over the lasso; these\npenalties may also be extended to the group selection problem, giving rise to\ngroup SCAD and group MCP methods. Here, we describe algorithms for fitting\nthese models stably and efficiently. In addition, we present simulation results\nand real data examples comparing and contrasting the statistical properties of\nthese methods.\n", "versions": [{"version": "v1", "created": "Mon, 10 Sep 2012 21:13:29 GMT"}, {"version": "v2", "created": "Fri, 23 Aug 2013 15:34:02 GMT"}], "update_date": "2016-07-20", "authors_parsed": [["Breheny", "Patrick", ""], ["Huang", "Jian", ""]]}, {"id": "1209.2388", "submitter": "Ohad Shamir", "authors": "Ohad Shamir", "title": "On the Complexity of Bandit and Derivative-Free Stochastic Convex\n  Optimization", "comments": "Version appearing in COLT (Conference on Learning Theory) 2013", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG math.OC stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The problem of stochastic convex optimization with bandit feedback (in the\nlearning community) or without knowledge of gradients (in the optimization\ncommunity) has received much attention in recent years, in the form of\nalgorithms and performance upper bounds. However, much less is known about the\ninherent complexity of these problems, and there are few lower bounds in the\nliterature, especially for nonlinear functions. In this paper, we investigate\nthe attainable error/regret in the bandit and derivative-free settings, as a\nfunction of the dimension d and the available number of queries T. We provide a\nprecise characterization of the attainable performance for strongly-convex and\nsmooth functions, which also imply a non-trivial lower bound for more general\nproblems. Moreover, we prove that in both the bandit and derivative-free\nsetting, the required number of queries must scale at least quadratically with\nthe dimension. Finally, we show that on the natural class of quadratic\nfunctions, it is possible to obtain a \"fast\" O(1/T) error rate in terms of T,\nunder mild assumptions, even without having access to gradients. To the best of\nour knowledge, this is the first such rate in a derivative-free stochastic\nsetting, and holds despite previous results which seem to imply the contrary.\n", "versions": [{"version": "v1", "created": "Tue, 11 Sep 2012 18:16:56 GMT"}, {"version": "v2", "created": "Fri, 26 Oct 2012 20:03:52 GMT"}, {"version": "v3", "created": "Mon, 29 Apr 2013 18:48:17 GMT"}], "update_date": "2013-04-30", "authors_parsed": [["Shamir", "Ohad", ""]]}, {"id": "1209.2434", "submitter": "Kevin Jamieson", "authors": "Kevin G. Jamieson, Robert D. Nowak, Benjamin Recht", "title": "Query Complexity of Derivative-Free Optimization", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper provides lower bounds on the convergence rate of Derivative Free\nOptimization (DFO) with noisy function evaluations, exposing a fundamental and\nunavoidable gap between the performance of algorithms with access to gradients\nand those with access to only function evaluations. However, there are\nsituations in which DFO is unavoidable, and for such situations we propose a\nnew DFO algorithm that is proved to be near optimal for the class of strongly\nconvex objective functions. A distinctive feature of the algorithm is that it\nuses only Boolean-valued function comparisons, rather than function\nevaluations. This makes the algorithm useful in an even wider range of\napplications, such as optimization based on paired comparisons from human\nsubjects, for example. We also show that regardless of whether DFO is based on\nnoisy function evaluations or Boolean-valued function comparisons, the\nconvergence rate is the same.\n", "versions": [{"version": "v1", "created": "Tue, 11 Sep 2012 20:37:02 GMT"}], "update_date": "2012-09-13", "authors_parsed": [["Jamieson", "Kevin G.", ""], ["Nowak", "Robert D.", ""], ["Recht", "Benjamin", ""]]}, {"id": "1209.2655", "submitter": "Marco Cuturi", "authors": "Marco Cuturi", "title": "Positivity and Transportation", "comments": "13 pages, 1 figure", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML math.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We prove in this paper that the weighted volume of the set of integral\ntransportation matrices between two integral histograms r and c of equal sum is\na positive definite kernel of r and c when the set of considered weights forms\na positive definite matrix. The computation of this quantity, despite being the\nsubject of a significant research effort in algebraic statistics, remains an\nintractable challenge for histograms of even modest dimensions. We propose an\nalternative kernel which, rather than considering all matrices of the\ntransportation polytope, only focuses on a sub-sample of its vertices known as\nits Northwestern corner solutions. The resulting kernel is positive definite\nand can be computed with a number of operations O(R^2d) that grows linearly in\nthe complexity of the dimension d, where R^2, the total amount of sampled\nvertices, is a parameter that controls the complexity of the kernel.\n", "versions": [{"version": "v1", "created": "Wed, 12 Sep 2012 16:33:08 GMT"}], "update_date": "2012-09-13", "authors_parsed": [["Cuturi", "Marco", ""]]}, {"id": "1209.2669", "submitter": "Deniz Akdemir", "authors": "Deniz Akdemir", "title": "Likelihood Estimation with Incomplete Array Variate Observations", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME math.ST stat.ML stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Missing data is an important challenge when dealing with high dimensional\ndata arranged in the form of an array. In this paper, we propose methods for\nestimation of the parameters of array variate normal probability model from\npartially observed multiway data. The methods developed here are useful for\nmissing data imputation, estimation of mean and covariance parameters for\nmultiway data. A multiway semi-parametric mixed effects model that allows\nseparation of multiway covariance effects is also defined and an efficient\nalgorithm for estimation is recommended. We provide simulation results along\nwith real life data from genetics to demonstrate these methods.\n", "versions": [{"version": "v1", "created": "Wed, 12 Sep 2012 17:08:36 GMT"}, {"version": "v2", "created": "Mon, 17 Sep 2012 16:10:45 GMT"}, {"version": "v3", "created": "Tue, 15 Oct 2013 18:59:47 GMT"}, {"version": "v4", "created": "Mon, 2 Jun 2014 19:45:27 GMT"}, {"version": "v5", "created": "Sun, 6 Jul 2014 00:28:39 GMT"}, {"version": "v6", "created": "Sat, 11 Oct 2014 11:18:04 GMT"}, {"version": "v7", "created": "Wed, 24 Dec 2014 18:48:16 GMT"}, {"version": "v8", "created": "Tue, 30 Dec 2014 14:49:14 GMT"}, {"version": "v9", "created": "Mon, 5 Jan 2015 15:54:01 GMT"}], "update_date": "2015-01-06", "authors_parsed": [["Akdemir", "Deniz", ""]]}, {"id": "1209.2693", "submitter": "Daniil Ryabko", "authors": "Ronald Ortner, Daniil Ryabko, Peter Auer, R\\'emi Munos", "title": "Regret Bounds for Restless Markov Bandits", "comments": "In proceedings of The 23rd International Conference on Algorithmic\n  Learning Theory (ALT 2012)", "journal-ref": "Proceedings of ALT, Lyon, France, LNCS 7568, pp.214-228, 2012", "doi": null, "report-no": null, "categories": "cs.LG math.OC stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider the restless Markov bandit problem, in which the state of each\narm evolves according to a Markov process independently of the learner's\nactions. We suggest an algorithm that after $T$ steps achieves\n$\\tilde{O}(\\sqrt{T})$ regret with respect to the best policy that knows the\ndistributions of all arms. No assumptions on the Markov chains are made except\nthat they are irreducible. In addition, we show that index-based policies are\nnecessarily suboptimal for the considered problem.\n", "versions": [{"version": "v1", "created": "Wed, 12 Sep 2012 19:14:21 GMT"}], "update_date": "2012-10-23", "authors_parsed": [["Ortner", "Ronald", ""], ["Ryabko", "Daniil", ""], ["Auer", "Peter", ""], ["Munos", "R\u00e9mi", ""]]}, {"id": "1209.2784", "submitter": "Nishant Mehta", "authors": "Nishant A. Mehta, Dongryeol Lee, Alexander G. Gray", "title": "Minimax Multi-Task Learning and a Generalized Loss-Compositional\n  Paradigm for MTL", "comments": "appearing at NIPS 2012", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Since its inception, the modus operandi of multi-task learning (MTL) has been\nto minimize the task-wise mean of the empirical risks. We introduce a\ngeneralized loss-compositional paradigm for MTL that includes a spectrum of\nformulations as a subfamily. One endpoint of this spectrum is minimax MTL: a\nnew MTL formulation that minimizes the maximum of the tasks' empirical risks.\nVia a certain relaxation of minimax MTL, we obtain a continuum of MTL\nformulations spanning minimax MTL and classical MTL. The full paradigm itself\nis loss-compositional, operating on the vector of empirical risks. It\nincorporates minimax MTL, its relaxations, and many new MTL formulations as\nspecial cases. We show theoretically that minimax MTL tends to avoid worst case\noutcomes on newly drawn test tasks in the learning to learn (LTL) test setting.\nThe results of several MTL formulations on synthetic and real problems in the\nMTL and LTL test settings are encouraging.\n", "versions": [{"version": "v1", "created": "Thu, 13 Sep 2012 06:14:31 GMT"}], "update_date": "2012-09-14", "authors_parsed": [["Mehta", "Nishant A.", ""], ["Lee", "Dongryeol", ""], ["Gray", "Alexander G.", ""]]}, {"id": "1209.3079", "submitter": "Nikhil Rao", "authors": "Nikhil Rao, Benjamin Recht and Robert Nowak", "title": "Signal Recovery in Unions of Subspaces with Applications to Compressive\n  Imaging", "comments": "arXiv admin note: substantial text overlap with arXiv:1106.4355", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML math.OC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In applications ranging from communications to genetics, signals can be\nmodeled as lying in a union of subspaces. Under this model, signal coefficients\nthat lie in certain subspaces are active or inactive together. The potential\nsubspaces are known in advance, but the particular set of subspaces that are\nactive (i.e., in the signal support) must be learned from measurements. We show\nthat exploiting knowledge of subspaces can further reduce the number of\nmeasurements required for exact signal recovery, and derive universal bounds\nfor the number of measurements needed. The bound is universal in the sense that\nit only depends on the number of subspaces under consideration, and their\norientation relative to each other. The particulars of the subspaces (e.g.,\ncompositions, dimensions, extents, overlaps, etc.) does not affect the results\nwe obtain. In the process, we derive sample complexity bounds for the special\ncase of the group lasso with overlapping groups (the latent group lasso), which\nis used in a variety of applications. Finally, we also show that wavelet\ntransform coefficients of images can be modeled as lying in groups, and hence\ncan be efficiently recovered using group lasso methods.\n", "versions": [{"version": "v1", "created": "Fri, 14 Sep 2012 02:39:26 GMT"}], "update_date": "2012-09-17", "authors_parsed": [["Rao", "Nikhil", ""], ["Recht", "Benjamin", ""], ["Nowak", "Robert", ""]]}, {"id": "1209.3230", "submitter": "Emile Richard", "authors": "Emile Richard, Stephane Gaiffas, Nicolas Vayatis", "title": "Link Prediction in Graphs with Autoregressive Features", "comments": "NIPS 2012", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In the paper, we consider the problem of link prediction in time-evolving\ngraphs. We assume that certain graph features, such as the node degree, follow\na vector autoregressive (VAR) model and we propose to use this information to\nimprove the accuracy of prediction. Our strategy involves a joint optimization\nprocedure over the space of adjacency matrices and VAR matrices which takes\ninto account both sparsity and low rank properties of the matrices. Oracle\ninequalities are derived and illustrate the trade-offs in the choice of\nsmoothing parameters when modeling the joint effect of sparsity and low rank\nproperty. The estimate is computed efficiently using proximal methods through a\ngeneralized forward-backward agorithm.\n", "versions": [{"version": "v1", "created": "Fri, 14 Sep 2012 15:27:45 GMT"}], "update_date": "2012-09-17", "authors_parsed": [["Richard", "Emile", ""], ["Gaiffas", "Stephane", ""], ["Vayatis", "Nicolas", ""]]}, {"id": "1209.3352", "submitter": "Shipra Agrawal", "authors": "Shipra Agrawal, Navin Goyal", "title": "Thompson Sampling for Contextual Bandits with Linear Payoffs", "comments": "Improvements from previous version: (1) dependence on d improved from\n  d^2 to d^{3/2} (2) Simpler and more modular proof techniques (3) bounds in\n  terms of log(N) added", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.DS stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Thompson Sampling is one of the oldest heuristics for multi-armed bandit\nproblems. It is a randomized algorithm based on Bayesian ideas, and has\nrecently generated significant interest after several studies demonstrated it\nto have better empirical performance compared to the state-of-the-art methods.\nHowever, many questions regarding its theoretical performance remained open. In\nthis paper, we design and analyze a generalization of Thompson Sampling\nalgorithm for the stochastic contextual multi-armed bandit problem with linear\npayoff functions, when the contexts are provided by an adaptive adversary. This\nis among the most important and widely studied versions of the contextual\nbandits problem. We provide the first theoretical guarantees for the contextual\nversion of Thompson Sampling. We prove a high probability regret bound of\n$\\tilde{O}(d^{3/2}\\sqrt{T})$ (or $\\tilde{O}(d\\sqrt{T \\log(N)})$), which is the\nbest regret bound achieved by any computationally efficient algorithm available\nfor this problem in the current literature, and is within a factor of\n$\\sqrt{d}$ (or $\\sqrt{\\log(N)}$) of the information-theoretic lower bound for\nthis problem.\n", "versions": [{"version": "v1", "created": "Sat, 15 Sep 2012 03:27:11 GMT"}, {"version": "v2", "created": "Fri, 22 Feb 2013 18:35:56 GMT"}, {"version": "v3", "created": "Thu, 30 Jan 2014 07:00:54 GMT"}, {"version": "v4", "created": "Mon, 3 Feb 2014 07:09:03 GMT"}], "update_date": "2014-02-04", "authors_parsed": [["Agrawal", "Shipra", ""], ["Goyal", "Navin", ""]]}, {"id": "1209.3353", "submitter": "Shipra Agrawal", "authors": "Shipra Agrawal, Navin Goyal", "title": "Further Optimal Regret Bounds for Thompson Sampling", "comments": "arXiv admin note: substantial text overlap with arXiv:1111.1797", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.DS stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Thompson Sampling is one of the oldest heuristics for multi-armed bandit\nproblems. It is a randomized algorithm based on Bayesian ideas, and has\nrecently generated significant interest after several studies demonstrated it\nto have better empirical performance compared to the state of the art methods.\nIn this paper, we provide a novel regret analysis for Thompson Sampling that\nsimultaneously proves both the optimal problem-dependent bound of\n$(1+\\epsilon)\\sum_i \\frac{\\ln T}{\\Delta_i}+O(\\frac{N}{\\epsilon^2})$ and the\nfirst near-optimal problem-independent bound of $O(\\sqrt{NT\\ln T})$ on the\nexpected regret of this algorithm. Our near-optimal problem-independent bound\nsolves a COLT 2012 open problem of Chapelle and Li. The optimal\nproblem-dependent regret bound for this problem was first proven recently by\nKaufmann et al. [ALT 2012]. Our novel martingale-based analysis techniques are\nconceptually simple, easily extend to distributions other than the Beta\ndistribution, and also extend to the more general contextual bandits setting\n[Manuscript, Agrawal and Goyal, 2012].\n", "versions": [{"version": "v1", "created": "Sat, 15 Sep 2012 03:41:18 GMT"}], "update_date": "2012-09-18", "authors_parsed": [["Agrawal", "Shipra", ""], ["Goyal", "Navin", ""]]}, {"id": "1209.3431", "submitter": "Mladen Kolar", "authors": "Sivaraman Balakrishnan, Mladen Kolar, Alessandro Rinaldo, Aarti Singh", "title": "Recovering Block-structured Activations Using Compressive Measurements", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider the problems of detection and localization of a contiguous block\nof weak activation in a large matrix, from a small number of noisy, possibly\nadaptive, compressive (linear) measurements. This is closely related to the\nproblem of compressed sensing, where the task is to estimate a sparse vector\nusing a small number of linear measurements. Contrary to results in compressed\nsensing, where it has been shown that neither adaptivity nor contiguous\nstructure help much, we show that for reliable localization the magnitude of\nthe weakest signals is strongly influenced by both structure and the ability to\nchoose measurements adaptively while for detection neither adaptivity nor\nstructure reduce the requirement on the magnitude of the signal. We\ncharacterize the precise tradeoffs between the various problem parameters, the\nsignal strength and the number of measurements required to reliably detect and\nlocalize the block of activation. The sufficient conditions are complemented\nwith information theoretic lower bounds.\n", "versions": [{"version": "v1", "created": "Sat, 15 Sep 2012 20:06:33 GMT"}, {"version": "v2", "created": "Mon, 24 Jun 2013 23:20:08 GMT"}], "update_date": "2013-06-26", "authors_parsed": [["Balakrishnan", "Sivaraman", ""], ["Kolar", "Mladen", ""], ["Rinaldo", "Alessandro", ""], ["Singh", "Aarti", ""]]}, {"id": "1209.3442", "submitter": "Mingyuan Zhou", "authors": "Mingyuan Zhou and Lawrence Carin", "title": "Negative Binomial Process Count and Mixture Modeling", "comments": "To appear in IEEE Trans. Pattern Analysis and Machine Intelligence:\n  Special Issue on Bayesian Nonparametrics. 14 pages, 4 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The seemingly disjoint problems of count and mixture modeling are united\nunder the negative binomial (NB) process. A gamma process is employed to model\nthe rate measure of a Poisson process, whose normalization provides a random\nprobability measure for mixture modeling and whose marginalization leads to an\nNB process for count modeling. A draw from the NB process consists of a Poisson\ndistributed finite number of distinct atoms, each of which is associated with a\nlogarithmic distributed number of data samples. We reveal relationships between\nvarious count- and mixture-modeling distributions and construct a\nPoisson-logarithmic bivariate distribution that connects the NB and Chinese\nrestaurant table distributions. Fundamental properties of the models are\ndeveloped, and we derive efficient Bayesian inference. It is shown that with\naugmentation and normalization, the NB process and gamma-NB process can be\nreduced to the Dirichlet process and hierarchical Dirichlet process,\nrespectively. These relationships highlight theoretical, structural and\ncomputational advantages of the NB process. A variety of NB processes,\nincluding the beta-geometric, beta-NB, marked-beta-NB, marked-gamma-NB and\nzero-inflated-NB processes, with distinct sharing mechanisms, are also\nconstructed. These models are applied to topic modeling, with connections made\nto existing algorithms under Poisson factor analysis. Example results show the\nimportance of inferring both the NB dispersion and probability parameters.\n", "versions": [{"version": "v1", "created": "Sat, 15 Sep 2012 21:55:36 GMT"}, {"version": "v2", "created": "Fri, 17 May 2013 19:29:50 GMT"}, {"version": "v3", "created": "Sun, 13 Oct 2013 01:01:39 GMT"}], "update_date": "2013-10-15", "authors_parsed": [["Zhou", "Mingyuan", ""], ["Carin", "Lawrence", ""]]}, {"id": "1209.3730", "submitter": "Coryn Bailer-Jones", "authors": "C.A.L. Bailer-Jones (Max Planck Institute for Astronomy, Heidelberg)", "title": "A Bayesian method for the analysis of deterministic and stochastic time\n  series", "comments": "Published in A&A as free access article. Software and additional\n  information available from http://www.mpia.de/~calj/ctsmod.html", "journal-ref": "Astronomy & Astrophysics 546, A89 (2012)", "doi": "10.1051/0004-6361/201220109", "report-no": null, "categories": "astro-ph.IM astro-ph.SR physics.data-an stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  I introduce a general, Bayesian method for modelling univariate time series\ndata assumed to be drawn from a continuous, stochastic process. The method\naccommodates arbitrary temporal sampling, and takes into account measurement\nuncertainties for arbitrary error models (not just Gaussian) on both the time\nand signal variables. Any model for the deterministic component of the\nvariation of the signal with time is supported, as is any model of the\nstochastic component on the signal and time variables. Models illustrated here\nare constant and sinusoidal models for the signal mean combined with a Gaussian\nstochastic component, as well as a purely stochastic model, the\nOrnstein-Uhlenbeck process. The posterior probability distribution over model\nparameters is determined via Monte Carlo sampling. Models are compared using\nthe \"cross-validation likelihood\", in which the posterior-averaged likelihood\nfor different partitions of the data are combined. In principle this is more\nrobust to changes in the prior than is the evidence (the prior-averaged\nlikelihood). The method is demonstrated by applying it to the light curves of\n11 ultra cool dwarf stars, claimed by a previous study to show statistically\nsignificant variability. This is reassessed here by calculating the\ncross-validation likelihood for various time series models, including a null\nhypothesis of no variability beyond the error bars. 10 of 11 light curves are\nconfirmed as being significantly variable, and one of these seems to be\nperiodic, with two plausible periods identified. Another object is best\ndescribed by the Ornstein-Uhlenbeck process, a conclusion which is obviously\nlimited to the set of models actually tested.\n", "versions": [{"version": "v1", "created": "Mon, 17 Sep 2012 17:44:29 GMT"}, {"version": "v2", "created": "Tue, 23 Oct 2012 06:37:11 GMT"}], "update_date": "2012-10-24", "authors_parsed": [["Bailer-Jones", "C. A. L.", "", "Max Planck Institute for Astronomy, Heidelberg"]]}, {"id": "1209.3761", "submitter": "Ming Sun", "authors": "Ming Sun, Carey E. Priebe, Minh Tang", "title": "Generalized Canonical Correlation Analysis for Disparate Data Fusion", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Manifold matching works to identify embeddings of multiple disparate data\nspaces into the same low-dimensional space, where joint inference can be\npursued. It is an enabling methodology for fusion and inference from multiple\nand massive disparate data sources. In this paper we focus on a method called\nCanonical Correlation Analysis (CCA) and its generalization Generalized\nCanonical Correlation Analysis (GCCA), which belong to the more general Reduced\nRank Regression (RRR) framework. We present an efficiency investigation of CCA\nand GCCA under different training conditions for a particular text document\nclassification task.\n", "versions": [{"version": "v1", "created": "Mon, 17 Sep 2012 19:52:38 GMT"}], "update_date": "2012-09-18", "authors_parsed": [["Sun", "Ming", ""], ["Priebe", "Carey E.", ""], ["Tang", "Minh", ""]]}, {"id": "1209.4115", "submitter": "Wojciech Samek", "authors": "Wojciech Samek, Frank C. Meinecke, Klaus-Robert M\\\"uller", "title": "Transferring Subspaces Between Subjects in Brain-Computer Interfacing", "comments": null, "journal-ref": "W. Samek, F. C. Meinecke and K-R. M\\\"uller, Transferring Subspaces\n  Between Subjects in Brain-Computer Interfacing, IEEE Transactions on\n  Biomedical Engineering, 2013", "doi": "10.1109/TBME.2013.2253608", "report-no": null, "categories": "stat.ML cs.HC cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Compensating changes between a subjects' training and testing session in\nBrain Computer Interfacing (BCI) is challenging but of great importance for a\nrobust BCI operation. We show that such changes are very similar between\nsubjects, thus can be reliably estimated using data from other users and\nutilized to construct an invariant feature space. This novel approach to\nlearning from other subjects aims to reduce the adverse effects of common\nnon-stationarities, but does not transfer discriminative information. This is\nan important conceptual difference to standard multi-subject methods that e.g.\nimprove the covariance matrix estimation by shrinking it towards the average of\nother users or construct a global feature space. These methods do not reduces\nthe shift between training and test data and may produce poor results when\nsubjects have very different signal characteristics. In this paper we compare\nour approach to two state-of-the-art multi-subject methods on toy data and two\ndata sets of EEG recordings from subjects performing motor imagery. We show\nthat it can not only achieve a significant increase in performance, but also\nthat the extracted change patterns allow for a neurophysiologically meaningful\ninterpretation.\n", "versions": [{"version": "v1", "created": "Tue, 18 Sep 2012 22:37:10 GMT"}, {"version": "v2", "created": "Wed, 3 Apr 2013 17:26:03 GMT"}], "update_date": "2013-04-04", "authors_parsed": [["Samek", "Wojciech", ""], ["Meinecke", "Frank C.", ""], ["M\u00fcller", "Klaus-Robert", ""]]}, {"id": "1209.4120", "submitter": "Elad Gilboa Elad Gilboa", "authors": "Elad Gilboa, Yunus Saat\\c{c}i, John P. Cunningham", "title": "Scaling Multidimensional Inference for Structured Gaussian Processes", "comments": "14 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Exact Gaussian Process (GP) regression has O(N^3) runtime for data size N,\nmaking it intractable for large N. Many algorithms for improving GP scaling\napproximate the covariance with lower rank matrices. Other work has exploited\nstructure inherent in particular covariance functions, including GPs with\nimplied Markov structure, and equispaced inputs (both enable O(N) runtime).\nHowever, these GP advances have not been extended to the multidimensional input\nsetting, despite the preponderance of multidimensional applications. This paper\nintroduces and tests novel extensions of structured GPs to multidimensional\ninputs. We present new methods for additive GPs, showing a novel connection\nbetween the classic backfitting method and the Bayesian framework. To achieve\noptimal accuracy-complexity tradeoff, we extend this model with a novel variant\nof projection pursuit regression. Our primary result -- projection pursuit\nGaussian Process Regression -- shows orders of magnitude speedup while\npreserving high accuracy. The natural second and third steps include\nnon-Gaussian observations and higher dimensional equispaced grid methods. We\nintroduce novel techniques to address both of these necessary directions. We\nthoroughly illustrate the power of these three advances on several datasets,\nachieving close performance to the naive Full GP at orders of magnitude less\ncost.\n", "versions": [{"version": "v1", "created": "Tue, 18 Sep 2012 23:03:01 GMT"}, {"version": "v2", "created": "Fri, 21 Sep 2012 15:35:06 GMT"}], "update_date": "2012-09-24", "authors_parsed": [["Gilboa", "Elad", ""], ["Saat\u00e7i", "Yunus", ""], ["Cunningham", "John P.", ""]]}, {"id": "1209.4129", "submitter": "John Duchi", "authors": "Yuchen Zhang and John C. Duchi and Martin Wainwright", "title": "Comunication-Efficient Algorithms for Statistical Optimization", "comments": "44 pages, to appear in Journal of Machine Learning Research (JMLR)", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We analyze two communication-efficient algorithms for distributed statistical\noptimization on large-scale data sets. The first algorithm is a standard\naveraging method that distributes the $N$ data samples evenly to $\\nummac$\nmachines, performs separate minimization on each subset, and then averages the\nestimates. We provide a sharp analysis of this average mixture algorithm,\nshowing that under a reasonable set of conditions, the combined parameter\nachieves mean-squared error that decays as $\\order(N^{-1}+(N/m)^{-2})$.\nWhenever $m \\le \\sqrt{N}$, this guarantee matches the best possible rate\nachievable by a centralized algorithm having access to all $\\totalnumobs$\nsamples. The second algorithm is a novel method, based on an appropriate form\nof bootstrap subsampling. Requiring only a single round of communication, it\nhas mean-squared error that decays as $\\order(N^{-1} + (N/m)^{-3})$, and so is\nmore robust to the amount of parallelization. In addition, we show that a\nstochastic gradient-based method attains mean-squared error decaying as\n$O(N^{-1} + (N/ m)^{-3/2})$, easing computation at the expense of penalties in\nthe rate of convergence. We also provide experimental evaluation of our\nmethods, investigating their performance both on simulated data and on a\nlarge-scale regression problem from the internet search domain. In particular,\nwe show that our methods can be used to efficiently solve an advertisement\nprediction problem from the Chinese SoSo Search Engine, which involves logistic\nregression with $N \\approx 2.4 \\times 10^8$ samples and $d \\approx 740,000$\ncovariates.\n", "versions": [{"version": "v1", "created": "Wed, 19 Sep 2012 01:27:40 GMT"}, {"version": "v2", "created": "Thu, 7 Mar 2013 03:12:01 GMT"}, {"version": "v3", "created": "Fri, 11 Oct 2013 19:23:38 GMT"}], "update_date": "2013-10-14", "authors_parsed": [["Zhang", "Yuchen", ""], ["Duchi", "John C.", ""], ["Wainwright", "Martin", ""]]}, {"id": "1209.4280", "submitter": "Ali Taylan Cemgil", "authors": "Y. Kenan Yilmaz and A. Taylan Cemgil", "title": "Alpha/Beta Divergences and Tweedie Models", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.IT math.IT math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We describe the underlying probabilistic interpretation of alpha and beta\ndivergences. We first show that beta divergences are inherently tied to Tweedie\ndistributions, a particular type of exponential family, known as exponential\ndispersion models. Starting from the variance function of a Tweedie model, we\noutline how to get alpha and beta divergences as special cases of Csisz\\'ar's\n$f$ and Bregman divergences. This result directly generalizes the well-known\nrelationship between the Gaussian distribution and least squares estimation to\nTweedie models and beta divergence minimization.\n", "versions": [{"version": "v1", "created": "Wed, 19 Sep 2012 15:20:47 GMT"}], "update_date": "2012-09-20", "authors_parsed": [["Yilmaz", "Y. Kenan", ""], ["Cemgil", "A. Taylan", ""]]}, {"id": "1209.4360", "submitter": "Chong Wang", "authors": "Chong Wang and David M. Blei", "title": "Variational Inference in Nonconjugate Models", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Mean-field variational methods are widely used for approximate posterior\ninference in many probabilistic models. In a typical application, mean-field\nmethods approximately compute the posterior with a coordinate-ascent\noptimization algorithm. When the model is conditionally conjugate, the\ncoordinate updates are easily derived and in closed form. However, many models\nof interest---like the correlated topic model and Bayesian logistic\nregression---are nonconjuate. In these models, mean-field methods cannot be\ndirectly applied and practitioners have had to develop variational algorithms\non a case-by-case basis. In this paper, we develop two generic methods for\nnonconjugate models, Laplace variational inference and delta method variational\ninference. Our methods have several advantages: they allow for easily derived\nvariational algorithms with a wide class of nonconjugate models; they extend\nand unify some of the existing algorithms that have been derived for specific\nmodels; and they work well on real-world datasets. We studied our methods on\nthe correlated topic model, Bayesian logistic regression, and hierarchical\nBayesian logistic regression.\n", "versions": [{"version": "v1", "created": "Wed, 19 Sep 2012 20:05:44 GMT"}, {"version": "v2", "created": "Fri, 21 Sep 2012 17:56:16 GMT"}, {"version": "v3", "created": "Fri, 11 Jan 2013 16:39:54 GMT"}, {"version": "v4", "created": "Tue, 12 Mar 2013 22:54:46 GMT"}], "update_date": "2013-03-14", "authors_parsed": [["Wang", "Chong", ""], ["Blei", "David M.", ""]]}, {"id": "1209.4551", "submitter": "Serge Iovleff", "authors": "Serge Iovleff (LPP, INRIA Lille - Nord Europe)", "title": "Probabilistic Auto-Associative Models and Semi-Linear PCA", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Auto-Associative models cover a large class of methods used in data analysis.\nIn this paper, we describe the generals properties of these models when the\nprojection component is linear and we propose and test an easy to implement\nProbabilistic Semi-Linear Auto- Associative model in a Gaussian setting. We\nshow it is a generalization of the PCA model to the semi-linear case. Numerical\nexperiments on simulated datasets and a real astronomical application highlight\nthe interest of this approach\n", "versions": [{"version": "v1", "created": "Thu, 20 Sep 2012 14:42:48 GMT"}], "update_date": "2012-09-21", "authors_parsed": [["Iovleff", "Serge", "", "LPP, INRIA Lille - Nord Europe"]]}, {"id": "1209.4690", "submitter": "Wei-Yin Loh", "authors": "Wei-Yin Loh, Wei Zheng", "title": "Regression trees for longitudinal and multiresponse data", "comments": "Published in at http://dx.doi.org/10.1214/12-AOAS596 the Annals of\n  Applied Statistics (http://www.imstat.org/aoas/) by the Institute of\n  Mathematical Statistics (http://www.imstat.org)", "journal-ref": "Annals of Applied Statistics 2013, Vol. 7, No. 1, 495-522", "doi": "10.1214/12-AOAS596", "report-no": "IMS-AOAS-AOAS596", "categories": "stat.ML stat.AP stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Previous algorithms for constructing regression tree models for longitudinal\nand multiresponse data have mostly followed the CART approach. Consequently,\nthey inherit the same selection biases and computational difficulties as CART.\nWe propose an alternative, based on the GUIDE approach, that treats each\nlongitudinal data series as a curve and uses chi-squared tests of the residual\ncurve patterns to select a variable to split each node of the tree. Besides\nbeing unbiased, the method is applicable to data with fixed and random time\npoints and with missing values in the response or predictor variables.\nSimulation results comparing its mean squared prediction error with that of\nMVPART are given, as well as examples comparing it with standard linear mixed\neffects and generalized estimating equation models. Conditions for asymptotic\nconsistency of regression tree function estimates are also given.\n", "versions": [{"version": "v1", "created": "Fri, 21 Sep 2012 02:07:31 GMT"}, {"version": "v2", "created": "Mon, 27 May 2013 05:34:09 GMT"}], "update_date": "2013-05-28", "authors_parsed": [["Loh", "Wei-Yin", ""], ["Zheng", "Wei", ""]]}, {"id": "1209.4825", "submitter": "Tapio Pahikkala", "authors": "Tapio Pahikkala, Antti Airola, Michiel Stock, Bernard De Baets, Willem\n  Waegeman", "title": "Efficient Regularized Least-Squares Algorithms for Conditional Ranking\n  on Relational Data", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In domains like bioinformatics, information retrieval and social network\nanalysis, one can find learning tasks where the goal consists of inferring a\nranking of objects, conditioned on a particular target object. We present a\ngeneral kernel framework for learning conditional rankings from various types\nof relational data, where rankings can be conditioned on unseen data objects.\nWe propose efficient algorithms for conditional ranking by optimizing squared\nregression and ranking loss functions. We show theoretically, that learning\nwith the ranking loss is likely to generalize better than with the regression\nloss. Further, we prove that symmetry or reciprocity properties of relations\ncan be efficiently enforced in the learned models. Experiments on synthetic and\nreal-world data illustrate that the proposed methods deliver state-of-the-art\nperformance in terms of predictive power and computational efficiency.\nMoreover, we also show empirically that incorporating symmetry or reciprocity\nproperties can improve the generalization performance.\n", "versions": [{"version": "v1", "created": "Fri, 21 Sep 2012 14:14:35 GMT"}, {"version": "v2", "created": "Sat, 8 Jun 2013 14:23:04 GMT"}], "update_date": "2013-06-11", "authors_parsed": [["Pahikkala", "Tapio", ""], ["Airola", "Antti", ""], ["Stock", "Michiel", ""], ["De Baets", "Bernard", ""], ["Waegeman", "Willem", ""]]}, {"id": "1209.4887", "submitter": "Cristian Rojas", "authors": "Cristian R. Rojas, Dimitrios Katselis and H{\\aa}kan Hjalmarsson", "title": "A Note on the SPICE Method", "comments": "5 pages, 1 figure. Submitted to the IEEE Transactions on Signal\n  Processing", "journal-ref": null, "doi": "10.1109/TSP.2013.2272291", "report-no": null, "categories": "stat.ML cs.SY", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this article, we analyze the SPICE method developed in [1], and establish\nits connections with other standard sparse estimation methods such as the Lasso\nand the LAD-Lasso. This result positions SPICE as a computationally efficient\ntechnique for the calculation of Lasso-type estimators. Conversely, this\nconnection is very useful for establishing the asymptotic properties of SPICE\nunder several problem scenarios and for suggesting suitable modifications in\ncases where the naive version of SPICE would not work.\n", "versions": [{"version": "v1", "created": "Fri, 21 Sep 2012 19:15:16 GMT"}], "update_date": "2015-06-11", "authors_parsed": [["Rojas", "Cristian R.", ""], ["Katselis", "Dimitrios", ""], ["Hjalmarsson", "H\u00e5kan", ""]]}, {"id": "1209.4951", "submitter": "Tu Xu", "authors": "Tu Xu and Junhui Wang", "title": "An efficient model-free estimation of multiclass conditional probability", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Conventional multiclass conditional probability estimation methods, such as\nFisher's discriminate analysis and logistic regression, often require\nrestrictive distributional model assumption. In this paper, a model-free\nestimation method is proposed to estimate multiclass conditional probability\nthrough a series of conditional quantile regression functions. Specifically,\nthe conditional class probability is formulated as difference of corresponding\ncumulative distribution functions, where the cumulative distribution functions\ncan be converted from the estimated conditional quantile regression functions.\nThe proposed estimation method is also efficient as its computation cost does\nnot increase exponentially with the number of classes. The theoretical and\nnumerical studies demonstrate that the proposed estimation method is highly\ncompetitive against the existing competitors, especially when the number of\nclasses is relatively large.\n", "versions": [{"version": "v1", "created": "Sat, 22 Sep 2012 01:50:43 GMT"}, {"version": "v2", "created": "Tue, 5 Feb 2013 15:08:40 GMT"}, {"version": "v3", "created": "Thu, 1 Aug 2013 17:44:53 GMT"}], "update_date": "2013-08-02", "authors_parsed": [["Xu", "Tu", ""], ["Wang", "Junhui", ""]]}, {"id": "1209.5019", "submitter": "Gungor Polatkan", "authors": "Gungor Polatkan and Mingyuan Zhou and Lawrence Carin and David Blei\n  and Ingrid Daubechies", "title": "A Bayesian Nonparametric Approach to Image Super-resolution", "comments": "30 pages, 11 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Super-resolution methods form high-resolution images from low-resolution\nimages. In this paper, we develop a new Bayesian nonparametric model for\nsuper-resolution. Our method uses a beta-Bernoulli process to learn a set of\nrecurring visual patterns, called dictionary elements, from the data. Because\nit is nonparametric, the number of elements found is also determined from the\ndata. We test the results on both benchmark and natural images, comparing with\nseveral other models from the research literature. We perform large-scale human\nevaluation experiments to assess the visual quality of the results. In a first\nimplementation, we use Gibbs sampling to approximate the posterior. However,\nthis algorithm is not feasible for large-scale data. To circumvent this, we\nthen develop an online variational Bayes (VB) algorithm. This algorithm finds\nhigh quality dictionaries in a fraction of the time needed by the Gibbs\nsampler.\n", "versions": [{"version": "v1", "created": "Sat, 22 Sep 2012 21:01:06 GMT"}], "update_date": "2012-09-25", "authors_parsed": [["Polatkan", "Gungor", ""], ["Zhou", "Mingyuan", ""], ["Carin", "Lawrence", ""], ["Blei", "David", ""], ["Daubechies", "Ingrid", ""]]}, {"id": "1209.5075", "submitter": "Shuheng Zhou", "authors": "Shuheng Zhou", "title": "Gemini: Graph estimation with matrix variate normal instances", "comments": "Published in at http://dx.doi.org/10.1214/13-AOS1187 the Annals of\n  Statistics (http://www.imstat.org/aos/) by the Institute of Mathematical\n  Statistics (http://www.imstat.org)", "journal-ref": "Annals of Statistics 2014, Vol. 42, No. 2, 532-562", "doi": "10.1214/13-AOS1187", "report-no": "IMS-AOS-AOS1187", "categories": "stat.ML math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Undirected graphs can be used to describe matrix variate distributions. In\nthis paper, we develop new methods for estimating the graphical structures and\nunderlying parameters, namely, the row and column covariance and inverse\ncovariance matrices from the matrix variate data. Under sparsity conditions, we\nshow that one is able to recover the graphs and covariance matrices with a\nsingle random matrix from the matrix variate normal distribution. Our method\nextends, with suitable adaptation, to the general setting where replicates are\navailable. We establish consistency and obtain the rates of convergence in the\noperator and the Frobenius norm. We show that having replicates will allow one\nto estimate more complicated graphical structures and achieve faster rates of\nconvergence. We provide simulation evidence showing that we can recover\ngraphical structures as well as estimating the precision matrices, as predicted\nby theory.\n", "versions": [{"version": "v1", "created": "Sun, 23 Sep 2012 15:22:02 GMT"}, {"version": "v2", "created": "Fri, 23 May 2014 10:25:58 GMT"}], "update_date": "2014-05-26", "authors_parsed": [["Zhou", "Shuheng", ""]]}, {"id": "1209.5350", "submitter": "Adel Javanmard", "authors": "Animashree Anandkumar, Daniel Hsu, Adel Javanmard, Sham M. Kakade", "title": "Learning Topic Models and Latent Bayesian Networks Under Expansion\n  Constraints", "comments": "38 pages, 6 figures, 2 tables, applications in topic models and\n  Bayesian networks are studied. Simulation section is added", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Unsupervised estimation of latent variable models is a fundamental problem\ncentral to numerous applications of machine learning and statistics. This work\npresents a principled approach for estimating broad classes of such models,\nincluding probabilistic topic models and latent linear Bayesian networks, using\nonly second-order observed moments. The sufficient conditions for\nidentifiability of these models are primarily based on weak expansion\nconstraints on the topic-word matrix, for topic models, and on the directed\nacyclic graph, for Bayesian networks. Because no assumptions are made on the\ndistribution among the latent variables, the approach can handle arbitrary\ncorrelations among the topics or latent factors. In addition, a tractable\nlearning method via $\\ell_1$ optimization is proposed and studied in numerical\nexperiments.\n", "versions": [{"version": "v1", "created": "Mon, 24 Sep 2012 18:11:02 GMT"}, {"version": "v2", "created": "Thu, 22 Nov 2012 06:34:45 GMT"}, {"version": "v3", "created": "Fri, 24 May 2013 18:25:32 GMT"}], "update_date": "2013-05-27", "authors_parsed": [["Anandkumar", "Animashree", ""], ["Hsu", "Daniel", ""], ["Javanmard", "Adel", ""], ["Kakade", "Sham M.", ""]]}, {"id": "1209.5375", "submitter": "Yannick Schwartz", "authors": "Yannick Schwartz (INRIA Saclay - Ile de France, LNAO), Ga\\\"el\n  Varoquaux (INRIA Saclay - Ile de France, LNAO), Christophe Pallier\n  (NEUROSPIN), Philippe Pinel (NEUROSPIN), Jean-Baptiste Poline (LNAO),\n  Bertrand Thirion (INRIA Saclay - Ile de France, LNAO)", "title": "Improving accuracy and power with transfer learning using a\n  meta-analytic database", "comments": "MICCAI, Nice : France (2012)", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Typical cohorts in brain imaging studies are not large enough for systematic\ntesting of all the information contained in the images. To build testable\nworking hypotheses, investigators thus rely on analysis of previous work,\nsometimes formalized in a so-called meta-analysis. In brain imaging, this\napproach underlies the specification of regions of interest (ROIs) that are\nusually selected on the basis of the coordinates of previously detected\neffects. In this paper, we propose to use a database of images, rather than\ncoordinates, and frame the problem as transfer learning: learning a\ndiscriminant model on a reference task to apply it to a different but related\nnew task. To facilitate statistical analysis of small cohorts, we use a sparse\ndiscriminant model that selects predictive voxels on the reference task and\nthus provides a principled procedure to define ROIs. The benefits of our\napproach are twofold. First it uses the reference database for prediction, i.e.\nto provide potential biomarkers in a clinical setting. Second it increases\nstatistical power on the new task. We demonstrate on a set of 18 pairs of\nfunctional MRI experimental conditions that our approach gives good prediction.\nIn addition, on a specific transfer situation involving different scanners at\ndifferent locations, we show that voxel selection based on transfer learning\nleads to higher detection power on small cohorts.\n", "versions": [{"version": "v1", "created": "Mon, 24 Sep 2012 19:38:46 GMT"}, {"version": "v2", "created": "Fri, 28 Sep 2012 14:35:16 GMT"}], "update_date": "2012-10-01", "authors_parsed": [["Schwartz", "Yannick", "", "INRIA Saclay - Ile de France, LNAO"], ["Varoquaux", "Ga\u00ebl", "", "INRIA Saclay - Ile de France, LNAO"], ["Pallier", "Christophe", "", "NEUROSPIN"], ["Pinel", "Philippe", "", "NEUROSPIN"], ["Poline", "Jean-Baptiste", "", "LNAO"], ["Thirion", "Bertrand", "", "INRIA Saclay - Ile de France, LNAO"]]}, {"id": "1209.5467", "submitter": "Berdakh Abibullaev", "authors": "Berdakh Abibullaev, Jinung An, Seung-Hyun Lee, Sang-Hyeon Jin, Jeon-Il\n  Moon", "title": "Minimizing inter-subject variability in fNIRS based Brain Computer\n  Interfaces via multiple-kernel support vector learning", "comments": "This paper has been withdrawn by the author due to an error in\n  equation 19", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Brain signal variability in the measurements obtained from different subjects\nduring different sessions significantly deteriorates the accuracy of most\nbrain-computer interface (BCI) systems. Moreover these variabilities, also\nknown as inter-subject or inter-session variabilities, require lengthy\ncalibration sessions before the BCI system can be used. Furthermore, the\ncalibration session has to be repeated for each subject independently and\nbefore use of the BCI due to the inter-session variability. In this study, we\npresent an algorithm in order to minimize the above-mentioned variabilities and\nto overcome the time-consuming and usually error-prone calibration time. Our\nalgorithm is based on linear programming support-vector machines and their\nextensions to a multiple kernel learning framework. We tackle the inter-subject\nor -session variability in the feature spaces of the classifiers. This is done\nby incorporating each subject- or session-specific feature spaces into much\nricher feature spaces with a set of optimal decision boundaries. Each decision\nboundary represents the subject- or a session specific spatio-temporal\nvariabilities of neural signals. Consequently, a single classifier with\nmultiple feature spaces will generalize well to new unseen test patterns even\nwithout the calibration steps. We demonstrate that classifiers maintain good\nperformances even under the presence of a large degree of BCI variability. The\npresent study analyzes BCI variability related to oxy-hemoglobin neural signals\nmeasured using a functional near-infrared spectroscopy.\n", "versions": [{"version": "v1", "created": "Tue, 25 Sep 2012 01:33:01 GMT"}, {"version": "v2", "created": "Mon, 24 Dec 2012 08:05:03 GMT"}, {"version": "v3", "created": "Mon, 25 Mar 2013 01:48:28 GMT"}, {"version": "v4", "created": "Wed, 8 May 2013 00:54:19 GMT"}], "update_date": "2013-05-09", "authors_parsed": [["Abibullaev", "Berdakh", ""], ["An", "Jinung", ""], ["Lee", "Seung-Hyun", ""], ["Jin", "Sang-Hyeon", ""], ["Moon", "Jeon-Il", ""]]}, {"id": "1209.5477", "submitter": "Yichao Lu", "authors": "Yichao Lu and Dean P. Foster", "title": "Optimal Weighting of Multi-View Data with Low Dimensional Hidden States", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In Natural Language Processing (NLP) tasks, data often has the following two\nproperties: First, data can be chopped into multi-views which has been\nsuccessfully used for dimension reduction purposes. For example, in topic\nclassification, every paper can be chopped into the title, the main text and\nthe references. However, it is common that some of the views are less noisier\nthan other views for supervised learning problems. Second, unlabeled data are\neasy to obtain while labeled data are relatively rare. For example, articles\noccurred on New York Times in recent 10 years are easy to grab but having them\nclassified as 'Politics', 'Finance' or 'Sports' need human labor. Hence less\nnoisy features are preferred before running supervised learning methods. In\nthis paper we propose an unsupervised algorithm which optimally weights\nfeatures from different views when these views are generated from a low\ndimensional hidden state, which occurs in widely used models like Mixture\nGaussian Model, Hidden Markov Model (HMM) and Latent Dirichlet Allocation\n(LDA).\n", "versions": [{"version": "v1", "created": "Tue, 25 Sep 2012 02:54:49 GMT"}, {"version": "v2", "created": "Wed, 26 Sep 2012 05:15:07 GMT"}], "update_date": "2012-09-27", "authors_parsed": [["Lu", "Yichao", ""], ["Foster", "Dean P.", ""]]}, {"id": "1209.5549", "submitter": "David Balduzzi", "authors": "David Balduzzi and Michel Besserve", "title": "Towards a learning-theoretic analysis of spike-timing dependent\n  plasticity", "comments": "To appear in Adv. Neural Inf. Proc. Systems", "journal-ref": null, "doi": null, "report-no": null, "categories": "q-bio.NC cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper suggests a learning-theoretic perspective on how synaptic\nplasticity benefits global brain functioning. We introduce a model, the\nselectron, that (i) arises as the fast time constant limit of leaky\nintegrate-and-fire neurons equipped with spiking timing dependent plasticity\n(STDP) and (ii) is amenable to theoretical analysis. We show that the selectron\nencodes reward estimates into spikes and that an error bound on spikes is\ncontrolled by a spiking margin and the sum of synaptic weights. Moreover, the\nefficacy of spikes (their usefulness to other reward maximizing selectrons)\nalso depends on total synaptic strength. Finally, based on our analysis, we\npropose a regularized version of STDP, and show the regularization improves the\nrobustness of neuronal learning when faced with multiple stimuli.\n", "versions": [{"version": "v1", "created": "Tue, 25 Sep 2012 09:23:41 GMT"}], "update_date": "2012-09-26", "authors_parsed": [["Balduzzi", "David", ""], ["Besserve", "Michel", ""]]}, {"id": "1209.5561", "submitter": "Leto Peel", "authors": "Leto Peel", "title": "Supervised Blockmodelling", "comments": "Workshop on Collective Learning and Inference on Structured Data 2012", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.SI stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Collective classification models attempt to improve classification\nperformance by taking into account the class labels of related instances.\nHowever, they tend not to learn patterns of interactions between classes and/or\nmake the assumption that instances of the same class link to each other\n(assortativity assumption). Blockmodels provide a solution to these issues,\nbeing capable of modelling assortative and disassortative interactions, and\nlearning the pattern of interactions in the form of a summary network. The\nSupervised Blockmodel provides good classification performance using link\nstructure alone, whilst simultaneously providing an interpretable summary of\nnetwork interactions to allow a better understanding of the data. This work\nexplores three variants of supervised blockmodels of varying complexity and\ntests them on four structurally different real world networks.\n", "versions": [{"version": "v1", "created": "Tue, 25 Sep 2012 09:59:56 GMT"}], "update_date": "2012-09-26", "authors_parsed": [["Peel", "Leto", ""]]}, {"id": "1209.5860", "submitter": "Yangbo He", "authors": "Yangbo He, Jinzhu Jia, Bin Yu", "title": "Reversible MCMC on Markov equivalence classes of sparse directed acyclic\n  graphs", "comments": "Published in at http://dx.doi.org/10.1214/13-AOS1125 the Annals of\n  Statistics (http://www.imstat.org/aos/) by the Institute of Mathematical\n  Statistics (http://www.imstat.org)", "journal-ref": "Annals of Statistics 2013, Vol. 41, No. 4, 1742-1779", "doi": "10.1214/13-AOS1125", "report-no": "IMS-AOS-AOS1125", "categories": "stat.ML cs.DM stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Graphical models are popular statistical tools which are used to represent\ndependent or causal complex systems. Statistically equivalent causal or\ndirected graphical models are said to belong to a Markov equivalent class. It\nis of great interest to describe and understand the space of such classes.\nHowever, with currently known algorithms, sampling over such classes is only\nfeasible for graphs with fewer than approximately 20 vertices. In this paper,\nwe design reversible irreducible Markov chains on the space of Markov\nequivalent classes by proposing a perfect set of operators that determine the\ntransitions of the Markov chain. The stationary distribution of a proposed\nMarkov chain has a closed form and can be computed easily. Specifically, we\nconstruct a concrete perfect set of operators on sparse Markov equivalence\nclasses by introducing appropriate conditions on each possible operator.\nAlgorithms and their accelerated versions are provided to efficiently generate\nMarkov chains and to explore properties of Markov equivalence classes of sparse\ndirected acyclic graphs (DAGs) with thousands of vertices. We find\nexperimentally that in most Markov equivalence classes of sparse DAGs, (1) most\nedges are directed, (2) most undirected subgraphs are small and (3) the number\nof these undirected subgraphs grows approximately linearly with the number of\nvertices. The article contains supplement arXiv:1303.0632,\nhttp://dx.doi.org/10.1214/13-AOS1125SUPP\n", "versions": [{"version": "v1", "created": "Wed, 26 Sep 2012 08:13:54 GMT"}, {"version": "v2", "created": "Mon, 4 Mar 2013 08:03:10 GMT"}, {"version": "v3", "created": "Mon, 27 Jan 2014 09:05:44 GMT"}], "update_date": "2014-01-30", "authors_parsed": [["He", "Yangbo", ""], ["Jia", "Jinzhu", ""], ["Yu", "Bin", ""]]}, {"id": "1209.5991", "submitter": "Satyaki Mahalanabis", "authors": "Satyaki Mahalanabis, Daniel Stefankovic", "title": "Subset Selection for Gaussian Markov Random Fields", "comments": "40 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Given a Gaussian Markov random field, we consider the problem of selecting a\nsubset of variables to observe which minimizes the total expected squared\nprediction error of the unobserved variables. We first show that finding an\nexact solution is NP-hard even for a restricted class of Gaussian Markov random\nfields, called Gaussian free fields, which arise in semi-supervised learning\nand computer vision. We then give a simple greedy approximation algorithm for\nGaussian free fields on arbitrary graphs. Finally, we give a message passing\nalgorithm for general Gaussian Markov random fields on bounded tree-width\ngraphs.\n", "versions": [{"version": "v1", "created": "Wed, 26 Sep 2012 16:31:32 GMT"}], "update_date": "2012-09-27", "authors_parsed": [["Mahalanabis", "Satyaki", ""], ["Stefankovic", "Daniel", ""]]}, {"id": "1209.6001", "submitter": "Jonathan Shapiro", "authors": "Ruefei He and Jonathan Shapiro", "title": "Bayesian Mixture Models for Frequent Itemset Discovery", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.IR stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In binary-transaction data-mining, traditional frequent itemset mining often\nproduces results which are not straightforward to interpret. To overcome this\nproblem, probability models are often used to produce more compact and\nconclusive results, albeit with some loss of accuracy. Bayesian statistics have\nbeen widely used in the development of probability models in machine learning\nin recent years and these methods have many advantages, including their\nabilities to avoid overfitting. In this paper, we develop two Bayesian mixture\nmodels with the Dirichlet distribution prior and the Dirichlet process (DP)\nprior to improve the previous non-Bayesian mixture model developed for\ntransaction dataset mining. We implement the inference of both mixture models\nusing two methods: a collapsed Gibbs sampling scheme and a variational\napproximation algorithm. Experiments in several benchmark problems have shown\nthat both mixture models achieve better performance than a non-Bayesian mixture\nmodel. The variational algorithm is the faster of the two approaches while the\nGibbs sampling method achieves a more accurate results. The Dirichlet process\nmixture model can automatically grow to a proper complexity for a better\napproximation. Once the model is built, it can be very fast to query and run\nanalysis on (typically 10 times faster than Eclat, as we will show in the\nexperiment section). However, these approaches also show that mixture models\nunderestimate the probabilities of frequent itemsets. Consequently, these\nmodels have a higher sensitivity but a lower specificity.\n", "versions": [{"version": "v1", "created": "Wed, 26 Sep 2012 16:41:59 GMT"}], "update_date": "2012-09-27", "authors_parsed": [["He", "Ruefei", ""], ["Shapiro", "Jonathan", ""]]}, {"id": "1209.6004", "submitter": "Sean Gerrish", "authors": "Sean M. Gerrish and David M. Blei", "title": "The Issue-Adjusted Ideal Point Model", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We develop a model of issue-specific voting behavior. This model can be used\nto explore lawmakers' personal voting patterns of voting by issue area,\nproviding an exploratory window into how the language of the law is correlated\nwith political support. We derive approximate posterior inference algorithms\nbased on variational methods. Across 12 years of legislative data, we\ndemonstrate both improvement in heldout prediction performance and the model's\nutility in interpreting an inherently multi-dimensional space.\n", "versions": [{"version": "v1", "created": "Wed, 26 Sep 2012 17:00:21 GMT"}], "update_date": "2012-09-27", "authors_parsed": [["Gerrish", "Sean M.", ""], ["Blei", "David M.", ""]]}, {"id": "1209.6342", "submitter": "Jie Cheng MS", "authors": "Jie Cheng, Elizaveta Levina, Pei Wang and Ji Zhu", "title": "Sparse Ising Models with Covariates", "comments": "32 pages (including 5 pages of appendix), 3 figures, 2 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  There has been a lot of work fitting Ising models to multivariate binary data\nin order to understand the conditional dependency relationships between the\nvariables. However, additional covariates are frequently recorded together with\nthe binary data, and may influence the dependence relationships. Motivated by\nsuch a dataset on genomic instability collected from tumor samples of several\ntypes, we propose a sparse covariate dependent Ising model to study both the\nconditional dependency within the binary data and its relationship with the\nadditional covariates. This results in subject-specific Ising models, where the\nsubject's covariates influence the strength of association between the genes.\nAs in all exploratory data analysis, interpretability of results is important,\nand we use L1 penalties to induce sparsity in the fitted graphs and in the\nnumber of selected covariates. Two algorithms to fit the model are proposed and\ncompared on a set of simulated data, and asymptotic results are established.\nThe results on the tumor dataset and their biological significance are\ndiscussed in detail.\n", "versions": [{"version": "v1", "created": "Thu, 27 Sep 2012 19:43:44 GMT"}], "update_date": "2012-09-28", "authors_parsed": [["Cheng", "Jie", ""], ["Levina", "Elizaveta", ""], ["Wang", "Pei", ""], ["Zhu", "Ji", ""]]}, {"id": "1209.6419", "submitter": "Xiaotong Yuan", "authors": "Xiao-Tong Yuan and Tong Zhang", "title": "Partial Gaussian Graphical Model Estimation", "comments": "32 pages, 5 figures, 4tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.IT math.IT stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper studies the partial estimation of Gaussian graphical models from\nhigh-dimensional empirical observations. We derive a convex formulation for\nthis problem using $\\ell_1$-regularized maximum-likelihood estimation, which\ncan be solved via a block coordinate descent algorithm. Statistical estimation\nperformance can be established for our method. The proposed approach has\ncompetitive empirical performance compared to existing methods, as demonstrated\nby various experiments on synthetic and real datasets.\n", "versions": [{"version": "v1", "created": "Fri, 28 Sep 2012 04:12:14 GMT"}], "update_date": "2012-10-01", "authors_parsed": [["Yuan", "Xiao-Tong", ""], ["Zhang", "Tong", ""]]}, {"id": "1209.6561", "submitter": "Giorgos Borboudakis", "authors": "Giorgos Borboudakis and Ioannis Tsamardinos", "title": "Scoring and Searching over Bayesian Networks with Causal and Associative\n  Priors", "comments": "Accepted for publication to the 29th Conference on Uncertainty in\n  Artificial Intelligence (UAI-2013). The content of the paper is identical to\n  the published one, but the compiler at arXiv produces a 11 page long paper,\n  whereas the compiler we used produces a 10 page long paper (page limit for\n  the conference)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A significant theoretical advantage of search-and-score methods for learning\nBayesian Networks is that they can accept informative prior beliefs for each\npossible network, thus complementing the data. In this paper, a method is\npresented for assigning priors based on beliefs on the presence or absence of\ncertain paths in the true network. Such beliefs correspond to knowledge about\nthe possible causal and associative relations between pairs of variables. This\ntype of knowledge naturally arises from prior experimental and observational\ndata, among others. In addition, a novel search-operator is proposed to take\nadvantage of such prior knowledge. Experiments show that, using path beliefs\nimproves the learning of the skeleton, as well as the edge directions in the\nnetwork.\n", "versions": [{"version": "v1", "created": "Fri, 28 Sep 2012 16:06:09 GMT"}, {"version": "v2", "created": "Wed, 31 Jul 2013 19:57:02 GMT"}], "update_date": "2013-08-01", "authors_parsed": [["Borboudakis", "Giorgos", ""], ["Tsamardinos", "Ioannis", ""]]}]