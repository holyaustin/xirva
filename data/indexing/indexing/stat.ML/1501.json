[{"id": "1501.00052", "submitter": "Jonathan Huggins", "authors": "Jonathan H. Huggins, Ardavan Saeedi, and Matthew J. Johnson", "title": "Detailed Derivations of Small-Variance Asymptotics for some Hierarchical\n  Bayesian Nonparametric Models", "comments": "7 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this note we provide detailed derivations of two versions of\nsmall-variance asymptotics for hierarchical Dirichlet process (HDP) mixture\nmodels and the HDP hidden Markov model (HDP-HMM, a.k.a. the infinite HMM). We\ninclude derivations for the probabilities of certain CRP and CRF partitions,\nwhich are of more general interest.\n", "versions": [{"version": "v1", "created": "Wed, 31 Dec 2014 00:23:35 GMT"}], "update_date": "2015-01-05", "authors_parsed": [["Huggins", "Jonathan H.", ""], ["Saeedi", "Ardavan", ""], ["Johnson", "Matthew J.", ""]]}, {"id": "1501.00192", "submitter": "Jason Jo", "authors": "Jason Jo", "title": "Learning Parameters for Weighted Matrix Completion via Empirical\n  Estimation", "comments": "Simplified the analysis", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recently theoretical guarantees have been obtained for matrix completion in\nthe non-uniform sampling regime. In particular, if the sampling distribution\naligns with the underlying matrix's leverage scores, then with high probability\nnuclear norm minimization will exactly recover the low rank matrix. In this\narticle, we analyze the scenario in which the non-uniform sampling distribution\nmay or may not not align with the underlying matrix's leverage scores. Here we\nexplore learning the parameters for weighted nuclear norm minimization in terms\nof the empirical sampling distribution. We provide a sufficiency condition for\nthese learned weights which provide an exact recovery guarantee for weighted\nnuclear norm minimization. It has been established that a specific choice of\nweights in terms of the true sampling distribution not only allows for weighted\nnuclear norm minimization to exactly recover the low rank matrix, but also\nallows for a quantifiable relaxation in the exact recovery conditions. In this\narticle we extend this quantifiable relaxation in exact recovery conditions for\na specific choice of weights defined analogously in terms of the empirical\ndistribution as opposed to the true sampling distribution. To accomplish this\nwe employ a concentration of measure bound and a large deviation bound. We also\npresent numerical evidence for the healthy robustness of the weighted nuclear\nnorm minimization algorithm to the choice of empirically learned weights. These\nnumerical experiments show that for a variety of easily computable empirical\nweights, weighted nuclear norm minimization outperforms unweighted nuclear norm\nminimization in the non-uniform sampling regime.\n", "versions": [{"version": "v1", "created": "Wed, 31 Dec 2014 19:10:52 GMT"}, {"version": "v2", "created": "Wed, 7 Jan 2015 19:56:41 GMT"}, {"version": "v3", "created": "Tue, 20 Jan 2015 01:22:29 GMT"}, {"version": "v4", "created": "Thu, 2 Apr 2015 02:52:12 GMT"}], "update_date": "2015-04-03", "authors_parsed": [["Jo", "Jason", ""]]}, {"id": "1501.00199", "submitter": "Alex Beutel", "authors": "Alex Beutel, Amr Ahmed and Alexander J. Smola", "title": "ACCAMS: Additive Co-Clustering to Approximate Matrices Succinctly", "comments": "22 pages, under review for conference publication", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Matrix completion and approximation are popular tools to capture a user's\npreferences for recommendation and to approximate missing data. Instead of\nusing low-rank factorization we take a drastically different approach, based on\nthe simple insight that an additive model of co-clusterings allows one to\napproximate matrices efficiently. This allows us to build a concise model that,\nper bit of model learned, significantly beats all factorization approaches to\nmatrix approximation. Even more surprisingly, we find that summing over small\nco-clusterings is more effective in modeling matrices than classic\nco-clustering, which uses just one large partitioning of the matrix.\n  Following Occam's razor principle suggests that the simple structure induced\nby our model better captures the latent preferences and decision making\nprocesses present in the real world than classic co-clustering or matrix\nfactorization. We provide an iterative minimization algorithm, a collapsed\nGibbs sampler, theoretical guarantees for matrix approximation, and excellent\nempirical evidence for the efficacy of our approach. We achieve\nstate-of-the-art results on the Netflix problem with a fraction of the model\ncomplexity.\n", "versions": [{"version": "v1", "created": "Wed, 31 Dec 2014 19:36:55 GMT"}], "update_date": "2015-01-05", "authors_parsed": [["Beutel", "Alex", ""], ["Ahmed", "Amr", ""], ["Smola", "Alexander J.", ""]]}, {"id": "1501.00208", "submitter": "Daniel Roy", "authors": "Daniel M. Roy", "title": "The continuum-of-urns scheme, generalized beta and Indian buffet\n  processes, and hierarchies thereof", "comments": "40 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.PR math.ST stat.ML stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We describe the combinatorial stochastic process underlying a sequence of\nconditionally independent Bernoulli processes with a shared beta process hazard\nmeasure. As shown by Thibaux and Jordan [TJ07], in the special case when the\nunderlying beta process has a constant concentration function and a finite and\nnonatomic mean, the combinatorial structure is that of the Indian buffet\nprocess (IBP) introduced by Griffiths and Ghahramani [GG05]. By reinterpreting\nthe beta process introduced by Hjort [Hjo90] as a measurable family of\nDirichlet processes, we obtain a simple predictive rule for the general case,\nwhich can be thought of as a continuum of Blackwell-MacQueen urn schemes (or\nequivalently, one-parameter Hoppe urn schemes). The corresponding measurable\nfamily of Perman-Pitman-Yor processes leads to a continuum of two-parameter\nHoppe urn schemes, whose ordinary component is the three-parameter IBP\nintroduced by Teh and G\\\"or\\\"ur [TG09], which exhibits power-law behavior, as\nfurther studied by Broderick, Jordan, and Pitman [BJP12]. The idea extends to\narbitrary measurable families of exchangeable partition probability functions\nand gives rise to generalizations of the beta process with matching buffet\nprocesses. Finally, in the same way that hierarchies of Dirichlet processes\nwere given Chinese restaurant franchise representations by Teh, Jordan, Beal,\nand Blei [Teh+06], one can construct representations of sequences of Bernoulli\nprocesses directed by hierarchies of beta processes (and their generalizations)\nusing the stochastic process we uncover.\n", "versions": [{"version": "v1", "created": "Wed, 31 Dec 2014 21:07:33 GMT"}], "update_date": "2015-01-05", "authors_parsed": [["Roy", "Daniel M.", ""]]}, {"id": "1501.00263", "submitter": "Lin Xiao", "authors": "Yuchen Zhang and Lin Xiao", "title": "Communication-Efficient Distributed Optimization of Self-Concordant\n  Empirical Loss", "comments": null, "journal-ref": null, "doi": null, "report-no": "MSR-TR-2015-1", "categories": "math.OC cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider distributed convex optimization problems originated from sample\naverage approximation of stochastic optimization, or empirical risk\nminimization in machine learning. We assume that each machine in the\ndistributed computing system has access to a local empirical loss function,\nconstructed with i.i.d. data sampled from a common distribution. We propose a\ncommunication-efficient distributed algorithm to minimize the overall empirical\nloss, which is the average of the local empirical losses. The algorithm is\nbased on an inexact damped Newton method, where the inexact Newton steps are\ncomputed by a distributed preconditioned conjugate gradient method. We analyze\nits iteration complexity and communication efficiency for minimizing\nself-concordant empirical loss functions, and discuss the results for\ndistributed ridge regression, logistic regression and binary classification\nwith a smoothed hinge loss. In a standard setting for supervised learning, the\nrequired number of communication rounds of the algorithm does not increase with\nthe sample size, and only grows slowly with the number of machines.\n", "versions": [{"version": "v1", "created": "Thu, 1 Jan 2015 09:21:57 GMT"}], "update_date": "2015-01-05", "authors_parsed": [["Zhang", "Yuchen", ""], ["Xiao", "Lin", ""]]}, {"id": "1501.00287", "submitter": "Harikrishna Narasimhan", "authors": "Harish G. Ramaswamy, Harikrishna Narasimhan, Shivani Agarwal", "title": "Consistent Classification Algorithms for Multi-class Non-Decomposable\n  Performance Metrics", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study consistency of learning algorithms for a multi-class performance\nmetric that is a non-decomposable function of the confusion matrix of a\nclassifier and cannot be expressed as a sum of losses on individual data\npoints; examples of such performance metrics include the macro F-measure\npopular in information retrieval and the G-mean metric used in class-imbalanced\nproblems. While there has been much work in recent years in understanding the\nconsistency properties of learning algorithms for `binary' non-decomposable\nmetrics, little is known either about the form of the optimal classifier for a\ngeneral multi-class non-decomposable metric, or about how these learning\nalgorithms generalize to the multi-class case. In this paper, we provide a\nunified framework for analysing a multi-class non-decomposable performance\nmetric, where the problem of finding the optimal classifier for the performance\nmetric is viewed as an optimization problem over the space of all confusion\nmatrices achievable under the given distribution. Using this framework, we show\nthat (under a continuous distribution) the optimal classifier for a multi-class\nperformance metric can be obtained as the solution of a cost-sensitive\nclassification problem, thus generalizing several previous results on specific\nbinary non-decomposable metrics. We then design a consistent learning algorithm\nfor concave multi-class performance metrics that proceeds via a sequence of\ncost-sensitive classification problems, and can be seen as applying the\nconditional gradient (CG) optimization method over the space of feasible\nconfusion matrices. To our knowledge, this is the first efficient learning\nalgorithm (whose running time is polynomial in the number of classes) that is\nconsistent for a large family of multi-class non-decomposable metrics. Our\nconsistency proof uses a novel technique based on the convergence analysis of\nthe CG method.\n", "versions": [{"version": "v1", "created": "Thu, 1 Jan 2015 16:22:58 GMT"}], "update_date": "2015-01-05", "authors_parsed": [["Ramaswamy", "Harish G.", ""], ["Narasimhan", "Harikrishna", ""], ["Agarwal", "Shivani", ""]]}, {"id": "1501.00312", "submitter": "Po-Ling Loh", "authors": "Po-Ling Loh", "title": "Statistical consistency and asymptotic normality for high-dimensional\n  robust M-estimators", "comments": "56 pages, 8 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST cs.IT math.IT stat.ML stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study theoretical properties of regularized robust M-estimators,\napplicable when data are drawn from a sparse high-dimensional linear model and\ncontaminated by heavy-tailed distributions and/or outliers in the additive\nerrors and covariates. We first establish a form of local statistical\nconsistency for the penalized regression estimators under fairly mild\nconditions on the error distribution: When the derivative of the loss function\nis bounded and satisfies a local restricted curvature condition, all stationary\npoints within a constant radius of the true regression vector converge at the\nminimax rate enjoyed by the Lasso with sub-Gaussian errors. When an appropriate\nnonconvex regularizer is used in place of an l_1-penalty, we show that such\nstationary points are in fact unique and equal to the local oracle solution\nwith the correct support---hence, results on asymptotic normality in the\nlow-dimensional case carry over immediately to the high-dimensional setting.\nThis has important implications for the efficiency of regularized nonconvex\nM-estimators when the errors are heavy-tailed. Our analysis of the local\ncurvature of the loss function also has useful consequences for optimization\nwhen the robust regression function and/or regularizer is nonconvex and the\nobjective function possesses stationary points outside the local region. We\nshow that as long as a composite gradient descent algorithm is initialized\nwithin a constant radius of the true regression vector, successive iterates\nwill converge at a linear rate to a stationary point within the local region.\nFurthermore, the global optimum of a convex regularized robust regression\nfunction may be used to obtain a suitable initialization. The result is a novel\ntwo-step procedure that uses a convex M-estimator to achieve consistency and a\nnonconvex M-estimator to increase efficiency.\n", "versions": [{"version": "v1", "created": "Thu, 1 Jan 2015 20:52:30 GMT"}], "update_date": "2015-01-05", "authors_parsed": [["Loh", "Po-Ling", ""]]}, {"id": "1501.00375", "submitter": "Wittawat Jitkrittum", "authors": "Wittawat Jitkrittum, Arthur Gretton, Nicolas Heess", "title": "Passing Expectation Propagation Messages with Kernel Methods", "comments": "Accepted to Advances in Variational Inference, NIPS 2014 Workshop", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose to learn a kernel-based message operator which takes as input all\nexpectation propagation (EP) incoming messages to a factor node and produces an\noutgoing message. In ordinary EP, computing an outgoing message involves\nestimating a multivariate integral which may not have an analytic expression.\nLearning such an operator allows one to bypass the expensive computation of the\nintegral during inference by directly mapping all incoming messages into an\noutgoing message. The operator can be learned from training data (examples of\ninput and output messages) which allows automated inference to be made on any\nkind of factor that can be sampled.\n", "versions": [{"version": "v1", "created": "Fri, 2 Jan 2015 10:00:07 GMT"}], "update_date": "2015-03-11", "authors_parsed": [["Jitkrittum", "Wittawat", ""], ["Gretton", "Arthur", ""], ["Heess", "Nicolas", ""]]}, {"id": "1501.00438", "submitter": "Sebastian Vollmer", "authors": "Sebastian J. Vollmer and Konstantinos C. Zygalakis and and Yee Whye\n  Teh", "title": "(Non-) asymptotic properties of Stochastic Gradient Langevin Dynamics", "comments": "42 pages, 7 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME math.ST stat.ML stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Applying standard Markov chain Monte Carlo (MCMC) algorithms to large data\nsets is computationally infeasible. The recently proposed stochastic gradient\nLangevin dynamics (SGLD) method circumvents this problem in three ways: it\ngenerates proposed moves using only a subset of the data, it skips the\nMetropolis-Hastings accept-reject step, and it uses sequences of decreasing\nstep sizes. In \\cite{TehThierryVollmerSGLD2014}, we provided the mathematical\nfoundations for the decreasing step size SGLD, including consistency and a\ncentral limit theorem. However, in practice the SGLD is run for a relatively\nsmall number of iterations, and its step size is not decreased to zero. The\npresent article investigates the behaviour of the SGLD with fixed step size. In\nparticular we characterise the asymptotic bias explicitly, along with its\ndependence on the step size and the variance of the stochastic gradient. On\nthat basis a modified SGLD which removes the asymptotic bias due to the\nvariance of the stochastic gradients up to first order in the step size is\nderived. Moreover, we are able to obtain bounds on the finite-time bias,\nvariance and mean squared error (MSE). The theory is illustrated with a\nGaussian toy model for which the bias and the MSE for the estimation of moments\ncan be obtained explicitly. For this toy model we study the gain of the SGLD\nover the standard Euler method in the limit of large data sets.\n", "versions": [{"version": "v1", "created": "Fri, 2 Jan 2015 17:18:56 GMT"}, {"version": "v2", "created": "Mon, 21 Sep 2015 11:00:30 GMT"}], "update_date": "2015-09-22", "authors_parsed": [["Vollmer", "Sebastian J.", ""], ["Zygalakis", "Konstantinos C.", ""], ["Teh", "and Yee Whye", ""]]}, {"id": "1501.00559", "submitter": "Hao-Chung Cheng", "authors": "Hao-Chung Cheng, Min-Hsiu Hsieh, Ping-Cheng Yeh", "title": "The Learnability of Unknown Quantum Measurements", "comments": null, "journal-ref": "QIC, Vol. 16, No. 7-8, 0615-0656 (2016)", "doi": null, "report-no": null, "categories": "quant-ph cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Quantum machine learning has received significant attention in recent years,\nand promising progress has been made in the development of quantum algorithms\nto speed up traditional machine learning tasks. In this work, however, we focus\non investigating the information-theoretic upper bounds of sample complexity -\nhow many training samples are sufficient to predict the future behaviour of an\nunknown target function. This kind of problem is, arguably, one of the most\nfundamental problems in statistical learning theory and the bounds for\npractical settings can be completely characterised by a simple measure of\ncomplexity.\n  Our main result in the paper is that, for learning an unknown quantum\nmeasurement, the upper bound, given by the fat-shattering dimension, is\nlinearly proportional to the dimension of the underlying Hilbert space.\nLearning an unknown quantum state becomes a dual problem to ours, and as a\nbyproduct, we can recover Aaronson's famous result [Proc. R. Soc. A\n463:3089-3144 (2007)] solely using a classical machine learning technique. In\naddition, other famous complexity measures like covering numbers and Rademacher\ncomplexities are derived explicitly. We are able to connect measures of sample\ncomplexity with various areas in quantum information science, e.g. quantum\nstate/measurement tomography, quantum state discrimination and quantum random\naccess codes, which may be of independent interest. Lastly, with the assistance\nof general Bloch-sphere representation, we show that learning quantum\nmeasurements/states can be mathematically formulated as a neural network.\nConsequently, classical ML algorithms can be applied to efficiently accomplish\nthe two quantum learning tasks.\n", "versions": [{"version": "v1", "created": "Sat, 3 Jan 2015 12:26:10 GMT"}], "update_date": "2016-08-25", "authors_parsed": [["Cheng", "Hao-Chung", ""], ["Hsieh", "Min-Hsiu", ""], ["Yeh", "Ping-Cheng", ""]]}, {"id": "1501.00604", "submitter": "Ernest Fokoue", "authors": "Ernest Fokoue", "title": "A Taxonomy of Big Data for Optimal Predictive Machine Learning and Data\n  Mining", "comments": "18 pages, 2 figures 3 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Big data comes in various ways, types, shapes, forms and sizes. Indeed,\nalmost all areas of science, technology, medicine, public health, economics,\nbusiness, linguistics and social science are bombarded by ever increasing flows\nof data begging to analyzed efficiently and effectively. In this paper, we\npropose a rough idea of a possible taxonomy of big data, along with some of the\nmost commonly used tools for handling each particular category of bigness. The\ndimensionality p of the input space and the sample size n are usually the main\ningredients in the characterization of data bigness. The specific statistical\nmachine learning technique used to handle a particular big data set will depend\non which category it falls in within the bigness taxonomy. Large p small n data\nsets for instance require a different set of tools from the large n small p\nvariety. Among other tools, we discuss Preprocessing, Standardization,\nImputation, Projection, Regularization, Penalization, Compression, Reduction,\nSelection, Kernelization, Hybridization, Parallelization, Aggregation,\nRandomization, Replication, Sequentialization. Indeed, it is important to\nemphasize right away that the so-called no free lunch theorem applies here, in\nthe sense that there is no universally superior method that outperforms all\nother methods on all categories of bigness. It is also important to stress the\nfact that simplicity in the sense of Ockham's razor non plurality principle of\nparsimony tends to reign supreme when it comes to massive data. We conclude\nwith a comparison of the predictive performance of some of the most commonly\nused methods on a few data sets.\n", "versions": [{"version": "v1", "created": "Sat, 3 Jan 2015 20:44:01 GMT"}], "update_date": "2015-01-06", "authors_parsed": [["Fokoue", "Ernest", ""]]}, {"id": "1501.00725", "submitter": "St\\'ephane Ga\\\"iffas", "authors": "Emmanuel Bacry, Martin Bompaire, St\\'ephane Ga\\\"iffas and\n  Jean-Fran\\c{c}ois Muzy", "title": "Sparse and low-rank multivariate Hawkes processes", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider the problem of unveiling the implicit network structure of node\ninteractions (such as user interactions in a social network), based only on\nhigh-frequency timestamps. Our inference is based on the minimization of the\nleast-squares loss associated with a multivariate Hawkes model, penalized by\n$\\ell_1$ and trace norm of the interaction tensor. We provide a first\ntheoretical analysis for this problem, that includes sparsity and low-rank\ninducing penalizations. This result involves a new data-driven concentration\ninequality for matrix martingales in continuous time with observable variance,\nwhich is a result of independent interest and a broad range of possible\napplications since it extends to matrix martingales former results restricted\nto the scalar case. A consequence of our analysis is the construction of\nsharply tuned $\\ell_1$ and trace-norm penalizations, that leads to a\ndata-driven scaling of the variability of information available for each users.\nNumerical experiments illustrate the significant improvements achieved by the\nuse of such data-driven penalizations.\n", "versions": [{"version": "v1", "created": "Sun, 4 Jan 2015 21:51:44 GMT"}, {"version": "v2", "created": "Sat, 15 Dec 2018 18:03:39 GMT"}, {"version": "v3", "created": "Mon, 24 Feb 2020 10:57:57 GMT"}], "update_date": "2020-02-25", "authors_parsed": [["Bacry", "Emmanuel", ""], ["Bompaire", "Martin", ""], ["Ga\u00efffas", "St\u00e9phane", ""], ["Muzy", "Jean-Fran\u00e7ois", ""]]}, {"id": "1501.00752", "submitter": "Alexander Wong", "authors": "Mohammad Shafiee, Zohreh Azimifar, and Alexander Wong", "title": "A Deep-structured Conditional Random Field Model for Object Silhouette\n  Tracking", "comments": "17 pages", "journal-ref": null, "doi": "10.1371/journal.pone.0133036", "report-no": null, "categories": "cs.CV cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this work, we introduce a deep-structured conditional random field\n(DS-CRF) model for the purpose of state-based object silhouette tracking. The\nproposed DS-CRF model consists of a series of state layers, where each state\nlayer spatially characterizes the object silhouette at a particular point in\ntime. The interactions between adjacent state layers are established by\ninter-layer connectivity dynamically determined based on inter-frame optical\nflow. By incorporate both spatial and temporal context in a dynamic fashion\nwithin such a deep-structured probabilistic graphical model, the proposed\nDS-CRF model allows us to develop a framework that can accurately and\nefficiently track object silhouettes that can change greatly over time, as well\nas under different situations such as occlusion and multiple targets within the\nscene. Experiment results using video surveillance datasets containing\ndifferent scenarios such as occlusion and multiple targets showed that the\nproposed DS-CRF approach provides strong object silhouette tracking performance\nwhen compared to baseline methods such as mean-shift tracking, as well as\nstate-of-the-art methods such as context tracking and boosted particle\nfiltering.\n", "versions": [{"version": "v1", "created": "Mon, 5 Jan 2015 03:09:34 GMT"}, {"version": "v2", "created": "Tue, 4 Aug 2015 18:27:20 GMT"}], "update_date": "2016-02-17", "authors_parsed": [["Shafiee", "Mohammad", ""], ["Azimifar", "Zohreh", ""], ["Wong", "Alexander", ""]]}, {"id": "1501.00756", "submitter": "Miguel \\'A. Carreira-Perpi\\~n\\'an", "authors": "Miguel \\'A. Carreira-Perpi\\~n\\'an, Ramin Raziperchikolaei", "title": "Hashing with binary autoencoders", "comments": "22 pages, 11 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV math.OC stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  An attractive approach for fast search in image databases is binary hashing,\nwhere each high-dimensional, real-valued image is mapped onto a\nlow-dimensional, binary vector and the search is done in this binary space.\nFinding the optimal hash function is difficult because it involves binary\nconstraints, and most approaches approximate the optimization by relaxing the\nconstraints and then binarizing the result. Here, we focus on the binary\nautoencoder model, which seeks to reconstruct an image from the binary code\nproduced by the hash function. We show that the optimization can be simplified\nwith the method of auxiliary coordinates. This reformulates the optimization as\nalternating two easier steps: one that learns the encoder and decoder\nseparately, and one that optimizes the code for each image. Image retrieval\nexperiments, using precision/recall and a measure of code utilization, show the\nresulting hash function outperforms or is competitive with state-of-the-art\nmethods for binary hashing.\n", "versions": [{"version": "v1", "created": "Mon, 5 Jan 2015 03:49:02 GMT"}], "update_date": "2015-01-23", "authors_parsed": [["Carreira-Perpi\u00f1\u00e1n", "Miguel \u00c1.", ""], ["Raziperchikolaei", "Ramin", ""]]}, {"id": "1501.00834", "submitter": "Shun Kataoka", "authors": "Kazuyuki Tanaka, Shun Kataoka, Muneki Yasuda and Masayuki Ohzeki", "title": "Inverse Renormalization Group Transformation in Bayesian Image\n  Segmentations", "comments": "6 pages, 2 figures", "journal-ref": "Journal of the Physical Society of Japan 84 (2015) 045001", "doi": "10.7566/JPSJ.84.045001", "report-no": null, "categories": "cs.CV cond-mat.stat-mech stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A new Bayesian image segmentation algorithm is proposed by combining a loopy\nbelief propagation with an inverse real space renormalization group\ntransformation to reduce the computational time. In results of our experiment,\nwe observe that the proposed method can reduce the computational time to less\nthan one-tenth of that taken by conventional Bayesian approaches.\n", "versions": [{"version": "v1", "created": "Mon, 5 Jan 2015 12:20:09 GMT"}], "update_date": "2015-03-16", "authors_parsed": [["Tanaka", "Kazuyuki", ""], ["Kataoka", "Shun", ""], ["Yasuda", "Muneki", ""], ["Ohzeki", "Masayuki", ""]]}, {"id": "1501.01029", "submitter": "Yinfei Kong", "authors": "Yingying Fan, Yinfei Kong, Daoji Li, Zemin Zheng", "title": "Innovated interaction screening for high-dimensional nonlinear\n  classification", "comments": "Published at http://dx.doi.org/10.1214/14-AOS1308 in the Annals of\n  Statistics (http://www.imstat.org/aos/) by the Institute of Mathematical\n  Statistics (http://www.imstat.org)", "journal-ref": "Annals of Statistics 2015, Vol. 43, No. 3, 1243-1272", "doi": "10.1214/14-AOS1308", "report-no": "IMS-AOS-AOS1308", "categories": "stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper is concerned with the problems of interaction screening and\nnonlinear classification in a high-dimensional setting. We propose a two-step\nprocedure, IIS-SQDA, where in the first step an innovated interaction screening\n(IIS) approach based on transforming the original $p$-dimensional feature\nvector is proposed, and in the second step a sparse quadratic discriminant\nanalysis (SQDA) is proposed for further selecting important interactions and\nmain effects and simultaneously conducting classification. Our IIS approach\nscreens important interactions by examining only $p$ features instead of all\ntwo-way interactions of order $O(p^2)$. Our theory shows that the proposed\nmethod enjoys sure screening property in interaction selection in the\nhigh-dimensional setting of $p$ growing exponentially with the sample size. In\nthe selection and classification step, we establish a sparse inequality on the\nestimated coefficient vector for QDA and prove that the classification error of\nour procedure can be upper-bounded by the oracle classification error plus some\nsmaller order term. Extensive simulation studies and real data analysis show\nthat our proposal compares favorably with existing methods in interaction\nselection and high-dimensional classification.\n", "versions": [{"version": "v1", "created": "Mon, 5 Jan 2015 22:45:31 GMT"}, {"version": "v2", "created": "Wed, 3 Jun 2015 08:05:14 GMT"}], "update_date": "2015-06-04", "authors_parsed": [["Fan", "Yingying", ""], ["Kong", "Yinfei", ""], ["Li", "Daoji", ""], ["Zheng", "Zemin", ""]]}, {"id": "1501.01209", "submitter": "Vikram Krishnamurthy", "authors": "Omid Namvar Gharehshiran and William Hoiles and Vikram Krishnamurthy", "title": "Reinforcement Learning and Nonparametric Detection of Game-Theoretic\n  Equilibrium Play in Social Networks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.GT cs.LG cs.SI stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper studies two important signal processing aspects of equilibrium\nbehavior in non-cooperative games arising in social networks, namely,\nreinforcement learning and detection of equilibrium play. The first part of the\npaper presents a reinforcement learning (adaptive filtering) algorithm that\nfacilitates learning an equilibrium by resorting to diffusion cooperation\nstrategies in a social network. Agents form homophilic social groups, within\nwhich they exchange past experiences over an undirected graph. It is shown\nthat, if all agents follow the proposed algorithm, their global behavior is\nattracted to the correlated equilibria set of the game. The second part of the\npaper provides a test to detect if the actions of agents are consistent with\nplay from the equilibrium of a concave potential game. The theory of revealed\npreference from microeconomics is used to construct a non-parametric decision\ntest and statistical test which only require the probe and associated actions\nof agents. A stochastic gradient algorithm is given to optimize the probe in\nreal time to minimize the Type-II error probabilities of the detection test\nsubject to specified Type-I error probability. We provide a real-world example\nusing the energy market, and a numerical example to detect malicious agents in\nan online social network.\n", "versions": [{"version": "v1", "created": "Thu, 11 Dec 2014 05:00:26 GMT"}], "update_date": "2015-01-07", "authors_parsed": [["Gharehshiran", "Omid Namvar", ""], ["Hoiles", "William", ""], ["Krishnamurthy", "Vikram", ""]]}, {"id": "1501.01239", "submitter": "Han Zhao", "authors": "Han Zhao, Mazen Melibari and Pascal Poupart", "title": "On the Relationship between Sum-Product Networks and Bayesian Networks", "comments": "Full version of the same paper to appear at ICML-2015", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we establish some theoretical connections between Sum-Product\nNetworks (SPNs) and Bayesian Networks (BNs). We prove that every SPN can be\nconverted into a BN in linear time and space in terms of the network size. The\nkey insight is to use Algebraic Decision Diagrams (ADDs) to compactly represent\nthe local conditional probability distributions at each node in the resulting\nBN by exploiting context-specific independence (CSI). The generated BN has a\nsimple directed bipartite graphical structure. We show that by applying the\nVariable Elimination algorithm (VE) to the generated BN with ADD\nrepresentations, we can recover the original SPN where the SPN can be viewed as\na history record or caching of the VE inference process. To help state the\nproof clearly, we introduce the notion of {\\em normal} SPN and present a\ntheoretical analysis of the consistency and decomposability properties. We\nconclude the paper with some discussion of the implications of the proof and\nestablish a connection between the depth of an SPN and a lower bound of the\ntree-width of its corresponding BN.\n", "versions": [{"version": "v1", "created": "Tue, 6 Jan 2015 17:14:11 GMT"}, {"version": "v2", "created": "Thu, 30 Apr 2015 18:15:12 GMT"}], "update_date": "2015-05-01", "authors_parsed": [["Zhao", "Han", ""], ["Melibari", "Mazen", ""], ["Poupart", "Pascal", ""]]}, {"id": "1501.01571", "submitter": "Joel Tropp", "authors": "Joel A. Tropp", "title": "An Introduction to Matrix Concentration Inequalities", "comments": "163 pages. To appear in Foundations and Trends in Machine Learning", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.PR cs.DS cs.IT cs.NA math.IT stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In recent years, random matrices have come to play a major role in\ncomputational mathematics, but most of the classical areas of random matrix\ntheory remain the province of experts. Over the last decade, with the advent of\nmatrix concentration inequalities, research has advanced to the point where we\ncan conquer many (formerly) challenging problems with a page or two of\narithmetic. The aim of this monograph is to describe the most successful\nmethods from this area along with some interesting examples that these\ntechniques can illuminate.\n", "versions": [{"version": "v1", "created": "Wed, 7 Jan 2015 17:46:02 GMT"}], "update_date": "2015-01-08", "authors_parsed": [["Tropp", "Joel A.", ""]]}, {"id": "1501.01617", "submitter": "Lucy Xia", "authors": "Jianqing Fan, Yang Feng, Lucy Xia", "title": "A Projection Based Conditional Dependence Measure with Applications to\n  High-dimensional Undirected Graphical Models", "comments": "39 pages, 5 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME math.ST stat.AP stat.ML stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Measuring conditional dependence is an important topic in statistics with\nbroad applications including graphical models. Under a factor model setting, a\nnew conditional dependence measure based on projection is proposed. The\ncorresponding conditional independence test is developed with the asymptotic\nnull distribution unveiled where the number of factors could be\nhigh-dimensional. It is also shown that the new test has control over the\nasymptotic significance level and can be calculated efficiently. A generic\nmethod for building dependency graphs without Gaussian assumption using the new\ntest is elaborated. Numerical results and real data analysis show the\nsuperiority of the new method.\n", "versions": [{"version": "v1", "created": "Wed, 7 Jan 2015 20:43:14 GMT"}, {"version": "v2", "created": "Thu, 8 Jan 2015 20:08:35 GMT"}, {"version": "v3", "created": "Wed, 23 Nov 2016 04:50:55 GMT"}, {"version": "v4", "created": "Tue, 14 Feb 2017 20:33:54 GMT"}, {"version": "v5", "created": "Fri, 11 Jan 2019 07:36:17 GMT"}], "update_date": "2019-01-14", "authors_parsed": [["Fan", "Jianqing", ""], ["Feng", "Yang", ""], ["Xia", "Lucy", ""]]}, {"id": "1501.02056", "submitter": "Simon Lacoste-Julien", "authors": "Simon Lacoste-Julien (LIENS, INRIA Paris - Rocquencourt, MSR - INRIA),\n  Fredrik Lindsten, Francis Bach (LIENS, INRIA Paris - Rocquencourt, MSR -\n  INRIA)", "title": "Sequential Kernel Herding: Frank-Wolfe Optimization for Particle\n  Filtering", "comments": "in 18th International Conference on Artificial Intelligence and\n  Statistics (AISTATS), May 2015, San Diego, United States. 38, JMLR Workshop\n  and Conference Proceedings", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recently, the Frank-Wolfe optimization algorithm was suggested as a procedure\nto obtain adaptive quadrature rules for integrals of functions in a reproducing\nkernel Hilbert space (RKHS) with a potentially faster rate of convergence than\nMonte Carlo integration (and \"kernel herding\" was shown to be a special case of\nthis procedure). In this paper, we propose to replace the random sampling step\nin a particle filter by Frank-Wolfe optimization. By optimizing the position of\nthe particles, we can obtain better accuracy than random or quasi-Monte Carlo\nsampling. In applications where the evaluation of the emission probabilities is\nexpensive (such as in robot localization), the additional computational cost to\ngenerate the particles through optimization can be justified. Experiments on\nstandard synthetic examples as well as on a robot localization task indicate\nindeed an improvement of accuracy over random and quasi-Monte Carlo sampling.\n", "versions": [{"version": "v1", "created": "Fri, 9 Jan 2015 07:17:27 GMT"}, {"version": "v2", "created": "Tue, 10 Feb 2015 07:19:38 GMT"}], "update_date": "2015-02-11", "authors_parsed": [["Lacoste-Julien", "Simon", "", "LIENS, INRIA Paris - Rocquencourt, MSR - INRIA"], ["Lindsten", "Fredrik", "", "LIENS, INRIA Paris - Rocquencourt, MSR -\n  INRIA"], ["Bach", "Francis", "", "LIENS, INRIA Paris - Rocquencourt, MSR -\n  INRIA"]]}, {"id": "1501.02102", "submitter": "Hangjin Jiang", "authors": "Hangjin Jiang, Kan Liu, Yiming Ding", "title": "Equitability of Dependence Measure", "comments": "This is draft version", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Measuring dependence between two random variables is very important, and\ncritical in many applied areas such as variable selection, brain network\nanalysis. However, we do not know what kind of functional relationship is\nbetween two covariates, which requires the dependence measure to be equitable.\nThat is, it gives similar scores to equally noisy relationship of different\ntypes. In fact, the dependence score is a continuous random variable taking\nvalues in $[0,1]$, thus it is theoretically impossible to give similar scores.\nIn this paper, we introduce a new definition of equitability of a dependence\nmeasure, i.e, power-equitable (weak-equitable) and show by simulation that HHG\nand Copula Dependence Coefficient (CDC) are weak-equitable.\n", "versions": [{"version": "v1", "created": "Fri, 9 Jan 2015 10:53:02 GMT"}, {"version": "v2", "created": "Fri, 25 Mar 2016 04:42:28 GMT"}, {"version": "v3", "created": "Tue, 3 Apr 2018 04:32:25 GMT"}, {"version": "v4", "created": "Wed, 11 Jul 2018 15:39:12 GMT"}], "update_date": "2018-07-12", "authors_parsed": [["Jiang", "Hangjin", ""], ["Liu", "Kan", ""], ["Ding", "Yiming", ""]]}, {"id": "1501.02103", "submitter": "Robin Evans", "authors": "Robin J. Evans", "title": "Margins of discrete Bayesian networks", "comments": "41 pages", "journal-ref": "Annals of Statistics 2018, Vol. 46, No. 6A, 2623-2656", "doi": "10.1214/17-AOS1631", "report-no": null, "categories": "math.ST stat.ML stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Bayesian network models with latent variables are widely used in statistics\nand machine learning. In this paper we provide a complete algebraic\ncharacterization of Bayesian network models with latent variables when the\nobserved variables are discrete and no assumption is made about the state-space\nof the latent variables. We show that it is algebraically equivalent to the\nso-called nested Markov model, meaning that the two are the same up to\ninequality constraints on the joint probabilities. In particular these two\nmodels have the same dimension. The nested Markov model is therefore the best\npossible description of the latent variable model that avoids consideration of\ninequalities, which are extremely complicated in general. A consequence of this\nis that the constraint finding algorithm of Tian and Pearl (UAI 2002,\npp519-527) is complete for finding equality constraints.\n  Latent variable models suffer from difficulties of unidentifiable parameters\nand non-regular asymptotics; in contrast the nested Markov model is fully\nidentifiable, represents a curved exponential family of known dimension, and\ncan easily be fitted using an explicit parameterization.\n", "versions": [{"version": "v1", "created": "Fri, 9 Jan 2015 11:04:03 GMT"}, {"version": "v2", "created": "Mon, 30 Jan 2017 16:56:36 GMT"}], "update_date": "2018-09-10", "authors_parsed": [["Evans", "Robin J.", ""]]}, {"id": "1501.02218", "submitter": "Emilie Chautru", "authors": "St\\'ephan Cl\\'emen\\c{c}on, Patrice Bertail, Emilie Chautru, Guillaume\n  Papa", "title": "Survey schemes for stochastic gradient descent with applications to\n  M-estimation", "comments": "31 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In certain situations that shall be undoubtedly more and more common in the\nBig Data era, the datasets available are so massive that computing statistics\nover the full sample is hardly feasible, if not unfeasible. A natural approach\nin this context consists in using survey schemes and substituting the \"full\ndata\" statistics with their counterparts based on the resulting random samples,\nof manageable size. It is the main purpose of this paper to investigate the\nimpact of survey sampling with unequal inclusion probabilities on stochastic\ngradient descent-based M-estimation methods in large-scale statistical and\nmachine-learning problems. Precisely, we prove that, in presence of some a\npriori information, one may significantly increase asymptotic accuracy when\nchoosing appropriate first order inclusion probabilities, without affecting\ncomplexity. These striking results are described here by limit theorems and are\nalso illustrated by numerical experiments.\n", "versions": [{"version": "v1", "created": "Fri, 9 Jan 2015 18:18:20 GMT"}], "update_date": "2015-01-12", "authors_parsed": [["Cl\u00e9men\u00e7on", "St\u00e9phan", ""], ["Bertail", "Patrice", ""], ["Chautru", "Emilie", ""], ["Papa", "Guillaume", ""]]}, {"id": "1501.02320", "submitter": "Po-Ling Loh", "authors": "Varun Jog and Po-Ling Loh", "title": "On model misspecification and KL separation for Gaussian graphical\n  models", "comments": "Accepted to ISIT 2015", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IT math.IT math.ST stat.ML stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We establish bounds on the KL divergence between two multivariate Gaussian\ndistributions in terms of the Hamming distance between the edge sets of the\ncorresponding graphical models. We show that the KL divergence is bounded below\nby a constant when the graphs differ by at least one edge; this is essentially\nthe tightest possible bound, since classes of graphs exist for which the edge\ndiscrepancy increases but the KL divergence remains bounded above by a\nconstant. As a natural corollary to our KL lower bound, we also establish a\nsample size requirement for correct model selection via maximum likelihood\nestimation. Our results rigorize the notion that it is essential to estimate\nthe edge structure of a Gaussian graphical model accurately in order to\napproximate the true distribution to close precision.\n", "versions": [{"version": "v1", "created": "Sat, 10 Jan 2015 08:50:57 GMT"}, {"version": "v2", "created": "Fri, 3 Apr 2015 19:13:27 GMT"}], "update_date": "2015-04-06", "authors_parsed": [["Jog", "Varun", ""], ["Loh", "Po-Ling", ""]]}, {"id": "1501.02467", "submitter": "Justin Yang Mr.", "authors": "Justin J. Yang, Xufei Wang, Pavlos Protopapas, Luke Bornn", "title": "Fast and optimal nonparametric sequential design for astronomical\n  observations", "comments": "22 pages, 9 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.AP stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The spectral energy distribution (SED) is a relatively easy way for\nastronomers to distinguish between different astronomical objects such as\ngalaxies, black holes, and stellar objects. By comparing the observations from\na source at different frequencies with template models, astronomers are able to\ninfer the type of this observed object. In this paper, we take a Bayesian model\naveraging perspective to learn astronomical objects, employing a Bayesian\nnonparametric approach to accommodate the deviation from convex combinations of\nknown log-SEDs. To effectively use telescope time for observations, we then\nstudy Bayesian nonparametric sequential experimental design without conjugacy,\nin which we use sequential Monte Carlo as an efficient tool to maximize the\nvolume of information stored in the posterior distribution of the parameters of\ninterest. A new technique for performing inferences in log-Gaussian Cox\nprocesses called the Poisson log-normal approximation is also proposed.\nSimulations show the speed, accuracy, and usefulness of our method. While the\nstrategy we propose in this paper is brand new in the astronomy literature, the\ninferential techniques developed apply to more general nonparametric sequential\nexperimental design problems.\n", "versions": [{"version": "v1", "created": "Sun, 11 Jan 2015 15:50:09 GMT"}], "update_date": "2015-01-13", "authors_parsed": [["Yang", "Justin J.", ""], ["Wang", "Xufei", ""], ["Protopapas", "Pavlos", ""], ["Bornn", "Luke", ""]]}, {"id": "1501.02497", "submitter": "Nhat Ho", "authors": "Nhat Ho and XuanLong Nguyen", "title": "Identifiability and optimal rates of convergence for parameters of\n  multiple types in finite mixtures", "comments": "95 pages, 17 figures, Technical report 536, Department of Statistics,\n  University of Michigan, Ann Arbor", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.ML stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper studies identifiability and convergence behaviors for parameters\nof multiple types in finite mixtures, and the effects of model fitting with\nextra mixing components. First, we present a general theory for strong\nidentifiability, which extends from the previous work of Nguyen [2013] and Chen\n[1995] to address a broad range of mixture models and to handle matrix-variate\nparameters. These models are shown to share the same Wasserstein distance based\noptimal rates of convergence for the space of mixing distributions ---\n$n^{-1/2}$ under $W_1$ for the exact-fitted and $n^{-1/4}$ under $W_2$ for the\nover-fitted setting, where $n$ is the sample size. This theory, however, is not\napplicable to several important model classes, including location-scale\nmultivariate Gaussian mixtures, shape-scale Gamma mixtures and\nlocation-scale-shape skew-normal mixtures. The second part of this work is\ndevoted to demonstrating that for these \"weakly identifiable\" classes,\nalgebraic structures of the density family play a fundamental role in\ndetermining convergence rates of the model parameters, which display a very\nrich spectrum of behaviors. For instance, the optimal rate of parameter\nestimation in an over-fitted location-covariance Gaussian mixture is precisely\ndetermined by the order of a solvable system of polynomial equations --- these\nrates deteriorate rapidly as more extra components are added to the model. The\nestablished rates for a variety of settings are illustrated by a simulation\nstudy.\n", "versions": [{"version": "v1", "created": "Sun, 11 Jan 2015 21:00:07 GMT"}], "update_date": "2015-01-13", "authors_parsed": [["Ho", "Nhat", ""], ["Nguyen", "XuanLong", ""]]}, {"id": "1501.02579", "submitter": "Martin Sundin", "authors": "Martin Sundin, Saikat Chatterjee and Magnus Jansson", "title": "Combined modeling of sparse and dense noise for improvement of Relevance\n  Vector Machine", "comments": "Submitted to IEEE Transactions on Signal Processing", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Using a Bayesian approach, we consider the problem of recovering sparse\nsignals under additive sparse and dense noise. Typically, sparse noise models\noutliers, impulse bursts or data loss. To handle sparse noise, existing methods\nsimultaneously estimate the sparse signal of interest and the sparse noise of\nno interest. For estimating the sparse signal, without the need of estimating\nthe sparse noise, we construct a robust Relevance Vector Machine (RVM). In the\nRVM, sparse noise and ever present dense noise are treated through a combined\nnoise model. The precision of combined noise is modeled by a diagonal matrix.\nWe show that the new RVM update equations correspond to a non-symmetric\nsparsity inducing cost function. Further, the combined modeling is found to be\ncomputationally more efficient. We also extend the method to block-sparse\nsignals and noise with known and unknown block structures. Through simulations,\nwe show the performance and computation efficiency of the new RVM in several\napplications: recovery of sparse and block sparse signals, housing price\nprediction and image denoising.\n", "versions": [{"version": "v1", "created": "Mon, 12 Jan 2015 09:34:27 GMT"}], "update_date": "2015-01-13", "authors_parsed": [["Sundin", "Martin", ""], ["Chatterjee", "Saikat", ""], ["Jansson", "Magnus", ""]]}, {"id": "1501.02627", "submitter": "Oliver Serang", "authors": "Oliver Serang", "title": "A fast numerical method for max-convolution and the application to\n  efficient max-product inference in Bayesian networks", "comments": null, "journal-ref": "Journal of Computational Biology. August 2015, 22(8): 770-783", "doi": "10.1089/cmb.2015.0013", "report-no": null, "categories": "cs.NA math.NA stat.CO stat.ME stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Observations depending on sums of random variables are common throughout many\nfields; however, no efficient solution is currently known for performing\nmax-product inference on these sums of general discrete distributions\n(max-product inference can be used to obtain maximum a posteriori estimates).\nThe limiting step to max-product inference is the max-convolution problem\n(sometimes presented in log-transformed form and denoted as \"infimal\nconvolution\", \"min-convolution\", or \"convolution on the tropical semiring\"),\nfor which no O(k log(k)) method is currently known. Here I present a O(k\nlog(k)) numerical method for estimating the max-convolution of two nonnegative\nvectors (e.g., two probability mass functions), where k is the length of the\nlarger vector. This numerical max-convolution method is then demonstrated by\nperforming fast max-product inference on a convolution tree, a data structure\nfor performing fast inference given information on the sum of n discrete random\nvariables in O(n k log(n k) log(n) ) steps (where each random variable has an\narbitrary prior distribution on k contiguous possible states). The numerical\nmax-convolution method can be applied to specialized classes of hidden Markov\nmodels to reduce the runtime of computing the Viterbi path from n k^2 to n k\nlog(k), and has potential application to the all-pairs shortest paths problem.\n", "versions": [{"version": "v1", "created": "Mon, 12 Jan 2015 12:57:01 GMT"}, {"version": "v2", "created": "Wed, 2 Sep 2015 01:49:25 GMT"}], "update_date": "2015-11-19", "authors_parsed": [["Serang", "Oliver", ""]]}, {"id": "1501.02629", "submitter": "Aur\\'elien Bellet", "authors": "St\\'ephan Cl\\'emen\\c{c}on, Aur\\'elien Bellet, Igor Colin", "title": "Scaling-up Empirical Risk Minimization: Optimization of Incomplete\n  U-statistics", "comments": "To appear in Journal of Machine Learning Research. 34 pages. v2:\n  minor correction to Theorem 4 and its proof, added 1 reference. v3: typo\n  corrected in Proposition 3. v4: improved presentation, added experiments on\n  model selection for clustering, fixed minor typos", "journal-ref": "Journal of Machine Learning Research 17(76):1-36, 2016", "doi": null, "report-no": null, "categories": "stat.ML cs.AI cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In a wide range of statistical learning problems such as ranking, clustering\nor metric learning among others, the risk is accurately estimated by\n$U$-statistics of degree $d\\geq 1$, i.e. functionals of the training data with\nlow variance that take the form of averages over $k$-tuples. From a\ncomputational perspective, the calculation of such statistics is highly\nexpensive even for a moderate sample size $n$, as it requires averaging\n$O(n^d)$ terms. This makes learning procedures relying on the optimization of\nsuch data functionals hardly feasible in practice. It is the major goal of this\npaper to show that, strikingly, such empirical risks can be replaced by\ndrastically computationally simpler Monte-Carlo estimates based on $O(n)$ terms\nonly, usually referred to as incomplete $U$-statistics, without damaging the\n$O_{\\mathbb{P}}(1/\\sqrt{n})$ learning rate of Empirical Risk Minimization (ERM)\nprocedures. For this purpose, we establish uniform deviation results describing\nthe error made when approximating a $U$-process by its incomplete version under\nappropriate complexity assumptions. Extensions to model selection, fast rate\nsituations and various sampling techniques are also considered, as well as an\napplication to stochastic gradient descent for ERM. Finally, numerical examples\nare displayed in order to provide strong empirical evidence that the approach\nwe promote largely surpasses more naive subsampling techniques.\n", "versions": [{"version": "v1", "created": "Mon, 12 Jan 2015 12:58:45 GMT"}, {"version": "v2", "created": "Tue, 14 Jul 2015 22:42:22 GMT"}, {"version": "v3", "created": "Wed, 21 Oct 2015 12:42:17 GMT"}, {"version": "v4", "created": "Tue, 19 Apr 2016 06:30:09 GMT"}], "update_date": "2019-01-25", "authors_parsed": [["Cl\u00e9men\u00e7on", "St\u00e9phan", ""], ["Bellet", "Aur\u00e9lien", ""], ["Colin", "Igor", ""]]}, {"id": "1501.02844", "submitter": "Yuan Ning", "authors": "Ryan Ning and Andrew E. Waters and Christoph Studer and Richard G.\n  Baraniuk", "title": "SPRITE: A Response Model For Multiple Choice Testing", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Item response theory (IRT) models for categorical response data are widely\nused in the analysis of educational data, computerized adaptive testing, and\npsychological surveys. However, most IRT models rely on both the assumption\nthat categories are strictly ordered and the assumption that this ordering is\nknown a priori. These assumptions are impractical in many real-world scenarios,\nsuch as multiple-choice exams where the levels of incorrectness for the\ndistractor categories are often unknown. While a number of results exist on IRT\nmodels for unordered categorical data, they tend to have restrictive modeling\nassumptions that lead to poor data fitting performance in practice.\nFurthermore, existing unordered categorical models have parameters that are\ndifficult to interpret. In this work, we propose a novel methodology for\nunordered categorical IRT that we call SPRITE (short for stochastic polytomous\nresponse item model) that: (i) analyzes both ordered and unordered categories,\n(ii) offers interpretable outputs, and (iii) provides improved data fitting\ncompared to existing models. We compare SPRITE to existing item response models\nand demonstrate its efficacy on both synthetic and real-world educational\ndatasets.\n", "versions": [{"version": "v1", "created": "Mon, 12 Jan 2015 22:51:08 GMT"}], "update_date": "2015-01-14", "authors_parsed": [["Ning", "Ryan", ""], ["Waters", "Andrew E.", ""], ["Studer", "Christoph", ""], ["Baraniuk", "Richard G.", ""]]}, {"id": "1501.02859", "submitter": "Saiprasad Ravishankar", "authors": "Saiprasad Ravishankar and Yoram Bresler", "title": "$\\ell_0$ Sparsifying Transform Learning with Efficient Optimal Updates\n  and Convergence Guarantees", "comments": "Accepted to IEEE Transactions on Signal Processing", "journal-ref": null, "doi": "10.1109/TSP.2015.2405503", "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Many applications in signal processing benefit from the sparsity of signals\nin a certain transform domain or dictionary. Synthesis sparsifying dictionaries\nthat are directly adapted to data have been popular in applications such as\nimage denoising, inpainting, and medical image reconstruction. In this work, we\nfocus instead on the sparsifying transform model, and study the learning of\nwell-conditioned square sparsifying transforms. The proposed algorithms\nalternate between a $\\ell_0$ \"norm\"-based sparse coding step, and a non-convex\ntransform update step. We derive the exact analytical solution for each of\nthese steps. The proposed solution for the transform update step achieves the\nglobal minimum in that step, and also provides speedups over iterative\nsolutions involving conjugate gradients. We establish that our alternating\nalgorithms are globally convergent to the set of local minimizers of the\nnon-convex transform learning problems. In practice, the algorithms are\ninsensitive to initialization. We present results illustrating the promising\nperformance and significant speed-ups of transform learning over synthesis\nK-SVD in image denoising.\n", "versions": [{"version": "v1", "created": "Tue, 13 Jan 2015 01:34:40 GMT"}], "update_date": "2015-06-23", "authors_parsed": [["Ravishankar", "Saiprasad", ""], ["Bresler", "Yoram", ""]]}, {"id": "1501.02923", "submitter": "Saiprasad Ravishankar", "authors": "Saiprasad Ravishankar and Yoram Bresler", "title": "Efficient Blind Compressed Sensing Using Sparsifying Transforms with\n  Convergence Guarantees and Application to MRI", "comments": "This work has been accepted for publication in the SIAM Journal on\n  Imaging Sciences. It also appears in Saiprasad Ravishankar's PhD thesis, that\n  was deposited with the University of Illinois on December 05, 2014", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Natural signals and images are well-known to be approximately sparse in\ntransform domains such as Wavelets and DCT. This property has been heavily\nexploited in various applications in image processing and medical imaging.\nCompressed sensing exploits the sparsity of images or image patches in a\ntransform domain or synthesis dictionary to reconstruct images from\nundersampled measurements. In this work, we focus on blind compressed sensing,\nwhere the underlying sparsifying transform is a priori unknown, and propose a\nframework to simultaneously reconstruct the underlying image as well as the\nsparsifying transform from highly undersampled measurements. The proposed block\ncoordinate descent type algorithms involve highly efficient optimal updates.\nImportantly, we prove that although the proposed blind compressed sensing\nformulations are highly nonconvex, our algorithms are globally convergent\n(i.e., they converge from any initialization) to the set of critical points of\nthe objectives defining the formulations. These critical points are guaranteed\nto be at least partial global and partial local minimizers. The exact point(s)\nof convergence may depend on initialization. We illustrate the usefulness of\nthe proposed framework for magnetic resonance image reconstruction from highly\nundersampled k-space measurements. As compared to previous methods involving\nthe synthesis dictionary model, our approach is much faster, while also\nproviding promising reconstruction quality.\n", "versions": [{"version": "v1", "created": "Tue, 13 Jan 2015 09:19:27 GMT"}, {"version": "v2", "created": "Fri, 23 Oct 2015 01:58:24 GMT"}], "update_date": "2015-10-26", "authors_parsed": [["Ravishankar", "Saiprasad", ""], ["Bresler", "Yoram", ""]]}, {"id": "1501.02990", "submitter": "Yi  Li", "authors": "Yi Wang, Yi Li, Momiao Xiong, Li Jin", "title": "Random Bits Regression: a Strong General Predictor for Big Data", "comments": "20 pages,1 figure, 2 tables, research article", "journal-ref": "Big Data Analytics 2016 1:12", "doi": "10.1186/s41044-016-0010-4", "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  To improve accuracy and speed of regressions and classifications, we present\na data-based prediction method, Random Bits Regression (RBR). This method first\ngenerates a large number of random binary intermediate/derived features based\non the original input matrix, and then performs regularized linear/logistic\nregression on those intermediate/derived features to predict the outcome.\nBenchmark analyses on a simulated dataset, UCI machine learning repository\ndatasets and a GWAS dataset showed that RBR outperforms other popular methods\nin accuracy and robustness. RBR (available on\nhttps://sourceforge.net/projects/rbr/) is very fast and requires reasonable\nmemories, therefore, provides a strong, robust and fast predictor in the big\ndata era.\n", "versions": [{"version": "v1", "created": "Tue, 13 Jan 2015 13:14:42 GMT"}], "update_date": "2016-11-04", "authors_parsed": [["Wang", "Yi", ""], ["Li", "Yi", ""], ["Xiong", "Momiao", ""], ["Jin", "Li", ""]]}, {"id": "1501.03001", "submitter": "Emilie Morvant", "authors": "Francois Laviolette, Emilie Morvant (LHC), Liva Ralaivola,\n  Jean-Francis Roy", "title": "On Generalizing the C-Bound to the Multiclass and Multi-label Settings", "comments": "NIPS 2014 Workshop on Representation and Learning Methods for Complex\n  Outputs, Dec 2014, Montr{\\'e}al, Canada", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The C-bound, introduced in Lacasse et al., gives a tight upper bound on the\nrisk of a binary majority vote classifier. In this work, we present a first\nstep towards extending this work to more complex outputs, by providing\ngeneralizations of the C-bound to the multiclass and multi-label settings.\n", "versions": [{"version": "v1", "created": "Tue, 13 Jan 2015 13:47:03 GMT"}], "update_date": "2015-01-14", "authors_parsed": [["Laviolette", "Francois", "", "LHC"], ["Morvant", "Emilie", "", "LHC"], ["Ralaivola", "Liva", ""], ["Roy", "Jean-Francis", ""]]}, {"id": "1501.03002", "submitter": "Emilie Morvant", "authors": "Pascal Germain, Amaury Habrard (LHC), Francois Laviolette, Emilie\n  Morvant (LHC)", "title": "An Improvement to the Domain Adaptation Bound in a PAC-Bayesian context", "comments": "NIPS 2014 Workshop on Transfer and Multi-task learning: Theory Meets\n  Practice, Dec 2014, Montr{\\'e}al, Canada", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper provides a theoretical analysis of domain adaptation based on the\nPAC-Bayesian theory. We propose an improvement of the previous domain\nadaptation bound obtained by Germain et al. in two ways. We first give another\ngeneralization bound tighter and easier to interpret. Moreover, we provide a\nnew analysis of the constant term appearing in the bound that can be of high\ninterest for developing new algorithmic solutions.\n", "versions": [{"version": "v1", "created": "Tue, 13 Jan 2015 13:50:58 GMT"}], "update_date": "2015-01-14", "authors_parsed": [["Germain", "Pascal", "", "LHC"], ["Habrard", "Amaury", "", "LHC"], ["Laviolette", "Francois", "", "LHC"], ["Morvant", "Emilie", "", "LHC"]]}, {"id": "1501.03227", "submitter": "Sylvain Chevallier", "authors": "Emmanuel K. Kalunga, Sylvain Chevallier, Quentin Barthelemy", "title": "Using Riemannian geometry for SSVEP-based Brain Computer Interface", "comments": "29 pages, 6 figures, 1 table, research report. Update on the overall\n  text, most of the figure are modified, the algorithm is explained more\n  clearly, updated comparisons with state of the art methods", "journal-ref": null, "doi": "10.1016/j.neucom.2016.01.007", "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Riemannian geometry has been applied to Brain Computer Interface (BCI) for\nbrain signals classification yielding promising results. Studying\nelectroencephalographic (EEG) signals from their associated covariance matrices\nallows a mitigation of common sources of variability (electronic, electrical,\nbiological) by constructing a representation which is invariant to these\nperturbations. While working in Euclidean space with covariance matrices is\nknown to be error-prone, one might take advantage of algorithmic advances in\ninformation geometry and matrix manifold to implement methods for Symmetric\nPositive-Definite (SPD) matrices. This paper proposes a comprehensive review of\nthe actual tools of information geometry and how they could be applied on\ncovariance matrices of EEG. In practice, covariance matrices should be\nestimated, thus a thorough study of all estimators is conducted on real EEG\ndataset. As a main contribution, this paper proposes an online implementation\nof a classifier in the Riemannian space and its subsequent assessment in\nSteady-State Visually Evoked Potential (SSVEP) experimentations.\n", "versions": [{"version": "v1", "created": "Wed, 14 Jan 2015 01:12:00 GMT"}, {"version": "v2", "created": "Wed, 21 Jan 2015 14:11:47 GMT"}, {"version": "v3", "created": "Tue, 24 Feb 2015 11:02:10 GMT"}], "update_date": "2021-02-11", "authors_parsed": [["Kalunga", "Emmanuel K.", ""], ["Chevallier", "Sylvain", ""], ["Barthelemy", "Quentin", ""]]}, {"id": "1501.03291", "submitter": "Michael Gutmann", "authors": "Michael U. Gutmann and Jukka Corander", "title": "Bayesian Optimization for Likelihood-Free Inference of Simulator-Based\n  Statistical Models", "comments": "In press with the Journal of Machine Learning Research (JMLR).\n  Accepted August 17, 2015", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML stat.CO stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Our paper deals with inferring simulator-based statistical models given some\nobserved data. A simulator-based model is a parametrized mechanism which\nspecifies how data are generated. It is thus also referred to as generative\nmodel. We assume that only a finite number of parameters are of interest and\nallow the generative process to be very general; it may be a noisy nonlinear\ndynamical system with an unrestricted number of hidden variables. This weak\nassumption is useful for devising realistic models but it renders statistical\ninference very difficult. The main challenge is the intractability of the\nlikelihood function. Several likelihood-free inference methods have been\nproposed which share the basic idea of identifying the parameters by finding\nvalues for which the discrepancy between simulated and observed data is small.\nA major obstacle to using these methods is their computational cost. The cost\nis largely due to the need to repeatedly simulate data sets and the lack of\nknowledge about how the parameters affect the discrepancy. We propose a\nstrategy which combines probabilistic modeling of the discrepancy with\noptimization to facilitate likelihood-free inference. The strategy is\nimplemented using Bayesian optimization and is shown to accelerate the\ninference through a reduction in the number of required simulations by several\norders of magnitude.\n", "versions": [{"version": "v1", "created": "Wed, 14 Jan 2015 09:34:15 GMT"}, {"version": "v2", "created": "Thu, 6 Aug 2015 09:37:28 GMT"}, {"version": "v3", "created": "Thu, 31 Dec 2015 15:19:21 GMT"}], "update_date": "2016-01-01", "authors_parsed": [["Gutmann", "Michael U.", ""], ["Corander", "Jukka", ""]]}, {"id": "1501.03326", "submitter": "Heiko Strathmann", "authors": "Heiko Strathmann, Dino Sejdinovic, Mark Girolami", "title": "Unbiased Bayes for Big Data: Paths of Partial Posteriors", "comments": "18 pages, 10 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A key quantity of interest in Bayesian inference are expectations of\nfunctions with respect to a posterior distribution. Markov Chain Monte Carlo is\na fundamental tool to consistently compute these expectations via averaging\nsamples drawn from an approximate posterior. However, its feasibility is being\nchallenged in the era of so called Big Data as all data needs to be processed\nin every iteration. Realising that such simulation is an unnecessarily hard\nproblem if the goal is estimation, we construct a computationally scalable\nmethodology that allows unbiased estimation of the required expectations --\nwithout explicit simulation from the full posterior. The scheme's variance is\nfinite by construction and straightforward to control, leading to algorithms\nthat are provably unbiased and naturally arrive at a desired error tolerance.\nThis is achieved at an average computational complexity that is sub-linear in\nthe size of the dataset and its free parameters are easy to tune. We\ndemonstrate the utility and generality of the methodology on a range of common\nstatistical models applied to large-scale benchmark and real-world datasets.\n", "versions": [{"version": "v1", "created": "Wed, 14 Jan 2015 12:15:14 GMT"}, {"version": "v2", "created": "Mon, 9 Feb 2015 16:21:20 GMT"}], "update_date": "2015-02-10", "authors_parsed": [["Strathmann", "Heiko", ""], ["Sejdinovic", "Dino", ""], ["Girolami", "Mark", ""]]}, {"id": "1501.03347", "submitter": "Faicel Chamroukhi", "authors": "Faicel Chamroukhi, Marius Bartcus, Herv\\'e Glotin", "title": "Dirichlet Process Parsimonious Mixtures for clustering", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The parsimonious Gaussian mixture models, which exploit an eigenvalue\ndecomposition of the group covariance matrices of the Gaussian mixture, have\nshown their success in particular in cluster analysis. Their estimation is in\ngeneral performed by maximum likelihood estimation and has also been considered\nfrom a parametric Bayesian prospective. We propose new Dirichlet Process\nParsimonious mixtures (DPPM) which represent a Bayesian nonparametric\nformulation of these parsimonious Gaussian mixture models. The proposed DPPM\nmodels are Bayesian nonparametric parsimonious mixture models that allow to\nsimultaneously infer the model parameters, the optimal number of mixture\ncomponents and the optimal parsimonious mixture structure from the data. We\ndevelop a Gibbs sampling technique for maximum a posteriori (MAP) estimation of\nthe developed DPMM models and provide a Bayesian model selection framework by\nusing Bayes factors. We apply them to cluster simulated data and real data\nsets, and compare them to the standard parsimonious mixture models. The\nobtained results highlight the effectiveness of the proposed nonparametric\nparsimonious mixture models as a good nonparametric alternative for the\nparametric parsimonious models.\n", "versions": [{"version": "v1", "created": "Wed, 14 Jan 2015 13:56:35 GMT"}, {"version": "v2", "created": "Wed, 17 Oct 2018 11:54:17 GMT"}], "update_date": "2018-10-18", "authors_parsed": [["Chamroukhi", "Faicel", ""], ["Bartcus", "Marius", ""], ["Glotin", "Herv\u00e9", ""]]}, {"id": "1501.03659", "submitter": "Azzimonti Dario", "authors": "Dario Azzimonti (IMSV), Julien Bect (GdR MASCOT-NUM, L2S), Cl\\'ement\n  Chevalier (UNINE), David Ginsbourger (Idiap, IMSV)", "title": "Quantifying uncertainties on excursion sets under a Gaussian random\n  field prior", "comments": null, "journal-ref": "SIAM/ASA Journal on Uncertainty Quantification, 4(1):850-874, 2016", "doi": "10.1137/141000749", "report-no": null, "categories": "math.ST stat.CO stat.ML stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We focus on the problem of estimating and quantifying uncertainties on the\nexcursion set of a function under a limited evaluation budget. We adopt a\nBayesian approach where the objective function is assumed to be a realization\nof a Gaussian random field. In this setting, the posterior distribution on the\nobjective function gives rise to a posterior distribution on excursion sets.\nSeveral approaches exist to summarize the distribution of such sets based on\nrandom closed set theory. While the recently proposed Vorob'ev approach\nexploits analytical formulae, further notions of variability require Monte\nCarlo estimators relying on Gaussian random field conditional simulations. In\nthe present work we propose a method to choose Monte Carlo simulation points\nand obtain quasi-realizations of the conditional field at fine designs through\naffine predictors. The points are chosen optimally in the sense that they\nminimize the posterior expected distance in measure between the excursion set\nand its reconstruction. The proposed method reduces the computational costs due\nto Monte Carlo simulations and enables the computation of quasi-realizations on\nfine designs in large dimensions. We apply this reconstruction approach to\nobtain realizations of an excursion set on a fine grid which allow us to give a\nnew measure of uncertainty based on the distance transform of the excursion\nset. Finally we present a safety engineering test case where the simulation\nmethod is employed to compute a Monte Carlo estimate of a contour line.\n", "versions": [{"version": "v1", "created": "Thu, 15 Jan 2015 12:58:28 GMT"}, {"version": "v2", "created": "Wed, 13 Apr 2016 11:24:30 GMT"}], "update_date": "2017-08-02", "authors_parsed": [["Azzimonti", "Dario", "", "IMSV"], ["Bect", "Julien", "", "GdR MASCOT-NUM, L2S"], ["Chevalier", "Cl\u00e9ment", "", "UNINE"], ["Ginsbourger", "David", "", "Idiap, IMSV"]]}, {"id": "1501.03766", "submitter": "Mariusz Tarnopolski", "authors": "Mariusz Tarnopolski", "title": "Correlation between the Hurst exponent and the maximal Lyapunov\n  exponent: examining some low-dimensional conservative maps", "comments": "20 pages, 9 figures; accepted in Physica A", "journal-ref": null, "doi": null, "report-no": null, "categories": "nlin.CD math.DS physics.data-an stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The Chirikov standard map and the 2D Froeschl\\'e map are investigated. A few\nthousand values of the Hurst exponent (HE) and the maximal Lyapunov exponent\n(mLE) are plotted in a mixed space of the nonlinear parameter versus the\ninitial condition. Both characteristic exponents reveal remarkably similar\nstructures in this space. A tight correlation between the HEs and mLEs is\nfound, with the Spearman rank $\\rho=0.83$ and $\\rho=0.75$ for the Chirikov and\n2D Froeschl\\'e maps, respectively. Based on this relation, a machine learning\n(ML) procedure, using the nearest neighbor algorithm, is performed to reproduce\nthe HE distribution based on the mLE distribution alone. A few thousand HE and\nmLE values from the mixed spaces were used for training, and then using\n$2-2.4\\times 10^5$ mLEs, the HEs were retrieved. The ML procedure allowed to\nreproduce the structure of the mixed spaces in great detail.\n", "versions": [{"version": "v1", "created": "Thu, 15 Jan 2015 18:14:32 GMT"}, {"version": "v2", "created": "Mon, 19 Oct 2015 18:05:12 GMT"}, {"version": "v3", "created": "Mon, 14 Mar 2016 08:16:42 GMT"}, {"version": "v4", "created": "Tue, 9 May 2017 17:20:53 GMT"}, {"version": "v5", "created": "Thu, 31 Aug 2017 17:03:32 GMT"}], "update_date": "2017-09-01", "authors_parsed": [["Tarnopolski", "Mariusz", ""]]}, {"id": "1501.03771", "submitter": "Anton Osokin", "authors": "Anton Osokin, Dmitry Vetrov", "title": "Submodular relaxation for inference in Markov random fields", "comments": "This paper is accepted for publication in IEEE Transactions on\n  Pattern Analysis and Machine Intelligence", "journal-ref": null, "doi": "10.1109/TPAMI.2014.2369046", "report-no": null, "categories": "cs.CV math.OC stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper we address the problem of finding the most probable state of a\ndiscrete Markov random field (MRF), also known as the MRF energy minimization\nproblem. The task is known to be NP-hard in general and its practical\nimportance motivates numerous approximate algorithms. We propose a submodular\nrelaxation approach (SMR) based on a Lagrangian relaxation of the initial\nproblem. Unlike the dual decomposition approach of Komodakis et al., 2011 SMR\ndoes not decompose the graph structure of the initial problem but constructs a\nsubmodular energy that is minimized within the Lagrangian relaxation. Our\napproach is applicable to both pairwise and high-order MRFs and allows to take\ninto account global potentials of certain types. We study theoretical\nproperties of the proposed approach and evaluate it experimentally.\n", "versions": [{"version": "v1", "created": "Thu, 15 Jan 2015 18:34:27 GMT"}], "update_date": "2015-01-16", "authors_parsed": [["Osokin", "Anton", ""], ["Vetrov", "Dmitry", ""]]}, {"id": "1501.03796", "submitter": "Akshay Balsubramani", "authors": "Akshay Balsubramani, Sanjoy Dasgupta, Yoav Freund", "title": "The Fast Convergence of Incremental PCA", "comments": "NIPS 2013", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider a situation in which we see samples in $\\mathbb{R}^d$ drawn\ni.i.d. from some distribution with mean zero and unknown covariance A. We wish\nto compute the top eigenvector of A in an incremental fashion - with an\nalgorithm that maintains an estimate of the top eigenvector in O(d) space, and\nincrementally adjusts the estimate with each new data point that arrives. Two\nclassical such schemes are due to Krasulina (1969) and Oja (1983). We give\nfinite-sample convergence rates for both.\n", "versions": [{"version": "v1", "created": "Thu, 15 Jan 2015 20:08:49 GMT"}], "update_date": "2015-01-16", "authors_parsed": [["Balsubramani", "Akshay", ""], ["Dasgupta", "Sanjoy", ""], ["Freund", "Yoav", ""]]}, {"id": "1501.03838", "submitter": "Akshay Balsubramani", "authors": "Akshay Balsubramani, Yoav Freund", "title": "PAC-Bayes with Minimax for Confidence-Rated Transduction", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider using an ensemble of binary classifiers for transductive\nprediction, when unlabeled test data are known in advance. We derive minimax\noptimal rules for confidence-rated prediction in this setting. By using\nPAC-Bayes analysis on these rules, we obtain data-dependent performance\nguarantees without distributional assumptions on the data. Our analysis\ntechniques are readily extended to a setting in which the predictor is allowed\nto abstain.\n", "versions": [{"version": "v1", "created": "Thu, 15 Jan 2015 21:59:39 GMT"}], "update_date": "2015-01-19", "authors_parsed": [["Balsubramani", "Akshay", ""], ["Freund", "Yoav", ""]]}, {"id": "1501.03844", "submitter": "Pan Zhang", "authors": "Pan Zhang", "title": "Evaluating accuracy of community detection using the relative normalized\n  mutual information", "comments": "comments are welcome", "journal-ref": null, "doi": "10.1088/1742-5468/2015/11/P11006", "report-no": null, "categories": "physics.soc-ph cond-mat.stat-mech cs.SI stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The Normalized Mutual Information (NMI) has been widely used to evaluate the\naccuracy of community detection algorithms. However in this article we show\nthat the NMI is seriously affected by systematic errors due to finite size of\nnetworks, and may give a wrong estimate of performance of algorithms in some\ncases. We give a simple theory to the finite-size effect of NMI and test our\ntheory numerically. Then we propose a new metric for the accuracy of community\ndetection, namely the relative Normalized Mutual Information (rNMI), which\nconsiders statistical significance of the NMI by comparing it with the expected\nNMI of random partitions. Our numerical experiments show that the rNMI\novercomes the finite-size effect of the NMI.\n", "versions": [{"version": "v1", "created": "Thu, 15 Jan 2015 22:48:26 GMT"}, {"version": "v2", "created": "Wed, 28 Oct 2015 08:52:29 GMT"}], "update_date": "2015-12-09", "authors_parsed": [["Zhang", "Pan", ""]]}, {"id": "1501.03854", "submitter": "Kevin Vu", "authors": "Kevin Vu, John Snyder, Li Li, Matthias Rupp, Brandon F. Chen, Tarek\n  Khelif, Klaus-Robert M\\\"uller, Kieron Burke", "title": "Understanding Kernel Ridge Regression: Common behaviors from simple\n  functions to density functionals", "comments": "15 pages, 20 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "physics.comp-ph cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Accurate approximations to density functionals have recently been obtained\nvia machine learning (ML). By applying ML to a simple function of one variable\nwithout any random sampling, we extract the qualitative dependence of errors on\nhyperparameters. We find universal features of the behavior in extreme limits,\nincluding both very small and very large length scales, and the noise-free\nlimit. We show how such features arise in ML models of density functionals.\n", "versions": [{"version": "v1", "created": "Fri, 16 Jan 2015 00:00:46 GMT"}, {"version": "v2", "created": "Wed, 28 Jan 2015 12:02:50 GMT"}], "update_date": "2015-01-29", "authors_parsed": [["Vu", "Kevin", ""], ["Snyder", "John", ""], ["Li", "Li", ""], ["Rupp", "Matthias", ""], ["Chen", "Brandon F.", ""], ["Khelif", "Tarek", ""], ["M\u00fcller", "Klaus-Robert", ""], ["Burke", "Kieron", ""]]}, {"id": "1501.03861", "submitter": "Alexander Spangher", "authors": "Alexander Spangher", "title": "Bayesian Nonparametrics in Topic Modeling: A Brief Tutorial", "comments": "7 pages, unpublished", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Using nonparametric methods has been increasingly explored in Bayesian\nhierarchical modeling as a way to increase model flexibility. Although the\nfield shows a lot of promise, inference in many models, including Hierachical\nDirichlet Processes (HDP), remain prohibitively slow. One promising path\nforward is to exploit the submodularity inherent in Indian Buffet Process (IBP)\nto derive near-optimal solutions in polynomial time. In this work, I will\npresent a brief tutorial on Bayesian nonparametric methods, especially as they\nare applied to topic modeling. I will show a comparison between different\nnon-parametric models and the current state-of-the-art parametric model, Latent\nDirichlet Allocation (LDA).\n", "versions": [{"version": "v1", "created": "Fri, 16 Jan 2015 01:59:34 GMT"}], "update_date": "2015-01-19", "authors_parsed": [["Spangher", "Alexander", ""]]}, {"id": "1501.03959", "submitter": "Kamil Ciosek", "authors": "Kamil Ciosek and David Silver", "title": "Value Iteration with Options and State Aggregation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper presents a way of solving Markov Decision Processes that combines\nstate abstraction and temporal abstraction. Specifically, we combine state\naggregation with the options framework and demonstrate that they work well\ntogether and indeed it is only after one combines the two that the full benefit\nof each is realized. We introduce a hierarchical value iteration algorithm\nwhere we first coarsely solve subgoals and then use these approximate solutions\nto exactly solve the MDP. This algorithm solved several problems faster than\nvanilla value iteration.\n", "versions": [{"version": "v1", "created": "Fri, 16 Jan 2015 12:02:51 GMT"}], "update_date": "2015-01-19", "authors_parsed": [["Ciosek", "Kamil", ""], ["Silver", "David", ""]]}, {"id": "1501.04053", "submitter": "Dionissios Hristopulos Prof.", "authors": "Dionissios T. Hristopulos", "title": "Stochastic Local Interaction (SLI) Model: Interfacing Machine Learning\n  and Geostatistics", "comments": "31 pages, 7 figures", "journal-ref": null, "doi": "10.1016/j.cageo.2015.05.018", "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Machine learning and geostatistics are powerful mathematical frameworks for\nmodeling spatial data. Both approaches, however, suffer from poor scaling of\nthe required computational resources for large data applications. We present\nthe Stochastic Local Interaction (SLI) model, which employs a local\nrepresentation to improve computational efficiency. SLI combines geostatistics\nand machine learning with ideas from statistical physics and computational\ngeometry. It is based on a joint probability density function defined by an\nenergy functional which involves local interactions implemented by means of\nkernel functions with adaptive local kernel bandwidths. SLI is expressed in\nterms of an explicit, typically sparse, precision (inverse covariance) matrix.\nThis representation leads to a semi-analytical expression for interpolation\n(prediction), which is valid in any number of dimensions and avoids the\ncomputationally costly covariance matrix inversion.\n", "versions": [{"version": "v1", "created": "Fri, 16 Jan 2015 17:09:01 GMT"}, {"version": "v2", "created": "Tue, 5 May 2015 13:03:58 GMT"}], "update_date": "2015-07-15", "authors_parsed": [["Hristopulos", "Dionissios T.", ""]]}, {"id": "1501.04080", "submitter": "Matthew Kusner", "authors": "Matt J. Kusner, Jacob R. Gardner, Roman Garnett, Kilian Q. Weinberger", "title": "Differentially Private Bayesian Optimization", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Bayesian optimization is a powerful tool for fine-tuning the hyper-parameters\nof a wide variety of machine learning models. The success of machine learning\nhas led practitioners in diverse real-world settings to learn classifiers for\npractical problems. As machine learning becomes commonplace, Bayesian\noptimization becomes an attractive method for practitioners to automate the\nprocess of classifier hyper-parameter tuning. A key observation is that the\ndata used for tuning models in these settings is often sensitive. Certain data\nsuch as genetic predisposition, personal email statistics, and car accident\nhistory, if not properly private, may be at risk of being inferred from\nBayesian optimization outputs. To address this, we introduce methods for\nreleasing the best hyper-parameters and classifier accuracy privately.\nLeveraging the strong theoretical guarantees of differential privacy and known\nBayesian optimization convergence bounds, we prove that under a GP assumption\nthese private quantities are also near-optimal. Finally, even if this\nassumption is not satisfied, we can use different smoothness guarantees to\nprotect privacy.\n", "versions": [{"version": "v1", "created": "Fri, 16 Jan 2015 19:18:15 GMT"}, {"version": "v2", "created": "Mon, 23 Feb 2015 04:09:52 GMT"}], "update_date": "2015-02-24", "authors_parsed": [["Kusner", "Matt J.", ""], ["Gardner", "Jacob R.", ""], ["Garnett", "Roman", ""], ["Weinberger", "Kilian Q.", ""]]}, {"id": "1501.04308", "submitter": "Enea Giuseppe Bongiorno", "authors": "Enea Bongiorno and Aldo Goia", "title": "Some Insights About the Small Ball Probability Factorization for Hilbert\n  Random Elements", "comments": "27 pages, 1 figure, 2 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.PR math.ST stat.AP stat.ME stat.ML stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Asymptotic factorizations for the small-ball probability (SmBP) of a Hilbert\nvalued random element $X$ are rigorously established and discussed. In\nparticular, given the first $d$ principal components (PCs) and as the radius\n$\\varepsilon$ of the ball tends to zero, the SmBP is asymptotically\nproportional to (a) the joint density of the first $d$ PCs, (b) the volume of\nthe $d$-dimensional ball with radius $\\varepsilon$, and (c) a correction factor\nweighting the use of a truncated version of the process expansion. Moreover,\nunder suitable assumptions on the spectrum of the covariance operator of $X$\nand as $d$ diverges to infinity when $\\varepsilon$ vanishes, some\nsimplifications occur. In particular, the SmBP factorizes asymptotically as the\nproduct of the joint density of the first $d$ PCs and a pure volume parameter.\nAll the provided factorizations allow to define a surrogate intensity of the\nSmBP that, in some cases, leads to a genuine intensity. To operationalize the\nstated results, a non-parametric estimator for the surrogate intensity is\nintroduced and it is proved that the use of estimated PCs, instead of the true\nones, does not affect the rate of convergence. Finally, as an illustration,\nsimulations in controlled frameworks are provided.\n", "versions": [{"version": "v1", "created": "Sun, 18 Jan 2015 14:48:30 GMT"}, {"version": "v2", "created": "Tue, 29 Mar 2016 08:31:19 GMT"}], "update_date": "2016-03-30", "authors_parsed": [["Bongiorno", "Enea", ""], ["Goia", "Aldo", ""]]}, {"id": "1501.04318", "submitter": "Teng Qiu", "authors": "Teng Qiu, Yongjie Li", "title": "Clustering based on the In-tree Graph Structure and Affinity Propagation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV stat.ML", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  A recently proposed clustering method, called the Nearest Descent (ND), can\norganize the whole dataset into a sparsely connected graph, called the In-tree.\nThis ND-based Intree structure proves able to reveal the clustering structure\nunderlying the dataset, except one imperfect place, that is, there are some\nundesired edges in this In-tree which require to be removed. Here, we propose\nan effective way to automatically remove the undesired edges in In-tree via an\neffective combination of the In-tree structure with affinity propagation (AP).\nThe key for the combination is to add edges between the reachable nodes in\nIn-tree before using AP to remove the undesired edges. The experiments on both\nsynthetic and real datasets demonstrate the effectiveness of the proposed\nmethod.\n", "versions": [{"version": "v1", "created": "Sun, 18 Jan 2015 15:34:19 GMT"}, {"version": "v2", "created": "Mon, 29 Jan 2018 00:34:26 GMT"}], "update_date": "2018-01-30", "authors_parsed": [["Qiu", "Teng", ""], ["Li", "Yongjie", ""]]}, {"id": "1501.04325", "submitter": "Lars Maaloe", "authors": "Lars Maaloe and Morten Arngren and Ole Winther", "title": "Deep Belief Nets for Topic Modeling", "comments": "Accepted to the ICML-2014 Workshop on Knowledge-Powered Deep Learning\n  for Text Mining", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Applying traditional collaborative filtering to digital publishing is\nchallenging because user data is very sparse due to the high volume of\ndocuments relative to the number of users. Content based approaches, on the\nother hand, is attractive because textual content is often very informative. In\nthis paper we describe large-scale content based collaborative filtering for\ndigital publishing. To solve the digital publishing recommender problem we\ncompare two approaches: latent Dirichlet allocation (LDA) and deep belief nets\n(DBN) that both find low-dimensional latent representations for documents.\nEfficient retrieval can be carried out in the latent representation. We work\nboth on public benchmarks and digital media content provided by Issuu, an\nonline publishing platform. This article also comes with a newly developed deep\nbelief nets toolbox for topic modeling tailored towards performance evaluation\nof the DBN model and comparisons to the LDA model.\n", "versions": [{"version": "v1", "created": "Sun, 18 Jan 2015 17:12:59 GMT"}], "update_date": "2015-01-20", "authors_parsed": [["Maaloe", "Lars", ""], ["Arngren", "Morten", ""], ["Winther", "Ole", ""]]}, {"id": "1501.04346", "submitter": "Divyanshu Vats", "authors": "Andrew S. Lan and Divyanshu Vats and Andrew E. Waters and Richard G.\n  Baraniuk", "title": "Mathematical Language Processing: Automatic Grading and Feedback for\n  Open Response Mathematical Questions", "comments": "ACM Conference on Learning at Scale, March 2015", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.AI cs.CL cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  While computer and communication technologies have provided effective means\nto scale up many aspects of education, the submission and grading of\nassessments such as homework assignments and tests remains a weak link. In this\npaper, we study the problem of automatically grading the kinds of open response\nmathematical questions that figure prominently in STEM (science, technology,\nengineering, and mathematics) courses. Our data-driven framework for\nmathematical language processing (MLP) leverages solution data from a large\nnumber of learners to evaluate the correctness of their solutions, assign\npartial-credit scores, and provide feedback to each learner on the likely\nlocations of any errors. MLP takes inspiration from the success of natural\nlanguage processing for text data and comprises three main steps. First, we\nconvert each solution to an open response mathematical question into a series\nof numerical features. Second, we cluster the features from several solutions\nto uncover the structures of correct, partially correct, and incorrect\nsolutions. We develop two different clustering approaches, one that leverages\ngeneric clustering algorithms and one based on Bayesian nonparametrics. Third,\nwe automatically grade the remaining (potentially large number of) solutions\nbased on their assigned cluster and one instructor-provided grade per cluster.\nAs a bonus, we can track the cluster assignment of each step of a multistep\nsolution and determine when it departs from a cluster of correct solutions,\nwhich enables us to indicate the likely locations of errors to learners. We\ntest and validate MLP on real-world MOOC data to demonstrate how it can\nsubstantially reduce the human effort required in large-scale educational\nplatforms.\n", "versions": [{"version": "v1", "created": "Sun, 18 Jan 2015 20:50:39 GMT"}], "update_date": "2015-01-20", "authors_parsed": [["Lan", "Andrew S.", ""], ["Vats", "Divyanshu", ""], ["Waters", "Andrew E.", ""], ["Baraniuk", "Richard G.", ""]]}, {"id": "1501.04370", "submitter": "Ru He", "authors": "Ru He, Jin Tian, Huaiqing Wu", "title": "Structure Learning in Bayesian Networks of Moderate Size by Efficient\n  Sampling", "comments": "51 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study the Bayesian model averaging approach to learning Bayesian network\nstructures (DAGs) from data. We develop new algorithms including the first\nalgorithm that is able to efficiently sample DAGs according to the exact\nstructure posterior. The DAG samples can then be used to construct estimators\nfor the posterior of any feature. We theoretically prove good properties of our\nestimators and empirically show that our estimators considerably outperform the\nestimators from the previous state-of-the-art methods.\n", "versions": [{"version": "v1", "created": "Mon, 19 Jan 2015 01:32:43 GMT"}], "update_date": "2015-01-20", "authors_parsed": [["He", "Ru", ""], ["Tian", "Jin", ""], ["Wu", "Huaiqing", ""]]}, {"id": "1501.04413", "submitter": "Masayuki Ohzeki", "authors": "Masayuki Ohzeki", "title": "Statistical-mechanical analysis of pre-training and fine tuning in deep\n  learning", "comments": "13 pages and 2 figures, to appear in JPSJ", "journal-ref": null, "doi": "10.7566/JPSJ.84.034003", "report-no": null, "categories": "stat.ML cond-mat.dis-nn cond-mat.stat-mech cs.AI cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we present a statistical-mechanical analysis of deep learning.\nWe elucidate some of the essential components of deep learning---pre-training\nby unsupervised learning and fine tuning by supervised learning. We formulate\nthe extraction of features from the training data as a margin criterion in a\nhigh-dimensional feature-vector space. The self-organized classifier is then\nsupplied with small amounts of labelled data, as in deep learning. Although we\nemploy a simple single-layer perceptron model, rather than directly analyzing a\nmulti-layer neural network, we find a nontrivial phase transition that is\ndependent on the number of unlabelled data in the generalization error of the\nresultant classifier. In this sense, we evaluate the efficacy of the\nunsupervised learning component of deep learning. The analysis is performed by\nthe replica method, which is a sophisticated tool in statistical mechanics. We\nvalidate our result in the manner of deep learning, using a simple iterative\nalgorithm to learn the weight vector on the basis of belief propagation.\n", "versions": [{"version": "v1", "created": "Mon, 19 Jan 2015 07:24:21 GMT"}], "update_date": "2015-06-23", "authors_parsed": [["Ohzeki", "Masayuki", ""]]}, {"id": "1501.04467", "submitter": "Alexandra Carpentier", "authors": "Alexandra Carpentier", "title": "Implementable confidence sets in high dimensional regression", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider the setting of linear regression in high dimension. We focus on\nthe problem of constructing adaptive and honest confidence sets for the sparse\nparameter \\theta, i.e. we want to construct a confidence set for theta that\ncontains theta with high probability, and that is as small as possible. The l_2\ndiameter of a such confidence set should depend on the sparsity S of \\theta -\nthe larger S, the wider the confidence set. However, in practice, S is unknown.\nThis paper focuses on constructing a confidence set for \\theta which contains\n\\theta with high probability, whose diameter is adaptive to the unknown\nsparsity S, and which is implementable in practice.\n", "versions": [{"version": "v1", "created": "Mon, 19 Jan 2015 12:13:44 GMT"}], "update_date": "2015-01-20", "authors_parsed": [["Carpentier", "Alexandra", ""]]}, {"id": "1501.04656", "submitter": "Marcus A. Brubaker", "authors": "Ali Punjani and Marcus A. Brubaker", "title": "Microscopic Advances with Large-Scale Learning: Stochastic Optimization\n  for Cryo-EM", "comments": "Presented at NIPS 2014 Workshop on Machine Learning in Computational\n  Biology http://mlcb.org", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.CV cs.LG q-bio.QM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Determining the 3D structures of biological molecules is a key problem for\nboth biology and medicine. Electron Cryomicroscopy (Cryo-EM) is a promising\ntechnique for structure estimation which relies heavily on computational\nmethods to reconstruct 3D structures from 2D images. This paper introduces the\nchallenging Cryo-EM density estimation problem as a novel application for\nstochastic optimization techniques. Structure discovery is formulated as MAP\nestimation in a probabilistic latent-variable model, resulting in an\noptimization problem to which an array of seven stochastic optimization methods\nare applied. The methods are tested on both real and synthetic data, with some\nmethods recovering reasonable structures in less than one epoch from a random\ninitialization. Complex quasi-Newton methods are found to converge more slowly\nthan simple gradient-based methods, but all stochastic methods are found to\nconverge to similar optima. This method represents a major improvement over\nexisting methods as it is significantly faster and is able to converge from a\nrandom initialization.\n", "versions": [{"version": "v1", "created": "Mon, 19 Jan 2015 22:07:27 GMT"}, {"version": "v2", "created": "Tue, 27 Jan 2015 21:13:28 GMT"}], "update_date": "2015-01-29", "authors_parsed": [["Punjani", "Ali", ""], ["Brubaker", "Marcus A.", ""]]}, {"id": "1501.04819", "submitter": "Ashley Prater", "authors": "Ashley Prater and Lixin Shen", "title": "Separation of undersampled composite signals using the Dantzig selector\n  with overcomplete dictionaries", "comments": "18 pages, 4 figures, preprint of a paper accepted by IET Signal\n  Processing", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.NA stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In many applications one may acquire a composition of several signals that\nmay be corrupted by noise, and it is a challenging problem to reliably separate\nthe components from one another without sacrificing significant details. Adding\nto the challenge, in a compressive sensing framework, one is given only an\nundersampled set of linear projections of the composite signal. In this paper,\nwe propose using the Dantzig selector model incorporating an overcomplete\ndictionary to separate a noisy undersampled collection of composite signals,\nand present an algorithm to efficiently solve the model.\n  The Dantzig selector is a statistical approach to finding a solution to a\nnoisy linear regression problem by minimizing the $\\ell_1$ norm of candidate\ncoefficient vectors while constraining the scope of the residuals. If the\nunderlying coefficient vector is sparse, then the Dantzig selector performs\nwell in the recovery and separation of the unknown composite signal. In the\nfollowing, we propose a proximity operator based algorithm to recover and\nseparate unknown noisy undersampled composite signals through the Dantzig\nselector. We present numerical simulations comparing the proposed algorithm\nwith the competing Alternating Direction Method, and the proposed algorithm is\nfound to be faster, while producing similar quality results. Additionally, we\ndemonstrate the utility of the proposed algorithm in several experiments by\napplying it in various domain applications including the recovery of\ncomplex-valued coefficient vectors, the removal of impulse noise from smooth\nsignals, and the separation and classification of a composition of handwritten\ndigits.\n", "versions": [{"version": "v1", "created": "Tue, 20 Jan 2015 14:22:38 GMT"}], "update_date": "2015-01-21", "authors_parsed": [["Prater", "Ashley", ""], ["Shen", "Lixin", ""]]}, {"id": "1501.04870", "submitter": "Luca Martino", "authors": "J. Read, L. Martino, P. Olmos, D. Luengo", "title": "Scalable Multi-Output Label Prediction: From Classifier Chains to\n  Classifier Trellises", "comments": "(accepted in Pattern Recognition)", "journal-ref": "Pattern Recognition, Volume 48, Issue 6, 2015, Pages 2096-2109", "doi": "10.1016/j.patcog.2015.01.004", "report-no": null, "categories": "stat.ML cs.CV cs.DS cs.LG stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Multi-output inference tasks, such as multi-label classification, have become\nincreasingly important in recent years. A popular method for multi-label\nclassification is classifier chains, in which the predictions of individual\nclassifiers are cascaded along a chain, thus taking into account inter-label\ndependencies and improving the overall performance. Several varieties of\nclassifier chain methods have been introduced, and many of them perform very\ncompetitively across a wide range of benchmark datasets. However, scalability\nlimitations become apparent on larger datasets when modeling a fully-cascaded\nchain. In particular, the methods' strategies for discovering and modeling a\ngood chain structure constitutes a mayor computational bottleneck. In this\npaper, we present the classifier trellis (CT) method for scalable multi-label\nclassification. We compare CT with several recently proposed classifier chain\nmethods to show that it occupies an important niche: it is highly competitive\non standard multi-label problems, yet it can also scale up to thousands or even\ntens of thousands of labels.\n", "versions": [{"version": "v1", "created": "Tue, 20 Jan 2015 16:33:40 GMT"}], "update_date": "2015-06-01", "authors_parsed": [["Read", "J.", ""], ["Martino", "L.", ""], ["Olmos", "P.", ""], ["Luengo", "D.", ""]]}, {"id": "1501.05068", "submitter": "Kevin H. Knuth", "authors": "Kevin H. Knuth", "title": "Difficulties applying recent blind source separation techniques to EEG\n  and MEG", "comments": "14 pages, 5 figures; Knuth K. H. 1998. Difficulties applying recent\n  blind source separation techniques to EEG and MEG. In: G.J. Erickson, J.T.\n  Rychert and C.R. Smith (eds.), Maximum Entropy and Bayesian Methods, Boise\n  1997, Kluwer, Dordrecht, pp. 209-222", "journal-ref": null, "doi": null, "report-no": null, "categories": "physics.data-an physics.med-ph stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  High temporal resolution measurements of human brain activity can be\nperformed by recording the electric potentials on the scalp surface\n(electroencephalography, EEG), or by recording the magnetic fields near the\nsurface of the head (magnetoencephalography, MEG). The analysis of the data is\nproblematic due to the fact that multiple neural generators may be\nsimultaneously active and the potentials and magnetic fields from these sources\nare superimposed on the detectors. It is highly desirable to un-mix the data\ninto signals representing the behaviors of the original individual generators.\nThis general problem is called blind source separation and several recent\ntechniques utilizing maximum entropy, minimum mutual information, and maximum\nlikelihood estimation have been applied. These techniques have had much success\nin separating signals such as natural sounds or speech, but appear to be\nineffective when applied to EEG or MEG signals. Many of these techniques\nimplicitly assume that the source distributions have a large kurtosis, whereas\nan analysis of EEG/MEG signals reveals that the distributions are multimodal.\nThis suggests that more effective separation techniques could be designed for\nEEG and MEG signals.\n", "versions": [{"version": "v1", "created": "Wed, 21 Jan 2015 06:17:50 GMT"}], "update_date": "2015-01-22", "authors_parsed": [["Knuth", "Kevin H.", ""]]}, {"id": "1501.05069", "submitter": "Kevin H. Knuth", "authors": "Kevin H. Knuth and Herbert G. Vaughan Jr", "title": "Convergent Bayesian formulations of blind source separation and\n  electromagnetic source estimation", "comments": "10 pages, Presented at the 1998 MaxEnt Workshop in Garching, Germany.\n  Knuth K.H., Vaughan H.G., Jr. 1999. Convergent Bayesian formulations of blind\n  source separation and electromagnetic source estimation. In: W. von der\n  Linden, V. Dose, R. Fischer and R. Preuss (eds.), Maximum Entropy and\n  Bayesian Methods, Munich 1998, Dordrecht. Kluwer, pp. 217-226", "journal-ref": "Maximum Entropy and Bayesian Methods, Munich 1998, Dordrecht.\n  Kluwer, pp. 217-226", "doi": null, "report-no": null, "categories": "physics.data-an physics.med-ph stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider two areas of research that have been developing in parallel over\nthe last decade: blind source separation (BSS) and electromagnetic source\nestimation (ESE). BSS deals with the recovery of source signals when only\nmixtures of signals can be obtained from an array of detectors and the only\nprior knowledge consists of some information about the nature of the source\nsignals. On the other hand, ESE utilizes knowledge of the electromagnetic\nforward problem to assign source signals to their respective generators, while\ninformation about the signals themselves is typically ignored. We demonstrate\nthat these two techniques can be derived from the same starting point using the\nBayesian formalism. This suggests a means by which new algorithms can be\ndeveloped that utilize as much relevant information as possible. We also\nbriefly mention some preliminary work that supports the value of integrating\ninformation used by these two techniques and review the kinds of information\nthat may be useful in addressing the ESE problem.\n", "versions": [{"version": "v1", "created": "Wed, 21 Jan 2015 06:18:25 GMT"}], "update_date": "2015-01-22", "authors_parsed": [["Knuth", "Kevin H.", ""], ["Vaughan", "Herbert G.", "Jr"]]}, {"id": "1501.05108", "submitter": "Reza Mohammadi", "authors": "Reza Mohammadi and Ernst C. Wit", "title": "BDgraph: An R Package for Bayesian Structure Learning in Graphical\n  Models", "comments": "Published at https://www.jstatsoft.org/article/view/v089i03 in the\n  Journal of Statistical Software", "journal-ref": "Journal of Statistical Software 2019, Volume 89, Issue 3, 1-30", "doi": "10.18637/jss.v089.i03", "report-no": null, "categories": "stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Graphical models provide powerful tools to uncover complicated patterns in\nmultivariate data and are commonly used in Bayesian statistics and machine\nlearning. In this paper, we introduce the R package BDgraph which performs\nBayesian structure learning for general undirected graphical models\n(decomposable and non-decomposable) with continuous, discrete, and mixed\nvariables. The package efficiently implements recent improvements in the\nBayesian literature, including that of Mohammadi and Wit (2015) and Dobra and\nMohammadi (2018). To speed up computations, the computationally intensive tasks\nhave been implemented in C++ and interfaced with R, and the package has\nparallel computing capabilities. In addition, the package contains several\nfunctions for simulation and visualization, as well as several multivariate\ndatasets taken from the literature and used to describe the package\ncapabilities. The paper includes a brief overview of the statistical methods\nwhich have been implemented in the package. The main part of the paper explains\nhow to use the package. Furthermore, we illustrate the package's functionality\nin both real and artificial examples.\n", "versions": [{"version": "v1", "created": "Wed, 21 Jan 2015 09:50:34 GMT"}, {"version": "v2", "created": "Fri, 6 Mar 2015 16:56:13 GMT"}, {"version": "v3", "created": "Sun, 19 Apr 2015 16:17:43 GMT"}, {"version": "v4", "created": "Sat, 5 Dec 2015 22:34:38 GMT"}, {"version": "v5", "created": "Thu, 7 Dec 2017 15:02:44 GMT"}, {"version": "v6", "created": "Mon, 13 May 2019 15:37:45 GMT"}], "update_date": "2019-05-14", "authors_parsed": [["Mohammadi", "Reza", ""], ["Wit", "Ernst C.", ""]]}, {"id": "1501.05144", "submitter": "Dennis Prangle", "authors": "Dennis Prangle", "title": "Lazier ABC", "comments": "Presented as contributed paper at \"ABC in Montreal\" NIPS workshop in\n  December 2014", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.CO stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  ABC algorithms involve a large number of simulations from the model of\ninterest, which can be very computationally costly. This paper summarises the\nlazy ABC algorithm of Prangle (2015), which reduces the computational demand by\nabandoning many unpromising simulations before completion. By using a random\nstopping decision and reweighting the output sample appropriately, the target\ndistribution is the same as for standard ABC. Lazy ABC is also extended here to\nthe case of non-uniform ABC kernels, which is shown to simplify the process of\ntuning the algorithm effectively.\n", "versions": [{"version": "v1", "created": "Wed, 21 Jan 2015 11:48:52 GMT"}], "update_date": "2015-01-22", "authors_parsed": [["Prangle", "Dennis", ""]]}, {"id": "1501.05194", "submitter": "Guillaume Marrelec", "authors": "Guillaume Marrelec, Arnaud Mess\\'e, Pierre Bellec", "title": "A Bayesian alternative to mutual information for the hierarchical\n  clustering of dependent random variables", "comments": null, "journal-ref": "G. Marrelec, A. Messe, P. Bellec (2015) A Bayesian alternative to\n  mutual information for the hierarchical clustering of dependent random\n  variables. PLoS ONE 10(9): e0137278", "doi": "10.1371/journal.pone.0137278", "report-no": null, "categories": "stat.ML cs.LG q-bio.QM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The use of mutual information as a similarity measure in agglomerative\nhierarchical clustering (AHC) raises an important issue: some correction needs\nto be applied for the dimensionality of variables. In this work, we formulate\nthe decision of merging dependent multivariate normal variables in an AHC\nprocedure as a Bayesian model comparison. We found that the Bayesian\nformulation naturally shrinks the empirical covariance matrix towards a matrix\nset a priori (e.g., the identity), provides an automated stopping rule, and\ncorrects for dimensionality using a term that scales up the measure as a\nfunction of the dimensionality of the variables. Also, the resulting log Bayes\nfactor is asymptotically proportional to the plug-in estimate of mutual\ninformation, with an additive correction for dimensionality in agreement with\nthe Bayesian information criterion. We investigated the behavior of these\nBayesian alternatives (in exact and asymptotic forms) to mutual information on\nsimulated and real data. An encouraging result was first derived on\nsimulations: the hierarchical clustering based on the log Bayes factor\noutperformed off-the-shelf clustering techniques as well as raw and normalized\nmutual information in terms of classification accuracy. On a toy example, we\nfound that the Bayesian approaches led to results that were similar to those of\nmutual information clustering techniques, with the advantage of an automated\nthresholding. On real functional magnetic resonance imaging (fMRI) datasets\nmeasuring brain activity, it identified clusters consistent with the\nestablished outcome of standard procedures. On this application, normalized\nmutual information had a highly atypical behavior, in the sense that it\nsystematically favored very large clusters. These initial experiments suggest\nthat the proposed Bayesian alternatives to mutual information are a useful new\ntool for hierarchical clustering.\n", "versions": [{"version": "v1", "created": "Wed, 21 Jan 2015 15:22:13 GMT"}, {"version": "v2", "created": "Mon, 19 Oct 2015 11:31:38 GMT"}], "update_date": "2016-08-07", "authors_parsed": [["Marrelec", "Guillaume", ""], ["Mess\u00e9", "Arnaud", ""], ["Bellec", "Pierre", ""]]}, {"id": "1501.05200", "submitter": "Mohammad Hossein Rohban", "authors": "Mohammad H. Rohban, Delaram Motamedvaziri, Venkatesh Saligrama", "title": "Minimax Optimal Sparse Signal Recovery with Poisson Statistics", "comments": "Submitted to IEEE Trans. on Signal Processing. arXiv admin note:\n  substantial text overlap with arXiv:1307.4666", "journal-ref": null, "doi": "10.1109/TSP.2016.2529588", "report-no": null, "categories": "stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We are motivated by problems that arise in a number of applications such as\nOnline Marketing and Explosives detection, where the observations are usually\nmodeled using Poisson statistics. We model each observation as a Poisson random\nvariable whose mean is a sparse linear superposition of known patterns. Unlike\nmany conventional problems observations here are not identically distributed\nsince they are associated with different sensing modalities. We analyze the\nperformance of a Maximum Likelihood (ML) decoder, which for our Poisson setting\ninvolves a non-linear optimization but yet is computationally tractable. We\nderive fundamental sample complexity bounds for sparse recovery when the\nmeasurements are contaminated with Poisson noise. In contrast to the\nleast-squares linear regression setting with Gaussian noise, we observe that in\naddition to sparsity, the scale of the parameters also fundamentally impacts\n$\\ell_2$ error in the Poisson setting. We show tightness of our upper bounds\nboth theoretically and experimentally. In particular, we derive a minimax\nmatching lower bound on the mean-squared error and show that our constrained ML\ndecoder is minimax optimal for this regime.\n", "versions": [{"version": "v1", "created": "Wed, 21 Jan 2015 15:45:01 GMT"}], "update_date": "2016-06-29", "authors_parsed": [["Rohban", "Mohammad H.", ""], ["Motamedvaziri", "Delaram", ""], ["Saligrama", "Venkatesh", ""]]}, {"id": "1501.05349", "submitter": "Yan Shang", "authors": "Yan Shang, David B. Dunson, Jing-Sheng Song", "title": "Exploiting Big Data in Logistics Risk Assessment via Bayesian\n  Nonparametrics", "comments": "35 pages, 15 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In cargo logistics, a key performance measure is transport risk, defined as\nthe deviation of the actual arrival time from the planned arrival time. Neither\nearliness nor tardiness is desirable for customer and freight forwarders. In\nthis paper, we investigate ways to assess and forecast transport risks using a\nhalf-year of air cargo data, provided by a leading forwarder on 1336 routes\nserved by 20 airlines. Interestingly, our preliminary data analysis shows a\nstrong multimodal feature in the transport risks, driven by unobserved events,\nsuch as cargo missing flights. To accommodate this feature, we introduce a\nBayesian nonparametric model -- the probit stick-breaking process (PSBP)\nmixture model -- for flexible estimation of the conditional (i.e.,\nstate-dependent) density function of transport risk. We demonstrate that using\nsimpler methods, such as OLS linear regression, can lead to misleading\ninferences. Our model provides a tool for the forwarder to offer customized\nprice and service quotes. It can also generate baseline airline performance to\nenable fair supplier evaluation. Furthermore, the method allows us to separate\nrecurrent risks from disruption risks. This is important, because hedging\nstrategies for these two kinds of risks are often drastically different.\n", "versions": [{"version": "v1", "created": "Wed, 21 Jan 2015 23:22:22 GMT"}, {"version": "v2", "created": "Thu, 21 Jul 2016 04:31:38 GMT"}], "update_date": "2016-07-22", "authors_parsed": [["Shang", "Yan", ""], ["Dunson", "David B.", ""], ["Song", "Jing-Sheng", ""]]}, {"id": "1501.05352", "submitter": "Miguel \\'A. Carreira-Perpi\\~n\\'an", "authors": "Ramin Raziperchikolaei and Miguel \\'A. Carreira-Perpi\\~n\\'an", "title": "Optimizing affinity-based binary hashing using auxiliary coordinates", "comments": "22 pages, 12 figures; added new experiments and references", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV math.OC stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In supervised binary hashing, one wants to learn a function that maps a\nhigh-dimensional feature vector to a vector of binary codes, for application to\nfast image retrieval. This typically results in a difficult optimization\nproblem, nonconvex and nonsmooth, because of the discrete variables involved.\nMuch work has simply relaxed the problem during training, solving a continuous\noptimization, and truncating the codes a posteriori. This gives reasonable\nresults but is quite suboptimal. Recent work has tried to optimize the\nobjective directly over the binary codes and achieved better results, but the\nhash function was still learned a posteriori, which remains suboptimal. We\npropose a general framework for learning hash functions using affinity-based\nloss functions that uses auxiliary coordinates. This closes the loop and\noptimizes jointly over the hash functions and the binary codes so that they\ngradually match each other. The resulting algorithm can be seen as a corrected,\niterated version of the procedure of optimizing first over the codes and then\nlearning the hash function. Compared to this, our optimization is guaranteed to\nobtain better hash functions while being not much slower, as demonstrated\nexperimentally in various supervised datasets. In addition, our framework\nfacilitates the design of optimization algorithms for arbitrary types of loss\nand hash functions.\n", "versions": [{"version": "v1", "created": "Wed, 21 Jan 2015 23:53:47 GMT"}, {"version": "v2", "created": "Fri, 5 Feb 2016 01:25:26 GMT"}], "update_date": "2016-02-08", "authors_parsed": [["Raziperchikolaei", "Ramin", ""], ["Carreira-Perpi\u00f1\u00e1n", "Miguel \u00c1.", ""]]}, {"id": "1501.05427", "submitter": "Maurizio Filippone", "authors": "Maurizio Filippone and Raphael Engler", "title": "Enabling scalable stochastic gradient-based inference for Gaussian\n  processes by employing the Unbiased LInear System SolvEr (ULISSE)", "comments": "10 pages - paper accepted at ICML 2015", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.CO stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In applications of Gaussian processes where quantification of uncertainty is\nof primary interest, it is necessary to accurately characterize the posterior\ndistribution over covariance parameters. This paper proposes an adaptation of\nthe Stochastic Gradient Langevin Dynamics algorithm to draw samples from the\nposterior distribution over covariance parameters with negligible bias and\nwithout the need to compute the marginal likelihood. In Gaussian process\nregression, this has the enormous advantage that stochastic gradients can be\ncomputed by solving linear systems only. A novel unbiased linear systems solver\nbased on parallelizable covariance matrix-vector products is developed to\naccelerate the unbiased estimation of gradients. The results demonstrate the\npossibility to enable scalable and exact (in a Monte Carlo sense)\nquantification of uncertainty in Gaussian processes without imposing any\nspecial structure on the covariance or reducing the number of input vectors.\n", "versions": [{"version": "v1", "created": "Thu, 22 Jan 2015 08:48:42 GMT"}, {"version": "v2", "created": "Fri, 6 Feb 2015 07:59:49 GMT"}, {"version": "v3", "created": "Mon, 18 May 2015 09:21:57 GMT"}, {"version": "v4", "created": "Thu, 3 Sep 2015 06:43:32 GMT"}], "update_date": "2015-09-04", "authors_parsed": [["Filippone", "Maurizio", ""], ["Engler", "Raphael", ""]]}, {"id": "1501.05590", "submitter": "Panagiotis Traganitis", "authors": "Panagiotis A. Traganitis, Konstantinos Slavakis, Georgios B. Giannakis", "title": "Sketch and Validate for Big Data Clustering", "comments": "The present paper will appear on Signal Processing for Big Data\n  special issue (June 2015) of the IEEE Journal of Selected Topics in Signal\n  Processing", "journal-ref": null, "doi": "10.1109/JSTSP.2015.2396477", "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In response to the need for learning tools tuned to big data analytics, the\npresent paper introduces a framework for efficient clustering of huge sets of\n(possibly high-dimensional) data. Building on random sampling and consensus\n(RANSAC) ideas pursued earlier in a different (computer vision) context for\nrobust regression, a suite of novel dimensionality and set-reduction algorithms\nis developed. The advocated sketch-and-validate (SkeVa) family includes two\nalgorithms that rely on K-means clustering per iteration on reduced number of\ndimensions and/or feature vectors: The first operates in a batch fashion, while\nthe second sequential one offers computational efficiency and suitability with\nstreaming modes of operation. For clustering even nonlinearly separable\nvectors, the SkeVa family offers also a member based on user-selected kernel\nfunctions. Further trading off performance for reduced complexity, a fourth\nmember of the SkeVa family is based on a divergence criterion for selecting\nproper minimal subsets of feature variables and vectors, thus bypassing the\nneed for K-means clustering per iteration. Extensive numerical tests on\nsynthetic and real data sets highlight the potential of the proposed\nalgorithms, and demonstrate their competitive performance relative to\nstate-of-the-art random projection alternatives.\n", "versions": [{"version": "v1", "created": "Thu, 22 Jan 2015 18:16:30 GMT"}], "update_date": "2016-11-17", "authors_parsed": [["Traganitis", "Panagiotis A.", ""], ["Slavakis", "Konstantinos", ""], ["Giannakis", "Georgios B.", ""]]}, {"id": "1501.05624", "submitter": "John Paisley", "authors": "San Gultekin and John Paisley", "title": "A Collaborative Kalman Filter for Time-Evolving Dyadic Processes", "comments": "Appeared at 2014 IEEE International Conference on Data Mining (ICDM)", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present the collaborative Kalman filter (CKF), a dynamic model for\ncollaborative filtering and related factorization models. Using the matrix\nfactorization approach to collaborative filtering, the CKF accounts for time\nevolution by modeling each low-dimensional latent embedding as a\nmultidimensional Brownian motion. Each observation is a random variable whose\ndistribution is parameterized by the dot product of the relevant Brownian\nmotions at that moment in time. This is naturally interpreted as a Kalman\nfilter with multiple interacting state space vectors. We also present a method\nfor learning a dynamically evolving drift parameter for each location by\nmodeling it as a geometric Brownian motion. We handle posterior intractability\nvia a mean-field variational approximation, which also preserves tractability\nfor downstream calculations in a manner similar to the Kalman filter. We\nevaluate the model on several large datasets, providing quantitative evaluation\non the 10 million Movielens and 100 million Netflix datasets and qualitative\nevaluation on a set of 39 million stock returns divided across roughly 6,500\ncompanies from the years 1962-2014.\n", "versions": [{"version": "v1", "created": "Thu, 22 Jan 2015 20:24:32 GMT"}], "update_date": "2015-01-23", "authors_parsed": [["Gultekin", "San", ""], ["Paisley", "John", ""]]}, {"id": "1501.05677", "submitter": "David Tolpin", "authors": "David Tolpin, Jan Willem van de Meent, Brooks Paige, Frank Wood", "title": "Output-Sensitive Adaptive Metropolis-Hastings for Probabilistic Programs", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We introduce an adaptive output-sensitive Metropolis-Hastings algorithm for\nprobabilistic models expressed as programs, Adaptive Lightweight\nMetropolis-Hastings (AdLMH). The algorithm extends Lightweight\nMetropolis-Hastings (LMH) by adjusting the probabilities of proposing random\nvariables for modification to improve convergence of the program output. We\nshow that AdLMH converges to the correct equilibrium distribution and compare\nconvergence of AdLMH to that of LMH on several test problems to highlight\ndifferent aspects of the adaptation scheme. We observe consistent improvement\nin convergence on the test problems.\n", "versions": [{"version": "v1", "created": "Thu, 22 Jan 2015 22:42:36 GMT"}, {"version": "v2", "created": "Tue, 5 May 2015 20:46:01 GMT"}], "update_date": "2015-05-07", "authors_parsed": [["Tolpin", "David", ""], ["van de Meent", "Jan Willem", ""], ["Paige", "Brooks", ""], ["Wood", "Frank", ""]]}, {"id": "1501.05684", "submitter": "Paul Honeine", "authors": "Paul Honeine, Fei Zhu", "title": "Bi-Objective Nonnegative Matrix Factorization: Linear Versus\n  Kernel-Based Models", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.CV cs.LG math.OC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Nonnegative matrix factorization (NMF) is a powerful class of feature\nextraction techniques that has been successfully applied in many fields, namely\nin signal and image processing. Current NMF techniques have been limited to a\nsingle-objective problem in either its linear or nonlinear kernel-based\nformulation. In this paper, we propose to revisit the NMF as a multi-objective\nproblem, in particular a bi-objective one, where the objective functions\ndefined in both input and feature spaces are taken into account. By taking the\nadvantage of the sum-weighted method from the literature of multi-objective\noptimization, the proposed bi-objective NMF determines a set of nondominated,\nPareto optimal, solutions instead of a single optimal decomposition. Moreover,\nthe corresponding Pareto front is studied and approximated. Experimental\nresults on unmixing real hyperspectral images confirm the efficiency of the\nproposed bi-objective NMF compared with the state-of-the-art methods.\n", "versions": [{"version": "v1", "created": "Thu, 22 Jan 2015 22:59:47 GMT"}], "update_date": "2015-01-26", "authors_parsed": [["Honeine", "Paul", ""], ["Zhu", "Fei", ""]]}, {"id": "1501.05740", "submitter": "Martin Sundin", "authors": "Martin Sundin, Cristian R. Rojas, Magnus Jansson and Saikat Chatterjee", "title": "Bayesian Learning for Low-Rank matrix reconstruction", "comments": "Submitted to IEEE Transactions on Signal Processing", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG cs.NA", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We develop latent variable models for Bayesian learning based low-rank matrix\ncompletion and reconstruction from linear measurements. For under-determined\nsystems, the developed methods are shown to reconstruct low-rank matrices when\nneither the rank nor the noise power is known a-priori. We derive relations\nbetween the latent variable models and several low-rank promoting penalty\nfunctions. The relations justify the use of Kronecker structured covariance\nmatrices in a Gaussian based prior. In the methods, we use evidence\napproximation and expectation-maximization to learn the model parameters. The\nperformance of the methods is evaluated through extensive numerical\nsimulations.\n", "versions": [{"version": "v1", "created": "Fri, 23 Jan 2015 08:52:35 GMT"}], "update_date": "2015-01-26", "authors_parsed": [["Sundin", "Martin", ""], ["Rojas", "Cristian R.", ""], ["Jansson", "Magnus", ""], ["Chatterjee", "Saikat", ""]]}, {"id": "1501.05892", "submitter": "Ramji Venkataramanan", "authors": "Cynthia Rush, Adam Greig, Ramji Venkataramanan", "title": "Capacity-achieving Sparse Superposition Codes via Approximate Message\n  Passing Decoding", "comments": "25 pages, 4 figures. IEEE Transactions on Information Theory", "journal-ref": "IEEE Transactions on Information Theory, vol. 63, no.3, pp.\n  1476-1500, March 2017", "doi": "10.1109/TIT.2017.2649460", "report-no": null, "categories": "cs.IT math.IT stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Sparse superposition codes were recently introduced by Barron and Joseph for\nreliable communication over the AWGN channel at rates approaching the channel\ncapacity. The codebook is defined in terms of a Gaussian design matrix, and\ncodewords are sparse linear combinations of columns of the matrix. In this\npaper, we propose an approximate message passing decoder for sparse\nsuperposition codes, whose decoding complexity scales linearly with the size of\nthe design matrix. The performance of the decoder is rigorously analyzed and it\nis shown to asymptotically achieve the AWGN capacity with an appropriate power\nallocation. Simulation results are provided to demonstrate the performance of\nthe decoder at finite blocklengths. We introduce a power allocation scheme to\nimprove the empirical performance, and demonstrate how the decoding complexity\ncan be significantly reduced by using Hadamard design matrices.\n", "versions": [{"version": "v1", "created": "Fri, 23 Jan 2015 18:09:25 GMT"}, {"version": "v2", "created": "Sun, 12 Apr 2015 05:43:15 GMT"}, {"version": "v3", "created": "Tue, 2 Aug 2016 13:13:26 GMT"}, {"version": "v4", "created": "Sun, 18 Dec 2016 19:39:29 GMT"}, {"version": "v5", "created": "Sat, 11 Mar 2017 11:09:16 GMT"}], "update_date": "2017-03-14", "authors_parsed": [["Rush", "Cynthia", ""], ["Greig", "Adam", ""], ["Venkataramanan", "Ramji", ""]]}, {"id": "1501.06060", "submitter": "Yi Wang", "authors": "Yi Wang", "title": "Consistency Analysis of Nearest Subspace Classifier", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The Nearest subspace classifier (NSS) finds an estimation of the underlying\nsubspace within each class and assigns data points to the class that\ncorresponds to its nearest subspace. This paper mainly studies how well NSS can\nbe generalized to new samples. It is proved that NSS is strongly consistent\nunder certain assumptions. For completeness, NSS is evaluated through\nexperiments on various simulated and real data sets, in comparison with some\nother linear model based classifiers. It is also shown that NSS can obtain\neffective classification results and is very efficient, especially for large\nscale data sets.\n", "versions": [{"version": "v1", "created": "Sat, 24 Jan 2015 16:41:55 GMT"}], "update_date": "2015-01-27", "authors_parsed": [["Wang", "Yi", ""]]}, {"id": "1501.06066", "submitter": "Boxiang Wang", "authors": "Boxiang Wang and Hui Zou", "title": "Sparse Distance Weighted Discrimination", "comments": "16 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Distance weighted discrimination (DWD) was originally proposed to handle the\ndata piling issue in the support vector machine. In this paper, we consider the\nsparse penalized DWD for high-dimensional classification. The state-of-the-art\nalgorithm for solving the standard DWD is based on second-order cone\nprogramming, however such an algorithm does not work well for the sparse\npenalized DWD with high-dimensional data. In order to overcome the challenging\ncomputation difficulty, we develop a very efficient algorithm to compute the\nsolution path of the sparse DWD at a given fine grid of regularization\nparameters. We implement the algorithm in a publicly available R package sdwd.\nWe conduct extensive numerical experiments to demonstrate the computational\nefficiency and classification performance of our method.\n", "versions": [{"version": "v1", "created": "Sat, 24 Jan 2015 17:34:36 GMT"}], "update_date": "2015-01-27", "authors_parsed": [["Wang", "Boxiang", ""], ["Zou", "Hui", ""]]}, {"id": "1501.06103", "submitter": "Arthur Gretton", "authors": "Arthur Gretton", "title": "A simpler condition for consistency of a kernel independence test", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A statistical test of independence may be constructed using the\nHilbert-Schmidt Independence Criterion (HSIC) as a test statistic. The HSIC is\ndefined as the distance between the embedding of the joint distribution, and\nthe embedding of the product of the marginals, in a Reproducing Kernel Hilbert\nSpace (RKHS). It has previously been shown that when the kernel used in\ndefining the joint embedding is characteristic (that is, the embedding of the\njoint distribution to the feature space is injective), then the HSIC-based test\nis consistent. In particular, it is sufficient for the product of kernels on\nthe individual domains to be characteristic on the joint domain. In this note,\nit is established via a result of Lyons (2013) that HSIC-based independence\ntests are consistent when kernels on the marginals are characteristic on their\nrespective domains, even when the product of kernels is not characteristic on\nthe joint domain.\n", "versions": [{"version": "v1", "created": "Sun, 25 Jan 2015 01:45:06 GMT"}], "update_date": "2015-01-27", "authors_parsed": [["Gretton", "Arthur", ""]]}, {"id": "1501.06116", "submitter": "Ernest Fokoue", "authors": "Ernest Fokou\\'e", "title": "Prediction Error Reduction Function as a Variable Importance Score", "comments": "7 pages, 12 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper introduces and develops a novel variable importance score function\nin the context of ensemble learning and demonstrates its appeal both\ntheoretically and empirically. Our proposed score function is simple and more\nstraightforward than its counterpart proposed in the context of random forest,\nand by avoiding permutations, it is by design computationally more efficient\nthan the random forest variable importance function. Just like the random\nforest variable importance function, our score handles both regression and\nclassification seamlessly. One of the distinct advantage of our proposed score\nis the fact that it offers a natural cut off at zero, with all the positive\nscores indicating importance and significance, while the negative scores are\ndeemed indications of insignificance. An extra advantage of our proposed score\nlies in the fact it works very well beyond ensemble of trees and can seamlessly\nbe used with any base learners in the random subspace learning context. Our\nexamples, both simulated and real, demonstrate that our proposed score does\ncompete mostly favorably with the random forest score.\n", "versions": [{"version": "v1", "created": "Sun, 25 Jan 2015 05:12:59 GMT"}], "update_date": "2015-01-27", "authors_parsed": [["Fokou\u00e9", "Ernest", ""]]}, {"id": "1501.06195", "submitter": "Yun Yang", "authors": "Yun Yang, Mert Pilanci, Martin J. Wainwright", "title": "Randomized sketches for kernels: Fast and optimal non-parametric\n  regression", "comments": "27 pages, 3 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.DS cs.LG stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Kernel ridge regression (KRR) is a standard method for performing\nnon-parametric regression over reproducing kernel Hilbert spaces. Given $n$\nsamples, the time and space complexity of computing the KRR estimate scale as\n$\\mathcal{O}(n^3)$ and $\\mathcal{O}(n^2)$ respectively, and so is prohibitive\nin many cases. We propose approximations of KRR based on $m$-dimensional\nrandomized sketches of the kernel matrix, and study how small the projection\ndimension $m$ can be chosen while still preserving minimax optimality of the\napproximate KRR estimate. For various classes of randomized sketches, including\nthose based on Gaussian and randomized Hadamard matrices, we prove that it\nsuffices to choose the sketch dimension $m$ proportional to the statistical\ndimension (modulo logarithmic factors). Thus, we obtain fast and minimax\noptimal approximations to the KRR estimate for non-parametric regression.\n", "versions": [{"version": "v1", "created": "Sun, 25 Jan 2015 19:06:59 GMT"}], "update_date": "2015-01-27", "authors_parsed": [["Yang", "Yun", ""], ["Pilanci", "Mert", ""], ["Wainwright", "Martin J.", ""]]}, {"id": "1501.06218", "submitter": "Mingyuan Zhou", "authors": "Mingyuan Zhou", "title": "Infinite Edge Partition Models for Overlapping Community Detection and\n  Link Prediction", "comments": "Appeared in Artificial Intelligence and Statistics (AISTATS), May\n  2015. 9 pages + 2 page appendix. The paper is updated to fix a typo in the\n  equation right after (25) in the Appendix", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.SI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A hierarchical gamma process infinite edge partition model is proposed to\nfactorize the binary adjacency matrix of an unweighted undirected relational\nnetwork under a Bernoulli-Poisson link. The model describes both homophily and\nstochastic equivalence, and is scalable to big sparse networks by focusing its\ncomputation on pairs of linked nodes. It can not only discover overlapping\ncommunities and inter-community interactions, but also predict missing edges. A\nsimplified version omitting inter-community interactions is also provided and\nwe reveal its interesting connections to existing models. The number of\ncommunities is automatically inferred in a nonparametric Bayesian manner, and\nefficient inference via Gibbs sampling is derived using novel data augmentation\ntechniques. Experimental results on four real networks demonstrate the models'\nscalability and state-of-the-art performance.\n", "versions": [{"version": "v1", "created": "Sun, 25 Jan 2015 23:14:59 GMT"}, {"version": "v2", "created": "Wed, 30 Dec 2015 15:56:40 GMT"}], "update_date": "2015-12-31", "authors_parsed": [["Zhou", "Mingyuan", ""]]}, {"id": "1501.06225", "submitter": "Shahin Shahrampour", "authors": "Ali Jadbabaie, Alexander Rakhlin, Shahin Shahrampour and Karthik\n  Sridharan", "title": "Online Optimization : Competing with Dynamic Comparators", "comments": "23 pages, To appear in International Conference on Artificial\n  Intelligence and Statistics (AISTATS) 2015", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG math.OC stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recent literature on online learning has focused on developing adaptive\nalgorithms that take advantage of a regularity of the sequence of observations,\nyet retain worst-case performance guarantees. A complementary direction is to\ndevelop prediction methods that perform well against complex benchmarks. In\nthis paper, we address these two directions together. We present a fully\nadaptive method that competes with dynamic benchmarks in which regret guarantee\nscales with regularity of the sequence of cost functions and comparators.\nNotably, the regret bound adapts to the smaller complexity measure in the\nproblem environment. Finally, we apply our results to drifting zero-sum,\ntwo-player games where both players achieve no regret guarantees against best\nsequences of actions in hindsight.\n", "versions": [{"version": "v1", "created": "Mon, 26 Jan 2015 00:40:08 GMT"}], "update_date": "2015-01-27", "authors_parsed": [["Jadbabaie", "Ali", ""], ["Rakhlin", "Alexander", ""], ["Shahrampour", "Shahin", ""], ["Sridharan", "Karthik", ""]]}, {"id": "1501.06241", "submitter": "Yao Xie", "authors": "Ruiyang Song, Yao Xie, Sebastian Pokutta", "title": "Sequential Sensing with Model Mismatch", "comments": "Submitted to IEEE for publication", "journal-ref": null, "doi": "10.1109/ISIT.2015.7282736", "report-no": null, "categories": "stat.ML cs.IT cs.LG math.IT math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We characterize the performance of sequential information guided sensing,\nInfo-Greedy Sensing, when there is a mismatch between the true signal model and\nthe assumed model, which may be a sample estimate. In particular, we consider a\nsetup where the signal is low-rank Gaussian and the measurements are taken in\nthe directions of eigenvectors of the covariance matrix in a decreasing order\nof eigenvalues. We establish a set of performance bounds when a mismatched\ncovariance matrix is used, in terms of the gap of signal posterior entropy, as\nwell as the additional amount of power required to achieve the same signal\nrecovery precision. Based on this, we further study how to choose an\ninitialization for Info-Greedy Sensing using the sample covariance matrix, or\nusing an efficient covariance sketching scheme.\n", "versions": [{"version": "v1", "created": "Mon, 26 Jan 2015 02:51:13 GMT"}, {"version": "v2", "created": "Mon, 16 Mar 2015 22:38:33 GMT"}], "update_date": "2016-11-18", "authors_parsed": [["Song", "Ruiyang", ""], ["Xie", "Yao", ""], ["Pokutta", "Sebastian", ""]]}, {"id": "1501.06243", "submitter": "Yang Cao", "authors": "Yang Cao and Yao Xie", "title": "Poisson Matrix Completion", "comments": "Submitted to IEEE for publication", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We extend the theory of matrix completion to the case where we make Poisson\nobservations for a subset of entries of a low-rank matrix. We consider the\n(now) usual matrix recovery formulation through maximum likelihood with proper\nconstraints on the matrix $M$, and establish theoretical upper and lower bounds\non the recovery error. Our bounds are nearly optimal up to a factor on the\norder of $\\mathcal{O}(\\log(d_1 d_2))$. These bounds are obtained by adapting\nthe arguments used for one-bit matrix completion \\cite{davenport20121}\n(although these two problems are different in nature) and the adaptation\nrequires new techniques exploiting properties of the Poisson likelihood\nfunction and tackling the difficulties posed by the locally sub-Gaussian\ncharacteristic of the Poisson distribution. Our results highlight a few\nimportant distinctions of Poisson matrix completion compared to the prior work\nin matrix completion including having to impose a minimum signal-to-noise\nrequirement on each observed entry. We also develop an efficient iterative\nalgorithm and demonstrate its good performance in recovering solar flare\nimages.\n", "versions": [{"version": "v1", "created": "Mon, 26 Jan 2015 03:02:51 GMT"}, {"version": "v2", "created": "Wed, 28 Jan 2015 05:14:30 GMT"}, {"version": "v3", "created": "Wed, 18 Feb 2015 16:52:34 GMT"}, {"version": "v4", "created": "Thu, 19 Feb 2015 04:04:57 GMT"}, {"version": "v5", "created": "Mon, 23 Mar 2015 16:24:58 GMT"}, {"version": "v6", "created": "Wed, 25 Mar 2015 18:03:15 GMT"}], "update_date": "2015-03-26", "authors_parsed": [["Cao", "Yang", ""], ["Xie", "Yao", ""]]}, {"id": "1501.06450", "submitter": "Teng Qiu", "authors": "Teng Qiu, Yongjie Li", "title": "IT-map: an Effective Nonlinear Dimensionality Reduction Method for\n  Interactive Clustering", "comments": "13 pages, 6 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.CV cs.LG", "license": "http://creativecommons.org/licenses/by-nc-sa/3.0/", "abstract": "  Scientists in many fields have the common and basic need of dimensionality\nreduction: visualizing the underlying structure of the massive multivariate\ndata in a low-dimensional space. However, many dimensionality reduction methods\nconfront the so-called \"crowding problem\" that clusters tend to overlap with\neach other in the embedding. Previously, researchers expect to avoid that\nproblem and seek to make clusters maximally separated in the embedding.\nHowever, the proposed in-tree (IT) based method, called IT-map, allows clusters\nin the embedding to be locally overlapped, while seeking to make them\ndistinguishable by some small yet key parts. IT-map provides a simple,\neffective and novel solution to cluster-preserving mapping, which makes it\npossible to cluster the original data points interactively and thus should be\nof general meaning in science and engineering.\n", "versions": [{"version": "v1", "created": "Mon, 26 Jan 2015 15:37:22 GMT"}, {"version": "v2", "created": "Wed, 18 Mar 2015 14:48:42 GMT"}], "update_date": "2015-03-19", "authors_parsed": [["Qiu", "Teng", ""], ["Li", "Yongjie", ""]]}, {"id": "1501.06521", "submitter": "Ankur Moitra", "authors": "Boaz Barak and Ankur Moitra", "title": "Noisy Tensor Completion via the Sum-of-Squares Hierarchy", "comments": "24 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.DS stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In the noisy tensor completion problem we observe $m$ entries (whose location\nis chosen uniformly at random) from an unknown $n_1 \\times n_2 \\times n_3$\ntensor $T$. We assume that $T$ is entry-wise close to being rank $r$. Our goal\nis to fill in its missing entries using as few observations as possible. Let $n\n= \\max(n_1, n_2, n_3)$. We show that if $m = n^{3/2} r$ then there is a\npolynomial time algorithm based on the sixth level of the sum-of-squares\nhierarchy for completing it. Our estimate agrees with almost all of $T$'s\nentries almost exactly and works even when our observations are corrupted by\nnoise. This is also the first algorithm for tensor completion that works in the\novercomplete case when $r > n$, and in fact it works all the way up to $r =\nn^{3/2-\\epsilon}$.\n  Our proofs are short and simple and are based on establishing a new\nconnection between noisy tensor completion (through the language of Rademacher\ncomplexity) and the task of refuting random constant satisfaction problems.\nThis connection seems to have gone unnoticed even in the context of matrix\ncompletion. Furthermore, we use this connection to show matching lower bounds.\nOur main technical result is in characterizing the Rademacher complexity of the\nsequence of norms that arise in the sum-of-squares relaxations to the tensor\nnuclear norm. These results point to an interesting new direction: Can we\nexplore computational vs. sample complexity tradeoffs through the\nsum-of-squares hierarchy?\n", "versions": [{"version": "v1", "created": "Mon, 26 Jan 2015 18:48:55 GMT"}, {"version": "v2", "created": "Thu, 2 Apr 2015 19:54:52 GMT"}, {"version": "v3", "created": "Thu, 18 Feb 2016 16:37:39 GMT"}], "update_date": "2016-02-19", "authors_parsed": [["Barak", "Boaz", ""], ["Moitra", "Ankur", ""]]}, {"id": "1501.06598", "submitter": "Alexander Rakhlin", "authors": "Alexander Rakhlin and Karthik Sridharan", "title": "Online Nonparametric Regression with General Loss Functions", "comments": "arXiv admin note: substantial text overlap with arXiv:1402.2594", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.IT cs.LG math.IT", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper establishes minimax rates for online regression with arbitrary\nclasses of functions and general losses. We show that below a certain threshold\nfor the complexity of the function class, the minimax rates depend on both the\ncurvature of the loss function and the sequential complexities of the class.\nAbove this threshold, the curvature of the loss does not affect the rates.\nFurthermore, for the case of square loss, our results point to the interesting\nphenomenon: whenever sequential and i.i.d. empirical entropies match, the rates\nfor statistical and online learning are the same.\n  In addition to the study of minimax regret, we derive a generic forecaster\nthat enjoys the established optimal rates. We also provide a recipe for\ndesigning online prediction algorithms that can be computationally efficient\nfor certain problems. We illustrate the techniques by deriving existing and new\nforecasters for the case of finite experts and for online linear regression.\n", "versions": [{"version": "v1", "created": "Mon, 26 Jan 2015 21:52:41 GMT"}], "update_date": "2015-01-28", "authors_parsed": [["Rakhlin", "Alexander", ""], ["Sridharan", "Karthik", ""]]}, {"id": "1501.06727", "submitter": "Jose M. Pe\\~na", "authors": "Jose M. Pe\\~na", "title": "Factorization, Inference and Parameter Learning in Discrete AMP Chain\n  Graphs", "comments": null, "journal-ref": "Proceedings of the 13th European Conference on Symbolic and\n  Quantitative Approaches to Reasoning under Uncertainty (ECSQARU 2015),\n  Lecture Notes in Artificial Intelligence 9161, 335-345", "doi": null, "report-no": null, "categories": "stat.ML cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We address some computational issues that may hinder the use of AMP chain\ngraphs in practice. Specifically, we show how a discrete probability\ndistribution that satisfies all the independencies represented by an AMP chain\ngraph factorizes according to it. We show how this factorization makes it\npossible to perform inference and parameter learning efficiently, by adapting\nexisting algorithms for Markov and Bayesian networks. Finally, we turn our\nattention to another issue that may hinder the use of AMP CGs, namely the lack\nof an intuitive interpretation of their edges. We provide one such\ninterpretation.\n", "versions": [{"version": "v1", "created": "Tue, 27 Jan 2015 10:28:19 GMT"}, {"version": "v2", "created": "Wed, 18 Nov 2015 15:10:19 GMT"}], "update_date": "2015-11-19", "authors_parsed": [["Pe\u00f1a", "Jose M.", ""]]}, {"id": "1501.06769", "submitter": "Jan-Willem van de Meent", "authors": "Jan-Willem van de Meent and Hongseok Yang and Vikash Mansinghka and\n  Frank Wood", "title": "Particle Gibbs with Ancestor Sampling for Probabilistic Programs", "comments": "9 pages, 2 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.AI cs.PL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Particle Markov chain Monte Carlo techniques rank among current\nstate-of-the-art methods for probabilistic program inference. A drawback of\nthese techniques is that they rely on importance resampling, which results in\ndegenerate particle trajectories and a low effective sample size for variables\nsampled early in a program. We here develop a formalism to adapt ancestor\nresampling, a technique that mitigates particle degeneracy, to the\nprobabilistic programming setting. We present empirical results that\ndemonstrate nontrivial performance gains.\n", "versions": [{"version": "v1", "created": "Tue, 27 Jan 2015 14:16:59 GMT"}, {"version": "v2", "created": "Wed, 28 Jan 2015 18:26:55 GMT"}, {"version": "v3", "created": "Mon, 2 Feb 2015 19:42:52 GMT"}, {"version": "v4", "created": "Wed, 4 Feb 2015 16:12:05 GMT"}, {"version": "v5", "created": "Mon, 9 Feb 2015 23:49:26 GMT"}], "update_date": "2015-02-11", "authors_parsed": [["van de Meent", "Jan-Willem", ""], ["Yang", "Hongseok", ""], ["Mansinghka", "Vikash", ""], ["Wood", "Frank", ""]]}, {"id": "1501.06794", "submitter": "Bernhard Sch\\\"olkopf", "authors": "Bernhard Sch\\\"olkopf, Krikamol Muandet, Kenji Fukumizu, Jonas Peters", "title": "Computing Functions of Random Variables via Reproducing Kernel Hilbert\n  Space Representations", "comments": null, "journal-ref": "Statistics and Computing 25:755-766 (2015)", "doi": null, "report-no": null, "categories": "stat.ML cs.DS cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We describe a method to perform functional operations on probability\ndistributions of random variables. The method uses reproducing kernel Hilbert\nspace representations of probability distributions, and it is applicable to all\noperations which can be applied to points drawn from the respective\ndistributions. We refer to our approach as {\\em kernel probabilistic\nprogramming}. We illustrate it on synthetic data, and show how it can be used\nfor nonparametric structural equation models, with an application to causal\ninference.\n", "versions": [{"version": "v1", "created": "Tue, 27 Jan 2015 15:36:22 GMT"}], "update_date": "2016-09-14", "authors_parsed": [["Sch\u00f6lkopf", "Bernhard", ""], ["Muandet", "Krikamol", ""], ["Fukumizu", "Kenji", ""], ["Peters", "Jonas", ""]]}, {"id": "1501.06929", "submitter": "Jesus Fernandez-Bes", "authors": "Jesus Fernandez-Bes, V\\'ictor Elvira, Steven Van Vaerenbergh", "title": "A Probabilistic Least-Mean-Squares Filter", "comments": null, "journal-ref": null, "doi": "10.1109/ICASSP.2015.7178361", "report-no": null, "categories": "stat.ML cs.SY stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We introduce a probabilistic approach to the LMS filter. By means of an\nefficient approximation, this approach provides an adaptable step-size LMS\nalgorithm together with a measure of uncertainty about the estimation. In\naddition, the proposed approximation preserves the linear complexity of the\nstandard LMS. Numerical results show the improved performance of the algorithm\nwith respect to standard LMS and state-of-the-art algorithms with similar\ncomplexity. The goal of this work, therefore, is to open the door to bring some\nmore Bayesian machine learning techniques to adaptive filtering.\n", "versions": [{"version": "v1", "created": "Tue, 27 Jan 2015 21:23:22 GMT"}], "update_date": "2016-04-11", "authors_parsed": [["Fernandez-Bes", "Jesus", ""], ["Elvira", "V\u00edctor", ""], ["Van Vaerenbergh", "Steven", ""]]}, {"id": "1501.07196", "submitter": "John Ehrlinger", "authors": "John Ehrlinger", "title": "ggRandomForests: Visually Exploring a Random Forest for Regression", "comments": "30 page, 16 figures. R Package vignette for ggRandomForests package\n  (http://cran.r-project.org/package=ggRandomForests) [Document Version 2]", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.CO stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Random Forests [Breiman:2001] (RF) are a fully non-parametric statistical\nmethod requiring no distributional assumptions on covariate relation to the\nresponse. RF are a robust, nonlinear technique that optimizes predictive\naccuracy by fitting an ensemble of trees to stabilize model estimates. The\nrandomForestSRC package (http://cran.r-project.org/package=randomForestSRC) is\na unified treatment of Breiman's random forests for survival, regression and\nclassification problems. Predictive accuracy make RF an attractive alternative\nto parametric models, though complexity and interpretability of the forest\nhinder wider application of the method. We introduce the ggRandomForests\npackage (http://cran.r-project.org/package=ggRandomForests), for visually\nunderstand random forest models grown in R with the randomForestSRC package.\n  The vignette is a tutorial for using the ggRandomForests package with the\nrandomForestSRC package for building and post-processing a regression random\nforest. In this tutorial, we explore a random forest model for the Boston\nHousing Data, available in the MASS package. We grow a random forest for\nregression and demonstrate how ggRandomForests can be used when determining\nvariable associations, interactions and how the response depends on predictive\nvariables within the model. The tutorial demonstrates the design and usage of\nmany of ggRandomForests functions and features how to modify and customize the\nresulting ggplot2 graphic objects along the way.\n  A development version of the ggRandomForests package is available on Github.\nWe invite comments, feature requests and bug reports for this package at\n(https://github.com/ehrlinger/ggRandomForests).\n", "versions": [{"version": "v1", "created": "Wed, 28 Jan 2015 17:03:26 GMT"}, {"version": "v2", "created": "Fri, 13 Feb 2015 21:34:05 GMT"}], "update_date": "2016-12-30", "authors_parsed": [["Ehrlinger", "John", ""]]}, {"id": "1501.07227", "submitter": "Robert Murphy", "authors": "Robert A. Murphy", "title": "A Neural Network Anomaly Detector Using the Random Cluster Model", "comments": "These writings are part of a longer writing which has been submitted\n  for publication. I plan to replace this writing (and the other 2 writings)\n  with the single writing that has been submitted for publication. The other\n  writings to be withdrawn are 1503.03488 and 1412.4178", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.NE stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The random cluster model is used to define an upper bound on a distance\nmeasure as a function of the number of data points to be classified and the\nexpected value of the number of classes to form in a hybrid K-means and\nregression classification methodology, with the intent of detecting anomalies.\nConditions are given for the identification of classes which contain anomalies\nand individual anomalies within identified classes. A neural network model\ndescribes the decision region-separating surface for offline storage and recall\nin any new anomaly detection.\n", "versions": [{"version": "v1", "created": "Wed, 28 Jan 2015 18:42:42 GMT"}, {"version": "v2", "created": "Sat, 7 Mar 2015 22:49:15 GMT"}, {"version": "v3", "created": "Wed, 11 Mar 2015 02:35:37 GMT"}, {"version": "v4", "created": "Sat, 14 Mar 2015 20:49:32 GMT"}, {"version": "v5", "created": "Wed, 10 Feb 2016 22:29:26 GMT"}], "update_date": "2016-02-12", "authors_parsed": [["Murphy", "Robert A.", ""]]}, {"id": "1501.07320", "submitter": "Arun Tejasvi Chaganty", "authors": "Volodymyr Kuleshov and Arun Tejasvi Chaganty and Percy Liang", "title": "Tensor Factorization via Matrix Factorization", "comments": "Appearing in Proceedings of the 18th International Conference on\n  Artificial Intelligence and Statistics (AISTATS) 2015, San Diego, CA, USA.\n  JMLR: W&CP volume 38", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Tensor factorization arises in many machine learning applications, such\nknowledge base modeling and parameter estimation in latent variable models.\nHowever, numerical methods for tensor factorization have not reached the level\nof maturity of matrix factorization methods. In this paper, we propose a new\nmethod for CP tensor factorization that uses random projections to reduce the\nproblem to simultaneous matrix diagonalization. Our method is conceptually\nsimple and also applies to non-orthogonal and asymmetric tensors of arbitrary\norder. We prove that a small number random projections essentially preserves\nthe spectral information in the tensor, allowing us to remove the dependence on\nthe eigengap that plagued earlier tensor-to-matrix reductions. Experimentally,\nour method outperforms existing tensor factorization methods on both simulated\ndata and two real datasets.\n", "versions": [{"version": "v1", "created": "Thu, 29 Jan 2015 01:01:34 GMT"}, {"version": "v2", "created": "Mon, 18 May 2015 21:53:34 GMT"}], "update_date": "2015-05-20", "authors_parsed": [["Kuleshov", "Volodymyr", ""], ["Chaganty", "Arun Tejasvi", ""], ["Liang", "Percy", ""]]}, {"id": "1501.07340", "submitter": "Alexander Rakhlin", "authors": "Alexander Rakhlin and Karthik Sridharan", "title": "Sequential Probability Assignment with Binary Alphabets and Large\n  Classes of Experts", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IT cs.LG math.IT stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We analyze the problem of sequential probability assignment for binary\noutcomes with side information and logarithmic loss, where regret---or,\nredundancy---is measured with respect to a (possibly infinite) class of\nexperts. We provide upper and lower bounds for minimax regret in terms of\nsequential complexities of the class. These complexities were recently shown to\ngive matching (up to logarithmic factors) upper and lower bounds for sequential\nprediction with general convex Lipschitz loss functions (Rakhlin and Sridharan,\n2015). To deal with unbounded gradients of the logarithmic loss, we present a\nnew analysis that employs a sequential chaining technique with a Bernstein-type\nbound. The introduced complexities are intrinsic to the problem of sequential\nprobability assignment, as illustrated by our lower bound.\n  We also consider an example of a large class of experts parametrized by\nvectors in a high-dimensional Euclidean ball (or a Hilbert ball). The typical\ndiscretization approach fails, while our techniques give a non-trivial bound.\nFor this problem we also present an algorithm based on regularization with a\nself-concordant barrier. This algorithm is of an independent interest, as it\nrequires a bound on the function values rather than gradients.\n", "versions": [{"version": "v1", "created": "Thu, 29 Jan 2015 04:02:39 GMT"}], "update_date": "2015-01-30", "authors_parsed": [["Rakhlin", "Alexander", ""], ["Sridharan", "Karthik", ""]]}, {"id": "1501.07422", "submitter": "Kohta Ishikawa", "authors": "Kohta Ishikawa, Ikuro Sato, Mitsuru Ambai", "title": "Pairwise Rotation Hashing for High-dimensional Features", "comments": "16 pages, 8 figures, wrote at Mar 2014", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Binary Hashing is widely used for effective approximate nearest neighbors\nsearch. Even though various binary hashing methods have been proposed, very few\nmethods are feasible for extremely high-dimensional features often used in\nvisual tasks today. We propose a novel highly sparse linear hashing method\nbased on pairwise rotations. The encoding cost of the proposed algorithm is\n$\\mathrm{O}(n \\log n)$ for n-dimensional features, whereas that of the existing\nstate-of-the-art method is typically $\\mathrm{O}(n^2)$. The proposed method is\nalso remarkably faster in the learning phase. Along with the efficiency, the\nretrieval accuracy is comparable to or slightly outperforming the\nstate-of-the-art. Pairwise rotations used in our method are formulated from an\nanalytical study of the trade-off relationship between quantization error and\nentropy of binary codes. Although these hashing criteria are widely used in\nprevious researches, its analytical behavior is rarely studied. All building\nblocks of our algorithm are based on the analytical solution, and it thus\nprovides a fairly simple and efficient procedure.\n", "versions": [{"version": "v1", "created": "Thu, 29 Jan 2015 11:50:33 GMT"}], "update_date": "2015-01-30", "authors_parsed": [["Ishikawa", "Kohta", ""], ["Sato", "Ikuro", ""], ["Ambai", "Mitsuru", ""]]}, {"id": "1501.07430", "submitter": "Seungjin Choi", "authors": "Juho Lee and Seungjin Choi", "title": "Bayesian Hierarchical Clustering with Exponential Family: Small-Variance\n  Asymptotics and Reducibility", "comments": "10 pages, 2 figures, AISTATS-2015", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Bayesian hierarchical clustering (BHC) is an agglomerative clustering method,\nwhere a probabilistic model is defined and its marginal likelihoods are\nevaluated to decide which clusters to merge. While BHC provides a few\nadvantages over traditional distance-based agglomerative clustering algorithms,\nsuccessive evaluation of marginal likelihoods and careful hyperparameter tuning\nare cumbersome and limit the scalability. In this paper we relax BHC into a\nnon-probabilistic formulation, exploring small-variance asymptotics in\nconjugate-exponential models. We develop a novel clustering algorithm, referred\nto as relaxed BHC (RBHC), from the asymptotic limit of the BHC model that\nexhibits the scalability of distance-based agglomerative clustering algorithms\nas well as the flexibility of Bayesian nonparametric models. We also\ninvestigate the reducibility of the dissimilarity measure emerged from the\nasymptotic limit of the BHC model, allowing us to use scalable algorithms such\nas the nearest neighbor chain algorithm. Numerical experiments on both\nsynthetic and real-world datasets demonstrate the validity and high performance\nof our method.\n", "versions": [{"version": "v1", "created": "Thu, 29 Jan 2015 12:13:01 GMT"}, {"version": "v2", "created": "Wed, 3 Jun 2015 00:45:09 GMT"}], "update_date": "2015-06-04", "authors_parsed": [["Lee", "Juho", ""], ["Choi", "Seungjin", ""]]}, {"id": "1501.07440", "submitter": "Jonathan Scarlett", "authors": "Jonathan Scarlett and Volkan Cevher", "title": "Limits on Support Recovery with Probabilistic Models: An\n  Information-Theoretic Framework", "comments": "Accepted to IEEE Transactions on Information Theory; presented in\n  part at ISIT 2015 and SODA 2016", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IT math.IT math.ST stat.ML stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The support recovery problem consists of determining a sparse subset of a set\nof variables that is relevant in generating a set of observations, and arises\nin a diverse range of settings such as compressive sensing, and subset\nselection in regression, and group testing. In this paper, we take a unified\napproach to support recovery problems, considering general probabilistic models\nrelating a sparse data vector to an observation vector. We study the\ninformation-theoretic limits of both exact and partial support recovery, taking\na novel approach motivated by thresholding techniques in channel coding. We\nprovide general achievability and converse bounds characterizing the trade-off\nbetween the error probability and number of measurements, and we specialize\nthese to the linear, 1-bit, and group testing models. In several cases, our\nbounds not only provide matching scaling laws in the necessary and sufficient\nnumber of measurements, but also sharp thresholds with matching constant\nfactors. Our approach has several advantages over previous approaches: For the\nachievability part, we obtain sharp thresholds under broader scalings of the\nsparsity level and other parameters (e.g., signal-to-noise ratio) compared to\nseveral previous works, and for the converse part, we not only provide\nconditions under which the error probability fails to vanish, but also\nconditions under which it tends to one.\n", "versions": [{"version": "v1", "created": "Thu, 29 Jan 2015 13:04:11 GMT"}, {"version": "v2", "created": "Wed, 11 Feb 2015 15:59:31 GMT"}, {"version": "v3", "created": "Tue, 30 Aug 2016 10:47:09 GMT"}], "update_date": "2016-08-31", "authors_parsed": [["Scarlett", "Jonathan", ""], ["Cevher", "Volkan", ""]]}, {"id": "1501.07518", "submitter": "Fabrizio Lecci", "authors": "Samrachana Adhikari, Fabrizio Lecci, James T. Becker, Brian W. Junker,\n  Lewis H. Kuller, Oscar L. Lopez, Ryan J. Tibshirani", "title": "High-Dimensional Longitudinal Classification with the Multinomial Fused\n  Lasso", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study regularized estimation in high-dimensional longitudinal\nclassification problems, using the lasso and fused lasso regularizers. The\nconstructed coefficient estimates are piecewise constant across the time\ndimension in the longitudinal problem, with adaptively selected change points\n(break points). We present an efficient algorithm for computing such estimates,\nbased on proximal gradient descent. We apply our proposed technique to a\nlongitudinal data set on Alzheimer's disease from the Cardiovascular Health\nStudy Cognition Study, and use this data set to motivate and demonstrate\nseveral practical considerations such as the selection of tuning parameters,\nand the assessment of model stability.\n", "versions": [{"version": "v1", "created": "Thu, 29 Jan 2015 17:16:36 GMT"}], "update_date": "2015-01-30", "authors_parsed": [["Adhikari", "Samrachana", ""], ["Lecci", "Fabrizio", ""], ["Becker", "James T.", ""], ["Junker", "Brian W.", ""], ["Kuller", "Lewis H.", ""], ["Lopez", "Oscar L.", ""], ["Tibshirani", "Ryan J.", ""]]}, {"id": "1501.07768", "submitter": "Cyrille Dubarry", "authors": "Cyrille Dubarry", "title": "Confidence intervals for AB-test", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  AB-testing is a very popular technique in web companies since it makes it\npossible to accurately predict the impact of a modification with the simplicity\nof a random split across users. One of the critical aspects of an AB-test is\nits duration and it is important to reliably compute confidence intervals\nassociated with the metric of interest to know when to stop the test. In this\npaper, we define a clean mathematical framework to model the AB-test process.\nWe then propose three algorithms based on bootstrapping and on the central\nlimit theorem to compute reliable confidence intervals which extend to other\nmetrics than the common probabilities of success. They apply to both absolute\nand relative increments of the most used comparison metrics, including the\nnumber of occurrences of a particular event and a click-through rate implying a\nratio.\n", "versions": [{"version": "v1", "created": "Fri, 30 Jan 2015 13:36:37 GMT"}], "update_date": "2015-02-02", "authors_parsed": [["Dubarry", "Cyrille", ""]]}]