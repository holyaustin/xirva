[{"id": "1002.0140", "submitter": "Aureli Alabert", "authors": "Aureli Alabert, Luz Ma. Rangel", "title": "Classifying the typefaces of the Gutenberg 42-line bible", "comments": "21 pages, 8 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML math.OC physics.data-an stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We have measured the dissimilarities among several printed characters of a\nsingle page in the Gutenberg 42-line bible and we prove statistically the\nexistence of several different matrices from which the metal types where\nconstructed. This is in contrast with the prevailing theory, which states that\nonly one matrix per character was used in the printing process of Gutenberg's\ngreatest work.\n  The main mathematical tool for this purpose is cluster analysis, combined\nwith a statistical test for outliers. We carry out the research with two\nletters, i and a. In the first case, an exact clustering method is employed; in\nthe second, with more specimens to be classified, we resort to an approximate\nagglomerative clustering method.\n  The results show that the letters form clusters according to their shape,\nwith significant shape differences among clusters, and allow to conclude, with\na very small probability of error, that indeed the metal types used to print\nthem were cast from several different matrices.\n  Mathematics Subject Classification: 62H30\n", "versions": [{"version": "v1", "created": "Sun, 31 Jan 2010 16:42:08 GMT"}], "update_date": "2010-02-04", "authors_parsed": [["Alabert", "Aureli", ""], ["Rangel", "Luz Ma.", ""]]}, {"id": "1002.0747", "submitter": "Omer Tamuz", "authors": "Elchanan Mossel and Noah Olsman and Omer Tamuz", "title": "Efficient Bayesian Learning in Social Networks with Gaussian Estimators", "comments": "Added coauthor. Added proofs for fast convergence on trees and\n  distance transitive graphs. Also, now analyzing a notion of privacy", "journal-ref": null, "doi": "10.1109/ALLERTON.2016.7852262", "report-no": null, "categories": "stat.AP cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider a group of Bayesian agents who try to estimate a state of the\nworld $\\theta$ through interaction on a social network. Each agent $v$\ninitially receives a private measurement of $\\theta$: a number $S_v$ picked\nfrom a Gaussian distribution with mean $\\theta$ and standard deviation one.\nThen, in each discrete time iteration, each reveals its estimate of $\\theta$ to\nits neighbors, and, observing its neighbors' actions, updates its belief using\nBayes' Law.\n  This process aggregates information efficiently, in the sense that all the\nagents converge to the belief that they would have, had they access to all the\nprivate measurements. We show that this process is computationally efficient,\nso that each agent's calculation can be easily carried out. We also show that\non any graph the process converges after at most $2N \\cdot D$ steps, where $N$\nis the number of agents and $D$ is the diameter of the network. Finally, we\nshow that on trees and on distance transitive-graphs the process converges\nafter $D$ steps, and that it preserves privacy, so that agents learn very\nlittle about the private signal of most other agents, despite the efficient\naggregation of information. Our results extend those in an unpublished\nmanuscript of the first and last authors.\n", "versions": [{"version": "v1", "created": "Wed, 3 Feb 2010 14:11:06 GMT"}, {"version": "v2", "created": "Sun, 18 Apr 2010 14:53:07 GMT"}, {"version": "v3", "created": "Sun, 26 Jun 2016 01:23:28 GMT"}], "update_date": "2018-04-24", "authors_parsed": [["Mossel", "Elchanan", ""], ["Olsman", "Noah", ""], ["Tamuz", "Omer", ""]]}, {"id": "1002.0832", "submitter": "Massimiliano Pontil", "authors": "Andreas Maurer Massimiliano Pontil", "title": "K-Dimensional Coding Schemes in Hilbert Spaces", "comments": "17 pages", "journal-ref": "IEEE Transactions on Information Theory, 56(11): 5839-5846, 2010", "doi": null, "report-no": null, "categories": "stat.ML math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper presents a general coding method where data in a Hilbert space are\nrepresented by finite dimensional coding vectors. The method is based on\nempirical risk minimization within a certain class of linear operators, which\nmap the set of coding vectors to the Hilbert space. Two results bounding the\nexpected reconstruction error of the method are derived, which highlight the\nrole played by the codebook and the class of linear operators. The results are\nspecialized to some cases of practical importance, including K-means\nclustering, nonnegative matrix factorization and other sparse coding methods.\n", "versions": [{"version": "v1", "created": "Wed, 3 Feb 2010 20:42:49 GMT"}], "update_date": "2011-09-05", "authors_parsed": [["Pontil", "Andreas Maurer Massimiliano", ""]]}, {"id": "1002.1247", "submitter": "Michael Wakin", "authors": "Michael B. Wakin", "title": "Manifold-Based Signal Recovery and Parameter Estimation from Compressive\n  Measurements", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A field known as Compressive Sensing (CS) has recently emerged to help\naddress the growing challenges of capturing and processing high-dimensional\nsignals and data sets. CS exploits the surprising fact that the information\ncontained in a sparse signal can be preserved in a small number of compressive\n(or random) linear measurements of that signal. Strong theoretical guarantees\nhave been established on the accuracy to which sparse or near-sparse signals\ncan be recovered from noisy compressive measurements. In this paper, we address\nsimilar questions in the context of a different modeling framework. Instead of\nsparse models, we focus on the broad class of manifold models, which can arise\nin both parametric and non-parametric signal families. Building upon recent\nresults concerning the stable embeddings of manifolds within the measurement\nspace, we establish both deterministic and probabilistic instance-optimal\nbounds in $\\ell_2$ for manifold-based signal recovery and parameter estimation\nfrom noisy compressive measurements. In line with analogous results for\nsparsity-based CS, we conclude that much stronger bounds are possible in the\nprobabilistic setting. Our work supports the growing empirical evidence that\nmanifold-based models can be used with high accuracy in compressive signal\nprocessing.\n", "versions": [{"version": "v1", "created": "Fri, 5 Feb 2010 15:30:26 GMT"}], "update_date": "2010-02-08", "authors_parsed": [["Wakin", "Michael B.", ""]]}, {"id": "1002.1994", "submitter": "Gilad Lerman Dr", "authors": "Gilad Lerman and Teng Zhang", "title": "Probabilistic Recovery of Multiple Subspaces in Point Clouds by\n  Geometric lp Minimization", "comments": "This paper was split into two different papers: 1.\n  http://arxiv.org/abs/1012.4116 2. http://arxiv.org/abs/1104.3770", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We assume data independently sampled from a mixture distribution on the unit\nball of the D-dimensional Euclidean space with K+1 components: the first\ncomponent is a uniform distribution on that ball representing outliers and the\nother K components are uniform distributions along K d-dimensional linear\nsubspaces restricted to that ball. We study both the simultaneous recovery of\nall K underlying subspaces and the recovery of the best l0 subspace (i.e., with\nlargest number of points) by minimizing the lp-averaged distances of data\npoints from d-dimensional subspaces of the D-dimensional space. Unlike other lp\nminimization problems, this minimization is non-convex for all p>0 and thus\nrequires different methods for its analysis. We show that if 0<p <= 1, then\nboth all underlying subspaces and the best l0 subspace can be precisely\nrecovered by lp minimization with overwhelming probability. This result extends\nto additive homoscedastic uniform noise around the subspaces (i.e., uniform\ndistribution in a strip around them) and near recovery with an error\nproportional to the noise level. On the other hand, if K>1 and p>1, then we\nshow that both all underlying subspaces and the best l0 subspace cannot be\nrecovered and even nearly recovered. Further relaxations are also discussed. We\nuse the results of this paper for partially justifying recent effective\nalgorithms for modeling data by mixtures of multiple subspaces as well as for\ndiscussing the effect of using variants of lp minimizations in RANSAC-type\nstrategies for single subspace recovery.\n", "versions": [{"version": "v1", "created": "Tue, 9 Feb 2010 22:30:14 GMT"}, {"version": "v2", "created": "Sun, 19 Dec 2010 11:20:33 GMT"}, {"version": "v3", "created": "Thu, 19 Apr 2012 19:35:55 GMT"}], "update_date": "2012-04-20", "authors_parsed": [["Lerman", "Gilad", ""], ["Zhang", "Teng", ""]]}, {"id": "1002.2243", "submitter": "Sam George", "authors": "Sam O. George, H. Bola George, Scott V. Nguyen", "title": "Effect of Wind Intermittency on the Electric Grid: Mitigating the Risk\n  of Energy Deficits", "comments": "8 pages, 12 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP physics.ao-ph physics.data-an physics.soc-ph stat.CO stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Successful implementation of California's Renewable Portfolio Standard (RPS)\nmandating 33 percent renewable energy generation by 2020 requires inclusion of\na robust strategy to mitigate increased risk of energy deficits (blackouts) due\nto short time-scale (sub 1 hour) intermittencies in renewable energy sources.\nOf these RPS sources, wind energy has the fastest growth rate--over 25%\nyear-over-year. If these growth trends continue, wind energy could make up 15\npercent of California's energy portfolio by 2016 (wRPS15). However, the\nhour-to-hour variations in wind energy (speed) will create large hourly energy\ndeficits that require installation of other, more predictable, compensation\ngeneration capacity and infrastructure. Compensating for the energy deficits of\nwRPS15 could potentially cost tens of billions in additional dollar-expenditure\nfor fossil and / or nuclear generation capacity. There is a real possibility\nthat carbon dioxide and other greenhouse gas (GHG) emission reductions will\nmiss the California Assembly Bill 32 (CA AB 32) target by a wide margin once\nthe wRPS15 compensation system is in place. This work presents a set of\nanalytics tools that show the impact of short-term intermittencies to help\npolicy makers understand and plan for wRPS15 integration. What are the right\npolicy choices for RPS that include wind energy?\n", "versions": [{"version": "v1", "created": "Wed, 10 Feb 2010 23:42:09 GMT"}], "update_date": "2016-09-08", "authors_parsed": [["George", "Sam O.", ""], ["George", "H. Bola", ""], ["Nguyen", "Scott V.", ""]]}, {"id": "1002.2313", "submitter": "Pierre Pudlo", "authors": "Bruno Pelletier (IRMAR), Pierre Pudlo (I3M)", "title": "Operator norm convergence of spectral clustering on level sets", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Following Hartigan, a cluster is defined as a connected component of the\nt-level set of the underlying density, i.e., the set of points for which the\ndensity is greater than t. A clustering algorithm which combines a density\nestimate with spectral clustering techniques is proposed. Our algorithm is\ncomposed of two steps. First, a nonparametric density estimate is used to\nextract the data points for which the estimated density takes a value greater\nthan t. Next, the extracted points are clustered based on the eigenvectors of a\ngraph Laplacian matrix. Under mild assumptions, we prove the almost sure\nconvergence in operator norm of the empirical graph Laplacian operator\nassociated with the algorithm. Furthermore, we give the typical behavior of the\nrepresentation of the dataset into the feature space, which establishes the\nstrong consistency of our proposed algorithm.\n", "versions": [{"version": "v1", "created": "Thu, 11 Feb 2010 10:20:35 GMT"}], "update_date": "2010-02-12", "authors_parsed": [["Pelletier", "Bruno", "", "IRMAR"], ["Pudlo", "Pierre", "", "I3M"]]}, {"id": "1002.3315", "submitter": "Yang Feng", "authors": "Jianqing Fan, Yang Feng, Yichao Wu", "title": "High-dimensional variable selection for Cox's proportional hazards model", "comments": "17 pages, 5 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Variable selection in high dimensional space has challenged many contemporary\nstatistical problems from many frontiers of scientific disciplines. Recent\ntechnology advance has made it possible to collect a huge amount of covariate\ninformation such as microarray, proteomic and SNP data via bioimaging\ntechnology while observing survival information on patients in clinical\nstudies. Thus, the same challenge applies to the survival analysis in order to\nunderstand the association between genomics information and clinical\ninformation about the survival time. In this work, we extend the sure screening\nprocedure Fan and Lv (2008) to Cox's proportional hazards model with an\niterative version available. Numerical simulation studies have shown\nencouraging performance of the proposed method in comparison with other\ntechniques such as LASSO. This demonstrates the utility and versatility of the\niterative sure independent screening scheme.\n", "versions": [{"version": "v1", "created": "Wed, 17 Feb 2010 17:23:19 GMT"}, {"version": "v2", "created": "Wed, 19 May 2010 19:34:18 GMT"}], "update_date": "2010-05-20", "authors_parsed": [["Fan", "Jianqing", ""], ["Feng", "Yang", ""], ["Wu", "Yichao", ""]]}, {"id": "1002.3509", "submitter": "J\\\"uri Lember", "authors": "Kristi Kuljus and J\\\"uri Lember", "title": "Asymptotic risks of Viterbi segmentation", "comments": "23 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.PR stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider the maximum likelihood (Viterbi) alignment of a hidden Markov\nmodel (HMM). In an HMM, the underlying Markov chain is usually hidden and the\nViterbi alignment is often used as the estimate of it. This approach will be\nreferred to as the Viterbi segmentation. The goodness of the Viterbi\nsegmentation can be measured by several risks. In this paper, we prove the\nexistence of asymptotic risks. Being independent of data, the asymptotic risks\ncan be considered as the characteristics of the model that illustrate the\nlong-run behavior of the Viterbi segmentation.\n", "versions": [{"version": "v1", "created": "Thu, 18 Feb 2010 16:07:04 GMT"}, {"version": "v2", "created": "Mon, 13 Dec 2010 15:24:00 GMT"}], "update_date": "2010-12-14", "authors_parsed": [["Kuljus", "Kristi", ""], ["Lember", "J\u00fcri", ""]]}, {"id": "1002.3684", "submitter": "Vicente Zarzoso", "authors": "Vicente Zarzoso, Pierre Comon", "title": "Robust Independent Component Analysis by Iterative Maximization of the\n  Kurtosis Contrast with Algebraic Optimal Step Size", "comments": null, "journal-ref": "IEEE Transactions on Neural Networks 21, 2 (2010) 248-261", "doi": "10.1109/TNN.2009.2035920", "report-no": null, "categories": "stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Independent component analysis (ICA) aims at decomposing an observed random\nvector into statistically independent variables. Deflation-based\nimplementations, such as the popular one-unit FastICA algorithm and its\nvariants, extract the independent components one after another. A novel method\nfor deflationary ICA, referred to as RobustICA, is put forward in this paper.\nThis simple technique consists of performing exact line search optimization of\nthe kurtosis contrast function. The step size leading to the global maximum of\nthe contrast along the search direction is found among the roots of a\nfourth-degree polynomial. This polynomial rooting can be performed\nalgebraically, and thus at low cost, at each iteration. Among other practical\nbenefits, RobustICA can avoid prewhitening and deals with real- and\ncomplex-valued mixtures of possibly noncircular sources alike. The absence of\nprewhitening improves asymptotic performance. The algorithm is robust to local\nextrema and shows a very high convergence speed in terms of the computational\ncost required to reach a given source extraction quality, particularly for\nshort data records. These features are demonstrated by a comparative numerical\nanalysis on synthetic data. RobustICA's capabilities in processing real-world\ndata involving noncircular complex strongly super-Gaussian sources are\nillustrated by the biomedical problem of atrial activity (AA) extraction in\natrial fibrillation (AF) electrocardiograms (ECGs), where it outperforms an\nalternative ICA-based technique.\n", "versions": [{"version": "v1", "created": "Fri, 19 Feb 2010 08:33:43 GMT"}], "update_date": "2016-11-15", "authors_parsed": [["Zarzoso", "Vicente", ""], ["Comon", "Pierre", ""]]}, {"id": "1002.3744", "submitter": "Robin  Girard", "authors": "R. Girard", "title": "Plugin procedure in segmentation and application to hyperspectral image\n  segmentation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this article we give our contribution to the problem of segmentation with\nplug-in procedures. We give general sufficient conditions under which plug in\nprocedure are efficient. We also give an algorithm that satisfy these\nconditions. We give an application of the used algorithm to hyperspectral\nimages segmentation. Hyperspectral images are images that have both spatial and\nspectral coherence with thousands of spectral bands on each pixel. In the\nproposed procedure we combine a reduction dimension technique and a spatial\nregularisation technique. This regularisation is based on the mixlet\nmodelisation of Kolaczyck and Al.\n", "versions": [{"version": "v1", "created": "Fri, 19 Feb 2010 13:48:57 GMT"}], "update_date": "2010-02-22", "authors_parsed": [["Girard", "R.", ""]]}, {"id": "1002.3946", "submitter": "Rosemary Braun", "authors": "Rosemary Braun, Gregory Leibon, Scott Pauls, and Daniel Rockmore", "title": "Partition Decoupling for Multi-gene Analysis of Gene Expression\n  Profiling Data", "comments": "Revised", "journal-ref": null, "doi": null, "report-no": null, "categories": "q-bio.QM q-bio.GN q-bio.MN stat.CO stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present the extention and application of a new unsupervised statistical\nlearning technique--the Partition Decoupling Method--to gene expression data.\nBecause it has the ability to reveal non-linear and non-convex geometries\npresent in the data, the PDM is an improvement over typical gene expression\nanalysis algorithms, permitting a multi-gene analysis that can reveal\nphenotypic differences even when the individual genes do not exhibit\ndifferential expression. Here, we apply the PDM to publicly-available gene\nexpression data sets, and demonstrate that we are able to identify cell types\nand treatments with higher accuracy than is obtained through other approaches.\nBy applying it in a pathway-by-pathway fashion, we demonstrate how the PDM may\nbe used to find sets of mechanistically-related genes that discriminate\nphenotypes.\n", "versions": [{"version": "v1", "created": "Sun, 21 Feb 2010 02:38:20 GMT"}, {"version": "v2", "created": "Thu, 17 Mar 2011 21:44:26 GMT"}], "update_date": "2015-09-24", "authors_parsed": [["Braun", "Rosemary", ""], ["Leibon", "Gregory", ""], ["Pauls", "Scott", ""], ["Rockmore", "Daniel", ""]]}, {"id": "1002.4019", "submitter": "Gowtham Bellala", "authors": "Gowtham Bellala, Suresh Bhavnani, Clayton Scott", "title": "Query Learning with Exponential Query Costs", "comments": "15 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.IT math.IT", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In query learning, the goal is to identify an unknown object while minimizing\nthe number of \"yes\" or \"no\" questions (queries) posed about that object. A\nwell-studied algorithm for query learning is known as generalized binary search\n(GBS). We show that GBS is a greedy algorithm to optimize the expected number\nof queries needed to identify the unknown object. We also generalize GBS in two\nways. First, we consider the case where the cost of querying grows\nexponentially in the number of queries and the goal is to minimize the expected\nexponential cost. Then, we consider the case where the objects are partitioned\ninto groups, and the objective is to identify only the group to which the\nobject belongs. We derive algorithms to address these issues in a common,\ninformation-theoretic framework. In particular, we present an exact formula for\nthe objective function in each case involving Shannon or Renyi entropy, and\ndevelop a greedy algorithm for minimizing it. Our algorithms are demonstrated\non two applications of query learning, active learning and emergency response.\n", "versions": [{"version": "v1", "created": "Sun, 21 Feb 2010 21:06:53 GMT"}], "update_date": "2010-02-23", "authors_parsed": [["Bellala", "Gowtham", ""], ["Bhavnani", "Suresh", ""], ["Scott", "Clayton", ""]]}, {"id": "1002.4112", "submitter": "Nicole Kraemer", "authors": "Nicole Kraemer, Masashi Sugiyama", "title": "The Degrees of Freedom of Partial Least Squares Regression", "comments": "to appear in the Journal of the American Statistical Association", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME math.ST stat.ML stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The derivation of statistical properties for Partial Least Squares regression\ncan be a challenging task. The reason is that the construction of latent\ncomponents from the predictor variables also depends on the response variable.\nWhile this typically leads to good performance and interpretable models in\npractice, it makes the statistical analysis more involved. In this work, we\nstudy the intrinsic complexity of Partial Least Squares Regression. Our\ncontribution is an unbiased estimate of its Degrees of Freedom. It is defined\nas the trace of the first derivative of the fitted values, seen as a function\nof the response. We establish two equivalent representations that rely on the\nclose connection of Partial Least Squares to matrix decompositions and Krylov\nsubspace techniques. We show that the Degrees of Freedom depend on the\ncollinearity of the predictor variables: The lower the collinearity is, the\nhigher the Degrees of Freedom are. In particular, they are typically higher\nthan the naive approach that defines the Degrees of Freedom as the number of\ncomponents. Further, we illustrate how the Degrees of Freedom approach can be\nused for the comparison of different regression methods. In the experimental\nsection, we show that our Degrees of Freedom estimate in combination with\ninformation criteria is useful for model selection.\n", "versions": [{"version": "v1", "created": "Mon, 22 Feb 2010 13:31:02 GMT"}, {"version": "v2", "created": "Tue, 25 Jan 2011 09:32:34 GMT"}, {"version": "v3", "created": "Wed, 9 Feb 2011 15:58:58 GMT"}], "update_date": "2015-03-13", "authors_parsed": [["Kraemer", "Nicole", ""], ["Sugiyama", "Masashi", ""]]}, {"id": "1002.4658", "submitter": "Huan Xu Dr.", "authors": "Huan Xu, Constantine Caramanis, Shie Mannor", "title": "Principal Component Analysis with Contaminated Data: The High\n  Dimensional Case", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider the dimensionality-reduction problem (finding a subspace\napproximation of observed data) for contaminated data in the high dimensional\nregime, where the number of observations is of the same magnitude as the number\nof variables of each observation, and the data set contains some (arbitrarily)\ncorrupted observations. We propose a High-dimensional Robust Principal\nComponent Analysis (HR-PCA) algorithm that is tractable, robust to contaminated\npoints, and easily kernelizable. The resulting subspace has a bounded deviation\nfrom the desired one, achieves maximal robustness -- a breakdown point of 50%\nwhile all existing algorithms have a breakdown point of zero, and unlike\nordinary PCA algorithms, achieves optimality in the limit case where the\nproportion of corrupted points goes to zero.\n", "versions": [{"version": "v1", "created": "Wed, 24 Feb 2010 23:24:17 GMT"}, {"version": "v2", "created": "Thu, 13 May 2010 03:06:22 GMT"}], "update_date": "2010-05-14", "authors_parsed": [["Xu", "Huan", ""], ["Caramanis", "Constantine", ""], ["Mannor", "Shie", ""]]}, {"id": "1002.4802", "submitter": "Ricardo Silva", "authors": "Ricardo Silva and Robert B. Gramacy", "title": "Gaussian Process Structural Equation Models with Latent Variables", "comments": "12 pages, 6 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In a variety of disciplines such as social sciences, psychology, medicine and\neconomics, the recorded data are considered to be noisy measurements of latent\nvariables connected by some causal structure. This corresponds to a family of\ngraphical models known as the structural equation model with latent variables.\nWhile linear non-Gaussian variants have been well-studied, inference in\nnonparametric structural equation models is still underdeveloped. We introduce\na sparse Gaussian process parameterization that defines a non-linear structure\nconnecting latent variables, unlike common formulations of Gaussian process\nlatent variable models. The sparse parameterization is given a full Bayesian\ntreatment without compromising Markov chain Monte Carlo efficiency. We compare\nthe stability of the sampling procedure and the predictive ability of the model\nagainst the current practice.\n", "versions": [{"version": "v1", "created": "Thu, 25 Feb 2010 15:10:06 GMT"}, {"version": "v2", "created": "Fri, 12 Mar 2010 10:41:26 GMT"}], "update_date": "2010-03-15", "authors_parsed": [["Silva", "Ricardo", ""], ["Gramacy", "Robert B.", ""]]}]