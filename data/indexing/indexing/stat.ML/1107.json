[{"id": "1107.0414", "submitter": "Francois Meyer", "authors": "Kye M. Taylor and Francois G. Meyer", "title": "A random walk on image patches", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "physics.data-an cs.DM stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper we address the problem of understanding the success of\nalgorithms that organize patches according to graph-based metrics. Algorithms\nthat analyze patches extracted from images or time series have led to\nstate-of-the art techniques for classification, denoising, and the study of\nnonlinear dynamics. The main contribution of this work is to provide a\ntheoretical explanation for the above experimental observations. Our approach\nrelies on a detailed analysis of the commute time metric on prototypical graph\nmodels that epitomize the geometry observed in general patch graphs. We prove\nthat a parametrization of the graph based on commute times shrinks the mutual\ndistances between patches that correspond to rapid local changes in the signal,\nwhile the distances between patches that correspond to slow local changes\nexpand. In effect, our results explain why the parametrization of the set of\npatches based on the eigenfunctions of the Laplacian can concentrate patches\nthat correspond to rapid local changes, which would otherwise be shattered in\nthe space of patches. While our results are based on a large sample analysis,\nnumerical experimentations on synthetic and real data indicate that the results\nhold for datasets that are very small in practice.\n", "versions": [{"version": "v1", "created": "Sat, 2 Jul 2011 20:37:07 GMT"}], "update_date": "2011-07-05", "authors_parsed": [["Taylor", "Kye M.", ""], ["Meyer", "Francois G.", ""]]}, {"id": "1107.0521", "submitter": "Luai Al Labadi", "authors": "Mahmoud Zarepour and Luai Al Labadi", "title": "On a Rapid Simulation of the Dirichlet Process", "comments": "Copy right regulations. The paper has been accepted", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We describe a simple and efficient procedure for approximating the L\\'evy\nmeasure of a $\\text{Gamma}(\\alpha,1)$ random variable. We use this\napproximation to derive a finite sum-representation that converges almost\nsurely to Ferguson's representation of the Dirichlet process based on arrivals\nof a homogeneous Poisson process. We compare the efficiency of our\napproximation to several other well known approximations of the Dirichlet\nprocess and demonstrate a substantial improvement.\n", "versions": [{"version": "v1", "created": "Mon, 4 Jul 2011 04:09:03 GMT"}, {"version": "v2", "created": "Sat, 10 Dec 2011 14:17:34 GMT"}, {"version": "v3", "created": "Wed, 25 Jan 2012 11:38:01 GMT"}], "update_date": "2012-01-26", "authors_parsed": [["Zarepour", "Mahmoud", ""], ["Labadi", "Luai Al", ""]]}, {"id": "1107.0662", "submitter": "Arijit Das", "authors": "Arijit Das and Anthony Quinn", "title": "A Variational Bayes Approach to Decoding in a Phase-Uncertain Digital\n  Receiver", "comments": "6 pages, 3 figures, Accepted at the Irish Signals and Systems\n  Conference 23-24 June 2011", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper presents a Bayesian approach to symbol and phase inference in a\nphase-unsynchronized digital receiver. It primarily extends [Quinn 2011] to the\nmulti-symbol case, using the variational Bayes (VB) approximation to deal with\nthe combinatorial complexity of the phase inference in this case. The work\nprovides a fully Bayesian extension of the EM-based framework underlying\ncurrent turbo-synchronization methods, since it induces a von Mises prior on\nthe time-invariant phase parmeter. As a result, we achieve tractable iterative\nalgorithms with improved robustness in low SNR regimes, compared to the current\nEM-based approaches. As a corollary to our analysis we also discover the\nimportance of prior regularization in elegantly tackling the significant\nproblem of phase ambiguity.\n", "versions": [{"version": "v1", "created": "Mon, 4 Jul 2011 15:37:19 GMT"}], "update_date": "2015-03-13", "authors_parsed": [["Das", "Arijit", ""], ["Quinn", "Anthony", ""]]}, {"id": "1107.0789", "submitter": "Lester Mackey", "authors": "Lester Mackey, Ameet Talwalkar, Michael I. Jordan", "title": "Distributed Matrix Completion and Robust Factorization", "comments": "35 pages, 6 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.DS cs.NA math.NA stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  If learning methods are to scale to the massive sizes of modern datasets, it\nis essential for the field of machine learning to embrace parallel and\ndistributed computing. Inspired by the recent development of matrix\nfactorization methods with rich theory but poor computational complexity and by\nthe relative ease of mapping matrices onto distributed architectures, we\nintroduce a scalable divide-and-conquer framework for noisy matrix\nfactorization. We present a thorough theoretical analysis of this framework in\nwhich we characterize the statistical errors introduced by the \"divide\" step\nand control their magnitude in the \"conquer\" step, so that the overall\nalgorithm enjoys high-probability estimation guarantees comparable to those of\nits base algorithm. We also present experiments in collaborative filtering and\nvideo background modeling that demonstrate the near-linear to superlinear\nspeed-ups attainable with this approach.\n", "versions": [{"version": "v1", "created": "Tue, 5 Jul 2011 06:03:44 GMT"}, {"version": "v2", "created": "Wed, 17 Aug 2011 00:59:30 GMT"}, {"version": "v3", "created": "Wed, 21 Sep 2011 01:38:14 GMT"}, {"version": "v4", "created": "Tue, 1 Nov 2011 05:37:48 GMT"}, {"version": "v5", "created": "Fri, 18 May 2012 09:28:27 GMT"}, {"version": "v6", "created": "Tue, 14 Aug 2012 17:33:30 GMT"}, {"version": "v7", "created": "Mon, 28 Oct 2013 06:02:12 GMT"}], "update_date": "2013-10-29", "authors_parsed": [["Mackey", "Lester", ""], ["Talwalkar", "Ameet", ""], ["Jordan", "Michael I.", ""]]}, {"id": "1107.1283", "submitter": "Daniel Hsu", "authors": "Animashree Anandkumar, Kamalika Chaudhuri, Daniel Hsu, Sham M. Kakade,\n  Le Song, Tong Zhang", "title": "Spectral Methods for Learning Multivariate Latent Tree Structure", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This work considers the problem of learning the structure of multivariate\nlinear tree models, which include a variety of directed tree graphical models\nwith continuous, discrete, and mixed latent variables such as linear-Gaussian\nmodels, hidden Markov models, Gaussian mixture models, and Markov evolutionary\ntrees. The setting is one where we only have samples from certain observed\nvariables in the tree, and our goal is to estimate the tree structure (i.e.,\nthe graph of how the underlying hidden variables are connected to each other\nand to the observed variables). We propose the Spectral Recursive Grouping\nalgorithm, an efficient and simple bottom-up procedure for recovering the tree\nstructure from independent samples of the observed variables. Our finite sample\nsize bounds for exact recovery of the tree structure reveal certain natural\ndependencies on underlying statistical and structural properties of the\nunderlying joint distribution. Furthermore, our sample complexity guarantees\nhave no explicit dependence on the dimensionality of the observed variables,\nmaking the algorithm applicable to many high-dimensional settings. At the heart\nof our algorithm is a spectral quartet test for determining the relative\ntopology of a quartet of variables from second-order statistics.\n", "versions": [{"version": "v1", "created": "Thu, 7 Jul 2011 02:33:31 GMT"}, {"version": "v2", "created": "Tue, 8 Nov 2011 15:42:32 GMT"}], "update_date": "2011-11-09", "authors_parsed": [["Anandkumar", "Animashree", ""], ["Chaudhuri", "Kamalika", ""], ["Hsu", "Daniel", ""], ["Kakade", "Sham M.", ""], ["Song", "Le", ""], ["Zhang", "Tong", ""]]}, {"id": "1107.1736", "submitter": "Animashree Anandkumar", "authors": "Animashree Anandkumar, Vincent Y. F. Tan, Furong Huang, Alan S.\n  Willsky", "title": "High-dimensional structure estimation in Ising models: Local separation\n  criterion", "comments": "Published in at http://dx.doi.org/10.1214/12-AOS1009 the Annals of\n  Statistics (http://www.imstat.org/aos/) by the Institute of Mathematical\n  Statistics (http://www.imstat.org)", "journal-ref": "Annals of Statistics 2012, Vol. 40, No. 3, 1346-1375", "doi": "10.1214/12-AOS1009", "report-no": "IMS-AOS-AOS1009", "categories": "stat.ML cs.LG math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider the problem of high-dimensional Ising (graphical) model\nselection. We propose a simple algorithm for structure estimation based on the\nthresholding of the empirical conditional variation distances. We introduce a\nnovel criterion for tractable graph families, where this method is efficient,\nbased on the presence of sparse local separators between node pairs in the\nunderlying graph. For such graphs, the proposed algorithm has a sample\ncomplexity of $n=\\Omega(J_{\\min}^{-2}\\log p)$, where $p$ is the number of\nvariables, and $J_{\\min}$ is the minimum (absolute) edge potential in the\nmodel. We also establish nonasymptotic necessary and sufficient conditions for\nstructure estimation.\n", "versions": [{"version": "v1", "created": "Fri, 8 Jul 2011 21:35:48 GMT"}, {"version": "v2", "created": "Thu, 24 Nov 2011 02:17:50 GMT"}, {"version": "v3", "created": "Sun, 4 Mar 2012 04:37:52 GMT"}, {"version": "v4", "created": "Mon, 20 Aug 2012 05:38:19 GMT"}], "update_date": "2012-08-21", "authors_parsed": [["Anandkumar", "Animashree", ""], ["Tan", "Vincent Y. F.", ""], ["Huang", "Furong", ""], ["Willsky", "Alan S.", ""]]}, {"id": "1107.1805", "submitter": "Maksims Volkovs", "authors": "Maksims N. Volkovs, Hugo Larochelle, Richard S. Zemel", "title": "Loss-sensitive Training of Probabilistic Conditional Random Fields", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider the problem of training probabilistic conditional random fields\n(CRFs) in the context of a task where performance is measured using a specific\nloss function. While maximum likelihood is the most common approach to training\nCRFs, it ignores the inherent structure of the task's loss function. We\ndescribe alternatives to maximum likelihood which take that loss into account.\nThese include a novel adaptation of a loss upper bound from the structured SVMs\nliterature to the CRF context, as well as a new loss-inspired KL divergence\nobjective which relies on the probabilistic nature of CRFs. These\nloss-sensitive objectives are compared to maximum likelihood using ranking as a\nbenchmark task. This comparison confirms the importance of incorporating loss\ninformation in the probabilistic training of CRFs, with the loss-inspired KL\noutperforming all other objectives.\n", "versions": [{"version": "v1", "created": "Sat, 9 Jul 2011 17:58:46 GMT"}], "update_date": "2015-03-19", "authors_parsed": [["Volkovs", "Maksims N.", ""], ["Larochelle", "Hugo", ""], ["Zemel", "Richard S.", ""]]}, {"id": "1107.2021", "submitter": "Sivan Sabato", "authors": "Sivan Sabato and Naftali Tishby", "title": "Multi-Instance Learning with Any Hypothesis Class", "comments": "Fixed typos and added some explanations", "journal-ref": "Journal of Machine Learning Research 13(Oct):1999-3039, 2012", "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In the supervised learning setting termed Multiple-Instance Learning (MIL),\nthe examples are bags of instances, and the bag label is a function of the\nlabels of its instances. Typically, this function is the Boolean OR. The\nlearner observes a sample of bags and the bag labels, but not the instance\nlabels that determine the bag labels. The learner is then required to emit a\nclassification rule for bags based on the sample. MIL has numerous\napplications, and many heuristic algorithms have been used successfully on this\nproblem, each adapted to specific settings or applications. In this work we\nprovide a unified theoretical analysis for MIL, which holds for any underlying\nhypothesis class, regardless of a specific application or problem domain. We\nshow that the sample complexity of MIL is only poly-logarithmically dependent\non the size of the bag, for any underlying hypothesis class. In addition, we\nintroduce a new PAC-learning algorithm for MIL, which uses a regular supervised\nlearning algorithm as an oracle. We prove that efficient PAC-learning for MIL\ncan be generated from any efficient non-MIL supervised learning algorithm that\nhandles one-sided error. The computational complexity of the resulting\nalgorithm is only polynomially dependent on the bag size.\n", "versions": [{"version": "v1", "created": "Mon, 11 Jul 2011 13:30:58 GMT"}, {"version": "v2", "created": "Tue, 29 May 2012 05:42:03 GMT"}, {"version": "v3", "created": "Mon, 13 Aug 2012 16:38:44 GMT"}], "update_date": "2015-03-19", "authors_parsed": [["Sabato", "Sivan", ""], ["Tishby", "Naftali", ""]]}, {"id": "1107.2104", "submitter": "Jon\\'as Velasco", "authors": "Jon\\'as Velasco, Mario A. Saucedo-Espinosa, Hugo Jair Escalante, Karlo\n  Mendoza, C\\'esar Emilio Villarreal-Rodr\\'iguez, \\'Oscar L.\n  Chac\\'on-Mondrag\\'on, Adri\\'an Rodr\\'iguez, Arturo Berrones", "title": "An estimation of distribution algorithm with adaptive Gibbs sampling for\n  unconstrained global optimization", "comments": "This paper has been withdrawn by the author by request of the journal\n  in which has been accepted for publication", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NE math.OC stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper is proposed a new heuristic approach belonging to the field of\nevolutionary Estimation of Distribution Algorithms (EDAs). EDAs builds a\nprobability model and a set of solutions is sampled from the model which\ncharacterizes the distribution of such solutions. The main framework of the\nproposed method is an estimation of distribution algorithm, in which an\nadaptive Gibbs sampling is used to generate new promising solutions and, in\ncombination with a local search strategy, it improves the individual solutions\nproduced in each iteration. The Estimation of Distribution Algorithm with\nAdaptive Gibbs Sampling we are proposing in this paper is called AGEDA. We\nexperimentally evaluate and compare this algorithm against two deterministic\nprocedures and several stochastic methods in three well known test problems for\nunconstrained global optimization. It is empirically shown that our heuristic\nis robust in problems that involve three central aspects that mainly determine\nthe difficulty of global optimization problems, namely high-dimensionality,\nmulti-modality and non-smoothness.\n", "versions": [{"version": "v1", "created": "Mon, 11 Jul 2011 19:51:49 GMT"}, {"version": "v2", "created": "Wed, 29 May 2013 19:08:37 GMT"}], "update_date": "2013-05-30", "authors_parsed": [["Velasco", "Jon\u00e1s", ""], ["Saucedo-Espinosa", "Mario A.", ""], ["Escalante", "Hugo Jair", ""], ["Mendoza", "Karlo", ""], ["Villarreal-Rodr\u00edguez", "C\u00e9sar Emilio", ""], ["Chac\u00f3n-Mondrag\u00f3n", "\u00d3scar L.", ""], ["Rodr\u00edguez", "Adri\u00e1n", ""], ["Berrones", "Arturo", ""]]}, {"id": "1107.2347", "submitter": "Gautam Pendse", "authors": "Gautam V. Pendse", "title": "BSVM: A Banded Suport Vector Machine", "comments": "16 pages, 4 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We describe a novel binary classification technique called Banded SVM\n(B-SVM). In the standard C-SVM formulation of Cortes et al. (1995), the\ndecision rule is encouraged to lie in the interval [1, \\infty]. The new B-SVM\nobjective function contains a penalty term that encourages the decision rule to\nlie in a user specified range [\\rho_1, \\rho_2]. In addition to the standard set\nof support vectors (SVs) near the class boundaries, B-SVM results in a second\nset of SVs in the interior of each class.\n", "versions": [{"version": "v1", "created": "Tue, 12 Jul 2011 16:56:14 GMT"}], "update_date": "2011-07-13", "authors_parsed": [["Pendse", "Gautam V.", ""]]}, {"id": "1107.2462", "submitter": "Timothy Rubin", "authors": "Timothy N. Rubin, America Chambers, Padhraic Smyth and Mark Steyvers", "title": "Statistical Topic Models for Multi-Label Document Classification", "comments": "44 Pages (Including Appendices). To be published in: The Machine\n  Learning Journal, special issue on Learning from Multi-Label Data. Version 2\n  corrects some typos, updates some of the notation used in the paper for\n  clarification of some equations, and incorporates several relatively minor\n  changes to the text throughout the paper", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Machine learning approaches to multi-label document classification have to\ndate largely relied on discriminative modeling techniques such as support\nvector machines. A drawback of these approaches is that performance rapidly\ndrops off as the total number of labels and the number of labels per document\nincrease. This problem is amplified when the label frequencies exhibit the type\nof highly skewed distributions that are often observed in real-world datasets.\nIn this paper we investigate a class of generative statistical topic models for\nmulti-label documents that associate individual word tokens with different\nlabels. We investigate the advantages of this approach relative to\ndiscriminative models, particularly with respect to classification problems\ninvolving large numbers of relatively rare labels. We compare the performance\nof generative and discriminative approaches on document labeling tasks ranging\nfrom datasets with several thousand labels to datasets with tens of labels. The\nexperimental results indicate that probabilistic generative models can achieve\ncompetitive multi-label classification performance compared to discriminative\nmethods, and have advantages for datasets with many labels and skewed label\nfrequencies.\n", "versions": [{"version": "v1", "created": "Wed, 13 Jul 2011 04:28:32 GMT"}, {"version": "v2", "created": "Thu, 10 Nov 2011 04:24:38 GMT"}], "update_date": "2011-11-11", "authors_parsed": [["Rubin", "Timothy N.", ""], ["Chambers", "America", ""], ["Smyth", "Padhraic", ""], ["Steyvers", "Mark", ""]]}, {"id": "1107.2699", "submitter": "Mauricio A. \\'Alvarez", "authors": "Mauricio A. \\'Alvarez and David Luengo and Neil D. Lawrence", "title": "Linear Latent Force Models using Gaussian Processes", "comments": "20 pages, 2 figures. Extended technical report of the Conference\n  Paper \"Latent force models\" in D. van Dyk and M. Welling (eds) Proceedings of\n  the Twelfth International Workshop on Artificial Intelligence and Statistics,\n  JMLR W&CP 5, Clearwater Beach, FL, pp 9--16", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Purely data driven approaches for machine learning present difficulties when\ndata is scarce relative to the complexity of the model or when the model is\nforced to extrapolate. On the other hand, purely mechanistic approaches need to\nidentify and specify all the interactions in the problem at hand (which may not\nbe feasible) and still leave the issue of how to parameterize the system. In\nthis paper, we present a hybrid approach using Gaussian processes and\ndifferential equations to combine data driven modelling with a physical model\nof the system. We show how different, physically-inspired, kernel functions can\nbe developed through sensible, simple, mechanistic assumptions about the\nunderlying system. The versatility of our approach is illustrated with three\ncase studies from motion capture, computational biology and geostatistics.\n", "versions": [{"version": "v1", "created": "Wed, 13 Jul 2011 23:26:42 GMT"}, {"version": "v2", "created": "Fri, 13 Mar 2020 09:47:06 GMT"}], "update_date": "2020-03-16", "authors_parsed": [["\u00c1lvarez", "Mauricio A.", ""], ["Luengo", "David", ""], ["Lawrence", "Neil D.", ""]]}, {"id": "1107.2848", "submitter": "Peter Richtarik", "authors": "Peter Richt\\'arik and Martin Tak\\'a\\v{c}", "title": "Iteration Complexity of Randomized Block-Coordinate Descent Methods for\n  Minimizing a Composite Function", "comments": "33 pages, 7 figures, 10 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.OC stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper we develop a randomized block-coordinate descent method for\nminimizing the sum of a smooth and a simple nonsmooth block-separable convex\nfunction and prove that it obtains an $\\epsilon$-accurate solution with\nprobability at least $1-\\rho$ in at most $O(\\tfrac{n}{\\epsilon} \\log\n\\tfrac{1}{\\rho})$ iterations, where $n$ is the number of blocks. For strongly\nconvex functions the method converges linearly. This extends recent results of\nNesterov [Efficiency of coordinate descent methods on huge-scale optimization\nproblems, CORE Discussion Paper #2010/2], which cover the smooth case, to\ncomposite minimization, while at the same time improving the complexity by the\nfactor of 4 and removing $\\epsilon$ from the logarithmic term. More\nimportantly, in contrast with the aforementioned work in which the author\nachieves the results by applying the method to a regularized version of the\nobjective function with an unknown scaling factor, we show that this is not\nnecessary, thus achieving true iteration complexity bounds. In the smooth case\nwe also allow for arbitrary probability vectors and non-Euclidean norms.\nFinally, we demonstrate numerically that the algorithm is able to solve\nhuge-scale $\\ell_1$-regularized least squares and support vector machine\nproblems with a billion variables.\n", "versions": [{"version": "v1", "created": "Thu, 14 Jul 2011 15:25:31 GMT"}], "update_date": "2011-07-15", "authors_parsed": [["Richt\u00e1rik", "Peter", ""], ["Tak\u00e1\u010d", "Martin", ""]]}, {"id": "1107.3059", "submitter": "Amin Karbasi", "authors": "Amin Karbasi, Stratis Ioannidis, Laurent Massoulie", "title": "From Small-World Networks to Comparison-Based Search", "comments": "42 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.DS cs.IT cs.SI math.IT stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The problem of content search through comparisons has recently received\nconsiderable attention. In short, a user searching for a target object\nnavigates through a database in the following manner: the user is asked to\nselect the object most similar to her target from a small list of objects. A\nnew object list is then presented to the user based on her earlier selection.\nThis process is repeated until the target is included in the list presented, at\nwhich point the search terminates. This problem is known to be strongly related\nto the small-world network design problem.\n  However, contrary to prior work, which focuses on cases where objects in the\ndatabase are equally popular, we consider here the case where the demand for\nobjects may be heterogeneous. We show that, under heterogeneous demand, the\nsmall-world network design problem is NP-hard. Given the above negative result,\nwe propose a novel mechanism for small-world design and provide an upper bound\non its performance under heterogeneous demand. The above mechanism has a\nnatural equivalent in the context of content search through comparisons, and we\nestablish both an upper bound and a lower bound for the performance of this\nmechanism. These bounds are intuitively appealing, as they depend on the\nentropy of the demand as well as its doubling constant, a quantity capturing\nthe topology of the set of target objects. They also illustrate interesting\nconnections between comparison-based search to classic results from information\ntheory. Finally, we propose an adaptive learning algorithm for content search\nthat meets the performance guarantees achieved by the above mechanisms.\n", "versions": [{"version": "v1", "created": "Fri, 15 Jul 2011 12:47:02 GMT"}, {"version": "v2", "created": "Thu, 16 Jan 2014 19:44:00 GMT"}, {"version": "v3", "created": "Mon, 10 Feb 2014 07:03:28 GMT"}], "update_date": "2014-02-11", "authors_parsed": [["Karbasi", "Amin", ""], ["Ioannidis", "Stratis", ""], ["Massoulie", "Laurent", ""]]}, {"id": "1107.3133", "submitter": "JooSeuk Kim", "authors": "JooSeuk Kim and Clayton D. Scott", "title": "Robust Kernel Density Estimation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a method for nonparametric density estimation that exhibits\nrobustness to contamination of the training sample. This method achieves\nrobustness by combining a traditional kernel density estimator (KDE) with ideas\nfrom classical $M$-estimation. We interpret the KDE based on a radial, positive\nsemi-definite kernel as a sample mean in the associated reproducing kernel\nHilbert space. Since the sample mean is sensitive to outliers, we estimate it\nrobustly via $M$-estimation, yielding a robust kernel density estimator (RKDE).\n  An RKDE can be computed efficiently via a kernelized iteratively re-weighted\nleast squares (IRWLS) algorithm. Necessary and sufficient conditions are given\nfor kernelized IRWLS to converge to the global minimizer of the $M$-estimator\nobjective function. The robustness of the RKDE is demonstrated with a\nrepresenter theorem, the influence function, and experimental results for\ndensity estimation and anomaly detection.\n", "versions": [{"version": "v1", "created": "Fri, 15 Jul 2011 19:05:48 GMT"}, {"version": "v2", "created": "Tue, 6 Sep 2011 03:18:45 GMT"}], "update_date": "2011-09-07", "authors_parsed": [["Kim", "JooSeuk", ""], ["Scott", "Clayton D.", ""]]}, {"id": "1107.3160", "submitter": "Omar Laurino", "authors": "Omar Laurino, Raffaele D'Abrusco, Giuseppe Longo, Giuseppe Riccio", "title": "Astroinformatics of galaxies and quasars: a new general method for\n  photometric redshifts estimation", "comments": "36 pages, 22 figures and 8 tables", "journal-ref": null, "doi": "10.1111/j.1365-2966.2011.19416.x", "report-no": null, "categories": "astro-ph.CO astro-ph.HE stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  With the availability of the huge amounts of data produced by current and\nfuture large multi-band photometric surveys, photometric redshifts have become\na crucial tool for extragalactic astronomy and cosmology. In this paper we\npresent a novel method, called Weak Gated Experts (WGE), which allows to derive\nphotometric redshifts through a combination of data mining techniques.\n\\noindent The WGE, like many other machine learning techniques, is based on the\nexploitation of a spectroscopic knowledge base composed by sources for which a\nspectroscopic value of the redshift is available. This method achieves a\nvariance \\sigma^2(\\Delta z)=2.3x10^{-4} (\\sigma^2(\\Delta z) =0.08), where\n\\Delta z = z_{phot} - z_{spec}) for the reconstruction of the photometric\nredshifts for the optical galaxies from the SDSS and for the optical quasars\nrespectively, while the Root Mean Square (RMS) of the \\Delta z variable\ndistributions for the two experiments is respectively equal to 0.021 and 0.35.\nThe WGE provides also a mechanism for the estimation of the accuracy of each\nphotometric redshift. We also present and discuss the catalogs obtained for the\noptical SDSS galaxies, for the optical candidate quasars extracted from the DR7\nSDSS photometric dataset {The sample of SDSS sources on which the accuracy of\nthe reconstruction has been assessed is composed of bright sources, for a\nsubset of which spectroscopic redshifts have been measured.}, and for optical\nSDSS candidate quasars observed by GALEX in the UV range. The WGE method\nexploits the new technological paradigm provided by the Virtual Observatory and\nthe emerging field of Astroinformatics.\n", "versions": [{"version": "v1", "created": "Fri, 15 Jul 2011 20:21:50 GMT"}], "update_date": "2015-05-28", "authors_parsed": [["Laurino", "Omar", ""], ["D'Abrusco", "Raffaele", ""], ["Longo", "Giuseppe", ""], ["Riccio", "Giuseppe", ""]]}, {"id": "1107.3258", "submitter": "Ali Jalali", "authors": "Ali Jalali and Chris Johnson and Pradeep Ravikumar", "title": "On Learning Discrete Graphical Models Using Greedy Methods", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG math.ST stat.ML stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we address the problem of learning the structure of a pairwise\ngraphical model from samples in a high-dimensional setting. Our first main\nresult studies the sparsistency, or consistency in sparsity pattern recovery,\nproperties of a forward-backward greedy algorithm as applied to general\nstatistical models. As a special case, we then apply this algorithm to learn\nthe structure of a discrete graphical model via neighborhood estimation. As a\ncorollary of our general result, we derive sufficient conditions on the number\nof samples n, the maximum node-degree d and the problem size p, as well as\nother conditions on the model parameters, so that the algorithm recovers all\nthe edges with high probability. Our result guarantees graph selection for\nsamples scaling as n = Omega(d^2 log(p)), in contrast to existing\nconvex-optimization based algorithms that require a sample complexity of\n\\Omega(d^3 log(p)). Further, the greedy algorithm only requires a restricted\nstrong convexity condition which is typically milder than irrepresentability\nassumptions. We corroborate these results using numerical simulations at the\nend.\n", "versions": [{"version": "v1", "created": "Sat, 16 Jul 2011 22:04:13 GMT"}], "update_date": "2012-02-28", "authors_parsed": [["Jalali", "Ali", ""], ["Johnson", "Chris", ""], ["Ravikumar", "Pradeep", ""]]}, {"id": "1107.3600", "submitter": "Oliver Kramer Oliver Kramer", "authors": "Oliver Kramer", "title": "Unsupervised K-Nearest Neighbor Regression", "comments": "4 pages, 12 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In many scientific disciplines structures in high-dimensional data have to be\nfound, e.g., in stellar spectra, in genome data, or in face recognition tasks.\nIn this work we present a novel approach to non-linear dimensionality\nreduction. It is based on fitting K-nearest neighbor regression to the\nunsupervised regression framework for learning of low-dimensional manifolds.\nSimilar to related approaches that are mostly based on kernel methods,\nunsupervised K-nearest neighbor (UNN) regression optimizes latent variables\nw.r.t. the data space reconstruction error employing the K-nearest neighbor\nheuristic. The problem of optimizing latent neighborhoods is difficult to\nsolve, but the UNN formulation allows the design of efficient strategies that\niteratively embed latent points to fixed neighborhood topologies. UNN is well\nappropriate for sorting of high-dimensional data. The iterative variants are\nanalyzed experimentally.\n", "versions": [{"version": "v1", "created": "Tue, 19 Jul 2011 00:48:41 GMT"}, {"version": "v2", "created": "Mon, 26 Sep 2011 10:02:43 GMT"}], "update_date": "2011-09-27", "authors_parsed": [["Kramer", "Oliver", ""]]}, {"id": "1107.3689", "submitter": "Taha Yasseri", "authors": "R\\'obert Sumi, Taha Yasseri, Andr\\'as Rung, Andr\\'as Kornai, J\\'anos\n  Kert\\'esz", "title": "Edit wars in Wikipedia", "comments": "4 pages, 2 figures, 3 tables. The current version is shortened to be\n  published in SocialCom 2011", "journal-ref": "Privacy, Security, Risk and Trust (PASSAT), 2011 IEEE Third\n  International Conference on and 2011 IEEE Third International Confernece on\n  Social Computing (SocialCom), 9-11 Oct. 2011, 724-727, Boston, MA, USA, ISBN:\n  978-1-4577-1931-8", "doi": "10.1109/PASSAT/SocialCom.2011.47", "report-no": null, "categories": "stat.ML cs.DL physics.data-an physics.soc-ph", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a new, efficient method for automatically detecting severe\nconflicts `edit wars' in Wikipedia and evaluate this method on six different\nlanguage WPs. We discuss how the number of edits, reverts, the length of\ndiscussions, the burstiness of edits and reverts deviate in such pages from\nthose following the general workflow, and argue that earlier work has\nsignificantly over-estimated the contentiousness of the Wikipedia editing\nprocess.\n", "versions": [{"version": "v1", "created": "Tue, 19 Jul 2011 11:57:03 GMT"}, {"version": "v2", "created": "Thu, 9 Feb 2012 17:28:06 GMT"}], "update_date": "2012-02-13", "authors_parsed": [["Sumi", "R\u00f3bert", ""], ["Yasseri", "Taha", ""], ["Rung", "Andr\u00e1s", ""], ["Kornai", "Andr\u00e1s", ""], ["Kert\u00e9sz", "J\u00e1nos", ""]]}, {"id": "1107.4067", "submitter": "Divyanshu Vats", "authors": "Divyanshu Vats, Jos\\'e M. F. Moura", "title": "Finding Non-overlapping Clusters for Generalized Inference Over\n  Graphical Models", "comments": "Extended the previous version to include extensive numerical\n  simulations. See http://www.ima.umn.edu/~dvats/GeneralizedInference.html for\n  code and data", "journal-ref": "IEEE Transactions on Signal Processing, vol. 60, no. 12, pp\n  6368-6381, Dec 2012", "doi": "10.1109/TSP.2012.2214216", "report-no": null, "categories": "stat.ML cs.IT math.IT", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Graphical models use graphs to compactly capture stochastic dependencies\namongst a collection of random variables. Inference over graphical models\ncorresponds to finding marginal probability distributions given joint\nprobability distributions. In general, this is computationally intractable,\nwhich has led to a quest for finding efficient approximate inference\nalgorithms. We propose a framework for generalized inference over graphical\nmodels that can be used as a wrapper for improving the estimates of approximate\ninference algorithms. Instead of applying an inference algorithm to the\noriginal graph, we apply the inference algorithm to a block-graph, defined as a\ngraph in which the nodes are non-overlapping clusters of nodes from the\noriginal graph. This results in marginal estimates of a cluster of nodes, which\nwe further marginalize to get the marginal estimates of each node. Our proposed\nblock-graph construction algorithm is simple, efficient, and motivated by the\nobservation that approximate inference is more accurate on graphs with longer\ncycles. We present extensive numerical simulations that illustrate our\nblock-graph framework with a variety of inference algorithms (e.g., those in\nthe libDAI software package). These simulations show the improvements provided\nby our framework.\n", "versions": [{"version": "v1", "created": "Wed, 20 Jul 2011 18:55:03 GMT"}, {"version": "v2", "created": "Sun, 18 Mar 2012 03:28:41 GMT"}], "update_date": "2013-04-02", "authors_parsed": [["Vats", "Divyanshu", ""], ["Moura", "Jos\u00e9 M. F.", ""]]}, {"id": "1107.4340", "submitter": "Daniel McDonald", "authors": "Darren Homrighausen and Daniel J. McDonald", "title": "Spectral approximations in machine learning", "comments": "11 pages, 4 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In many areas of machine learning, it becomes necessary to find the\neigenvector decompositions of large matrices. We discuss two methods for\nreducing the computational burden of spectral decompositions: the more\nvenerable Nystom extension and a newly introduced algorithm based on random\nprojections. Previous work has centered on the ability to reconstruct the\noriginal matrix. We argue that a more interesting and relevant comparison is\ntheir relative performance in clustering and classification tasks using the\napproximate eigenvectors as features. We demonstrate that performance is task\nspecific and depends on the rank of the approximation.\n", "versions": [{"version": "v1", "created": "Thu, 21 Jul 2011 18:44:51 GMT"}], "update_date": "2011-07-22", "authors_parsed": [["Homrighausen", "Darren", ""], ["McDonald", "Daniel J.", ""]]}, {"id": "1107.4390", "submitter": "Sergey Feldman", "authors": "Sergey Feldman, Bela A. Frigyik, Maya R. Gupta", "title": "Multi-Task Averaging", "comments": "totally redone paper", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a multi-task learning approach to jointly estimate the means of\nmultiple independent data sets. The proposed multi-task averaging (MTA)\nalgorithm results in a convex combination of the single-task maximum likelihood\nestimates. We derive the optimal minimum risk estimator and the minimax\nestimator, and show that these estimators can be efficiently estimated.\nSimulations and real data experiments demonstrate that MTA estimators often\noutperform both single-task and James-Stein estimators.\n", "versions": [{"version": "v1", "created": "Thu, 21 Jul 2011 22:10:22 GMT"}, {"version": "v2", "created": "Mon, 25 Jul 2011 17:46:38 GMT"}, {"version": "v3", "created": "Wed, 27 Jul 2011 19:09:36 GMT"}, {"version": "v4", "created": "Fri, 24 Aug 2012 22:35:38 GMT"}], "update_date": "2015-03-19", "authors_parsed": [["Feldman", "Sergey", ""], ["Frigyik", "Bela A.", ""], ["Gupta", "Maya R.", ""]]}, {"id": "1107.4506", "submitter": "Antoine Salomon", "authors": "Antoine Salomon, Jean-Yves Audibert", "title": "Robustness of Anytime Bandit Policies", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper studies the deviations of the regret in a stochastic multi-armed\nbandit problem. When the total number of plays n is known beforehand by the\nagent, Audibert et al. (2009) exhibit a policy such that with probability at\nleast 1-1/n, the regret of the policy is of order log(n). They have also shown\nthat such a property is not shared by the popular ucb1 policy of Auer et al.\n(2002). This work first answers an open question: it extends this negative\nresult to any anytime policy. The second contribution of this paper is to\ndesign anytime robust policies for specific multi-armed bandit problems in\nwhich some restrictions are put on the set of possible distributions of the\ndifferent arms.\n", "versions": [{"version": "v1", "created": "Fri, 22 Jul 2011 12:55:34 GMT"}, {"version": "v2", "created": "Mon, 25 Jul 2011 12:52:17 GMT"}], "update_date": "2011-07-26", "authors_parsed": [["Salomon", "Antoine", ""], ["Audibert", "Jean-Yves", ""]]}, {"id": "1107.4623", "submitter": "Sohail Bahmani", "authors": "Sohail Bahmani and Bhiksha Raj", "title": "A Unifying Analysis of Projected Gradient Descent for\n  $\\ell_p$-constrained Least Squares", "comments": "16 pages, 3 Figures", "journal-ref": "Applied and Computational Harmonic Analysis, 34(3):366-378, 2013", "doi": "10.1016/j.acha.2012.07.004", "report-no": null, "categories": "math.NA cs.IT math.IT math.OC stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper we study the performance of the Projected Gradient Descent(PGD)\nalgorithm for $\\ell_{p}$-constrained least squares problems that arise in the\nframework of Compressed Sensing. Relying on the Restricted Isometry Property,\nwe provide convergence guarantees for this algorithm for the entire range of\n$0\\leq p\\leq1$, that include and generalize the existing results for the\nIterative Hard Thresholding algorithm and provide a new accuracy guarantee for\nthe Iterative Soft Thresholding algorithm as special cases. Our results suggest\nthat in this group of algorithms, as $p$ increases from zero to one, conditions\nrequired to guarantee accuracy become stricter and robustness to noise\ndeteriorates.\n", "versions": [{"version": "v1", "created": "Fri, 22 Jul 2011 20:42:47 GMT"}, {"version": "v2", "created": "Thu, 4 Aug 2011 17:20:46 GMT"}, {"version": "v3", "created": "Sat, 6 Aug 2011 18:09:10 GMT"}, {"version": "v4", "created": "Mon, 5 Mar 2012 12:24:12 GMT"}, {"version": "v5", "created": "Tue, 26 Jun 2012 18:00:36 GMT"}], "update_date": "2013-03-05", "authors_parsed": [["Bahmani", "Sohail", ""], ["Raj", "Bhiksha", ""]]}, {"id": "1107.4637", "submitter": "George Papandreou", "authors": "George Papandreou and Alan Yuille", "title": "Efficient variational inference in large-scale Bayesian compressed\n  sensing", "comments": "8 pages, 3 figures, appears in Proc. IEEE Workshop on Information\n  Theory in Computer Vision and Pattern Recognition (in conjunction with\n  ICCV-11), Barcelona, Spain, Nov. 2011", "journal-ref": "Proc. IEEE Workshop on Information Theory in Computer Vision and\n  Pattern Recognition (in conjunction with ICCV-11), pp. 1332-1339, Barcelona,\n  Spain, Nov. 2011", "doi": "10.1109/ICCVW.2011.6130406", "report-no": null, "categories": "cs.CV cs.IT math.IT stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study linear models under heavy-tailed priors from a probabilistic\nviewpoint. Instead of computing a single sparse most probable (MAP) solution as\nin standard deterministic approaches, the focus in the Bayesian compressed\nsensing framework shifts towards capturing the full posterior distribution on\nthe latent variables, which allows quantifying the estimation uncertainty and\nlearning model parameters using maximum likelihood. The exact posterior\ndistribution under the sparse linear model is intractable and we concentrate on\nvariational Bayesian techniques to approximate it. Repeatedly computing\nGaussian variances turns out to be a key requisite and constitutes the main\ncomputational bottleneck in applying variational techniques in large-scale\nproblems. We leverage on the recently proposed Perturb-and-MAP algorithm for\ndrawing exact samples from Gaussian Markov random fields (GMRF). The main\ntechnical contribution of our paper is to show that estimating Gaussian\nvariances using a relatively small number of such efficiently drawn random\nsamples is much more effective than alternative general-purpose variance\nestimation techniques. By reducing the problem of variance estimation to\nstandard optimization primitives, the resulting variational algorithms are\nfully scalable and parallelizable, allowing Bayesian computations in extremely\nlarge-scale problems with the same memory and time complexity requirements as\nconventional point estimation techniques. We illustrate these ideas with\nexperiments in image deblurring.\n", "versions": [{"version": "v1", "created": "Fri, 22 Jul 2011 22:10:38 GMT"}, {"version": "v2", "created": "Mon, 5 Sep 2011 02:04:00 GMT"}], "update_date": "2014-03-05", "authors_parsed": [["Papandreou", "George", ""], ["Yuille", "Alan", ""]]}, {"id": "1107.4976", "submitter": "Artin Armagan", "authors": "Artin Armagan, David B. Dunson and Merlise Clyde", "title": "Generalized Beta Mixtures of Gaussians", "comments": "Advances in Neural Information Processing Systems 24 edited by J.\n  Shawe-Taylor and R.S. Zemel and P. Bartlett and F. Pereira and K.Q.\n  Weinberger (2011)", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In recent years, a rich variety of shrinkage priors have been proposed that\nhave great promise in addressing massive regression problems. In general, these\nnew priors can be expressed as scale mixtures of normals, but have more complex\nforms and better properties than traditional Cauchy and double exponential\npriors. We first propose a new class of normal scale mixtures through a novel\ngeneralized beta distribution that encompasses many interesting priors as\nspecial cases. This encompassing framework should prove useful in comparing\ncompeting priors, considering properties and revealing close connections. We\nthen develop a class of variational Bayes approximations through the new\nhierarchy presented that will scale more efficiently to the types of truly\nmassive data sets that are now encountered routinely.\n", "versions": [{"version": "v1", "created": "Mon, 25 Jul 2011 15:21:06 GMT"}, {"version": "v2", "created": "Tue, 13 Mar 2012 21:22:21 GMT"}], "update_date": "2012-03-15", "authors_parsed": [["Armagan", "Artin", ""], ["Dunson", "David B.", ""], ["Clyde", "Merlise", ""]]}, {"id": "1107.4985", "submitter": "Andreas Damianou Mr", "authors": "Andreas C. Damianou, Michalis K. Titsias, Neil D. Lawrence", "title": "Variational Gaussian Process Dynamical Systems", "comments": "16 pages, 19 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.AI cs.CV math.PR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  High dimensional time series are endemic in applications of machine learning\nsuch as robotics (sensor data), computational biology (gene expression data),\nvision (video sequences) and graphics (motion capture data). Practical\nnonlinear probabilistic approaches to this data are required. In this paper we\nintroduce the variational Gaussian process dynamical system. Our work builds on\nrecent variational approximations for Gaussian process latent variable models\nto allow for nonlinear dimensionality reduction simultaneously with learning a\ndynamical prior in the latent space. The approach also allows for the\nappropriate dimensionality of the latent space to be automatically determined.\nWe demonstrate the model on a human motion capture data set and a series of\nhigh resolution video sequences.\n", "versions": [{"version": "v1", "created": "Mon, 25 Jul 2011 15:54:05 GMT"}], "update_date": "2011-07-26", "authors_parsed": [["Damianou", "Andreas C.", ""], ["Titsias", "Michalis K.", ""], ["Lawrence", "Neil D.", ""]]}, {"id": "1107.5959", "submitter": "Simon Barthelm\\'e", "authors": "Simon Barthelm\\'e, Nicolas Chopin", "title": "Expectation-Propagation for Likelihood-Free Inference", "comments": "Revised version following peer-review", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.CO stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Many models of interest in the natural and social sciences have no\nclosed-form likelihood function, which means that they cannot be treated using\nthe usual techniques of statistical inference. In the case where such models\ncan be efficiently simulated, Bayesian inference is still possible thanks to\nthe Approximate Bayesian Computation (ABC) algorithm. Although many refinements\nhave been suggested, ABC inference is still far from routine. ABC is often\nexcruciatingly slow due to very low acceptance rates. In addition, ABC requires\nintroducing a vector of \"summary statistics\", the choice of which is relatively\narbitrary, and often require some trial and error, making the whole process\nquite laborious for the user.\n  We introduce in this work the EP-ABC algorithm, which is an adaptation to the\nlikelihood-free context of the variational approximation algorithm known as\nExpectation Propagation (Minka, 2001). The main advantage of EP-ABC is that it\nis faster by a few orders of magnitude than standard algorithms, while\nproducing an overall approximation error which is typically negligible. A\nsecond advantage of EP-ABC is that it replaces the usual global ABC constraint\non the vector of summary statistics computed on the whole dataset, by n local\nconstraints of the form that apply separately to each data-point. As a\nconsequence, it is often possible to do away with summary statistics entirely.\nIn that case, EP-ABC approximates directly the evidence (marginal likelihood)\nof the model.\n  Comparisons are performed in three real-world applications which are typical\nof likelihood-free inference, including one application in neuroscience which\nis novel, and possibly too challenging for standard ABC techniques.\n", "versions": [{"version": "v1", "created": "Fri, 29 Jul 2011 13:15:50 GMT"}, {"version": "v2", "created": "Wed, 18 Jul 2012 08:47:33 GMT"}], "update_date": "2012-07-19", "authors_parsed": [["Barthelm\u00e9", "Simon", ""], ["Chopin", "Nicolas", ""]]}, {"id": "1107.6027", "submitter": "Jiantao Jiao", "authors": "Jiantao Jiao, Lin Zhang and Robert Nowak", "title": "Minimax-Optimal Bounds for Detectors Based on Estimated Prior\n  Probabilities", "comments": "Submitted to IEEE Transactions on Information Theory", "journal-ref": null, "doi": "10.1109/TIT.2012.2201914", "report-no": null, "categories": "cs.IT math.IT stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In many signal detection and classification problems, we have knowledge of\nthe distribution under each hypothesis, but not the prior probabilities. This\npaper is aimed at providing theory to quantify the performance of detection via\nestimating prior probabilities from either labeled or unlabeled training data.\nThe error or {\\em risk} is considered as a function of the prior probabilities.\nWe show that the risk function is locally Lipschitz in the vicinity of the true\nprior probabilities, and the error of detectors based on estimated prior\nprobabilities depends on the behavior of the risk function in this locality. In\ngeneral, we show that the error of detectors based on the Maximum Likelihood\nEstimate (MLE) of the prior probabilities converges to the Bayes error at a\nrate of $n^{-1/2}$, where $n$ is the number of training data. If the behavior\nof the risk function is more favorable, then detectors based on the MLE have\nerrors converging to the corresponding Bayes errors at optimal rates of the\nform $n^{-(1+\\alpha)/2}$, where $\\alpha>0$ is a parameter governing the\nbehavior of the risk function with a typical value $\\alpha = 1$. The limit\n$\\alpha \\rightarrow \\infty$ corresponds to a situation where the risk function\nis flat near the true probabilities, and thus insensitive to small errors in\nthe MLE; in this case the error of the detector based on the MLE converges to\nthe Bayes error exponentially fast with $n$. We show the bounds are achievable\nno matter given labeled or unlabeled training data and are minimax-optimal in\nlabeled case.\n", "versions": [{"version": "v1", "created": "Fri, 29 Jul 2011 17:33:59 GMT"}, {"version": "v2", "created": "Tue, 28 Feb 2012 01:13:43 GMT"}], "update_date": "2016-11-17", "authors_parsed": [["Jiao", "Jiantao", ""], ["Zhang", "Lin", ""], ["Nowak", "Robert", ""]]}]