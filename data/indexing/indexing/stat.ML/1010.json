[{"id": "1010.0535", "submitter": "Robert Hable", "authors": "Robert Hable", "title": "Asymptotic Normality of Support Vector Machine Variants and Other\n  Regularized Kernel Methods", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In nonparametric classification and regression problems, regularized kernel\nmethods, in particular support vector machines, attract much attention in\ntheoretical and in applied statistics. In an abstract sense, regularized kernel\nmethods (simply called SVMs here) can be seen as regularized M-estimators for a\nparameter in a (typically infinite dimensional) reproducing kernel Hilbert\nspace. For smooth loss functions, it is shown that the difference between the\nestimator, i.e.\\ the empirical SVM, and the theoretical SVM is asymptotically\nnormal with rate $\\sqrt{n}$. That is, the standardized difference converges\nweakly to a Gaussian process in the reproducing kernel Hilbert space. As common\nin real applications, the choice of the regularization parameter may depend on\nthe data. The proof is done by an application of the functional delta-method\nand by showing that the SVM-functional is suitably Hadamard-differentiable.\n", "versions": [{"version": "v1", "created": "Mon, 4 Oct 2010 10:46:32 GMT"}, {"version": "v2", "created": "Tue, 5 Oct 2010 06:08:38 GMT"}, {"version": "v3", "created": "Tue, 12 Apr 2011 07:08:05 GMT"}], "update_date": "2011-04-13", "authors_parsed": [["Hable", "Robert", ""]]}, {"id": "1010.0556", "submitter": "Jean Manuel Morales", "authors": "Charles A. Micchelli, Jean M. Morales, Massimiliano Pontil", "title": "Regularizers for Structured Sparsity", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study the problem of learning a sparse linear regression vector under\nadditional conditions on the structure of its sparsity pattern. This problem is\nrelevant in machine learning, statistics and signal processing. It is well\nknown that a linear regression can benefit from knowledge that the underlying\nregression vector is sparse. The combinatorial problem of selecting the nonzero\ncomponents of this vector can be \"relaxed\" by regularizing the squared error\nwith a convex penalty function like the $\\ell_1$ norm. However, in many\napplications, additional conditions on the structure of the regression vector\nand its sparsity pattern are available. Incorporating this information into the\nlearning method may lead to a significant decrease of the estimation error. In\nthis paper, we present a family of convex penalty functions, which encode prior\nknowledge on the structure of the vector formed by the absolute values of the\nregression coefficients. This family subsumes the $\\ell_1$ norm and is flexible\nenough to include different models of sparsity patterns, which are of practical\nand theoretical importance. We establish the basic properties of these penalty\nfunctions and discuss some examples where they can be computed explicitly.\nMoreover, we present a convergent optimization algorithm for solving\nregularized least squares with these penalty functions. Numerical simulations\nhighlight the benefit of structured sparsity and the advantage offered by our\napproach over the Lasso method and other related methods.\n", "versions": [{"version": "v1", "created": "Mon, 4 Oct 2010 12:04:44 GMT"}, {"version": "v2", "created": "Wed, 30 Mar 2011 11:24:17 GMT"}], "update_date": "2015-03-17", "authors_parsed": [["Micchelli", "Charles A.", ""], ["Morales", "Jean M.", ""], ["Pontil", "Massimiliano", ""]]}, {"id": "1010.0621", "submitter": "Shuanghong Yang", "authors": "Shuang Hong Yang", "title": "Local Optimality of User Choices and Collaborative Competitive Filtering", "comments": "27 pages, 4 figure", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.IR cs.SI stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  While a user's preference is directly reflected in the interactive choice\nprocess between her and the recommender, this wealth of information was not\nfully exploited for learning recommender models. In particular, existing\ncollaborative filtering (CF) approaches take into account only the binary\nevents of user actions but totally disregard the contexts in which users'\ndecisions are made. In this paper, we propose Collaborative Competitive\nFiltering (CCF), a framework for learning user preferences by modeling the\nchoice process in recommender systems. CCF employs a multiplicative latent\nfactor model to characterize the dyadic utility function. But unlike CF, CCF\nmodels the user behavior of choices by encoding a local competition effect. In\nthis way, CCF allows us to leverage dyadic data that was previously lumped\ntogether with missing data in existing CF models. We present two formulations\nand an efficient large scale optimization algorithm. Experiments on three\nreal-world recommendation data sets demonstrate that CCF significantly\noutperforms standard CF approaches in both offline and online evaluations.\n", "versions": [{"version": "v1", "created": "Mon, 4 Oct 2010 15:29:33 GMT"}, {"version": "v2", "created": "Fri, 25 Feb 2011 21:37:16 GMT"}], "update_date": "2011-03-01", "authors_parsed": [["Yang", "Shuang Hong", ""]]}, {"id": "1010.0703", "submitter": "Michael Mahoney", "authors": "Michael W. Mahoney and Lorenzo Orecchia", "title": "Implementing regularization implicitly via approximate eigenvector\n  computation", "comments": "11 pages; a few clarifications", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS stat.CO stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Regularization is a powerful technique for extracting useful information from\nnoisy data. Typically, it is implemented by adding some sort of norm constraint\nto an objective function and then exactly optimizing the modified objective\nfunction. This procedure often leads to optimization problems that are\ncomputationally more expensive than the original problem, a fact that is\nclearly problematic if one is interested in large-scale applications. On the\nother hand, a large body of empirical work has demonstrated that heuristics,\nand in some cases approximation algorithms, developed to speed up computations\nsometimes have the side-effect of performing regularization implicitly. Thus,\nwe consider the question: What is the regularized optimization objective that\nan approximation algorithm is exactly optimizing?\n  We address this question in the context of computing approximations to the\nsmallest nontrivial eigenvector of a graph Laplacian; and we consider three\nrandom-walk-based procedures: one based on the heat kernel of the graph, one\nbased on computing the the PageRank vector associated with the graph, and one\nbased on a truncated lazy random walk. In each case, we provide a precise\ncharacterization of the manner in which the approximation method can be viewed\nas implicitly computing the exact solution to a regularized problem.\nInterestingly, the regularization is not on the usual vector form of the\noptimization problem, but instead it is on a related semidefinite program.\n", "versions": [{"version": "v1", "created": "Mon, 4 Oct 2010 20:49:15 GMT"}, {"version": "v2", "created": "Wed, 27 Apr 2011 03:52:25 GMT"}], "update_date": "2011-04-28", "authors_parsed": [["Mahoney", "Michael W.", ""], ["Orecchia", "Lorenzo", ""]]}, {"id": "1010.0772", "submitter": "Fantine Mordelet", "authors": "Fantine Mordelet (CBIO), Jean-Philippe Vert (CBIO)", "title": "A bagging SVM to learn from positive and unlabeled examples", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider the problem of learning a binary classifier from a training set\nof positive and unlabeled examples, both in the inductive and in the\ntransductive setting. This problem, often referred to as \\emph{PU learning},\ndiffers from the standard supervised classification problem by the lack of\nnegative examples in the training set. It corresponds to an ubiquitous\nsituation in many applications such as information retrieval or gene ranking,\nwhen we have identified a set of data of interest sharing a particular\nproperty, and we wish to automatically retrieve additional data sharing the\nsame property among a large and easily available pool of unlabeled data. We\npropose a conceptually simple method, akin to bagging, to approach both\ninductive and transductive PU learning problems, by converting them into series\nof supervised binary classification problems discriminating the known positive\nexamples from random subsamples of the unlabeled set. We empirically\ndemonstrate the relevance of the method on simulated and real data, where it\nperforms at least as well as existing methods while being faster.\n", "versions": [{"version": "v1", "created": "Tue, 5 Oct 2010 06:03:09 GMT"}], "update_date": "2010-10-06", "authors_parsed": [["Mordelet", "Fantine", "", "CBIO"], ["Vert", "Jean-Philippe", "", "CBIO"]]}, {"id": "1010.0789", "submitter": "Ryota Tomioka", "authors": "Ryota Tomioka, Kohei Hayashi, Hisashi Kashima", "title": "Estimation of low-rank tensors via convex optimization", "comments": "19 pages, 7 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML math.NA", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we propose three approaches for the estimation of the Tucker\ndecomposition of multi-way arrays (tensors) from partial observations. All\napproaches are formulated as convex minimization problems. Therefore, the\nminimum is guaranteed to be unique. The proposed approaches can automatically\nestimate the number of factors (rank) through the optimization. Thus, there is\nno need to specify the rank beforehand. The key technique we employ is the\ntrace norm regularization, which is a popular approach for the estimation of\nlow-rank matrices. In addition, we propose a simple heuristic to improve the\ninterpretability of the obtained factorization. The advantages and\ndisadvantages of three proposed approaches are demonstrated through numerical\nexperiments on both synthetic and real world datasets. We show that the\nproposed convex optimization based approaches are more accurate in predictive\nperformance, faster, and more reliable in recovering a known multilinear\nstructure than conventional approaches.\n", "versions": [{"version": "v1", "created": "Tue, 5 Oct 2010 08:00:33 GMT"}, {"version": "v2", "created": "Wed, 2 Mar 2011 08:12:24 GMT"}], "update_date": "2015-03-17", "authors_parsed": [["Tomioka", "Ryota", ""], ["Hayashi", "Kohei", ""], ["Kashima", "Hisashi", ""]]}, {"id": "1010.1437", "submitter": "Mahdi Shafiei", "authors": "Mahdi Shafiei and Hugh Chipman", "title": "Mixed-Membership Stochastic Block-Models for Transactional Networks", "comments": "22 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.AI cs.SI stat.AP stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Transactional network data can be thought of as a list of one-to-many\ncommunications(e.g., email) between nodes in a social network. Most social\nnetwork models convert this type of data into binary relations between pairs of\nnodes. We develop a latent mixed membership model capable of modeling richer\nforms of transactional network data, including relations between more than two\nnodes. The model can cluster nodes and predict transactions. The block-model\nnature of the model implies that groups can be characterized in very general\nways. This flexible notion of group structure enables discovery of rich\nstructure in transactional networks. Estimation and inference are accomplished\nvia a variational EM algorithm. Simulations indicate that the learning\nalgorithm can recover the correct generative model. Interesting structure is\ndiscovered in the Enron email dataset and another dataset extracted from the\nReddit website. Analysis of the Reddit data is facilitated by a novel\nperformance measure for comparing two soft clusterings. The new model is\nsuperior at discovering mixed membership in groups and in predicting\ntransactions.\n", "versions": [{"version": "v1", "created": "Thu, 7 Oct 2010 14:16:38 GMT"}], "update_date": "2010-10-08", "authors_parsed": [["Shafiei", "Mahdi", ""], ["Chipman", "Hugh", ""]]}, {"id": "1010.1609", "submitter": "Michael Mahoney", "authors": "Michael W. Mahoney", "title": "Algorithmic and Statistical Perspectives on Large-Scale Data Analysis", "comments": "33 pages. To appear in Uwe Naumann and Olaf Schenk, editors,\n  \"Combinatorial Scientific Computing,\" Chapman and Hall/CRC Press, 2011", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS stat.CO stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In recent years, ideas from statistics and scientific computing have begun to\ninteract in increasingly sophisticated and fruitful ways with ideas from\ncomputer science and the theory of algorithms to aid in the development of\nimproved worst-case algorithms that are useful for large-scale scientific and\nInternet data analysis problems. In this chapter, I will describe two recent\nexamples---one having to do with selecting good columns or features from a (DNA\nSingle Nucleotide Polymorphism) data matrix, and the other having to do with\nselecting good clusters or communities from a data graph (representing a social\nor information network)---that drew on ideas from both areas and that may serve\nas a model for exploiting complementary algorithmic and statistical\nperspectives in order to solve applied large-scale data analysis problems.\n", "versions": [{"version": "v1", "created": "Fri, 8 Oct 2010 07:02:11 GMT"}], "update_date": "2010-10-11", "authors_parsed": [["Mahoney", "Michael W.", ""]]}, {"id": "1010.1868", "submitter": "Qirong Ho", "authors": "Qirong Ho, Ankur P. Parikh, Le Song and Eric P. Xing", "title": "Infinite Hierarchical MMSB Model for Nested Communities/Groups in Social\n  Networks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Actors in realistic social networks play not one but a number of diverse\nroles depending on whom they interact with, and a large number of such\nrole-specific interactions collectively determine social communities and their\norganizations. Methods for analyzing social networks should capture these\nmulti-faceted role-specific interactions, and, more interestingly, discover the\nlatent organization or hierarchy of social communities. We propose a\nhierarchical Mixed Membership Stochastic Blockmodel to model the generation of\nhierarchies in social communities, selective membership of actors to subsets of\nthese communities, and the resultant networks due to within- and\ncross-community interactions. Furthermore, to automatically discover these\nlatent structures from social networks, we develop a Gibbs sampling algorithm\nfor our model. We conduct extensive validation of our model using synthetic\nnetworks, and demonstrate the utility of our model in real-world datasets such\nas predator-prey networks and citation networks.\n", "versions": [{"version": "v1", "created": "Sat, 9 Oct 2010 19:43:56 GMT"}], "update_date": "2010-10-12", "authors_parsed": [["Ho", "Qirong", ""], ["Parikh", "Ankur P.", ""], ["Song", "Le", ""], ["Xing", "Eric P.", ""]]}, {"id": "1010.2457", "submitter": "Yohann de Castro", "authors": "Yohann de Castro (LM-Orsay)", "title": "Optimal designs for Lasso and Dantzig selector using Expander Codes", "comments": "Last version with optimal bounds", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST cs.IT math.IT math.PR stat.ME stat.ML stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We investigate the high-dimensional regression problem using adjacency\nmatrices of unbalanced expander graphs. In this frame, we prove that the\n$\\ell_{2}$-prediction error and the $\\ell_{1}$-risk of the lasso and the\nDantzig selector are optimal up to an explicit multiplicative constant. Thus we\ncan estimate a high-dimensional target vector with an error term similar to the\none obtained in a situation where one knows the support of the largest\ncoordinates in advance.\n  Moreover, we show that these design matrices have an explicit restricted\neigenvalue. Precisely, they satisfy the restricted eigenvalue assumption and\nthe compatibility condition with an explicit constant.\n  Eventually, we capitalize on the recent construction of unbalanced expander\ngraphs due to Guruswami, Umans, and Vadhan, to provide a deterministic\npolynomial time construction of these design matrices.\n", "versions": [{"version": "v1", "created": "Tue, 12 Oct 2010 18:03:23 GMT"}, {"version": "v2", "created": "Fri, 5 Nov 2010 10:43:31 GMT"}, {"version": "v3", "created": "Thu, 18 Nov 2010 09:19:54 GMT"}, {"version": "v4", "created": "Thu, 14 Apr 2011 09:57:04 GMT"}, {"version": "v5", "created": "Wed, 19 Jun 2013 15:07:26 GMT"}, {"version": "v6", "created": "Tue, 22 Jul 2014 08:56:44 GMT"}], "update_date": "2015-03-17", "authors_parsed": [["de Castro", "Yohann", "", "LM-Orsay"]]}, {"id": "1010.2770", "submitter": "Andr\\'e Filipe Torres Martins", "authors": "Andre F.T. Martins, Mario A. T. Figueiredo, Pedro M. Q. Aguiar, Noah\n  A. Smith, Eric P. Xing", "title": "Online Multiple Kernel Learning for Structured Prediction", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Despite the recent progress towards efficient multiple kernel learning (MKL),\nthe structured output case remains an open research front. Current approaches\ninvolve repeatedly solving a batch learning problem, which makes them\ninadequate for large scale scenarios. We propose a new family of online\nproximal algorithms for MKL (as well as for group-lasso and variants thereof),\nwhich overcomes that drawback. We show regret, convergence, and generalization\nbounds for the proposed method. Experiments on handwriting recognition and\ndependency parsing testify for the successfulness of the approach.\n", "versions": [{"version": "v1", "created": "Wed, 13 Oct 2010 20:48:30 GMT"}], "update_date": "2010-10-15", "authors_parsed": [["Martins", "Andre F. T.", ""], ["Figueiredo", "Mario A. T.", ""], ["Aguiar", "Pedro M. Q.", ""], ["Smith", "Noah A.", ""], ["Xing", "Eric P.", ""]]}, {"id": "1010.3320", "submitter": "Rina Foygel", "authors": "Rina Foygel and Mathias Drton", "title": "Exact block-wise optimization in group lasso and sparse group lasso for\n  linear regression", "comments": "We have been made aware of the earlier work by Puig et al. (2009)\n  which derives the same result for the (non-sparse) group lasso setting. We\n  leave this manuscript available as a technical report, to serve as a\n  reference for the previously untreated sparse group lasso case, and for\n  timing comparisons of various methods in the group lasso setting. The\n  manuscript is updated to include this reference", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The group lasso is a penalized regression method, used in regression problems\nwhere the covariates are partitioned into groups to promote sparsity at the\ngroup level. Existing methods for finding the group lasso estimator either use\ngradient projection methods to update the entire coefficient vector\nsimultaneously at each step, or update one group of coefficients at a time\nusing an inexact line search to approximate the optimal value for the group of\ncoefficients when all other groups' coefficients are fixed. We present a new\nmethod of computation for the group lasso in the linear regression case, the\nSingle Line Search (SLS) algorithm, which operates by computing the exact\noptimal value for each group (when all other coefficients are fixed) with one\nunivariate line search. We perform simulations demonstrating that the SLS\nalgorithm is often more efficient than existing computational methods. We also\nextend the SLS algorithm to the sparse group lasso problem via the Signed\nSingle Line Search (SSLS) algorithm, and give theoretical results to support\nboth algorithms.\n", "versions": [{"version": "v1", "created": "Sat, 16 Oct 2010 05:15:50 GMT"}, {"version": "v2", "created": "Thu, 11 Nov 2010 20:05:04 GMT"}], "update_date": "2010-11-12", "authors_parsed": [["Foygel", "Rina", ""], ["Drton", "Mathias", ""]]}, {"id": "1010.3460", "submitter": "Gilad Lerman Dr", "authors": "Teng Zhang, Arthur Szlam, Yi Wang, Gilad Lerman", "title": "Hybrid Linear Modeling via Local Best-fit Flats", "comments": "This version adds some clarifications and numerical experiments as\n  well as strengthens the previous theorem. For face experiments, we use here\n  the Extended Yale Face Database B (cropped faces unlike previous version).\n  This database points to a failure mode of our algorithms, but we suggest and\n  successfully test a workaround", "journal-ref": "International Journal of Computer Vision Volume 100, Issue 3\n  (2012), Page 217-240", "doi": "10.1007/s11263-012-0535-6", "report-no": null, "categories": "cs.CV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a simple and fast geometric method for modeling data by a union of\naffine subspaces. The method begins by forming a collection of local best-fit\naffine subspaces, i.e., subspaces approximating the data in local\nneighborhoods. The correct sizes of the local neighborhoods are determined\nautomatically by the Jones' $\\beta_2$ numbers (we prove under certain geometric\nconditions that our method finds the optimal local neighborhoods). The\ncollection of subspaces is further processed by a greedy selection procedure or\na spectral method to generate the final model. We discuss applications to\ntracking-based motion segmentation and clustering of faces under different\nilluminating conditions. We give extensive experimental evidence demonstrating\nthe state of the art accuracy and speed of the suggested algorithms on these\nproblems and also on synthetic hybrid linear data as well as the MNIST\nhandwritten digits data; and we demonstrate how to use our algorithms for fast\ndetermination of the number of affine subspaces.\n", "versions": [{"version": "v1", "created": "Sun, 17 Oct 2010 23:27:35 GMT"}, {"version": "v2", "created": "Tue, 1 May 2012 21:26:48 GMT"}], "update_date": "2012-10-08", "authors_parsed": [["Zhang", "Teng", ""], ["Szlam", "Arthur", ""], ["Wang", "Yi", ""], ["Lerman", "Gilad", ""]]}, {"id": "1010.3812", "submitter": "Purushottam Kar", "authors": "Aman Dhesi and Purushottam Kar", "title": "Random Projection Trees Revisited", "comments": "Accepted for publication at NIPS 2010. This version corrects an\n  incorrect usage of the term Assouad dimension - acknowledgments : James Lee", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.CG math.DG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The Random Projection Tree structures proposed in [Freund-Dasgupta STOC08]\nare space partitioning data structures that automatically adapt to various\nnotions of intrinsic dimensionality of data. We prove new results for both the\nRPTreeMax and the RPTreeMean data structures. Our result for RPTreeMax gives a\nnear-optimal bound on the number of levels required by this data structure to\nreduce the size of its cells by a factor $s \\geq 2$. We also prove a packing\nlemma for this data structure. Our final result shows that low-dimensional\nmanifolds have bounded Local Covariance Dimension. As a consequence we show\nthat RPTreeMean adapts to manifold dimension as well.\n", "versions": [{"version": "v1", "created": "Tue, 19 Oct 2010 06:53:46 GMT"}, {"version": "v2", "created": "Wed, 20 Oct 2010 08:44:58 GMT"}], "update_date": "2012-11-01", "authors_parsed": [["Dhesi", "Aman", ""], ["Kar", "Purushottam", ""]]}, {"id": "1010.4207", "submitter": "Francis Bach", "authors": "Francis Bach (INRIA Rocquencourt, LIENS)", "title": "Convex Analysis and Optimization with Submodular Functions: a Tutorial", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG math.OC stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Set-functions appear in many areas of computer science and applied\nmathematics, such as machine learning, computer vision, operations research or\nelectrical networks. Among these set-functions, submodular functions play an\nimportant role, similar to convex functions on vector spaces. In this tutorial,\nthe theory of submodular functions is presented, in a self-contained way, with\nall results shown from first principles. A good knowledge of convex analysis is\nassumed.\n", "versions": [{"version": "v1", "created": "Wed, 20 Oct 2010 14:02:21 GMT"}, {"version": "v2", "created": "Sun, 14 Nov 2010 17:19:42 GMT"}], "update_date": "2010-11-17", "authors_parsed": [["Bach", "Francis", "", "INRIA Rocquencourt, LIENS"]]}, {"id": "1010.4236", "submitter": "Leonid Perlovsky", "authors": "Leonid I. Perlovsky and Ross W. Deming", "title": "Maximum Likelihood Joint Tracking and Association in a Strong Clutter\n  without Combinatorial Complexity", "comments": "13 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We have developed an efficient algorithm for the maximum likelihood joint\ntracking and association problem in a strong clutter for GMTI data. By using an\niterative procedure of the dynamic logic process \"from vague-to-crisp,\" the new\ntracker overcomes combinatorial complexity of tracking in highly-cluttered\nscenarios and results in a significant improvement in signal-to-clutter ratio.\n", "versions": [{"version": "v1", "created": "Wed, 20 Oct 2010 16:03:40 GMT"}], "update_date": "2010-10-21", "authors_parsed": [["Perlovsky", "Leonid I.", ""], ["Deming", "Ross W.", ""]]}, {"id": "1010.4237", "submitter": "Constantine Caramanis", "authors": "Huan Xu, Constantine Caramanis and Sujay Sanghavi", "title": "Robust PCA via Outlier Pursuit", "comments": "26 pages, appeared in NIPS 2010. v2 has typos corrected, some\n  re-writing. results essentially unchanged", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.IT math.IT stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Singular Value Decomposition (and Principal Component Analysis) is one of the\nmost widely used techniques for dimensionality reduction: successful and\nefficiently computable, it is nevertheless plagued by a well-known,\nwell-documented sensitivity to outliers. Recent work has considered the setting\nwhere each point has a few arbitrarily corrupted components. Yet, in\napplications of SVD or PCA such as robust collaborative filtering or\nbioinformatics, malicious agents, defective genes, or simply corrupted or\ncontaminated experiments may effectively yield entire points that are\ncompletely corrupted.\n  We present an efficient convex optimization-based algorithm we call Outlier\nPursuit, that under some mild assumptions on the uncorrupted points (satisfied,\ne.g., by the standard generative assumption in PCA problems) recovers the exact\noptimal low-dimensional subspace, and identifies the corrupted points. Such\nidentification of corrupted points that do not conform to the low-dimensional\napproximation, is of paramount interest in bioinformatics and financial\napplications, and beyond. Our techniques involve matrix decomposition using\nnuclear norm minimization, however, our results, setup, and approach,\nnecessarily differ considerably from the existing line of work in matrix\ncompletion and matrix decomposition, since we develop an approach to recover\nthe correct column space of the uncorrupted matrix, rather than the exact\nmatrix itself. In any problem where one seeks to recover a structure rather\nthan the exact initial matrices, techniques developed thus far relying on\ncertificates of optimality, will fail. We present an important extension of\nthese methods, that allows the treatment of such problems.\n", "versions": [{"version": "v1", "created": "Wed, 20 Oct 2010 16:05:28 GMT"}, {"version": "v2", "created": "Fri, 31 Dec 2010 18:36:49 GMT"}], "update_date": "2011-01-04", "authors_parsed": [["Xu", "Huan", ""], ["Caramanis", "Constantine", ""], ["Sanghavi", "Sujay", ""]]}, {"id": "1010.4504", "submitter": "Jose M. Pe\\~na", "authors": "Jose M. Pe\\~na", "title": "Reading Dependencies from Covariance Graphs", "comments": "Changes from v1 to v2: Minor cosmetic changes, plus the addition of\n  reference (Richardson and Spirtes, 2002) in page 8. Changes from v2 to v3:\n  Addition of some references; International Journal of Approximate Reasoning,\n  2012", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.AI math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The covariance graph (aka bi-directed graph) of a probability distribution\n$p$ is the undirected graph $G$ where two nodes are adjacent iff their\ncorresponding random variables are marginally dependent in $p$. In this paper,\nwe present a graphical criterion for reading dependencies from $G$, under the\nassumption that $p$ satisfies the graphoid properties as well as weak\ntransitivity and composition. We prove that the graphical criterion is sound\nand complete in certain sense. We argue that our assumptions are not too\nrestrictive. For instance, all the regular Gaussian probability distributions\nsatisfy them.\n", "versions": [{"version": "v1", "created": "Thu, 21 Oct 2010 15:46:17 GMT"}, {"version": "v2", "created": "Thu, 13 Jan 2011 07:33:01 GMT"}, {"version": "v3", "created": "Tue, 26 Jun 2012 09:12:53 GMT"}], "update_date": "2012-06-27", "authors_parsed": [["Pe\u00f1a", "Jose M.", ""]]}, {"id": "1010.4515", "submitter": "Andrew Nobel", "authors": "Terrence M. Adams and Andrew B. Nobel", "title": "Uniform Approximation of Vapnik-Chervonenkis Classes", "comments": "13 pages, no figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.PR math.ST stat.ML stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  For any family of measurable sets in a probability space, we show that either\n(i) the family has infinite Vapnik-Chervonenkis (VC) dimension or (ii) for\nevery epsilon > 0 there is a finite partition pi such the pi-boundary of each\nset has measure at most epsilon. Immediate corollaries include the fact that a\nfamily with finite VC dimension has finite bracketing numbers, and satisfies\nuniform laws of large numbers for every ergodic process. From these\ncorollaries, we derive analogous results for VC major and VC graph families of\nfunctions.\n", "versions": [{"version": "v1", "created": "Thu, 21 Oct 2010 16:22:37 GMT"}], "update_date": "2010-10-22", "authors_parsed": [["Adams", "Terrence M.", ""], ["Nobel", "Andrew B.", ""]]}, {"id": "1010.4945", "submitter": "Takafumi Kanamori Dr.", "authors": "Takafumi Kanamori and Taiji Suzuki and Masashi Sugiyama", "title": "f-divergence estimation and two-sample homogeneity test under\n  semiparametric density-ratio models", "comments": "28 pages, 3 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A density ratio is defined by the ratio of two probability densities. We\nstudy the inference problem of density ratios and apply a semi-parametric\ndensity-ratio estimator to the two-sample homogeneity test. In the proposed\ntest procedure, the f-divergence between two probability densities is estimated\nusing a density-ratio estimator. The f-divergence estimator is then exploited\nfor the two-sample homogeneity test. We derive the optimal estimator of\nf-divergence in the sense of the asymptotic variance, and then investigate the\nrelation between the proposed test procedure and the existing score test based\non empirical likelihood estimator. Through numerical studies, we illustrate the\nadequacy of the asymptotic theory for finite-sample inference.\n", "versions": [{"version": "v1", "created": "Sun, 24 Oct 2010 09:11:40 GMT"}], "update_date": "2010-10-26", "authors_parsed": [["Kanamori", "Takafumi", ""], ["Suzuki", "Taiji", ""], ["Sugiyama", "Masashi", ""]]}, {"id": "1010.5233", "submitter": "Jelena Bradic", "authors": "Jelena Bradic, Jianqing Fan, Jiancheng Jiang", "title": "Regularization for Cox's proportional hazards model with\n  NP-dimensionality", "comments": "Published in at http://dx.doi.org/10.1214/11-AOS911 the Annals of\n  Statistics (http://www.imstat.org/aos/) by the Institute of Mathematical\n  Statistics (http://www.imstat.org)", "journal-ref": "Annals of Statistics 2011, Vol. 39, No. 6, 3092-3120", "doi": "10.1214/11-AOS911", "report-no": "IMS-AOS-AOS911", "categories": "math.ST stat.ME stat.ML stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  High throughput genetic sequencing arrays with thousands of measurements per\nsample and a great amount of related censored clinical data have increased\ndemanding need for better measurement specific model selection. In this paper\nwe establish strong oracle properties of nonconcave penalized methods for\nnonpolynomial (NP) dimensional data with censoring in the framework of Cox's\nproportional hazards model. A class of folded-concave penalties are employed\nand both LASSO and SCAD are discussed specifically. We unveil the question\nunder which dimensionality and correlation restrictions can an oracle estimator\nbe constructed and grasped. It is demonstrated that nonconcave penalties lead\nto significant reduction of the \"irrepresentable condition\" needed for LASSO\nmodel selection consistency. The large deviation result for martingales,\nbearing interests of its own, is developed for characterizing the strong oracle\nproperty. Moreover, the nonconcave regularized estimator, is shown to achieve\nasymptotically the information bound of the oracle estimator. A coordinate-wise\nalgorithm is developed for finding the grid of solution paths for penalized\nhazard regression problems, and its performance is evaluated on simulated and\ngene association study examples.\n", "versions": [{"version": "v1", "created": "Mon, 25 Oct 2010 19:51:46 GMT"}, {"version": "v2", "created": "Tue, 26 Oct 2010 00:24:07 GMT"}, {"version": "v3", "created": "Fri, 25 May 2012 10:39:39 GMT"}], "update_date": "2019-07-31", "authors_parsed": [["Bradic", "Jelena", ""], ["Fan", "Jianqing", ""], ["Jiang", "Jiancheng", ""]]}, {"id": "1010.5496", "submitter": "Ran Rubin", "authors": "Ran Rubin, Remi Monasson and Haim Sompolinsky", "title": "Theory of spike timing based neural classifiers", "comments": "4 page, 4 figures, Accepted to Physical Review Letters on 19th Oct.\n  2010", "journal-ref": "Phys. Rev. Lett. 105, 218102 (2010)", "doi": "10.1103/PhysRevLett.105.218102", "report-no": null, "categories": "q-bio.NC stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study the computational capacity of a model neuron, the Tempotron, which\nclassifies sequences of spikes by linear-threshold operations. We use\nstatistical mechanics and extreme value theory to derive the capacity of the\nsystem in random classification tasks. In contrast to its static analog, the\nPerceptron, the Tempotron's solutions space consists of a large number of small\nclusters of weight vectors. The capacity of the system per synapse is finite in\nthe large size limit and weakly diverges with the stimulus duration relative to\nthe membrane and synaptic time constants.\n", "versions": [{"version": "v1", "created": "Tue, 26 Oct 2010 19:43:44 GMT"}], "update_date": "2010-11-30", "authors_parsed": [["Rubin", "Ran", ""], ["Monasson", "Remi", ""], ["Sompolinsky", "Haim", ""]]}, {"id": "1010.5503", "submitter": "Jiangang Hao", "authors": "Jiangang Hao, Timothy A. McKay, Benjamin P. Koester, Eli S. Rykoff,\n  Eduardo Rozo, James Annis, Risa H. Wechsler, August Evrard, Seth R. Siegel,\n  Matthew Becker, Michael Busha, David Gerdes, David E. Johnston and Erin\n  Sheldon", "title": "A GMBCG Galaxy Cluster Catalog of 55,424 Rich Clusters from SDSS DR7", "comments": "Updated to match the published version. The catalog can be accessed\n  from: http://home.fnal.gov/~jghao/gmbcg_sdss_catalog.html", "journal-ref": "Astrophys.J.Suppl.191:254-274,2010", "doi": "10.1088/0067-0049/191/2/254", "report-no": "FERMILAB-PUB-10-287-A", "categories": "astro-ph.CO stat.CO stat.ML", "license": "http://creativecommons.org/licenses/by/3.0/", "abstract": "  We present a large catalog of optically selected galaxy clusters from the\napplication of a new Gaussian Mixture Brightest Cluster Galaxy (GMBCG)\nalgorithm to SDSS Data Release 7 data. The algorithm detects clusters by\nidentifying the red sequence plus Brightest Cluster Galaxy (BCG) feature, which\nis unique for galaxy clusters and does not exist among field galaxies. Red\nsequence clustering in color space is detected using an Error Corrected\nGaussian Mixture Model. We run GMBCG on 8240 square degrees of photometric data\nfrom SDSS DR7 to assemble the largest ever optical galaxy cluster catalog,\nconsisting of over 55,000 rich clusters across the redshift range from 0.1 < z\n< 0.55. We present Monte Carlo tests of completeness and purity and perform\ncross-matching with X-ray clusters and with the maxBCG sample at low redshift.\nThese tests indicate high completeness and purity across the full redshift\nrange for clusters with 15 or more members.\n", "versions": [{"version": "v1", "created": "Tue, 26 Oct 2010 20:01:37 GMT"}, {"version": "v2", "created": "Tue, 21 Dec 2010 19:59:29 GMT"}, {"version": "v3", "created": "Wed, 22 Dec 2010 03:38:34 GMT"}], "update_date": "2010-12-23", "authors_parsed": [["Hao", "Jiangang", ""], ["McKay", "Timothy A.", ""], ["Koester", "Benjamin P.", ""], ["Rykoff", "Eli S.", ""], ["Rozo", "Eduardo", ""], ["Annis", "James", ""], ["Wechsler", "Risa H.", ""], ["Evrard", "August", ""], ["Siegel", "Seth R.", ""], ["Becker", "Matthew", ""], ["Busha", "Michael", ""], ["Gerdes", "David", ""], ["Johnston", "David E.", ""], ["Sheldon", "Erin", ""]]}]