[{"id": "1612.00081", "submitter": "Yihao Feng", "authors": "Qiang Liu, Yihao Feng", "title": "Two Methods For Wild Variational Inference", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Variational inference provides a powerful tool for approximate probabilistic\nin- ference on complex, structured models. Typical variational inference\nmethods, however, require to use inference networks with computationally\ntractable proba- bility density functions. This largely limits the design and\nimplementation of vari- ational inference methods. We consider wild variational\ninference methods that do not require tractable density functions on the\ninference networks, and hence can be applied in more challenging cases. As an\nexample of application, we treat stochastic gradient Langevin dynamics (SGLD)\nas an inference network, and use our methods to automatically adjust the step\nsizes of SGLD, yielding significant improvement over the hand-designed step\nsize schemes\n", "versions": [{"version": "v1", "created": "Wed, 30 Nov 2016 23:55:57 GMT"}, {"version": "v2", "created": "Mon, 30 Oct 2017 22:05:18 GMT"}], "update_date": "2017-11-01", "authors_parsed": [["Liu", "Qiang", ""], ["Feng", "Yihao", ""]]}, {"id": "1612.00086", "submitter": "Ehsan Amid", "authors": "Ehsan Amid, Aristides Gionis, Antti Ukkonen", "title": "Semi-supervised Kernel Metric Learning Using Relative Comparisons", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider the problem of metric learning subject to a set of constraints on\nrelative-distance comparisons between the data items. Such constraints are\nmeant to reflect side-information that is not expressed directly in the feature\nvectors of the data items. The relative-distance constraints used in this work\nare particularly effective in expressing structures at finer level of detail\nthan must-link (ML) and cannot-link (CL) constraints, which are most commonly\nused for semi-supervised clustering. Relative-distance constraints are thus\nuseful in settings where providing an ML or a CL constraint is difficult\nbecause the granularity of the true clustering is unknown.\n  Our main contribution is an efficient algorithm for learning a kernel matrix\nusing the log determinant divergence --- a variant of the Bregman divergence\n--- subject to a set of relative-distance constraints. The learned kernel\nmatrix can then be employed by many different kernel methods in a wide range of\napplications. In our experimental evaluations, we consider a semi-supervised\nclustering setting and show empirically that kernels found by our algorithm\nyield clusterings of higher quality than existing approaches that either use\nML/CL constraints or a different means to implement the supervision using\nrelative comparisons.\n", "versions": [{"version": "v1", "created": "Thu, 1 Dec 2016 00:16:53 GMT"}, {"version": "v2", "created": "Sat, 3 Dec 2016 09:09:05 GMT"}], "update_date": "2016-12-06", "authors_parsed": [["Amid", "Ehsan", ""], ["Gionis", "Aristides", ""], ["Ukkonen", "Antti", ""]]}, {"id": "1612.00099", "submitter": "Nishant Subramani", "authors": "Nishant Subramani", "title": "PAG2ADMG: An Algorithm for the Complete Causal Enumeration of a Markov\n  Equivalence Class", "comments": "V2 and V1 are different enough to be different papers. V2 will be\n  significantly extended and resubmitted as a different arxiv paper", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Causal graphs, such as directed acyclic graphs (DAGs) and partial ancestral\ngraphs (PAGs), represent causal relationships among variables in a model.\nMethods exist for learning DAGs and PAGs from data and for converting DAGs to\nPAGs. However, these methods are significantly limited in that they only output\na single causal graph consistent with the independencies and dependencies (the\nMarkov equivalence class $M$) estimated from the data. This is problematic and\ninsufficient because many distinct graphs may be consistent with $M$. A data\nmodeler may wish to select among these numerous consistent graphs using domain\nknowledge or other model selection algorithms. Enumeration of the set of\nconsistent graphs is the bottleneck. In this paper, we present a method that\nmakes this desired enumeration possible. We introduce PAG2ADMG, the first\nalgorithm for enumerating all causal graphs consistent with $M$. PAG2ADMG\nconverts a given PAG into the complete set of acyclic directed mixed graphs\n(ADMGs) consistent with $M$. We prove the correctness of the approach and\ndemonstrate its efficiency relative to brute-force enumeration.\n", "versions": [{"version": "v1", "created": "Thu, 1 Dec 2016 01:08:27 GMT"}, {"version": "v2", "created": "Tue, 17 Oct 2017 18:34:29 GMT"}, {"version": "v3", "created": "Thu, 18 Jan 2018 18:19:39 GMT"}], "update_date": "2018-01-19", "authors_parsed": [["Subramani", "Nishant", ""]]}, {"id": "1612.00100", "submitter": "Hongyang Zhang", "authors": "Maria-Florina Balcan and Hongyang Zhang", "title": "Noise-Tolerant Life-Long Matrix Completion via Adaptive Sampling", "comments": "24 pages, 5 figures in NIPS 2016", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study the problem of recovering an incomplete $m\\times n$ matrix of rank\n$r$ with columns arriving online over time. This is known as the problem of\nlife-long matrix completion, and is widely applied to recommendation system,\ncomputer vision, system identification, etc. The challenge is to design\nprovable algorithms tolerant to a large amount of noises, with small sample\ncomplexity. In this work, we give algorithms achieving strong guarantee under\ntwo realistic noise models. In bounded deterministic noise, an adversary can\nadd any bounded yet unstructured noise to each column. For this problem, we\npresent an algorithm that returns a matrix of a small error, with sample\ncomplexity almost as small as the best prior results in the noiseless case. For\nsparse random noise, where the corrupted columns are sparse and drawn randomly,\nwe give an algorithm that exactly recovers an $\\mu_0$-incoherent matrix by\nprobability at least $1-\\delta$ with sample complexity as small as\n$O\\left(\\mu_0rn\\log (r/\\delta)\\right)$. This result advances the\nstate-of-the-art work and matches the lower bound in a worst case. We also\nstudy the scenario where the hidden matrix lies on a mixture of subspaces and\nshow that the sample complexity can be even smaller. Our proposed algorithms\nperform well experimentally in both synthetic and real-world datasets.\n", "versions": [{"version": "v1", "created": "Thu, 1 Dec 2016 01:10:07 GMT"}], "update_date": "2016-12-04", "authors_parsed": [["Balcan", "Maria-Florina", ""], ["Zhang", "Hongyang", ""]]}, {"id": "1612.00151", "submitter": "Vijendra Singh", "authors": "Singh Vijendra, Hemjyotsana Parashar and Nisha Vasudeva", "title": "A New Method for Classification of Datasets for Data Mining", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.DB stat.ML", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Decision tree is an important method for both induction research and data\nmining, which is mainly used for model classification and prediction. ID3\nalgorithm is the most widely used algorithm in the decision tree so far. In\nthis paper, the shortcoming of ID3's inclining to choose attributes with many\nvalues is discussed, and then a new decision tree algorithm which is improved\nversion of ID3. In our proposed algorithm attributes are divided into groups\nand then we apply the selection measure 5 for these groups. If information gain\nis not good then again divide attributes values into groups. These steps are\ndone until we get good classification/misclassification ratio. The proposed\nalgorithms classify the data sets more accurately and efficiently.\n", "versions": [{"version": "v1", "created": "Thu, 1 Dec 2016 05:24:36 GMT"}], "update_date": "2016-12-02", "authors_parsed": [["Vijendra", "Singh", ""], ["Parashar", "Hemjyotsana", ""], ["Vasudeva", "Nisha", ""]]}, {"id": "1612.00193", "submitter": "Gr\\'egoire Ferr\\'e", "authors": "G. Ferr\\'e, T. Haut and K. Barros", "title": "Learning molecular energies using localized graph kernels", "comments": null, "journal-ref": "The Journal of Chemical Physics, 146(11), 114107 (2017)", "doi": "10.1063/1.4978623", "report-no": null, "categories": "physics.comp-ph cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recent machine learning methods make it possible to model potential energy of\natomic configurations with chemical-level accuracy (as calculated from\nab-initio calculations) and at speeds suitable for molecular dynam- ics\nsimulation. Best performance is achieved when the known physical constraints\nare encoded in the machine learning models. For example, the atomic energy is\ninvariant under global translations and rotations, it is also invariant to\npermutations of same-species atoms. Although simple to state, these symmetries\nare complicated to encode into machine learning algorithms. In this paper, we\npresent a machine learning approach based on graph theory that naturally\nincorporates translation, rotation, and permutation symmetries. Specifically,\nwe use a random walk graph kernel to measure the similarity of two adjacency\nmatrices, each of which represents a local atomic environment. This Graph\nApproximated Energy (GRAPE) approach is flexible and admits many possible\nextensions. We benchmark a simple version of GRAPE by predicting atomization\nenergies on a standard dataset of organic molecules.\n", "versions": [{"version": "v1", "created": "Thu, 1 Dec 2016 10:23:59 GMT"}, {"version": "v2", "created": "Tue, 25 Apr 2017 10:03:41 GMT"}], "update_date": "2017-05-03", "authors_parsed": [["Ferr\u00e9", "G.", ""], ["Haut", "T.", ""], ["Barros", "K.", ""]]}, {"id": "1612.00312", "submitter": "Igor Altsybeev", "authors": "Igor Altsybeev, Vladimir Kovalenko", "title": "Classifiers for centrality determination in proton-nucleus and\n  nucleus-nucleus collisions", "comments": "To be published in proceedings of the \"XIIth Quark Confinement and\n  the Hadron Spectrum\" conference (Thessaloniki, 2016)", "journal-ref": "EPJ Web of Conferences 137, 11001 (2017)", "doi": "10.1051/epjconf/201713711001", "report-no": null, "categories": "physics.data-an nucl-ex stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Centrality, as a geometrical property of the collision, is crucial for the\nphysical interpretation of nucleus-nucleus and proton-nucleus experimental\ndata. However, it cannot be directly accessed in event-by-event data analysis.\nCommon methods for centrality estimation in A-A and p-A collisions usually rely\non a single detector (either on the signal in zero-degree calorimeters or on\nthe multiplicity in some semi-central rapidity range). In the present work, we\nmade an attempt to develop an approach for centrality determination that is\nbased on machine-learning techniques and utilizes information from several\ndetector subsystems simultaneously. Different event classifiers are suggested\nand evaluated for their selectivity power in terms of the number of\nnucleons-participants and the impact parameter of the collision. Finer\ncentrality resolution may allow to reduce impact from so-called volume\nfluctuations on physical observables being studied in heavy-ion experiments\nlike ALICE at the LHC and fixed target experiment NA61/SHINE on SPS.\n", "versions": [{"version": "v1", "created": "Wed, 30 Nov 2016 13:51:52 GMT"}], "update_date": "2017-03-24", "authors_parsed": [["Altsybeev", "Igor", ""], ["Kovalenko", "Vladimir", ""]]}, {"id": "1612.00367", "submitter": "Damien Lefortier", "authors": "Damien Lefortier, Adith Swaminathan, Xiaotao Gu, Thorsten Joachims,\n  Maarten de Rijke", "title": "Large-scale Validation of Counterfactual Learning Methods: A Test-Bed", "comments": "10 pages, What If workshop NIPS 2016", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The ability to perform effective off-policy learning would revolutionize the\nprocess of building better interactive systems, such as search engines and\nrecommendation systems for e-commerce, computational advertising and news.\nRecent approaches for off-policy evaluation and learning in these settings\nappear promising. With this paper, we provide real-world data and a\nstandardized test-bed to systematically investigate these algorithms using data\nfrom display advertising. In particular, we consider the problem of filling a\nbanner ad with an aggregate of multiple products the user may want to purchase.\nThis paper presents our test-bed, the sanity checks we ran to ensure its\nvalidity, and shows results comparing state-of-the-art off-policy learning\nmethods like doubly robust optimization, POEM, and reductions to supervised\nlearning using regression baselines. Our results show experimental evidence\nthat recent off-policy learning methods can improve upon state-of-the-art\nsupervised learning techniques on a large-scale real-world data set.\n", "versions": [{"version": "v1", "created": "Thu, 1 Dec 2016 17:59:53 GMT"}, {"version": "v2", "created": "Sun, 25 Jun 2017 11:00:30 GMT"}], "update_date": "2017-06-27", "authors_parsed": [["Lefortier", "Damien", ""], ["Swaminathan", "Adith", ""], ["Gu", "Xiaotao", ""], ["Joachims", "Thorsten", ""], ["de Rijke", "Maarten", ""]]}, {"id": "1612.00374", "submitter": "Ingrid Blaschzyk", "authors": "Philipp Thomann and Ingrid Blaschzyk and Mona Meister and Ingo\n  Steinwart", "title": "Spatial Decompositions for Large Scale SVMs", "comments": null, "journal-ref": "Proceedings of Machine Learning Research Volume 54: Proceedings of\n  the 20th International Conference on Artificial Intelligence and Statistics\n  2017 (A. Singh and J. Zhu, eds.), pp. 1329-1337, 2017", "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Although support vector machines (SVMs) are theoretically well understood,\ntheir underlying optimization problem becomes very expensive, if, for example,\nhundreds of thousands of samples and a non-linear kernel are considered.\nSeveral approaches have been proposed in the past to address this serious\nlimitation. In this work we investigate a decomposition strategy that learns on\nsmall, spatially defined data chunks. Our contributions are two fold: On the\ntheoretical side we establish an oracle inequality for the overall learning\nmethod using the hinge loss, and show that the resulting rates match those\nknown for SVMs solving the complete optimization problem with Gaussian kernels.\nOn the practical side we compare our approach to learning SVMs on small,\nrandomly chosen chunks. Here it turns out that for comparable training times\nour approach is significantly faster during testing and also reduces the test\nerror in most cases significantly. Furthermore, we show that our approach\neasily scales up to 10 million training samples: including hyper-parameter\nselection using cross validation, the entire training only takes a few hours on\na single machine. Finally, we report an experiment on 32 million training\nsamples. All experiments used liquidSVM (Steinwart and Thomann, 2017).\n", "versions": [{"version": "v1", "created": "Thu, 1 Dec 2016 18:14:33 GMT"}, {"version": "v2", "created": "Thu, 8 Feb 2018 14:54:51 GMT"}], "update_date": "2018-02-09", "authors_parsed": [["Thomann", "Philipp", ""], ["Blaschzyk", "Ingrid", ""], ["Meister", "Mona", ""], ["Steinwart", "Ingo", ""]]}, {"id": "1612.00380", "submitter": "Nantas Nardelli", "authors": "Shehroze Bhatti, Alban Desmaison, Ondrej Miksik, Nantas Nardelli, N.\n  Siddharth, Philip H. S. Torr", "title": "Playing Doom with SLAM-Augmented Deep Reinforcement Learning", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.CV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A number of recent approaches to policy learning in 2D game domains have been\nsuccessful going directly from raw input images to actions. However when\nemployed in complex 3D environments, they typically suffer from challenges\nrelated to partial observability, combinatorial exploration spaces, path\nplanning, and a scarcity of rewarding scenarios. Inspired from prior work in\nhuman cognition that indicates how humans employ a variety of semantic concepts\nand abstractions (object categories, localisation, etc.) to reason about the\nworld, we build an agent-model that incorporates such abstractions into its\npolicy-learning framework. We augment the raw image input to a Deep Q-Learning\nNetwork (DQN), by adding details of objects and structural elements\nencountered, along with the agent's localisation. The different components are\nautomatically extracted and composed into a topological representation using\non-the-fly object detection and 3D-scene reconstruction.We evaluate the\nefficacy of our approach in Doom, a 3D first-person combat game that exhibits a\nnumber of challenges discussed, and show that our augmented framework\nconsistently learns better, more effective policies.\n", "versions": [{"version": "v1", "created": "Thu, 1 Dec 2016 18:54:51 GMT"}], "update_date": "2016-12-02", "authors_parsed": [["Bhatti", "Shehroze", ""], ["Desmaison", "Alban", ""], ["Miksik", "Ondrej", ""], ["Nardelli", "Nantas", ""], ["Siddharth", "N.", ""], ["Torr", "Philip H. S.", ""]]}, {"id": "1612.00383", "submitter": "Valentin Dalibard", "authors": "Valentin Dalibard, Michael Schaarschmidt, Eiko Yoneki", "title": "Tuning the Scheduling of Distributed Stochastic Gradient Descent with\n  Bayesian Optimization", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present an optimizer which uses Bayesian optimization to tune the system\nparameters of distributed stochastic gradient descent (SGD). Given a specific\ncontext, our goal is to quickly find efficient configurations which\nappropriately balance the load between the available machines to minimize the\naverage SGD iteration time. Our experiments consider setups with over thirty\nparameters. Traditional Bayesian optimization, which uses a Gaussian process as\nits model, is not well suited to such high dimensional domains. To reduce\nconvergence time, we exploit the available structure. We design a probabilistic\nmodel which simulates the behavior of distributed SGD and use it within\nBayesian optimization. Our model can exploit many runtime measurements for\ninference per evaluation of the objective function. Our experiments show that\nour resulting optimizer converges to efficient configurations within ten\niterations, the optimized configurations outperform those found by generic\noptimizer in thirty iterations by up to 2X.\n", "versions": [{"version": "v1", "created": "Thu, 1 Dec 2016 19:08:12 GMT"}], "update_date": "2016-12-04", "authors_parsed": [["Dalibard", "Valentin", ""], ["Schaarschmidt", "Michael", ""], ["Yoneki", "Eiko", ""]]}, {"id": "1612.00388", "submitter": "Wesley Tansey", "authors": "Wesley Tansey and Edward W. Lowe Jr. and James G. Scott", "title": "Diet2Vec: Multi-scale analysis of massive dietary data", "comments": "Accepted to the NIPS 2016 Workshop on Machine Learning for Health", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Smart phone apps that enable users to easily track their diets have become\nwidespread in the last decade. This has created an opportunity to discover new\ninsights into obesity and weight loss by analyzing the eating habits of the\nusers of such apps. In this paper, we present diet2vec: an approach to modeling\nlatent structure in a massive database of electronic diet journals. Through an\niterative contract-and-expand process, our model learns real-valued embeddings\nof users' diets, as well as embeddings for individual foods and meals. We\ndemonstrate the effectiveness of our approach on a real dataset of 55K users of\nthe popular diet-tracking app LoseIt\\footnote{http://www.loseit.com/}. To the\nbest of our knowledge, this is the largest fine-grained diet tracking study in\nthe history of nutrition and obesity research. Our results suggest that\ndiet2vec finds interpretable results at all levels, discovering intuitive\nrepresentations of foods, meals, and diets.\n", "versions": [{"version": "v1", "created": "Thu, 1 Dec 2016 19:21:22 GMT"}], "update_date": "2016-12-02", "authors_parsed": [["Tansey", "Wesley", ""], ["Lowe", "Edward W.", "Jr."], ["Scott", "James G.", ""]]}, {"id": "1612.00393", "submitter": "Joachim van der Herten", "authors": "Joachim van der Herten and Ivo Couckuyt and Tom Dhaene", "title": "Hypervolume-based Multi-objective Bayesian Optimization with Student-t\n  Processes", "comments": "5 pages, 3 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Student-$t$ processes have recently been proposed as an appealing alternative\nnon-parameteric function prior. They feature enhanced flexibility and\npredictive variance. In this work the use of Student-$t$ processes are explored\nfor multi-objective Bayesian optimization. In particular, an analytical\nexpression for the hypervolume-based probability of improvement is developed\nfor independent Student-$t$ process priors of the objectives. Its effectiveness\nis shown on a multi-objective optimization problem which is known to be\ndifficult with traditional Gaussian processes.\n", "versions": [{"version": "v1", "created": "Thu, 1 Dec 2016 19:41:50 GMT"}], "update_date": "2016-12-02", "authors_parsed": [["van der Herten", "Joachim", ""], ["Couckuyt", "Ivo", ""], ["Dhaene", "Tom", ""]]}, {"id": "1612.00475", "submitter": "Taylor Killian", "authors": "Taylor Killian, George Konidaris, Finale Doshi-Velez", "title": "Transfer Learning Across Patient Variations with Hidden Parameter Markov\n  Decision Processes", "comments": "Brief abstract for poster submission to Machine Learning for\n  Healthcare workshop at NIPS 2016", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.AI cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Due to physiological variation, patients diagnosed with the same condition\nmay exhibit divergent, but related, responses to the same treatments. Hidden\nParameter Markov Decision Processes (HiP-MDPs) tackle this transfer-learning\nproblem by embedding these tasks into a low-dimensional space. However, the\noriginal formulation of HiP-MDP had a critical flaw: the embedding uncertainty\nwas modeled independently of the agent's state uncertainty, requiring an\nunnatural training procedure in which all tasks visited every part of the state\nspace---possible for robots that can be moved to a particular location,\nimpossible for human patients. We update the HiP-MDP framework and extend it to\nmore robustly develop personalized medicine strategies for HIV treatment.\n", "versions": [{"version": "v1", "created": "Thu, 1 Dec 2016 21:26:52 GMT"}], "update_date": "2016-12-05", "authors_parsed": [["Killian", "Taylor", ""], ["Konidaris", "George", ""], ["Doshi-Velez", "Finale", ""]]}, {"id": "1612.00516", "submitter": "Corinne Jones", "authors": "Corinne L. Jones, Sham M. Kakade, Lucas W. Thornblade, David R. Flum,\n  Abraham D. Flaxman", "title": "Canonical Correlation Analysis for Analyzing Sequences of Medical\n  Billing Codes", "comments": "Accepted at NIPS 2016 Workshop on Machine Learning for Health", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose using canonical correlation analysis (CCA) to generate features\nfrom sequences of medical billing codes. Applying this novel use of CCA to a\ndatabase of medical billing codes for patients with diverticulitis, we first\ndemonstrate that the CCA embeddings capture meaningful relationships among the\ncodes. We then generate features from these embeddings and establish their\nusefulness in predicting future elective surgery for diverticulitis, an\nimportant marker in efforts for reducing costs in healthcare.\n", "versions": [{"version": "v1", "created": "Thu, 1 Dec 2016 23:38:34 GMT"}, {"version": "v2", "created": "Fri, 6 Jan 2017 16:42:36 GMT"}], "update_date": "2017-01-09", "authors_parsed": [["Jones", "Corinne L.", ""], ["Kakade", "Sham M.", ""], ["Thornblade", "Lucas W.", ""], ["Flum", "David R.", ""], ["Flaxman", "Abraham D.", ""]]}, {"id": "1612.00525", "submitter": "Turki Turki", "authors": "Turki Turki and Zhi Wei", "title": "A Noise-Filtering Approach for Cancer Drug Sensitivity Prediction", "comments": "Accepted at NIPS 2016 Workshop on Machine Learning for Health", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG q-bio.GN stat.ML", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Accurately predicting drug responses to cancer is an important problem\nhindering oncologists' efforts to find the most effective drugs to treat\ncancer, which is a core goal in precision medicine. The scientific community\nhas focused on improving this prediction based on genomic, epigenomic, and\nproteomic datasets measured in human cancer cell lines. Real-world cancer cell\nlines contain noise, which degrades the performance of machine learning\nalgorithms. This problem is rarely addressed in the existing approaches. In\nthis paper, we present a noise-filtering approach that integrates techniques\nfrom numerical linear algebra and information retrieval targeted at filtering\nout noisy cancer cell lines. By filtering out noisy cancer cell lines, we can\ntrain machine learning algorithms on better quality cancer cell lines. We\nevaluate the performance of our approach and compare it with an existing\napproach using the Area Under the ROC Curve (AUC) on clinical trial data. The\nexperimental results show that our proposed approach is stable and also yields\nthe highest AUC at a statistically significant level.\n", "versions": [{"version": "v1", "created": "Fri, 2 Dec 2016 00:41:11 GMT"}, {"version": "v2", "created": "Mon, 5 Dec 2016 05:15:51 GMT"}], "update_date": "2016-12-06", "authors_parsed": [["Turki", "Turki", ""], ["Wei", "Zhi", ""]]}, {"id": "1612.00555", "submitter": "Elizabeth Lorenzi", "authors": "Elizabeth C Lorenzi, Zhifei Sun, Erich Huang, Ricardo Henao, Katherine\n  A Heller", "title": "Transfer Learning via Latent Factor Modeling to Improve Prediction of\n  Surgical Complications", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We aim to create a framework for transfer learning using latent factor models\nto learn the dependence structure between a larger source dataset and a target\ndataset. The methodology is motivated by our goal of building a risk-assessment\nmodel for surgery patients, using both institutional and national surgical\noutcomes data. The national surgical outcomes data is collected through NSQIP\n(National Surgery Quality Improvement Program), a database housing almost 4\nmillion patients from over 700 different hospitals. We build a latent factor\nmodel with a hierarchical prior on the loadings matrix to appropriately account\nfor the different covariance structure in our data. We extend this model to\nhandle more complex relationships between the populations by deriving a scale\nmixture formulation using stick-breaking properties. Our model provides a\ntransfer learning framework that utilizes all information from both the source\nand target data, while modeling the underlying inherent differences between\nthem.\n", "versions": [{"version": "v1", "created": "Fri, 2 Dec 2016 03:35:48 GMT"}], "update_date": "2016-12-05", "authors_parsed": [["Lorenzi", "Elizabeth C", ""], ["Sun", "Zhifei", ""], ["Huang", "Erich", ""], ["Henao", "Ricardo", ""], ["Heller", "Katherine A", ""]]}, {"id": "1612.00583", "submitter": "Yifei Ma", "authors": "Yifei Ma and Roman Garnett and Jeff Schneider", "title": "Active Search for Sparse Signals with Region Sensing", "comments": "aaai 2017 preprint; nips exhibition of rejections", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.AI cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Autonomous systems can be used to search for sparse signals in a large space;\ne.g., aerial robots can be deployed to localize threats, detect gas leaks, or\nrespond to distress calls. Intuitively, search algorithms may increase\nefficiency by collecting aggregate measurements summarizing large contiguous\nregions. However, most existing search methods either ignore the possibility of\nsuch region observations (e.g., Bayesian optimization and multi-armed bandits)\nor make strong assumptions about the sensing mechanism that allow each\nmeasurement to arbitrarily encode all signals in the entire environment (e.g.,\ncompressive sensing). We propose an algorithm that actively collects data to\nsearch for sparse signals using only noisy measurements of the average values\non rectangular regions (including single points), based on the greedy\nmaximization of information gain. We analyze our algorithm in 1d and show that\nit requires $\\tilde{O}(\\frac{n}{\\mu^2}+k^2)$ measurements to recover all of $k$\nsignal locations with small Bayes error, where $\\mu$ and $n$ are the signal\nstrength and the size of the search space, respectively. We also show that\nactive designs can be fundamentally more efficient than passive designs with\nregion sensing, contrasting with the results of Arias-Castro, Candes, and\nDavenport (2013). We demonstrate the empirical performance of our algorithm on\na search problem using satellite image data and in high dimensions.\n", "versions": [{"version": "v1", "created": "Fri, 2 Dec 2016 07:44:45 GMT"}], "update_date": "2016-12-05", "authors_parsed": [["Ma", "Yifei", ""], ["Garnett", "Roman", ""], ["Schneider", "Jeff", ""]]}, {"id": "1612.00585", "submitter": "Soumi Chaki", "authors": "Soumi Chaki, Aurobinda Routray, William K. Mohanty, Mamata Jenamani", "title": "Development of a hybrid learning system based on SVM, ANFIS and domain\n  knowledge: DKFIS", "comments": "6 pages, 5 figures, 3tables Presented at Indicon 2015", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CE stat.AP stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper presents the development of a hybrid learning system based on\nSupport Vector Machines (SVM), Adaptive Neuro-Fuzzy Inference System (ANFIS)\nand domain knowledge to solve prediction problem. The proposed two-stage Domain\nKnowledge based Fuzzy Information System (DKFIS) improves the prediction\naccuracy attained by ANFIS alone. The proposed framework has been implemented\non a noisy and incomplete dataset acquired from a hydrocarbon field located at\nwestern part of India. Here, oil saturation has been predicted from four\ndifferent well logs i.e. gamma ray, resistivity, density, and clay volume. In\nthe first stage, depending on zero or near zero and non-zero oil saturation\nlevels the input vector is classified into two classes (Class 0 and Class 1)\nusing SVM. The classification results have been further fine-tuned applying\nexpert knowledge based on the relationship among predictor variables i.e. well\nlogs and target variable - oil saturation. Second, an ANFIS is designed to\npredict non-zero (Class 1) oil saturation values from predictor logs. The\npredicted output has been further refined based on expert knowledge. It is\napparent from the experimental results that the expert intervention with\nqualitative judgment at each stage has rendered the prediction into the\nfeasible and realistic ranges. The performance analysis of the prediction in\nterms of four performance metrics such as correlation coefficient (CC), root\nmean square error (RMSE), and absolute error mean (AEM), scatter index (SI) has\nestablished DKFIS as a useful tool for reservoir characterization.\n", "versions": [{"version": "v1", "created": "Fri, 2 Dec 2016 07:56:23 GMT"}], "update_date": "2016-12-05", "authors_parsed": [["Chaki", "Soumi", ""], ["Routray", "Aurobinda", ""], ["Mohanty", "William K.", ""], ["Jenamani", "Mamata", ""]]}, {"id": "1612.00595", "submitter": "Jun Song", "authors": "Jun Song, David A. Moore", "title": "Parallel Chromatic MCMC with Spatial Partitioning", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We introduce a novel approach for parallelizing MCMC inference in models with\nspatially determined conditional independence relationships, for which existing\ntechniques exploiting graphical model structure are not applicable. Our\napproach is motivated by a model of seismic events and signals, where events\ndetected in distant regions are approximately independent given those in\nintermediate regions. We perform parallel inference by coloring a factor graph\ndefined over regions of latent space, rather than individual model variables.\nEvaluating on a model of seismic event detection, we achieve significant\nspeedups over serial MCMC with no degradation in inference quality.\n", "versions": [{"version": "v1", "created": "Fri, 2 Dec 2016 08:53:02 GMT"}, {"version": "v2", "created": "Thu, 8 Dec 2016 00:08:48 GMT"}], "update_date": "2016-12-09", "authors_parsed": [["Song", "Jun", ""], ["Moore", "David A.", ""]]}, {"id": "1612.00599", "submitter": "Zihao Chen", "authors": "Zihao Chen, Luo Luo, Zhihua Zhang", "title": "Communication Lower Bounds for Distributed Convex Optimization:\n  Partition Data on Features", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recently, there has been an increasing interest in designing distributed\nconvex optimization algorithms under the setting where the data matrix is\npartitioned on features. Algorithms under this setting sometimes have many\nadvantages over those under the setting where data is partitioned on samples,\nespecially when the number of features is huge. Therefore, it is important to\nunderstand the inherent limitations of these optimization problems. In this\npaper, with certain restrictions on the communication allowed in the\nprocedures, we develop tight lower bounds on communication rounds for a broad\nclass of non-incremental algorithms under this setting. We also provide a lower\nbound on communication rounds for a class of (randomized) incremental\nalgorithms.\n", "versions": [{"version": "v1", "created": "Fri, 2 Dec 2016 09:01:57 GMT"}], "update_date": "2016-12-05", "authors_parsed": [["Chen", "Zihao", ""], ["Luo", "Luo", ""], ["Zhang", "Zhihua", ""]]}, {"id": "1612.00615", "submitter": "Samuele Fiorini", "authors": "Samuele Fiorini, Andrea Tacchino, Giampaolo Brichetto, Alessandro\n  Verri, Annalisa Barla", "title": "A temporal model for multiple sclerosis course evolution", "comments": "NIPS Machine Learning for health Workshop 2016", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Multiple Sclerosis is a degenerative condition of the central nervous system\nthat affects nearly 2.5 million of individuals in terms of their physical,\ncognitive, psychological and social capabilities. Researchers are currently\ninvestigating on the use of patient reported outcome measures for the\nassessment of impact and evolution of the disease on the life of the patients.\nTo date, a clear understanding on the use of such measures to predict the\nevolution of the disease is still lacking. In this work we resort to\nregularized machine learning methods for binary classification and multiple\noutput regression. We propose a pipeline that can be used to predict the\ndisease progression from patient reported measures. The obtained model is\ntested on a data set collected from an ongoing clinical research project.\n", "versions": [{"version": "v1", "created": "Fri, 2 Dec 2016 10:13:16 GMT"}], "update_date": "2016-12-05", "authors_parsed": [["Fiorini", "Samuele", ""], ["Tacchino", "Andrea", ""], ["Brichetto", "Giampaolo", ""], ["Verri", "Alessandro", ""], ["Barla", "Annalisa", ""]]}, {"id": "1612.00653", "submitter": "Antti Kangasr\\\"a\\\"asi\\\"o", "authors": "Antti Kangasr\\\"a\\\"asi\\\"o, Kumaripaba Athukorala, Andrew Howes, Jukka\n  Corander, Samuel Kaski, Antti Oulasvirta", "title": "Inferring Cognitive Models from Data using Approximate Bayesian\n  Computation", "comments": "To appear in CHI'2017", "journal-ref": null, "doi": "10.1145/3025453.3025576", "report-no": null, "categories": "cs.HC cs.AI cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  An important problem for HCI researchers is to estimate the parameter values\nof a cognitive model from behavioral data. This is a difficult problem, because\nof the substantial complexity and variety in human behavioral strategies. We\nreport an investigation into a new approach using approximate Bayesian\ncomputation (ABC) to condition model parameters to data and prior knowledge. As\nthe case study we examine menu interaction, where we have click time data only\nto infer a cognitive model that implements a search behaviour with parameters\nsuch as fixation duration and recall probability. Our results demonstrate that\nABC (i) improves estimates of model parameter values, (ii) enables meaningful\ncomparisons between model variants, and (iii) supports fitting models to\nindividual users. ABC provides ample opportunities for theoretical HCI research\nby allowing principled inference of model parameter values and their\nuncertainty.\n", "versions": [{"version": "v1", "created": "Fri, 2 Dec 2016 12:20:47 GMT"}, {"version": "v2", "created": "Fri, 13 Jan 2017 12:15:47 GMT"}], "update_date": "2017-01-16", "authors_parsed": [["Kangasr\u00e4\u00e4si\u00f6", "Antti", ""], ["Athukorala", "Kumaripaba", ""], ["Howes", "Andrew", ""], ["Corander", "Jukka", ""], ["Kaski", "Samuel", ""], ["Oulasvirta", "Antti", ""]]}, {"id": "1612.00662", "submitter": "Adam McCarthy", "authors": "Adam McCarthy and Christopher K.I. Williams", "title": "Predicting Patient State-of-Health using Sliding Window and Recurrent\n  Classifiers", "comments": "NIPS 2016 Workshop on Machine Learning for Health", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Bedside monitors in Intensive Care Units (ICUs) frequently sound incorrectly,\nslowing response times and desensitising nurses to alarms (Chambrin, 2001),\ncausing true alarms to be missed (Hug et al., 2011). We compare sliding window\npredictors with recurrent predictors to classify patient state-of-health from\nICU multivariate time series; we report slightly improved performance for the\nRNN for three out of four targets.\n", "versions": [{"version": "v1", "created": "Fri, 2 Dec 2016 12:44:31 GMT"}], "update_date": "2016-12-05", "authors_parsed": [["McCarthy", "Adam", ""], ["Williams", "Christopher K. I.", ""]]}, {"id": "1612.00667", "submitter": "Santi Puch", "authors": "Santi Puch, Asier Aduriz, Adri\\`a Casamitjana, Veronica Vilaplana,\n  Paula Petrone, Gr\\'egory Operto, Raffaele Cacciaglia, Stavros Skouras, Carles\n  Falcon, Jos\\'e Luis Molinuevo, Juan Domingo Gispert", "title": "Voxelwise nonlinear regression toolbox for neuroimage analysis:\n  Application to aging and neurodegenerative disease modeling", "comments": "4 pages + 1 page for acknowledgements and references. NIPS 2016\n  Workshop on Machine Learning for Health (NIPS ML4HC)", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.CV cs.LG q-bio.NC stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper describes a new neuroimaging analysis toolbox that allows for the\nmodeling of nonlinear effects at the voxel level, overcoming limitations of\nmethods based on linear models like the GLM. We illustrate its features using a\nrelevant example in which distinct nonlinear trajectories of Alzheimer's\ndisease related brain atrophy patterns were found across the full biological\nspectrum of the disease. The open-source toolbox presented in this paper is\navailable at https://github.com/imatge-upc/VNeAT.\n", "versions": [{"version": "v1", "created": "Fri, 2 Dec 2016 12:59:11 GMT"}, {"version": "v2", "created": "Sun, 5 Mar 2017 10:58:16 GMT"}, {"version": "v3", "created": "Tue, 18 Apr 2017 20:12:16 GMT"}], "update_date": "2017-04-20", "authors_parsed": [["Puch", "Santi", ""], ["Aduriz", "Asier", ""], ["Casamitjana", "Adri\u00e0", ""], ["Vilaplana", "Veronica", ""], ["Petrone", "Paula", ""], ["Operto", "Gr\u00e9gory", ""], ["Cacciaglia", "Raffaele", ""], ["Skouras", "Stavros", ""], ["Falcon", "Carles", ""], ["Molinuevo", "Jos\u00e9 Luis", ""], ["Gispert", "Juan Domingo", ""]]}, {"id": "1612.00750", "submitter": "Vladimir  Gligorijevi\\'c", "authors": "Vladimir Gligorijevic, Yannis Panagakis, Stefanos Zafeiriou", "title": "Non-Negative Matrix Factorizations for Multiplex Network Analysis", "comments": "12 pages, 4 figures, 3 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.SI stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Networks have been a general tool for representing, analyzing, and modeling\nrelational data arising in several domains. One of the most important aspect of\nnetwork analysis is community detection or network clustering. Until recently,\nthe major focus have been on discovering community structure in single (i.e.,\nmonoplex) networks. However, with the advent of relational data with multiple\nmodalities, multiplex networks, i.e., networks composed of multiple layers\nrepresenting different aspects of relations, have emerged. Consequently,\ncommunity detection in multiplex network, i.e., detecting clusters of nodes\nshared by all layers, has become a new challenge. In this paper, we propose\nNetwork Fusion for Composite Community Extraction (NF-CCE), a new class of\nalgorithms, based on four different non-negative matrix factorization models,\ncapable of extracting composite communities in multiplex networks. Each\nalgorithm works in two steps: first, it finds a non-negative, low-dimensional\nfeature representation of each network layer; then, it fuses the feature\nrepresentation of layers into a common non-negative, low-dimensional feature\nrepresentation via collective factorization. The composite clusters are\nextracted from the common feature representation. We demonstrate the superior\nperformance of our algorithms over the state-of-the-art methods on various\ntypes of multiplex networks, including biological, social, economic, citation,\nphone communication, and brain multiplex networks.\n", "versions": [{"version": "v1", "created": "Thu, 1 Dec 2016 04:39:30 GMT"}, {"version": "v2", "created": "Wed, 25 Jan 2017 20:54:08 GMT"}], "update_date": "2017-01-27", "authors_parsed": [["Gligorijevic", "Vladimir", ""], ["Panagakis", "Yannis", ""], ["Zafeiriou", "Stefanos", ""]]}, {"id": "1612.00767", "submitter": "Jost Tobias Springenberg", "authors": "Jost Tobias Springenberg, Aaron Klein, Stefan Falkner, Frank Hutter", "title": "Asynchronous Stochastic Gradient MCMC with Elastic Coupling", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.AI cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider parallel asynchronous Markov Chain Monte Carlo (MCMC) sampling\nfor problems where we can leverage (stochastic) gradients to define continuous\ndynamics which explore the target distribution. We outline a solution strategy\nfor this setting based on stochastic gradient Hamiltonian Monte Carlo sampling\n(SGHMC) which we alter to include an elastic coupling term that ties together\nmultiple MCMC instances. The proposed strategy turns inherently sequential HMC\nalgorithms into asynchronous parallel versions. First experiments empirically\nshow that the resulting parallel sampler significantly speeds up exploration of\nthe target distribution, when compared to standard SGHMC, and is less prone to\nthe harmful effects of stale gradients than a naive parallelization approach.\n", "versions": [{"version": "v1", "created": "Fri, 2 Dec 2016 17:43:33 GMT"}, {"version": "v2", "created": "Thu, 8 Dec 2016 09:19:30 GMT"}], "update_date": "2016-12-09", "authors_parsed": [["Springenberg", "Jost Tobias", ""], ["Klein", "Aaron", ""], ["Falkner", "Stefan", ""], ["Hutter", "Frank", ""]]}, {"id": "1612.00775", "submitter": "Christopher Beckham", "authors": "Christopher Beckham, Christopher Pal", "title": "A simple squared-error reformulation for ordinal classification", "comments": "v1: Camera-ready abstract for NIPS for Health Workshop (2016) v2:\n  Clean-up of some sections, added appendix section where we briefly explore\n  optimisation of quadratic weighted kappa (QWK)", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we explore ordinal classification (in the context of deep\nneural networks) through a simple modification of the squared error loss which\nnot only allows it to not only be sensitive to class ordering, but also allows\nthe possibility of having a discrete probability distribution over the classes.\nOur formulation is based on the use of a softmax hidden layer, which has\nreceived relatively little attention in the literature. We empirically evaluate\nits performance on the Kaggle diabetic retinopathy dataset, an ordinal and\nhigh-resolution dataset and show that it outperforms all of the baselines\nemployed.\n", "versions": [{"version": "v1", "created": "Fri, 2 Dec 2016 17:57:04 GMT"}, {"version": "v2", "created": "Mon, 9 Jan 2017 16:04:38 GMT"}], "update_date": "2017-01-10", "authors_parsed": [["Beckham", "Christopher", ""], ["Pal", "Christopher", ""]]}, {"id": "1612.00796", "submitter": "Raia Hadsell", "authors": "James Kirkpatrick, Razvan Pascanu, Neil Rabinowitz, Joel Veness,\n  Guillaume Desjardins, Andrei A. Rusu, Kieran Milan, John Quan, Tiago Ramalho,\n  Agnieszka Grabska-Barwinska, Demis Hassabis, Claudia Clopath, Dharshan\n  Kumaran, Raia Hadsell", "title": "Overcoming catastrophic forgetting in neural networks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The ability to learn tasks in a sequential fashion is crucial to the\ndevelopment of artificial intelligence. Neural networks are not, in general,\ncapable of this and it has been widely thought that catastrophic forgetting is\nan inevitable feature of connectionist models. We show that it is possible to\novercome this limitation and train networks that can maintain expertise on\ntasks which they have not experienced for a long time. Our approach remembers\nold tasks by selectively slowing down learning on the weights important for\nthose tasks. We demonstrate our approach is scalable and effective by solving a\nset of classification tasks based on the MNIST hand written digit dataset and\nby learning several Atari 2600 games sequentially.\n", "versions": [{"version": "v1", "created": "Fri, 2 Dec 2016 19:18:37 GMT"}, {"version": "v2", "created": "Wed, 25 Jan 2017 13:01:51 GMT"}], "update_date": "2017-01-26", "authors_parsed": [["Kirkpatrick", "James", ""], ["Pascanu", "Razvan", ""], ["Rabinowitz", "Neil", ""], ["Veness", "Joel", ""], ["Desjardins", "Guillaume", ""], ["Rusu", "Andrei A.", ""], ["Milan", "Kieran", ""], ["Quan", "John", ""], ["Ramalho", "Tiago", ""], ["Grabska-Barwinska", "Agnieszka", ""], ["Hassabis", "Demis", ""], ["Clopath", "Claudia", ""], ["Kumaran", "Dharshan", ""], ["Hadsell", "Raia", ""]]}, {"id": "1612.00804", "submitter": "Ethan R. Elenberg", "authors": "Ethan R. Elenberg, Rajiv Khanna, Alexandros G. Dimakis, Sahand\n  Negahban", "title": "Restricted Strong Convexity Implies Weak Submodularity", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.IT cs.LG math.IT", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We connect high-dimensional subset selection and submodular maximization. Our\nresults extend the work of Das and Kempe (2011) from the setting of linear\nregression to arbitrary objective functions. For greedy feature selection, this\nconnection allows us to obtain strong multiplicative performance bounds on\nseveral methods without statistical modeling assumptions. We also derive\nrecovery guarantees of this form under standard assumptions. Our work shows\nthat greedy algorithms perform within a constant factor from the best possible\nsubset-selection solution for a broad class of general objective functions. Our\nmethods allow a direct control over the number of obtained features as opposed\nto regularization parameters that only implicitly control sparsity. Our proof\ntechnique uses the concept of weak submodularity initially defined by Das and\nKempe. We draw a connection between convex analysis and submodular set function\ntheory which may be of independent interest for other statistical learning\napplications that have combinatorial structure.\n", "versions": [{"version": "v1", "created": "Fri, 2 Dec 2016 19:32:55 GMT"}, {"version": "v2", "created": "Thu, 12 Oct 2017 05:25:40 GMT"}], "update_date": "2017-10-13", "authors_parsed": [["Elenberg", "Ethan R.", ""], ["Khanna", "Rajiv", ""], ["Dimakis", "Alexandros G.", ""], ["Negahban", "Sahand", ""]]}, {"id": "1612.00824", "submitter": "Ingo Steinwart", "authors": "Ingo Steinwart and Philipp Thomann and Nico Schmid", "title": "Learning with Hierarchical Gaussian Kernels", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We investigate iterated compositions of weighted sums of Gaussian kernels and\nprovide an interpretation of the construction that shows some similarities with\nthe architectures of deep neural networks. On the theoretical side, we show\nthat these kernels are universal and that SVMs using these kernels are\nuniversally consistent. We further describe a parameter optimization method for\nthe kernel parameters and empirically compare this method to SVMs, random\nforests, a multiple kernel learning approach, and to some deep neural networks.\n", "versions": [{"version": "v1", "created": "Fri, 2 Dec 2016 20:23:31 GMT"}], "update_date": "2016-12-05", "authors_parsed": [["Steinwart", "Ingo", ""], ["Thomann", "Philipp", ""], ["Schmid", "Nico", ""]]}, {"id": "1612.00840", "submitter": "Soumi Chaki", "authors": "Soumi Chaki, Aurobinda Routray, William K. Mohanty, Mamata Jenamani", "title": "A novel multiclassSVM based framework to classify lithology from well\n  logs: a real-world application", "comments": "5 pages, 5 figures, 4 tables Presented at INDICON 2015 at New Delhi,\n  India", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.AP stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Support vector machines (SVMs) have been recognized as a potential tool for\nsupervised classification analyses in different domains of research. In\nessence, SVM is a binary classifier. Therefore, in case of a multiclass\nproblem, the problem is divided into a series of binary problems which are\nsolved by binary classifiers, and finally the classification results are\ncombined following either the one-against-one or one-against-all strategies. In\nthis paper, an attempt has been made to classify lithology using a multiclass\nSVM based framework using well logs as predictor variables. Here, the lithology\nis classified into four classes such as sand, shaly sand, sandy shale and shale\nbased on the relative values of sand and shale fractions as suggested by an\nexpert geologist. The available dataset consisting well logs (gamma ray,\nneutron porosity, density, and P-sonic) and class information from four closely\nspaced wells from an onshore hydrocarbon field is divided into training and\ntesting sets. We have used one-against-all strategy to combine the results of\nmultiple binary classifiers. The reported results established the superiority\nof multiclass SVM compared to other classifiers in terms of classification\naccuracy. The selection of kernel function and associated parameters has also\nbeen investigated here. It can be envisaged from the results achieved in this\nstudy that the proposed framework based on multiclass SVM can further be used\nto solve classification problems. In future research endeavor, seismic\nattributes can be introduced in the framework to classify the lithology\nthroughout a study area from seismic inputs.\n", "versions": [{"version": "v1", "created": "Fri, 2 Dec 2016 07:55:16 GMT"}], "update_date": "2016-12-06", "authors_parsed": [["Chaki", "Soumi", ""], ["Routray", "Aurobinda", ""], ["Mohanty", "William K.", ""], ["Jenamani", "Mamata", ""]]}, {"id": "1612.00841", "submitter": "Soumi Chaki", "authors": "Soumi Chaki, Akhilesh Kumar Verma, Aurobinda Routray, William K.\n  Mohanty, Mamata Jenamani", "title": "A Novel Framework based on SVDD to Classify Water Saturation from\n  Seismic Attributes", "comments": "6 pages, 8 figures, 2table Presented at Fourth International\n  Conference on Emerging Applications of Information Technology (EAIT 2014),\n  ISI Kolkata, India", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Water saturation is an important property in reservoir engineering domain.\nThus, satisfactory classification of water saturation from seismic attributes\nis beneficial for reservoir characterization. However, diverse and non-linear\nnature of subsurface attributes makes the classification task difficult. In\nthis context, this paper proposes a generalized Support Vector Data Description\n(SVDD) based novel classification framework to classify water saturation into\ntwo classes (Class high and Class low) from three seismic attributes seismic\nimpedance, amplitude envelop, and seismic sweetness. G-metric means and program\nexecution time are used to quantify the performance of the proposed framework\nalong with established supervised classifiers. The documented results imply\nthat the proposed framework is superior to existing classifiers. The present\nstudy is envisioned to contribute in further reservoir modeling.\n", "versions": [{"version": "v1", "created": "Fri, 2 Dec 2016 07:57:08 GMT"}], "update_date": "2016-12-06", "authors_parsed": [["Chaki", "Soumi", ""], ["Verma", "Akhilesh Kumar", ""], ["Routray", "Aurobinda", ""], ["Mohanty", "William K.", ""], ["Jenamani", "Mamata", ""]]}, {"id": "1612.00951", "submitter": "Tom Rainforth", "authors": "Tom Rainforth, Robert Cornish, Hongseok Yang, Frank Wood", "title": "On the Pitfalls of Nested Monte Carlo", "comments": "Appearing in NIPS Workshop on Advances in Approximate Bayesian\n  Inference 2016", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.CO stat.ME stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  There is an increasing interest in estimating expectations outside of the\nclassical inference framework, such as for models expressed as probabilistic\nprograms. Many of these contexts call for some form of nested inference to be\napplied. In this paper, we analyse the behaviour of nested Monte Carlo (NMC)\nschemes, for which classical convergence proofs are insufficient. We give\nconditions under which NMC will converge, establish a rate of convergence, and\nprovide empirical data that suggests that this rate is observable in practice.\nFinally, we prove that general-purpose nested inference schemes are inherently\nbiased. Our results serve to warn of the dangers associated with naive\ncomposition of inference and models.\n", "versions": [{"version": "v1", "created": "Sat, 3 Dec 2016 10:44:50 GMT"}], "update_date": "2016-12-06", "authors_parsed": [["Rainforth", "Tom", ""], ["Cornish", "Robert", ""], ["Yang", "Hongseok", ""], ["Wood", "Frank", ""]]}, {"id": "1612.00962", "submitter": "Leen De Baets", "authors": "Leen De Baets, Joeri Ruyssinck, Thomas Peiffer, Johan Decruyenaere,\n  Filip De Turck, Femke Ongenae, Tom Dhaene", "title": "Positive blood culture detection in time series data using a BiLSTM\n  network", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.NE q-bio.QM stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The presence of bacteria or fungi in the bloodstream of patients is abnormal\nand can lead to life-threatening conditions. A computational model based on a\nbidirectional long short-term memory artificial neural network, is explored to\nassist doctors in the intensive care unit to predict whether examination of\nblood cultures of patients will return positive. As input it uses nine\nmonitored clinical parameters, presented as time series data, collected from\n2177 ICU admissions at the Ghent University Hospital. Our main goal is to\ndetermine if general machine learning methods and more specific, temporal\nmodels, can be used to create an early detection system. This preliminary\nresearch obtains an area of 71.95% under the precision recall curve, proving\nthe potential of temporal neural networks in this context.\n", "versions": [{"version": "v1", "created": "Sat, 3 Dec 2016 12:16:21 GMT"}], "update_date": "2016-12-06", "authors_parsed": [["De Baets", "Leen", ""], ["Ruyssinck", "Joeri", ""], ["Peiffer", "Thomas", ""], ["Decruyenaere", "Johan", ""], ["De Turck", "Filip", ""], ["Ongenae", "Femke", ""], ["Dhaene", "Tom", ""]]}, {"id": "1612.00984", "submitter": "Corrado Monti", "authors": "Corrado Monti and Paolo Boldi", "title": "Estimating latent feature-feature interactions in large feature-rich\n  graphs", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.SI cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Real-world complex networks describe connections between objects; in reality,\nthose objects are often endowed with some kind of features. How does the\npresence or absence of such features interplay with the network link structure?\nAlthough the situation here described is truly ubiquitous, there is a limited\nbody of research dealing with large graphs of this kind. Many previous works\nconsidered homophily as the only possible transmission mechanism translating\nnode features into links. Other authors, instead, developed more sophisticated\nmodels, that are able to handle complex feature interactions, but are unfit to\nscale to very large networks. We expand on the MGJ model, where interactions\nbetween pairs of features can foster or discourage link formation. In this\nwork, we will investigate how to estimate the latent feature-feature\ninteractions in this model. We shall propose two solutions: the first one\nassumes feature independence and it is essentially based on Naive Bayes; the\nsecond one, which relaxes the independence assumption assumption, is based on\nperceptrons. In fact, we show it is possible to cast the model equation in\norder to see it as the prediction rule of a perceptron. We analyze how\nclassical results for the perceptrons can be interpreted in this context; then,\nwe define a fast and simple perceptron-like algorithm for this task, which can\nprocess $10^8$ links in minutes. We then compare these two techniques, first\nwith synthetic datasets that follows our model, gaining evidence that the Naive\nindependence assumptions are detrimental in practice. Secondly, we consider a\nreal, large-scale citation network where each node (i.e., paper) can be\ndescribed by different types of characteristics; there, our algorithm can\nassess how well each set of features can explain the links, and thus finding\nmeaningful latent feature-feature interactions.\n", "versions": [{"version": "v1", "created": "Sat, 3 Dec 2016 16:42:59 GMT"}, {"version": "v2", "created": "Sat, 7 Oct 2017 18:18:55 GMT"}], "update_date": "2017-10-10", "authors_parsed": [["Monti", "Corrado", ""], ["Boldi", "Paolo", ""]]}, {"id": "1612.01020", "submitter": "Simon Du", "authors": "Simon Shaolei Du, Jayanth Koushik, Aarti Singh, and Barnabas Poczos", "title": "Hypothesis Transfer Learning via Transformation Functions", "comments": "Accepted by NIPS 2017", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider the Hypothesis Transfer Learning (HTL) problem where one\nincorporates a hypothesis trained on the source domain into the learning\nprocedure of the target domain. Existing theoretical analysis either only\nstudies specific algorithms or only presents upper bounds on the generalization\nerror but not on the excess risk. In this paper, we propose a unified\nalgorithm-dependent framework for HTL through a novel notion of transformation\nfunction, which characterizes the relation between the source and the target\ndomains. We conduct a general risk analysis of this framework and in\nparticular, we show for the first time, if two domains are related, HTL enjoys\nfaster convergence rates of excess risks for Kernel Smoothing and Kernel Ridge\nRegression than those of the classical non-transfer learning settings.\nExperiments on real world data demonstrate the effectiveness of our framework.\n", "versions": [{"version": "v1", "created": "Sat, 3 Dec 2016 21:22:43 GMT"}, {"version": "v2", "created": "Sun, 26 Feb 2017 23:00:23 GMT"}, {"version": "v3", "created": "Sat, 27 May 2017 21:28:27 GMT"}, {"version": "v4", "created": "Sun, 5 Nov 2017 16:24:27 GMT"}], "update_date": "2017-11-07", "authors_parsed": [["Du", "Simon Shaolei", ""], ["Koushik", "Jayanth", ""], ["Singh", "Aarti", ""], ["Poczos", "Barnabas", ""]]}, {"id": "1612.01030", "submitter": "Alexandre Drouin", "authors": "Alexandre Drouin, Fr\\'ed\\'eric Raymond, Ga\\\"el Letarte St-Pierre,\n  Mario Marchand, Jacques Corbeil, Fran\\c{c}ois Laviolette", "title": "Large scale modeling of antimicrobial resistance with interpretable\n  classifiers", "comments": "Peer-reviewed and accepted for presentation at the Machine Learning\n  for Health Workshop, NIPS 2016, Barcelona, Spain", "journal-ref": null, "doi": null, "report-no": null, "categories": "q-bio.GN cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Antimicrobial resistance is an important public health concern that has\nimplications in the practice of medicine worldwide. Accurately predicting\nresistance phenotypes from genome sequences shows great promise in promoting\nbetter use of antimicrobial agents, by determining which antibiotics are likely\nto be effective in specific clinical cases. In healthcare, this would allow for\nthe design of treatment plans tailored for specific individuals, likely\nresulting in better clinical outcomes for patients with bacterial infections.\nIn this work, we present the recent work of Drouin et al. (2016) on using Set\nCovering Machines to learn highly interpretable models of antibiotic resistance\nand complement it by providing a large scale application of their method to the\nentire PATRIC database. We report prediction results for 36 new datasets and\npresent the Kover AMR platform, a new web-based tool allowing the visualization\nand interpretation of the generated models.\n", "versions": [{"version": "v1", "created": "Sat, 3 Dec 2016 22:52:44 GMT"}], "update_date": "2016-12-06", "authors_parsed": [["Drouin", "Alexandre", ""], ["Raymond", "Fr\u00e9d\u00e9ric", ""], ["St-Pierre", "Ga\u00ebl Letarte", ""], ["Marchand", "Mario", ""], ["Corbeil", "Jacques", ""], ["Laviolette", "Fran\u00e7ois", ""]]}, {"id": "1612.01055", "submitter": "Anna Goldenberg", "authors": "Lauren Erdman, Ekansh Sharma, Eva Unternahrer, Shantala Hari Dass,\n  Kieran ODonnell, Sara Mostafavi, Rachel Edgar, Michael Kobor, Helene\n  Gaudreau, Michael Meaney, Anna Goldenberg", "title": "Modeling trajectories of mental health: challenges and opportunities", "comments": "extended abstract for ML4HC at NIPS 2016, 4 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  More than two thirds of mental health problems have their onset during\nchildhood or adolescence. Identifying children at risk for mental illness later\nin life and predicting the type of illness is not easy. We set out to develop a\nplatform to define subtypes of childhood social-emotional development using\nlongitudinal, multifactorial trait-based measures. Subtypes discovered through\nthis study could ultimately advance psychiatric knowledge of the early\nbehavioural signs of mental illness. To this extent we have examined two types\nof models: latent class mixture models and GP-based models. Our findings\nindicate that while GP models come close in accuracy of predicting future\ntrajectories, LCMMs predict the trajectories as well in a fraction of the time.\nUnfortunately, neither of the models are currently accurate enough to lead to\nimmediate clinical impact. The available data related to the development of\nchildhood mental health is often sparse with only a few time points measured\nand require novel methods with improved efficiency and accuracy.\n", "versions": [{"version": "v1", "created": "Sun, 4 Dec 2016 03:20:54 GMT"}], "update_date": "2016-12-06", "authors_parsed": [["Erdman", "Lauren", ""], ["Sharma", "Ekansh", ""], ["Unternahrer", "Eva", ""], ["Dass", "Shantala Hari", ""], ["ODonnell", "Kieran", ""], ["Mostafavi", "Sara", ""], ["Edgar", "Rachel", ""], ["Kobor", "Michael", ""], ["Gaudreau", "Helene", ""], ["Meaney", "Michael", ""], ["Goldenberg", "Anna", ""]]}, {"id": "1612.01095", "submitter": "Jose M. Pe\\~na", "authors": "Jose M. Pe\\~na", "title": "Representing Independence Models with Elementary Triplets", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In an independence model, the triplets that represent conditional\nindependences between singletons are called elementary. It is known that the\nelementary triplets represent the independence model unambiguously under some\nconditions. In this paper, we show how this representation helps performing\nsome operations with independence models, such as finding the dominant triplets\nor a minimal independence map of an independence model, or computing the union\nor intersection of a pair of independence models, or performing causal\nreasoning. For the latter, we rephrase in terms of conditional independences\nsome of Pearl's results for computing causal effects.\n", "versions": [{"version": "v1", "created": "Sun, 4 Dec 2016 10:41:33 GMT"}], "update_date": "2016-12-06", "authors_parsed": [["Pe\u00f1a", "Jose M.", ""]]}, {"id": "1612.01103", "submitter": "Michael Tschannen", "authors": "Michael Tschannen and Helmut B\\\"olcskei", "title": "Robust nonparametric nearest neighbor random process clustering", "comments": "15 pages, 7 figures", "journal-ref": "IEEE Transactions on Signal Processing, Vol. 65, No. 22, pp.\n  6009-6023, Nov. 2017", "doi": "10.1109/TSP.2017.2736513", "report-no": null, "categories": "cs.LG cs.IT math.IT stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider the problem of clustering noisy finite-length observations of\nstationary ergodic random processes according to their generative models\nwithout prior knowledge of the model statistics and the number of generative\nmodels. Two algorithms, both using the $L^1$-distance between estimated power\nspectral densities (PSDs) as a measure of dissimilarity, are analyzed. The\nfirst one, termed nearest neighbor process clustering (NNPC), relies on\npartitioning the nearest neighbor graph of the observations via spectral\nclustering. The second algorithm, simply referred to as $k$-means (KM),\nconsists of a single $k$-means iteration with farthest point initialization and\nwas considered before in the literature, albeit with a different dissimilarity\nmeasure. We prove that both algorithms succeed with high probability in the\npresence of noise and missing entries, and even when the generative process\nPSDs overlap significantly, all provided that the observation length is\nsufficiently large. Our results quantify the tradeoff between the overlap of\nthe generative process PSDs, the observation length, the fraction of missing\nentries, and the noise variance. Finally, we provide extensive numerical\nresults for synthetic and real data and find that NNPC outperforms\nstate-of-the-art algorithms in human motion sequence clustering.\n", "versions": [{"version": "v1", "created": "Sun, 4 Dec 2016 11:38:06 GMT"}, {"version": "v2", "created": "Mon, 7 Aug 2017 16:11:31 GMT"}, {"version": "v3", "created": "Thu, 28 Sep 2017 05:27:08 GMT"}], "update_date": "2017-09-29", "authors_parsed": [["Tschannen", "Michael", ""], ["B\u00f6lcskei", "Helmut", ""]]}, {"id": "1612.01158", "submitter": "Andee Kaplan", "authors": "Andee Kaplan, Daniel Nordman, and Stephen Vardeman", "title": "Properties and Bayesian fitting of restricted Boltzmann machines", "comments": "20 pages, 13 figures", "journal-ref": null, "doi": "10.1002/sam.11396", "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A restricted Boltzmann machine (RBM) is an undirected graphical model\nconstructed for discrete or continuous random variables, with two layers, one\nhidden and one visible, and no conditional dependency within a layer. In recent\nyears, RBMs have risen to prominence due to their connection to deep learning.\nBy treating a hidden layer of one RBM as the visible layer in a second RBM, a\ndeep architecture can be created. RBMs are thought to thereby have the ability\nto encode very complex and rich structures in data, making them attractive for\nsupervised learning. However, the generative behavior of RBMs is largely\nunexplored and typical fitting methodology does not easily allow for\nuncertainty quantification in addition to point estimates. In this paper, we\ndiscuss the relationship between RBM parameter specification in the binary case\nand model properties such as degeneracy, instability and uninterpretability. We\nalso describe the associated difficulties that can arise with likelihood-based\ninference and further discuss the potential Bayes fitting of such (highly\nflexible) models, especially as Gibbs sampling (quasi-Bayes) methods are often\nadvocated for the RBM model structure.\n", "versions": [{"version": "v1", "created": "Sun, 4 Dec 2016 18:12:56 GMT"}, {"version": "v2", "created": "Thu, 7 Dec 2017 01:34:45 GMT"}, {"version": "v3", "created": "Tue, 25 Sep 2018 02:45:10 GMT"}], "update_date": "2019-09-12", "authors_parsed": [["Kaplan", "Andee", ""], ["Nordman", "Daniel", ""], ["Vardeman", "Stephen", ""]]}, {"id": "1612.01200", "submitter": "David Kale", "authors": "Tom Quisel, David C. Kale, Luca Foschini", "title": "Intra-day Activity Better Predicts Chronic Conditions", "comments": "Presented at the NIPS 2016 Workshop on Machine Learning for Health", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this work we investigate intra-day patterns of activity on a population of\n7,261 users of mobile health wearable devices and apps. We show that: (1) using\nintra-day step and sleep data recorded from passive trackers significantly\nimproves classification performance on self-reported chronic conditions related\nto mental health and nervous system disorders, (2) Convolutional Neural\nNetworks achieve top classification performance vs. baseline models when\ntrained directly on multivariate time series of activity data, and (3) jointly\npredicting all condition classes via multi-task learning can be leveraged to\nextract features that generalize across data sets and achieve the highest\nclassification performance.\n", "versions": [{"version": "v1", "created": "Sun, 4 Dec 2016 23:00:45 GMT"}], "update_date": "2016-12-06", "authors_parsed": [["Quisel", "Tom", ""], ["Kale", "David C.", ""], ["Foschini", "Luca", ""]]}, {"id": "1612.01205", "submitter": "Yu-Xiang Wang", "authors": "Yu-Xiang Wang and Alekh Agarwal and Miroslav Dudik", "title": "Optimal and Adaptive Off-policy Evaluation in Contextual Bandits", "comments": null, "journal-ref": "International Conference on Machine Learning (pp. 3589-3597)\n  (2017)", "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study the off-policy evaluation problem---estimating the value of a target\npolicy using data collected by another policy---under the contextual bandit\nmodel. We consider the general (agnostic) setting without access to a\nconsistent model of rewards and establish a minimax lower bound on the mean\nsquared error (MSE). The bound is matched up to constants by the inverse\npropensity scoring (IPS) and doubly robust (DR) estimators. This highlights the\ndifficulty of the agnostic contextual setting, in contrast with multi-armed\nbandits and contextual bandits with access to a consistent reward model, where\nIPS is suboptimal. We then propose the SWITCH estimator, which can use an\nexisting reward model (not necessarily consistent) to achieve a better\nbias-variance tradeoff than IPS and DR. We prove an upper bound on its MSE and\ndemonstrate its benefits empirically on a diverse collection of data sets,\noften outperforming prior work by orders of magnitude.\n", "versions": [{"version": "v1", "created": "Sun, 4 Dec 2016 23:24:17 GMT"}, {"version": "v2", "created": "Sat, 11 Nov 2017 05:57:11 GMT"}], "update_date": "2017-11-15", "authors_parsed": [["Wang", "Yu-Xiang", ""], ["Agarwal", "Alekh", ""], ["Dudik", "Miroslav", ""]]}, {"id": "1612.01216", "submitter": "Hoi-To Wai", "authors": "Hoi-To Wai and Jean Lafond and Anna Scaglione and Eric Moulines", "title": "Decentralized Frank-Wolfe Algorithm for Convex and Non-convex Problems", "comments": "Accepted to IEEE Transactions on Automatic Control. 33 pages, 7\n  figures, include an improved constant in Lemma 2", "journal-ref": null, "doi": "10.1109/TAC.2017.2685559", "report-no": null, "categories": "math.OC cs.SY stat.ML", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Decentralized optimization algorithms have received much attention due to the\nrecent advances in network information processing. However, conventional\ndecentralized algorithms based on projected gradient descent are incapable of\nhandling high dimensional constrained problems, as the projection step becomes\ncomputationally prohibitive to compute. To address this problem, this paper\nadopts a projection-free optimization approach, a.k.a.~the Frank-Wolfe (FW) or\nconditional gradient algorithm. We first develop a decentralized FW (DeFW)\nalgorithm from the classical FW algorithm. The convergence of the proposed\nalgorithm is studied by viewing the decentralized algorithm as an inexact FW\nalgorithm. Using a diminishing step size rule and letting $t$ be the iteration\nnumber, we show that the DeFW algorithm's convergence rate is ${\\cal O}(1/t)$\nfor convex objectives; is ${\\cal O}(1/t^2)$ for strongly convex objectives with\nthe optimal solution in the interior of the constraint set; and is ${\\cal\nO}(1/\\sqrt{t})$ towards a stationary point for smooth but non-convex\nobjectives. We then show that a consensus-based DeFW algorithm meets the above\nguarantees with two communication rounds per iteration. Furthermore, we\ndemonstrate the advantages of the proposed DeFW algorithm on low-complexity\nrobust matrix completion and communication efficient sparse learning. Numerical\nresults on synthetic and real data are presented to support our findings.\n", "versions": [{"version": "v1", "created": "Mon, 5 Dec 2016 01:40:36 GMT"}, {"version": "v2", "created": "Fri, 17 Mar 2017 18:37:59 GMT"}, {"version": "v3", "created": "Tue, 28 Aug 2018 14:38:40 GMT"}], "update_date": "2018-08-29", "authors_parsed": [["Wai", "Hoi-To", ""], ["Lafond", "Jean", ""], ["Scaglione", "Anna", ""], ["Moulines", "Eric", ""]]}, {"id": "1612.01251", "submitter": "Pedro Tabacof", "authors": "Ramon Oliveira, Pedro Tabacof, Eduardo Valle", "title": "Known Unknowns: Uncertainty Quality in Bayesian Neural Networks", "comments": "Workshop on Bayesian Deep Learning, NIPS 2016, Barcelona, Spain;\n  EDIT: Changed analysis from Logit-AUC space to AUC (with changes to Figs. 2\n  and 3)", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We evaluate the uncertainty quality in neural networks using anomaly\ndetection. We extract uncertainty measures (e.g. entropy) from the predictions\nof candidate models, use those measures as features for an anomaly detector,\nand gauge how well the detector differentiates known from unknown classes. We\nassign higher uncertainty quality to candidate models that lead to better\ndetectors. We also propose a novel method for sampling a variational\napproximation of a Bayesian neural network, called One-Sample Bayesian\nApproximation (OSBA). We experiment on two datasets, MNIST and CIFAR10. We\ncompare the following candidate neural network models: Maximum Likelihood,\nBayesian Dropout, OSBA, and --- for MNIST --- the standard variational\napproximation. We show that Bayesian Dropout and OSBA provide better\nuncertainty information than Maximum Likelihood, and are essentially equivalent\nto the standard variational approximation, but much faster.\n", "versions": [{"version": "v1", "created": "Mon, 5 Dec 2016 05:21:42 GMT"}, {"version": "v2", "created": "Fri, 23 Dec 2016 00:24:27 GMT"}], "update_date": "2016-12-26", "authors_parsed": [["Oliveira", "Ramon", ""], ["Tabacof", "Pedro", ""], ["Valle", "Eduardo", ""]]}, {"id": "1612.01254", "submitter": "Soheil Bahrampour", "authors": "Shengdong Zhang and Soheil Bahrampour and Naveen Ramakrishnan and\n  Mohak Shah", "title": "Deep Symbolic Representation Learning for Heterogeneous Time-series\n  Classification", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we consider the problem of event classification with\nmulti-variate time series data consisting of heterogeneous (continuous and\ncategorical) variables. The complex temporal dependencies between the variables\ncombined with sparsity of the data makes the event classification problem\nparticularly challenging. Most state-of-art approaches address this either by\ndesigning hand-engineered features or breaking up the problem over homogeneous\nvariates. In this work, we propose and compare three representation learning\nalgorithms over symbolized sequences which enables classification of\nheterogeneous time-series data using a deep architecture. The proposed\nrepresentations are trained jointly along with the rest of the network\narchitecture in an end-to-end fashion that makes the learned features\ndiscriminative for the given task. Experiments on three real-world datasets\ndemonstrate the effectiveness of the proposed approaches.\n", "versions": [{"version": "v1", "created": "Mon, 5 Dec 2016 05:53:47 GMT"}], "update_date": "2016-12-06", "authors_parsed": [["Zhang", "Shengdong", ""], ["Bahrampour", "Soheil", ""], ["Ramakrishnan", "Naveen", ""], ["Shah", "Mohak", ""]]}, {"id": "1612.01316", "submitter": "Konstantinos Sechidis", "authors": "Konstantinos Sechidis, Emily Turner, Paul D. Metcalfe, James\n  Weatherall and Gavin Brown", "title": "Ranking Biomarkers Through Mutual Information", "comments": "Accepted at NIPS 2016 Workshop on Machine Learning for Health", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study information theoretic methods for ranking biomarkers. In clinical\ntrials there are two, closely related, types of biomarkers: predictive and\nprognostic, and disentangling them is a key challenge. Our first step is to\nphrase biomarker ranking in terms of optimizing an information theoretic\nquantity. This formalization of the problem will enable us to derive rankings\nof predictive/prognostic biomarkers, by estimating different, high dimensional,\nconditional mutual information terms. To estimate these terms, we suggest\nefficient low dimensional approximations, and we derive an empirical Bayes\nestimator, which is suitable for small or sparse datasets. Finally, we\nintroduce a new visualisation tool that captures the prognostic and the\npredictive strength of a set of biomarkers. We believe this representation will\nprove to be a powerful tool in biomarker discovery.\n", "versions": [{"version": "v1", "created": "Mon, 5 Dec 2016 11:44:32 GMT"}], "update_date": "2016-12-06", "authors_parsed": [["Sechidis", "Konstantinos", ""], ["Turner", "Emily", ""], ["Metcalfe", "Paul D.", ""], ["Weatherall", "James", ""], ["Brown", "Gavin", ""]]}, {"id": "1612.01349", "submitter": "Soumi Chaki", "authors": "Soumi Chaki, Akhilesh Kumar Verma, Aurobinda Routray, William K.\n  Mohanty, Mamata Jenamani", "title": "A One class Classifier based Framework using SVDD : Application to an\n  Imbalanced Geological Dataset", "comments": "presented at IEEE Students Technology Symposium (TechSym), 28\n  February to 2 March 2014, IIT Kharagpur, India. 6 pages, 7 figures, 2tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.AP stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Evaluation of hydrocarbon reservoir requires classification of petrophysical\nproperties from available dataset. However, characterization of reservoir\nattributes is difficult due to the nonlinear and heterogeneous nature of the\nsubsurface physical properties. In this context, present study proposes a\ngeneralized one class classification framework based on Support Vector Data\nDescription (SVDD) to classify a reservoir characteristic water saturation into\ntwo classes (Class high and Class low) from four logs namely gamma ray, neutron\nporosity, bulk density, and P sonic using an imbalanced dataset. A comparison\nis carried out among proposed framework and different supervised classification\nalgorithms in terms of g metric means and execution time. Experimental results\nshow that proposed framework has outperformed other classifiers in terms of\nthese performance evaluators. It is envisaged that the classification analysis\nperformed in this study will be useful in further reservoir modeling.\n", "versions": [{"version": "v1", "created": "Fri, 2 Dec 2016 07:54:23 GMT"}], "update_date": "2016-12-06", "authors_parsed": [["Chaki", "Soumi", ""], ["Verma", "Akhilesh Kumar", ""], ["Routray", "Aurobinda", ""], ["Mohanty", "William K.", ""], ["Jenamani", "Mamata", ""]]}, {"id": "1612.01414", "submitter": "Alexander Jung", "authors": "Alexander Jung, Alfred O. Hero III, Alexandru Mara, and Saeed Jahromi", "title": "Semi-Supervised Learning via Sparse Label Propagation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This work proposes a novel method for semi-supervised learning from partially\nlabeled massive network-structured datasets, i.e., big data over networks. We\nmodel the underlying hypothesis, which relates data points to labels, as a\ngraph signal, defined over some graph (network) structure intrinsic to the\ndataset. Following the key principle of supervised learning, i.e., similar\ninputs yield similar outputs, we require the graph signals induced by labels to\nhave small total variation. Accordingly, we formulate the problem of learning\nthe labels of data points as a non-smooth convex optimization problem which\namounts to balancing between the empirical loss, i.e., the discrepancy with\nsome partially available label information, and the smoothness quantified by\nthe total variation of the learned graph signal. We solve this optimization\nproblem by appealing to a recently proposed preconditioned variant of the\npopular primal-dual method by Pock and Chambolle, which results in a sparse\nlabel propagation algorithm. This learning algorithm allows for a highly\nscalable implementation as message passing over the underlying data graph. By\napplying concepts of compressed sensing to the learning problem, we are also\nable to provide a transparent sufficient condition on the underlying network\nstructure such that accurate learning of the labels is possible. We also\npresent an implementation of the message passing formulation allows for a\nhighly scalable implementation in big data frameworks.\n", "versions": [{"version": "v1", "created": "Mon, 5 Dec 2016 16:04:38 GMT"}, {"version": "v2", "created": "Thu, 22 Dec 2016 15:41:31 GMT"}, {"version": "v3", "created": "Wed, 10 May 2017 16:57:05 GMT"}, {"version": "v4", "created": "Mon, 15 May 2017 07:53:13 GMT"}], "update_date": "2017-05-16", "authors_parsed": [["Jung", "Alexander", ""], ["Hero", "Alfred O.", "III"], ["Mara", "Alexandru", ""], ["Jahromi", "Saeed", ""]]}, {"id": "1612.01474", "submitter": "Balaji Lakshminarayanan", "authors": "Balaji Lakshminarayanan, Alexander Pritzel and Charles Blundell", "title": "Simple and Scalable Predictive Uncertainty Estimation using Deep\n  Ensembles", "comments": "NIPS 2017", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Deep neural networks (NNs) are powerful black box predictors that have\nrecently achieved impressive performance on a wide spectrum of tasks.\nQuantifying predictive uncertainty in NNs is a challenging and yet unsolved\nproblem. Bayesian NNs, which learn a distribution over weights, are currently\nthe state-of-the-art for estimating predictive uncertainty; however these\nrequire significant modifications to the training procedure and are\ncomputationally expensive compared to standard (non-Bayesian) NNs. We propose\nan alternative to Bayesian NNs that is simple to implement, readily\nparallelizable, requires very little hyperparameter tuning, and yields high\nquality predictive uncertainty estimates. Through a series of experiments on\nclassification and regression benchmarks, we demonstrate that our method\nproduces well-calibrated uncertainty estimates which are as good or better than\napproximate Bayesian NNs. To assess robustness to dataset shift, we evaluate\nthe predictive uncertainty on test examples from known and unknown\ndistributions, and show that our method is able to express higher uncertainty\non out-of-distribution examples. We demonstrate the scalability of our method\nby evaluating predictive uncertainty estimates on ImageNet.\n", "versions": [{"version": "v1", "created": "Mon, 5 Dec 2016 18:54:43 GMT"}, {"version": "v2", "created": "Mon, 27 Feb 2017 06:03:39 GMT"}, {"version": "v3", "created": "Sat, 4 Nov 2017 01:33:43 GMT"}], "update_date": "2017-11-07", "authors_parsed": [["Lakshminarayanan", "Balaji", ""], ["Pritzel", "Alexander", ""], ["Blundell", "Charles", ""]]}, {"id": "1612.01480", "submitter": "{\\L}ukasz Struski", "authors": "{\\L}ukasz Struski, Marek \\'Smieja, Jacek Tabor", "title": "Generalized RBF kernel for incomplete data", "comments": "9 pages, 7 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We construct $\\bf genRBF$ kernel, which generalizes the classical Gaussian\nRBF kernel to the case of incomplete data. We model the uncertainty contained\nin missing attributes making use of data distribution and associate every point\nwith a conditional probability density function. This allows to embed\nincomplete data into the function space and to define a kernel between two\nmissing data points based on scalar product in $L_2$. Experiments show that\nintroduced kernel applied to SVM classifier gives better results than other\nstate-of-the-art methods, especially in the case when large number of features\nis missing. Moreover, it is easy to implement and can be used together with any\nkernel approaches with no additional modifications.\n", "versions": [{"version": "v1", "created": "Mon, 5 Dec 2016 19:07:06 GMT"}, {"version": "v2", "created": "Tue, 2 May 2017 07:50:48 GMT"}], "update_date": "2017-05-03", "authors_parsed": [["Struski", "\u0141ukasz", ""], ["\u015amieja", "Marek", ""], ["Tabor", "Jacek", ""]]}, {"id": "1612.01481", "submitter": "Ehtsham Elahi", "authors": "Ehtsham Elahi", "title": "A Nonparametric Latent Factor Model For Location-Aware Video\n  Recommendations", "comments": "NIPS 2016 Workshop on Practical Bayesian Nonparametrics", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We are interested in learning customers' video preferences from their\nhistoric viewing patterns and geographical location. We consider a Bayesian\nlatent factor modeling approach for this task. In order to tune the complexity\nof the model to best represent the data, we make use of Bayesian nonparameteric\ntechniques. We describe an inference technique that can scale to large\nreal-world data sets. Finally we show results obtained by applying the model to\na large internal Netflix data set, that illustrates that the model was able to\ncapture interesting relationships between viewing patterns and geographical\nlocation.\n", "versions": [{"version": "v1", "created": "Mon, 5 Dec 2016 19:12:00 GMT"}], "update_date": "2016-12-06", "authors_parsed": [["Elahi", "Ehtsham", ""]]}, {"id": "1612.01489", "submitter": "S\\'ebastien Loustau", "authors": "Yves Darmaillac and S\\'ebastien Loustau", "title": "MCMC Louvain for Online Community Detection", "comments": "12 pages, in progress, experiments are coming", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.SI physics.soc-ph stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We introduce a novel algorithm of community detection that maintains\ndynamically a community structure of a large network that evolves with time.\nThe algorithm maximizes the modularity index thanks to the construction of a\nrandomized hierarchical clustering based on a Monte Carlo Markov Chain (MCMC)\nmethod. Interestingly, it could be seen as a dynamization of Louvain algorithm\n(see Blondel et Al, 2008) where the aggregation step is replaced by the\nhierarchical instrumental probability.\n", "versions": [{"version": "v1", "created": "Mon, 5 Dec 2016 19:25:36 GMT"}], "update_date": "2016-12-06", "authors_parsed": [["Darmaillac", "Yves", ""], ["Loustau", "S\u00e9bastien", ""]]}, {"id": "1612.01490", "submitter": "Fang Liu", "authors": "Yinan Li and Fang Liu", "title": "Whiteout: Gaussian Adaptive Noise Regularization in Deep Neural Networks", "comments": null, "journal-ref": "Advances in Neural Networks (ISNN 2020): pp 176-189", "doi": "10.1007/978-3-030-64221-1_16", "report-no": "https://link.springer.com/chapter/10.1007/978-3-030-64221-1_16", "categories": "stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Noise injection (NI) is an efficient technique to mitigate over-fitting in\nneural networks (NNs). The Bernoulli NI procedure as implemented in dropout and\nshakeout has connections with $l_1$ and $l_2$ regularization for the NN model\nparameters. We propose whiteout, a family NI regularization techniques (NIRT)\nthrough injecting adaptive Gaussian noises during the training of NNs. Whiteout\nis the first NIRT than imposes a broad range of the $l_{\\gamma}$ sparsity\nregularization $(\\gamma\\in(0,2))$ without having to involving the $l_2$\nregularization. Whiteout can also be extended to offer regularizations similar\nto the adaptive lasso and group lasso. We establish the regularization effect\nof whiteout in the framework of generalized linear models with closed-form\npenalty terms and show that whiteout stabilizes the training of NNs with\ndecreased sensitivity to small perturbations in the input. We establish that\nthe noise-perturbed empirical loss function (pelf) with whiteout converges\nalmost surely to the ideal loss function (ilf), and the minimizer of the pelf\nis consistent for the minimizer of the ilf. We derive the tail bound on the\npelf to establish the practical feasibility in its minimization. The\nsuperiority of whiteout over the Bernoulli NIRTs, dropout and shakeout, in\nlearning NNs with relatively small-sized training sets and non-inferiority in\nlarge-sized training sets is demonstrated in both simulated and real-life data\nsets. This work represents the first in-depth theoretical, methodological, and\npractical examination of the regularization effects of both additive and\nmultiplicative Gaussian NI in deep NNs.\n", "versions": [{"version": "v1", "created": "Mon, 5 Dec 2016 19:27:59 GMT"}, {"version": "v2", "created": "Thu, 2 Mar 2017 00:38:39 GMT"}, {"version": "v3", "created": "Wed, 9 Aug 2017 05:13:34 GMT"}, {"version": "v4", "created": "Wed, 9 May 2018 23:17:20 GMT"}], "update_date": "2021-06-28", "authors_parsed": [["Li", "Yinan", ""], ["Liu", "Fang", ""]]}, {"id": "1612.01504", "submitter": "Shanshan Cao", "authors": "Shanshan Cao, Yao Xie", "title": "Dynamic change-point detection using similarity networks", "comments": "appeared in Asilomar Conference 2016", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.ML stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  From a sequence of similarity networks, with edges representing certain\nsimilarity measures between nodes, we are interested in detecting a\nchange-point which changes the statistical property of the networks. After the\nchange, a subset of anomalous nodes which compares dissimilarly with the normal\nnodes. We study a simple sequential change detection procedure based on\nnode-wise average similarity measures, and study its theoretical property.\nSimulation and real-data examples demonstrate such a simply stopping procedure\nhas reasonably good performance. We further discuss the faulty sensor isolation\n(estimating anomalous nodes) using community detection.\n", "versions": [{"version": "v1", "created": "Mon, 5 Dec 2016 20:27:29 GMT"}], "update_date": "2016-12-06", "authors_parsed": [["Cao", "Shanshan", ""], ["Xie", "Yao", ""]]}, {"id": "1612.01551", "submitter": "Patrick Komiske", "authors": "Patrick T. Komiske, Eric M. Metodiev, and Matthew D. Schwartz", "title": "Deep learning in color: towards automated quark/gluon jet discrimination", "comments": "23 pages, 9 figures, updated to JHEP version, added table of\n  contents, minor typos fixed", "journal-ref": "JHEP 01 (2017) 110", "doi": "10.1007/JHEP01(2017)110", "report-no": "MIT-CTP 4866", "categories": "hep-ph stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Artificial intelligence offers the potential to automate challenging\ndata-processing tasks in collider physics. To establish its prospects, we\nexplore to what extent deep learning with convolutional neural networks can\ndiscriminate quark and gluon jets better than observables designed by\nphysicists. Our approach builds upon the paradigm that a jet can be treated as\nan image, with intensity given by the local calorimeter deposits. We supplement\nthis construction by adding color to the images, with red, green and blue\nintensities given by the transverse momentum in charged particles, transverse\nmomentum in neutral particles, and pixel-level charged particle counts.\nOverall, the deep networks match or outperform traditional jet variables. We\nalso find that, while various simulations produce different quark and gluon\njets, the neural networks are surprisingly insensitive to these differences,\nsimilar to traditional observables. This suggests that the networks can extract\nrobust physical information from imperfect simulations.\n", "versions": [{"version": "v1", "created": "Mon, 5 Dec 2016 21:18:00 GMT"}, {"version": "v2", "created": "Fri, 3 Feb 2017 01:13:44 GMT"}, {"version": "v3", "created": "Tue, 4 Sep 2018 20:40:14 GMT"}], "update_date": "2018-09-06", "authors_parsed": [["Komiske", "Patrick T.", ""], ["Metodiev", "Eric M.", ""], ["Schwartz", "Matthew D.", ""]]}, {"id": "1612.01589", "submitter": "Konrad \\.Zo{\\l}na", "authors": "Konrad Zolna", "title": "Improving the Performance of Neural Networks in Regression Tasks Using\n  Drawering", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI cs.NE stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The method presented extends a given regression neural network to make its\nperformance improve. The modification affects the learning procedure only,\nhence the extension may be easily omitted during evaluation without any change\nin prediction. It means that the modified model may be evaluated as quickly as\nthe original one but tends to perform better.\n  This improvement is possible because the modification gives better expressive\npower, provides better behaved gradients and works as a regularization. The\nknowledge gained by the temporarily extended neural network is contained in the\nparameters shared with the original neural network.\n  The only cost is an increase in learning time.\n", "versions": [{"version": "v1", "created": "Mon, 5 Dec 2016 23:28:54 GMT"}], "update_date": "2016-12-07", "authors_parsed": [["Zolna", "Konrad", ""]]}, {"id": "1612.01600", "submitter": "Cesar A. Uribe", "authors": "Angelia Nedi\\'c, Alex Olshevsky and C\\'esar A. Uribe", "title": "Distributed Gaussian Learning over Time-varying Directed Graphs", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.OC cs.LG cs.MA cs.SY stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a distributed (non-Bayesian) learning algorithm for the problem of\nparameter estimation with Gaussian noise. The algorithm is expressed as\nexplicit updates on the parameters of the Gaussian beliefs (i.e. means and\nprecision). We show a convergence rate of $O(1/k)$ with the constant term\ndepending on the number of agents and the topology of the network. Moreover, we\nshow almost sure convergence to the optimal solution of the estimation problem\nfor the general case of time-varying directed graphs.\n", "versions": [{"version": "v1", "created": "Tue, 6 Dec 2016 00:15:33 GMT"}, {"version": "v2", "created": "Wed, 7 Dec 2016 03:39:59 GMT"}], "update_date": "2016-12-08", "authors_parsed": [["Nedi\u0107", "Angelia", ""], ["Olshevsky", "Alex", ""], ["Uribe", "C\u00e9sar A.", ""]]}, {"id": "1612.01678", "submitter": "Michael Hughes", "authors": "Michael C. Hughes, Huseyin Melih Elibol, Thomas McCoy, Roy Perlis, and\n  Finale Doshi-Velez", "title": "Supervised topic models for clinical interpretability", "comments": "Accepted poster presentation at NIPS 2016 Workshop on Machine\n  Learning for Health (http://www.nipsml4hc.ws/)", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Supervised topic models can help clinical researchers find interpretable\ncooccurence patterns in count data that are relevant for diagnostics. However,\nstandard formulations of supervised Latent Dirichlet Allocation have two\nproblems. First, when documents have many more words than labels, the influence\nof the labels will be negligible. Second, due to conditional independence\nassumptions in the graphical model the impact of supervised labels on the\nlearned topic-word probabilities is often minimal, leading to poor predictions\non heldout data. We investigate penalized optimization methods for training\nsLDA that produce interpretable topic-word parameters and useful heldout\npredictions, using recognition networks to speed-up inference. We report\npreliminary results on synthetic data and on predicting successful\nanti-depressant medication given a patient's diagnostic history.\n", "versions": [{"version": "v1", "created": "Tue, 6 Dec 2016 06:07:55 GMT"}], "update_date": "2016-12-07", "authors_parsed": [["Hughes", "Michael C.", ""], ["Elibol", "Huseyin Melih", ""], ["McCoy", "Thomas", ""], ["Perlis", "Roy", ""], ["Doshi-Velez", "Finale", ""]]}, {"id": "1612.01746", "submitter": "Peter Karkus", "authors": "Peter Karkus, Andras Kupcsik, David Hsu, Wee Sun Lee", "title": "Factored Contextual Policy Search with Bayesian Optimization", "comments": "BayesOpt 2016, NeurIPS Workshop. A full paper extension is available\n  at arXiv:1904.11761", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI cs.RO stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Scarce data is a major challenge to scaling robot learning to truly complex\ntasks, as we need to generalize locally learned policies over different\n\"contexts\". Bayesian optimization approaches to contextual policy search (CPS)\noffer data-efficient policy learning that generalize over a context space. We\npropose to improve data-efficiency by factoring typically considered contexts\ninto two components: target-type contexts that correspond to a desired outcome\nof the learned behavior, e.g. target position for throwing a ball; and\nenvironment type contexts that correspond to some state of the environment,\ne.g. initial ball position or wind speed. Our key observation is that\nexperience can be directly generalized over target-type contexts. Based on that\nwe introduce Factored Contextual Policy Search with Bayesian Optimization for\nboth passive and active learning settings. Preliminary results show faster\npolicy generalization on a simulated toy problem. A full paper extension is\navailable at arXiv:1904.11761\n", "versions": [{"version": "v1", "created": "Tue, 6 Dec 2016 10:51:51 GMT"}, {"version": "v2", "created": "Tue, 28 May 2019 04:08:29 GMT"}], "update_date": "2019-05-29", "authors_parsed": [["Karkus", "Peter", ""], ["Kupcsik", "Andras", ""], ["Hsu", "David", ""], ["Lee", "Wee Sun", ""]]}, {"id": "1612.01756", "submitter": "Francesco Cricri", "authors": "Francesco Cricri, Xingyang Ni, Mikko Honkala, Emre Aksu, Moncef\n  Gabbouj", "title": "Video Ladder Networks", "comments": "This version extends the paper accepted at the NIPS 2016 workshop on\n  ML for Spatiotemporal Forecasting, with more details and more experimental\n  results", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present the Video Ladder Network (VLN) for efficiently generating future\nvideo frames. VLN is a neural encoder-decoder model augmented at all layers by\nboth recurrent and feedforward lateral connections. At each layer, these\nconnections form a lateral recurrent residual block, where the feedforward\nconnection represents a skip connection and the recurrent connection represents\nthe residual. Thanks to the recurrent connections, the decoder can exploit\ntemporal summaries generated from all layers of the encoder. This way, the top\nlayer is relieved from the pressure of modeling lower-level spatial and\ntemporal details. Furthermore, we extend the basic version of VLN to\nincorporate ResNet-style residual blocks in the encoder and decoder, which help\nimproving the prediction results. VLN is trained in self-supervised regime on\nthe Moving MNIST dataset, achieving competitive results while having very\nsimple structure and providing fast inference.\n", "versions": [{"version": "v1", "created": "Tue, 6 Dec 2016 11:15:28 GMT"}, {"version": "v2", "created": "Wed, 7 Dec 2016 11:35:22 GMT"}, {"version": "v3", "created": "Fri, 30 Dec 2016 09:01:02 GMT"}], "update_date": "2017-01-02", "authors_parsed": [["Cricri", "Francesco", ""], ["Ni", "Xingyang", ""], ["Honkala", "Mikko", ""], ["Aksu", "Emre", ""], ["Gabbouj", "Moncef", ""]]}, {"id": "1612.01871", "submitter": "Lei Wang", "authors": "Li Huang, Yi-feng Yang and Lei Wang", "title": "Recommender Engine for Continuous Time Quantum Monte Carlo Methods", "comments": "4.5 pages + half page supplementary material", "journal-ref": "Phys. Rev. E 95, 031301 (2017)", "doi": "10.1103/PhysRevE.95.031301", "report-no": null, "categories": "cond-mat.str-el physics.comp-ph stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recommender systems play an essential role in the modern business world. They\nrecommend favorable items like books, movies, and search queries to users based\non their past preferences. Applying similar ideas and techniques to Monte Carlo\nsimulations of physical systems boosts their efficiency without sacrificing\naccuracy. Exploiting the quantum to classical mapping inherent in the\ncontinuous-time quantum Monte Carlo methods, we construct a classical molecular\ngas model to reproduce the quantum distributions. We then utilize powerful\nmolecular simulation techniques to propose efficient quantum Monte Carlo\nupdates. The recommender engine approach provides a general way to speed up the\nquantum impurity solvers.\n", "versions": [{"version": "v1", "created": "Tue, 6 Dec 2016 15:39:09 GMT"}], "update_date": "2017-04-05", "authors_parsed": [["Huang", "Li", ""], ["Yang", "Yi-feng", ""], ["Wang", "Lei", ""]]}, {"id": "1612.01928", "submitter": "Dmitriy Serdyuk", "authors": "Dmitriy Serdyuk, Kartik Audhkhasi, Phil\\'emon Brakel, Bhuvana\n  Ramabhadran, Samuel Thomas, Yoshua Bengio", "title": "Invariant Representations for Noisy Speech Recognition", "comments": "5 pages, 1 figure, 1 table, NIPS workshop on end-to-end speech\n  recognition", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.CV cs.LG cs.SD stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Modern automatic speech recognition (ASR) systems need to be robust under\nacoustic variability arising from environmental, speaker, channel, and\nrecording conditions. Ensuring such robustness to variability is a challenge in\nmodern day neural network-based ASR systems, especially when all types of\nvariability are not seen during training. We attempt to address this problem by\nencouraging the neural network acoustic model to learn invariant feature\nrepresentations. We use ideas from recent research on image generation using\nGenerative Adversarial Networks and domain adaptation ideas extending\nadversarial gradient-based training. A recent work from Ganin et al. proposes\nto use adversarial training for image domain adaptation by using an\nintermediate representation from the main target classification network to\ndeteriorate the domain classifier performance through a separate neural\nnetwork. Our work focuses on investigating neural architectures which produce\nrepresentations invariant to noise conditions for ASR. We evaluate the proposed\narchitecture on the Aurora-4 task, a popular benchmark for noise robust ASR. We\nshow that our method generalizes better than the standard multi-condition\ntraining especially when only a few noise categories are seen during training.\n", "versions": [{"version": "v1", "created": "Sun, 27 Nov 2016 22:20:51 GMT"}], "update_date": "2016-12-07", "authors_parsed": [["Serdyuk", "Dmitriy", ""], ["Audhkhasi", "Kartik", ""], ["Brakel", "Phil\u00e9mon", ""], ["Ramabhadran", "Bhuvana", ""], ["Thomas", "Samuel", ""], ["Bengio", "Yoshua", ""]]}, {"id": "1612.01930", "submitter": "Jarno Hartog", "authors": "Jarno Hartog, Harry van Zanten", "title": "Nonparametric Bayesian label prediction on a graph", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.CO stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  An implementation of a nonparametric Bayesian approach to solving binary\nclassification problems on graphs is described. A hierarchical Bayesian\napproach with a randomly scaled Gaussian prior is considered. The prior uses\nthe graph Laplacian to take into account the underlying geometry of the graph.\nA method based on a theoretically optimal prior and a more flexible variant\nusing partial conjugacy are proposed. Two simulated data examples and two\nexamples using real data are used in order to illustrate the proposed methods.\n", "versions": [{"version": "v1", "created": "Tue, 6 Dec 2016 18:04:09 GMT"}, {"version": "v2", "created": "Thu, 15 Jun 2017 12:16:29 GMT"}], "update_date": "2017-06-16", "authors_parsed": [["Hartog", "Jarno", ""], ["van Zanten", "Harry", ""]]}, {"id": "1612.01936", "submitter": "Tan Nguyen", "authors": "Ankit B. Patel, Tan Nguyen, Richard G. Baraniuk", "title": "A Probabilistic Framework for Deep Learning", "comments": "arXiv admin note: substantial text overlap with arXiv:1504.00641", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We develop a probabilistic framework for deep learning based on the Deep\nRendering Mixture Model (DRMM), a new generative probabilistic model that\nexplicitly capture variations in data due to latent task nuisance variables. We\ndemonstrate that max-sum inference in the DRMM yields an algorithm that exactly\nreproduces the operations in deep convolutional neural networks (DCNs),\nproviding a first principles derivation. Our framework provides new insights\ninto the successes and shortcomings of DCNs as well as a principled route to\ntheir improvement. DRMM training via the Expectation-Maximization (EM)\nalgorithm is a powerful alternative to DCN back-propagation, and initial\ntraining results are promising. Classification based on the DRMM and other\nvariants outperforms DCNs in supervised digit classification, training 2-3x\nfaster while achieving similar accuracy. Moreover, the DRMM is applicable to\nsemi-supervised and unsupervised learning tasks, achieving results that are\nstate-of-the-art in several categories on the MNIST benchmark and comparable to\nstate of the art on the CIFAR10 benchmark.\n", "versions": [{"version": "v1", "created": "Tue, 6 Dec 2016 18:15:40 GMT"}], "update_date": "2016-12-07", "authors_parsed": [["Patel", "Ankit B.", ""], ["Nguyen", "Tan", ""], ["Baraniuk", "Richard G.", ""]]}, {"id": "1612.01942", "submitter": "Tan Nguyen", "authors": "Tan Nguyen, Wanjia Liu, Ethan Perez, Richard G. Baraniuk, Ankit B.\n  Patel", "title": "Semi-Supervised Learning with the Deep Rendering Mixture Model", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Semi-supervised learning algorithms reduce the high cost of acquiring labeled\ntraining data by using both labeled and unlabeled data during learning. Deep\nConvolutional Networks (DCNs) have achieved great success in supervised tasks\nand as such have been widely employed in the semi-supervised learning. In this\npaper we leverage the recently developed Deep Rendering Mixture Model (DRMM), a\nprobabilistic generative model that models latent nuisance variation, and whose\ninference algorithm yields DCNs. We develop an EM algorithm for the DRMM to\nlearn from both labeled and unlabeled data. Guided by the theory of the DRMM,\nwe introduce a novel non-negativity constraint and a variational inference\nterm. We report state-of-the-art performance on MNIST and SVHN and competitive\nresults on CIFAR10. We also probe deeper into how a DRMM trained in a\nsemi-supervised setting represents latent nuisance variation using\nsynthetically rendered images. Taken together, our work provides a unified\nframework for supervised, unsupervised, and semi-supervised learning.\n", "versions": [{"version": "v1", "created": "Tue, 6 Dec 2016 18:32:53 GMT"}], "update_date": "2016-12-07", "authors_parsed": [["Nguyen", "Tan", ""], ["Liu", "Wanjia", ""], ["Perez", "Ethan", ""], ["Baraniuk", "Richard G.", ""], ["Patel", "Ankit B.", ""]]}, {"id": "1612.01943", "submitter": "Yuhao Zhang", "authors": "Yuhao Zhang, Sandeep Ayyar, Long-Huei Chen, Ethan J. Li", "title": "Segmental Convolutional Neural Networks for Detection of Cardiac\n  Abnormality With Noisy Heart Sound Recordings", "comments": "This work was finished in May 2016, and remains unpublished until\n  December 2016 due to a request from the data provider", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.SD cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Heart diseases constitute a global health burden, and the problem is\nexacerbated by the error-prone nature of listening to and interpreting heart\nsounds. This motivates the development of automated classification to screen\nfor abnormal heart sounds. Existing machine learning-based systems achieve\naccurate classification of heart sound recordings but rely on expert features\nthat have not been thoroughly evaluated on noisy recordings. Here we propose a\nsegmental convolutional neural network architecture that achieves automatic\nfeature learning from noisy heart sound recordings. Our experiments show that\nour best model, trained on noisy recording segments acquired with an existing\nhidden semi-markov model-based approach, attains a classification accuracy of\n87.5% on the 2016 PhysioNet/CinC Challenge dataset, compared to the 84.6%\naccuracy of the state-of-the-art statistical classifier trained and evaluated\non the same dataset. Our results indicate the potential of using neural\nnetwork-based methods to increase the accuracy of automated classification of\nheart sound recordings for improved screening of heart diseases.\n", "versions": [{"version": "v1", "created": "Tue, 6 Dec 2016 18:37:30 GMT"}], "update_date": "2016-12-07", "authors_parsed": [["Zhang", "Yuhao", ""], ["Ayyar", "Sandeep", ""], ["Chen", "Long-Huei", ""], ["Li", "Ethan J.", ""]]}, {"id": "1612.01988", "submitter": "Abhishek Kumar", "authors": "Anant Raj, Abhishek Kumar, Youssef Mroueh, P. Thomas Fletcher,\n  Bernhard Sch\\\"olkopf", "title": "Local Group Invariant Representations via Orbit Embeddings", "comments": "AISTATS 2017 accepted version including appendix, 18 pages, 1 figure", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Invariance to nuisance transformations is one of the desirable properties of\neffective representations. We consider transformations that form a \\emph{group}\nand propose an approach based on kernel methods to derive local group invariant\nrepresentations. Locality is achieved by defining a suitable probability\ndistribution over the group which in turn induces distributions in the input\nfeature space. We learn a decision function over these distributions by\nappealing to the powerful framework of kernel methods and generate local\ninvariant random feature maps via kernel approximations. We show uniform\nconvergence bounds for kernel approximation and provide excess risk bounds for\nlearning with these features. We evaluate our method on three real datasets,\nincluding Rotated MNIST and CIFAR-10, and observe that it outperforms competing\nkernel based approaches. The proposed method also outperforms deep CNN on\nRotated-MNIST and performs comparably to the recently proposed\ngroup-equivariant CNN.\n", "versions": [{"version": "v1", "created": "Tue, 6 Dec 2016 20:46:39 GMT"}, {"version": "v2", "created": "Wed, 24 May 2017 16:50:08 GMT"}], "update_date": "2017-05-25", "authors_parsed": [["Raj", "Anant", ""], ["Kumar", "Abhishek", ""], ["Mroueh", "Youssef", ""], ["Fletcher", "P. Thomas", ""], ["Sch\u00f6lkopf", "Bernhard", ""]]}, {"id": "1612.02095", "submitter": "Evan Racah Mr.", "authors": "Evan Racah, Christopher Beckham, Tegan Maharaj, Samira Ebrahimi Kahou,\n  Prabhat, Christopher Pal", "title": "ExtremeWeather: A large-scale climate dataset for semi-supervised\n  detection, localization, and understanding of extreme weather events", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Then detection and identification of extreme weather events in large-scale\nclimate simulations is an important problem for risk management, informing\ngovernmental policy decisions and advancing our basic understanding of the\nclimate system. Recent work has shown that fully supervised convolutional\nneural networks (CNNs) can yield acceptable accuracy for classifying well-known\ntypes of extreme weather events when large amounts of labeled data are\navailable. However, many different types of spatially localized climate\npatterns are of interest including hurricanes, extra-tropical cyclones, weather\nfronts, and blocking events among others. Existing labeled data for these\npatterns can be incomplete in various ways, such as covering only certain years\nor geographic areas and having false negatives. This type of climate data\ntherefore poses a number of interesting machine learning challenges. We present\na multichannel spatiotemporal CNN architecture for semi-supervised bounding box\nprediction and exploratory data analysis. We demonstrate that our approach is\nable to leverage temporal information and unlabeled data to improve the\nlocalization of extreme weather events. Further, we explore the representations\nlearned by our model in order to better understand this important data. We\npresent a dataset, ExtremeWeather, to encourage machine learning research in\nthis area and to help facilitate further work in understanding and mitigating\nthe effects of climate change. The dataset is available at\nextremeweatherdataset.github.io and the code is available at\nhttps://github.com/eracah/hur-detect.\n", "versions": [{"version": "v1", "created": "Wed, 7 Dec 2016 01:46:09 GMT"}, {"version": "v2", "created": "Sat, 25 Nov 2017 23:44:46 GMT"}], "update_date": "2017-11-28", "authors_parsed": [["Racah", "Evan", ""], ["Beckham", "Christopher", ""], ["Maharaj", "Tegan", ""], ["Kahou", "Samira Ebrahimi", ""], ["Prabhat", "", ""], ["Pal", "Christopher", ""]]}, {"id": "1612.02099", "submitter": "Yu Lu", "authors": "Yu Lu and Harrison H. Zhou", "title": "Statistical and Computational Guarantees of Lloyd's Algorithm and its\n  Variants", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST cs.LG stat.ML stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Clustering is a fundamental problem in statistics and machine learning.\nLloyd's algorithm, proposed in 1957, is still possibly the most widely used\nclustering algorithm in practice due to its simplicity and empirical\nperformance. However, there has been little theoretical investigation on the\nstatistical and computational guarantees of Lloyd's algorithm. This paper is an\nattempt to bridge this gap between practice and theory. We investigate the\nperformance of Lloyd's algorithm on clustering sub-Gaussian mixtures. Under an\nappropriate initialization for labels or centers, we show that Lloyd's\nalgorithm converges to an exponentially small clustering error after an order\nof $\\log n$ iterations, where $n$ is the sample size. The error rate is shown\nto be minimax optimal. For the two-mixture case, we only require the\ninitializer to be slightly better than random guess.\n  In addition, we extend the Lloyd's algorithm and its analysis to community\ndetection and crowdsourcing, two problems that have received a lot of attention\nrecently in statistics and machine learning. Two variants of Lloyd's algorithm\nare proposed respectively for community detection and crowdsourcing. On the\ntheoretical side, we provide statistical and computational guarantees of the\ntwo algorithms, and the results improve upon some previous signal-to-noise\nratio conditions in literature for both problems. Experimental results on\nsimulated and real data sets demonstrate competitive performance of our\nalgorithms to the state-of-the-art methods.\n", "versions": [{"version": "v1", "created": "Wed, 7 Dec 2016 02:35:54 GMT"}], "update_date": "2016-12-08", "authors_parsed": [["Lu", "Yu", ""], ["Zhou", "Harrison H.", ""]]}, {"id": "1612.02130", "submitter": "Niek Tax", "authors": "Niek Tax, Ilya Verenich, Marcello La Rosa, Marlon Dumas", "title": "Predictive Business Process Monitoring with LSTM Neural Networks", "comments": "Accepted at the International Conference on Advanced Information\n  Systems Engineering (CAiSE) 2017", "journal-ref": "Lecture Notes in Computer Science, 10253 (2017) 477-492", "doi": "10.1007/978-3-319-59536-8_30", "report-no": null, "categories": "stat.AP cs.DB cs.LG cs.NE stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Predictive business process monitoring methods exploit logs of completed\ncases of a process in order to make predictions about running cases thereof.\nExisting methods in this space are tailor-made for specific prediction tasks.\nMoreover, their relative accuracy is highly sensitive to the dataset at hand,\nthus requiring users to engage in trial-and-error and tuning when applying them\nin a specific setting. This paper investigates Long Short-Term Memory (LSTM)\nneural networks as an approach to build consistently accurate models for a wide\nrange of predictive process monitoring tasks. First, we show that LSTMs\noutperform existing techniques to predict the next event of a running case and\nits timestamp. Next, we show how to use models for predicting the next task in\norder to predict the full continuation of a running case. Finally, we apply the\nsame approach to predict the remaining time, and show that this approach\noutperforms existing tailor-made methods.\n", "versions": [{"version": "v1", "created": "Wed, 7 Dec 2016 07:04:17 GMT"}, {"version": "v2", "created": "Tue, 16 May 2017 18:51:41 GMT"}], "update_date": "2017-12-20", "authors_parsed": [["Tax", "Niek", ""], ["Verenich", "Ilya", ""], ["La Rosa", "Marcello", ""], ["Dumas", "Marlon", ""]]}, {"id": "1612.02141", "submitter": "Aditya Balu", "authors": "Aditya Balu, Sambit Ghadai, Kin Gwn Lore, Gavin Young, Adarsh\n  Krishnamurthy and Soumik Sarkar", "title": "Learning Localized Geometric Features Using 3D-CNN: An Application to\n  Manufacturability Analysis of Drilled Holes", "comments": "9 Pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  3D convolutional neural networks (3D-CNN) have been used for object\nrecognition based on the voxelized shape of an object. In this paper, we\npresent a 3D-CNN based method to learn distinct local geometric features of\ninterest within an object. In this context, the voxelized representation may\nnot be sufficient to capture the distinguishing information about such local\nfeatures. To enable efficient learning, we augment the voxel data with surface\nnormals of the object boundary. We then train a 3D-CNN with this augmented data\nand identify the local features critical for decision-making using 3D\ngradient-weighted class activation maps. An application of this feature\nidentification framework is to recognize difficult-to-manufacture drilled hole\nfeatures in a complex CAD geometry. The framework can be extended to identify\ndifficult-to-manufacture features at multiple spatial scales leading to a\nreal-time decision support system for design for manufacturability.\n", "versions": [{"version": "v1", "created": "Wed, 7 Dec 2016 08:07:05 GMT"}, {"version": "v2", "created": "Thu, 22 Jun 2017 01:28:06 GMT"}], "update_date": "2017-07-18", "authors_parsed": [["Balu", "Aditya", ""], ["Ghadai", "Sambit", ""], ["Lore", "Kin Gwn", ""], ["Young", "Gavin", ""], ["Krishnamurthy", "Adarsh", ""], ["Sarkar", "Soumik", ""]]}, {"id": "1612.02161", "submitter": "Marco Cusumano-Towner", "authors": "Marco F. Cusumano-Towner, Vikash K. Mansinghka", "title": "Measuring the non-asymptotic convergence of sequential Monte Carlo\n  samplers using probabilistic programming", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A key limitation of sampling algorithms for approximate inference is that it\nis difficult to quantify their approximation error. Widely used sampling\nschemes, such as sequential importance sampling with resampling and\nMetropolis-Hastings, produce output samples drawn from a distribution that may\nbe far from the target posterior distribution. This paper shows how to\nupper-bound the symmetric KL divergence between the output distribution of a\nbroad class of sequential Monte Carlo (SMC) samplers and their target posterior\ndistributions, subject to assumptions about the accuracy of a separate\ngold-standard sampler. The proposed method applies to samplers that combine\nmultiple particles, multinomial resampling, and rejuvenation kernels. The\nexperiments show the technique being used to estimate bounds on the divergence\nof SMC samplers for posterior inference in a Bayesian linear regression model\nand a Dirichlet process mixture model.\n", "versions": [{"version": "v1", "created": "Wed, 7 Dec 2016 09:29:58 GMT"}, {"version": "v2", "created": "Sat, 6 May 2017 20:33:52 GMT"}], "update_date": "2017-05-09", "authors_parsed": [["Cusumano-Towner", "Marco F.", ""], ["Mansinghka", "Vikash K.", ""]]}, {"id": "1612.02179", "submitter": "Nir Ben-Zrihem", "authors": "Nir Baram, Oron Anschel, Shie Mannor", "title": "Model-based Adversarial Imitation Learning", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Generative adversarial learning is a popular new approach to training\ngenerative models which has been proven successful for other related problems\nas well. The general idea is to maintain an oracle $D$ that discriminates\nbetween the expert's data distribution and that of the generative model $G$.\nThe generative model is trained to capture the expert's distribution by\nmaximizing the probability of $D$ misclassifying the data it generates.\nOverall, the system is \\emph{differentiable} end-to-end and is trained using\nbasic backpropagation. This type of learning was successfully applied to the\nproblem of policy imitation in a model-free setup. However, a model-free\napproach does not allow the system to be differentiable, which requires the use\nof high-variance gradient estimations. In this paper we introduce the Model\nbased Adversarial Imitation Learning (MAIL) algorithm. A model-based approach\nfor the problem of adversarial imitation learning. We show how to use a forward\nmodel to make the system fully differentiable, which enables us to train\npolicies using the (stochastic) gradient of $D$. Moreover, our approach\nrequires relatively few environment interactions, and fewer hyper-parameters to\ntune. We test our method on the MuJoCo physics simulator and report initial\nresults that surpass the current state-of-the-art.\n", "versions": [{"version": "v1", "created": "Wed, 7 Dec 2016 10:10:31 GMT"}], "update_date": "2016-12-08", "authors_parsed": [["Baram", "Nir", ""], ["Anschel", "Oron", ""], ["Mannor", "Shie", ""]]}, {"id": "1612.02189", "submitter": "Evrim Acar", "authors": "Evrim Acar, Yuri Levin-Schwartz, Vince D. Calhoun and T\\\"ulay Adal{\\i}", "title": "Tensor-Based Fusion of EEG and FMRI to Understand Neurological Changes\n  in Schizophrenia", "comments": null, "journal-ref": null, "doi": "10.1109/ISCAS.2017.8050303", "report-no": null, "categories": "stat.AP q-bio.NC stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Neuroimaging modalities such as functional magnetic resonance imaging (fMRI)\nand electroencephalography (EEG) provide information about neurological\nfunctions in complementary spatiotemporal resolutions; therefore, fusion of\nthese modalities is expected to provide better understanding of brain activity.\nIn this paper, we jointly analyze fMRI and multi-channel EEG signals collected\nduring an auditory oddball task with the goal of capturing brain activity\npatterns that differ between patients with schizophrenia and healthy controls.\nRather than selecting a single electrode or matricizing the third-order tensor\nthat can be naturally used to represent multi-channel EEG signals, we preserve\nthe multi-way structure of EEG data and use a coupled matrix and tensor\nfactorization (CMTF) model to jointly analyze fMRI and EEG signals. Our\nanalysis reveals that (i) joint analysis of EEG and fMRI using a CMTF model can\ncapture meaningful temporal and spatial signatures of patterns that behave\ndifferently in patients and controls, and (ii) these differences and the\ninterpretability of the associated components increase by including multiple\nelectrodes from frontal, motor and parietal areas, but not necessarily by\nincluding all electrodes in the analysis.\n", "versions": [{"version": "v1", "created": "Wed, 7 Dec 2016 10:40:16 GMT"}], "update_date": "2020-12-23", "authors_parsed": [["Acar", "Evrim", ""], ["Levin-Schwartz", "Yuri", ""], ["Calhoun", "Vince D.", ""], ["Adal\u0131", "T\u00fclay", ""]]}, {"id": "1612.02192", "submitter": "Sergey Bartunov", "authors": "Sergey Bartunov, Dmitry P. Vetrov", "title": "Fast Adaptation in Generative Models with Generative Matching Networks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Despite recent advances, the remaining bottlenecks in deep generative models\nare necessity of extensive training and difficulties with generalization from\nsmall number of training examples. We develop a new generative model called\nGenerative Matching Network which is inspired by the recently proposed matching\nnetworks for one-shot learning in discriminative tasks. By conditioning on the\nadditional input dataset, our model can instantly learn new concepts that were\nnot available in the training data but conform to a similar generative process.\nThe proposed framework does not explicitly restrict diversity of the\nconditioning data and also does not require an extensive inference procedure\nfor training or adaptation. Our experiments on the Omniglot dataset demonstrate\nthat Generative Matching Networks significantly improve predictive performance\non the fly as more additional data is available and outperform existing state\nof the art conditional generative models.\n", "versions": [{"version": "v1", "created": "Wed, 7 Dec 2016 10:50:37 GMT"}, {"version": "v2", "created": "Tue, 5 Sep 2017 07:41:15 GMT"}], "update_date": "2017-09-06", "authors_parsed": [["Bartunov", "Sergey", ""], ["Vetrov", "Dmitry P.", ""]]}, {"id": "1612.02222", "submitter": "Binghong Chen", "authors": "Binghong Chen, Jun Zhu", "title": "A Communication-Efficient Parallel Method for Group-Lasso", "comments": "7 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Group-Lasso (gLasso) identifies important explanatory factors in predicting\nthe response variable by considering the grouping structure over input\nvariables. However, most existing algorithms for gLasso are not scalable to\ndeal with large-scale datasets, which are becoming a norm in many applications.\nIn this paper, we present a divide-and-conquer based parallel algorithm\n(DC-gLasso) to scale up gLasso in the tasks of regression with grouping\nstructures. DC-gLasso only needs two iterations to collect and aggregate the\nlocal estimates on subsets of the data, and is provably correct to recover the\ntrue model under certain conditions. We further extend it to deal with\noverlappings between groups. Empirical results on a wide range of synthetic and\nreal-world datasets show that DC-gLasso can significantly improve the time\nefficiency without sacrificing regression accuracy.\n", "versions": [{"version": "v1", "created": "Wed, 7 Dec 2016 12:32:44 GMT"}], "update_date": "2016-12-08", "authors_parsed": [["Chen", "Binghong", ""], ["Zhu", "Jun", ""]]}, {"id": "1612.02295", "submitter": "Weiyang Liu", "authors": "Weiyang Liu, Yandong Wen, Zhiding Yu, Meng Yang", "title": "Large-Margin Softmax Loss for Convolutional Neural Networks", "comments": "Published in ICML 2016 (with typo fixed)", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Cross-entropy loss together with softmax is arguably one of the most common\nused supervision components in convolutional neural networks (CNNs). Despite\nits simplicity, popularity and excellent performance, the component does not\nexplicitly encourage discriminative learning of features. In this paper, we\npropose a generalized large-margin softmax (L-Softmax) loss which explicitly\nencourages intra-class compactness and inter-class separability between learned\nfeatures. Moreover, L-Softmax not only can adjust the desired margin but also\ncan avoid overfitting. We also show that the L-Softmax loss can be optimized by\ntypical stochastic gradient descent. Extensive experiments on four benchmark\ndatasets demonstrate that the deeply-learned features with L-softmax loss\nbecome more discriminative, hence significantly boosting the performance on a\nvariety of visual classification and verification tasks.\n", "versions": [{"version": "v1", "created": "Wed, 7 Dec 2016 15:36:11 GMT"}, {"version": "v2", "created": "Tue, 17 Jan 2017 07:28:18 GMT"}, {"version": "v3", "created": "Mon, 23 Jan 2017 08:32:08 GMT"}, {"version": "v4", "created": "Fri, 17 Nov 2017 23:23:09 GMT"}], "update_date": "2017-11-21", "authors_parsed": [["Liu", "Weiyang", ""], ["Wen", "Yandong", ""], ["Yu", "Zhiding", ""], ["Yang", "Meng", ""]]}, {"id": "1612.02334", "submitter": "Xingguo Li", "authors": "Xingguo Li and Jarvis Haupt", "title": "Robust Low-Complexity Randomized Methods for Locating Outliers in Large\n  Matrices", "comments": "16 pages, 4 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IT cs.LG math.IT stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper examines the problem of locating outlier columns in a large,\notherwise low-rank matrix, in settings where {}{the data} are noisy, or where\nthe overall matrix has missing elements. We propose a randomized two-step\ninference framework, and establish sufficient conditions on the required sample\ncomplexities under which these methods succeed (with high probability) in\naccurately locating the outliers for each task. Comprehensive numerical\nexperimental results are provided to verify the theoretical bounds and\ndemonstrate the computational efficiency of the proposed algorithm.\n", "versions": [{"version": "v1", "created": "Wed, 7 Dec 2016 17:18:10 GMT"}], "update_date": "2016-12-12", "authors_parsed": [["Li", "Xingguo", ""], ["Haupt", "Jarvis", ""]]}, {"id": "1612.02460", "submitter": "Fahad Alhasoun", "authors": "Fahad Alhasoun, May Alhazzani, Marta C. Gonz\\'alez", "title": "Demographical Priors for Health Conditions Diagnosis Using Medicare Data", "comments": "NIPS 2016 Workshop on Machine Learning for Health", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper presents an example of how demographical characteristics of\npatients influence their susceptibility to certain medical conditions. In this\npaper, we investigate the association of health conditions to age of patients\nin a heterogeneous population. We show that besides the symptoms a patients is\nhaving, the age has the potential of aiding the diagnostic process in\nhospitals. Working with Electronic Health Records (EHR), we show that medical\nconditions group into clusters that share distinctive population age densities.\nWe use Electronic Health Records from Brazil for a period of 15 months from\nMarch of 2013 to July of 2014. The number of patients in the data is 1.7\nmillion patients and the number of records is 47 million records. The findings\nhas the potential of helping in a setting where an automated system undergoes\nthe task of predicting the condition of a patient given their symptoms and\ndemographical information.\n", "versions": [{"version": "v1", "created": "Wed, 7 Dec 2016 21:27:36 GMT"}, {"version": "v2", "created": "Mon, 9 Jan 2017 18:32:14 GMT"}], "update_date": "2017-01-10", "authors_parsed": [["Alhasoun", "Fahad", ""], ["Alhazzani", "May", ""], ["Gonz\u00e1lez", "Marta C.", ""]]}, {"id": "1612.02487", "submitter": "Luana Micallef", "authors": "Luana Micallef, Iiris Sundin, Pekka Marttinen, Muhammad Ammad-ud-din,\n  Tomi Peltola, Marta Soare, Giulio Jacucci, Samuel Kaski", "title": "Interactive Elicitation of Knowledge on Feature Relevance Improves\n  Predictions in Small Data Sets", "comments": "in Proceedings of the 22nd International Conference on Intelligent\n  User Interfaces (IUI 2017)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.LG stat.ML", "license": "http://creativecommons.org/licenses/by-sa/4.0/", "abstract": "  Providing accurate predictions is challenging for machine learning algorithms\nwhen the number of features is larger than the number of samples in the data.\nPrior knowledge can improve machine learning models by indicating relevant\nvariables and parameter values. Yet, this prior knowledge is often tacit and\nonly available from domain experts. We present a novel approach that uses\ninteractive visualization to elicit the tacit prior knowledge and uses it to\nimprove the accuracy of prediction models. The main component of our approach\nis a user model that models the domain expert's knowledge of the relevance of\ndifferent features for a prediction task. In particular, based on the expert's\nearlier input, the user model guides the selection of the features on which to\nelicit user's knowledge next. The results of a controlled user study show that\nthe user model significantly improves prior knowledge elicitation and\nprediction accuracy, when predicting the relative citation counts of scientific\ndocuments in a specific domain.\n", "versions": [{"version": "v1", "created": "Wed, 7 Dec 2016 23:31:26 GMT"}, {"version": "v2", "created": "Mon, 16 Jan 2017 15:16:05 GMT"}], "update_date": "2017-01-17", "authors_parsed": [["Micallef", "Luana", ""], ["Sundin", "Iiris", ""], ["Marttinen", "Pekka", ""], ["Ammad-ud-din", "Muhammad", ""], ["Peltola", "Tomi", ""], ["Soare", "Marta", ""], ["Jacucci", "Giulio", ""], ["Kaski", "Samuel", ""]]}, {"id": "1612.02516", "submitter": "Yichen Chen", "authors": "Yichen Chen and Mengdi Wang", "title": "Stochastic Primal-Dual Methods and Sample Complexity of Reinforcement\n  Learning", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.AI math.OC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study the online estimation of the optimal policy of a Markov decision\nprocess (MDP). We propose a class of Stochastic Primal-Dual (SPD) methods which\nexploit the inherent minimax duality of Bellman equations. The SPD methods\nupdate a few coordinates of the value and policy estimates as a new state\ntransition is observed. These methods use small storage and has low\ncomputational complexity per iteration. The SPD methods find an\nabsolute-$\\epsilon$-optimal policy, with high probability, using\n$\\mathcal{O}\\left(\\frac{|\\mathcal{S}|^4 |\\mathcal{A}|^2\\sigma^2\n}{(1-\\gamma)^6\\epsilon^2} \\right)$ iterations/samples for the infinite-horizon\ndiscounted-reward MDP and $\\mathcal{O}\\left(\\frac{|\\mathcal{S}|^4\n|\\mathcal{A}|^2H^6\\sigma^2 }{\\epsilon^2} \\right)$ for the finite-horizon MDP.\n", "versions": [{"version": "v1", "created": "Thu, 8 Dec 2016 03:05:41 GMT"}], "update_date": "2016-12-09", "authors_parsed": [["Chen", "Yichen", ""], ["Wang", "Mengdi", ""]]}, {"id": "1612.02526", "submitter": "Vatsal Sharan", "authors": "Vatsal Sharan, Sham Kakade, Percy Liang, Gregory Valiant", "title": "Prediction with a Short Memory", "comments": "Updates for STOC camera ready", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI cs.CC stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider the problem of predicting the next observation given a sequence\nof past observations, and consider the extent to which accurate prediction\nrequires complex algorithms that explicitly leverage long-range dependencies.\nPerhaps surprisingly, our positive results show that for a broad class of\nsequences, there is an algorithm that predicts well on average, and bases its\npredictions only on the most recent few observation together with a set of\nsimple summary statistics of the past observations. Specifically, we show that\nfor any distribution over observations, if the mutual information between past\nobservations and future observations is upper bounded by $I$, then a simple\nMarkov model over the most recent $I/\\epsilon$ observations obtains expected KL\nerror $\\epsilon$---and hence $\\ell_1$ error $\\sqrt{\\epsilon}$---with respect to\nthe optimal predictor that has access to the entire past and knows the data\ngenerating distribution. For a Hidden Markov Model with $n$ hidden states, $I$\nis bounded by $\\log n$, a quantity that does not depend on the mixing time, and\nwe show that the trivial prediction algorithm based on the empirical\nfrequencies of length $O(\\log n/\\epsilon)$ windows of observations achieves\nthis error, provided the length of the sequence is $d^{\\Omega(\\log\nn/\\epsilon)}$, where $d$ is the size of the observation alphabet.\n  We also establish that this result cannot be improved upon, even for the\nclass of HMMs, in the following two senses: First, for HMMs with $n$ hidden\nstates, a window length of $\\log n/\\epsilon$ is information-theoretically\nnecessary to achieve expected $\\ell_1$ error $\\sqrt{\\epsilon}$. Second, the\n$d^{\\Theta(\\log n/\\epsilon)}$ samples required to estimate the Markov model for\nan observation alphabet of size $d$ is necessary for any computationally\ntractable learning algorithm, assuming the hardness of strongly refuting a\ncertain class of CSPs.\n", "versions": [{"version": "v1", "created": "Thu, 8 Dec 2016 04:18:09 GMT"}, {"version": "v2", "created": "Mon, 10 Apr 2017 17:51:39 GMT"}, {"version": "v3", "created": "Thu, 9 Nov 2017 07:01:47 GMT"}, {"version": "v4", "created": "Sun, 27 May 2018 01:30:15 GMT"}, {"version": "v5", "created": "Thu, 28 Jun 2018 01:54:04 GMT"}], "update_date": "2018-06-29", "authors_parsed": [["Sharan", "Vatsal", ""], ["Kakade", "Sham", ""], ["Liang", "Percy", ""], ["Valiant", "Gregory", ""]]}, {"id": "1612.02528", "submitter": "Andreas Buja", "authors": "Andreas Buja and Werner Stuetzle", "title": "Smoothing Effects of Bagging: Von Mises Expansions of Bagged Statistical\n  Functionals", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Bagging is a device intended for reducing the prediction error of learning\nalgorithms. In its simplest form, bagging draws bootstrap samples from the\ntraining sample, applies the learning algorithm to each bootstrap sample, and\nthen averages the resulting prediction rules.\n  We extend the definition of bagging from statistics to statistical\nfunctionals and study the von Mises expansion of bagged statistical\nfunctionals. We show that the expansion is related to the Efron-Stein ANOVA\nexpansion of the raw (unbagged) functional. The basic observation is that a\nbagged functional is always smooth in the sense that the von Mises expansion\nexists and is finite of length 1 + resample size $M$. This holds even if the\nraw functional is rough or unstable. The resample size $M$ acts as a smoothing\nparameter, where a smaller $M$ means more smoothing.\n", "versions": [{"version": "v1", "created": "Thu, 8 Dec 2016 04:28:59 GMT"}], "update_date": "2016-12-09", "authors_parsed": [["Buja", "Andreas", ""], ["Stuetzle", "Werner", ""]]}, {"id": "1612.02572", "submitter": "Giovanni Montana", "authors": "James H Cole, Rudra PK Poudel, Dimosthenis Tsagkrasoulis, Matthan WA\n  Caan, Claire Steves, Tim D Spector, Giovanni Montana", "title": "Predicting brain age with deep learning from raw imaging data results in\n  a reliable and heritable biomarker", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.CV cs.LG q-bio.NC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Machine learning analysis of neuroimaging data can accurately predict\nchronological age in healthy people and deviations from healthy brain ageing\nhave been associated with cognitive impairment and disease. Here we sought to\nfurther establish the credentials of \"brain-predicted age\" as a biomarker of\nindividual differences in the brain ageing process, using a predictive\nmodelling approach based on deep learning, and specifically convolutional\nneural networks (CNN), and applied to both pre-processed and raw T1-weighted\nMRI data. Firstly, we aimed to demonstrate the accuracy of CNN brain-predicted\nage using a large dataset of healthy adults (N = 2001). Next, we sought to\nestablish the heritability of brain-predicted age using a sample of monozygotic\nand dizygotic female twins (N = 62). Thirdly, we examined the test-retest and\nmulti-centre reliability of brain-predicted age using two samples\n(within-scanner N = 20; between-scanner N = 11). CNN brain-predicted ages were\ngenerated and compared to a Gaussian Process Regression (GPR) approach, on all\ndatasets. Input data were grey matter (GM) or white matter (WM) volumetric maps\ngenerated by Statistical Parametric Mapping (SPM) or raw data. Brain-predicted\nage represents an accurate, highly reliable and genetically-valid phenotype,\nthat has potential to be used as a biomarker of brain ageing. Moreover, age\npredictions can be accurately generated on raw T1-MRI data, substantially\nreducing computation time for novel data, bringing the process closer to giving\nreal-time information on brain health in clinical settings.\n", "versions": [{"version": "v1", "created": "Thu, 8 Dec 2016 09:26:08 GMT"}], "update_date": "2016-12-09", "authors_parsed": [["Cole", "James H", ""], ["Poudel", "Rudra PK", ""], ["Tsagkrasoulis", "Dimosthenis", ""], ["Caan", "Matthan WA", ""], ["Steves", "Claire", ""], ["Spector", "Tim D", ""], ["Montana", "Giovanni", ""]]}, {"id": "1612.02589", "submitter": "Stergios Christodoulidis Mr.", "authors": "Stergios Christodoulidis, Marios Anthimopoulos, Lukas Ebner, Andreas\n  Christe and Stavroula Mougiakakou", "title": "Multi-source Transfer Learning with Convolutional Neural Networks for\n  Lung Pattern Analysis", "comments": null, "journal-ref": null, "doi": "10.1109/JBHI.2016.2636929", "report-no": null, "categories": "cs.CV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Early diagnosis of interstitial lung diseases is crucial for their treatment,\nbut even experienced physicians find it difficult, as their clinical\nmanifestations are similar. In order to assist with the diagnosis,\ncomputer-aided diagnosis (CAD) systems have been developed. These commonly rely\non a fixed scale classifier that scans CT images, recognizes textural lung\npatterns and generates a map of pathologies. In a previous study, we proposed a\nmethod for classifying lung tissue patterns using a deep convolutional neural\nnetwork (CNN), with an architecture designed for the specific problem. In this\nstudy, we present an improved method for training the proposed network by\ntransferring knowledge from the similar domain of general texture\nclassification. Six publicly available texture databases are used to pretrain\nnetworks with the proposed architecture, which are then fine-tuned on the lung\ntissue data. The resulting CNNs are combined in an ensemble and their fused\nknowledge is compressed back to a network with the original architecture. The\nproposed approach resulted in an absolute increase of about 2% in the\nperformance of the proposed CNN. The results demonstrate the potential of\ntransfer learning in the field of medical image analysis, indicate the textural\nnature of the problem and show that the method used for training a network can\nbe as important as designing its architecture.\n", "versions": [{"version": "v1", "created": "Thu, 8 Dec 2016 10:43:03 GMT"}], "update_date": "2016-12-13", "authors_parsed": [["Christodoulidis", "Stergios", ""], ["Anthimopoulos", "Marios", ""], ["Ebner", "Lukas", ""], ["Christe", "Andreas", ""], ["Mougiakakou", "Stavroula", ""]]}, {"id": "1612.02666", "submitter": "Barack Wanjawa Mr.", "authors": "Barack Wamkaya Wanjawa", "title": "Evaluating the Performance of ANN Prediction System at Shanghai Stock\n  Market in the Period 21-Sep-2016 to 11-Oct-2016", "comments": "13 pages, 4 figures, 8 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "q-fin.ST cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This research evaluates the performance of an Artificial Neural Network based\nprediction system that was employed on the Shanghai Stock Exchange for the\nperiod 21-Sep-2016 to 11-Oct-2016. It is a follow-up to a previous paper in\nwhich the prices were predicted and published before September 21. Stock market\nprice prediction remains an important quest for investors and researchers. This\nresearch used an Artificial Intelligence system, being an Artificial Neural\nNetwork that is feedforward multi-layer perceptron with error backpropagation\nfor prediction, unlike other methods such as technical, fundamental or time\nseries analysis. While these alternative methods tend to guide on trends and\nnot the exact likely prices, neural networks on the other hand have the ability\nto predict the real value prices, as was done on this research. Nonetheless,\ndetermination of suitable network parameters remains a challenge in neural\nnetwork design, with this research settling on a configuration of 5:21:21:1\nwith 80% training data or 4-year of training data as a good enough model for\nstock prediction, as already determined in a previous research by the author.\nThe comparative results indicate that neural network can predict typical stock\nmarket prices with mean absolute percentage errors that are as low as 1.95%\nover the ten prediction instances that was studied in this research.\n", "versions": [{"version": "v1", "created": "Mon, 5 Dec 2016 20:25:29 GMT"}], "update_date": "2016-12-09", "authors_parsed": [["Wanjawa", "Barack Wamkaya", ""]]}, {"id": "1612.02695", "submitter": "Jan Chorowski", "authors": "Jan Chorowski and Navdeep Jaitly", "title": "Towards better decoding and language model integration in sequence to\n  sequence models", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NE cs.CL cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The recently proposed Sequence-to-Sequence (seq2seq) framework advocates\nreplacing complex data processing pipelines, such as an entire automatic speech\nrecognition system, with a single neural network trained in an end-to-end\nfashion. In this contribution, we analyse an attention-based seq2seq speech\nrecognition system that directly transcribes recordings into characters. We\nobserve two shortcomings: overconfidence in its predictions and a tendency to\nproduce incomplete transcriptions when language models are used. We propose\npractical solutions to both problems achieving competitive speaker independent\nword error rates on the Wall Street Journal dataset: without separate language\nmodels we reach 10.6% WER, while together with a trigram language model, we\nreach 6.7% WER.\n", "versions": [{"version": "v1", "created": "Thu, 8 Dec 2016 15:23:44 GMT"}], "update_date": "2016-12-09", "authors_parsed": [["Chorowski", "Jan", ""], ["Jaitly", "Navdeep", ""]]}, {"id": "1612.02696", "submitter": "Sven Kosub", "authors": "Sven Kosub", "title": "A note on the triangle inequality for the Jaccard distance", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DM cs.IR cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Two simple proofs of the triangle inequality for the Jaccard distance in\nterms of nonnegative, monotone, submodular functions are given and discussed.\n", "versions": [{"version": "v1", "created": "Thu, 8 Dec 2016 15:25:35 GMT"}], "update_date": "2016-12-09", "authors_parsed": [["Kosub", "Sven", ""]]}, {"id": "1612.02707", "submitter": "Lovedeep Gondara", "authors": "Lovedeep Gondara", "title": "CrowdMI: Multiple Imputation via Crowdsourcing", "comments": "Updated version", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.HC stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Can humans impute missing data with similar proficiency as machines? This is\nthe question we aim to answer in this paper. We present a novel idea of\nconverting observations with missing data in to a survey questionnaire, which\nis presented to crowdworkers for completion. We replicate a multiple imputation\nframework by having multiple unique crowdworkers complete our questionnaire.\nExperimental results demonstrate that using our method, it is possible to\ngenerate valid imputations for qualitative and quantitative missing data, with\nresults comparable to imputations generated by complex statistical models.\n", "versions": [{"version": "v1", "created": "Thu, 8 Dec 2016 16:10:08 GMT"}, {"version": "v2", "created": "Wed, 1 Mar 2017 15:15:14 GMT"}, {"version": "v3", "created": "Mon, 20 Nov 2017 18:19:11 GMT"}, {"version": "v4", "created": "Fri, 23 Feb 2018 16:00:34 GMT"}], "update_date": "2018-02-26", "authors_parsed": [["Gondara", "Lovedeep", ""]]}, {"id": "1612.02712", "submitter": "Yingyu Liang", "authors": "Nan Du, Yingyu Liang, Maria-Florina Balcan, Manuel Gomez-Rodriguez,\n  Hongyuan Zha, Le Song", "title": "Scalable Influence Maximization for Multiple Products in Continuous-Time\n  Diffusion Networks", "comments": "45 pages, to appear in Journal of Machine Learning Research. arXiv\n  admin note: substantial text overlap with arXiv:1312.2164, arXiv:1311.3669", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.SI cs.DS cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A typical viral marketing model identifies influential users in a social\nnetwork to maximize a single product adoption assuming unlimited user\nattention, campaign budgets, and time. In reality, multiple products need\ncampaigns, users have limited attention, convincing users incurs costs, and\nadvertisers have limited budgets and expect the adoptions to be maximized soon.\nFacing these user, monetary, and timing constraints, we formulate the problem\nas a submodular maximization task in a continuous-time diffusion model under\nthe intersection of a matroid and multiple knapsack constraints. We propose a\nrandomized algorithm estimating the user influence in a network\n($|\\mathcal{V}|$ nodes, $|\\mathcal{E}|$ edges) to an accuracy of $\\epsilon$\nwith $n=\\mathcal{O}(1/\\epsilon^2)$ randomizations and\n$\\tilde{\\mathcal{O}}(n|\\mathcal{E}|+n|\\mathcal{V}|)$ computations. By\nexploiting the influence estimation algorithm as a subroutine, we develop an\nadaptive threshold greedy algorithm achieving an approximation factor $k_a/(2+2\nk)$ of the optimal when $k_a$ out of the $k$ knapsack constraints are active.\nExtensive experiments on networks of millions of nodes demonstrate that the\nproposed algorithms achieve the state-of-the-art in terms of effectiveness and\nscalability.\n", "versions": [{"version": "v1", "created": "Thu, 8 Dec 2016 16:15:57 GMT"}, {"version": "v2", "created": "Sun, 29 Jan 2017 07:29:49 GMT"}], "update_date": "2017-01-31", "authors_parsed": [["Du", "Nan", ""], ["Liang", "Yingyu", ""], ["Balcan", "Maria-Florina", ""], ["Gomez-Rodriguez", "Manuel", ""], ["Zha", "Hongyuan", ""], ["Song", "Le", ""]]}, {"id": "1612.02751", "submitter": "David Koes", "authors": "Matthew Ragoza (1), Joshua Hochuli (1), Elisa Idrobo (2), Jocelyn\n  Sunseri (1) and David Ryan Koes (1) ((1) University of Pittsburgh, (2) The\n  College of New Jersey)", "title": "Protein-Ligand Scoring with Convolutional Neural Networks", "comments": null, "journal-ref": null, "doi": "10.1021/acs.jcim.6b00740", "report-no": null, "categories": "stat.ML cs.LG q-bio.BM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Computational approaches to drug discovery can reduce the time and cost\nassociated with experimental assays and enable the screening of novel\nchemotypes. Structure-based drug design methods rely on scoring functions to\nrank and predict binding affinities and poses. The ever-expanding amount of\nprotein-ligand binding and structural data enables the use of deep machine\nlearning techniques for protein-ligand scoring.\n  We describe convolutional neural network (CNN) scoring functions that take as\ninput a comprehensive 3D representation of a protein-ligand interaction. A CNN\nscoring function automatically learns the key features of protein-ligand\ninteractions that correlate with binding. We train and optimize our CNN scoring\nfunctions to discriminate between correct and incorrect binding poses and known\nbinders and non-binders. We find that our CNN scoring function outperforms the\nAutoDock Vina scoring function when ranking poses both for pose prediction and\nvirtual screening.\n", "versions": [{"version": "v1", "created": "Thu, 8 Dec 2016 18:18:29 GMT"}], "update_date": "2020-10-19", "authors_parsed": [["Ragoza", "Matthew", ""], ["Hochuli", "Joshua", ""], ["Idrobo", "Elisa", ""], ["Sunseri", "Jocelyn", ""], ["Koes", "David Ryan", ""]]}, {"id": "1612.02780", "submitter": "Ben Poole", "authors": "Ben Poole, Alexander A. Alemi, Jascha Sohl-Dickstein, Anelia Angelova", "title": "Improved generator objectives for GANs", "comments": "NIPS 2016 Workshop on Adversarial Training", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a framework to understand GAN training as alternating density\nratio estimation and approximate divergence minimization. This provides an\ninterpretation for the mismatched GAN generator and discriminator objectives\noften used in practice, and explains the problem of poor sample diversity. We\nalso derive a family of generator objectives that target arbitrary\n$f$-divergences without minimizing a lower bound, and use them to train\ngenerative image models that target either improved sample quality or greater\nsample diversity.\n", "versions": [{"version": "v1", "created": "Thu, 8 Dec 2016 19:32:04 GMT"}], "update_date": "2016-12-09", "authors_parsed": [["Poole", "Ben", ""], ["Alemi", "Alexander A.", ""], ["Sohl-Dickstein", "Jascha", ""], ["Angelova", "Anelia", ""]]}, {"id": "1612.02803", "submitter": "Lin Yang", "authors": "Lin F. Yang, R. Arora, V. Braverman, Tuo Zhao", "title": "The Physical Systems Behind Optimization Algorithms", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG math.OC stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We use differential equations based approaches to provide some {\\it\n\\textbf{physics}} insights into analyzing the dynamics of popular optimization\nalgorithms in machine learning. In particular, we study gradient descent,\nproximal gradient descent, coordinate gradient descent, proximal coordinate\ngradient, and Newton's methods as well as their Nesterov's accelerated variants\nin a unified framework motivated by a natural connection of optimization\nalgorithms to physical systems. Our analysis is applicable to more general\nalgorithms and optimization problems {\\it \\textbf{beyond}} convexity and strong\nconvexity, e.g. Polyak-\\L ojasiewicz and error bound conditions (possibly\nnonconvex).\n", "versions": [{"version": "v1", "created": "Thu, 8 Dec 2016 20:36:30 GMT"}, {"version": "v2", "created": "Mon, 6 Mar 2017 17:44:34 GMT"}, {"version": "v3", "created": "Fri, 12 May 2017 19:01:38 GMT"}, {"version": "v4", "created": "Mon, 22 May 2017 18:24:39 GMT"}, {"version": "v5", "created": "Thu, 25 Oct 2018 04:04:13 GMT"}], "update_date": "2018-10-26", "authors_parsed": [["Yang", "Lin F.", ""], ["Arora", "R.", ""], ["Braverman", "V.", ""], ["Zhao", "Tuo", ""]]}, {"id": "1612.02814", "submitter": "Ting Chen Ting Chen", "authors": "Ting Chen and Yizhou Sun", "title": "Task-Guided and Path-Augmented Heterogeneous Network Embedding for\n  Author Identification", "comments": "Accepted by WSDM 2017. This is an extended version with appendix", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI cs.IR stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we study the problem of author identification under\ndouble-blind review setting, which is to identify potential authors given\ninformation of an anonymized paper. Different from existing approaches that\nrely heavily on feature engineering, we propose to use network embedding\napproach to address the problem, which can automatically represent nodes into\nlower dimensional feature vectors. However, there are two major limitations in\nrecent studies on network embedding: (1) they are usually general-purpose\nembedding methods, which are independent of the specific tasks; and (2) most of\nthese approaches can only deal with homogeneous networks, where the\nheterogeneity of the network is ignored. Hence, challenges faced here are two\nfolds: (1) how to embed the network under the guidance of the author\nidentification task, and (2) how to select the best type of information due to\nthe heterogeneity of the network.\n  To address the challenges, we propose a task-guided and path-augmented\nheterogeneous network embedding model. In our model, nodes are first embedded\nas vectors in latent feature space. Embeddings are then shared and jointly\ntrained according to task-specific and network-general objectives. We extend\nthe existing unsupervised network embedding to incorporate meta paths in\nheterogeneous networks, and select paths according to the specific task. The\nguidance from author identification task for network embedding is provided both\nexplicitly in joint training and implicitly during meta path selection. Our\nexperiments demonstrate that by using path-augmented network embedding with\ntask guidance, our model can obtain significantly better accuracy at\nidentifying the true authors comparing to existing methods.\n", "versions": [{"version": "v1", "created": "Thu, 8 Dec 2016 20:56:48 GMT"}, {"version": "v2", "created": "Sat, 17 Dec 2016 04:20:03 GMT"}], "update_date": "2016-12-20", "authors_parsed": [["Chen", "Ting", ""], ["Sun", "Yizhou", ""]]}, {"id": "1612.02842", "submitter": "Andrew Stevens", "authors": "Andrew Stevens, Yunchen Pu, Yannan Sun, Greg Spell, Lawrence Carin", "title": "Tensor-Dictionary Learning with Deep Kruskal-Factor Analysis", "comments": "AISTATS 2017", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A multi-way factor analysis model is introduced for tensor-variate data of\nany order. Each data item is represented as a (sparse) sum of Kruskal\ndecompositions, a Kruskal-factor analysis (KFA). KFA is nonparametric and can\ninfer both the tensor-rank of each dictionary atom and the number of dictionary\natoms. The model is adapted for online learning, which allows dictionary\nlearning on large data sets. After KFA is introduced, the model is extended to\na deep convolutional tensor-factor analysis, supervised by a Bayesian SVM. The\nexperiments section demonstrates the improvement of KFA over vectorized\napproaches (e.g., BPFA), tensor decompositions, and convolutional neural\nnetworks (CNN) in multi-way denoising, blind inpainting, and image\nclassification. The improvement in PSNR for the inpainting results over other\nmethods exceeds 1dB in several cases and we achieve state of the art results on\nCaltech101 image classification.\n", "versions": [{"version": "v1", "created": "Thu, 8 Dec 2016 21:21:50 GMT"}, {"version": "v2", "created": "Mon, 12 Dec 2016 19:38:37 GMT"}, {"version": "v3", "created": "Sun, 5 Mar 2017 07:41:03 GMT"}], "update_date": "2017-03-07", "authors_parsed": [["Stevens", "Andrew", ""], ["Pu", "Yunchen", ""], ["Sun", "Yannan", ""], ["Spell", "Greg", ""], ["Carin", "Lawrence", ""]]}, {"id": "1612.02879", "submitter": "Vivek Veeriah", "authors": "Vivek Veeriah, Shangtong Zhang, Richard S. Sutton", "title": "Learning Representations by Stochastic Meta-Gradient Descent in Neural\n  Networks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Representations are fundamental to artificial intelligence. The performance\nof a learning system depends on the type of representation used for\nrepresenting the data. Typically, these representations are hand-engineered\nusing domain knowledge. More recently, the trend is to learn these\nrepresentations through stochastic gradient descent in multi-layer neural\nnetworks, which is called backprop. Learning the representations directly from\nthe incoming data stream reduces the human labour involved in designing a\nlearning system. More importantly, this allows in scaling of a learning system\nfor difficult tasks. In this paper, we introduce a new incremental learning\nalgorithm called crossprop, which learns incoming weights of hidden units based\non the meta-gradient descent approach, that was previously introduced by Sutton\n(1992) and Schraudolph (1999) for learning step-sizes. The final update\nequation introduces an additional memory parameter for each of these weights\nand generalizes the backprop update equation. From our experiments, we show\nthat crossprop learns and reuses its feature representation while tackling new\nand unseen tasks whereas backprop relearns a new feature representation.\n", "versions": [{"version": "v1", "created": "Fri, 9 Dec 2016 00:56:42 GMT"}, {"version": "v2", "created": "Thu, 27 Apr 2017 14:53:00 GMT"}], "update_date": "2017-04-28", "authors_parsed": [["Veeriah", "Vivek", ""], ["Zhang", "Shangtong", ""], ["Sutton", "Richard S.", ""]]}, {"id": "1612.02897", "submitter": "Kareem Abdelfatah", "authors": "Kareem Abdelfatah, Junshu Bao, Gabriel Terejanu", "title": "Environmental Modeling Framework using Stacked Gaussian Processes", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A network of independently trained Gaussian processes (StackedGP) is\nintroduced to obtain predictions of quantities of interest with quantified\nuncertainties. The main applications of the StackedGP framework are to\nintegrate different datasets through model composition, enhance predictions of\nquantities of interest through a cascade of intermediate predictions, and to\npropagate uncertainties through emulated dynamical systems driven by uncertain\nforcing variables. By using analytical first and second-order moments of a\nGaussian process with uncertain inputs using squared exponential and polynomial\nkernels, approximated expectations of quantities of interests that require an\narbitrary composition of functions can be obtained. The StackedGP model is\nextended to any number of layers and nodes per layer, and it provides\nflexibility in kernel selection for the input nodes. The proposed nonparametric\nstacked model is validated using synthetic datasets, and its performance in\nmodel composition and cascading predictions is measured in two applications\nusing real data.\n", "versions": [{"version": "v1", "created": "Fri, 9 Dec 2016 02:53:45 GMT"}, {"version": "v2", "created": "Sun, 18 Jun 2017 19:21:16 GMT"}], "update_date": "2017-06-20", "authors_parsed": [["Abdelfatah", "Kareem", ""], ["Bao", "Junshu", ""], ["Terejanu", "Gabriel", ""]]}, {"id": "1612.02965", "submitter": "Nathan Lazar", "authors": "Nathan H Lazar, Mehmet G\\\"onen, Kemal S\\\"onmez", "title": "BaTFLED: Bayesian Tensor Factorization Linked to External Data", "comments": "4 main pages with 14 supplemental pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG q-bio.QM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The vast majority of current machine learning algorithms are designed to\npredict single responses or a vector of responses, yet many types of response\nare more naturally organized as matrices or higher-order tensor objects where\ncharacteristics are shared across modes. We present a new machine learning\nalgorithm BaTFLED (Bayesian Tensor Factorization Linked to External Data) that\npredicts values in a three-dimensional response tensor using input features for\neach of the dimensions. BaTFLED uses a probabilistic Bayesian framework to\nlearn projection matrices mapping input features for each mode into latent\nrepresentations that multiply to form the response tensor. By utilizing a\nTucker decomposition, the model can capture weights for interactions between\nlatent factors for each mode in a small core tensor. Priors that encourage\nsparsity in the projection matrices and core tensor allow for feature selection\nand model regularization. This method is shown to far outperform elastic net\nand neural net models on 'cold start' tasks from data simulated in a three-mode\nstructure. Additionally, we apply the model to predict dose-response curves in\na panel of breast cancer cell lines treated with drug compounds that was used\nas a Dialogue for Reverse Engineering Assessments and Methods (DREAM)\nchallenge.\n", "versions": [{"version": "v1", "created": "Fri, 9 Dec 2016 10:22:58 GMT"}, {"version": "v2", "created": "Thu, 22 Dec 2016 17:44:29 GMT"}], "update_date": "2016-12-23", "authors_parsed": [["Lazar", "Nathan H", ""], ["G\u00f6nen", "Mehmet", ""], ["S\u00f6nmez", "Kemal", ""]]}, {"id": "1612.03080", "submitter": "Charles-Alban Deledalle", "authors": "Charles-Alban Deledalle (IMB), Nicolas Papadakis (IMB), Joseph Salmon\n  (LTCI), Samuel Vaiter (IMB)", "title": "Characterizing the maximum parameter of the total-variation denoising\n  through the pseudo-inverse of the divergence", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We focus on the maximum regularization parameter for anisotropic\ntotal-variation denoising. It corresponds to the minimum value of the\nregularization parameter above which the solution remains constant. While this\nvalue is well know for the Lasso, such a critical value has not been\ninvestigated in details for the total-variation. Though, it is of importance\nwhen tuning the regularization parameter as it allows fixing an upper-bound on\nthe grid for which the optimal parameter is sought. We establish a closed form\nexpression for the one-dimensional case, as well as an upper-bound for the\ntwo-dimensional case, that appears reasonably tight in practice. This problem\nis directly linked to the computation of the pseudo-inverse of the divergence,\nwhich can be quickly obtained by performing convolutions in the Fourier domain.\n", "versions": [{"version": "v1", "created": "Thu, 8 Dec 2016 12:30:21 GMT"}], "update_date": "2016-12-12", "authors_parsed": [["Deledalle", "Charles-Alban", "", "IMB"], ["Papadakis", "Nicolas", "", "IMB"], ["Salmon", "Joseph", "", "LTCI"], ["Vaiter", "Samuel", "", "IMB"]]}, {"id": "1612.03117", "submitter": "Kim Peter Wabersich", "authors": "Kim Peter Wabersich and Marc Toussaint", "title": "Advancing Bayesian Optimization: The Mixed-Global-Local (MGL) Kernel and\n  Length-Scale Cool Down", "comments": "Long version of accepted NIPS BayesOpt 2016 paper", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Bayesian Optimization (BO) has become a core method for solving expensive\nblack-box optimization problems. While much research focussed on the choice of\nthe acquisition function, we focus on online length-scale adaption and the\nchoice of kernel function. Instead of choosing hyperparameters in view of\nmaximum likelihood on past data, we propose to use the acquisition function to\ndecide on hyperparameter adaptation more robustly and in view of the future\noptimization progress. Further, we propose a particular kernel function that\nincludes non-stationarity and local anisotropy and thereby implicitly\nintegrates the efficiency of local convex optimization with global Bayesian\noptimization. Comparisons to state-of-the art BO methods underline the\nefficiency of these mechanisms on global optimization benchmarks.\n", "versions": [{"version": "v1", "created": "Fri, 9 Dec 2016 18:20:00 GMT"}], "update_date": "2016-12-12", "authors_parsed": [["Wabersich", "Kim Peter", ""], ["Toussaint", "Marc", ""]]}, {"id": "1612.03132", "submitter": "Daniele Tantari", "authors": "Adriano Barra, Giuseppe Genovese, Peter Sollich, Daniele Tantari", "title": "Phase transitions in Restricted Boltzmann Machines with generic priors", "comments": "5 pages, 4 figures; extensive simulations and 2 new figures added;\n  corrected typos; added references", "journal-ref": "Phys. Rev. E 96, 042156 (2017)", "doi": "10.1103/PhysRevE.96.042156", "report-no": null, "categories": "cond-mat.dis-nn cs.LG physics.data-an stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study Generalised Restricted Boltzmann Machines with generic priors for\nunits and weights, interpolating between Boolean and Gaussian variables. We\npresent a complete analysis of the replica symmetric phase diagram of these\nsystems, which can be regarded as Generalised Hopfield models. We underline the\nrole of the retrieval phase for both inference and learning processes and we\nshow that retrieval is robust for a large class of weight and unit priors,\nbeyond the standard Hopfield scenario. Furthermore we show how the paramagnetic\nphase boundary is directly related to the optimal size of the training set\nnecessary for good generalisation in a teacher-student scenario of unsupervised\nlearning.\n", "versions": [{"version": "v1", "created": "Fri, 9 Dec 2016 19:08:11 GMT"}, {"version": "v2", "created": "Wed, 6 Sep 2017 09:33:27 GMT"}], "update_date": "2017-11-01", "authors_parsed": [["Barra", "Adriano", ""], ["Genovese", "Giuseppe", ""], ["Sollich", "Peter", ""], ["Tantari", "Daniele", ""]]}, {"id": "1612.03164", "submitter": "Constantinos Daskalakis", "authors": "Constantinos Daskalakis, Qinxuan Pan", "title": "Square Hellinger Subadditivity for Bayesian Networks and its\n  Applications to Identity Testing", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.IT math.IT math.PR math.ST stat.ML stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We show that the square Hellinger distance between two Bayesian networks on\nthe same directed graph, $G$, is subadditive with respect to the neighborhoods\nof $G$. Namely, if $P$ and $Q$ are the probability distributions defined by two\nBayesian networks on the same DAG, our inequality states that the square\nHellinger distance, $H^2(P,Q)$, between $P$ and $Q$ is upper bounded by the\nsum, $\\sum_v H^2(P_{\\{v\\} \\cup \\Pi_v}, Q_{\\{v\\} \\cup \\Pi_v})$, of the square\nHellinger distances between the marginals of $P$ and $Q$ on every node $v$ and\nits parents $\\Pi_v$ in the DAG. Importantly, our bound does not involve the\nconditionals but the marginals of $P$ and $Q$. We derive a similar inequality\nfor more general Markov Random Fields.\n  As an application of our inequality, we show that distinguishing whether two\nBayesian networks $P$ and $Q$ on the same (but potentially unknown) DAG satisfy\n$P=Q$ vs $d_{\\rm TV}(P,Q)>\\epsilon$ can be performed from\n$\\tilde{O}(|\\Sigma|^{3/4(d+1)} \\cdot n/\\epsilon^2)$ samples, where $d$ is the\nmaximum in-degree of the DAG and $\\Sigma$ the domain of each variable of the\nBayesian networks. If $P$ and $Q$ are defined on potentially different and\npotentially unknown trees, the sample complexity becomes\n$\\tilde{O}(|\\Sigma|^{4.5} n/\\epsilon^2)$, whose dependence on $n, \\epsilon$ is\noptimal up to logarithmic factors. Lastly, if $P$ and $Q$ are product\ndistributions over $\\{0,1\\}^n$ and $Q$ is known, the sample complexity becomes\n$O(\\sqrt{n}/\\epsilon^2)$, which is optimal up to constant factors.\n", "versions": [{"version": "v1", "created": "Fri, 9 Dec 2016 20:58:12 GMT"}], "update_date": "2016-12-12", "authors_parsed": [["Daskalakis", "Constantinos", ""], ["Pan", "Qinxuan", ""]]}, {"id": "1612.03186", "submitter": "Christian Grussler", "authors": "Christian Grussler and Pontus Giselsson", "title": "Low-Rank Inducing Norms with Optimality Interpretations", "comments": null, "journal-ref": "SIAM J. Optim., 28(4), 3057-3078, 2018", "doi": "10.1137/17M1115770", "report-no": null, "categories": "math.OC cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Optimization problems with rank constraints appear in many diverse fields\nsuch as control, machine learning and image analysis. Since the rank constraint\nis non-convex, these problems are often approximately solved via convex\nrelaxations. Nuclear norm regularization is the prevailing convexifying\ntechnique for dealing with these types of problem. This paper introduces a\nfamily of low-rank inducing norms and regularizers which includes the nuclear\nnorm as a special case. A posteriori guarantees on solving an underlying rank\nconstrained optimization problem with these convex relaxations are provided. We\nevaluate the performance of the low-rank inducing norms on three matrix\ncompletion problems. In all examples, the nuclear norm heuristic is\noutperformed by convex relaxations based on other low-rank inducing norms. For\ntwo of the problems there exist low-rank inducing norms that succeed in\nrecovering the partially unknown matrix, while the nuclear norm fails. These\nlow-rank inducing norms are shown to be representable as semi-definite\nprograms. Moreover, these norms have cheaply computable proximal mappings,\nwhich makes it possible to also solve problems of large size using first-order\nmethods.\n", "versions": [{"version": "v1", "created": "Fri, 9 Dec 2016 21:40:40 GMT"}, {"version": "v2", "created": "Mon, 11 Jun 2018 11:14:57 GMT"}], "update_date": "2018-11-12", "authors_parsed": [["Grussler", "Christian", ""], ["Giselsson", "Pontus", ""]]}, {"id": "1612.03225", "submitter": "Katya Scheinberg", "authors": "Oktay Gunluk, Jayant Kalagnanam, Minhan Li, Matt Menickelly, Katya\n  Scheinberg", "title": "Optimal Generalized Decision Trees via Integer Programming", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG math.OC stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Decision trees have been a very popular class of predictive models for\ndecades due to their interpretability and good performance on categorical\nfeatures. However, they are not always robust and tend to overfit the data.\nAdditionally, if allowed to grow large, they lose interpretability. In this\npaper, we present a mixed integer programming formulation to construct optimal\ndecision trees of a prespecified size. We take the special structure of\ncategorical features into account and allow combinatorial decisions (based on\nsubsets of values of features) at each node. Our approach can also handle\nnumerical features via thresholding. We show that very good accuracy can be\nachieved with small trees using moderately-sized training sets. The\noptimization problems we solve are tractable with modern solvers.\n", "versions": [{"version": "v1", "created": "Sat, 10 Dec 2016 00:05:37 GMT"}, {"version": "v2", "created": "Sun, 14 Jan 2018 20:56:14 GMT"}, {"version": "v3", "created": "Tue, 13 Aug 2019 17:19:17 GMT"}], "update_date": "2019-08-14", "authors_parsed": [["Gunluk", "Oktay", ""], ["Kalagnanam", "Jayant", ""], ["Li", "Minhan", ""], ["Menickelly", "Matt", ""], ["Scheinberg", "Katya", ""]]}, {"id": "1612.03226", "submitter": "Jiaji Huang Dr.", "authors": "Jiaji Huang, Rewon Child, Vinay Rao, Hairong Liu, Sanjeev Satheesh,\n  Adam Coates", "title": "Active Learning for Speech Recognition: the Power of Gradients", "comments": "published as a workshop paper at NIPS 2016", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In training speech recognition systems, labeling audio clips can be\nexpensive, and not all data is equally valuable. Active learning aims to label\nonly the most informative samples to reduce cost. For speech recognition,\nconfidence scores and other likelihood-based active learning methods have been\nshown to be effective. Gradient-based active learning methods, however, are\nstill not well-understood. This work investigates the Expected Gradient Length\n(EGL) approach in active learning for end-to-end speech recognition. We justify\nEGL from a variance reduction perspective, and observe that EGL's measure of\ninformativeness picks novel samples uncorrelated with confidence scores.\nExperimentally, we show that EGL can reduce word errors by 11\\%, or\nalternatively, reduce the number of samples to label by 50\\%, when compared to\nrandom sampling.\n", "versions": [{"version": "v1", "created": "Sat, 10 Dec 2016 00:09:45 GMT"}], "update_date": "2016-12-13", "authors_parsed": [["Huang", "Jiaji", ""], ["Child", "Rewon", ""], ["Rao", "Vinay", ""], ["Liu", "Hairong", ""], ["Satheesh", "Sanjeev", ""], ["Coates", "Adam", ""]]}, {"id": "1612.03242", "submitter": "Han Zhang", "authors": "Han Zhang, Tao Xu, Hongsheng Li, Shaoting Zhang, Xiaogang Wang,\n  Xiaolei Huang, Dimitris Metaxas", "title": "StackGAN: Text to Photo-realistic Image Synthesis with Stacked\n  Generative Adversarial Networks", "comments": "ICCV 2017 Oral Presentation", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Synthesizing high-quality images from text descriptions is a challenging\nproblem in computer vision and has many practical applications. Samples\ngenerated by existing text-to-image approaches can roughly reflect the meaning\nof the given descriptions, but they fail to contain necessary details and vivid\nobject parts. In this paper, we propose Stacked Generative Adversarial Networks\n(StackGAN) to generate 256x256 photo-realistic images conditioned on text\ndescriptions. We decompose the hard problem into more manageable sub-problems\nthrough a sketch-refinement process. The Stage-I GAN sketches the primitive\nshape and colors of the object based on the given text description, yielding\nStage-I low-resolution images. The Stage-II GAN takes Stage-I results and text\ndescriptions as inputs, and generates high-resolution images with\nphoto-realistic details. It is able to rectify defects in Stage-I results and\nadd compelling details with the refinement process. To improve the diversity of\nthe synthesized images and stabilize the training of the conditional-GAN, we\nintroduce a novel Conditioning Augmentation technique that encourages\nsmoothness in the latent conditioning manifold. Extensive experiments and\ncomparisons with state-of-the-arts on benchmark datasets demonstrate that the\nproposed method achieves significant improvements on generating photo-realistic\nimages conditioned on text descriptions.\n", "versions": [{"version": "v1", "created": "Sat, 10 Dec 2016 03:11:37 GMT"}, {"version": "v2", "created": "Sat, 5 Aug 2017 02:18:21 GMT"}], "update_date": "2017-08-08", "authors_parsed": [["Zhang", "Han", ""], ["Xu", "Tao", ""], ["Li", "Hongsheng", ""], ["Zhang", "Shaoting", ""], ["Wang", "Xiaogang", ""], ["Huang", "Xiaolei", ""], ["Metaxas", "Dimitris", ""]]}, {"id": "1612.03278", "submitter": "Samir Bhatt Dr", "authors": "Samir Bhatt, Ewan Cameron, Seth R Flaxman, Daniel J Weiss, David L\n  Smith and Peter W Gething", "title": "Improved prediction accuracy for disease risk mapping using Gaussian\n  Process stacked generalisation", "comments": "Under Submission", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP stat.ML", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Maps of infectious disease---charting spatial variations in the force of\ninfection, degree of endemicity, and the burden on human health---provide an\nessential evidence base to support planning towards global health targets.\nContemporary disease mapping efforts have embraced statistical modelling\napproaches to properly acknowledge uncertainties in both the available\nmeasurements and their spatial interpolation. The most common such approach is\nthat of Gaussian process regression, a mathematical framework comprised of two\ncomponents: a mean function harnessing the predictive power of multiple\nindependent variables, and a covariance function yielding spatio-temporal\nshrinkage against residual variation from the mean. Though many techniques have\nbeen developed to improve the flexibility and fitting of the covariance\nfunction, models for the mean function have typically been restricted to simple\nlinear terms. For infectious diseases, known to be driven by complex\ninteractions between environmental and socio-economic factors, improved\nmodelling of the mean function can greatly boost predictive power. Here we\npresent an ensemble approach based on stacked generalisation that allows for\nmultiple, non-linear algorithmic mean functions to be jointly embedded within\nthe Gaussian process framework. We apply this method to mapping Plasmodium\nfalciparum prevalence data in Sub-Saharan Africa and show that the generalised\nensemble approach markedly out-performs any individual method.\n", "versions": [{"version": "v1", "created": "Sat, 10 Dec 2016 11:00:36 GMT"}], "update_date": "2017-09-21", "authors_parsed": [["Bhatt", "Samir", ""], ["Cameron", "Ewan", ""], ["Flaxman", "Seth R", ""], ["Weiss", "Daniel J", ""], ["Smith", "David L", ""], ["Gething", "Peter W", ""]]}, {"id": "1612.03301", "submitter": "Qi Lei", "authors": "Rashish Tandon, Qi Lei, Alexandros G. Dimakis and Nikos Karampatziakis", "title": "Gradient Coding", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.DC cs.IT cs.LG math.IT stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a novel coding theoretic framework for mitigating stragglers in\ndistributed learning. We show how carefully replicating data blocks and coding\nacross gradients can provide tolerance to failures and stragglers for\nSynchronous Gradient Descent. We implement our schemes in python (using MPI) to\nrun on Amazon EC2, and show how we compare against baseline approaches in\nrunning time and generalization error.\n", "versions": [{"version": "v1", "created": "Sat, 10 Dec 2016 14:25:00 GMT"}, {"version": "v2", "created": "Wed, 8 Mar 2017 01:00:33 GMT"}], "update_date": "2017-03-09", "authors_parsed": [["Tandon", "Rashish", ""], ["Lei", "Qi", ""], ["Dimakis", "Alexandros G.", ""], ["Karampatziakis", "Nikos", ""]]}, {"id": "1612.03319", "submitter": "Lawrence Murray", "authors": "Lawrence M. Murray, Sumeetpal Singh and Anthony Lee", "title": "Anytime Monte Carlo", "comments": "20 pages, 7 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.CO stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Monte Carlo algorithms simulate some prescribed number of samples, taking\nsome random real time to complete the computations necessary. This work\nconsiders the converse: to impose a real-time budget on the computation, which\nresults in the number of samples simulated being random. To complicate matters,\nthe real time taken for each simulation may depend on the sample produced, so\nthat the samples themselves are not independent of their number, and a length\nbias with respect to compute time is apparent. This is especially problematic\nwhen a Markov chain Monte Carlo (MCMC) algorithm is used and the final state of\nthe Markov chain -- rather than an average over all states -- is required,\nwhich is the case in parallel tempering implementations of MCMC. The length\nbias does not diminish with the compute budget in this case. It also occurs in\nsequential Monte Carlo (SMC) algorithms, which is the focus of this paper. We\npropose an anytime framework to address the concern, using a continuous-time\nMarkov jump process to study the progress of the computation in real time. We\nfirst show that for any MCMC algorithm, the length bias of the final state's\ndistribution due to the imposed real-time computing budget can be eliminated by\nusing a multiple chain construction. The utility of this construction is then\ndemonstrated on a large-scale SMC^2 implementation, using four billion\nparticles distributed across a cluster of 128 graphics processing units on the\nAmazon EC2 service. The anytime framework imposes a real-time budget on the\nMCMC move steps within the SMC$^{2}$ algorithm, ensuring that all processors\nare simultaneously ready for the resampling step, demonstrably reducing\nidleness to due waiting times and providing substantial control over the total\ncompute budget.\n", "versions": [{"version": "v1", "created": "Sat, 10 Dec 2016 16:36:07 GMT"}, {"version": "v2", "created": "Wed, 7 Jun 2017 14:34:35 GMT"}, {"version": "v3", "created": "Thu, 10 Jun 2021 19:27:05 GMT"}], "update_date": "2021-06-14", "authors_parsed": [["Murray", "Lawrence M.", ""], ["Singh", "Sumeetpal", ""], ["Lee", "Anthony", ""]]}, {"id": "1612.03328", "submitter": "Pedram Daee", "authors": "Pedram Daee, Tomi Peltola, Marta Soare, Samuel Kaski", "title": "Knowledge Elicitation via Sequential Probabilistic Inference for\n  High-Dimensional Prediction", "comments": "22 pages, 9 figures. The paper is published in Machine Learning\n  journal (http://rdcu.be/t9KF). Codes and data available at\n  https://github.com/HIIT/knowledge-elicitation-for-linear-regression, Machine\n  Learning, (2017)", "journal-ref": null, "doi": "10.1007/s10994-017-5651-7", "report-no": null, "categories": "cs.AI cs.HC cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Prediction in a small-sized sample with a large number of covariates, the\n\"small n, large p\" problem, is challenging. This setting is encountered in\nmultiple applications, such as precision medicine, where obtaining additional\nsamples can be extremely costly or even impossible, and extensive research\neffort has recently been dedicated to finding principled solutions for accurate\nprediction. However, a valuable source of additional information, domain\nexperts, has not yet been efficiently exploited. We formulate knowledge\nelicitation generally as a probabilistic inference process, where expert\nknowledge is sequentially queried to improve predictions. In the specific case\nof sparse linear regression, where we assume the expert has knowledge about the\nvalues of the regression coefficients or about the relevance of the features,\nwe propose an algorithm and computational approximation for fast and efficient\ninteraction, which sequentially identifies the most informative features on\nwhich to query expert knowledge. Evaluations of our method in experiments with\nsimulated and real users show improved prediction accuracy already with a small\neffort from the expert.\n", "versions": [{"version": "v1", "created": "Sat, 10 Dec 2016 18:11:32 GMT"}, {"version": "v2", "created": "Thu, 13 Jul 2017 13:08:56 GMT"}], "update_date": "2017-07-14", "authors_parsed": [["Daee", "Pedram", ""], ["Peltola", "Tomi", ""], ["Soare", "Marta", ""], ["Kaski", "Samuel", ""]]}, {"id": "1612.03350", "submitter": "Zheng Xu", "authors": "Zheng Xu, Furong Huang, Louiqa Raschid, Tom Goldstein", "title": "Non-negative Factorization of the Occurrence Tensor from Financial\n  Contracts", "comments": "NIPS tensor workshop", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CE cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose an algorithm for the non-negative factorization of an occurrence\ntensor built from heterogeneous networks. We use l0 norm to model sparse errors\nover discrete values (occurrences), and use decomposed factors to model the\nembedded groups of nodes. An efficient splitting method is developed to\noptimize the nonconvex and nonsmooth objective. We study both synthetic\nproblems and a new dataset built from financial documents, resMBS.\n", "versions": [{"version": "v1", "created": "Sat, 10 Dec 2016 22:26:30 GMT"}], "update_date": "2016-12-19", "authors_parsed": [["Xu", "Zheng", ""], ["Huang", "Furong", ""], ["Raschid", "Louiqa", ""], ["Goldstein", "Tom", ""]]}, {"id": "1612.03364", "submitter": "Baojian Zhou", "authors": "Feng Chen, Baojian Zhou", "title": "Technical Report: A Generalized Matching Pursuit Approach for\n  Graph-Structured Sparsity", "comments": "International Joint Conference on Artificial Intelligence, 2016", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI stat.ML", "license": "http://creativecommons.org/publicdomain/zero/1.0/", "abstract": "  Sparsity-constrained optimization is an important and challenging problem\nthat has wide applicability in data mining, machine learning, and statistics.\nIn this paper, we focus on sparsity-constrained optimization in cases where the\ncost function is a general nonlinear function and, in particular, the sparsity\nconstraint is defined by a graph-structured sparsity model. Existing methods\nexplore this problem in the context of sparse estimation in linear models. To\nthe best of our knowledge, this is the first work to present an efficient\napproximation algorithm, namely, Graph-structured Matching Pursuit (Graph-Mp),\nto optimize a general nonlinear function subject to graph-structured\nconstraints. We prove that our algorithm enjoys the strong guarantees analogous\nto those designed for linear models in terms of convergence rate and\napproximation accuracy. As a case study, we specialize Graph-Mp to optimize a\nnumber of well-known graph scan statistic models for the connected subgraph\ndetection task, and empirical evidence demonstrates that our general algorithm\nperforms superior over state-of-the-art methods that are designed specifically\nfor the task of connected subgraph detection.\n", "versions": [{"version": "v1", "created": "Sun, 11 Dec 2016 01:57:50 GMT"}], "update_date": "2016-12-19", "authors_parsed": [["Chen", "Feng", ""], ["Zhou", "Baojian", ""]]}, {"id": "1612.03409", "submitter": "Matteo Ruffini MR", "authors": "Matteo Ruffini, Marta Casanellas, Ricard Gavald\\`a", "title": "A New Spectral Method for Latent Variable Models", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper presents an algorithm for the unsupervised learning of latent\nvariable models from unlabeled sets of data. We base our technique on spectral\ndecomposition, providing a technique that proves to be robust both in theory\nand in practice. We also describe how to use this algorithm to learn the\nparameters of two well known text mining models: single topic model and Latent\nDirichlet Allocation, providing in both cases an efficient technique to\nretrieve the parameters to feed the algorithm. We compare the results of our\nalgorithm with those of existing algorithms on synthetic data, and we provide\nexamples of applications to real world text corpora for both single topic model\nand LDA, obtaining meaningful results.\n", "versions": [{"version": "v1", "created": "Sun, 11 Dec 2016 13:31:58 GMT"}, {"version": "v2", "created": "Tue, 4 Apr 2017 09:06:11 GMT"}], "update_date": "2017-04-05", "authors_parsed": [["Ruffini", "Matteo", ""], ["Casanellas", "Marta", ""], ["Gavald\u00e0", "Ricard", ""]]}, {"id": "1612.03412", "submitter": "Yochai Blau", "authors": "Yochai Blau and Tomer Michaeli", "title": "Non-Redundant Spectral Dimensionality Reduction", "comments": null, "journal-ref": "European Conference on Machine Learning and Knowledge Discovery in\n  Databases (ECML PKDD), Part I, LNAI 10534, pp. 256-271, 2017", "doi": "10.1007/978-3-319-71249-9_16", "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Spectral dimensionality reduction algorithms are widely used in numerous\ndomains, including for recognition, segmentation, tracking and visualization.\nHowever, despite their popularity, these algorithms suffer from a major\nlimitation known as the \"repeated Eigen-directions\" phenomenon. That is, many\nof the embedding coordinates they produce typically capture the same direction\nalong the data manifold. This leads to redundant and inefficient\nrepresentations that do not reveal the true intrinsic dimensionality of the\ndata. In this paper, we propose a general method for avoiding redundancy in\nspectral algorithms. Our approach relies on replacing the orthogonality\nconstraints underlying those methods by unpredictability constraints.\nSpecifically, we require that each embedding coordinate be unpredictable (in\nthe statistical sense) from all previous ones. We prove that these constraints\nnecessarily prevent redundancy, and provide a simple technique to incorporate\nthem into existing methods. As we illustrate on challenging high-dimensional\nscenarios, our approach produces significantly more informative and compact\nrepresentations, which improve visualization and classification tasks.\n", "versions": [{"version": "v1", "created": "Sun, 11 Dec 2016 14:04:33 GMT"}, {"version": "v2", "created": "Thu, 20 Apr 2017 12:58:06 GMT"}], "update_date": "2018-01-03", "authors_parsed": [["Blau", "Yochai", ""], ["Michaeli", "Tomer", ""]]}, {"id": "1612.03441", "submitter": "Zhao Shen-Yi", "authors": "Shen-Yi Zhao, Gong-Duo Zhang, Wu-Jun Li", "title": "Lock-Free Optimization for Non-Convex Problems", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Stochastic gradient descent~(SGD) and its variants have attracted much\nattention in machine learning due to their efficiency and effectiveness for\noptimization. To handle large-scale problems, researchers have recently\nproposed several lock-free strategy based parallel SGD~(LF-PSGD) methods for\nmulti-core systems. However, existing works have only proved the convergence of\nthese LF-PSGD methods for convex problems. To the best of our knowledge, no\nwork has proved the convergence of the LF-PSGD methods for non-convex problems.\nIn this paper, we provide the theoretical proof about the convergence of two\nrepresentative LF-PSGD methods, Hogwild! and AsySVRG, for non-convex problems.\nEmpirical results also show that both Hogwild! and AsySVRG are convergent on\nnon-convex problems, which successfully verifies our theoretical results.\n", "versions": [{"version": "v1", "created": "Sun, 11 Dec 2016 17:26:43 GMT"}], "update_date": "2016-12-19", "authors_parsed": [["Zhao", "Shen-Yi", ""], ["Zhang", "Gong-Duo", ""], ["Li", "Wu-Jun", ""]]}, {"id": "1612.03450", "submitter": "Michael Tschannen", "authors": "Michael Tschannen and Helmut B\\\"olcskei", "title": "Noisy subspace clustering via matching pursuits", "comments": "24 pages, 5 figures", "journal-ref": "IEEE Transactions on Information Theory, Vol. 64, No. 6, pp.\n  4081-4104, June 2018", "doi": "10.1109/TIT.2018.2812824", "report-no": null, "categories": "cs.LG cs.IT math.IT stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Sparsity-based subspace clustering algorithms have attracted significant\nattention thanks to their excellent performance in practical applications. A\nprominent example is the sparse subspace clustering (SSC) algorithm by\nElhamifar and Vidal, which performs spectral clustering based on an adjacency\nmatrix obtained by sparsely representing each data point in terms of all the\nother data points via the Lasso. When the number of data points is large or the\ndimension of the ambient space is high, the computational complexity of SSC\nquickly becomes prohibitive. Dyer et al. observed that SSC-OMP obtained by\nreplacing the Lasso by the greedy orthogonal matching pursuit (OMP) algorithm\nresults in significantly lower computational complexity, while often yielding\ncomparable performance. The central goal of this paper is an analytical\nperformance characterization of SSC-OMP for noisy data. Moreover, we introduce\nand analyze the SSC-MP algorithm, which employs matching pursuit (MP) in lieu\nof OMP. Both SSC-OMP and SSC-MP are proven to succeed even when the subspaces\nintersect and when the data points are contaminated by severe noise. The\nclustering conditions we obtain for SSC-OMP and SSC-MP are similar to those for\nSSC and for the thresholding-based subspace clustering (TSC) algorithm due to\nHeckel and B\\\"olcskei. Analytical results in combination with numerical results\nindicate that both SSC-OMP and SSC-MP with a data-dependent stopping criterion\nautomatically detect the dimensions of the subspaces underlying the data.\nMoreover, experiments on synthetic and on real data show that SSC-MP compares\nvery favorably to SSC, SSC-OMP, TSC, and the nearest subspace neighbor\nalgorithm, both in terms of clustering performance and running time. In\naddition, we find that, in contrast to SSC-OMP, the performance of SSC-MP is\nvery robust with respect to the choice of parameters in the stopping criteria.\n", "versions": [{"version": "v1", "created": "Sun, 11 Dec 2016 18:33:31 GMT"}, {"version": "v2", "created": "Fri, 8 Jun 2018 12:34:29 GMT"}], "update_date": "2018-06-11", "authors_parsed": [["Tschannen", "Michael", ""], ["B\u00f6lcskei", "Helmut", ""]]}, {"id": "1612.03480", "submitter": "Cengiz Pehlevan", "authors": "Yuansi Chen, Cengiz Pehlevan, Dmitri B. Chklovskii", "title": "Self-calibrating Neural Networks for Dimensionality Reduction", "comments": "2016 Asilomar Conference on Signals, Systems and Computers", "journal-ref": null, "doi": "10.1109/ACSSC.2016.7869625", "report-no": null, "categories": "cs.LG cs.NE q-bio.NC stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recently, a novel family of biologically plausible online algorithms for\nreducing the dimensionality of streaming data has been derived from the\nsimilarity matching principle. In these algorithms, the number of output\ndimensions can be determined adaptively by thresholding the singular values of\nthe input data matrix. However, setting such threshold requires knowing the\nmagnitude of the desired singular values in advance. Here we propose online\nalgorithms where the threshold is self-calibrating based on the singular values\ncomputed from the existing observations. To derive these algorithms from the\nsimilarity matching cost function we propose novel regularizers. As before,\nthese online algorithms can be implemented by Hebbian/anti-Hebbian neural\nnetworks in which the learning rule depends on the chosen regularizer. We\ndemonstrate both mathematically and via simulation the effectiveness of these\nonline algorithms in various settings.\n", "versions": [{"version": "v1", "created": "Sun, 11 Dec 2016 21:15:05 GMT"}], "update_date": "2017-03-21", "authors_parsed": [["Chen", "Yuansi", ""], ["Pehlevan", "Cengiz", ""], ["Chklovskii", "Dmitri B.", ""]]}, {"id": "1612.03615", "submitter": "Daniel Romero", "authors": "Daniel Romero, Vassilis N. Ioannidis, Georgios B. Giannakis", "title": "Kernel-based Reconstruction of Space-time Functions on Dynamic Graphs", "comments": "Submitted to IEEE Journal of Selected Topics in Signal processing,\n  Oct. 2016", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG eess.SP stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Graph-based methods pervade the inference toolkits of numerous disciplines\nincluding sociology, biology, neuroscience, physics, chemistry, and\nengineering. A challenging problem encountered in this context pertains to\ndetermining the attributes of a set of vertices given those of another subset\nat possibly different time instants. Leveraging spatiotemporal dynamics can\ndrastically reduce the number of observed vertices, and hence the cost of\nsampling. Alleviating the limited flexibility of existing approaches, the\npresent paper broadens the existing kernel-based graph function reconstruction\nframework to accommodate time-evolving functions over possibly time-evolving\ntopologies. This approach inherits the versatility and generality of\nkernel-based methods, for which no knowledge on distributions or second-order\nstatistics is required. Systematic guidelines are provided to construct two\nfamilies of space-time kernels with complementary strengths. The first\nfacilitates judicious control of regularization on a space-time frequency\nplane, whereas the second can afford time-varying topologies. Batch and online\nestimators are also put forth, and a novel kernel Kalman filter is developed to\nobtain these estimates at affordable computational cost. Numerical tests with\nreal data sets corroborate the merits of the proposed methods relative to\ncompeting alternatives.\n", "versions": [{"version": "v1", "created": "Mon, 12 Dec 2016 11:12:00 GMT"}, {"version": "v2", "created": "Sat, 20 May 2017 08:29:08 GMT"}], "update_date": "2018-09-25", "authors_parsed": [["Romero", "Daniel", ""], ["Ioannidis", "Vassilis N.", ""], ["Giannakis", "Georgios B.", ""]]}, {"id": "1612.03663", "submitter": "Maksim Lapin", "authors": "Maksim Lapin, Matthias Hein, and Bernt Schiele", "title": "Analysis and Optimization of Loss Functions for Multiclass, Top-k, and\n  Multilabel Classification", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Top-k error is currently a popular performance measure on large scale image\nclassification benchmarks such as ImageNet and Places. Despite its wide\nacceptance, our understanding of this metric is limited as most of the previous\nresearch is focused on its special case, the top-1 error. In this work, we\nexplore two directions that shed more light on the top-k error. First, we\nprovide an in-depth analysis of established and recently proposed single-label\nmulticlass methods along with a detailed account of efficient optimization\nalgorithms for them. Our results indicate that the softmax loss and the smooth\nmulticlass SVM are surprisingly competitive in top-k error uniformly across all\nk, which can be explained by our analysis of multiclass top-k calibration.\nFurther improvements for a specific k are possible with a number of proposed\ntop-k loss functions. Second, we use the top-k methods to explore the\ntransition from multiclass to multilabel learning. In particular, we find that\nit is possible to obtain effective multilabel classifiers on Pascal VOC using a\nsingle label per image for training, while the gap between multiclass and\nmultilabel methods on MS COCO is more significant. Finally, our contribution of\nefficient algorithms for training with the considered top-k and multilabel loss\nfunctions is of independent interest.\n", "versions": [{"version": "v1", "created": "Mon, 12 Dec 2016 13:20:09 GMT"}], "update_date": "2016-12-19", "authors_parsed": [["Lapin", "Maksim", ""], ["Hein", "Matthias", ""], ["Schiele", "Bernt", ""]]}, {"id": "1612.03689", "submitter": "Olivier Roustant", "authors": "Olivier Roustant (FAYOL-EMSE, GdR MASCOT-NUM), Franck Barthe (IMT),\n  Bertrand Iooss (GdR MASCOT-NUM, IMT)", "title": "Poincar\\'e inequalities on intervals -- application to sensitivity\n  analysis", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST math.PR stat.ML stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The development of global sensitivity analysis of numerical model outputs has\nrecently raised new issues on 1-dimensional Poincar\\'e inequalities. Typically\ntwo kind of sensitivity indices are linked by a Poincar\\'e type inequality,\nwhich provide upper bounds of the most interpretable index by using the other\none, cheaper to compute. This allows performing a low-cost screening of\nunessential variables. The efficiency of this screening then highly depends on\nthe accuracy of the upper bounds in Poincar\\'e inequalities. The novelty in the\nquestions concern the wide range of probability distributions involved, which\nare often truncated on intervals. After providing an overview of the existing\nknowledge and techniques, we add some theory about Poincar\\'e constants on\nintervals, with improvements for symmetric intervals. Then we exploit the\nspectral interpretation for computing exact value of Poincar\\'e constants of\nany admissible distribution on a given interval. We give semi-analytical\nresults for some frequent distributions (truncated exponential, triangular,\ntruncated normal), and present a numerical method in the general case. Finally,\nan application is made to a hydrological problem, showing the benefits of the\nnew results in Poincar\\'e inequalities to sensitivity analysis.\n", "versions": [{"version": "v1", "created": "Mon, 12 Dec 2016 14:08:54 GMT"}], "update_date": "2016-12-13", "authors_parsed": [["Roustant", "Olivier", "", "FAYOL-EMSE, GdR MASCOT-NUM"], ["Barthe", "Franck", "", "IMT"], ["Iooss", "Bertrand", "", "GdR MASCOT-NUM, IMT"]]}, {"id": "1612.03770", "submitter": "James Aimone", "authors": "Timothy J. Draelos, Nadine E. Miner, Christopher C. Lamb, Jonathan A.\n  Cox, Craig M. Vineyard, Kristofor D. Carlson, William M. Severa, Conrad D.\n  James, and James B. Aimone", "title": "Neurogenesis Deep Learning", "comments": "8 pages, 8 figures, Accepted to 2017 International Joint Conference\n  on Neural Networks (IJCNN 2017)", "journal-ref": null, "doi": "10.1109/IJCNN.2017.7965898", "report-no": "SAND2017-2174 C", "categories": "cs.NE cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Neural machine learning methods, such as deep neural networks (DNN), have\nachieved remarkable success in a number of complex data processing tasks. These\nmethods have arguably had their strongest impact on tasks such as image and\naudio processing - data processing domains in which humans have long held clear\nadvantages over conventional algorithms. In contrast to biological neural\nsystems, which are capable of learning continuously, deep artificial networks\nhave a limited ability for incorporating new information in an already trained\nnetwork. As a result, methods for continuous learning are potentially highly\nimpactful in enabling the application of deep networks to dynamic data sets.\nHere, inspired by the process of adult neurogenesis in the hippocampus, we\nexplore the potential for adding new neurons to deep layers of artificial\nneural networks in order to facilitate their acquisition of novel information\nwhile preserving previously trained data representations. Our results on the\nMNIST handwritten digit dataset and the NIST SD 19 dataset, which includes\nlower and upper case letters and digits, demonstrate that neurogenesis is well\nsuited for addressing the stability-plasticity dilemma that has long challenged\nadaptive machine learning algorithms.\n", "versions": [{"version": "v1", "created": "Mon, 12 Dec 2016 16:25:23 GMT"}, {"version": "v2", "created": "Tue, 28 Mar 2017 16:45:11 GMT"}], "update_date": "2017-07-11", "authors_parsed": [["Draelos", "Timothy J.", ""], ["Miner", "Nadine E.", ""], ["Lamb", "Christopher C.", ""], ["Cox", "Jonathan A.", ""], ["Vineyard", "Craig M.", ""], ["Carlson", "Kristofor D.", ""], ["Severa", "William M.", ""], ["James", "Conrad D.", ""], ["Aimone", "James B.", ""]]}, {"id": "1612.03772", "submitter": "Hadi Fanaee-T", "authors": "Hadi Fanaee-T and Joao Gama", "title": "SimTensor: A synthetic tensor data generator", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.MS math.NA stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  SimTensor is a multi-platform, open-source software for generating artificial\ntensor data (either with CP/PARAFAC or Tucker structure) for reproducible\nresearch on tensor factorization algorithms. SimTensor is a stand-alone\napplication based on MATALB. It provides a wide range of facilities for\ngenerating tensor data with various configurations. It comes with a\nuser-friendly graphical user interface, which enables the user to generate\ntensors with complicated settings in an easy way. It also has this facility to\nexport generated data to universal formats such as CSV and HDF5, which can be\nimported via a wide range of programming languages (C, C++, Java, R, Fortran,\nMATLAB, Perl, Python, and many more). The most innovative part of SimTensor is\nthis that can generate temporal tensors with periodic waves, seasonal effects\nand streaming structure. it can apply constraints such as non-negativity and\ndifferent kinds of sparsity to the data. SimTensor also provides this facility\nto simulate different kinds of change-points and inject various types of\nanomalies. The source code and binary versions of SimTensor is available for\ndownload in http://www.simtensor.org.\n", "versions": [{"version": "v1", "created": "Fri, 9 Dec 2016 19:13:03 GMT"}], "update_date": "2016-12-13", "authors_parsed": [["Fanaee-T", "Hadi", ""], ["Gama", "Joao", ""]]}, {"id": "1612.03809", "submitter": "Mehdi Mirza", "authors": "Mehdi Mirza, Aaron Courville, Yoshua Bengio", "title": "Generalizable Features From Unsupervised Learning", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Humans learn a predictive model of the world and use this model to reason\nabout future events and the consequences of actions. In contrast to most\nmachine predictors, we exhibit an impressive ability to generalize to unseen\nscenarios and reason intelligently in these settings. One important aspect of\nthis ability is physical intuition(Lake et al., 2016). In this work, we explore\nthe potential of unsupervised learning to find features that promote better\ngeneralization to settings outside the supervised training distribution. Our\ntask is predicting the stability of towers of square blocks. We demonstrate\nthat an unsupervised model, trained to predict future frames of a video\nsequence of stable and unstable block configurations, can yield features that\nsupport extrapolating stability prediction to blocks configurations outside the\ntraining set distribution\n", "versions": [{"version": "v1", "created": "Mon, 12 Dec 2016 17:45:48 GMT"}], "update_date": "2016-12-13", "authors_parsed": [["Mirza", "Mehdi", ""], ["Courville", "Aaron", ""], ["Bengio", "Yoshua", ""]]}, {"id": "1612.03839", "submitter": "Yun S. Song", "authors": "Miaoyan Wang and Yun S. Song", "title": "Tensor Decompositions via Two-Mode Higher-Order SVD (HOSVD)", "comments": "33 pages, 5 figures", "journal-ref": "Proceedings of the 20th International Conference on Artificial\n  Intelligence and Statistics (AISTATS), PMLR, Vol. 54 (2017) 614-622", "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Tensor decompositions have rich applications in statistics and machine\nlearning, and developing efficient, accurate algorithms for the problem has\nreceived much attention recently. Here, we present a new method built on\nKruskal's uniqueness theorem to decompose symmetric, nearly orthogonally\ndecomposable tensors. Unlike the classical higher-order singular value\ndecomposition which unfolds a tensor along a single mode, we consider\nunfoldings along two modes and use rank-1 constraints to characterize the\nunderlying components. This tensor decomposition method provably handles a\ngreater level of noise compared to previous methods and achieves a high\nestimation accuracy. Numerical results demonstrate that our algorithm is robust\nto various noise distributions and that it performs especially favorably as the\norder increases.\n", "versions": [{"version": "v1", "created": "Mon, 12 Dec 2016 18:38:40 GMT"}, {"version": "v2", "created": "Wed, 19 Apr 2017 01:08:35 GMT"}], "update_date": "2017-04-20", "authors_parsed": [["Wang", "Miaoyan", ""], ["Song", "Yun S.", ""]]}, {"id": "1612.03871", "submitter": "Ashish Sabharwal", "authors": "Hanie Sedghi and Ashish Sabharwal", "title": "Knowledge Completion for Generics using Guided Tensor Factorization", "comments": "To appear in TACL", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Given a knowledge base or KB containing (noisy) facts about common nouns or\ngenerics, such as \"all trees produce oxygen\" or \"some animals live in forests\",\nwe consider the problem of inferring additional such facts at a precision\nsimilar to that of the starting KB. Such KBs capture general knowledge about\nthe world, and are crucial for various applications such as question answering.\nDifferent from commonly studied named entity KBs such as Freebase, generics KBs\ninvolve quantification, have more complex underlying regularities, tend to be\nmore incomplete, and violate the commonly used locally closed world assumption\n(LCWA). We show that existing KB completion methods struggle with this new\ntask, and present the first approach that is successful. Our results\ndemonstrate that external information, such as relation schemas and entity\ntaxonomies, if used appropriately, can be a surprisingly powerful tool in this\nsetting. First, our simple yet effective knowledge guided tensor factorization\napproach achieves state-of-the-art results on two generics KBs (80% precise)\nfor science, doubling their size at 74%-86% precision. Second, our novel\ntaxonomy guided, submodular, active learning method for collecting annotations\nabout rare entities (e.g., oriole, a bird) is 6x more effective at inferring\nfurther new facts about them than multiple active learning baselines.\n", "versions": [{"version": "v1", "created": "Mon, 12 Dec 2016 19:53:04 GMT"}, {"version": "v2", "created": "Fri, 24 Feb 2017 09:08:44 GMT"}, {"version": "v3", "created": "Wed, 28 Mar 2018 18:58:58 GMT"}], "update_date": "2018-03-30", "authors_parsed": [["Sedghi", "Hanie", ""], ["Sabharwal", "Ashish", ""]]}, {"id": "1612.03948", "submitter": "Boian Alexandrov S", "authors": "Valentin G. Stanev, Filip L. Iliev, Scott Hansen, Velimir V.\n  Vesselinov, Boian S. Alexandrov", "title": "Identification of release sources in advection-diffusion system by\n  machine learning combined with Green function inverse method", "comments": null, "journal-ref": null, "doi": "10.1016/j.apm.2018.03.006", "report-no": "LA-UR-16-27231", "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The identification of sources of advection-diffusion transport is based\nusually on solving complex ill-posed inverse models against the available\nstate- variable data records. However, if there are several sources with\ndifferent locations and strengths, the data records represent mixtures rather\nthan the separate influences of the original sources. Importantly, the number\nof these original release sources is typically unknown, which hinders\nreliability of the classical inverse-model analyses. To address this challenge,\nwe present here a novel hybrid method for identification of the unknown number\nof release sources. Our hybrid method, called HNMF, couples unsupervised\nlearning based on Nonnegative Matrix Factorization (NMF) and inverse-analysis\nGreen functions method. HNMF synergistically performs decomposition of the\nrecorded mixtures, finds the number of the unknown sources and uses the Green\nfunction of advection-diffusion equation to identify their characteristics. In\nthe paper, we introduce the method and demonstrate that it is capable of\nidentifying the advection velocity and dispersivity of the medium as well as\nthe unknown number, locations, and properties of various sets of synthetic\nrelease sources with different space and time dependencies, based only on the\nrecorded data. HNMF can be applied directly to any problem controlled by a\npartial-differential parabolic equation where mixtures of an unknown number of\nsources are measured at multiple locations.\n", "versions": [{"version": "v1", "created": "Mon, 12 Dec 2016 22:05:10 GMT"}, {"version": "v2", "created": "Fri, 3 Mar 2017 05:02:43 GMT"}, {"version": "v3", "created": "Fri, 23 Mar 2018 22:13:59 GMT"}], "update_date": "2018-03-28", "authors_parsed": [["Stanev", "Valentin G.", ""], ["Iliev", "Filip L.", ""], ["Hansen", "Scott", ""], ["Vesselinov", "Velimir V.", ""], ["Alexandrov", "Boian S.", ""]]}, {"id": "1612.03950", "submitter": "Boian Alexandrov S", "authors": "Filip L. Iliev, Valentin G. Stanev, Velimir V. Vesselinov, Boian S.\n  Alexandrov", "title": "Nonnegative Matrix Factorization for identification of unknown number of\n  sources emitting delayed signals", "comments": null, "journal-ref": "PloS one. 2018 Mar 8;13(3):e0193974", "doi": null, "report-no": "LA-UR-16-27232", "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Factor analysis is broadly used as a powerful unsupervised machine learning\ntool for reconstruction of hidden features in recorded mixtures of signals. In\nthe case of a linear approximation, the mixtures can be decomposed by a variety\nof model-free Blind Source Separation (BSS) algorithms. Most of the available\nBSS algorithms consider an instantaneous mixing of signals, while the case when\nthe mixtures are linear combinations of signals with delays is less explored.\nEspecially difficult is the case when the number of sources of the signals with\ndelays is unknown and has to be determined from the data as well. To address\nthis problem, in this paper, we present a new method based on Nonnegative\nMatrix Factorization (NMF) that is capable of identifying: (a) the unknown\nnumber of the sources, (b) the delays and speed of propagation of the signals,\nand (c) the locations of the sources. Our method can be used to decompose\nrecords of mixtures of signals with delays emitted by an unknown number of\nsources in a nondispersive medium, based only on recorded data. This is the\ncase, for example, when electromagnetic signals from multiple antennas are\nreceived asynchronously; or mixtures of acoustic or seismic signals recorded by\nsensors located at different positions; or when a shift in frequency is induced\nby the Doppler effect. By applying our method to synthetic datasets, we\ndemonstrate its ability to identify the unknown number of sources as well as\nthe waveforms, the delays, and the strengths of the signals. Using Bayesian\nanalysis, we also evaluate estimation uncertainties and identify the region of\nlikelihood where the positions of the sources can be found.\n", "versions": [{"version": "v1", "created": "Mon, 12 Dec 2016 22:21:41 GMT"}, {"version": "v2", "created": "Fri, 23 Mar 2018 21:50:23 GMT"}], "update_date": "2018-03-28", "authors_parsed": [["Iliev", "Filip L.", ""], ["Stanev", "Valentin G.", ""], ["Vesselinov", "Velimir V.", ""], ["Alexandrov", "Boian S.", ""]]}, {"id": "1612.03957", "submitter": "Rishit Sheth", "authors": "Rishit Sheth, Roni Khardon", "title": "Monte Carlo Structured SVI for Two-Level Non-Conjugate Models", "comments": "Updated w/ mixed effects model", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The stochastic variational inference (SVI) paradigm, which combines\nvariational inference, natural gradients, and stochastic updates, was recently\nproposed for large-scale data analysis in conjugate Bayesian models and\ndemonstrated to be effective in several problems. This paper studies a family\nof Bayesian latent variable models with two levels of hidden variables but\nwithout any conjugacy requirements, making several contributions in this\ncontext. The first is observing that SVI, with an improved structured\nvariational approximation, is applicable under more general conditions than\npreviously thought with the only requirement being that the approximating\nvariational distribution be in the same family as the prior. The resulting\napproach, Monte Carlo Structured SVI (MC-SSVI), significantly extends the scope\nof SVI, enabling large-scale learning in non-conjugate models. For models with\nlatent Gaussian variables we propose a hybrid algorithm, using both standard\nand natural gradients, which is shown to improve stability and convergence.\nApplications in mixed effects models, sparse Gaussian processes, probabilistic\nmatrix factorization and correlated topic models demonstrate the generality of\nthe approach and the advantages of the proposed algorithms.\n", "versions": [{"version": "v1", "created": "Mon, 12 Dec 2016 22:36:04 GMT"}, {"version": "v2", "created": "Mon, 20 Feb 2017 19:42:50 GMT"}, {"version": "v3", "created": "Fri, 2 Feb 2018 18:52:56 GMT"}], "update_date": "2018-02-05", "authors_parsed": [["Sheth", "Rishit", ""], ["Khardon", "Roni", ""]]}, {"id": "1612.03964", "submitter": "Peter Frazier", "authors": "Peter I. Frazier, Shane G. Henderson, Rolf Waeber", "title": "Probabilistic Bisection Converges Almost as Quickly as Stochastic\n  Approximation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.PR math.OC stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The probabilistic bisection algorithm (PBA) solves a class of stochastic\nroot-finding problems in one dimension by successively updating a prior belief\non the location of the root based on noisy responses to queries at chosen\npoints. The responses indicate the direction of the root from the queried\npoint, and are incorrect with a fixed probability. The fixed-probability\nassumption is problematic in applications, and so we extend the PBA to apply\nwhen this assumption is relaxed. The extension involves the use of a power-one\ntest at each queried point. We explore the convergence behavior of the extended\nPBA, showing that it converges at a rate arbitrarily close to, but slower than,\nthe canonical \"square root\" rate of stochastic approximation.\n", "versions": [{"version": "v1", "created": "Mon, 12 Dec 2016 22:57:05 GMT"}], "update_date": "2016-12-14", "authors_parsed": [["Frazier", "Peter I.", ""], ["Henderson", "Shane G.", ""], ["Waeber", "Rolf", ""]]}, {"id": "1612.03981", "submitter": "Brett Israelsen", "authors": "Brett Israelsen and Nisar Ahmed", "title": "Hybrid Repeat/Multi-point Sampling for Highly Volatile Objective\n  Functions", "comments": null, "journal-ref": "BayesOpt Workshop, NIPS 2016", "doi": null, "report-no": null, "categories": "stat.ML cs.AI cs.LG cs.RO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A key drawback of the current generation of artificial decision-makers is\nthat they do not adapt well to changes in unexpected situations. This paper\naddresses the situation in which an AI for aerial dog fighting, with tunable\nparameters that govern its behavior, will optimize behavior with respect to an\nobjective function that must be evaluated and learned through simulations. Once\nthis objective function has been modeled, the agent can then choose its desired\nbehavior in different situations. Bayesian optimization with a Gaussian Process\nsurrogate is used as the method for investigating the objective function. One\nkey benefit is that during optimization the Gaussian Process learns a global\nestimate of the true objective function, with predicted outcomes and a\nstatistical measure of confidence in areas that haven't been investigated yet.\nHowever, standard Bayesian optimization does not perform consistently or\nprovide an accurate Gaussian Process surrogate function for highly volatile\nobjective functions. We treat these problems by introducing a novel sampling\ntechnique called Hybrid Repeat/Multi-point Sampling. This technique gives the\nAI ability to learn optimum behaviors in a highly uncertain environment. More\nimportantly, it not only improves the reliability of the optimization, but also\ncreates a better model of the entire objective surface. With this improved\nmodel the agent is equipped to better adapt behaviors.\n", "versions": [{"version": "v1", "created": "Tue, 13 Dec 2016 00:21:45 GMT"}], "update_date": "2016-12-14", "authors_parsed": [["Israelsen", "Brett", ""], ["Ahmed", "Nisar", ""]]}, {"id": "1612.04021", "submitter": "Daniel Jiwoong Im", "authors": "Daniel Jiwoong Im, He Ma, Chris Dongjoo Kim, Graham Taylor", "title": "Generative Adversarial Parallelization", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Generative Adversarial Networks have become one of the most studied\nframeworks for unsupervised learning due to their intuitive formulation. They\nhave also been shown to be capable of generating convincing examples in limited\ndomains, such as low-resolution images. However, they still prove difficult to\ntrain in practice and tend to ignore modes of the data generating distribution.\nQuantitatively capturing effects such as mode coverage and more generally the\nquality of the generative model still remain elusive. We propose Generative\nAdversarial Parallelization, a framework in which many GANs or their variants\nare trained simultaneously, exchanging their discriminators. This eliminates\nthe tight coupling between a generator and discriminator, leading to improved\nconvergence and improved coverage of modes. We also propose an improved variant\nof the recently proposed Generative Adversarial Metric and show how it can\nscore individual GANs or their collections under the GAP model.\n", "versions": [{"version": "v1", "created": "Tue, 13 Dec 2016 04:19:04 GMT"}], "update_date": "2016-12-14", "authors_parsed": [["Im", "Daniel Jiwoong", ""], ["Ma", "He", ""], ["Kim", "Chris Dongjoo", ""], ["Taylor", "Graham", ""]]}, {"id": "1612.04022", "submitter": "Sulin Liu", "authors": "Sulin Liu, Sinno Jialin Pan, Qirong Ho", "title": "Distributed Multi-Task Relationship Learning", "comments": "To appear in KDD 2017", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Multi-task learning aims to learn multiple tasks jointly by exploiting their\nrelatedness to improve the generalization performance for each task.\nTraditionally, to perform multi-task learning, one needs to centralize data\nfrom all the tasks to a single machine. However, in many real-world\napplications, data of different tasks may be geo-distributed over different\nlocal machines. Due to heavy communication caused by transmitting the data and\nthe issue of data privacy and security, it is impossible to send data of\ndifferent task to a master machine to perform multi-task learning. Therefore,\nin this paper, we propose a distributed multi-task learning framework that\nsimultaneously learns predictive models for each task as well as task\nrelationships between tasks alternatingly in the parameter server paradigm. In\nour framework, we first offer a general dual form for a family of regularized\nmulti-task relationship learning methods. Subsequently, we propose a\ncommunication-efficient primal-dual distributed optimization algorithm to solve\nthe dual problem by carefully designing local subproblems to make the dual\nproblem decomposable. Moreover, we provide a theoretical convergence analysis\nfor the proposed algorithm, which is specific for distributed multi-task\nrelationship learning. We conduct extensive experiments on both synthetic and\nreal-world datasets to evaluate our proposed framework in terms of\neffectiveness and convergence.\n", "versions": [{"version": "v1", "created": "Tue, 13 Dec 2016 04:22:10 GMT"}, {"version": "v2", "created": "Fri, 17 Feb 2017 14:09:19 GMT"}, {"version": "v3", "created": "Tue, 20 Jun 2017 12:00:03 GMT"}], "update_date": "2017-06-21", "authors_parsed": [["Liu", "Sulin", ""], ["Pan", "Sinno Jialin", ""], ["Ho", "Qirong", ""]]}, {"id": "1612.04052", "submitter": "Bodo Rueckauer", "authors": "Bodo Rueckauer, Iulia-Alexandra Lungu, Yuhuang Hu, Michael Pfeiffer", "title": "Theory and Tools for the Conversion of Analog to Spiking Convolutional\n  Neural Networks", "comments": "9 pages, 2 figures, presented at the workshop \"Computing with Spikes\"\n  at NIPS 2016, Barcelona, Spain", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.CV cs.LG cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Deep convolutional neural networks (CNNs) have shown great potential for\nnumerous real-world machine learning applications, but performing inference in\nlarge CNNs in real-time remains a challenge. We have previously demonstrated\nthat traditional CNNs can be converted into deep spiking neural networks\n(SNNs), which exhibit similar accuracy while reducing both latency and\ncomputational load as a consequence of their data-driven, event-based style of\ncomputing. Here we provide a novel theory that explains why this conversion is\nsuccessful, and derive from it several new tools to convert a larger and more\npowerful class of deep networks into SNNs. We identify the main sources of\napproximation errors in previous conversion methods, and propose simple\nmechanisms to fix these issues. Furthermore, we develop spiking implementations\nof common CNN operations such as max-pooling, softmax, and batch-normalization,\nwhich allow almost loss-less conversion of arbitrary CNN architectures into the\nspiking domain. Empirical evaluation of different network architectures on the\nMNIST and CIFAR10 benchmarks leads to the best SNN results reported to date.\n", "versions": [{"version": "v1", "created": "Tue, 13 Dec 2016 07:58:34 GMT"}], "update_date": "2016-12-14", "authors_parsed": [["Rueckauer", "Bodo", ""], ["Lungu", "Iulia-Alexandra", ""], ["Hu", "Yuhuang", ""], ["Pfeiffer", "Michael", ""]]}, {"id": "1612.04111", "submitter": "Alec Koppel", "authors": "Alec Koppel, Garrett Warnell, Ethan Stump, Alejandro Ribeiro", "title": "Parsimonious Online Learning with Kernels via Sparse Projections in\n  Function Space", "comments": "Submitted to JMLR on 11/24/2016", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Despite their attractiveness, popular perception is that techniques for\nnonparametric function approximation do not scale to streaming data due to an\nintractable growth in the amount of storage they require. To solve this problem\nin a memory-affordable way, we propose an online technique based on functional\nstochastic gradient descent in tandem with supervised sparsification based on\ngreedy function subspace projections. The method, called parsimonious online\nlearning with kernels (POLK), provides a controllable tradeoff? between its\nsolution accuracy and the amount of memory it requires. We derive conditions\nunder which the generated function sequence converges almost surely to the\noptimal function, and we establish that the memory requirement remains finite.\nWe evaluate POLK for kernel multi-class logistic regression and kernel\nhinge-loss classification on three canonical data sets: a synthetic Gaussian\nmixture model, the MNIST hand-written digits, and the Brodatz texture database.\nOn all three tasks, we observe a favorable tradeoff of objective function\nevaluation, classification performance, and complexity of the nonparametric\nregressor extracted the proposed method.\n", "versions": [{"version": "v1", "created": "Tue, 13 Dec 2016 12:01:28 GMT"}], "update_date": "2016-12-14", "authors_parsed": [["Koppel", "Alec", ""], ["Warnell", "Garrett", ""], ["Stump", "Ethan", ""], ["Ribeiro", "Alejandro", ""]]}, {"id": "1612.04112", "submitter": "Naoki Hayashi", "authors": "Naoki Hayashi, Sumio Watanabe", "title": "Upper Bound of Bayesian Generalization Error in Non-negative Matrix\n  Factorization", "comments": "21 pages, 1 table. / Neurocomputing Vol. 266. / ERRATA: Proof of\n  Lemma 3.3 and Discussion is corrected", "journal-ref": "Neurocomputing, Volume 266C, 29 November 2017, pp.21-28", "doi": "10.1016/j.neucom.2017.04.068", "report-no": null, "categories": "math.ST cs.LG stat.ML stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Non-negative matrix factorization (NMF) is a new knowledge discovery method\nthat is used for text mining, signal processing, bioinformatics, and consumer\nanalysis. However, its basic property as a learning machine is not yet\nclarified, as it is not a regular statistical model, resulting that theoretical\noptimization method of NMF has not yet established. In this paper, we study the\nreal log canonical threshold of NMF and give an upper bound of the\ngeneralization error in Bayesian learning. The results show that the\ngeneralization error of the matrix factorization can be made smaller than\nregular statistical models if Bayesian learning is applied.\n", "versions": [{"version": "v1", "created": "Tue, 13 Dec 2016 12:02:24 GMT"}, {"version": "v2", "created": "Wed, 15 Feb 2017 07:24:03 GMT"}, {"version": "v3", "created": "Wed, 22 Feb 2017 10:30:36 GMT"}, {"version": "v4", "created": "Fri, 16 Jun 2017 03:54:56 GMT"}, {"version": "v5", "created": "Sun, 1 Oct 2017 03:41:30 GMT"}], "update_date": "2019-06-10", "authors_parsed": [["Hayashi", "Naoki", ""], ["Watanabe", "Sumio", ""]]}, {"id": "1612.04174", "submitter": "Bruno Nicenboim", "authors": "Bruno Nicenboim and Shravan Vasishth", "title": "Models of retrieval in sentence comprehension: A computational\n  evaluation using Bayesian hierarchical modeling", "comments": "Accepted in Journal of Memory and Language", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL stat.AP stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Research on interference has provided evidence that the formation of\ndependencies between non-adjacent words relies on a cue-based retrieval\nmechanism. Two different models can account for one of the main predictions of\ninterference, i.e., a slowdown at a retrieval site, when several items share a\nfeature associated with a retrieval cue: Lewis and Vasishth's (2005)\nactivation-based model and McElree's (2000) direct access model. Even though\nthese two models have been used almost interchangeably, they are based on\ndifferent assumptions and predict differences in the relationship between\nreading times and response accuracy. The activation-based model follows the\nassumptions of ACT-R, and its retrieval process behaves as a lognormal race\nbetween accumulators of evidence with a single variance. Under this model,\naccuracy of the retrieval is determined by the winner of the race and retrieval\ntime by its rate of accumulation. In contrast, the direct access model assumes\na model of memory where only the probability of retrieval varies between items;\nin this model, differences in latencies are a by-product of the possibility and\nrepairing incorrect retrievals. We implemented both models in a Bayesian\nhierarchical framework in order to evaluate them and compare them. We show that\nsome aspects of the data are better fit under the direct access model than\nunder the activation-based model. We suggest that this finding does not rule\nout the possibility that retrieval may be behaving as a race model with\nassumptions that follow less closely the ones from the ACT-R framework. We show\nthat by introducing a modification of the activation model, i.e, by assuming\nthat the accumulation of evidence for retrieval of incorrect items is not only\nslower but noisier (i.e., different variances for the correct and incorrect\nitems), the model can provide a fit as good as the one of the direct access\nmodel.\n", "versions": [{"version": "v1", "created": "Tue, 13 Dec 2016 13:55:39 GMT"}, {"version": "v2", "created": "Thu, 24 Aug 2017 11:05:55 GMT"}], "update_date": "2017-08-25", "authors_parsed": [["Nicenboim", "Bruno", ""], ["Vasishth", "Shravan", ""]]}, {"id": "1612.04262", "submitter": "Long-Gang Pang", "authors": "Long-Gang Pang, Kai Zhou, Nan Su, Hannah Petersen, Horst St\\\"ocker and\n  Xin-Nian Wang", "title": "An equation-of-state-meter of QCD transition from deep learning", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "hep-ph cs.LG hep-th nucl-th stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Supervised learning with a deep convolutional neural network is used to\nidentify the QCD equation of state (EoS) employed in relativistic hydrodynamic\nsimulations of heavy-ion collisions from the simulated final-state particle\nspectra $\\rho(p_T,\\Phi)$. High-level correlations of $\\rho(p_T,\\Phi)$ learned\nby the neural network act as an effective \"EoS-meter\" in detecting the nature\nof the QCD transition. The EoS-meter is model independent and insensitive to\nother simulation inputs, especially the initial conditions. Thus it provides a\npowerful direct-connection of heavy-ion collision observables with the bulk\nproperties of QCD.\n", "versions": [{"version": "v1", "created": "Tue, 13 Dec 2016 16:19:00 GMT"}, {"version": "v2", "created": "Mon, 20 Mar 2017 23:53:50 GMT"}, {"version": "v3", "created": "Wed, 2 Aug 2017 01:50:06 GMT"}], "update_date": "2017-08-03", "authors_parsed": [["Pang", "Long-Gang", ""], ["Zhou", "Kai", ""], ["Su", "Nan", ""], ["Petersen", "Hannah", ""], ["St\u00f6cker", "Horst", ""], ["Wang", "Xin-Nian", ""]]}, {"id": "1612.04315", "submitter": "Brett Israelsen", "authors": "Brett W. Israelsen, Nisar Ahmed, Kenneth Center, Roderick Green,\n  Winston Bennett Jr", "title": "Towards Adaptive Training of Agent-based Sparring Partners for Fighter\n  Pilots", "comments": "submitted copy", "journal-ref": "SciTech 2017, paper 2545524", "doi": null, "report-no": null, "categories": "stat.ML cs.AI cs.LG cs.RO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A key requirement for the current generation of artificial decision-makers is\nthat they should adapt well to changes in unexpected situations. This paper\naddresses the situation in which an AI for aerial dog fighting, with tunable\nparameters that govern its behavior, must optimize behavior with respect to an\nobjective function that is evaluated and learned through simulations. Bayesian\noptimization with a Gaussian Process surrogate is used as the method for\ninvestigating the objective function. One key benefit is that during\noptimization, the Gaussian Process learns a global estimate of the true\nobjective function, with predicted outcomes and a statistical measure of\nconfidence in areas that haven't been investigated yet. Having a model of the\nobjective function is important for being able to understand possible outcomes\nin the decision space; for example this is crucial for training and providing\nfeedback to human pilots. However, standard Bayesian optimization does not\nperform consistently or provide an accurate Gaussian Process surrogate function\nfor highly volatile objective functions. We treat these problems by introducing\na novel sampling technique called Hybrid Repeat/Multi-point Sampling. This\ntechnique gives the AI ability to learn optimum behaviors in a highly uncertain\nenvironment. More importantly, it not only improves the reliability of the\noptimization, but also creates a better model of the entire objective surface.\nWith this improved model the agent is equipped to more accurately/efficiently\npredict performance in unexplored scenarios.\n", "versions": [{"version": "v1", "created": "Tue, 13 Dec 2016 18:43:59 GMT"}], "update_date": "2016-12-14", "authors_parsed": [["Israelsen", "Brett W.", ""], ["Ahmed", "Nisar", ""], ["Center", "Kenneth", ""], ["Green", "Roderick", ""], ["Bennett", "Winston", "Jr"]]}, {"id": "1612.04340", "submitter": "Senthil Yogamani", "authors": "Ahmad El Sallab, Mohammed Abdou, Etienne Perot and Senthil Yogamani", "title": "End-to-End Deep Reinforcement Learning for Lane Keeping Assist", "comments": "Presented at the Machine Learning for Intelligent Transportation\n  Systems Workshop, NIPS 2016", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG cs.RO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Reinforcement learning is considered to be a strong AI paradigm which can be\nused to teach machines through interaction with the environment and learning\nfrom their mistakes, but it has not yet been successfully used for automotive\napplications. There has recently been a revival of interest in the topic,\nhowever, driven by the ability of deep learning algorithms to learn good\nrepresentations of the environment. Motivated by Google DeepMind's successful\ndemonstrations of learning for games from Breakout to Go, we will propose\ndifferent methods for autonomous driving using deep reinforcement learning.\nThis is of particular interest as it is difficult to pose autonomous driving as\na supervised learning problem as it has a strong interaction with the\nenvironment including other vehicles, pedestrians and roadworks. As this is a\nrelatively new area of research for autonomous driving, we will formulate two\nmain categories of algorithms: 1) Discrete actions category, and 2) Continuous\nactions category. For the discrete actions category, we will deal with Deep\nQ-Network Algorithm (DQN) while for the continuous actions category, we will\ndeal with Deep Deterministic Actor Critic Algorithm (DDAC). In addition to\nthat, We will also discover the performance of these two categories on an open\nsource car simulator for Racing called (TORCS) which stands for The Open Racing\ncar Simulator. Our simulation results demonstrate learning of autonomous\nmaneuvering in a scenario of complex road curvatures and simple interaction\nwith other vehicles. Finally, we explain the effect of some restricted\nconditions, put on the car during the learning phase, on the convergence time\nfor finishing its learning phase.\n", "versions": [{"version": "v1", "created": "Tue, 13 Dec 2016 20:19:42 GMT"}], "update_date": "2016-12-14", "authors_parsed": [["Sallab", "Ahmad El", ""], ["Abdou", "Mohammed", ""], ["Perot", "Etienne", ""], ["Yogamani", "Senthil", ""]]}, {"id": "1612.04357", "submitter": "Xun Huang", "authors": "Xun Huang, Yixuan Li, Omid Poursaeed, John Hopcroft, Serge Belongie", "title": "Stacked Generative Adversarial Networks", "comments": "CVPR 2017, camera-ready version", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG cs.NE stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we propose a novel generative model named Stacked Generative\nAdversarial Networks (SGAN), which is trained to invert the hierarchical\nrepresentations of a bottom-up discriminative network. Our model consists of a\ntop-down stack of GANs, each learned to generate lower-level representations\nconditioned on higher-level representations. A representation discriminator is\nintroduced at each feature hierarchy to encourage the representation manifold\nof the generator to align with that of the bottom-up discriminative network,\nleveraging the powerful discriminative representations to guide the generative\nmodel. In addition, we introduce a conditional loss that encourages the use of\nconditional information from the layer above, and a novel entropy loss that\nmaximizes a variational lower bound on the conditional entropy of generator\noutputs. We first train each stack independently, and then train the whole\nmodel end-to-end. Unlike the original GAN that uses a single noise vector to\nrepresent all the variations, our SGAN decomposes variations into multiple\nlevels and gradually resolves uncertainties in the top-down generative process.\nBased on visual inspection, Inception scores and visual Turing test, we\ndemonstrate that SGAN is able to generate images of much higher quality than\nGANs without stacking.\n", "versions": [{"version": "v1", "created": "Tue, 13 Dec 2016 20:48:58 GMT"}, {"version": "v2", "created": "Tue, 31 Jan 2017 01:21:55 GMT"}, {"version": "v3", "created": "Tue, 7 Mar 2017 07:50:27 GMT"}, {"version": "v4", "created": "Wed, 12 Apr 2017 15:04:01 GMT"}], "update_date": "2017-04-13", "authors_parsed": [["Huang", "Xun", ""], ["Li", "Yixuan", ""], ["Poursaeed", "Omid", ""], ["Hopcroft", "John", ""], ["Belongie", "Serge", ""]]}, {"id": "1612.04413", "submitter": "Rahul Gupta", "authors": "Rahul Gupta, Shrikanth Narayanan", "title": "Inferring object rankings based on noisy pairwise comparisons from\n  multiple annotators", "comments": "4 figures, includes appendix", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Ranking a set of objects involves establishing an order allowing for\ncomparisons between any pair of objects in the set. Oftentimes, due to the\nunavailability of a ground truth of ranked orders, researchers resort to\nobtaining judgments from multiple annotators followed by inferring the ground\ntruth based on the collective knowledge of the crowd. However, the aggregation\nis often ad-hoc and involves imposing stringent assumptions in inferring the\nground truth (e.g. majority vote). In this work, we propose\nExpectation-Maximization (EM) based algorithms that rely on the judgments from\nmultiple annotators and the object attributes for inferring the latent ground\ntruth. The algorithm learns the relation between the latent ground truth and\nobject attributes as well as annotator specific probabilities of flipping, a\nmetric to assess annotator quality. We further extend the EM algorithm to allow\nfor a variable probability of flipping based on the pair of objects at hand. We\ntest our algorithms on two data sets with synthetic annotations and investigate\nthe impact of annotator quality and quantity on the inferred ground truth. We\nalso obtain the results on two other data sets with annotations from\nmachine/human annotators and interpret the output trends based on the data\ncharacteristics.\n", "versions": [{"version": "v1", "created": "Tue, 13 Dec 2016 21:52:12 GMT"}], "update_date": "2016-12-19", "authors_parsed": [["Gupta", "Rahul", ""], ["Narayanan", "Shrikanth", ""]]}, {"id": "1612.04418", "submitter": "Alexey Drutsa", "authors": "Alexey Drutsa (Yandex, Moscow, Russia), Andrey Shutovich (Yandex,\n  Moscow, Russia), Philipp Pushnyakov (Yandex, Moscow, Russia), Evgeniy\n  Krokhalyov (Yandex, Moscow, Russia), Gleb Gusev (Yandex, Moscow, Russia),\n  Pavel Serdyukov (Yandex, Moscow, Russia)", "title": "User Model-Based Intent-Aware Metrics for Multilingual Search Evaluation", "comments": "7 pages, 1 figure, 3 tables", "journal-ref": "NIPS 2016 Workshop \"What If? Inference and Learning of\n  Hypothetical and Counterfactual Interventions in Complex Systems\" (What If\n  2016) pre-print", "doi": null, "report-no": null, "categories": "cs.IR cs.CL cs.HC cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Despite the growing importance of multilingual aspect of web search, no\nappropriate offline metrics to evaluate its quality are proposed so far. At the\nsame time, personal language preferences can be regarded as intents of a query.\nThis approach translates the multilingual search problem into a particular task\nof search diversification. Furthermore, the standard intent-aware approach\ncould be adopted to build a diversified metric for multilingual search on the\nbasis of a classical IR metric such as ERR. The intent-aware approach estimates\nuser satisfaction under a user behavior model. We show however that the\nunderlying user behavior models is not realistic in the multilingual case, and\nthe produced intent-aware metric do not appropriately estimate the user\nsatisfaction. We develop a novel approach to build intent-aware user behavior\nmodels, which overcome these limitations and convert to quality metrics that\nbetter correlate with standard online metrics of user satisfaction.\n", "versions": [{"version": "v1", "created": "Tue, 13 Dec 2016 22:09:24 GMT"}], "update_date": "2016-12-15", "authors_parsed": [["Drutsa", "Alexey", "", "Yandex, Moscow, Russia"], ["Shutovich", "Andrey", "", "Yandex,\n  Moscow, Russia"], ["Pushnyakov", "Philipp", "", "Yandex, Moscow, Russia"], ["Krokhalyov", "Evgeniy", "", "Yandex, Moscow, Russia"], ["Gusev", "Gleb", "", "Yandex, Moscow, Russia"], ["Serdyukov", "Pavel", "", "Yandex, Moscow, Russia"]]}, {"id": "1612.04423", "submitter": "Bethany Lusch", "authors": "Bethany Lusch, Jake Weholt, Pedro D. Maia, J. Nathan Kutz", "title": "Modeling cognitive deficits following neurodegenerative diseases and\n  traumatic brain injuries with deep convolutional neural networks", "comments": "13 pages, 11 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "q-bio.NC q-bio.QM stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The accurate diagnosis and assessment of neurodegenerative disease and\ntraumatic brain injuries (TBI) remain open challenges. Both cause cognitive and\nfunctional deficits due to focal axonal swellings (FAS), but it is difficult to\ndeliver a prognosis due to our limited ability to assess damaged neurons at a\ncellular level in vivo. We simulate the effects of neurodegenerative disease\nand TBI using convolutional neural networks (CNNs) as our model of cognition.\nWe utilize biophysically relevant statistical data on FAS to damage the\nconnections in CNNs in a functionally relevant way. We incorporate energy\nconstraints on the brain by pruning the CNNs to be less over-engineered.\nQualitatively, we demonstrate that damage leads to human-like mistakes. Our\nexperiments also provide quantitative assessments of how accuracy is affected\nby various types and levels of damage. The deficit resulting from a fixed\namount of damage greatly depends on which connections are randomly injured,\nproviding intuition for why it is difficult to predict impairments. There is a\nlarge degree of subjectivity when it comes to interpreting cognitive deficits\nfrom complex systems such as the human brain. However, we provide important\ninsight and a quantitative framework for disorders in which FAS are implicated.\n", "versions": [{"version": "v1", "created": "Tue, 13 Dec 2016 22:50:56 GMT"}], "update_date": "2016-12-15", "authors_parsed": [["Lusch", "Bethany", ""], ["Weholt", "Jake", ""], ["Maia", "Pedro D.", ""], ["Kutz", "J. Nathan", ""]]}, {"id": "1612.04425", "submitter": "Yangyang Xu", "authors": "Zhimin Peng, Yangyang Xu, Ming Yan, Wotao Yin", "title": "On the Convergence of Asynchronous Parallel Iteration with Unbounded\n  Delays", "comments": "accepted to JORSC", "journal-ref": "Journal of the Operations Research Society of China, 7 (2019),\n  5-42", "doi": "10.1007/s40305-017-0183-1", "report-no": null, "categories": "math.OC cs.DC math.NA stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recent years have witnessed the surge of asynchronous parallel\n(async-parallel) iterative algorithms due to problems involving very\nlarge-scale data and a large number of decision variables. Because of\nasynchrony, the iterates are computed with outdated information, and the age of\nthe outdated information, which we call delay, is the number of times it has\nbeen updated since its creation. Almost all recent works prove convergence\nunder the assumption of a finite maximum delay and set their stepsize\nparameters accordingly. However, the maximum delay is practically unknown.\n  This paper presents convergence analysis of an async-parallel method from a\nprobabilistic viewpoint, and it allows for large unbounded delays. An explicit\nformula of stepsize that guarantees convergence is given depending on delays'\nstatistics. With $p+1$ identical processors, we empirically measured that\ndelays closely follow the Poisson distribution with parameter $p$, matching our\ntheoretical model, and thus the stepsize can be set accordingly. Simulations on\nboth convex and nonconvex optimization problems demonstrate the validness of\nour analysis and also show that the existing maximum-delay induced stepsize is\ntoo conservative, often slowing down the convergence of the algorithm.\n", "versions": [{"version": "v1", "created": "Tue, 13 Dec 2016 23:02:26 GMT"}, {"version": "v2", "created": "Wed, 15 Nov 2017 16:17:32 GMT"}], "update_date": "2021-02-05", "authors_parsed": [["Peng", "Zhimin", ""], ["Xu", "Yangyang", ""], ["Yan", "Ming", ""], ["Yin", "Wotao", ""]]}, {"id": "1612.04440", "submitter": "Will Grathwohl", "authors": "Will Grathwohl, Aaron Wilson", "title": "Disentangling Space and Time in Video with Hierarchical Variational\n  Auto-encoders", "comments": "fixed typo in equation 16", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  There are many forms of feature information present in video data. Principle\namong them are object identity information which is largely static across\nmultiple video frames, and object pose and style information which continuously\ntransforms from frame to frame. Most existing models confound these two types\nof representation by mapping them to a shared feature space. In this paper we\npropose a probabilistic approach for learning separable representations of\nobject identity and pose information using unsupervised video data. Our\napproach leverages a deep generative model with a factored prior distribution\nthat encodes properties of temporal invariances in the hidden feature set.\nLearning is achieved via variational inference. We present results of learning\nidentity and pose information on a dataset of moving characters as well as a\ndataset of rotating 3D objects. Our experimental results demonstrate our\nmodel's success in factoring its representation, and demonstrate that the model\nachieves improved performance in transfer learning tasks.\n", "versions": [{"version": "v1", "created": "Wed, 14 Dec 2016 00:20:46 GMT"}, {"version": "v2", "created": "Mon, 19 Dec 2016 17:17:26 GMT"}], "update_date": "2016-12-20", "authors_parsed": [["Grathwohl", "Will", ""], ["Wilson", "Aaron", ""]]}, {"id": "1612.04468", "submitter": "Jason J Corso", "authors": "Parker Koch and Jason J. Corso", "title": "Sparse Factorization Layers for Neural Networks with Limited Supervision", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Whereas CNNs have demonstrated immense progress in many vision problems, they\nsuffer from a dependence on monumental amounts of labeled training data. On the\nother hand, dictionary learning does not scale to the size of problems that\nCNNs can handle, despite being very effective at low-level vision tasks such as\ndenoising and inpainting. Recently, interest has grown in adapting dictionary\nlearning methods for supervised tasks such as classification and inverse\nproblems. We propose two new network layers that are based on dictionary\nlearning: a sparse factorization layer and a convolutional sparse factorization\nlayer, analogous to fully-connected and convolutional layers, respectively.\nUsing our derivations, these layers can be dropped in to existing CNNs, trained\ntogether in an end-to-end fashion with back-propagation, and leverage\nsemisupervision in ways classical CNNs cannot. We experimentally compare\nnetworks with these two new layers against a baseline CNN. Our results\ndemonstrate that networks with either of the sparse factorization layers are\nable to outperform classical CNNs when supervised data are few. They also show\nperformance improvements in certain tasks when compared to the CNN with no\nsparse factorization layers with the same exact number of parameters.\n", "versions": [{"version": "v1", "created": "Wed, 14 Dec 2016 03:13:29 GMT"}], "update_date": "2016-12-15", "authors_parsed": [["Koch", "Parker", ""], ["Corso", "Jason J.", ""]]}, {"id": "1612.04526", "submitter": "Remi Flamary", "authors": "R\\'emi Flamary", "title": "Astronomical image reconstruction with convolutional neural networks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV astro-ph.IM stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  State of the art methods in astronomical image reconstruction rely on the\nresolution of a regularized or constrained optimization problem. Solving this\nproblem can be computationally intensive and usually leads to a quadratic or at\nleast superlinear complexity w.r.t. the number of pixels in the image. We\ninvestigate in this work the use of convolutional neural networks for image\nreconstruction in astronomy. With neural networks, the computationally\nintensive tasks is the training step, but the prediction step has a fixed\ncomplexity per pixel, i.e. a linear complexity. Numerical experiments show that\nour approach is both computationally efficient and competitive with other state\nof the art methods in addition to being interpretable.\n", "versions": [{"version": "v1", "created": "Wed, 14 Dec 2016 08:17:07 GMT"}, {"version": "v2", "created": "Wed, 7 Jun 2017 08:25:27 GMT"}], "update_date": "2017-06-08", "authors_parsed": [["Flamary", "R\u00e9mi", ""]]}, {"id": "1612.04530", "submitter": "Nicholas Guttenberg", "authors": "Nicholas Guttenberg and Nathaniel Virgo and Olaf Witkowski and\n  Hidetoshi Aoki and Ryota Kanai", "title": "Permutation-equivariant neural networks applied to dynamics prediction", "comments": "7 pages, 4 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The introduction of convolutional layers greatly advanced the performance of\nneural networks on image tasks due to innately capturing a way of encoding and\nlearning translation-invariant operations, matching one of the underlying\nsymmetries of the image domain. In comparison, there are a number of problems\nin which there are a number of different inputs which are all 'of the same\ntype' --- multiple particles, multiple agents, multiple stock prices, etc. The\ncorresponding symmetry to this is permutation symmetry, in that the algorithm\nshould not depend on the specific ordering of the input data. We discuss a\npermutation-invariant neural network layer in analogy to convolutional layers,\nand show the ability of this architecture to learn to predict the motion of a\nvariable number of interacting hard discs in 2D. In the same way that\nconvolutional layers can generalize to different image sizes, the permutation\nlayer we describe generalizes to different numbers of objects.\n", "versions": [{"version": "v1", "created": "Wed, 14 Dec 2016 08:31:53 GMT"}], "update_date": "2016-12-15", "authors_parsed": [["Guttenberg", "Nicholas", ""], ["Virgo", "Nathaniel", ""], ["Witkowski", "Olaf", ""], ["Aoki", "Hidetoshi", ""], ["Kanai", "Ryota", ""]]}, {"id": "1612.04555", "submitter": "S{\\o}ren F{\\o}ns Vind Nielsen", "authors": "Jesper L. Hinrich, S{\\o}ren F. V. Nielsen, Nicolai A. B. Riis, Casper\n  T. Eriksen, Jacob Fr{\\o}sig, Marco D. F. Kristensen, Mikkel N. Schmidt,\n  Kristoffer H. Madsen and Morten M{\\o}rup", "title": "Scalable Group Level Probabilistic Sparse Factor Analysis", "comments": "10 pages plus 5 pages appendix, Submitted to ICASSP 17", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Many data-driven approaches exist to extract neural representations of\nfunctional magnetic resonance imaging (fMRI) data, but most of them lack a\nproper probabilistic formulation. We propose a group level scalable\nprobabilistic sparse factor analysis (psFA) allowing spatially sparse maps,\ncomponent pruning using automatic relevance determination (ARD) and subject\nspecific heteroscedastic spatial noise modeling. For task-based and resting\nstate fMRI, we show that the sparsity constraint gives rise to components\nsimilar to those obtained by group independent component analysis. The noise\nmodeling shows that noise is reduced in areas typically associated with\nactivation by the experimental design. The psFA model identifies sparse\ncomponents and the probabilistic setting provides a natural way to handle\nparameter uncertainties. The variational Bayesian framework easily extends to\nmore complex noise models than the presently considered.\n", "versions": [{"version": "v1", "created": "Wed, 14 Dec 2016 09:59:51 GMT"}], "update_date": "2016-12-15", "authors_parsed": [["Hinrich", "Jesper L.", ""], ["Nielsen", "S\u00f8ren F. V.", ""], ["Riis", "Nicolai A. B.", ""], ["Eriksen", "Casper T.", ""], ["Fr\u00f8sig", "Jacob", ""], ["Kristensen", "Marco D. F.", ""], ["Schmidt", "Mikkel N.", ""], ["Madsen", "Kristoffer H.", ""], ["M\u00f8rup", "Morten", ""]]}, {"id": "1612.04600", "submitter": "Joerg Evermann", "authors": "Joerg Evermann, Jana-Rebecca Rehse, Peter Fettke", "title": "Predicting Process Behaviour using Deep Learning", "comments": "34 pages, 10 figures", "journal-ref": null, "doi": "10.1016/j.dss.2017.04.003", "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Predicting business process behaviour is an important aspect of business\nprocess management. Motivated by research in natural language processing, this\npaper describes an application of deep learning with recurrent neural networks\nto the problem of predicting the next event in a business process. This is both\na novel method in process prediction, which has largely relied on explicit\nprocess models, and also a novel application of deep learning methods. The\napproach is evaluated on two real datasets and our results surpass the\nstate-of-the-art in prediction precision.\n", "versions": [{"version": "v1", "created": "Wed, 14 Dec 2016 12:33:28 GMT"}, {"version": "v2", "created": "Wed, 22 Mar 2017 17:22:08 GMT"}], "update_date": "2017-05-05", "authors_parsed": [["Evermann", "Joerg", ""], ["Rehse", "Jana-Rebecca", ""], ["Fettke", "Peter", ""]]}, {"id": "1612.04642", "submitter": "Daniel Worrall", "authors": "Daniel E. Worrall, Stephan J. Garbin, Daniyar Turmukhambetov and\n  Gabriel J. Brostow", "title": "Harmonic Networks: Deep Translation and Rotation Equivariance", "comments": "Submitted to CVPR 2017", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Translating or rotating an input image should not affect the results of many\ncomputer vision tasks. Convolutional neural networks (CNNs) are already\ntranslation equivariant: input image translations produce proportionate feature\nmap translations. This is not the case for rotations. Global rotation\nequivariance is typically sought through data augmentation, but patch-wise\nequivariance is more difficult. We present Harmonic Networks or H-Nets, a CNN\nexhibiting equivariance to patch-wise translation and 360-rotation. We achieve\nthis by replacing regular CNN filters with circular harmonics, returning a\nmaximal response and orientation for every receptive field patch.\n  H-Nets use a rich, parameter-efficient and low computational complexity\nrepresentation, and we show that deep feature maps within the network encode\ncomplicated rotational invariants. We demonstrate that our layers are general\nenough to be used in conjunction with the latest architectures and techniques,\nsuch as deep supervision and batch normalization. We also achieve\nstate-of-the-art classification on rotated-MNIST, and competitive results on\nother benchmark challenges.\n", "versions": [{"version": "v1", "created": "Wed, 14 Dec 2016 14:01:11 GMT"}, {"version": "v2", "created": "Tue, 11 Apr 2017 13:34:17 GMT"}], "update_date": "2017-04-12", "authors_parsed": [["Worrall", "Daniel E.", ""], ["Garbin", "Stephan J.", ""], ["Turmukhambetov", "Daniyar", ""], ["Brostow", "Gabriel J.", ""]]}, {"id": "1612.04679", "submitter": "Hadi Zare", "authors": "Mahdi Hajiabadi, Hadi Zare, Hossein Bobarshad", "title": "IEDC: An Integrated Approach for Overlapping and Non-overlapping\n  Community Detection", "comments": "The paper is accepted in Knowledge-Based Systems journal, 12 Figures,\n  6 Tables", "journal-ref": null, "doi": "10.1016/j.knosys.2017.02.018", "report-no": null, "categories": "cs.SI stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Community detection is a task of fundamental importance in social network\nanalysis that can be used in a variety of knowledge-based domains. While there\nexist many works on community detection based on connectivity structures, they\nsuffer from either considering the overlapping or non-overlapping communities.\nIn this work, we propose a novel approach for general community detection\nthrough an integrated framework to extract the overlapping and non-overlapping\ncommunity structures without assuming prior structural connectivity on\nnetworks. Our general framework is based on a primary node based criterion\nwhich consists of the internal association degree along with the external\nassociation degree. The evaluation of the proposed method is investigated\nthrough the extensive simulation experiments and several benchmark real network\ndatasets. The experimental results show that the proposed method outperforms\nthe earlier state-of-the-art algorithms based on the well-known evaluation\ncriteria.\n", "versions": [{"version": "v1", "created": "Wed, 14 Dec 2016 15:14:45 GMT"}, {"version": "v2", "created": "Mon, 13 Feb 2017 11:27:37 GMT"}], "update_date": "2017-02-14", "authors_parsed": [["Hajiabadi", "Mahdi", ""], ["Zare", "Hadi", ""], ["Bobarshad", "Hossein", ""]]}, {"id": "1612.04717", "submitter": "Tianxi Li", "authors": "Tianxi Li, Elizaveta Levina, Ji Zhu", "title": "Network cross-validation by edge sampling", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  While many statistical models and methods are now available for network\nanalysis, resampling network data remains a challenging problem.\nCross-validation is a useful general tool for model selection and parameter\ntuning, but is not directly applicable to networks since splitting network\nnodes into groups requires deleting edges and destroys some of the network\nstructure. Here we propose a new network resampling strategy based on splitting\nnode pairs rather than nodes applicable to cross-validation for a wide range of\nnetwork model selection tasks. We provide a theoretical justification for our\nmethod in a general setting and examples of how our method can be used in\nspecific network model selection and parameter tuning tasks. Numerical results\non simulated networks and on a citation network of statisticians show that this\ncross-validation approach works well for model selection.\n", "versions": [{"version": "v1", "created": "Wed, 14 Dec 2016 16:36:30 GMT"}, {"version": "v2", "created": "Thu, 15 Dec 2016 19:34:19 GMT"}, {"version": "v3", "created": "Sun, 1 Jan 2017 21:03:42 GMT"}, {"version": "v4", "created": "Wed, 13 Sep 2017 20:05:34 GMT"}, {"version": "v5", "created": "Wed, 9 May 2018 18:49:21 GMT"}, {"version": "v6", "created": "Thu, 14 Mar 2019 02:00:51 GMT"}, {"version": "v7", "created": "Fri, 1 May 2020 07:38:59 GMT"}], "update_date": "2020-05-04", "authors_parsed": [["Li", "Tianxi", ""], ["Levina", "Elizaveta", ""], ["Zhu", "Ji", ""]]}, {"id": "1612.04759", "submitter": "Marco Cusumano-Towner", "authors": "Marco F. Cusumano-Towner, Vikash K. Mansinghka", "title": "Encapsulating models and approximate inference programs in probabilistic\n  modules", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper introduces the probabilistic module interface, which allows\nencapsulation of complex probabilistic models with latent variables alongside\ncustom stochastic approximate inference machinery, and provides a\nplatform-agnostic abstraction barrier separating the model internals from the\nhost probabilistic inference system. The interface can be seen as a stochastic\ngeneralization of a standard simulation and density interface for probabilistic\nprimitives. We show that sound approximate inference algorithms can be\nconstructed for networks of probabilistic modules, and we demonstrate that the\ninterface can be implemented using learned stochastic inference networks and\nMCMC and SMC approximate inference programs.\n", "versions": [{"version": "v1", "created": "Wed, 14 Dec 2016 18:14:59 GMT"}, {"version": "v2", "created": "Sat, 6 May 2017 21:13:42 GMT"}], "update_date": "2017-05-09", "authors_parsed": [["Cusumano-Towner", "Marco F.", ""], ["Mansinghka", "Vikash K.", ""]]}, {"id": "1612.04785", "submitter": "Masayuki Ohzeki", "authors": "Masayuki Ohzeki", "title": "Quantum Monte Carlo simulation of a particular class of non-stoquastic\n  Hamiltonians in quantum annealing", "comments": "9 pages, published in Scientifc Reports", "journal-ref": null, "doi": null, "report-no": null, "categories": "quant-ph cond-mat.dis-nn cond-mat.stat-mech stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Quantum annealing is a generic solver of the optimization problem that uses\nfictitious quantum fluctuation. Its simulation in classical computing is often\nperformed using the quantum Monte Carlo simulation via the Suzuki--Trotter\ndecomposition. However, the negative sign problem sometimes emerges in the\nsimulation of quantum annealing with an elaborate driver Hamiltonian, since it\nbelongs to a class of non-stoquastic Hamiltonians. In the present study, we\npropose an alternative way to avoid the negative sign problem involved in a\nparticular class of the non-stoquastic Hamiltonians. To check the validity of\nthe method, we demonstrate our method by applying it to a simple problem that\nincludes the anti-ferromagnetic XX interaction, which is a typical instance of\nthe non-stoquastic Hamiltonians.\n", "versions": [{"version": "v1", "created": "Wed, 14 Dec 2016 19:59:54 GMT"}], "update_date": "2016-12-15", "authors_parsed": [["Ohzeki", "Masayuki", ""]]}, {"id": "1612.04799", "submitter": "William Guss", "authors": "William H. Guss", "title": "Deep Function Machines: Generalized Neural Networks for Topological\n  Layer Expression", "comments": "23 pages, 9 figures, with experiments", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper we propose a generalization of deep neural networks called deep\nfunction machines (DFMs). DFMs act on vector spaces of arbitrary (possibly\ninfinite) dimension and we show that a family of DFMs are invariant to the\ndimension of input data; that is, the parameterization of the model does not\ndirectly hinge on the quality of the input (eg. high resolution images). Using\nthis generalization we provide a new theory of universal approximation of\nbounded non-linear operators between function spaces. We then suggest that DFMs\nprovide an expressive framework for designing new neural network layer types\nwith topological considerations in mind. Finally, we introduce a novel\narchitecture, RippLeNet, for resolution invariant computer vision, which\nempirically achieves state of the art invariance.\n", "versions": [{"version": "v1", "created": "Wed, 14 Dec 2016 20:39:23 GMT"}, {"version": "v2", "created": "Mon, 6 Nov 2017 20:51:33 GMT"}], "update_date": "2017-11-08", "authors_parsed": [["Guss", "William H.", ""]]}, {"id": "1612.04831", "submitter": "Utkarsh Upadhyay", "authors": "Utkarsh Upadhyay, Isabel Valera, Manuel Gomez-Rodriguez", "title": "Uncovering the Dynamics of Crowdlearning and the Value of Knowledge", "comments": "To appear in Tenth ACM International conference on Web Search and\n  Data Mining (WSDM) in 2017", "journal-ref": null, "doi": "10.1145/3018661.3018685", "report-no": null, "categories": "cs.SI cs.LG physics.soc-ph stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Learning from the crowd has become increasingly popular in the Web and social\nmedia. There is a wide variety of crowdlearning sites in which, on the one\nhand, users learn from the knowledge that other users contribute to the site,\nand, on the other hand, knowledge is reviewed and curated by the same users\nusing assessment measures such as upvotes or likes.\n  In this paper, we present a probabilistic modeling framework of\ncrowdlearning, which uncovers the evolution of a user's expertise over time by\nleveraging other users' assessments of her contributions. The model allows for\nboth off-site and on-site learning and captures forgetting of knowledge. We\nthen develop a scalable estimation method to fit the model parameters from\nmillions of recorded learning and contributing events. We show the\neffectiveness of our model by tracing activity of ~25 thousand users in Stack\nOverflow over a 4.5 year period. We find that answers with high knowledge value\nare rare. Newbies and experts tend to acquire less knowledge than users in the\nmiddle range. Prolific learners tend to be also proficient contributors that\npost answers with high knowledge value.\n", "versions": [{"version": "v1", "created": "Wed, 14 Dec 2016 21:00:11 GMT"}], "update_date": "2016-12-16", "authors_parsed": [["Upadhyay", "Utkarsh", ""], ["Valera", "Isabel", ""], ["Gomez-Rodriguez", "Manuel", ""]]}, {"id": "1612.04853", "submitter": "Hoel Le Capitaine", "authors": "Hoel Le Capitaine", "title": "Constraint Selection in Metric Learning", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A number of machine learning algorithms are using a metric, or a distance, in\norder to compare individuals. The Euclidean distance is usually employed, but\nit may be more efficient to learn a parametric distance such as Mahalanobis\nmetric. Learning such a metric is a hot topic since more than ten years now,\nand a number of methods have been proposed to efficiently learn it. However,\nthe nature of the problem makes it quite difficult for large scale data, as\nwell as data for which classes overlap. This paper presents a simple way of\nimproving accuracy and scalability of any iterative metric learning algorithm,\nwhere constraints are obtained prior to the algorithm. The proposed approach\nrelies on a loss-dependent weighted selection of constraints that are used for\nlearning the metric. Using the corresponding dedicated loss function, the\nmethod clearly allows to obtain better results than state-of-the-art methods,\nboth in terms of accuracy and time complexity. Some experimental results on\nreal world, and potentially large, datasets are demonstrating the effectiveness\nof our proposition.\n", "versions": [{"version": "v1", "created": "Wed, 14 Dec 2016 21:45:14 GMT"}], "update_date": "2016-12-16", "authors_parsed": [["Capitaine", "Hoel Le", ""]]}, {"id": "1612.04875", "submitter": "Bhavya Kailkhura", "authors": "Jayaraman J. Thiagarajan, Prasanna Sattigeri, Karthikeyan Natesan\n  Ramamurthy, Bhavya Kailkhura", "title": "Robust Local Scaling using Conditional Quantiles of Graph Similarities", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Spectral analysis of neighborhood graphs is one of the most widely used\ntechniques for exploratory data analysis, with applications ranging from\nmachine learning to social sciences. In such applications, it is typical to\nfirst encode relationships between the data samples using an appropriate\nsimilarity function. Popular neighborhood construction techniques such as\nk-nearest neighbor (k-NN) graphs are known to be very sensitive to the choice\nof parameters, and more importantly susceptible to noise and varying densities.\nIn this paper, we propose the use of quantile analysis to obtain local scale\nestimates for neighborhood graph construction. To this end, we build an\nauto-encoding neural network approach for inferring conditional quantiles of a\nsimilarity function, which are subsequently used to obtain robust estimates of\nthe local scales. In addition to being highly resilient to noise or outlying\ndata, the proposed approach does not require extensive parameter tuning unlike\nseveral existing methods. Using applications in spectral clustering and\nsingle-example label propagation, we show that the proposed neighborhood graphs\noutperform existing locally scaled graph construction approaches.\n", "versions": [{"version": "v1", "created": "Wed, 14 Dec 2016 23:11:49 GMT"}], "update_date": "2016-12-16", "authors_parsed": [["Thiagarajan", "Jayaraman J.", ""], ["Sattigeri", "Prasanna", ""], ["Ramamurthy", "Karthikeyan Natesan", ""], ["Kailkhura", "Bhavya", ""]]}, {"id": "1612.04891", "submitter": "Aaron Lee", "authors": "Cecilia S. Lee, Doug M. Baughman, Aaron Y. Lee", "title": "Deep learning is effective for the classification of OCT images of\n  normal versus Age-related Macular Degeneration", "comments": "4 Figures, 1 Table", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.CV cs.LG", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Objective: The advent of Electronic Medical Records (EMR) with large\nelectronic imaging databases along with advances in deep neural networks with\nmachine learning has provided a unique opportunity to achieve milestones in\nautomated image analysis. Optical coherence tomography (OCT) is the most\ncommonly obtained imaging modality in ophthalmology and represents a dense and\nrich dataset when combined with labels derived from the EMR. We sought to\ndetermine if deep learning could be utilized to distinguish normal OCT images\nfrom images from patients with Age-related Macular Degeneration (AMD). Methods:\nAutomated extraction of an OCT imaging database was performed and linked to\nclinical endpoints from the EMR. OCT macula scans were obtained by Heidelberg\nSpectralis, and each OCT scan was linked to EMR clinical endpoints extracted\nfrom EPIC. The central 11 images were selected from each OCT scan of two\ncohorts of patients: normal and AMD. Cross-validation was performed using a\nrandom subset of patients. Area under receiver operator curves (auROC) were\nconstructed at an independent image level, macular OCT level, and patient\nlevel. Results: Of an extraction of 2.6 million OCT images linked to clinical\ndatapoints from the EMR, 52,690 normal and 48,312 AMD macular OCT images were\nselected. A deep neural network was trained to categorize images as either\nnormal or AMD. At the image level, we achieved an auROC of 92.78% with an\naccuracy of 87.63%. At the macula level, we achieved an auROC of 93.83% with an\naccuracy of 88.98%. At a patient level, we achieved an auROC of 97.45% with an\naccuracy of 93.45%. Peak sensitivity and specificity with optimal cutoffs were\n92.64% and 93.69% respectively. Conclusions: Deep learning techniques are\neffective for classifying OCT images. These findings have important\nimplications in utilizing OCT in automated screening and computer aided\ndiagnosis tools.\n", "versions": [{"version": "v1", "created": "Thu, 15 Dec 2016 00:23:43 GMT"}], "update_date": "2016-12-16", "authors_parsed": [["Lee", "Cecilia S.", ""], ["Baughman", "Doug M.", ""], ["Lee", "Aaron Y.", ""]]}, {"id": "1612.04895", "submitter": "Louis-Francois Arsenault", "authors": "Louis-Francois Arsenault, Richard Neuberg, Lauren A. Hannah, Andrew J.\n  Millis", "title": "Projected Regression Methods for Inverting Fredholm Integrals: Formalism\n  and Application to Analytical Continuation", "comments": "main text 7 pages, supporting info 2 pages, 7 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cond-mat.str-el stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a machine learning approach to the inversion of Fredholm integrals\nof the first kind. The approach provides a natural regularization in cases\nwhere the inverse of the Fredholm kernel is ill-conditioned. It also provides\nan efficient and stable treatment of constraints. The key observation is that\nthe stability of the forward problem permits the construction of a large\ndatabase of outputs for physically meaningful inputs. We apply machine learning\nto this database to generate a regression function of controlled complexity,\nwhich returns approximate solutions for previously unseen inputs; the\napproximate solutions are then projected onto the subspace of functions\nsatisfying relevant constraints. We also derive and present uncertainty\nestimates. We illustrate the approach by applying it to the analytical\ncontinuation problem of quantum many-body physics, which involves\nreconstructing the frequency dependence of physical excitation spectra from\ndata obtained at specific points in the complex frequency plane. Under standard\nerror metrics the method performs as well or better than the Maximum Entropy\nmethod for low input noise and is substantially more robust to increased input\nnoise. We expect the methodology to be similarly effective for any problem\ninvolving a formally ill-conditioned inversion, provided that the forward\nproblem can be efficiently solved.\n", "versions": [{"version": "v1", "created": "Thu, 15 Dec 2016 00:51:08 GMT"}], "update_date": "2016-12-16", "authors_parsed": [["Arsenault", "Louis-Francois", ""], ["Neuberg", "Richard", ""], ["Hannah", "Lauren A.", ""], ["Millis", "Andrew J.", ""]]}, {"id": "1612.04897", "submitter": "Takayuki Osogami Ph.D.", "authors": "Takayuki Osogami", "title": "Learning binary or real-valued time-series via spike-timing dependent\n  plasticity", "comments": "This paper was accepted and presented at Computing with Spikes NIPS\n  2016 Workshop, Barcelona, Spain, December 2016", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NE stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A dynamic Boltzmann machine (DyBM) has been proposed as a model of a spiking\nneural network, and its learning rule of maximizing the log-likelihood of given\ntime-series has been shown to exhibit key properties of spike-timing dependent\nplasticity (STDP), which had been postulated and experimentally confirmed in\nthe field of neuroscience as a learning rule that refines the Hebbian rule.\nHere, we relax some of the constraints in the DyBM in a way that it becomes\nmore suitable for computation and learning. We show that learning the DyBM can\nbe considered as logistic regression for binary-valued time-series. We also\nshow how the DyBM can learn real-valued data in the form of a Gaussian DyBM and\ndiscuss its relation to the vector autoregressive (VAR) model. The Gaussian\nDyBM extends the VAR by using additional explanatory variables, which\ncorrespond to the eligibility traces of the DyBM and capture long term\ndependency of the time-series. Numerical experiments show that the Gaussian\nDyBM significantly improves the predictive accuracy over VAR.\n", "versions": [{"version": "v1", "created": "Thu, 15 Dec 2016 00:57:51 GMT"}], "update_date": "2016-12-16", "authors_parsed": [["Osogami", "Takayuki", ""]]}, {"id": "1612.04898", "submitter": "Sunil Thulasidasan", "authors": "Sunil Thulasidasan, Jeffrey Bilmes, Garrett Kenyon", "title": "Efficient Distributed Semi-Supervised Learning using Stochastic\n  Regularization over Affinity Graphs", "comments": "NIPS 2016 Workshop on Machine Learning Systems", "journal-ref": null, "doi": null, "report-no": "LA-UR-16-28681", "categories": "stat.ML cs.DC cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We describe a computationally efficient, stochastic graph-regularization\ntechnique that can be utilized for the semi-supervised training of deep neural\nnetworks in a parallel or distributed setting. We utilize a technique, first\ndescribed in [13] for the construction of mini-batches for stochastic gradient\ndescent (SGD) based on synthesized partitions of an affinity graph that are\nconsistent with the graph structure, but also preserve enough stochasticity for\nconvergence of SGD to good local minima. We show how our technique allows a\ngraph-based semi-supervised loss function to be decomposed into a sum over\nobjectives, facilitating data parallelism for scalable training of machine\nlearning models. Empirical results indicate that our method significantly\nimproves classification accuracy compared to the fully-supervised case when the\nfraction of labeled data is low, and in the parallel case, achieves significant\nspeed-up in terms of wall-clock time to convergence. We show the results for\nboth sequential and distributed-memory semi-supervised DNN training on a speech\ncorpus.\n", "versions": [{"version": "v1", "created": "Thu, 15 Dec 2016 01:00:23 GMT"}, {"version": "v2", "created": "Wed, 30 May 2018 17:23:25 GMT"}], "update_date": "2018-05-31", "authors_parsed": [["Thulasidasan", "Sunil", ""], ["Bilmes", "Jeffrey", ""], ["Kenyon", "Garrett", ""]]}, {"id": "1612.04899", "submitter": "Sunil Thulasidasan", "authors": "Sunil Thulasidasan, Jeffrey Bilmes", "title": "Semi-Supervised Phone Classification using Deep Neural Networks and\n  Stochastic Graph-Based Entropic Regularization", "comments": "InterSpeech Workshop on Machine Learning in Speech and Language\n  Processing, 2016. Based on and extends work in arXiv:1612.04898", "journal-ref": null, "doi": null, "report-no": "LA-UR-16-24599", "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We describe a graph-based semi-supervised learning framework in the context\nof deep neural networks that uses a graph-based entropic regularizer to favor\nsmooth solutions over a graph induced by the data. The main contribution of\nthis work is a computationally efficient, stochastic graph-regularization\ntechnique that uses mini-batches that are consistent with the graph structure,\nbut also provides enough stochasticity (in terms of mini-batch data diversity)\nfor convergence of stochastic gradient descent methods to good solutions. For\nthis work, we focus on results of frame-level phone classification accuracy on\nthe TIMIT speech corpus but our method is general and scalable to much larger\ndata sets. Results indicate that our method significantly improves\nclassification accuracy compared to the fully-supervised case when the fraction\nof labeled data is low, and it is competitive with other methods in the fully\nlabeled case.\n", "versions": [{"version": "v1", "created": "Thu, 15 Dec 2016 01:00:45 GMT"}, {"version": "v2", "created": "Wed, 30 May 2018 16:04:46 GMT"}], "update_date": "2018-06-05", "authors_parsed": [["Thulasidasan", "Sunil", ""], ["Bilmes", "Jeffrey", ""]]}, {"id": "1612.04933", "submitter": "Benjamin Jantzen", "authors": "Benjamin C. Jantzen", "title": "Dynamical Kinds and their Discovery", "comments": "Accepted for the proceedings of the Causation: Foundation to\n  Application Workshop, UAI 2016", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.AI cs.LG cs.SY", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We demonstrate the possibility of classifying causal systems into kinds that\nshare a common structure without first constructing an explicit dynamical model\nor using prior knowledge of the system dynamics. The algorithmic ability to\ndetermine whether arbitrary systems are governed by causal relations of the\nsame form offers significant practical applications in the development and\nvalidation of dynamical models. It is also of theoretical interest as an\nessential stage in the scientific inference of laws from empirical data. The\nalgorithm presented is based on the dynamical symmetry approach to dynamical\nkinds. A dynamical symmetry with respect to time is an intervention on one or\nmore variables of a system that commutes with the time evolution of the system.\nA dynamical kind is a class of systems sharing a set of dynamical symmetries.\nThe algorithm presented classifies deterministic, time-dependent causal systems\nby directly comparing their exhibited symmetries. Using simulated, noisy data\nfrom a variety of nonlinear systems, we show that this algorithm correctly\nsorts systems into dynamical kinds. It is robust under significant sampling\nerror, is immune to violations of normality in sampling error, and fails\ngracefully with increasing dynamical similarity. The algorithm we demonstrate\nis the first to address this aspect of automated scientific discovery.\n", "versions": [{"version": "v1", "created": "Thu, 15 Dec 2016 05:25:41 GMT"}], "update_date": "2016-12-16", "authors_parsed": [["Jantzen", "Benjamin C.", ""]]}, {"id": "1612.05001", "submitter": "Leto Peel", "authors": "Leto Peel", "title": "Graph-based semi-supervised learning for relational networks", "comments": "11 pages, 8 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.SI cs.LG physics.data-an stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We address the problem of semi-supervised learning in relational networks,\nnetworks in which nodes are entities and links are the relationships or\ninteractions between them. Typically this problem is confounded with the\nproblem of graph-based semi-supervised learning (GSSL), because both problems\nrepresent the data as a graph and predict the missing class labels of nodes.\nHowever, not all graphs are created equally. In GSSL a graph is constructed,\noften from independent data, based on similarity. As such, edges tend to\nconnect instances with the same class label. Relational networks, however, can\nbe more heterogeneous and edges do not always indicate similarity. For\ninstance, instead of links being more likely to connect nodes with the same\nclass label, they may occur more frequently between nodes with different class\nlabels (link-heterogeneity). Or nodes with the same class label do not\nnecessarily have the same type of connectivity across the whole network\n(class-heterogeneity), e.g. in a network of sexual interactions we may observe\nlinks between opposite genders in some parts of the graph and links between the\nsame genders in others. Performing classification in networks with different\ntypes of heterogeneity is a hard problem that is made harder still when we do\nnot know a-priori the type or level of heterogeneity. Here we present two\nscalable approaches for graph-based semi-supervised learning for the more\ngeneral case of relational networks. We demonstrate these approaches on\nsynthetic and real-world networks that display different link patterns within\nand between classes. Compared to state-of-the-art approaches, ours give better\nclassification performance without prior knowledge of how classes interact. In\nparticular, our two-step label propagation algorithm gives consistently good\naccuracy and runs on networks of over 1.6 million nodes and 30 million edges in\naround 12 seconds.\n", "versions": [{"version": "v1", "created": "Thu, 15 Dec 2016 10:15:57 GMT"}], "update_date": "2016-12-16", "authors_parsed": [["Peel", "Leto", ""]]}, {"id": "1612.05024", "submitter": "Andrey Y. Lokhov", "authors": "Andrey Y. Lokhov, Marc Vuffray, Sidhant Misra, Michael Chertkov", "title": "Optimal structure and parameter learning of Ising models", "comments": "11 pages, 10 pages of supplementary materials", "journal-ref": null, "doi": null, "report-no": null, "categories": "cond-mat.stat-mech cs.LG physics.data-an stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Reconstruction of structure and parameters of an Ising model from binary\nsamples is a problem of practical importance in a variety of disciplines,\nranging from statistical physics and computational biology to image processing\nand machine learning. The focus of the research community shifted towards\ndeveloping universal reconstruction algorithms which are both computationally\nefficient and require the minimal amount of expensive data. We introduce a new\nmethod, Interaction Screening, which accurately estimates the model parameters\nusing local optimization problems. The algorithm provably achieves perfect\ngraph structure recovery with an information-theoretically optimal number of\nsamples, notably in the low-temperature regime which is known to be the hardest\nfor learning. The efficacy of Interaction Screening is assessed through\nextensive numerical tests on synthetic Ising models of various topologies with\ndifferent types of interactions, as well as on a real data produced by a D-Wave\nquantum computer. This study shows that the Interaction Screening method is an\nexact, tractable and optimal technique universally solving the inverse Ising\nproblem.\n", "versions": [{"version": "v1", "created": "Thu, 15 Dec 2016 11:30:40 GMT"}, {"version": "v2", "created": "Tue, 26 Dec 2017 11:02:50 GMT"}], "update_date": "2017-12-27", "authors_parsed": [["Lokhov", "Andrey Y.", ""], ["Vuffray", "Marc", ""], ["Misra", "Sidhant", ""], ["Chertkov", "Michael", ""]]}, {"id": "1612.05048", "submitter": "Theofanis Karaletsos", "authors": "Theofanis Karaletsos", "title": "Adversarial Message Passing For Graphical Models", "comments": "(12 pages, 2 figures) Presented at NIPS Advances In Approximate\n  Inference 2016 (AABI 2016)", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Bayesian inference on structured models typically relies on the ability to\ninfer posterior distributions of underlying hidden variables. However,\ninference in implicit models or complex posterior distributions is hard. A\npopular tool for learning implicit models are generative adversarial networks\n(GANs) which learn parameters of generators by fooling discriminators.\nTypically, GANs are considered to be models themselves and are not understood\nin the context of inference. Current techniques rely on inefficient global\ndiscrimination of joint distributions to perform learning, or only consider\ndiscriminating a single output variable. We overcome these limitations by\ntreating GANs as a basis for likelihood-free inference in generative models and\ngeneralize them to Bayesian posterior inference over factor graphs. We propose\nlocal learning rules based on message passing minimizing a global divergence\ncriterion involving cooperating local adversaries used to sidestep explicit\nlikelihood evaluations. This allows us to compose models and yields a unified\ninference and learning framework for adversarial learning. Our framework treats\nmodel specification and inference separately and facilitates richly structured\nmodels within the family of Directed Acyclic Graphs, including components such\nas intractable likelihoods, non-differentiable models, simulators and generally\ncumbersome models. A key result of our treatment is the insight that Bayesian\ninference on structured models can be performed only with sampling and\ndiscrimination when using nonparametric variational families, without access to\nexplicit distributions. As a side-result, we discuss the link to likelihood\nmaximization. These approaches hold promise to be useful in the toolbox of\nprobabilistic modelers and enrich the gamut of current probabilistic\nprogramming applications.\n", "versions": [{"version": "v1", "created": "Thu, 15 Dec 2016 12:59:33 GMT"}], "update_date": "2016-12-16", "authors_parsed": [["Karaletsos", "Theofanis", ""]]}, {"id": "1612.05053", "submitter": "Guillaume Dehaene", "authors": "Guillaume P. Dehaene", "title": "Expectation Propagation performs a smoothed gradient descent", "comments": "This article was submitted and accepted to the Advances in\n  Approximate Bayesian Inference NIPS 2016 workshop\n  (www.approximateinference.org)", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Bayesian inference is a popular method to build learning algorithms but it is\nhampered by the fact that its key object, the posterior probability\ndistribution, is often uncomputable. Expectation Propagation (EP) (Minka\n(2001)) is a popular algorithm that solves this issue by computing a parametric\napproximation (e.g: Gaussian) to the density of the posterior. However, while\nit is known empirically to quickly compute fine approximations, EP is extremely\npoorly understood which prevents it from being adopted by a larger fraction of\nthe community.\n  The object of the present article is to shed intuitive light on EP, by\nrelating it to other better understood methods. More precisely, we link it to\nusing gradient descent to compute the Laplace approximation of a target\nprobability distribution. We show that EP is exactly equivalent to performing\ngradient descent on a smoothed energy landscape: i.e: the original energy\nlandscape convoluted with some smoothing kernel. This also relates EP to\nalgorithms that compute the Gaussian approximation which minimizes the reverse\nKL divergence to the target distribution, a link that has been conjectured\nbefore but has not been proved rigorously yet. These results can help\npractitioners to get a better feel for how EP works, as well as lead to other\nnew results on this important method.\n", "versions": [{"version": "v1", "created": "Thu, 15 Dec 2016 13:16:46 GMT"}], "update_date": "2016-12-16", "authors_parsed": [["Dehaene", "Guillaume P.", ""]]}, {"id": "1612.05086", "submitter": "Lukas Balles", "authors": "Lukas Balles and Javier Romero and Philipp Hennig", "title": "Coupling Adaptive Batch Sizes with Learning Rates", "comments": "Thirty-Third Conference on Uncertainty in Artificial Intelligence\n  (UAI), 2017, (accepted)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Mini-batch stochastic gradient descent and variants thereof have become\nstandard for large-scale empirical risk minimization like the training of\nneural networks. These methods are usually used with a constant batch size\nchosen by simple empirical inspection. The batch size significantly influences\nthe behavior of the stochastic optimization algorithm, though, since it\ndetermines the variance of the gradient estimates. This variance also changes\nover the optimization process; when using a constant batch size, stability and\nconvergence is thus often enforced by means of a (manually tuned) decreasing\nlearning rate schedule.\n  We propose a practical method for dynamic batch size adaptation. It estimates\nthe variance of the stochastic gradients and adapts the batch size to decrease\nthe variance proportionally to the value of the objective function, removing\nthe need for the aforementioned learning rate decrease. In contrast to recent\nrelated work, our algorithm couples the batch size to the learning rate,\ndirectly reflecting the known relationship between the two. On popular image\nclassification benchmarks, our batch size adaptation yields faster optimization\nconvergence, while simultaneously simplifying learning rate tuning. A\nTensorFlow implementation is available.\n", "versions": [{"version": "v1", "created": "Thu, 15 Dec 2016 14:42:45 GMT"}, {"version": "v2", "created": "Wed, 28 Jun 2017 12:07:02 GMT"}], "update_date": "2017-06-29", "authors_parsed": [["Balles", "Lukas", ""], ["Romero", "Javier", ""], ["Hennig", "Philipp", ""]]}, {"id": "1612.05231", "submitter": "Li Jing", "authors": "Li Jing, Yichen Shen, Tena Dub\\v{c}ek, John Peurifoy, Scott Skirlo,\n  Yann LeCun, Max Tegmark, Marin Solja\\v{c}i\\'c", "title": "Tunable Efficient Unitary Neural Networks (EUNN) and their application\n  to RNNs", "comments": "9 pages, 4 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.NE stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Using unitary (instead of general) matrices in artificial neural networks\n(ANNs) is a promising way to solve the gradient explosion/vanishing problem, as\nwell as to enable ANNs to learn long-term correlations in the data. This\napproach appears particularly promising for Recurrent Neural Networks (RNNs).\nIn this work, we present a new architecture for implementing an Efficient\nUnitary Neural Network (EUNNs); its main advantages can be summarized as\nfollows. Firstly, the representation capacity of the unitary space in an EUNN\nis fully tunable, ranging from a subspace of SU(N) to the entire unitary space.\nSecondly, the computational complexity for training an EUNN is merely\n$\\mathcal{O}(1)$ per parameter. Finally, we test the performance of EUNNs on\nthe standard copying task, the pixel-permuted MNIST digit recognition benchmark\nas well as the Speech Prediction Test (TIMIT). We find that our architecture\nsignificantly outperforms both other state-of-the-art unitary RNNs and the LSTM\narchitecture, in terms of the final performance and/or the wall-clock training\nspeed. EUNNs are thus promising alternatives to RNNs and LSTMs for a wide\nvariety of applications.\n", "versions": [{"version": "v1", "created": "Thu, 15 Dec 2016 20:39:15 GMT"}, {"version": "v2", "created": "Sun, 26 Feb 2017 19:00:50 GMT"}, {"version": "v3", "created": "Mon, 3 Apr 2017 17:13:38 GMT"}], "update_date": "2017-04-04", "authors_parsed": [["Jing", "Li", ""], ["Shen", "Yichen", ""], ["Dub\u010dek", "Tena", ""], ["Peurifoy", "John", ""], ["Skirlo", "Scott", ""], ["LeCun", "Yann", ""], ["Tegmark", "Max", ""], ["Solja\u010di\u0107", "Marin", ""]]}, {"id": "1612.05251", "submitter": "Franck Dernoncourt", "authors": "Franck Dernoncourt, Ji Young Lee, Peter Szolovits", "title": "Neural Networks for Joint Sentence Classification in Medical Paper\n  Abstracts", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI cs.NE stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Existing models based on artificial neural networks (ANNs) for sentence\nclassification often do not incorporate the context in which sentences appear,\nand classify sentences individually. However, traditional sentence\nclassification approaches have been shown to greatly benefit from jointly\nclassifying subsequent sentences, such as with conditional random fields. In\nthis work, we present an ANN architecture that combines the effectiveness of\ntypical ANN models to classify sentences in isolation, with the strength of\nstructured prediction. Our model achieves state-of-the-art results on two\ndifferent datasets for sequential sentence classification in medical abstracts.\n", "versions": [{"version": "v1", "created": "Thu, 15 Dec 2016 20:57:56 GMT"}], "update_date": "2016-12-16", "authors_parsed": [["Dernoncourt", "Franck", ""], ["Lee", "Ji Young", ""], ["Szolovits", "Peter", ""]]}, {"id": "1612.05270", "submitter": "Daniela Moctezuma", "authors": "Eric S. Tellez, Sabino Miranda Jim\\'enez, Mario Graff, Daniela\n  Moctezuma, Ranyart R. Su\\'arez, Oscar S. Siordia", "title": "A Simple Approach to Multilingual Polarity Classification in Twitter", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recently, sentiment analysis has received a lot of attention due to the\ninterest in mining opinions of social media users. Sentiment analysis consists\nin determining the polarity of a given text, i.e., its degree of positiveness\nor negativeness. Traditionally, Sentiment Analysis algorithms have been\ntailored to a specific language given the complexity of having a number of\nlexical variations and errors introduced by the people generating content. In\nthis contribution, our aim is to provide a simple to implement and easy to use\nmultilingual framework, that can serve as a baseline for sentiment analysis\ncontests, and as starting point to build new sentiment analysis systems. We\ncompare our approach in eight different languages, three of them have important\ninternational contests, namely, SemEval (English), TASS (Spanish), and\nSENTIPOLC (Italian). Within the competitions our approach reaches from medium\nto high positions in the rankings; whereas in the remaining languages our\napproach outperforms the reported results.\n", "versions": [{"version": "v1", "created": "Thu, 15 Dec 2016 21:07:12 GMT"}], "update_date": "2016-12-19", "authors_parsed": [["Tellez", "Eric S.", ""], ["Jim\u00e9nez", "Sabino Miranda", ""], ["Graff", "Mario", ""], ["Moctezuma", "Daniela", ""], ["Su\u00e1rez", "Ranyart R.", ""], ["Siordia", "Oscar S.", ""]]}, {"id": "1612.05276", "submitter": "Markus Abel", "authors": "Julien Gout and Markus Quade and Kamran Shafi and Robert K. Niven and\n  Markus Abel", "title": "Learning Optimal Control of Synchronization in Networks of Coupled\n  Oscillators using Genetic Programming-based Symbolic Regression", "comments": "Submitted to nonlinear dynamics", "journal-ref": "Nonlinear Dynamics 91 (2), 1001-1021, 2018", "doi": "10.1007/s11071-017-3925-z", "report-no": null, "categories": "nlin.AO cs.SY stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Networks of coupled dynamical systems provide a powerful way to model systems\nwith enormously complex dynamics, such as the human brain. Control of\nsynchronization in such networked systems has far reaching applications in many\ndomains, including engineering and medicine. In this paper, we formulate the\nsynchronization control in dynamical systems as an optimization problem and\npresent a multi-objective genetic programming-based approach to infer optimal\ncontrol functions that drive the system from a synchronized to a\nnon-synchronized state and vice-versa. The genetic programming-based controller\nallows learning optimal control functions in an interpretable symbolic form.\nThe effectiveness of the proposed approach is demonstrated in controlling\nsynchronization in coupled oscillator systems linked in networks of increasing\norder complexity, ranging from a simple coupled oscillator system to a\nhierarchical network of coupled oscillators. The results show that the proposed\nmethod can learn highly-effective and interpretable control functions for such\nsystems.\n", "versions": [{"version": "v1", "created": "Thu, 15 Dec 2016 21:20:27 GMT"}, {"version": "v2", "created": "Tue, 20 Mar 2018 14:28:22 GMT"}], "update_date": "2018-03-21", "authors_parsed": [["Gout", "Julien", ""], ["Quade", "Markus", ""], ["Shafi", "Kamran", ""], ["Niven", "Robert K.", ""], ["Abel", "Markus", ""]]}, {"id": "1612.05356", "submitter": "Jie Liu", "authors": "Jie Liu, Martin Takac", "title": "Projected Semi-Stochastic Gradient Descent Method with Mini-Batch Scheme\n  under Weak Strong Convexity Assumption", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG math.OC stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a projected semi-stochastic gradient descent method with\nmini-batch for improving both the theoretical complexity and practical\nperformance of the general stochastic gradient descent method (SGD). We are\nable to prove linear convergence under weak strong convexity assumption. This\nrequires no strong convexity assumption for minimizing the sum of smooth convex\nfunctions subject to a compact polyhedral set, which remains popular across\nmachine learning community. Our PS2GD preserves the low-cost per iteration and\nhigh optimization accuracy via stochastic gradient variance-reduced technique,\nand admits a simple parallel implementation with mini-batches. Moreover, PS2GD\nis also applicable to dual problem of SVM with hinge loss.\n", "versions": [{"version": "v1", "created": "Fri, 16 Dec 2016 03:54:30 GMT"}, {"version": "v2", "created": "Tue, 7 Mar 2017 02:40:16 GMT"}, {"version": "v3", "created": "Fri, 5 May 2017 00:44:45 GMT"}], "update_date": "2017-05-08", "authors_parsed": [["Liu", "Jie", ""], ["Takac", "Martin", ""]]}, {"id": "1612.05519", "submitter": "Diana Cai", "authors": "Diana Cai, Trevor Campbell, Tamara Broderick", "title": "Edge-exchangeable graphs and sparsity (NIPS 2016)", "comments": "In the proceedings of the Advances in Neural Information Processing\n  Systems 29 (NIPS), 2016. Preliminary work appeared in the 2015 NIPS workshops\n  on Networks in the Social and Information Sciences\n  (http://stanford.edu/~jugander/NetworksNIPS2015/) and Bayesian\n  Nonparametrics: The Next Generation\n  (https://sites.google.com/site/nipsbnp2015/). 26 pages, 4 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Many popular network models rely on the assumption of (vertex)\nexchangeability, in which the distribution of the graph is invariant to\nrelabelings of the vertices. However, the Aldous-Hoover theorem guarantees that\nthese graphs are dense or empty with probability one, whereas many real-world\ngraphs are sparse. We present an alternative notion of exchangeability for\nrandom graphs, which we call edge exchangeability, in which the distribution of\na graph sequence is invariant to the order of the edges. We demonstrate that\nedge-exchangeable models, unlike models that are traditionally vertex\nexchangeable, can exhibit sparsity. To do so, we outline a general framework\nfor graph generative models; by contrast to the pioneering work of Caron and\nFox (2015), models within our framework are stationary across steps of the\ngraph sequence. In particular, our model grows the graph by instantiating more\nlatent atoms of a single random measure as the dataset size increases, rather\nthan adding new atoms to the measure.\n", "versions": [{"version": "v1", "created": "Fri, 16 Dec 2016 15:57:28 GMT"}, {"version": "v2", "created": "Fri, 3 Feb 2017 21:31:17 GMT"}], "update_date": "2017-02-07", "authors_parsed": [["Cai", "Diana", ""], ["Campbell", "Trevor", ""], ["Broderick", "Tamara", ""]]}, {"id": "1612.05535", "submitter": "Lucas Lamata", "authors": "Unai Alvarez-Rodriguez, Lucas Lamata, Pablo Escandell-Montero, Jos\\'e\n  D. Mart\\'in-Guerrero, Enrique Solano", "title": "Supervised Quantum Learning without Measurements", "comments": null, "journal-ref": "Scientific Reports 7, 13645 (2017)", "doi": "10.1038/s41598-017-13378-0", "report-no": null, "categories": "quant-ph cond-mat.mes-hall cond-mat.supr-con cs.AI stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a quantum machine learning algorithm for efficiently solving a\nclass of problems encoded in quantum controlled unitary operations. The central\nphysical mechanism of the protocol is the iteration of a quantum time-delayed\nequation that introduces feedback in the dynamics and eliminates the necessity\nof intermediate measurements. The performance of the quantum algorithm is\nanalyzed by comparing the results obtained in numerical simulations with the\noutcome of classical machine learning methods for the same problem. The use of\ntime-delayed equations enhances the toolbox of the field of quantum machine\nlearning, which may enable unprecedented applications in quantum technologies.\n", "versions": [{"version": "v1", "created": "Fri, 16 Dec 2016 16:15:45 GMT"}, {"version": "v2", "created": "Mon, 23 Oct 2017 07:44:58 GMT"}], "update_date": "2017-10-24", "authors_parsed": [["Alvarez-Rodriguez", "Unai", ""], ["Lamata", "Lucas", ""], ["Escandell-Montero", "Pablo", ""], ["Mart\u00edn-Guerrero", "Jos\u00e9 D.", ""], ["Solano", "Enrique", ""]]}, {"id": "1612.05612", "submitter": "Feng Ruan", "authors": "John Duchi and Feng Ruan", "title": "Asymptotic Optimality in Stochastic Optimization", "comments": null, "journal-ref": "Annals of Statistics 2019", "doi": null, "report-no": null, "categories": "math.ST math.OC stat.ML stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study local complexity measures for stochastic convex optimization\nproblems, providing a local minimax theory analogous to that of H\\'{a}jek and\nLe Cam for classical statistical problems. We give complementary optimality\nresults, developing fully online methods that adaptively achieve optimal\nconvergence guarantees. Our results provide function-specific lower bounds and\nconvergence results that make precise a correspondence between statistical\ndifficulty and the geometric notion of tilt-stability from optimization. As\npart of this development, we show how variants of Nesterov's dual averaging---a\nstochastic gradient-based procedure---guarantee finite time identification of\nconstraints in optimization problems, while stochastic gradient procedures\nfail. Additionally, we highlight a gap between problems with linear and\nnonlinear constraints: standard stochastic-gradient-based procedures are\nsuboptimal even for the simplest nonlinear constraints, necessitating the\ndevelopment of asymptotically optimal Riemannian stochastic gradient methods.\n", "versions": [{"version": "v1", "created": "Fri, 16 Dec 2016 19:54:22 GMT"}, {"version": "v2", "created": "Wed, 2 Aug 2017 09:08:42 GMT"}, {"version": "v3", "created": "Thu, 3 Aug 2017 02:32:45 GMT"}, {"version": "v4", "created": "Fri, 2 Nov 2018 09:10:18 GMT"}], "update_date": "2019-06-05", "authors_parsed": [["Duchi", "John", ""], ["Ruan", "Feng", ""]]}, {"id": "1612.05614", "submitter": "Jason Xu", "authors": "Jason Xu, Eric C. Chi, Meng Yang, Kenneth Lange", "title": "An MM Algorithm for Split Feasibility Problems", "comments": "31 pages, 5 figures, 1 table", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.OC math.NA stat.CO stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The classical multi-set split feasibility problem seeks a point in the\nintersection of finitely many closed convex domain constraints, whose image\nunder a linear mapping also lies in the intersection of finitely many closed\nconvex range constraints. Split feasibility generalizes important inverse\nproblems including convex feasibility, linear complementarity, and regression\nwith constraint sets. When a feasible point does not exist, solution methods\nthat proceed by minimizing a proximity function can be used to obtain optimal\napproximate solutions to the problem. We present an extension of the proximity\nfunction approach that generalizes the linear split feasibility problem to\nallow for non-linear mappings. Our algorithm is based on the principle of\nmajorization-minimization, is amenable to quasi-Newton acceleration, and comes\ncomplete with convergence guarantees under mild assumptions. Furthermore, we\nshow that the Euclidean norm appearing in the proximity function of the\nnon-linear split feasibility problem can be replaced by arbitrary Bregman\ndivergences. We explore several examples illustrating the merits of non-linear\nformulations over the linear case, with a focus on optimization for\nintensity-modulated radiation therapy.\n", "versions": [{"version": "v1", "created": "Fri, 16 Dec 2016 20:15:09 GMT"}, {"version": "v2", "created": "Wed, 18 Jan 2017 00:26:34 GMT"}], "update_date": "2017-01-19", "authors_parsed": [["Xu", "Jason", ""], ["Chi", "Eric C.", ""], ["Yang", "Meng", ""], ["Lange", "Kenneth", ""]]}, {"id": "1612.05628", "submitter": "Kavosh Asadi", "authors": "Kavosh Asadi, Michael L. Littman", "title": "An Alternative Softmax Operator for Reinforcement Learning", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A softmax operator applied to a set of values acts somewhat like the\nmaximization function and somewhat like an average. In sequential decision\nmaking, softmax is often used in settings where it is necessary to maximize\nutility but also to hedge against problems that arise from putting all of one's\nweight behind a single maximum utility decision. The Boltzmann softmax operator\nis the most commonly used softmax operator in this setting, but we show that\nthis operator is prone to misbehavior. In this work, we study a differentiable\nsoftmax operator that, among other properties, is a non-expansion ensuring a\nconvergent behavior in learning and planning. We introduce a variant of SARSA\nalgorithm that, by utilizing the new operator, computes a Boltzmann policy with\na state-dependent temperature parameter. We show that the algorithm is\nconvergent and that it performs favorably in practice.\n", "versions": [{"version": "v1", "created": "Fri, 16 Dec 2016 20:49:35 GMT"}, {"version": "v2", "created": "Mon, 19 Dec 2016 19:02:06 GMT"}, {"version": "v3", "created": "Wed, 21 Dec 2016 02:05:31 GMT"}, {"version": "v4", "created": "Tue, 13 Jun 2017 05:28:04 GMT"}, {"version": "v5", "created": "Wed, 14 Jun 2017 14:29:04 GMT"}], "update_date": "2017-06-15", "authors_parsed": [["Asadi", "Kavosh", ""], ["Littman", "Michael L.", ""]]}, {"id": "1612.05678", "submitter": "Steven Hill", "authors": "Steven M. Hill and Chris. J. Oates and Duncan A. Blythe and Sach\n  Mukherjee", "title": "Causal Learning via Manifold Regularization", "comments": null, "journal-ref": "Journal of Machine Learning Research 20(127):1-32, 2019", "doi": null, "report-no": null, "categories": "stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper frames causal structure estimation as a machine learning task. The\nidea is to treat indicators of causal relationships between variables as\n`labels' and to exploit available data on the variables of interest to provide\nfeatures for the labelling task. Background scientific knowledge or any\navailable interventional data provide labels on some causal relationships and\nthe remainder are treated as unlabelled. To illustrate the key ideas, we\ndevelop a distance-based approach (based on bivariate histograms) within a\nmanifold regularization framework. We present empirical results on three\ndifferent biological data sets (including examples where causal effects can be\nverified by experimental intervention), that together demonstrate the efficacy\nand general nature of the approach as well as its simplicity from a user's\npoint of view.\n", "versions": [{"version": "v1", "created": "Fri, 16 Dec 2016 23:05:31 GMT"}, {"version": "v2", "created": "Wed, 1 Aug 2018 09:27:23 GMT"}, {"version": "v3", "created": "Thu, 25 Apr 2019 20:51:31 GMT"}, {"version": "v4", "created": "Thu, 29 Aug 2019 09:18:41 GMT"}], "update_date": "2019-10-17", "authors_parsed": [["Hill", "Steven M.", ""], ["Oates", "Chris. J.", ""], ["Blythe", "Duncan A.", ""], ["Mukherjee", "Sach", ""]]}, {"id": "1612.05708", "submitter": "Jacob Hunter", "authors": "Jacob S. Hunter (1) and Nathan O. Hodas (1) ((1) Pacific Northwest\n  National Laboratory)", "title": "Mutual information for fitting deep nonlinear models", "comments": null, "journal-ref": null, "doi": null, "report-no": "PNNL-SA-121434", "categories": "math.OC cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Deep nonlinear models pose a challenge for fitting parameters due to lack of\nknowledge of the hidden layer and the potentially non-affine relation of the\ninitial and observed layers. In the present work we investigate the use of\ninformation theoretic measures such as mutual information and Kullback-Leibler\n(KL) divergence as objective functions for fitting such models without\nknowledge of the hidden layer. We investigate one model as a proof of concept\nand one application of cogntive performance. We further investigate the use of\noptimizers with these methods. Mutual information is largely successful as an\nobjective, depending on the parameters. KL divergence is found to be similarly\nsuccesful, given some knowledge of the statistics of the hidden layer.\n", "versions": [{"version": "v1", "created": "Sat, 17 Dec 2016 05:26:46 GMT"}], "update_date": "2016-12-20", "authors_parsed": [["Hunter", "Jacob S.", ""], ["Hodas", "Nathan O.", ""]]}, {"id": "1612.05730", "submitter": "Snehasis Banerjee", "authors": "Snehasis Banerjee, Tanushyam Chattopadhyay, Swagata Biswas, Rohan\n  Banerjee, Anirban Dutta Choudhury, Arpan Pal and Utpal Garain", "title": "Towards Wide Learning: Experiments in Healthcare", "comments": "4 pages, Machine Learning for Health Workshop, NIPS 2016", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, a Wide Learning architecture is proposed that attempts to\nautomate the feature engineering portion of the machine learning (ML) pipeline.\nFeature engineering is widely considered as the most time consuming and expert\nknowledge demanding portion of any ML task. The proposed feature recommendation\napproach is tested on 3 healthcare datasets: a) PhysioNet Challenge 2016\ndataset of phonocardiogram (PCG) signals, b) MIMIC II blood pressure\nclassification dataset of photoplethysmogram (PPG) signals and c) an emotion\nclassification dataset of PPG signals. While the proposed method beats the\nstate of the art techniques for 2nd and 3rd dataset, it reaches 94.38% of the\naccuracy level of the winner of PhysioNet Challenge 2016. In all cases, the\neffort to reach a satisfactory performance was drastically less (a few days)\nthan manual feature engineering.\n", "versions": [{"version": "v1", "created": "Sat, 17 Dec 2016 11:00:49 GMT"}, {"version": "v2", "created": "Wed, 21 Dec 2016 13:53:15 GMT"}], "update_date": "2016-12-22", "authors_parsed": [["Banerjee", "Snehasis", ""], ["Chattopadhyay", "Tanushyam", ""], ["Biswas", "Swagata", ""], ["Banerjee", "Rohan", ""], ["Choudhury", "Anirban Dutta", ""], ["Pal", "Arpan", ""], ["Garain", "Utpal", ""]]}, {"id": "1612.05740", "submitter": "Bohdan Pavlyshenko", "authors": "B. Pavlyshenko", "title": "Machine Learning, Linear and Bayesian Models for Logistic Regression in\n  Failure Detection Problems", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this work, we study the use of logistic regression in manufacturing\nfailures detection. As a data set for the analysis, we used the data from\nKaggle competition Bosch Production Line Performance. We considered the use of\nmachine learning, linear and Bayesian models. For machine learning approach, we\nanalyzed XGBoost tree based classifier to obtain high scored classification.\nUsing the generalized linear model for logistic regression makes it possible to\nanalyze the influence of the factors under study. The Bayesian approach for\nlogistic regression gives the statistical distribution for the parameters of\nthe model. It can be useful in the probabilistic analysis, e.g. risk\nassessment.\n", "versions": [{"version": "v1", "created": "Sat, 17 Dec 2016 11:57:45 GMT"}], "update_date": "2016-12-31", "authors_parsed": [["Pavlyshenko", "B.", ""]]}, {"id": "1612.05846", "submitter": "Evan Schwab", "authors": "Evan Schwab, Ren\\'e Vidal, Nicolas Charon", "title": "Joint Spatial-Angular Sparse Coding for dMRI with Separable Dictionaries", "comments": null, "journal-ref": "Evan Schwab, Rene Vidal, Nicolas Charon, Joint spatial-angular\n  sparse coding for dMRI with separable dictionaries, Medical Image Analysis,\n  Volume 48, August 2018, Pages 25-42, ISSN 1361-8415", "doi": "10.1016/j.media.2018.05.002.", "report-no": null, "categories": "stat.ML cs.CV q-bio.QM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Diffusion MRI (dMRI) provides the ability to reconstruct neuronal fibers in\nthe brain, $\\textit{in vivo}$, by measuring water diffusion along angular\ngradient directions in q-space. High angular resolution diffusion imaging\n(HARDI) can produce better estimates of fiber orientation than the popularly\nused diffusion tensor imaging, but the high number of samples needed to\nestimate diffusivity requires longer patient scan times. To accelerate dMRI,\ncompressed sensing (CS) has been utilized by exploiting a sparse dictionary\nrepresentation of the data, discovered through sparse coding. The sparser the\nrepresentation, the fewer samples are needed to reconstruct a high resolution\nsignal with limited information loss, and so an important area of research has\nfocused on finding the sparsest possible representation of dMRI. Current\nreconstruction methods however, rely on an angular representation $\\textit{per\nvoxel}$ with added spatial regularization, and so, for non-zero signals, one is\nrequired to have at least one non-zero coefficient per voxel. This means that\nthe global level of sparsity must be greater than the number of voxels. In\ncontrast, we propose a joint spatial-angular representation of dMRI that will\nallow us to achieve levels of global sparsity that are below the number of\nvoxels. A major challenge, however, is the computational complexity of solving\na global sparse coding problem over large-scale dMRI. In this work, we present\nnovel adaptations of popular sparse coding algorithms that become better suited\nfor solving large-scale problems by exploiting spatial-angular separability.\nOur experiments show that our method achieves significantly sparser\nrepresentations of HARDI than is possible by the state of the art.\n", "versions": [{"version": "v1", "created": "Sun, 18 Dec 2016 02:08:42 GMT"}, {"version": "v2", "created": "Tue, 25 Apr 2017 16:33:07 GMT"}, {"version": "v3", "created": "Tue, 29 May 2018 15:05:26 GMT"}], "update_date": "2018-05-30", "authors_parsed": [["Schwab", "Evan", ""], ["Vidal", "Ren\u00e9", ""], ["Charon", "Nicolas", ""]]}, {"id": "1612.05888", "submitter": "Jiuyong Li", "authors": "Jiuyong Li, Lin Liu, Jixue Liu and Ryan Green", "title": "Building Diversified Multiple Trees for Classification in High\n  Dimensional Noisy Biomedical Data", "comments": "12 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  It is common that a trained classification model is applied to the operating\ndata that is deviated from the training data because of noise. This paper\ndemonstrates that an ensemble classifier, Diversified Multiple Tree (DMT), is\nmore robust in classifying noisy data than other widely used ensemble methods.\nDMT is tested on three real world biomedical data sets from different\nlaboratories in comparison with four benchmark ensemble classifiers.\nExperimental results show that DMT is significantly more accurate than other\nbenchmark ensemble classifiers on noisy test data. We also discuss a limitation\nof DMT and its possible variations.\n", "versions": [{"version": "v1", "created": "Sun, 18 Dec 2016 10:21:20 GMT"}, {"version": "v2", "created": "Mon, 26 Jun 2017 10:48:23 GMT"}], "update_date": "2017-06-27", "authors_parsed": [["Li", "Jiuyong", ""], ["Liu", "Lin", ""], ["Liu", "Jixue", ""], ["Green", "Ryan", ""]]}, {"id": "1612.05907", "submitter": "Guang Cheng", "authors": "Ganggang Xu, Zuofeng Shang, Guang Cheng", "title": "Distributed Generalized Cross-Validation for Divide-and-Conquer Kernel\n  Ridge Regression and its Asymptotic Optimality", "comments": "To appear in Journal of Computational and Graphical Statistics as an\n  extended version of http://proceedings.mlr.press/v80/xu18f.html", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Tuning parameter selection is of critical importance for kernel ridge\nregression. To this date, data driven tuning method for divide-and-conquer\nkernel ridge regression (d-KRR) has been lacking in the literature, which\nlimits the applicability of d-KRR for large data sets. In this paper, by\nmodifying the Generalized Cross-validation (GCV, Wahba, 1990) score, we propose\na distributed Generalized Cross-Validation (dGCV) as a data-driven tool for\nselecting the tuning parameters in d-KRR. Not only the proposed dGCV is\ncomputationally scalable for massive data sets, it is also shown, under mild\nconditions, to be asymptotically optimal in the sense that minimizing the dGCV\nscore is equivalent to minimizing the true global conditional empirical loss of\nthe averaged function estimator, extending the existing optimality results of\nGCV to the divide-and-conquer framework.\n", "versions": [{"version": "v1", "created": "Sun, 18 Dec 2016 12:41:08 GMT"}, {"version": "v2", "created": "Mon, 18 Feb 2019 23:07:49 GMT"}], "update_date": "2019-02-20", "authors_parsed": [["Xu", "Ganggang", ""], ["Shang", "Zuofeng", ""], ["Cheng", "Guang", ""]]}, {"id": "1612.06000", "submitter": "Kavosh Asadi", "authors": "Kavosh Asadi, Jason D. Williams", "title": "Sample-efficient Deep Reinforcement Learning for Dialog Control", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Representing a dialog policy as a recurrent neural network (RNN) is\nattractive because it handles partial observability, infers a latent\nrepresentation of state, and can be optimized with supervised learning (SL) or\nreinforcement learning (RL). For RL, a policy gradient approach is natural, but\nis sample inefficient. In this paper, we present 3 methods for reducing the\nnumber of dialogs required to optimize an RNN-based dialog policy with RL. The\nkey idea is to maintain a second RNN which predicts the value of the current\npolicy, and to apply experience replay to both networks. On two tasks, these\nmethods reduce the number of dialogs/episodes required by about a third, vs.\nstandard policy gradient methods.\n", "versions": [{"version": "v1", "created": "Sun, 18 Dec 2016 21:51:10 GMT"}], "update_date": "2016-12-20", "authors_parsed": [["Asadi", "Kavosh", ""], ["Williams", "Jason D.", ""]]}, {"id": "1612.06003", "submitter": "Bin Gu", "authors": "Bin Gu and De Wang and Zhouyuan Huo and Heng Huang", "title": "Inexact Proximal Gradient Methods for Non-convex and Non-smooth\n  Optimization", "comments": "AAAI 2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In machine learning research, the proximal gradient methods are popular for\nsolving various optimization problems with non-smooth regularization. Inexact\nproximal gradient methods are extremely important when exactly solving the\nproximal operator is time-consuming, or the proximal operator does not have an\nanalytic solution. However, existing inexact proximal gradient methods only\nconsider convex problems. The knowledge of inexact proximal gradient methods in\nthe non-convex setting is very limited. % Moreover, for some machine learning\nmodels, there is still no proposed solver for exactly solving the proximal\noperator. To address this challenge, in this paper, we first propose three\ninexact proximal gradient algorithms, including the basic version and\nNesterov's accelerated version. After that, we provide the theoretical analysis\nto the basic and Nesterov's accelerated versions. The theoretical results show\nthat our inexact proximal gradient algorithms can have the same convergence\nrates as the ones of exact proximal gradient algorithms in the non-convex\nsetting.\n  Finally, we show the applications of our inexact proximal gradient algorithms\non three representative non-convex learning problems. All experimental results\nconfirm the superiority of our new inexact proximal gradient algorithms.\n", "versions": [{"version": "v1", "created": "Sun, 18 Dec 2016 22:14:36 GMT"}, {"version": "v2", "created": "Sat, 8 Sep 2018 12:38:31 GMT"}], "update_date": "2018-09-11", "authors_parsed": [["Gu", "Bin", ""], ["Wang", "De", ""], ["Huo", "Zhouyuan", ""], ["Huang", "Heng", ""]]}, {"id": "1612.06007", "submitter": "Ahmed Alaa", "authors": "Ahmed M. Alaa and Mihaela van der Schaar", "title": "A Hidden Absorbing Semi-Markov Model for Informatively Censored Temporal\n  Data: Learning and Inference", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Modeling continuous-time physiological processes that manifest a patient's\nevolving clinical states is a key step in approaching many problems in\nhealthcare. In this paper, we develop the Hidden Absorbing Semi-Markov Model\n(HASMM): a versatile probabilistic model that is capable of capturing the\nmodern electronic health record (EHR) data. Unlike exist- ing models, an HASMM\naccommodates irregularly sampled, temporally correlated, and informatively\ncensored physiological data, and can describe non-stationary clinical state\ntransitions. Learning an HASMM from the EHR data is achieved via a novel\nforward- filtering backward-sampling Monte-Carlo EM algorithm that exploits the\nknowledge of the end-point clinical outcomes (informative censoring) in the EHR\ndata, and implements the E-step by sequentially sampling the patients' clinical\nstates in the reverse-time direction while conditioning on the future states.\nReal-time inferences are drawn via a forward- filtering algorithm that operates\non a virtually constructed discrete-time embedded Markov chain that mirrors the\npatient's continuous-time state trajectory. We demonstrate the di- agnostic and\nprognostic utility of the HASMM in a critical care prognosis setting using a\nreal-world dataset for patients admitted to the Ronald Reagan UCLA Medical\nCenter.\n", "versions": [{"version": "v1", "created": "Sun, 18 Dec 2016 23:02:02 GMT"}, {"version": "v2", "created": "Tue, 27 Dec 2016 13:44:59 GMT"}], "update_date": "2016-12-28", "authors_parsed": [["Alaa", "Ahmed M.", ""], ["van der Schaar", "Mihaela", ""]]}, {"id": "1612.06061", "submitter": "Dimitrios Katselis", "authors": "Dimitrios Katselis, Carolyn L. Beck, R. Srikant", "title": "Mixing Times and Structural Inference for Bernoulli Autoregressive\n  Processes", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We introduce a novel multivariate random process producing Bernoulli outputs\nper dimension, that can possibly formalize binary interactions in various\ngraphical structures and can be used to model opinion dynamics, epidemics,\nfinancial and biological time series data, etc. We call this a Bernoulli\nAutoregressive Process (BAR). A BAR process models a discrete-time vector\nrandom sequence of $p$ scalar Bernoulli processes with autoregressive dynamics\nand corresponds to a particular Markov Chain. The benefit from the\nautoregressive dynamics is the description of a $2^p\\times 2^p$ transition\nmatrix by at most $pd$ effective parameters for some $d\\ll p$ or by two sparse\nmatrices of dimensions $p\\times p^2$ and $p\\times p$, respectively,\nparameterizing the transitions. Additionally, we show that the BAR process\nmixes rapidly, by proving that the mixing time is $O(\\log p)$. The hidden\nconstant in the previous mixing time bound depends explicitly on the values of\nthe chain parameters and implicitly on the maximum allowed in-degree of a node\nin the corresponding graph. For a network with $p$ nodes, where each node has\nin-degree at most $d$ and corresponds to a scalar Bernoulli process generated\nby a BAR, we provide a greedy algorithm that can efficiently learn the\nstructure of the underlying directed graph with a sample complexity\nproportional to the mixing time of the BAR process. The sample complexity of\nthe proposed algorithm is nearly order-optimal as it is only a $\\log p$ factor\naway from an information-theoretic lower bound. We present simulation results\nillustrating the performance of our algorithm in various setups, including a\nmodel for a biological signaling network.\n", "versions": [{"version": "v1", "created": "Mon, 19 Dec 2016 06:59:40 GMT"}], "update_date": "2016-12-20", "authors_parsed": [["Katselis", "Dimitrios", ""], ["Beck", "Carolyn L.", ""], ["Srikant", "R.", ""]]}, {"id": "1612.06067", "submitter": "Babhru Joshi", "authors": "Paul Hand and Babhru Joshi", "title": "A Convex Program for Mixed Linear Regression with a Recovery Guarantee\n  for Well-Separated Data", "comments": null, "journal-ref": "Information and Inference 7 (2018) 563-579", "doi": "10.1093/imaiai/iax018", "report-no": null, "categories": "math.OC stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We introduce a convex approach for mixed linear regression over $d$ features.\nThis approach is a second-order cone program, based on L1 minimization, which\nassigns an estimate regression coefficient in $\\mathbb{R}^{d}$ for each data\npoint. These estimates can then be clustered using, for example, $k$-means. For\nproblems with two or more mixture classes, we prove that the convex program\nexactly recovers all of the mixture components in the noiseless setting under\ntechnical conditions that include a well-separation assumption on the data.\nUnder these assumptions, recovery is possible if each class has at least $d$\nindependent measurements. We also explore an iteratively reweighted least\nsquares implementation of this method on real and synthetic data.\n", "versions": [{"version": "v1", "created": "Mon, 19 Dec 2016 07:58:00 GMT"}, {"version": "v2", "created": "Thu, 21 Dec 2017 18:40:39 GMT"}], "update_date": "2019-01-09", "authors_parsed": [["Hand", "Paul", ""], ["Joshi", "Babhru", ""]]}, {"id": "1612.06083", "submitter": "Yannis Papanikolaou", "authors": "Yannis Papanikolaou, Ioannis Katakis, Grigorios Tsoumakas", "title": "Hierarchical Partitioning of the Output Space in Multi-label Data", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Hierarchy Of Multi-label classifiers (HOMER) is a multi-label learning\nalgorithm that breaks the initial learning task to several, easier sub-tasks by\nfirst constructing a hierarchy of labels from a given label set and secondly\nemploying a given base multi-label classifier (MLC) to the resulting\nsub-problems. The primary goal is to effectively address class imbalance and\nscalability issues that often arise in real-world multi-label classification\nproblems. In this work, we present the general setup for a HOMER model and a\nsimple extension of the algorithm that is suited for MLCs that output rankings.\nFurthermore, we provide a detailed analysis of the properties of the algorithm,\nboth from an aspect of effectiveness and computational complexity. A secondary\ncontribution involves the presentation of a balanced variant of the k means\nalgorithm, which serves in the first step of the label hierarchy construction.\nWe conduct extensive experiments on six real-world datasets, studying\nempirically HOMER's parameters and providing examples of instantiations of the\nalgorithm with different clustering approaches and MLCs, The empirical results\ndemonstrate a significant improvement over the given base MLC.\n", "versions": [{"version": "v1", "created": "Mon, 19 Dec 2016 09:08:59 GMT"}], "update_date": "2016-12-31", "authors_parsed": [["Papanikolaou", "Yannis", ""], ["Katakis", "Ioannis", ""], ["Tsoumakas", "Grigorios", ""]]}, {"id": "1612.06131", "submitter": "Stefano Martiniani", "authors": "Daan Frenkel, K. Julian Schrenk, Stefano Martiniani", "title": "Monte Carlo sampling for stochastic weight functions", "comments": "7 pages, 4 figures", "journal-ref": "Proceedings of the National Academy of Sciences, 114(27),\n  6924-6929 (2017)", "doi": "10.1073/pnas.1620497114", "report-no": null, "categories": "cond-mat.stat-mech physics.comp-ph stat.ME stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Conventional Monte Carlo simulations are stochastic in the sense that the\nacceptance of a trial move is decided by comparing a computed acceptance\nprobability with a random number, uniformly distributed between 0 and 1. Here\nwe consider the case that the weight determining the acceptance probability\nitself is fluctuating. This situation is common in many numerical studies. We\nshow that it is possible to construct a rigorous Monte Carlo algorithm that\nvisits points in state space with a probability proportional to their average\nweight. The same approach has the potential to transform the methodology of a\ncertain class of high-throughput experiments or the analysis of noisy datasets.\n", "versions": [{"version": "v1", "created": "Mon, 19 Dec 2016 11:32:01 GMT"}], "update_date": "2018-05-24", "authors_parsed": [["Frenkel", "Daan", ""], ["Schrenk", "K. Julian", ""], ["Martiniani", "Stefano", ""]]}, {"id": "1612.06176", "submitter": "Dirk Lorenz", "authors": "Lars M. Mescheder, Dirk A. Lorenz", "title": "An extended Perona-Malik model based on probabilistic models", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV math.NA stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The Perona-Malik model has been very successful at restoring images from\nnoisy input. In this paper, we reinterpret the Perona-Malik model in the\nlanguage of Gaussian scale mixtures and derive some extensions of the model.\nSpecifically, we show that the expectation-maximization (EM) algorithm applied\nto Gaussian scale mixtures leads to the lagged-diffusivity algorithm for\ncomputing stationary points of the Perona-Malik diffusion equations. Moreover,\nwe show how mean field approximations to these Gaussian scale mixtures lead to\na modification of the lagged-diffusivity algorithm that better captures the\nuncertainties in the restoration. Since this modification can be hard to\ncompute in practice we propose relaxations to the mean field objective to make\nthe algorithm computationally feasible. Our numerical experiments show that\nthis modified lagged-diffusivity algorithm often performs better at restoring\ntextured areas and fuzzy edges than the unmodified algorithm. As a second\napplication of the Gaussian scale mixture framework, we show how an efficient\nsampling procedure can be obtained for the probabilistic model, making the\ncomputation of the conditional mean and other expectations algorithmically\nfeasible. Again, the resulting algorithm has a strong resemblance to the\nlagged-diffusivity algorithm. Finally, we show that a probabilistic version of\nthe Mumford-Shah segementation model can be obtained in the same framework with\na discrete edge-prior.\n", "versions": [{"version": "v1", "created": "Mon, 19 Dec 2016 13:39:45 GMT"}], "update_date": "2016-12-20", "authors_parsed": [["Mescheder", "Lars M.", ""], ["Lorenz", "Dirk A.", ""]]}, {"id": "1612.06246", "submitter": "Haipeng Luo", "authors": "Alekh Agarwal, Haipeng Luo, Behnam Neyshabur and Robert E. Schapire", "title": "Corralling a Band of Bandit Algorithms", "comments": "Accepted to COLT 2017", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study the problem of combining multiple bandit algorithms (that is, online\nlearning algorithms with partial feedback) with the goal of creating a master\nalgorithm that performs almost as well as the best base algorithm if it were to\nbe run on its own. The main challenge is that when run with a master, base\nalgorithms unavoidably receive much less feedback and it is thus critical that\nthe master not starve a base algorithm that might perform uncompetitively\ninitially but would eventually outperform others if given enough feedback. We\naddress this difficulty by devising a version of Online Mirror Descent with a\nspecial mirror map together with a sophisticated learning rate scheme. We show\nthat this approach manages to achieve a more delicate balance between\nexploiting and exploring base algorithms than previous works yielding superior\nregret bounds.\n  Our results are applicable to many settings, such as multi-armed bandits,\ncontextual bandits, and convex bandits. As examples, we present two main\napplications. The first is to create an algorithm that enjoys worst-case\nrobustness while at the same time performing much better when the environment\nis relatively easy. The second is to create an algorithm that works\nsimultaneously under different assumptions of the environment, such as\ndifferent priors or different loss structures.\n", "versions": [{"version": "v1", "created": "Mon, 19 Dec 2016 16:17:56 GMT"}, {"version": "v2", "created": "Fri, 6 Jan 2017 15:41:31 GMT"}, {"version": "v3", "created": "Tue, 6 Jun 2017 03:21:09 GMT"}], "update_date": "2017-06-07", "authors_parsed": [["Agarwal", "Alekh", ""], ["Luo", "Haipeng", ""], ["Neyshabur", "Behnam", ""], ["Schapire", "Robert E.", ""]]}, {"id": "1612.06299", "submitter": "Shiva Kasiviswanathan", "authors": "Nina Narodytska, Shiva Prasad Kasiviswanathan", "title": "Simple Black-Box Adversarial Perturbations for Deep Networks", "comments": "19 Pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CR stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Deep neural networks are powerful and popular learning models that achieve\nstate-of-the-art pattern recognition performance on many computer vision,\nspeech, and language processing tasks. However, these networks have also been\nshown susceptible to carefully crafted adversarial perturbations which force\nmisclassification of the inputs. Adversarial examples enable adversaries to\nsubvert the expected system behavior leading to undesired consequences and\ncould pose a security risk when these systems are deployed in the real world.\n  In this work, we focus on deep convolutional neural networks and demonstrate\nthat adversaries can easily craft adversarial examples even without any\ninternal knowledge of the target network. Our attacks treat the network as an\noracle (black-box) and only assume that the output of the network can be\nobserved on the probed inputs. Our first attack is based on a simple idea of\nadding perturbation to a randomly selected single pixel or a small set of them.\nWe then improve the effectiveness of this attack by carefully constructing a\nsmall set of pixels to perturb by using the idea of greedy local-search. Our\nproposed attacks also naturally extend to a stronger notion of\nmisclassification. Our extensive experimental results illustrate that even\nthese elementary attacks can reveal a deep neural network's vulnerabilities.\nThe simplicity and effectiveness of our proposed schemes mean that they could\nserve as a litmus test for designing robust networks.\n", "versions": [{"version": "v1", "created": "Mon, 19 Dec 2016 18:12:20 GMT"}], "update_date": "2016-12-20", "authors_parsed": [["Narodytska", "Nina", ""], ["Kasiviswanathan", "Shiva Prasad", ""]]}, {"id": "1612.06340", "submitter": "Sam Ganzfried", "authors": "Sam Ganzfried and Farzana Yusuf", "title": "Computing Human-Understandable Strategies", "comments": "Earlier version appeared in Proceedings of the Workshop on Computer\n  Poker and Imperfect Information Games at AAAI Conference on Artificial\n  Intelligence, 2017", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.GT cs.AI cs.LG cs.MA stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Algorithms for equilibrium computation generally make no attempt to ensure\nthat the computed strategies are understandable by humans. For instance the\nstrategies for the strongest poker agents are represented as massive binary\nfiles. In many situations, we would like to compute strategies that can\nactually be implemented by humans, who may have computational limitations and\nmay only be able to remember a small number of features or components of the\nstrategies that have been computed. We study poker games where private\ninformation distributions can be arbitrary. We create a large training set of\ngame instances and solutions, by randomly selecting the information\nprobabilities, and present algorithms that learn from the training instances in\norder to perform well in games with unseen information distributions. We are\nable to conclude several new fundamental rules about poker strategy that can be\neasily implemented by humans.\n", "versions": [{"version": "v1", "created": "Mon, 19 Dec 2016 20:40:19 GMT"}, {"version": "v2", "created": "Mon, 20 Feb 2017 17:54:11 GMT"}], "update_date": "2019-01-23", "authors_parsed": [["Ganzfried", "Sam", ""], ["Yusuf", "Farzana", ""]]}, {"id": "1612.06370", "submitter": "Deepak Pathak", "authors": "Deepak Pathak, Ross Girshick, Piotr Doll\\'ar, Trevor Darrell, Bharath\n  Hariharan", "title": "Learning Features by Watching Objects Move", "comments": "CVPR 2017", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI cs.LG cs.NE stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper presents a novel yet intuitive approach to unsupervised feature\nlearning. Inspired by the human visual system, we explore whether low-level\nmotion-based grouping cues can be used to learn an effective visual\nrepresentation. Specifically, we use unsupervised motion-based segmentation on\nvideos to obtain segments, which we use as 'pseudo ground truth' to train a\nconvolutional network to segment objects from a single frame. Given the\nextensive evidence that motion plays a key role in the development of the human\nvisual system, we hope that this straightforward approach to unsupervised\nlearning will be more effective than cleverly designed 'pretext' tasks studied\nin the literature. Indeed, our extensive experiments show that this is the\ncase. When used for transfer learning on object detection, our representation\nsignificantly outperforms previous unsupervised approaches across multiple\nsettings, especially when training data for the target task is scarce.\n", "versions": [{"version": "v1", "created": "Mon, 19 Dec 2016 20:56:04 GMT"}, {"version": "v2", "created": "Wed, 12 Apr 2017 04:28:47 GMT"}], "update_date": "2017-04-13", "authors_parsed": [["Pathak", "Deepak", ""], ["Girshick", "Ross", ""], ["Doll\u00e1r", "Piotr", ""], ["Darrell", "Trevor", ""], ["Hariharan", "Bharath", ""]]}, {"id": "1612.06404", "submitter": "Benjamin Bloem-Reddy", "authors": "Benjamin Bloem-Reddy and Peter Orbanz", "title": "Random Walk Models of Network Formation and Sequential Monte Carlo\n  Methods for Graphs", "comments": null, "journal-ref": null, "doi": "10.1111/rssb.12289", "report-no": null, "categories": "stat.ME stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We introduce a class of generative network models that insert edges by\nconnecting the starting and terminal vertices of a random walk on the network\ngraph. Within the taxonomy of statistical network models, this class is\ndistinguished by permitting the location of a new edge to explicitly depend on\nthe structure of the graph, but being nonetheless statistically and\ncomputationally tractable. In the limit of infinite walk length, the model\nconverges to an extension of the preferential attachment model---in this sense,\nit can be motivated alternatively by asking what preferential attachment is an\napproximation to. Theoretical properties, including the limiting degree\nsequence, are studied analytically. If the entire history of the graph is\nobserved, parameters can be estimated by maximum likelihood. If only the final\ngraph is available, its history can be imputed using MCMC. We develop a class\nof sequential Monte Carlo algorithms that are more generally applicable to\nsequential network models, and may be of interest in their own right. The model\nparameters can be recovered from a single graph generated by the model.\nApplications to data clarify the role of the random walk length as a length\nscale of interactions within the graph.\n", "versions": [{"version": "v1", "created": "Mon, 19 Dec 2016 21:01:36 GMT"}, {"version": "v2", "created": "Sat, 7 Jul 2018 20:23:57 GMT"}], "update_date": "2018-07-11", "authors_parsed": [["Bloem-Reddy", "Benjamin", ""], ["Orbanz", "Peter", ""]]}, {"id": "1612.06470", "submitter": "Farhad Pourkamali-Anaraki", "authors": "Farhad Pourkamali-Anaraki, Stephen Becker", "title": "Randomized Clustered Nystrom for Large-Scale Kernel Machines", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The Nystrom method has been popular for generating the low-rank approximation\nof kernel matrices that arise in many machine learning problems. The\napproximation quality of the Nystrom method depends crucially on the number of\nselected landmark points and the selection procedure. In this paper, we present\na novel algorithm to compute the optimal Nystrom low-approximation when the\nnumber of landmark points exceed the target rank. Moreover, we introduce a\nrandomized algorithm for generating landmark points that is scalable to\nlarge-scale data sets. The proposed method performs K-means clustering on\nlow-dimensional random projections of a data set and, thus, leads to\nsignificant savings for high-dimensional data sets. Our theoretical results\ncharacterize the tradeoffs between the accuracy and efficiency of our proposed\nmethod. Extensive experiments demonstrate the competitive performance as well\nas the efficiency of our proposed method.\n", "versions": [{"version": "v1", "created": "Tue, 20 Dec 2016 01:07:04 GMT"}], "update_date": "2016-12-21", "authors_parsed": [["Pourkamali-Anaraki", "Farhad", ""], ["Becker", "Stephen", ""]]}, {"id": "1612.06565", "submitter": "Hoi-To Wai", "authors": "Hoi-To Wai, Anna Scaglione, Uzi Harush, Baruch Barzel, Amir Leshem", "title": "RIDS: Robust Identification of Sparse Gene Regulatory Networks from\n  Perturbation Experiments", "comments": "20 pages, 6 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "q-bio.QM cs.IT math.IT q-bio.MN stat.ML", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Reconstructing the causal network in a complex dynamical system plays a\ncrucial role in many applications, from sub-cellular biology to economic\nsystems. Here we focus on inferring gene regulation networks (GRNs) from\nperturbation or gene deletion experiments. Despite their scientific merit, such\nperturbation experiments are not often used for such inference due to their\ncostly experimental procedure, requiring significant resources to complete the\nmeasurement of every single experiment. To overcome this challenge, we develop\nthe Robust IDentification of Sparse networks (RIDS) method that reconstructs\nthe GRN from a small number of perturbation experiments. Our method uses the\ngene expression data observed in each experiment and translates that into a\nsteady state condition of the system's nonlinear interaction dynamics. Applying\na sparse optimization criterion, we are able to extract the parameters of the\nunderlying weighted network, even from very few experiments. In fact, we\ndemonstrate analytically that, under certain conditions, the GRN can be\nperfectly reconstructed using $K = \\Omega (d_{max})$ perturbation experiments,\nwhere $d_{max}$ is the maximum in-degree of the GRN, a small value for\nrealistic sparse networks, indicating that RIDS can achieve high performance\nwith a scalable number of experiments. We test our method on both synthetic and\nexperimental data extracted from the DREAM5 network inference challenge. We\nshow that the RIDS achieves superior performance compared to the\nstate-of-the-art methods, while requiring as few as ~60% less experimental\ndata. Moreover, as opposed to almost all competing methods, RIDS allows us to\ninfer the directionality of the GRN links, allowing us to infer empirical GRNs,\nwithout relying on the commonly provided list of transcription factors.\n", "versions": [{"version": "v1", "created": "Tue, 20 Dec 2016 09:34:27 GMT"}], "update_date": "2016-12-21", "authors_parsed": [["Wai", "Hoi-To", ""], ["Scaglione", "Anna", ""], ["Harush", "Uzi", ""], ["Barzel", "Baruch", ""], ["Leshem", "Amir", ""]]}, {"id": "1612.06598", "submitter": "Muhammad Yousefnezhad", "authors": "Muhammad Yousefnezhad, Sheng-Jun Huang, Daoqiang Zhang", "title": "WoCE: a framework for clustering ensemble by exploiting the wisdom of\n  Crowds theory", "comments": "Accepted in IEEE Transactions on Cybernetics", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The Wisdom of Crowds (WOC), as a theory in the social science, gets a new\nparadigm in computer science. The WOC theory explains that the aggregate\ndecision made by a group is often better than those of its individual members\nif specific conditions are satisfied. This paper presents a novel framework for\nunsupervised and semi-supervised cluster ensemble by exploiting the WOC theory.\nWe employ four conditions in the WOC theory, i.e., diversity, independency,\ndecentralization and aggregation, to guide both the constructing of individual\nclustering results and the final combination for clustering ensemble. Firstly,\nindependency criterion, as a novel mapping system on the raw data set, removes\nthe correlation between features on our proposed method. Then, decentralization\nas a novel mechanism generates high-quality individual clustering results.\nNext, uniformity as a new diversity metric evaluates the generated clustering\nresults. Further, weighted evidence accumulation clustering method is proposed\nfor the final aggregation without using thresholding procedure. Experimental\nstudy on varied data sets demonstrates that the proposed approach achieves\nsuperior performance to state-of-the-art methods.\n", "versions": [{"version": "v1", "created": "Tue, 20 Dec 2016 10:39:45 GMT"}], "update_date": "2016-12-21", "authors_parsed": [["Yousefnezhad", "Muhammad", ""], ["Huang", "Sheng-Jun", ""], ["Zhang", "Daoqiang", ""]]}, {"id": "1612.06650", "submitter": "Lisa Handl", "authors": "Lisa Handl, Adrin Jalali, Michael Scherer, Nico Pfeifer", "title": "Partially blind domain adaptation for age prediction from DNA\n  methylation data", "comments": "NIPS 2016 Workshop on Machine Learning for Health, Barcelona, Spain", "journal-ref": null, "doi": null, "report-no": null, "categories": "q-bio.QM stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Over the last years, huge resources of biological and medical data have\nbecome available for research. This data offers great chances for machine\nlearning applications in health care, e.g. for precision medicine, but is also\nchallenging to analyze. Typical challenges include a large number of possibly\ncorrelated features and heterogeneity in the data. One flourishing field of\nbiological research in which this is relevant is epigenetics. Here, especially\nlarge amounts of DNA methylation data have emerged. This epigenetic mark has\nbeen used to predict a donor's 'epigenetic age' and increased epigenetic aging\nhas been linked to lifestyle and disease history. In this paper we propose an\nadaptive model which performs feature selection for each test sample\nindividually based on the distribution of the input data. The method can be\nseen as partially blind domain adaptation. We apply the model to the problem of\nage prediction based on DNA methylation data from a variety of tissues, and\ncompare it to a standard model, which does not take heterogeneity into account.\nThe standard approach has particularly bad performance on one tissue type on\nwhich we show substantial improvement with our new adaptive approach even\nthough no samples of that tissue were part of the training data.\n", "versions": [{"version": "v1", "created": "Tue, 20 Dec 2016 13:26:57 GMT"}], "update_date": "2016-12-21", "authors_parsed": [["Handl", "Lisa", ""], ["Jalali", "Adrin", ""], ["Scherer", "Michael", ""], ["Pfeifer", "Nico", ""]]}, {"id": "1612.06669", "submitter": "Siddharth Bhela", "authors": "Siddharth Bhela, Vassilis Kekatos, Sriharsha Veeramachaneni", "title": "Enhancing Observability in Distribution Grids using Smart Meter Data", "comments": "8 pages, 8 figures, submitted to IEEE Transactions on Smart Grid", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.OC cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Due to limited metering infrastructure, distribution grids are currently\nchallenged by observability issues. On the other hand, smart meter data,\nincluding local voltage magnitudes and power injections, are communicated to\nthe utility operator from grid buses with renewable generation and\ndemand-response programs. This work employs grid data from metered buses\ntowards inferring the underlying grid state. To this end, a coupled formulation\nof the power flow problem (CPF) is put forth. Exploiting the high variability\nof injections at metered buses, the controllability of solar inverters, and the\nrelative time-invariance of conventional loads, the idea is to solve the\nnon-linear power flow equations jointly over consecutive time instants. An\nintuitive and easily verifiable rule pertaining to the locations of metered and\nnon-metered buses on the physical grid is shown to be a necessary and\nsufficient criterion for local observability in radial networks. To account for\nnoisy smart meter readings, a coupled power system state estimation (CPSSE)\nproblem is further developed. Both CPF and CPSSE tasks are tackled via\naugmented semi-definite program relaxations. The observability criterion along\nwith the CPF and CPSSE solvers are numerically corroborated using synthetic and\nactual solar generation and load data on the IEEE 34-bus benchmark feeder.\n", "versions": [{"version": "v1", "created": "Tue, 20 Dec 2016 14:12:58 GMT"}], "update_date": "2016-12-21", "authors_parsed": [["Bhela", "Siddharth", ""], ["Kekatos", "Vassilis", ""], ["Veeramachaneni", "Sriharsha", ""]]}, {"id": "1612.06676", "submitter": "Andrey Lavrentyev Andrey Lavrentyev", "authors": "Pavel Filonov, Andrey Lavrentyev, Artem Vorontsov", "title": "Multivariate Industrial Time Series with Cyber-Attack Simulation: Fault\n  Detection Using an LSTM-based Predictive Data Model", "comments": "Accepted at NIPS Time Series Workshop 2016, Barcelona, Spain, 2016.\n  Reference update in this version,\n  https://sites.google.com/site/nipsts2016/NIPS_2016_TSW_paper_10.pdf?attredirects=0&d=1", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We adopted an approach based on an LSTM neural network to monitor and detect\nfaults in industrial multivariate time series data. To validate the approach we\ncreated a Modelica model of part of a real gasoil plant. By introducing hacks\ninto the logic of the Modelica model, we were able to generate both the roots\nand causes of fault behavior in the plant. Having a self-consistent data set\nwith labeled faults, we used an LSTM architecture with a forecasting error\nthreshold to obtain precision and recall quality metrics. The dependency of the\nquality metric on the threshold level is considered. An appropriate mechanism\nsuch as \"one handle\" was introduced for filtering faults that are outside of\nthe plant operator field of interest.\n", "versions": [{"version": "v1", "created": "Tue, 20 Dec 2016 14:24:49 GMT"}, {"version": "v2", "created": "Mon, 26 Dec 2016 11:26:03 GMT"}], "update_date": "2016-12-28", "authors_parsed": [["Filonov", "Pavel", ""], ["Lavrentyev", "Andrey", ""], ["Vorontsov", "Artem", ""]]}, {"id": "1612.06879", "submitter": "Faicel Chamroukhi", "authors": "Faicel Chamroukhi", "title": "Robust mixture of experts modeling using the skew $t$ distribution", "comments": "arXiv admin note: substantial text overlap with arXiv:1506.06707", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Mixture of Experts (MoE) is a popular framework in the fields of statistics\nand machine learning for modeling heterogeneity in data for regression,\nclassification and clustering. MoE for continuous data are usually based on the\nnormal distribution. However, it is known that for data with asymmetric\nbehavior, heavy tails and atypical observations, the use of the normal\ndistribution is unsuitable. We introduce a new robust non-normal mixture of\nexperts modeling using the skew $t$ distribution. The proposed skew $t$ mixture\nof experts, named STMoE, handles these issues of the normal mixtures experts\nregarding possibly skewed, heavy-tailed and noisy data. We develop a dedicated\nexpectation conditional maximization (ECM) algorithm to estimate the model\nparameters by monotonically maximizing the observed data log-likelihood. We\ndescribe how the presented model can be used in prediction and in model-based\nclustering of regression data. Numerical experiments carried out on simulated\ndata show the effectiveness and the robustness of the proposed model in fitting\nnon-linear regression functions as well as in model-based clustering. Then, the\nproposed model is applied to the real-world data of tone perception for musical\ndata analysis, and the one of temperature anomalies for the analysis of climate\nchange data. The obtained results confirm the usefulness of the model for\npractical data analysis applications.\n", "versions": [{"version": "v1", "created": "Fri, 9 Dec 2016 19:25:27 GMT"}], "update_date": "2016-12-22", "authors_parsed": [["Chamroukhi", "Faicel", ""]]}, {"id": "1612.07019", "submitter": "Badong Chen", "authors": "Badong Chen, Lei Xing, Xin Wang, Jing Qin, Nanning Zheng", "title": "Robust Learning with Kernel Mean p-Power Error Loss", "comments": "11 pages, 7 figures, 10 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Correntropy is a second order statistical measure in kernel space, which has\nbeen successfully applied in robust learning and signal processing. In this\npaper, we define a nonsecond order statistical measure in kernel space, called\nthe kernel mean-p power error (KMPE), including the correntropic loss (CLoss)\nas a special case. Some basic properties of KMPE are presented. In particular,\nwe apply the KMPE to extreme learning machine (ELM) and principal component\nanalysis (PCA), and develop two robust learning algorithms, namely ELM-KMPE and\nPCA-KMPE. Experimental results on synthetic and benchmark data show that the\ndeveloped algorithms can achieve consistently better performance when compared\nwith some existing methods.\n", "versions": [{"version": "v1", "created": "Wed, 21 Dec 2016 09:10:48 GMT"}], "update_date": "2016-12-22", "authors_parsed": [["Chen", "Badong", ""], ["Xing", "Lei", ""], ["Wang", "Xin", ""], ["Qin", "Jing", ""], ["Zheng", "Nanning", ""]]}, {"id": "1612.07222", "submitter": "Kevin Jiao", "authors": "Xi Chen, Kevin Jiao, Qihang Lin", "title": "Bayesian Decision Process for Cost-Efficient Dynamic Ranking via\n  Crowdsourcing", "comments": null, "journal-ref": "Journal of Machine Learning Research 17 (2016) 1-40", "doi": null, "report-no": null, "categories": "stat.ML cs.LG stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Rank aggregation based on pairwise comparisons over a set of items has a wide\nrange of applications. Although considerable research has been devoted to the\ndevelopment of rank aggregation algorithms, one basic question is how to\nefficiently collect a large amount of high-quality pairwise comparisons for the\nranking purpose. Because of the advent of many crowdsourcing services, a crowd\nof workers are often hired to conduct pairwise comparisons with a small\nmonetary reward for each pair they compare. Since different workers have\ndifferent levels of reliability and different pairs have different levels of\nambiguity, it is desirable to wisely allocate the limited budget for\ncomparisons among the pairs of items and workers so that the global ranking can\nbe accurately inferred from the comparison results. To this end, we model the\nactive sampling problem in crowdsourced ranking as a Bayesian Markov decision\nprocess, which dynamically selects item pairs and workers to improve the\nranking accuracy under a budget constraint. We further develop a\ncomputationally efficient sampling policy based on knowledge gradient as well\nas a moment matching technique for posterior approximation. Experimental\nevaluations on both synthetic and real data show that the proposed policy\nachieves high ranking accuracy with a lower labeling cost.\n", "versions": [{"version": "v1", "created": "Wed, 21 Dec 2016 16:24:27 GMT"}], "update_date": "2016-12-22", "authors_parsed": [["Chen", "Xi", ""], ["Jiao", "Kevin", ""], ["Lin", "Qihang", ""]]}, {"id": "1612.07374", "submitter": "Charmgil Hong", "authors": "Charmgil Hong, Milos Hauskrecht", "title": "Detecting Unusual Input-Output Associations in Multivariate Conditional\n  Data", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Despite tremendous progress in outlier detection research in recent years,\nthe majority of existing methods are designed only to detect unconditional\noutliers that correspond to unusual data patterns expressed in the joint space\nof all data attributes. Such methods are not applicable when we seek to detect\nconditional outliers that reflect unusual responses associated with a given\ncontext or condition. This work focuses on multivariate conditional outlier\ndetection, a special type of the conditional outlier detection problem, where\ndata instances consist of multi-dimensional input (context) and output\n(responses) pairs. We present a novel outlier detection framework that\nidentifies abnormal input-output associations in data with the help of a\ndecomposable conditional probabilistic model that is learned from all data\ninstances. Since components of this model can vary in their quality, we combine\nthem with the help of weights reflecting their reliability in assessment of\noutliers. We study two ways of calculating the component weights: global that\nrelies on all data, and local that relies only on instances similar to the\ntarget instance. Experimental results on data from various domains demonstrate\nthe ability of our framework to successfully identify multivariate conditional\noutliers.\n", "versions": [{"version": "v1", "created": "Wed, 21 Dec 2016 22:43:08 GMT"}], "update_date": "2016-12-23", "authors_parsed": [["Hong", "Charmgil", ""], ["Hauskrecht", "Milos", ""]]}, {"id": "1612.07401", "submitter": "Ruijin Cang", "authors": "Ruijin Cang, Yaopengxiao Xu, Shaohua Chen, Yongming Liu, Yang Jiao,\n  Max Yi Ren", "title": "Microstructure Representation and Reconstruction of Heterogeneous\n  Materials via Deep Belief Network for Computational Material Design", "comments": "27 pages, 17 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cond-mat.mtrl-sci cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Integrated Computational Materials Engineering (ICME) aims to accelerate\noptimal design of complex material systems by integrating material science and\ndesign automation. For tractable ICME, it is required that (1) a structural\nfeature space be identified to allow reconstruction of new designs, and (2) the\nreconstruction process be property-preserving. The majority of existing\nstructural presentation schemes rely on the designer's understanding of\nspecific material systems to identify geometric and statistical features, which\ncould be biased and insufficient for reconstructing physically meaningful\nmicrostructures of complex material systems. In this paper, we develop a\nfeature learning mechanism based on convolutional deep belief network to\nautomate a two-way conversion between microstructures and their\nlower-dimensional feature representations, and to achieves a 1000-fold\ndimension reduction from the microstructure space. The proposed model is\napplied to a wide spectrum of heterogeneous material systems with distinct\nmicrostructural features including Ti-6Al-4V alloy, Pb63-Sn37 alloy,\nFontainebleau sandstone, and Spherical colloids, to produce material\nreconstructions that are close to the original samples with respect to 2-point\ncorrelation functions and mean critical fracture strength. This capability is\nnot achieved by existing synthesis methods that rely on the Markovian\nassumption of material microstructures.\n", "versions": [{"version": "v1", "created": "Thu, 22 Dec 2016 00:29:25 GMT"}, {"version": "v2", "created": "Mon, 6 Mar 2017 19:36:09 GMT"}, {"version": "v3", "created": "Fri, 28 Apr 2017 00:11:29 GMT"}], "update_date": "2017-05-01", "authors_parsed": [["Cang", "Ruijin", ""], ["Xu", "Yaopengxiao", ""], ["Chen", "Shaohua", ""], ["Liu", "Yongming", ""], ["Jiao", "Yang", ""], ["Ren", "Max Yi", ""]]}, {"id": "1612.07454", "submitter": "Angshul Majumdar Dr.", "authors": "Vanika Singhal, Shikha Singh and Angshul Majumdar", "title": "How to Train Your Deep Neural Network with Dictionary Learning", "comments": "DCC 2017 poster", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Currently there are two predominant ways to train deep neural networks. The\nfirst one uses restricted Boltzmann machine (RBM) and the second one\nautoencoders. RBMs are stacked in layers to form deep belief network (DBN); the\nfinal representation layer is attached to the target to complete the deep\nneural network. Autoencoders are nested one inside the other to form stacked\nautoencoders; once the stcaked autoencoder is learnt the decoder portion is\ndetached and the target attached to the deepest layer of the encoder to form\nthe deep neural network. This work proposes a new approach to train deep neural\nnetworks using dictionary learning as the basic building block; the idea is to\nuse the features from the shallower layer as inputs for training the next\ndeeper layer. One can use any type of dictionary learning (unsupervised,\nsupervised, discriminative etc.) as basic units till the pre-final layer. In\nthe final layer one needs to use the label consistent dictionary learning\nformulation for classification. We compare our proposed framework with existing\nstate-of-the-art deep learning techniques on benchmark problems; we are always\nwithin the top 10 results. In actual problems of age and gender classification,\nwe are better than the best known techniques.\n", "versions": [{"version": "v1", "created": "Thu, 22 Dec 2016 06:17:01 GMT"}], "update_date": "2016-12-23", "authors_parsed": [["Singhal", "Vanika", ""], ["Singh", "Shikha", ""], ["Majumdar", "Angshul", ""]]}, {"id": "1612.07512", "submitter": "Jose M. Pe\\~na", "authors": "Jose M. Pe\\~na, Marcus Bendtsen", "title": "Causal Effect Identification in Acyclic Directed Mixed Graphs and Gated\n  Models", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We introduce a new family of graphical models that consists of graphs with\npossibly directed, undirected and bidirected edges but without directed cycles.\nWe show that these models are suitable for representing causal models with\nadditive error terms. We provide a set of sufficient graphical criteria for the\nidentification of arbitrary causal effects when the new models contain directed\nand undirected edges but no bidirected edge. We also provide a necessary and\nsufficient graphical criterion for the identification of the causal effect of a\nsingle variable on the rest of the variables. Moreover, we develop an exact\nalgorithm for learning the new models from observational and interventional\ndata via answer set programming. Finally, we introduce gated models for causal\neffect identification, a new family of graphical models that exploits context\nspecific independences to identify additional causal effects.\n", "versions": [{"version": "v1", "created": "Thu, 22 Dec 2016 10:00:38 GMT"}, {"version": "v2", "created": "Sun, 28 May 2017 12:44:27 GMT"}], "update_date": "2017-05-30", "authors_parsed": [["Pe\u00f1a", "Jose M.", ""], ["Bendtsen", "Marcus", ""]]}, {"id": "1612.07523", "submitter": "Monisankha Pal", "authors": "Monisankha Pal, Dipjyoti Paul, Md Sahidullah, Goutam Saha", "title": "Robustness of Voice Conversion Techniques Under Mismatched Conditions", "comments": "5 pages, 2 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.SD cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Most of the existing studies on voice conversion (VC) are conducted in\nacoustically matched conditions between source and target signal. However, the\nrobustness of VC methods in presence of mismatch remains unknown. In this\npaper, we report a comparative analysis of different VC techniques under\nmismatched conditions. The extensive experiments with five different VC\ntechniques on CMU ARCTIC corpus suggest that performance of VC methods\nsubstantially degrades in noisy conditions. We have found that bilinear\nfrequency warping with amplitude scaling (BLFWAS) outperforms other methods in\nmost of the noisy conditions. We further explore the suitability of different\nspeech enhancement techniques for robust conversion. The objective evaluation\nresults indicate that spectral subtraction and log minimum mean square error\n(logMMSE) based speech enhancement techniques can be used to improve the\nperformance in specific noisy conditions.\n", "versions": [{"version": "v1", "created": "Thu, 22 Dec 2016 10:14:59 GMT"}], "update_date": "2016-12-23", "authors_parsed": [["Pal", "Monisankha", ""], ["Paul", "Dipjyoti", ""], ["Sahidullah", "Md", ""], ["Saha", "Goutam", ""]]}, {"id": "1612.07548", "submitter": "Wendelin B\\\"ohmer", "authors": "Wendelin B\\\"ohmer and Rong Guo and Klaus Obermayer", "title": "Non-Deterministic Policy Improvement Stabilizes Approximated\n  Reinforcement Learning", "comments": "This paper has been presented at the 13th European Workshop on\n  Reinforcement Learning (EWRL 2016) on the 3rd and 4th of December 2016 in\n  Barcelona, Spain", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper investigates a type of instability that is linked to the greedy\npolicy improvement in approximated reinforcement learning. We show empirically\nthat non-deterministic policy improvement can stabilize methods like LSPI by\ncontrolling the improvements' stochasticity. Additionally we show that a\nsuitable representation of the value function also stabilizes the solution to\nsome degree. The presented approach is simple and should also be easily\ntransferable to more sophisticated algorithms like deep reinforcement learning.\n", "versions": [{"version": "v1", "created": "Thu, 22 Dec 2016 11:30:35 GMT"}], "update_date": "2016-12-23", "authors_parsed": [["B\u00f6hmer", "Wendelin", ""], ["Guo", "Rong", ""], ["Obermayer", "Klaus", ""]]}, {"id": "1612.07597", "submitter": "Andreas Henelius", "authors": "Andreas Henelius, Antti Ukkonen, Kai Puolam\\\"aki", "title": "Finding Statistically Significant Attribute Interactions", "comments": "9 pages, 4 tables, 1 figure", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In many data exploration tasks it is meaningful to identify groups of\nattribute interactions that are specific to a variable of interest. For\ninstance, in a dataset where the attributes are medical markers and the\nvariable of interest (class variable) is binary indicating presence/absence of\ndisease, we would like to know which medical markers interact with respect to\nthe binary class label. These interactions are useful in several practical\napplications, for example, to gain insight into the structure of the data, in\nfeature selection, and in data anonymisation. We present a novel method, based\non statistical significance testing, that can be used to verify if the data set\nhas been created by a given factorised class-conditional joint distribution,\nwhere the distribution is parametrised by a partition of its attributes.\nFurthermore, we provide a method, named ASTRID, for automatically finding a\npartition of attributes describing the distribution that has generated the\ndata. State-of-the-art classifiers are utilised to capture the interactions\npresent in the data by systematically breaking attribute interactions and\nobserving the effect of this breaking on classifier performance. We empirically\ndemonstrate the utility of the proposed method with examples using real and\nsynthetic data.\n", "versions": [{"version": "v1", "created": "Thu, 22 Dec 2016 13:53:42 GMT"}, {"version": "v2", "created": "Thu, 16 Mar 2017 12:21:36 GMT"}], "update_date": "2017-03-17", "authors_parsed": [["Henelius", "Andreas", ""], ["Ukkonen", "Antti", ""], ["Puolam\u00e4ki", "Kai", ""]]}, {"id": "1612.07640", "submitter": "Rui Zhao", "authors": "Rui Zhao, Ruqiang Yan, Zhenghua Chen, Kezhi Mao, Peng Wang and Robert\n  X. Gao", "title": "Deep Learning and Its Applications to Machine Health Monitoring: A\n  Survey", "comments": "14 pages, 9 figures, submitted to IEEE Transactions on Neural\n  Networks and Learning Systems", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Since 2006, deep learning (DL) has become a rapidly growing research\ndirection, redefining state-of-the-art performances in a wide range of areas\nsuch as object recognition, image segmentation, speech recognition and machine\ntranslation. In modern manufacturing systems, data-driven machine health\nmonitoring is gaining in popularity due to the widespread deployment of\nlow-cost sensors and their connection to the Internet. Meanwhile, deep learning\nprovides useful tools for processing and analyzing these big machinery data.\nThe main purpose of this paper is to review and summarize the emerging research\nwork of deep learning on machine health monitoring. After the brief\nintroduction of deep learning techniques, the applications of deep learning in\nmachine health monitoring systems are reviewed mainly from the following\naspects: Auto-encoder (AE) and its variants, Restricted Boltzmann Machines and\nits variants including Deep Belief Network (DBN) and Deep Boltzmann Machines\n(DBM), Convolutional Neural Networks (CNN) and Recurrent Neural Networks (RNN).\nFinally, some new trends of DL-based machine health monitoring methods are\ndiscussed.\n", "versions": [{"version": "v1", "created": "Fri, 16 Dec 2016 04:56:30 GMT"}], "update_date": "2016-12-23", "authors_parsed": [["Zhao", "Rui", ""], ["Yan", "Ruqiang", ""], ["Chen", "Zhenghua", ""], ["Mao", "Kezhi", ""], ["Wang", "Peng", ""], ["Gao", "Robert X.", ""]]}, {"id": "1612.07659", "submitter": "Youngjoo Seo", "authors": "Youngjoo Seo, Micha\\\"el Defferrard, Pierre Vandergheynst, Xavier\n  Bresson", "title": "Structured Sequence Modeling with Graph Convolutional Recurrent Networks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper introduces Graph Convolutional Recurrent Network (GCRN), a deep\nlearning model able to predict structured sequences of data. Precisely, GCRN is\na generalization of classical recurrent neural networks (RNN) to data\nstructured by an arbitrary graph. Such structured sequences can represent\nseries of frames in videos, spatio-temporal measurements on a network of\nsensors, or random walks on a vocabulary graph for natural language modeling.\nThe proposed model combines convolutional neural networks (CNN) on graphs to\nidentify spatial structures and RNN to find dynamic patterns. We study two\npossible architectures of GCRN, and apply the models to two practical problems:\npredicting moving MNIST data, and modeling natural language with the Penn\nTreebank dataset. Experiments show that exploiting simultaneously graph spatial\nand dynamic information about data can improve both precision and learning\nspeed.\n", "versions": [{"version": "v1", "created": "Thu, 22 Dec 2016 15:53:57 GMT"}], "update_date": "2016-12-23", "authors_parsed": [["Seo", "Youngjoo", ""], ["Defferrard", "Micha\u00ebl", ""], ["Vandergheynst", "Pierre", ""], ["Bresson", "Xavier", ""]]}, {"id": "1612.07728", "submitter": "Alexander Wein", "authors": "Amelia Perry and Alexander S. Wein and Afonso S. Bandeira", "title": "Statistical limits of spiked tensor models", "comments": "39 pages, 5 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.PR cs.IT math.IT math.ST stat.ML stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study the statistical limits of both detecting and estimating a rank-one\ndeformation of a symmetric random Gaussian tensor. We establish upper and lower\nbounds on the critical signal-to-noise ratio, under a variety of priors for the\nplanted vector: (i) a uniformly sampled unit vector, (ii) i.i.d. $\\pm 1$\nentries, and (iii) a sparse vector where a constant fraction $\\rho$ of entries\nare i.i.d. $\\pm 1$ and the rest are zero. For each of these cases, our upper\nand lower bounds match up to a $1+o(1)$ factor as the order $d$ of the tensor\nbecomes large. For sparse signals (iii), our bounds are also asymptotically\ntight in the sparse limit $\\rho \\to 0$ for any fixed $d$ (including the $d=2$\ncase of sparse PCA). Our upper bounds for (i) demonstrate a phenomenon\nreminiscent of the work of Baik, Ben Arous and P\\'ech\\'e: an `eigenvalue' of a\nperturbed tensor emerges from the bulk at a strictly lower signal-to-noise\nratio than when the perturbation itself exceeds the bulk; we quantify the size\nof this effect. We also provide some general results for larger classes of\npriors. In particular, the large $d$ asymptotics of the threshold location\ndiffers between problems with discrete priors versus continuous priors.\nFinally, for priors (i) and (ii) we carry out the replica prediction from\nstatistical physics, which is conjectured to give the exact\ninformation-theoretic threshold for any fixed $d$.\n  Of independent interest, we introduce a new improvement to the second moment\nmethod for contiguity, on which our lower bounds are based. Our technique\nconditions away from rare `bad' events that depend on interactions between the\nsignal and noise. This enables us to close $\\sqrt{2}$-factor gaps present in\nseveral previous works.\n", "versions": [{"version": "v1", "created": "Thu, 22 Dec 2016 18:04:30 GMT"}, {"version": "v2", "created": "Tue, 24 Jan 2017 18:50:28 GMT"}], "update_date": "2017-01-25", "authors_parsed": [["Perry", "Amelia", ""], ["Wein", "Alexander S.", ""], ["Bandeira", "Afonso S.", ""]]}, {"id": "1612.07843", "submitter": "Wojciech Samek", "authors": "Leila Arras, Franziska Horn, Gr\\'egoire Montavon, Klaus-Robert\n  M\\\"uller, Wojciech Samek", "title": "\"What is Relevant in a Text Document?\": An Interpretable Machine\n  Learning Approach", "comments": "19 pages, 7 figures", "journal-ref": null, "doi": "10.1371/journal.pone.0181142", "report-no": null, "categories": "cs.CL cs.IR cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Text documents can be described by a number of abstract concepts such as\nsemantic category, writing style, or sentiment. Machine learning (ML) models\nhave been trained to automatically map documents to these abstract concepts,\nallowing to annotate very large text collections, more than could be processed\nby a human in a lifetime. Besides predicting the text's category very\naccurately, it is also highly desirable to understand how and why the\ncategorization process takes place. In this paper, we demonstrate that such\nunderstanding can be achieved by tracing the classification decision back to\nindividual words using layer-wise relevance propagation (LRP), a recently\ndeveloped technique for explaining predictions of complex non-linear\nclassifiers. We train two word-based ML models, a convolutional neural network\n(CNN) and a bag-of-words SVM classifier, on a topic categorization task and\nadapt the LRP method to decompose the predictions of these models onto words.\nResulting scores indicate how much individual words contribute to the overall\nclassification decision. This enables one to distill relevant information from\ntext documents without an explicit semantic information extraction step. We\nfurther use the word-wise relevance scores for generating novel vector-based\ndocument representations which capture semantic information. Based on these\ndocument vectors, we introduce a measure of model explanatory power and show\nthat, although the SVM and CNN models perform similarly in terms of\nclassification accuracy, the latter exhibits a higher level of explainability\nwhich makes it more comprehensible for humans and potentially more useful for\nother applications.\n", "versions": [{"version": "v1", "created": "Fri, 23 Dec 2016 00:31:30 GMT"}], "update_date": "2017-11-01", "authors_parsed": [["Arras", "Leila", ""], ["Horn", "Franziska", ""], ["Montavon", "Gr\u00e9goire", ""], ["M\u00fcller", "Klaus-Robert", ""], ["Samek", "Wojciech", ""]]}, {"id": "1612.07846", "submitter": "Daniel Durstewitz", "authors": "Daniel Durstewitz", "title": "A State Space Approach for Piecewise-Linear Recurrent Neural Networks\n  for Reconstructing Nonlinear Dynamics from Neural Measurements", "comments": null, "journal-ref": null, "doi": "10.1371/journal.pcbi.1005542", "report-no": null, "categories": "q-bio.NC cs.NE q-bio.QM stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The computational properties of neural systems are often thought to be\nimplemented in terms of their network dynamics. Hence, recovering the system\ndynamics from experimentally observed neuronal time series, like multiple\nsingle-unit (MSU) recordings or neuroimaging data, is an important step toward\nunderstanding its computations. Ideally, one would not only seek a state space\nrepresentation of the dynamics, but would wish to have access to its governing\nequations for in-depth analysis. Recurrent neural networks (RNNs) are a\ncomputationally powerful and dynamically universal formal framework which has\nbeen extensively studied from both the computational and the dynamical systems\nperspective. Here we develop a semi-analytical maximum-likelihood estimation\nscheme for piecewise-linear RNNs (PLRNNs) within the statistical framework of\nstate space models, which accounts for noise in both the underlying latent\ndynamics and the observation process. The Expectation-Maximization algorithm is\nused to infer the latent state distribution, through a global Laplace\napproximation, and the PLRNN parameters iteratively. After validating the\nprocedure on toy examples, the approach is applied to MSU recordings from the\nrodent anterior cingulate cortex obtained during performance of a classical\nworking memory task, delayed alternation. A model with 5 states turned out to\nbe sufficient to capture the essential computational dynamics underlying task\nperformance, including stimulus-selective delay activity. The estimated models\nwere rarely multi-stable, but rather were tuned to exhibit slow dynamics in the\nvicinity of a bifurcation point. In summary, the present work advances a\nsemi-analytical (thus reasonably fast) maximum-likelihood estimation framework\nfor PLRNNs that may enable to recover the relevant dynamics underlying observed\nneuronal time series, and directly link them to computational properties.\n", "versions": [{"version": "v1", "created": "Fri, 23 Dec 2016 01:01:52 GMT"}], "update_date": "2017-07-05", "authors_parsed": [["Durstewitz", "Daniel", ""]]}, {"id": "1612.07857", "submitter": "Waheed Bajwa", "authors": "Tong Wu, Prudhvi Gurram, Raghuveer M. Rao, and Waheed U. Bajwa", "title": "Human Action Attribute Learning From Video Data Using Low-Rank\n  Representations", "comments": "26 pages; 8 figures; 2 tables; Rutgers University Technical Report\n  #2020-07-001", "journal-ref": null, "doi": "10.7282/t3-t7fe-4a02", "report-no": "Rutgers University Technical Report #2020-07-001", "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Representation of human actions as a sequence of human body movements or\naction attributes enables the development of models for human activity\nrecognition and summarization. We present an extension of the low-rank\nrepresentation (LRR) model, termed the clustering-aware structure-constrained\nlow-rank representation (CS-LRR) model, for unsupervised learning of human\naction attributes from video data. Our model is based on the union-of-subspaces\n(UoS) framework, and integrates spectral clustering into the LRR optimization\nproblem for better subspace clustering results. We lay out an efficient linear\nalternating direction method to solve the CS-LRR optimization problem. We also\nintroduce a hierarchical subspace clustering approach, termed hierarchical\nCS-LRR, to learn the attributes without the need for a priori specification of\ntheir number. By visualizing and labeling these action attributes, the\nhierarchical model can be used to semantically summarize long video sequences\nof human actions at multiple resolutions. A human action or activity can also\nbe uniquely represented as a sequence of transitions from one action attribute\nto another, which can then be used for human action recognition. We demonstrate\nthe effectiveness of the proposed model for semantic summarization and action\nrecognition through comprehensive experiments on five real-world human action\ndatasets.\n", "versions": [{"version": "v1", "created": "Fri, 23 Dec 2016 02:28:04 GMT"}, {"version": "v2", "created": "Sun, 5 Jul 2020 01:19:57 GMT"}], "update_date": "2020-07-14", "authors_parsed": [["Wu", "Tong", ""], ["Gurram", "Prudhvi", ""], ["Rao", "Raghuveer M.", ""], ["Bajwa", "Waheed U.", ""]]}, {"id": "1612.07866", "submitter": "Nike Sun", "authors": "Andrea Montanari and Nike Sun", "title": "Spectral algorithms for tensor completion", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS math.ST stat.ML stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In the tensor completion problem, one seeks to estimate a low-rank tensor\nbased on a random sample of revealed entries. In terms of the required sample\nsize, earlier work revealed a large gap between estimation with unbounded\ncomputational resources (using, for instance, tensor nuclear norm minimization)\nand polynomial-time algorithms. Among the latter, the best statistical\nguarantees have been proved, for third-order tensors, using the sixth level of\nthe sum-of-squares (SOS) semidefinite programming hierarchy (Barak and Moitra,\n2014). However, the SOS approach does not scale well to large problem\ninstances. By contrast, spectral methods --- based on unfolding or matricizing\nthe tensor --- are attractive for their low complexity, but have been believed\nto require a much larger sample size.\n  This paper presents two main contributions. First, we propose a new\nunfolding-based method, which outperforms naive ones for symmetric $k$-th order\ntensors of rank $r$. For this result we make a study of singular space\nestimation for partially revealed matrices of large aspect ratio, which may be\nof independent interest. For third-order tensors, our algorithm matches the SOS\nmethod in terms of sample size (requiring about $rd^{3/2}$ revealed entries),\nsubject to a worse rank condition ($r\\ll d^{3/4}$ rather than $r\\ll d^{3/2}$).\nWe complement this result with a different spectral algorithm for third-order\ntensors in the overcomplete ($r\\ge d$) regime. Under a random model, this\nsecond approach succeeds in estimating tensors of rank $d\\le r \\ll d^{3/2}$\nfrom about $rd^{3/2}$ revealed entries.\n", "versions": [{"version": "v1", "created": "Fri, 23 Dec 2016 03:59:29 GMT"}], "update_date": "2016-12-26", "authors_parsed": [["Montanari", "Andrea", ""], ["Sun", "Nike", ""]]}, {"id": "1612.07919", "submitter": "Mehdi S. M. Sajjadi", "authors": "Mehdi S. M. Sajjadi and Bernhard Sch\\\"olkopf and Michael Hirsch", "title": "EnhanceNet: Single Image Super-Resolution Through Automated Texture\n  Synthesis", "comments": "main paper and supplementary material", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Single image super-resolution is the task of inferring a high-resolution\nimage from a single low-resolution input. Traditionally, the performance of\nalgorithms for this task is measured using pixel-wise reconstruction measures\nsuch as peak signal-to-noise ratio (PSNR) which have been shown to correlate\npoorly with the human perception of image quality. As a result, algorithms\nminimizing these metrics tend to produce over-smoothed images that lack\nhigh-frequency textures and do not look natural despite yielding high PSNR\nvalues.\n  We propose a novel application of automated texture synthesis in combination\nwith a perceptual loss focusing on creating realistic textures rather than\noptimizing for a pixel-accurate reproduction of ground truth images during\ntraining. By using feed-forward fully convolutional neural networks in an\nadversarial training setting, we achieve a significant boost in image quality\nat high magnification ratios. Extensive experiments on a number of datasets\nshow the effectiveness of our approach, yielding state-of-the-art results in\nboth quantitative and qualitative benchmarks.\n", "versions": [{"version": "v1", "created": "Fri, 23 Dec 2016 10:16:26 GMT"}, {"version": "v2", "created": "Sun, 30 Jul 2017 21:52:23 GMT"}], "update_date": "2018-01-16", "authors_parsed": [["Sajjadi", "Mehdi S. M.", ""], ["Sch\u00f6lkopf", "Bernhard", ""], ["Hirsch", "Michael", ""]]}, {"id": "1612.07976", "submitter": "Kuniaki Saito Saito Kuniaki", "authors": "Kuniaki Saito, Yusuke Mukuta, Yoshitaka Ushiku, Tatsuya Harada", "title": "DeMIAN: Deep Modality Invariant Adversarial Network", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Obtaining common representations from different modalities is important in\nthat they are interchangeable with each other in a classification problem. For\nexample, we can train a classifier on image features in the common\nrepresentations and apply it to the testing of the text features in the\nrepresentations. Existing multi-modal representation learning methods mainly\naim to extract rich information from paired samples and train a classifier by\nthe corresponding labels; however, collecting paired samples and their labels\nsimultaneously involves high labor costs. Addressing paired modal samples\nwithout their labels and single modal data with their labels independently is\nmuch easier than addressing labeled multi-modal data. To obtain the common\nrepresentations under such a situation, we propose to make the distributions\nover different modalities similar in the learned representations, namely\nmodality-invariant representations. In particular, we propose a novel algorithm\nfor modality-invariant representation learning, named Deep Modality Invariant\nAdversarial Network (DeMIAN), which utilizes the idea of Domain Adaptation\n(DA). Using the modality-invariant representations learned by DeMIAN, we\nachieved better classification accuracy than with the state-of-the-art methods,\nespecially for some benchmark datasets of zero-shot learning.\n", "versions": [{"version": "v1", "created": "Fri, 23 Dec 2016 14:07:01 GMT"}, {"version": "v2", "created": "Wed, 28 Dec 2016 02:29:15 GMT"}], "update_date": "2016-12-30", "authors_parsed": [["Saito", "Kuniaki", ""], ["Mukuta", "Yusuke", ""], ["Ushiku", "Yoshitaka", ""], ["Harada", "Tatsuya", ""]]}, {"id": "1612.07993", "submitter": "Jesse Krijthe", "authors": "Jesse H. Krijthe", "title": "RSSL: Semi-supervised Learning in R", "comments": "Presented at RRPR 2016: 1st Workshop on Reproducible Research in\n  Pattern Recognition", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we introduce a package for semi-supervised learning research\nin the R programming language called RSSL. We cover the purpose of the package,\nthe methods it includes and comment on their use and implementation. We then\nshow, using several code examples, how the package can be used to replicate\nwell-known results from the semi-supervised learning literature.\n", "versions": [{"version": "v1", "created": "Fri, 23 Dec 2016 15:02:54 GMT"}], "update_date": "2016-12-26", "authors_parsed": [["Krijthe", "Jesse H.", ""]]}, {"id": "1612.08082", "submitter": "Onur Atan", "authors": "Onur Atan, William R. Zame, Qiaojun Feng, Mihaela van der Schaar", "title": "Constructing Effective Personalized Policies Using Counterfactual\n  Inference from Biased Data Sets with Many Features", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper proposes a novel approach for constructing effective personalized\npolicies when the observed data lacks counter-factual information, is biased\nand possesses many features. The approach is applicable in a wide variety of\nsettings from healthcare to advertising to education to finance. These settings\nhave in common that the decision maker can observe, for each previous instance,\nan array of features of the instance, the action taken in that instance, and\nthe reward realized -- but not the rewards of actions that were not taken: the\ncounterfactual information. Learning in such settings is made even more\ndifficult because the observed data is typically biased by the existing policy\n(that generated the data) and because the array of features that might affect\nthe reward in a particular instance -- and hence should be taken into account\nin deciding on an action in each particular instance -- is often vast. The\napproach presented here estimates propensity scores for the observed data,\ninfers counterfactuals, identifies a (relatively small) number of features that\nare (most) relevant for each possible action and instance, and prescribes a\npolicy to be followed. Comparison of the proposed algorithm against the\nstate-of-art algorithm on actual datasets demonstrates that the proposed\nalgorithm achieves a significant improvement in performance.\n", "versions": [{"version": "v1", "created": "Fri, 23 Dec 2016 20:29:52 GMT"}, {"version": "v2", "created": "Thu, 13 Jul 2017 05:14:03 GMT"}, {"version": "v3", "created": "Tue, 10 Jul 2018 08:16:55 GMT"}], "update_date": "2018-07-11", "authors_parsed": [["Atan", "Onur", ""], ["Zame", "William R.", ""], ["Feng", "Qiaojun", ""], ["van der Schaar", "Mihaela", ""]]}, {"id": "1612.08116", "submitter": "Anna Seigal", "authors": "Anna Seigal, Mariano Beguerisse-D\\'iaz, Birgit Schoeberl, Mario\n  Niepel, Heather A. Harrington", "title": "Tensor clustering with algebraic constraints gives interpretable groups\n  of crosstalk mechanisms in breast cancer", "comments": "22 pages, 12 figures, 4 tables", "journal-ref": "Journal of The Royal Society Interface, volume 16 (2019) issue\n  151, 20180661", "doi": "10.1098/rsif.2018.0661", "report-no": null, "categories": "q-bio.QM math.OC physics.soc-ph q-bio.MN stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We introduce a tensor-based clustering method to extract sparse,\nlow-dimensional structure from high-dimensional, multi-indexed datasets. This\nframework is designed to enable detection of clusters of data in the presence\nof structural requirements which we encode as algebraic constraints in a linear\nprogram. Our clustering method is general and can be tailored to a variety of\napplications in science and industry. We illustrate our method on a collection\nof experiments measuring the response of genetically diverse breast cancer cell\nlines to an array of ligands. Each experiment consists of a cell line-ligand\ncombination, and contains time-course measurements of the early-signalling\nkinases MAPK and AKT at two different ligand dose levels. By imposing\nappropriate structural constraints and respecting the multi-indexed structure\nof the data, the analysis of clusters can be optimized for biological\ninterpretation and therapeutic understanding. We then perform a systematic,\nlarge-scale exploration of mechanistic models of MAPK-AKT crosstalk for each\ncluster. This analysis allows us to quantify the heterogeneity of breast cancer\ncell subtypes, and leads to hypotheses about the signalling mechanisms that\nmediate the response of the cell lines to ligands.\n", "versions": [{"version": "v1", "created": "Sat, 24 Dec 2016 00:00:43 GMT"}, {"version": "v2", "created": "Fri, 28 Apr 2017 14:49:37 GMT"}, {"version": "v3", "created": "Fri, 8 Feb 2019 18:16:47 GMT"}], "update_date": "2019-02-11", "authors_parsed": [["Seigal", "Anna", ""], ["Beguerisse-D\u00edaz", "Mariano", ""], ["Schoeberl", "Birgit", ""], ["Niepel", "Mario", ""], ["Harrington", "Heather A.", ""]]}, {"id": "1612.08321", "submitter": "Nathan Kallus", "authors": "Nathan Kallus", "title": "Generalized Optimal Matching Methods for Causal Inference", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML math.OC math.ST stat.ME stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We develop an encompassing framework for matching, covariate balancing, and\ndoubly-robust methods for causal inference from observational data called\ngeneralized optimal matching (GOM). The framework is given by generalizing a\nnew functional-analytical formulation of optimal matching, giving rise to the\nclass of GOM methods, for which we provide a single unified theory to analyze\ntractability, consistency, and efficiency. Many commonly used existing methods\nare included in GOM and, using their GOM interpretation, can be extended to\noptimally and automatically trade off balance for variance and outperform their\nstandard counterparts. As a subclass, GOM gives rise to kernel optimal matching\n(KOM), which, as supported by new theoretical and empirical results, is notable\nfor combining many of the positive properties of other methods in one. KOM,\nwhich is solved as a linearly-constrained convex-quadratic optimization\nproblem, inherits both the interpretability and model-free consistency of\nmatching but can also achieve the $\\sqrt{n}$-consistency of well-specified\nregression and the efficiency and robustness of doubly robust methods. In\nsettings of limited overlap, KOM enables a very transparent method for interval\nestimation for partial identification and robust coverage. We demonstrate these\nbenefits in examples with both synthetic and real data\n", "versions": [{"version": "v1", "created": "Mon, 26 Dec 2016 03:58:42 GMT"}, {"version": "v2", "created": "Thu, 15 Jun 2017 17:50:00 GMT"}, {"version": "v3", "created": "Fri, 27 Oct 2017 14:29:37 GMT"}], "update_date": "2017-10-30", "authors_parsed": [["Kallus", "Nathan", ""]]}, {"id": "1612.08388", "submitter": "Cesar Comin PhD", "authors": "Mayra Z. Rodriguez, Cesar H. Comin, Dalcimar Casanova, Odemir M.\n  Bruno, Diego R. Amancio, Francisco A. Rodrigues, Luciano da F. Costa", "title": "Clustering Algorithms: A Comparative Approach", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Many real-world systems can be studied in terms of pattern recognition tasks,\nso that proper use (and understanding) of machine learning methods in practical\napplications becomes essential. While a myriad of classification methods have\nbeen proposed, there is no consensus on which methods are more suitable for a\ngiven dataset. As a consequence, it is important to comprehensively compare\nmethods in many possible scenarios. In this context, we performed a systematic\ncomparison of 7 well-known clustering methods available in the R language. In\norder to account for the many possible variations of data, we considered\nartificial datasets with several tunable properties (number of classes,\nseparation between classes, etc). In addition, we also evaluated the\nsensitivity of the clustering methods with regard to their parameters\nconfiguration. The results revealed that, when considering the default\nconfigurations of the adopted methods, the spectral approach usually\noutperformed the other clustering algorithms. We also found that the default\nconfiguration of the adopted implementations was not accurate. In these cases,\na simple approach based on random selection of parameters values proved to be a\ngood alternative to improve the performance. All in all, the reported approach\nprovides subsidies guiding the choice of clustering algorithms.\n", "versions": [{"version": "v1", "created": "Mon, 26 Dec 2016 14:25:32 GMT"}], "update_date": "2016-12-28", "authors_parsed": [["Rodriguez", "Mayra Z.", ""], ["Comin", "Cesar H.", ""], ["Casanova", "Dalcimar", ""], ["Bruno", "Odemir M.", ""], ["Amancio", "Diego R.", ""], ["Rodrigues", "Francisco A.", ""], ["Costa", "Luciano da F.", ""]]}, {"id": "1612.08392", "submitter": "Muhammad Yousefnezhad", "authors": "Muhammad Yousefnezhad, Daoqiang Zhang", "title": "Multi-Region Neural Representation: A novel model for decoding visual\n  stimuli in human brains", "comments": "Accepted in SIAM International Conference on Data Mininig (SDM),\n  Houston, Texas, USA, April/27-29, 2017", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG q-bio.NC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Multivariate Pattern (MVP) classification holds enormous potential for\ndecoding visual stimuli in the human brain by employing task-based fMRI data\nsets. There is a wide range of challenges in the MVP techniques, i.e.\ndecreasing noise and sparsity, defining effective regions of interest (ROIs),\nvisualizing results, and the cost of brain studies. In overcoming these\nchallenges, this paper proposes a novel model of neural representation, which\ncan automatically detect the active regions for each visual stimulus and then\nutilize these anatomical regions for visualizing and analyzing the functional\nactivities. Therefore, this model provides an opportunity for neuroscientists\nto ask this question: what is the effect of a stimulus on each of the detected\nregions instead of just study the fluctuation of voxels in the manually\nselected ROIs. Moreover, our method introduces analyzing snapshots of brain\nimage for decreasing sparsity rather than using the whole of fMRI time series.\nFurther, a new Gaussian smoothing method is proposed for removing noise of\nvoxels in the level of ROIs. The proposed method enables us to combine\ndifferent fMRI data sets for reducing the cost of brain studies. Experimental\nstudies on 4 visual categories (words, consonants, objects and nonsense photos)\nconfirm that the proposed method achieves superior performance to\nstate-of-the-art methods.\n", "versions": [{"version": "v1", "created": "Mon, 26 Dec 2016 14:37:57 GMT"}], "update_date": "2016-12-28", "authors_parsed": [["Yousefnezhad", "Muhammad", ""], ["Zhang", "Daoqiang", ""]]}, {"id": "1612.08406", "submitter": "Torsten Ensslin", "authors": "Torsten A. En{\\ss}lin, Jakob Knollm\\\"uller", "title": "Correlated signal inference by free energy exploration", "comments": "19 pages, 5 figures, submitted, updated acknowledgements", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML astro-ph.IM cs.IT cs.LG math.IT", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The inference of correlated signal fields with unknown correlation structures\nis of high scientific and technological relevance, but poses significant\nconceptual and numerical challenges. To address these, we develop the\ncorrelated signal inference (CSI) algorithm within information field theory\n(IFT) and discuss its numerical implementation. To this end, we introduce the\nfree energy exploration (FrEE) strategy for numerical information field theory\n(NIFTy) applications. The FrEE strategy is to let the mathematical structure of\nthe inference problem determine the dynamics of the numerical solver. FrEE uses\nthe Gibbs free energy formalism for all involved unknown fields and correlation\nstructures without marginalization of nuisance quantities. It thereby avoids\nthe complexity marginalization often impose to IFT equations. FrEE\nsimultaneously solves for the mean and the uncertainties of signal, nuisance,\nand auxiliary fields, while exploiting any analytically calculable quantity.\nFinally, FrEE uses a problem specific and self-tuning exploration strategy to\nswiftly identify the optimal field estimates as well as their uncertainty maps.\nFor all estimated fields, properly weighted posterior samples drawn from their\nexact, fully non-Gaussian distributions can be generated. Here, we develop the\nFrEE strategies for the CSI of a normal, a log-normal, and a Poisson log-normal\nIFT signal inference problem and demonstrate their performances via their NIFTy\nimplementations.\n", "versions": [{"version": "v1", "created": "Mon, 26 Dec 2016 15:42:22 GMT"}, {"version": "v2", "created": "Mon, 13 Feb 2017 22:21:17 GMT"}], "update_date": "2017-02-15", "authors_parsed": [["En\u00dflin", "Torsten A.", ""], ["Knollm\u00fcller", "Jakob", ""]]}, {"id": "1612.08425", "submitter": "Chris Hodapp", "authors": "Chris Hodapp", "title": "Unsupervised Learning for Computational Phenotyping", "comments": "5 pages, 5 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://creativecommons.org/licenses/by-sa/4.0/", "abstract": "  With large volumes of health care data comes the research area of\ncomputational phenotyping, making use of techniques such as machine learning to\ndescribe illnesses and other clinical concepts from the data itself. The\n\"traditional\" approach of using supervised learning relies on a domain expert,\nand has two main limitations: requiring skilled humans to supply correct labels\nlimits its scalability and accuracy, and relying on existing clinical\ndescriptions limits the sorts of patterns that can be found. For instance, it\nmay fail to acknowledge that a disease treated as a single condition may really\nhave several subtypes with different phenotypes, as seems to be the case with\nasthma and heart disease. Some recent papers cite successes instead using\nunsupervised learning. This shows great potential for finding patterns in\nElectronic Health Records that would otherwise be hidden and that can lead to\ngreater understanding of conditions and treatments. This work implements a\nmethod derived strongly from Lasko et al., but implements it in Apache Spark\nand Python and generalizes it to laboratory time-series data in MIMIC-III. It\nis released as an open-source tool for exploration, analysis, and\nvisualization, available at https://github.com/Hodapp87/mimic3_phenotyping\n", "versions": [{"version": "v1", "created": "Mon, 26 Dec 2016 18:47:11 GMT"}, {"version": "v2", "created": "Thu, 29 Dec 2016 16:25:34 GMT"}], "update_date": "2016-12-30", "authors_parsed": [["Hodapp", "Chris", ""]]}, {"id": "1612.08498", "submitter": "Taco Cohen", "authors": "Taco S. Cohen, Max Welling", "title": "Steerable CNNs", "comments": null, "journal-ref": "Proceedings of the International Conference on Learning\n  Representations, 2017", "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  It has long been recognized that the invariance and equivariance properties\nof a representation are critically important for success in many vision tasks.\nIn this paper we present Steerable Convolutional Neural Networks, an efficient\nand flexible class of equivariant convolutional networks. We show that\nsteerable CNNs achieve state of the art results on the CIFAR image\nclassification benchmark. The mathematical theory of steerable representations\nreveals a type system in which any steerable representation is a composition of\nelementary feature types, each one associated with a particular kind of\nsymmetry. We show how the parameter cost of a steerable filter bank depends on\nthe types of the input and output features, and show how to use this knowledge\nto construct CNNs that utilize parameters effectively.\n", "versions": [{"version": "v1", "created": "Tue, 27 Dec 2016 04:38:28 GMT"}], "update_date": "2019-04-23", "authors_parsed": [["Cohen", "Taco S.", ""], ["Welling", "Max", ""]]}, {"id": "1612.08543", "submitter": "Amir Hossein Akhavan Rahnama", "authors": "Amir Hossein Akhavan Rahnama", "title": "Distributed Real-Time Sentiment Analysis for Big Data Social Streams", "comments": null, "journal-ref": "IEEE 2014 International Conference on Control, Decision and\n  Information Technologies (CoDIT)", "doi": "10.1109/CoDIT.2014.6996998", "report-no": null, "categories": "stat.ML cs.CL cs.DB cs.DC cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Big data trend has enforced the data-centric systems to have continuous fast\ndata streams. In recent years, real-time analytics on stream data has formed\ninto a new research field, which aims to answer queries about\nwhat-is-happening-now with a negligible delay. The real challenge with\nreal-time stream data processing is that it is impossible to store instances of\ndata, and therefore online analytical algorithms are utilized. To perform\nreal-time analytics, pre-processing of data should be performed in a way that\nonly a short summary of stream is stored in main memory. In addition, due to\nhigh speed of arrival, average processing time for each instance of data should\nbe in such a way that incoming instances are not lost without being captured.\nLastly, the learner needs to provide high analytical accuracy measures.\nSentinel is a distributed system written in Java that aims to solve this\nchallenge by enforcing both the processing and learning process to be done in\ndistributed form. Sentinel is built on top of Apache Storm, a distributed\ncomputing platform. Sentinels learner, Vertical Hoeffding Tree, is a parallel\ndecision tree-learning algorithm based on the VFDT, with ability of enabling\nparallel classification in distributed environments. Sentinel also uses\nSpaceSaving to keep a summary of the data stream and stores its summary in a\nsynopsis data structure. Application of Sentinel on Twitter Public Stream API\nis shown and the results are discussed.\n", "versions": [{"version": "v1", "created": "Tue, 27 Dec 2016 09:10:18 GMT"}], "update_date": "2016-12-28", "authors_parsed": [["Rahnama", "Amir Hossein Akhavan", ""]]}, {"id": "1612.08544", "submitter": "Anuj Karpatne", "authors": "Anuj Karpatne, Gowtham Atluri, James Faghmous, Michael Steinbach,\n  Arindam Banerjee, Auroop Ganguly, Shashi Shekhar, Nagiza Samatova, and Vipin\n  Kumar", "title": "Theory-guided Data Science: A New Paradigm for Scientific Discovery from\n  Data", "comments": null, "journal-ref": "IEEE Transactions on Knowledge and Data Engineering, 29(10),\n  pp.2318-2331. 2017", "doi": "10.1109/TKDE.2017.2720168", "report-no": null, "categories": "cs.LG cs.AI stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Data science models, although successful in a number of commercial domains,\nhave had limited applicability in scientific problems involving complex\nphysical phenomena. Theory-guided data science (TGDS) is an emerging paradigm\nthat aims to leverage the wealth of scientific knowledge for improving the\neffectiveness of data science models in enabling scientific discovery. The\noverarching vision of TGDS is to introduce scientific consistency as an\nessential component for learning generalizable models. Further, by producing\nscientifically interpretable models, TGDS aims to advance our scientific\nunderstanding by discovering novel domain insights. Indeed, the paradigm of\nTGDS has started to gain prominence in a number of scientific disciplines such\nas turbulence modeling, material discovery, quantum chemistry, bio-medical\nscience, bio-marker discovery, climate science, and hydrology. In this paper,\nwe formally conceptualize the paradigm of TGDS and present a taxonomy of\nresearch themes in TGDS. We describe several approaches for integrating domain\nknowledge in different research themes using illustrative examples from\ndifferent disciplines. We also highlight some of the promising avenues of novel\nresearch for realizing the full potential of theory-guided data science.\n", "versions": [{"version": "v1", "created": "Tue, 27 Dec 2016 09:14:16 GMT"}, {"version": "v2", "created": "Mon, 13 Nov 2017 17:42:12 GMT"}], "update_date": "2017-11-15", "authors_parsed": [["Karpatne", "Anuj", ""], ["Atluri", "Gowtham", ""], ["Faghmous", "James", ""], ["Steinbach", "Michael", ""], ["Banerjee", "Arindam", ""], ["Ganguly", "Auroop", ""], ["Shekhar", "Shashi", ""], ["Samatova", "Nagiza", ""], ["Kumar", "Vipin", ""]]}, {"id": "1612.08549", "submitter": "Zhaoqiang Liu", "authors": "Zhaoqiang Liu and Vincent Y. F. Tan", "title": "Rank-One NMF-Based Initialization for NMF and Relative Error Bounds\n  under a Geometric Assumption", "comments": "Accepted by the IEEE Transactions on Signal Processing", "journal-ref": null, "doi": "10.1109/TSP.2017.2713761", "report-no": null, "categories": "stat.ML cs.IT math.IT stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a geometric assumption on nonnegative data matrices such that\nunder this assumption, we are able to provide upper bounds (both deterministic\nand probabilistic) on the relative error of nonnegative matrix factorization\n(NMF). The algorithm we propose first uses the geometric assumption to obtain\nan exact clustering of the columns of the data matrix; subsequently, it employs\nseveral rank-one NMFs to obtain the final decomposition. When applied to data\nmatrices generated from our statistical model, we observe that our proposed\nalgorithm produces factor matrices with comparable relative errors vis-\\`a-vis\nclassical NMF algorithms but with much faster speeds. On face image and\nhyperspectral imaging datasets, we demonstrate that our algorithm provides an\nexcellent initialization for applying other NMF algorithms at a low\ncomputational cost. Finally, we show on face and text datasets that the\ncombinations of our algorithm and several classical NMF algorithms outperform\nother algorithms in terms of clustering performance.\n", "versions": [{"version": "v1", "created": "Tue, 27 Dec 2016 09:27:10 GMT"}, {"version": "v2", "created": "Fri, 2 Jun 2017 08:04:07 GMT"}], "update_date": "2017-08-02", "authors_parsed": [["Liu", "Zhaoqiang", ""], ["Tan", "Vincent Y. F.", ""]]}, {"id": "1612.08633", "submitter": "Vishal Kakkar", "authors": "Vishal Kakkar, Shirish K. Shevade, S Sundararajan, Dinesh Garg", "title": "A Sparse Nonlinear Classifier Design Using AUC Optimization", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  AUC (Area under the ROC curve) is an important performance measure for\napplications where the data is highly imbalanced. Learning to maximize AUC\nperformance is thus an important research problem. Using a max-margin based\nsurrogate loss function, AUC optimization problem can be approximated as a\npairwise rankSVM learning problem. Batch learning methods for solving the\nkernelized version of this problem suffer from scalability and may not result\nin sparse classifiers. Recent years have witnessed an increased interest in the\ndevelopment of online or single-pass online learning algorithms that design a\nclassifier by maximizing the AUC performance. The AUC performance of nonlinear\nclassifiers, designed using online methods, is not comparable with that of\nnonlinear classifiers designed using batch learning algorithms on many\nreal-world datasets. Motivated by these observations, we design a scalable\nalgorithm for maximizing AUC performance by greedily adding the required number\nof basis functions into the classifier model. The resulting sparse classifiers\nperform faster inference. Our experimental results show that the level of\nsparsity achievable can be order of magnitude smaller than the Kernel RankSVM\nmodel without affecting the AUC performance much.\n", "versions": [{"version": "v1", "created": "Tue, 27 Dec 2016 13:52:56 GMT"}], "update_date": "2016-12-28", "authors_parsed": [["Kakkar", "Vishal", ""], ["Shevade", "Shirish K.", ""], ["Sundararajan", "S", ""], ["Garg", "Dinesh", ""]]}, {"id": "1612.08650", "submitter": "Jesse Krijthe", "authors": "Jesse H. Krijthe and Marco Loog", "title": "Reproducible Pattern Recognition Research: The Case of Optimistic SSL", "comments": "Presented at RRPR 2016: 1st Workshop on Reproducible Research in\n  Pattern Recognition", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we discuss the approaches we took and trade-offs involved in\nmaking a paper on a conceptual topic in pattern recognition research fully\nreproducible. We discuss our definition of reproducibility, the tools used, how\nthe analysis was set up, show some examples of alternative analyses the code\nenables and discuss our views on reproducibility.\n", "versions": [{"version": "v1", "created": "Tue, 27 Dec 2016 14:57:22 GMT"}], "update_date": "2016-12-28", "authors_parsed": [["Krijthe", "Jesse H.", ""], ["Loog", "Marco", ""]]}, {"id": "1612.08714", "submitter": "Andreas Henelius", "authors": "Andreas Henelius, Kai Puolam\\\"aki, Henrik Bostr\\\"om, Panagiotis\n  Papapetrou", "title": "Clustering with Confidence: Finding Clusters with Statistical Guarantees", "comments": "30 pages, 5 figures, 5 tables. Added URL to the source code", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Clustering is a widely used unsupervised learning method for finding\nstructure in the data. However, the resulting clusters are typically presented\nwithout any guarantees on their robustness; slightly changing the used data\nsample or re-running a clustering algorithm involving some stochastic component\nmay lead to completely different clusters. There is, hence, a need for\ntechniques that can quantify the instability of the generated clusters. In this\nstudy, we propose a technique for quantifying the instability of a clustering\nsolution and for finding robust clusters, termed core clusters, which\ncorrespond to clusters where the co-occurrence probability of each data item\nwithin a cluster is at least $1 - \\alpha$. We demonstrate how solving the core\nclustering problem is linked to finding the largest maximal cliques in a graph.\nWe show that the method can be used with both clustering and classification\nalgorithms. The proposed method is tested on both simulated and real datasets.\nThe results show that the obtained clusters indeed meet the guarantees on\nrobustness.\n", "versions": [{"version": "v1", "created": "Tue, 27 Dec 2016 19:39:23 GMT"}, {"version": "v2", "created": "Fri, 30 Dec 2016 17:56:48 GMT"}], "update_date": "2017-01-02", "authors_parsed": [["Henelius", "Andreas", ""], ["Puolam\u00e4ki", "Kai", ""], ["Bostr\u00f6m", "Henrik", ""], ["Papapetrou", "Panagiotis", ""]]}, {"id": "1612.08795", "submitter": "Tengyu Ma", "authors": "Sanjeev Arora, Rong Ge, Tengyu Ma, Andrej Risteski", "title": "Provable learning of Noisy-or Networks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.DS stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Many machine learning applications use latent variable models to explain\nstructure in data, whereby visible variables (= coordinates of the given\ndatapoint) are explained as a probabilistic function of some hidden variables.\nFinding parameters with the maximum likelihood is NP-hard even in very simple\nsettings. In recent years, provably efficient algorithms were nevertheless\ndeveloped for models with linear structures: topic models, mixture models,\nhidden markov models, etc. These algorithms use matrix or tensor decomposition,\nand make some reasonable assumptions about the parameters of the underlying\nmodel.\n  But matrix or tensor decomposition seems of little use when the latent\nvariable model has nonlinearities. The current paper shows how to make\nprogress: tensor decomposition is applied for learning the single-layer {\\em\nnoisy or} network, which is a textbook example of a Bayes net, and used for\nexample in the classic QMR-DT software for diagnosing which disease(s) a\npatient may have by observing the symptoms he/she exhibits.\n  The technical novelty here, which should be useful in other settings in\nfuture, is analysis of tensor decomposition in presence of systematic error\n(i.e., where the noise/error is correlated with the signal, and doesn't\ndecrease as number of samples goes to infinity). This requires rethinking all\nsteps of tensor decomposition methods from the ground up.\n  For simplicity our analysis is stated assuming that the network parameters\nwere chosen from a probability distribution but the method seems more generally\napplicable.\n", "versions": [{"version": "v1", "created": "Wed, 28 Dec 2016 03:35:59 GMT"}], "update_date": "2016-12-30", "authors_parsed": [["Arora", "Sanjeev", ""], ["Ge", "Rong", ""], ["Ma", "Tengyu", ""], ["Risteski", "Andrej", ""]]}, {"id": "1612.08875", "submitter": "Jesse Krijthe", "authors": "Jesse H. Krijthe and Marco Loog", "title": "The Pessimistic Limits and Possibilities of Margin-based Losses in\n  Semi-supervised Learning", "comments": "32nd Conference on Neural Information Processing Systems (NeurIPS\n  2018), Montreal, Canada", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Consider a classification problem where we have both labeled and unlabeled\ndata available. We show that for linear classifiers defined by convex\nmargin-based surrogate losses that are decreasing, it is impossible to\nconstruct any semi-supervised approach that is able to guarantee an improvement\nover the supervised classifier measured by this surrogate loss on the labeled\nand unlabeled data. For convex margin-based loss functions that also increase,\nwe demonstrate safe improvements are possible.\n", "versions": [{"version": "v1", "created": "Wed, 28 Dec 2016 13:17:07 GMT"}, {"version": "v2", "created": "Mon, 29 Oct 2018 15:20:33 GMT"}, {"version": "v3", "created": "Tue, 8 Jan 2019 11:01:35 GMT"}], "update_date": "2019-01-09", "authors_parsed": [["Krijthe", "Jesse H.", ""], ["Loog", "Marco", ""]]}, {"id": "1612.08915", "submitter": "V\\'ictor Pe\\~na", "authors": "Michael Jauch and V\\'ictor Pe\\~na", "title": "Bayesian Optimization with Shape Constraints", "comments": "NIPS 2016 Bayesian Optimization Workshop", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In typical applications of Bayesian optimization, minimal assumptions are\nmade about the objective function being optimized. This is true even when\nresearchers have prior information about the shape of the function with respect\nto one or more argument. We make the case that shape constraints are often\nappropriate in at least two important application areas of Bayesian\noptimization: (1) hyperparameter tuning of machine learning algorithms and (2)\ndecision analysis with utility functions. We describe a methodology for\nincorporating a variety of shape constraints within the usual Bayesian\noptimization framework and present positive results from simple applications\nwhich suggest that Bayesian optimization with shape constraints is a promising\ntopic for further research.\n", "versions": [{"version": "v1", "created": "Wed, 28 Dec 2016 16:12:28 GMT"}], "update_date": "2016-12-30", "authors_parsed": [["Jauch", "Michael", ""], ["Pe\u00f1a", "V\u00edctor", ""]]}, {"id": "1612.08932", "submitter": "Praneeth Vepakomma Praneeth Vepakomma", "authors": "Susovan Pal, Praneeth Vepakomma", "title": "Optimal bandwidth estimation for a fast manifold learning algorithm to\n  detect circular structure in high-dimensional data", "comments": "19 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We provide a way to infer about existence of topological circularity in\nhigh-dimensional data sets in $\\mathbb{R}^d$ from its projection in\n$\\mathbb{R}^2$ obtained through a fast manifold learning map as a function of\nthe high-dimensional dataset $\\mathbb{X}$ and a particular choice of a positive\nreal $\\sigma$ known as bandwidth parameter. At the same time we also provide a\nway to estimate the optimal bandwidth for fast manifold learning in this\nsetting through minimization of these functions of bandwidth. We also provide\nlimit theorems to characterize the behavior of our proposed functions of\nbandwidth.\n", "versions": [{"version": "v1", "created": "Wed, 28 Dec 2016 17:17:08 GMT"}], "update_date": "2016-12-30", "authors_parsed": [["Pal", "Susovan", ""], ["Vepakomma", "Praneeth", ""]]}, {"id": "1612.08936", "submitter": "Alina Zare", "authors": "Chao Chen, Alina Zare, Huy Trinh, Gbeng Omotara, J. Tory Cobb,\n  Timotius Lagaunne", "title": "Partial Membership Latent Dirichlet Allocation", "comments": "Version 1, Sent for Review. arXiv admin note: substantial text\n  overlap with arXiv:1511.02821", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Topic models (e.g., pLSA, LDA, sLDA) have been widely used for segmenting\nimagery. However, these models are confined to crisp segmentation, forcing a\nvisual word (i.e., an image patch) to belong to one and only one topic. Yet,\nthere are many images in which some regions cannot be assigned a crisp\ncategorical label (e.g., transition regions between a foggy sky and the ground\nor between sand and water at a beach). In these cases, a visual word is best\nrepresented with partial memberships across multiple topics. To address this,\nwe present a partial membership latent Dirichlet allocation (PM-LDA) model and\nan associated parameter estimation algorithm. This model can be useful for\nimagery where a visual word may be a mixture of multiple topics. Experimental\nresults on visual and sonar imagery show that PM-LDA can produce both crisp and\nsoft semantic image segmentations; a capability previous topic modeling methods\ndo not have.\n", "versions": [{"version": "v1", "created": "Wed, 28 Dec 2016 17:32:52 GMT"}], "update_date": "2016-12-30", "authors_parsed": [["Chen", "Chao", ""], ["Zare", "Alina", ""], ["Trinh", "Huy", ""], ["Omotara", "Gbeng", ""], ["Cobb", "J. Tory", ""], ["Lagaunne", "Timotius", ""]]}, {"id": "1612.08974", "submitter": "John Ehrlinger", "authors": "John Ehrlinger", "title": "ggRandomForests: Exploring Random Forest Survival", "comments": "39 pages, 23 figures, 4 tables. Draft working document. R package\n  vignette for ggRandomForests package\n  (https://cran.r-project.org/package=ggRandomForests). arXiv admin note: text\n  overlap with arXiv:1501.07196", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.CO stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Random forest (Leo Breiman 2001a) (RF) is a non-parametric statistical method\nrequiring no distributional assumptions on covariate relation to the response.\nRF is a robust, nonlinear technique that optimizes predictive accuracy by\nfitting an ensemble of trees to stabilize model estimates. Random survival\nforests (RSF) (Ishwaran and Kogalur 2007; Ishwaran et al. 2008) are an\nextension of Breimans RF techniques allowing efficient nonparametric analysis\nof time to event data. The randomForestSRC package (Ishwaran and Kogalur 2014)\nis a unified treatment of Breimans random forest for survival, regression and\nclassification problems.\n  Predictive accuracy makes RF an attractive alternative to parametric models,\nthough complexity and interpretability of the forest hinder wider application\nof the method. We introduce the ggRandomForests package, tools for visually\nunderstand random forest models grown in R (R Core Team 2014) with the\nrandomForestSRC package. The ggRandomForests package is structured to extract\nintermediate data objects from randomForestSRC objects and generate figures\nusing the ggplot2 (Wickham 2009) graphics package.\n  This document is structured as a tutorial for building random forest for\nsurvival with the randomForestSRC package and using the ggRandomForests package\nfor investigating how the forest is constructed. We analyse the Primary Biliary\nCirrhosis of the liver data from a clinical trial at the Mayo Clinic (Fleming\nand Harrington 1991). Our aim is to demonstrate the strength of using Random\nForest methods for both prediction and information retrieval, specifically in\ntime to event data settings.\n", "versions": [{"version": "v1", "created": "Wed, 28 Dec 2016 20:17:58 GMT"}], "update_date": "2016-12-30", "authors_parsed": [["Ehrlinger", "John", ""]]}, {"id": "1612.09007", "submitter": "Huan Song", "authors": "Huan Song, Jayaraman J. Thiagarajan, Prasanna Sattigeri, Karthikeyan\n  Natesan Ramamurthy, Andreas Spanias", "title": "A Deep Learning Approach To Multiple Kernel Fusion", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Kernel fusion is a popular and effective approach for combining multiple\nfeatures that characterize different aspects of data. Traditional approaches\nfor Multiple Kernel Learning (MKL) attempt to learn the parameters for\ncombining the kernels through sophisticated optimization procedures. In this\npaper, we propose an alternative approach that creates dense embeddings for\ndata using the kernel similarities and adopts a deep neural network\narchitecture for fusing the embeddings. In order to improve the effectiveness\nof this network, we introduce the kernel dropout regularization strategy\ncoupled with the use of an expanded set of composition kernels. Experiment\nresults on a real-world activity recognition dataset show that the proposed\narchitecture is effective in fusing kernels and achieves state-of-the-art\nperformance.\n", "versions": [{"version": "v1", "created": "Wed, 28 Dec 2016 23:43:27 GMT"}], "update_date": "2016-12-30", "authors_parsed": [["Song", "Huan", ""], ["Thiagarajan", "Jayaraman J.", ""], ["Sattigeri", "Prasanna", ""], ["Ramamurthy", "Karthikeyan Natesan", ""], ["Spanias", "Andreas", ""]]}, {"id": "1612.09022", "submitter": "Fathi Salem", "authors": "Fathi M. Salem", "title": "A Basic Recurrent Neural Network Model", "comments": "8 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NE stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a model of a basic recurrent neural network (or bRNN) that\nincludes a separate linear term with a slightly \"stable\" fixed matrix to\nguarantee bounded solutions and fast dynamic response. We formulate a state\nspace viewpoint and adapt the constrained optimization Lagrange Multiplier\n(CLM) technique and the vector Calculus of Variations (CoV) to derive the\n(stochastic) gradient descent. In this process, one avoids the commonly used\nre-application of the circular chain-rule and identifies the error\nback-propagation with the co-state backward dynamic equations. We assert that\nthis bRNN can successfully perform regression tracking of time-series.\nMoreover, the \"vanishing and exploding\" gradients are explicitly quantified and\nexplained through the co-state dynamics and the update laws. The adapted CoV\nframework, in addition, can correctly and principally integrate new loss\nfunctions in the network on any variable and for varied goals, e.g., for\nsupervised learning on the outputs and unsupervised learning on the internal\n(hidden) states.\n", "versions": [{"version": "v1", "created": "Thu, 29 Dec 2016 02:10:50 GMT"}], "update_date": "2016-12-30", "authors_parsed": [["Salem", "Fathi M.", ""]]}, {"id": "1612.09034", "submitter": "Shiqian Ma", "authors": "Shixiang Chen, Shiqian Ma, Wei Liu", "title": "Geometric descent method for convex composite minimization", "comments": "Updated numerical results", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.OC cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we extend the geometric descent method recently proposed by\nBubeck, Lee and Singh to tackle nonsmooth and strongly convex composite\nproblems. We prove that our proposed algorithm, dubbed geometric proximal\ngradient method (GeoPG), converges with a linear rate $(1-1/\\sqrt{\\kappa})$ and\nthus achieves the optimal rate among first-order methods, where $\\kappa$ is the\ncondition number of the problem. Numerical results on linear regression and\nlogistic regression with elastic net regularization show that GeoPG compares\nfavorably with Nesterov's accelerated proximal gradient method, especially when\nthe problem is ill-conditioned.\n", "versions": [{"version": "v1", "created": "Thu, 29 Dec 2016 04:25:28 GMT"}, {"version": "v2", "created": "Wed, 4 Jan 2017 02:50:34 GMT"}, {"version": "v3", "created": "Sat, 21 Jan 2017 02:53:48 GMT"}, {"version": "v4", "created": "Tue, 30 May 2017 06:20:50 GMT"}], "update_date": "2017-05-31", "authors_parsed": [["Chen", "Shixiang", ""], ["Ma", "Shiqian", ""], ["Liu", "Wei", ""]]}, {"id": "1612.09076", "submitter": "Yunlong Liu", "authors": "Yunlong Liu and Hexing Zhu", "title": "Selecting Bases in Spectral learning of Predictive State Representations\n  via Model Entropy", "comments": "9 papges, 4 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Predictive State Representations (PSRs) are powerful techniques for modelling\ndynamical systems, which represent a state as a vector of predictions about\nfuture observable events (tests). In PSRs, one of the fundamental problems is\nthe learning of the PSR model of the underlying system. Recently, spectral\nmethods have been successfully used to address this issue by treating the\nlearning problem as the task of computing an singular value decomposition (SVD)\nover a submatrix of a special type of matrix called the Hankel matrix. Under\nthe assumptions that the rows and columns of the submatrix of the Hankel Matrix\nare sufficient~(which usually means a very large number of rows and columns,\nand almost fails in practice) and the entries of the matrix can be estimated\naccurately, it has been proven that the spectral approach for learning PSRs is\nstatistically consistent and the learned parameters can converge to the true\nparameters. However, in practice, due to the limit of the computation ability,\nonly a finite set of rows or columns can be chosen to be used for the spectral\nlearning. While different sets of columns usually lead to variant accuracy of\nthe learned model, in this paper, we propose an approach for selecting the set\nof columns, namely basis selection, by adopting a concept of model entropy to\nmeasure the accuracy of the learned model. Experimental results are shown to\ndemonstrate the effectiveness of the proposed approach.\n", "versions": [{"version": "v1", "created": "Thu, 29 Dec 2016 08:53:20 GMT"}], "update_date": "2016-12-30", "authors_parsed": [["Liu", "Yunlong", ""], ["Zhu", "Hexing", ""]]}, {"id": "1612.09158", "submitter": "Gianluigi Pillonetto Dr.", "authors": "Gianluigi Pillonetto", "title": "The interplay between system identification and machine learning", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.SY cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Learning from examples is one of the key problems in science and engineering.\nIt deals with function reconstruction from a finite set of direct and noisy\nsamples. Regularization in reproducing kernel Hilbert spaces (RKHSs) is widely\nused to solve this task and includes powerful estimators such as regularization\nnetworks. Recent achievements include the proof of the statistical consistency\nof these kernel- based approaches. Parallel to this, many different system\nidentification techniques have been developed but the interaction with machine\nlearning does not appear so strong yet. One reason is that the RKHSs usually\nemployed in machine learning do not embed the information available on dynamic\nsystems, e.g. BIBO stability. In addition, in system identification the\nindependent data assumptions routinely adopted in machine learning are never\nsatisfied in practice. This paper provides new results which strengthen the\nconnection between system identification and machine learning. Our starting\npoint is the introduction of RKHSs of dynamic systems. They contain functionals\nover spaces defined by system inputs and allow to interpret system\nidentification as learning from examples. In both linear and nonlinear\nsettings, it is shown that this perspective permits to derive in a relatively\nsimple way conditions on RKHS stability (i.e. the property of containing only\nBIBO stable systems or predictors), also facilitating the design of new kernels\nfor system identification. Furthermore, we prove the convergence of the\nregularized estimator to the optimal predictor under conditions typical of\ndynamic systems.\n", "versions": [{"version": "v1", "created": "Thu, 29 Dec 2016 14:32:51 GMT"}], "update_date": "2016-12-30", "authors_parsed": [["Pillonetto", "Gianluigi", ""]]}, {"id": "1612.09162", "submitter": "Christian A. Naesseth", "authors": "Christian A. Naesseth and Fredrik Lindsten and Thomas B. Sch\\\"on", "title": "High-dimensional Filtering using Nested Sequential Monte Carlo", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.CO stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Sequential Monte Carlo (SMC) methods comprise one of the most successful\napproaches to approximate Bayesian filtering. However, SMC without good\nproposal distributions struggle in high dimensions. We propose nested\nsequential Monte Carlo (NSMC), a methodology that generalises the SMC framework\nby requiring only approximate, properly weighted, samples from the SMC proposal\ndistribution, while still resulting in a correct SMC algorithm. This way we can\nexactly approximate the locally optimal proposal, and extend the class of\nmodels for which we can perform efficient inference using SMC. We show improved\naccuracy over other state-of-the-art methods on several spatio-temporal state\nspace models.\n", "versions": [{"version": "v1", "created": "Thu, 29 Dec 2016 14:58:55 GMT"}], "update_date": "2016-12-30", "authors_parsed": [["Naesseth", "Christian A.", ""], ["Lindsten", "Fredrik", ""], ["Sch\u00f6n", "Thomas B.", ""]]}, {"id": "1612.09199", "submitter": "Mahajabin Rahman", "authors": "Mahajabin Rahman, Davi Geiger", "title": "Quantum Clustering and Gaussian Mixtures", "comments": "16 pages, 13 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The mixture of Gaussian distributions, a soft version of k-means , is\nconsidered a state-of-the-art clustering algorithm. It is widely used in\ncomputer vision for selecting classes, e.g., color, texture, and shapes. In\nthis algorithm, each class is described by a Gaussian distribution, defined by\nits mean and covariance. The data is described by a weighted sum of these\nGaussian distributions. We propose a new method, inspired by quantum\ninterference in physics. Instead of modeling each class distribution directly,\nwe model a class wave function such that its magnitude square is the class\nGaussian distribution. We then mix the class wave functions to create the\nmixture wave function. The final mixture distribution is then the magnitude\nsquare of the mixture wave function. As a result, we observe the quantum class\ninterference phenomena, not present in the Gaussian mixture model. We show that\nthe quantum method outperforms the Gaussian mixture method in every aspect of\nthe estimations. It provides more accurate estimations of all distribution\nparameters, with much less fluctuations, and it is also more robust to data\ndeformations from the Gaussian assumptions. We illustrate our method for color\nsegmentation as an example application.\n", "versions": [{"version": "v1", "created": "Thu, 29 Dec 2016 16:55:49 GMT"}], "update_date": "2016-12-30", "authors_parsed": [["Rahman", "Mahajabin", ""], ["Geiger", "Davi", ""]]}, {"id": "1612.09259", "submitter": "Austin Benson", "authors": "Ashwin Paranjape, Austin R. Benson, Jure Leskovec", "title": "Motifs in Temporal Networks", "comments": null, "journal-ref": "Proceedings of the Tenth ACM International Conference on Web\n  Search and Data Mining, 2017", "doi": "10.1145/3018661.3018731", "report-no": null, "categories": "cs.SI physics.soc-ph stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Networks are a fundamental tool for modeling complex systems in a variety of\ndomains including social and communication networks as well as biology and\nneuroscience. Small subgraph patterns in networks, called network motifs, are\ncrucial to understanding the structure and function of these systems. However,\nthe role of network motifs in temporal networks, which contain many timestamped\nlinks between the nodes, is not yet well understood.\n  Here we develop a notion of a temporal network motif as an elementary unit of\ntemporal networks and provide a general methodology for counting such motifs.\nWe define temporal network motifs as induced subgraphs on sequences of temporal\nedges, design fast algorithms for counting temporal motifs, and prove their\nruntime complexity. Our fast algorithms achieve up to 56.5x speedup compared to\na baseline method. Furthermore, we use our algorithms to count temporal motifs\nin a variety of networks. Results show that networks from different domains\nhave significantly different motif counts, whereas networks from the same\ndomain tend to have similar motif counts. We also find that different motifs\noccur at different time scales, which provides further insights into structure\nand function of temporal networks.\n", "versions": [{"version": "v1", "created": "Thu, 29 Dec 2016 19:41:06 GMT"}], "update_date": "2018-01-08", "authors_parsed": [["Paranjape", "Ashwin", ""], ["Benson", "Austin R.", ""], ["Leskovec", "Jure", ""]]}, {"id": "1612.09283", "submitter": "Ping Li", "authors": "Ping Li", "title": "Generalized Intersection Kernel", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Following the very recent line of work on the ``generalized min-max'' (GMM)\nkernel, this study proposes the ``generalized intersection'' (GInt) kernel and\nthe related ``normalized generalized min-max'' (NGMM) kernel. In computer\nvision, the (histogram) intersection kernel has been popular, and the GInt\nkernel generalizes it to data which can have both negative and positive\nentries. Through an extensive empirical classification study on 40 datasets\nfrom the UCI repository, we are able to show that this (tuning-free) GInt\nkernel performs fairly well.\n  The empirical results also demonstrate that the NGMM kernel typically\noutperforms the GInt kernel. Interestingly, the NGMM kernel has another\ninterpretation --- it is the ``asymmetrically transformed'' version of the GInt\nkernel, based on the idea of ``asymmetric hashing''. Just like the GMM kernel,\nthe NGMM kernel can be efficiently linearized through (e.g.,) generalized\nconsistent weighted sampling (GCWS), as empirically validated in our study.\nOwing to the discrete nature of hashed values, it also provides a scheme for\napproximate near neighbor search.\n", "versions": [{"version": "v1", "created": "Thu, 29 Dec 2016 20:40:52 GMT"}], "update_date": "2016-12-30", "authors_parsed": [["Li", "Ping", ""]]}, {"id": "1612.09296", "submitter": "Tuo Zhao", "authors": "Xingguo Li, Junwei Lu, Raman Arora, Jarvis Haupt, Han Liu, Zhaoran\n  Wang, Tuo Zhao", "title": "Symmetry, Saddle Points, and Global Optimization Landscape of Nonconvex\n  Matrix Factorization", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG math.OC stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a general theory for studying the \\xl{landscape} of nonconvex\n\\xl{optimization} with underlying symmetric structures \\tz{for a class of\nmachine learning problems (e.g., low-rank matrix factorization, phase\nretrieval, and deep linear neural networks)}. In specific, we characterize the\nlocations of stationary points and the null space of Hessian matrices \\xl{of\nthe objective function} via the lens of invariant groups\\removed{for associated\noptimization problems, including low-rank matrix factorization, phase\nretrieval, and deep linear neural networks}. As a major motivating example, we\napply the proposed general theory to characterize the global \\xl{landscape} of\nthe \\xl{nonconvex optimization in} low-rank matrix factorization problem. In\nparticular, we illustrate how the rotational symmetry group gives rise to\ninfinitely many nonisolated strict saddle points and equivalent global minima\nof the objective function. By explicitly identifying all stationary points, we\ndivide the entire parameter space into three regions: ($\\cR_1$) the region\ncontaining the neighborhoods of all strict saddle points, where the objective\nhas negative curvatures; ($\\cR_2$) the region containing neighborhoods of all\nglobal minima, where the objective enjoys strong convexity along certain\ndirections; and ($\\cR_3$) the complement of the above regions, where the\ngradient has sufficiently large magnitudes. We further extend our result to the\nmatrix sensing problem. Such global landscape implies strong global convergence\nguarantees for popular iterative algorithms with arbitrary initial solutions.\n", "versions": [{"version": "v1", "created": "Thu, 29 Dec 2016 20:57:19 GMT"}, {"version": "v2", "created": "Sun, 1 Jan 2017 18:14:33 GMT"}, {"version": "v3", "created": "Sat, 20 Jan 2018 02:45:55 GMT"}], "update_date": "2018-01-23", "authors_parsed": [["Li", "Xingguo", ""], ["Lu", "Junwei", ""], ["Arora", "Raman", ""], ["Haupt", "Jarvis", ""], ["Liu", "Han", ""], ["Wang", "Zhaoran", ""], ["Zhao", "Tuo", ""]]}, {"id": "1612.09297", "submitter": "Quanquan Gu", "authors": "Pan Xu and Lu Tian and Quanquan Gu", "title": "Communication-efficient Distributed Estimation and Inference for\n  Transelliptical Graphical Models", "comments": "42 pages, 3 figures, 4 tables. This work has been presented at the\n  ENAR 2016 Spring Meeting on March 8, 2016", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose communication-efficient distributed estimation and inference\nmethods for the transelliptical graphical model, a semiparametric extension of\nthe elliptical distribution in the high dimensional regime. In detail, the\nproposed method distributes the $d$-dimensional data of size $N$ generated from\na transelliptical graphical model into $m$ worker machines, and estimates the\nlatent precision matrix on each worker machine based on the data of size\n$n=N/m$. It then debiases the local estimators on the worker machines and send\nthem back to the master machine. Finally, on the master machine, it aggregates\nthe debiased local estimators by averaging and hard thresholding. We show that\nthe aggregated estimator attains the same statistical rate as the centralized\nestimator based on all the data, provided that the number of machines satisfies\n$m \\lesssim \\min\\{N\\log d/d,\\sqrt{N/(s^2\\log d)}\\}$, where $s$ is the maximum\nnumber of nonzero entries in each column of the latent precision matrix. It is\nworth noting that our algorithm and theory can be directly applied to Gaussian\ngraphical models, Gaussian copula graphical models and elliptical graphical\nmodels, since they are all special cases of transelliptical graphical models.\nThorough experiments on synthetic data back up our theory.\n", "versions": [{"version": "v1", "created": "Thu, 29 Dec 2016 20:57:32 GMT"}], "update_date": "2016-12-30", "authors_parsed": [["Xu", "Pan", ""], ["Tian", "Lu", ""], ["Gu", "Quanquan", ""]]}, {"id": "1612.09328", "submitter": "Hongyuan Mei", "authors": "Hongyuan Mei and Jason Eisner", "title": "The Neural Hawkes Process: A Neurally Self-Modulating Multivariate Point\n  Process", "comments": "NIPS 2017 camera-ready. New experiments including intensity\n  prediction evaluation, sensitivity to # of parameters, training speed\n  analysis. Results updated to use final test data instead of devtest. Improved\n  exposition, especially of continuous-time LSTM and thinning algorithm", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Many events occur in the world. Some event types are stochastically excited\nor inhibited---in the sense of having their probabilities elevated or\ndecreased---by patterns in the sequence of previous events. Discovering such\npatterns can help us predict which type of event will happen next and when. We\nmodel streams of discrete events in continuous time, by constructing a neurally\nself-modulating multivariate point process in which the intensities of multiple\nevent types evolve according to a novel continuous-time LSTM. This generative\nmodel allows past events to influence the future in complex and realistic ways,\nby conditioning future event intensities on the hidden state of a recurrent\nneural network that has consumed the stream of past events. Our model has\ndesirable qualitative properties. It achieves competitive likelihood and\npredictive accuracy on real and synthetic datasets, including under\nmissing-data conditions.\n", "versions": [{"version": "v1", "created": "Thu, 29 Dec 2016 22:02:53 GMT"}, {"version": "v2", "created": "Tue, 23 May 2017 04:31:56 GMT"}, {"version": "v3", "created": "Tue, 21 Nov 2017 16:04:21 GMT"}], "update_date": "2017-11-22", "authors_parsed": [["Mei", "Hongyuan", ""], ["Eisner", "Jason", ""]]}, {"id": "1612.09413", "submitter": "Mingyuan Zhou", "authors": "Quan Zhang and Mingyuan Zhou", "title": "Permuted and Augmented Stick-Breaking Bayesian Multinomial Regression", "comments": null, "journal-ref": "Journal of Machine Learning Research, vol. 18, pp. 1-33, 2018", "doi": null, "report-no": null, "categories": "stat.ME stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  To model categorical response variables given their covariates, we propose a\npermuted and augmented stick-breaking (paSB) construction that one-to-one maps\nthe observed categories to randomly permuted latent sticks. This new\nconstruction transforms multinomial regression into regression analysis of\nstick-specific binary random variables that are mutually independent given\ntheir covariate-dependent stick success probabilities, which are parameterized\nby the regression coefficients of their corresponding categories. The paSB\nconstruction allows transforming an arbitrary cross-entropy-loss binary\nclassifier into a Bayesian multinomial one. Specifically, we parameterize the\nnegative logarithms of the stick failure probabilities with a family of\ncovariate-dependent softplus functions to construct nonparametric Bayesian\nmultinomial softplus regression, and transform Bayesian support vector machine\n(SVM) into Bayesian multinomial SVM. These Bayesian multinomial regression\nmodels are not only capable of providing probability estimates, quantifying\nuncertainty, increasing robustness, and producing nonlinear classification\ndecision boundaries, but also amenable to posterior simulation. Example results\ndemonstrate their attractive properties and performance.\n", "versions": [{"version": "v1", "created": "Fri, 30 Dec 2016 07:54:49 GMT"}, {"version": "v2", "created": "Mon, 2 Jan 2017 23:45:04 GMT"}, {"version": "v3", "created": "Tue, 19 Jun 2018 07:00:49 GMT"}], "update_date": "2018-06-20", "authors_parsed": [["Zhang", "Quan", ""], ["Zhou", "Mingyuan", ""]]}, {"id": "1612.09465", "submitter": "Hugo Penedones", "authors": "Timothy A. Mann and Hugo Penedones and Shie Mannor and Todd Hester", "title": "Adaptive Lambda Least-Squares Temporal Difference Learning", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Temporal Difference learning or TD($\\lambda$) is a fundamental algorithm in\nthe field of reinforcement learning. However, setting TD's $\\lambda$ parameter,\nwhich controls the timescale of TD updates, is generally left up to the\npractitioner. We formalize the $\\lambda$ selection problem as a bias-variance\ntrade-off where the solution is the value of $\\lambda$ that leads to the\nsmallest Mean Squared Value Error (MSVE). To solve this trade-off we suggest\napplying Leave-One-Trajectory-Out Cross-Validation (LOTO-CV) to search the\nspace of $\\lambda$ values. Unfortunately, this approach is too computationally\nexpensive for most practical applications. For Least Squares TD (LSTD) we show\nthat LOTO-CV can be implemented efficiently to automatically tune $\\lambda$ and\napply function optimization methods to efficiently search the space of\n$\\lambda$ values. The resulting algorithm, ALLSTD, is parameter free and our\nexperiments demonstrate that ALLSTD is significantly computationally faster\nthan the na\\\"{i}ve LOTO-CV implementation while achieving similar performance.\n", "versions": [{"version": "v1", "created": "Fri, 30 Dec 2016 11:51:14 GMT"}], "update_date": "2017-01-02", "authors_parsed": [["Mann", "Timothy A.", ""], ["Penedones", "Hugo", ""], ["Mannor", "Shie", ""], ["Hester", "Todd", ""]]}, {"id": "1612.09466", "submitter": "Xiao-Feng Gong", "authors": "Xiao-Feng Gong, Qiu-Hua Lin, Feng-Yu Cong, Lieven De Lathauwer", "title": "Double Coupled Canonical Polyadic Decomposition for Joint Blind Source\n  Separation", "comments": "Accepted by IEEE Transactions on Signal Processing", "journal-ref": null, "doi": "10.1109/TSP.2018.2830317", "report-no": null, "categories": "stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Joint blind source separation (J-BSS) is an emerging data-driven technique\nfor multi-set data-fusion. In this paper, J-BSS is addressed from a tensorial\nperspective. We show how, by using second-order multi-set statistics in J-BSS,\na specific double coupled canonical polyadic decomposition (DC-CPD) problem can\nbe formulated. We propose an algebraic DC-CPD algorithm based on a coupled\nrank-1 detection mapping. This algorithm converts a possibly underdetermined\nDC-CPD to a set of overdetermined CPDs. The latter can be solved algebraically\nvia a generalized eigenvalue decomposition based scheme. Therefore, this\nalgorithm is deterministic and returns the exact solution in the noiseless\ncase. In the noisy case, it can be used to effectively initialize optimization\nbased DC-CPD algorithms. In addition, we obtain the determini- stic and generic\nuniqueness conditions for DC-CPD, which are shown to be more relaxed than their\nCPD counterpart. Experiment results are given to illustrate the superiority of\nDC-CPD over standard CPD based BSS methods and several existing J-BSS methods,\nwith regards to uniqueness and accuracy.\n", "versions": [{"version": "v1", "created": "Fri, 30 Dec 2016 11:57:31 GMT"}, {"version": "v2", "created": "Wed, 7 Jun 2017 06:28:19 GMT"}, {"version": "v3", "created": "Thu, 25 Jan 2018 07:16:26 GMT"}, {"version": "v4", "created": "Sat, 28 Apr 2018 01:59:28 GMT"}], "update_date": "2018-05-01", "authors_parsed": [["Gong", "Xiao-Feng", ""], ["Lin", "Qiu-Hua", ""], ["Cong", "Feng-Yu", ""], ["De Lathauwer", "Lieven", ""]]}, {"id": "1612.09506", "submitter": "Mundher Al-Shabi", "authors": "Tee Connie, Mundher Al-Shabi, Michael Goh", "title": "Smart Content Recognition from Images Using a Mixture of Convolutional\n  Neural Networks", "comments": "To be published in LNEE, Code: github.com/mundher/NSFW", "journal-ref": null, "doi": "10.1007/978-981-10-6451-7_2", "report-no": null, "categories": "stat.ML cs.CV cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  With rapid development of the Internet, web contents become huge. Most of the\nwebsites are publicly available, and anyone can access the contents from\nanywhere such as workplace, home and even schools. Nevertheless, not all the\nweb contents are appropriate for all users, especially children. An example of\nthese contents is pornography images which should be restricted to certain age\ngroup. Besides, these images are not safe for work (NSFW) in which employees\nshould not be seen accessing such contents during work. Recently, convolutional\nneural networks have been successfully applied to many computer vision\nproblems. Inspired by these successes, we propose a mixture of convolutional\nneural networks for adult content recognition. Unlike other works, our method\nis formulated on a weighted sum of multiple deep neural network models. The\nweights of each CNN models are expressed as a linear regression problem learned\nusing Ordinary Least Squares (OLS). Experimental results demonstrate that the\nproposed model outperforms both single CNN model and the average sum of CNN\nmodels in adult content recognition.\n", "versions": [{"version": "v1", "created": "Fri, 30 Dec 2016 15:18:39 GMT"}, {"version": "v2", "created": "Fri, 14 Jul 2017 09:03:57 GMT"}], "update_date": "2019-04-15", "authors_parsed": [["Connie", "Tee", ""], ["Al-Shabi", "Mundher", ""], ["Goh", "Michael", ""]]}, {"id": "1612.09596", "submitter": "Matt Taddy", "authors": "Jason Hartford, Greg Lewis, Kevin Leyton-Brown, Matt Taddy", "title": "Counterfactual Prediction with Deep Instrumental Variables Networks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We are in the middle of a remarkable rise in the use and capability of\nartificial intelligence. Much of this growth has been fueled by the success of\ndeep learning architectures: models that map from observables to outputs via\nmultiple layers of latent representations. These deep learning algorithms are\neffective tools for unstructured prediction, and they can be combined in AI\nsystems to solve complex automated reasoning problems. This paper provides a\nrecipe for combining ML algorithms to solve for causal effects in the presence\nof instrumental variables -- sources of treatment randomization that are\nconditionally independent from the response. We show that a flexible IV\nspecification resolves into two prediction tasks that can be solved with deep\nneural nets: a first-stage network for treatment prediction and a second-stage\nnetwork whose loss function involves integration over the conditional treatment\ndistribution. This Deep IV framework imposes some specific structure on the\nstochastic gradient descent routine used for training, but it is general enough\nthat we can take advantage of off-the-shelf ML capabilities and avoid extensive\nalgorithm customization. We outline how to obtain out-of-sample causal\nvalidation in order to avoid over-fit. We also introduce schemes for both\nBayesian and frequentist inference: the former via a novel adaptation of\ndropout training, and the latter via a data splitting routine.\n", "versions": [{"version": "v1", "created": "Fri, 30 Dec 2016 20:56:41 GMT"}], "update_date": "2017-01-02", "authors_parsed": [["Hartford", "Jason", ""], ["Lewis", "Greg", ""], ["Leyton-Brown", "Kevin", ""], ["Taddy", "Matt", ""]]}]