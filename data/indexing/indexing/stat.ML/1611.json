[{"id": "1611.00035", "submitter": "Thomas Powers", "authors": "Scott Wisdom, Thomas Powers, John R. Hershey, Jonathan Le Roux, and\n  Les Atlas", "title": "Full-Capacity Unitary Recurrent Neural Networks", "comments": "9 pages, to appear in NIPS", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recurrent neural networks are powerful models for processing sequential data,\nbut they are generally plagued by vanishing and exploding gradient problems.\nUnitary recurrent neural networks (uRNNs), which use unitary recurrence\nmatrices, have recently been proposed as a means to avoid these issues.\nHowever, in previous experiments, the recurrence matrices were restricted to be\na product of parameterized unitary matrices, and an open question remains: when\ndoes such a parameterization fail to represent all unitary matrices, and how\ndoes this restricted representational capacity limit what can be learned? To\naddress this question, we propose full-capacity uRNNs that optimize their\nrecurrence matrix over all unitary matrices, leading to significantly improved\nperformance over uRNNs that use a restricted-capacity recurrence matrix. Our\ncontribution consists of two main components. First, we provide a theoretical\nargument to determine if a unitary parameterization has restricted capacity.\nUsing this argument, we show that a recently proposed unitary parameterization\nhas restricted capacity for hidden state dimension greater than 7. Second, we\nshow how a complete, full-capacity unitary recurrence matrix can be optimized\nover the differentiable manifold of unitary matrices. The resulting\nmultiplicative gradient step is very simple and does not require gradient\nclipping or learning rate adaptation. We confirm the utility of our claims by\nempirically evaluating our new full-capacity uRNNs on both synthetic and\nnatural data, achieving superior performance compared to both LSTMs and the\noriginal restricted-capacity uRNNs.\n", "versions": [{"version": "v1", "created": "Mon, 31 Oct 2016 20:43:21 GMT"}], "update_date": "2016-11-02", "authors_parsed": [["Wisdom", "Scott", ""], ["Powers", "Thomas", ""], ["Hershey", "John R.", ""], ["Roux", "Jonathan Le", ""], ["Atlas", "Les", ""]]}, {"id": "1611.00058", "submitter": "Sergiy Peredriy", "authors": "Sergiy Peredriy, Deovrat Kakde, Arin Chaudhuri", "title": "Kernel Bandwidth Selection for SVDD: Peak Criterion Approach for Large\n  Data", "comments": null, "journal-ref": null, "doi": "10.1109/BigData.2017.8258344", "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Support Vector Data Description (SVDD) provides a useful approach to\nconstruct a description of multivariate data for single-class classification\nand outlier detection with various practical applications. Gaussian kernel used\nin SVDD formulation allows flexible data description defined by observations\ndesignated as support vectors. The data boundary of such description is\nnon-spherical and conforms to the geometric features of the data. By varying\nthe Gaussian kernel bandwidth parameter, the SVDD-generated boundary can be\nmade either smoother (more spherical) or tighter/jagged. The former case may\nlead to under-fitting, whereas the latter may result in overfitting. Peak\ncriterion has been proposed to select an optimal value of the kernel bandwidth\nto strike the balance between the data boundary smoothness and its ability to\ncapture the general geometric shape of the data. Peak criterion involves\ntraining SVDD at various values of the kernel bandwidth parameter. When\ntraining datasets are large, the time required to obtain the optimal value of\nthe Gaussian kernel bandwidth parameter according to Peak method can become\nprohibitively large. This paper proposes an extension of Peak method for the\ncase of large data. The proposed method gives good results when applied to\nseveral datasets. Two existing alternative methods of computing the Gaussian\nkernel bandwidth parameter (Coefficient of Variation and Distance to the\nFarthest Neighbor) were modified to allow comparison with the proposed method\non convergence. Empirical comparison demonstrates the advantage of the proposed\nmethod.\n", "versions": [{"version": "v1", "created": "Mon, 31 Oct 2016 22:04:54 GMT"}, {"version": "v2", "created": "Fri, 24 Feb 2017 20:30:59 GMT"}, {"version": "v3", "created": "Fri, 19 May 2017 19:13:20 GMT"}], "update_date": "2018-11-02", "authors_parsed": [["Peredriy", "Sergiy", ""], ["Kakde", "Deovrat", ""], ["Chaudhuri", "Arin", ""]]}, {"id": "1611.00065", "submitter": "Samuel Elder", "authors": "Sam Elder", "title": "Bayesian Adaptive Data Analysis Guarantees from Subgaussianity", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG math.PR stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The new field of adaptive data analysis seeks to provide algorithms and\nprovable guarantees for models of machine learning that allow researchers to\nreuse their data, which normally falls outside of the usual statistical\nparadigm of static data analysis. In 2014, Dwork, Feldman, Hardt, Pitassi,\nReingold and Roth introduced one potential model and proposed several solutions\nbased on differential privacy. In previous work in 2016, we described a problem\nwith this model and instead proposed a Bayesian variant, but also found that\nthe analogous Bayesian methods cannot achieve the same statistical guarantees\nas in the static case.\n  In this paper, we prove the first positive results for the Bayesian model,\nshowing that with a Dirichlet prior, the posterior mean algorithm indeed\nmatches the statistical guarantees of the static case. The main ingredient is a\nnew theorem showing that the $\\mathrm{Beta}(\\alpha,\\beta)$ distribution is\nsubgaussian with variance proxy $O(1/(\\alpha+\\beta+1))$, a concentration result\nalso of independent interest. We provide two proofs of this result: a\nprobabilistic proof utilizing a simple condition for the raw moments of a\npositive random variable and a learning-theoretic proof based on considering\nthe beta distribution as a posterior, both of which have implications to other\nrelated problems.\n", "versions": [{"version": "v1", "created": "Mon, 31 Oct 2016 22:24:49 GMT"}, {"version": "v2", "created": "Fri, 2 Dec 2016 19:58:08 GMT"}, {"version": "v3", "created": "Mon, 20 Mar 2017 20:07:55 GMT"}], "update_date": "2017-03-22", "authors_parsed": [["Elder", "Sam", ""]]}, {"id": "1611.00170", "submitter": "Simone Carlo Surace", "authors": "Simone Carlo Surace and Jean-Pascal Pfister", "title": "Online Maximum Likelihood Estimation of the Parameters of Partially\n  Observed Diffusion Processes", "comments": "15 pages, 5 figures, revised version, accepted at IEEE Transactions\n  on Automatic Control", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.OC math.PR stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We revisit the problem of estimating the parameters of a partially observed\ndiffusion process, consisting of a hidden state process and an observed\nprocess, with a continuous time parameter. The estimation is to be done online,\ni.e. the parameter estimate should be updated recursively based on the\nobservation filtration. We provide a theoretical analysis of the stochastic\ngradient ascent algorithm on the incomplete-data log-likelihood. The\nconvergence of the algorithm is proved under suitable conditions regarding the\nergodicity of the process consisting of state, filter, and tangent filter.\nAdditionally, our parameter estimation is shown numerically to have the\npotential of improving suboptimal filters, and can be applied even when the\nsystem is not identifiable due to parameter redundancies. Online parameter\nestimation is a challenging problem that is ubiquitous in fields such as\nrobotics, neuroscience, or finance in order to design adaptive filters and\noptimal controllers for unknown or changing systems. Despite this, theoretical\nanalysis of convergence is currently lacking for most of these algorithms. This\narticle sheds new light on the theory of convergence in continuous time.\n", "versions": [{"version": "v1", "created": "Tue, 1 Nov 2016 09:47:15 GMT"}, {"version": "v2", "created": "Tue, 6 Dec 2016 19:13:01 GMT"}, {"version": "v3", "created": "Tue, 19 Sep 2017 14:13:09 GMT"}, {"version": "v4", "created": "Mon, 15 Oct 2018 16:59:06 GMT"}], "update_date": "2018-10-16", "authors_parsed": [["Surace", "Simone Carlo", ""], ["Pfister", "Jean-Pascal", ""]]}, {"id": "1611.00255", "submitter": "Nathanael Perraudin N. P.", "authors": "Andreas Loukas and Nathana\\\"el Perraudin", "title": "Stationary time-vertex signal processing", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.DS stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper considers regression tasks involving high-dimensional multivariate\nprocesses whose structure is dependent on some {known} graph topology. We put\nforth a new definition of time-vertex wide-sense stationarity, or joint\nstationarity for short, that goes beyond product graphs. Joint stationarity\nhelps by reducing the estimation variance and recovery complexity. In\nparticular, for any jointly stationary process (a) one reliably learns the\ncovariance structure from as little as a single realization of the process, and\n(b) solves MMSE recovery problems, such as interpolation and denoising, in\ncomputational time nearly linear on the number of edges and timesteps.\nExperiments with three datasets suggest that joint stationarity can yield\naccuracy improvements in the recovery of high-dimensional processes evolving\nover a graph, even when the latter is only approximately known, or the process\nis not strictly stationary.\n", "versions": [{"version": "v1", "created": "Tue, 1 Nov 2016 14:56:33 GMT"}, {"version": "v2", "created": "Tue, 23 May 2017 18:35:44 GMT"}, {"version": "v3", "created": "Mon, 8 Jul 2019 12:34:27 GMT"}], "update_date": "2019-07-09", "authors_parsed": [["Loukas", "Andreas", ""], ["Perraudin", "Nathana\u00ebl", ""]]}, {"id": "1611.00261", "submitter": "Aleksander Wieczorek", "authors": "Aleksander Wieczorek and Volker Roth", "title": "Causal Compression", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a new method of discovering causal relationships in temporal data\nbased on the notion of causal compression. To this end, we adopt the Pearlian\ngraph setting and the directed information as an information theoretic tool for\nquantifying causality. We introduce chain rule for directed information and use\nit to motivate causal sparsity. We show two applications of the proposed\nmethod: causal time series segmentation which selects time points capturing the\nincoming and outgoing causal flow between time points belonging to different\nsignals, and causal bipartite graph recovery. We prove that modelling of\ncausality in the adopted set-up only requires estimating the copula density of\nthe data distribution and thus does not depend on its marginals. We evaluate\nthe method on time resolved gene expression data.\n", "versions": [{"version": "v1", "created": "Tue, 1 Nov 2016 15:08:09 GMT"}], "update_date": "2016-11-02", "authors_parsed": [["Wieczorek", "Aleksander", ""], ["Roth", "Volker", ""]]}, {"id": "1611.00303", "submitter": "Timothy O'Shea", "authors": "Timothy J. O'Shea, Nathan West, Matthew Vondal, T. Charles Clancy", "title": "Semi-Supervised Radio Signal Identification", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.IT math.IT stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Radio emitter recognition in dense multi-user environments is an important\ntool for optimizing spectrum utilization, identifying and minimizing\ninterference, and enforcing spectrum policy. Radio data is readily available\nand easy to obtain from an antenna, but labeled and curated data is often\nscarce making supervised learning strategies difficult and time consuming in\npractice. We demonstrate that semi-supervised learning techniques can be used\nto scale learning beyond supervised datasets, allowing for discerning and\nrecalling new radio signals by using sparse signal representations based on\nboth unsupervised and supervised methods for nonlinear feature learning and\nclustering methods.\n", "versions": [{"version": "v1", "created": "Tue, 1 Nov 2016 17:21:50 GMT"}, {"version": "v2", "created": "Tue, 17 Jan 2017 18:23:49 GMT"}], "update_date": "2017-01-18", "authors_parsed": [["O'Shea", "Timothy J.", ""], ["West", "Nathan", ""], ["Vondal", "Matthew", ""], ["Clancy", "T. Charles", ""]]}, {"id": "1611.00326", "submitter": "Pengfei Sun", "authors": "Pengfei Sun and Jun Qin", "title": "Enhanced Factored Three-Way Restricted Boltzmann Machines for Speech\n  Detection", "comments": "8 pages, Pattern Recognition Letter 2016", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.SD cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this letter, we propose enhanced factored three way restricted Boltzmann\nmachines (EFTW-RBMs) for speech detection. The proposed model incorporates\nconditional feature learning by multiplying the dynamical state of the third\nunit, which allows a modulation over the visible-hidden node pairs. Instead of\nstacking previous frames of speech as the third unit in a recursive manner, the\ncorrelation related weighting coefficients are assigned to the contextual\nneighboring frames. Specifically, a threshold function is designed to capture\nthe long-term features and blend the globally stored speech structure. A\nfactored low rank approximation is introduced to reduce the parameters of the\nthree-dimensional interaction tensor, on which non-negative constraint is\nimposed to address the sparsity characteristic. The validations through the\narea-under-ROC-curve (AUC) and signal distortion ratio (SDR) show that our\napproach outperforms several existing 1D and 2D (i.e., time and time-frequency\ndomain) speech detection algorithms in various noisy environments.\n", "versions": [{"version": "v1", "created": "Tue, 1 Nov 2016 18:38:12 GMT"}, {"version": "v2", "created": "Fri, 27 Jan 2017 06:01:20 GMT"}, {"version": "v3", "created": "Thu, 20 Apr 2017 18:43:29 GMT"}], "update_date": "2017-04-24", "authors_parsed": [["Sun", "Pengfei", ""], ["Qin", "Jun", ""]]}, {"id": "1611.00328", "submitter": "Adji Bousso Dieng", "authors": "Adji B. Dieng, Dustin Tran, Rajesh Ranganath, John Paisley, David M.\n  Blei", "title": "Variational Inference via $\\chi$-Upper Bound Minimization", "comments": "Neural Information Processing Systems, 2017", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG stat.CO stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Variational inference (VI) is widely used as an efficient alternative to\nMarkov chain Monte Carlo. It posits a family of approximating distributions $q$\nand finds the closest member to the exact posterior $p$. Closeness is usually\nmeasured via a divergence $D(q || p)$ from $q$ to $p$. While successful, this\napproach also has problems. Notably, it typically leads to underestimation of\nthe posterior variance. In this paper we propose CHIVI, a black-box variational\ninference algorithm that minimizes $D_{\\chi}(p || q)$, the $\\chi$-divergence\nfrom $p$ to $q$. CHIVI minimizes an upper bound of the model evidence, which we\nterm the $\\chi$ upper bound (CUBO). Minimizing the CUBO leads to improved\nposterior uncertainty, and it can also be used with the classical VI lower\nbound (ELBO) to provide a sandwich estimate of the model evidence. We study\nCHIVI on three models: probit regression, Gaussian process classification, and\na Cox process model of basketball plays. When compared to expectation\npropagation and classical VI, CHIVI produces better error rates and more\naccurate estimates of posterior variance.\n", "versions": [{"version": "v1", "created": "Tue, 1 Nov 2016 18:40:23 GMT"}, {"version": "v2", "created": "Mon, 27 Feb 2017 03:00:03 GMT"}, {"version": "v3", "created": "Mon, 6 Nov 2017 00:29:21 GMT"}, {"version": "v4", "created": "Sun, 12 Nov 2017 19:00:57 GMT"}], "update_date": "2017-11-15", "authors_parsed": [["Dieng", "Adji B.", ""], ["Tran", "Dustin", ""], ["Ranganath", "Rajesh", ""], ["Paisley", "John", ""], ["Blei", "David M.", ""]]}, {"id": "1611.00336", "submitter": "Andrew Wilson", "authors": "Andrew Gordon Wilson, Zhiting Hu, Ruslan Salakhutdinov, Eric P. Xing", "title": "Stochastic Variational Deep Kernel Learning", "comments": "13 pages, 6 tables, 3 figures. Appearing in NIPS 2016", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Deep kernel learning combines the non-parametric flexibility of kernel\nmethods with the inductive biases of deep learning architectures. We propose a\nnovel deep kernel learning model and stochastic variational inference procedure\nwhich generalizes deep kernel learning approaches to enable classification,\nmulti-task learning, additive covariance structures, and stochastic gradient\ntraining. Specifically, we apply additive base kernels to subsets of output\nfeatures from deep neural architectures, and jointly learn the parameters of\nthe base kernels and deep network through a Gaussian process marginal\nlikelihood objective. Within this framework, we derive an efficient form of\nstochastic variational inference which leverages local kernel interpolation,\ninducing points, and structure exploiting algebra. We show improved performance\nover stand alone deep networks, SVMs, and state of the art scalable Gaussian\nprocesses on several classification benchmarks, including an airline delay\ndataset containing 6 million training points, CIFAR, and ImageNet.\n", "versions": [{"version": "v1", "created": "Tue, 1 Nov 2016 19:04:47 GMT"}, {"version": "v2", "created": "Wed, 2 Nov 2016 18:06:16 GMT"}], "update_date": "2016-11-03", "authors_parsed": [["Wilson", "Andrew Gordon", ""], ["Hu", "Zhiting", ""], ["Salakhutdinov", "Ruslan", ""], ["Xing", "Eric P.", ""]]}, {"id": "1611.00340", "submitter": "Mijung Park", "authors": "Mijung Park, James Foulds, Kamalika Chaudhuri, Max Welling", "title": "Variational Bayes In Private Settings (VIPS)", "comments": "The previous version of this paper had an error in the composition\n  method we used. This version fixed that error", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.CR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Many applications of Bayesian data analysis involve sensitive information,\nmotivating methods which ensure that privacy is protected. We introduce a\ngeneral privacy-preserving framework for Variational Bayes (VB), a widely used\noptimization-based Bayesian inference method. Our framework respects\ndifferential privacy, the gold-standard privacy criterion, and encompasses a\nlarge class of probabilistic models, called the Conjugate Exponential (CE)\nfamily. We observe that we can straightforwardly privatise VB's approximate\nposterior distributions for models in the CE family, by perturbing the expected\nsufficient statistics of the complete-data likelihood. For a broadly-used class\nof non-CE models, those with binomial likelihoods, we show how to bring such\nmodels into the CE family, such that inferences in the modified model resemble\nthe private variational Bayes algorithm as closely as possible, using the\nPolya-Gamma data augmentation scheme. The iterative nature of variational Bayes\npresents a further challenge since iterations increase the amount of noise\nneeded. We overcome this by combining: (1) an improved composition method for\ndifferential privacy, called the moments accountant, which provides a tight\nbound on the privacy cost of multiple VB iterations and thus significantly\ndecreases the amount of additive noise; and (2) the privacy amplification\neffect of subsampling mini-batches from large-scale data in stochastic\nlearning. We empirically demonstrate the effectiveness of our method in CE and\nnon-CE models including latent Dirichlet allocation, Bayesian logistic\nregression, and sigmoid belief networks, evaluated on real-world datasets.\n", "versions": [{"version": "v1", "created": "Tue, 1 Nov 2016 19:19:49 GMT"}, {"version": "v2", "created": "Mon, 28 Nov 2016 21:09:16 GMT"}, {"version": "v3", "created": "Wed, 21 Dec 2016 23:16:31 GMT"}, {"version": "v4", "created": "Tue, 6 Mar 2018 20:52:01 GMT"}, {"version": "v5", "created": "Mon, 3 Dec 2018 20:32:56 GMT"}], "update_date": "2018-12-05", "authors_parsed": [["Park", "Mijung", ""], ["Foulds", "James", ""], ["Chaudhuri", "Kamalika", ""], ["Welling", "Max", ""]]}, {"id": "1611.00350", "submitter": "Justin Khim", "authors": "Justin Khim, Varun Jog, Po-Ling Loh", "title": "Adversarial Influence Maximization", "comments": "30 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.SI cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider the problem of influence maximization in fixed networks for\ncontagion models in an adversarial setting. The goal is to select an optimal\nset of nodes to seed the influence process, such that the number of influenced\nnodes at the conclusion of the campaign is as large as possible. We formulate\nthe problem as a repeated game between a player and adversary, where the\nadversary specifies the edges along which the contagion may spread, and the\nplayer chooses sets of nodes to influence in an online fashion. We establish\nupper and lower bounds on the minimax pseudo-regret in both undirected and\ndirected networks.\n", "versions": [{"version": "v1", "created": "Tue, 1 Nov 2016 19:46:01 GMT"}, {"version": "v2", "created": "Sat, 19 Jan 2019 16:55:50 GMT"}], "update_date": "2019-01-23", "authors_parsed": [["Khim", "Justin", ""], ["Jog", "Varun", ""], ["Loh", "Po-Ling", ""]]}, {"id": "1611.00448", "submitter": "Hao Wang", "authors": "Hao Wang, Xingjian Shi, Dit-Yan Yeung", "title": "Natural-Parameter Networks: A Class of Probabilistic Neural Networks", "comments": "To appear at NIPS 2016", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI cs.CL cs.CV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Neural networks (NN) have achieved state-of-the-art performance in various\napplications. Unfortunately in applications where training data is\ninsufficient, they are often prone to overfitting. One effective way to\nalleviate this problem is to exploit the Bayesian approach by using Bayesian\nneural networks (BNN). Another shortcoming of NN is the lack of flexibility to\ncustomize different distributions for the weights and neurons according to the\ndata, as is often done in probabilistic graphical models. To address these\nproblems, we propose a class of probabilistic neural networks, dubbed\nnatural-parameter networks (NPN), as a novel and lightweight Bayesian treatment\nof NN. NPN allows the usage of arbitrary exponential-family distributions to\nmodel the weights and neurons. Different from traditional NN and BNN, NPN takes\ndistributions as input and goes through layers of transformation before\nproducing distributions to match the target output distributions. As a Bayesian\ntreatment, efficient backpropagation (BP) is performed to learn the natural\nparameters for the distributions over both the weights and neurons. The output\ndistributions of each layer, as byproducts, may be used as second-order\nrepresentations for the associated tasks such as link prediction. Experiments\non real-world datasets show that NPN can achieve state-of-the-art performance.\n", "versions": [{"version": "v1", "created": "Wed, 2 Nov 2016 02:32:05 GMT"}], "update_date": "2016-11-03", "authors_parsed": [["Wang", "Hao", ""], ["Shi", "Xingjian", ""], ["Yeung", "Dit-Yan", ""]]}, {"id": "1611.00454", "submitter": "Hao Wang", "authors": "Hao Wang, Xingjian Shi, Dit-Yan Yeung", "title": "Collaborative Recurrent Autoencoder: Recommend while Learning to Fill in\n  the Blanks", "comments": "To appear at NIPS 2016", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI cs.CL cs.CV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Hybrid methods that utilize both content and rating information are commonly\nused in many recommender systems. However, most of them use either handcrafted\nfeatures or the bag-of-words representation as a surrogate for the content\ninformation but they are neither effective nor natural enough. To address this\nproblem, we develop a collaborative recurrent autoencoder (CRAE) which is a\ndenoising recurrent autoencoder (DRAE) that models the generation of content\nsequences in the collaborative filtering (CF) setting. The model generalizes\nrecent advances in recurrent deep learning from i.i.d. input to non-i.i.d.\n(CF-based) input and provides a new denoising scheme along with a novel\nlearnable pooling scheme for the recurrent autoencoder. To do this, we first\ndevelop a hierarchical Bayesian model for the DRAE and then generalize it to\nthe CF setting. The synergy between denoising and CF enables CRAE to make\naccurate recommendations while learning to fill in the blanks in sequences.\nExperiments on real-world datasets from different domains (CiteULike and\nNetflix) show that, by jointly modeling the order-aware generation of sequences\nfor the content information and performing CF for the ratings, CRAE is able to\nsignificantly outperform the state of the art on both the recommendation task\nbased on ratings and the sequence generation task based on content information.\n", "versions": [{"version": "v1", "created": "Wed, 2 Nov 2016 02:49:44 GMT"}], "update_date": "2016-11-03", "authors_parsed": [["Wang", "Hao", ""], ["Shi", "Xingjian", ""], ["Yeung", "Dit-Yan", ""]]}, {"id": "1611.00514", "submitter": "Abbas Khosravani", "authors": "Abbas Khosravani, Cornelius Glackin, Nazim Dugan, G\\'erard Chollet,\n  Nigel Cannings", "title": "The Intelligent Voice 2016 Speaker Recognition System", "comments": "7 pages, 3 figures, NIST SRE 2016 Workshop", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.SD cs.CL stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper presents the Intelligent Voice (IV) system submitted to the NIST\n2016 Speaker Recognition Evaluation (SRE). The primary emphasis of SRE this\nyear was on developing speaker recognition technology which is robust for novel\nlanguages that are much more heterogeneous than those used in the current\nstate-of-the-art, using significantly less training data, that does not contain\nmeta-data from those languages. The system is based on the state-of-the-art\ni-vector/PLDA which is developed on the fixed training condition, and the\nresults are reported on the protocol defined on the development set of the\nchallenge.\n", "versions": [{"version": "v1", "created": "Wed, 2 Nov 2016 09:24:10 GMT"}], "update_date": "2016-11-03", "authors_parsed": [["Khosravani", "Abbas", ""], ["Glackin", "Cornelius", ""], ["Dugan", "Nazim", ""], ["Chollet", "G\u00e9rard", ""], ["Cannings", "Nigel", ""]]}, {"id": "1611.00544", "submitter": "Lloyd Elliott", "authors": "Lloyd T. Elliott and Yee Whye Teh", "title": "A nonparametric HMM for genetic imputation and coalescent inference", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Genetic sequence data are well described by hidden Markov models (HMMs) in\nwhich latent states correspond to clusters of similar mutation patterns. Theory\nfrom statistical genetics suggests that these HMMs are nonhomogeneous (their\ntransition probabilities vary along the chromosome) and have large support for\nself transitions. We develop a new nonparametric model of genetic sequence\ndata, based on the hierarchical Dirichlet process, which supports these self\ntransitions and nonhomogeneity. Our model provides a parameterization of the\ngenetic process that is more parsimonious than other more general nonparametric\nmodels which have previously been applied to population genetics. We provide\ntruncation-free MCMC inference for our model using a new auxiliary sampling\nscheme for Bayesian nonparametric HMMs. In a series of experiments on male X\nchromosome data from the Thousand Genomes Project and also on data simulated\nfrom a population bottleneck we show the benefits of our model over the popular\nfinite model fastPHASE, which can itself be seen as a parametric truncation of\nour model. We find that the number of HMM states found by our model is\ncorrelated with the time to the most recent common ancestor in population\nbottlenecks. This work demonstrates the flexibility of Bayesian nonparametrics\napplied to large and complex genetic data.\n", "versions": [{"version": "v1", "created": "Wed, 2 Nov 2016 10:48:04 GMT"}], "update_date": "2016-11-03", "authors_parsed": [["Elliott", "Lloyd T.", ""], ["Teh", "Yee Whye", ""]]}, {"id": "1611.00555", "submitter": "Adri\\'an P\\'erez-Suay", "authors": "Adri\\'an P\\'erez-Suay and Gustau Camps-Valls", "title": "Sensitivity Maps of the Hilbert-Schmidt Independence Criterion", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Kernel dependence measures yield accurate estimates of nonlinear relations\nbetween random variables, and they are also endorsed with solid theoretical\nproperties and convergence rates. Besides, the empirical estimates are easy to\ncompute in closed form just involving linear algebra operations. However, they\nare hampered by two important problems: the high computational cost involved,\nas two kernel matrices of the sample size have to be computed and stored, and\nthe interpretability of the measure, which remains hidden behind the implicit\nfeature map. We here address these two issues. We introduce the Sensitivity\nMaps (SMs) for the Hilbert-Schmidt independence criterion (HSIC). Sensitivity\nmaps allow us to explicitly analyze and visualize the relative relevance of\nboth examples and features on the dependence measure. We also present the\nrandomized HSIC (RHSIC) and its corresponding sensitivity maps to cope with\nlarge scale problems. We build upon the framework of random features and the\nBochner's theorem to approximate the involved kernels in the canonical HSIC.\nThe power of the RHSIC measure scales favourably with the number of samples,\nand it approximates HSIC and the sensitivity maps efficiently. Convergence\nbounds of both the measure and the sensitivity map are also provided. Our\nproposal is illustrated in synthetic examples, and challenging real problems of\ndependence estimation, feature selection, and causal inference from empirical\ndata.\n", "versions": [{"version": "v1", "created": "Wed, 2 Nov 2016 11:45:34 GMT"}], "update_date": "2016-11-03", "authors_parsed": [["P\u00e9rez-Suay", "Adri\u00e1n", ""], ["Camps-Valls", "Gustau", ""]]}, {"id": "1611.00565", "submitter": "Olga Isupova", "authors": "Olga Isupova, Danil Kuzin, Lyudmila Mihaylova", "title": "Learning Methods for Dynamic Topic Modeling in Automated Behaviour\n  Analysis", "comments": "15 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Semi-supervised and unsupervised systems provide operators with invaluable\nsupport and can tremendously reduce the operators load. In the light of the\nnecessity to process large volumes of video data and provide autonomous\ndecisions, this work proposes new learning algorithms for activity analysis in\nvideo. The activities and behaviours are described by a dynamic topic model.\nTwo novel learning algorithms based on the expectation maximisation approach\nand variational Bayes inference are proposed. Theoretical derivations of the\nposterior of model parameters are given. The designed learning algorithms are\ncompared with the Gibbs sampling inference scheme introduced earlier in the\nliterature. A detailed comparison of the learning algorithms is presented on\nreal video data. We also propose an anomaly localisation procedure, elegantly\nembedded in the topic modeling framework. The proposed framework can be applied\nto a number of areas, including transportation systems, security and\nsurveillance.\n", "versions": [{"version": "v1", "created": "Wed, 2 Nov 2016 12:08:41 GMT"}, {"version": "v2", "created": "Mon, 18 Sep 2017 20:56:40 GMT"}], "update_date": "2017-09-20", "authors_parsed": [["Isupova", "Olga", ""], ["Kuzin", "Danil", ""], ["Mihaylova", "Lyudmila", ""]]}, {"id": "1611.00683", "submitter": "Jack Raymond", "authors": "Jack Raymond and Federico Ricci-Tersenghi", "title": "Improving variational methods via pairwise linear response identities", "comments": "36 pages, 17 figures", "journal-ref": "Journal of Machine Learning Research 18(6), 1-36, 2017", "doi": null, "report-no": null, "categories": "stat.ML cond-mat.dis-nn", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Inference methods are often formulated as variational approximations: these\napproximations allow easy evaluation of statistics by marginalization or linear\nresponse, but these estimates can be inconsistent. We show that by introducing\nconstraints on covariance, one can ensure consistency of linear response with\nthe variational parameters, and in so doing inference of marginal probability\ndistributions is improved. For the Bethe approximation and its generalizations,\nimprovements are achieved with simple choices of the constraints. The\napproximations are presented as variational frameworks; iterative procedures\nrelated to message passing are provided for finding the minima.\n", "versions": [{"version": "v1", "created": "Wed, 2 Nov 2016 16:51:00 GMT"}], "update_date": "2017-04-27", "authors_parsed": [["Raymond", "Jack", ""], ["Ricci-Tersenghi", "Federico", ""]]}, {"id": "1611.00712", "submitter": "Chris J. Maddison", "authors": "Chris J. Maddison, Andriy Mnih, Yee Whye Teh", "title": "The Concrete Distribution: A Continuous Relaxation of Discrete Random\n  Variables", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The reparameterization trick enables optimizing large scale stochastic\ncomputation graphs via gradient descent. The essence of the trick is to\nrefactor each stochastic node into a differentiable function of its parameters\nand a random variable with fixed distribution. After refactoring, the gradients\nof the loss propagated by the chain rule through the graph are low variance\nunbiased estimators of the gradients of the expected loss. While many\ncontinuous random variables have such reparameterizations, discrete random\nvariables lack useful reparameterizations due to the discontinuous nature of\ndiscrete states. In this work we introduce Concrete random\nvariables---continuous relaxations of discrete random variables. The Concrete\ndistribution is a new family of distributions with closed form densities and a\nsimple reparameterization. Whenever a discrete stochastic node of a computation\ngraph can be refactored into a one-hot bit representation that is treated\ncontinuously, Concrete stochastic nodes can be used with automatic\ndifferentiation to produce low-variance biased gradients of objectives\n(including objectives that depend on the log-probability of latent stochastic\nnodes) on the corresponding discrete graph. We demonstrate the effectiveness of\nConcrete relaxations on density estimation and structured prediction tasks\nusing neural networks.\n", "versions": [{"version": "v1", "created": "Wed, 2 Nov 2016 18:25:40 GMT"}, {"version": "v2", "created": "Sun, 6 Nov 2016 23:25:23 GMT"}, {"version": "v3", "created": "Sun, 5 Mar 2017 16:59:44 GMT"}], "update_date": "2017-03-07", "authors_parsed": [["Maddison", "Chris J.", ""], ["Mnih", "Andriy", ""], ["Teh", "Yee Whye", ""]]}, {"id": "1611.00798", "submitter": "Daniel Bartz", "authors": "Daniel Bartz", "title": "Cross-validation based Nonlinear Shrinkage", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Many machine learning algorithms require precise estimates of covariance\nmatrices. The sample covariance matrix performs poorly in high-dimensional\nsettings, which has stimulated the development of alternative methods, the\nmajority based on factor models and shrinkage. Recent work of Ledoit and Wolf\nhas extended the shrinkage framework to Nonlinear Shrinkage (NLS), a more\npowerful covariance estimator based on Random Matrix Theory.\n  Our contribution shows that, contrary to claims in the literature,\ncross-validation based covariance matrix estimation (CVC) yields comparable\nperformance at strongly reduced complexity and runtime. On two real world data\nsets, we show that the CVC estimator yields superior results than competing\nshrinkage and factor based methods.\n", "versions": [{"version": "v1", "created": "Wed, 2 Nov 2016 20:49:38 GMT"}], "update_date": "2016-11-04", "authors_parsed": [["Bartz", "Daniel", ""]]}, {"id": "1611.00800", "submitter": "Andy Jinhua Ma", "authors": "Frodo Kin Sun Chan, Andy J Ma, Pong C Yuen, Terry Cheuk-Fung Yip,\n  Yee-Kit Tse, Vincent Wai-Sun Wong and Grace Lai-Hung Wong", "title": "Temporal Matrix Completion with Locally Linear Latent Factors for\n  Medical Applications", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Regular medical records are useful for medical practitioners to analyze and\nmonitor patient health status especially for those with chronic disease, but\nsuch records are usually incomplete due to unpunctuality and absence of\npatients. In order to resolve the missing data problem over time, tensor-based\nmodel is suggested for missing data imputation in recent papers because this\napproach makes use of low rank tensor assumption for highly correlated data.\nHowever, when the time intervals between records are long, the data correlation\nis not high along temporal direction and such assumption is not valid. To\naddress this problem, we propose to decompose a matrix with missing data into\nits latent factors. Then, the locally linear constraint is imposed on these\nfactors for matrix completion in this paper. By using a publicly available\ndataset and two medical datasets collected from hospital, experimental results\nshow that the proposed algorithm achieves the best performance by comparing\nwith the existing methods.\n", "versions": [{"version": "v1", "created": "Mon, 31 Oct 2016 12:02:53 GMT"}], "update_date": "2016-11-04", "authors_parsed": [["Chan", "Frodo Kin Sun", ""], ["Ma", "Andy J", ""], ["Yuen", "Pong C", ""], ["Yip", "Terry Cheuk-Fung", ""], ["Tse", "Yee-Kit", ""], ["Wong", "Vincent Wai-Sun", ""], ["Wong", "Grace Lai-Hung", ""]]}, {"id": "1611.00817", "submitter": "Nicolas Rivera Nicol\\'as Rivera", "authors": "Tamara Fern\\'andez, Nicol\\'as Rivera and Yee Whye Teh", "title": "Gaussian Processes for Survival Analysis", "comments": "To appear in NIPS 2016", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We introduce a semi-parametric Bayesian model for survival analysis. The\nmodel is centred on a parametric baseline hazard, and uses a Gaussian process\nto model variations away from it nonparametrically, as well as dependence on\ncovariates. As opposed to many other methods in survival analysis, our\nframework does not impose unnecessary constraints in the hazard rate or in the\nsurvival function. Furthermore, our model handles left, right and interval\ncensoring mechanisms common in survival analysis. We propose a MCMC algorithm\nto perform inference and an approximation scheme based on random Fourier\nfeatures to make computations faster. We report experimental results on\nsynthetic and real data, showing that our model performs better than competing\nmodels such as Cox proportional hazards, ANOVA-DDP and random survival forests.\n", "versions": [{"version": "v1", "created": "Wed, 2 Nov 2016 21:26:26 GMT"}], "update_date": "2016-11-04", "authors_parsed": [["Fern\u00e1ndez", "Tamara", ""], ["Rivera", "Nicol\u00e1s", ""], ["Teh", "Yee Whye", ""]]}, {"id": "1611.00838", "submitter": "Da Tang", "authors": "Da Tang and Tony Jebara", "title": "Initialization and Coordinate Optimization for Multi-way Matching", "comments": "Artificial Intelligence and Statistics (AISTATS), 2017", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider the problem of consistently matching multiple sets of elements to\neach other, which is a common task in fields such as computer vision. To solve\nthe underlying NP-hard objective, existing methods often relax or approximate\nit, but end up with unsatisfying empirical performance due to a misaligned\nobjective. We propose a coordinate update algorithm that directly optimizes the\ntarget objective. By using pairwise alignment information to build an\nundirected graph and initializing the permutation matrices along the edges of\nits Maximum Spanning Tree, our algorithm successfully avoids bad local optima.\nTheoretically, with high probability our algorithm guarantees an optimal\nsolution under reasonable noise assumptions. Empirically, our algorithm\nconsistently and significantly outperforms existing methods on several\nbenchmark tasks on real datasets.\n", "versions": [{"version": "v1", "created": "Wed, 2 Nov 2016 23:12:05 GMT"}, {"version": "v2", "created": "Sat, 4 Mar 2017 20:10:26 GMT"}, {"version": "v3", "created": "Wed, 8 Mar 2017 07:32:24 GMT"}, {"version": "v4", "created": "Mon, 8 Jul 2019 14:25:37 GMT"}, {"version": "v5", "created": "Thu, 18 Jul 2019 05:34:20 GMT"}], "update_date": "2019-07-19", "authors_parsed": [["Tang", "Da", ""], ["Jebara", "Tony", ""]]}, {"id": "1611.00866", "submitter": "Bin Liu Bin Liu", "authors": "Bin Liu, Zenglin Xu, Yingming Li", "title": "Tensor Decomposition via Variational Auto-Encoder", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Tensor decomposition is an important technique for capturing the high-order\ninteractions among multiway data. Multi-linear tensor composition methods, such\nas the Tucker decomposition and the CANDECOMP/PARAFAC (CP), assume that the\ncomplex interactions among objects are multi-linear, and are thus insufficient\nto represent nonlinear relationships in data. Another assumption of these\nmethods is that a predefined rank should be known. However, the rank of tensors\nis hard to estimate, especially for cases with missing values. To address these\nissues, we design a Bayesian generative model for tensor decomposition.\nDifferent from the traditional Bayesian methods, the high-order interactions of\ntensor entries are modeled with variational auto-encoder. The proposed model\ntakes advantages of Neural Networks and nonparametric Bayesian models, by\nreplacing the multi-linear product in traditional Bayesian tensor decomposition\nwith a complex nonlinear function (via Neural Networks) whose parameters can be\nlearned from data. Experimental results on synthetic data and real-world\nchemometrics tensor data have demonstrated that our new model can achieve\nsignificantly higher prediction performance than the state-of-the-art tensor\ndecomposition approaches.\n", "versions": [{"version": "v1", "created": "Thu, 3 Nov 2016 02:57:26 GMT"}], "update_date": "2016-11-04", "authors_parsed": [["Liu", "Bin", ""], ["Xu", "Zenglin", ""], ["Li", "Yingming", ""]]}, {"id": "1611.00938", "submitter": "Johann Paratte", "authors": "Johan Paratte and Lionel Martin", "title": "Fast Eigenspace Approximation using Random Signals", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We focus in this work on the estimation of the first $k$ eigenvectors of any\ngraph Laplacian using filtering of Gaussian random signals. We prove that we\nonly need $k$ such signals to be able to exactly recover as many of the\nsmallest eigenvectors, regardless of the number of nodes in the graph. In\naddition, we address key issues in implementing the theoretical concepts in\npractice using accurate approximated methods. We also propose fast algorithms\nboth for eigenspace approximation and for the determination of the $k$th\nsmallest eigenvalue $\\lambda_k$. The latter proves to be extremely efficient\nunder the assumption of locally uniform distribution of the eigenvalue over the\nspectrum. Finally, we present experiments which show the validity of our method\nin practice and compare it to state-of-the-art methods for clustering and\nvisualization both on synthetic small-scale datasets and larger real-world\nproblems of millions of nodes. We show that our method allows a better scaling\nwith the number of nodes than all previous methods while achieving an almost\nperfect reconstruction of the eigenspace formed by the first $k$ eigenvectors.\n", "versions": [{"version": "v1", "created": "Thu, 3 Nov 2016 10:08:22 GMT"}, {"version": "v2", "created": "Fri, 4 Nov 2016 09:25:41 GMT"}], "update_date": "2016-11-07", "authors_parsed": [["Paratte", "Johan", ""], ["Martin", "Lionel", ""]]}, {"id": "1611.00953", "submitter": "Frank Dondelinger", "authors": "Frank Dondelinger, Sach Mukherjee and The Alzheimer's Disease\n  Neuroimaging Initiative", "title": "High-dimensional regression over disease subgroups", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider high-dimensional regression over subgroups of observations. Our\nwork is motivated by biomedical problems, where disease subtypes, for example,\nmay differ with respect to underlying regression models, but sample sizes at\nthe subgroup-level may be limited. We focus on the case in which\nsubgroup-specific models may be expected to be similar but not necessarily\nidentical. Our approach is to treat subgroups as related problem instances and\njointly estimate subgroup-specific regression coefficients. This is done in a\npenalized framework, combining an $\\ell_1$ term with an additional term that\npenalizes differences between subgroup-specific coefficients. This gives\nsolutions that are globally sparse but that allow information-sharing between\nthe subgroups. We present algorithms for estimation and empirical results on\nsimulated data and using Alzheimer's disease, amyotrophic lateral sclerosis and\ncancer datasets. These examples demonstrate the gains our approach can offer in\nterms of prediction and the ability to estimate subgroup-specific sparsity\npatterns.\n", "versions": [{"version": "v1", "created": "Thu, 3 Nov 2016 10:58:20 GMT"}, {"version": "v2", "created": "Fri, 9 Dec 2016 13:51:38 GMT"}], "update_date": "2016-12-12", "authors_parsed": [["Dondelinger", "Frank", ""], ["Mukherjee", "Sach", ""], ["Initiative", "The Alzheimer's Disease Neuroimaging", ""]]}, {"id": "1611.00962", "submitter": "Marco Frasca", "authors": "Marco Frasca and Nicol\\`o Cesa Bianchi", "title": "Multitask Protein Function Prediction Through Task Dissimilarity", "comments": "12 pages, 5 figures", "journal-ref": null, "doi": "10.1109/TCBB.2017.2684127", "report-no": null, "categories": "stat.ML cs.LG q-bio.QM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Automated protein function prediction is a challenging problem with\ndistinctive features, such as the hierarchical organization of protein\nfunctions and the scarcity of annotated proteins for most biological functions.\nWe propose a multitask learning algorithm addressing both issues. Unlike\nstandard multitask algorithms, which use task (protein functions) similarity\ninformation as a bias to speed up learning, we show that dissimilarity\ninformation enforces separation of rare class labels from frequent class\nlabels, and for this reason is better suited for solving unbalanced protein\nfunction prediction problems. We support our claim by showing that a multitask\nextension of the label propagation algorithm empirically works best when the\ntask relatedness information is represented using a dissimilarity matrix as\nopposed to a similarity matrix. Moreover, the experimental comparison carried\nout on three model organism shows that our method has a more stable performance\nin both \"protein-centric\" and \"function-centric\" evaluation settings.\n", "versions": [{"version": "v1", "created": "Thu, 3 Nov 2016 11:40:59 GMT"}], "update_date": "2019-04-09", "authors_parsed": [["Frasca", "Marco", ""], ["Bianchi", "Nicol\u00f2 Cesa", ""]]}, {"id": "1611.01046", "submitter": "Gilles Louppe", "authors": "Gilles Louppe, Michael Kagan, Kyle Cranmer", "title": "Learning to Pivot with Adversarial Networks", "comments": "v1: Original submission. v2: Fixed references. v3: version submitted\n  to NIPS'2017. Code available at\n  https://github.com/glouppe/paper-learning-to-pivot", "journal-ref": "Advances in Neural Information Processing Systems 30, pages\n  981-990, 2017", "doi": null, "report-no": null, "categories": "stat.ML cs.LG cs.NE physics.data-an stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Several techniques for domain adaptation have been proposed to account for\ndifferences in the distribution of the data used for training and testing. The\nmajority of this work focuses on a binary domain label. Similar problems occur\nin a scientific context where there may be a continuous family of plausible\ndata generation processes associated to the presence of systematic\nuncertainties. Robust inference is possible if it is based on a pivot -- a\nquantity whose distribution does not depend on the unknown values of the\nnuisance parameters that parametrize this family of data generation processes.\nIn this work, we introduce and derive theoretical results for a training\nprocedure based on adversarial networks for enforcing the pivotal property (or,\nequivalently, fairness with respect to continuous attributes) on a predictive\nmodel. The method includes a hyperparameter to control the trade-off between\naccuracy and robustness. We demonstrate the effectiveness of this approach with\na toy example and examples from particle physics.\n", "versions": [{"version": "v1", "created": "Thu, 3 Nov 2016 14:41:40 GMT"}, {"version": "v2", "created": "Sat, 19 Nov 2016 12:31:03 GMT"}, {"version": "v3", "created": "Thu, 1 Jun 2017 19:04:01 GMT"}], "update_date": "2020-02-18", "authors_parsed": [["Louppe", "Gilles", ""], ["Kagan", "Michael", ""], ["Cranmer", "Kyle", ""]]}, {"id": "1611.01060", "submitter": "Renato Cordeiro de Amorim", "authors": "Renato Cordeiro de Amorim, Vladimir Makarenkov, Boris Mirkin", "title": "A-Ward_p\\b{eta}: Effective hierarchical clustering using the Minkowski\n  metric and a fast k -means initialisation", "comments": null, "journal-ref": "Information Sciences, 370, 343-354 (2016)", "doi": "10.1016/j.ins.2016.07.076", "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper we make two novel contributions to hierarchical clustering.\nFirst, we introduce an anomalous pattern initialisation method for hierarchical\nclustering algorithms, called A-Ward, capable of substantially reducing the\ntime they take to converge. This method generates an initial partition with a\nsufficiently large number of clusters. This allows the cluster merging process\nto start from this partition rather than from a trivial partition composed\nsolely of singletons. Our second contribution is an extension of the Ward and\nWard p algorithms to the situation where the feature weight exponent can differ\nfrom the exponent of the Minkowski distance. This new method, called A-Ward\np\\b{eta} , is able to generate a much wider variety of clustering solutions. We\nalso demonstrate that its parameters can be estimated reasonably well by using\na cluster validity index. We perform numerous experiments using data sets with\ntwo types of noise, insertion of noise features and blurring within-cluster\nvalues of some features. These experiments allow us to conclude: (i) our\nanomalous pattern initialisation method does indeed reduce the time a\nhierarchical clustering algorithm takes to complete, without negatively\nimpacting its cluster recovery ability; (ii) A-Ward p\\b{eta} provides better\ncluster recovery than both Ward and Ward p.\n", "versions": [{"version": "v1", "created": "Thu, 3 Nov 2016 15:23:53 GMT"}], "update_date": "2016-11-04", "authors_parsed": [["de Amorim", "Renato Cordeiro", ""], ["Makarenkov", "Vladimir", ""], ["Mirkin", "Boris", ""]]}, {"id": "1611.01096", "submitter": "Hafiz Tiomoko Ali", "authors": "Hafiz Tiomoko Ali and Romain Couillet", "title": "Spectral community detection in heterogeneous large networks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this article, we study spectral methods for community detection based on $\n\\alpha$-parametrized normalized modularity matrix hereafter called $ {\\bf\nL}_\\alpha $ in heterogeneous graph models. We show, in a regime where community\ndetection is not asymptotically trivial, that $ {\\bf L}_\\alpha $ can be well\napproximated by a more tractable random matrix which falls in the family of\nspiked random matrices. The analysis of this equivalent spiked random matrix\nallows us to improve spectral methods for community detection and assess their\nperformances in the regime under study. In particular, we prove the existence\nof an optimal value $ \\alpha_{\\rm opt} $ of the parameter $ \\alpha $ for which\nthe detection of communities is best ensured and we provide an on-line\nestimation of $ \\alpha_{\\rm opt} $ only based on the knowledge of the graph\nadjacency matrix. Unlike classical spectral methods for community detection\nwhere clustering is performed on the eigenvectors associated with extreme\neigenvalues, we show through our theoretical analysis that a regularization\nshould instead be performed on those eigenvectors prior to clustering in\nheterogeneous graphs. Finally, through a deeper study of the regularized\neigenvectors used for clustering, we assess the performances of our new\nalgorithm for community detection. Numerical simulations in the course of the\narticle show that our methods outperform state-of-the-art spectral methods on\ndense heterogeneous graphs.\n", "versions": [{"version": "v1", "created": "Thu, 3 Nov 2016 17:10:51 GMT"}], "update_date": "2016-11-04", "authors_parsed": [["Ali", "Hafiz Tiomoko", ""], ["Couillet", "Romain", ""]]}, {"id": "1611.01129", "submitter": "Anru Zhang", "authors": "Anru Zhang", "title": "Cross: Efficient Low-rank Tensor Completion", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME cs.LG math.ST stat.ML stat.TH", "license": "http://creativecommons.org/licenses/by-sa/4.0/", "abstract": "  The completion of tensors, or high-order arrays, attracts significant\nattention in recent research. Current literature on tensor completion primarily\nfocuses on recovery from a set of uniformly randomly measured entries, and the\nrequired number of measurements to achieve recovery is not guaranteed to be\noptimal. In addition, the implementation of some previous methods is NP-hard.\nIn this article, we propose a framework for low-rank tensor completion via a\nnovel tensor measurement scheme we name Cross. The proposed procedure is\nefficient and easy to implement. In particular, we show that a third order\ntensor of Tucker rank-$(r_1, r_2, r_3)$ in $p_1$-by-$p_2$-by-$p_3$ dimensional\nspace can be recovered from as few as $r_1r_2r_3 + r_1(p_1-r_1) + r_2(p_2-r_2)\n+ r_3(p_3-r_3)$ noiseless measurements, which matches the sample complexity\nlower-bound. In the case of noisy measurements, we also develop a theoretical\nupper bound and the matching minimax lower bound for recovery error over\ncertain classes of low-rank tensors for the proposed procedure. The results can\nbe further extended to fourth or higher-order tensors. Simulation studies show\nthat the method performs well under a variety of settings. Finally, the\nprocedure is illustrated through a real dataset in neuroimaging.\n", "versions": [{"version": "v1", "created": "Thu, 3 Nov 2016 19:02:02 GMT"}, {"version": "v2", "created": "Tue, 27 Nov 2018 18:08:38 GMT"}], "update_date": "2018-11-28", "authors_parsed": [["Zhang", "Anru", ""]]}, {"id": "1611.01144", "submitter": "Eric Jang", "authors": "Eric Jang, Shixiang Gu, Ben Poole", "title": "Categorical Reparameterization with Gumbel-Softmax", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Categorical variables are a natural choice for representing discrete\nstructure in the world. However, stochastic neural networks rarely use\ncategorical latent variables due to the inability to backpropagate through\nsamples. In this work, we present an efficient gradient estimator that replaces\nthe non-differentiable sample from a categorical distribution with a\ndifferentiable sample from a novel Gumbel-Softmax distribution. This\ndistribution has the essential property that it can be smoothly annealed into a\ncategorical distribution. We show that our Gumbel-Softmax estimator outperforms\nstate-of-the-art gradient estimators on structured output prediction and\nunsupervised generative modeling tasks with categorical latent variables, and\nenables large speedups on semi-supervised classification.\n", "versions": [{"version": "v1", "created": "Thu, 3 Nov 2016 19:48:08 GMT"}, {"version": "v2", "created": "Tue, 22 Nov 2016 23:18:13 GMT"}, {"version": "v3", "created": "Fri, 17 Mar 2017 05:16:36 GMT"}, {"version": "v4", "created": "Sat, 1 Apr 2017 15:33:06 GMT"}, {"version": "v5", "created": "Sat, 5 Aug 2017 22:45:19 GMT"}], "update_date": "2017-08-08", "authors_parsed": [["Jang", "Eric", ""], ["Gu", "Shixiang", ""], ["Poole", "Ben", ""]]}, {"id": "1611.01146", "submitter": "Zeyuan Allen-Zhu", "authors": "Naman Agarwal, Zeyuan Allen-Zhu, Brian Bullins, Elad Hazan, Tengyu Ma", "title": "Finding Approximate Local Minima Faster than Gradient Descent", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.OC cs.DS cs.NE stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We design a non-convex second-order optimization algorithm that is guaranteed\nto return an approximate local minimum in time which scales linearly in the\nunderlying dimension and the number of training examples. The time complexity\nof our algorithm to find an approximate local minimum is even faster than that\nof gradient descent to find a critical point. Our algorithm applies to a\ngeneral class of optimization problems including training a neural network and\nother non-convex objectives arising in machine learning.\n", "versions": [{"version": "v1", "created": "Thu, 3 Nov 2016 19:50:32 GMT"}, {"version": "v2", "created": "Fri, 4 Nov 2016 18:38:50 GMT"}, {"version": "v3", "created": "Fri, 3 Feb 2017 18:13:20 GMT"}, {"version": "v4", "created": "Mon, 24 Apr 2017 19:20:07 GMT"}], "update_date": "2017-04-26", "authors_parsed": [["Agarwal", "Naman", ""], ["Allen-Zhu", "Zeyuan", ""], ["Bullins", "Brian", ""], ["Hazan", "Elad", ""], ["Ma", "Tengyu", ""]]}, {"id": "1611.01170", "submitter": "Wei Xie", "authors": "Wei Xie, Yang Wang, Steven M. Boker, Donald E. Brown", "title": "PrivLogit: Efficient Privacy-preserving Logistic Regression by Tailoring\n  Numerical Optimizers", "comments": "24 pages, 4 figures. Work done and circulated since 2015", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CR stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Safeguarding privacy in machine learning is highly desirable, especially in\ncollaborative studies across many organizations. Privacy-preserving distributed\nmachine learning (based on cryptography) is popular to solve the problem.\nHowever, existing cryptographic protocols still incur excess computational\noverhead. Here, we make a novel observation that this is partially due to naive\nadoption of mainstream numerical optimization (e.g., Newton method) and failing\nto tailor for secure computing. This work presents a contrasting perspective:\ncustomizing numerical optimization specifically for secure settings. We propose\na seemingly less-favorable optimization method that can in fact significantly\naccelerate privacy-preserving logistic regression. Leveraging this new method,\nwe propose two new secure protocols for conducting logistic regression in a\nprivacy-preserving and distributed manner. Extensive theoretical and empirical\nevaluations prove the competitive performance of our two secure proposals while\nwithout compromising accuracy or privacy: with speedup up to 2.3x and 8.1x,\nrespectively, over state-of-the-art; and even faster as data scales up. Such\ndrastic speedup is on top of and in addition to performance improvements from\nexisting (and future) state-of-the-art cryptography. Our work provides a new\nway towards efficient and practical privacy-preserving logistic regression for\nlarge-scale studies which are common for modern science.\n", "versions": [{"version": "v1", "created": "Thu, 3 Nov 2016 20:04:29 GMT"}], "update_date": "2016-11-07", "authors_parsed": [["Xie", "Wei", ""], ["Wang", "Yang", ""], ["Boker", "Steven M.", ""], ["Brown", "Donald E.", ""]]}, {"id": "1611.01179", "submitter": "Wenjing Liao", "authors": "Wenjing Liao and Mauro Maggioni", "title": "Adaptive Geometric Multiscale Approximations for Intrinsically\n  Low-dimensional Data", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.IT math.IT math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider the problem of efficiently approximating and encoding\nhigh-dimensional data sampled from a probability distribution $\\rho$ in\n$\\mathbb{R}^D$, that is nearly supported on a $d$-dimensional set $\\mathcal{M}$\n- for example supported on a $d$-dimensional Riemannian manifold. Geometric\nMulti-Resolution Analysis (GMRA) provides a robust and computationally\nefficient procedure to construct low-dimensional geometric approximations of\n$\\mathcal{M}$ at varying resolutions. We introduce a thresholding algorithm on\nthe geometric wavelet coefficients, leading to what we call adaptive GMRA\napproximations. We show that these data-driven, empirical approximations\nperform well, when the threshold is chosen as a suitable universal function of\nthe number of samples $n$, on a wide variety of measures $\\rho$, that are\nallowed to exhibit different regularity at different scales and locations,\nthereby efficiently encoding data from more complex measures than those\nsupported on manifolds. These approximations yield a data-driven dictionary,\ntogether with a fast transform mapping data to coefficients, and an inverse of\nsuch a map. The algorithms for both the dictionary construction and the\ntransforms have complexity $C n \\log n$ with the constant linear in $D$ and\nexponential in $d$. Our work therefore establishes adaptive GMRA as a fast\ndictionary learning algorithm with approximation guarantees. We include several\nnumerical experiments on both synthetic and real data, confirming our\ntheoretical results and demonstrating the effectiveness of adaptive GMRA.\n", "versions": [{"version": "v1", "created": "Thu, 3 Nov 2016 20:26:08 GMT"}, {"version": "v2", "created": "Tue, 18 Jul 2017 12:38:50 GMT"}], "update_date": "2017-07-19", "authors_parsed": [["Liao", "Wenjing", ""], ["Maggioni", "Mauro", ""]]}, {"id": "1611.01186", "submitter": "Sihan Li", "authors": "Sihan Li, Jiantao Jiao, Yanjun Han, Tsachy Weissman", "title": "Demystifying ResNet", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NE cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The Residual Network (ResNet), proposed in He et al. (2015), utilized\nshortcut connections to significantly reduce the difficulty of training, which\nresulted in great performance boosts in terms of both training and\ngeneralization error.\n  It was empirically observed in He et al. (2015) that stacking more layers of\nresidual blocks with shortcut 2 results in smaller training error, while it is\nnot true for shortcut of length 1 or 3. We provide a theoretical explanation\nfor the uniqueness of shortcut 2.\n  We show that with or without nonlinearities, by adding shortcuts that have\ndepth two, the condition number of the Hessian of the loss function at the zero\ninitial point is depth-invariant, which makes training very deep models no more\ndifficult than shallow ones. Shortcuts of higher depth result in an extremely\nflat (high-order) stationary point initially, from which the optimization\nalgorithm is hard to escape. The shortcut 1, however, is essentially equivalent\nto no shortcuts, which has a condition number exploding to infinity as the\nnumber of layers grows. We further argue that as the number of layers tends to\ninfinity, it suffices to only look at the loss function at the zero initial\npoint.\n  Extensive experiments are provided accompanying our theoretical results. We\nshow that initializing the network to small weights with shortcut 2 achieves\nsignificantly better results than random Gaussian (Xavier) initialization,\northogonal initialization, and shortcuts of deeper depth, from various\nperspectives ranging from final loss, learning dynamics and stability, to the\nbehavior of the Hessian along the learning process.\n", "versions": [{"version": "v1", "created": "Thu, 3 Nov 2016 20:55:49 GMT"}, {"version": "v2", "created": "Sat, 20 May 2017 10:18:06 GMT"}], "update_date": "2017-05-23", "authors_parsed": [["Li", "Sihan", ""], ["Jiao", "Jiantao", ""], ["Han", "Yanjun", ""], ["Weissman", "Tsachy", ""]]}, {"id": "1611.01211", "submitter": "Zachary Lipton", "authors": "Zachary C. Lipton, Kamyar Azizzadenesheli, Abhishek Kumar, Lihong Li,\n  Jianfeng Gao, Li Deng", "title": "Combating Reinforcement Learning's Sisyphean Curse with Intrinsic Fear", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.NE stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Many practical environments contain catastrophic states that an optimal agent\nwould visit infrequently or never. Even on toy problems, Deep Reinforcement\nLearning (DRL) agents tend to periodically revisit these states upon forgetting\ntheir existence under a new policy. We introduce intrinsic fear (IF), a learned\nreward shaping that guards DRL agents against periodic catastrophes. IF agents\npossess a fear model trained to predict the probability of imminent\ncatastrophe. This score is then used to penalize the Q-learning objective. Our\ntheoretical analysis bounds the reduction in average return due to learning on\nthe perturbed objective. We also prove robustness to classification errors. As\na bonus, IF models tend to learn faster, owing to reward shaping. Experiments\ndemonstrate that intrinsic-fear DQNs solve otherwise pathological environments\nand improve on several Atari games.\n", "versions": [{"version": "v1", "created": "Thu, 3 Nov 2016 22:30:10 GMT"}, {"version": "v2", "created": "Mon, 7 Nov 2016 04:22:31 GMT"}, {"version": "v3", "created": "Thu, 1 Dec 2016 01:27:56 GMT"}, {"version": "v4", "created": "Tue, 21 Mar 2017 21:32:25 GMT"}, {"version": "v5", "created": "Mon, 15 May 2017 05:05:08 GMT"}, {"version": "v6", "created": "Tue, 23 May 2017 01:39:00 GMT"}, {"version": "v7", "created": "Sun, 8 Oct 2017 05:40:45 GMT"}, {"version": "v8", "created": "Tue, 13 Mar 2018 21:24:47 GMT"}], "update_date": "2018-03-15", "authors_parsed": [["Lipton", "Zachary C.", ""], ["Azizzadenesheli", "Kamyar", ""], ["Kumar", "Abhishek", ""], ["Li", "Lihong", ""], ["Gao", "Jianfeng", ""], ["Deng", "Li", ""]]}, {"id": "1611.01230", "submitter": "Jie Sun", "authors": "Jie Sun, Fernando J. Quevedo, Erik Bollt", "title": "Bayesian Optical Flow with Uncertainty Quantification", "comments": "Published 20 August 2018", "journal-ref": "Inverse Problems 34, 105008 (2018)", "doi": "10.1088/1361-6420/aad7cc", "report-no": null, "categories": "stat.ML cs.CV physics.data-an", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Optical flow refers to the visual motion observed between two consecutive\nimages. Since the degree of freedom is typically much larger than the\nconstraints imposed by the image observations, the straightforward formulation\nof optical flow as an inverse problem is ill-posed. Standard approaches to\ndetermine optical flow rely on formulating and solving an optimization problem\nthat contains both a data fidelity term and a regularization term, the latter\neffectively resolves the otherwise ill-posedness of the inverse problem. In\nthis work, we depart from the deterministic formalism, and instead treat\noptical flow as a statistical inverse problem. We discuss how a classical\noptical flow solution can be interpreted as a point estimate in this more\ngeneral framework. The statistical approach, whose \"solution\" is a distribution\nof flow fields, which we refer to as Bayesian optical flow, allows not only\n\"point\" estimates (e.g., the computation of average flow field), but also\nstatistical estimates (e.g., quantification of uncertainty) that are beyond any\nstandard method for optical flow. As application, we benchmark Bayesian optical\nflow together with uncertainty quantification using several types of prescribed\nground-truth flow fields and images.\n", "versions": [{"version": "v1", "created": "Fri, 4 Nov 2016 00:29:02 GMT"}, {"version": "v2", "created": "Mon, 20 Aug 2018 11:41:49 GMT"}], "update_date": "2018-08-21", "authors_parsed": [["Sun", "Jie", ""], ["Quevedo", "Fernando J.", ""], ["Bollt", "Erik", ""]]}, {"id": "1611.01232", "submitter": "Samuel Schoenholz", "authors": "Samuel S. Schoenholz, Justin Gilmer, Surya Ganguli and Jascha\n  Sohl-Dickstein", "title": "Deep Information Propagation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study the behavior of untrained neural networks whose weights and biases\nare randomly distributed using mean field theory. We show the existence of\ndepth scales that naturally limit the maximum depth of signal propagation\nthrough these random networks. Our main practical result is to show that random\nnetworks may be trained precisely when information can travel through them.\nThus, the depth scales that we identify provide bounds on how deep a network\nmay be trained for a specific choice of hyperparameters. As a corollary to\nthis, we argue that in networks at the edge of chaos, one of these depth scales\ndiverges. Thus arbitrarily deep networks may be trained only sufficiently close\nto criticality. We show that the presence of dropout destroys the\norder-to-chaos critical point and therefore strongly limits the maximum\ntrainable depth for random networks. Finally, we develop a mean field theory\nfor backpropagation and we show that the ordered and chaotic phases correspond\nto regions of vanishing and exploding gradient respectively.\n", "versions": [{"version": "v1", "created": "Fri, 4 Nov 2016 00:44:32 GMT"}, {"version": "v2", "created": "Tue, 4 Apr 2017 19:36:14 GMT"}], "update_date": "2017-04-06", "authors_parsed": [["Schoenholz", "Samuel S.", ""], ["Gilmer", "Justin", ""], ["Ganguli", "Surya", ""], ["Sohl-Dickstein", "Jascha", ""]]}, {"id": "1611.01236", "submitter": "Alexey Kurakin", "authors": "Alexey Kurakin, Ian Goodfellow, Samy Bengio", "title": "Adversarial Machine Learning at Scale", "comments": "17 pages, 5 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.CR cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Adversarial examples are malicious inputs designed to fool machine learning\nmodels. They often transfer from one model to another, allowing attackers to\nmount black box attacks without knowledge of the target model's parameters.\nAdversarial training is the process of explicitly training a model on\nadversarial examples, in order to make it more robust to attack or to reduce\nits test error on clean inputs. So far, adversarial training has primarily been\napplied to small problems. In this research, we apply adversarial training to\nImageNet. Our contributions include: (1) recommendations for how to succesfully\nscale adversarial training to large models and datasets, (2) the observation\nthat adversarial training confers robustness to single-step attack methods, (3)\nthe finding that multi-step attack methods are somewhat less transferable than\nsingle-step attack methods, so single-step attacks are the best for mounting\nblack-box attacks, and (4) resolution of a \"label leaking\" effect that causes\nadversarially trained models to perform better on adversarial examples than on\nclean examples, because the adversarial example construction process uses the\ntrue label and the model can learn to exploit regularities in the construction\nprocess.\n", "versions": [{"version": "v1", "created": "Fri, 4 Nov 2016 01:11:02 GMT"}, {"version": "v2", "created": "Sat, 11 Feb 2017 00:15:46 GMT"}], "update_date": "2017-02-14", "authors_parsed": [["Kurakin", "Alexey", ""], ["Goodfellow", "Ian", ""], ["Bengio", "Samy", ""]]}, {"id": "1611.01239", "submitter": "Seiya Tokui", "authors": "Seiya Tokui and Issei sato", "title": "Reparameterization trick for discrete variables", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Low-variance gradient estimation is crucial for learning directed graphical\nmodels parameterized by neural networks, where the reparameterization trick is\nwidely used for those with continuous variables. While this technique gives\nlow-variance gradient estimates, it has not been directly applicable to\ndiscrete variables, the sampling of which inherently requires discontinuous\noperations. We argue that the discontinuity can be bypassed by marginalizing\nout the variable of interest, which results in a new reparameterization trick\nfor discrete variables. This reparameterization greatly reduces the variance,\nwhich is understood by regarding the method as an application of common random\nnumbers to the estimation. The resulting estimator is theoretically guaranteed\nto have a variance not larger than that of the likelihood-ratio method with the\noptimal input-dependent baseline. We give empirical results for variational\nlearning of sigmoid belief networks.\n", "versions": [{"version": "v1", "created": "Fri, 4 Nov 2016 01:46:47 GMT"}], "update_date": "2016-11-07", "authors_parsed": [["Tokui", "Seiya", ""], ["sato", "Issei", ""]]}, {"id": "1611.01353", "submitter": "Alessandro Achille", "authors": "Alessandro Achille, Stefano Soatto", "title": "Information Dropout: Learning Optimal Representations Through Noisy\n  Computation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The cross-entropy loss commonly used in deep learning is closely related to\nthe defining properties of optimal representations, but does not enforce some\nof the key properties. We show that this can be solved by adding a\nregularization term, which is in turn related to injecting multiplicative noise\nin the activations of a Deep Neural Network, a special case of which is the\ncommon practice of dropout. We show that our regularized loss function can be\nefficiently minimized using Information Dropout, a generalization of dropout\nrooted in information theoretic principles that automatically adapts to the\ndata and can better exploit architectures of limited capacity. When the task is\nthe reconstruction of the input, we show that our loss function yields a\nVariational Autoencoder as a special case, thus providing a link between\nrepresentation learning, information theory and variational inference. Finally,\nwe prove that we can promote the creation of disentangled representations\nsimply by enforcing a factorized prior, a fact that has been observed\nempirically in recent work. Our experiments validate the theoretical intuitions\nbehind our method, and we find that information dropout achieves a comparable\nor better generalization performance than binary dropout, especially on smaller\nmodels, since it can automatically adapt the noise to the structure of the\nnetwork, as well as to the test sample.\n", "versions": [{"version": "v1", "created": "Fri, 4 Nov 2016 12:46:37 GMT"}, {"version": "v2", "created": "Tue, 17 Jan 2017 00:40:19 GMT"}, {"version": "v3", "created": "Sun, 12 Feb 2017 09:26:25 GMT"}], "update_date": "2017-02-14", "authors_parsed": [["Achille", "Alessandro", ""], ["Soatto", "Stefano", ""]]}, {"id": "1611.01455", "submitter": "Hanock Kwak", "authors": "Hanock Kwak and Byoung-Tak Zhang", "title": "Ways of Conditioning Generative Adversarial Networks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The GANs are generative models whose random samples realistically reflect\nnatural images. It also can generate samples with specific attributes by\nconcatenating a condition vector into the input, yet research on this field is\nnot well studied. We propose novel methods of conditioning generative\nadversarial networks (GANs) that achieve state-of-the-art results on MNIST and\nCIFAR-10. We mainly introduce two models: an information retrieving model that\nextracts conditional information from the samples, and a spatial bilinear\npooling model that forms bilinear features derived from the spatial cross\nproduct of an image and a condition vector. These methods significantly enhance\nlog-likelihood of test data under the conditional distributions compared to the\nmethods of concatenation.\n", "versions": [{"version": "v1", "created": "Fri, 4 Nov 2016 17:08:54 GMT"}], "update_date": "2016-11-07", "authors_parsed": [["Kwak", "Hanock", ""], ["Zhang", "Byoung-Tak", ""]]}, {"id": "1611.01456", "submitter": "Dorina Thanou", "authors": "Dorina Thanou, Xiaowen Dong, Daniel Kressner, and Pascal Frossard", "title": "Learning heat diffusion graphs", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.SI stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Effective information analysis generally boils down to properly identifying\nthe structure or geometry of the data, which is often represented by a graph.\nIn some applications, this structure may be partly determined by design\nconstraints or pre-determined sensing arrangements, like in road transportation\nnetworks for example. In general though, the data structure is not readily\navailable and becomes pretty difficult to define. In particular, the global\nsmoothness assumptions, that most of the existing works adopt, are often too\ngeneral and unable to properly capture localized properties of data. In this\npaper, we go beyond this classical data model and rather propose to represent\ninformation as a sparse combination of localized functions that live on a data\nstructure represented by a graph. Based on this model, we focus on the problem\nof inferring the connectivity that best explains the data samples at different\nvertices of a graph that is a priori unknown. We concentrate on the case where\nthe observed data is actually the sum of heat diffusion processes, which is a\nquite common model for data on networks or other irregular structures. We cast\na new graph learning problem and solve it with an efficient nonconvex\noptimization algorithm. Experiments on both synthetic and real world data\nfinally illustrate the benefits of the proposed graph learning framework and\nconfirm that the data structure can be efficiently learned from data\nobservations only. We believe that our algorithm will help solving key\nquestions in diverse application domains such as social and biological network\nanalysis where it is crucial to unveil proper geometry for data understanding\nand inference.\n", "versions": [{"version": "v1", "created": "Fri, 4 Nov 2016 17:16:17 GMT"}], "update_date": "2016-11-07", "authors_parsed": [["Thanou", "Dorina", ""], ["Dong", "Xiaowen", ""], ["Kressner", "Daniel", ""], ["Frossard", "Pascal", ""]]}, {"id": "1611.01462", "submitter": "Hakan Inan", "authors": "Hakan Inan, Khashayar Khosravi, Richard Socher", "title": "Tying Word Vectors and Word Classifiers: A Loss Framework for Language\n  Modeling", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CL stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recurrent neural networks have been very successful at predicting sequences\nof words in tasks such as language modeling. However, all such models are based\non the conventional classification framework, where the model is trained\nagainst one-hot targets, and each word is represented both as an input and as\nan output in isolation. This causes inefficiencies in learning both in terms of\nutilizing all of the information and in terms of the number of parameters\nneeded to train. We introduce a novel theoretical framework that facilitates\nbetter learning in language modeling, and show that our framework leads to\ntying together the input embedding and the output projection matrices, greatly\nreducing the number of trainable variables. Our framework leads to state of the\nart performance on the Penn Treebank with a variety of network models.\n", "versions": [{"version": "v1", "created": "Fri, 4 Nov 2016 17:36:20 GMT"}, {"version": "v2", "created": "Sun, 5 Mar 2017 22:30:23 GMT"}, {"version": "v3", "created": "Sat, 11 Mar 2017 19:13:52 GMT"}], "update_date": "2017-03-14", "authors_parsed": [["Inan", "Hakan", ""], ["Khosravi", "Khashayar", ""], ["Socher", "Richard", ""]]}, {"id": "1611.01480", "submitter": "Damjan Krstajic", "authors": "Damjan Krstajic", "title": "Why comparing survival curves between two subgroups may be misleading", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.AP stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We analyse an issue when comparing survival curves between two subgroups. We\nshow that there is a direct relationship between estimates of subgroups'\nsurvival at a time point and positive and negative predictive values in the\nbinary classification settings. Our findings present a case where current\nmethods of comparing survival curves between subgroups may be misleading. We\nthink that this ought to be taken into account during the validation of\nprognostic diagnostic tests that predict two prognostic subgroups for a given\ndisease or treatment, when the validation data set consists of censored data.\n", "versions": [{"version": "v1", "created": "Fri, 4 Nov 2016 18:26:00 GMT"}, {"version": "v2", "created": "Fri, 14 May 2021 14:56:08 GMT"}], "update_date": "2021-05-17", "authors_parsed": [["Krstajic", "Damjan", ""]]}, {"id": "1611.01491", "submitter": "Anirbit Mukherjee", "authors": "Raman Arora, Amitabh Basu, Poorya Mianjy and Anirbit Mukherjee", "title": "Understanding Deep Neural Networks with Rectified Linear Units", "comments": "The poly(data) exact training algorithm has been improved to now be\n  applicable to any single hidden layer R^n-> R ReLU DNN and there is a cleaner\n  pseudocode for it given on page 8. Also now on page 7 there is a more precise\n  description about when and how the Zonotope construction improves on the\n  Theorem 4 of this paper, arXiv:1402.1869", "journal-ref": "ICLR 2028", "doi": null, "report-no": null, "categories": "cs.LG cond-mat.dis-nn cs.AI cs.CC stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper we investigate the family of functions representable by deep\nneural networks (DNN) with rectified linear units (ReLU). We give an algorithm\nto train a ReLU DNN with one hidden layer to *global optimality* with runtime\npolynomial in the data size albeit exponential in the input dimension. Further,\nwe improve on the known lower bounds on size (from exponential to super\nexponential) for approximating a ReLU deep net function by a shallower ReLU\nnet. Our gap theorems hold for smoothly parametrized families of \"hard\"\nfunctions, contrary to countable, discrete families known in the literature. An\nexample consequence of our gap theorems is the following: for every natural\nnumber $k$ there exists a function representable by a ReLU DNN with $k^2$\nhidden layers and total size $k^3$, such that any ReLU DNN with at most $k$\nhidden layers will require at least $\\frac{1}{2}k^{k+1}-1$ total nodes.\nFinally, for the family of $\\mathbb{R}^n\\to \\mathbb{R}$ DNNs with ReLU\nactivations, we show a new lowerbound on the number of affine pieces, which is\nlarger than previous constructions in certain regimes of the network\narchitecture and most distinctively our lowerbound is demonstrated by an\nexplicit construction of a *smoothly parameterized* family of functions\nattaining this scaling. Our construction utilizes the theory of zonotopes from\npolyhedral theory.\n", "versions": [{"version": "v1", "created": "Fri, 4 Nov 2016 18:54:50 GMT"}, {"version": "v2", "created": "Fri, 11 Nov 2016 20:25:56 GMT"}, {"version": "v3", "created": "Sat, 26 Nov 2016 17:38:11 GMT"}, {"version": "v4", "created": "Mon, 29 May 2017 20:06:50 GMT"}, {"version": "v5", "created": "Tue, 18 Jul 2017 17:17:14 GMT"}, {"version": "v6", "created": "Wed, 28 Feb 2018 02:23:47 GMT"}], "update_date": "2018-03-01", "authors_parsed": [["Arora", "Raman", ""], ["Basu", "Amitabh", ""], ["Mianjy", "Poorya", ""], ["Mukherjee", "Anirbit", ""]]}, {"id": "1611.01504", "submitter": "Krzysztof Chalupka", "authors": "Krzysztof Chalupka, Frederick Eberhardt and Pietro Perona", "title": "Estimating Causal Direction and Confounding of Two Discrete Variables", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.AI cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a method to classify the causal relationship between two discrete\nvariables given only the joint distribution of the variables, acknowledging\nthat the method is subject to an inherent baseline error. We assume that the\ncausal system is acyclicity, but we do allow for hidden common causes. Our\nalgorithm presupposes that the probability distributions $P(C)$ of a cause $C$\nis independent from the probability distribution $P(E\\mid C)$ of the\ncause-effect mechanism. While our classifier is trained with a Bayesian\nassumption of flat hyperpriors, we do not make this assumption about our test\ndata. This work connects to recent developments on the identifiability of\ncausal models over continuous variables under the assumption of \"independent\nmechanisms\". Carefully-commented Python notebooks that reproduce all our\nexperiments are available online at\nhttp://vision.caltech.edu/~kchalupk/code.html.\n", "versions": [{"version": "v1", "created": "Fri, 4 Nov 2016 19:33:35 GMT"}], "update_date": "2016-11-07", "authors_parsed": [["Chalupka", "Krzysztof", ""], ["Eberhardt", "Frederick", ""], ["Perona", "Pietro", ""]]}, {"id": "1611.01511", "submitter": "Brian Gaines", "authors": "Brian R. Gaines and Hua Zhou", "title": "Algorithms for Fitting the Constrained Lasso", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We compare alternative computing strategies for solving the constrained lasso\nproblem. As its name suggests, the constrained lasso extends the widely-used\nlasso to handle linear constraints, which allow the user to incorporate prior\ninformation into the model. In addition to quadratic programming, we employ the\nalternating direction method of multipliers (ADMM) and also derive an efficient\nsolution path algorithm. Through both simulations and real data examples, we\ncompare the different algorithms and provide practical recommendations in terms\nof efficiency and accuracy for various sizes of data. We also show that, for an\narbitrary penalty matrix, the generalized lasso can be transformed to a\nconstrained lasso, while the converse is not true. Thus, our methods can also\nbe used for estimating a generalized lasso, which has wide-ranging\napplications. Code for implementing the algorithms is freely available in the\nMatlab toolbox SparseReg.\n", "versions": [{"version": "v1", "created": "Fri, 28 Oct 2016 19:06:55 GMT"}], "update_date": "2016-11-08", "authors_parsed": [["Gaines", "Brian R.", ""], ["Zhou", "Hua", ""]]}, {"id": "1611.01540", "submitter": "Daniel Freeman", "authors": "C. Daniel Freeman and Joan Bruna", "title": "Topology and Geometry of Half-Rectified Network Optimization", "comments": "22 Pages (10 main + Appendices), 4 Figures, 1 Table, Published as a\n  conference paper at ICLR 2017", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The loss surface of deep neural networks has recently attracted interest in\nthe optimization and machine learning communities as a prime example of\nhigh-dimensional non-convex problem. Some insights were recently gained using\nspin glass models and mean-field approximations, but at the expense of strongly\nsimplifying the nonlinear nature of the model.\n  In this work, we do not make any such assumption and study conditions on the\ndata distribution and model architecture that prevent the existence of bad\nlocal minima. Our theoretical work quantifies and formalizes two important\n\\emph{folklore} facts: (i) the landscape of deep linear networks has a\nradically different topology from that of deep half-rectified ones, and (ii)\nthat the energy landscape in the non-linear case is fundamentally controlled by\nthe interplay between the smoothness of the data distribution and model\nover-parametrization. Our main theoretical contribution is to prove that\nhalf-rectified single layer networks are asymptotically connected, and we\nprovide explicit bounds that reveal the aforementioned interplay.\n  The conditioning of gradient descent is the next challenge we address. We\nstudy this question through the geometry of the level sets, and we introduce an\nalgorithm to efficiently estimate the regularity of such sets on large-scale\nnetworks. Our empirical results show that these level sets remain connected\nthroughout all the learning phase, suggesting a near convex behavior, but they\nbecome exponentially more curvy as the energy level decays, in accordance to\nwhat is observed in practice with very low curvature attractors.\n", "versions": [{"version": "v1", "created": "Fri, 4 Nov 2016 21:17:42 GMT"}, {"version": "v2", "created": "Sun, 20 Nov 2016 00:26:16 GMT"}, {"version": "v3", "created": "Sat, 25 Mar 2017 04:17:46 GMT"}, {"version": "v4", "created": "Thu, 1 Jun 2017 19:46:41 GMT"}], "update_date": "2017-06-05", "authors_parsed": [["Freeman", "C. Daniel", ""], ["Bruna", "Joan", ""]]}, {"id": "1611.01541", "submitter": "Yanming Li", "authors": "Yanming Li, Hyokyoung Hong, Jian Kang, Kevin He, Ji Zhu, Yi Li", "title": "Classification with Ultrahigh-Dimensional Features", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Although much progress has been made in classification with high-dimensional\nfeatures \\citep{Fan_Fan:2008, JGuo:2010, CaiSun:2014, PRXu:2014},\nclassification with ultrahigh-dimensional features, wherein the features much\noutnumber the sample size, defies most existing work. This paper introduces a\nnovel and computationally feasible multivariate screening and classification\nmethod for ultrahigh-dimensional data. Leveraging inter-feature correlations,\nthe proposed method enables detection of marginally weak and sparse signals and\nrecovery of the true informative feature set, and achieves asymptotic optimal\nmisclassification rates. We also show that the proposed procedure provides more\npowerful discovery boundaries compared to those in \\citet{CaiSun:2014} and\n\\citet{JJin:2009}. The performance of the proposed procedure is evaluated using\nsimulation studies and demonstrated via classification of patients with\ndifferent post-transplantation renal functional types.\n", "versions": [{"version": "v1", "created": "Fri, 4 Nov 2016 21:21:53 GMT"}], "update_date": "2016-11-14", "authors_parsed": [["Li", "Yanming", ""], ["Hong", "Hyokyoung", ""], ["Kang", "Jian", ""], ["He", "Kevin", ""], ["Zhu", "Ji", ""], ["Li", "Yi", ""]]}, {"id": "1611.01586", "submitter": "Gang Niu", "authors": "Marthinus C. du Plessis, Gang Niu, and Masashi Sugiyama", "title": "Class-prior Estimation for Learning from Positive and Unlabeled Data", "comments": "To appear in Machine Learning", "journal-ref": null, "doi": "10.1007/s10994-016-5604-6", "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider the problem of estimating the class prior in an unlabeled\ndataset. Under the assumption that an additional labeled dataset is available,\nthe class prior can be estimated by fitting a mixture of class-wise data\ndistributions to the unlabeled data distribution. However, in practice, such an\nadditional labeled dataset is often not available. In this paper, we show that,\nwith additional samples coming only from the positive class, the class prior of\nthe unlabeled dataset can be estimated correctly. Our key idea is to use\nproperly penalized divergences for model fitting to cancel the error caused by\nthe absence of negative samples. We further show that the use of the penalized\n$L_1$-distance gives a computationally efficient algorithm with an analytic\nsolution. The consistency, stability, and estimation error are theoretically\nanalyzed. Finally, we experimentally demonstrate the usefulness of the proposed\nmethod.\n", "versions": [{"version": "v1", "created": "Sat, 5 Nov 2016 01:58:12 GMT"}], "update_date": "2016-11-08", "authors_parsed": [["Plessis", "Marthinus C. du", ""], ["Niu", "Gang", ""], ["Sugiyama", "Masashi", ""]]}, {"id": "1611.01606", "submitter": "Alexander Schwing", "authors": "Frank S. He and Yang Liu and Alexander G. Schwing and Jian Peng", "title": "Learning to Play in a Day: Faster Deep Reinforcement Learning by\n  Optimality Tightening", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a novel training algorithm for reinforcement learning which\ncombines the strength of deep Q-learning with a constrained optimization\napproach to tighten optimality and encourage faster reward propagation. Our\nnovel technique makes deep reinforcement learning more practical by drastically\nreducing the training time. We evaluate the performance of our approach on the\n49 games of the challenging Arcade Learning Environment, and report significant\nimprovements in both training time and accuracy.\n", "versions": [{"version": "v1", "created": "Sat, 5 Nov 2016 05:42:40 GMT"}], "update_date": "2016-11-08", "authors_parsed": [["He", "Frank S.", ""], ["Liu", "Yang", ""], ["Schwing", "Alexander G.", ""], ["Peng", "Jian", ""]]}, {"id": "1611.01626", "submitter": "Brendan O'Donoghue", "authors": "Brendan O'Donoghue, Remi Munos, Koray Kavukcuoglu and Volodymyr Mnih", "title": "Combining policy gradient and Q-learning", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI math.OC stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Policy gradient is an efficient technique for improving a policy in a\nreinforcement learning setting. However, vanilla online variants are on-policy\nonly and not able to take advantage of off-policy data. In this paper we\ndescribe a new technique that combines policy gradient with off-policy\nQ-learning, drawing experience from a replay buffer. This is motivated by\nmaking a connection between the fixed points of the regularized policy gradient\nalgorithm and the Q-values. This connection allows us to estimate the Q-values\nfrom the action preferences of the policy, to which we apply Q-learning\nupdates. We refer to the new technique as 'PGQL', for policy gradient and\nQ-learning. We also establish an equivalency between action-value fitting\ntechniques and actor-critic algorithms, showing that regularized policy\ngradient techniques can be interpreted as advantage function learning\nalgorithms. We conclude with some numerical examples that demonstrate improved\ndata efficiency and stability of PGQL. In particular, we tested PGQL on the\nfull suite of Atari games and achieved performance exceeding that of both\nasynchronous advantage actor-critic (A3C) and Q-learning.\n", "versions": [{"version": "v1", "created": "Sat, 5 Nov 2016 10:49:37 GMT"}, {"version": "v2", "created": "Mon, 6 Mar 2017 12:38:42 GMT"}, {"version": "v3", "created": "Fri, 7 Apr 2017 15:20:05 GMT"}], "update_date": "2017-04-10", "authors_parsed": [["O'Donoghue", "Brendan", ""], ["Munos", "Remi", ""], ["Kavukcuoglu", "Koray", ""], ["Mnih", "Volodymyr", ""]]}, {"id": "1611.01702", "submitter": "Adji Bousso Dieng", "authors": "Adji B. Dieng, Chong Wang, Jianfeng Gao, John Paisley", "title": "TopicRNN: A Recurrent Neural Network with Long-Range Semantic Dependency", "comments": "International Conference on Learning Representations", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we propose TopicRNN, a recurrent neural network (RNN)-based\nlanguage model designed to directly capture the global semantic meaning\nrelating words in a document via latent topics. Because of their sequential\nnature, RNNs are good at capturing the local structure of a word sequence -\nboth semantic and syntactic - but might face difficulty remembering long-range\ndependencies. Intuitively, these long-range dependencies are of semantic\nnature. In contrast, latent topic models are able to capture the global\nunderlying semantic structure of a document but do not account for word\nordering. The proposed TopicRNN model integrates the merits of RNNs and latent\ntopic models: it captures local (syntactic) dependencies using an RNN and\nglobal (semantic) dependencies using latent topics. Unlike previous work on\ncontextual RNN language modeling, our model is learned end-to-end. Empirical\nresults on word prediction show that TopicRNN outperforms existing contextual\nRNN baselines. In addition, TopicRNN can be used as an unsupervised feature\nextractor for documents. We do this for sentiment analysis on the IMDB movie\nreview dataset and report an error rate of $6.28\\%$. This is comparable to the\nstate-of-the-art $5.91\\%$ resulting from a semi-supervised approach. Finally,\nTopicRNN also yields sensible topics, making it a useful alternative to\ndocument models such as latent Dirichlet allocation.\n", "versions": [{"version": "v1", "created": "Sat, 5 Nov 2016 21:25:07 GMT"}, {"version": "v2", "created": "Mon, 27 Feb 2017 03:03:38 GMT"}], "update_date": "2017-02-28", "authors_parsed": [["Dieng", "Adji B.", ""], ["Wang", "Chong", ""], ["Gao", "Jianfeng", ""], ["Paisley", "John", ""]]}, {"id": "1611.01708", "submitter": "Feras Saad", "authors": "Feras Saad, Vikash Mansinghka", "title": "Detecting Dependencies in Sparse, Multivariate Databases Using\n  Probabilistic Programming and Non-parametric Bayes", "comments": null, "journal-ref": "Proceedings of the 20th International Conference on Artificial\n  Intelligence and Statistics, PMLR 54:632-641, 2017", "doi": null, "report-no": null, "categories": "stat.ML cs.AI cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Datasets with hundreds of variables and many missing values are commonplace.\nIn this setting, it is both statistically and computationally challenging to\ndetect true predictive relationships between variables and also to suppress\nfalse positives. This paper proposes an approach that combines probabilistic\nprogramming, information theory, and non-parametric Bayes. It shows how to use\nBayesian non-parametric modeling to (i) build an ensemble of joint probability\nmodels for all the variables; (ii) efficiently detect marginal independencies;\nand (iii) estimate the conditional mutual information between arbitrary subsets\nof variables, subject to a broad class of constraints. Users can access these\ncapabilities using BayesDB, a probabilistic programming platform for\nprobabilistic data analysis, by writing queries in a simple, SQL-like language.\nThis paper demonstrates empirically that the method can (i) detect\ncontext-specific (in)dependencies on challenging synthetic problems and (ii)\nyield improved sensitivity and specificity over baselines from statistics and\nmachine learning, on a real-world database of over 300 sparsely observed\nindicators of macroeconomic development and public health.\n", "versions": [{"version": "v1", "created": "Sat, 5 Nov 2016 23:02:25 GMT"}, {"version": "v2", "created": "Mon, 27 Mar 2017 03:07:49 GMT"}], "update_date": "2018-04-03", "authors_parsed": [["Saad", "Feras", ""], ["Mansinghka", "Vikash", ""]]}, {"id": "1611.01722", "submitter": "Dilin Wang", "authors": "Dilin Wang, Qiang Liu", "title": "Learning to Draw Samples: With Application to Amortized MLE for\n  Generative Adversarial Learning", "comments": "Under review as a conference paper at ICLR 2017", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a simple algorithm to train stochastic neural networks to draw\nsamples from given target distributions for probabilistic inference. Our method\nis based on iteratively adjusting the neural network parameters so that the\noutput changes along a Stein variational gradient that maximumly decreases the\nKL divergence with the target distribution. Our method works for any target\ndistribution specified by their unnormalized density function, and can train\nany black-box architectures that are differentiable in terms of the parameters\nwe want to adapt. As an application of our method, we propose an amortized MLE\nalgorithm for training deep energy model, where a neural sampler is adaptively\ntrained to approximate the likelihood function. Our method mimics an\nadversarial game between the deep energy model and the neural sampler, and\nobtains realistic-looking images competitive with the state-of-the-art results.\n", "versions": [{"version": "v1", "created": "Sun, 6 Nov 2016 02:40:41 GMT"}, {"version": "v2", "created": "Sat, 26 Nov 2016 01:08:47 GMT"}], "update_date": "2016-11-29", "authors_parsed": [["Wang", "Dilin", ""], ["Liu", "Qiang", ""]]}, {"id": "1611.01767", "submitter": "Xianhua Peng", "authors": "Steven Kou, Xianhua Peng, Xingbo Xu", "title": "EM Algorithm and Stochastic Control in Economics", "comments": "46 pages, 9 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "q-fin.EC math.OC stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Generalising the idea of the classical EM algorithm that is widely used for\ncomputing maximum likelihood estimates, we propose an EM-Control (EM-C)\nalgorithm for solving multi-period finite time horizon stochastic control\nproblems. The new algorithm sequentially updates the control policies in each\ntime period using Monte Carlo simulation in a forward-backward manner; in other\nwords, the algorithm goes forward in simulation and backward in optimization in\neach iteration. Similar to the EM algorithm, the EM-C algorithm has the\nmonotonicity of performance improvement in each iteration, leading to good\nconvergence properties. We demonstrate the effectiveness of the algorithm by\nsolving stochastic control problems in the monopoly pricing of perishable\nassets and in the study of real business cycle.\n", "versions": [{"version": "v1", "created": "Sun, 6 Nov 2016 12:37:29 GMT"}], "update_date": "2016-11-08", "authors_parsed": [["Kou", "Steven", ""], ["Peng", "Xianhua", ""], ["Xu", "Xingbo", ""]]}, {"id": "1611.01838", "submitter": "Pratik Chaudhari", "authors": "Pratik Chaudhari, Anna Choromanska, Stefano Soatto, Yann LeCun, Carlo\n  Baldassi, Christian Borgs, Jennifer Chayes, Levent Sagun, Riccardo Zecchina", "title": "Entropy-SGD: Biasing Gradient Descent Into Wide Valleys", "comments": "ICLR '17", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper proposes a new optimization algorithm called Entropy-SGD for\ntraining deep neural networks that is motivated by the local geometry of the\nenergy landscape. Local extrema with low generalization error have a large\nproportion of almost-zero eigenvalues in the Hessian with very few positive or\nnegative eigenvalues. We leverage upon this observation to construct a\nlocal-entropy-based objective function that favors well-generalizable solutions\nlying in large flat regions of the energy landscape, while avoiding\npoorly-generalizable solutions located in the sharp valleys. Conceptually, our\nalgorithm resembles two nested loops of SGD where we use Langevin dynamics in\nthe inner loop to compute the gradient of the local entropy before each update\nof the weights. We show that the new objective has a smoother energy landscape\nand show improved generalization over SGD using uniform stability, under\ncertain assumptions. Our experiments on convolutional and recurrent networks\ndemonstrate that Entropy-SGD compares favorably to state-of-the-art techniques\nin terms of generalization error and training time.\n", "versions": [{"version": "v1", "created": "Sun, 6 Nov 2016 20:22:49 GMT"}, {"version": "v2", "created": "Sun, 13 Nov 2016 23:49:08 GMT"}, {"version": "v3", "created": "Mon, 5 Dec 2016 15:17:18 GMT"}, {"version": "v4", "created": "Sat, 14 Jan 2017 04:25:53 GMT"}, {"version": "v5", "created": "Fri, 21 Apr 2017 07:16:30 GMT"}], "update_date": "2017-04-24", "authors_parsed": [["Chaudhari", "Pratik", ""], ["Choromanska", "Anna", ""], ["Soatto", "Stefano", ""], ["LeCun", "Yann", ""], ["Baldassi", "Carlo", ""], ["Borgs", "Christian", ""], ["Chayes", "Jennifer", ""], ["Sagun", "Levent", ""], ["Zecchina", "Riccardo", ""]]}, {"id": "1611.01843", "submitter": "Misha Denil", "authors": "Misha Denil, Pulkit Agrawal, Tejas D Kulkarni, Tom Erez, Peter\n  Battaglia, Nando de Freitas", "title": "Learning to Perform Physics Experiments via Deep Reinforcement Learning", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.AI cs.CV cs.LG cs.NE physics.soc-ph", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  When encountering novel objects, humans are able to infer a wide range of\nphysical properties such as mass, friction and deformability by interacting\nwith them in a goal driven way. This process of active interaction is in the\nsame spirit as a scientist performing experiments to discover hidden facts.\nRecent advances in artificial intelligence have yielded machines that can\nachieve superhuman performance in Go, Atari, natural language processing, and\ncomplex control problems; however, it is not clear that these systems can rival\nthe scientific intuition of even a young child. In this work we introduce a\nbasic set of tasks that require agents to estimate properties such as mass and\ncohesion of objects in an interactive simulated environment where they can\nmanipulate the objects and observe the consequences. We found that state of art\ndeep reinforcement learning methods can learn to perform the experiments\nnecessary to discover such hidden properties. By systematically manipulating\nthe problem difficulty and the cost incurred by the agent for performing\nexperiments, we found that agents learn different strategies that balance the\ncost of gathering information against the cost of making mistakes in different\nsituations.\n", "versions": [{"version": "v1", "created": "Sun, 6 Nov 2016 20:55:19 GMT"}, {"version": "v2", "created": "Mon, 14 Nov 2016 16:40:58 GMT"}, {"version": "v3", "created": "Thu, 17 Aug 2017 19:51:29 GMT"}], "update_date": "2017-08-21", "authors_parsed": [["Denil", "Misha", ""], ["Agrawal", "Pulkit", ""], ["Kulkarni", "Tejas D", ""], ["Erez", "Tom", ""], ["Battaglia", "Peter", ""], ["de Freitas", "Nando", ""]]}, {"id": "1611.01845", "submitter": "Yizheng Liao", "authors": "Yizheng Liao, Yang Weng, Guangyi Liu, Ram Rajagopal", "title": "Urban MV and LV Distribution Grid Topology Estimation via Group Lasso", "comments": "15 pages, 16 figures", "journal-ref": "IEEE transactions on power systems 2018", "doi": null, "report-no": null, "categories": "stat.ML cs.SY math.OC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The increasing penetration of distributed energy resources poses numerous\nreliability issues to the urban distribution grid. The topology estimation is a\ncritical step to ensure the robustness of distribution grid operation. However,\nthe bus connectivity and grid topology estimation are usually hard in\ndistribution grids. For example, it is technically challenging and costly to\nmonitor the bus connectivity in urban grids, e.g., underground lines. It is\nalso inappropriate to use the radial topology assumption exclusively because\nthe grids of metropolitan cities and regions with dense loads could be with\nmany mesh structures. To resolve these drawbacks, we propose a data-driven\ntopology estimation method for MV and LV distribution grids by only utilizing\nthe historical smart meter measurements. Particularly, a probabilistic\ngraphical model is utilized to capture the statistical dependencies amongst bus\nvoltages. We prove that the bus connectivity and grid topology estimation\nproblems, in radial and mesh structures, can be formulated as a linear\nregression with a least absolute shrinkage regularization on grouped variables\n(\\textit{group lasso}). Simulations show highly accurate results in eight MV\nand LV distribution networks at different sizes and 22 topology configurations\nusing PG\\&E residential smart meter data.\n", "versions": [{"version": "v1", "created": "Sun, 6 Nov 2016 20:59:10 GMT"}, {"version": "v2", "created": "Thu, 13 Sep 2018 04:43:21 GMT"}], "update_date": "2018-09-14", "authors_parsed": [["Liao", "Yizheng", ""], ["Weng", "Yang", ""], ["Liu", "Guangyi", ""], ["Rajagopal", "Ram", ""]]}, {"id": "1611.01886", "submitter": "Wentao Huang", "authors": "Wentao Huang and Kechen Zhang", "title": "An Information-Theoretic Framework for Fast and Robust Unsupervised\n  Learning via Neural Population Infomax", "comments": "25 pages, 7 figures, 5th International Conference on Learning\n  Representations (ICLR 2017)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI cs.IT math.IT q-bio.NC stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A framework is presented for unsupervised learning of representations based\non infomax principle for large-scale neural populations. We use an asymptotic\napproximation to the Shannon's mutual information for a large neural population\nto demonstrate that a good initial approximation to the global\ninformation-theoretic optimum can be obtained by a hierarchical infomax method.\nStarting from the initial solution, an efficient algorithm based on gradient\ndescent of the final objective function is proposed to learn representations\nfrom the input datasets, and the method works for complete, overcomplete, and\nundercomplete bases. As confirmed by numerical experiments, our method is\nrobust and highly efficient for extracting salient features from input\ndatasets. Compared with the main existing methods, our algorithm has a distinct\nadvantage in both the training speed and the robustness of unsupervised\nrepresentation learning. Furthermore, the proposed method is easily extended to\nthe supervised or unsupervised model for training deep structure networks.\n", "versions": [{"version": "v1", "created": "Mon, 7 Nov 2016 04:17:28 GMT"}, {"version": "v2", "created": "Thu, 19 Jan 2017 17:53:31 GMT"}, {"version": "v3", "created": "Mon, 6 Feb 2017 17:11:34 GMT"}, {"version": "v4", "created": "Fri, 10 Mar 2017 16:41:16 GMT"}], "update_date": "2017-03-13", "authors_parsed": [["Huang", "Wentao", ""], ["Zhang", "Kechen", ""]]}, {"id": "1611.01891", "submitter": "Masahiro Suzuki", "authors": "Masahiro Suzuki, Kotaro Nakayama, Yutaka Matsuo", "title": "Joint Multimodal Learning with Deep Generative Models", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We investigate deep generative models that can exchange multiple modalities\nbi-directionally, e.g., generating images from corresponding texts and vice\nversa. Recently, some studies handle multiple modalities on deep generative\nmodels, such as variational autoencoders (VAEs). However, these models\ntypically assume that modalities are forced to have a conditioned relation,\ni.e., we can only generate modalities in one direction. To achieve our\nobjective, we should extract a joint representation that captures high-level\nconcepts among all modalities and through which we can exchange them\nbi-directionally. As described herein, we propose a joint multimodal\nvariational autoencoder (JMVAE), in which all modalities are independently\nconditioned on joint representation. In other words, it models a joint\ndistribution of modalities. Furthermore, to be able to generate missing\nmodalities from the remaining modalities properly, we develop an additional\nmethod, JMVAE-kl, that is trained by reducing the divergence between JMVAE's\nencoder and prepared networks of respective modalities. Our experiments show\nthat our proposed method can obtain appropriate joint representation from\nmultiple modalities and that it can generate and reconstruct them more properly\nthan conventional VAEs. We further demonstrate that JMVAE can generate multiple\nmodalities bi-directionally.\n", "versions": [{"version": "v1", "created": "Mon, 7 Nov 2016 04:45:05 GMT"}], "update_date": "2016-11-08", "authors_parsed": [["Suzuki", "Masahiro", ""], ["Nakayama", "Kotaro", ""], ["Matsuo", "Yutaka", ""]]}, {"id": "1611.01900", "submitter": "Abhishake Rastogi", "authors": "Abhishake Rastogi and Sivananthan Sampath", "title": "Optimal rates for the regularized learning algorithms under general\n  source condition", "comments": null, "journal-ref": "Front. Appl. Math. Stat. 3:3", "doi": "10.3389/fams.2017.00003", "report-no": null, "categories": "stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider the learning algorithms under general source condition with the\npolynomial decay of the eigenvalues of the integral operator in vector-valued\nfunction setting. We discuss the upper convergence rates of Tikhonov\nregularizer under general source condition corresponding to increasing monotone\nindex function. The convergence issues are studied for general regularization\nschemes by using the concept of operator monotone index functions in minimax\nsetting. Further we also address the minimum possible error for any learning\nalgorithm.\n", "versions": [{"version": "v1", "created": "Mon, 7 Nov 2016 06:06:45 GMT"}, {"version": "v2", "created": "Tue, 28 Mar 2017 16:59:29 GMT"}], "update_date": "2017-03-29", "authors_parsed": [["Rastogi", "Abhishake", ""], ["Sampath", "Sivananthan", ""]]}, {"id": "1611.01929", "submitter": "Oron Anschel", "authors": "Oron Anschel, Nir Baram, Nahum Shimkin", "title": "Averaged-DQN: Variance Reduction and Stabilization for Deep\n  Reinforcement Learning", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Instability and variability of Deep Reinforcement Learning (DRL) algorithms\ntend to adversely affect their performance. Averaged-DQN is a simple extension\nto the DQN algorithm, based on averaging previously learned Q-values estimates,\nwhich leads to a more stable training procedure and improved performance by\nreducing approximation error variance in the target values. To understand the\neffect of the algorithm, we examine the source of value function estimation\nerrors and provide an analytical comparison within a simplified model. We\nfurther present experiments on the Arcade Learning Environment benchmark that\ndemonstrate significantly improved stability and performance due to the\nproposed extension.\n", "versions": [{"version": "v1", "created": "Mon, 7 Nov 2016 08:12:53 GMT"}, {"version": "v2", "created": "Tue, 8 Nov 2016 08:40:02 GMT"}, {"version": "v3", "created": "Wed, 8 Mar 2017 13:50:38 GMT"}, {"version": "v4", "created": "Fri, 10 Mar 2017 09:52:52 GMT"}], "update_date": "2017-03-13", "authors_parsed": [["Anschel", "Oron", ""], ["Baram", "Nir", ""], ["Shimkin", "Nahum", ""]]}, {"id": "1611.01957", "submitter": "Chao Qu", "authors": "Chao Qu, Yan Li, Huan Xu", "title": "Linear Convergence of SVRG in Statistical Estimation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  SVRG and its variants are among the state of art optimization algorithms for\nlarge scale machine learning problems. It is well known that SVRG converges\nlinearly when the objective function is strongly convex. However this setup can\nbe restrictive, and does not include several important formulations such as\nLasso, group Lasso, logistic regression, and some non-convex models including\ncorrected Lasso and SCAD. In this paper, we prove that, for a class of\nstatistical M-estimators covering examples mentioned above, SVRG solves the\nformulation with {\\em a linear convergence rate} without strong convexity or\neven convexity. Our analysis makes use of {\\em restricted strong convexity},\nunder which we show that SVRG converges linearly to the fundamental statistical\nprecision of the model, i.e., the difference between true unknown parameter\n$\\theta^*$ and the optimal solution $\\hat{\\theta}$ of the model.\n", "versions": [{"version": "v1", "created": "Mon, 7 Nov 2016 09:38:12 GMT"}, {"version": "v2", "created": "Sat, 28 Jan 2017 16:23:41 GMT"}, {"version": "v3", "created": "Thu, 27 Jul 2017 06:34:45 GMT"}], "update_date": "2017-07-28", "authors_parsed": [["Qu", "Chao", ""], ["Li", "Yan", ""], ["Xu", "Huan", ""]]}, {"id": "1611.01971", "submitter": "Nicolas Goix", "authors": "Nicolas Goix (LTCI), Nicolas Drougard (ISAE), Romain Brault (LTCI),\n  Ma\\\"el Chiapino (LTCI)", "title": "One Class Splitting Criteria for Random Forests", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Random Forests (RFs) are strong machine learning tools for classification and\nregression. However, they remain supervised algorithms, and no extension of RFs\nto the one-class setting has been proposed, except for techniques based on\nsecond-class sampling. This work fills this gap by proposing a natural\nmethodology to extend standard splitting criteria to the one-class setting,\nstructurally generalizing RFs to one-class classification. An extensive\nbenchmark of seven state-of-the-art anomaly detection algorithms is also\npresented. This empirically demonstrates the relevance of our approach.\n", "versions": [{"version": "v1", "created": "Mon, 7 Nov 2016 10:25:15 GMT"}, {"version": "v2", "created": "Fri, 18 Nov 2016 13:11:22 GMT"}, {"version": "v3", "created": "Mon, 21 Nov 2016 08:54:54 GMT"}], "update_date": "2016-11-22", "authors_parsed": [["Goix", "Nicolas", "", "LTCI"], ["Drougard", "Nicolas", "", "ISAE"], ["Brault", "Romain", "", "LTCI"], ["Chiapino", "Ma\u00ebl", "", "LTCI"]]}, {"id": "1611.02010", "submitter": "Jian Du", "authors": "Jian Du, Shaodan Ma, Yik-Chung Wu, Soummya Kar, and Jos\\'e M. F. Moura", "title": "Convergence Analysis of Distributed Inference with Vector-Valued\n  Gaussian Belief Propagation", "comments": "38 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.IT math.IT", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper considers inference over distributed linear Gaussian models using\nfactor graphs and Gaussian belief propagation (BP). The distributed inference\nalgorithm involves only local computation of the information matrix and of the\nmean vector, and message passing between neighbors. Under broad conditions, it\nis shown that the message information matrix converges to a unique positive\ndefinite limit matrix for arbitrary positive semidefinite initialization, and\nit approaches an arbitrarily small neighborhood of this limit matrix at a\ndoubly exponential rate. A necessary and sufficient convergence condition for\nthe belief mean vector to converge to the optimal centralized estimator is\nprovided under the assumption that the message information matrix is\ninitialized as a positive semidefinite matrix. Further, it is shown that\nGaussian BP always converges when the underlying factor graph is given by the\nunion of a forest and a single loop. The proposed convergence condition in the\nsetup of distributed linear Gaussian models is shown to be strictly weaker than\nother existing convergence conditions and requirements, including the Gaussian\nMarkov random field based walk-summability condition, and applicable to a large\nclass of scenarios.\n", "versions": [{"version": "v1", "created": "Mon, 7 Nov 2016 12:14:59 GMT"}, {"version": "v2", "created": "Tue, 18 Jul 2017 01:42:32 GMT"}, {"version": "v3", "created": "Fri, 29 Dec 2017 04:08:03 GMT"}], "update_date": "2018-01-01", "authors_parsed": [["Du", "Jian", ""], ["Ma", "Shaodan", ""], ["Wu", "Yik-Chung", ""], ["Kar", "Soummya", ""], ["Moura", "Jos\u00e9 M. F.", ""]]}, {"id": "1611.02041", "submitter": "Weihua Hu", "authors": "Weihua Hu, Gang Niu, Issei Sato, Masashi Sugiyama", "title": "Does Distributionally Robust Supervised Learning Give Robust\n  Classifiers?", "comments": "ICML 2018 camera-ready (final submission version)", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Distributionally Robust Supervised Learning (DRSL) is necessary for building\nreliable machine learning systems. When machine learning is deployed in the\nreal world, its performance can be significantly degraded because test data may\nfollow a different distribution from training data. DRSL with f-divergences\nexplicitly considers the worst-case distribution shift by minimizing the\nadversarially reweighted training loss. In this paper, we analyze this DRSL,\nfocusing on the classification scenario. Since the DRSL is explicitly\nformulated for a distribution shift scenario, we naturally expect it to give a\nrobust classifier that can aggressively handle shifted distributions. However,\nsurprisingly, we prove that the DRSL just ends up giving a classifier that\nexactly fits the given training distribution, which is too pessimistic. This\npessimism comes from two sources: the particular losses used in classification\nand the fact that the variety of distributions to which the DRSL tries to be\nrobust is too wide. Motivated by our analysis, we propose simple DRSL that\novercomes this pessimism and empirically demonstrate its effectiveness.\n", "versions": [{"version": "v1", "created": "Mon, 7 Nov 2016 13:19:45 GMT"}, {"version": "v2", "created": "Mon, 27 Feb 2017 06:38:24 GMT"}, {"version": "v3", "created": "Mon, 23 Oct 2017 10:48:48 GMT"}, {"version": "v4", "created": "Mon, 12 Feb 2018 10:23:50 GMT"}, {"version": "v5", "created": "Thu, 7 Jun 2018 14:14:15 GMT"}, {"version": "v6", "created": "Sun, 22 Jul 2018 07:49:28 GMT"}], "update_date": "2018-07-24", "authors_parsed": [["Hu", "Weihua", ""], ["Niu", "Gang", ""], ["Sato", "Issei", ""], ["Sugiyama", "Masashi", ""]]}, {"id": "1611.02047", "submitter": "Andrey Filchenkov", "authors": "Ivan Smetannikov, Ilya Isaev, Andrey Filchenkov", "title": "Reinforcement Learning Approach for Parallelization in Filters\n  Aggregation Based Feature Selection Algorithms", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  One of the classical problems in machine learning and data mining is feature\nselection. A feature selection algorithm is expected to be quick, and at the\nsame time it should show high performance. MeLiF algorithm effectively solves\nthis problem using ensembles of ranking filters. This article describes two\ndifferent ways to improve MeLiF algorithm performance with parallelization.\nExperiments show that proposed schemes significantly improves algorithm\nperformance and increase feature selection quality.\n", "versions": [{"version": "v1", "created": "Mon, 7 Nov 2016 13:43:38 GMT"}], "update_date": "2016-11-08", "authors_parsed": [["Smetannikov", "Ivan", ""], ["Isaev", "Ilya", ""], ["Filchenkov", "Andrey", ""]]}, {"id": "1611.02053", "submitter": "Andrey Filchenkov", "authors": "Valeria Efimova, Andrey Filchenkov, Anatoly Shalyto", "title": "Reinforcement-based Simultaneous Algorithm and its Hyperparameters\n  Selection", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Many algorithms for data analysis exist, especially for classification\nproblems. To solve a data analysis problem, a proper algorithm should be\nchosen, and also its hyperparameters should be selected. In this paper, we\npresent a new method for the simultaneous selection of an algorithm and its\nhyperparameters. In order to do so, we reduced this problem to the multi-armed\nbandit problem. We consider an algorithm as an arm and algorithm\nhyperparameters search during a fixed time as the corresponding arm play. We\nalso suggest a problem-specific reward function. We performed the experiments\non 10 real datasets and compare the suggested method with the existing one\nimplemented in Auto-WEKA. The results show that our method is significantly\nbetter in most of the cases and never worse than the Auto-WEKA.\n", "versions": [{"version": "v1", "created": "Mon, 7 Nov 2016 13:55:00 GMT"}], "update_date": "2016-11-08", "authors_parsed": [["Efimova", "Valeria", ""], ["Filchenkov", "Andrey", ""], ["Shalyto", "Anatoly", ""]]}, {"id": "1611.02101", "submitter": "Ilya Trofimov", "authors": "Ilya Trofimov, Alexander Genkin", "title": "Distributed Coordinate Descent for Generalized Linear Models with\n  Regularization", "comments": "fix typos. arXiv admin note: text overlap with arXiv:1411.6520", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.DC cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Generalized linear model with $L_1$ and $L_2$ regularization is a widely used\ntechnique for solving classification, class probability estimation and\nregression problems. With the numbers of both features and examples growing\nrapidly in the fields like text mining and clickstream data analysis\nparallelization and the use of cluster architectures becomes important. We\npresent a novel algorithm for fitting regularized generalized linear models in\nthe distributed environment. The algorithm splits data between nodes by\nfeatures, uses coordinate descent on each node and line search to merge results\nglobally. Convergence proof is provided. A modifications of the algorithm\naddresses slow node problem. For an important particular case of logistic\nregression we empirically compare our program with several state-of-the art\napproaches that rely on different algorithmic and data spitting methods.\nExperiments demonstrate that our approach is scalable and superior when\ntraining on large and sparse datasets.\n", "versions": [{"version": "v1", "created": "Mon, 7 Nov 2016 15:19:54 GMT"}, {"version": "v2", "created": "Mon, 26 Jun 2017 13:35:23 GMT"}], "update_date": "2017-06-28", "authors_parsed": [["Trofimov", "Ilya", ""], ["Genkin", "Alexander", ""]]}, {"id": "1611.02163", "submitter": "Luke Metz", "authors": "Luke Metz, Ben Poole, David Pfau, Jascha Sohl-Dickstein", "title": "Unrolled Generative Adversarial Networks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We introduce a method to stabilize Generative Adversarial Networks (GANs) by\ndefining the generator objective with respect to an unrolled optimization of\nthe discriminator. This allows training to be adjusted between using the\noptimal discriminator in the generator's objective, which is ideal but\ninfeasible in practice, and using the current value of the discriminator, which\nis often unstable and leads to poor solutions. We show how this technique\nsolves the common problem of mode collapse, stabilizes training of GANs with\ncomplex recurrent generators, and increases diversity and coverage of the data\ndistribution by the generator.\n", "versions": [{"version": "v1", "created": "Mon, 7 Nov 2016 16:42:09 GMT"}, {"version": "v2", "created": "Sun, 15 Jan 2017 04:35:50 GMT"}, {"version": "v3", "created": "Tue, 31 Jan 2017 18:12:26 GMT"}, {"version": "v4", "created": "Fri, 12 May 2017 23:52:12 GMT"}], "update_date": "2017-05-16", "authors_parsed": [["Metz", "Luke", ""], ["Poole", "Ben", ""], ["Pfau", "David", ""], ["Sohl-Dickstein", "Jascha", ""]]}, {"id": "1611.02181", "submitter": "Wen Dong", "authors": "Zhen Xu, Wen Dong and Sargur Srihari", "title": "Using Social Dynamics to Make Individual Predictions: Variational\n  Inference with a Stochastic Kinetic Model", "comments": "In proceedings of 29th Conference on Neural Information Processing\n  Systems (NIPS 2016)", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://creativecommons.org/publicdomain/zero/1.0/", "abstract": "  Social dynamics is concerned primarily with interactions among individuals\nand the resulting group behaviors, modeling the temporal evolution of social\nsystems via the interactions of individuals within these systems. In\nparticular, the availability of large-scale data from social networks and\nsensor networks offers an unprecedented opportunity to predict state-changing\nevents at the individual level. Examples of such events include disease\ntransmission, opinion transition in elections, and rumor propagation. Unlike\nprevious research focusing on the collective effects of social systems, this\nstudy makes efficient inferences at the individual level. In order to cope with\ndynamic interactions among a large number of individuals, we introduce the\nstochastic kinetic model to capture adaptive transition probabilities and\npropose an efficient variational inference algorithm the complexity of which\ngrows linearly --- rather than exponentially --- with the number of\nindividuals. To validate this method, we have performed epidemic-dynamics\nexperiments on wireless sensor network data collected from more than ten\nthousand people over three years. The proposed algorithm was used to track\ndisease transmission and predict the probability of infection for each\nindividual. Our results demonstrate that this method is more efficient than\nsampling while nonetheless achieving high accuracy.\n", "versions": [{"version": "v1", "created": "Mon, 7 Nov 2016 17:29:51 GMT"}], "update_date": "2016-11-08", "authors_parsed": [["Xu", "Zhen", ""], ["Dong", "Wen", ""], ["Srihari", "Sargur", ""]]}, {"id": "1611.02221", "submitter": "Amit Moscovich", "authors": "Amit Moscovich, Ariel Jaffe, Boaz Nadler", "title": "Minimax-optimal semi-supervised regression on unknown manifolds", "comments": null, "journal-ref": "Proceedings of the 20th International Conference on Artificial\n  Intelligence and Statistics, PMLR 54 (2017) 933-942", "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider semi-supervised regression when the predictor variables are drawn\nfrom an unknown manifold. A simple two step approach to this problem is to: (i)\nestimate the manifold geodesic distance between any pair of points using both\nthe labeled and unlabeled instances; and (ii) apply a k nearest neighbor\nregressor based on these distance estimates. We prove that given sufficiently\nmany unlabeled points, this simple method of geodesic kNN regression achieves\nthe optimal finite-sample minimax bound on the mean squared error, as if the\nmanifold were known. Furthermore, we show how this approach can be efficiently\nimplemented, requiring only O(k N log N) operations to estimate the regression\nfunction at all N labeled and unlabeled points. We illustrate this approach on\ntwo datasets with a manifold structure: indoor localization using WiFi\nfingerprints and facial pose estimation. In both cases, geodesic kNN is more\naccurate and much faster than the popular Laplacian eigenvector regressor.\n", "versions": [{"version": "v1", "created": "Mon, 7 Nov 2016 19:26:15 GMT"}, {"version": "v2", "created": "Mon, 6 Mar 2017 19:13:07 GMT"}], "update_date": "2019-09-16", "authors_parsed": [["Moscovich", "Amit", ""], ["Jaffe", "Ariel", ""], ["Nadler", "Boaz", ""]]}, {"id": "1611.02252", "submitter": "Miguel L\\'azaro-Gredilla", "authors": "Miguel L\\'azaro-Gredilla, Yi Liu, D. Scott Phoenix, Dileep George", "title": "Hierarchical compositional feature learning", "comments": "Removed the \"under review\" header from every page, no changes to\n  content", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We introduce the hierarchical compositional network (HCN), a directed\ngenerative model able to discover and disentangle, without supervision, the\nbuilding blocks of a set of binary images. The building blocks are binary\nfeatures defined hierarchically as a composition of some of the features in the\nlayer immediately below, arranged in a particular manner. At a high level, HCN\nis similar to a sigmoid belief network with pooling. Inference and learning in\nHCN are very challenging and existing variational approximations do not work\nsatisfactorily. A main contribution of this work is to show that both can be\naddressed using max-product message passing (MPMP) with a particular schedule\n(no EM required). Also, using MPMP as an inference engine for HCN makes new\ntasks simple: adding supervision information, classifying images, or performing\ninpainting all correspond to clamping some variables of the model to their\nknown values and running MPMP on the rest. When used for classification, fast\ninference with HCN has exactly the same functional form as a convolutional\nneural network (CNN) with linear activations and binary weights. However, HCN's\nfeatures are qualitatively very different.\n", "versions": [{"version": "v1", "created": "Mon, 7 Nov 2016 20:25:08 GMT"}, {"version": "v2", "created": "Thu, 26 Oct 2017 01:23:40 GMT"}], "update_date": "2017-10-27", "authors_parsed": [["L\u00e1zaro-Gredilla", "Miguel", ""], ["Liu", "Yi", ""], ["Phoenix", "D. Scott", ""], ["George", "Dileep", ""]]}, {"id": "1611.02258", "submitter": "Roy Adams Roy Adams", "authors": "Roy J. Adams, Benjamin M. Marlin", "title": "Learning Time Series Detection Models from Temporally Imprecise Labels", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we consider a new low-quality label learning problem: learning\ntime series detection models from temporally imprecise labels. In this problem,\nthe data consist of a set of input time series, and supervision is provided by\na sequence of noisy time stamps corresponding to the occurrence of positive\nclass events. Such temporally imprecise labels commonly occur in areas like\nmobile health research where human annotators are tasked with labeling the\noccurrence of very short duration events. We propose a general learning\nframework for this problem that can accommodate different base classifiers and\nnoise models. We present results on real mobile health data showing that the\nproposed framework significantly outperforms a number of alternatives including\nassuming that the label time stamps are noise-free, transforming the problem\ninto the multiple instance learning framework, and learning on labels that were\nmanually re-aligned.\n", "versions": [{"version": "v1", "created": "Mon, 7 Nov 2016 20:36:56 GMT"}, {"version": "v2", "created": "Thu, 13 Apr 2017 17:36:23 GMT"}], "update_date": "2017-04-14", "authors_parsed": [["Adams", "Roy J.", ""], ["Marlin", "Benjamin M.", ""]]}, {"id": "1611.02266", "submitter": "Ryota Tomioka", "authors": "Liwen Zhang and John Winn and Ryota Tomioka", "title": "Gaussian Attention Model and Its Application to Knowledge Base Embedding\n  and Question Answering", "comments": "16 pages, 4 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.AI cs.CL cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose the Gaussian attention model for content-based neural memory\naccess. With the proposed attention model, a neural network has the additional\ndegree of freedom to control the focus of its attention from a laser sharp\nattention to a broad attention. It is applicable whenever we can assume that\nthe distance in the latent space reflects some notion of semantics. We use the\nproposed attention model as a scoring function for the embedding of a knowledge\nbase into a continuous vector space and then train a model that performs\nquestion answering about the entities in the knowledge base. The proposed\nattention model can handle both the propagation of uncertainty when following a\nseries of relations and also the conjunction of conditions in a natural way. On\na dataset of soccer players who participated in the FIFA World Cup 2014, we\ndemonstrate that our model can handle both path queries and conjunctive queries\nwell.\n", "versions": [{"version": "v1", "created": "Mon, 7 Nov 2016 20:57:24 GMT"}, {"version": "v2", "created": "Wed, 30 Nov 2016 16:44:17 GMT"}], "update_date": "2016-12-01", "authors_parsed": [["Zhang", "Liwen", ""], ["Winn", "John", ""], ["Tomioka", "Ryota", ""]]}, {"id": "1611.02268", "submitter": "Akshay Balsubramani", "authors": "Akshay Balsubramani", "title": "Optimal Binary Autoencoding with Pairwise Correlations", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We formulate learning of a binary autoencoder as a biconvex optimization\nproblem which learns from the pairwise correlations between encoded and decoded\nbits. Among all possible algorithms that use this information, ours finds the\nautoencoder that reconstructs its inputs with worst-case optimal loss. The\noptimal decoder is a single layer of artificial neurons, emerging entirely from\nthe minimax loss minimization, and with weights learned by convex optimization.\nAll this is reflected in competitive experimental results, demonstrating that\nbinary autoencoding can be done efficiently by conveying information in\npairwise correlations in an optimal fashion.\n", "versions": [{"version": "v1", "created": "Mon, 7 Nov 2016 20:58:58 GMT"}], "update_date": "2016-11-08", "authors_parsed": [["Balsubramani", "Akshay", ""]]}, {"id": "1611.02304", "submitter": "Mevlana Gemici", "authors": "Mevlana C. Gemici, Danilo Rezende, Shakir Mohamed", "title": "Normalizing Flows on Riemannian Manifolds", "comments": "3 pages, 2 figures, Submitted to Workshop on Bayesian Deep Learning\n  at NIPS 2016", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.AI math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider the problem of density estimation on Riemannian manifolds.\nDensity estimation on manifolds has many applications in fluid-mechanics,\noptics and plasma physics and it appears often when dealing with angular\nvariables (such as used in protein folding, robot limbs, gene-expression) and\nin general directional statistics. In spite of the multitude of algorithms\navailable for density estimation in the Euclidean spaces $\\mathbf{R}^n$ that\nscale to large n (e.g. normalizing flows, kernel methods and variational\napproximations), most of these methods are not immediately suitable for density\nestimation in more general Riemannian manifolds. We revisit techniques related\nto homeomorphisms from differential geometry for projecting densities to\nsub-manifolds and use it to generalize the idea of normalizing flows to more\ngeneral Riemannian manifolds. The resulting algorithm is scalable, simple to\nimplement and suitable for use with automatic differentiation. We demonstrate\nconcrete examples of this method on the n-sphere $\\mathbf{S}^n$.\n", "versions": [{"version": "v1", "created": "Mon, 7 Nov 2016 21:21:34 GMT"}, {"version": "v2", "created": "Wed, 9 Nov 2016 07:16:26 GMT"}], "update_date": "2016-11-10", "authors_parsed": [["Gemici", "Mevlana C.", ""], ["Rezende", "Danilo", ""], ["Mohamed", "Shakir", ""]]}, {"id": "1611.02305", "submitter": "Xinran He", "authors": "Xinran He, Ke Xu, David Kempe and Yan Liu", "title": "Learning Influence Functions from Incomplete Observations", "comments": "Full version of paper \"Learning Influence Functions from Incomplete\n  Observations\" in NIPS16", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.SI cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study the problem of learning influence functions under incomplete\nobservations of node activations. Incomplete observations are a major concern\nas most (online and real-world) social networks are not fully observable. We\nestablish both proper and improper PAC learnability of influence functions\nunder randomly missing observations. Proper PAC learnability under the\nDiscrete-Time Linear Threshold (DLT) and Discrete-Time Independent Cascade\n(DIC) models is established by reducing incomplete observations to complete\nobservations in a modified graph. Our improper PAC learnability result applies\nfor the DLT and DIC models as well as the Continuous-Time Independent Cascade\n(CIC) model. It is based on a parametrization in terms of reachability\nfeatures, and also gives rise to an efficient and practical heuristic.\nExperiments on synthetic and real-world datasets demonstrate the ability of our\nmethod to compensate even for a fairly large fraction of missing observations.\n", "versions": [{"version": "v1", "created": "Mon, 7 Nov 2016 21:28:40 GMT"}], "update_date": "2016-11-09", "authors_parsed": [["He", "Xinran", ""], ["Xu", "Ke", ""], ["Kempe", "David", ""], ["Liu", "Yan", ""]]}, {"id": "1611.02320", "submitter": "Juan Maro\\~nas", "authors": "Juan Maro\\~nas Molano, Alberto Albiol Colomer, Roberto Paredes\n  Palacios", "title": "Adversarial Ladder Networks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NE cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The use of unsupervised data in addition to supervised data in training\ndiscriminative neural networks has improved the performance of this clas-\nsification scheme. However, the best results were achieved with a training\nprocess that is divided in two parts: first an unsupervised pre-training step\nis done for initializing the weights of the network and after these weights are\nrefined with the use of supervised data. On the other hand adversarial noise\nhas improved the results of clas- sical supervised learning. Recently, a new\nneural network topology called Ladder Network, where the key idea is based in\nsome properties of hierar- chichal latent variable models, has been proposed as\na technique to train a neural network using supervised and unsupervised data at\nthe same time with what is called semi-supervised learning. This technique has\nreached state of the art classification. In this work we add adversarial noise\nto the ladder network and get state of the art classification, with several\nimportant conclusions on how adversarial noise can help in addition with new\npossible lines of investi- gation. We also propose an alternative to add\nadversarial noise to unsu- pervised data.\n", "versions": [{"version": "v1", "created": "Mon, 7 Nov 2016 22:03:43 GMT"}, {"version": "v2", "created": "Thu, 26 Apr 2018 15:22:31 GMT"}, {"version": "v3", "created": "Fri, 27 Apr 2018 08:16:36 GMT"}], "update_date": "2018-04-30", "authors_parsed": [["Molano", "Juan Maro\u00f1as", ""], ["Colomer", "Alberto Albiol", ""], ["Palacios", "Roberto Paredes", ""]]}, {"id": "1611.02345", "submitter": "David Balduzzi", "authors": "David Balduzzi, Brian McWilliams, Tony Butler-Yeoman", "title": "Neural Taylor Approximations: Convergence and Exploration in Rectifier\n  Networks", "comments": "ICML 2017, final version", "journal-ref": "PMLR volume 70, 2017", "doi": null, "report-no": null, "categories": "cs.LG cs.NE stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Modern convolutional networks, incorporating rectifiers and max-pooling, are\nneither smooth nor convex; standard guarantees therefore do not apply.\nNevertheless, methods from convex optimization such as gradient descent and\nAdam are widely used as building blocks for deep learning algorithms. This\npaper provides the first convergence guarantee applicable to modern convnets,\nwhich furthermore matches a lower bound for convex nonsmooth functions. The key\ntechnical tool is the neural Taylor approximation -- a straightforward\napplication of Taylor expansions to neural networks -- and the associated\nTaylor loss. Experiments on a range of optimizers, layers, and tasks provide\nevidence that the analysis accurately captures the dynamics of neural\noptimization. The second half of the paper applies the Taylor approximation to\nisolate the main difficulty in training rectifier nets -- that gradients are\nshattered -- and investigates the hypothesis that, by exploring the space of\nactivation configurations more thoroughly, adaptive optimizers such as RMSProp\nand Adam are able to converge to better solutions.\n", "versions": [{"version": "v1", "created": "Mon, 7 Nov 2016 23:47:05 GMT"}, {"version": "v2", "created": "Fri, 24 Feb 2017 02:26:15 GMT"}, {"version": "v3", "created": "Wed, 6 Jun 2018 12:41:26 GMT"}], "update_date": "2018-06-07", "authors_parsed": [["Balduzzi", "David", ""], ["McWilliams", "Brian", ""], ["Butler-Yeoman", "Tony", ""]]}, {"id": "1611.02365", "submitter": "Christopher Xie", "authors": "Christopher Xie, Avleen Bijral, Juan Lavista Ferres", "title": "NonSTOP: A NonSTationary Online Prediction Method for Time Series", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present online prediction methods for time series that let us explicitly\nhandle nonstationary artifacts (e.g. trend and seasonality) present in most\nreal time series. Specifically, we show that applying appropriate\ntransformations to such time series before prediction can lead to improved\ntheoretical and empirical prediction performance. Moreover, since these\ntransformations are usually unknown, we employ the learning with experts\nsetting to develop a fully online method (NonSTOP-NonSTationary Online\nPrediction) for predicting nonstationary time series. This framework allows for\nseasonality and/or other trends in univariate time series and cointegration in\nmultivariate time series. Our algorithms and regret analysis subsume recent\nrelated work while significantly expanding the applicability of such methods.\nFor all the methods, we provide sub-linear regret bounds using relaxed\nassumptions. The theoretical guarantees do not fully capture the benefits of\nthe transformations, thus we provide a data-dependent analysis of the\nfollow-the-leader algorithm that provides insight into the success of using\nsuch transformations. We support all of our results with experiments on\nsimulated and real data.\n", "versions": [{"version": "v1", "created": "Tue, 8 Nov 2016 02:20:46 GMT"}, {"version": "v2", "created": "Thu, 22 Jun 2017 23:10:15 GMT"}, {"version": "v3", "created": "Tue, 3 Oct 2017 07:35:16 GMT"}, {"version": "v4", "created": "Sun, 26 Aug 2018 19:23:06 GMT"}], "update_date": "2018-08-28", "authors_parsed": [["Xie", "Christopher", ""], ["Bijral", "Avleen", ""], ["Ferres", "Juan Lavista", ""]]}, {"id": "1611.02385", "submitter": "Alexander Peysakhovich", "authors": "Alexander Peysakhovich, Akos Lada", "title": "Combining observational and experimental data to find heterogeneous\n  treatment effects", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Every design choice will have different effects on different units. However\ntraditional A/B tests are often underpowered to identify these heterogeneous\neffects. This is especially true when the set of unit-level attributes is\nhigh-dimensional and our priors are weak about which particular covariates are\nimportant. However, there are often observational data sets available that are\norders of magnitude larger. We propose a method to combine these two data\nsources to estimate heterogeneous treatment effects. First, we use\nobservational time series data to estimate a mapping from covariates to\nunit-level effects. These estimates are likely biased but under some conditions\nthe bias preserves unit-level relative rank orderings. If these conditions\nhold, we only need sufficient experimental data to identify a monotonic,\none-dimensional transformation from observationally predicted treatment effects\nto real treatment effects. This reduces power demands greatly and makes the\ndetection of heterogeneous effects much easier. As an application, we show how\nour method can be used to improve Facebook page recommendations.\n", "versions": [{"version": "v1", "created": "Tue, 8 Nov 2016 04:40:27 GMT"}], "update_date": "2016-11-09", "authors_parsed": [["Peysakhovich", "Alexander", ""], ["Lada", "Akos", ""]]}, {"id": "1611.02401", "submitter": "Joan Bruna", "authors": "Alex Nowak-Vila, David Folqu\\'e and Joan Bruna", "title": "Divide and Conquer Networks", "comments": "ICLR 2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider the learning of algorithmic tasks by mere observation of\ninput-output pairs. Rather than studying this as a black-box discrete\nregression problem with no assumption whatsoever on the input-output mapping,\nwe concentrate on tasks that are amenable to the principle of divide and\nconquer, and study what are its implications in terms of learning. This\nprinciple creates a powerful inductive bias that we leverage with neural\narchitectures that are defined recursively and dynamically, by learning two\nscale-invariant atomic operations: how to split a given input into smaller\nsets, and how to merge two partially solved tasks into a larger partial\nsolution. Our model can be trained in weakly supervised environments, namely by\njust observing input-output pairs, and in even weaker environments, using a\nnon-differentiable reward signal. Moreover, thanks to the dynamic aspect of our\narchitecture, we can incorporate the computational complexity as a\nregularization term that can be optimized by backpropagation. We demonstrate\nthe flexibility and efficiency of the Divide-and-Conquer Network on several\ncombinatorial and geometric tasks: convex hull, clustering, knapsack and\neuclidean TSP. Thanks to the dynamic programming nature of our model, we show\nsignificant improvements in terms of generalization error and computational\ncomplexity.\n", "versions": [{"version": "v1", "created": "Tue, 8 Nov 2016 06:07:25 GMT"}, {"version": "v2", "created": "Wed, 9 Nov 2016 01:58:20 GMT"}, {"version": "v3", "created": "Sun, 13 Nov 2016 05:25:00 GMT"}, {"version": "v4", "created": "Sat, 27 May 2017 12:01:13 GMT"}, {"version": "v5", "created": "Wed, 31 May 2017 04:57:49 GMT"}, {"version": "v6", "created": "Tue, 22 May 2018 20:47:33 GMT"}, {"version": "v7", "created": "Sun, 14 Oct 2018 18:11:39 GMT"}], "update_date": "2018-10-16", "authors_parsed": [["Nowak-Vila", "Alex", ""], ["Folqu\u00e9", "David", ""], ["Bruna", "Joan", ""]]}, {"id": "1611.02440", "submitter": "Victor Picheny", "authors": "Victor Picheny, Mickael Binois, Abderrahmane Habbal", "title": "A Bayesian optimization approach to find Nash equilibria", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Game theory finds nowadays a broad range of applications in engineering and\nmachine learning. However, in a derivative-free, expensive black-box context,\nvery few algorithmic solutions are available to find game equilibria. Here, we\npropose a novel Gaussian-process based approach for solving games in this\ncontext. We follow a classical Bayesian optimization framework, with sequential\nsampling decisions based on acquisition functions. Two strategies are proposed,\nbased either on the probability of achieving equilibrium or on the Stepwise\nUncertainty Reduction paradigm. Practical and numerical aspects are discussed\nin order to enhance the scalability and reduce computation time. Our approach\nis evaluated on several synthetic game problems with varying number of players\nand decision space dimensions. We show that equilibria can be found reliably\nfor a fraction of the cost (in terms of black-box evaluations) compared to\nclassical, derivative-based algorithms. The method is available in the R\npackage GPGame available on CRAN at https://cran.r-project.org/package=GPGame.\n", "versions": [{"version": "v1", "created": "Tue, 8 Nov 2016 09:17:17 GMT"}, {"version": "v2", "created": "Tue, 27 Feb 2018 10:43:51 GMT"}], "update_date": "2018-02-28", "authors_parsed": [["Picheny", "Victor", ""], ["Binois", "Mickael", ""], ["Habbal", "Abderrahmane", ""]]}, {"id": "1611.02648", "submitter": "Nat Dilokthanakul", "authors": "Nat Dilokthanakul, Pedro A.M. Mediano, Marta Garnelo, Matthew C.H.\n  Lee, Hugh Salimbeni, Kai Arulkumaran, Murray Shanahan", "title": "Deep Unsupervised Clustering with Gaussian Mixture Variational\n  Autoencoders", "comments": "12 pages, 6 figures, Under review as a conference paper at ICLR 2017", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.NE stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study a variant of the variational autoencoder model (VAE) with a Gaussian\nmixture as a prior distribution, with the goal of performing unsupervised\nclustering through deep generative models. We observe that the known problem of\nover-regularisation that has been shown to arise in regular VAEs also manifests\nitself in our model and leads to cluster degeneracy. We show that a heuristic\ncalled minimum information constraint that has been shown to mitigate this\neffect in VAEs can also be applied to improve unsupervised clustering\nperformance with our model. Furthermore we analyse the effect of this heuristic\nand provide an intuition of the various processes with the help of\nvisualizations. Finally, we demonstrate the performance of our model on\nsynthetic data, MNIST and SVHN, showing that the obtained clusters are\ndistinct, interpretable and result in achieving competitive performance on\nunsupervised clustering to the state-of-the-art results.\n", "versions": [{"version": "v1", "created": "Tue, 8 Nov 2016 18:36:36 GMT"}, {"version": "v2", "created": "Fri, 13 Jan 2017 17:53:10 GMT"}], "update_date": "2017-01-16", "authors_parsed": [["Dilokthanakul", "Nat", ""], ["Mediano", "Pedro A. M.", ""], ["Garnelo", "Marta", ""], ["Lee", "Matthew C. H.", ""], ["Salimbeni", "Hugh", ""], ["Arulkumaran", "Kai", ""], ["Shanahan", "Murray", ""]]}, {"id": "1611.02704", "submitter": "Sebastian Liem", "authors": "Gianfranco Bertone, Marc Peter Deisenroth, Jong Soo Kim, Sebastian\n  Liem, Roberto Ruiz de Austri, Max Welling", "title": "Accelerating the BSM interpretation of LHC data with machine learning", "comments": "5 pages, 2 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "hep-ph stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The interpretation of Large Hadron Collider (LHC) data in the framework of\nBeyond the Standard Model (BSM) theories is hampered by the need to run\ncomputationally expensive event generators and detector simulators. Performing\nstatistically convergent scans of high-dimensional BSM theories is consequently\nchallenging, and in practice unfeasible for very high-dimensional BSM theories.\nWe present here a new machine learning method that accelerates the\ninterpretation of LHC data, by learning the relationship between BSM theory\nparameters and data. As a proof-of-concept, we demonstrate that this technique\naccurately predicts natural SUSY signal events in two signal regions at the\nHigh Luminosity LHC, up to four orders of magnitude faster than standard\ntechniques. The new approach makes it possible to rapidly and accurately\nreconstruct the theory parameters of complex BSM theories, should an excess in\nthe data be discovered at the LHC.\n", "versions": [{"version": "v1", "created": "Tue, 8 Nov 2016 21:00:00 GMT"}], "update_date": "2016-11-14", "authors_parsed": [["Bertone", "Gianfranco", ""], ["Deisenroth", "Marc Peter", ""], ["Kim", "Jong Soo", ""], ["Liem", "Sebastian", ""], ["de Austri", "Roberto Ruiz", ""], ["Welling", "Max", ""]]}, {"id": "1611.02731", "submitter": "Xi Chen", "authors": "Xi Chen, Diederik P. Kingma, Tim Salimans, Yan Duan, Prafulla\n  Dhariwal, John Schulman, Ilya Sutskever, Pieter Abbeel", "title": "Variational Lossy Autoencoder", "comments": "Added CIFAR10 experiments; ICLR 2017", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Representation learning seeks to expose certain aspects of observed data in a\nlearned representation that's amenable to downstream tasks like classification.\nFor instance, a good representation for 2D images might be one that describes\nonly global structure and discards information about detailed texture. In this\npaper, we present a simple but principled method to learn such global\nrepresentations by combining Variational Autoencoder (VAE) with neural\nautoregressive models such as RNN, MADE and PixelRNN/CNN. Our proposed VAE\nmodel allows us to have control over what the global latent code can learn and\n, by designing the architecture accordingly, we can force the global latent\ncode to discard irrelevant information such as texture in 2D images, and hence\nthe VAE only \"autoencodes\" data in a lossy fashion. In addition, by leveraging\nautoregressive models as both prior distribution $p(z)$ and decoding\ndistribution $p(x|z)$, we can greatly improve generative modeling performance\nof VAEs, achieving new state-of-the-art results on MNIST, OMNIGLOT and\nCaltech-101 Silhouettes density estimation tasks.\n", "versions": [{"version": "v1", "created": "Tue, 8 Nov 2016 21:43:34 GMT"}, {"version": "v2", "created": "Sat, 4 Mar 2017 06:19:22 GMT"}], "update_date": "2017-03-07", "authors_parsed": [["Chen", "Xi", ""], ["Kingma", "Diederik P.", ""], ["Salimans", "Tim", ""], ["Duan", "Yan", ""], ["Dhariwal", "Prafulla", ""], ["Schulman", "John", ""], ["Sutskever", "Ilya", ""], ["Abbeel", "Pieter", ""]]}, {"id": "1611.02755", "submitter": "Abram Friesen", "authors": "Abram L. Friesen and Pedro Domingos", "title": "Recursive Decomposition for Nonconvex Optimization", "comments": "11 pages, 7 figures, pdflatex", "journal-ref": "Proceedings of the 24th International Joint Conference on\n  Artificial Intelligence (2015), pp. 253-259", "doi": null, "report-no": null, "categories": "cs.AI cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Continuous optimization is an important problem in many areas of AI,\nincluding vision, robotics, probabilistic inference, and machine learning.\nUnfortunately, most real-world optimization problems are nonconvex, causing\nstandard convex techniques to find only local optima, even with extensions like\nrandom restarts and simulated annealing. We observe that, in many cases, the\nlocal modes of the objective function have combinatorial structure, and thus\nideas from combinatorial optimization can be brought to bear. Based on this, we\npropose a problem-decomposition approach to nonconvex optimization. Similarly\nto DPLL-style SAT solvers and recursive conditioning in probabilistic\ninference, our algorithm, RDIS, recursively sets variables so as to simplify\nand decompose the objective function into approximately independent\nsub-functions, until the remaining functions are simple enough to be optimized\nby standard techniques like gradient descent. The variables to set are chosen\nby graph partitioning, ensuring decomposition whenever possible. We show\nanalytically that RDIS can solve a broad class of nonconvex optimization\nproblems exponentially faster than gradient descent with random restarts.\nExperimentally, RDIS outperforms standard techniques on problems like structure\nfrom motion and protein folding.\n", "versions": [{"version": "v1", "created": "Tue, 8 Nov 2016 22:52:08 GMT"}], "update_date": "2016-11-10", "authors_parsed": [["Friesen", "Abram L.", ""], ["Domingos", "Pedro", ""]]}, {"id": "1611.02779", "submitter": "Yan Duan", "authors": "Yan Duan, John Schulman, Xi Chen, Peter L. Bartlett, Ilya Sutskever,\n  Pieter Abbeel", "title": "RL$^2$: Fast Reinforcement Learning via Slow Reinforcement Learning", "comments": "14 pages. Under review as a conference paper at ICLR 2017", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.LG cs.NE stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Deep reinforcement learning (deep RL) has been successful in learning\nsophisticated behaviors automatically; however, the learning process requires a\nhuge number of trials. In contrast, animals can learn new tasks in just a few\ntrials, benefiting from their prior knowledge about the world. This paper seeks\nto bridge this gap. Rather than designing a \"fast\" reinforcement learning\nalgorithm, we propose to represent it as a recurrent neural network (RNN) and\nlearn it from data. In our proposed method, RL$^2$, the algorithm is encoded in\nthe weights of the RNN, which are learned slowly through a general-purpose\n(\"slow\") RL algorithm. The RNN receives all information a typical RL algorithm\nwould receive, including observations, actions, rewards, and termination flags;\nand it retains its state across episodes in a given Markov Decision Process\n(MDP). The activations of the RNN store the state of the \"fast\" RL algorithm on\nthe current (previously unseen) MDP. We evaluate RL$^2$ experimentally on both\nsmall-scale and large-scale problems. On the small-scale side, we train it to\nsolve randomly generated multi-arm bandit problems and finite MDPs. After\nRL$^2$ is trained, its performance on new MDPs is close to human-designed\nalgorithms with optimality guarantees. On the large-scale side, we test RL$^2$\non a vision-based navigation task and show that it scales up to\nhigh-dimensional problems.\n", "versions": [{"version": "v1", "created": "Wed, 9 Nov 2016 00:13:29 GMT"}, {"version": "v2", "created": "Thu, 10 Nov 2016 01:17:36 GMT"}], "update_date": "2016-11-11", "authors_parsed": [["Duan", "Yan", ""], ["Schulman", "John", ""], ["Chen", "Xi", ""], ["Bartlett", "Peter L.", ""], ["Sutskever", "Ilya", ""], ["Abbeel", "Pieter", ""]]}, {"id": "1611.02869", "submitter": "Jens Sj\\\"olund", "authors": "Jens Sj\\\"olund, Anders Eklund, Evren \\\"Ozarslan and Hans Knutsson", "title": "Gaussian process regression can turn non-uniform and undersampled\n  diffusion MRI data into diffusion spectrum imaging", "comments": "5 pages", "journal-ref": "2017 IEEE 14th International Symposium on Biomedical Imaging (ISBI\n  2017)", "doi": "10.1109/ISBI.2017.7950634", "report-no": null, "categories": "stat.AP cs.CV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose to use Gaussian process regression to accurately estimate the\ndiffusion MRI signal at arbitrary locations in q-space. By estimating the\nsignal on a grid, we can do synthetic diffusion spectrum imaging:\nreconstructing the ensemble averaged propagator (EAP) by an inverse Fourier\ntransform. We also propose an alternative reconstruction method guaranteeing a\nnonnegative EAP that integrates to unity. The reconstruction is validated on\ndata simulated from two Gaussians at various crossing angles. Moreover, we\ndemonstrate on non-uniformly sampled in vivo data that the method is far\nsuperior to linear interpolation, and allows a drastic undersampling of the\ndata with only a minor loss of accuracy. We envision the method as a potential\nreplacement for standard diffusion spectrum imaging, in particular when\nacquistion time is limited.\n", "versions": [{"version": "v1", "created": "Wed, 9 Nov 2016 09:54:47 GMT"}], "update_date": "2020-01-03", "authors_parsed": [["Sj\u00f6lund", "Jens", ""], ["Eklund", "Anders", ""], ["\u00d6zarslan", "Evren", ""], ["Knutsson", "Hans", ""]]}, {"id": "1611.02941", "submitter": "Jun Sun", "authors": "Jun Sun, J\\'er\\^ome Kunegis, Steffen Staab", "title": "Predicting User Roles in Social Networks using Transfer Learning with\n  Feature Transformation", "comments": "8 pages, 5 figures, IEEE ICDMW 2016", "journal-ref": null, "doi": "10.1109/ICDMW.2016.0026", "report-no": null, "categories": "cs.SI stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  How can we recognise social roles of people, given a completely unlabelled\nsocial network? We present a transfer learning approach to network role\nclassification based on feature transformations from each network's local\nfeature distribution to a global feature space. Experiments are carried out on\nreal-world datasets. (See manuscript for the full abstract.)\n", "versions": [{"version": "v1", "created": "Wed, 9 Nov 2016 14:15:14 GMT"}], "update_date": "2017-03-23", "authors_parsed": [["Sun", "Jun", ""], ["Kunegis", "J\u00e9r\u00f4me", ""], ["Staab", "Steffen", ""]]}, {"id": "1611.03028", "submitter": "Weicong Ding", "authors": "Weicong Ding, Christy Lin, Prakash Ishwar", "title": "Node Embedding via Word Embedding for Network Community Discovery", "comments": "This version has been accepted for publication in a joint special\n  issue between IEEE JSTSP and TSIPN", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.SI physics.soc-ph stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Neural node embeddings have recently emerged as a powerful representation for\nsupervised learning tasks involving graph-structured data. We leverage this\nrecent advance to develop a novel algorithm for unsupervised community\ndiscovery in graphs. Through extensive experimental studies on simulated and\nreal-world data, we demonstrate that the proposed approach consistently\nimproves over the current state-of-the-art. Specifically, our approach\nempirically attains the information-theoretic limits for community recovery\nunder the benchmark Stochastic Block Models for graph generation and exhibits\nbetter stability and accuracy over both Spectral Clustering and Acyclic Belief\nPropagation in the community recovery limits.\n", "versions": [{"version": "v1", "created": "Wed, 9 Nov 2016 17:59:13 GMT"}, {"version": "v2", "created": "Wed, 7 Dec 2016 04:34:48 GMT"}, {"version": "v3", "created": "Wed, 28 Jun 2017 18:52:03 GMT"}], "update_date": "2017-06-30", "authors_parsed": [["Ding", "Weicong", ""], ["Lin", "Christy", ""], ["Ishwar", "Prakash", ""]]}, {"id": "1611.03033", "submitter": "Stefan Steinerberger", "authors": "Xiuyuan Cheng, Manas Rachh, Stefan Steinerberger", "title": "On the Diffusion Geometry of Graph Laplacians and Applications", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.SP math-ph math.AP math.MP stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study directed, weighted graphs $G=(V,E)$ and consider the (not\nnecessarily symmetric) averaging operator $$ (\\mathcal{L}u)(i) = -\\sum_{j\n\\sim_{} i}{p_{ij} (u(j) - u(i))},$$ where $p_{ij}$ are normalized edge weights.\nGiven a vertex $i \\in V$, we define the diffusion distance to a set $B \\subset\nV$ as the smallest number of steps $d_{B}(i) \\in \\mathbb{N}$ required for half\nof all random walks started in $i$ and moving randomly with respect to the\nweights $p_{ij}$ to visit $B$ within $d_{B}(i)$ steps. Our main result is that\nthe eigenfunctions interact nicely with this notion of distance. In particular,\nif $u$ satisfies $\\mathcal{L}u = \\lambda u$ on $V$ and $$ B = \\left\\{ i \\in V:\n- \\varepsilon \\leq u(i) \\leq \\varepsilon \\right\\} \\neq \\emptyset,$$ then, for\nall $i \\in V$, $$ d_{B}(i) \\log{\\left( \\frac{1}{|1-\\lambda|} \\right) } \\geq\n\\log{\\left( \\frac{ |u(i)| }{\\|u\\|_{L^{\\infty}}} \\right)} -\n\\log{\\left(\\frac{1}{2} + \\varepsilon\\right)}.$$ $d_B(i)$ is a remarkably good\napproximation of $|u|$ in the sense of having very high correlation. The result\nimplies that the classical one-dimensional spectral embedding preserves\nparticular aspects of geometry in the presence of clustered data. We also give\na continuous variant of the result which has a connection to the hot spots\nconjecture.\n", "versions": [{"version": "v1", "created": "Wed, 9 Nov 2016 18:16:17 GMT"}], "update_date": "2016-11-10", "authors_parsed": [["Cheng", "Xiuyuan", ""], ["Rachh", "Manas", ""], ["Steinerberger", "Stefan", ""]]}, {"id": "1611.03131", "submitter": "Yingyu Liang", "authors": "Bo Xie, Yingyu Liang, Le Song", "title": "Diverse Neural Network Learns True Target Functions", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Neural networks are a powerful class of functions that can be trained with\nsimple gradient descent to achieve state-of-the-art performance on a variety of\napplications. Despite their practical success, there is a paucity of results\nthat provide theoretical guarantees on why they are so effective. Lying in the\ncenter of the problem is the difficulty of analyzing the non-convex loss\nfunction with potentially numerous local minima and saddle points. Can neural\nnetworks corresponding to the stationary points of the loss function learn the\ntrue target function? If yes, what are the key factors contributing to such\nnice optimization properties?\n  In this paper, we answer these questions by analyzing one-hidden-layer neural\nnetworks with ReLU activation, and show that despite the non-convexity, neural\nnetworks with diverse units have no spurious local minima. We bypass the\nnon-convexity issue by directly analyzing the first order optimality condition,\nand show that the loss can be made arbitrarily small if the minimum singular\nvalue of the \"extended feature matrix\" is large enough. We make novel use of\ntechniques from kernel methods and geometric discrepancy, and identify a new\nrelation linking the smallest singular value to the spectrum of a kernel\nfunction associated with the activation function and to the diversity of the\nunits. Our results also suggest a novel regularization function to promote unit\ndiversity for potentially better generalization.\n", "versions": [{"version": "v1", "created": "Wed, 9 Nov 2016 23:19:21 GMT"}, {"version": "v2", "created": "Thu, 8 Dec 2016 21:25:42 GMT"}, {"version": "v3", "created": "Fri, 3 Mar 2017 04:38:29 GMT"}], "update_date": "2017-03-06", "authors_parsed": [["Xie", "Bo", ""], ["Liang", "Yingyu", ""], ["Song", "Le", ""]]}, {"id": "1611.03193", "submitter": "Tejal Bhamre", "authors": "Tejal Bhamre, Zhizhen Zhao, Amit Singer", "title": "Mahalanobis Distance for Class Averaging of Cryo-EM Images", "comments": "Final version accepted to the 14th IEEE International Symposium on\n  Biomedical Imaging (ISBI 2017)", "journal-ref": null, "doi": "10.1109/ISBI.2017.7950605", "report-no": null, "categories": "stat.AP cs.CV q-bio.BM stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Single particle reconstruction (SPR) from cryo-electron microscopy (EM) is a\ntechnique in which the 3D structure of a molecule needs to be determined from\nits contrast transfer function (CTF) affected, noisy 2D projection images taken\nat unknown viewing directions. One of the main challenges in cryo-EM is the\ntypically low signal to noise ratio (SNR) of the acquired images. 2D\nclassification of images, followed by class averaging, improves the SNR of the\nresulting averages, and is used for selecting particles from micrographs and\nfor inspecting the particle images. We introduce a new affinity measure, akin\nto the Mahalanobis distance, to compare cryo-EM images belonging to different\ndefocus groups. The new similarity measure is employed to detect similar\nimages, thereby leading to an improved algorithm for class averaging. We\nevaluate the performance of the proposed class averaging procedure on synthetic\ndatasets, obtaining state of the art classification.\n", "versions": [{"version": "v1", "created": "Thu, 10 Nov 2016 05:55:27 GMT"}, {"version": "v2", "created": "Sat, 12 Nov 2016 15:11:24 GMT"}, {"version": "v3", "created": "Fri, 18 Nov 2016 15:56:31 GMT"}, {"version": "v4", "created": "Wed, 25 Jan 2017 03:40:18 GMT"}], "update_date": "2017-06-30", "authors_parsed": [["Bhamre", "Tejal", ""], ["Zhao", "Zhizhen", ""], ["Singer", "Amit", ""]]}, {"id": "1611.03199", "submitter": "Bharath Ramsundar", "authors": "Han Altae-Tran, Bharath Ramsundar, Aneesh S. Pappu, and Vijay Pande", "title": "Low Data Drug Discovery with One-shot Learning", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recent advances in machine learning have made significant contributions to\ndrug discovery. Deep neural networks in particular have been demonstrated to\nprovide significant boosts in predictive power when inferring the properties\nand activities of small-molecule compounds. However, the applicability of these\ntechniques has been limited by the requirement for large amounts of training\ndata. In this work, we demonstrate how one-shot learning can be used to\nsignificantly lower the amounts of data required to make meaningful predictions\nin drug discovery applications. We introduce a new architecture, the residual\nLSTM embedding, that, when combined with graph convolutional neural networks,\nsignificantly improves the ability to learn meaningful distance metrics over\nsmall-molecules. We open source all models introduced in this work as part of\nDeepChem, an open-source framework for deep-learning in drug discovery.\n", "versions": [{"version": "v1", "created": "Thu, 10 Nov 2016 07:00:43 GMT"}], "update_date": "2016-11-11", "authors_parsed": [["Altae-Tran", "Han", ""], ["Ramsundar", "Bharath", ""], ["Pappu", "Aneesh S.", ""], ["Pande", "Vijay", ""]]}, {"id": "1611.03227", "submitter": "Vincenzo Lagani", "authors": "Vincenzo Lagani, Giorgos Athineou, Alessio Farcomeni, Michail Tsagris,\n  Ioannis Tsamardinos", "title": "Feature Selection with the R Package MXM: Discovering\n  Statistically-Equivalent Feature Subsets", "comments": "Accepted for publication in Journal of Statistical Software", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML q-bio.QM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The statistically equivalent signature (SES) algorithm is a method for\nfeature selection inspired by the principles of constrained-based learning of\nBayesian Networks. Most of the currently available feature-selection methods\nreturn only a single subset of features, supposedly the one with the highest\npredictive power. We argue that in several domains multiple subsets can achieve\nclose to maximal predictive accuracy, and that arbitrarily providing only one\nhas several drawbacks. The SES method attempts to identify multiple, predictive\nfeature subsets whose performances are statistically equivalent. Under that\nrespect SES subsumes and extends previous feature selection algorithms, like\nthe max-min parent children algorithm. SES is implemented in an homonym\nfunction included in the R package MXM, standing for mens ex machina, meaning\n'mind from the machine' in Latin. The MXM implementation of SES handles several\ndata-analysis tasks, namely classification, regression and survival analysis.\nIn this paper we present the SES algorithm, its implementation, and provide\nexamples of use of the SES function in R. Furthermore, we analyze three\npublicly available data sets to illustrate the equivalence of the signatures\nretrieved by SES and to contrast SES against the state-of-the-art feature\nselection method LASSO. Our results provide initial evidence that the two\nmethods perform comparably well in terms of predictive accuracy and that\nmultiple, equally predictive signatures are actually present in real world\ndata.\n", "versions": [{"version": "v1", "created": "Thu, 10 Nov 2016 09:08:41 GMT"}], "update_date": "2016-11-11", "authors_parsed": [["Lagani", "Vincenzo", ""], ["Athineou", "Giorgos", ""], ["Farcomeni", "Alessio", ""], ["Tsagris", "Michail", ""], ["Tsamardinos", "Ioannis", ""]]}, {"id": "1611.03231", "submitter": "Voot Tangkaratt", "authors": "Voot Tangkaratt, Herke van Hoof, Simone Parisi, Gerhard Neumann, Jan\n  Peters, Masashi Sugiyama", "title": "Policy Search with High-Dimensional Context Variables", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Direct contextual policy search methods learn to improve policy parameters\nand simultaneously generalize these parameters to different context or task\nvariables. However, learning from high-dimensional context variables, such as\ncamera images, is still a prominent problem in many real-world tasks. A naive\napplication of unsupervised dimensionality reduction methods to the context\nvariables, such as principal component analysis, is insufficient as\ntask-relevant input may be ignored. In this paper, we propose a contextual\npolicy search method in the model-based relative entropy stochastic search\nframework with integrated dimensionality reduction. We learn a model of the\nreward that is locally quadratic in both the policy parameters and the context\nvariables. Furthermore, we perform supervised linear dimensionality reduction\non the context variables by nuclear norm regularization. The experimental\nresults show that the proposed method outperforms naive dimensionality\nreduction via principal component analysis and a state-of-the-art contextual\npolicy search method.\n", "versions": [{"version": "v1", "created": "Thu, 10 Nov 2016 09:25:12 GMT"}], "update_date": "2016-11-11", "authors_parsed": [["Tangkaratt", "Voot", ""], ["van Hoof", "Herke", ""], ["Parisi", "Simone", ""], ["Neumann", "Gerhard", ""], ["Peters", "Jan", ""], ["Sugiyama", "Masashi", ""]]}, {"id": "1611.03328", "submitter": "Mohammad Amin Rahimian", "authors": "M. Amin Rahimian and Ali Jadbabaie", "title": "Distributed Estimation and Learning over Heterogeneous Networks", "comments": "In Proceedings of the 53rd Annual Allerton Conference on\n  Communication, Control, and Computing, 2016", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP cs.SI cs.SY math.ST stat.ML stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider several estimation and learning problems that networked agents\nface when making decisions given their uncertainty about an unknown variable.\nOur methods are designed to efficiently deal with heterogeneity in both size\nand quality of the observed data, as well as heterogeneity over time\n(intermittence). The goal of the studied aggregation schemes is to efficiently\ncombine the observed data that is spread over time and across several network\nnodes, accounting for all the network heterogeneities. Moreover, we require no\nform of coordination beyond the local neighborhood of every network agent or\nsensor node. The three problems that we consider are (i) maximum likelihood\nestimation of the unknown given initial data sets, (ii) learning the true model\nparameter from streams of data that the agents receive intermittently over\ntime, and (iii) minimum variance estimation of a complete sufficient statistic\nfrom several data points that the networked agents collect over time. In each\ncase we rely on an aggregation scheme to combine the observations of all\nagents; moreover, when the agents receive streams of data over time, we modify\nthe update rules to accommodate the most recent observations. In every case, we\ndemonstrate the efficiency of our algorithms by proving convergence to the\nglobally efficient estimators given the observations of all agents. We\nsupplement these results by investigating the rate of convergence and providing\nfinite-time performance guarantees.\n", "versions": [{"version": "v1", "created": "Thu, 10 Nov 2016 15:05:34 GMT"}], "update_date": "2016-11-11", "authors_parsed": [["Rahimian", "M. Amin", ""], ["Jadbabaie", "Ali", ""]]}, {"id": "1611.03383", "submitter": "Junbo Zhao", "authors": "Michael Mathieu, Junbo Zhao, Pablo Sprechmann, Aditya Ramesh, Yann\n  LeCun", "title": "Disentangling factors of variation in deep representations using\n  adversarial training", "comments": "Conference paper in NIPS 2016", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We introduce a conditional generative model for learning to disentangle the\nhidden factors of variation within a set of labeled observations, and separate\nthem into complementary codes. One code summarizes the specified factors of\nvariation associated with the labels. The other summarizes the remaining\nunspecified variability. During training, the only available source of\nsupervision comes from our ability to distinguish among different observations\nbelonging to the same class. Examples of such observations include images of a\nset of labeled objects captured at different viewpoints, or recordings of set\nof speakers dictating multiple phrases. In both instances, the intra-class\ndiversity is the source of the unspecified factors of variation: each object is\nobserved at multiple viewpoints, and each speaker dictates multiple phrases.\nLearning to disentangle the specified factors from the unspecified ones becomes\neasier when strong supervision is possible. Suppose that during training, we\nhave access to pairs of images, where each pair shows two different objects\ncaptured from the same viewpoint. This source of alignment allows us to solve\nour task using existing methods. However, labels for the unspecified factors\nare usually unavailable in realistic scenarios where data acquisition is not\nstrictly controlled. We address the problem of disentanglement in this more\ngeneral setting by combining deep convolutional autoencoders with a form of\nadversarial training. Both factors of variation are implicitly captured in the\norganization of the learned embedding space, and can be used for solving\nsingle-image analogies. Experimental results on synthetic and real datasets\nshow that the proposed method is capable of generalizing to unseen classes and\nintra-class variabilities.\n", "versions": [{"version": "v1", "created": "Thu, 10 Nov 2016 16:24:16 GMT"}], "update_date": "2016-11-11", "authors_parsed": [["Mathieu", "Michael", ""], ["Zhao", "Junbo", ""], ["Sprechmann", "Pablo", ""], ["Ramesh", "Aditya", ""], ["LeCun", "Yann", ""]]}, {"id": "1611.03404", "submitter": "Jeffrey Regier", "authors": "Jeffrey Regier, Kiran Pamnany, Ryan Giordano, Rollin Thomas, David\n  Schlegel, Jon McAuliffe and Prabhat", "title": "Learning an Astronomical Catalog of the Visible Universe through\n  Scalable Bayesian Inference", "comments": "submitting to IPDPS'17", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC astro-ph.IM cs.LG stat.AP stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Celeste is a procedure for inferring astronomical catalogs that attains\nstate-of-the-art scientific results. To date, Celeste has been scaled to at\nmost hundreds of megabytes of astronomical images: Bayesian posterior inference\nis notoriously demanding computationally. In this paper, we report on a\nscalable, parallel version of Celeste, suitable for learning catalogs from\nmodern large-scale astronomical datasets. Our algorithmic innovations include a\nfast numerical optimization routine for Bayesian posterior inference and a\nstatistically efficient scheme for decomposing astronomical optimization\nproblems into subproblems.\n  Our scalable implementation is written entirely in Julia, a new high-level\ndynamic programming language designed for scientific and numerical computing.\nWe use Julia's high-level constructs for shared and distributed memory\nparallelism, and demonstrate effective load balancing and efficient scaling on\nup to 8192 Xeon cores on the NERSC Cori supercomputer.\n", "versions": [{"version": "v1", "created": "Thu, 10 Nov 2016 17:16:04 GMT"}], "update_date": "2016-11-11", "authors_parsed": [["Regier", "Jeffrey", ""], ["Pamnany", "Kiran", ""], ["Giordano", "Ryan", ""], ["Thomas", "Rollin", ""], ["Schlegel", "David", ""], ["McAuliffe", "Jon", ""], ["Prabhat", "", ""]]}, {"id": "1611.03426", "submitter": "Ernesto Diaz-Aviles", "authors": "Avar\\'e Stewart, Sara Romano, Nattiya Kanhabua, Sergio Di Martino,\n  Wolf Siberski, Antonino Mazzeo, Wolfgang Nejdl, and Ernesto Diaz-Aviles", "title": "Why is it Difficult to Detect Sudden and Unexpected Epidemic Outbreaks\n  in Twitter?", "comments": "ACM CCS Concepts: Applied computing - Health informatics; Information\n  systems - Web mining; Document filtering; Novelty in information retrieval;\n  Recommender systems; Human-centered computing - Social media", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CY cs.IR cs.SI stat.ML", "license": "http://creativecommons.org/licenses/by-sa/4.0/", "abstract": "  Social media services such as Twitter are a valuable source of information\nfor decision support systems. Many studies have shown that this also holds for\nthe medical domain, where Twitter is considered a viable tool for public health\nofficials to sift through relevant information for the early detection,\nmanagement, and control of epidemic outbreaks. This is possible due to the\ninherent capability of social media services to transmit information faster\nthan traditional channels. However, the majority of current studies have\nlimited their scope to the detection of common and seasonal health recurring\nevents (e.g., Influenza-like Illness), partially due to the noisy nature of\nTwitter data, which makes outbreak detection and management very challenging.\n  Within the European project M-Eco, we developed a Twitter-based Epidemic\nIntelligence (EI) system, which is designed to also handle a more general class\nof unexpected and aperiodic outbreaks. In particular, we faced three main\nresearch challenges in this endeavor:\n  1) dynamic classification to manage terminology evolution of Twitter\nmessages, 2) alert generation to produce reliable outbreak alerts analyzing the\n(noisy) tweet time series, and 3) ranking and recommendation to support domain\nexperts for better assessment of the generated alerts.\n  In this paper, we empirically evaluate our proposed approach to these\nchallenges using real-world outbreak datasets and a large collection of tweets.\nWe validate our solution with domain experts, describe our experiences, and\ngive a more realistic view on the benefits and issues of analyzing social media\nfor public health.\n", "versions": [{"version": "v1", "created": "Thu, 10 Nov 2016 17:53:33 GMT"}], "update_date": "2016-11-11", "authors_parsed": [["Stewart", "Avar\u00e9", ""], ["Romano", "Sara", ""], ["Kanhabua", "Nattiya", ""], ["Di Martino", "Sergio", ""], ["Siberski", "Wolf", ""], ["Mazzeo", "Antonino", ""], ["Nejdl", "Wolfgang", ""], ["Diaz-Aviles", "Ernesto", ""]]}, {"id": "1611.03427", "submitter": "Keerthiram Murugesan", "authors": "Keerthiram Murugesan, Jaime Carbonell", "title": "Multi-Task Multiple Kernel Relationship Learning", "comments": "17th SIAM International Conference on Data Mining (SDM 2017),\n  Houston, Texas, USA, 2017", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  This paper presents a novel multitask multiple kernel learning framework that\nefficiently learns the kernel weights leveraging the relationship across\nmultiple tasks. The idea is to automatically infer this task relationship in\nthe \\textit{RKHS} space corresponding to the given base kernels. The problem is\nformulated as a regularization-based approach called \\textit{Multi-Task\nMultiple Kernel Relationship Learning} (\\textit{MK-MTRL}), which models the\ntask relationship matrix from the weights learned from latent feature spaces of\ntask-specific base kernels. Unlike in previous work, the proposed formulation\nallows one to incorporate prior knowledge for simultaneously learning several\nrelated tasks. We propose an alternating minimization algorithm to learn the\nmodel parameters, kernel weights and task relationship matrix. In order to\ntackle large-scale problems, we further propose a two-stage \\textit{MK-MTRL}\nonline learning algorithm and show that it significantly reduces the\ncomputational time, and also achieves performance comparable to that of the\njoint learning framework. Experimental results on benchmark datasets show that\nthe proposed formulations outperform several state-of-the-art multitask\nlearning methods.\n", "versions": [{"version": "v1", "created": "Thu, 10 Nov 2016 17:54:22 GMT"}, {"version": "v2", "created": "Thu, 2 Mar 2017 22:09:54 GMT"}], "update_date": "2017-03-06", "authors_parsed": [["Murugesan", "Keerthiram", ""], ["Carbonell", "Jaime", ""]]}, {"id": "1611.03451", "submitter": "Philip Thomas", "authors": "Philip S. Thomas and Emma Brunskill", "title": "Importance Sampling with Unequal Support", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Importance sampling is often used in machine learning when training and\ntesting data come from different distributions. In this paper we propose a new\nvariant of importance sampling that can reduce the variance of importance\nsampling-based estimates by orders of magnitude when the supports of the\ntraining and testing distributions differ. After motivating and presenting our\nnew importance sampling estimator, we provide a detailed theoretical analysis\nthat characterizes both its bias and variance relative to the ordinary\nimportance sampling estimator (in various settings, which include cases where\nordinary importance sampling is biased, while our new estimator is not, and\nvice versa). We conclude with an example of how our new importance sampling\nestimator can be used to improve estimates of how well a new treatment policy\nfor diabetes will work for an individual, using only data from when the\nindividual used a previous treatment policy.\n", "versions": [{"version": "v1", "created": "Thu, 10 Nov 2016 19:11:09 GMT"}], "update_date": "2016-11-11", "authors_parsed": [["Thomas", "Philip S.", ""], ["Brunskill", "Emma", ""]]}, {"id": "1611.03531", "submitter": "Daniel Luckett", "authors": "Daniel J. Luckett, Eric B. Laber, Anna R. Kahkoska, David M. Maahs,\n  Elizabeth Mayer-Davis, Michael R. Kosorok", "title": "Estimating Dynamic Treatment Regimes in Mobile Health Using V-learning", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The vision for precision medicine is to use individual patient\ncharacteristics to inform a personalized treatment plan that leads to the best\nhealthcare possible for each patient. Mobile technologies have an important\nrole to play in this vision as they offer a means to monitor a patient's health\nstatus in real-time and subsequently to deliver interventions if, when, and in\nthe dose that they are needed. Dynamic treatment regimes formalize\nindividualized treatment plans as sequences of decision rules, one per stage of\nclinical intervention, that map current patient information to a recommended\ntreatment. However, existing methods for estimating optimal dynamic treatment\nregimes are designed for a small number of fixed decision points occurring on a\ncoarse time-scale. We propose a new reinforcement learning method for\nestimating an optimal treatment regime that is applicable to data collected\nusing mobile technologies in an outpatient setting. The proposed method\naccommodates an indefinite time horizon and minute-by-minute decision making\nthat are common in mobile health applications. We show the proposed estimators\nare consistent and asymptotically normal under mild conditions. The proposed\nmethods are applied to estimate an optimal dynamic treatment regime for\ncontrolling blood glucose levels in patients with type 1 diabetes.\n", "versions": [{"version": "v1", "created": "Thu, 10 Nov 2016 22:04:13 GMT"}, {"version": "v2", "created": "Sat, 14 Oct 2017 17:16:40 GMT"}], "update_date": "2017-10-17", "authors_parsed": [["Luckett", "Daniel J.", ""], ["Laber", "Eric B.", ""], ["Kahkoska", "Anna R.", ""], ["Maahs", "David M.", ""], ["Mayer-Davis", "Elizabeth", ""], ["Kosorok", "Michael R.", ""]]}, {"id": "1611.03578", "submitter": "Guangxi Li", "authors": "Guangxi Li, Zenglin Xu, Linnan Wang, Jinmian Ye, Irwin King, Michael\n  Lyu", "title": "Simple and Efficient Parallelization for Probabilistic Temporal Tensor\n  Factorization", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Probabilistic Temporal Tensor Factorization (PTTF) is an effective algorithm\nto model the temporal tensor data. It leverages a time constraint to capture\nthe evolving properties of tensor data. Nowadays the exploding dataset demands\na large scale PTTF analysis, and a parallel solution is critical to accommodate\nthe trend. Whereas, the parallelization of PTTF still remains unexplored. In\nthis paper, we propose a simple yet efficient Parallel Probabilistic Temporal\nTensor Factorization, referred to as P$^2$T$^2$F, to provide a scalable PTTF\nsolution. P$^2$T$^2$F is fundamentally disparate from existing parallel tensor\nfactorizations by considering the probabilistic decomposition and the temporal\neffects of tensor data. It adopts a new tensor data split strategy to subdivide\na large tensor into independent sub-tensors, the computation of which is\ninherently parallel. We train P$^2$T$^2$F with an efficient algorithm of\nstochastic Alternating Direction Method of Multipliers, and show that the\nconvergence is guaranteed. Experiments on several real-word tensor datasets\ndemonstrate that P$^2$T$^2$F is a highly effective and efficiently scalable\nalgorithm dedicated for large scale probabilistic temporal tensor analysis.\n", "versions": [{"version": "v1", "created": "Fri, 11 Nov 2016 03:54:00 GMT"}], "update_date": "2016-11-14", "authors_parsed": [["Li", "Guangxi", ""], ["Xu", "Zenglin", ""], ["Wang", "Linnan", ""], ["Ye", "Jinmian", ""], ["King", "Irwin", ""], ["Lyu", "Michael", ""]]}, {"id": "1611.03777", "submitter": "Barak Pearlmutter", "authors": "At{\\i}l{\\i}m G\\\"une\\c{s} Baydin and Barak A. Pearlmutter and Jeffrey\n  Mark Siskind", "title": "Tricks from Deep Learning", "comments": "Extended abstract presented at the AD 2016 Conference, Sep 2016,\n  Oxford UK", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The deep learning community has devised a diverse set of methods to make\ngradient optimization, using large datasets, of large and highly complex models\nwith deeply cascaded nonlinearities, practical. Taken as a whole, these methods\nconstitute a breakthrough, allowing computational structures which are quite\nwide, very deep, and with an enormous number and variety of free parameters to\nbe effectively optimized. The result now dominates much of practical machine\nlearning, with applications in machine translation, computer vision, and speech\nrecognition. Many of these methods, viewed through the lens of algorithmic\ndifferentiation (AD), can be seen as either addressing issues with the gradient\nitself, or finding ways of achieving increased efficiency using tricks that are\nAD-related, but not provided by current AD systems.\n  The goal of this paper is to explain not just those methods of most relevance\nto AD, but also the technical constraints and mindset which led to their\ndiscovery. After explaining this context, we present a \"laundry list\" of\nmethods developed by the deep learning community. Two of these are discussed in\nfurther mathematical detail: a way to dramatically reduce the size of the tape\nwhen performing reverse-mode AD on a (theoretically) time-reversible process\nlike an ODE integrator; and a new mathematical insight that allows for the\nimplementation of a stochastic Newton's method.\n", "versions": [{"version": "v1", "created": "Thu, 10 Nov 2016 17:57:19 GMT"}], "update_date": "2016-11-14", "authors_parsed": [["Baydin", "At\u0131l\u0131m G\u00fcne\u015f", ""], ["Pearlmutter", "Barak A.", ""], ["Siskind", "Jeffrey Mark", ""]]}, {"id": "1611.03787", "submitter": "Danica J. Sutherland", "authors": "Seth Flaxman and Danica J. Sutherland and Yu-Xiang Wang and Yee Whye\n  Teh", "title": "Understanding the 2016 US Presidential Election using ecological\n  inference and distribution regression with census microdata", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We combine fine-grained spatially referenced census data with the vote\noutcomes from the 2016 US presidential election. Using this dataset, we perform\necological inference using distribution regression (Flaxman et al, KDD 2015)\nwith a multinomial-logit regression so as to model the vote outcome Trump,\nClinton, Other / Didn't vote as a function of demographic and socioeconomic\nfeatures. Ecological inference allows us to estimate \"exit poll\" style results\nlike what was Trump's support among white women, but for entirely novel\ncategories. We also perform exploratory data analysis to understand which\ncensus variables are predictive of voting for Trump, voting for Clinton, or not\nvoting for either. All of our methods are implemented in Python and R, and are\navailable online for replication.\n", "versions": [{"version": "v1", "created": "Fri, 11 Nov 2016 17:17:07 GMT"}, {"version": "v2", "created": "Thu, 14 Jan 2021 06:58:49 GMT"}], "update_date": "2021-01-15", "authors_parsed": [["Flaxman", "Seth", ""], ["Sutherland", "Danica J.", ""], ["Wang", "Yu-Xiang", ""], ["Teh", "Yee Whye", ""]]}, {"id": "1611.03819", "submitter": "Yingyu Liang", "authors": "Yuanzhi Li, Yingyu Liang, Andrej Risteski", "title": "Recovery Guarantee of Non-negative Matrix Factorization via Alternating\n  Updates", "comments": "To appear in NIPS 2016. 8 pages of extended abstract; 48 pages in\n  total", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.DS stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Non-negative matrix factorization is a popular tool for decomposing data into\nfeature and weight matrices under non-negativity constraints. It enjoys\npractical success but is poorly understood theoretically. This paper proposes\nan algorithm that alternates between decoding the weights and updating the\nfeatures, and shows that assuming a generative model of the data, it provably\nrecovers the ground-truth under fairly mild conditions. In particular, its only\nessential requirement on features is linear independence. Furthermore, the\nalgorithm uses ReLU to exploit the non-negativity for decoding the weights, and\nthus can tolerate adversarial noise that can potentially be as large as the\nsignal, and can tolerate unbiased noise much larger than the signal. The\nanalysis relies on a carefully designed coupling between two potential\nfunctions, which we believe is of independent interest.\n", "versions": [{"version": "v1", "created": "Fri, 11 Nov 2016 19:13:37 GMT"}], "update_date": "2016-11-15", "authors_parsed": [["Li", "Yuanzhi", ""], ["Liang", "Yingyu", ""], ["Risteski", "Andrej", ""]]}, {"id": "1611.03824", "submitter": "Yutian Chen", "authors": "Yutian Chen, Matthew W. Hoffman, Sergio Gomez Colmenarejo, Misha\n  Denil, Timothy P. Lillicrap, Matt Botvinick, Nando de Freitas", "title": "Learning to Learn without Gradient Descent by Gradient Descent", "comments": "Accepted by ICML 2017. Previous version \"Learning to Learn for Global\n  Optimization of Black Box Functions\" was published in the Deep Reinforcement\n  Learning Workshop, NIPS 2016", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We learn recurrent neural network optimizers trained on simple synthetic\nfunctions by gradient descent. We show that these learned optimizers exhibit a\nremarkable degree of transfer in that they can be used to efficiently optimize\na broad range of derivative-free black-box functions, including Gaussian\nprocess bandits, simple control objectives, global optimization benchmarks and\nhyper-parameter tuning tasks. Up to the training horizon, the learned\noptimizers learn to trade-off exploration and exploitation, and compare\nfavourably with heavily engineered Bayesian optimization packages for\nhyper-parameter tuning.\n", "versions": [{"version": "v1", "created": "Fri, 11 Nov 2016 19:33:01 GMT"}, {"version": "v2", "created": "Fri, 18 Nov 2016 14:13:13 GMT"}, {"version": "v3", "created": "Tue, 29 Nov 2016 21:32:13 GMT"}, {"version": "v4", "created": "Tue, 9 May 2017 07:59:07 GMT"}, {"version": "v5", "created": "Mon, 5 Jun 2017 14:15:01 GMT"}, {"version": "v6", "created": "Mon, 12 Jun 2017 11:19:30 GMT"}], "update_date": "2017-06-13", "authors_parsed": [["Chen", "Yutian", ""], ["Hoffman", "Matthew W.", ""], ["Colmenarejo", "Sergio Gomez", ""], ["Denil", "Misha", ""], ["Lillicrap", "Timothy P.", ""], ["Botvinick", "Matt", ""], ["de Freitas", "Nando", ""]]}, {"id": "1611.03879", "submitter": "Chun-Liang Li", "authors": "Chun-Liang Li, Siamak Ravanbakhsh, Barnabas Poczos", "title": "Annealing Gaussian into ReLU: a New Sampling Strategy for Leaky-ReLU RBM", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Restricted Boltzmann Machine (RBM) is a bipartite graphical model that is\nused as the building block in energy-based deep generative models. Due to\nnumerical stability and quantifiability of the likelihood, RBM is commonly used\nwith Bernoulli units. Here, we consider an alternative member of exponential\nfamily RBM with leaky rectified linear units -- called leaky RBM. We first\nstudy the joint and marginal distributions of leaky RBM under different\nleakiness, which provides us important insights by connecting the leaky RBM\nmodel and truncated Gaussian distributions. The connection leads us to a simple\nyet efficient method for sampling from this model, where the basic idea is to\nanneal the leakiness rather than the energy; -- i.e., start from a fully\nGaussian/Linear unit and gradually decrease the leakiness over iterations. This\nserves as an alternative to the annealing of the temperature parameter and\nenables numerical estimation of the likelihood that are more efficient and more\naccurate than the commonly used annealed importance sampling (AIS). We further\ndemonstrate that the proposed sampling algorithm enjoys faster mixing property\nthan contrastive divergence algorithm, which benefits the training without any\nadditional computational cost.\n", "versions": [{"version": "v1", "created": "Fri, 11 Nov 2016 21:08:36 GMT"}], "update_date": "2016-11-15", "authors_parsed": [["Li", "Chun-Liang", ""], ["Ravanbakhsh", "Siamak", ""], ["Poczos", "Barnabas", ""]]}, {"id": "1611.03898", "submitter": "Thai Pham", "authors": "Derek Farren and Thai Pham and Marco Alban-Hidalgo", "title": "Low Latency Anomaly Detection and Bayesian Network Prediction of Anomaly\n  Likelihood", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We develop a supervised machine learning model that detects anomalies in\nsystems in real time. Our model processes unbounded streams of data into time\nseries which then form the basis of a low-latency anomaly detection model.\nMoreover, we extend our preliminary goal of just anomaly detection to\nsimultaneous anomaly prediction. We approach this very challenging problem by\ndeveloping a Bayesian Network framework that captures the information about the\nparameters of the lagged regressors calibrated in the first part of our\napproach and use this structure to learn local conditional probability\ndistributions.\n", "versions": [{"version": "v1", "created": "Fri, 11 Nov 2016 22:20:41 GMT"}], "update_date": "2016-11-16", "authors_parsed": [["Farren", "Derek", ""], ["Pham", "Thai", ""], ["Alban-Hidalgo", "Marco", ""]]}, {"id": "1611.03907", "submitter": "Kamyar Azizzadenesheli Ph.D.", "authors": "Kamyar Azizzadenesheli, Alessandro Lazaric, Animashree Anandkumar", "title": "Reinforcement Learning in Rich-Observation MDPs using Spectral Methods", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Reinforcement learning (RL) in Markov decision processes (MDPs) with large\nstate spaces is a challenging problem. The performance of standard RL\nalgorithms degrades drastically with the dimensionality of state space.\nHowever, in practice, these large MDPs typically incorporate a latent or hidden\nlow-dimensional structure. In this paper, we study the setting of\nrich-observation Markov decision processes (ROMDP), where there are a small\nnumber of hidden states which possess an injective mapping to the observation\nstates. In other words, every observation state is generated through a single\nhidden state, and this mapping is unknown a priori. We introduce a spectral\ndecomposition method that consistently learns this mapping, and more\nimportantly, achieves it with low regret. The estimated mapping is integrated\ninto an optimistic RL algorithm (UCRL), which operates on the estimated hidden\nspace. We derive finite-time regret bounds for our algorithm with a weak\ndependence on the dimensionality of the observed space. In fact, our algorithm\nasymptotically achieves the same average regret as the oracle UCRL algorithm,\nwhich has the knowledge of the mapping from hidden to observed spaces. Thus, we\nderive an efficient spectral RL algorithm for ROMDPs.\n", "versions": [{"version": "v1", "created": "Fri, 11 Nov 2016 22:39:01 GMT"}, {"version": "v2", "created": "Mon, 19 Jun 2017 01:52:32 GMT"}, {"version": "v3", "created": "Mon, 18 Jun 2018 01:03:33 GMT"}, {"version": "v4", "created": "Tue, 19 Jun 2018 20:14:54 GMT"}], "update_date": "2018-06-21", "authors_parsed": [["Azizzadenesheli", "Kamyar", ""], ["Lazaric", "Alessandro", ""], ["Anandkumar", "Animashree", ""]]}, {"id": "1611.03969", "submitter": "Hien Nguyen", "authors": "Hien D. Nguyen", "title": "An Introduction to MM Algorithms for Machine Learning and Statistical", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.CO cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  MM (majorization--minimization) algorithms are an increasingly popular tool\nfor solving optimization problems in machine learning and statistical\nestimation. This article introduces the MM algorithm framework in general and\nvia three popular example applications: Gaussian mixture regressions,\nmultinomial logistic regressions, and support vector machines. Specific\nalgorithms for the three examples are derived and numerical demonstrations are\npresented. Theoretical and practical aspects of MM algorithm design are\ndiscussed.\n", "versions": [{"version": "v1", "created": "Sat, 12 Nov 2016 08:18:38 GMT"}], "update_date": "2016-11-16", "authors_parsed": [["Nguyen", "Hien D.", ""]]}, {"id": "1611.03979", "submitter": "Nicole M\\\"ucke", "authors": "Gilles Blanchard, Nicole M\\\"ucke", "title": "Kernel regression, minimax rates and effective dimensionality: beyond\n  the regular case", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We investigate if kernel regularization methods can achieve minimax\nconvergence rates over a source condition regularity assumption for the target\nfunction. These questions have been considered in past literature, but only\nunder specific assumptions about the decay, typically polynomial, of the\nspectrum of the the kernel mapping covariance operator. In the perspective of\ndistribution-free results, we investigate this issue under much weaker\nassumption on the eigenvalue decay, allowing for more complex behavior that can\nreflect different structure of the data at different scales.\n", "versions": [{"version": "v1", "created": "Sat, 12 Nov 2016 09:51:28 GMT"}], "update_date": "2016-11-15", "authors_parsed": [["Blanchard", "Gilles", ""], ["M\u00fccke", "Nicole", ""]]}, {"id": "1611.03981", "submitter": "Fuqiang Liu", "authors": "Fuqaing Liu, Chenwei Deng, Fukun Bi, Yiding Yang", "title": "Dual Teaching: A Practical Semi-supervised Wrapper Method", "comments": "7 pages, 4 figures, 1 table", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Semi-supervised wrapper methods are concerned with building effective\nsupervised classifiers from partially labeled data. Though previous works have\nsucceeded in some fields, it is still difficult to apply semi-supervised\nwrapper methods to practice because the assumptions those methods rely on tend\nto be unrealistic in practice. For practical use, this paper proposes a novel\nsemi-supervised wrapper method, Dual Teaching, whose assumptions are easy to\nset up. Dual Teaching adopts two external classifiers to estimate the false\npositives and false negatives of the base learner. Only if the recall of every\nexternal classifier is greater than zero and the sum of the precision is\ngreater than one, Dual Teaching will train a base learner from partially\nlabeled data as effectively as the fully-labeled-data-trained classifier. The\neffectiveness of Dual Teaching is proved in both theory and practice.\n", "versions": [{"version": "v1", "created": "Sat, 12 Nov 2016 10:23:53 GMT"}], "update_date": "2016-11-15", "authors_parsed": [["Liu", "Fuqaing", ""], ["Deng", "Chenwei", ""], ["Bi", "Fukun", ""], ["Yang", "Yiding", ""]]}, {"id": "1611.03993", "submitter": "Tengfei Zhou", "authors": "Tengfei Zhou, Hui Qian, Zebang Shen, Congfu Xu", "title": "Riemannian Tensor Completion with Side Information", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG cs.NA", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  By restricting the iterate on a nonlinear manifold, the recently proposed\nRiemannian optimization methods prove to be both efficient and effective in low\nrank tensor completion problems. However, existing methods fail to exploit the\neasily accessible side information, due to their format mismatch. Consequently,\nthere is still room for improvement in such methods. To fill the gap, in this\npaper, a novel Riemannian model is proposed to organically integrate the\noriginal model and the side information by overcoming their inconsistency. For\nthis particular model, an efficient Riemannian conjugate gradient descent\nsolver is devised based on a new metric that captures the curvature of the\nobjective.Numerical experiments suggest that our solver is more accurate than\nthe state-of-the-art without compromising the efficiency.\n", "versions": [{"version": "v1", "created": "Sat, 12 Nov 2016 11:58:17 GMT"}, {"version": "v2", "created": "Thu, 23 Feb 2017 09:09:58 GMT"}], "update_date": "2017-02-24", "authors_parsed": [["Zhou", "Tengfei", ""], ["Qian", "Hui", ""], ["Shen", "Zebang", ""], ["Xu", "Congfu", ""]]}, {"id": "1611.04035", "submitter": "Murat Kocaoglu", "authors": "Murat Kocaoglu, Alexandros G. Dimakis, Sriram Vishwanath, Babak\n  Hassibi", "title": "Entropic Causal Inference", "comments": "To appear in AAAI 2017", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.IT math.IT stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider the problem of identifying the causal direction between two\ndiscrete random variables using observational data. Unlike previous work, we\nkeep the most general functional model but make an assumption on the unobserved\nexogenous variable: Inspired by Occam's razor, we assume that the exogenous\nvariable is simple in the true causal direction. We quantify simplicity using\nR\\'enyi entropy. Our main result is that, under natural assumptions, if the\nexogenous variable has low $H_0$ entropy (cardinality) in the true direction,\nit must have high $H_0$ entropy in the wrong direction. We establish several\nalgorithmic hardness results about estimating the minimum entropy exogenous\nvariable. We show that the problem of finding the exogenous variable with\nminimum entropy is equivalent to the problem of finding minimum joint entropy\ngiven $n$ marginal distributions, also known as minimum entropy coupling\nproblem. We propose an efficient greedy algorithm for the minimum entropy\ncoupling problem, that for $n=2$ provably finds a local optimum. This gives a\ngreedy algorithm for finding the exogenous variable with minimum $H_1$ (Shannon\nEntropy). Our greedy entropy-based causal inference algorithm has similar\nperformance to the state of the art additive noise models in real datasets. One\nadvantage of our approach is that we make no use of the values of random\nvariables but only their distributions. Our method can therefore be used for\ncausal inference for both ordinal and also categorical data, unlike additive\nnoise models.\n", "versions": [{"version": "v1", "created": "Sat, 12 Nov 2016 18:56:34 GMT"}, {"version": "v2", "created": "Tue, 15 Nov 2016 03:09:53 GMT"}], "update_date": "2016-11-16", "authors_parsed": [["Kocaoglu", "Murat", ""], ["Dimakis", "Alexandros G.", ""], ["Vishwanath", "Sriram", ""], ["Hassibi", "Babak", ""]]}, {"id": "1611.04051", "submitter": "Matthew Kusner", "authors": "Matt J. Kusner, Jos\\'e Miguel Hern\\'andez-Lobato", "title": "GANS for Sequences of Discrete Elements with the Gumbel-softmax\n  Distribution", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Generative Adversarial Networks (GAN) have limitations when the goal is to\ngenerate sequences of discrete elements. The reason for this is that samples\nfrom a distribution on discrete objects such as the multinomial are not\ndifferentiable with respect to the distribution parameters. This problem can be\navoided by using the Gumbel-softmax distribution, which is a continuous\napproximation to a multinomial distribution parameterized in terms of the\nsoftmax function. In this work, we evaluate the performance of GANs based on\nrecurrent neural networks with Gumbel-softmax output distributions in the task\nof generating sequences of discrete elements.\n", "versions": [{"version": "v1", "created": "Sat, 12 Nov 2016 22:54:45 GMT"}], "update_date": "2016-11-16", "authors_parsed": [["Kusner", "Matt J.", ""], ["Hern\u00e1ndez-Lobato", "Jos\u00e9 Miguel", ""]]}, {"id": "1611.04067", "submitter": "Jaroslaw Zola", "authors": "Frank Schoeneman, Suchismit Mahapatra, Varun Chandola, Nils Napp,\n  Jaroslaw Zola", "title": "Error Metrics for Learning Reliable Manifolds from Streaming Data", "comments": null, "journal-ref": null, "doi": "10.1137/1.9781611974973.84", "report-no": null, "categories": "stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Spectral dimensionality reduction is frequently used to identify\nlow-dimensional structure in high-dimensional data. However, learning\nmanifolds, especially from the streaming data, is computationally and memory\nexpensive. In this paper, we argue that a stable manifold can be learned using\nonly a fraction of the stream, and the remaining stream can be mapped to the\nmanifold in a significantly less costly manner. Identifying the transition\npoint at which the manifold is stable is the key step. We present error metrics\nthat allow us to identify the transition point for a given stream by\nquantitatively assessing the quality of a manifold learned using Isomap. We\nfurther propose an efficient mapping algorithm, called S-Isomap, that can be\nused to map new samples onto the stable manifold. We describe experiments on a\nvariety of data sets that show that the proposed approach is computationally\nefficient without sacrificing accuracy.\n", "versions": [{"version": "v1", "created": "Sun, 13 Nov 2016 02:14:01 GMT"}, {"version": "v2", "created": "Wed, 11 Jan 2017 17:05:07 GMT"}], "update_date": "2017-10-20", "authors_parsed": [["Schoeneman", "Frank", ""], ["Mahapatra", "Suchismit", ""], ["Chandola", "Varun", ""], ["Napp", "Nils", ""], ["Zola", "Jaroslaw", ""]]}, {"id": "1611.04069", "submitter": "Saiprasad Ravishankar", "authors": "Saiprasad Ravishankar, Brian E. Moore, Raj Rao Nadakuditi, and Jeffrey\n  A. Fessler", "title": "Low-rank and Adaptive Sparse Signal (LASSI) Models for Highly\n  Accelerated Dynamic Imaging", "comments": null, "journal-ref": "IEEE Tr. Med. Imaging 36(5):1116-28 May 2017", "doi": "10.1109/TMI.2017.2650960", "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Sparsity-based approaches have been popular in many applications in image\nprocessing and imaging. Compressed sensing exploits the sparsity of images in a\ntransform domain or dictionary to improve image recovery from undersampled\nmeasurements. In the context of inverse problems in dynamic imaging, recent\nresearch has demonstrated the promise of sparsity and low-rank techniques. For\nexample, the patches of the underlying data are modeled as sparse in an\nadaptive dictionary domain, and the resulting image and dictionary estimation\nfrom undersampled measurements is called dictionary-blind compressed sensing,\nor the dynamic image sequence is modeled as a sum of low-rank and sparse (in\nsome transform domain) components (L+S model) that are estimated from limited\nmeasurements. In this work, we investigate a data-adaptive extension of the L+S\nmodel, dubbed LASSI, where the temporal image sequence is decomposed into a\nlow-rank component and a component whose spatiotemporal (3D) patches are sparse\nin some adaptive dictionary domain. We investigate various formulations and\nefficient methods for jointly estimating the underlying dynamic signal\ncomponents and the spatiotemporal dictionary from limited measurements. We also\nobtain efficient sparsity penalized dictionary-blind compressed sensing methods\nas special cases of our LASSI approaches. Our numerical experiments demonstrate\nthe promising performance of LASSI schemes for dynamic magnetic resonance image\nreconstruction from limited k-t space data compared to recent methods such as\nk-t SLR and L+S, and compared to the proposed dictionary-blind compressed\nsensing method.\n", "versions": [{"version": "v1", "created": "Sun, 13 Nov 2016 02:21:07 GMT"}, {"version": "v2", "created": "Mon, 9 Jan 2017 10:27:05 GMT"}], "update_date": "2019-06-14", "authors_parsed": [["Ravishankar", "Saiprasad", ""], ["Moore", "Brian E.", ""], ["Nadakuditi", "Raj Rao", ""], ["Fessler", "Jeffrey A.", ""]]}, {"id": "1611.04149", "submitter": "Zebang Shen", "authors": "Zebang Shen, Hui Qian, Chao Zhang, and Tengfei Zhou", "title": "Accelerated Variance Reduced Block Coordinate Descent", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Algorithms with fast convergence, small number of data access, and low\nper-iteration complexity are particularly favorable in the big data era, due to\nthe demand for obtaining \\emph{highly accurate solutions} to problems with\n\\emph{a large number of samples} in \\emph{ultra-high} dimensional space.\nExisting algorithms lack at least one of these qualities, and thus are\ninefficient in handling such big data challenge. In this paper, we propose a\nmethod enjoying all these merits with an accelerated convergence rate\n$O(\\frac{1}{k^2})$. Empirical studies on large scale datasets with more than\none million features are conducted to show the effectiveness of our methods in\npractice.\n", "versions": [{"version": "v1", "created": "Sun, 13 Nov 2016 16:01:10 GMT"}], "update_date": "2016-11-16", "authors_parsed": [["Shen", "Zebang", ""], ["Qian", "Hui", ""], ["Zhang", "Chao", ""], ["Zhou", "Tengfei", ""]]}, {"id": "1611.04199", "submitter": "Michael Lash", "authors": "Michael T. Lash and W. Nick Street", "title": "Realistic risk-mitigating recommendations via inverse classification", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Inverse classification, the process of making meaningful perturbations to a\ntest point such that it is more likely to have a desired classification, has\npreviously been addressed using data from a single static point in time. Such\nan approach yields inflated probability estimates, stemming from an implicitly\nmade assumption that recommendations are implemented instantaneously. We\npropose using longitudinal data to alleviate such issues in two ways. First, we\nuse past outcome probabilities as features in the present. Use of such past\nprobabilities ties historical behavior to the present, allowing for more\ninformation to be taken into account when making initial probability estimates\nand subsequently performing inverse classification. Secondly, following inverse\nclassification application, optimized instances' unchangeable features\n(e.g.,~age) are updated using values from the next longitudinal time period.\nOptimized test instance probabilities are then reassessed. Updating the\nunchangeable features in this manner reflects the notion that improvements in\noutcome likelihood, which result from following the inverse classification\nrecommendations, do not materialize instantaneously. As our experiments\ndemonstrate, more realistic estimates of probability can be obtained by\nfactoring in such considerations.\n", "versions": [{"version": "v1", "created": "Sun, 13 Nov 2016 22:53:51 GMT"}], "update_date": "2016-11-15", "authors_parsed": [["Lash", "Michael T.", ""], ["Street", "W. Nick", ""]]}, {"id": "1611.04208", "submitter": "Michael Hornstein", "authors": "Michael Hornstein, Roger Fan, Kerby Shedden, and Shuheng Zhou", "title": "Joint mean and covariance estimation with unreplicated matrix-variate\n  data", "comments": "15 Figures; 79 pages and 4 tables; to appear in the Journal of the\n  American Statistical Association; Technical Report 540, Department of\n  Statistics, University of Michigan; removed condition (A1') and corrected\n  condition (A1) and (A2)", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  It has been proposed that complex populations, such as those that arise in\ngenomics studies, may exhibit dependencies among observations as well as among\nvariables. This gives rise to the challenging problem of analyzing unreplicated\nhigh-dimensional data with unknown mean and dependence structures.\nMatrix-variate approaches that impose various forms of (inverse) covariance\nsparsity allow flexible dependence structures to be estimated, but cannot\ndirectly be applied when the mean and covariance matrices are estimated\njointly. We present a practical method utilizing generalized least squares and\npenalized (inverse) covariance estimation to address this challenge. We\nestablish consistency and obtain rates of convergence for estimating the mean\nparameters and covariance matrices. The advantages of our approaches are: (i)\ndependence graphs and covariance structures can be estimated in the presence of\nunknown mean structure, (ii) the mean structure becomes more efficiently\nestimated when accounting for the dependence structure among observations; and\n(iii) inferences about the mean parameters become correctly calibrated. We use\nsimulation studies and analysis of genomic data from a twin study of ulcerative\ncolitis to illustrate the statistical convergence and the performance of our\nmethods in practical settings. Several lines of evidence show that the test\nstatistics for differential gene expression produced by our methods are\ncorrectly calibrated and improve power over conventional methods.\n", "versions": [{"version": "v1", "created": "Sun, 13 Nov 2016 23:54:03 GMT"}, {"version": "v2", "created": "Tue, 15 Nov 2016 04:29:42 GMT"}, {"version": "v3", "created": "Sat, 6 Jan 2018 21:00:30 GMT"}, {"version": "v4", "created": "Thu, 7 Jun 2018 15:41:21 GMT"}], "update_date": "2018-06-08", "authors_parsed": [["Hornstein", "Michael", ""], ["Fan", "Roger", ""], ["Shedden", "Kerby", ""], ["Zhou", "Shuheng", ""]]}, {"id": "1611.04218", "submitter": "Suriya Gunasekar", "authors": "Suriya Gunasekar, Oluwasanmi Koyejo, Joydeep Ghosh", "title": "Preference Completion from Partial Rankings", "comments": "NIPS 2016", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a novel and efficient algorithm for the collaborative preference\ncompletion problem, which involves jointly estimating individualized rankings\nfor a set of entities over a shared set of items, based on a limited number of\nobserved affinity values. Our approach exploits the observation that while\npreferences are often recorded as numerical scores, the predictive quantity of\ninterest is the underlying rankings. Thus, attempts to closely match the\nrecorded scores may lead to overfitting and impair generalization performance.\nInstead, we propose an estimator that directly fits the underlying preference\norder, combined with nuclear norm constraints to encourage low--rank\nparameters. Besides (approximate) correctness of the ranking order, the\nproposed estimator makes no generative assumption on the numerical scores of\nthe observations. One consequence is that the proposed estimator can fit any\nconsistent partial ranking over a subset of the items represented as a directed\nacyclic graph (DAG), generalizing standard techniques that can only fit\npreference scores. Despite this generality, for supervision representing total\nor blockwise total orders, the computational complexity of our algorithm is\nwithin a $\\log$ factor of the standard algorithms for nuclear norm\nregularization based estimates for matrix completion. We further show promising\nempirical results for a novel and challenging application of collaboratively\nranking of the associations between brain--regions and cognitive neuroscience\nterms.\n", "versions": [{"version": "v1", "created": "Mon, 14 Nov 2016 01:17:14 GMT"}], "update_date": "2016-11-16", "authors_parsed": [["Gunasekar", "Suriya", ""], ["Koyejo", "Oluwasanmi", ""], ["Ghosh", "Joydeep", ""]]}, {"id": "1611.04231", "submitter": "Tengyu Ma", "authors": "Moritz Hardt and Tengyu Ma", "title": "Identity Matters in Deep Learning", "comments": "ICLR 2017; fixed minor typos in the previous version", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.NE stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  An emerging design principle in deep learning is that each layer of a deep\nartificial neural network should be able to easily express the identity\ntransformation. This idea not only motivated various normalization techniques,\nsuch as \\emph{batch normalization}, but was also key to the immense success of\n\\emph{residual networks}.\n  In this work, we put the principle of \\emph{identity parameterization} on a\nmore solid theoretical footing alongside further empirical progress. We first\ngive a strikingly simple proof that arbitrarily deep linear residual networks\nhave no spurious local optima. The same result for linear feed-forward networks\nin their standard parameterization is substantially more delicate. Second, we\nshow that residual networks with ReLu activations have universal finite-sample\nexpressivity in the sense that the network can represent any function of its\nsample provided that the model has more parameters than the sample size.\n  Directly inspired by our theory, we experiment with a radically simple\nresidual architecture consisting of only residual convolutional layers and ReLu\nactivations, but no batch normalization, dropout, or max pool. Our model\nimproves significantly on previous all-convolutional networks on the CIFAR10,\nCIFAR100, and ImageNet classification benchmarks.\n", "versions": [{"version": "v1", "created": "Mon, 14 Nov 2016 02:44:18 GMT"}, {"version": "v2", "created": "Sun, 11 Dec 2016 11:39:00 GMT"}, {"version": "v3", "created": "Fri, 20 Jul 2018 04:38:23 GMT"}], "update_date": "2018-07-23", "authors_parsed": [["Hardt", "Moritz", ""], ["Ma", "Tengyu", ""]]}, {"id": "1611.04281", "submitter": "Johannes Berg", "authors": "Johannes Berg", "title": "Statistical mechanics of the inverse Ising problem and the optimal\n  objective function", "comments": "16 pages", "journal-ref": null, "doi": "10.1088/1742-5468/aa7df6", "report-no": null, "categories": "cond-mat.dis-nn q-bio.QM stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The inverse Ising problem seeks to reconstruct the parameters of an Ising\nHamiltonian on the basis of spin configurations sampled from the Boltzmann\nmeasure. Over the last decade, many applications of the inverse Ising problem\nhave arisen, driven by the advent of large-scale data across different\nscientific disciplines. Recently, strategies to solve the inverse Ising problem\nbased on convex optimisation have proven to be very successful. These\napproaches maximise particular objective functions with respect to the model\nparameters. Examples are the pseudolikelihood method and interaction screening.\nIn this paper, we establish a link between approaches to the inverse Ising\nproblem based on convex optimisation and the statistical physics of disordered\nsystems. We characterise the performance of an arbitrary objective function and\ncalculate the objective function which optimally reconstructs the model\nparameters. We evaluate the optimal objective function within a\nreplica-symmetric ansatz and compare the results of the optimal objective\nfunction with other reconstruction methods. Apart from giving a theoretical\nunderpinning to solving the inverse Ising problem by convex optimisation, the\noptimal objective function outperforms state-of-the-art methods, albeit by a\nsmall margin.\n", "versions": [{"version": "v1", "created": "Mon, 14 Nov 2016 08:30:47 GMT"}, {"version": "v2", "created": "Tue, 31 Jan 2017 07:43:13 GMT"}, {"version": "v3", "created": "Wed, 31 May 2017 13:37:28 GMT"}, {"version": "v4", "created": "Fri, 30 Jun 2017 07:31:10 GMT"}], "update_date": "2017-09-13", "authors_parsed": [["Berg", "Johannes", ""]]}, {"id": "1611.04416", "submitter": "Alexis Roche", "authors": "Alexis Roche", "title": "On numerical approximation schemes for expectation propagation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.CO cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Several numerical approximation strategies for the expectation-propagation\nalgorithm are studied in the context of large-scale learning: the Laplace\nmethod, a faster variant of it, Gaussian quadrature, and a deterministic\nversion of variational sampling (i.e., combining quadrature with variational\napproximation). Experiments in training linear binary classifiers show that the\nexpectation-propagation algorithm converges best using variational sampling,\nwhile it also converges well using Laplace-style methods with smooth factors\nbut tends to be unstable with non-differentiable ones. Gaussian quadrature\nyields unstable behavior or convergence to a sub-optimal solution in most\nexperiments.\n", "versions": [{"version": "v1", "created": "Mon, 14 Nov 2016 15:21:23 GMT"}], "update_date": "2016-11-16", "authors_parsed": [["Roche", "Alexis", ""]]}, {"id": "1611.04482", "submitter": "Keith Bonawitz", "authors": "Keith Bonawitz, Vladimir Ivanov, Ben Kreuter, Antonio Marcedone, H.\n  Brendan McMahan, Sarvar Patel, Daniel Ramage, Aaron Segal, Karn Seth", "title": "Practical Secure Aggregation for Federated Learning on User-Held Data", "comments": "5 pages, 1 figure. To appear at the NIPS 2016 workshop on Private\n  Multi-Party Machine Learning", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CR stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Secure Aggregation protocols allow a collection of mutually distrust parties,\neach holding a private value, to collaboratively compute the sum of those\nvalues without revealing the values themselves. We consider training a deep\nneural network in the Federated Learning model, using distributed stochastic\ngradient descent across user-held training data on mobile devices, wherein\nSecure Aggregation protects each user's model gradient. We design a novel,\ncommunication-efficient Secure Aggregation protocol for high-dimensional data\nthat tolerates up to 1/3 users failing to complete the protocol. For 16-bit\ninput values, our protocol offers 1.73x communication expansion for $2^{10}$\nusers and $2^{20}$-dimensional vectors, and 1.98x expansion for $2^{14}$ users\nand $2^{24}$ dimensional vectors.\n", "versions": [{"version": "v1", "created": "Mon, 14 Nov 2016 17:14:55 GMT"}], "update_date": "2016-11-16", "authors_parsed": [["Bonawitz", "Keith", ""], ["Ivanov", "Vladimir", ""], ["Kreuter", "Ben", ""], ["Marcedone", "Antonio", ""], ["McMahan", "H. Brendan", ""], ["Patel", "Sarvar", ""], ["Ramage", "Daniel", ""], ["Segal", "Aaron", ""], ["Seth", "Karn", ""]]}, {"id": "1611.04488", "submitter": "Danica J. Sutherland", "authors": "Danica J. Sutherland, Hsiao-Yu Tung, Heiko Strathmann, Soumyajit De,\n  Aaditya Ramdas, Alex Smola, Arthur Gretton", "title": "Generative Models and Model Criticism via Optimized Maximum Mean\n  Discrepancy", "comments": "Published at ICLR 2017 (public comments:\n  http://openreview.net/forum?id=HJWHIKqgl )", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.AI cs.LG cs.NE stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a method to optimize the representation and distinguishability of\nsamples from two probability distributions, by maximizing the estimated power\nof a statistical test based on the maximum mean discrepancy (MMD). This\noptimized MMD is applied to the setting of unsupervised learning by generative\nadversarial networks (GAN), in which a model attempts to generate realistic\nsamples, and a discriminator attempts to tell these apart from data samples. In\nthis context, the MMD may be used in two roles: first, as a discriminator,\neither directly on the samples, or on features of the samples. Second, the MMD\ncan be used to evaluate the performance of a generative model, by testing the\nmodel's samples against a reference data set. In the latter role, the optimized\nMMD is particularly helpful, as it gives an interpretable indication of how the\nmodel and data distributions differ, even in cases where individual model\nsamples are not easily distinguished either by eye or by classifier.\n", "versions": [{"version": "v1", "created": "Mon, 14 Nov 2016 17:28:27 GMT"}, {"version": "v2", "created": "Tue, 15 Nov 2016 13:07:50 GMT"}, {"version": "v3", "created": "Thu, 17 Nov 2016 20:30:37 GMT"}, {"version": "v4", "created": "Fri, 10 Feb 2017 18:28:49 GMT"}, {"version": "v5", "created": "Thu, 6 Jun 2019 19:54:37 GMT"}, {"version": "v6", "created": "Thu, 14 Jan 2021 06:14:42 GMT"}], "update_date": "2021-01-15", "authors_parsed": [["Sutherland", "Danica J.", ""], ["Tung", "Hsiao-Yu", ""], ["Strathmann", "Heiko", ""], ["De", "Soumyajit", ""], ["Ramdas", "Aaditya", ""], ["Smola", "Alex", ""], ["Gretton", "Arthur", ""]]}, {"id": "1611.04499", "submitter": "Thomas Moreau", "authors": "Thomas Moreau and Julien Audiffren", "title": "Post Training in Deep Learning with Last Kernel", "comments": "submitted to ICLR 2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  One of the main challenges of deep learning methods is the choice of an\nappropriate training strategy. In particular, additional steps, such as\nunsupervised pre-training, have been shown to greatly improve the performances\nof deep structures. In this article, we propose an extra training step, called\npost-training, which only optimizes the last layer of the network. We show that\nthis procedure can be analyzed in the context of kernel theory, with the first\nlayers computing an embedding of the data and the last layer a statistical\nmodel to solve the task based on this embedding. This step makes sure that the\nembedding, or representation, of the data is used in the best possible way for\nthe considered task. This idea is then tested on multiple architectures with\nvarious data sets, showing that it consistently provides a boost in\nperformance.\n", "versions": [{"version": "v1", "created": "Mon, 14 Nov 2016 17:54:28 GMT"}, {"version": "v2", "created": "Tue, 31 Oct 2017 09:25:13 GMT"}], "update_date": "2017-11-01", "authors_parsed": [["Moreau", "Thomas", ""], ["Audiffren", "Julien", ""]]}, {"id": "1611.04500", "submitter": "Siamak Ravanbakhsh", "authors": "Siamak Ravanbakhsh and Jeff Schneider and Barnabas Poczos", "title": "Deep Learning with Sets and Point Clouds", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We introduce a simple permutation equivariant layer for deep learning with\nset structure.This type of layer, obtained by parameter-sharing, has a simple\nimplementation and linear-time complexity in the size of each set. We use deep\npermutation-invariant networks to perform point-could classification and\nMNIST-digit summation, where in both cases the output is invariant to\npermutations of the input. In a semi-supervised setting, where the goal is make\npredictions for each instance within a set, we demonstrate the usefulness of\nthis type of layer in set-outlier detection as well as semi-supervised learning\nwith clustering side-information.\n", "versions": [{"version": "v1", "created": "Mon, 14 Nov 2016 17:55:34 GMT"}, {"version": "v2", "created": "Sat, 26 Nov 2016 22:01:55 GMT"}, {"version": "v3", "created": "Fri, 24 Feb 2017 01:54:59 GMT"}], "update_date": "2017-02-27", "authors_parsed": [["Ravanbakhsh", "Siamak", ""], ["Schneider", "Jeff", ""], ["Poczos", "Barnabas", ""]]}, {"id": "1611.04520", "submitter": "Mengye Ren", "authors": "Mengye Ren, Renjie Liao, Raquel Urtasun, Fabian H. Sinz, Richard S.\n  Zemel", "title": "Normalizing the Normalizers: Comparing and Extending Network\n  Normalization Schemes", "comments": "Published as a conference paper at ICLR 2017", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Normalization techniques have only recently begun to be exploited in\nsupervised learning tasks. Batch normalization exploits mini-batch statistics\nto normalize the activations. This was shown to speed up training and result in\nbetter models. However its success has been very limited when dealing with\nrecurrent neural networks. On the other hand, layer normalization normalizes\nthe activations across all activities within a layer. This was shown to work\nwell in the recurrent setting. In this paper we propose a unified view of\nnormalization techniques, as forms of divisive normalization, which includes\nlayer and batch normalization as special cases. Our second contribution is the\nfinding that a small modification to these normalization schemes, in\nconjunction with a sparse regularizer on the activations, leads to significant\nbenefits over standard normalization techniques. We demonstrate the\neffectiveness of our unified divisive normalization framework in the context of\nconvolutional neural nets and recurrent neural networks, showing improvements\nover baselines in image classification, language modeling as well as\nsuper-resolution.\n", "versions": [{"version": "v1", "created": "Mon, 14 Nov 2016 19:04:58 GMT"}, {"version": "v2", "created": "Mon, 6 Mar 2017 21:03:03 GMT"}], "update_date": "2017-03-08", "authors_parsed": [["Ren", "Mengye", ""], ["Liao", "Renjie", ""], ["Urtasun", "Raquel", ""], ["Sinz", "Fabian H.", ""], ["Zemel", "Richard S.", ""]]}, {"id": "1611.04528", "submitter": "Yanbo Xue", "authors": "Dmytro Korenkevych, Yanbo Xue, Zhengbing Bian, Fabian Chudak, William\n  G. Macready, Jason Rolfe, Evgeny Andriyash", "title": "Benchmarking Quantum Hardware for Training of Fully Visible Boltzmann\n  Machines", "comments": "22 pages, 13 figures, D-Wave quantum system for sampling Boltzmann\n  machines", "journal-ref": null, "doi": null, "report-no": null, "categories": "quant-ph cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Quantum annealing (QA) is a hardware-based heuristic optimization and\nsampling method applicable to discrete undirected graphical models. While\nsimilar to simulated annealing, QA relies on quantum, rather than thermal,\neffects to explore complex search spaces. For many classes of problems, QA is\nknown to offer computational advantages over simulated annealing. Here we\nreport on the ability of recent QA hardware to accelerate training of fully\nvisible Boltzmann machines. We characterize the sampling distribution of QA\nhardware, and show that in many cases, the quantum distributions differ\nsignificantly from classical Boltzmann distributions. In spite of this\ndifference, training (which seeks to match data and model statistics) using\nstandard classical gradient updates is still effective. We investigate the use\nof QA for seeding Markov chains as an alternative to contrastive divergence\n(CD) and persistent contrastive divergence (PCD). Using $k=50$ Gibbs steps, we\nshow that for problems with high-energy barriers between modes, QA-based seeds\ncan improve upon chains with CD and PCD initializations. For these hard\nproblems, QA gradient estimates are more accurate, and allow for faster\nlearning. Furthermore, and interestingly, even the case of raw QA samples (that\nis, $k=0$) achieved similar improvements. We argue that this relates to the\nfact that we are training a quantum rather than classical Boltzmann\ndistribution in this case. The learned parameters give rise to hardware QA\ndistributions closely approximating classical Boltzmann distributions that are\nhard to train with CD/PCD.\n", "versions": [{"version": "v1", "created": "Mon, 14 Nov 2016 19:15:57 GMT"}], "update_date": "2016-11-15", "authors_parsed": [["Korenkevych", "Dmytro", ""], ["Xue", "Yanbo", ""], ["Bian", "Zhengbing", ""], ["Chudak", "Fabian", ""], ["Macready", "William G.", ""], ["Rolfe", "Jason", ""], ["Andriyash", "Evgeny", ""]]}, {"id": "1611.04561", "submitter": "Tal Galili", "authors": "Tal Galili, Isaac Meilijson", "title": "Splitting matters: how monotone transformation of predictor variables\n  may improve the predictions of decision tree models", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  It is widely believed that the prediction accuracy of decision tree models is\ninvariant under any strictly monotone transformation of the individual\npredictor variables. However, this statement may be false when predicting new\nobservations with values that were not seen in the training-set and are close\nto the location of the split point of a tree rule. The sensitivity of the\nprediction error to the split point interpolation is high when the split point\nof the tree is estimated based on very few observations, reaching 9%\nmisclassification error when only 10 observations are used for constructing a\nsplit, and shrinking to 1% when relying on 100 observations. This study\ncompares the performance of alternative methods for split point interpolation\nand concludes that the best choice is taking the mid-point between the two\nclosest points to the split point of the tree. Furthermore, if the (continuous)\ndistribution of the predictor variable is known, then using its probability\nintegral for transforming the variable (\"quantile transformation\") will reduce\nthe model's interpolation error by up to about a half on average. Accordingly,\nthis study provides guidelines for both developers and users of decision tree\nmodels (including bagging and random forest).\n", "versions": [{"version": "v1", "created": "Mon, 14 Nov 2016 20:34:29 GMT"}], "update_date": "2016-11-16", "authors_parsed": [["Galili", "Tal", ""], ["Meilijson", "Isaac", ""]]}, {"id": "1611.04701", "submitter": "Shuheng Zhou", "authors": "Mark Rudelson and Shuheng Zhou", "title": "Errors-in-variables models with dependent measurements", "comments": "8 Figures, one table, 99 pages. arXiv admin note: text overlap with\n  arXiv:1502.02355", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Suppose that we observe $y \\in \\mathbb{R}^n$ and $X \\in \\mathbb{R}^{n \\times\nm}$ in the following errors-in-variables model: \\begin{eqnarray*} y & = & X_0\n\\beta^* +\\epsilon \\\\ X & = & X_0 + W, \\end{eqnarray*} where $X_0$ is an $n\n\\times m$ design matrix with independent subgaussian row vectors, $\\epsilon \\in\n\\mathbb{R}^n$ is a noise vector and $W$ is a mean zero $n \\times m$ random\nnoise matrix with independent subgaussian column vectors, independent of $X_0$\nand $\\epsilon$. This model is significantly different from those analyzed in\nthe literature in the sense that we allow the measurement error for each\ncovariate to be a dependent vector across its $n$ observations. Such error\nstructures appear in the science literature when modeling the trial-to-trial\nfluctuations in response strength shared across a set of neurons.\n  Under sparsity and restrictive eigenvalue type of conditions, we show that\none is able to recover a sparse vector $\\beta^* \\in \\mathbb{R}^m$ from the\nmodel given a single observation matrix $X$ and the response vector $y$. We\nestablish consistency in estimating $\\beta^*$ and obtain the rates of\nconvergence in the $\\ell_q$ norm, where $q = 1, 2$. We show error bounds which\napproach that of the regular Lasso and the Dantzig selector in case the errors\nin $W$ are tending to 0. We analyze the convergence rates of the gradient\ndescent methods for solving the nonconvex programs and show that the composite\ngradient descent algorithm is guaranteed to converge at a geometric rate to a\nneighborhood of the global minimizers: the size of the neighborhood is bounded\nby the statistical error in the $\\ell_2$ norm. Our analysis reveals interesting\nconnections between computational and statistical efficiency and the\nconcentration of measure phenomenon in random matrix theory. We provide\nsimulation evidence illuminating the theoretical predictions.\n", "versions": [{"version": "v1", "created": "Tue, 15 Nov 2016 04:12:08 GMT"}, {"version": "v2", "created": "Sat, 1 Apr 2017 03:25:39 GMT"}], "update_date": "2017-04-04", "authors_parsed": [["Rudelson", "Mark", ""], ["Zhou", "Shuheng", ""]]}, {"id": "1611.04709", "submitter": "Jin Tian", "authors": "Jin Tian", "title": "Recoverability of Joint Distribution from Missing Data", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A probabilistic query may not be estimable from observed data corrupted by\nmissing values if the data are not missing at random (MAR). It is therefore of\ntheoretical interest and practical importance to determine in principle whether\na probabilistic query is estimable from missing data or not when the data are\nnot MAR. We present an algorithm that systematically determines whether the\njoint probability is estimable from observed data with missing values, assuming\nthat the data-generation model is represented as a Bayesian network containing\nunobserved latent variables that not only encodes the dependencies among the\nvariables but also explicitly portrays the mechanisms responsible for the\nmissingness process. The result significantly advances the existing work.\n", "versions": [{"version": "v1", "created": "Tue, 15 Nov 2016 06:06:42 GMT"}], "update_date": "2016-11-16", "authors_parsed": [["Tian", "Jin", ""]]}, {"id": "1611.04790", "submitter": "Kira Kempinska", "authors": "Kira Kempinska, John Shawe-Taylor", "title": "Improved Particle Filters for Vehicle Localisation", "comments": "Presented as a poster at NIPS 2016 Time Series Workshop", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The ability to track a moving vehicle is of crucial importance in numerous\napplications. The task has often been approached by the importance sampling\ntechnique of particle filters due to its ability to model non-linear and\nnon-Gaussian dynamics, of which a vehicle travelling on a road network is a\ngood example. Particle filters perform poorly when observations are highly\ninformative. In this paper, we address this problem by proposing particle\nfilters that sample around the most recent observation. The proposal leads to\nan order of magnitude improvement in accuracy and efficiency over conventional\nparticle filters, especially when observations are infrequent but low-noise.\n", "versions": [{"version": "v1", "created": "Tue, 15 Nov 2016 11:17:33 GMT"}], "update_date": "2016-11-16", "authors_parsed": [["Kempinska", "Kira", ""], ["Shawe-Taylor", "John", ""]]}, {"id": "1611.04831", "submitter": "Kfir Levy Yehuda", "authors": "Kfir Y. Levy", "title": "The Power of Normalization: Faster Evasion of Saddle Points", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG math.OC stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A commonly used heuristic in non-convex optimization is Normalized Gradient\nDescent (NGD) - a variant of gradient descent in which only the direction of\nthe gradient is taken into account and its magnitude ignored. We analyze this\nheuristic and show that with carefully chosen parameters and noise injection,\nthis method can provably evade saddle points. We establish the convergence of\nNGD to a local minimum, and demonstrate rates which improve upon the fastest\nknown first order algorithm due to Ge e al. (2015).\n  The effectiveness of our method is demonstrated via an application to the\nproblem of online tensor decomposition; a task for which saddle point evasion\nis known to result in convergence to global minima.\n", "versions": [{"version": "v1", "created": "Tue, 15 Nov 2016 13:56:24 GMT"}], "update_date": "2016-11-22", "authors_parsed": [["Levy", "Kfir Y.", ""]]}, {"id": "1611.04835", "submitter": "Nauman Shahid", "authors": "Nauman Shahid, Francesco Grassi, Pierre Vandergheynst", "title": "Multilinear Low-Rank Tensors on Graphs & Applications", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a new framework for the analysis of low-rank tensors which lies at\nthe intersection of spectral graph theory and signal processing. As a first\nstep, we present a new graph based low-rank decomposition which approximates\nthe classical low-rank SVD for matrices and multi-linear SVD for tensors. Then,\nbuilding on this novel decomposition we construct a general class of convex\noptimization problems for approximately solving low-rank tensor inverse\nproblems, such as tensor Robust PCA. The whole framework is named as\n'Multilinear Low-rank tensors on Graphs (MLRTG)'. Our theoretical analysis\nshows: 1) MLRTG stands on the notion of approximate stationarity of\nmulti-dimensional signals on graphs and 2) the approximation error depends on\nthe eigen gaps of the graphs. We demonstrate applications for a wide variety of\n4 artificial and 12 real tensor datasets, such as EEG, FMRI, BCI, surveillance\nvideos and hyperspectral images. Generalization of the tensor concepts to\nnon-euclidean domain, orders of magnitude speed-up, low-memory requirement and\nsignificantly enhanced performance at low SNR are the key aspects of our\nframework.\n", "versions": [{"version": "v1", "created": "Tue, 15 Nov 2016 14:05:43 GMT"}], "update_date": "2016-11-16", "authors_parsed": [["Shahid", "Nauman", ""], ["Grassi", "Francesco", ""], ["Vandergheynst", "Pierre", ""]]}, {"id": "1611.04920", "submitter": "Qinliang Su", "authors": "Qinliang Su, Xuejun Liao, Chunyuan Li, Zhe Gan, Lawrence Carin", "title": "Unsupervised Learning with Truncated Gaussian Graphical Models", "comments": "To appear in AAAI 2017", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Gaussian graphical models (GGMs) are widely used for statistical modeling,\nbecause of ease of inference and the ubiquitous use of the normal distribution\nin practical approximations. However, they are also known for their limited\nmodeling abilities, due to the Gaussian assumption. In this paper, we introduce\na novel variant of GGMs, which relaxes the Gaussian restriction and yet admits\nefficient inference. Specifically, we impose a bipartite structure on the GGM\nand govern the hidden variables by truncated normal distributions. The\nnonlinearity of the model is revealed by its connection to rectified linear\nunit (ReLU) neural networks. Meanwhile, thanks to the bipartite structure and\nappealing properties of truncated normals, we are able to train the models\nefficiently using contrastive divergence. We consider three output constructs,\naccounting for real-valued, binary and count data. We further extend the model\nto deep constructions and show that deep models can be used for unsupervised\npre-training of rectifier neural networks. Extensive experimental results are\nprovided to validate the proposed models and demonstrate their superiority over\ncompeting models.\n", "versions": [{"version": "v1", "created": "Tue, 15 Nov 2016 16:26:17 GMT"}, {"version": "v2", "created": "Sun, 20 Nov 2016 19:08:51 GMT"}], "update_date": "2016-11-22", "authors_parsed": [["Su", "Qinliang", ""], ["Liao", "Xuejun", ""], ["Li", "Chunyuan", ""], ["Gan", "Zhe", ""], ["Carin", "Lawrence", ""]]}, {"id": "1611.04967", "submitter": "Julius Adebayo", "authors": "Julius Adebayo, Lalana Kagal", "title": "Iterative Orthogonal Feature Projection for Diagnosing Bias in Black-Box\n  Models", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Predictive models are increasingly deployed for the purpose of determining\naccess to services such as credit, insurance, and employment. Despite potential\ngains in productivity and efficiency, several potential problems have yet to be\naddressed, particularly the potential for unintentional discrimination. We\npresent an iterative procedure, based on orthogonal projection of input\nattributes, for enabling interpretability of black-box predictive models.\nThrough our iterative procedure, one can quantify the relative dependence of a\nblack-box model on its input attributes.The relative significance of the inputs\nto a predictive model can then be used to assess the fairness (or\ndiscriminatory extent) of such a model.\n", "versions": [{"version": "v1", "created": "Tue, 15 Nov 2016 18:10:24 GMT"}], "update_date": "2016-11-16", "authors_parsed": [["Adebayo", "Julius", ""], ["Kagal", "Lalana", ""]]}, {"id": "1611.04982", "submitter": "Yossi Arjevani", "authors": "Yossi Arjevani and Ohad Shamir", "title": "Oracle Complexity of Second-Order Methods for Finite-Sum Problems", "comments": "30 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.OC cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Finite-sum optimization problems are ubiquitous in machine learning, and are\ncommonly solved using first-order methods which rely on gradient computations.\nRecently, there has been growing interest in \\emph{second-order} methods, which\nrely on both gradients and Hessians. In principle, second-order methods can\nrequire much fewer iterations than first-order methods, and hold the promise\nfor more efficient algorithms. Although computing and manipulating Hessians is\nprohibitive for high-dimensional problems in general, the Hessians of\nindividual functions in finite-sum problems can often be efficiently computed,\ne.g. because they possess a low-rank structure. Can second-order information\nindeed be used to solve such problems more efficiently? In this paper, we\nprovide evidence that the answer -- perhaps surprisingly -- is negative, at\nleast in terms of worst-case guarantees. However, we also discuss what\nadditional assumptions and algorithmic approaches might potentially circumvent\nthis negative result.\n", "versions": [{"version": "v1", "created": "Tue, 15 Nov 2016 18:41:55 GMT"}, {"version": "v2", "created": "Wed, 8 Mar 2017 11:05:59 GMT"}], "update_date": "2017-03-09", "authors_parsed": [["Arjevani", "Yossi", ""], ["Shamir", "Ohad", ""]]}, {"id": "1611.05010", "submitter": "Kejun Huang", "authors": "Kejun Huang, Xiao Fu, Nicholas D. Sidiropoulos", "title": "Anchor-Free Correlated Topic Modeling: Identifiability and Algorithm", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.CL cs.IR cs.SI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In topic modeling, many algorithms that guarantee identifiability of the\ntopics have been developed under the premise that there exist anchor words --\ni.e., words that only appear (with positive probability) in one topic.\nFollow-up work has resorted to three or higher-order statistics of the data\ncorpus to relax the anchor word assumption. Reliable estimates of higher-order\nstatistics are hard to obtain, however, and the identification of topics under\nthose models hinges on uncorrelatedness of the topics, which can be\nunrealistic. This paper revisits topic modeling based on second-order moments,\nand proposes an anchor-free topic mining framework. The proposed approach\nguarantees the identification of the topics under a much milder condition\ncompared to the anchor-word assumption, thereby exhibiting much better\nrobustness in practice. The associated algorithm only involves one\neigen-decomposition and a few small linear programs. This makes it easy to\nimplement and scale up to very large problem instances. Experiments using the\nTDT2 and Reuters-21578 corpus demonstrate that the proposed anchor-free\napproach exhibits very favorable performance (measured using coherence,\nsimilarity count, and clustering accuracy metrics) compared to the prior art.\n", "versions": [{"version": "v1", "created": "Tue, 15 Nov 2016 20:06:40 GMT"}], "update_date": "2016-11-16", "authors_parsed": [["Huang", "Kejun", ""], ["Fu", "Xiao", ""], ["Sidiropoulos", "Nicholas D.", ""]]}, {"id": "1611.05126", "submitter": "Jan Hamaekers", "authors": "James Barker, Johannes Bulin, Jan Hamaekers and Sonja Mathias", "title": "Localized Coulomb Descriptors for the Gaussian Approximation Potential", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML physics.chem-ph", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We introduce a novel class of localized atomic environment representations,\nbased upon the Coulomb matrix. By combining these functions with the Gaussian\napproximation potential approach, we present LC-GAP, a new system for\ngenerating atomic potentials through machine learning (ML). Tests on the QM7,\nQM7b and GDB9 biomolecular datasets demonstrate that potentials created with\nLC-GAP can successfully predict atomization energies for molecules larger than\nthose used for training to chemical accuracy, and can (in the case of QM7b)\nalso be used to predict a range of other atomic properties with accuracy in\nline with the recent literature. As the best-performing representation has only\nlinear dimensionality in the number of atoms in a local atomic environment,\nthis represents an improvement both in prediction accuracy and computational\ncost when considered against similar Coulomb matrix-based methods.\n", "versions": [{"version": "v1", "created": "Wed, 16 Nov 2016 02:57:40 GMT"}, {"version": "v2", "created": "Tue, 6 Dec 2016 12:01:13 GMT"}], "update_date": "2016-12-07", "authors_parsed": [["Barker", "James", ""], ["Bulin", "Johannes", ""], ["Hamaekers", "Jan", ""], ["Mathias", "Sonja", ""]]}, {"id": "1611.05136", "submitter": "Mahtab J. Fard", "authors": "Mahtab J. Fard, Sattar Ameri, Ratna B. Chinnam, Abhilash K. Pandya,\n  Michael D. Klein, and R. Darin Ellis", "title": "Machine Learning Approach for Skill Evaluation in Robotic-Assisted\n  Surgery", "comments": null, "journal-ref": "Lecture Notes in Engineering and Computer Science: Proceedings of\n  The World Congress on Engineering and Computer Science 2016, 19-21 October,\n  2016, San Francisco, USA", "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Evaluating surgeon skill has predominantly been a subjective task.\nDevelopment of objective methods for surgical skill assessment are of increased\ninterest. Recently, with technological advances such as robotic-assisted\nminimally invasive surgery (RMIS), new opportunities for objective and\nautomated assessment frameworks have arisen. In this paper, we applied machine\nlearning methods to automatically evaluate performance of the surgeon in RMIS.\nSix important movement features were used in the evaluation including\ncompletion time, path length, depth perception, speed, smoothness and\ncurvature. Different classification methods applied to discriminate expert and\nnovice surgeons. We test our method on real surgical data for suturing task and\ncompare the classification result with the ground truth data (obtained by\nmanual labeling). The experimental results show that the proposed framework can\nclassify surgical skill level with relatively high accuracy of 85.7%. This\nstudy demonstrates the ability of machine learning methods to automatically\nclassify expert and novice surgeons using movement features for different RMIS\ntasks. Due to the simplicity and generalizability of the introduced\nclassification method, it is easy to implement in existing trainers.\n", "versions": [{"version": "v1", "created": "Wed, 16 Nov 2016 03:45:12 GMT"}], "update_date": "2016-11-18", "authors_parsed": [["Fard", "Mahtab J.", ""], ["Ameri", "Sattar", ""], ["Chinnam", "Ratna B.", ""], ["Pandya", "Abhilash K.", ""], ["Klein", "Michael D.", ""], ["Ellis", "R. Darin", ""]]}, {"id": "1611.05146", "submitter": "Ahmed Alaa", "authors": "Ahmed M. Alaa, Jinsung Yoon, Scott Hu, Mihaela van der Schaar", "title": "A Semi-Markov Switching Linear Gaussian Model for Censored Physiological\n  Data", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Critically ill patients in regular wards are vulnerable to unanticipated\nclinical dete- rioration which requires timely transfer to the intensive care\nunit (ICU). To allow for risk scoring and patient monitoring in such a setting,\nwe develop a novel Semi- Markov Switching Linear Gaussian Model (SSLGM) for the\ninpatients' physiol- ogy. The model captures the patients' latent clinical\nstates and their corresponding observable lab tests and vital signs. We present\nan efficient unsupervised learn- ing algorithm that capitalizes on the\ninformatively censored data in the electronic health records (EHR) to learn the\nparameters of the SSLGM; the learned model is then used to assess the new\ninpatients' risk for clinical deterioration in an online fashion, allowing for\ntimely ICU admission. Experiments conducted on a het- erogeneous cohort of\n6,094 patients admitted to a large academic medical center show that the\nproposed model significantly outperforms the currently deployed risk scores\nsuch as Rothman index, MEWS, SOFA and APACHE.\n", "versions": [{"version": "v1", "created": "Wed, 16 Nov 2016 05:11:36 GMT"}], "update_date": "2016-11-17", "authors_parsed": [["Alaa", "Ahmed M.", ""], ["Yoon", "Jinsung", ""], ["Hu", "Scott", ""], ["van der Schaar", "Mihaela", ""]]}, {"id": "1611.05162", "submitter": "Alireza Aghasi", "authors": "Alireza Aghasi, Afshin Abdi, Nam Nguyen, Justin Romberg", "title": "Net-Trim: Convex Pruning of Deep Neural Networks with Performance\n  Guarantee", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We introduce and analyze a new technique for model reduction for deep neural\nnetworks. While large networks are theoretically capable of learning\narbitrarily complex models, overfitting and model redundancy negatively affects\nthe prediction accuracy and model variance. Our Net-Trim algorithm prunes\n(sparsifies) a trained network layer-wise, removing connections at each layer\nby solving a convex optimization program. This program seeks a sparse set of\nweights at each layer that keeps the layer inputs and outputs consistent with\nthe originally trained model. The algorithms and associated analysis are\napplicable to neural networks operating with the rectified linear unit (ReLU)\nas the nonlinear activation. We present both parallel and cascade versions of\nthe algorithm. While the latter can achieve slightly simpler models with the\nsame generalization performance, the former can be computed in a distributed\nmanner. In both cases, Net-Trim significantly reduces the number of connections\nin the network, while also providing enough regularization to slightly reduce\nthe generalization error. We also provide a mathematical analysis of the\nconsistency between the initial network and the retrained model. To analyze the\nmodel sample complexity, we derive the general sufficient conditions for the\nrecovery of a sparse transform matrix. For a single layer taking independent\nGaussian random vectors of length $N$ as inputs, we show that if the network\nresponse can be described using a maximum number of $s$ non-zero weights per\nnode, these weights can be learned from $\\mathcal{O}(s\\log N)$ samples.\n", "versions": [{"version": "v1", "created": "Wed, 16 Nov 2016 06:34:41 GMT"}, {"version": "v2", "created": "Thu, 17 Nov 2016 23:07:52 GMT"}, {"version": "v3", "created": "Sun, 19 Mar 2017 22:38:08 GMT"}, {"version": "v4", "created": "Thu, 23 Nov 2017 09:34:28 GMT"}], "update_date": "2017-11-27", "authors_parsed": [["Aghasi", "Alireza", ""], ["Abdi", "Afshin", ""], ["Nguyen", "Nam", ""], ["Romberg", "Justin", ""]]}, {"id": "1611.05181", "submitter": "Hilmi Enes Egilmez", "authors": "Hilmi E. Egilmez, Eduardo Pavez, Antonio Ortega", "title": "Graph Learning from Data under Structural and Laplacian Constraints", "comments": "To appear in IEEE Journal of Selected Topics in Signal Processing.\n  The implementations of the algorithms proposed in this paper are available\n  at: https://github.com/STAC-USC/Graph_Learning", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Graphs are fundamental mathematical structures used in various fields to\nrepresent data, signals and processes. In this paper, we propose a novel\nframework for learning/estimating graphs from data. The proposed framework\nincludes (i) formulation of various graph learning problems, (ii) their\nprobabilistic interpretations and (iii) associated algorithms. Specifically,\ngraph learning problems are posed as estimation of graph Laplacian matrices\nfrom some observed data under given structural constraints (e.g., graph\nconnectivity and sparsity level). From a probabilistic perspective, the\nproblems of interest correspond to maximum a posteriori (MAP) parameter\nestimation of Gaussian-Markov random field (GMRF) models, whose precision\n(inverse covariance) is a graph Laplacian matrix. For the proposed graph\nlearning problems, specialized algorithms are developed by incorporating the\ngraph Laplacian and structural constraints. The experimental results\ndemonstrate that the proposed algorithms outperform the current\nstate-of-the-art methods in terms of accuracy and computational efficiency.\n", "versions": [{"version": "v1", "created": "Wed, 16 Nov 2016 08:11:14 GMT"}, {"version": "v2", "created": "Wed, 17 May 2017 17:03:55 GMT"}, {"version": "v3", "created": "Thu, 6 Jul 2017 03:26:33 GMT"}], "update_date": "2017-07-07", "authors_parsed": [["Egilmez", "Hilmi E.", ""], ["Pavez", "Eduardo", ""], ["Ortega", "Antonio", ""]]}, {"id": "1611.05209", "submitter": "Siddharth Agrawal", "authors": "Siddharth Agrawal, Ambedkar Dukkipati", "title": "Deep Variational Inference Without Pixel-Wise Reconstruction", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Variational autoencoders (VAEs), that are built upon deep neural networks\nhave emerged as popular generative models in computer vision. Most of the work\ntowards improving variational autoencoders has focused mainly on making the\napproximations to the posterior flexible and accurate, leading to tremendous\nprogress. However, there have been limited efforts to replace pixel-wise\nreconstruction, which have known shortcomings. In this work, we use real-valued\nnon-volume preserving transformations (real NVP) to exactly compute the\nconditional likelihood of the data given the latent distribution. We show that\na simple VAE with this form of reconstruction is competitive with complicated\nVAE structures, on image modeling tasks. As part of our model, we develop\npowerful conditional coupling layers that enable real NVP to learn with fewer\nintermediate layers.\n", "versions": [{"version": "v1", "created": "Wed, 16 Nov 2016 10:20:10 GMT"}], "update_date": "2016-11-17", "authors_parsed": [["Agrawal", "Siddharth", ""], ["Dukkipati", "Ambedkar", ""]]}, {"id": "1611.05368", "submitter": "Jeremiah Johnson", "authors": "Jeremiah Johnson", "title": "Neural Style Representations and the Large-Scale Classification of\n  Artistic Style", "comments": "10 pages, 4 figures, 2 tables", "journal-ref": "Proceedings of the Future Technologies Conference, 2017", "doi": null, "report-no": null, "categories": "cs.CV cs.AI cs.IR stat.AP stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The artistic style of a painting is a subtle aesthetic judgment used by art\nhistorians for grouping and classifying artwork. The recently introduced\n`neural-style' algorithm substantially succeeds in merging the perceived\nartistic style of one image or set of images with the perceived content of\nanother. In light of this and other recent developments in image analysis via\nconvolutional neural networks, we investigate the effectiveness of a\n`neural-style' representation for classifying the artistic style of paintings.\n", "versions": [{"version": "v1", "created": "Wed, 16 Nov 2016 17:04:04 GMT"}], "update_date": "2019-06-04", "authors_parsed": [["Johnson", "Jeremiah", ""]]}, {"id": "1611.05378", "submitter": "Maria Francesca", "authors": "Maria Francesca and Arthur Hughes and David Gregg", "title": "Spectral Convolution Networks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Previous research has shown that computation of convolution in the frequency\ndomain provides a significant speedup versus traditional convolution network\nimplementations. However, this performance increase comes at the expense of\nrepeatedly computing the transform and its inverse in order to apply other\nnetwork operations such as activation, pooling, and dropout. We show,\nmathematically, how convolution and activation can both be implemented in the\nfrequency domain using either the Fourier or Laplace transformation. The main\ncontributions are a description of spectral activation under the Fourier\ntransform and a further description of an efficient algorithm for computing\nboth convolution and activation under the Laplace transform. By computing both\nthe convolution and activation functions in the frequency domain, we can reduce\nthe number of transforms required, as well as reducing overall complexity. Our\ndescription of a spectral activation function, together with existing spectral\nanalogs of other network functions may then be used to compose a fully spectral\nimplementation of a convolution network.\n", "versions": [{"version": "v1", "created": "Wed, 16 Nov 2016 17:32:09 GMT"}], "update_date": "2016-11-17", "authors_parsed": [["Francesca", "Maria", ""], ["Hughes", "Arthur", ""], ["Gregg", "David", ""]]}, {"id": "1611.05402", "submitter": "Hantian Zhang", "authors": "Hantian Zhang, Jerry Li, Kaan Kara, Dan Alistarh, Ji Liu, Ce Zhang", "title": "The ZipML Framework for Training Models with End-to-End Low Precision:\n  The Cans, the Cannots, and a Little Bit of Deep Learning", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recently there has been significant interest in training machine-learning\nmodels at low precision: by reducing precision, one can reduce computation and\ncommunication by one order of magnitude. We examine training at reduced\nprecision, both from a theoretical and practical perspective, and ask: is it\npossible to train models at end-to-end low precision with provable guarantees?\nCan this lead to consistent order-of-magnitude speedups? We present a framework\ncalled ZipML to answer these questions. For linear models, the answer is yes.\nWe develop a simple framework based on one simple but novel strategy called\ndouble sampling. Our framework is able to execute training at low precision\nwith no bias, guaranteeing convergence, whereas naive quantization would\nintroduce significant bias. We validate our framework across a range of\napplications, and show that it enables an FPGA prototype that is up to 6.5x\nfaster than an implementation using full 32-bit precision. We further develop a\nvariance-optimal stochastic quantization strategy and show that it can make a\nsignificant difference in a variety of settings. When applied to linear models\ntogether with double sampling, we save up to another 1.7x in data movement\ncompared with uniform quantization. When training deep networks with quantized\nmodels, we achieve higher accuracy than the state-of-the-art XNOR-Net. Finally,\nwe extend our framework through approximation to non-linear models, such as\nSVM. We show that, although using low-precision data induces bias, we can\nappropriately bound and control the bias. We find in practice 8-bit precision\nis often sufficient to converge to the correct solution. Interestingly,\nhowever, in practice we notice that our framework does not always outperform\nthe naive rounding approach. We discuss this negative result in detail.\n", "versions": [{"version": "v1", "created": "Wed, 16 Nov 2016 18:45:09 GMT"}, {"version": "v2", "created": "Wed, 8 Mar 2017 23:33:12 GMT"}, {"version": "v3", "created": "Mon, 19 Jun 2017 14:36:00 GMT"}], "update_date": "2017-06-20", "authors_parsed": [["Zhang", "Hantian", ""], ["Li", "Jerry", ""], ["Kara", "Kaan", ""], ["Alistarh", "Dan", ""], ["Liu", "Ji", ""], ["Zhang", "Ce", ""]]}, {"id": "1611.05407", "submitter": "David Choi", "authors": "David Choi", "title": "A Semidefinite Program for Structured Blockmodels", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.ML stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Semidefinite programs have recently been developed for the problem of\ncommunity detection, which may be viewed as a special case of the stochastic\nblockmodel. Here, we develop a semidefinite program that can be tailored to\nother instances of the blockmodel, such as non-assortative networks and\noverlapping communities. We establish label recovery in sparse settings, with\nconditions that are analogous to recent results for community detection. In\nsettings where the data is not generated by a blockmodel, we give an oracle\ninequality that bounds excess risk relative to the best blockmodel\napproximation. Simulations are presented for community detection, for\noverlapping communities, and for latent space models.\n", "versions": [{"version": "v1", "created": "Wed, 16 Nov 2016 19:00:47 GMT"}], "update_date": "2016-11-17", "authors_parsed": [["Choi", "David", ""]]}, {"id": "1611.05425", "submitter": "Tim Weninger PhD", "authors": "Baoxu Shi and Tim Weninger", "title": "ProjE: Embedding Projection for Knowledge Graph Completion", "comments": "14 pages, Accepted to AAAI 2017", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI stat.ML", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  With the large volume of new information created every day, determining the\nvalidity of information in a knowledge graph and filling in its missing parts\nare crucial tasks for many researchers and practitioners. To address this\nchallenge, a number of knowledge graph completion methods have been developed\nusing low-dimensional graph embeddings. Although researchers continue to\nimprove these models using an increasingly complex feature space, we show that\nsimple changes in the architecture of the underlying model can outperform\nstate-of-the-art models without the need for complex feature engineering. In\nthis work, we present a shared variable neural network model called ProjE that\nfills-in missing information in a knowledge graph by learning joint embeddings\nof the knowledge graph's entities and edges, and through subtle, but important,\nchanges to the standard loss function. In doing so, ProjE has a parameter size\nthat is smaller than 11 out of 15 existing methods while performing $37\\%$\nbetter than the current-best method on standard datasets. We also show, via a\nnew fact checking task, that ProjE is capable of accurately determining the\nveracity of many declarative statements.\n", "versions": [{"version": "v1", "created": "Wed, 16 Nov 2016 20:09:08 GMT"}], "update_date": "2016-11-17", "authors_parsed": [["Shi", "Baoxu", ""], ["Weninger", "Tim", ""]]}, {"id": "1611.05433", "submitter": "Jacqueline Meulman", "authors": "Jacqueline J. Meulman and Anita J. van der Kooij", "title": "ROS Regression: Integrating Regularization and Optimal Scaling\n  Regression", "comments": "36 pages, 13 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper we combine two important extensions of ordinary least squares\nregression: regularization and optimal scaling. Optimal scaling (sometimes also\ncalled optimal scoring) has originally been developed for categorical data, and\nthe process finds quantifications for the categories that are optimal for the\nregression model in the sense that they maximize the multiple correlation.\nAlthough the optimal scaling method was developed initially for variables with\na limited number of categories, optimal transformations of continuous variables\nare a special case. We will consider a variety of transformation types;\ntypically we use step functions for categorical variables, and smooth (spline)\nfunctions for continuous variables. Both types of functions can be restricted\nto be monotonic, preserving the ordinal information in the data. In addition to\noptimal scaling, three regularization methods will be considered: Ridge\nregression, the Lasso, and the Elastic Net. The resulting method will be called\nROS Regression (Regularized Optimal Scaling Regression. We will show that the\nbasic OS algorithm provides straightforward and efficient estimation of the\nregularized regression coefficients, automatically gives the Group Lasso and\nBlockwise Sparse Regression, and extends them with monotonicity properties. We\nwill show that Optimal Scaling linearizes nonlinear relationships between\npredictors and outcome, and improves upon the condition of the predictor\ncorrelation matrix, increasing (on average) the conditional independence of the\npredictors. Alternative options for regularization of either regression\ncoefficients or category quantifications are mentioned. Extended examples are\nprovided.\n  Keywords: Categorical Data, Optimal Scaling, Conditional Independence, Step\nFunctions, Splines, Monotonic Transformations, Regularization, Lasso, Elastic\nNet, Group Lasso, Blockwise Sparse Regression.\n", "versions": [{"version": "v1", "created": "Wed, 16 Nov 2016 20:40:19 GMT"}], "update_date": "2016-11-17", "authors_parsed": [["Meulman", "Jacqueline J.", ""], ["van der Kooij", "Anita J.", ""]]}, {"id": "1611.05469", "submitter": "Daniel Smilkov", "authors": "Daniel Smilkov, Nikhil Thorat, Charles Nicholson, Emily Reif, Fernanda\n  B. Vi\\'egas, Martin Wattenberg", "title": "Embedding Projector: Interactive Visualization and Interpretation of\n  Embeddings", "comments": "Presented at NIPS 2016 Workshop on Interpretable Machine Learning in\n  Complex Systems", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Embeddings are ubiquitous in machine learning, appearing in recommender\nsystems, NLP, and many other applications. Researchers and developers often\nneed to explore the properties of a specific embedding, and one way to analyze\nembeddings is to visualize them. We present the Embedding Projector, a tool for\ninteractive visualization and interpretation of embeddings.\n", "versions": [{"version": "v1", "created": "Wed, 16 Nov 2016 21:21:11 GMT"}], "update_date": "2016-11-18", "authors_parsed": [["Smilkov", "Daniel", ""], ["Thorat", "Nikhil", ""], ["Nicholson", "Charles", ""], ["Reif", "Emily", ""], ["Vi\u00e9gas", "Fernanda B.", ""], ["Wattenberg", "Martin", ""]]}, {"id": "1611.05487", "submitter": "Ilya Safro", "authors": "Ehsan Sadrfaridpour, Sandeep Jeereddy, Ken Kennedy, Andre Luckow,\n  Talayeh Razzaghi, Ilya Safro", "title": "Algebraic multigrid support vector machines", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.DS cs.LG stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The support vector machine is a flexible optimization-based technique widely\nused for classification problems. In practice, its training part becomes\ncomputationally expensive on large-scale data sets because of such reasons as\nthe complexity and number of iterations in parameter fitting methods,\nunderlying optimization solvers, and nonlinearity of kernels. We introduce a\nfast multilevel framework for solving support vector machine models that is\ninspired by the algebraic multigrid. Significant improvement in the running has\nbeen achieved without any loss in the quality. The proposed technique is highly\nbeneficial on imbalanced sets. We demonstrate computational results on publicly\navailable and industrial data sets.\n", "versions": [{"version": "v1", "created": "Wed, 16 Nov 2016 22:32:50 GMT"}, {"version": "v2", "created": "Thu, 24 Nov 2016 01:10:21 GMT"}], "update_date": "2016-11-28", "authors_parsed": [["Sadrfaridpour", "Ehsan", ""], ["Jeereddy", "Sandeep", ""], ["Kennedy", "Ken", ""], ["Luckow", "Andre", ""], ["Razzaghi", "Talayeh", ""], ["Safro", "Ilya", ""]]}, {"id": "1611.05527", "submitter": "Tsubasa Ochiai", "authors": "Tsubasa Ochiai, Shigeki Matsuda, Hideyuki Watanabe, Shigeru Katagiri", "title": "Automatic Node Selection for Deep Neural Networks using Group Lasso\n  Regularization", "comments": "Submitted to ICASSP 2017", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We examine the effect of the Group Lasso (gLasso) regularizer in selecting\nthe salient nodes of Deep Neural Network (DNN) hidden layers by applying a\nDNN-HMM hybrid speech recognizer to TED Talks speech data. We test two types of\ngLasso regularization, one for outgoing weight vectors and another for incoming\nweight vectors, as well as two sizes of DNNs: 2048 hidden layer nodes and 4096\nnodes. Furthermore, we compare gLasso and L2 regularizers. Our experiment\nresults demonstrate that our DNN training, in which the gLasso regularizer was\nembedded, successfully selected the hidden layer nodes that are necessary and\nsufficient for achieving high classification power.\n", "versions": [{"version": "v1", "created": "Thu, 17 Nov 2016 01:43:01 GMT"}], "update_date": "2016-11-18", "authors_parsed": [["Ochiai", "Tsubasa", ""], ["Matsuda", "Shigeki", ""], ["Watanabe", "Hideyuki", ""], ["Katagiri", "Shigeru", ""]]}, {"id": "1611.05545", "submitter": "Justin Sirignano", "authors": "Justin Sirignano and Konstantinos Spiliopoulos", "title": "Stochastic Gradient Descent in Continuous Time", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.PR math.OC math.ST stat.ML stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Stochastic gradient descent in continuous time (SGDCT) provides a\ncomputationally efficient method for the statistical learning of\ncontinuous-time models, which are widely used in science, engineering, and\nfinance. The SGDCT algorithm follows a (noisy) descent direction along a\ncontinuous stream of data. SGDCT performs an online parameter update in\ncontinuous time, with the parameter updates $\\theta_t$ satisfying a stochastic\ndifferential equation. We prove that $\\lim_{t \\rightarrow \\infty} \\nabla \\bar\ng(\\theta_t) = 0$ where $\\bar g$ is a natural objective function for the\nestimation of the continuous-time dynamics. The convergence proof leverages\nergodicity by using an appropriate Poisson equation to help describe the\nevolution of the parameters for large times. SGDCT can also be used to solve\ncontinuous-time optimization problems, such as American options. For certain\ncontinuous-time problems, SGDCT has some promising advantages compared to a\ntraditional stochastic gradient descent algorithm. As an example application,\nSGDCT is combined with a deep neural network to price high-dimensional American\noptions (up to 100 dimensions).\n", "versions": [{"version": "v1", "created": "Thu, 17 Nov 2016 03:02:01 GMT"}, {"version": "v2", "created": "Sun, 27 Nov 2016 15:53:10 GMT"}, {"version": "v3", "created": "Mon, 17 Apr 2017 19:11:30 GMT"}, {"version": "v4", "created": "Sun, 29 Oct 2017 13:14:07 GMT"}], "update_date": "2017-10-31", "authors_parsed": [["Sirignano", "Justin", ""], ["Spiliopoulos", "Konstantinos", ""]]}, {"id": "1611.05559", "submitter": "Fangjian Guo", "authors": "Fangjian Guo, Xiangyu Wang, Kai Fan, Tamara Broderick and David B.\n  Dunson", "title": "Boosting Variational Inference", "comments": "17 pages, 7 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Variational inference (VI) provides fast approximations of a Bayesian\nposterior in part because it formulates posterior approximation as an\noptimization problem: to find the closest distribution to the exact posterior\nover some family of distributions. For practical reasons, the family of\ndistributions in VI is usually constrained so that it does not include the\nexact posterior, even as a limit point. Thus, no matter how long VI is run, the\nresulting approximation will not approach the exact posterior. We propose to\ninstead consider a more flexible approximating family consisting of all\npossible finite mixtures of a parametric base distribution (e.g., Gaussian).\nFor efficient inference, we borrow ideas from gradient boosting to develop an\nalgorithm we call boosting variational inference (BVI). BVI iteratively\nimproves the current approximation by mixing it with a new component from the\nbase distribution family and thereby yields progressively more accurate\nposterior approximations as more computing time is spent. Unlike a number of\ncommon VI variants including mean-field VI, BVI is able to capture\nmultimodality, general posterior covariance, and nonstandard posterior shapes.\n", "versions": [{"version": "v1", "created": "Thu, 17 Nov 2016 04:19:16 GMT"}, {"version": "v2", "created": "Wed, 1 Mar 2017 21:54:11 GMT"}], "update_date": "2017-03-03", "authors_parsed": [["Guo", "Fangjian", ""], ["Wang", "Xiangyu", ""], ["Fan", "Kai", ""], ["Broderick", "Tamara", ""], ["Dunson", "David B.", ""]]}, {"id": "1611.05722", "submitter": "Gilles Vandewiele", "authors": "Gilles Vandewiele, Olivier Janssens, Femke Ongenae, Filip De Turck,\n  Sofie Van Hoecke", "title": "GENESIM: genetic extraction of a single, interpretable model", "comments": "Presented at NIPS 2016 Workshop on Interpretable Machine Learning in\n  Complex Systems", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Models obtained by decision tree induction techniques excel in being\ninterpretable.However, they can be prone to overfitting, which results in a low\npredictive performance. Ensemble techniques are able to achieve a higher\naccuracy. However, this comes at a cost of losing interpretability of the\nresulting model. This makes ensemble techniques impractical in applications\nwhere decision support, instead of decision making, is crucial.\n  To bridge this gap, we present the GENESIM algorithm that transforms an\nensemble of decision trees to a single decision tree with an enhanced\npredictive performance by using a genetic algorithm. We compared GENESIM to\nprevalent decision tree induction and ensemble techniques using twelve publicly\navailable data sets. The results show that GENESIM achieves a better predictive\nperformance on most of these data sets than decision tree induction techniques\nand a predictive performance in the same order of magnitude as the ensemble\ntechniques. Moreover, the resulting model of GENESIM has a very low complexity,\nmaking it very interpretable, in contrast to ensemble techniques.\n", "versions": [{"version": "v1", "created": "Thu, 17 Nov 2016 14:58:35 GMT"}], "update_date": "2016-11-18", "authors_parsed": [["Vandewiele", "Gilles", ""], ["Janssens", "Olivier", ""], ["Ongenae", "Femke", ""], ["De Turck", "Filip", ""], ["Van Hoecke", "Sofie", ""]]}, {"id": "1611.05724", "submitter": "Stefano Paladino", "authors": "Stefano Paladino and Francesco Trov\\`o and Marcello Restelli and\n  Nicola Gatti", "title": "Unimodal Thompson Sampling for Graph-Structured Arms", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study, to the best of our knowledge, the first Bayesian algorithm for\nunimodal Multi-Armed Bandit (MAB) problems with graph structure. In this\nsetting, each arm corresponds to a node of a graph and each edge provides a\nrelationship, unknown to the learner, between two nodes in terms of expected\nreward. Furthermore, for any node of the graph there is a path leading to the\nunique node providing the maximum expected reward, along which the expected\nreward is monotonically increasing. Previous results on this setting describe\nthe behavior of frequentist MAB algorithms. In our paper, we design a Thompson\nSampling-based algorithm whose asymptotic pseudo-regret matches the lower bound\nfor the considered setting. We show that -as it happens in a wide number of\nscenarios- Bayesian MAB algorithms dramatically outperform frequentist ones. In\nparticular, we provide a thorough experimental evaluation of the performance of\nour and state-of-the-art algorithms as the properties of the graph vary.\n", "versions": [{"version": "v1", "created": "Thu, 17 Nov 2016 14:59:55 GMT"}, {"version": "v2", "created": "Tue, 22 Nov 2016 10:13:02 GMT"}], "update_date": "2016-11-23", "authors_parsed": [["Paladino", "Stefano", ""], ["Trov\u00f2", "Francesco", ""], ["Restelli", "Marcello", ""], ["Gatti", "Nicola", ""]]}, {"id": "1611.05751", "submitter": "Hamid Reza Hassanzadeh", "authors": "Hamid Reza Hassanzadeh, John H. Phan, May D. Wang", "title": "A Multi-Modal Graph-Based Semi-Supervised Pipeline for Predicting Cancer\n  Survival", "comments": "in 2016 IEEE International Conference on Bioinformatics and\n  Biomedicine (BIBM)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Cancer survival prediction is an active area of research that can help\nprevent unnecessary therapies and improve patient's quality of life. Gene\nexpression profiling is being widely used in cancer studies to discover\ninformative biomarkers that aid predict different clinical endpoint prediction.\nWe use multiple modalities of data derived from RNA deep-sequencing (RNA-seq)\nto predict survival of cancer patients. Despite the wealth of information\navailable in expression profiles of cancer tumors, fulfilling the\naforementioned objective remains a big challenge, for the most part, due to the\npaucity of data samples compared to the high dimension of the expression\nprofiles. As such, analysis of transcriptomic data modalities calls for\nstate-of-the-art big-data analytics techniques that can maximally use all the\navailable data to discover the relevant information hidden within a significant\namount of noise. In this paper, we propose a pipeline that predicts cancer\npatients' survival by exploiting the structure of the input (manifold learning)\nand by leveraging the unlabeled samples using Laplacian support vector\nmachines, a graph-based semi supervised learning (GSSL) paradigm. We show that\nunder certain circumstances, no single modality per se will result in the best\naccuracy and by fusing different models together via a stacked generalization\nstrategy, we may boost the accuracy synergistically. We apply our approach to\ntwo cancer datasets and present promising results. We maintain that a similar\npipeline can be used for predictive tasks where labeled samples are expensive\nto acquire.\n", "versions": [{"version": "v1", "created": "Thu, 17 Nov 2016 16:01:36 GMT"}], "update_date": "2016-11-18", "authors_parsed": [["Hassanzadeh", "Hamid Reza", ""], ["Phan", "John H.", ""], ["Wang", "May D.", ""]]}, {"id": "1611.05763", "submitter": "Jane Wang", "authors": "Jane X Wang, Zeb Kurth-Nelson, Dhruva Tirumala, Hubert Soyer, Joel Z\n  Leibo, Remi Munos, Charles Blundell, Dharshan Kumaran, Matt Botvinick", "title": "Learning to reinforcement learn", "comments": "17 pages, 7 figures, 1 table", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In recent years deep reinforcement learning (RL) systems have attained\nsuperhuman performance in a number of challenging task domains. However, a\nmajor limitation of such applications is their demand for massive amounts of\ntraining data. A critical present objective is thus to develop deep RL methods\nthat can adapt rapidly to new tasks. In the present work we introduce a novel\napproach to this challenge, which we refer to as deep meta-reinforcement\nlearning. Previous work has shown that recurrent networks can support\nmeta-learning in a fully supervised context. We extend this approach to the RL\nsetting. What emerges is a system that is trained using one RL algorithm, but\nwhose recurrent dynamics implement a second, quite separate RL procedure. This\nsecond, learned RL algorithm can differ from the original one in arbitrary\nways. Importantly, because it is learned, it is configured to exploit structure\nin the training domain. We unpack these points in a series of seven\nproof-of-concept experiments, each of which examines a key aspect of deep\nmeta-RL. We consider prospects for extending and scaling up the approach, and\nalso point out some potentially important implications for neuroscience.\n", "versions": [{"version": "v1", "created": "Thu, 17 Nov 2016 16:29:11 GMT"}, {"version": "v2", "created": "Thu, 24 Nov 2016 15:35:02 GMT"}, {"version": "v3", "created": "Mon, 23 Jan 2017 12:38:24 GMT"}], "update_date": "2017-01-24", "authors_parsed": [["Wang", "Jane X", ""], ["Kurth-Nelson", "Zeb", ""], ["Tirumala", "Dhruva", ""], ["Soyer", "Hubert", ""], ["Leibo", "Joel Z", ""], ["Munos", "Remi", ""], ["Blundell", "Charles", ""], ["Kumaran", "Dharshan", ""], ["Botvinick", "Matt", ""]]}, {"id": "1611.05778", "submitter": "Alessandro Bessi", "authors": "Alessandro Bessi", "title": "Towards the Modeling of Behavioral Trajectories of Users in Online\n  Social Media", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CY cs.SI physics.soc-ph stat.AP stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we introduce a methodology that allows to model behavioral\ntrajectories of users in online social media. First, we illustrate how to\nleverage the probabilistic framework provided by Hidden Markov Models (HMMs) to\nrepresent users by embedding the temporal sequences of actions they performed\nonline. We then derive a model-based distance between trained HMMs, and we use\nspectral clustering to find homogeneous clusters of users showing similar\nbehavioral trajectories. To provide platform-agnostic results, we apply the\nproposed approach to two different online social media --- i.e. Facebook and\nYouTube. We conclude discussing merits and limitations of our approach as well\nas future and promising research directions.\n", "versions": [{"version": "v1", "created": "Thu, 17 Nov 2016 16:53:50 GMT"}, {"version": "v2", "created": "Wed, 7 Dec 2016 21:34:40 GMT"}], "update_date": "2016-12-09", "authors_parsed": [["Bessi", "Alessandro", ""]]}, {"id": "1611.05780", "submitter": "Eugene Ndiaye", "authors": "Eugene Ndiaye, Olivier Fercoq, Alexandre Gramfort and Joseph Salmon", "title": "Gap Safe screening rules for sparsity enforcing penalties", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG math.OC stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In high dimensional regression settings, sparsity enforcing penalties have\nproved useful to regularize the data-fitting term. A recently introduced\ntechnique called screening rules propose to ignore some variables in the\noptimization leveraging the expected sparsity of the solutions and consequently\nleading to faster solvers. When the procedure is guaranteed not to discard\nvariables wrongly the rules are said to be safe. In this work, we propose a\nunifying framework for generalized linear models regularized with standard\nsparsity enforcing penalties such as $\\ell_1$ or $\\ell_1/\\ell_2$ norms. Our\ntechnique allows to discard safely more variables than previously considered\nsafe rules, particularly for low regularization parameters. Our proposed Gap\nSafe rules (so called because they rely on duality gap computation) can cope\nwith any iterative solver but are particularly well suited to (block)\ncoordinate descent methods. Applied to many standard learning tasks, Lasso,\nSparse-Group Lasso, multi-task Lasso, binary and multinomial logistic\nregression, etc., we report significant speed-ups compared to previously\nproposed safe rules on all tested data sets.\n", "versions": [{"version": "v1", "created": "Thu, 17 Nov 2016 16:55:12 GMT"}, {"version": "v2", "created": "Mon, 10 Apr 2017 16:38:35 GMT"}, {"version": "v3", "created": "Wed, 13 Sep 2017 08:05:19 GMT"}, {"version": "v4", "created": "Wed, 27 Dec 2017 17:26:38 GMT"}], "update_date": "2017-12-29", "authors_parsed": [["Ndiaye", "Eugene", ""], ["Fercoq", "Olivier", ""], ["Gramfort", "Alexandre", ""], ["Salmon", "Joseph", ""]]}, {"id": "1611.05817", "submitter": "Marco Tulio Ribeiro", "authors": "Marco Tulio Ribeiro, Sameer Singh, Carlos Guestrin", "title": "Nothing Else Matters: Model-Agnostic Explanations By Identifying\n  Prediction Invariance", "comments": "Presented at NIPS 2016 Workshop on Interpretable Machine Learning in\n  Complex Systems", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.AI cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  At the core of interpretable machine learning is the question of whether\nhumans are able to make accurate predictions about a model's behavior. Assumed\nin this question are three properties of the interpretable output: coverage,\nprecision, and effort. Coverage refers to how often humans think they can\npredict the model's behavior, precision to how accurate humans are in those\npredictions, and effort is either the up-front effort required in interpreting\nthe model, or the effort required to make predictions about a model's behavior.\n  In this work, we propose anchor-LIME (aLIME), a model-agnostic technique that\nproduces high-precision rule-based explanations for which the coverage\nboundaries are very clear. We compare aLIME to linear LIME with simulated\nexperiments, and demonstrate the flexibility of aLIME with qualitative examples\nfrom a variety of domains and tasks.\n", "versions": [{"version": "v1", "created": "Thu, 17 Nov 2016 19:07:00 GMT"}], "update_date": "2016-11-18", "authors_parsed": [["Ribeiro", "Marco Tulio", ""], ["Singh", "Sameer", ""], ["Guestrin", "Carlos", ""]]}, {"id": "1611.05923", "submitter": "Michael Wojnowicz", "authors": "Mike Wojnowicz, Ben Cruz, Xuan Zhao, Brian Wallace, Matt Wolff, Jay\n  Luan, and Caleb Crable", "title": "\"Influence Sketching\": Finding Influential Samples In Large-Scale\n  Regressions", "comments": "fixed additional typos", "journal-ref": "Big Data (Big Data), 2016 IEEE International Conference on, pp.\n  3601 - 3612. IEEE, 2016", "doi": "10.1109/BigData.2016.7841024", "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  There is an especially strong need in modern large-scale data analysis to\nprioritize samples for manual inspection. For example, the inspection could\ntarget important mislabeled samples or key vulnerabilities exploitable by an\nadversarial attack. In order to solve the \"needle in the haystack\" problem of\nwhich samples to inspect, we develop a new scalable version of Cook's distance,\na classical statistical technique for identifying samples which unusually\nstrongly impact the fit of a regression model (and its downstream predictions).\nIn order to scale this technique up to very large and high-dimensional\ndatasets, we introduce a new algorithm which we call \"influence sketching.\"\nInfluence sketching embeds random projections within the influence computation;\nin particular, the influence score is calculated using the randomly projected\npseudo-dataset from the post-convergence Generalized Linear Model (GLM). We\nvalidate that influence sketching can reliably and successfully discover\ninfluential samples by applying the technique to a malware detection dataset of\nover 2 million executable files, each represented with almost 100,000 features.\nFor example, we find that randomly deleting approximately 10% of training\nsamples reduces predictive accuracy only slightly from 99.47% to 99.45%,\nwhereas deleting the same number of samples with high influence sketch scores\nreduces predictive accuracy all the way down to 90.24%. Moreover, we find that\ninfluential samples are especially likely to be mislabeled. In the case study,\nwe manually inspect the most influential samples, and find that influence\nsketching pointed us to new, previously unidentified pieces of malware.\n", "versions": [{"version": "v1", "created": "Thu, 17 Nov 2016 22:23:08 GMT"}, {"version": "v2", "created": "Fri, 30 Dec 2016 20:15:16 GMT"}, {"version": "v3", "created": "Thu, 23 Mar 2017 05:55:24 GMT"}], "update_date": "2017-05-11", "authors_parsed": [["Wojnowicz", "Mike", ""], ["Cruz", "Ben", ""], ["Zhao", "Xuan", ""], ["Wallace", "Brian", ""], ["Wolff", "Matt", ""], ["Luan", "Jay", ""], ["Crable", "Caleb", ""]]}, {"id": "1611.05934", "submitter": "Viktoriya Krakovna", "authors": "Viktoriya Krakovna and Finale Doshi-Velez", "title": "Increasing the Interpretability of Recurrent Neural Networks Using\n  Hidden Markov Models", "comments": "Presented at NIPS 2016 Workshop on Interpretable Machine Learning in\n  Complex Systems. arXiv admin note: substantial text overlap with\n  arXiv:1606.05320", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  As deep neural networks continue to revolutionize various application\ndomains, there is increasing interest in making these powerful models more\nunderstandable and interpretable, and narrowing down the causes of good and bad\npredictions. We focus on recurrent neural networks, state of the art models in\nspeech recognition and translation. Our approach to increasing interpretability\nis by combining a long short-term memory (LSTM) model with a hidden Markov\nmodel (HMM), a simpler and more transparent model. We add the HMM state\nprobabilities to the output layer of the LSTM, and then train the HMM and LSTM\neither sequentially or jointly. The LSTM can make use of the information from\nthe HMM, and fill in the gaps when the HMM is not performing well. A small\nhybrid model usually performs better than a standalone LSTM of the same size,\nespecially on smaller data sets. We test the algorithms on text data and\nmedical time series data, and find that the LSTM and HMM learn complementary\ninformation about the features in the text.\n", "versions": [{"version": "v1", "created": "Fri, 18 Nov 2016 00:13:32 GMT"}], "update_date": "2016-11-21", "authors_parsed": [["Krakovna", "Viktoriya", ""], ["Doshi-Velez", "Finale", ""]]}, {"id": "1611.05940", "submitter": "Satoshi Hara", "authors": "Satoshi Hara, Takanori Maehara", "title": "Finding Alternate Features in Lasso", "comments": "Presented at NIPS 2016 Workshop on Interpretable Machine Learning in\n  Complex Systems", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a method for finding alternate features missing in the Lasso\noptimal solution. In ordinary Lasso problem, one global optimum is obtained and\nthe resulting features are interpreted as task-relevant features. However, this\ncan overlook possibly relevant features not selected by the Lasso. With the\nproposed method, we can provide not only the Lasso optimal solution but also\npossible alternate features to the Lasso solution. We show that such alternate\nfeatures can be computed efficiently by avoiding redundant computations. We\nalso demonstrate how the proposed method works in the 20 newsgroup data, which\nshows that reasonable features are found as alternate features.\n", "versions": [{"version": "v1", "created": "Fri, 18 Nov 2016 01:11:34 GMT"}, {"version": "v2", "created": "Thu, 24 Nov 2016 03:39:11 GMT"}], "update_date": "2016-11-28", "authors_parsed": [["Hara", "Satoshi", ""], ["Maehara", "Takanori", ""]]}, {"id": "1611.05977", "submitter": "Mostafa Rahmani", "authors": "Mostafa Rahmani, George Atia", "title": "Robust and Scalable Column/Row Sampling from Corrupted Big Data", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.NA stat.AP stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Conventional sampling techniques fall short of drawing descriptive sketches\nof the data when the data is grossly corrupted as such corruptions break the\nlow rank structure required for them to perform satisfactorily. In this paper,\nwe present new sampling algorithms which can locate the informative columns in\npresence of severe data corruptions. In addition, we develop new scalable\nrandomized designs of the proposed algorithms. The proposed approach is\nsimultaneously robust to sparse corruption and outliers and substantially\noutperforms the state-of-the-art robust sampling algorithms as demonstrated by\nexperiments conducted using both real and synthetic data.\n", "versions": [{"version": "v1", "created": "Fri, 18 Nov 2016 05:07:21 GMT"}], "update_date": "2016-11-21", "authors_parsed": [["Rahmani", "Mostafa", ""], ["Atia", "George", ""]]}, {"id": "1611.06066", "submitter": "Alexandre Abraham", "authors": "Alexandre Abraham (NEUROSPIN, PARIETAL), Michael Milham (NKI), Adriana\n  Di Martino, R. Cameron Craddock (NKI), Dimitris Samaras (SUNY), Bertrand\n  Thirion (PARIETAL, NEUROSPIN), Ga\\\"el Varoquaux (PARIETAL, NEUROSPIN)", "title": "Deriving reproducible biomarkers from multi-site resting-state data: An\n  Autism-based example", "comments": "in NeuroImage, Elsevier, 2016", "journal-ref": null, "doi": "10.1016/j.neuroimage.2016.10.045", "report-no": null, "categories": "stat.ML q-bio.NC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Resting-state functional Magnetic Resonance Imaging (R-fMRI) holds the\npromise to reveal functional biomarkers of neuropsychiatric disorders. However,\nextracting such biomarkers is challenging for complex multi-faceted\nneuropatholo-gies, such as autism spectrum disorders. Large multi-site datasets\nincrease sample sizes to compensate for this complexity, at the cost of\nuncontrolled heterogeneity. This heterogeneity raises new challenges, akin to\nthose face in realistic diagnostic applications. Here, we demonstrate the\nfeasibility of inter-site classification of neuropsychiatric status, with an\napplication to the Autism Brain Imaging Data Exchange (ABIDE) database, a large\n(N=871) multi-site autism dataset. For this purpose, we investigate pipelines\nthat extract the most predictive biomarkers from the data. These R-fMRI\npipelines build participant-specific connectomes from functionally-defined\nbrain areas. Connectomes are then compared across participants to learn\npatterns of connectivity that differentiate typical controls from individuals\nwith autism. We predict this neuropsychiatric status for participants from the\nsame acquisition sites or different, unseen, ones. Good choices of methods for\nthe various steps of the pipeline lead to 67% prediction accuracy on the full\nABIDE data, which is significantly better than previously reported results. We\nperform extensive validation on multiple subsets of the data defined by\ndifferent inclusion criteria. These enables detailed analysis of the factors\ncontributing to successful connectome-based prediction. First, prediction\naccuracy improves as we include more subjects, up to the maximum amount of\nsubjects available. Second, the definition of functional brain areas is of\nparamount importance for biomarker discovery: brain areas extracted from large\nR-fMRI datasets outperform reference atlases in the classification tasks.\n", "versions": [{"version": "v1", "created": "Fri, 18 Nov 2016 13:31:47 GMT"}], "update_date": "2016-11-21", "authors_parsed": [["Abraham", "Alexandre", "", "NEUROSPIN, PARIETAL"], ["Milham", "Michael", "", "NKI"], ["Di Martino", "Adriana", "", "NKI"], ["Craddock", "R. Cameron", "", "NKI"], ["Samaras", "Dimitris", "", "SUNY"], ["Thirion", "Bertrand", "", "PARIETAL, NEUROSPIN"], ["Varoquaux", "Ga\u00ebl", "", "PARIETAL, NEUROSPIN"]]}, {"id": "1611.06080", "submitter": "Kian Hsiang Low", "authors": "Quang Minh Hoang, Trong Nghia Hoang, Kian Hsiang Low", "title": "A Generalized Stochastic Variational Bayesian Hyperparameter Learning\n  Framework for Sparse Spectrum Gaussian Process Regression", "comments": "31st AAAI Conference on Artificial Intelligence (AAAI 2017), Extended\n  version with proofs, 11 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  While much research effort has been dedicated to scaling up sparse Gaussian\nprocess (GP) models based on inducing variables for big data, little attention\nis afforded to the other less explored class of low-rank GP approximations that\nexploit the sparse spectral representation of a GP kernel. This paper presents\nsuch an effort to advance the state of the art of sparse spectrum GP models to\nachieve competitive predictive performance for massive datasets. Our\ngeneralized framework of stochastic variational Bayesian sparse spectrum GP\n(sVBSSGP) models addresses their shortcomings by adopting a Bayesian treatment\nof the spectral frequencies to avoid overfitting, modeling these frequencies\njointly in its variational distribution to enable their interaction a\nposteriori, and exploiting local data for boosting the predictive performance.\nHowever, such structural improvements result in a variational lower bound that\nis intractable to be optimized. To resolve this, we exploit a variational\nparameterization trick to make it amenable to stochastic optimization.\nInterestingly, the resulting stochastic gradient has a linearly decomposable\nstructure that can be exploited to refine our stochastic optimization method to\nincur constant time per iteration while preserving its property of being an\nunbiased estimator of the exact gradient of the variational lower bound.\nEmpirical evaluation on real-world datasets shows that sVBSSGP outperforms\nstate-of-the-art stochastic implementations of sparse GP models.\n", "versions": [{"version": "v1", "created": "Fri, 18 Nov 2016 14:00:48 GMT"}], "update_date": "2016-11-21", "authors_parsed": [["Hoang", "Quang Minh", ""], ["Hoang", "Trong Nghia", ""], ["Low", "Kian Hsiang", ""]]}, {"id": "1611.06094", "submitter": "Martin Stoll", "authors": "Jessica Bosch, Steffen Klamt, Martin Stoll", "title": "Generalizing diffuse interface methods on graphs: non-smooth potentials\n  and hypergraphs", "comments": "13 pages, 11 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML math.NA", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Diffuse interface methods have recently been introduced for the task of\nsemi-supervised learning. The underlying model is well-known in materials\nscience but was extended to graphs using a Ginzburg--Landau functional and the\ngraph Laplacian. We here generalize the previously proposed model by a\nnon-smooth potential function. Additionally, we show that the diffuse interface\nmethod can be used for the segmentation of data coming from hypergraphs. For\nthis we show that the graph Laplacian in almost all cases is derived from\nhypergraph information. Additionally, we show that the formerly introduced\nhypergraph Laplacian coming from a relaxed optimization problem is well suited\nto be used within the diffuse interface method. We present computational\nexperiments for graph and hypergraph Laplacians.\n", "versions": [{"version": "v1", "created": "Fri, 18 Nov 2016 14:23:10 GMT"}], "update_date": "2016-11-21", "authors_parsed": [["Bosch", "Jessica", ""], ["Klamt", "Steffen", ""], ["Stoll", "Martin", ""]]}, {"id": "1611.06132", "submitter": "Pavel Izmailov", "authors": "Pavel Izmailov and Dmitry Kropotov", "title": "Faster variational inducing input Gaussian process classification", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Gaussian processes (GP) provide a prior over functions and allow finding\ncomplex regularities in data. Gaussian processes are successfully used for\nclassification/regression problems and dimensionality reduction. In this work\nwe consider the classification problem only. The complexity of standard methods\nfor GP-classification scales cubically with the size of the training dataset.\nThis complexity makes them inapplicable to big data problems. Therefore, a\nvariety of methods were introduced to overcome this limitation. In the paper we\nfocus on methods based on so called inducing inputs. This approach is based on\nvariational inference and proposes a particular lower bound for marginal\nlikelihood (evidence). This bound is then maximized w.r.t. parameters of kernel\nfunction of the Gaussian process, thus fitting the model to data. The\ncomputational complexity of this method is $O(nm^2)$, where $m$ is the number\nof inducing inputs used by the model and is assumed to be substantially smaller\nthan the size of the dataset $n$. Recently, a new evidence lower bound for\nGP-classification problem was introduced. It allows using stochastic\noptimization, which makes it suitable for big data problems. However, the new\nlower bound depends on $O(m^2)$ variational parameter, which makes optimization\nchallenging in case of big m. In this work we develop a new approach for\ntraining inducing input GP models for classification problems. Here we use\nquadratic approximation of several terms in the aforementioned evidence lower\nbound, obtaining analytical expressions for optimal values of most of the\nparameters in the optimization, thus sufficiently reducing the dimension of\noptimization space. In our experiments we achieve as well or better results,\ncompared to the existing method. Moreover, our method doesn't require the user\nto manually set the learning rate, making it more practical, than the existing\nmethod.\n", "versions": [{"version": "v1", "created": "Fri, 18 Nov 2016 15:53:50 GMT"}], "update_date": "2016-11-21", "authors_parsed": [["Izmailov", "Pavel", ""], ["Kropotov", "Dmitry", ""]]}, {"id": "1611.06148", "submitter": "Yotaro Kubo", "authors": "Yotaro Kubo, George Tucker, Simon Wiesler", "title": "Compacting Neural Network Classifiers via Dropout Training", "comments": "Submitted to AISTATS 2017 (Short-version is accepted to NIPS Workshop\n  on Efficient Methods for Deep Neural Networks)", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We introduce dropout compaction, a novel method for training feed-forward\nneural networks which realizes the performance gains of training a large model\nwith dropout regularization, yet extracts a compact neural network for run-time\nefficiency. In the proposed method, we introduce a sparsity-inducing prior on\nthe per unit dropout retention probability so that the optimizer can\neffectively prune hidden units during training. By changing the prior\nhyperparameters, we can control the size of the resulting network. We performed\na systematic comparison of dropout compaction and competing methods on several\nreal-world speech recognition tasks and found that dropout compaction achieved\ncomparable accuracy with fewer than 50% of the hidden units, translating to a\n2.5x speedup in run-time.\n", "versions": [{"version": "v1", "created": "Fri, 18 Nov 2016 16:20:41 GMT"}, {"version": "v2", "created": "Wed, 24 May 2017 12:26:43 GMT"}], "update_date": "2017-05-25", "authors_parsed": [["Kubo", "Yotaro", ""], ["Tucker", "George", ""], ["Wiesler", "Simon", ""]]}, {"id": "1611.06172", "submitter": "Shihao Ji", "authors": "Shihao Ji, Nadathur Satish, Sheng Li, and Pradeep Dubey", "title": "Parallelizing Word2Vec in Multi-Core and Many-Core Architectures", "comments": "NIPS Workshop on Efficient Methods for Deep Neural Networks (2016)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Word2vec is a widely used algorithm for extracting low-dimensional vector\nrepresentations of words. State-of-the-art algorithms including those by\nMikolov et al. have been parallelized for multi-core CPU architectures, but are\nbased on vector-vector operations with \"Hogwild\" updates that are\nmemory-bandwidth intensive and do not efficiently use computational resources.\nIn this paper, we propose \"HogBatch\" by improving reuse of various data\nstructures in the algorithm through the use of minibatching and negative sample\nsharing, hence allowing us to express the problem using matrix multiply\noperations. We also explore different techniques to distribute word2vec\ncomputation across nodes in a compute cluster, and demonstrate good strong\nscalability up to 32 nodes. The new algorithm is particularly suitable for\nmodern multi-core/many-core architectures, especially Intel's latest Knights\nLanding processors, and allows us to scale up the computation near linearly\nacross cores and nodes, and process hundreds of millions of words per second,\nwhich is the fastest word2vec implementation to the best of our knowledge.\n", "versions": [{"version": "v1", "created": "Fri, 18 Nov 2016 17:47:44 GMT"}, {"version": "v2", "created": "Fri, 23 Dec 2016 19:20:25 GMT"}], "update_date": "2016-12-26", "authors_parsed": [["Ji", "Shihao", ""], ["Satish", "Nadathur", ""], ["Li", "Sheng", ""], ["Dubey", "Pradeep", ""]]}, {"id": "1611.06175", "submitter": "Adrien Bibal", "authors": "Adrien Bibal and Benoit Fr\\'enay", "title": "Learning Interpretability for Visualizations using Adapted Cox Models\n  through a User Experiment", "comments": "Presented at NIPS 2016 Workshop on Interpretable Machine Learning in\n  Complex Systems", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.AI cs.HC cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In order to be useful, visualizations need to be interpretable. This paper\nuses a user-based approach to combine and assess quality measures in order to\nbetter model user preferences. Results show that cluster separability measures\nare outperformed by a neighborhood conservation measure, even though the former\nare usually considered as intuitively representative of user motives. Moreover,\ncombining measures, as opposed to using a single measure, further improves\nprediction performances.\n", "versions": [{"version": "v1", "created": "Fri, 18 Nov 2016 17:52:23 GMT"}], "update_date": "2016-11-21", "authors_parsed": [["Bibal", "Adrien", ""], ["Fr\u00e9nay", "Benoit", ""]]}, {"id": "1611.06188", "submitter": "Yacine Jernite", "authors": "Yacine Jernite, Edouard Grave, Armand Joulin, Tomas Mikolov", "title": "Variable Computation in Recurrent Neural Networks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.AI cs.CL cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recurrent neural networks (RNNs) have been used extensively and with\nincreasing success to model various types of sequential data. Much of this\nprogress has been achieved through devising recurrent units and architectures\nwith the flexibility to capture complex statistics in the data, such as long\nrange dependency or localized attention phenomena. However, while many\nsequential data (such as video, speech or language) can have highly variable\ninformation flow, most recurrent models still consume input features at a\nconstant rate and perform a constant number of computations per time step,\nwhich can be detrimental to both speed and model capacity. In this paper, we\nexplore a modification to existing recurrent units which allows them to learn\nto vary the amount of computation they perform at each step, without prior\nknowledge of the sequence's time structure. We show experimentally that not\nonly do our models require fewer operations, they also lead to better\nperformance overall on evaluation tasks.\n", "versions": [{"version": "v1", "created": "Fri, 18 Nov 2016 18:13:46 GMT"}, {"version": "v2", "created": "Thu, 2 Mar 2017 19:47:59 GMT"}], "update_date": "2017-03-06", "authors_parsed": [["Jernite", "Yacine", ""], ["Grave", "Edouard", ""], ["Joulin", "Armand", ""], ["Mikolov", "Tomas", ""]]}, {"id": "1611.06194", "submitter": "Rahaf Aljundi", "authors": "Rahaf Aljundi, Punarjay Chakravarty and Tinne Tuytelaars", "title": "Expert Gate: Lifelong Learning with a Network of Experts", "comments": "CVPR 2017 paper", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper we introduce a model of lifelong learning, based on a Network\nof Experts. New tasks / experts are learned and added to the model\nsequentially, building on what was learned before. To ensure scalability of\nthis process,data from previous tasks cannot be stored and hence is not\navailable when learning a new task. A critical issue in such context, not\naddressed in the literature so far, relates to the decision which expert to\ndeploy at test time. We introduce a set of gating autoencoders that learn a\nrepresentation for the task at hand, and, at test time, automatically forward\nthe test sample to the relevant expert. This also brings memory efficiency as\nonly one expert network has to be loaded into memory at any given time.\nFurther, the autoencoders inherently capture the relatedness of one task to\nanother, based on which the most relevant prior model to be used for training a\nnew expert, with finetuning or learning without-forgetting, can be selected. We\nevaluate our method on image classification and video prediction problems.\n", "versions": [{"version": "v1", "created": "Fri, 18 Nov 2016 18:50:15 GMT"}, {"version": "v2", "created": "Wed, 19 Apr 2017 09:25:55 GMT"}], "update_date": "2017-04-20", "authors_parsed": [["Aljundi", "Rahaf", ""], ["Chakravarty", "Punarjay", ""], ["Tuytelaars", "Tinne", ""]]}, {"id": "1611.06213", "submitter": "Wei Zhang", "authors": "Wei Zhang, Minwei Feng, Yunhui Zheng, Yufei Ren, Yandong Wang, Ji Liu,\n  Peng Liu, Bing Xiang, Li Zhang, Bowen Zhou, Fei Wang", "title": "GaDei: On Scale-up Training As A Service For Deep Learning", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.DC cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Deep learning (DL) training-as-a-service (TaaS) is an important emerging\nindustrial workload. The unique challenge of TaaS is that it must satisfy a\nwide range of customers who have no experience and resources to tune DL\nhyper-parameters, and meticulous tuning for each user's dataset is\nprohibitively expensive. Therefore, TaaS hyper-parameters must be fixed with\nvalues that are applicable to all users. IBM Watson Natural Language Classifier\n(NLC) service, the most popular IBM cognitive service used by thousands of\nenterprise-level clients around the globe, is a typical TaaS service. By\nevaluating the NLC workloads, we show that only the conservative\nhyper-parameter setup (e.g., small mini-batch size and small learning rate) can\nguarantee acceptable model accuracy for a wide range of customers. We further\njustify theoretically why such a setup guarantees better model convergence in\ngeneral. Unfortunately, the small mini-batch size causes a high volume of\ncommunication traffic in a parameter-server based system. We characterize the\nhigh communication bandwidth requirement of TaaS using representative\nindustrial deep learning workloads and demonstrate that none of the\nstate-of-the-art scale-up or scale-out solutions can satisfy such a\nrequirement. We then present GaDei, an optimized shared-memory based scale-up\nparameter server design. We prove that the designed protocol is deadlock-free\nand it processes each gradient exactly once. Our implementation is evaluated on\nboth commercial benchmarks and public benchmarks to demonstrate that it\nsignificantly outperforms the state-of-the-art parameter-server based\nimplementation while maintaining the required accuracy and our implementation\nreaches near the best possible runtime performance, constrained only by the\nhardware limitation. Furthermore, to the best of our knowledge, GaDei is the\nonly scale-up DL system that provides fault-tolerance.\n", "versions": [{"version": "v1", "created": "Fri, 18 Nov 2016 20:06:27 GMT"}, {"version": "v2", "created": "Tue, 3 Oct 2017 20:30:19 GMT"}], "update_date": "2017-10-05", "authors_parsed": [["Zhang", "Wei", ""], ["Feng", "Minwei", ""], ["Zheng", "Yunhui", ""], ["Ren", "Yufei", ""], ["Wang", "Yandong", ""], ["Liu", "Ji", ""], ["Liu", "Peng", ""], ["Xiang", "Bing", ""], ["Zhang", "Li", ""], ["Zhou", "Bowen", ""], ["Wang", "Fei", ""]]}, {"id": "1611.06245", "submitter": "Anders S{\\o}gaard Anders S{\\o}gaard", "authors": "Anders S{\\o}gaard", "title": "Spikes as regularizers", "comments": "Computing with Spikes at NIPS 2016", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NE cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a confidence-based single-layer feed-forward learning algorithm\nSPIRAL (Spike Regularized Adaptive Learning) relying on an encoding of\nactivation spikes. We adaptively update a weight vector relying on confidence\nestimates and activation offsets relative to previous activity. We regularize\nupdates proportionally to item-level confidence and weight-specific support,\nloosely inspired by the observation from neurophysiology that high spike rates\nare sometimes accompanied by low temporal precision. Our experiments suggest\nthat the new learning algorithm SPIRAL is more robust and less prone to\noverfitting than both the averaged perceptron and AROW.\n", "versions": [{"version": "v1", "created": "Fri, 18 Nov 2016 21:09:16 GMT"}], "update_date": "2016-11-23", "authors_parsed": [["S\u00f8gaard", "Anders", ""]]}, {"id": "1611.06265", "submitter": "Yi Luo", "authors": "Yi Luo, Zhuo Chen, John R. Hershey, Jonathan Le Roux, Nima Mesgarani", "title": "Deep Clustering and Conventional Networks for Music Separation: Stronger\n  Together", "comments": "Published in ICASSP 2017", "journal-ref": null, "doi": "10.1109/ICASSP.2017.7952118", "report-no": null, "categories": "stat.ML cs.LG cs.SD", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Deep clustering is the first method to handle general audio separation\nscenarios with multiple sources of the same type and an arbitrary number of\nsources, performing impressively in speaker-independent speech separation\ntasks. However, little is known about its effectiveness in other challenging\nsituations such as music source separation. Contrary to conventional networks\nthat directly estimate the source signals, deep clustering generates an\nembedding for each time-frequency bin, and separates sources by clustering the\nbins in the embedding space. We show that deep clustering outperforms\nconventional networks on a singing voice separation task, in both matched and\nmismatched conditions, even though conventional networks have the advantage of\nend-to-end training for best signal approximation, presumably because its more\nflexible objective engenders better regularization. Since the strengths of deep\nclustering and conventional network architectures appear complementary, we\nexplore combining them in a single hybrid network trained via an approach akin\nto multi-task learning. Remarkably, the combination significantly outperforms\neither of its components.\n", "versions": [{"version": "v1", "created": "Fri, 18 Nov 2016 22:33:05 GMT"}, {"version": "v2", "created": "Thu, 15 Jun 2017 16:23:58 GMT"}], "update_date": "2017-11-30", "authors_parsed": [["Luo", "Yi", ""], ["Chen", "Zhuo", ""], ["Hershey", "John R.", ""], ["Roux", "Jonathan Le", ""], ["Mesgarani", "Nima", ""]]}, {"id": "1611.06310", "submitter": "Razvan Pascanu", "authors": "Grzegorz Swirszcz, Wojciech Marian Czarnecki and Razvan Pascanu", "title": "Local minima in training of neural networks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  There has been a lot of recent interest in trying to characterize the error\nsurface of deep models. This stems from a long standing question. Given that\ndeep networks are highly nonlinear systems optimized by local gradient methods,\nwhy do they not seem to be affected by bad local minima? It is widely believed\nthat training of deep models using gradient methods works so well because the\nerror surface either has no local minima, or if they exist they need to be\nclose in value to the global minimum. It is known that such results hold under\nvery strong assumptions which are not satisfied by real models. In this paper\nwe present examples showing that for such theorem to be true additional\nassumptions on the data, initialization schemes and/or the model classes have\nto be made. We look at the particular case of finite size datasets. We\ndemonstrate that in this scenario one can construct counter-examples (datasets\nor initialization schemes) when the network does become susceptible to bad\nlocal minima over the weight space.\n", "versions": [{"version": "v1", "created": "Sat, 19 Nov 2016 05:49:22 GMT"}, {"version": "v2", "created": "Fri, 17 Feb 2017 14:51:54 GMT"}], "update_date": "2017-02-20", "authors_parsed": [["Swirszcz", "Grzegorz", ""], ["Czarnecki", "Wojciech Marian", ""], ["Pascanu", "Razvan", ""]]}, {"id": "1611.06314", "submitter": "Georgios Giasemidis Dr", "authors": "Georgios Giasemidis, Colin Singleton, Ioannis Agrafiotis, Jason R.C.\n  Nurse, Alan Pilgrim, Chris Willis, Danica Vukadinovic Greetham", "title": "Determining the Veracity of Rumours on Twitter", "comments": "21 pages, 6 figures, 2 tables", "journal-ref": "SocInfo 2016, Part I, LNCS 10046, pp. 185-205, 2016", "doi": "10.1007/978-3-319-47880-7_12", "report-no": null, "categories": "cs.SI stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  While social networks can provide an ideal platform for up-to-date\ninformation from individuals across the world, it has also proved to be a place\nwhere rumours fester and accidental or deliberate misinformation often emerges.\nIn this article, we aim to support the task of making sense from social media\ndata, and specifically, seek to build an autonomous message-classifier that\nfilters relevant and trustworthy information from Twitter. For our work, we\ncollected about 100 million public tweets, including users' past tweets, from\nwhich we identified 72 rumours (41 true, 31 false). We considered over 80\ntrustworthiness measures including the authors' profile and past behaviour, the\nsocial network connections (graphs), and the content of tweets themselves. We\nran modern machine-learning classifiers over those measures to produce\ntrustworthiness scores at various time windows from the outbreak of the rumour.\nSuch time-windows were key as they allowed useful insight into the progression\nof the rumours. From our findings, we identified that our model was\nsignificantly more accurate than similar studies in the literature. We also\nidentified critical attributes of the data that give rise to the\ntrustworthiness scores assigned. Finally we developed a software demonstration\nthat provides a visual user interface to allow the user to examine the\nanalysis.\n", "versions": [{"version": "v1", "created": "Sat, 19 Nov 2016 06:22:50 GMT"}], "update_date": "2016-11-22", "authors_parsed": [["Giasemidis", "Georgios", ""], ["Singleton", "Colin", ""], ["Agrafiotis", "Ioannis", ""], ["Nurse", "Jason R. C.", ""], ["Pilgrim", "Alan", ""], ["Willis", "Chris", ""], ["Greetham", "Danica Vukadinovic", ""]]}, {"id": "1611.06426", "submitter": "Abbas Kazerouni", "authors": "Abbas Kazerouni, Mohammad Ghavamzadeh, Yasin Abbasi-Yadkori and\n  Benjamin Van Roy", "title": "Conservative Contextual Linear Bandits", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Safety is a desirable property that can immensely increase the applicability\nof learning algorithms in real-world decision-making problems. It is much\neasier for a company to deploy an algorithm that is safe, i.e., guaranteed to\nperform at least as well as a baseline. In this paper, we study the issue of\nsafety in contextual linear bandits that have application in many different\nfields including personalized ad recommendation in online marketing. We\nformulate a notion of safety for this class of algorithms. We develop a safe\ncontextual linear bandit algorithm, called conservative linear UCB (CLUCB),\nthat simultaneously minimizes its regret and satisfies the safety constraint,\ni.e., maintains its performance above a fixed percentage of the performance of\na baseline strategy, uniformly over time. We prove an upper-bound on the regret\nof CLUCB and show that it can be decomposed into two terms: 1) an upper-bound\nfor the regret of the standard linear UCB algorithm that grows with the time\nhorizon and 2) a constant (does not grow with the time horizon) term that\naccounts for the loss of being conservative in order to satisfy the safety\nconstraint. We empirically show that our algorithm is safe and validate our\ntheoretical analysis.\n", "versions": [{"version": "v1", "created": "Sat, 19 Nov 2016 20:36:30 GMT"}, {"version": "v2", "created": "Sat, 4 Mar 2017 01:28:26 GMT"}], "update_date": "2017-03-07", "authors_parsed": [["Kazerouni", "Abbas", ""], ["Ghavamzadeh", "Mohammad", ""], ["Abbasi-Yadkori", "Yasin", ""], ["Van Roy", "Benjamin", ""]]}, {"id": "1611.06440", "submitter": "Pavlo Molchanov", "authors": "Pavlo Molchanov, Stephen Tyree, Tero Karras, Timo Aila, Jan Kautz", "title": "Pruning Convolutional Neural Networks for Resource Efficient Inference", "comments": "17 pages, 14 figures, ICLR 2017 paper", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a new formulation for pruning convolutional kernels in neural\nnetworks to enable efficient inference. We interleave greedy criteria-based\npruning with fine-tuning by backpropagation - a computationally efficient\nprocedure that maintains good generalization in the pruned network. We propose\na new criterion based on Taylor expansion that approximates the change in the\ncost function induced by pruning network parameters. We focus on transfer\nlearning, where large pretrained networks are adapted to specialized tasks. The\nproposed criterion demonstrates superior performance compared to other\ncriteria, e.g. the norm of kernel weights or feature map activation, for\npruning large CNNs after adaptation to fine-grained classification tasks\n(Birds-200 and Flowers-102) relaying only on the first order gradient\ninformation. We also show that pruning can lead to more than 10x theoretical\n(5x practical) reduction in adapted 3D-convolutional filters with a small drop\nin accuracy in a recurrent gesture classifier. Finally, we show results for the\nlarge-scale ImageNet dataset to emphasize the flexibility of our approach.\n", "versions": [{"version": "v1", "created": "Sat, 19 Nov 2016 22:48:30 GMT"}, {"version": "v2", "created": "Thu, 8 Jun 2017 19:53:26 GMT"}], "update_date": "2017-06-12", "authors_parsed": [["Molchanov", "Pavlo", ""], ["Tyree", "Stephen", ""], ["Karras", "Tero", ""], ["Aila", "Timo", ""], ["Kautz", "Jan", ""]]}, {"id": "1611.06455", "submitter": "Zhiguang Wang", "authors": "Zhiguang Wang, Weizhong Yan, Tim Oates", "title": "Time Series Classification from Scratch with Deep Neural Networks: A\n  Strong Baseline", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.NE stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a simple but strong baseline for time series classification from\nscratch with deep neural networks. Our proposed baseline models are pure\nend-to-end without any heavy preprocessing on the raw data or feature crafting.\nThe proposed Fully Convolutional Network (FCN) achieves premium performance to\nother state-of-the-art approaches and our exploration of the very deep neural\nnetworks with the ResNet structure is also competitive. The global average\npooling in our convolutional model enables the exploitation of the Class\nActivation Map (CAM) to find out the contributing region in the raw data for\nthe specific labels. Our models provides a simple choice for the real world\napplication and a good starting point for the future research. An overall\nanalysis is provided to discuss the generalization capability of our models,\nlearned features, network structures and the classification semantics.\n", "versions": [{"version": "v1", "created": "Sun, 20 Nov 2016 00:34:09 GMT"}, {"version": "v2", "created": "Tue, 22 Nov 2016 02:32:09 GMT"}, {"version": "v3", "created": "Wed, 30 Nov 2016 06:39:49 GMT"}, {"version": "v4", "created": "Wed, 14 Dec 2016 06:58:08 GMT"}], "update_date": "2016-12-15", "authors_parsed": [["Wang", "Zhiguang", ""], ["Yan", "Weizhong", ""], ["Oates", "Tim", ""]]}, {"id": "1611.06475", "submitter": "Vitaly Feldman", "authors": "Vitaly Feldman", "title": "Dealing with Range Anxiety in Mean Estimation via Statistical Queries", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We give algorithms for estimating the expectation of a given real-valued\nfunction $\\phi:X\\to {\\bf R}$ on a sample drawn randomly from some unknown\ndistribution $D$ over domain $X$, namely ${\\bf E}_{{\\bf x}\\sim D}[\\phi({\\bf\nx})]$. Our algorithms work in two well-studied models of restricted access to\ndata samples. The first one is the statistical query (SQ) model in which an\nalgorithm has access to an SQ oracle for the input distribution $D$ over $X$\ninstead of i.i.d. samples from $D$. Given a query function $\\phi:X \\to [0,1]$,\nthe oracle returns an estimate of ${\\bf E}_{{\\bf x}\\sim D}[\\phi({\\bf x})]$\nwithin some tolerance $\\tau$. The second, is a model in which only a single bit\nis communicated from each sample. In both of these models the error obtained\nusing a naive implementation would scale polynomially with the range of the\nrandom variable $\\phi({\\bf x})$ (which might even be infinite). In contrast,\nwithout restrictions on access to data the expected error scales with the\nstandard deviation of $\\phi({\\bf x})$. Here we give a simple algorithm whose\nerror scales linearly in standard deviation of $\\phi({\\bf x})$ and\nlogarithmically with an upper bound on the second moment of $\\phi({\\bf x})$.\n  As corollaries, we obtain algorithms for high dimensional mean estimation and\nstochastic convex optimization in these models that work in more general\nsettings than previously known solutions.\n", "versions": [{"version": "v1", "created": "Sun, 20 Nov 2016 06:12:43 GMT"}, {"version": "v2", "created": "Sat, 26 Aug 2017 02:25:41 GMT"}], "update_date": "2017-08-29", "authors_parsed": [["Feldman", "Vitaly", ""]]}, {"id": "1611.06534", "submitter": "Marc Abeille", "authors": "Marc Abeille, Alessandro Lazaric", "title": "Linear Thompson Sampling Revisited", "comments": null, "journal-ref": null, "doi": "10.1214/154957804100000000", "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We derive an alternative proof for the regret of Thompson sampling (\\ts) in\nthe stochastic linear bandit setting. While we obtain a regret bound of order\n$\\widetilde{O}(d^{3/2}\\sqrt{T})$ as in previous results, the proof sheds new\nlight on the functioning of the \\ts. We leverage on the structure of the\nproblem to show how the regret is related to the sensitivity (i.e., the\ngradient) of the objective function and how selecting optimal arms associated\nto \\textit{optimistic} parameters does control it. Thus we show that \\ts can be\nseen as a generic randomized algorithm where the sampling distribution is\ndesigned to have a fixed probability of being optimistic, at the cost of an\nadditional $\\sqrt{d}$ regret factor compared to a UCB-like approach.\nFurthermore, we show that our proof can be readily applied to regularized\nlinear optimization and generalized linear model problems.\n", "versions": [{"version": "v1", "created": "Sun, 20 Nov 2016 15:52:41 GMT"}, {"version": "v2", "created": "Mon, 27 Mar 2017 16:50:53 GMT"}, {"version": "v3", "created": "Tue, 5 Nov 2019 16:35:05 GMT"}], "update_date": "2019-11-06", "authors_parsed": [["Abeille", "Marc", ""], ["Lazaric", "Alessandro", ""]]}, {"id": "1611.06585", "submitter": "Andrew Miller", "authors": "Andrew C. Miller, Nicholas Foti, Ryan P. Adams", "title": "Variational Boosting: Iteratively Refining Posterior Approximations", "comments": "25 pages, 9 figures, 2 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a black-box variational inference method to approximate\nintractable distributions with an increasingly rich approximating class. Our\nmethod, termed variational boosting, iteratively refines an existing\nvariational approximation by solving a sequence of optimization problems,\nallowing the practitioner to trade computation time for accuracy. We show how\nto expand the variational approximating class by incorporating additional\ncovariance structure and by introducing new components to form a mixture. We\napply variational boosting to synthetic and real statistical models, and show\nthat resulting posterior inferences compare favorably to existing posterior\napproximation algorithms in both accuracy and efficiency.\n", "versions": [{"version": "v1", "created": "Sun, 20 Nov 2016 20:25:39 GMT"}, {"version": "v2", "created": "Sun, 19 Feb 2017 17:30:28 GMT"}], "update_date": "2017-02-21", "authors_parsed": [["Miller", "Andrew C.", ""], ["Foti", "Nicholas", ""], ["Adams", "Ryan P.", ""]]}, {"id": "1611.06652", "submitter": "Brian McWilliams", "authors": "Gabriel Krummenacher and Brian McWilliams and Yannic Kilcher and\n  Joachim M. Buhmann and Nicolai Meinshausen", "title": "Scalable Adaptive Stochastic Optimization Using Random Projections", "comments": "To appear in Advances in Neural Information Processing Systems 29\n  (NIPS 2016)", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Adaptive stochastic gradient methods such as AdaGrad have gained popularity\nin particular for training deep neural networks. The most commonly used and\nstudied variant maintains a diagonal matrix approximation to second order\ninformation by accumulating past gradients which are used to tune the step size\nadaptively. In certain situations the full-matrix variant of AdaGrad is\nexpected to attain better performance, however in high dimensions it is\ncomputationally impractical. We present Ada-LR and RadaGrad two computationally\nefficient approximations to full-matrix AdaGrad based on randomized\ndimensionality reduction. They are able to capture dependencies between\nfeatures and achieve similar performance to full-matrix AdaGrad but at a much\nsmaller computational cost. We show that the regret of Ada-LR is close to the\nregret of full-matrix AdaGrad which can have an up-to exponentially smaller\ndependence on the dimension than the diagonal variant. Empirically, we show\nthat Ada-LR and RadaGrad perform similarly to full-matrix AdaGrad. On the task\nof training convolutional neural networks as well as recurrent neural networks,\nRadaGrad achieves faster convergence than diagonal AdaGrad.\n", "versions": [{"version": "v1", "created": "Mon, 21 Nov 2016 05:15:50 GMT"}], "update_date": "2016-11-22", "authors_parsed": [["Krummenacher", "Gabriel", ""], ["McWilliams", "Brian", ""], ["Kilcher", "Yannic", ""], ["Buhmann", "Joachim M.", ""], ["Meinshausen", "Nicolai", ""]]}, {"id": "1611.06684", "submitter": "Lars Mescheder", "authors": "Lars Mescheder, Sebastian Nowozin and Andreas Geiger", "title": "Probabilistic Duality for Parallel Gibbs Sampling without Graph Coloring", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG math.PR stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a new notion of probabilistic duality for random variables\ninvolving mixture distributions. Using this notion, we show how to implement a\nhighly-parallelizable Gibbs sampler for weakly coupled discrete pairwise\ngraphical models with strictly positive factors that requires almost no\npreprocessing and is easy to implement. Moreover, we show how our method can be\ncombined with blocking to improve mixing. Even though our method leads to\ninferior mixing times compared to a sequential Gibbs sampler, we argue that our\nmethod is still very useful for large dynamic networks, where factors are added\nand removed on a continuous basis, as it is hard to maintain a graph coloring\nin this setup. Similarly, our method is useful for parallelizing Gibbs sampling\nin graphical models that do not allow for graph colorings with a small number\nof colors such as densely connected graphs.\n", "versions": [{"version": "v1", "created": "Mon, 21 Nov 2016 08:57:58 GMT"}], "update_date": "2016-11-23", "authors_parsed": [["Mescheder", "Lars", ""], ["Nowozin", "Sebastian", ""], ["Geiger", "Andreas", ""]]}, {"id": "1611.06686", "submitter": "Murat A. Erdogdu", "authors": "Murat A. Erdogdu, Mohsen Bayati, Lee H. Dicker", "title": "Scalable Approximations for Generalized Linear Problems", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In stochastic optimization, the population risk is generally approximated by\nthe empirical risk. However, in the large-scale setting, minimization of the\nempirical risk may be computationally restrictive. In this paper, we design an\nefficient algorithm to approximate the population risk minimizer in generalized\nlinear problems such as binary classification with surrogate losses and\ngeneralized linear regression models. We focus on large-scale problems, where\nthe iterative minimization of the empirical risk is computationally\nintractable, i.e., the number of observations $n$ is much larger than the\ndimension of the parameter $p$, i.e. $n \\gg p \\gg 1$. We show that under random\nsub-Gaussian design, the true minimizer of the population risk is approximately\nproportional to the corresponding ordinary least squares (OLS) estimator. Using\nthis relation, we design an algorithm that achieves the same accuracy as the\nempirical risk minimizer through iterations that attain up to a cubic\nconvergence rate, and that are cheaper than any batch optimization algorithm by\nat least a factor of $\\mathcal{O}(p)$. We provide theoretical guarantees for\nour algorithm, and analyze the convergence behavior in terms of data\ndimensions. Finally, we demonstrate the performance of our algorithm on\nwell-known classification and regression problems, through extensive numerical\nstudies on large-scale datasets, and show that it achieves the highest\nperformance compared to several other widely used and specialized optimization\nalgorithms.\n", "versions": [{"version": "v1", "created": "Mon, 21 Nov 2016 09:10:05 GMT"}], "update_date": "2016-11-22", "authors_parsed": [["Erdogdu", "Murat A.", ""], ["Bayati", "Mohsen", ""], ["Dicker", "Lee H.", ""]]}, {"id": "1611.06740", "submitter": "James Hensman", "authors": "James Hensman, Nicolas Durrande and Arno Solin", "title": "Variational Fourier features for Gaussian processes", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This work brings together two powerful concepts in Gaussian processes: the\nvariational approach to sparse approximation and the spectral representation of\nGaussian processes. This gives rise to an approximation that inherits the\nbenefits of the variational approach but with the representational power and\ncomputational scalability of spectral representations. The work hinges on a key\nresult that there exist spectral features related to a finite domain of the\nGaussian process which exhibit almost-independent covariances. We derive these\nexpressions for Matern kernels in one dimension, and generalize to more\ndimensions using kernels with specific structures. Under the assumption of\nadditive Gaussian noise, our method requires only a single pass through the\ndataset, making for very fast and accurate computation. We fit a model to 4\nmillion training points in just a few minutes on a standard laptop. With\nnon-conjugate likelihoods, our MCMC scheme reduces the cost of computation from\nO(NM2) (for a sparse Gaussian process) to O(NM) per iteration, where N is the\nnumber of data and M is the number of features.\n", "versions": [{"version": "v1", "created": "Mon, 21 Nov 2016 11:57:38 GMT"}, {"version": "v2", "created": "Wed, 8 Nov 2017 10:09:30 GMT"}], "update_date": "2017-11-09", "authors_parsed": [["Hensman", "James", ""], ["Durrande", "Nicolas", ""], ["Solin", "Arno", ""]]}, {"id": "1611.06759", "submitter": "J\\'er\\^ome Tubiana", "authors": "J\\'er\\^ome Tubiana (LPTENS), R\\'emi Monasson (LPTENS)", "title": "Emergence of Compositional Representations in Restricted Boltzmann\n  Machines", "comments": "Supplementary material available at the authors' webpage", "journal-ref": "Phys. Rev. Lett. 118, 138301 (2017)", "doi": "10.1103/PhysRevLett.118.138301", "report-no": null, "categories": "physics.data-an cond-mat.dis-nn cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Extracting automatically the complex set of features composing real\nhigh-dimensional data is crucial for achieving high performance in\nmachine--learning tasks. Restricted Boltzmann Machines (RBM) are empirically\nknown to be efficient for this purpose, and to be able to generate distributed\nand graded representations of the data. We characterize the structural\nconditions (sparsity of the weights, low effective temperature, nonlinearities\nin the activation functions of hidden units, and adaptation of fields\nmaintaining the activity in the visible layer) allowing RBM to operate in such\na compositional phase. Evidence is provided by the replica analysis of an\nadequate statistical ensemble of random RBMs and by RBM trained on the\nhandwritten digits dataset MNIST.\n", "versions": [{"version": "v1", "created": "Mon, 21 Nov 2016 12:46:25 GMT"}, {"version": "v2", "created": "Thu, 2 Mar 2017 21:50:02 GMT"}], "update_date": "2017-04-05", "authors_parsed": [["Tubiana", "J\u00e9r\u00f4me", "", "LPTENS"], ["Monasson", "R\u00e9mi", "", "LPTENS"]]}, {"id": "1611.06800", "submitter": "Boris Hayete", "authors": "Boris Hayete, Matthew Valko, Alex Greenfield, Raymond Yan", "title": "MDL-motivated compression of GLM ensembles increases interpretability\n  and retains predictive power", "comments": "The authors would like to acknowledge Leon Furchtgott and Fred Gruber\n  for their invaluable feedback on the manuscript, and Fred Gruber for his help\n  with LATEX. Presented at NIPS 2016 Workshop on Interpretable Machine Learning\n  in Complex Systems", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Over the years, ensemble methods have become a staple of machine learning.\nSimilarly, generalized linear models (GLMs) have become very popular for a wide\nvariety of statistical inference tasks. The former have been shown to enhance\nout- of-sample predictive power and the latter possess easy interpretability.\nRecently, ensembles of GLMs have been proposed as a possibility. On the\ndownside, this approach loses the interpretability that GLMs possess. We show\nthat minimum description length (MDL)-motivated compression of the inferred\nensembles can be used to recover interpretability without much, if any,\ndownside to performance and illustrate on a number of standard classification\ndata sets.\n", "versions": [{"version": "v1", "created": "Mon, 21 Nov 2016 14:20:04 GMT"}], "update_date": "2016-11-22", "authors_parsed": [["Hayete", "Boris", ""], ["Valko", "Matthew", ""], ["Greenfield", "Alex", ""], ["Yan", "Raymond", ""]]}, {"id": "1611.06863", "submitter": "Tom Rainforth", "authors": "David Janz, Brooks Paige, Tom Rainforth, Jan-Willem van de Meent,\n  Frank Wood", "title": "Probabilistic structure discovery in time series data", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Existing methods for structure discovery in time series data construct\ninterpretable, compositional kernels for Gaussian process regression models.\nWhile the learned Gaussian process model provides posterior mean and variance\nestimates, typically the structure is learned via a greedy optimization\nprocedure. This restricts the space of possible solutions and leads to\nover-confident uncertainty estimates. We introduce a fully Bayesian approach,\ninferring a full posterior over structures, which more reliably captures the\nuncertainty of the model.\n", "versions": [{"version": "v1", "created": "Mon, 21 Nov 2016 16:08:12 GMT"}], "update_date": "2016-11-22", "authors_parsed": [["Janz", "David", ""], ["Paige", "Brooks", ""], ["Rainforth", "Tom", ""], ["van de Meent", "Jan-Willem", ""], ["Wood", "Frank", ""]]}, {"id": "1611.06882", "submitter": "Luca de Alfaro", "authors": "Rakshit Agrawal, Luca de Alfaro, Vassilis Polychronopoulos", "title": "Learning From Graph Neighborhoods Using LSTMs", "comments": null, "journal-ref": null, "doi": null, "report-no": "Technical Report UCSC-SOE-16-17, School of Engineering, University\n  of California, Santa Cruz", "categories": "cs.LG cs.AI stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Many prediction problems can be phrased as inferences over local\nneighborhoods of graphs. The graph represents the interaction between entities,\nand the neighborhood of each entity contains information that allows the\ninferences or predictions. We present an approach for applying machine learning\ndirectly to such graph neighborhoods, yielding predicitons for graph nodes on\nthe basis of the structure of their local neighborhood and the features of the\nnodes in it. Our approach allows predictions to be learned directly from\nexamples, bypassing the step of creating and tuning an inference model or\nsummarizing the neighborhoods via a fixed set of hand-crafted features. The\napproach is based on a multi-level architecture built from Long Short-Term\nMemory neural nets (LSTMs); the LSTMs learn how to summarize the neighborhood\nfrom data. We demonstrate the effectiveness of the proposed technique on a\nsynthetic example and on real-world data related to crowdsourced grading,\nBitcoin transactions, and Wikipedia edit reversions.\n", "versions": [{"version": "v1", "created": "Mon, 21 Nov 2016 16:25:34 GMT"}], "update_date": "2016-11-22", "authors_parsed": [["Agrawal", "Rakshit", ""], ["de Alfaro", "Luca", ""], ["Polychronopoulos", "Vassilis", ""]]}, {"id": "1611.06928", "submitter": "Christoph Dann", "authors": "Christoph Dann, Katja Hofmann, Sebastian Nowozin", "title": "Memory Lens: How Much Memory Does an Agent Use?", "comments": "Presented at NIPS 2016 Workshop on Interpretable Machine Learning in\n  Complex Systems", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a new method to study the internal memory used by reinforcement\nlearning policies. We estimate the amount of relevant past information by\nestimating mutual information between behavior histories and the current action\nof an agent. We perform this estimation in the passive setting, that is, we do\nnot intervene but merely observe the natural behavior of the agent. Moreover,\nwe provide a theoretical justification for our approach by showing that it\nyields an implementation-independent lower bound on the minimal memory capacity\nof any agent that implement the observed policy. We demonstrate our approach by\nestimating the use of memory of DQN policies on concatenated Atari frames,\ndemonstrating sharply different use of memory across 49 games. The study of\nmemory as information that flows from the past to the current action opens\navenues to understand and improve successful reinforcement learning algorithms.\n", "versions": [{"version": "v1", "created": "Mon, 21 Nov 2016 18:22:27 GMT"}], "update_date": "2016-11-22", "authors_parsed": [["Dann", "Christoph", ""], ["Hofmann", "Katja", ""], ["Nowozin", "Sebastian", ""]]}, {"id": "1611.06933", "submitter": "Jacob Eisenstein", "authors": "Jacob Eisenstein", "title": "Unsupervised Learning for Lexicon-Based Classification", "comments": "to appear in AAAI 2017", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CL stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In lexicon-based classification, documents are assigned labels by comparing\nthe number of words that appear from two opposed lexicons, such as positive and\nnegative sentiment. Creating such words lists is often easier than labeling\ninstances, and they can be debugged by non-experts if classification\nperformance is unsatisfactory. However, there is little analysis or\njustification of this classification heuristic. This paper describes a set of\nassumptions that can be used to derive a probabilistic justification for\nlexicon-based classification, as well as an analysis of its expected accuracy.\nOne key assumption behind lexicon-based classification is that all words in\neach lexicon are equally predictive. This is rarely true in practice, which is\nwhy lexicon-based approaches are usually outperformed by supervised classifiers\nthat learn distinct weights on each word from labeled instances. This paper\nshows that it is possible to learn such weights without labeled data, by\nleveraging co-occurrence statistics across the lexicons. This offers the best\nof both worlds: light supervision in the form of lexicons, and data-driven\nclassification with higher accuracy than traditional word-counting heuristics.\n", "versions": [{"version": "v1", "created": "Mon, 21 Nov 2016 18:30:17 GMT"}], "update_date": "2016-11-22", "authors_parsed": [["Eisenstein", "Jacob", ""]]}, {"id": "1611.06972", "submitter": "Jack Gorham", "authors": "Jackson Gorham, Andrew B. Duncan, Sebastian J. Vollmer, and Lester\n  Mackey", "title": "Measuring Sample Quality with Diffusions", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG math.PR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Stein's method for measuring convergence to a continuous target distribution\nrelies on an operator characterizing the target and Stein factor bounds on the\nsolutions of an associated differential equation. While such operators and\nbounds are readily available for a diversity of univariate targets, few\nmultivariate targets have been analyzed. We introduce a new class of\ncharacterizing operators based on Ito diffusions and develop explicit\nmultivariate Stein factor bounds for any target with a fast-coupling Ito\ndiffusion. As example applications, we develop computable and\nconvergence-determining diffusion Stein discrepancies for log-concave,\nheavy-tailed, and multimodal targets and use these quality measures to select\nthe hyperparameters of biased Markov chain Monte Carlo (MCMC) samplers, compare\nrandom and deterministic quadrature rules, and quantify bias-variance tradeoffs\nin approximate MCMC. Our results establish a near-linear relationship between\ndiffusion Stein discrepancies and Wasserstein distances, improving upon past\nwork even for strongly log-concave targets. The exposed relationship between\nStein factors and Markov process coupling may be of independent interest.\n", "versions": [{"version": "v1", "created": "Mon, 21 Nov 2016 19:47:08 GMT"}, {"version": "v2", "created": "Wed, 23 Nov 2016 19:46:21 GMT"}, {"version": "v3", "created": "Mon, 19 Jun 2017 18:42:43 GMT"}, {"version": "v4", "created": "Wed, 31 Jan 2018 22:47:59 GMT"}, {"version": "v5", "created": "Wed, 21 Feb 2018 01:20:25 GMT"}, {"version": "v6", "created": "Tue, 13 Nov 2018 01:25:12 GMT"}], "update_date": "2018-11-14", "authors_parsed": [["Gorham", "Jackson", ""], ["Duncan", "Andrew B.", ""], ["Vollmer", "Sebastian J.", ""], ["Mackey", "Lester", ""]]}, {"id": "1611.06996", "submitter": "Elad Hoffer", "authors": "Elad Hoffer, Itay Hubara, Nir Ailon", "title": "Spatial contrasting for deep unsupervised learning", "comments": "Presented at NIPS 2016 Workshop on Interpretable Machine Learning in\n  Complex Systems", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Convolutional networks have marked their place over the last few years as the\nbest performing model for various visual tasks. They are, however, most suited\nfor supervised learning from large amounts of labeled data. Previous attempts\nhave been made to use unlabeled data to improve model performance by applying\nunsupervised techniques. These attempts require different architectures and\ntraining methods. In this work we present a novel approach for unsupervised\ntraining of Convolutional networks that is based on contrasting between spatial\nregions within images. This criterion can be employed within conventional\nneural networks and trained using standard techniques such as SGD and\nback-propagation, thus complementing supervised methods.\n", "versions": [{"version": "v1", "created": "Mon, 21 Nov 2016 20:24:58 GMT"}], "update_date": "2016-11-23", "authors_parsed": [["Hoffer", "Elad", ""], ["Hubara", "Itay", ""], ["Ailon", "Nir", ""]]}, {"id": "1611.07012", "submitter": "Edward Choi", "authors": "Edward Choi, Mohammad Taha Bahadori, Le Song, Walter F. Stewart,\n  Jimeng Sun", "title": "GRAM: Graph-based Attention Model for Healthcare Representation Learning", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Deep learning methods exhibit promising performance for predictive modeling\nin healthcare, but two important challenges remain: -Data insufficiency:Often\nin healthcare predictive modeling, the sample size is insufficient for deep\nlearning methods to achieve satisfactory results. -Interpretation:The\nrepresentations learned by deep learning methods should align with medical\nknowledge. To address these challenges, we propose a GRaph-based Attention\nModel, GRAM that supplements electronic health records (EHR) with hierarchical\ninformation inherent to medical ontologies. Based on the data volume and the\nontology structure, GRAM represents a medical concept as a combination of its\nancestors in the ontology via an attention mechanism. We compared predictive\nperformance (i.e. accuracy, data needs, interpretability) of GRAM to various\nmethods including the recurrent neural network (RNN) in two sequential\ndiagnoses prediction tasks and one heart failure prediction task. Compared to\nthe basic RNN, GRAM achieved 10% higher accuracy for predicting diseases rarely\nobserved in the training data and 3% improved area under the ROC curve for\npredicting heart failure using an order of magnitude less training data.\nAdditionally, unlike other methods, the medical concept representations learned\nby GRAM are well aligned with the medical ontology. Finally, GRAM exhibits\nintuitive attention behaviors by adaptively generalizing to higher level\nconcepts when facing data insufficiency at the lower level concepts.\n", "versions": [{"version": "v1", "created": "Mon, 21 Nov 2016 20:59:22 GMT"}, {"version": "v2", "created": "Sat, 25 Feb 2017 16:51:18 GMT"}, {"version": "v3", "created": "Sat, 1 Apr 2017 22:21:35 GMT"}], "update_date": "2017-04-04", "authors_parsed": [["Choi", "Edward", ""], ["Bahadori", "Mohammad Taha", ""], ["Song", "Le", ""], ["Stewart", "Walter F.", ""], ["Sun", "Jimeng", ""]]}, {"id": "1611.07051", "submitter": "Feras Saad", "authors": "Ulrich Schaechtle, Feras Saad, Alexey Radul, and Vikash Mansinghka", "title": "Time Series Structure Discovery via Probabilistic Program Synthesis", "comments": "The first two authors contributed equally to this work", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  There is a widespread need for techniques that can discover structure from\ntime series data. Recently introduced techniques such as Automatic Bayesian\nCovariance Discovery (ABCD) provide a way to find structure within a single\ntime series by searching through a space of covariance kernels that is\ngenerated using a simple grammar. While ABCD can identify a broad class of\ntemporal patterns, it is difficult to extend and can be brittle in practice.\nThis paper shows how to extend ABCD by formulating it in terms of probabilistic\nprogram synthesis. The key technical ideas are to (i) represent models using\nabstract syntax trees for a domain-specific probabilistic language, and (ii)\nrepresent the time series model prior, likelihood, and search strategy using\nprobabilistic programs in a sufficiently expressive language. The final\nprobabilistic program is written in under 70 lines of probabilistic code in\nVenture. The paper demonstrates an application to time series clustering that\ninvolves a non-parametric extension to ABCD, experiments for interpolation and\nextrapolation on real-world econometric data, and improvements in accuracy over\nboth non-parametric and standard regression baselines.\n", "versions": [{"version": "v1", "created": "Mon, 21 Nov 2016 21:03:01 GMT"}, {"version": "v2", "created": "Thu, 23 Mar 2017 21:08:06 GMT"}, {"version": "v3", "created": "Mon, 22 May 2017 15:11:24 GMT"}], "update_date": "2017-05-23", "authors_parsed": [["Schaechtle", "Ulrich", ""], ["Saad", "Feras", ""], ["Radul", "Alexey", ""], ["Mansinghka", "Vikash", ""]]}, {"id": "1611.07054", "submitter": "Sebastian P\\\"olsterl", "authors": "Sebastian P\\\"olsterl, Nassir Navab, Amin Katouzian", "title": "An Efficient Training Algorithm for Kernel Survival Support Vector\n  Machines", "comments": "ECML PKDD MLLS 2016: 3rd Workshop on Machine Learning in Life\n  Sciences", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Survival analysis is a fundamental tool in medical research to identify\npredictors of adverse events and develop systems for clinical decision support.\nIn order to leverage large amounts of patient data, efficient optimisation\nroutines are paramount. We propose an efficient training algorithm for the\nkernel survival support vector machine (SSVM). We directly optimise the primal\nobjective function and employ truncated Newton optimisation and order statistic\ntrees to significantly lower computational costs compared to previous training\nalgorithms, which require $O(n^4)$ space and $O(p n^6)$ time for datasets with\n$n$ samples and $p$ features. Our results demonstrate that our proposed\noptimisation scheme allows analysing data of a much larger scale with no loss\nin prediction performance. Experiments on synthetic and 5 real-world datasets\nshow that our technique outperforms existing kernel SSVM formulations if the\namount of right censoring is high ($\\geq85\\%$), and performs comparably\notherwise.\n", "versions": [{"version": "v1", "created": "Mon, 21 Nov 2016 21:09:33 GMT"}], "update_date": "2016-11-23", "authors_parsed": [["P\u00f6lsterl", "Sebastian", ""], ["Navab", "Nassir", ""], ["Katouzian", "Amin", ""]]}, {"id": "1611.07056", "submitter": "Luca Martino", "authors": "Luca Martino, Victor Elvira, Gustau Camps-Valls", "title": "The Recycling Gibbs Sampler for Efficient Learning", "comments": "The MATLAB code of the numerical examples is provided at\n  http://isp.uv.es/code/RG.zip", "journal-ref": "Digital Signal Processing, Volume 74, 2018", "doi": "10.1016/j.dsp.2017.11.012", "report-no": null, "categories": "stat.CO cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Monte Carlo methods are essential tools for Bayesian inference. Gibbs\nsampling is a well-known Markov chain Monte Carlo (MCMC) algorithm, extensively\nused in signal processing, machine learning, and statistics, employed to draw\nsamples from complicated high-dimensional posterior distributions. The key\npoint for the successful application of the Gibbs sampler is the ability to\ndraw efficiently samples from the full-conditional probability density\nfunctions. Since in the general case this is not possible, in order to speed up\nthe convergence of the chain, it is required to generate auxiliary samples\nwhose information is eventually disregarded. In this work, we show that these\nauxiliary samples can be recycled within the Gibbs estimators, improving their\nefficiency with no extra cost. This novel scheme arises naturally after\npointing out the relationship between the standard Gibbs sampler and the chain\nrule used for sampling purposes. Numerical simulations involving simple and\nreal inference problems confirm the excellent performance of the proposed\nscheme in terms of accuracy and computational efficiency. In particular we give\nempirical evidence of performance in a toy example, inference of Gaussian\nprocesses hyperparameters, and learning dependence graphs through regression.\n", "versions": [{"version": "v1", "created": "Mon, 21 Nov 2016 21:13:00 GMT"}, {"version": "v2", "created": "Wed, 20 Dec 2017 14:59:08 GMT"}], "update_date": "2017-12-21", "authors_parsed": [["Martino", "Luca", ""], ["Elvira", "Victor", ""], ["Camps-Valls", "Gustau", ""]]}, {"id": "1611.07078", "submitter": "Felix Leibfried", "authors": "Felix Leibfried, Nate Kushman, Katja Hofmann", "title": "A Deep Learning Approach for Joint Video Frame and Reward Prediction in\n  Atari Games", "comments": "Presented at the ICML 2017 Workshop on Principled Approaches to Deep\n  Learning, Sydney, Australia, 2017", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Reinforcement learning is concerned with identifying reward-maximizing\nbehaviour policies in environments that are initially unknown. State-of-the-art\nreinforcement learning approaches, such as deep Q-networks, are model-free and\nlearn to act effectively across a wide range of environments such as Atari\ngames, but require huge amounts of data. Model-based techniques are more\ndata-efficient, but need to acquire explicit knowledge about the environment.\n  In this paper, we take a step towards using model-based techniques in\nenvironments with a high-dimensional visual state space by demonstrating that\nit is possible to learn system dynamics and the reward structure jointly. Our\ncontribution is to extend a recently developed deep neural network for video\nframe prediction in Atari games to enable reward prediction as well. To this\nend, we phrase a joint optimization problem for minimizing both video frame and\nreward reconstruction loss, and adapt network parameters accordingly. Empirical\nevaluations on five Atari games demonstrate accurate cumulative reward\nprediction of up to 200 frames. We consider these results as opening up\nimportant directions for model-based reinforcement learning in complex,\ninitially unknown environments.\n", "versions": [{"version": "v1", "created": "Mon, 21 Nov 2016 22:06:23 GMT"}, {"version": "v2", "created": "Thu, 17 Aug 2017 09:00:01 GMT"}], "update_date": "2017-08-18", "authors_parsed": [["Leibfried", "Felix", ""], ["Kushman", "Nate", ""], ["Hofmann", "Katja", ""]]}, {"id": "1611.07093", "submitter": "Ashkan Esmaeili", "authors": "Ahmadreza Moradipari, Sina Shahsavari, Ashkan Esmaeili, and Farokh\n  Marvasti", "title": "Using Empirical Covariance Matrix in Enhancing Prediction Accuracy of\n  Linear Models with Missing Information", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Inference and Estimation in Missing Information (MI) scenarios are important\ntopics in Statistical Learning Theory and Machine Learning (ML). In ML\nliterature, attempts have been made to enhance prediction through precise\nfeature selection methods. In sparse linear models, LASSO is well-known in\nextracting the desired support of the signal and resisting against noisy\nsystems. When sparse models are also suffering from MI, the sparse recovery and\ninference of the missing models are taken into account simultaneously. In this\npaper, we will introduce an approach which enjoys sparse regression and\ncovariance matrix estimation to improve matrix completion accuracy, and as a\nresult enhancing feature selection preciseness which leads to reduction in\nprediction Mean Squared Error (MSE). We will compare the effect of employing\ncovariance matrix in enhancing estimation accuracy to the case it is not used\nin feature selection. Simulations show the improvement in the performance as\ncompared to the case where the covariance matrix estimation is not used.\n", "versions": [{"version": "v1", "created": "Mon, 21 Nov 2016 23:07:51 GMT"}, {"version": "v2", "created": "Wed, 1 Feb 2017 23:26:32 GMT"}, {"version": "v3", "created": "Tue, 25 Jul 2017 11:29:34 GMT"}], "update_date": "2017-07-26", "authors_parsed": [["Moradipari", "Ahmadreza", ""], ["Shahsavari", "Sina", ""], ["Esmaeili", "Ashkan", ""], ["Marvasti", "Farokh", ""]]}, {"id": "1611.07096", "submitter": "Chong Yang Goh", "authors": "Chong Yang Goh, Patrick Jaillet", "title": "Structured Prediction by Conditional Risk Minimization", "comments": "19 pages, with supplements", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a general approach for supervised learning with structured output\nspaces, such as combinatorial and polyhedral sets, that is based on minimizing\nestimated conditional risk functions. Given a loss function defined over pairs\nof output labels, we first estimate the conditional risk function by solving a\n(possibly infinite) collection of regularized least squares problems. A\nprediction is made by solving an inference problem that minimizes the estimated\nconditional risk function over the output space. We show that this approach\nenables, in some cases, efficient training and inference without explicitly\nintroducing a convex surrogate for the original loss function, even when it is\ndiscontinuous. Empirical evaluations on real-world and synthetic data sets\ndemonstrate the effectiveness of our method in adapting to a variety of loss\nfunctions.\n", "versions": [{"version": "v1", "created": "Mon, 21 Nov 2016 23:14:58 GMT"}, {"version": "v2", "created": "Sun, 26 Feb 2017 01:03:51 GMT"}], "update_date": "2017-02-28", "authors_parsed": [["Goh", "Chong Yang", ""], ["Jaillet", "Patrick", ""]]}, {"id": "1611.07100", "submitter": "Qin Lin", "authors": "Christian Albert Hammerschmidt, Sicco Verwer, Qin Lin, Radu State", "title": "Interpreting Finite Automata for Sequential Data", "comments": "Presented at NIPS 2016 Workshop on Interpretable Machine Learning in\n  Complex Systems", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.AI", "license": "http://creativecommons.org/licenses/by-sa/4.0/", "abstract": "  Automaton models are often seen as interpretable models. Interpretability\nitself is not well defined: it remains unclear what interpretability means\nwithout first explicitly specifying objectives or desired attributes. In this\npaper, we identify the key properties used to interpret automata and propose a\nmodification of a state-merging approach to learn variants of finite state\nautomata. We apply the approach to problems beyond typical grammar inference\ntasks. Additionally, we cover several use-cases for prediction, classification,\nand clustering on sequential data in both supervised and unsupervised scenarios\nto show how the identified key properties are applicable in a wide range of\ncontexts.\n", "versions": [{"version": "v1", "created": "Mon, 21 Nov 2016 23:21:13 GMT"}, {"version": "v2", "created": "Thu, 24 Nov 2016 20:52:50 GMT"}], "update_date": "2016-11-28", "authors_parsed": [["Hammerschmidt", "Christian Albert", ""], ["Verwer", "Sicco", ""], ["Lin", "Qin", ""], ["State", "Radu", ""]]}, {"id": "1611.07103", "submitter": "Giacomo Aletti", "authors": "Giacomo Aletti", "title": "Generation of discrete random variables in scalable frameworks", "comments": "The first sections of the paper have been almost completely\n  rewritten. A deep revision of the English has been made", "journal-ref": "Statistics & Probability Letters, Volume 132, January 2018, Pages\n  99-106", "doi": "10.1016/j.spl.2017.09.004", "report-no": null, "categories": "stat.ME math.ST stat.ML stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we face the problem of simulating discrete random variables\nwith general and varying distributions in a scalable framework, where fully\nparallelizable operations should be preferred. The new paradigm is inspired by\nthe context of discrete choice models. Compared to classical algorithms, we add\nparallelized randomness, and we leave the final simulation of the random\nvariable to a single associative operation. We characterize the set of\nalgorithms that work in this way, and those algorithms that may have an\nadditive or multiplicative local noise. As a consequence, we could define a\nnatural way to solve some popular simulation problems.\n", "versions": [{"version": "v1", "created": "Mon, 21 Nov 2016 23:37:31 GMT"}, {"version": "v2", "created": "Sun, 1 Jan 2017 15:29:43 GMT"}, {"version": "v3", "created": "Mon, 10 Jul 2017 02:47:02 GMT"}], "update_date": "2018-01-04", "authors_parsed": [["Aletti", "Giacomo", ""]]}, {"id": "1611.07115", "submitter": "Sarah Tan", "authors": "Sarah Tan, Matvey Soloviev, Giles Hooker, Martin T. Wells", "title": "Tree Space Prototypes: Another Look at Making Tree Ensembles\n  Interpretable", "comments": "Camera-ready version for ACM-IMS FODS 2020. A short version was\n  presented at NIPS 2016 Workshop on Interpretable Machine Learning for Complex\n  Systems", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Ensembles of decision trees perform well on many problems, but are not\ninterpretable. In contrast to existing approaches in interpretability that\nfocus on explaining relationships between features and predictions, we propose\nan alternative approach to interpret tree ensemble classifiers by surfacing\nrepresentative points for each class -- prototypes. We introduce a new distance\nfor Gradient Boosted Tree models, and propose new, adaptive prototype selection\nmethods with theoretical guarantees, with the flexibility to choose a different\nnumber of prototypes in each class. We demonstrate our methods on random\nforests and gradient boosted trees, showing that the prototypes can perform as\nwell as or even better than the original tree ensemble when used as a\nnearest-prototype classifier. In a user study, humans were better at predicting\nthe output of a tree ensemble classifier when using prototypes than when using\nShapley values, a popular feature attribution method. Hence, prototypes present\na viable alternative to feature-based explanations for tree ensembles.\n", "versions": [{"version": "v1", "created": "Tue, 22 Nov 2016 00:53:29 GMT"}, {"version": "v2", "created": "Sun, 24 Nov 2019 22:56:22 GMT"}, {"version": "v3", "created": "Tue, 25 Aug 2020 08:01:26 GMT"}], "update_date": "2020-08-26", "authors_parsed": [["Tan", "Sarah", ""], ["Soloviev", "Matvey", ""], ["Hooker", "Giles", ""], ["Wells", "Martin T.", ""]]}, {"id": "1611.07119", "submitter": "Chongxuan Li", "authors": "Chongxuan Li and Jun Zhu and Bo Zhang", "title": "Max-Margin Deep Generative Models for (Semi-)Supervised Learning", "comments": "arXiv admin note: substantial text overlap with arXiv:1504.06787", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Deep generative models (DGMs) are effective on learning multilayered\nrepresentations of complex data and performing inference of input data by\nexploring the generative ability. However, it is relatively insufficient to\nempower the discriminative ability of DGMs on making accurate predictions. This\npaper presents max-margin deep generative models (mmDGMs) and a\nclass-conditional variant (mmDCGMs), which explore the strongly discriminative\nprinciple of max-margin learning to improve the predictive performance of DGMs\nin both supervised and semi-supervised learning, while retaining the generative\ncapability. In semi-supervised learning, we use the predictions of a max-margin\nclassifier as the missing labels instead of performing full posterior inference\nfor efficiency; we also introduce additional max-margin and label-balance\nregularization terms of unlabeled data for effectiveness. We develop an\nefficient doubly stochastic subgradient algorithm for the piecewise linear\nobjectives in different settings. Empirical results on various datasets\ndemonstrate that: (1) max-margin learning can significantly improve the\nprediction performance of DGMs and meanwhile retain the generative ability; (2)\nin supervised learning, mmDGMs are competitive to the best fully discriminative\nnetworks when employing convolutional neural networks as the generative and\nrecognition models; and (3) in semi-supervised learning, mmDCGMs can perform\nefficient inference and achieve state-of-the-art classification results on\nseveral benchmarks.\n", "versions": [{"version": "v1", "created": "Tue, 22 Nov 2016 01:36:29 GMT"}], "update_date": "2016-11-23", "authors_parsed": [["Li", "Chongxuan", ""], ["Zhu", "Jun", ""], ["Zhang", "Bo", ""]]}, {"id": "1611.07138", "submitter": "Patrick Rebeschini", "authors": "Patrick Rebeschini and Sekhar Tatikonda", "title": "A New Approach to Laplacian Solvers and Flow Problems", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.OC cs.DS stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper investigates the behavior of the Min-Sum message passing scheme to\nsolve systems of linear equations in the Laplacian matrices of graphs and to\ncompute electric flows. Voltage and flow problems involve the minimization of\nquadratic functions and are fundamental primitives that arise in several\ndomains. Algorithms that have been proposed are typically centralized and\ninvolve multiple graph-theoretic constructions or sampling mechanisms that make\nthem difficult to implement and analyze. On the other hand, message passing\nroutines are distributed, simple, and easy to implement. In this paper we\nestablish a framework to analyze Min-Sum to solve voltage and flow problems. We\ncharacterize the error committed by the algorithm on general weighted graphs in\nterms of hitting times of random walks defined on the computation trees that\nsupport the operations of the algorithms with time. For $d$-regular graphs with\nequal weights, we show that the convergence of the algorithms is controlled by\nthe total variation distance between the distributions of non-backtracking\nrandom walks defined on the original graph that start from neighboring nodes.\nThe framework that we introduce extends the analysis of Min-Sum to settings\nwhere the contraction arguments previously considered in the literature (based\non the assumption of walk summability or scaled diagonal dominance) can not be\nused, possibly in the presence of constraints.\n", "versions": [{"version": "v1", "created": "Tue, 22 Nov 2016 03:39:07 GMT"}, {"version": "v2", "created": "Thu, 7 Mar 2019 12:44:30 GMT"}], "update_date": "2019-03-08", "authors_parsed": [["Rebeschini", "Patrick", ""], ["Tatikonda", "Sekhar", ""]]}, {"id": "1611.07161", "submitter": "Xinyu He", "authors": "Xinyu He and Warren B. Powell", "title": "Optimal Learning for Stochastic Optimization with Nonlinear Parametric\n  Belief Models", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider the problem of estimating the expected value of information (the\nknowledge gradient) for Bayesian learning problems where the belief model is\nnonlinear in the parameters. Our goal is to maximize some metric, while\nsimultaneously learning the unknown parameters of the nonlinear belief model,\nby guiding a sequential experimentation process which is expensive. We overcome\nthe problem of computing the expected value of an experiment, which is\ncomputationally intractable, by using a sampled approximation, which helps to\nguide experiments but does not provide an accurate estimate of the unknown\nparameters. We then introduce a resampling process which allows the sampled\nmodel to adapt to new information, exploiting past experiments. We show\ntheoretically that the method converges asymptotically to the true parameters,\nwhile simultaneously maximizing our metric. We show empirically that the\nprocess exhibits rapid convergence, yielding good results with a very small\nnumber of experiments.\n", "versions": [{"version": "v1", "created": "Tue, 22 Nov 2016 06:41:57 GMT"}], "update_date": "2016-11-23", "authors_parsed": [["He", "Xinyu", ""], ["Powell", "Warren B.", ""]]}, {"id": "1611.07252", "submitter": "Scott Wisdom", "authors": "Scott Wisdom, Thomas Powers, James Pitton, Les Atlas", "title": "Interpretable Recurrent Neural Networks Using Sequential Sparse Recovery", "comments": "Presented at NIPS 2016 Workshop on Interpretable Machine Learning in\n  Complex Systems", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recurrent neural networks (RNNs) are powerful and effective for processing\nsequential data. However, RNNs are usually considered \"black box\" models whose\ninternal structure and learned parameters are not interpretable. In this paper,\nwe propose an interpretable RNN based on the sequential iterative\nsoft-thresholding algorithm (SISTA) for solving the sequential sparse recovery\nproblem, which models a sequence of correlated observations with a sequence of\nsparse latent vectors. The architecture of the resulting SISTA-RNN is\nimplicitly defined by the computational structure of SISTA, which results in a\nnovel stacked RNN architecture. Furthermore, the weights of the SISTA-RNN are\nperfectly interpretable as the parameters of a principled statistical model,\nwhich in this case include a sparsifying dictionary, iterative step size, and\nregularization parameters. In addition, on a particular sequential compressive\nsensing task, the SISTA-RNN trains faster and achieves better performance than\nconventional state-of-the-art black box RNNs, including long-short term memory\n(LSTM) RNNs.\n", "versions": [{"version": "v1", "created": "Tue, 22 Nov 2016 11:29:15 GMT"}], "update_date": "2016-11-23", "authors_parsed": [["Wisdom", "Scott", ""], ["Powers", "Thomas", ""], ["Pitton", "James", ""], ["Atlas", "Les", ""]]}, {"id": "1611.07256", "submitter": "Azzimonti Dario", "authors": "Dario Azzimonti (IDSIA), David Ginsbourger (Idiap, IMSV), Cl\\'ement\n  Chevalier (UNINE), Julien Bect (L2S, GdR MASCOT-NUM), Yann Richet (IRSN, GdR\n  MASCOT-NUM)", "title": "Adaptive Design of Experiments for Conservative Estimation of Excursion\n  Sets", "comments": null, "journal-ref": "Technometrics, 63(1):13-26, 2021", "doi": "10.1080/00401706.2019.1693427", "report-no": null, "categories": "stat.ME math.ST stat.ML stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider the problem of estimating the set of all inputs that leads a\nsystem to some particular behavior. The system is modeled by an\nexpensive-to-evaluate function, such as a computer experiment, and we are\ninterested in its excursion set, i.e. the set of points where the function\ntakes values above or below some prescribed threshold. The objective function\nis emulated with a Gaussian Process (GP) model based on an initial design of\nexperiments enriched with evaluation results at (batch-)sequentially determined\ninput points. The GP model provides conservative estimates for the excursion\nset, which control false positives while minimizing false negatives. We\nintroduce adaptive strategies that sequentially select new evaluations of the\nfunction by reducing the uncertainty on conservative estimates. Following the\nStepwise Uncertainty Reduction approach we obtain new evaluations by minimizing\nadapted criteria. Tractable formulae for the conservative criteria are derived,\nwhich allow more convenient optimization. The method is benchmarked on random\nfunctions generated under the model assumptions in different scenarios of noise\nand batch size. We then apply it to a reliability engineering test case.\nOverall, the proposed strategy of minimizing false negatives in conservative\nestimation achieves competitive performance both in terms of model-based and\nmodel-free indicators.\n", "versions": [{"version": "v1", "created": "Tue, 22 Nov 2016 11:40:25 GMT"}, {"version": "v2", "created": "Wed, 11 Jan 2017 09:36:00 GMT"}, {"version": "v3", "created": "Tue, 10 Apr 2018 08:01:43 GMT"}, {"version": "v4", "created": "Fri, 30 Nov 2018 12:48:22 GMT"}, {"version": "v5", "created": "Fri, 25 Oct 2019 13:14:35 GMT"}, {"version": "v6", "created": "Tue, 4 Feb 2020 07:22:36 GMT"}], "update_date": "2021-05-10", "authors_parsed": [["Azzimonti", "Dario", "", "IDSIA"], ["Ginsbourger", "David", "", "Idiap, IMSV"], ["Chevalier", "Cl\u00e9ment", "", "UNINE"], ["Bect", "Julien", "", "L2S, GdR MASCOT-NUM"], ["Richet", "Yann", "", "IRSN, GdR\n  MASCOT-NUM"]]}, {"id": "1611.07270", "submitter": "Pieter-Jan Kindermans", "authors": "Pieter-Jan Kindermans, Kristof Sch\\\"utt, Klaus-Robert M\\\"uller, Sven\n  D\\\"ahne", "title": "Investigating the influence of noise and distractors on the\n  interpretation of neural networks", "comments": "Presented at NIPS 2016 Workshop on Interpretable Machine Learning in\n  Complex Systems", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Understanding neural networks is becoming increasingly important. Over the\nlast few years different types of visualisation and explanation methods have\nbeen proposed. However, none of them explicitly considered the behaviour in the\npresence of noise and distracting elements. In this work, we will show how\nnoise and distracting dimensions can influence the result of an explanation\nmodel. This gives a new theoretical insights to aid selection of the most\nappropriate explanation model within the deep-Taylor decomposition framework.\n", "versions": [{"version": "v1", "created": "Tue, 22 Nov 2016 12:23:07 GMT"}], "update_date": "2016-11-23", "authors_parsed": [["Kindermans", "Pieter-Jan", ""], ["Sch\u00fctt", "Kristof", ""], ["M\u00fcller", "Klaus-Robert", ""], ["D\u00e4hne", "Sven", ""]]}, {"id": "1611.07308", "submitter": "Thomas Kipf", "authors": "Thomas N. Kipf, Max Welling", "title": "Variational Graph Auto-Encoders", "comments": "Bayesian Deep Learning Workshop (NIPS 2016)", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We introduce the variational graph auto-encoder (VGAE), a framework for\nunsupervised learning on graph-structured data based on the variational\nauto-encoder (VAE). This model makes use of latent variables and is capable of\nlearning interpretable latent representations for undirected graphs. We\ndemonstrate this model using a graph convolutional network (GCN) encoder and a\nsimple inner product decoder. Our model achieves competitive results on a link\nprediction task in citation networks. In contrast to most existing models for\nunsupervised learning on graph-structured data and link prediction, our model\ncan naturally incorporate node features, which significantly improves\npredictive performance on a number of benchmark datasets.\n", "versions": [{"version": "v1", "created": "Mon, 21 Nov 2016 11:37:17 GMT"}], "update_date": "2016-11-23", "authors_parsed": [["Kipf", "Thomas N.", ""], ["Welling", "Max", ""]]}, {"id": "1611.07343", "submitter": "Jean-Baptiste Mouret", "authors": "Antoine Cully, Konstantinos Chatzilygeroudis, Federico Allocati,\n  Jean-Baptiste Mouret", "title": "Limbo: A Fast and Flexible Library for Bayesian Optimization", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI cs.RO stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Limbo is an open-source C++11 library for Bayesian optimization which is\ndesigned to be both highly flexible and very fast. It can be used to optimize\nfunctions for which the gradient is unknown, evaluations are expensive, and\nruntime cost matters (e.g., on embedded systems or robots). Benchmarks on\nstandard functions show that Limbo is about 2 times faster than BayesOpt\n(another C++ library) for a similar accuracy.\n", "versions": [{"version": "v1", "created": "Tue, 22 Nov 2016 15:00:08 GMT"}], "update_date": "2016-11-23", "authors_parsed": [["Cully", "Antoine", ""], ["Chatzilygeroudis", "Konstantinos", ""], ["Allocati", "Federico", ""], ["Mouret", "Jean-Baptiste", ""]]}, {"id": "1611.07422", "submitter": "Jiequn Han", "authors": "Jiequn Han, Weinan E", "title": "Deep Learning Approximation for Stochastic Control Problems", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI cs.NE math.OC stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Many real world stochastic control problems suffer from the \"curse of\ndimensionality\". To overcome this difficulty, we develop a deep learning\napproach that directly solves high-dimensional stochastic control problems\nbased on Monte-Carlo sampling. We approximate the time-dependent controls as\nfeedforward neural networks and stack these networks together through model\ndynamics. The objective function for the control problem plays the role of the\nloss function for the deep neural network. We test this approach using examples\nfrom the areas of optimal trading and energy storage. Our results suggest that\nthe algorithm presented here achieves satisfactory accuracy and at the same\ntime, can handle rather high dimensional problems.\n", "versions": [{"version": "v1", "created": "Wed, 2 Nov 2016 02:47:26 GMT"}], "update_date": "2016-11-23", "authors_parsed": [["Han", "Jiequn", ""], ["E", "Weinan", ""]]}, {"id": "1611.07429", "submitter": "Jayaraman J. Thiagarajan", "authors": "Jayaraman J. Thiagarajan, Bhavya Kailkhura, Prasanna Sattigeri and\n  Karthikeyan Natesan Ramamurthy", "title": "TreeView: Peeking into Deep Neural Networks Via Feature-Space\n  Partitioning", "comments": "Presented at NIPS 2016 Workshop on Interpretable Machine Learning in\n  Complex Systems", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  With the advent of highly predictive but opaque deep learning models, it has\nbecome more important than ever to understand and explain the predictions of\nsuch models. Existing approaches define interpretability as the inverse of\ncomplexity and achieve interpretability at the cost of accuracy. This\nintroduces a risk of producing interpretable but misleading explanations. As\nhumans, we are prone to engage in this kind of behavior \\cite{mythos}. In this\npaper, we take a step in the direction of tackling the problem of\ninterpretability without compromising the model accuracy. We propose to build a\nTreeview representation of the complex model via hierarchical partitioning of\nthe feature space, which reveals the iterative rejection of unlikely class\nlabels until the correct association is predicted.\n", "versions": [{"version": "v1", "created": "Tue, 22 Nov 2016 17:32:59 GMT"}], "update_date": "2016-11-23", "authors_parsed": [["Thiagarajan", "Jayaraman J.", ""], ["Kailkhura", "Bhavya", ""], ["Sattigeri", "Prasanna", ""], ["Ramamurthy", "Karthikeyan Natesan", ""]]}, {"id": "1611.07443", "submitter": "Corey Hudson", "authors": "Leanne S. Whitmore, Anthe George, Corey M. Hudson", "title": "Mapping chemical performance on molecular structures using locally\n  interpretable explanations", "comments": "Presented at NIPS 2016 Workshop on Interpretable Machine Learning in\n  Complex Systems", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML physics.chem-ph", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  In this work, we present an application of Locally Interpretable\nMachine-Agnostic Explanations to 2-D chemical structures. Using this framework\nwe are able to provide a structural interpretation for an existing black-box\nmodel for classifying biologically produced fuel compounds with regard to\nResearch Octane Number. This method of \"painting\" locally interpretable\nexplanations onto 2-D chemical structures replicates the chemical intuition of\nsynthetic chemists, allowing researchers in the field to directly accept,\nreject, inform and evaluate decisions underlying inscrutably complex\nquantitative structure-activity relationship models.\n", "versions": [{"version": "v1", "created": "Tue, 22 Nov 2016 18:14:45 GMT"}], "update_date": "2016-11-23", "authors_parsed": [["Whitmore", "Leanne S.", ""], ["George", "Anthe", ""], ["Hudson", "Corey M.", ""]]}, {"id": "1611.07450", "submitter": "Michael Cogswell", "authors": "Ramprasaath R Selvaraju, Abhishek Das, Ramakrishna Vedantam, Michael\n  Cogswell, Devi Parikh, Dhruv Batra", "title": "Grad-CAM: Why did you say that?", "comments": "Presented at NIPS 2016 Workshop on Interpretable Machine Learning in\n  Complex Systems. This is an extended abstract version of arXiv:1610.02391\n  (CVPR format)", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a technique for making Convolutional Neural Network (CNN)-based\nmodels more transparent by visualizing input regions that are 'important' for\npredictions -- or visual explanations. Our approach, called Gradient-weighted\nClass Activation Mapping (Grad-CAM), uses class-specific gradient information\nto localize important regions. These localizations are combined with existing\npixel-space visualizations to create a novel high-resolution and\nclass-discriminative visualization called Guided Grad-CAM. These methods help\nbetter understand CNN-based models, including image captioning and visual\nquestion answering (VQA) models. We evaluate our visual explanations by\nmeasuring their ability to discriminate between classes, to inspire trust in\nhumans, and their correlation with occlusion maps. Grad-CAM provides a new way\nto understand CNN-based models.\n  We have released code, an online demo hosted on CloudCV, and a full version\nof this extended abstract.\n", "versions": [{"version": "v1", "created": "Tue, 22 Nov 2016 18:34:36 GMT"}, {"version": "v2", "created": "Wed, 25 Jan 2017 16:33:29 GMT"}], "update_date": "2017-01-26", "authors_parsed": [["Selvaraju", "Ramprasaath R", ""], ["Das", "Abhishek", ""], ["Vedantam", "Ramakrishna", ""], ["Cogswell", "Michael", ""], ["Parikh", "Devi", ""], ["Batra", "Dhruv", ""]]}, {"id": "1611.07460", "submitter": "Valerio Perrone", "authors": "Valerio Perrone, Paul A. Jenkins, Dario Spano, Yee Whye Teh", "title": "Poisson Random Fields for Dynamic Feature Models", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present the Wright-Fisher Indian buffet process (WF-IBP), a probabilistic\nmodel for time-dependent data assumed to have been generated by an unknown\nnumber of latent features. This model is suitable as a prior in Bayesian\nnonparametric feature allocation models in which the features underlying the\nobserved data exhibit a dependency structure over time. More specifically, we\nestablish a new framework for generating dependent Indian buffet processes,\nwhere the Poisson random field model from population genetics is used as a way\nof constructing dependent beta processes. Inference in the model is complex,\nand we describe a sophisticated Markov Chain Monte Carlo algorithm for exact\nposterior simulation. We apply our construction to develop a nonparametric\nfocused topic model for collections of time-stamped text documents and test it\non the full corpus of NIPS papers published from 1987 to 2015.\n", "versions": [{"version": "v1", "created": "Tue, 22 Nov 2016 18:53:32 GMT"}], "update_date": "2016-11-23", "authors_parsed": [["Perrone", "Valerio", ""], ["Jenkins", "Paul A.", ""], ["Spano", "Dario", ""], ["Teh", "Yee Whye", ""]]}, {"id": "1611.07492", "submitter": "Siddharth Narayanaswamy", "authors": "N. Siddharth and Brooks Paige and Alban Desmaison and Jan-Willem Van\n  de Meent and Frank Wood and Noah D. Goodman and Pushmeet Kohli and Philip\n  H.S. Torr", "title": "Inducing Interpretable Representations with Variational Autoencoders", "comments": "Presented at NIPS 2016 Workshop on Interpretable Machine Learning in\n  Complex Systems", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We develop a framework for incorporating structured graphical models in the\n\\emph{encoders} of variational autoencoders (VAEs) that allows us to induce\ninterpretable representations through approximate variational inference. This\nallows us to both perform reasoning (e.g. classification) under the structural\nconstraints of a given graphical model, and use deep generative models to deal\nwith messy, high-dimensional domains where it is often difficult to model all\nthe variation. Learning in this framework is carried out end-to-end with a\nvariational objective, applying to both unsupervised and semi-supervised\nschemes.\n", "versions": [{"version": "v1", "created": "Tue, 22 Nov 2016 20:04:59 GMT"}], "update_date": "2016-11-23", "authors_parsed": [["Siddharth", "N.", ""], ["Paige", "Brooks", ""], ["Desmaison", "Alban", ""], ["Van de Meent", "Jan-Willem", ""], ["Wood", "Frank", ""], ["Goodman", "Noah D.", ""], ["Kohli", "Pushmeet", ""], ["Torr", "Philip H. S.", ""]]}, {"id": "1611.07555", "submitter": "Peter Richtarik", "authors": "Jakub Kone\\v{c}n\\'y and Peter Richt\\'arik", "title": "Randomized Distributed Mean Estimation: Accuracy vs Communication", "comments": "19 pages, 1 figure", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC math.NA stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider the problem of estimating the arithmetic average of a finite\ncollection of real vectors stored in a distributed fashion across several\ncompute nodes subject to a communication budget constraint. Our analysis does\nnot rely on any statistical assumptions about the source of the vectors. This\nproblem arises as a subproblem in many applications, including reduce-all\noperations within algorithms for distributed and federated optimization and\nlearning. We propose a flexible family of randomized algorithms exploring the\ntrade-off between expected communication cost and estimation error. Our family\ncontains the full-communication and zero-error method on one extreme, and an\n$\\epsilon$-bit communication and ${\\cal O}\\left(1/(\\epsilon n)\\right)$ error\nmethod on the opposite extreme. In the special case where we communicate, in\nexpectation, a single bit per coordinate of each vector, we improve upon\nexisting results by obtaining $\\mathcal{O}(r/n)$ error, where $r$ is the number\nof bits used to represent a floating point value.\n", "versions": [{"version": "v1", "created": "Tue, 22 Nov 2016 22:18:36 GMT"}], "update_date": "2016-11-24", "authors_parsed": [["Kone\u010dn\u00fd", "Jakub", ""], ["Richt\u00e1rik", "Peter", ""]]}, {"id": "1611.07567", "submitter": "Marina Vidovic", "authors": "Marina M.-C. Vidovic, Nico G\\\"ornitz, Klaus-Robert M\\\"uller, Marius\n  Kloft", "title": "Feature Importance Measure for Non-linear Learning Algorithms", "comments": "Presented at NIPS 2016 Workshop on Interpretable Machine Learning in\n  Complex Systems", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Complex problems may require sophisticated, non-linear learning methods such\nas kernel machines or deep neural networks to achieve state of the art\nprediction accuracies. However, high prediction accuracies are not the only\nobjective to consider when solving problems using machine learning. Instead,\nparticular scientific applications require some explanation of the learned\nprediction function. Unfortunately, most methods do not come with out of the\nbox straight forward interpretation. Even linear prediction functions are not\nstraight forward to explain if features exhibit complex correlation structure.\n  In this paper, we propose the Measure of Feature Importance (MFI). MFI is\ngeneral and can be applied to any arbitrary learning machine (including kernel\nmachines and deep learning). MFI is intrinsically non-linear and can detect\nfeatures that by itself are inconspicuous and only impact the prediction\nfunction through their interaction with other features. Lastly, MFI can be used\nfor both --- model-based feature importance and instance-based feature\nimportance (i.e, measuring the importance of a feature for a particular data\npoint).\n", "versions": [{"version": "v1", "created": "Tue, 22 Nov 2016 22:36:31 GMT"}], "update_date": "2016-11-24", "authors_parsed": [["Vidovic", "Marina M. -C.", ""], ["G\u00f6rnitz", "Nico", ""], ["M\u00fcller", "Klaus-Robert", ""], ["Kloft", "Marius", ""]]}, {"id": "1611.07579", "submitter": "Sameer Singh", "authors": "Sameer Singh and Marco Tulio Ribeiro and Carlos Guestrin", "title": "Programs as Black-Box Explanations", "comments": "Presented at NIPS 2016 Workshop on Interpretable Machine Learning in\n  Complex Systems", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.AI cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recent work in model-agnostic explanations of black-box machine learning has\ndemonstrated that interpretability of complex models does not have to come at\nthe cost of accuracy or model flexibility. However, it is not clear what kind\nof explanations, such as linear models, decision trees, and rule lists, are the\nappropriate family to consider, and different tasks and models may benefit from\ndifferent kinds of explanations. Instead of picking a single family of\nrepresentations, in this work we propose to use \"programs\" as model-agnostic\nexplanations. We show that small programs can be expressive yet intuitive as\nexplanations, and generalize over a number of existing interpretable families.\nWe propose a prototype program induction method based on simulated annealing\nthat approximates the local behavior of black-box classifiers around a specific\nprediction using random perturbations. Finally, we present preliminary\napplication on small datasets and show that the generated explanations are\nintuitive and accurate for a number of classifiers.\n", "versions": [{"version": "v1", "created": "Tue, 22 Nov 2016 23:35:03 GMT"}], "update_date": "2016-11-24", "authors_parsed": [["Singh", "Sameer", ""], ["Ribeiro", "Marco Tulio", ""], ["Guestrin", "Carlos", ""]]}, {"id": "1611.07588", "submitter": "Ashkan Zeinalzadeh", "authors": "Ashkan Zeinalzadeh, Tom Wenska, Gordon Okimoto", "title": "A Neural Network Model to Classify Liver Cancer Patients Using Data\n  Expansion and Compression", "comments": "5 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG q-bio.QM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We develop a neural network model to classify liver cancer patients into\nhigh-risk and low-risk groups using genomic data. Our approach provides a novel\ntechnique to classify big data sets using neural network models. We preprocess\nthe data before training the neural network models. We first expand the data\nusing wavelet analysis. We then compress the wavelet coefficients by mapping\nthem onto a new scaled orthonormal coordinate system. Then the data is used to\ntrain a neural network model that enables us to classify cancer patients into\ntwo different classes of high-risk and low-risk patients. We use the\nleave-one-out approach to build a neural network model. This neural network\nmodel enables us to classify a patient using genomic data as a high-risk or\nlow-risk patient without any information about the survival time of the\npatient. The results from genomic data analysis are compared with survival time\nanalysis. It is shown that the expansion and compression of data using wavelet\nanalysis and singular value decomposition (SVD) is essential to train the\nneural network model.\n", "versions": [{"version": "v1", "created": "Wed, 23 Nov 2016 00:11:41 GMT"}, {"version": "v2", "created": "Fri, 25 Nov 2016 18:02:21 GMT"}], "update_date": "2016-11-28", "authors_parsed": [["Zeinalzadeh", "Ashkan", ""], ["Wenska", "Tom", ""], ["Okimoto", "Gordon", ""]]}, {"id": "1611.07609", "submitter": "Tianbao Yang", "authors": "Mingrui Liu, Tianbao Yang", "title": "Adaptive Accelerated Gradient Converging Methods under Holderian Error\n  Bound Condition", "comments": "added results for \\theta>1/2 and some experimental results", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.OC stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recent studies have shown that proximal gradient (PG) method and accelerated\ngradient method (APG) with restarting can enjoy a linear convergence under a\nweaker condition than strong convexity, namely a quadratic growth condition\n(QGC). However, the faster convergence of restarting APG method relies on the\npotentially unknown constant in QGC to appropriately restart APG, which\nrestricts its applicability. We address this issue by developing a novel\nadaptive gradient converging methods, i.e., leveraging the magnitude of\nproximal gradient as a criterion for restart and termination. Our analysis\nextends to a much more general condition beyond the QGC, namely the\nH\\\"{o}lderian error bound (HEB) condition. {\\it The key technique} for our\ndevelopment is a novel synthesis of {\\it adaptive regularization and a\nconditional restarting scheme}, which extends previous work focusing on\nstrongly convex problems to a much broader family of problems. Furthermore, we\ndemonstrate that our results have important implication and applications in\nmachine learning: (i) if the objective function is coercive and semi-algebraic,\nPG's convergence speed is essentially $o(\\frac{1}{t})$, where $t$ is the total\nnumber of iterations; (ii) if the objective function consists of an $\\ell_1$,\n$\\ell_\\infty$, $\\ell_{1,\\infty}$, or huber norm regularization and a convex\nsmooth piecewise quadratic loss (e.g., squares loss, squared hinge loss and\nhuber loss), the proposed algorithm is parameter-free and enjoys a {\\it faster\nlinear convergence} than PG without any other assumptions (e.g., restricted\neigen-value condition). It is notable that our linear convergence results for\nthe aforementioned problems are global instead of local. To the best of our\nknowledge, these improved results are the first shown in this work.\n", "versions": [{"version": "v1", "created": "Wed, 23 Nov 2016 02:36:00 GMT"}, {"version": "v2", "created": "Sun, 14 May 2017 16:53:44 GMT"}], "update_date": "2017-05-16", "authors_parsed": [["Liu", "Mingrui", ""], ["Yang", "Tianbao", ""]]}, {"id": "1611.07634", "submitter": "Yotam Hechtlinger", "authors": "Yotam Hechtlinger", "title": "Interpretation of Prediction Models Using the Input Gradient", "comments": "Presented at NIPS 2016 Workshop on Interpretable Machine Learning in\n  Complex Systems", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  State of the art machine learning algorithms are highly optimized to provide\nthe optimal prediction possible, naturally resulting in complex models. While\nthese models often outperform simpler more interpretable models by order of\nmagnitudes, in terms of understanding the way the model functions, we are often\nfacing a \"black box\".\n  In this paper we suggest a simple method to interpret the behavior of any\npredictive model, both for regression and classification. Given a particular\nmodel, the information required to interpret it can be obtained by studying the\npartial derivatives of the model with respect to the input. We exemplify this\ninsight by interpreting convolutional and multi-layer neural networks in the\nfield of natural language processing.\n", "versions": [{"version": "v1", "created": "Wed, 23 Nov 2016 04:21:51 GMT"}], "update_date": "2016-11-24", "authors_parsed": [["Hechtlinger", "Yotam", ""]]}, {"id": "1611.07663", "submitter": "Himabindu Lakkaraju", "authors": "Himabindu Lakkaraju, Cynthia Rudin", "title": "Learning Cost-Effective and Interpretable Regimes for Treatment\n  Recommendation", "comments": "Presented at NIPS 2016 Workshop on Interpretable Machine Learning in\n  Complex Systems", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Decision makers, such as doctors and judges, make crucial decisions such as\nrecommending treatments to patients, and granting bails to defendants on a\ndaily basis. Such decisions typically involve weighting the potential benefits\nof taking an action against the costs involved. In this work, we aim to\nautomate this task of learning {cost-effective, interpretable and actionable\ntreatment regimes. We formulate this as a problem of learning a decision list\n-- a sequence of if-then-else rules -- which maps characteristics of subjects\n(eg., diagnostic test results of patients) to treatments. We propose a novel\nobjective to construct a decision list which maximizes outcomes for the\npopulation, and minimizes overall costs. We model the problem of learning such\na list as a Markov Decision Process (MDP) and employ a variant of the Upper\nConfidence Bound for Trees (UCT) strategy which leverages customized checks for\npruning the search space effectively. Experimental results on real world\nobservational data capturing treatment recommendations for asthma patients\ndemonstrate the effectiveness of our approach.\n", "versions": [{"version": "v1", "created": "Wed, 23 Nov 2016 07:09:04 GMT"}], "update_date": "2016-11-24", "authors_parsed": [["Lakkaraju", "Himabindu", ""], ["Rudin", "Cynthia", ""]]}, {"id": "1611.07710", "submitter": "Ali Zarezade", "authors": "Ali Zarezade, Sina Jafarzadeh, Hamid R. Rabiee", "title": "Spatio-Temporal Modeling of Users' Check-ins in Location-Based Social\n  Networks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.SI stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Social networks are getting closer to our real physical world. People share\nthe exact location and time of their check-ins and are influenced by their\nfriends. Modeling the spatio-temporal behavior of users in social networks is\nof great importance for predicting the future behavior of users, controlling\nthe users' movements, and finding the latent influence network. It is observed\nthat users have periodic patterns in their movements. Also, they are influenced\nby the locations that their close friends recently visited. Leveraging these\ntwo observations, we propose a probabilistic model based on a doubly stochastic\npoint process with a periodic decaying kernel for the time of check-ins and a\ntime-varying multinomial distribution for the location of check-ins of users in\nthe location-based social networks. We learn the model parameters using an\nefficient EM algorithm, which distributes over the users. Experiments on\nsynthetic and real data gathered from Foursquare show that the proposed\ninference algorithm learns the parameters efficiently and our model outperforms\nthe other alternatives in the prediction of time and location of check-ins.\n", "versions": [{"version": "v1", "created": "Wed, 23 Nov 2016 09:57:04 GMT"}, {"version": "v2", "created": "Mon, 10 Apr 2017 11:32:36 GMT"}], "update_date": "2017-04-11", "authors_parsed": [["Zarezade", "Ali", ""], ["Jafarzadeh", "Sina", ""], ["Rabiee", "Hamid R.", ""]]}, {"id": "1611.07725", "submitter": "Christoph H. Lampert", "authors": "Sylvestre-Alvise Rebuffi, Alexander Kolesnikov, Georg Sperl, Christoph\n  H. Lampert", "title": "iCaRL: Incremental Classifier and Representation Learning", "comments": "Accepted paper at CVPR 2017", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A major open problem on the road to artificial intelligence is the\ndevelopment of incrementally learning systems that learn about more and more\nconcepts over time from a stream of data. In this work, we introduce a new\ntraining strategy, iCaRL, that allows learning in such a class-incremental way:\nonly the training data for a small number of classes has to be present at the\nsame time and new classes can be added progressively. iCaRL learns strong\nclassifiers and a data representation simultaneously. This distinguishes it\nfrom earlier works that were fundamentally limited to fixed data\nrepresentations and therefore incompatible with deep learning architectures. We\nshow by experiments on CIFAR-100 and ImageNet ILSVRC 2012 data that iCaRL can\nlearn many classes incrementally over a long period of time where other\nstrategies quickly fail.\n", "versions": [{"version": "v1", "created": "Wed, 23 Nov 2016 10:24:11 GMT"}, {"version": "v2", "created": "Fri, 14 Apr 2017 16:41:02 GMT"}], "update_date": "2017-04-17", "authors_parsed": [["Rebuffi", "Sylvestre-Alvise", ""], ["Kolesnikov", "Alexander", ""], ["Sperl", "Georg", ""], ["Lampert", "Christoph H.", ""]]}, {"id": "1611.07743", "submitter": "Gil Keren", "authors": "Gil Keren, Sivan Sabato, Bj\\\"orn Schuller", "title": "Tunable Sensitivity to Large Errors in Neural Network Training", "comments": "The paper is accepted to the AAAI 2017 conference", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  When humans learn a new concept, they might ignore examples that they cannot\nmake sense of at first, and only later focus on such examples, when they are\nmore useful for learning. We propose incorporating this idea of tunable\nsensitivity for hard examples in neural network learning, using a new\ngeneralization of the cross-entropy gradient step, which can be used in place\nof the gradient in any gradient-based training method. The generalized gradient\nis parameterized by a value that controls the sensitivity of the training\nprocess to harder training examples. We tested our method on several benchmark\ndatasets. We propose, and corroborate in our experiments, that the optimal\nlevel of sensitivity to hard example is positively correlated with the depth of\nthe network. Moreover, the test prediction error obtained by our method is\ngenerally lower than that of the vanilla cross-entropy gradient learner. We\ntherefore conclude that tunable sensitivity can be helpful for neural network\nlearning.\n", "versions": [{"version": "v1", "created": "Wed, 23 Nov 2016 11:14:01 GMT"}], "update_date": "2016-11-24", "authors_parsed": [["Keren", "Gil", ""], ["Sabato", "Sivan", ""], ["Schuller", "Bj\u00f6rn", ""]]}, {"id": "1611.07800", "submitter": "Ehsan Abbasnejad M", "authors": "Ehsan Abbasnejad, Anthony Dick, Anton van den Hengel", "title": "Infinite Variational Autoencoder for Semi-Supervised Learning", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper presents an infinite variational autoencoder (VAE) whose capacity\nadapts to suit the input data. This is achieved using a mixture model where the\nmixing coefficients are modeled by a Dirichlet process, allowing us to\nintegrate over the coefficients when performing inference. Critically, this\nthen allows us to automatically vary the number of autoencoders in the mixture\nbased on the data. Experiments show the flexibility of our method, particularly\nfor semi-supervised learning, where only a small number of training samples are\navailable.\n", "versions": [{"version": "v1", "created": "Wed, 23 Nov 2016 13:59:57 GMT"}, {"version": "v2", "created": "Thu, 24 Nov 2016 01:28:08 GMT"}], "update_date": "2016-11-28", "authors_parsed": [["Abbasnejad", "Ehsan", ""], ["Dick", "Anthony", ""], ["Hengel", "Anton van den", ""]]}, {"id": "1611.07850", "submitter": "Randall Balestriero", "authors": "Randall Balestriero, Behnaam Aazhang", "title": "Robust Unsupervised Transient Detection With Invariant Representation\n  based on the Scattering Network", "comments": "10 pages + 1 reference page", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML stat.AP", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  We present a sparse and invariant representation with low asymptotic\ncomplexity for robust unsupervised transient and onset zone detection in noisy\nenvironments. This unsupervised approach is based on wavelet transforms and\nleverages the scattering network from Mallat et al. by deriving frequency\ninvariance. This frequency invariance is a key concept to enforce robust\nrepresentations of transients in presence of possible frequency shifts and\nperturbations occurring in the original signal. Implementation details as well\nas complexity analysis are provided in addition of the theoretical framework\nand the invariance properties. In this work, our primary application consists\nof predicting the onset of seizure in epileptic patients from subdural\nrecordings as well as detecting inter-ictal spikes.\n", "versions": [{"version": "v1", "created": "Wed, 23 Nov 2016 15:54:00 GMT"}], "update_date": "2016-11-24", "authors_parsed": [["Balestriero", "Randall", ""], ["Aazhang", "Behnaam", ""]]}, {"id": "1611.07873", "submitter": "Paul Fearnhead", "authors": "Paul Fearnhead, Joris Bierkens, Murray Pollock and Gareth O Roberts", "title": "Piecewise Deterministic Markov Processes for Continuous-Time Monte Carlo", "comments": null, "journal-ref": "Statist. Sci., Volume 33, Number 3 (2018), 386-412", "doi": "10.1214/18-STS648", "report-no": null, "categories": "stat.CO math.ST stat.ME stat.ML stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recently there have been exciting developments in Monte Carlo methods, with\nthe development of new MCMC and sequential Monte Carlo (SMC) algorithms which\nare based on continuous-time, rather than discrete-time, Markov processes. This\nhas led to some fundamentally new Monte Carlo algorithms which can be used to\nsample from, say, a posterior distribution. Interestingly, continuous-time\nalgorithms seem particularly well suited to Bayesian analysis in big-data\nsettings as they need only access a small sub-set of data points at each\niteration, and yet are still guaranteed to target the true posterior\ndistribution. Whilst continuous-time MCMC and SMC methods have been developed\nindependently we show here that they are related by the fact that both involve\nsimulating a piecewise deterministic Markov process. Furthermore we show that\nthe methods developed to date are just specific cases of a potentially much\nwider class of continuous-time Monte Carlo algorithms. We give an informal\nintroduction to piecewise deterministic Markov processes, covering the aspects\nrelevant to these new Monte Carlo algorithms, with a view to making the\ndevelopment of new continuous-time Monte Carlo more accessible. We focus on how\nand why sub-sampling ideas can be used with these algorithms, and aim to give\ninsight into how these new algorithms can be implemented, and what are some of\nthe issues that affect their efficiency.\n", "versions": [{"version": "v1", "created": "Wed, 23 Nov 2016 16:42:29 GMT"}], "update_date": "2020-09-29", "authors_parsed": [["Fearnhead", "Paul", ""], ["Bierkens", "Joris", ""], ["Pollock", "Murray", ""], ["Roberts", "Gareth O", ""]]}, {"id": "1611.08024", "submitter": "Vernon Lawhern", "authors": "Vernon J. Lawhern, Amelia J. Solon, Nicholas R. Waytowich, Stephen M.\n  Gordon, Chou P. Hung, Brent J. Lance", "title": "EEGNet: A Compact Convolutional Network for EEG-based Brain-Computer\n  Interfaces", "comments": "30 pages, 10 figures. Added additional feature relevance analyses.\n  Minor change to EEGNet architecture. Source code can be found at\n  https://github.com/vlawhern/arl-eegmodels", "journal-ref": null, "doi": "10.1088/1741-2552/aace8c", "report-no": null, "categories": "cs.LG q-bio.NC stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Brain computer interfaces (BCI) enable direct communication with a computer,\nusing neural activity as the control signal. This neural signal is generally\nchosen from a variety of well-studied electroencephalogram (EEG) signals. For a\ngiven BCI paradigm, feature extractors and classifiers are tailored to the\ndistinct characteristics of its expected EEG control signal, limiting its\napplication to that specific signal. Convolutional Neural Networks (CNNs),\nwhich have been used in computer vision and speech recognition, have\nsuccessfully been applied to EEG-based BCIs; however, they have mainly been\napplied to single BCI paradigms and thus it remains unclear how these\narchitectures generalize to other paradigms. Here, we ask if we can design a\nsingle CNN architecture to accurately classify EEG signals from different BCI\nparadigms, while simultaneously being as compact as possible. In this work we\nintroduce EEGNet, a compact convolutional network for EEG-based BCIs. We\nintroduce the use of depthwise and separable convolutions to construct an\nEEG-specific model which encapsulates well-known EEG feature extraction\nconcepts for BCI. We compare EEGNet to current state-of-the-art approaches\nacross four BCI paradigms: P300 visual-evoked potentials, error-related\nnegativity responses (ERN), movement-related cortical potentials (MRCP), and\nsensory motor rhythms (SMR). We show that EEGNet generalizes across paradigms\nbetter than the reference algorithms when only limited training data is\navailable. We demonstrate three different approaches to visualize the contents\nof a trained EEGNet model to enable interpretation of the learned features. Our\nresults suggest that EEGNet is robust enough to learn a wide variety of\ninterpretable features over a range of BCI tasks, suggesting that the observed\nperformances were not due to artifact or noise sources in the data.\n", "versions": [{"version": "v1", "created": "Wed, 23 Nov 2016 22:36:58 GMT"}, {"version": "v2", "created": "Tue, 9 May 2017 16:03:13 GMT"}, {"version": "v3", "created": "Fri, 9 Mar 2018 01:02:21 GMT"}, {"version": "v4", "created": "Wed, 16 May 2018 01:14:34 GMT"}], "update_date": "2018-06-28", "authors_parsed": [["Lawhern", "Vernon J.", ""], ["Solon", "Amelia J.", ""], ["Waytowich", "Nicholas R.", ""], ["Gordon", "Stephen M.", ""], ["Hung", "Chou P.", ""], ["Lance", "Brent J.", ""]]}, {"id": "1611.08083", "submitter": "Maithra Raghu", "authors": "Maithra Raghu, Ben Poole, Jon Kleinberg, Surya Ganguli, Jascha\n  Sohl-Dickstein", "title": "Survey of Expressivity in Deep Neural Networks", "comments": "Presented at NIPS 2016 Workshop on Interpretable Machine Learning in\n  Complex Systems", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We survey results on neural network expressivity described in \"On the\nExpressive Power of Deep Neural Networks\". The paper motivates and develops\nthree natural measures of expressiveness, which all display an exponential\ndependence on the depth of the network. In fact, all of these measures are\nrelated to a fourth quantity, trajectory length. This quantity grows\nexponentially in the depth of the network, and is responsible for the depth\nsensitivity observed. These results translate to consequences for networks\nduring and after training. They suggest that parameters earlier in a network\nhave greater influence on its expressive power -- in particular, given a layer,\nits influence on expressivity is determined by the remaining depth of the\nnetwork after that layer. This is verified with experiments on MNIST and\nCIFAR-10. We also explore the effect of training on the input-output map, and\nfind that it trades off between the stability and expressivity.\n", "versions": [{"version": "v1", "created": "Thu, 24 Nov 2016 07:09:24 GMT"}], "update_date": "2016-11-28", "authors_parsed": [["Raghu", "Maithra", ""], ["Poole", "Ben", ""], ["Kleinberg", "Jon", ""], ["Ganguli", "Surya", ""], ["Sohl-Dickstein", "Jascha", ""]]}, {"id": "1611.08104", "submitter": "Peter Wittek", "authors": "Peter Wittek, Christian Gogolin", "title": "Quantum Enhanced Inference in Markov Logic Networks", "comments": "8 pages, 1 figure", "journal-ref": "Scientific Reports 7, 45672 (2017)", "doi": "10.1038/srep45672", "report-no": null, "categories": "stat.ML cs.AI quant-ph", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Markov logic networks (MLNs) reconcile two opposing schools in machine\nlearning and artificial intelligence: causal networks, which account for\nuncertainty extremely well, and first-order logic, which allows for formal\ndeduction. An MLN is essentially a first-order logic template to generate\nMarkov networks. Inference in MLNs is probabilistic and it is often performed\nby approximate methods such as Markov chain Monte Carlo (MCMC) Gibbs sampling.\nAn MLN has many regular, symmetric structures that can be exploited at both\nfirst-order level and in the generated Markov network. We analyze the graph\nstructures that are produced by various lifting methods and investigate the\nextent to which quantum protocols can be used to speed up Gibbs sampling with\nstate preparation and measurement schemes. We review different such approaches,\ndiscuss their advantages, theoretical limitations, and their appeal to\nimplementations. We find that a straightforward application of a recent result\nyields exponential speedup compared to classical heuristics in approximate\nprobabilistic inference, thereby demonstrating another example where advanced\nquantum resources can potentially prove useful in machine learning.\n", "versions": [{"version": "v1", "created": "Thu, 24 Nov 2016 09:07:22 GMT"}], "update_date": "2017-04-20", "authors_parsed": [["Wittek", "Peter", ""], ["Gogolin", "Christian", ""]]}, {"id": "1611.08191", "submitter": "Wojciech Samek", "authors": "Wojciech Samek, Gr\\'egoire Montavon, Alexander Binder, Sebastian\n  Lapuschkin, Klaus-Robert M\\\"uller", "title": "Interpreting the Predictions of Complex ML Models by Layer-wise\n  Relevance Propagation", "comments": "Presented at NIPS 2016 Workshop on Interpretable Machine Learning in\n  Complex Systems", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Complex nonlinear models such as deep neural network (DNNs) have become an\nimportant tool for image classification, speech recognition, natural language\nprocessing, and many other fields of application. These models however lack\ntransparency due to their complex nonlinear structure and to the complex data\ndistributions to which they typically apply. As a result, it is difficult to\nfully characterize what makes these models reach a particular decision for a\ngiven input. This lack of transparency can be a drawback, especially in the\ncontext of sensitive applications such as medical analysis or security. In this\nshort paper, we summarize a recent technique introduced by Bach et al. [1] that\nexplains predictions by decomposing the classification decision of DNN models\nin terms of input variables.\n", "versions": [{"version": "v1", "created": "Thu, 24 Nov 2016 14:16:12 GMT"}], "update_date": "2016-11-28", "authors_parsed": [["Samek", "Wojciech", ""], ["Montavon", "Gr\u00e9goire", ""], ["Binder", "Alexander", ""], ["Lapuschkin", "Sebastian", ""], ["M\u00fcller", "Klaus-Robert", ""]]}, {"id": "1611.08207", "submitter": "Nikolay Jetchev", "authors": "Nikolay Jetchev, Urs Bergmann, Roland Vollgraf", "title": "Texture Synthesis with Spatial Generative Adversarial Networks", "comments": "presented at the NIPS 2016 adversarial learning workshop, Barcelona,\n  Spain", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Generative adversarial networks (GANs) are a recent approach to train\ngenerative models of data, which have been shown to work particularly well on\nimage data. In the current paper we introduce a new model for texture synthesis\nbased on GAN learning. By extending the input noise distribution space from a\nsingle vector to a whole spatial tensor, we create an architecture with\nproperties well suited to the task of texture synthesis, which we call spatial\nGAN (SGAN). To our knowledge, this is the first successful completely\ndata-driven texture synthesis method based on GANs.\n  Our method has the following features which make it a state of the art\nalgorithm for texture synthesis: high image quality of the generated textures,\nvery high scalability w.r.t. the output texture size, fast real-time forward\ngeneration, the ability to fuse multiple diverse source images in complex\ntextures. To illustrate these capabilities we present multiple experiments with\ndifferent classes of texture images and use cases. We also discuss some\nlimitations of our method with respect to the types of texture images it can\nsynthesize, and compare it to other neural techniques for texture generation.\n", "versions": [{"version": "v1", "created": "Thu, 24 Nov 2016 15:01:42 GMT"}, {"version": "v2", "created": "Thu, 1 Dec 2016 11:32:11 GMT"}, {"version": "v3", "created": "Thu, 18 May 2017 13:42:04 GMT"}, {"version": "v4", "created": "Fri, 8 Sep 2017 15:09:52 GMT"}], "update_date": "2017-09-11", "authors_parsed": [["Jetchev", "Nikolay", ""], ["Bergmann", "Urs", ""], ["Vollgraf", "Roland", ""]]}, {"id": "1611.08229", "submitter": "Cristian Rusu", "authors": "Cristian Rusu, Nuria Gonzalez-Prelcic, Robert Heath", "title": "Fast Orthonormal Sparsifying Transforms Based on Householder Reflectors", "comments": null, "journal-ref": null, "doi": "10.1109/TSP.2016.2612168", "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Dictionary learning is the task of determining a data-dependent transform\nthat yields a sparse representation of some observed data. The dictionary\nlearning problem is non-convex, and usually solved via computationally complex\niterative algorithms. Furthermore, the resulting transforms obtained generally\nlack structure that permits their fast application to data. To address this\nissue, this paper develops a framework for learning orthonormal dictionaries\nwhich are built from products of a few Householder reflectors. Two algorithms\nare proposed to learn the reflector coefficients: one that considers a\nsequential update of the reflectors and one with a simultaneous update of all\nreflectors that imposes an additional internal orthogonal constraint. The\nproposed methods have low computational complexity and are shown to converge to\nlocal minimum points which can be described in terms of the spectral properties\nof the matrices involved. The resulting dictionaries balance between the\ncomputational complexity and the quality of the sparse representations by\ncontrolling the number of Householder reflectors in their product. Simulations\nof the proposed algorithms are shown in the image processing setting where\nwell-known fast transforms are available for comparisons. The proposed\nalgorithms have favorable reconstruction error and the advantage of a fast\nimplementation relative to the classical, unstructured, dictionaries.\n", "versions": [{"version": "v1", "created": "Thu, 24 Nov 2016 15:53:59 GMT"}], "update_date": "2016-11-30", "authors_parsed": [["Rusu", "Cristian", ""], ["Gonzalez-Prelcic", "Nuria", ""], ["Heath", "Robert", ""]]}, {"id": "1611.08256", "submitter": "Pietro Vischia", "authors": "Pietro Vischia and Tommaso Dorigo", "title": "The Inverse Bagging Algorithm: Anomaly Detection by Inverse Bootstrap\n  Aggregating", "comments": "8 pages, 5 figures. Proceedings of the XIIth Quark Confinement and\n  Hadron Spectrum conference, 28/8-2/9 2016, Thessaloniki, Greece", "journal-ref": null, "doi": "10.1051/epjconf/201713711009", "report-no": null, "categories": "stat.ML hep-ex physics.data-an", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  For data sets populated by a very well modeled process and by another process\nof unknown probability density function (PDF), a desired feature when\nmanipulating the fraction of the unknown process (either for enhancing it or\nsuppressing it) consists in avoiding to modify the kinematic distributions of\nthe well modeled one. A bootstrap technique is used to identify sub-samples\nrich in the well modeled process, and classify each event according to the\nfrequency of it being part of such sub-samples. Comparisons with general MVA\nalgorithms will be shown, as well as a study of the asymptotic properties of\nthe method, making use of a public domain data set that models a typical search\nfor new physics as performed at hadronic colliders such as the Large Hadron\nCollider (LHC).\n", "versions": [{"version": "v1", "created": "Thu, 24 Nov 2016 17:00:12 GMT"}], "update_date": "2017-04-05", "authors_parsed": [["Vischia", "Pietro", ""], ["Dorigo", "Tommaso", ""]]}, {"id": "1611.08292", "submitter": "Zhe Zhang", "authors": "Zhe Zhang and Daniel B. Neill", "title": "Identifying Significant Predictive Bias in Classifiers", "comments": "Presented as a poster at the 2017 Workshop on Fairness,\n  Accountability, and Transparency in Machine Learning (FAT/ML 2017); earlier\n  version presented at NIPS 2016 Workshop on Interpretable Machine Learning in\n  Complex Systems", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a novel subset scan method to detect if a probabilistic binary\nclassifier has statistically significant bias -- over or under predicting the\nrisk -- for some subgroup, and identify the characteristics of this subgroup.\nThis form of model checking and goodness-of-fit test provides a way to\ninterpretably detect the presence of classifier bias or regions of poor\nclassifier fit. This allows consideration of not just subgroups of a priori\ninterest or small dimensions, but the space of all possible subgroups of\nfeatures. To address the difficulty of considering these exponentially many\npossible subgroups, we use subset scan and parametric bootstrap-based methods.\nExtending this method, we can penalize the complexity of the detected subgroup\nand also identify subgroups with high classification errors. We demonstrate\nthese methods and find interesting results on the COMPAS crime recidivism and\ncredit delinquency data.\n", "versions": [{"version": "v1", "created": "Thu, 24 Nov 2016 19:30:13 GMT"}, {"version": "v2", "created": "Tue, 4 Jul 2017 14:12:17 GMT"}], "update_date": "2017-07-05", "authors_parsed": [["Zhang", "Zhe", ""], ["Neill", "Daniel B.", ""]]}, {"id": "1611.08331", "submitter": "Guoqiang Zhong", "authors": "Guoqiang Zhong, Li-Na Wang, Junyu Dong", "title": "An Overview on Data Representation Learning: From Traditional Feature\n  Learning to Recent Deep Learning", "comments": "About 20 pages. Submitted to Journal of Finance and Data Science as\n  an invited paper", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Since about 100 years ago, to learn the intrinsic structure of data, many\nrepresentation learning approaches have been proposed, including both linear\nones and nonlinear ones, supervised ones and unsupervised ones. Particularly,\ndeep architectures are widely applied for representation learning in recent\nyears, and have delivered top results in many tasks, such as image\nclassification, object detection and speech recognition. In this paper, we\nreview the development of data representation learning methods. Specifically,\nwe investigate both traditional feature learning algorithms and\nstate-of-the-art deep learning models. The history of data representation\nlearning is introduced, while available resources (e.g. online course, tutorial\nand book information) and toolboxes are provided. Finally, we conclude this\npaper with remarks and some interesting research directions on data\nrepresentation learning.\n", "versions": [{"version": "v1", "created": "Fri, 25 Nov 2016 00:53:37 GMT"}], "update_date": "2016-11-28", "authors_parsed": [["Zhong", "Guoqiang", ""], ["Wang", "Li-Na", ""], ["Dong", "Junyu", ""]]}, {"id": "1611.08366", "submitter": "Muhammad Yousefnezhad", "authors": "Muhammad Yousefnezhad, Daoqiang Zhang", "title": "Local Discriminant Hyperalignment for multi-subject fMRI data alignment", "comments": "Published in the Thirty-First AAAI Conference on Artificial\n  Intelligence (AAAI-17)", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.AI cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Multivariate Pattern (MVP) classification can map different cognitive states\nto the brain tasks. One of the main challenges in MVP analysis is validating\nthe generated results across subjects. However, analyzing multi-subject fMRI\ndata requires accurate functional alignments between neuronal activities of\ndifferent subjects, which can rapidly increase the performance and robustness\nof the final results. Hyperalignment (HA) is one of the most effective\nfunctional alignment methods, which can be mathematically formulated by the\nCanonical Correlation Analysis (CCA) methods. Since HA mostly uses the\nunsupervised CCA techniques, its solution may not be optimized for MVP\nanalysis. By incorporating the idea of Local Discriminant Analysis (LDA) into\nCCA, this paper proposes Local Discriminant Hyperalignment (LDHA) as a novel\nsupervised HA method, which can provide better functional alignment for MVP\nanalysis. Indeed, the locality is defined based on the stimuli categories in\nthe train-set, where the correlation between all stimuli in the same category\nwill be maximized and the correlation between distinct categories of stimuli\napproaches to near zero. Experimental studies on multi-subject MVP analysis\nconfirm that the LDHA method achieves superior performance to other\nstate-of-the-art HA algorithms.\n", "versions": [{"version": "v1", "created": "Fri, 25 Nov 2016 07:27:34 GMT"}], "update_date": "2016-11-28", "authors_parsed": [["Yousefnezhad", "Muhammad", ""], ["Zhang", "Daoqiang", ""]]}, {"id": "1611.08372", "submitter": "Chen Xu", "authors": "Chen Xu, Zhouchen Lin, Hongbin Zha", "title": "A Unified Convex Surrogate for the Schatten-$p$ Norm", "comments": "The paper is accepted by AAAI-17. We show that multi-factor matrix\n  factorization enjoys superiority over the traditional two-factor case", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG math.NA math.OC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The Schatten-$p$ norm ($0<p<1$) has been widely used to replace the nuclear\nnorm for better approximating the rank function. However, existing methods are\neither 1) not scalable for large scale problems due to relying on singular\nvalue decomposition (SVD) in every iteration, or 2) specific to some $p$\nvalues, e.g., $1/2$, and $2/3$. In this paper, we show that for any $p$, $p_1$,\nand $p_2 >0$ satisfying $1/p=1/p_1+1/p_2$, there is an equivalence between the\nSchatten-$p$ norm of one matrix and the Schatten-$p_1$ and the Schatten-$p_2$\nnorms of its two factor matrices. We further extend the equivalence to multiple\nfactor matrices and show that all the factor norms can be convex and smooth for\nany $p>0$. In contrast, the original Schatten-$p$ norm for $0<p<1$ is\nnon-convex and non-smooth. As an example we conduct experiments on matrix\ncompletion. To utilize the convexity of the factor matrix norms, we adopt the\naccelerated proximal alternating linearized minimization algorithm and\nestablish its sequence convergence. Experiments on both synthetic and real\ndatasets exhibit its superior performance over the state-of-the-art methods.\nIts speed is also highly competitive.\n", "versions": [{"version": "v1", "created": "Fri, 25 Nov 2016 08:03:31 GMT"}], "update_date": "2016-11-28", "authors_parsed": [["Xu", "Chen", ""], ["Lin", "Zhouchen", ""], ["Zha", "Hongbin", ""]]}, {"id": "1611.08373", "submitter": "Raghav Chalapathy", "authors": "Raghavendra Chalapathy, Ehsan Zare Borzeshi, Massimo Piccardi", "title": "Bidirectional LSTM-CRF for Clinical Concept Extraction", "comments": "This paper \"Bidirectional LSTM-CRF for Clinical Concept Extraction\"\n  is accepted for short paper presentation at Clinical Natural Language\n  Processing Workshop at COLING 2016 Osaka, Japan. December 11, 2016", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.CL cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Automated extraction of concepts from patient clinical records is an\nessential facilitator of clinical research. For this reason, the 2010 i2b2/VA\nNatural Language Processing Challenges for Clinical Records introduced a\nconcept extraction task aimed at identifying and classifying concepts into\npredefined categories (i.e., treatments, tests and problems). State-of-the-art\nconcept extraction approaches heavily rely on handcrafted features and\ndomain-specific resources which are hard to collect and define. For this\nreason, this paper proposes an alternative, streamlined approach: a recurrent\nneural network (the bidirectional LSTM with CRF decoding) initialized with\ngeneral-purpose, off-the-shelf word embeddings. The experimental results\nachieved on the 2010 i2b2/VA reference corpora using the proposed framework\noutperform all recent methods and ranks closely to the best submission from the\noriginal 2010 i2b2/VA challenge.\n", "versions": [{"version": "v1", "created": "Fri, 25 Nov 2016 08:11:23 GMT"}], "update_date": "2016-11-28", "authors_parsed": [["Chalapathy", "Raghavendra", ""], ["Borzeshi", "Ehsan Zare", ""], ["Piccardi", "Massimo", ""]]}, {"id": "1611.08480", "submitter": "Maximilian Alber", "authors": "Maximilian Alber, Julian Zimmert, Urun Dogan, Marius Kloft", "title": "Distributed Optimization of Multi-Class SVMs", "comments": null, "journal-ref": null, "doi": "10.1371/journal.pone.0178161", "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Training of one-vs.-rest SVMs can be parallelized over the number of classes\nin a straight forward way. Given enough computational resources, one-vs.-rest\nSVMs can thus be trained on data involving a large number of classes. The same\ncannot be stated, however, for the so-called all-in-one SVMs, which require\nsolving a quadratic program of size quadratically in the number of classes. We\ndevelop distributed algorithms for two all-in-one SVM formulations (Lee et al.\nand Weston and Watkins) that parallelize the computation evenly over the number\nof classes. This allows us to compare these models to one-vs.-rest SVMs on\nunprecedented scale. The results indicate superior accuracy on text\nclassification data.\n", "versions": [{"version": "v1", "created": "Fri, 25 Nov 2016 15:07:06 GMT"}, {"version": "v2", "created": "Thu, 8 Dec 2016 08:52:10 GMT"}], "update_date": "2017-07-05", "authors_parsed": [["Alber", "Maximilian", ""], ["Zimmert", "Julian", ""], ["Dogan", "Urun", ""], ["Kloft", "Marius", ""]]}, {"id": "1611.08568", "submitter": "Rui Shu", "authors": "Rui Shu, Hung H. Bui, Mohammad Ghavamzadeh", "title": "Bottleneck Conditional Density Estimation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We introduce a new framework for training deep generative models for\nhigh-dimensional conditional density estimation. The Bottleneck Conditional\nDensity Estimator (BCDE) is a variant of the conditional variational\nautoencoder (CVAE) that employs layer(s) of stochastic variables as the\nbottleneck between the input $x$ and target $y$, where both are\nhigh-dimensional. Crucially, we propose a new hybrid training method that\nblends the conditional generative model with a joint generative model. Hybrid\nblending is the key to effective training of the BCDE, which avoids overfitting\nand provides a novel mechanism for leveraging unlabeled data. We show that our\nhybrid training procedure enables models to achieve competitive results in the\nMNIST quadrant prediction task in the fully-supervised setting, and sets new\nbenchmarks in the semi-supervised regime for MNIST, SVHN, and CelebA.\n", "versions": [{"version": "v1", "created": "Fri, 25 Nov 2016 19:42:53 GMT"}, {"version": "v2", "created": "Mon, 27 Feb 2017 08:28:08 GMT"}, {"version": "v3", "created": "Fri, 30 Jun 2017 12:27:01 GMT"}], "update_date": "2017-07-03", "authors_parsed": [["Shu", "Rui", ""], ["Bui", "Hung H.", ""], ["Ghavamzadeh", "Mohammad", ""]]}, {"id": "1611.08618", "submitter": "Yazhou Yang", "authors": "Yazhou Yang, Marco Loog", "title": "A Benchmark and Comparison of Active Learning for Logistic Regression", "comments": "accepted by Pattern Recognition", "journal-ref": "Pattern Recognition 83C (2018) pp. 401-415", "doi": "10.1016/j.patcog.2018.06.004", "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Logistic regression is by far the most widely used classifier in real-world\napplications. In this paper, we benchmark the state-of-the-art active learning\nmethods for logistic regression and discuss and illustrate their underlying\ncharacteristics. Experiments are carried out on three synthetic datasets and 44\nreal-world datasets, providing insight into the behaviors of these active\nlearning methods with respect to the area of the learning curve (which plots\nclassification accuracy as a function of the number of queried examples) and\ntheir computational costs. Surprisingly, one of the earliest and simplest\nsuggested active learning methods, i.e., uncertainty sampling, performs\nexceptionally well overall. Another remarkable finding is that random sampling,\nwhich is the rudimentary baseline to improve upon, is not overwhelmed by\nindividual active learning techniques in many cases.\n", "versions": [{"version": "v1", "created": "Fri, 25 Nov 2016 21:33:57 GMT"}, {"version": "v2", "created": "Thu, 21 Jun 2018 12:49:47 GMT"}], "update_date": "2018-07-04", "authors_parsed": [["Yang", "Yazhou", ""], ["Loog", "Marco", ""]]}, {"id": "1611.08648", "submitter": "Berkay Celik", "authors": "Z. Berkay Celik, David Lopez-Paz, Patrick McDaniel", "title": "Patient-Driven Privacy Control through Generalized Distillation", "comments": "IEEE Symposium on Privacy-Aware Computing (IEEE PAC), 2017", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CR cs.CY cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The introduction of data analytics into medicine has changed the nature of\npatient treatment. In this, patients are asked to disclose personal information\nsuch as genetic markers, lifestyle habits, and clinical history. This data is\nthen used by statistical models to predict personalized treatments. However,\ndue to privacy concerns, patients often desire to withhold sensitive\ninformation. This self-censorship can impede proper diagnosis and treatment,\nwhich may lead to serious health complications and even death over time. In\nthis paper, we present privacy distillation, a mechanism which allows patients\nto control the type and amount of information they wish to disclose to the\nhealthcare providers for use in statistical models. Meanwhile, it retains the\naccuracy of models that have access to all patient data under a sufficient but\nnot full set of privacy-relevant information. We validate privacy distillation\nusing a corpus of patients prescribed to warfarin for a personalized dosage. We\nuse a deep neural network to implement privacy distillation for training and\nmaking dose predictions. We find that privacy distillation with sufficient\nprivacy-relevant information i) retains accuracy almost as good as having all\npatient data (only 3\\% worse), and ii) is effective at preventing errors that\nintroduce health-related risks (only 3.9\\% worse under- or over-prescriptions).\n", "versions": [{"version": "v1", "created": "Sat, 26 Nov 2016 01:47:00 GMT"}, {"version": "v2", "created": "Fri, 13 Oct 2017 23:49:10 GMT"}], "update_date": "2017-10-17", "authors_parsed": [["Celik", "Z. Berkay", ""], ["Lopez-Paz", "David", ""], ["McDaniel", "Patrick", ""]]}, {"id": "1611.08699", "submitter": "Colin Brown J", "authors": "Colin J Brown, Ghassan Hamarneh", "title": "Machine Learning on Human Connectome Data from MRI", "comments": "51 pages, 6 figures. To be submitted to a journal", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG q-bio.NC stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Functional MRI (fMRI) and diffusion MRI (dMRI) are non-invasive imaging\nmodalities that allow in-vivo analysis of a patient's brain network (known as a\nconnectome). Use of these technologies has enabled faster and better diagnoses\nand treatments of neurological disorders and a deeper understanding of the\nhuman brain. Recently, researchers have been exploring the application of\nmachine learning models to connectome data in order to predict clinical\noutcomes and analyze the importance of subnetworks in the brain. Connectome\ndata has unique properties, which present both special challenges and\nopportunities when used for machine learning. The purpose of this work is to\nreview the literature on the topic of applying machine learning models to\nMRI-based connectome data. This field is growing rapidly and now encompasses a\nlarge body of research. To summarize the research done to date, we provide a\ncomparative, structured summary of 77 relevant works, tabulated according to\ndifferent criteria, that represent the majority of the literature on this\ntopic. (We also published a living version of this table online at\nhttp://connectomelearning.cs.sfu.ca that the community can continue to\ncontribute to.) After giving an overview of how connectomes are constructed\nfrom dMRI and fMRI data, we discuss the variety of machine learning tasks that\nhave been explored with connectome data. We then compare the advantages and\ndrawbacks of different machine learning approaches that have been employed,\ndiscussing different feature selection and feature extraction schemes, as well\nas the learning models and regularization penalties themselves. Throughout this\ndiscussion, we focus particularly on how the methods are adapted to the unique\nnature of graphical connectome data. Finally, we conclude by summarizing the\ncurrent state of the art and by outlining what we believe are strategic\ndirections for future research.\n", "versions": [{"version": "v1", "created": "Sat, 26 Nov 2016 11:14:22 GMT"}], "update_date": "2016-12-06", "authors_parsed": [["Brown", "Colin J", ""], ["Hamarneh", "Ghassan", ""]]}, {"id": "1611.08737", "submitter": "Shuangfei Zhai", "authors": "Nana Li, Shuangfei Zhai, Zhongfei Zhang, Boying Liu", "title": "Structural Correspondence Learning for Cross-lingual Sentiment\n  Classification with One-to-many Mappings", "comments": "To appear in AAAI 2017. arXiv admin note: text overlap with\n  arXiv:1008.0716 by other authors", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CL stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Structural correspondence learning (SCL) is an effective method for\ncross-lingual sentiment classification. This approach uses unlabeled documents\nalong with a word translation oracle to automatically induce task specific,\ncross-lingual correspondences. It transfers knowledge through identifying\nimportant features, i.e., pivot features. For simplicity, however, it assumes\nthat the word translation oracle maps each pivot feature in source language to\nexactly only one word in target language. This one-to-one mapping between words\nin different languages is too strict. Also the context is not considered at\nall. In this paper, we propose a cross-lingual SCL based on distributed\nrepresentation of words; it can learn meaningful one-to-many mappings for pivot\nwords using large amounts of monolingual data and a small dictionary. We\nconduct experiments on NLP\\&CC 2013 cross-lingual sentiment analysis dataset,\nemploying English as source language, and Chinese as target language. Our\nmethod does not rely on the parallel corpora and the experimental results show\nthat our approach is more competitive than the state-of-the-art methods in\ncross-lingual sentiment classification.\n", "versions": [{"version": "v1", "created": "Sat, 26 Nov 2016 20:11:00 GMT"}], "update_date": "2016-12-06", "authors_parsed": [["Li", "Nana", ""], ["Zhai", "Shuangfei", ""], ["Zhang", "Zhongfei", ""], ["Liu", "Boying", ""]]}, {"id": "1611.08791", "submitter": "Mohammad Amin Rahimian", "authors": "M. Amin Rahimian and Ali Jadbabaie", "title": "Learning without recall in directed circles and rooted trees", "comments": "American Control Conference (ACC), 2015", "journal-ref": null, "doi": "10.1109/ACC.2015.7171992", "report-no": null, "categories": "stat.AP cs.IT cs.SI math.IT math.PR stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This work investigates the case of a network of agents that attempt to learn\nsome unknown state of the world amongst the finitely many possibilities. At\neach time step, agents all receive random, independently distributed private\nsignals whose distributions are dependent on the unknown state of the world.\nHowever, it may be the case that some or any of the agents cannot distinguish\nbetween two or more of the possible states based only on their private\nobservations, as when several states result in the same distribution of the\nprivate signals. In our model, the agents form some initial belief (probability\ndistribution) about the unknown state and then refine their beliefs in\naccordance with their private observations, as well as the beliefs of their\nneighbors. An agent learns the unknown state when her belief converges to a\npoint mass that is concentrated at the true state. A rational agent would use\nthe Bayes' rule to incorporate her neighbors' beliefs and own private signals\nover time. While such repeated applications of the Bayes' rule in networks can\nbecome computationally intractable, in this paper, we show that in the\ncanonical cases of directed star, circle or path networks and their\ncombinations, one can derive a class of memoryless update rules that replicate\nthat of a single Bayesian agent but replace the self beliefs with the beliefs\nof the neighbors. This way, one can realize an exponentially fast rate of\nlearning similar to the case of Bayesian (fully rational) agents. The proposed\nrules are a special case of the Learning without Recall.\n", "versions": [{"version": "v1", "created": "Sun, 27 Nov 2016 05:22:30 GMT"}], "update_date": "2016-11-29", "authors_parsed": [["Rahimian", "M. Amin", ""], ["Jadbabaie", "Ali", ""]]}, {"id": "1611.08903", "submitter": "Martin Schrimpf", "authors": "Martin Schrimpf", "title": "Should I use TensorFlow", "comments": "Seminar Paper", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Google's Machine Learning framework TensorFlow was open-sourced in November\n2015 [1] and has since built a growing community around it. TensorFlow is\nsupposed to be flexible for research purposes while also allowing its models to\nbe deployed productively. This work is aimed towards people with experience in\nMachine Learning considering whether they should use TensorFlow in their\nenvironment. Several aspects of the framework important for such a decision are\nexamined, such as the heterogenity, extensibility and its computation graph. A\npure Python implementation of linear classification is compared with an\nimplementation utilizing TensorFlow. I also contrast TensorFlow to other\npopular frameworks with respect to modeling capability, deployment and\nperformance and give a brief description of the current adaption of the\nframework.\n", "versions": [{"version": "v1", "created": "Sun, 27 Nov 2016 20:20:06 GMT"}], "update_date": "2016-12-06", "authors_parsed": [["Schrimpf", "Martin", ""]]}, {"id": "1611.08945", "submitter": "Arvind Neelakantan", "authors": "Arvind Neelakantan, Quoc V. Le, Martin Abadi, Andrew McCallum, Dario\n  Amodei", "title": "Learning a Natural Language Interface with Neural Programmer", "comments": "Published as a conference paper at ICLR 2017", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Learning a natural language interface for database tables is a challenging\ntask that involves deep language understanding and multi-step reasoning. The\ntask is often approached by mapping natural language queries to logical forms\nor programs that provide the desired response when executed on the database. To\nour knowledge, this paper presents the first weakly supervised, end-to-end\nneural network model to induce such programs on a real-world dataset. We\nenhance the objective function of Neural Programmer, a neural network with\nbuilt-in discrete operations, and apply it on WikiTableQuestions, a natural\nlanguage question-answering dataset. The model is trained end-to-end with weak\nsupervision of question-answer pairs, and does not require domain-specific\ngrammars, rules, or annotations that are key elements in previous approaches to\nprogram induction. The main experimental result in this paper is that a single\nNeural Programmer model achieves 34.2% accuracy using only 10,000 examples with\nweak supervision. An ensemble of 15 models, with a trivial combination\ntechnique, achieves 37.7% accuracy, which is competitive to the current\nstate-of-the-art accuracy of 37.1% obtained by a traditional natural language\nsemantic parser.\n", "versions": [{"version": "v1", "created": "Mon, 28 Nov 2016 00:54:34 GMT"}, {"version": "v2", "created": "Mon, 20 Feb 2017 16:18:14 GMT"}, {"version": "v3", "created": "Tue, 21 Feb 2017 14:43:12 GMT"}, {"version": "v4", "created": "Thu, 2 Mar 2017 16:02:00 GMT"}], "update_date": "2017-03-03", "authors_parsed": [["Neelakantan", "Arvind", ""], ["Le", "Quoc V.", ""], ["Abadi", "Martin", ""], ["McCallum", "Andrew", ""], ["Amodei", "Dario", ""]]}, {"id": "1611.09083", "submitter": "Alexey Drutsa", "authors": "Alexey Drutsa (Yandex, Moscow, Russia), Gleb Gusev (Yandex, Moscow,\n  Russia), Pavel Serdyukov (Yandex, Moscow, Russia)", "title": "Prediction of Video Popularity in the Absence of Reliable Data from\n  Video Hosting Services: Utility of Traces Left by Users on the Web", "comments": "23 pages, 4 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR cs.HC cs.MM stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  With the growth of user-generated content, we observe the constant rise of\nthe number of companies, such as search engines, content aggregators, etc.,\nthat operate with tremendous amounts of web content not being the services\nhosting it. Thus, aiming to locate the most important content and promote it to\nthe users, they face the need of estimating the current and predicting the\nfuture content popularity.\n  In this paper, we approach the problem of video popularity prediction not\nfrom the side of a video hosting service, as done in all previous studies, but\nfrom the side of an operating company, which provides a popular video search\nservice that aggregates content from different video hosting websites. We\ninvestigate video popularity prediction based on features from three primary\nsources available for a typical operating company: first, the content hosting\nprovider may deliver its data via its API, second, the operating company makes\nuse of its own search and browsing logs, third, the company crawls information\nabout embeds of a video and links to a video page from publicly available\nresources on the Web. We show that video popularity prediction based on the\nembed and link data coupled with the internal search and browsing data\nsignificantly improves video popularity prediction based only on the data\nprovided by the video hosting and can even adequately replace the API data in\nthe cases when it is partly or completely unavailable.\n", "versions": [{"version": "v1", "created": "Mon, 28 Nov 2016 11:43:31 GMT"}], "update_date": "2016-11-29", "authors_parsed": [["Drutsa", "Alexey", "", "Yandex, Moscow, Russia"], ["Gusev", "Gleb", "", "Yandex, Moscow,\n  Russia"], ["Serdyukov", "Pavel", "", "Yandex, Moscow, Russia"]]}, {"id": "1611.09130", "submitter": "Arjun Viswanathan Arjun Viswanathan", "authors": "Arjun Viswanathan", "title": "Generalizing the Kelly strategy", "comments": "8 pages pdf, working note v2 29th nov. current version v4 (latex\n  typeset, added upper bound on domain of H)", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.GT", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Prompted by a recent experiment by Victor Haghani and Richard Dewey, this\nnote generalises the Kelly strategy (optimal for simple investment games with\nlog utility) to a large class of practical utility functions and including the\neffect of extraneous wealth.\n  A counterintuitive result is proved : for any continuous, concave,\ndifferentiable utility function, the optimal choice at every point depends only\non the probability of reaching that point.\n  The practical calculation of the optimal action at every stage is made\npossible through use of the binomial expansion, reducing the problem size from\nexponential to quadratic.\n  Applications include (better) automatic investing and risk taking under\nuncertainty.\n", "versions": [{"version": "v1", "created": "Mon, 28 Nov 2016 14:17:19 GMT"}, {"version": "v2", "created": "Tue, 29 Nov 2016 13:06:20 GMT"}, {"version": "v3", "created": "Tue, 6 Dec 2016 10:14:52 GMT"}], "update_date": "2016-12-07", "authors_parsed": [["Viswanathan", "Arjun", ""]]}, {"id": "1611.09139", "submitter": "William Herlands", "authors": "Andrew Gordon Wilson, Been Kim, William Herlands", "title": "Proceedings of NIPS 2016 Workshop on Interpretable Machine Learning for\n  Complex Systems", "comments": "31 papers", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This is the Proceedings of NIPS 2016 Workshop on Interpretable Machine\nLearning for Complex Systems, held in Barcelona, Spain on December 9, 2016\n", "versions": [{"version": "v1", "created": "Mon, 28 Nov 2016 14:34:15 GMT"}], "update_date": "2016-11-29", "authors_parsed": [["Wilson", "Andrew Gordon", ""], ["Kim", "Been", ""], ["Herlands", "William", ""]]}, {"id": "1611.09207", "submitter": "Brian Patton", "authors": "Brian Patton, Yannis Agiomyrgiannakis, Michael Terry, Kevin Wilson,\n  Rif A. Saurous, D. Sculley", "title": "AutoMOS: Learning a non-intrusive assessor of naturalness-of-speech", "comments": "4 pages, 2 figures, 2 tables, NIPS 2016 End-to-end Learning for\n  Speech and Audio Processing Workshop", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Developers of text-to-speech synthesizers (TTS) often make use of human\nraters to assess the quality of synthesized speech. We demonstrate that we can\nmodel human raters' mean opinion scores (MOS) of synthesized speech using a\ndeep recurrent neural network whose inputs consist solely of a raw waveform.\nOur best models provide utterance-level estimates of MOS only moderately\ninferior to sampled human ratings, as shown by Pearson and Spearman\ncorrelations. When multiple utterances are scored and averaged, a scenario\ncommon in synthesizer quality assessment, AutoMOS achieves correlations\napproaching those of human raters. The AutoMOS model has a number of\napplications, such as the ability to explore the parameter space of a speech\nsynthesizer without requiring a human-in-the-loop.\n", "versions": [{"version": "v1", "created": "Mon, 28 Nov 2016 15:51:25 GMT"}], "update_date": "2016-11-29", "authors_parsed": [["Patton", "Brian", ""], ["Agiomyrgiannakis", "Yannis", ""], ["Terry", "Michael", ""], ["Wilson", "Kevin", ""], ["Saurous", "Rif A.", ""], ["Sculley", "D.", ""]]}, {"id": "1611.09226", "submitter": "Michael Figurnov", "authors": "Michael Figurnov, Kirill Struminsky, Dmitry Vetrov", "title": "Robust Variational Inference", "comments": "NIPS 2016 Workshop, Advances in Approximate Bayesian Inference", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Variational inference is a powerful tool for approximate inference. However,\nit mainly focuses on the evidence lower bound as variational objective and the\ndevelopment of other measures for variational inference is a promising area of\nresearch. This paper proposes a robust modification of evidence and a lower\nbound for the evidence, which is applicable when the majority of the training\nset samples are random noise objects. We provide experiments for variational\nautoencoders to show advantage of the objective over the evidence lower bound\non synthetic datasets obtained by adding uninformative noise objects to MNIST\nand OMNIGLOT. Additionally, for the original MNIST and OMNIGLOT datasets we\nobserve a small improvement over the non-robust evidence lower bound.\n", "versions": [{"version": "v1", "created": "Mon, 28 Nov 2016 16:28:41 GMT"}], "update_date": "2016-12-06", "authors_parsed": [["Figurnov", "Michael", ""], ["Struminsky", "Kirill", ""], ["Vetrov", "Dmitry", ""]]}, {"id": "1611.09232", "submitter": "Meshia C\\'edric Oveneke", "authors": "Meshia C\\'edric Oveneke, Mitchel Aliosha-Perez, Yong Zhao, Dongmei\n  Jiang and Hichem Sahli", "title": "Efficient Convolutional Auto-Encoding via Random Convexification and\n  Frequency-Domain Minimization", "comments": "Accepted at NIPS 2016 Workshop on Efficient Methods for Deep Neural\n  Networks (EMDNN)", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The omnipresence of deep learning architectures such as deep convolutional\nneural networks (CNN)s is fueled by the synergistic combination of\never-increasing labeled datasets and specialized hardware. Despite the\nindisputable success, the reliance on huge amounts of labeled data and\nspecialized hardware can be a limiting factor when approaching new\napplications. To help alleviating these limitations, we propose an efficient\nlearning strategy for layer-wise unsupervised training of deep CNNs on\nconventional hardware in acceptable time. Our proposed strategy consists of\nrandomly convexifying the reconstruction contractive auto-encoding (RCAE)\nlearning objective and solving the resulting large-scale convex minimization\nproblem in the frequency domain via coordinate descent (CD). The main\nadvantages of our proposed learning strategy are: (1) single tunable\noptimization parameter; (2) fast and guaranteed convergence; (3) possibilities\nfor full parallelization. Numerical experiments show that our proposed learning\nstrategy scales (in the worst case) linearly with image size, number of filters\nand filter size.\n", "versions": [{"version": "v1", "created": "Mon, 28 Nov 2016 16:42:11 GMT"}], "update_date": "2016-11-29", "authors_parsed": [["Oveneke", "Meshia C\u00e9dric", ""], ["Aliosha-Perez", "Mitchel", ""], ["Zhao", "Yong", ""], ["Jiang", "Dongmei", ""], ["Sahli", "Hichem", ""]]}, {"id": "1611.09328", "submitter": "Yangchen Pan", "authors": "Yangchen Pan, Adam White, Martha White", "title": "Accelerated Gradient Temporal Difference Learning", "comments": "AAAI Conference on Artificial Intelligence (AAAI), 2017", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The family of temporal difference (TD) methods span a spectrum from\ncomputationally frugal linear methods like TD({\\lambda}) to data efficient\nleast squares methods. Least square methods make the best use of available data\ndirectly computing the TD solution and thus do not require tuning a typically\nhighly sensitive learning rate parameter, but require quadratic computation and\nstorage. Recent algorithmic developments have yielded several sub-quadratic\nmethods that use an approximation to the least squares TD solution, but incur\nbias. In this paper, we propose a new family of accelerated gradient TD (ATD)\nmethods that (1) provide similar data efficiency benefits to least-squares\nmethods, at a fraction of the computation and storage (2) significantly reduce\nparameter sensitivity compared to linear TD methods, and (3) are asymptotically\nunbiased. We illustrate these claims with a proof of convergence in expectation\nand experiments on several benchmark domains and a large-scale industrial\nenergy allocation domain.\n", "versions": [{"version": "v1", "created": "Mon, 28 Nov 2016 20:33:15 GMT"}, {"version": "v2", "created": "Thu, 9 Mar 2017 22:36:45 GMT"}], "update_date": "2017-03-13", "authors_parsed": [["Pan", "Yangchen", ""], ["White", "Adam", ""], ["White", "Martha", ""]]}, {"id": "1611.09340", "submitter": "Adriana Romero", "authors": "Adriana Romero, Pierre Luc Carrier, Akram Erraqabi, Tristan Sylvain,\n  Alex Auvolat, Etienne Dejoie, Marc-Andr\\'e Legault, Marie-Pierre Dub\\'e,\n  Julie G. Hussin, Yoshua Bengio", "title": "Diet Networks: Thin Parameters for Fat Genomics", "comments": null, "journal-ref": "ICLR 2017", "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Learning tasks such as those involving genomic data often poses a serious\nchallenge: the number of input features can be orders of magnitude larger than\nthe number of training examples, making it difficult to avoid overfitting, even\nwhen using the known regularization techniques. We focus here on tasks in which\nthe input is a description of the genetic variation specific to a patient, the\nsingle nucleotide polymorphisms (SNPs), yielding millions of ternary inputs.\nImproving the ability of deep learning to handle such datasets could have an\nimportant impact in precision medicine, where high-dimensional data regarding a\nparticular patient is used to make predictions of interest. Even though the\namount of data for such tasks is increasing, this mismatch between the number\nof examples and the number of inputs remains a concern. Naive implementations\nof classifier neural networks involve a huge number of free parameters in their\nfirst layer: each input feature is associated with as many parameters as there\nare hidden units. We propose a novel neural network parametrization which\nconsiderably reduces the number of free parameters. It is based on the idea\nthat we can first learn or provide a distributed representation for each input\nfeature (e.g. for each position in the genome where variations are observed),\nand then learn (with another neural network called the parameter prediction\nnetwork) how to map a feature's distributed representation to the vector of\nparameters specific to that feature in the classifier neural network (the\nweights which link the value of the feature to each of the hidden units). We\nshow experimentally on a population stratification task of interest to medical\nstudies that the proposed approach can significantly reduce both the number of\nparameters and the error rate of the classifier.\n", "versions": [{"version": "v1", "created": "Mon, 28 Nov 2016 20:50:32 GMT"}, {"version": "v2", "created": "Fri, 6 Jan 2017 18:51:52 GMT"}, {"version": "v3", "created": "Thu, 16 Mar 2017 21:09:28 GMT"}], "update_date": "2017-03-21", "authors_parsed": [["Romero", "Adriana", ""], ["Carrier", "Pierre Luc", ""], ["Erraqabi", "Akram", ""], ["Sylvain", "Tristan", ""], ["Auvolat", "Alex", ""], ["Dejoie", "Etienne", ""], ["Legault", "Marc-Andr\u00e9", ""], ["Dub\u00e9", "Marie-Pierre", ""], ["Hussin", "Julie G.", ""], ["Bengio", "Yoshua", ""]]}, {"id": "1611.09347", "submitter": "Nicola Pancotti Mr.", "authors": "Jacob Biamonte, Peter Wittek, Nicola Pancotti, Patrick Rebentrost,\n  Nathan Wiebe and Seth Lloyd", "title": "Quantum Machine Learning", "comments": "24 pages, 2 figures", "journal-ref": "Nature 549, 195-202 (2017)", "doi": "10.1038/nature23474", "report-no": null, "categories": "quant-ph cond-mat.str-el stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Fuelled by increasing computer power and algorithmic advances, machine\nlearning techniques have become powerful tools for finding patterns in data.\nSince quantum systems produce counter-intuitive patterns believed not to be\nefficiently produced by classical systems, it is reasonable to postulate that\nquantum computers may outperform classical computers on machine learning tasks.\nThe field of quantum machine learning explores how to devise and implement\nconcrete quantum software that offers such advantages. Recent work has made\nclear that the hardware and software challenges are still considerable but has\nalso opened paths towards solutions.\n", "versions": [{"version": "v1", "created": "Mon, 28 Nov 2016 20:59:46 GMT"}, {"version": "v2", "created": "Thu, 10 May 2018 19:44:58 GMT"}], "update_date": "2018-05-14", "authors_parsed": [["Biamonte", "Jacob", ""], ["Wittek", "Peter", ""], ["Pancotti", "Nicola", ""], ["Rebentrost", "Patrick", ""], ["Wiebe", "Nathan", ""], ["Lloyd", "Seth", ""]]}, {"id": "1611.09384", "submitter": "Brenden Lake", "authors": "Brenden M. Lake, Neil D. Lawrence, Joshua B. Tenenbaum", "title": "The Emergence of Organizing Structure in Conceptual Representation", "comments": "In press at Cognitive Science", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Both scientists and children make important structural discoveries, yet their\ncomputational underpinnings are not well understood. Structure discovery has\npreviously been formalized as probabilistic inference about the right\nstructural form --- where form could be a tree, ring, chain, grid, etc. [Kemp &\nTenenbaum (2008). The discovery of structural form. PNAS, 105(3), 10687-10692].\nWhile this approach can learn intuitive organizations, including a tree for\nanimals and a ring for the color circle, it assumes a strong inductive bias\nthat considers only these particular forms, and each form is explicitly\nprovided as initial knowledge. Here we introduce a new computational model of\nhow organizing structure can be discovered, utilizing a broad hypothesis space\nwith a preference for sparse connectivity. Given that the inductive bias is\nmore general, the model's initial knowledge shows little qualitative\nresemblance to some of the discoveries it supports. As a consequence, the model\ncan also learn complex structures for domains that lack intuitive description,\nas well as predict human property induction judgments without explicit\nstructural forms. By allowing form to emerge from sparsity, our approach\nclarifies how both the richness and flexibility of human conceptual\norganization can coexist.\n", "versions": [{"version": "v1", "created": "Mon, 28 Nov 2016 21:13:25 GMT"}, {"version": "v2", "created": "Wed, 22 Nov 2017 00:41:32 GMT"}], "update_date": "2017-11-23", "authors_parsed": [["Lake", "Brenden M.", ""], ["Lawrence", "Neil D.", ""], ["Tenenbaum", "Joshua B.", ""]]}, {"id": "1611.09391", "submitter": "Will Wei Sun", "authors": "Botao Hao, Will Wei Sun, Yufeng Liu, Guang Cheng", "title": "Simultaneous Clustering and Estimation of Heterogeneous Graphical Models", "comments": "61 pages. Accepted by Journal of Machine Learning Research", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider joint estimation of multiple graphical models arising from\nheterogeneous and high-dimensional observations. Unlike most previous\napproaches which assume that the cluster structure is given in advance, an\nappealing feature of our method is to learn cluster structure while estimating\nheterogeneous graphical models. This is achieved via a high dimensional version\nof Expectation Conditional Maximization (ECM) algorithm (Meng and Rubin, 1993).\nA joint graphical lasso penalty is imposed on the conditional maximization step\nto extract both homogeneity and heterogeneity components across all clusters.\nOur algorithm is computationally efficient due to fast sparse learning routines\nand can be implemented without unsupervised learning knowledge. The superior\nperformance of our method is demonstrated by extensive experiments and its\napplication to a Glioblastoma cancer dataset reveals some new insights in\nunderstanding the Glioblastoma cancer. In theory, a non-asymptotic error bound\nis established for the output directly from our high dimensional ECM algorithm,\nand it consists of two quantities: statistical error (statistical accuracy) and\noptimization error (computational complexity). Such a result gives a\ntheoretical guideline in terminating our ECM iterations.\n", "versions": [{"version": "v1", "created": "Mon, 28 Nov 2016 21:28:13 GMT"}, {"version": "v2", "created": "Fri, 12 Jan 2018 23:00:23 GMT"}], "update_date": "2018-01-16", "authors_parsed": [["Hao", "Botao", ""], ["Sun", "Will Wei", ""], ["Liu", "Yufeng", ""], ["Cheng", "Guang", ""]]}, {"id": "1611.09444", "submitter": "Alden Walker", "authors": "Kevin K. Chen, Anthony Gamst, Alden Walker", "title": "The empirical size of trained neural networks", "comments": "6 pages, 5 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  ReLU neural networks define piecewise linear functions of their inputs.\nHowever, initializing and training a neural network is very different from\nfitting a linear spline. In this paper, we expand empirically upon previous\ntheoretical work to demonstrate features of trained neural networks. Standard\nnetwork initialization and training produce networks vastly simpler than a\nnaive parameter count would suggest and can impart odd features to the trained\nnetwork. However, we also show the forced simplicity is beneficial and, indeed,\ncritical for the wide success of these networks.\n", "versions": [{"version": "v1", "created": "Tue, 29 Nov 2016 00:39:45 GMT"}], "update_date": "2016-11-30", "authors_parsed": [["Chen", "Kevin K.", ""], ["Gamst", "Anthony", ""], ["Walker", "Alden", ""]]}, {"id": "1611.09448", "submitter": "Kevin Chen", "authors": "Kevin K. Chen", "title": "The Upper Bound on Knots in Neural Networks", "comments": "19 pages, 8 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Neural networks with rectified linear unit activations are essentially\nmultivariate linear splines. As such, one of many ways to measure the\n\"complexity\" or \"expressivity\" of a neural network is to count the number of\nknots in the spline model. We study the number of knots in fully-connected\nfeedforward neural networks with rectified linear unit activation functions. We\nintentionally keep the neural networks very simple, so as to make theoretical\nanalyses more approachable. An induction on the number of layers $l$ reveals a\ntight upper bound on the number of knots in $\\mathbb{R} \\to \\mathbb{R}^p$ deep\nneural networks. With $n_i \\gg 1$ neurons in layer $i = 1, \\dots, l$, the upper\nbound is approximately $n_1 \\dots n_l$. We then show that the exact upper bound\nis tight, and we demonstrate the upper bound with an example. The purpose of\nthese analyses is to pave a path for understanding the behavior of general\n$\\mathbb{R}^q \\to \\mathbb{R}^p$ neural networks.\n", "versions": [{"version": "v1", "created": "Tue, 29 Nov 2016 01:07:58 GMT"}, {"version": "v2", "created": "Wed, 30 Nov 2016 02:00:48 GMT"}], "update_date": "2016-12-01", "authors_parsed": [["Chen", "Kevin K.", ""]]}, {"id": "1611.09461", "submitter": "Yao-Yuan Yang", "authors": "Yao-Yuan Yang, Kuan-Hao Huang, Chih-Wei Chang, Hsuan-Tien Lin", "title": "Cost-Sensitive Reference Pair Encoding for Multi-Label Learning", "comments": "Accepted in 22nd Pacific-Asia Conference on Knowledge Discovery and\n  Data Mining (PAKDD), 2018", "journal-ref": null, "doi": "10.1007/978-3-319-93034-3_12", "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Label space expansion for multi-label classification (MLC) is a methodology\nthat encodes the original label vectors to higher dimensional codes before\ntraining and decodes the predicted codes back to the label vectors during\ntesting. The methodology has been demonstrated to improve the performance of\nMLC algorithms when coupled with off-the-shelf error-correcting codes for\nencoding and decoding. Nevertheless, such a coding scheme can be complicated to\nimplement, and cannot easily satisfy a common application need of\ncost-sensitive MLC---adapting to different evaluation criteria of interest. In\nthis work, we show that a simpler coding scheme based on the concept of a\nreference pair of label vectors achieves cost-sensitivity more naturally. In\nparticular, our proposed cost-sensitive reference pair encoding (CSRPE)\nalgorithm contains cluster-based encoding, weight-based training and\nvoting-based decoding steps, all utilizing the cost information. Furthermore,\nwe leverage the cost information embedded in the code space of CSRPE to propose\na novel active learning algorithm for cost-sensitive MLC. Extensive\nexperimental results verify that CSRPE performs better than state-of-the-art\nalgorithms across different MLC criteria. The results also demonstrate that the\nCSRPE-backed active learning algorithm is superior to existing algorithms for\nactive MLC, and further justify the usefulness of CSRPE.\n", "versions": [{"version": "v1", "created": "Tue, 29 Nov 2016 02:41:28 GMT"}, {"version": "v2", "created": "Fri, 18 Aug 2017 07:26:05 GMT"}, {"version": "v3", "created": "Fri, 26 Oct 2018 05:09:59 GMT"}], "update_date": "2018-10-29", "authors_parsed": [["Yang", "Yao-Yuan", ""], ["Huang", "Kuan-Hao", ""], ["Chang", "Chih-Wei", ""], ["Lin", "Hsuan-Tien", ""]]}, {"id": "1611.09510", "submitter": "Shay Deutsch Dr.", "authors": "Shay Deutsch, Antonio Ortega, Gerard Medioni", "title": "Graph-Based Manifold Frequency Analysis for Denoising", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a new framework for manifold denoising based on processing in the\ngraph Fourier frequency domain, derived from the spectral decomposition of the\ndiscrete graph Laplacian. Our approach uses the Spectral Graph Wavelet\ntransform in order to per- form non-iterative denoising directly in the graph\nfrequency domain, an approach inspired by conventional wavelet-based signal\ndenoising methods. We theoretically justify our approach, based on the fact\nthat for smooth manifolds the coordinate information energy is localized in the\nlow spectral graph wavelet sub-bands, while the noise affects all frequency\nbands in a similar way. Experimental results show that our proposed manifold\nfrequency denoising (MFD) approach significantly outperforms the state of the\nart denoising meth- ods, and is robust to a wide range of parameter selections,\ne.g., the choice of k nearest neighbor connectivity of the graph.\n", "versions": [{"version": "v1", "created": "Tue, 29 Nov 2016 06:50:31 GMT"}], "update_date": "2016-11-30", "authors_parsed": [["Deutsch", "Shay", ""], ["Ortega", "Antonio", ""], ["Medioni", "Gerard", ""]]}, {"id": "1611.09621", "submitter": "Ankit Singh Rawat", "authors": "Arya Mazumdar and Ankit Singh Rawat", "title": "Associative Memory using Dictionary Learning and Expander Decoding", "comments": "To appear in AAAI 2017", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.IT cs.LG math.IT", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  An associative memory is a framework of content-addressable memory that\nstores a collection of message vectors (or a dataset) over a neural network\nwhile enabling a neurally feasible mechanism to recover any message in the\ndataset from its noisy version. Designing an associative memory requires\naddressing two main tasks: 1) learning phase: given a dataset, learn a concise\nrepresentation of the dataset in the form of a graphical model (or a neural\nnetwork), 2) recall phase: given a noisy version of a message vector from the\ndataset, output the correct message vector via a neurally feasible algorithm\nover the network learnt during the learning phase. This paper studies the\nproblem of designing a class of neural associative memories which learns a\nnetwork representation for a large dataset that ensures correction against a\nlarge number of adversarial errors during the recall phase. Specifically, the\nassociative memories designed in this paper can store dataset containing\n$\\exp(n)$ $n$-length message vectors over a network with $O(n)$ nodes and can\ntolerate $\\Omega(\\frac{n}{{\\rm polylog} n})$ adversarial errors. This paper\ncarries out this memory design by mapping the learning phase and recall phase\nto the tasks of dictionary learning with a square dictionary and iterative\nerror correction in an expander code, respectively.\n", "versions": [{"version": "v1", "created": "Tue, 29 Nov 2016 13:27:18 GMT"}], "update_date": "2016-11-30", "authors_parsed": [["Mazumdar", "Arya", ""], ["Rawat", "Ankit Singh", ""]]}, {"id": "1611.09630", "submitter": "Jakub Tomczak Ph.D.", "authors": "Jakub M. Tomczak and Max Welling", "title": "Improving Variational Auto-Encoders using Householder Flow", "comments": "A corrected version of the paper submitted to Bayesian Deep Learning\n  Workshop (NIPS 2016)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Variational auto-encoders (VAE) are scalable and powerful generative models.\nHowever, the choice of the variational posterior determines tractability and\nflexibility of the VAE. Commonly, latent variables are modeled using the normal\ndistribution with a diagonal covariance matrix. This results in computational\nefficiency but typically it is not flexible enough to match the true posterior\ndistribution. One fashion of enriching the variational posterior distribution\nis application of normalizing flows, i.e., a series of invertible\ntransformations to latent variables with a simple posterior. In this paper, we\nfollow this line of thinking and propose a volume-preserving flow that uses a\nseries of Householder transformations. We show empirically on MNIST dataset and\nhistopathology data that the proposed flow allows to obtain more flexible\nvariational posterior and competitive results comparing to other normalizing\nflows.\n", "versions": [{"version": "v1", "created": "Tue, 29 Nov 2016 13:49:31 GMT"}, {"version": "v2", "created": "Wed, 7 Dec 2016 12:04:28 GMT"}, {"version": "v3", "created": "Sun, 22 Jan 2017 18:49:14 GMT"}, {"version": "v4", "created": "Fri, 27 Jan 2017 00:36:51 GMT"}], "update_date": "2017-01-30", "authors_parsed": [["Tomczak", "Jakub M.", ""], ["Welling", "Max", ""]]}, {"id": "1611.09706", "submitter": "Kira Kempinska", "authors": "Kira Kempinska, Toby Davies, John Shawe-Taylor", "title": "Probabilistic map-matching using particle filters", "comments": "Proceedings of GISRUK 2016 conference", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Increasing availability of vehicle GPS data has created potentially\ntransformative opportunities for traffic management, route planning and other\nlocation-based services. Critical to the utility of the data is their accuracy.\nMap-matching is the process of improving the accuracy by aligning GPS data with\nthe road network. In this paper, we propose a purely probabilistic approach to\nmap-matching based on a sequential Monte Carlo algorithm known as particle\nfilters. The approach performs map-matching by producing a range of candidate\nsolutions, each with an associated probability score. We outline implementation\ndetails and thoroughly validate the technique on GPS data of varied quality.\n", "versions": [{"version": "v1", "created": "Tue, 29 Nov 2016 16:23:18 GMT"}], "update_date": "2016-11-30", "authors_parsed": [["Kempinska", "Kira", ""], ["Davies", "Toby", ""], ["Shawe-Taylor", "John", ""]]}, {"id": "1611.09726", "submitter": "Michael Blot", "authors": "Michael Blot, David Picard, Matthieu Cord, Nicolas Thome", "title": "Gossip training for deep learning", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We address the issue of speeding up the training of convolutional networks.\nHere we study a distributed method adapted to stochastic gradient descent\n(SGD). The parallel optimization setup uses several threads, each applying\nindividual gradient descents on a local variable. We propose a new way to share\ninformation between different threads inspired by gossip algorithms and showing\ngood consensus convergence properties. Our method called GoSGD has the\nadvantage to be fully asynchronous and decentralized. We compared our method to\nthe recent EASGD in \\cite{elastic} on CIFAR-10 show encouraging results.\n", "versions": [{"version": "v1", "created": "Tue, 29 Nov 2016 17:01:31 GMT"}], "update_date": "2016-11-30", "authors_parsed": [["Blot", "Michael", ""], ["Picard", "David", ""], ["Cord", "Matthieu", ""], ["Thome", "Nicolas", ""]]}, {"id": "1611.09791", "submitter": "Saptarshi Das", "authors": "Wasifa Jamal, Saptarshi Das, Koushik Maharatna, Fabio Apicella,\n  Georgia Chronaki, Federico Sicca, David Cohen, Filippo Muratori", "title": "On the Existence of Synchrostates in Multichannel EEG Signals during\n  Face-perception Tasks", "comments": "30 pages, 22 figures, 2 tables", "journal-ref": "Biomedical Physics & Engineering Express, vol. 1, no. 1, pp.\n  015002, 2015", "doi": "10.1088/2057-1976/1/1/015002", "report-no": null, "categories": "physics.med-ph cs.CV stat.AP stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Phase synchronisation in multichannel EEG is known as the manifestation of\nfunctional brain connectivity. Traditional phase synchronisation studies are\nmostly based on time average synchrony measures hence do not preserve the\ntemporal evolution of the phase difference. Here we propose a new method to\nshow the existence of a small set of unique phase synchronised patterns or\n\"states\" in multi-channel EEG recordings, each \"state\" being stable of the\norder of ms, from typical and pathological subjects during face perception\ntasks. The proposed methodology bridges the concepts of EEG microstates and\nphase synchronisation in time and frequency domain respectively. The analysis\nis reported for four groups of children including typical, Autism Spectrum\nDisorder (ASD), low and high anxiety subjects - a total of 44 subjects. In all\ncases, we observe consistent existence of these states - termed as\nsynchrostates - within specific cognition related frequency bands (beta and\ngamma bands), though the topographies of these synchrostates differ for\ndifferent subject groups with different pathological conditions. The\ninter-synchrostate switching follows a well-defined sequence capturing the\nunderlying inter-electrode phase relation dynamics in stimulus- and\nperson-centric manner. Our study is motivated from the well-known EEG\nmicrostate exhibiting stable potential maps over the scalp. However, here we\nreport a similar observation of quasi-stable phase synchronised states in\nmultichannel EEG. The existence of the synchrostates coupled with their unique\nswitching sequence characteristics could be considered as a potentially new\nfield over contemporary EEG phase synchronisation studies.\n", "versions": [{"version": "v1", "created": "Tue, 29 Nov 2016 19:03:29 GMT"}], "update_date": "2016-11-30", "authors_parsed": [["Jamal", "Wasifa", ""], ["Das", "Saptarshi", ""], ["Maharatna", "Koushik", ""], ["Apicella", "Fabio", ""], ["Chronaki", "Georgia", ""], ["Sicca", "Federico", ""], ["Cohen", "David", ""], ["Muratori", "Filippo", ""]]}, {"id": "1611.09805", "submitter": "Ming Yan", "authors": "Ming Yan", "title": "A new primal-dual algorithm for minimizing the sum of three functions\n  with a linear operator", "comments": "v2 added the ergodic and nonergodic convergence rates for the\n  primal-dual gap; v3 added the infimal convolution result and changed the\n  title; v4 modifies the primal-dual gap and add more recent references", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.OC math.NA stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we propose a new primal-dual algorithm for minimizing $f(x) +\ng(x) + h(Ax)$, where $f$, $g$, and $h$ are proper lower semi-continuous convex\nfunctions, $f$ is differentiable with a Lipschitz continuous gradient, and $A$\nis a bounded linear operator. The proposed algorithm has some famous\nprimal-dual algorithms for minimizing the sum of two functions as special\ncases. E.g., it reduces to the Chambolle-Pock algorithm when $f = 0$ and the\nproximal alternating predictor-corrector when $g = 0$. For the general convex\ncase, we prove the convergence of this new algorithm in terms of the distance\nto a fixed point by showing that the iteration is a nonexpansive operator. In\naddition, we prove the $O(1/k)$ ergodic convergence rate in the primal-dual\ngap. With additional assumptions, we derive the linear convergence rate in\nterms of the distance to the fixed point. Comparing to other primal-dual\nalgorithms for solving the same problem, this algorithm extends the range of\nacceptable parameters to ensure its convergence and has a smaller per-iteration\ncost. The numerical experiments show the efficiency of this algorithm.\n", "versions": [{"version": "v1", "created": "Tue, 29 Nov 2016 19:40:25 GMT"}, {"version": "v2", "created": "Wed, 26 Apr 2017 18:50:00 GMT"}, {"version": "v3", "created": "Fri, 12 May 2017 22:37:52 GMT"}, {"version": "v4", "created": "Mon, 29 Jan 2018 15:06:08 GMT"}], "update_date": "2018-01-30", "authors_parsed": [["Yan", "Ming", ""]]}, {"id": "1611.09816", "submitter": "Michael Rabadi", "authors": "Michael Rabadi", "title": "Co-adaptive learning over a countable space", "comments": "6 pages, 1 figure, NIPS 2016 Time Series Workshop", "journal-ref": "In NIPS 2016 Time Series Workshop. Barcelona, Spain", "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Co-adaptation is a special form of on-line learning where an algorithm\n$\\mathcal{A}$ must assist an unknown algorithm $\\mathcal{B}$ to perform some\ntask. This is a general framework and has applications in recommendation\nsystems, search, education, and much more. Today, the most common use of\nco-adaptive algorithms is in brain-computer interfacing (BCI), where algorithms\nhelp patients gain and maintain control over prosthetic devices. While previous\nstudies have shown strong empirical results Kowalski et al. (2013); Orsborn et\nal. (2014) or have been studied in specific examples Merel et al. (2013, 2015),\nthere is no general analysis of the co-adaptive learning problem. Here we will\nstudy the co-adaptive learning problem in the online, closed-loop setting. We\nwill prove that, with high probability, co-adaptive learning is guaranteed to\noutperform learning with a fixed decoder as long as a particular condition is\nmet.\n", "versions": [{"version": "v1", "created": "Tue, 29 Nov 2016 20:07:32 GMT"}, {"version": "v2", "created": "Wed, 30 Nov 2016 20:25:36 GMT"}], "update_date": "2016-12-01", "authors_parsed": [["Rabadi", "Michael", ""]]}, {"id": "1611.09820", "submitter": "Saptarshi Das", "authors": "Shre Kumar Chatterjee, Saptarshi Das, Koushik Maharatna, Elisa Masi,\n  Luisa Santopolo, Stefano Mancuso and Andrea Vitaletti", "title": "Exploring Strategies for Classification of External Stimuli Using\n  Statistical Features of the Plant Electrical Response", "comments": "22 pages, 7 figures, 9 tables", "journal-ref": "Journal of the Royal Society Interface, vol. 12, no. 104, pp.\n  20141225, March 2015", "doi": "10.1098/rsif.2014.1225", "report-no": null, "categories": "physics.bio-ph q-bio.QM stat.AP stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Plants sense their environment by producing electrical signals which in\nessence represent changes in underlying physiological processes. These\nelectrical signals, when monitored, show both stochastic and deterministic\ndynamics. In this paper, we compute 11 statistical features from the raw\nnon-stationary plant electrical signal time series to classify the stimulus\napplied (causing the electrical signal). By using different discriminant\nanalysis based classification techniques, we successfully establish that there\nis enough information in the raw electrical signal to classify the stimuli. In\nthe process, we also propose two standard features which consistently give good\nclassification results for three types of stimuli - Sodium Chloride (NaCl),\nSulphuric Acid (H2SO4) and Ozone (O3). This may facilitate reduction in the\ncomplexity involved in computing all the features for online classification of\nsimilar external stimuli in future.\n", "versions": [{"version": "v1", "created": "Tue, 29 Nov 2016 20:16:11 GMT"}], "update_date": "2016-11-30", "authors_parsed": [["Chatterjee", "Shre Kumar", ""], ["Das", "Saptarshi", ""], ["Maharatna", "Koushik", ""], ["Masi", "Elisa", ""], ["Santopolo", "Luisa", ""], ["Mancuso", "Stefano", ""], ["Vitaletti", "Andrea", ""]]}, {"id": "1611.09827", "submitter": "John Thickstun", "authors": "John Thickstun, Zaid Harchaoui, Sham Kakade", "title": "Learning Features of Music from Scratch", "comments": "14 pages; camera-ready version; updated experiments and related\n  works; additional MIR metrics (Appendix C)", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG cs.SD", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper introduces a new large-scale music dataset, MusicNet, to serve as\na source of supervision and evaluation of machine learning methods for music\nresearch. MusicNet consists of hundreds of freely-licensed classical music\nrecordings by 10 composers, written for 11 instruments, together with\ninstrument/note annotations resulting in over 1 million temporal labels on 34\nhours of chamber music performances under various studio and microphone\nconditions.\n  The paper defines a multi-label classification task to predict notes in\nmusical recordings, along with an evaluation protocol, and benchmarks several\nmachine learning architectures for this task: i) learning from spectrogram\nfeatures; ii) end-to-end learning with a neural net; iii) end-to-end learning\nwith a convolutional neural net. These experiments show that end-to-end models\ntrained for note prediction learn frequency selective filters as a low-level\nrepresentation of audio.\n", "versions": [{"version": "v1", "created": "Tue, 29 Nov 2016 20:26:00 GMT"}, {"version": "v2", "created": "Thu, 6 Apr 2017 01:13:41 GMT"}], "update_date": "2017-04-07", "authors_parsed": [["Thickstun", "John", ""], ["Harchaoui", "Zaid", ""], ["Kakade", "Sham", ""]]}, {"id": "1611.09878", "submitter": "Jian Tang", "authors": "Jian Tang, Meng Qu, and Qiaozhu Mei", "title": "Identity-sensitive Word Embedding through Heterogeneous Networks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.LG stat.ML", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Most existing word embedding approaches do not distinguish the same words in\ndifferent contexts, therefore ignoring their contextual meanings. As a result,\nthe learned embeddings of these words are usually a mixture of multiple\nmeanings. In this paper, we acknowledge multiple identities of the same word in\ndifferent contexts and learn the \\textbf{identity-sensitive} word embeddings.\nBased on an identity-labeled text corpora, a heterogeneous network of words and\nword identities is constructed to model different-levels of word\nco-occurrences. The heterogeneous network is further embedded into a\nlow-dimensional space through a principled network embedding approach, through\nwhich we are able to obtain the embeddings of words and the embeddings of word\nidentities. We study three different types of word identities including topics,\nsentiments and categories. Experimental results on real-world data sets show\nthat the identity-sensitive word embeddings learned by our approach indeed\ncapture different meanings of words and outperforms competitive methods on\ntasks including text classification and word similarity computation.\n", "versions": [{"version": "v1", "created": "Tue, 29 Nov 2016 21:12:04 GMT"}], "update_date": "2016-12-04", "authors_parsed": [["Tang", "Jian", ""], ["Qu", "Meng", ""], ["Mei", "Qiaozhu", ""]]}, {"id": "1611.09894", "submitter": "Sai Praveen Bangaru", "authors": "Sai Praveen Bangaru, JS Suhas and Balaraman Ravindran", "title": "Exploration for Multi-task Reinforcement Learning with Deep Generative\n  Models", "comments": "9 pages, 5 figures; NIPS Deep Reinforcement Learning Workshop 2016,\n  Barcelona", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Exploration in multi-task reinforcement learning is critical in training\nagents to deduce the underlying MDP. Many of the existing exploration\nframeworks such as $E^3$, $R_{max}$, Thompson sampling assume a single\nstationary MDP and are not suitable for system identification in the multi-task\nsetting. We present a novel method to facilitate exploration in multi-task\nreinforcement learning using deep generative models. We supplement our method\nwith a low dimensional energy model to learn the underlying MDP distribution\nand provide a resilient and adaptive exploration signal to the agent. We\nevaluate our method on a new set of environments and provide intuitive\ninterpretation of our results.\n", "versions": [{"version": "v1", "created": "Tue, 29 Nov 2016 21:32:25 GMT"}], "update_date": "2016-12-04", "authors_parsed": [["Bangaru", "Sai Praveen", ""], ["Suhas", "JS", ""], ["Ravindran", "Balaraman", ""]]}, {"id": "1611.09897", "submitter": "Rushil Anirudh", "authors": "Rushil Anirudh, Jayaraman J. Thiagarajan, Irene Kim, Wolfgang Polonik", "title": "Autism Spectrum Disorder Classification using Graph Kernels on\n  Multidimensional Time Series", "comments": "Under review as a conference paper to BHI '17", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present an approach to model time series data from resting state fMRI for\nautism spectrum disorder (ASD) severity classification. We propose to adopt\nkernel machines and employ graph kernels that define a kernel dot product\nbetween two graphs. This enables us to take advantage of spatio-temporal\ninformation to capture the dynamics of the brain network, as opposed to\naggregating them in the spatial or temporal dimension. In addition to the\nconventional similarity graphs, we explore the use of L1 graph using sparse\ncoding, and the persistent homology of time delay embeddings, in the proposed\npipeline for ASD classification. In our experiments on two datasets from the\nABIDE collection, we demonstrate a consistent and significant advantage in\nusing graph kernels over traditional linear or non linear kernels for a variety\nof time series features.\n", "versions": [{"version": "v1", "created": "Tue, 29 Nov 2016 21:39:23 GMT"}], "update_date": "2016-12-04", "authors_parsed": [["Anirudh", "Rushil", ""], ["Thiagarajan", "Jayaraman J.", ""], ["Kim", "Irene", ""], ["Polonik", "Wolfgang", ""]]}, {"id": "1611.09913", "submitter": "Jasmine Collins", "authors": "Jasmine Collins, Jascha Sohl-Dickstein and David Sussillo", "title": "Capacity and Trainability in Recurrent Neural Networks", "comments": "Published as a conference paper at ICLR 2017", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.AI cs.LG cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Two potential bottlenecks on the expressiveness of recurrent neural networks\n(RNNs) are their ability to store information about the task in their\nparameters, and to store information about the input history in their units. We\nshow experimentally that all common RNN architectures achieve nearly the same\nper-task and per-unit capacity bounds with careful training, for a variety of\ntasks and stacking depths. They can store an amount of task information which\nis linear in the number of parameters, and is approximately 5 bits per\nparameter. They can additionally store approximately one real number from their\ninput history per hidden unit. We further find that for several tasks it is the\nper-task parameter capacity bound that determines performance. These results\nsuggest that many previous results comparing RNN architectures are driven\nprimarily by differences in training effectiveness, rather than differences in\ncapacity. Supporting this observation, we compare training difficulty for\nseveral architectures, and show that vanilla RNNs are far more difficult to\ntrain, yet have slightly higher capacity. Finally, we propose two novel RNN\narchitectures, one of which is easier to train than the LSTM or GRU for deeply\nstacked architectures.\n", "versions": [{"version": "v1", "created": "Tue, 29 Nov 2016 22:13:20 GMT"}, {"version": "v2", "created": "Tue, 28 Feb 2017 22:21:44 GMT"}, {"version": "v3", "created": "Fri, 3 Mar 2017 17:39:34 GMT"}], "update_date": "2017-03-06", "authors_parsed": [["Collins", "Jasmine", ""], ["Sohl-Dickstein", "Jascha", ""], ["Sussillo", "David", ""]]}, {"id": "1611.09940", "submitter": "Irwan Bello", "authors": "Irwan Bello, Hieu Pham, Quoc V. Le, Mohammad Norouzi, Samy Bengio", "title": "Neural Combinatorial Optimization with Reinforcement Learning", "comments": "Under review as a conference paper at ICLR 2017", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper presents a framework to tackle combinatorial optimization problems\nusing neural networks and reinforcement learning. We focus on the traveling\nsalesman problem (TSP) and train a recurrent network that, given a set of city\ncoordinates, predicts a distribution over different city permutations. Using\nnegative tour length as the reward signal, we optimize the parameters of the\nrecurrent network using a policy gradient method. We compare learning the\nnetwork parameters on a set of training graphs against learning them on\nindividual test graphs. Despite the computational expense, without much\nengineering and heuristic designing, Neural Combinatorial Optimization achieves\nclose to optimal results on 2D Euclidean graphs with up to 100 nodes. Applied\nto the KnapSack, another NP-hard problem, the same method obtains optimal\nsolutions for instances with up to 200 items.\n", "versions": [{"version": "v1", "created": "Tue, 29 Nov 2016 23:22:39 GMT"}, {"version": "v2", "created": "Sun, 11 Dec 2016 00:31:39 GMT"}, {"version": "v3", "created": "Thu, 12 Jan 2017 23:55:36 GMT"}], "update_date": "2017-01-16", "authors_parsed": [["Bello", "Irwan", ""], ["Pham", "Hieu", ""], ["Le", "Quoc V.", ""], ["Norouzi", "Mohammad", ""], ["Bengio", "Samy", ""]]}, {"id": "1611.09957", "submitter": "Ehsan Amid", "authors": "Ehsan Amid, Nikos Vlassis, Manfred K. Warmuth", "title": "Low-dimensional Data Embedding via Robust Ranking", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We describe a new method called t-ETE for finding a low-dimensional embedding\nof a set of objects in Euclidean space. We formulate the embedding problem as a\njoint ranking problem over a set of triplets, where each triplet captures the\nrelative similarities between three objects in the set. By exploiting recent\nadvances in robust ranking, t-ETE produces high-quality embeddings even in the\npresence of a significant amount of noise and better preserves local scale than\nknown methods, such as t-STE and t-SNE. In particular, our method produces\nsignificantly better results than t-SNE on signature datasets while also being\nfaster to compute.\n", "versions": [{"version": "v1", "created": "Wed, 30 Nov 2016 01:03:11 GMT"}, {"version": "v2", "created": "Tue, 16 May 2017 21:21:03 GMT"}], "update_date": "2017-05-18", "authors_parsed": [["Amid", "Ehsan", ""], ["Vlassis", "Nikos", ""], ["Warmuth", "Manfred K.", ""]]}, {"id": "1611.09958", "submitter": "Young-Jun Yu", "authors": "Young-jun Yu", "title": "Machine Learning for Dental Image Analysis", "comments": "This study was reviewed and approved by the institutional review\n  board of the Pusan National University Dental Hospital (PNUPH-2015-034)", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In order to study the application of artificial intelligence (AI) to dental\nimaging, we applied AI technology to classify a set of panoramic radiographs\nusing (a) a convolutional neural network (CNN) which is a form of an artificial\nneural network (ANN), (b) representative image cognition algorithms that\nimplement scale-invariant feature transform (SIFT), and (c) histogram of\noriented gradients (HOG).\n", "versions": [{"version": "v1", "created": "Wed, 30 Nov 2016 01:17:34 GMT"}, {"version": "v2", "created": "Fri, 2 Dec 2016 11:27:37 GMT"}], "update_date": "2016-12-05", "authors_parsed": [["Yu", "Young-jun", ""]]}, {"id": "1611.09972", "submitter": "Asad Haris", "authors": "Asad Haris, Ali Shojaie, Noah Simon", "title": "Nonparametric Regression with Adaptive Truncation via a Convex\n  Hierarchical Penalty", "comments": null, "journal-ref": "Biometrika 2018, Vol. 106, No. 1, 87-107", "doi": "10.1093/biomet/asy056", "report-no": null, "categories": "stat.ME math.ST stat.ML stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider the problem of non-parametric regression with a potentially large\nnumber of covariates. We propose a convex, penalized estimation framework that\nis particularly well-suited for high-dimensional sparse additive models. The\nproposed approach combines appealing features of finite basis representation\nand smoothing penalties for non-parametric estimation. In particular, in the\ncase of additive models, a finite basis representation provides a parsimonious\nrepresentation for fitted functions but is not adaptive when component\nfunctions posses different levels of complexity. On the other hand, a smoothing\nspline type penalty on the component functions is adaptive but does not offer a\nparsimonious representation of the estimated function. The proposed approach\nsimultaneously achieves parsimony and adaptivity in a computationally efficient\nframework. We demonstrate these properties through empirical studies on both\nreal and simulated datasets. We show that our estimator converges at the\nminimax rate for functions within a hierarchical class. We further establish\nminimax rates for a large class of sparse additive models. The proposed method\nis implemented using an efficient algorithm that scales similarly to the Lasso\nwith the number of covariates and samples size.\n", "versions": [{"version": "v1", "created": "Wed, 30 Nov 2016 02:22:31 GMT"}, {"version": "v2", "created": "Thu, 1 Dec 2016 22:28:45 GMT"}, {"version": "v3", "created": "Thu, 13 Jun 2019 15:43:02 GMT"}, {"version": "v4", "created": "Tue, 18 Jun 2019 12:22:21 GMT"}], "update_date": "2019-06-19", "authors_parsed": [["Haris", "Asad", ""], ["Shojaie", "Ali", ""], ["Simon", "Noah", ""]]}, {"id": "1611.10031", "submitter": "Peng Liu", "authors": "Peng Liu, Hui Zhang, and Kie B. Eom", "title": "Active Deep Learning for Classification of Hyperspectral Images", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Active deep learning classification of hyperspectral images is considered in\nthis paper. Deep learning has achieved success in many applications, but\ngood-quality labeled samples are needed to construct a deep learning network.\nIt is expensive getting good labeled samples in hyperspectral images for remote\nsensing applications. An active learning algorithm based on a weighted\nincremental dictionary learning is proposed for such applications. The proposed\nalgorithm selects training samples that maximize two selection criteria, namely\nrepresentative and uncertainty. This algorithm trains a deep network\nefficiently by actively selecting training samples at each iteration. The\nproposed algorithm is applied for the classification of hyperspectral images,\nand compared with other classification algorithms employing active learning. It\nis shown that the proposed algorithm is efficient and effective in classifying\nhyperspectral images.\n", "versions": [{"version": "v1", "created": "Wed, 30 Nov 2016 07:34:46 GMT"}], "update_date": "2016-12-04", "authors_parsed": [["Liu", "Peng", ""], ["Zhang", "Hui", ""], ["Eom", "Kie B.", ""]]}, {"id": "1611.10041", "submitter": "Arthur Mensch", "authors": "Arthur Mensch (PARIETAL), Julien Mairal (LEAR), Ga\\\"el Varoquaux\n  (PARIETAL), Bertrand Thirion (PARIETAL)", "title": "Subsampled online matrix factorization with convergence guarantees", "comments": null, "journal-ref": "9th NIPS Workshop on Optimization for Machine Learning, Dec 2016,\n  Barcelone, Spain", "doi": null, "report-no": null, "categories": "math.OC cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a matrix factorization algorithm that scales to input matrices\nthat are large in both dimensions (i.e., that contains morethan 1TB of data).\nThe algorithm streams the matrix columns while subsampling them, resulting in\nlow complexity per iteration andreasonable memory footprint. In contrast to\nprevious online matrix factorization methods, our approach relies on\nlow-dimensional statistics from past iterates to control the extra variance\nintroduced by subsampling. We present a convergence analysis that guarantees us\nto reach a stationary point of the problem. Large speed-ups can be obtained\ncompared to previous online algorithms that do not perform subsampling, thanks\nto the feature redundancy that often exists in high-dimensional settings.\n", "versions": [{"version": "v1", "created": "Wed, 30 Nov 2016 08:10:58 GMT"}], "update_date": "2016-12-04", "authors_parsed": [["Mensch", "Arthur", "", "PARIETAL"], ["Mairal", "Julien", "", "LEAR"], ["Varoquaux", "Ga\u00ebl", "", "PARIETAL"], ["Thirion", "Bertrand", "", "PARIETAL"]]}, {"id": "1611.10073", "submitter": "Luca Ambrogioni", "authors": "Luca Ambrogioni and Eric Maris", "title": "Complex-valued Gaussian Process Regression for Time Series Analysis", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The construction of synthetic complex-valued signals from real-valued\nobservations is an important step in many time series analysis techniques. The\nmost widely used approach is based on the Hilbert transform, which maps the\nreal-valued signal into its quadrature component. In this paper, we define a\nprobabilistic generalization of this approach. We model the observable\nreal-valued signal as the real part of a latent complex-valued Gaussian\nprocess. In order to obtain the appropriate statistical relationship between\nits real and imaginary parts, we define two new classes of complex-valued\ncovariance functions. Through an analysis of simulated chirplets and stochastic\noscillations, we show that the resulting Gaussian process complex-valued signal\nprovides a better estimate of the instantaneous amplitude and frequency than\nthe established approaches. Furthermore, the complex-valued Gaussian process\nregression allows to incorporate prior information about the structure in\nsignal and noise and thereby to tailor the analysis to the features of the\nsignal. As a example, we analyze the non-stationary dynamics of brain\noscillations in the alpha band, as measured using magneto-encephalography.\n", "versions": [{"version": "v1", "created": "Wed, 30 Nov 2016 10:02:01 GMT"}, {"version": "v2", "created": "Thu, 7 Dec 2017 00:35:40 GMT"}], "update_date": "2017-12-08", "authors_parsed": [["Ambrogioni", "Luca", ""], ["Maris", "Eric", ""]]}, {"id": "1611.10171", "submitter": "Janek Thomas", "authors": "Janek Thomas, Andreas Mayr, Bernd Bischl, Matthias Schmid, Adam Smith,\n  Benjamin Hofner", "title": "Stability selection for component-wise gradient boosting in multiple\n  dimensions", "comments": "16 pages", "journal-ref": null, "doi": "10.1007/s11222-017-9754-6", "report-no": null, "categories": "stat.CO stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a new algorithm for boosting generalized additive models for\nlocation, scale and shape (GAMLSS) that allows to incorporate stability\nselection, an increasingly popular way to obtain stable sets of covariates\nwhile controlling the per-family error rate (PFER). The model is fitted\nrepeatedly to subsampled data and variables with high selection frequencies are\nextracted. To apply stability selection to boosted GAMLSS, we develop a new\n\"noncyclical\" fitting algorithm that incorporates an additional selection step\nof the best-fitting distribution parameter in each iteration. This new\nalgorithms has the additional advantage that optimizing the tuning parameters\nof boosting is reduced from a multi-dimensional to a one-dimensional problem\nwith vastly decreased complexity. The performance of the novel algorithm is\nevaluated in an extensive simulation study. We apply this new algorithm to a\nstudy to estimate abundance of common eider in Massachusetts, USA, featuring\nexcess zeros, overdispersion, non-linearity and spatio-temporal structures.\nEider abundance is estimated via boosted GAMLSS, allowing both mean and\noverdispersion to be regressed on covariates. Stability selection is used to\nobtain a sparse set of stable predictors.\n", "versions": [{"version": "v1", "created": "Wed, 30 Nov 2016 14:28:09 GMT"}], "update_date": "2017-05-16", "authors_parsed": [["Thomas", "Janek", ""], ["Mayr", "Andreas", ""], ["Bischl", "Bernd", ""], ["Schmid", "Matthias", ""], ["Smith", "Adam", ""], ["Hofner", "Benjamin", ""]]}, {"id": "1611.10242", "submitter": "Owen Thomas", "authors": "Owen Thomas, Ritabrata Dutta, Jukka Corander, Samuel Kaski, Michael U.\n  Gutmann", "title": "Likelihood-free inference by ratio estimation", "comments": "Accepted to Bayesian Analysis (2020)", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML stat.CO stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider the problem of parametric statistical inference when likelihood\ncomputations are prohibitively expensive but sampling from the model is\npossible. Several so-called likelihood-free methods have been developed to\nperform inference in the absence of a likelihood function. The popular\nsynthetic likelihood approach infers the parameters by modelling summary\nstatistics of the data by a Gaussian probability distribution. In another\npopular approach called approximate Bayesian computation, the inference is\nperformed by identifying parameter values for which the summary statistics of\nthe simulated data are close to those of the observed data. Synthetic\nlikelihood is easier to use as no measure of `closeness' is required but the\nGaussianity assumption is often limiting. Moreover, both approaches require\njudiciously chosen summary statistics. We here present an alternative inference\napproach that is as easy to use as synthetic likelihood but not as restricted\nin its assumptions, and that, in a natural way, enables automatic selection of\nrelevant summary statistic from a large set of candidates. The basic idea is to\nframe the problem of estimating the posterior as a problem of estimating the\nratio between the data generating distribution and the marginal distribution.\nThis problem can be solved by logistic regression, and including regularising\npenalty terms enables automatic selection of the summary statistics relevant to\nthe inference task. We illustrate the general theory on canonical examples and\nemploy it to perform inference for challenging stochastic nonlinear dynamical\nsystems and high-dimensional summary statistics.\n", "versions": [{"version": "v1", "created": "Wed, 30 Nov 2016 16:03:07 GMT"}, {"version": "v2", "created": "Fri, 17 Mar 2017 19:38:59 GMT"}, {"version": "v3", "created": "Mon, 21 Aug 2017 15:44:21 GMT"}, {"version": "v4", "created": "Tue, 18 Dec 2018 10:14:43 GMT"}, {"version": "v5", "created": "Wed, 16 Oct 2019 15:09:31 GMT"}, {"version": "v6", "created": "Fri, 11 Sep 2020 09:28:01 GMT"}], "update_date": "2020-09-14", "authors_parsed": [["Thomas", "Owen", ""], ["Dutta", "Ritabrata", ""], ["Corander", "Jukka", ""], ["Kaski", "Samuel", ""], ["Gutmann", "Michael U.", ""]]}, {"id": "1611.10258", "submitter": "Varun Kanade", "authors": "Surbhi Goel, Varun Kanade, Adam Klivans, Justin Thaler", "title": "Reliably Learning the ReLU in Polynomial Time", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CC stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We give the first dimension-efficient algorithms for learning Rectified\nLinear Units (ReLUs), which are functions of the form $\\mathbf{x} \\mapsto\n\\max(0, \\mathbf{w} \\cdot \\mathbf{x})$ with $\\mathbf{w} \\in \\mathbb{S}^{n-1}$.\nOur algorithm works in the challenging Reliable Agnostic learning model of\nKalai, Kanade, and Mansour (2009) where the learner is given access to a\ndistribution $\\cal{D}$ on labeled examples but the labeling may be arbitrary.\nWe construct a hypothesis that simultaneously minimizes the false-positive rate\nand the loss on inputs given positive labels by $\\cal{D}$, for any convex,\nbounded, and Lipschitz loss function.\n  The algorithm runs in polynomial-time (in $n$) with respect to any\ndistribution on $\\mathbb{S}^{n-1}$ (the unit sphere in $n$ dimensions) and for\nany error parameter $\\epsilon = \\Omega(1/\\log n)$ (this yields a PTAS for a\nquestion raised by F. Bach on the complexity of maximizing ReLUs). These\nresults are in contrast to known efficient algorithms for reliably learning\nlinear threshold functions, where $\\epsilon$ must be $\\Omega(1)$ and strong\nassumptions are required on the marginal distribution. We can compose our\nresults to obtain the first set of efficient algorithms for learning\nconstant-depth networks of ReLUs.\n  Our techniques combine kernel methods and polynomial approximations with a\n\"dual-loss\" approach to convex programming. As a byproduct we obtain a number\nof applications including the first set of efficient algorithms for \"convex\npiecewise-linear fitting\" and the first efficient algorithms for noisy\npolynomial reconstruction of low-weight polynomials on the unit sphere.\n", "versions": [{"version": "v1", "created": "Wed, 30 Nov 2016 16:42:23 GMT"}], "update_date": "2016-12-06", "authors_parsed": [["Goel", "Surbhi", ""], ["Kanade", "Varun", ""], ["Klivans", "Adam", ""], ["Thaler", "Justin", ""]]}, {"id": "1611.10277", "submitter": "Ryan Gallagher", "authors": "Ryan J. Gallagher, Kyle Reing, David Kale, Greg Ver Steeg", "title": "Anchored Correlation Explanation: Topic Modeling with Minimal Domain\n  Knowledge", "comments": "21 pages, 7 figures. 2018/09/03: Updated citation for HA/DR dataset", "journal-ref": "Transactions of the Association for Computational Linguistics\n  (TACL), Vol. 5, 2017", "doi": null, "report-no": null, "categories": "cs.CL cs.IR cs.IT math.IT stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  While generative models such as Latent Dirichlet Allocation (LDA) have proven\nfruitful in topic modeling, they often require detailed assumptions and careful\nspecification of hyperparameters. Such model complexity issues only compound\nwhen trying to generalize generative models to incorporate human input. We\nintroduce Correlation Explanation (CorEx), an alternative approach to topic\nmodeling that does not assume an underlying generative model, and instead\nlearns maximally informative topics through an information-theoretic framework.\nThis framework naturally generalizes to hierarchical and semi-supervised\nextensions with no additional modeling assumptions. In particular, word-level\ndomain knowledge can be flexibly incorporated within CorEx through anchor\nwords, allowing topic separability and representation to be promoted with\nminimal human intervention. Across a variety of datasets, metrics, and\nexperiments, we demonstrate that CorEx produces topics that are comparable in\nquality to those produced by unsupervised and semi-supervised variants of LDA.\n", "versions": [{"version": "v1", "created": "Wed, 30 Nov 2016 17:32:17 GMT"}, {"version": "v2", "created": "Fri, 28 Jul 2017 17:41:04 GMT"}, {"version": "v3", "created": "Mon, 4 Dec 2017 03:53:19 GMT"}, {"version": "v4", "created": "Mon, 3 Sep 2018 15:23:40 GMT"}], "update_date": "2018-09-05", "authors_parsed": [["Gallagher", "Ryan J.", ""], ["Reing", "Kyle", ""], ["Kale", "David", ""], ["Steeg", "Greg Ver", ""]]}, {"id": "1611.10283", "submitter": "L.A. Prashanth", "authors": "Aditya Gopalan, L.A. Prashanth, Michael Fu and Steve Marcus", "title": "Weighted bandits or: How bandits learn distorted values that are not\n  expected", "comments": "Longer version of the paper to be published as part of the\n  proceedings of AAAI 2017", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Motivated by models of human decision making proposed to explain commonly\nobserved deviations from conventional expected value preferences, we formulate\ntwo stochastic multi-armed bandit problems with distorted probabilities on the\ncost distributions: the classic $K$-armed bandit and the linearly parameterized\nbandit. In both settings, we propose algorithms that are inspired by Upper\nConfidence Bound (UCB), incorporate cost distortions, and exhibit sublinear\nregret assuming \\holder continuous weight distortion functions. For the\n$K$-armed setting, we show that the algorithm, called W-UCB, achieves\nproblem-dependent regret $O(L^2 M^2 \\log n/ \\Delta^{\\frac{2}{\\alpha}-1})$,\nwhere $n$ is the number of plays, $\\Delta$ is the gap in distorted expected\nvalue between the best and next best arm, $L$ and $\\alpha$ are the H\\\"{o}lder\nconstants for the distortion function, and $M$ is an upper bound on costs, and\na problem-independent regret bound of\n$O((KL^2M^2)^{\\alpha/2}n^{(2-\\alpha)/2})$. We also present a matching lower\nbound on the regret, showing that the regret of W-UCB is essentially\nunimprovable over the class of H\\\"{o}lder-continuous weight distortions. For\nthe linearly parameterized setting, we develop a new algorithm, a variant of\nthe Optimism in the Face of Uncertainty Linear bandit (OFUL) algorithm called\nWOFUL (Weight-distorted OFUL), and show that it has regret $O(d\\sqrt{n} \\;\n\\mbox{polylog}(n))$ with high probability, for sub-Gaussian cost distributions.\nFinally, numerical examples demonstrate the advantages resulting from using\ndistortion-aware learning algorithms.\n", "versions": [{"version": "v1", "created": "Wed, 30 Nov 2016 17:37:51 GMT"}], "update_date": "2016-12-01", "authors_parsed": [["Gopalan", "Aditya", ""], ["Prashanth", "L. A.", ""], ["Fu", "Michael", ""], ["Marcus", "Steve", ""]]}, {"id": "1611.10305", "submitter": "Qunwei Li", "authors": "Qunwei Li, Bhavya Kailkhura, Jayaraman J. Thiagarajan, Zhenliang\n  Zhang, Pramod K. Varshney", "title": "Influential Node Detection in Implicit Social Networks using Multi-task\n  Gaussian Copula Models", "comments": "NIPS 2016 Workshop, JMLR: Workshop and Conference Proceedings", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.SI cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Influential node detection is a central research topic in social network\nanalysis. Many existing methods rely on the assumption that the network\nstructure is completely known \\textit{a priori}. However, in many applications,\nnetwork structure is unavailable to explain the underlying information\ndiffusion phenomenon. To address the challenge of information diffusion\nanalysis with incomplete knowledge of network structure, we develop a\nmulti-task low rank linear influence model. By exploiting the relationships\nbetween contagions, our approach can simultaneously predict the volume (i.e.\ntime series prediction) for each contagion (or topic) and automatically\nidentify the most influential nodes for each contagion. The proposed model is\nvalidated using synthetic data and an ISIS twitter dataset. In addition to\nimproving the volume prediction performance significantly, we show that the\nproposed approach can reliably infer the most influential users for specific\ncontagions.\n", "versions": [{"version": "v1", "created": "Wed, 30 Nov 2016 18:46:55 GMT"}], "update_date": "2016-12-01", "authors_parsed": [["Li", "Qunwei", ""], ["Kailkhura", "Bhavya", ""], ["Thiagarajan", "Jayaraman J.", ""], ["Zhang", "Zhenliang", ""], ["Varshney", "Pramod K.", ""]]}, {"id": "1611.10349", "submitter": "Garvesh Raskutti", "authors": "Han Chen, Garvesh Raskutti, Ming Yuan", "title": "Non-Convex Projected Gradient Descent for Generalized Low-Rank Tensor\n  Regression", "comments": "42 pages, 6 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we consider the problem of learning high-dimensional tensor\nregression problems with low-rank structure. One of the core challenges\nassociated with learning high-dimensional models is computation since the\nunderlying optimization problems are often non-convex. While convex relaxations\ncould lead to polynomial-time algorithms they are often slow in practice. On\nthe other hand, limited theoretical guarantees exist for non-convex methods. In\nthis paper we provide a general framework that provides theoretical guarantees\nfor learning high-dimensional tensor regression models under different low-rank\nstructural assumptions using the projected gradient descent algorithm applied\nto a potentially non-convex constraint set $\\Theta$ in terms of its\n\\emph{localized Gaussian width}. We juxtapose our theoretical results for\nnon-convex projected gradient descent algorithms with previous results on\nregularized convex approaches. The two main differences between the convex and\nnon-convex approach are: (i) from a computational perspective whether the\nnon-convex projection operator is computable and whether the projection has\ndesirable contraction properties and (ii) from a statistical upper bound\nperspective, the non-convex approach has a superior rate for a number of\nexamples. We provide three concrete examples of low-dimensional structure which\naddress these issues and explain the pros and cons for the non-convex and\nconvex approaches. We supplement our theoretical results with simulations which\nshow that, under several common settings of generalized low rank tensor\nregression, the projected gradient descent approach is superior both in terms\nof statistical error and run-time provided the step-sizes of the projected\ndescent algorithm are suitably chosen.\n", "versions": [{"version": "v1", "created": "Wed, 30 Nov 2016 20:45:23 GMT"}], "update_date": "2016-12-01", "authors_parsed": [["Chen", "Han", ""], ["Raskutti", "Garvesh", ""], ["Yuan", "Ming", ""]]}, {"id": "1611.10351", "submitter": "Joris Mooij", "authors": "Joris M. Mooij, Sara Magliacane, Tom Claassen", "title": "Joint Causal Inference from Multiple Contexts", "comments": "Final version, as published by JMLR", "journal-ref": "Journal of Machine Learning Research 21(99):1-108, 2020", "doi": null, "report-no": null, "categories": "cs.LG cs.AI stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The gold standard for discovering causal relations is by means of\nexperimentation. Over the last decades, alternative methods have been proposed\nthat can infer causal relations between variables from certain statistical\npatterns in purely observational data. We introduce Joint Causal Inference\n(JCI), a novel approach to causal discovery from multiple data sets from\ndifferent contexts that elegantly unifies both approaches. JCI is a causal\nmodeling framework rather than a specific algorithm, and it can be implemented\nusing any causal discovery algorithm that can take into account certain\nbackground knowledge. JCI can deal with different types of interventions (e.g.,\nperfect, imperfect, stochastic, etc.) in a unified fashion, and does not\nrequire knowledge of intervention targets or types in case of interventional\ndata. We explain how several well-known causal discovery algorithms can be seen\nas addressing special cases of the JCI framework, and we also propose novel\nimplementations that extend existing causal discovery methods for purely\nobservational data to the JCI setting. We evaluate different JCI\nimplementations on synthetic data and on flow cytometry protein expression data\nand conclude that JCI implementations can considerably outperform\nstate-of-the-art causal discovery algorithms.\n", "versions": [{"version": "v1", "created": "Wed, 30 Nov 2016 20:50:33 GMT"}, {"version": "v2", "created": "Thu, 9 Mar 2017 12:43:04 GMT"}, {"version": "v3", "created": "Thu, 29 Mar 2018 21:18:18 GMT"}, {"version": "v4", "created": "Mon, 8 Apr 2019 12:42:55 GMT"}, {"version": "v5", "created": "Fri, 14 Feb 2020 12:14:39 GMT"}, {"version": "v6", "created": "Thu, 20 Aug 2020 08:23:07 GMT"}], "update_date": "2020-08-21", "authors_parsed": [["Mooij", "Joris M.", ""], ["Magliacane", "Sara", ""], ["Claassen", "Tom", ""]]}]