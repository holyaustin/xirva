[{"id": "1111.0352", "submitter": "Brian Kulis", "authors": "Brian Kulis and Michael I. Jordan", "title": "Revisiting k-means: New Algorithms via Bayesian Nonparametrics", "comments": "14 pages. Updated based on the corresponding ICML paper", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Bayesian models offer great flexibility for clustering\napplications---Bayesian nonparametrics can be used for modeling infinite\nmixtures, and hierarchical Bayesian models can be utilized for sharing clusters\nacross multiple data sets. For the most part, such flexibility is lacking in\nclassical clustering methods such as k-means. In this paper, we revisit the\nk-means clustering algorithm from a Bayesian nonparametric viewpoint. Inspired\nby the asymptotic connection between k-means and mixtures of Gaussians, we show\nthat a Gibbs sampling algorithm for the Dirichlet process mixture approaches a\nhard clustering algorithm in the limit, and further that the resulting\nalgorithm monotonically minimizes an elegant underlying k-means-like clustering\nobjective that includes a penalty for the number of clusters. We generalize\nthis analysis to the case of clustering multiple data sets through a similar\nasymptotic argument with the hierarchical Dirichlet process. We also discuss\nfurther extensions that highlight the benefits of our analysis: i) a spectral\nrelaxation involving thresholded eigenvectors, and ii) a normalized cut graph\nclustering algorithm that does not fix the number of clusters in the graph.\n", "versions": [{"version": "v1", "created": "Wed, 2 Nov 2011 00:09:18 GMT"}, {"version": "v2", "created": "Thu, 14 Jun 2012 15:05:55 GMT"}], "update_date": "2012-06-15", "authors_parsed": [["Kulis", "Brian", ""], ["Jordan", "Michael I.", ""]]}, {"id": "1111.0559", "submitter": "Mihai Cucuringu", "authors": "Mihai Cucuringu, Jesus Puente, David Shue", "title": "Model Selection in Undirected Graphical Models with the Elastic Net", "comments": "15 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.OT stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Structure learning in random fields has attracted considerable attention due\nto its difficulty and importance in areas such as remote sensing, computational\nbiology, natural language processing, protein networks, and social network\nanalysis. We consider the problem of estimating the probabilistic graph\nstructure associated with a Gaussian Markov Random Field (GMRF), the Ising\nmodel and the Potts model, by extending previous work on $l_1$ regularized\nneighborhood estimation to include the elastic net $l_1+l_2$ penalty.\nAdditionally, we show numerical evidence that the edge density plays a role in\nthe graph recovery process. Finally, we introduce a novel method for augmenting\nneighborhood estimation by leveraging pair-wise neighborhood union estimates.\n", "versions": [{"version": "v1", "created": "Wed, 2 Nov 2011 16:40:40 GMT"}], "update_date": "2011-11-03", "authors_parsed": [["Cucuringu", "Mihai", ""], ["Puente", "Jesus", ""], ["Shue", "David", ""]]}, {"id": "1111.0708", "submitter": "Pedro Alejandro Ortega", "authors": "Pedro A. Ortega", "title": "Bayesian Causal Induction", "comments": "4 pages, 4 figures; 2011 NIPS Workshop on Philosophy and Machine\n  Learning", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Discovering causal relationships is a hard task, often hindered by the need\nfor intervention, and often requiring large amounts of data to resolve\nstatistical uncertainty. However, humans quickly arrive at useful causal\nrelationships. One possible reason is that humans extrapolate from past\nexperience to new, unseen situations: that is, they encode beliefs over causal\ninvariances, allowing for sound generalization from the observations they\nobtain from directly acting in the world.\n  Here we outline a Bayesian model of causal induction where beliefs over\ncompeting causal hypotheses are modeled using probability trees. Based on this\nmodel, we illustrate why, in the general case, we need interventions plus\nconstraints on our causal hypotheses in order to extract causal information\nfrom our experience.\n", "versions": [{"version": "v1", "created": "Thu, 3 Nov 2011 01:32:44 GMT"}, {"version": "v2", "created": "Wed, 30 Nov 2011 01:12:16 GMT"}], "update_date": "2011-12-01", "authors_parsed": [["Ortega", "Pedro A.", ""]]}, {"id": "1111.1020", "submitter": "Nima Noorshams", "authors": "Nima Noorshams and Martin J. Wainwright", "title": "Stochastic Belief Propagation: A Low-Complexity Alternative to the\n  Sum-Product Algorithm", "comments": "Portions of the results were initially reported at the Allerton\n  Conference on Communications, Control, and Computing (September 2011). The\n  work was also submitted to IEEE Transaction on Information Theory in November\n  2011", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IT math.IT stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The sum-product or belief propagation (BP) algorithm is a widely-used\nmessage-passing algorithm for computing marginal distributions in graphical\nmodels with discrete variables. At the core of the BP message updates, when\napplied to a graphical model with pairwise interactions, lies a matrix-vector\nproduct with complexity that is quadratic in the state dimension $d$, and\nrequires transmission of a $(d-1)$-dimensional vector of real numbers\n(messages) to its neighbors. Since various applications involve very large\nstate dimensions, such computation and communication complexities can be\nprohibitively complex. In this paper, we propose a low-complexity variant of\nBP, referred to as stochastic belief propagation (SBP). As suggested by the\nname, it is an adaptively randomized version of the BP message updates in which\neach node passes randomly chosen information to each of its neighbors. The SBP\nmessage updates reduce the computational complexity (per iteration) from\nquadratic to linear in $d$, without assuming any particular structure of the\npotentials, and also reduce the communication complexity significantly,\nrequiring only $\\log{d}$ bits transmission per edge. Moreover, we establish a\nnumber of theoretical guarantees for the performance of SBP, showing that it\nconverges almost surely to the BP fixed point for any tree-structured graph,\nand for graphs with cycles satisfying a contractivity condition. In addition,\nfor these graphical models, we provide non-asymptotic upper bounds on the\nconvergence rate, showing that the $\\ell_{\\infty}$ norm of the error vector\ndecays no slower than $O(1/\\sqrt{t})$ with the number of iterations $t$ on\ntrees and the mean square error decays as $O(1/t)$ for general graphs. These\nanalysis show that SBP can provably yield reductions in computational and\ncommunication complexities for various classes of graphical models.\n", "versions": [{"version": "v1", "created": "Fri, 4 Nov 2011 00:28:59 GMT"}, {"version": "v2", "created": "Fri, 25 May 2012 01:31:14 GMT"}], "update_date": "2012-05-28", "authors_parsed": [["Noorshams", "Nima", ""], ["Wainwright", "Martin J.", ""]]}, {"id": "1111.1037", "submitter": "Haizhang Zhang", "authors": "Haizhang Zhang, Jun Zhang", "title": "Vector-valued Reproducing Kernel Banach Spaces with Applications to\n  Multi-task Learning", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.FA stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Motivated by multi-task machine learning with Banach spaces, we propose the\nnotion of vector-valued reproducing kernel Banach spaces (RKBS). Basic\nproperties of the spaces and the associated reproducing kernels are\ninvestigated. We also present feature map constructions and several concrete\nexamples of vector-valued RKBS. The theory is then applied to multi-task\nmachine learning. Especially, the representer theorem and characterization\nequations for the minimizer of regularized learning schemes in vector-valued\nRKBS are established.\n", "versions": [{"version": "v1", "created": "Fri, 4 Nov 2011 04:04:08 GMT"}, {"version": "v2", "created": "Fri, 17 Feb 2012 08:51:38 GMT"}], "update_date": "2012-02-20", "authors_parsed": [["Zhang", "Haizhang", ""], ["Zhang", "Jun", ""]]}, {"id": "1111.1687", "submitter": "Noah Simon", "authors": "Noah Simon and Rob Tibshirani", "title": "Discriminant Analysis with Adaptively Pooled Covariance", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML stat.CO stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Linear and Quadratic Discriminant analysis (LDA/QDA) are common tools for\nclassification problems. For these methods we assume observations are normally\ndistributed within group. We estimate a mean and covariance matrix for each\ngroup and classify using Bayes theorem. With LDA, we estimate a single, pooled\ncovariance matrix, while for QDA we estimate a separate covariance matrix for\neach group. Rarely do we believe in a homogeneous covariance structure between\ngroups, but often there is insufficient data to separately estimate covariance\nmatrices. We propose L1- PDA, a regularized model which adaptively pools\nelements of the precision matrices. Adaptively pooling these matrices decreases\nthe variance of our estimates (as in LDA), without overly biasing them. In this\npaper, we propose and discuss this method, give an efficient algorithm to fit\nit for moderate sized problems, and show its efficacy on real and simulated\ndatasets.\n", "versions": [{"version": "v1", "created": "Mon, 7 Nov 2011 19:30:13 GMT"}, {"version": "v2", "created": "Tue, 6 Dec 2011 23:45:34 GMT"}], "update_date": "2011-12-08", "authors_parsed": [["Simon", "Noah", ""], ["Tibshirani", "Rob", ""]]}, {"id": "1111.1784", "submitter": "Ravi Ganti", "authors": "Ravi Ganti and Alexander Gray", "title": "UPAL: Unbiased Pool Based Active Learning", "comments": "20 pages, 4 figures, 2 tables, a few minor typos were corrected", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.AI cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper we address the problem of pool based active learning, and\nprovide an algorithm, called UPAL, that works by minimizing the unbiased\nestimator of the risk of a hypothesis in a given hypothesis space. For the\nspace of linear classifiers and the squared loss we show that UPAL is\nequivalent to an exponentially weighted average forecaster. Exploiting some\nrecent results regarding the spectra of random matrices allows us to establish\nconsistency of UPAL when the true hypothesis is a linear hypothesis. Empirical\ncomparison with an active learner implementation in Vowpal Wabbit, and a\npreviously proposed pool based active learner implementation show good\nempirical performance and better scalability.\n", "versions": [{"version": "v1", "created": "Tue, 8 Nov 2011 02:41:48 GMT"}, {"version": "v2", "created": "Sun, 13 Nov 2011 17:28:34 GMT"}], "update_date": "2011-11-15", "authors_parsed": [["Ganti", "Ravi", ""], ["Gray", "Alexander", ""]]}, {"id": "1111.1788", "submitter": "Gonzalo Mateos", "authors": "Gonzalo Mateos and Georgios B. Giannakis", "title": "Robust PCA as Bilinear Decomposition with Outlier-Sparsity\n  Regularization", "comments": "30 pages, submitted to IEEE Transactions on Signal Processing", "journal-ref": null, "doi": "10.1109/TSP.2012.2204986", "report-no": null, "categories": "stat.ML cs.IT math.IT", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Principal component analysis (PCA) is widely used for dimensionality\nreduction, with well-documented merits in various applications involving\nhigh-dimensional data, including computer vision, preference measurement, and\nbioinformatics. In this context, the fresh look advocated here permeates\nbenefits from variable selection and compressive sampling, to robustify PCA\nagainst outliers. A least-trimmed squares estimator of a low-rank bilinear\nfactor analysis model is shown closely related to that obtained from an\n$\\ell_0$-(pseudo)norm-regularized criterion encouraging sparsity in a matrix\nexplicitly modeling the outliers. This connection suggests robust PCA schemes\nbased on convex relaxation, which lead naturally to a family of robust\nestimators encompassing Huber's optimal M-class as a special case. Outliers are\nidentified by tuning a regularization parameter, which amounts to controlling\nsparsity of the outlier matrix along the whole robustification path of (group)\nleast-absolute shrinkage and selection operator (Lasso) solutions. Beyond its\nneat ties to robust statistics, the developed outlier-aware PCA framework is\nversatile to accommodate novel and scalable algorithms to: i) track the\nlow-rank signal subspace robustly, as new data are acquired in real time; and\nii) determine principal components robustly in (possibly) infinite-dimensional\nfeature spaces. Synthetic and real data tests corroborate the effectiveness of\nthe proposed robust PCA schemes, when used to identify aberrant responses in\npersonality assessment surveys, as well as unveil communities in social\nnetworks, and intruders from video surveillance data.\n", "versions": [{"version": "v1", "created": "Tue, 8 Nov 2011 03:19:39 GMT"}], "update_date": "2013-10-01", "authors_parsed": [["Mateos", "Gonzalo", ""], ["Giannakis", "Georgios B.", ""]]}, {"id": "1111.1802", "submitter": "Tamara Broderick", "authors": "Tamara Broderick, Lester Mackey, John Paisley, Michael I. Jordan", "title": "Combinatorial clustering and the beta negative binomial process", "comments": "56 pages, 4 figures, 6 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We develop a Bayesian nonparametric approach to a general family of latent\nclass problems in which individuals can belong simultaneously to multiple\nclasses and where each class can be exhibited multiple times by an individual.\nWe introduce a combinatorial stochastic process known as the negative binomial\nprocess (NBP) as an infinite-dimensional prior appropriate for such problems.\nWe show that the NBP is conjugate to the beta process, and we characterize the\nposterior distribution under the beta-negative binomial process (BNBP) and\nhierarchical models based on the BNBP (the HBNBP). We study the asymptotic\nproperties of the BNBP and develop a three-parameter extension of the BNBP that\nexhibits power-law behavior. We derive MCMC algorithms for posterior inference\nunder the HBNBP, and we present experiments using these algorithms in the\ndomains of image segmentation, object recognition, and document analysis.\n", "versions": [{"version": "v1", "created": "Tue, 8 Nov 2011 04:46:01 GMT"}, {"version": "v2", "created": "Tue, 22 Nov 2011 21:49:49 GMT"}, {"version": "v3", "created": "Fri, 17 Feb 2012 00:17:58 GMT"}, {"version": "v4", "created": "Mon, 15 Oct 2012 18:27:13 GMT"}, {"version": "v5", "created": "Mon, 10 Jun 2013 13:40:24 GMT"}], "update_date": "2013-06-11", "authors_parsed": [["Broderick", "Tamara", ""], ["Mackey", "Lester", ""], ["Paisley", "John", ""], ["Jordan", "Michael I.", ""]]}, {"id": "1111.1830", "submitter": "Robert Hable", "authors": "Robert Hable and Andreas Christmann", "title": "Estimation of scale functions to model heteroscedasticity by support\n  vector machines", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A main goal of regression is to derive statistical conclusions on the\nconditional distribution of the output variable Y given the input values x. Two\nof the most important characteristics of a single distribution are location and\nscale. Support vector machines (SVMs) are well established to estimate location\nfunctions like the conditional median or the conditional mean. We investigate\nthe estimation of scale functions by SVMs when the conditional median is\nunknown, too. Estimation of scale functions is important e.g. to estimate the\nvolatility in finance. We consider the median absolute deviation (MAD) and the\ninterquantile range (IQR) as measures of scale. Our main result shows the\nconsistency of MAD-type SVMs.\n", "versions": [{"version": "v1", "created": "Tue, 8 Nov 2011 08:51:17 GMT"}], "update_date": "2011-11-09", "authors_parsed": [["Hable", "Robert", ""], ["Christmann", "Andreas", ""]]}, {"id": "1111.1876", "submitter": "Andreas Christmann", "authors": "Andreas Christmann, Matias Salibian-Barrera, and Stefan Van Aelst", "title": "On the stability of bootstrap estimators", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.ML stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  It is shown that bootstrap approximations of an estimator which is based on a\ncontinuous operator from the set of Borel probability measures defined on a\ncompact metric space into a complete separable metric space is stable in the\nsense of qualitative robustness. Support vector machines based on shifted loss\nfunctions are treated as special cases.\n", "versions": [{"version": "v1", "created": "Tue, 8 Nov 2011 11:45:08 GMT"}], "update_date": "2011-11-09", "authors_parsed": [["Christmann", "Andreas", ""], ["Salibian-Barrera", "Matias", ""], ["Van Aelst", "Stefan", ""]]}, {"id": "1111.1915", "submitter": "Nancy Heckman", "authors": "Nancy Heckman", "title": "The theory and application of penalized methods or Reproducing Kernel\n  Hilbert Spaces made easy", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The popular cubic smoothing spline estimate of a regression function arises\nas the minimizer of the penalized sum of squares $\\sum_j(Y_j - {\\mu}(t_j))^2 +\n{\\lambda}\\int_a^b [{\\mu}\"(t)]^2 dt$, where the data are $t_j,Y_j$, $j=1,...,\nn$. The minimization is taken over an infinite-dimensional function space, the\nspace of all functions with square integrable second derivatives. But the\ncalculations can be carried out in a finite-dimensional space. The reduction\nfrom minimizing over an infinite dimensional space to minimizing over a finite\ndimensional space occurs for more general objective functions: the data may be\nrelated to the function ${\\mu}$ in another way, the sum of squares may be\nreplaced by a more suitable expression, or the penalty, $\\int_a^b [{\\mu}\"(t)]^2\ndt$, might take a different form. This paper reviews the Reproducing Kernel\nHilbert Space structure that provides a finite-dimensional solution for a\ngeneral minimization problem. Particular attention is paid to penalties based\non linear differential operators. In this case, one can sometimes easily\ncalculate the minimizer explicitly, using Green's functions.\n", "versions": [{"version": "v1", "created": "Tue, 8 Nov 2011 14:21:17 GMT"}], "update_date": "2011-11-09", "authors_parsed": [["Heckman", "Nancy", ""]]}, {"id": "1111.2546", "submitter": "Anatoli Juditsky", "authors": "Anatoli Juditsky, Fatma K{\\i}l{\\i}n\\c{c} Karzan, Arkadi Nemirovski,\n  Boris Polyak", "title": "Accuracy guaranties for $\\ell_1$ recovery of block-sparse signals", "comments": "Published in at http://dx.doi.org/10.1214/12-AOS1057 the Annals of\n  Statistics (http://www.imstat.org/aos/) by the Institute of Mathematical\n  Statistics (http://www.imstat.org)", "journal-ref": "Annals of Statistics 2012, Vol. 40, No. 6, 3077-3107", "doi": "10.1214/12-AOS1057", "report-no": "IMS-AOS-AOS1057", "categories": "math.ST math.OC stat.ML stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We introduce a general framework to handle structured models (sparse and\nblock-sparse with possibly overlapping blocks). We discuss new methods for\ntheir recovery from incomplete observation, corrupted with deterministic and\nstochastic noise, using block-$\\ell_1$ regularization. While the current theory\nprovides promising bounds for the recovery errors under a number of different,\nyet mostly hard to verify conditions, our emphasis is on verifiable conditions\non the problem parameters (sensing matrix and the block structure) which\nguarantee accurate recovery. Verifiability of our conditions not only leads to\nefficiently computable bounds for the recovery error but also allows us to\noptimize these error bounds with respect to the method parameters, and\ntherefore construct estimators with improved statistical properties. To justify\nour approach, we also provide an oracle inequality, which links the properties\nof the proposed recovery algorithms and the best estimation performance.\nFurthermore, utilizing these verifiable conditions, we develop a\ncomputationally cheap alternative to block-$\\ell_1$ minimization, the\nnon-Euclidean Block Matching Pursuit algorithm. We close by presenting a\nnumerical study to investigate the effect of different block regularizations\nand demonstrate the performance of the proposed recoveries.\n", "versions": [{"version": "v1", "created": "Thu, 10 Nov 2011 18:16:50 GMT"}, {"version": "v2", "created": "Wed, 27 Feb 2013 12:30:22 GMT"}], "update_date": "2013-02-28", "authors_parsed": [["Juditsky", "Anatoli", ""], ["Karzan", "Fatma K\u0131l\u0131n\u00e7", ""], ["Nemirovski", "Arkadi", ""], ["Polyak", "Boris", ""]]}, {"id": "1111.2667", "submitter": "Benjamin Rolfs", "authors": "Benjamin T. Rolfs, Bala Rajaratnam", "title": "A note on the lack of symmetry in the graphical lasso", "comments": "9 pages, 2 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The graphical lasso (glasso) is a widely-used fast algorithm for estimating\nsparse inverse covariance matrices. The glasso solves an L1 penalized maximum\nlikelihood problem and is available as an R library on CRAN. The output from\nthe glasso, a regularized covariance matrix estimate a sparse inverse\ncovariance matrix estimate, not only identify a graphical model but can also\nserve as intermediate inputs into multivariate procedures such as PCA, LDA,\nMANOVA, and others. The glasso indeed produces a covariance matrix estimate\nwhich solves the L1 penalized optimization problem in a dual sense; however,\nthe method for producing the inverse covariance matrix estimator after this\noptimization is inexact and may produce asymmetric estimates. This problem is\nexacerbated when the amount of L1 regularization that is applied is small,\nwhich in turn is more likely to occur if the true underlying inverse covariance\nmatrix is not sparse. The lack of symmetry can potentially have consequences.\nFirst, it implies that the covariance and inverse covariance estimates are not\nnumerical inverses of one another, and second, asymmetry can possibly lead to\nnegative or complex eigenvalues,rendering many multivariate procedures which\nmay depend on the inverse covariance estimator unusable. We demonstrate this\nproblem, explain its causes, and propose possible remedies.\n", "versions": [{"version": "v1", "created": "Fri, 11 Nov 2011 05:51:44 GMT"}, {"version": "v2", "created": "Mon, 23 Jul 2012 22:58:52 GMT"}], "update_date": "2012-07-25", "authors_parsed": [["Rolfs", "Benjamin T.", ""], ["Rajaratnam", "Bala", ""]]}, {"id": "1111.3404", "submitter": "Daniel McDonald", "authors": "Daniel J. McDonald, Cosma Rohilla Shalizi, Mark Schervish", "title": "Estimated VC dimension for risk bounds", "comments": "11 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Vapnik-Chervonenkis (VC) dimension is a fundamental measure of the\ngeneralization capacity of learning algorithms. However, apart from a few\nspecial cases, it is hard or impossible to calculate analytically. Vapnik et\nal. [10] proposed a technique for estimating the VC dimension empirically.\nWhile their approach behaves well in simulations, it could not be used to bound\nthe generalization risk of classifiers, because there were no bounds for the\nestimation error of the VC dimension itself. We rectify this omission,\nproviding high probability concentration results for the proposed estimator and\nderiving corresponding generalization bounds.\n", "versions": [{"version": "v1", "created": "Tue, 15 Nov 2011 01:44:12 GMT"}], "update_date": "2011-11-16", "authors_parsed": [["McDonald", "Daniel J.", ""], ["Shalizi", "Cosma Rohilla", ""], ["Schervish", "Mark", ""]]}, {"id": "1111.3421", "submitter": "Nabin Malakar", "authors": "N. K. Malakar, K. H. Knuth, and D. J. Lary", "title": "Maximum Joint Entropy and Information-Based Collaboration of Automated\n  Learning Machines", "comments": "8 pages, 1 figure, to appear in the proceedings of MaxEnt 2011 held\n  in Waterloo, Canada", "journal-ref": null, "doi": "10.1063/1.3703640", "report-no": null, "categories": "stat.ML math.OC math.PR physics.comp-ph physics.data-an", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We are working to develop automated intelligent agents, which can act and\nreact as learning machines with minimal human intervention. To accomplish this,\nan intelligent agent is viewed as a question-asking machine, which is designed\nby coupling the processes of inference and inquiry to form a model-based\nlearning unit. In order to select maximally-informative queries, the\nintelligent agent needs to be able to compute the relevance of a question. This\nis accomplished by employing the inquiry calculus, which is dual to the\nprobability calculus, and extends information theory by explicitly requiring\ncontext. Here, we consider the interaction between two question-asking\nintelligent agents, and note that there is a potential information redundancy\nwith respect to the two questions that the agents may choose to pose. We show\nthat the information redundancy is minimized by maximizing the joint entropy of\nthe questions, which simultaneously maximizes the relevance of each question\nwhile minimizing the mutual information between them. Maximum joint entropy is\ntherefore an important principle of information-based collaboration, which\nenables intelligent agents to efficiently learn together.\n", "versions": [{"version": "v1", "created": "Tue, 15 Nov 2011 03:13:52 GMT"}], "update_date": "2015-06-03", "authors_parsed": [["Malakar", "N. K.", ""], ["Knuth", "K. H.", ""], ["Lary", "D. J.", ""]]}, {"id": "1111.3781", "submitter": "Taiji Suzuki", "authors": "Taiji Suzuki", "title": "Fast Learning Rate of Non-Sparse Multiple Kernel Learning and Optimal\n  Regularization Strategies", "comments": "45 pages, 5 figures. The short version is accepted by NIPS2011", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we give a new generalization error bound of Multiple Kernel\nLearning (MKL) for a general class of regularizations, and discuss what kind of\nregularization gives a favorable predictive accuracy. Our main target in this\npaper is dense type regularizations including \\ellp-MKL. According to the\nrecent numerical experiments, the sparse regularization does not necessarily\nshow a good performance compared with dense type regularizations. Motivated by\nthis fact, this paper gives a general theoretical tool to derive fast learning\nrates of MKL that is applicable to arbitrary mixed-norm-type regularizations in\na unifying manner. This enables us to compare the generalization performances\nof various types of regularizations. As a consequence, we observe that the\nhomogeneity of the complexities of candidate reproducing kernel Hilbert spaces\n(RKHSs) affects which regularization strategy (\\ell1 or dense) is preferred. In\nfact, in homogeneous complexity settings where the complexities of all RKHSs\nare evenly same, \\ell1-regularization is optimal among all isotropic norms. On\nthe other hand, in inhomogeneous complexity settings, dense type\nregularizations can show better learning rate than sparse \\ell1-regularization.\nWe also show that our learning rate achieves the minimax lower bound in\nhomogeneous complexity settings.\n", "versions": [{"version": "v1", "created": "Wed, 16 Nov 2011 12:35:01 GMT"}], "update_date": "2011-11-17", "authors_parsed": [["Suzuki", "Taiji", ""]]}, {"id": "1111.4118", "submitter": "Adam Charles", "authors": "Adam S. Charles, Pierre Garrigues, and Christopher J. Rozell", "title": "Analog Sparse Approximation with Applications to Compressed Sensing", "comments": "19 pages, 9 figures, submitted manuscript", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.OC stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recent research has shown that performance in signal processing tasks can\noften be significantly improved by using signal models based on sparse\nrepresentations, where a signal is approximated using a small number of\nelements from a fixed dictionary. Unfortunately, inference in this model\ninvolves solving non-smooth optimization problems that are computationally\nexpensive. While significant efforts have focused on developing digital\nalgorithms specifically for this problem, these algorithms are inappropriate\nfor many applications because of the time and power requirements necessary to\nsolve large optimization problems. Based on recent work in computational\nneuroscience, we explore the potential advantages of continuous time dynamical\nsystems for solving sparse approximation problems if they were implemented in\nanalog VLSI. Specifically, in the simulated task of recovering synthetic and\nMRI data acquired via compressive sensing techniques, we show that these\nsystems can potentially perform recovery at time scales of 10-20{\\mu}s,\nsupporting datarates of 50-100 kHz (orders of magnitude faster that digital\nalgorithms). Furthermore, we show analytically that a wide range of sparse\napproximation problems can be solved in the same basic architecture, including\napproximate $\\ell^p$ norms, modified $\\ell^1$ norms, re-weighted $\\ell^1$ and\n$\\ell^2$, the block $\\ell^1$ norm and classic Tikhonov regularization.\n", "versions": [{"version": "v1", "created": "Thu, 17 Nov 2011 14:58:18 GMT"}], "update_date": "2011-11-18", "authors_parsed": [["Charles", "Adam S.", ""], ["Garrigues", "Pierre", ""], ["Rozell", "Christopher J.", ""]]}, {"id": "1111.4226", "submitter": "Emily Fox", "authors": "Emily B. Fox, Erik B. Sudderth, Michael I. Jordan, and Alan S. Willsky", "title": "Joint Modeling of Multiple Related Time Series via the Beta Process", "comments": "33 pages, 8 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a Bayesian nonparametric approach to the problem of jointly\nmodeling multiple related time series. Our approach is based on the discovery\nof a set of latent, shared dynamical behaviors. Using a beta process prior, the\nsize of the set and the sharing pattern are both inferred from data. We develop\nefficient Markov chain Monte Carlo methods based on the Indian buffet process\nrepresentation of the predictive distribution of the beta process, without\nrelying on a truncated model. In particular, our approach uses the sum-product\nalgorithm to efficiently compute Metropolis-Hastings acceptance probabilities,\nand explores new dynamical behaviors via birth and death proposals. We examine\nthe benefits of our proposed feature-based model on several synthetic datasets,\nand also demonstrate promising results on unsupervised segmentation of visual\nmotion capture data.\n", "versions": [{"version": "v1", "created": "Thu, 17 Nov 2011 21:28:04 GMT"}], "update_date": "2011-11-21", "authors_parsed": [["Fox", "Emily B.", ""], ["Sudderth", "Erik B.", ""], ["Jordan", "Michael I.", ""], ["Willsky", "Alan S.", ""]]}, {"id": "1111.4259", "submitter": "Oriol Vinyals", "authors": "Oriol Vinyals and Daniel Povey", "title": "Krylov Subspace Descent for Deep Learning", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML math.OC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we propose a second order optimization method to learn models\nwhere both the dimensionality of the parameter space and the number of training\nsamples is high. In our method, we construct on each iteration a Krylov\nsubspace formed by the gradient and an approximation to the Hessian matrix, and\nthen use a subset of the training data samples to optimize over this subspace.\nAs with the Hessian Free (HF) method of [7], the Hessian matrix is never\nexplicitly constructed, and is computed using a subset of data. In practice, as\nin HF, we typically use a positive definite substitute for the Hessian matrix\nsuch as the Gauss-Newton matrix. We investigate the effectiveness of our\nproposed method on deep neural networks, and compare its performance to widely\nused methods such as stochastic gradient descent, conjugate gradient descent\nand L-BFGS, and also to HF. Our method leads to faster convergence than either\nL-BFGS or HF, and generally performs better than either of them in\ncross-validation accuracy. It is also simpler and more general than HF, as it\ndoes not require a positive semi-definite approximation of the Hessian matrix\nto work well nor the setting of a damping parameter. The chief drawback versus\nHF is the need for memory to store a basis for the Krylov subspace.\n", "versions": [{"version": "v1", "created": "Fri, 18 Nov 2011 02:15:32 GMT"}], "update_date": "2011-11-21", "authors_parsed": [["Vinyals", "Oriol", ""], ["Povey", "Daniel", ""]]}, {"id": "1111.4500", "submitter": "James P. Crutchfield", "authors": "Nicholas F. Travers and James P. Crutchfield", "title": "Equivalence of History and Generator Epsilon-Machines", "comments": "23 pages, 5 figures;\n  http://csc.ucdavis.edu/~cmg/compmech/pubs/hgem.htm; Expanded literature\n  review, additional examples and figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.PR cond-mat.stat-mech cs.IT math.IT nlin.CD stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Epsilon-machines are minimal, unifilar presentations of stationary stochastic\nprocesses. They were originally defined in the history machine sense, as hidden\nMarkov models whose states are the equivalence classes of infinite pasts with\nthe same probability distribution over futures. In analyzing synchronization,\nthough, an alternative generator definition was given: unifilar, edge-emitting\nhidden Markov models with probabilistically distinct states. The key difference\nis that history epsilon-machines are defined by a process, whereas generator\nepsilon-machines define a process. We show here that these two definitions are\nequivalent in the finite-state case.\n", "versions": [{"version": "v1", "created": "Fri, 18 Nov 2011 21:39:58 GMT"}, {"version": "v2", "created": "Fri, 14 Dec 2012 19:45:58 GMT"}], "update_date": "2015-03-19", "authors_parsed": [["Travers", "Nicholas F.", ""], ["Crutchfield", "James P.", ""]]}, {"id": "1111.4601", "submitter": "Daniel Kaslovsky", "authors": "Daniel N. Kaslovsky and Francois G. Meyer", "title": "Non-Asymptotic Analysis of Tangent Space Perturbation", "comments": "53 pages. Revised manuscript with new content addressing application\n  of results to real data sets", "journal-ref": null, "doi": null, "report-no": null, "categories": "physics.data-an cs.NA stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Constructing an efficient parameterization of a large, noisy data set of\npoints lying close to a smooth manifold in high dimension remains a fundamental\nproblem. One approach consists in recovering a local parameterization using the\nlocal tangent plane. Principal component analysis (PCA) is often the tool of\nchoice, as it returns an optimal basis in the case of noise-free samples from a\nlinear subspace. To process noisy data samples from a nonlinear manifold, PCA\nmust be applied locally, at a scale small enough such that the manifold is\napproximately linear, but at a scale large enough such that structure may be\ndiscerned from noise. Using eigenspace perturbation theory and non-asymptotic\nrandom matrix theory, we study the stability of the subspace estimated by PCA\nas a function of scale, and bound (with high probability) the angle it forms\nwith the true tangent space. By adaptively selecting the scale that minimizes\nthis bound, our analysis reveals an appropriate scale for local tangent plane\nrecovery. We also introduce a geometric uncertainty principle quantifying the\nlimits of noise-curvature perturbation for stable recovery. With the purpose of\nproviding perturbation bounds that can be used in practice, we propose plug-in\nestimates that make it possible to directly apply the theoretical results to\nreal data sets.\n", "versions": [{"version": "v1", "created": "Sun, 20 Nov 2011 02:14:52 GMT"}, {"version": "v2", "created": "Thu, 5 Jul 2012 03:14:28 GMT"}, {"version": "v3", "created": "Wed, 2 Jan 2013 23:57:36 GMT"}, {"version": "v4", "created": "Fri, 6 Dec 2013 02:17:44 GMT"}], "update_date": "2013-12-09", "authors_parsed": [["Kaslovsky", "Daniel N.", ""], ["Meyer", "Francois G.", ""]]}, {"id": "1111.5280", "submitter": "Silv\\`ere Bonnabel", "authors": "Silvere Bonnabel", "title": "Stochastic gradient descent on Riemannian manifolds", "comments": "A slightly shorter version has been published in IEEE Transactions\n  Automatic Control", "journal-ref": "IEEE Transactions on Automatic Control, Vol 58 (9), pages 2217 -\n  2229, Sept 2013", "doi": "10.1109/TAC.2013.2254619", "report-no": null, "categories": "math.OC cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Stochastic gradient descent is a simple approach to find the local minima of\na cost function whose evaluations are corrupted by noise. In this paper, we\ndevelop a procedure extending stochastic gradient descent algorithms to the\ncase where the function is defined on a Riemannian manifold. We prove that, as\nin the Euclidian case, the gradient descent algorithm converges to a critical\npoint of the cost function. The algorithm has numerous potential applications,\nand is illustrated here by four examples. In particular a novel gossip\nalgorithm on the set of covariance matrices is derived and tested numerically.\n", "versions": [{"version": "v1", "created": "Tue, 22 Nov 2011 18:41:12 GMT"}, {"version": "v2", "created": "Thu, 19 Jan 2012 16:08:29 GMT"}, {"version": "v3", "created": "Mon, 25 Feb 2013 10:41:46 GMT"}, {"version": "v4", "created": "Tue, 19 Nov 2013 11:56:10 GMT"}], "update_date": "2016-11-17", "authors_parsed": [["Bonnabel", "Silvere", ""]]}, {"id": "1111.5312", "submitter": "Ryan Rossi", "authors": "Ryan A. Rossi and Jennifer Neville", "title": "Representations and Ensemble Methods for Dynamic Relational\n  Classification", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.SI physics.soc-ph stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Temporal networks are ubiquitous and evolve over time by the addition,\ndeletion, and changing of links, nodes, and attributes. Although many\nrelational datasets contain temporal information, the majority of existing\ntechniques in relational learning focus on static snapshots and ignore the\ntemporal dynamics. We propose a framework for discovering temporal\nrepresentations of relational data to increase the accuracy of statistical\nrelational learning algorithms. The temporal relational representations serve\nas a basis for classification, ensembles, and pattern mining in evolving\ndomains. The framework includes (1) selecting the time-varying relational\ncomponents (links, attributes, nodes), (2) selecting the temporal granularity,\n(3) predicting the temporal influence of each time-varying relational\ncomponent, and (4) choosing the weighted relational classifier. Additionally,\nwe propose temporal ensemble methods that exploit the temporal-dimension of\nrelational data. These ensembles outperform traditional and more sophisticated\nrelational ensembles while avoiding the issue of learning the most optimal\nrepresentation. Finally, the space of temporal-relational models are evaluated\nusing a sample of classifiers. In all cases, the proposed temporal-relational\nclassifiers outperform competing models that ignore the temporal information.\nThe results demonstrate the capability and necessity of the temporal-relational\nrepresentations for classification, ensembles, and for mining temporal\ndatasets.\n", "versions": [{"version": "v1", "created": "Tue, 22 Nov 2011 20:21:19 GMT"}], "update_date": "2011-11-23", "authors_parsed": [["Rossi", "Ryan A.", ""], ["Neville", "Jennifer", ""]]}, {"id": "1111.5379", "submitter": "Ziyu Wang", "authors": "Firas Hamze, Ziyu Wang, Nando de Freitas", "title": "Self-Avoiding Random Dynamics on Integer Complex Systems", "comments": "22 pages. 9 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.CO cond-mat.dis-nn physics.comp-ph stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper introduces a new specialized algorithm for equilibrium Monte Carlo\nsampling of binary-valued systems, which allows for large moves in the state\nspace. This is achieved by constructing self-avoiding walks (SAWs) in the state\nspace. As a consequence, many bits are flipped in a single MCMC step. We name\nthe algorithm SARDONICS, an acronym for Self-Avoiding Random Dynamics on\nInteger Complex Systems. The algorithm has several free parameters, but we show\nthat Bayesian optimization can be used to automatically tune them. SARDONICS\nperforms remarkably well in a broad number of sampling tasks: toroidal\nferromagnetic and frustrated Ising models, 3D Ising models, restricted\nBoltzmann machines and chimera graphs arising in the design of quantum\ncomputers.\n", "versions": [{"version": "v1", "created": "Wed, 23 Nov 2011 00:50:15 GMT"}, {"version": "v2", "created": "Fri, 25 Nov 2011 23:20:54 GMT"}], "update_date": "2011-11-29", "authors_parsed": [["Hamze", "Firas", ""], ["Wang", "Ziyu", ""], ["de Freitas", "Nando", ""]]}, {"id": "1111.5479", "submitter": "Rahul Mazumder", "authors": "Rahul Mazumder, Trevor Hastie", "title": "The Graphical Lasso: New Insights and Alternatives", "comments": "This is a revised version of our previous manuscript with the same\n  name ArXiv id: http://arxiv.org/abs/1111.5479", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The graphical lasso \\citep{FHT2007a} is an algorithm for learning the\nstructure in an undirected Gaussian graphical model, using $\\ell_1$\nregularization to control the number of zeros in the precision matrix\n${\\B\\Theta}={\\B\\Sigma}^{-1}$ \\citep{BGA2008,yuan_lin_07}. The {\\texttt R}\npackage \\GL\\ \\citep{FHT2007a} is popular, fast, and allows one to efficiently\nbuild a path of models for different values of the tuning parameter.\nConvergence of \\GL\\ can be tricky; the converged precision matrix might not be\nthe inverse of the estimated covariance, and occasionally it fails to converge\nwith warm starts. In this paper we explain this behavior, and propose new\nalgorithms that appear to outperform \\GL.\n  By studying the \"normal equations\" we see that, \\GL\\ is solving the {\\em\ndual} of the graphical lasso penalized likelihood, by block coordinate ascent;\na result which can also be found in \\cite{BGA2008}.\n  In this dual, the target of estimation is $\\B\\Sigma$, the covariance matrix,\nrather than the precision matrix $\\B\\Theta$. We propose similar primal\nalgorithms \\PGL\\ and \\DPGL, that also operate by block-coordinate descent,\nwhere $\\B\\Theta$ is the optimization target. We study all of these algorithms,\nand in particular different approaches to solving their coordinate\nsub-problems. We conclude that \\DPGL\\ is superior from several points of view.\n", "versions": [{"version": "v1", "created": "Wed, 23 Nov 2011 12:47:50 GMT"}, {"version": "v2", "created": "Tue, 7 Aug 2012 23:11:40 GMT"}], "update_date": "2012-08-09", "authors_parsed": [["Mazumder", "Rahul", ""], ["Hastie", "Trevor", ""]]}, {"id": "1111.5648", "submitter": "David Balduzzi", "authors": "David Balduzzi", "title": "Falsification and future performance", "comments": "10 pages, 2 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.IT cs.LG math.IT", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We information-theoretically reformulate two measures of capacity from\nstatistical learning theory: empirical VC-entropy and empirical Rademacher\ncomplexity. We show these capacity measures count the number of hypotheses\nabout a dataset that a learning algorithm falsifies when it finds the\nclassifier in its repertoire minimizing empirical risk. It then follows from\nthat the future performance of predictors on unseen data is controlled in part\nby how many hypotheses the learner falsifies. As a corollary we show that\nempirical VC-entropy quantifies the message length of the true hypothesis in\nthe optimal code of a particular probability distribution, the so-called actual\nrepertoire.\n", "versions": [{"version": "v1", "created": "Wed, 23 Nov 2011 23:25:57 GMT"}], "update_date": "2011-11-28", "authors_parsed": [["Balduzzi", "David", ""]]}, {"id": "1111.5848", "submitter": "Carles Navarro Manch\\'on", "authors": "Carles Navarro Manch\\'on, Gunvor E. Kirkelund, Erwin Riegler, Lars P.\n  B. Christensen and Bernard H. Fleury", "title": "Receiver Architectures for MIMO-OFDM Based on a Combined VMP-SP\n  Algorithm", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.IT math.IT", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Iterative information processing, either based on heuristics or analytical\nframeworks, has been shown to be a very powerful tool for the design of\nefficient, yet feasible, wireless receiver architectures. Within this context,\nalgorithms performing message-passing on a probabilistic graph, such as the\nsum-product (SP) and variational message passing (VMP) algorithms, have become\nincreasingly popular.\n  In this contribution, we apply a combined VMP-SP message-passing technique to\nthe design of receivers for MIMO-ODFM systems. The message-passing equations of\nthe combined scheme can be obtained from the equations of the stationary points\nof a constrained region-based free energy approximation. When applied to a\nMIMO-OFDM probabilistic model, we obtain a generic receiver architecture\nperforming iterative channel weight and noise precision estimation,\nequalization and data decoding. We show that this generic scheme can be\nparticularized to a variety of different receiver structures, ranging from\nhigh-performance iterative structures to low complexity receivers. This allows\nfor a flexible design of the signal processing specially tailored for the\nrequirements of each specific application. The numerical assessment of our\nsolutions, based on Monte Carlo simulations, corroborates the high performance\nof the proposed algorithms and their superiority to heuristic approaches.\n", "versions": [{"version": "v1", "created": "Thu, 24 Nov 2011 20:57:18 GMT"}], "update_date": "2011-11-28", "authors_parsed": [["Manch\u00f3n", "Carles Navarro", ""], ["Kirkelund", "Gunvor E.", ""], ["Riegler", "Erwin", ""], ["Christensen", "Lars P. B.", ""], ["Fleury", "Bernard H.", ""]]}, {"id": "1111.5948", "submitter": "Bo Wahlberg", "authors": "Bo Wahlberg, Cristian R. Rojas and Mariette Annergren", "title": "On l_1 Mean and Variance Filtering", "comments": "The 45th Annual Asilomar Conference on Signals, Systems, and\n  Computers, November 6-9, 2011, Pacific Grove, California, USA", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper addresses the problem of segmenting a time-series with respect to\nchanges in the mean value or in the variance. The first case is when the time\ndata is modeled as a sequence of independent and normal distributed random\nvariables with unknown, possibly changing, mean value but fixed variance. The\nmain assumption is that the mean value is piecewise constant in time, and the\ntask is to estimate the change times and the mean values within the segments.\nThe second case is when the mean value is constant, but the variance can\nchange. The assumption is that the variance is piecewise constant in time, and\nwe want to estimate change times and the variance values within the segments.\nTo find solutions to these problems, we will study an l_1 regularized maximum\nlikelihood method, related to the fused lasso method and l_1 trend filtering,\nwhere the parameters to be estimated are free to vary at each sample. To\npenalize variations in the estimated parameters, the l_1-norm of the time\ndifference of the parameters is used as a regularization term. This idea is\nclosely related to total variation denoising. The main contribution is that a\nconvex formulation of this variance estimation problem, where the\nparametrization is based on the inverse of the variance, can be formulated as a\ncertain l_1 mean estimation problem. This implies that results and methods for\nmean estimation can be applied to the challenging problem of variance\nsegmentation/estimation.\n", "versions": [{"version": "v1", "created": "Fri, 25 Nov 2011 11:22:23 GMT"}], "update_date": "2011-11-28", "authors_parsed": [["Wahlberg", "Bo", ""], ["Rojas", "Cristian R.", ""], ["Annergren", "Mariette", ""]]}, {"id": "1111.6085", "submitter": "Vincent Tan", "authors": "Vincent Y. F. Tan and C\\'edric F\\'evotte", "title": "Automatic Relevance Determination in Nonnegative Matrix Factorization\n  with the \\beta-Divergence", "comments": "Accepted by the IEEE Transactions on Pattern Analysis and Machine\n  Intelligence", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper addresses the estimation of the latent dimensionality in\nnonnegative matrix factorization (NMF) with the \\beta-divergence. The\n\\beta-divergence is a family of cost functions that includes the squared\nEuclidean distance, Kullback-Leibler and Itakura-Saito divergences as special\ncases. Learning the model order is important as it is necessary to strike the\nright balance between data fidelity and overfitting. We propose a Bayesian\nmodel based on automatic relevance determination in which the columns of the\ndictionary matrix and the rows of the activation matrix are tied together\nthrough a common scale parameter in their prior. A family of\nmajorization-minimization algorithms is proposed for maximum a posteriori (MAP)\nestimation. A subset of scale parameters is driven to a small lower bound in\nthe course of inference, with the effect of pruning the corresponding spurious\ncomponents. We demonstrate the efficacy and robustness of our algorithms by\nperforming extensive experiments on synthetic data, the swimmer dataset, a\nmusic decomposition example and a stock price prediction task.\n", "versions": [{"version": "v1", "created": "Fri, 25 Nov 2011 19:03:21 GMT"}, {"version": "v2", "created": "Mon, 23 Apr 2012 09:21:21 GMT"}, {"version": "v3", "created": "Fri, 5 Oct 2012 11:39:35 GMT"}], "update_date": "2012-10-08", "authors_parsed": [["Tan", "Vincent Y. F.", ""], ["F\u00e9votte", "C\u00e9dric", ""]]}, {"id": "1111.6160", "submitter": "Vladimir Temlyakov", "authors": "N.I. Pentacaput", "title": "Optimal exponential bounds on the accuracy of classification", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider a standard binary classification problem. The performance of any\nbinary classifier based on the training data is characterized by the excess\nrisk. We study Bahadur's type exponential bounds on the minimax accuracy\nconfidence function based on the excess risk. We study how this quantity\ndepends on the complexity of the class of distributions characterized by\nexponents of entropies of the class of regression functions or of the class of\nBayes classifiers corresponding to the distributions from the class. We also\nstudy its dependence on margin parameters of the classification problem.\n", "versions": [{"version": "v1", "created": "Sat, 26 Nov 2011 13:43:40 GMT"}], "update_date": "2011-11-29", "authors_parsed": [["Pentacaput", "N. I.", ""]]}, {"id": "1111.6201", "submitter": "Yi-Hao Kao", "authors": "Yi-Hao Kao and Benjamin Van Roy", "title": "Learning a Factor Model via Regularized PCA", "comments": null, "journal-ref": "Machine Learning, Volume 91, Number 3, pp. 279-303 (2013)", "doi": "10.1007/s10994-013-5345-8", "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider the problem of learning a linear factor model. We propose a\nregularized form of principal component analysis (PCA) and demonstrate through\nexperiments with synthetic and real data the superiority of resulting estimates\nto those produced by pre-existing factor analysis approaches. We also establish\ntheoretical results that explain how our algorithm corrects the biases induced\nby conventional approaches. An important feature of our algorithm is that its\ncomputational requirements are similar to those of PCA, which enjoys wide use\nin large part due to its efficiency.\n", "versions": [{"version": "v1", "created": "Sat, 26 Nov 2011 23:36:40 GMT"}, {"version": "v2", "created": "Sat, 17 Mar 2012 21:50:49 GMT"}, {"version": "v3", "created": "Tue, 17 Jul 2012 20:52:14 GMT"}, {"version": "v4", "created": "Sun, 24 Feb 2013 05:01:59 GMT"}], "update_date": "2013-05-31", "authors_parsed": [["Kao", "Yi-Hao", ""], ["Van Roy", "Benjamin", ""]]}, {"id": "1111.6233", "submitter": "Nicolas Durrande", "authors": "Nicolas Durrande (CROCUS-ENSMSE), David Ginsbourger, Olivier Roustant\n  (CROCUS-ENSMSE, - M\\'ethodes d'Analyse Stochastique des Codes et Traitements\n  Num\\'eriques), Laurent Carraro (LAMUSE)", "title": "Additive Covariance Kernels for High-Dimensional Gaussian Process\n  Modeling", "comments": "arXiv admin note: substantial text overlap with arXiv:1103.4023", "journal-ref": "Annales de la Facult\\'e de Sciences de Toulouse Tome 21, num\\'ero\n  3 (2012) p. 481-499", "doi": null, "report-no": null, "categories": "stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Gaussian process models -also called Kriging models- are often used as\nmathematical approximations of expensive experiments. However, the number of\nobservation required for building an emulator becomes unrealistic when using\nclassical covariance kernels when the dimension of input increases. In oder to\nget round the curse of dimensionality, a popular approach is to consider\nsimplified models such as additive models. The ambition of the present work is\nto give an insight into covariance kernels that are well suited for building\nadditive Kriging models and to describe some properties of the resulting\nmodels.\n", "versions": [{"version": "v1", "created": "Sun, 27 Nov 2011 08:05:17 GMT"}], "update_date": "2012-12-17", "authors_parsed": [["Durrande", "Nicolas", "", "CROCUS-ENSMSE"], ["Ginsbourger", "David", "", "CROCUS-ENSMSE, - M\u00e9thodes d'Analyse Stochastique des Codes et Traitements\n  Num\u00e9riques"], ["Roustant", "Olivier", "", "CROCUS-ENSMSE, - M\u00e9thodes d'Analyse Stochastique des Codes et Traitements\n  Num\u00e9riques"], ["Carraro", "Laurent", "", "LAMUSE"]]}, {"id": "1111.6254", "submitter": "Fionn Murtagh", "authors": "Fionn Murtagh and Pedro Contreras", "title": "Fast, Linear Time, m-Adic Hierarchical Clustering for Search and\n  Retrieval using the Baire Metric, with linkages to Generalized Ultrametrics,\n  Hashing, Formal Concept Analysis, and Precision of Data Measurement", "comments": "17 pages, 45 citations, 2 figures", "journal-ref": "P-Adic Numbers, Ultrametric Analysis, and Applications, 4 (1),\n  45-56, 2012", "doi": "10.1134/S2070046612010062", "report-no": null, "categories": "stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We describe many vantage points on the Baire metric and its use in clustering\ndata, or its use in preprocessing and structuring data in order to support\nsearch and retrieval operations. In some cases, we proceed directly to clusters\nand do not directly determine the distances. We show how a hierarchical\nclustering can be read directly from one pass through the data. We offer\ninsights also on practical implications of precision of data measurement. As a\nmechanism for treating multidimensional data, including very high dimensional\ndata, we use random projections.\n", "versions": [{"version": "v1", "created": "Sun, 27 Nov 2011 12:59:32 GMT"}], "update_date": "2016-09-20", "authors_parsed": [["Murtagh", "Fionn", ""], ["Contreras", "Pedro", ""]]}, {"id": "1111.6285", "submitter": "Fionn Murtagh", "authors": "Fionn Murtagh and Pierre Legendre", "title": "Ward's Hierarchical Clustering Method: Clustering Criterion and\n  Agglomerative Algorithm", "comments": "20 pages, 21 citations, 4 figures", "journal-ref": "Journal of Classification, 31 (3), 274-295, 2014", "doi": "10.1007/s00357-014-9161-z", "report-no": null, "categories": "stat.ML cs.CV stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The Ward error sum of squares hierarchical clustering method has been very\nwidely used since its first description by Ward in a 1963 publication. It has\nalso been generalized in various ways. However there are different\ninterpretations in the literature and there are different implementations of\nthe Ward agglomerative algorithm in commonly used software systems, including\ndiffering expressions of the agglomerative criterion. Our survey work and case\nstudies will be useful for all those involved in developing software for data\nanalysis using Ward's hierarchical clustering method.\n", "versions": [{"version": "v1", "created": "Sun, 27 Nov 2011 18:39:14 GMT"}, {"version": "v2", "created": "Sun, 11 Dec 2011 23:58:45 GMT"}], "update_date": "2016-09-20", "authors_parsed": [["Murtagh", "Fionn", ""], ["Legendre", "Pierre", ""]]}, {"id": "1111.6410", "submitter": "Martin Azizyan", "authors": "Martin Azizyan, Aarti Singh, Larry Wasserman", "title": "Adaptive Semisupervised Inference", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.ML stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Semisupervised methods inevitably invoke some assumption that links the\nmarginal distribution of the features to the regression function of the label.\nMost commonly, the cluster or manifold assumptions are used which imply that\nthe regression function is smooth over high-density clusters or manifolds\nsupporting the data. A generalization of these assumptions is that the\nregression function is smooth with respect to some density sensitive distance.\nThis motivates the use of a density based metric for semisupervised learning.\nWe analyze this setting and make the following contributions - (a) we propose a\nsemi-supervised learner that uses a density-sensitive kernel and show that it\nprovides better performance than any supervised learner if the density support\nset has a small condition number and (b) we show that it is possible to adapt\nto the degree of semi-supervisedness using data-dependent choice of a parameter\nthat controls sensitivity of the distance metric to the density. This ensures\nthat the semisupervised learner never performs worse than a supervised learner\neven if the assumptions fail to hold.\n", "versions": [{"version": "v1", "created": "Mon, 28 Nov 2011 11:20:25 GMT"}, {"version": "v2", "created": "Thu, 1 Dec 2011 01:34:14 GMT"}], "update_date": "2011-12-02", "authors_parsed": [["Azizyan", "Martin", ""], ["Singh", "Aarti", ""], ["Wasserman", "Larry", ""]]}, {"id": "1111.6473", "submitter": "Antti Airola", "authors": "Willem Waegeman, Tapio Pahikkala, Antti Airola, Tapio Salakoski,\n  Michiel Stock, Bernard De Baets", "title": "A kernel-based framework for learning graded relations from data", "comments": "This work has been submitted to the IEEE for possible publication.\n  Copyright may be transferred without notice, after which this version may no\n  longer be accessible", "journal-ref": null, "doi": "10.1109/TFUZZ.2012.2194151", "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Driven by a large number of potential applications in areas like\nbioinformatics, information retrieval and social network analysis, the problem\nsetting of inferring relations between pairs of data objects has recently been\ninvestigated quite intensively in the machine learning community. To this end,\ncurrent approaches typically consider datasets containing crisp relations, so\nthat standard classification methods can be adopted. However, relations between\nobjects like similarities and preferences are often expressed in a graded\nmanner in real-world applications. A general kernel-based framework for\nlearning relations from data is introduced here. It extends existing approaches\nbecause both crisp and graded relations are considered, and it unifies existing\napproaches because different types of graded relations can be modeled,\nincluding symmetric and reciprocal relations. This framework establishes\nimportant links between recent developments in fuzzy set theory and machine\nlearning. Its usefulness is demonstrated through various experiments on\nsynthetic and real-world data.\n", "versions": [{"version": "v1", "created": "Mon, 28 Nov 2011 15:28:53 GMT"}], "update_date": "2013-05-01", "authors_parsed": [["Waegeman", "Willem", ""], ["Pahikkala", "Tapio", ""], ["Airola", "Antti", ""], ["Salakoski", "Tapio", ""], ["Stock", "Michiel", ""], ["De Baets", "Bernard", ""]]}, {"id": "1111.6832", "submitter": "John Cunningham", "authors": "John P. Cunningham and Philipp Hennig and Simon Lacoste-Julien", "title": "Gaussian Probabilities and Expectation Propagation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  While Gaussian probability densities are omnipresent in applied mathematics,\nGaussian cumulative probabilities are hard to calculate in any but the\nunivariate case. We study the utility of Expectation Propagation (EP) as an\napproximate integration method for this problem. For rectangular integration\nregions, the approximation is highly accurate. We also extend the derivations\nto the more general case of polyhedral integration regions. However, we find\nthat in this polyhedral case, EP's answer, though often accurate, can be almost\narbitrarily wrong. We consider these unexpected results empirically and\ntheoretically, both for the problem of Gaussian probabilities and for EP more\ngenerally. These results elucidate an interesting and non-obvious feature of EP\nnot yet studied in detail.\n", "versions": [{"version": "v1", "created": "Tue, 29 Nov 2011 14:59:12 GMT"}, {"version": "v2", "created": "Thu, 28 Nov 2013 18:58:52 GMT"}], "update_date": "2013-12-02", "authors_parsed": [["Cunningham", "John P.", ""], ["Hennig", "Philipp", ""], ["Lacoste-Julien", "Simon", ""]]}, {"id": "1111.6923", "submitter": "Akshay Soni", "authors": "Akshay Soni and Jarvis Haupt", "title": "Efficient Adaptive Compressive Sensing Using Sparse Hierarchical Learned\n  Dictionaries", "comments": "5 pages, 6 figures, conference", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.CV cs.IT math.IT math.PR stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recent breakthrough results in compressed sensing (CS) have established that\nmany high dimensional objects can be accurately recovered from a relatively\nsmall number of non- adaptive linear projection observations, provided that the\nobjects possess a sparse representation in some basis. Subsequent efforts have\nshown that the performance of CS can be improved by exploiting the structure in\nthe location of the non-zero signal coefficients (structured sparsity) or using\nsome form of online measurement focusing (adaptivity) in the sensing process.\nIn this paper we examine a powerful hybrid of these two techniques. First, we\ndescribe a simple adaptive sensing procedure and show that it is a provably\neffective method for acquiring sparse signals that exhibit structured sparsity\ncharacterized by tree-based coefficient dependencies. Next, employing\ntechniques from sparse hierarchical dictionary learning, we show that\nrepresentations exhibiting the appropriate form of structured sparsity can be\nlearned from collections of training data. The combination of these techniques\nresults in an effective and efficient adaptive compressive acquisition\nprocedure.\n", "versions": [{"version": "v1", "created": "Tue, 29 Nov 2011 18:31:54 GMT"}], "update_date": "2011-11-30", "authors_parsed": [["Soni", "Akshay", ""], ["Haupt", "Jarvis", ""]]}, {"id": "1111.6925", "submitter": "Yang Zhou", "authors": "Yang Zhou", "title": "Structure Learning of Probabilistic Graphical Models: A Comprehensive\n  Survey", "comments": "survey on structure learning", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Probabilistic graphical models combine the graph theory and probability\ntheory to give a multivariate statistical modeling. They provide a unified\ndescription of uncertainty using probability and complexity using the graphical\nmodel. Especially, graphical models provide the following several useful\nproperties:\n  - Graphical models provide a simple and intuitive interpretation of the\nstructures of probabilistic models. On the other hand, they can be used to\ndesign and motivate new models.\n  - Graphical models provide additional insights into the properties of the\nmodel, including the conditional independence properties.\n  - Complex computations which are required to perform inference and learning\nin sophisticated models can be expressed in terms of graphical manipulations,\nin which the underlying mathematical expressions are carried along implicitly.\n  The graphical models have been applied to a large number of fields, including\nbioinformatics, social science, control theory, image processing, marketing\nanalysis, among others. However, structure learning for graphical models\nremains an open challenge, since one must cope with a combinatorial search over\nthe space of all possible structures.\n  In this paper, we present a comprehensive survey of the existing structure\nlearning algorithms.\n", "versions": [{"version": "v1", "created": "Tue, 29 Nov 2011 18:33:01 GMT"}], "update_date": "2011-11-30", "authors_parsed": [["Zhou", "Yang", ""]]}]