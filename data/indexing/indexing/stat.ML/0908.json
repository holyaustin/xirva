[{"id": "0908.0050", "submitter": "Julien Mairal", "authors": "Julien Mairal (INRIA Rocquencourt), Francis Bach (INRIA Rocquencourt),\n  Jean Ponce (INRIA Rocquencourt, LIENS), Guillermo Sapiro", "title": "Online Learning for Matrix Factorization and Sparse Coding", "comments": "revised version", "journal-ref": "Journal of Machine Learning Research 11 (2010) 19--60", "doi": null, "report-no": null, "categories": "stat.ML cs.LG math.OC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Sparse coding--that is, modelling data vectors as sparse linear combinations\nof basis elements--is widely used in machine learning, neuroscience, signal\nprocessing, and statistics. This paper focuses on the large-scale matrix\nfactorization problem that consists of learning the basis set, adapting it to\nspecific data. Variations of this problem include dictionary learning in signal\nprocessing, non-negative matrix factorization and sparse principal component\nanalysis. In this paper, we propose to address these tasks with a new online\noptimization algorithm, based on stochastic approximations, which scales up\ngracefully to large datasets with millions of training samples, and extends\nnaturally to various matrix factorization formulations, making it suitable for\na wide range of learning problems. A proof of convergence is presented, along\nwith experiments with natural images and genomic data demonstrating that it\nleads to state-of-the-art performance in terms of speed and optimization for\nboth small and large datasets.\n", "versions": [{"version": "v1", "created": "Sat, 1 Aug 2009 06:09:18 GMT"}, {"version": "v2", "created": "Thu, 11 Feb 2010 07:33:02 GMT"}], "update_date": "2010-02-11", "authors_parsed": [["Mairal", "Julien", "", "INRIA Rocquencourt"], ["Bach", "Francis", "", "INRIA Rocquencourt"], ["Ponce", "Jean", "", "INRIA Rocquencourt, LIENS"], ["Sapiro", "Guillermo", ""]]}, {"id": "0908.0319", "submitter": "Sarah Filippi", "authors": "Sarah Filippi (LTCI), Olivier Capp\\'e (LTCI), Aur\\'elien Garivier\n  (LTCI)", "title": "Regret Bounds for Opportunistic Channel Access", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.AI cs.LG cs.NI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider the task of opportunistic channel access in a primary system\ncomposed of independent Gilbert-Elliot channels where the secondary (or\nopportunistic) user does not dispose of a priori information regarding the\nstatistical characteristics of the system. It is shown that this problem may be\ncast into the framework of model-based learning in a specific class of\nPartially Observed Markov Decision Processes (POMDPs) for which we introduce an\nalgorithm aimed at striking an optimal tradeoff between the exploration (or\nestimation) and exploitation requirements. We provide finite horizon regret\nbounds for this algorithm as well as a numerical evaluation of its performance\nin the single channel model as well as in the case of stochastically identical\nchannels.\n", "versions": [{"version": "v1", "created": "Mon, 3 Aug 2009 19:25:58 GMT"}], "update_date": "2009-08-04", "authors_parsed": [["Filippi", "Sarah", "", "LTCI"], ["Capp\u00e9", "Olivier", "", "LTCI"], ["Garivier", "Aur\u00e9lien", "", "LTCI"]]}, {"id": "0908.0570", "submitter": "Piyush Rai", "authors": "Piyush Rai and Hal Daum\\'e III", "title": "The Infinite Hierarchical Factor Regression Model", "comments": null, "journal-ref": "NIPS 2008", "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a nonparametric Bayesian factor regression model that accounts for\nuncertainty in the number of factors, and the relationship between factors. To\naccomplish this, we propose a sparse variant of the Indian Buffet Process and\ncouple this with a hierarchical model over factors, based on Kingman's\ncoalescent. We apply this model to two problems (factor analysis and factor\nregression) in gene-expression data analysis.\n", "versions": [{"version": "v1", "created": "Wed, 5 Aug 2009 01:10:09 GMT"}], "update_date": "2009-08-06", "authors_parsed": [["Rai", "Piyush", ""], ["Daum\u00e9", "Hal", "III"]]}, {"id": "0908.0572", "submitter": "Piyush Rai", "authors": "Piyush Rai, Hal Daum\\'e III, Suresh Venkatasubramanian", "title": "Streamed Learning: One-Pass SVMs", "comments": null, "journal-ref": "IJCAI 2009", "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a streaming model for large-scale classification (in the context\nof $\\ell_2$-SVM) by leveraging connections between learning and computational\ngeometry. The streaming model imposes the constraint that only a single pass\nover the data is allowed. The $\\ell_2$-SVM is known to have an equivalent\nformulation in terms of the minimum enclosing ball (MEB) problem, and an\nefficient algorithm based on the idea of \\emph{core sets} exists (Core Vector\nMachine, CVM). CVM learns a $(1+\\varepsilon)$-approximate MEB for a set of\npoints and yields an approximate solution to corresponding SVM instance.\nHowever CVM works in batch mode requiring multiple passes over the data. This\npaper presents a single-pass SVM which is based on the minimum enclosing ball\nof streaming data. We show that the MEB updates for the streaming case can be\neasily adapted to learn the SVM weight vector in a way similar to using online\nstochastic gradient updates. Our algorithm performs polylogarithmic computation\nat each example, and requires very small and constant storage. Experimental\nresults show that, even in such restrictive settings, we can learn efficiently\nin just one pass and get accuracies comparable to other state-of-the-art SVM\nsolvers (batch and online). We also give an analysis of the algorithm, and\ndiscuss some open issues and possible extensions.\n", "versions": [{"version": "v1", "created": "Wed, 5 Aug 2009 00:40:23 GMT"}], "update_date": "2009-08-06", "authors_parsed": [["Rai", "Piyush", ""], ["Daum\u00e9", "Hal", "III"], ["Venkatasubramanian", "Suresh", ""]]}, {"id": "0908.1258", "submitter": "Eric Xing", "authors": "Steve Hanneke, Wenjie Fu, and Eric Xing", "title": "Discrete Temporal Models of Social Networks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a family of statistical models for social network evolution over\ntime, which represents an extension of Exponential Random Graph Models (ERGMs).\nMany of the methods for ERGMs are readily adapted for these models, including\nmaximum likelihood estimation algorithms. We discuss models of this type and\ntheir properties, and give examples, as well as a demonstration of their use\nfor hypothesis testing and classification. We believe our temporal ERG models\nrepresent a useful new framework for modeling time-evolving social networks,\nand rewiring networks from other domains such as gene regulation circuitry, and\ncommunication networks.\n", "versions": [{"version": "v1", "created": "Sun, 9 Aug 2009 22:35:14 GMT"}], "update_date": "2009-08-11", "authors_parsed": [["Hanneke", "Steve", ""], ["Fu", "Wenjie", ""], ["Xing", "Eric", ""]]}, {"id": "0908.2284", "submitter": "Jacob Bien", "authors": "Jacob Bien, Robert Tibshirani", "title": "Classification by Set Cover: The Prototype Vector Machine", "comments": "24 pages, 11 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We introduce a new nearest-prototype classifier, the prototype vector machine\n(PVM). It arises from a combinatorial optimization problem which we cast as a\nvariant of the set cover problem. We propose two algorithms for approximating\nits solution. The PVM selects a relatively small number of representative\npoints which can then be used for classification. It contains 1-NN as a special\ncase. The method is compatible with any dissimilarity measure, making it\namenable to situations in which the data are not embedded in an underlying\nfeature space or in which using a non-Euclidean metric is desirable. Indeed, we\ndemonstrate on the much studied ZIP code data how the PVM can reap the benefits\nof a problem-specific metric. In this example, the PVM outperforms the highly\nsuccessful 1-NN with tangent distance, and does so retaining fewer than half of\nthe data points. This example highlights the strengths of the PVM in yielding a\nlow-error, highly interpretable model. Additionally, we apply the PVM to a\nprotein classification problem in which a kernel-based distance is used.\n", "versions": [{"version": "v1", "created": "Mon, 17 Aug 2009 05:10:22 GMT"}], "update_date": "2009-08-18", "authors_parsed": [["Bien", "Jacob", ""], ["Tibshirani", "Robert", ""]]}, {"id": "0908.2359", "submitter": "Olivier Cappe", "authors": "Olivier Capp\\'e (LTCI)", "title": "Online EM Algorithm for Hidden Markov Models", "comments": "Revised version, to appear in J. Comput. Graph. Statist", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.CO stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Online (also called \"recursive\" or \"adaptive\") estimation of fixed model\nparameters in hidden Markov models is a topic of much interest in times series\nmodelling. In this work, we propose an online parameter estimation algorithm\nthat combines two key ideas. The first one, which is deeply rooted in the\nExpectation-Maximization (EM) methodology consists in reparameterizing the\nproblem using complete-data sufficient statistics. The second ingredient\nconsists in exploiting a purely recursive form of smoothing in HMMs based on an\nauxiliary recursion. Although the proposed online EM algorithm resembles a\nclassical stochastic approximation (or Robbins-Monro) algorithm, it is\nsufficiently different to resist conventional analysis of convergence. We thus\nprovide limited results which identify the potential limiting points of the\nrecursion as well as the large-sample behavior of the quantities involved in\nthe algorithm. The performance of the proposed algorithm is numerically\nevaluated through simulations in the case of a noisily observed Markov chain.\nIn this case, the algorithm reaches estimation results that are comparable to\nthat of the maximum likelihood estimator for large sample sizes.\n", "versions": [{"version": "v1", "created": "Mon, 17 Aug 2009 14:24:32 GMT"}, {"version": "v2", "created": "Tue, 15 Feb 2011 05:49:04 GMT"}], "update_date": "2011-02-16", "authors_parsed": [["Capp\u00e9", "Olivier", "", "LTCI"]]}, {"id": "0908.2579", "submitter": "Tom Diethe", "authors": "Tom Diethe, John Shawe-Taylor", "title": "Convex Multiview Fisher Discriminant Analysis", "comments": "This paper has been withdrawn", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Section 1.3 was incorrect, and 2.1 will be removed from further submissions.\nA rewritten version will be posted in the future.\n", "versions": [{"version": "v1", "created": "Tue, 18 Aug 2009 14:35:03 GMT"}, {"version": "v2", "created": "Thu, 29 Oct 2009 19:40:49 GMT"}], "update_date": "2009-10-29", "authors_parsed": [["Diethe", "Tom", ""], ["Shawe-Taylor", "John", ""]]}, {"id": "0908.2644", "submitter": "Marvin Weinstein", "authors": "Marvin Weinstein, David Horn", "title": "Dynamic quantum clustering: a method for visual exploration of\n  structures in data", "comments": "15 pages, 9 figures", "journal-ref": "Phys.Rev.E80:066117,2009", "doi": "10.1103/PhysRevE.80.066117", "report-no": "SLAC-PUB-13759", "categories": "physics.data-an cs.DS hep-ex physics.comp-ph stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A given set of data-points in some feature space may be associated with a\nSchrodinger equation whose potential is determined by the data. This is known\nto lead to good clustering solutions. Here we extend this approach into a\nfull-fledged dynamical scheme using a time-dependent Schrodinger equation.\nMoreover, we approximate this Hamiltonian formalism by a truncated calculation\nwithin a set of Gaussian wave functions (coherent states) centered around the\noriginal points. This allows for analytic evaluation of the time evolution of\nall such states, opening up the possibility of exploration of relationships\namong data-points through observation of varying dynamical-distances among\npoints and convergence of points into clusters. This formalism may be further\nsupplemented by preprocessing, such as dimensional reduction through singular\nvalue decomposition or feature filtering.\n", "versions": [{"version": "v1", "created": "Tue, 18 Aug 2009 21:40:03 GMT"}], "update_date": "2010-02-16", "authors_parsed": [["Weinstein", "Marvin", ""], ["Horn", "David", ""]]}, {"id": "0908.2724", "submitter": "David Hardoon", "authors": "David R. Hardoon, John Shawe-Taylor", "title": "Sparse Canonical Correlation Analysis", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a novel method for solving Canonical Correlation Analysis (CCA) in\na sparse convex framework using a least squares approach. The presented method\nfocuses on the scenario when one is interested in (or limited to) a primal\nrepresentation for the first view while having a dual representation for the\nsecond view. Sparse CCA (SCCA) minimises the number of features used in both\nthe primal and dual projections while maximising the correlation between the\ntwo views. The method is demonstrated on two paired corpuses of English-French\nand English-Spanish for mate-retrieval. We are able to observe, in the\nmate-retreival, that when the number of the original features is large SCCA\noutperforms Kernel CCA (KCCA), learning the common semantic space from a sparse\nset of features.\n", "versions": [{"version": "v1", "created": "Wed, 19 Aug 2009 11:14:51 GMT"}], "update_date": "2009-08-20", "authors_parsed": [["Hardoon", "David R.", ""], ["Shawe-Taylor", "John", ""]]}, {"id": "0908.3321", "submitter": "{\\L}ukasz {\\L}aniewski-Wo{\\l}{\\l}k", "authors": "{\\L}ukasz {\\L}aniewski-Wo{\\l}{\\l}k", "title": "Relative Expected Improvement in Kriging Based Optimization", "comments": "EUROGEN2009 Kwakow conference", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose an extension of the concept of Expected Improvement criterion\ncommonly used in Kriging based optimization. We extend it for more complex\nKriging models, e.g. models using derivatives. The target field of application\nare CFD problems, where objective function are extremely expensive to evaluate,\nbut the theory can be also used in other fields.\n", "versions": [{"version": "v1", "created": "Sun, 23 Aug 2009 17:45:07 GMT"}], "update_date": "2009-08-25", "authors_parsed": [["\u0141aniewski-Wo\u0142\u0142k", "\u0141ukasz", ""]]}, {"id": "0908.3458", "submitter": "Steffen Gr\\\"unew\\\"alder", "authors": "Steffen Gr\\\"unew\\\"alder and Klaus Obermayer", "title": "The Optimal Unbiased Value Estimator and its Relation to LSTD, TD and MC", "comments": "Final version is under review. 38 pages, 8 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this analytical study we derive the optimal unbiased value estimator (MVU)\nand compare its statistical risk to three well known value estimators: Temporal\nDifference learning (TD), Monte Carlo estimation (MC) and Least-Squares\nTemporal Difference Learning (LSTD). We demonstrate that LSTD is equivalent to\nthe MVU if the Markov Reward Process (MRP) is acyclic and show that both differ\nfor most cyclic MRPs as LSTD is then typically biased. More generally, we show\nthat estimators that fulfill the Bellman equation can only be unbiased for\nspecial cyclic MRPs. The main reason being the probability measures with which\nthe expectations are taken. These measure vary from state to state and due to\nthe strong coupling by the Bellman equation it is typically not possible for a\nset of value estimators to be unbiased with respect to each of these measures.\nFurthermore, we derive relations of the MVU to MC and TD. The most important\none being the equivalence of MC to the MVU and to LSTD for undiscounted MRPs in\nwhich MC has the same amount of information. In the discounted case this\nequivalence does not hold anymore. For TD we show that it is essentially\nunbiased for acyclic MRPs and biased for cyclic MRPs. We also order estimators\naccording to their risk and present counter-examples to show that no general\nordering exists between the MVU and LSTD, between MC and LSTD and between TD\nand MC. Theoretical results are supported by examples and an empirical\nevaluation.\n", "versions": [{"version": "v1", "created": "Mon, 24 Aug 2009 16:02:38 GMT"}], "update_date": "2009-08-25", "authors_parsed": [["Gr\u00fcnew\u00e4lder", "Steffen", ""], ["Obermayer", "Klaus", ""]]}, {"id": "0908.3817", "submitter": "Marco Scutari", "authors": "Marco Scutari", "title": "Learning Bayesian Networks with the bnlearn R Package", "comments": "22 pages, 4 pictures", "journal-ref": "Journal of Statistical Software (2010), 35(3), 1-22", "doi": null, "report-no": null, "categories": "stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  bnlearn is an R package which includes several algorithms for learning the\nstructure of Bayesian networks with either discrete or continuous variables.\nBoth constraint-based and score-based algorithms are implemented, and can use\nthe functionality provided by the snow package to improve their performance via\nparallel computing. Several network scores and conditional independence\nalgorithms are available for both the learning algorithms and independent use.\nAdvanced plotting options are provided by the Rgraphviz package.\n", "versions": [{"version": "v1", "created": "Wed, 26 Aug 2009 13:36:18 GMT"}, {"version": "v2", "created": "Sat, 10 Jul 2010 22:11:40 GMT"}], "update_date": "2010-07-13", "authors_parsed": [["Scutari", "Marco", ""]]}, {"id": "0908.4425", "submitter": "Mar\\'ia Ang\\'elica Cueto", "authors": "Maria Angelica Cueto, Jason Morton and Bernd Sturmfels", "title": "Geometry of the restricted Boltzmann machine", "comments": "18 pages, 5 figures, 1 table", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML math.AG math.MG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The restricted Boltzmann machine is a graphical model for binary random\nvariables. Based on a complete bipartite graph separating hidden and observed\nvariables, it is the binary analog to the factor analysis model. We study this\ngraphical model from the perspectives of algebraic statistics and tropical\ngeometry, starting with the observation that its Zariski closure is a Hadamard\npower of the first secant variety of the Segre variety of projective lines. We\nderive a dimension formula for the tropicalized model, and we use it to show\nthat the restricted Boltzmann machine is identifiable in many cases. Our\nmethods include coding theory and geometry of linear threshold functions.\n", "versions": [{"version": "v1", "created": "Sun, 30 Aug 2009 21:14:42 GMT"}], "update_date": "2009-09-01", "authors_parsed": [["Cueto", "Maria Angelica", ""], ["Morton", "Jason", ""], ["Sturmfels", "Bernd", ""]]}, {"id": "0908.4489", "submitter": "Nicolas Dobigeon", "authors": "Nicolas Dobigeon and Jean-Yves Tourneret", "title": "Bayesian orthogonal component analysis for sparse representation", "comments": "Revised version. Accepted to IEEE Trans. Signal Processing", "journal-ref": "IEEE Trans. Signal Processing, vol. 58. no. 5, pp. 2675-2685, May\n  2010", "doi": "10.1109/TSP.2010.2041594", "report-no": null, "categories": "stat.ME stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper addresses the problem of identifying a lower dimensional space\nwhere observed data can be sparsely represented. This under-complete dictionary\nlearning task can be formulated as a blind separation problem of sparse sources\nlinearly mixed with an unknown orthogonal mixing matrix. This issue is\nformulated in a Bayesian framework. First, the unknown sparse sources are\nmodeled as Bernoulli-Gaussian processes. To promote sparsity, a weighted\nmixture of an atom at zero and a Gaussian distribution is proposed as prior\ndistribution for the unobserved sources. A non-informative prior distribution\ndefined on an appropriate Stiefel manifold is elected for the mixing matrix.\nThe Bayesian inference on the unknown parameters is conducted using a Markov\nchain Monte Carlo (MCMC) method. A partially collapsed Gibbs sampler is\ndesigned to generate samples asymptotically distributed according to the joint\nposterior distribution of the unknown model parameters and hyperparameters.\nThese samples are then used to approximate the joint maximum a posteriori\nestimator of the sources and mixing matrix. Simulations conducted on synthetic\ndata are reported to illustrate the performance of the method for recovering\nsparse representations. An application to sparse coding on under-complete\ndictionary is finally investigated.\n", "versions": [{"version": "v1", "created": "Mon, 31 Aug 2009 09:44:06 GMT"}, {"version": "v2", "created": "Tue, 17 Nov 2009 21:33:07 GMT"}, {"version": "v3", "created": "Mon, 4 Jan 2010 17:46:54 GMT"}], "update_date": "2010-08-30", "authors_parsed": [["Dobigeon", "Nicolas", ""], ["Tourneret", "Jean-Yves", ""]]}]