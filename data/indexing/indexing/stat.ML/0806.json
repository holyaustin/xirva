[{"id": "0806.0729", "submitter": "Robin  Girard", "authors": "Robin Girard", "title": "High dimensional gaussian classification", "comments": "62 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.ML stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  High dimensional data analysis is known to be as a challenging problem. In\nthis article, we give a theoretical analysis of high dimensional classification\nof Gaussian data which relies on a geometrical analysis of the error measure.\nIt links a problem of classification with a problem of nonparametric\nregression. We give an algorithm designed for high dimensional data which\nappears straightforward in the light of our theoretical work, together with the\nthresholding estimation theory. We finally attempt to give a general treatment\nof the problem that can be extended to frameworks other than gaussian.\n", "versions": [{"version": "v1", "created": "Wed, 4 Jun 2008 12:05:13 GMT"}, {"version": "v2", "created": "Wed, 2 Jul 2008 15:41:49 GMT"}, {"version": "v3", "created": "Thu, 10 Jul 2008 11:56:04 GMT"}], "update_date": "2008-07-10", "authors_parsed": [["Girard", "Robin", ""]]}, {"id": "0806.1652", "submitter": "Ulrike Schneider", "authors": "Benedikt M. P\\\"otscher, Ulrike Schneider", "title": "Confidence Sets Based on Penalized Maximum Likelihood Estimators in\n  Gaussian Regression", "comments": "second revision: new title, some comments added, proofs moved to\n  appendix", "journal-ref": "Electron. J. Statist. 4 (2010), 334-360", "doi": "10.1214/09-EJS523", "report-no": null, "categories": "math.ST stat.ME stat.ML stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Confidence intervals based on penalized maximum likelihood estimators such as\nthe LASSO, adaptive LASSO, and hard-thresholding are analyzed. In the\nknown-variance case, the finite-sample coverage properties of such intervals\nare determined and it is shown that symmetric intervals are the shortest. The\nlength of the shortest intervals based on the hard-thresholding estimator is\nlarger than the length of the shortest interval based on the adaptive LASSO,\nwhich is larger than the length of the shortest interval based on the LASSO,\nwhich in turn is larger than the standard interval based on the maximum\nlikelihood estimator. In the case where the penalized estimators are tuned to\npossess the `sparsity property', the intervals based on these estimators are\nlarger than the standard interval by an order of magnitude. Furthermore, a\nsimple asymptotic confidence interval construction in the `sparse' case, that\nalso applies to the smoothly clipped absolute deviation estimator, is\ndiscussed. The results for the known-variance case are shown to carry over to\nthe unknown-variance case in an appropriate asymptotic sense.\n", "versions": [{"version": "v1", "created": "Tue, 10 Jun 2008 13:31:03 GMT"}, {"version": "v2", "created": "Tue, 30 Jun 2009 13:47:06 GMT"}, {"version": "v3", "created": "Mon, 1 Feb 2010 17:33:42 GMT"}], "update_date": "2010-03-16", "authors_parsed": [["P\u00f6tscher", "Benedikt M.", ""], ["Schneider", "Ulrike", ""]]}, {"id": "0806.2646", "submitter": "Yair Goldberg", "authors": "Y. Goldberg, A. Zakai, D. Kushnir, Y. Ritov", "title": "Manifold Learning: The Price of Normalization", "comments": "Submitted to JMLR", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We analyze the performance of a class of manifold-learning algorithms that\nfind their output by minimizing a quadratic form under some normalization\nconstraints. This class consists of Locally Linear Embedding (LLE), Laplacian\nEigenmap, Local Tangent Space Alignment (LTSA), Hessian Eigenmaps (HLLE), and\nDiffusion maps. We present and prove conditions on the manifold that are\nnecessary for the success of the algorithms. Both the finite sample case and\nthe limit case are analyzed. We show that there are simple manifolds in which\nthe necessary conditions are violated, and hence the algorithms cannot recover\nthe underlying manifolds. Finally, we present numerical results that\ndemonstrate our claims.\n", "versions": [{"version": "v1", "created": "Mon, 16 Jun 2008 19:54:49 GMT"}], "update_date": "2008-06-17", "authors_parsed": [["Goldberg", "Y.", ""], ["Zakai", "A.", ""], ["Kushnir", "D.", ""], ["Ritov", "Y.", ""]]}, {"id": "0806.2669", "submitter": "Yair Goldberg", "authors": "Y. Goldberg, Y. Ritov", "title": "Local Procrustes for Manifold Embedding: A Measure of Embedding Quality\n  and Embedding Algorithms", "comments": "Submitted to Journal of Machine Learning", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present the Procrustes measure, a novel measure based on Procrustes\nrotation that enables quantitative comparison of the output of manifold-based\nembedding algorithms (such as LLE (Roweis and Saul, 2000) and Isomap (Tenenbaum\net al, 2000)). The measure also serves as a natural tool when choosing\ndimension-reduction parameters. We also present two novel dimension-reduction\ntechniques that attempt to minimize the suggested measure, and compare the\nresults of these techniques to the results of existing algorithms. Finally, we\nsuggest a simple iterative method that can be used to improve the output of\nexisting algorithms.\n", "versions": [{"version": "v1", "created": "Mon, 16 Jun 2008 20:41:57 GMT"}], "update_date": "2008-06-18", "authors_parsed": [["Goldberg", "Y.", ""], ["Ritov", "Y.", ""]]}, {"id": "0806.2831", "submitter": "Amparo Baillo", "authors": "Amparo Baillo and Antonio Cuevas", "title": "Supervised functional classification: A theoretical remark and some\n  comparisons", "comments": "18 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The problem of supervised classification (or discrimination) with functional\ndata is considered, with a special interest on the popular k-nearest neighbors\n(k-NN) classifier. First, relying on a recent result by Cerou and Guyader\n(2006), we prove the consistency of the k-NN classifier for functional data\nwhose distribution belongs to a broad family of Gaussian processes with\ntriangular covariance functions. Second, on a more practical side, we check the\nbehavior of the k-NN method when compared with a few other functional\nclassifiers. This is carried out through a small simulation study and the\nanalysis of several real functional data sets. While no global \"uniform\" winner\nemerges from such comparisons, the overall performance of the k-NN method,\ntogether with its sound intuitive motivation and relative simplicity, suggests\nthat it could represent a reasonable benchmark for the classification problem\nwith functional data.\n", "versions": [{"version": "v1", "created": "Tue, 17 Jun 2008 16:20:35 GMT"}], "update_date": "2008-06-18", "authors_parsed": [["Baillo", "Amparo", ""], ["Cuevas", "Antonio", ""]]}, {"id": "0806.2850", "submitter": "Nick Costiris J.", "authors": "N. J. Costiris, E. Mavrommatis, K. A. Gernoth, J. W. Clark", "title": "Decoding Beta-Decay Systematics: A Global Statistical Model for Beta^-\n  Halflives", "comments": "20 pages, 19 figures", "journal-ref": "Phys.Rev.C80:044332,2009", "doi": "10.1103/PhysRevC.80.044332", "report-no": null, "categories": "nucl-th astro-ph cond-mat.dis-nn cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Statistical modeling of nuclear data provides a novel approach to nuclear\nsystematics complementary to established theoretical and phenomenological\napproaches based on quantum theory. Continuing previous studies in which global\nstatistical modeling is pursued within the general framework of machine\nlearning theory, we implement advances in training algorithms designed to\nimproved generalization, in application to the problem of reproducing and\npredicting the halflives of nuclear ground states that decay 100% by the beta^-\nmode. More specifically, fully-connected, multilayer feedforward artificial\nneural network models are developed using the Levenberg-Marquardt optimization\nalgorithm together with Bayesian regularization and cross-validation. The\npredictive performance of models emerging from extensive computer experiments\nis compared with that of traditional microscopic and phenomenological models as\nwell as with the performance of other learning systems, including earlier\nneural network models as well as the support vector machines recently applied\nto the same problem. In discussing the results, emphasis is placed on\npredictions for nuclei that are far from the stability line, and especially\nthose involved in the r-process nucleosynthesis. It is found that the new\nstatistical models can match or even surpass the predictive performance of\nconventional models for beta-decay systematics and accordingly should provide a\nvaluable additional tool for exploring the expanding nuclear landscape.\n", "versions": [{"version": "v1", "created": "Tue, 17 Jun 2008 18:23:15 GMT"}], "update_date": "2009-11-06", "authors_parsed": [["Costiris", "N. J.", ""], ["Mavrommatis", "E.", ""], ["Gernoth", "K. A.", ""], ["Clark", "J. W.", ""]]}, {"id": "0806.3286", "submitter": "Hugh A. Chipman", "authors": "Hugh A. Chipman, Edward I. George, Robert E. McCulloch", "title": "BART: Bayesian additive regression trees", "comments": "Published in at http://dx.doi.org/10.1214/09-AOAS285 the Annals of\n  Applied Statistics (http://www.imstat.org/aoas/) by the Institute of\n  Mathematical Statistics (http://www.imstat.org)", "journal-ref": "Annals of Applied Statistics 2010, Vol. 4, No. 1, 266-298", "doi": "10.1214/09-AOAS285", "report-no": "IMS-AOAS-AOAS285", "categories": "stat.ME stat.AP stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We develop a Bayesian \"sum-of-trees\" model where each tree is constrained by\na regularization prior to be a weak learner, and fitting and inference are\naccomplished via an iterative Bayesian backfitting MCMC algorithm that\ngenerates samples from a posterior. Effectively, BART is a nonparametric\nBayesian regression approach which uses dimensionally adaptive random basis\nelements. Motivated by ensemble methods in general, and boosting algorithms in\nparticular, BART is defined by a statistical model: a prior and a likelihood.\nThis approach enables full posterior inference including point and interval\nestimates of the unknown regression function as well as the marginal effects of\npotential predictors. By keeping track of predictor inclusion frequencies, BART\ncan also be used for model-free variable selection. BART's many features are\nillustrated with a bake-off against competing methods on 42 different data\nsets, with a simulation experiment and on a drug discovery classification\nproblem.\n", "versions": [{"version": "v1", "created": "Thu, 19 Jun 2008 21:00:03 GMT"}, {"version": "v2", "created": "Thu, 7 Oct 2010 09:21:37 GMT"}], "update_date": "2010-10-08", "authors_parsed": [["Chipman", "Hugh A.", ""], ["George", "Edward I.", ""], ["McCulloch", "Robert E.", ""]]}, {"id": "0806.4115", "submitter": "Lukas Meier", "authors": "Lukas Meier, Sara van de Geer, Peter B\\\"uhlmann", "title": "High-dimensional additive modeling", "comments": "Published in at http://dx.doi.org/10.1214/09-AOS692 the Annals of\n  Statistics (http://www.imstat.org/aos/) by the Institute of Mathematical\n  Statistics (http://www.imstat.org)", "journal-ref": "Annals of Statistics 2009, Vol. 37, No. 6B, 3779-3821", "doi": "10.1214/09-AOS692", "report-no": "IMS-AOS-AOS692", "categories": "stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a new sparsity-smoothness penalty for high-dimensional generalized\nadditive models. The combination of sparsity and smoothness is crucial for\nmathematical theory as well as performance for finite-sample data. We present a\ncomputationally efficient algorithm, with provable numerical convergence\nproperties, for optimizing the penalized likelihood. Furthermore, we provide\noracle results which yield asymptotic optimality of our estimator for high\ndimensional but sparse additive models. Finally, an adaptive version of our\nsparsity-smoothness penalized approach yields large additional performance\ngains.\n", "versions": [{"version": "v1", "created": "Wed, 25 Jun 2008 14:54:21 GMT"}, {"version": "v2", "created": "Wed, 3 Dec 2008 16:29:43 GMT"}, {"version": "v3", "created": "Mon, 2 Feb 2009 13:42:29 GMT"}, {"version": "v4", "created": "Wed, 18 Nov 2009 09:27:00 GMT"}], "update_date": "2009-11-18", "authors_parsed": [["Meier", "Lukas", ""], ["van de Geer", "Sara", ""], ["B\u00fchlmann", "Peter", ""]]}, {"id": "0806.4642", "submitter": "Amy Gansell", "authors": "Amy Rebecca Gansell, Irene K.Tamaru, Aleks Jakulin, and Chris H.\n  Wiggins", "title": "Predicting Regional Classification of Levantine Ivory Sculptures: A\n  Machine Learning Approach", "comments": "presented at 34th Computer Applications & Quantitative Methods in\n  Archaeology Conference, Fargo, North Dakota, April 2006. appears in Digital\n  Discovery: Exploring New Frontiers in Human Heritage; CAA 2006. ISBN\n  978-963-8046-90-1. pp 483-492 (2007)", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Art historians and archaeologists have long grappled with the regional\nclassification of ancient Near Eastern ivory carvings. Based on the visual\nsimilarity of sculptures, individuals within these fields have proposed object\nassemblages linked to hypothesized regional production centers. Using\nquantitative rather than visual methods, we here approach this classification\ntask by exploiting computational methods from machine learning currently used\nwith success in a variety of statistical problems in science and engineering.\nWe first construct a prediction function using 66 categorical features as\ninputs and regional style as output. The model assigns regional style group\n(RSG), with 98 percent prediction accuracy. We then rank these features by\ntheir mutual information with RSG, quantifying single-feature predictive power.\nUsing the highest- ranking features in combination with nomographic\nvisualization, we have found previously unknown relationships that may aid in\nthe regional classification of these ivories and their interpretation in art\nhistorical context.\n", "versions": [{"version": "v1", "created": "Sat, 28 Jun 2008 01:59:38 GMT"}], "update_date": "2008-07-01", "authors_parsed": [["Gansell", "Amy Rebecca", ""], ["Tamaru", "Irene K.", ""], ["Jakulin", "Aleks", ""], ["Wiggins", "Chris H.", ""]]}]