[{"id": "1307.0032", "submitter": "Ioannis Mitliagkas", "authors": "Ioannis Mitliagkas, Constantine Caramanis, Prateek Jain", "title": "Memory Limited, Streaming PCA", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.IT cs.LG math.IT", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider streaming, one-pass principal component analysis (PCA), in the\nhigh-dimensional regime, with limited memory. Here, $p$-dimensional samples are\npresented sequentially, and the goal is to produce the $k$-dimensional subspace\nthat best approximates these points. Standard algorithms require $O(p^2)$\nmemory; meanwhile no algorithm can do better than $O(kp)$ memory, since this is\nwhat the output itself requires. Memory (or storage) complexity is most\nmeaningful when understood in the context of computational and sample\ncomplexity. Sample complexity for high-dimensional PCA is typically studied in\nthe setting of the {\\em spiked covariance model}, where $p$-dimensional points\nare generated from a population covariance equal to the identity (white noise)\nplus a low-dimensional perturbation (the spike) which is the signal to be\nrecovered. It is now well-understood that the spike can be recovered when the\nnumber of samples, $n$, scales proportionally with the dimension, $p$. Yet, all\nalgorithms that provably achieve this, have memory complexity $O(p^2)$.\nMeanwhile, algorithms with memory-complexity $O(kp)$ do not have provable\nbounds on sample complexity comparable to $p$. We present an algorithm that\nachieves both: it uses $O(kp)$ memory (meaning storage of any kind) and is able\nto compute the $k$-dimensional spike with $O(p \\log p)$ sample-complexity --\nthe first algorithm of its kind. While our theoretical analysis focuses on the\nspiked covariance model, our simulations show that our algorithm is successful\non much more general models for the data.\n", "versions": [{"version": "v1", "created": "Fri, 28 Jun 2013 21:38:17 GMT"}], "update_date": "2013-07-02", "authors_parsed": [["Mitliagkas", "Ioannis", ""], ["Caramanis", "Constantine", ""], ["Jain", "Prateek", ""]]}, {"id": "1307.0048", "submitter": "Kun  Yang", "authors": "Kun Yang", "title": "Simple one-pass algorithm for penalized linear regression with\n  cross-validation on MapReduce", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.DC cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we propose a one-pass algorithm on MapReduce for penalized\nlinear regression\n  \\[f_\\lambda(\\alpha, \\beta) = \\|Y - \\alpha\\mathbf{1} - X\\beta\\|_2^2 +\np_{\\lambda}(\\beta)\\] where $\\alpha$ is the intercept which can be omitted\ndepending on application; $\\beta$ is the coefficients and $p_{\\lambda}$ is the\npenalized function with penalizing parameter $\\lambda$. $f_\\lambda(\\alpha,\n\\beta)$ includes interesting classes such as Lasso, Ridge regression and\nElastic-net. Compared to latest iterative distributed algorithms requiring\nmultiple MapReduce jobs, our algorithm achieves huge performance improvement;\nmoreover, our algorithm is exact compared to the approximate algorithms such as\nparallel stochastic gradient decent. Moreover, what our algorithm distinguishes\nwith others is that it trains the model with cross validation to choose optimal\n$\\lambda$ instead of user specified one.\n  Key words: penalized linear regression, lasso, elastic-net, ridge, MapReduce\n", "versions": [{"version": "v1", "created": "Fri, 28 Jun 2013 23:32:11 GMT"}, {"version": "v2", "created": "Wed, 13 Apr 2016 05:52:11 GMT"}, {"version": "v3", "created": "Thu, 14 Apr 2016 01:55:55 GMT"}], "update_date": "2016-04-15", "authors_parsed": [["Yang", "Kun", ""]]}, {"id": "1307.0060", "submitter": "Tejas Kulkarni", "authors": "Vikash K. Mansinghka, Tejas D. Kulkarni, Yura N. Perov, Joshua B.\n  Tenenbaum", "title": "Approximate Bayesian Image Interpretation using Generative Probabilistic\n  Graphics Programs", "comments": "The first two authors contributed equally to this work", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.CV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The idea of computer vision as the Bayesian inverse problem to computer\ngraphics has a long history and an appealing elegance, but it has proved\ndifficult to directly implement. Instead, most vision tasks are approached via\ncomplex bottom-up processing pipelines. Here we show that it is possible to\nwrite short, simple probabilistic graphics programs that define flexible\ngenerative models and to automatically invert them to interpret real-world\nimages. Generative probabilistic graphics programs consist of a stochastic\nscene generator, a renderer based on graphics software, a stochastic likelihood\nmodel linking the renderer's output and the data, and latent variables that\nadjust the fidelity of the renderer and the tolerance of the likelihood model.\nRepresentations and algorithms from computer graphics, originally designed to\nproduce high-quality images, are instead used as the deterministic backbone for\nhighly approximate and stochastic generative models. This formulation combines\nprobabilistic programming, computer graphics, and approximate Bayesian\ncomputation, and depends only on general-purpose, automatic inference\ntechniques. We describe two applications: reading sequences of degraded and\nadversarially obscured alphanumeric characters, and inferring 3D road models\nfrom vehicle-mounted camera images. Each of the probabilistic graphics programs\nwe present relies on under 20 lines of probabilistic code, and supports\naccurate, approximately Bayesian inferences about ambiguous real-world images.\n", "versions": [{"version": "v1", "created": "Sat, 29 Jun 2013 02:36:45 GMT"}], "update_date": "2013-07-02", "authors_parsed": [["Mansinghka", "Vikash K.", ""], ["Kulkarni", "Tejas D.", ""], ["Perov", "Yura N.", ""], ["Tenenbaum", "Joshua B.", ""]]}, {"id": "1307.0127", "submitter": "Tor Lattimore", "authors": "Tor Lattimore and Marcus Hutter and Peter Sunehag", "title": "Concentration and Confidence for Discrete Bayesian Sequence Predictors", "comments": "17 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Bayesian sequence prediction is a simple technique for predicting future\nsymbols sampled from an unknown measure on infinite sequences over a countable\nalphabet. While strong bounds on the expected cumulative error are known, there\nare only limited results on the distribution of this error. We prove tight\nhigh-probability bounds on the cumulative error, which is measured in terms of\nthe Kullback-Leibler (KL) divergence. We also consider the problem of\nconstructing upper confidence bounds on the KL and Hellinger errors similar to\nthose constructed from Hoeffding-like bounds in the i.i.d. case. The new\nresults are applied to show that Bayesian sequence prediction can be used in\nthe Knows What It Knows (KWIK) framework with bounds that match the\nstate-of-the-art.\n", "versions": [{"version": "v1", "created": "Sat, 29 Jun 2013 16:36:30 GMT"}], "update_date": "2013-07-02", "authors_parsed": [["Lattimore", "Tor", ""], ["Hutter", "Marcus", ""], ["Sunehag", "Peter", ""]]}, {"id": "1307.0164", "submitter": "Han Liu", "authors": "Zhaoran Wang, Fang Han and Han Liu", "title": "Sparse Principal Component Analysis for High Dimensional Vector\n  Autoregressive Models", "comments": "28 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study sparse principal component analysis for high dimensional vector\nautoregressive time series under a doubly asymptotic framework, which allows\nthe dimension $d$ to scale with the series length $T$. We treat the transition\nmatrix of time series as a nuisance parameter and directly apply sparse\nprincipal component analysis on multivariate time series as if the data are\nindependent. We provide explicit non-asymptotic rates of convergence for\nleading eigenvector estimation and extend this result to principal subspace\nestimation. Our analysis illustrates that the spectral norm of the transition\nmatrix plays an essential role in determining the final rates. We also\ncharacterize sufficient conditions under which sparse principal component\nanalysis attains the optimal parametric rate. Our theoretical results are\nbacked up by thorough numerical studies.\n", "versions": [{"version": "v1", "created": "Sun, 30 Jun 2013 00:40:09 GMT"}], "update_date": "2013-07-02", "authors_parsed": [["Wang", "Zhaoran", ""], ["Han", "Fang", ""], ["Liu", "Han", ""]]}, {"id": "1307.0252", "submitter": "Eric Bair", "authors": "Eric Bair", "title": "Semi-supervised clustering methods", "comments": "28 pages, 5 figures", "journal-ref": "WIREs Comp Stat, 2013, 5(5): 349-361", "doi": "10.1002/wics.1270", "report-no": null, "categories": "stat.ME cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Cluster analysis methods seek to partition a data set into homogeneous\nsubgroups. It is useful in a wide variety of applications, including document\nprocessing and modern genetics. Conventional clustering methods are\nunsupervised, meaning that there is no outcome variable nor is anything known\nabout the relationship between the observations in the data set. In many\nsituations, however, information about the clusters is available in addition to\nthe values of the features. For example, the cluster labels of some\nobservations may be known, or certain observations may be known to belong to\nthe same cluster. In other cases, one may wish to identify clusters that are\nassociated with a particular outcome variable. This review describes several\nclustering algorithms (known as \"semi-supervised clustering\" methods) that can\nbe applied in these situations. The majority of these methods are modifications\nof the popular k-means clustering method, and several of them will be described\nin detail. A brief description of some other semi-supervised clustering\nalgorithms is also provided.\n", "versions": [{"version": "v1", "created": "Mon, 1 Jul 2013 00:51:07 GMT"}], "update_date": "2014-07-11", "authors_parsed": [["Bair", "Eric", ""]]}, {"id": "1307.0293", "submitter": "Fang Han", "authors": "Fang Han, Huanran Lu, and Han Liu", "title": "A Direct Estimation of High Dimensional Stationary Vector\n  Autoregressions", "comments": "36 pages, 3 figure", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The vector autoregressive (VAR) model is a powerful tool in modeling complex\ntime series and has been exploited in many fields. However, fitting high\ndimensional VAR model poses some unique challenges: On one hand, the\ndimensionality, caused by modeling a large number of time series and higher\norder autoregressive processes, is usually much higher than the time series\nlength; On the other hand, the temporal dependence structure in the VAR model\ngives rise to extra theoretical challenges. In high dimensions, one popular\napproach is to assume the transition matrix is sparse and fit the VAR model\nusing the \"least squares\" method with a lasso-type penalty. In this manuscript,\nwe propose an alternative way in estimating the VAR model. The main idea is,\nvia exploiting the temporal dependence structure, to formulate the estimating\nproblem into a linear program. There is instant advantage for the proposed\napproach over the lasso-type estimators: The estimation equation can be\ndecomposed into multiple sub-equations and accordingly can be efficiently\nsolved in a parallel fashion. In addition, our method brings new theoretical\ninsights into the VAR model analysis. So far the theoretical results developed\nin high dimensions (e.g., Song and Bickel (2011) and Kock and Callot (2012))\nmainly pose assumptions on the design matrix of the formulated regression\nproblems. Such conditions are indirect about the transition matrices and not\ntransparent. In contrast, our results show that the operator norm of the\ntransition matrices plays an important role in estimation accuracy. We provide\nexplicit rates of convergence for both estimation and prediction. In addition,\nwe provide thorough experiments on both synthetic and real-world equity data to\nshow that there are empirical advantages of our method over the lasso-type\nestimators in both parameter estimation and forecasting.\n", "versions": [{"version": "v1", "created": "Mon, 1 Jul 2013 07:53:20 GMT"}, {"version": "v2", "created": "Mon, 25 Nov 2013 22:49:24 GMT"}, {"version": "v3", "created": "Wed, 29 Oct 2014 02:28:49 GMT"}], "update_date": "2014-10-30", "authors_parsed": [["Han", "Fang", ""], ["Lu", "Huanran", ""], ["Liu", "Han", ""]]}, {"id": "1307.0317", "submitter": "Jaka \\v{S}peh", "authors": "Jaka \\v{S}peh, Andrej Muhi\\v{c}, Jan Rupnik", "title": "Algorithms of the LDA model [REPORT]", "comments": "5 pages, 4 figures, report", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.IR stat.ML", "license": "http://creativecommons.org/licenses/by/3.0/", "abstract": "  We review three algorithms for Latent Dirichlet Allocation (LDA). Two of them\nare variational inference algorithms: Variational Bayesian inference and Online\nVariational Bayesian inference and one is Markov Chain Monte Carlo (MCMC)\nalgorithm -- Collapsed Gibbs sampling. We compare their time complexity and\nperformance. We find that online variational Bayesian inference is the fastest\nalgorithm and still returns reasonably good results.\n", "versions": [{"version": "v1", "created": "Mon, 1 Jul 2013 10:03:58 GMT"}], "update_date": "2013-07-02", "authors_parsed": [["\u0160peh", "Jaka", ""], ["Muhi\u010d", "Andrej", ""], ["Rupnik", "Jan", ""]]}, {"id": "1307.0323", "submitter": "Anthony Coolen", "authors": "James Barrett and Anthony C. C. Coolen", "title": "Dimensionality Detection and Integration of Multiple Data Sources via\n  the GP-LVM", "comments": "15 pages, 3 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The Gaussian Process Latent Variable Model (GP-LVM) is a non-linear\nprobabilistic method of embedding a high dimensional dataset in terms low\ndimensional `latent' variables. In this paper we illustrate that maximum a\nposteriori (MAP) estimation of the latent variables and hyperparameters can be\nused for model selection and hence we can determine the optimal number or\nlatent variables and the most appropriate model. This is an alternative to the\nvariational approaches developed recently and may be useful when we want to use\na non-Gaussian prior or kernel functions that don't have automatic relevance\ndetermination (ARD) parameters. Using a second order expansion of the latent\nvariable posterior we can marginalise the latent variables and obtain an\nestimate for the hyperparameter posterior. Secondly, we use the GP-LVM to\nintegrate multiple data sources by simultaneously embedding them in terms of\ncommon latent variables. We present results from synthetic data to illustrate\nthe successful detection and retrieval of low dimensional structure from high\ndimensional data. We demonstrate that the integration of multiple data sources\nleads to more robust performance. Finally, we show that when the data are used\nfor binary classification tasks we can attain a significant gain in prediction\naccuracy when the low dimensional representation is used.\n", "versions": [{"version": "v1", "created": "Mon, 1 Jul 2013 10:44:13 GMT"}], "update_date": "2013-07-02", "authors_parsed": [["Barrett", "James", ""], ["Coolen", "Anthony C. C.", ""]]}, {"id": "1307.0373", "submitter": "Jos\\'e Miguel Hern\\'andez-Lobato", "authors": "Jos\\'e Miguel Hern\\'andez-Lobato, James Robert Lloyd and Daniel\n  Hern\\'andez-Lobato", "title": "Gaussian Process Conditional Copulas with Applications to Financial Time\n  Series", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The estimation of dependencies between multiple variables is a central\nproblem in the analysis of financial time series. A common approach is to\nexpress these dependencies in terms of a copula function. Typically the copula\nfunction is assumed to be constant but this may be inaccurate when there are\ncovariates that could have a large influence on the dependence structure of the\ndata. To account for this, a Bayesian framework for the estimation of\nconditional copulas is proposed. In this framework the parameters of a copula\nare non-linearly related to some arbitrary conditioning variables. We evaluate\nthe ability of our method to predict time-varying dependencies on several\nequities and currencies and observe consistent performance gains compared to\nstatic copula models and other time-varying copula methods.\n", "versions": [{"version": "v1", "created": "Mon, 1 Jul 2013 14:14:55 GMT"}], "update_date": "2013-07-02", "authors_parsed": [["Hern\u00e1ndez-Lobato", "Jos\u00e9 Miguel", ""], ["Lloyd", "James Robert", ""], ["Hern\u00e1ndez-Lobato", "Daniel", ""]]}, {"id": "1307.0414", "submitter": "Ian Goodfellow", "authors": "Ian J. Goodfellow, Dumitru Erhan, Pierre Luc Carrier, Aaron Courville,\n  Mehdi Mirza, Ben Hamner, Will Cukierski, Yichuan Tang, David Thaler,\n  Dong-Hyun Lee, Yingbo Zhou, Chetan Ramaiah, Fangxiang Feng, Ruifan Li,\n  Xiaojie Wang, Dimitris Athanasakis, John Shawe-Taylor, Maxim Milakov, John\n  Park, Radu Ionescu, Marius Popescu, Cristian Grozea, James Bergstra, Jingjing\n  Xie, Lukasz Romaszko, Bing Xu, Zhang Chuang, and Yoshua Bengio", "title": "Challenges in Representation Learning: A report on three machine\n  learning contests", "comments": "8 pages, 2 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The ICML 2013 Workshop on Challenges in Representation Learning focused on\nthree challenges: the black box learning challenge, the facial expression\nrecognition challenge, and the multimodal learning challenge. We describe the\ndatasets created for these challenges and summarize the results of the\ncompetitions. We provide suggestions for organizers of future challenges and\nsome comments on what kind of knowledge can be gained from machine learning\ncompetitions.\n", "versions": [{"version": "v1", "created": "Mon, 1 Jul 2013 15:53:22 GMT"}], "update_date": "2013-07-02", "authors_parsed": [["Goodfellow", "Ian J.", ""], ["Erhan", "Dumitru", ""], ["Carrier", "Pierre Luc", ""], ["Courville", "Aaron", ""], ["Mirza", "Mehdi", ""], ["Hamner", "Ben", ""], ["Cukierski", "Will", ""], ["Tang", "Yichuan", ""], ["Thaler", "David", ""], ["Lee", "Dong-Hyun", ""], ["Zhou", "Yingbo", ""], ["Ramaiah", "Chetan", ""], ["Feng", "Fangxiang", ""], ["Li", "Ruifan", ""], ["Wang", "Xiaojie", ""], ["Athanasakis", "Dimitris", ""], ["Shawe-Taylor", "John", ""], ["Milakov", "Maxim", ""], ["Park", "John", ""], ["Ionescu", "Radu", ""], ["Popescu", "Marius", ""], ["Grozea", "Cristian", ""], ["Bergstra", "James", ""], ["Xie", "Jingjing", ""], ["Romaszko", "Lukasz", ""], ["Xu", "Bing", ""], ["Chuang", "Zhang", ""], ["Bengio", "Yoshua", ""]]}, {"id": "1307.0578", "submitter": "Ava Bargi", "authors": "Ava Bargi, Richard Yi Da Xu, Massimo Piccardi", "title": "A non-parametric conditional factor regression model for\n  high-dimensional input and response", "comments": "9 pages, 3 figures, NIPS submission", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we propose a non-parametric conditional factor regression\n(NCFR)model for domains with high-dimensional input and response. NCFR enhances\nlinear regression in two ways: a) introducing low-dimensional latent factors\nleading to dimensionality reduction and b) integrating an Indian Buffet Process\nas a prior for the latent factors to derive unlimited sparse dimensions.\nExperimental results comparing NCRF to several alternatives give evidence to\nremarkable prediction performance.\n", "versions": [{"version": "v1", "created": "Tue, 2 Jul 2013 02:54:09 GMT"}], "update_date": "2013-07-03", "authors_parsed": [["Bargi", "Ava", ""], ["Da Xu", "Richard Yi", ""], ["Piccardi", "Massimo", ""]]}, {"id": "1307.0781", "submitter": "Cem Tekin", "authors": "Cem Tekin, Mihaela van der Schaar", "title": "Distributed Online Big Data Classification Using Context Information", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Distributed, online data mining systems have emerged as a result of\napplications requiring analysis of large amounts of correlated and\nhigh-dimensional data produced by multiple distributed data sources. We propose\na distributed online data classification framework where data is gathered by\ndistributed data sources and processed by a heterogeneous set of distributed\nlearners which learn online, at run-time, how to classify the different data\nstreams either by using their locally available classification functions or by\nhelping each other by classifying each other's data. Importantly, since the\ndata is gathered at different locations, sending the data to another learner to\nprocess incurs additional costs such as delays, and hence this will be only\nbeneficial if the benefits obtained from a better classification will exceed\nthe costs. We model the problem of joint classification by the distributed and\nheterogeneous learners from multiple data sources as a distributed contextual\nbandit problem where each data is characterized by a specific context. We\ndevelop a distributed online learning algorithm for which we can prove\nsublinear regret. Compared to prior work in distributed online data mining, our\nwork is the first to provide analytic regret results characterizing the\nperformance of the proposed algorithm.\n", "versions": [{"version": "v1", "created": "Tue, 2 Jul 2013 18:09:59 GMT"}], "update_date": "2013-07-03", "authors_parsed": [["Tekin", "Cem", ""], ["van der Schaar", "Mihaela", ""]]}, {"id": "1307.0802", "submitter": "Jonathan Huggins", "authors": "Jonathan H. Huggins and Cynthia Rudin", "title": "A Statistical Learning Theory Framework for Supervised Pattern Discovery", "comments": "12 pages, 1 figure. Title change. Full version. Appearing at the SIAM\n  International Conference on Data Mining (SDM) 2014", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper formalizes a latent variable inference problem we call {\\em\nsupervised pattern discovery}, the goal of which is to find sets of\nobservations that belong to a single ``pattern.'' We discuss two versions of\nthe problem and prove uniform risk bounds for both. In the first version,\ncollections of patterns can be generated in an arbitrary manner and the data\nconsist of multiple labeled collections. In the second version, the patterns\nare assumed to be generated independently by identically distributed processes.\nThese processes are allowed to take an arbitrary form, so observations within a\npattern are not in general independent of each other. The bounds for the second\nversion of the problem are stated in terms of a new complexity measure, the\nquasi-Rademacher complexity.\n", "versions": [{"version": "v1", "created": "Tue, 2 Jul 2013 19:32:17 GMT"}, {"version": "v2", "created": "Fri, 7 Feb 2014 15:41:23 GMT"}], "update_date": "2014-02-10", "authors_parsed": [["Huggins", "Jonathan H.", ""], ["Rudin", "Cynthia", ""]]}, {"id": "1307.0803", "submitter": "Marinka Zitnik", "authors": "Marinka \\v{Z}itnik and Bla\\v{z} Zupan", "title": "Data Fusion by Matrix Factorization", "comments": "Short preprint, 13 pages, 3 Figures, 3 Tables. Full paper in\n  10.1109/TPAMI.2014.2343973", "journal-ref": "Marinka Zitnik and Blaz Zupan. IEEE Transactions on Pattern\n  Analysis and Machine Intelligence, 37(1):41-53 (2015)", "doi": "10.1109/TPAMI.2014.2343973", "report-no": null, "categories": "cs.LG cs.AI cs.DB stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  For most problems in science and engineering we can obtain data sets that\ndescribe the observed system from various perspectives and record the behavior\nof its individual components. Heterogeneous data sets can be collectively mined\nby data fusion. Fusion can focus on a specific target relation and exploit\ndirectly associated data together with contextual data and data about system's\nconstraints. In the paper we describe a data fusion approach with penalized\nmatrix tri-factorization (DFMF) that simultaneously factorizes data matrices to\nreveal hidden associations. The approach can directly consider any data that\ncan be expressed in a matrix, including those from feature-based\nrepresentations, ontologies, associations and networks. We demonstrate the\nutility of DFMF for gene function prediction task with eleven different data\nsources and for prediction of pharmacologic actions by fusing six data sources.\nOur data fusion algorithm compares favorably to alternative data integration\napproaches and achieves higher accuracy than can be obtained from any single\ndata source alone.\n", "versions": [{"version": "v1", "created": "Tue, 2 Jul 2013 19:35:21 GMT"}, {"version": "v2", "created": "Fri, 6 Feb 2015 16:15:38 GMT"}], "update_date": "2015-02-09", "authors_parsed": [["\u017ditnik", "Marinka", ""], ["Zupan", "Bla\u017e", ""]]}, {"id": "1307.0813", "submitter": "Marc Deisenroth", "authors": "Marc Peter Deisenroth, Peter Englert, Jan Peters and Dieter Fox", "title": "Multi-Task Policy Search", "comments": "8 pages, double column. IEEE International Conference on Robotics and\n  Automation, 2014", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.AI cs.LG cs.RO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Learning policies that generalize across multiple tasks is an important and\nchallenging research topic in reinforcement learning and robotics. Training\nindividual policies for every single potential task is often impractical,\nespecially for continuous task variations, requiring more principled approaches\nto share and transfer knowledge among similar tasks. We present a novel\napproach for learning a nonlinear feedback policy that generalizes across\nmultiple tasks. The key idea is to define a parametrized policy as a function\nof both the state and the task, which allows learning a single policy that\ngeneralizes across multiple known and unknown tasks. Applications of our novel\napproach to reinforcement and imitation learning in real-robot experiments are\nshown.\n", "versions": [{"version": "v1", "created": "Tue, 2 Jul 2013 07:59:32 GMT"}, {"version": "v2", "created": "Wed, 12 Feb 2014 09:17:52 GMT"}], "update_date": "2014-02-13", "authors_parsed": [["Deisenroth", "Marc Peter", ""], ["Englert", "Peter", ""], ["Peters", "Jan", ""], ["Fox", "Dieter", ""]]}, {"id": "1307.0846", "submitter": "Evgeni Tsivtsivadze", "authors": "Evgeni Tsivtsivadze and Tom Heskes", "title": "Semi-supervised Ranking Pursuit", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.IR cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a novel sparse preference learning/ranking algorithm. Our\nalgorithm approximates the true utility function by a weighted sum of basis\nfunctions using the squared loss on pairs of data points, and is a\ngeneralization of the kernel matching pursuit method. It can operate both in a\nsupervised and a semi-supervised setting and allows efficient search for\nmultiple, near-optimal solutions. Furthermore, we describe the extension of the\nalgorithm suitable for combined ranking and regression tasks. In our\nexperiments we demonstrate that the proposed algorithm outperforms several\nstate-of-the-art learning methods when taking into account unlabeled data and\nperforms comparably in a supervised learning scenario, while providing sparser\nsolutions.\n", "versions": [{"version": "v1", "created": "Tue, 2 Jul 2013 20:51:40 GMT"}], "update_date": "2013-07-04", "authors_parsed": [["Tsivtsivadze", "Evgeni", ""], ["Heskes", "Tom", ""]]}, {"id": "1307.0915", "submitter": "Yar Muhamad Mr", "authors": "Yar M. Mughal, A. Krivoshei, P. Annus", "title": "Separation of cardiac and respiratory components from the electrical\n  bio-impedance signal using PCA and fast ICA", "comments": "4 pages, International Conference on Control, Engineering and\n  Information Technology (CEIT'13)", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP physics.ins-det stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper is an attempt to separate cardiac and respiratory signals from an\nelectrical bio-impedance (EBI) dataset. For this two well-known algorithms,\nnamely Principal Component Analysis (PCA) and Independent Component Analysis\n(ICA), were used to accomplish the task. The ability of the PCA and the ICA\nmethods first reduces the dimension and attempt to separate the useful\ncomponents of the EBI, the cardiac and respiratory ones accordingly. It was\ninvestigated with an assumption, that no motion artefacts are present. To carry\nout this procedure the two channel complex EBI measurements were provided using\nclassical Kelvin type four electrode configurations for the each complex\nchannel. Thus four real signals were used as inputs for the PCA and fast ICA.\nThe results showed, that neither PCA nor ICA nor combination of them can not\naccurately separate the components at least are used only two complex (four\nreal valued) input components.\n", "versions": [{"version": "v1", "created": "Wed, 3 Jul 2013 05:51:43 GMT"}], "update_date": "2013-07-04", "authors_parsed": [["Mughal", "Yar M.", ""], ["Krivoshei", "A.", ""], ["Annus", "P.", ""]]}, {"id": "1307.0995", "submitter": "Ji Won Yoon Ph.D.", "authors": "Ji Won Yoon", "title": "An Efficient Model Selection for Gaussian Mixture Model in a Bayesian\n  Framework", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In order to cluster or partition data, we often use\nExpectation-and-Maximization (EM) or Variational approximation with a Gaussian\nMixture Model (GMM), which is a parametric probability density function\nrepresented as a weighted sum of $\\hat{K}$ Gaussian component densities.\nHowever, model selection to find underlying $\\hat{K}$ is one of the key\nconcerns in GMM clustering, since we can obtain the desired clusters only when\n$\\hat{K}$ is known. In this paper, we propose a new model selection algorithm\nto explore $\\hat{K}$ in a Bayesian framework. The proposed algorithm builds the\ndensity of the model order which any information criterions such as AIC and BIC\nbasically fail to reconstruct. In addition, this algorithm reconstructs the\ndensity quickly as compared to the time-consuming Monte Carlo simulation.\n", "versions": [{"version": "v1", "created": "Wed, 3 Jul 2013 12:54:25 GMT"}], "update_date": "2013-07-04", "authors_parsed": [["Yoon", "Ji Won", ""]]}, {"id": "1307.1192", "submitter": "Paul Grigas", "authors": "Robert M. Freund, Paul Grigas, Rahul Mazumder", "title": "AdaBoost and Forward Stagewise Regression are First-Order Convex\n  Optimization Methods", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG math.OC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Boosting methods are highly popular and effective supervised learning methods\nwhich combine weak learners into a single accurate model with good statistical\nperformance. In this paper, we analyze two well-known boosting methods,\nAdaBoost and Incremental Forward Stagewise Regression (FS$_\\varepsilon$), by\nestablishing their precise connections to the Mirror Descent algorithm, which\nis a first-order method in convex optimization. As a consequence of these\nconnections we obtain novel computational guarantees for these boosting\nmethods. In particular, we characterize convergence bounds of AdaBoost, related\nto both the margin and log-exponential loss function, for any step-size\nsequence. Furthermore, this paper presents, for the first time, precise\ncomputational complexity results for FS$_\\varepsilon$.\n", "versions": [{"version": "v1", "created": "Thu, 4 Jul 2013 03:17:23 GMT"}], "update_date": "2013-07-05", "authors_parsed": [["Freund", "Robert M.", ""], ["Grigas", "Paul", ""], ["Mazumder", "Rahul", ""]]}, {"id": "1307.1493", "submitter": "Stefan Wager", "authors": "Stefan Wager, Sida Wang, and Percy Liang", "title": "Dropout Training as Adaptive Regularization", "comments": "11 pages. Advances in Neural Information Processing Systems (NIPS),\n  2013", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Dropout and other feature noising schemes control overfitting by artificially\ncorrupting the training data. For generalized linear models, dropout performs a\nform of adaptive regularization. Using this viewpoint, we show that the dropout\nregularizer is first-order equivalent to an L2 regularizer applied after\nscaling the features by an estimate of the inverse diagonal Fisher information\nmatrix. We also establish a connection to AdaGrad, an online learning\nalgorithm, and find that a close relative of AdaGrad operates by repeatedly\nsolving linear dropout-regularized problems. By casting dropout as\nregularization, we develop a natural semi-supervised algorithm that uses\nunlabeled data to create a better adaptive regularizer. We apply this idea to\ndocument classification tasks, and show that it consistently boosts the\nperformance of dropout training, improving on state-of-the-art results on the\nIMDB reviews dataset.\n", "versions": [{"version": "v1", "created": "Thu, 4 Jul 2013 21:33:56 GMT"}, {"version": "v2", "created": "Fri, 1 Nov 2013 17:56:35 GMT"}], "update_date": "2013-11-04", "authors_parsed": [["Wager", "Stefan", ""], ["Wang", "Sida", ""], ["Liang", "Percy", ""]]}, {"id": "1307.1599", "submitter": "Uwe Aickelin", "authors": "Chris Roadknight, Uwe Aickelin, Guoping Qiu, John Scholefield, Lindy\n  Durrant", "title": "Supervised Learning and Anti-learning of Colorectal Cancer Classes and\n  Survival Rates from Cellular Biology Parameters", "comments": "IEEE International Conference on Systems, Man, and Cybernetics, pp\n  797-802, 2012", "journal-ref": null, "doi": "10.1109/ICSMC.2012.6377825", "report-no": null, "categories": "cs.LG cs.CE stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we describe a dataset relating to cellular and physical\nconditions of patients who are operated upon to remove colorectal tumours. This\ndata provides a unique insight into immunological status at the point of tumour\nremoval, tumour classification and post-operative survival. Attempts are made\nto learn relationships between attributes (physical and immunological) and the\nresulting tumour stage and survival. Results for conventional machine learning\napproaches can be considered poor, especially for predicting tumour stages for\nthe most important types of cancer. This poor performance is further\ninvestigated and compared with a synthetic, dataset based on the logical\nexclusive-OR function and it is shown that there is a significant level of\n'anti-learning' present in all supervised methods used and this can be\nexplained by the highly dimensional, complex and sparsely representative\ndataset. For predicting the stage of cancer from the immunological attributes,\nanti-learning approaches outperform a range of popular algorithms.\n", "versions": [{"version": "v1", "created": "Fri, 5 Jul 2013 12:53:28 GMT"}], "update_date": "2016-11-17", "authors_parsed": [["Roadknight", "Chris", ""], ["Aickelin", "Uwe", ""], ["Qiu", "Guoping", ""], ["Scholefield", "John", ""], ["Durrant", "Lindy", ""]]}, {"id": "1307.1674", "submitter": "Raman Arora", "authors": "Raman Arora, Andrew Cotter, and Nathan Srebro", "title": "Stochastic Optimization of PCA with Capped MSG", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study PCA as a stochastic optimization problem and propose a novel\nstochastic approximation algorithm which we refer to as \"Matrix Stochastic\nGradient\" (MSG), as well as a practical variant, Capped MSG. We study the\nmethod both theoretically and empirically.\n", "versions": [{"version": "v1", "created": "Fri, 5 Jul 2013 17:39:40 GMT"}], "update_date": "2013-07-08", "authors_parsed": [["Arora", "Raman", ""], ["Cotter", "Andrew", ""], ["Srebro", "Nathan", ""]]}, {"id": "1307.1769", "submitter": "Lior Rokach", "authors": "Lior Rokach, Alon Schclar, Ehud Itach", "title": "Ensemble Methods for Multi-label Classification", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Ensemble methods have been shown to be an effective tool for solving\nmulti-label classification tasks. In the RAndom k-labELsets (RAKEL) algorithm,\neach member of the ensemble is associated with a small randomly-selected subset\nof k labels. Then, a single label classifier is trained according to each\ncombination of elements in the subset. In this paper we adopt a similar\napproach, however, instead of randomly choosing subsets, we select the minimum\nrequired subsets of k labels that cover all labels and meet additional\nconstraints such as coverage of inter-label correlations. Construction of the\ncover is achieved by formulating the subset selection as a minimum set covering\nproblem (SCP) and solving it by using approximation algorithms. Every cover\nneeds only to be prepared once by offline algorithms. Once prepared, a cover\nmay be applied to the classification of any given multi-label dataset whose\nproperties conform with those of the cover. The contribution of this paper is\ntwo-fold. First, we introduce SCP as a general framework for constructing label\ncovers while allowing the user to incorporate cover construction constraints.\nWe demonstrate the effectiveness of this framework by proposing two\nconstruction constraints whose enforcement produces covers that improve the\nprediction performance of random selection. Second, we provide theoretical\nbounds that quantify the probabilities of random selection to produce covers\nthat meet the proposed construction criteria. The experimental results indicate\nthat the proposed methods improve multi-label classification accuracy and\nstability compared with the RAKEL algorithm and to other state-of-the-art\nalgorithms.\n", "versions": [{"version": "v1", "created": "Sat, 6 Jul 2013 10:17:44 GMT"}], "update_date": "2013-07-09", "authors_parsed": [["Rokach", "Lior", ""], ["Schclar", "Alon", ""], ["Itach", "Ehud", ""]]}, {"id": "1307.1827", "submitter": "Sivan Sabato", "authors": "Daniel Hsu and Sivan Sabato", "title": "Loss minimization and parameter estimation with heavy tails", "comments": "Final version as published in JMLR", "journal-ref": "Journal of Machine Learning Research, 17(18):1--40, 2016", "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This work studies applications and generalizations of a simple estimation\ntechnique that provides exponential concentration under heavy-tailed\ndistributions, assuming only bounded low-order moments. We show that the\ntechnique can be used for approximate minimization of smooth and strongly\nconvex losses, and specifically for least squares linear regression. For\ninstance, our $d$-dimensional estimator requires just\n$\\tilde{O}(d\\log(1/\\delta))$ random samples to obtain a constant factor\napproximation to the optimal least squares loss with probability $1-\\delta$,\nwithout requiring the covariates or noise to be bounded or subgaussian. We\nprovide further applications to sparse linear regression and low-rank\ncovariance matrix estimation with similar allowances on the noise and covariate\ndistributions. The core technique is a generalization of the median-of-means\nestimator to arbitrary metric spaces.\n", "versions": [{"version": "v1", "created": "Sun, 7 Jul 2013 01:38:16 GMT"}, {"version": "v2", "created": "Wed, 7 Aug 2013 19:48:02 GMT"}, {"version": "v3", "created": "Fri, 9 Aug 2013 16:50:15 GMT"}, {"version": "v4", "created": "Tue, 5 Nov 2013 16:32:14 GMT"}, {"version": "v5", "created": "Fri, 28 Feb 2014 02:57:37 GMT"}, {"version": "v6", "created": "Tue, 8 Jul 2014 19:55:25 GMT"}, {"version": "v7", "created": "Mon, 18 Apr 2016 09:05:38 GMT"}], "update_date": "2016-04-19", "authors_parsed": [["Hsu", "Daniel", ""], ["Sabato", "Sivan", ""]]}, {"id": "1307.1954", "submitter": "Matthew Blaschko", "authors": "Wojciech Zaremba (INRIA Saclay - Ile de France, CVN), Arthur Gretton,\n  Matthew Blaschko (INRIA Saclay - Ile de France, CVN)", "title": "B-tests: Low Variance Kernel Two-Sample Tests", "comments": "Neural Information Processing Systems (2013)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A family of maximum mean discrepancy (MMD) kernel two-sample tests is\nintroduced. Members of the test family are called Block-tests or B-tests, since\nthe test statistic is an average over MMDs computed on subsets of the samples.\nThe choice of block size allows control over the tradeoff between test power\nand computation time. In this respect, the $B$-test family combines favorable\nproperties of previously proposed MMD two-sample tests: B-tests are more\npowerful than a linear time test where blocks are just pairs of samples, yet\nthey are more computationally efficient than a quadratic time test where a\nsingle large block incorporating all the samples is used to compute a\nU-statistic. A further important advantage of the B-tests is their\nasymptotically Normal null distribution: this is by contrast with the\nU-statistic, which is degenerate under the null hypothesis, and for which\nestimates of the null distribution are computationally demanding. Recent\nresults on kernel selection for hypothesis testing transfer seamlessly to the\nB-tests, yielding a means to optimize test power via kernel choice.\n", "versions": [{"version": "v1", "created": "Mon, 8 Jul 2013 06:10:58 GMT"}, {"version": "v2", "created": "Thu, 21 Nov 2013 09:51:44 GMT"}, {"version": "v3", "created": "Mon, 10 Feb 2014 20:39:40 GMT"}], "update_date": "2014-02-11", "authors_parsed": [["Zaremba", "Wojciech", "", "INRIA Saclay - Ile de France, CVN"], ["Gretton", "Arthur", "", "INRIA Saclay - Ile de France, CVN"], ["Blaschko", "Matthew", "", "INRIA Saclay - Ile de France, CVN"]]}, {"id": "1307.2302", "submitter": "Karl Rohe", "authors": "Karl Rohe and Tai Qin", "title": "The blessing of transitivity in sparse and stochastic networks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The interaction between transitivity and sparsity, two common features in\nempirical networks, implies that there are local regions of large sparse\nnetworks that are dense. We call this the blessing of transitivity and it has\nconsequences for both modeling and inference. Extant research suggests that\nstatistical inference for the Stochastic Blockmodel is more difficult when the\nedges are sparse. However, this conclusion is confounded by the fact that the\nasymptotic limit in all of the previous studies is not merely sparse, but also\nnon-transitive. To retain transitivity, the blocks cannot grow faster than the\nexpected degree. Thus, in sparse models, the blocks must remain asymptotically\nsmall. \\n Previous algorithmic research demonstrates that small \"local\"\nclusters are more amenable to computation, visualization, and interpretation\nwhen compared to \"global\" graph partitions. This paper provides the first\nstatistical results that demonstrate how these small transitive clusters are\nalso more amenable to statistical estimation. Theorem 2 shows that a \"local\"\nclustering algorithm can, with high probability, detect a transitive stochastic\nblock of a fixed size (e.g. 30 nodes) embedded in a large graph. The only\nconstraint on the ambient graph is that it is large and sparse--it could be\ngenerated at random or by an adversary--suggesting a theoretical explanation\nfor the robust empirical performance of local clustering algorithms.\n", "versions": [{"version": "v1", "created": "Mon, 8 Jul 2013 23:14:03 GMT"}, {"version": "v2", "created": "Thu, 1 Aug 2013 15:26:59 GMT"}], "update_date": "2013-08-02", "authors_parsed": [["Rohe", "Karl", ""], ["Qin", "Tai", ""]]}, {"id": "1307.2307", "submitter": "Kun Zhang", "authors": "Kun Zhang, Heng Peng, Laiwan Chan, Aapo Hyvarinen", "title": "Bridging Information Criteria and Parameter Shrinkage for Model\n  Selection", "comments": "16 pages, 3 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Model selection based on classical information criteria, such as BIC, is\ngenerally computationally demanding, but its properties are well studied. On\nthe other hand, model selection based on parameter shrinkage by $\\ell_1$-type\npenalties is computationally efficient. In this paper we make an attempt to\ncombine their strengths, and propose a simple approach that penalizes the\nlikelihood with data-dependent $\\ell_1$ penalties as in adaptive Lasso and\nexploits a fixed penalization parameter. Even for finite samples, its model\nselection results approximately coincide with those based on information\ncriteria; in particular, we show that in some special cases, this approach and\nthe corresponding information criterion produce exactly the same model. One can\nalso consider this approach as a way to directly determine the penalization\nparameter in adaptive Lasso to achieve information criteria-like model\nselection. As extensions, we apply this idea to complex models including\nGaussian mixture model and mixture of factor analyzers, whose model selection\nis traditionally difficult to do; by adopting suitable penalties, we provide\ncontinuous approximators to the corresponding information criteria, which are\neasy to optimize and enable efficient model selection.\n", "versions": [{"version": "v1", "created": "Mon, 8 Jul 2013 23:52:55 GMT"}], "update_date": "2013-07-10", "authors_parsed": [["Zhang", "Kun", ""], ["Peng", "Heng", ""], ["Chan", "Laiwan", ""], ["Hyvarinen", "Aapo", ""]]}, {"id": "1307.2312", "submitter": "Diane Oyen", "authors": "Diane Oyen and Terran Lane", "title": "Bayesian Discovery of Multiple Bayesian Networks via Transfer Learning", "comments": "10 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Bayesian network structure learning algorithms with limited data are being\nused in domains such as systems biology and neuroscience to gain insight into\nthe underlying processes that produce observed data. Learning reliable networks\nfrom limited data is difficult, therefore transfer learning can improve the\nrobustness of learned networks by leveraging data from related tasks. Existing\ntransfer learning algorithms for Bayesian network structure learning give a\nsingle maximum a posteriori estimate of network models. Yet, many other models\nmay be equally likely, and so a more informative result is provided by Bayesian\nstructure discovery. Bayesian structure discovery algorithms estimate posterior\nprobabilities of structural features, such as edges. We present transfer\nlearning for Bayesian structure discovery which allows us to explore the shared\nand unique structural features among related tasks. Efficient computation\nrequires that our transfer learning objective factors into local calculations,\nwhich we prove is given by a broad class of transfer biases. Theoretically, we\nshow the efficiency of our approach. Empirically, we show that compared to\nsingle task learning, transfer learning is better able to positively identify\ntrue edges. We apply the method to whole-brain neuroimaging data.\n", "versions": [{"version": "v1", "created": "Tue, 9 Jul 2013 00:58:10 GMT"}], "update_date": "2013-07-10", "authors_parsed": [["Oyen", "Diane", ""], ["Lane", "Terran", ""]]}, {"id": "1307.2579", "submitter": "Jonathan Huang", "authors": "Chris Piech, Jonathan Huang, Zhenghao Chen, Chuong Do, Andrew Ng,\n  Daphne Koller", "title": "Tuned Models of Peer Assessment in MOOCs", "comments": "Proceedings of The 6th International Conference on Educational Data\n  Mining (EDM 2013)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI cs.HC stat.AP stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In massive open online courses (MOOCs), peer grading serves as a critical\ntool for scaling the grading of complex, open-ended assignments to courses with\ntens or hundreds of thousands of students. But despite promising initial\ntrials, it does not always deliver accurate results compared to human experts.\nIn this paper, we develop algorithms for estimating and correcting for grader\nbiases and reliabilities, showing significant improvement in peer grading\naccuracy on real data with 63,199 peer grades from Coursera's HCI course\nofferings --- the largest peer grading networks analysed to date. We relate\ngrader biases and reliabilities to other student factors such as student\nengagement, performance as well as commenting style. We also show that our\nmodel can lead to more intelligent assignment of graders to gradees.\n", "versions": [{"version": "v1", "created": "Tue, 9 Jul 2013 20:03:51 GMT"}], "update_date": "2013-07-11", "authors_parsed": [["Piech", "Chris", ""], ["Huang", "Jonathan", ""], ["Chen", "Zhenghao", ""], ["Do", "Chuong", ""], ["Ng", "Andrew", ""], ["Koller", "Daphne", ""]]}, {"id": "1307.2611", "submitter": "Diane Oyen", "authors": "Diane Oyen, Alexandru Niculescu-Mizil, Rachel Ostroff, Alex Stewart,\n  Vincent P. Clark", "title": "Controlling the Precision-Recall Tradeoff in Differential Dependency\n  Network Analysis", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Graphical models have gained a lot of attention recently as a tool for\nlearning and representing dependencies among variables in multivariate data.\nOften, domain scientists are looking specifically for differences among the\ndependency networks of different conditions or populations (e.g. differences\nbetween regulatory networks of different species, or differences between\ndependency networks of diseased versus healthy populations). The standard\nmethod for finding these differences is to learn the dependency networks for\neach condition independently and compare them. We show that this approach is\nprone to high false discovery rates (low precision) that can render the\nanalysis useless. We then show that by imposing a bias towards learning similar\ndependency networks for each condition the false discovery rates can be reduced\nto acceptable levels, at the cost of finding a reduced number of differences.\nAlgorithms developed in the transfer learning literature can be used to vary\nthe strength of the imposed similarity bias and provide a natural mechanism to\nsmoothly adjust this differential precision-recall tradeoff to cater to the\nrequirements of the analysis conducted. We present real case studies\n(oncological and neurological) where domain experts use the proposed technique\nto extract useful differential networks that shed light on the biological\nprocesses involved in cancer and brain function.\n", "versions": [{"version": "v1", "created": "Tue, 9 Jul 2013 22:07:55 GMT"}], "update_date": "2013-07-11", "authors_parsed": [["Oyen", "Diane", ""], ["Niculescu-Mizil", "Alexandru", ""], ["Ostroff", "Rachel", ""], ["Stewart", "Alex", ""], ["Clark", "Vincent P.", ""]]}, {"id": "1307.2674", "submitter": "Hongwei Li", "authors": "Hongwei Li, Bin Yu and Dengyong Zhou", "title": "Error Rate Bounds in Crowdsourcing Models", "comments": "13 pages, 3 figures, downloadable supplementary files", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Crowdsourcing is an effective tool for human-powered computation on many\ntasks challenging for computers. In this paper, we provide finite-sample\nexponential bounds on the error rate (in probability and in expectation) of\nhyperplane binary labeling rules under the Dawid-Skene crowdsourcing model. The\nbounds can be applied to analyze many common prediction methods, including the\nmajority voting and weighted majority voting. These bound results could be\nuseful for controlling the error rate and designing better algorithms. We show\nthat the oracle Maximum A Posterior (MAP) rule approximately optimizes our\nupper bound on the mean error rate for any hyperplane binary labeling rule, and\npropose a simple data-driven weighted majority voting (WMV) rule (called\none-step WMV) that attempts to approximate the oracle MAP and has a provable\ntheoretical guarantee on the error rate. Moreover, we use simulated and real\ndata to demonstrate that the data-driven EM-MAP rule is a good approximation to\nthe oracle MAP rule, and to demonstrate that the mean error rate of the\ndata-driven EM-MAP rule is also bounded by the mean error rate bound of the\noracle MAP rule with estimated parameters plugging into the bound.\n", "versions": [{"version": "v1", "created": "Wed, 10 Jul 2013 05:19:10 GMT"}], "update_date": "2013-07-11", "authors_parsed": [["Li", "Hongwei", ""], ["Yu", "Bin", ""], ["Zhou", "Dengyong", ""]]}, {"id": "1307.2715", "submitter": "Michel Plantie", "authors": "Michel Crampes, Michel Planti\\'e, Marie Lopez", "title": "Optimisation dans la d\\'etection de communaut\\'es recouvrantes et\n  \\'equilibre de Nash", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Community detection in graphs has been the subject of many algorithms. Recent\nmethods want to optimize a modularity function which shows a maximum of\nrelationships within communities and found a minimum of inter-community\nrelations. these algorithms are applied to unipartite, multipartite and\ndirected graphs. However, given the NP-completeness of the problem, these\nalgorithms are heuristics that do not guarantee an optimum. In this paper we\nintroduce an algorithm which, based on an approximate solution obtained through\na efficient detection algorithm, modifie it to achieve a local optimum based on\na function. this reassignment function is a potential function and therefore\nthe computed optimum is a Nash equilibrium. We supplement our method with an\noverlap function that allows to have simultaneously the two detection modes.\nSeveral experiments show the interest of our approach.\n", "versions": [{"version": "v1", "created": "Wed, 10 Jul 2013 08:54:00 GMT"}], "update_date": "2013-07-11", "authors_parsed": [["Crampes", "Michel", ""], ["Planti\u00e9", "Michel", ""], ["Lopez", "Marie", ""]]}, {"id": "1307.2855", "submitter": "Zeyuan Allen Zhu", "authors": "Lorenzo Orecchia, Zeyuan Allen Zhu", "title": "Flow-Based Algorithms for Local Graph Clustering", "comments": "A shorter version of this paper has appeared in the proceedings of\n  the 25th ACM-SIAM Symposium on Discrete Algorithms (SODA) 2014", "journal-ref": null, "doi": "10.1137/1.9781611973402.94", "report-no": null, "categories": "cs.DS cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Given a subset S of vertices of an undirected graph G, the cut-improvement\nproblem asks us to find a subset S that is similar to A but has smaller\nconductance. A very elegant algorithm for this problem has been given by\nAndersen and Lang [AL08] and requires solving a small number of\nsingle-commodity maximum flow computations over the whole graph G. In this\npaper, we introduce LocalImprove, the first cut-improvement algorithm that is\nlocal, i.e. that runs in time dependent on the size of the input set A rather\nthan on the size of the entire graph. Moreover, LocalImprove achieves this\nlocal behaviour while essentially matching the same theoretical guarantee as\nthe global algorithm of Andersen and Lang.\n  The main application of LocalImprove is to the design of better\nlocal-graph-partitioning algorithms. All previously known local algorithms for\ngraph partitioning are random-walk based and can only guarantee an output\nconductance of O(\\sqrt{OPT}) when the target set has conductance OPT \\in [0,1].\nVery recently, Zhu, Lattanzi and Mirrokni [ZLM13] improved this to O(OPT /\n\\sqrt{CONN}) where the internal connectivity parameter CONN \\in [0,1] is\ndefined as the reciprocal of the mixing time of the random walk over the graph\ninduced by the target set. In this work, we show how to use LocalImprove to\nobtain a constant approximation O(OPT) as long as CONN/OPT = Omega(1). This\nyields the first flow-based algorithm. Moreover, its performance strictly\noutperforms the ones based on random walks and surprisingly matches that of the\nbest known global algorithm, which is SDP-based, in this parameter regime\n[MMV12].\n  Finally, our results show that spectral methods are not the only viable\napproach to the construction of local graph partitioning algorithm and open\ndoor to the study of algorithms with even better approximation and locality\nguarantees.\n", "versions": [{"version": "v1", "created": "Wed, 10 Jul 2013 17:04:35 GMT"}, {"version": "v2", "created": "Sun, 13 Oct 2013 19:44:03 GMT"}], "update_date": "2014-11-07", "authors_parsed": [["Orecchia", "Lorenzo", ""], ["Zhu", "Zeyuan Allen", ""]]}, {"id": "1307.2965", "submitter": "Quan Wang", "authors": "Quan Wang, Dijia Wu, Le Lu, Meizhu Liu, Kim L. Boyer and Shaohua Kevin\n  Zhou", "title": "Semantic Context Forests for Learning-Based Knee Cartilage Segmentation\n  in 3D MR Images", "comments": "MICCAI 2013: Workshop on Medical Computer Vision", "journal-ref": null, "doi": "10.1007/978-3-319-05530-5_11", "report-no": null, "categories": "cs.CV cs.LG q-bio.TO stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The automatic segmentation of human knee cartilage from 3D MR images is a\nuseful yet challenging task due to the thin sheet structure of the cartilage\nwith diffuse boundaries and inhomogeneous intensities. In this paper, we\npresent an iterative multi-class learning method to segment the femoral, tibial\nand patellar cartilage simultaneously, which effectively exploits the spatial\ncontextual constraints between bone and cartilage, and also between different\ncartilages. First, based on the fact that the cartilage grows in only certain\narea of the corresponding bone surface, we extract the distance features of not\nonly to the surface of the bone, but more informatively, to the densely\nregistered anatomical landmarks on the bone surface. Second, we introduce a set\nof iterative discriminative classifiers that at each iteration, probability\ncomparison features are constructed from the class confidence maps derived by\npreviously learned classifiers. These features automatically embed the semantic\ncontext information between different cartilages of interest. Validated on a\ntotal of 176 volumes from the Osteoarthritis Initiative (OAI) dataset, the\nproposed approach demonstrates high robustness and accuracy of segmentation in\ncomparison with existing state-of-the-art MR cartilage segmentation methods.\n", "versions": [{"version": "v1", "created": "Thu, 11 Jul 2013 03:29:51 GMT"}, {"version": "v2", "created": "Tue, 22 Apr 2014 16:01:12 GMT"}], "update_date": "2017-11-07", "authors_parsed": [["Wang", "Quan", ""], ["Wu", "Dijia", ""], ["Lu", "Le", ""], ["Liu", "Meizhu", ""], ["Boyer", "Kim L.", ""], ["Zhou", "Shaohua Kevin", ""]]}, {"id": "1307.2971", "submitter": "Ana Georgina Flesia MS", "authors": "Ana Georgina Flesia, Josef Baumgartner, Javier Gimenez, Jorge Martinez", "title": "Accuracy of MAP segmentation with hidden Potts and Markov mesh prior\n  models via Path Constrained Viterbi Training, Iterated Conditional Modes and\n  Graph Cut based algorithms", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we study statistical classification accuracy of two different\nMarkov field environments for pixelwise image segmentation, considering the\nlabels of the image as hidden states and solving the estimation of such labels\nas a solution of the MAP equation. The emission distribution is assumed the\nsame in all models, and the difference lays in the Markovian prior hypothesis\nmade over the labeling random field. The a priori labeling knowledge will be\nmodeled with a) a second order anisotropic Markov Mesh and b) a classical\nisotropic Potts model. Under such models, we will consider three different\nsegmentation procedures, 2D Path Constrained Viterbi training for the Hidden\nMarkov Mesh, a Graph Cut based segmentation for the first order isotropic Potts\nmodel, and ICM (Iterated Conditional Modes) for the second order isotropic\nPotts model.\n  We provide a unified view of all three methods, and investigate goodness of\nfit for classification, studying the influence of parameter estimation,\ncomputational gain, and extent of automation in the statistical measures\nOverall Accuracy, Relative Improvement and Kappa coefficient, allowing robust\nand accurate statistical analysis on synthetic and real-life experimental data\ncoming from the field of Dental Diagnostic Radiography. All algorithms, using\nthe learned parameters, generate good segmentations with little interaction\nwhen the images have a clear multimodal histogram. Suboptimal learning proves\nto be frail in the case of non-distinctive modes, which limits the complexity\nof usable models, and hence the achievable error rate as well.\n  All Matlab code written is provided in a toolbox available for download from\nour website, following the Reproducible Research Paradigm.\n", "versions": [{"version": "v1", "created": "Thu, 11 Jul 2013 04:49:11 GMT"}], "update_date": "2013-07-12", "authors_parsed": [["Flesia", "Ana Georgina", ""], ["Baumgartner", "Josef", ""], ["Gimenez", "Javier", ""], ["Martinez", "Jorge", ""]]}, {"id": "1307.3102", "submitter": "Vitaly Feldman", "authors": "Maria Florina Balcan, Vitaly Feldman", "title": "Statistical Active Learning Algorithms for Noise Tolerance and\n  Differential Privacy", "comments": "Extended abstract appears in NIPS 2013", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.DS stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We describe a framework for designing efficient active learning algorithms\nthat are tolerant to random classification noise and are\ndifferentially-private. The framework is based on active learning algorithms\nthat are statistical in the sense that they rely on estimates of expectations\nof functions of filtered random examples. It builds on the powerful statistical\nquery framework of Kearns (1993).\n  We show that any efficient active statistical learning algorithm can be\nautomatically converted to an efficient active learning algorithm which is\ntolerant to random classification noise as well as other forms of\n\"uncorrelated\" noise. The complexity of the resulting algorithms has\ninformation-theoretically optimal quadratic dependence on $1/(1-2\\eta)$, where\n$\\eta$ is the noise rate.\n  We show that commonly studied concept classes including thresholds,\nrectangles, and linear separators can be efficiently actively learned in our\nframework. These results combined with our generic conversion lead to the first\ncomputationally-efficient algorithms for actively learning some of these\nconcept classes in the presence of random classification noise that provide\nexponential improvement in the dependence on the error $\\epsilon$ over their\npassive counterparts. In addition, we show that our algorithms can be\nautomatically converted to efficient active differentially-private algorithms.\nThis leads to the first differentially-private active learning algorithms with\nexponential label savings over the passive case.\n", "versions": [{"version": "v1", "created": "Thu, 11 Jul 2013 13:31:21 GMT"}, {"version": "v2", "created": "Mon, 15 Jul 2013 02:13:05 GMT"}, {"version": "v3", "created": "Fri, 3 Jan 2014 05:38:04 GMT"}, {"version": "v4", "created": "Wed, 5 Nov 2014 06:41:07 GMT"}], "update_date": "2014-11-06", "authors_parsed": [["Balcan", "Maria Florina", ""], ["Feldman", "Vitaly", ""]]}, {"id": "1307.3176", "submitter": "L.A. Prashanth", "authors": "Nathaniel Korda, Prashanth L.A. and R\\'emi Munos", "title": "Fast gradient descent for drifting least squares regression, with\n  application to bandits", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Online learning algorithms require to often recompute least squares\nregression estimates of parameters. We study improving the computational\ncomplexity of such algorithms by using stochastic gradient descent (SGD) type\nschemes in place of classic regression solvers. We show that SGD schemes\nefficiently track the true solutions of the regression problems, even in the\npresence of a drift. This finding coupled with an $O(d)$ improvement in\ncomplexity, where $d$ is the dimension of the data, make them attractive for\nimplementation in the big data settings. In the case when strong convexity in\nthe regression problem is guaranteed, we provide bounds on the error both in\nexpectation and high probability (the latter is often needed to provide\ntheoretical guarantees for higher level algorithms), despite the drifting least\nsquares solution. As an example of this case we prove that the regret\nperformance of an SGD version of the PEGE linear bandit algorithm\n[Rusmevichientong and Tsitsiklis 2010] is worse that that of PEGE itself only\nby a factor of $O(\\log^4 n)$. When strong convexity of the regression problem\ncannot be guaranteed, we investigate using an adaptive regularisation. We make\nan empirical study of an adaptively regularised, SGD version of LinUCB [Li et\nal. 2010] in a news article recommendation application, which uses the large\nscale news recommendation dataset from Yahoo! front page. These experiments\nshow a large gain in computational complexity, with a consistently low tracking\nerror and click-through-rate (CTR) performance that is $75\\%$ close.\n", "versions": [{"version": "v1", "created": "Thu, 11 Jul 2013 16:36:29 GMT"}, {"version": "v2", "created": "Wed, 19 Feb 2014 00:27:18 GMT"}, {"version": "v3", "created": "Thu, 24 Jul 2014 14:29:52 GMT"}, {"version": "v4", "created": "Thu, 20 Nov 2014 12:40:48 GMT"}], "update_date": "2014-11-21", "authors_parsed": [["Korda", "Nathaniel", ""], ["A.", "Prashanth L.", ""], ["Munos", "R\u00e9mi", ""]]}, {"id": "1307.3227", "submitter": "Aurelie Lozano C", "authors": "Aur\\'elie C. Lozano and Nicolai Meinshausen", "title": "Minimum Distance Estimation for Robust High-Dimensional Regression", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a minimum distance estimation method for robust regression in\nsparse high-dimensional settings. The traditional likelihood-based estimators\nlack resilience against outliers, a critical issue when dealing with\nhigh-dimensional noisy data. Our method, Minimum Distance Lasso (MD-Lasso),\ncombines minimum distance functionals, customarily used in nonparametric\nestimation for their robustness, with l1-regularization for high-dimensional\nregression. The geometry of MD-Lasso is key to its consistency and robustness.\nThe estimator is governed by a scaling parameter that caps the influence of\noutliers: the loss per observation is locally convex and close to quadratic for\nsmall squared residuals, and flattens for squared residuals larger than the\nscaling parameter. As the parameter approaches infinity, the estimator becomes\nequivalent to least-squares Lasso. MD-Lasso enjoys fast convergence rates under\nmild conditions on the model error distribution, which hold for any of the\nsolutions in a convexity region around the true parameter and in certain cases\nfor every solution. Remarkably, a first-order optimization method is able to\nproduce iterates very close to the consistent solutions, with geometric\nconvergence and regardless of the initialization. A connection is established\nwith re-weighted least-squares that intuitively explains MD-Lasso robustness.\nThe merits of our method are demonstrated through simulation and eQTL data\nanalysis.\n", "versions": [{"version": "v1", "created": "Thu, 11 Jul 2013 19:41:00 GMT"}], "update_date": "2013-07-12", "authors_parsed": [["Lozano", "Aur\u00e9lie C.", ""], ["Meinshausen", "Nicolai", ""]]}, {"id": "1307.3400", "submitter": "Nathaniel Korda", "authors": "Nathaniel Korda, Emilie Kaufmann and Remi Munos", "title": "Thompson Sampling for 1-Dimensional Exponential Family Bandits", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Thompson Sampling has been demonstrated in many complex bandit models,\nhowever the theoretical guarantees available for the parametric multi-armed\nbandit are still limited to the Bernoulli case. Here we extend them by proving\nasymptotic optimality of the algorithm using the Jeffreys prior for\n1-dimensional exponential family bandits. Our proof builds on previous work,\nbut also makes extensive use of closed forms for Kullback-Leibler divergence\nand Fisher information (and thus Jeffreys prior) available in an exponential\nfamily. This allow us to give a finite time exponential concentration\ninequality for posterior distributions on exponential families that may be of\ninterest in its own right. Moreover our analysis covers some distributions for\nwhich no optimistic algorithm has yet been proposed, including heavy-tailed\nexponential families.\n", "versions": [{"version": "v1", "created": "Fri, 12 Jul 2013 10:36:56 GMT"}], "update_date": "2013-07-15", "authors_parsed": [["Korda", "Nathaniel", ""], ["Kaufmann", "Emilie", ""], ["Munos", "Remi", ""]]}, {"id": "1307.3490", "submitter": "Aditya Tulsyan", "authors": "Aditya Tulsyan, Biao Huang, R. Bhushan Gopaluni and J. Fraser Forbes", "title": "On-line Bayesian parameter estimation in general non-linear state-space\n  models: A tutorial and new results", "comments": "A condensed version of this article has been published in: Tulsyan,\n  A., Huang, B., Gopaluni, R.B., Forbes, J.F. \"On simultaneous on-line state\n  and parameter estimation in non-linear state-space models\". Journal of\n  Process Control, vol 23, no. 4, 2013", "journal-ref": "Journal of Process Control, vol 23, no. 4, 2013", "doi": null, "report-no": null, "categories": "stat.CO stat.AP stat.ME stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  On-line estimation plays an important role in process control and monitoring.\nObtaining a theoretical solution to the simultaneous state-parameter estimation\nproblem for non-linear stochastic systems involves solving complex\nmulti-dimensional integrals that are not amenable to analytical solution. While\nbasic sequential Monte-Carlo (SMC) or particle filtering (PF) algorithms for\nsimultaneous estimation exist, it is well recognized that there is a need for\nmaking these on-line algorithms non-degenerate, fast and applicable to\nprocesses with missing measurements. To overcome the deficiencies in\ntraditional algorithms, this work proposes a Bayesian approach to on-line state\nand parameter estimation. Its extension to handle missing data in real-time is\nalso provided. The simultaneous estimation is performed by filtering an\nextended vector of states and parameters using an adaptive\nsequential-importance-resampling (SIR) filter with a kernel density estimation\nmethod. The approach uses an on-line optimization algorithm based on\nKullback-Leibler (KL) divergence to allow adaptation of the SIR filter for\ncombined state-parameter estimation. An optimal tuning rule to control the\nwidth of the kernel and the variance of the artificial noise added to the\nparameters is also proposed. The approach is illustrated through numerical\nexamples.\n", "versions": [{"version": "v1", "created": "Fri, 12 Jul 2013 15:30:38 GMT"}], "update_date": "2013-07-15", "authors_parsed": [["Tulsyan", "Aditya", ""], ["Huang", "Biao", ""], ["Gopaluni", "R. Bhushan", ""], ["Forbes", "J. Fraser", ""]]}, {"id": "1307.3598", "submitter": "Paul McNicholas", "authors": "Irene Vrbik and Paul D. McNicholas", "title": "Fractionally-Supervised Classification", "comments": null, "journal-ref": null, "doi": "10.1007/s00357-015-9188-9", "report-no": null, "categories": "stat.ME stat.AP stat.CO stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Traditionally, there are three species of classification: unsupervised,\nsupervised, and semi-supervised. Supervised and semi-supervised classification\ndiffer by whether or not weight is given to unlabelled observations in the\nclassification procedure. In unsupervised classification, or clustering, all\nobservations are unlabeled and hence full weight is given to unlabelled\nobservations. When some observations are unlabelled, it can be very difficult\nto \\textit{a~priori} choose the optimal level of supervision, and the\nconsequences of a sub-optimal choice can be non-trivial. A flexible\nfractionally-supervised approach to classification is introduced, where any\nlevel of supervision --- ranging from unsupervised to supervised --- can be\nattained. Our approach uses a weighted likelihood, wherein weights control the\nrelative role that labelled and unlabelled data have in building a classifier.\nA comparison between our approach and the traditional species is presented\nusing simulated and real data. Gaussian mixture models are used as a vehicle to\nillustrate our fractionally-supervised classification approach; however, it is\nbroadly applicable and variations on the postulated model can be easily made.\n", "versions": [{"version": "v1", "created": "Sat, 13 Jul 2013 00:41:37 GMT"}, {"version": "v2", "created": "Mon, 22 Jul 2013 16:15:30 GMT"}, {"version": "v3", "created": "Mon, 5 Jan 2015 05:29:13 GMT"}, {"version": "v4", "created": "Sun, 13 Sep 2015 18:04:19 GMT"}, {"version": "v5", "created": "Wed, 23 Sep 2015 18:16:39 GMT"}], "update_date": "2017-10-09", "authors_parsed": [["Vrbik", "Irene", ""], ["McNicholas", "Paul D.", ""]]}, {"id": "1307.3617", "submitter": "Varun Kanade", "authors": "Varun Kanade, Elchanan Mossel", "title": "MCMC Learning", "comments": "28 pages, 1 figure", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The theory of learning under the uniform distribution is rich and deep, with\nconnections to cryptography, computational complexity, and the analysis of\nboolean functions to name a few areas. This theory however is very limited due\nto the fact that the uniform distribution and the corresponding Fourier basis\nare rarely encountered as a statistical model.\n  A family of distributions that vastly generalizes the uniform distribution on\nthe Boolean cube is that of distributions represented by Markov Random Fields\n(MRF). Markov Random Fields are one of the main tools for modeling high\ndimensional data in many areas of statistics and machine learning.\n  In this paper we initiate the investigation of extending central ideas,\nmethods and algorithms from the theory of learning under the uniform\ndistribution to the setup of learning concepts given examples from MRF\ndistributions. In particular, our results establish a novel connection between\nproperties of MCMC sampling of MRFs and learning under the MRF distribution.\n", "versions": [{"version": "v1", "created": "Sat, 13 Jul 2013 07:00:00 GMT"}, {"version": "v2", "created": "Fri, 12 Jun 2015 08:11:27 GMT"}], "update_date": "2015-06-15", "authors_parsed": [["Kanade", "Varun", ""], ["Mossel", "Elchanan", ""]]}, {"id": "1307.3785", "submitter": "Christos Dimitrakakis", "authors": "Aristide C. Y. Tossou and Christos Dimitrakakis", "title": "Probabilistic inverse reinforcement learning in unknown environments", "comments": "UAI 2013", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider the problem of learning by demonstration from agents acting in\nunknown stochastic Markov environments or games. Our aim is to estimate agent\npreferences in order to construct improved policies for the same task that the\nagents are trying to solve. To do so, we extend previous probabilistic\napproaches for inverse reinforcement learning in known MDPs to the case of\nunknown dynamics or opponents. We do this by deriving two simplified\nprobabilistic models of the demonstrator's policy and utility. For\ntractability, we use maximum a posteriori estimation rather than full Bayesian\ninference. Under a flat prior, this results in a convex optimisation problem.\nWe find that the resulting algorithms are highly competitive against a variety\nof other methods for inverse reinforcement learning that do have knowledge of\nthe dynamics.\n", "versions": [{"version": "v1", "created": "Sun, 14 Jul 2013 22:06:12 GMT"}], "update_date": "2013-07-16", "authors_parsed": [["Tossou", "Aristide C. Y.", ""], ["Dimitrakakis", "Christos", ""]]}, {"id": "1307.3846", "submitter": "S\\'ebastien Brati\\`eres", "authors": "Sebastien Bratieres, Novi Quadrianto, Zoubin Ghahramani", "title": "Bayesian Structured Prediction Using Gaussian Processes", "comments": "8 pages with figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We introduce a conceptually novel structured prediction model, GPstruct,\nwhich is kernelized, non-parametric and Bayesian, by design. We motivate the\nmodel with respect to existing approaches, among others, conditional random\nfields (CRFs), maximum margin Markov networks (M3N), and structured support\nvector machines (SVMstruct), which embody only a subset of its properties. We\npresent an inference procedure based on Markov Chain Monte Carlo. The framework\ncan be instantiated for a wide range of structured objects such as linear\nchains, trees, grids, and other general graphs. As a proof of concept, the\nmodel is benchmarked on several natural language processing tasks and a video\ngesture segmentation task involving a linear chain structure. We show\nprediction accuracies for GPstruct which are comparable to or exceeding those\nof CRFs and SVMstruct.\n", "versions": [{"version": "v1", "created": "Mon, 15 Jul 2013 07:57:56 GMT"}], "update_date": "2013-07-16", "authors_parsed": [["Bratieres", "Sebastien", ""], ["Quadrianto", "Novi", ""], ["Ghahramani", "Zoubin", ""]]}, {"id": "1307.3949", "submitter": "Steffen Borgwardt", "authors": "Steffen Borgwardt", "title": "On Soft Power Diagrams", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG math.OC stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Many applications in data analysis begin with a set of points in a Euclidean\nspace that is partitioned into clusters. Common tasks then are to devise a\nclassifier deciding which of the clusters a new point is associated to, finding\noutliers with respect to the clusters, or identifying the type of clustering\nused for the partition.\n  One of the common kinds of clusterings are (balanced) least-squares\nassignments with respect to a given set of sites. For these, there is a\n'separating power diagram' for which each cluster lies in its own cell.\n  In the present paper, we aim for efficient algorithms for outlier detection\nand the computation of thresholds that measure how similar a clustering is to a\nleast-squares assignment for fixed sites. For this purpose, we devise a new\nmodel for the computation of a 'soft power diagram', which allows a soft\nseparation of the clusters with 'point counting properties'; e.g. we are able\nto prescribe how many points we want to classify as outliers.\n  As our results hold for a more general non-convex model of free sites, we\ndescribe it and our proofs in this more general way. Its locally optimal\nsolutions satisfy the aforementioned point counting properties. For our target\napplications that use fixed sites, our algorithms are efficiently solvable to\nglobal optimality by linear programming.\n", "versions": [{"version": "v1", "created": "Mon, 15 Jul 2013 14:04:39 GMT"}, {"version": "v2", "created": "Tue, 24 Jun 2014 14:21:00 GMT"}], "update_date": "2014-06-25", "authors_parsed": [["Borgwardt", "Steffen", ""]]}, {"id": "1307.3964", "submitter": "Alejandro Edera", "authors": "Alejandro Edera, Federico Schl\\\"uter, Facundo Bromberg", "title": "Learning Markov networks with context-specific independences", "comments": "8 pages, 6 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Learning the Markov network structure from data is a problem that has\nreceived considerable attention in machine learning, and in many other\napplication fields. This work focuses on a particular approach for this purpose\ncalled independence-based learning. Such approach guarantees the learning of\nthe correct structure efficiently, whenever data is sufficient for representing\nthe underlying distribution. However, an important issue of such approach is\nthat the learned structures are encoded in an undirected graph. The problem\nwith graphs is that they cannot encode some types of independence relations,\nsuch as the context-specific independences. They are a particular case of\nconditional independences that is true only for a certain assignment of its\nconditioning set, in contrast to conditional independences that must hold for\nall its assignments. In this work we present CSPC, an independence-based\nalgorithm for learning structures that encode context-specific independences,\nand encoding them in a log-linear model, instead of a graph. The central idea\nof CSPC is combining the theoretical guarantees provided by the\nindependence-based approach with the benefits of representing complex\nstructures by using features in a log-linear model. We present experiments in a\nsynthetic case, showing that CSPC is more accurate than the state-of-the-art IB\nalgorithms when the underlying distribution contains CSIs.\n", "versions": [{"version": "v1", "created": "Mon, 15 Jul 2013 14:31:44 GMT"}], "update_date": "2013-07-16", "authors_parsed": [["Edera", "Alejandro", ""], ["Schl\u00fcter", "Federico", ""], ["Bromberg", "Facundo", ""]]}, {"id": "1307.4048", "submitter": "Pavan Kumar D S", "authors": "D. S. Pavan Kumar, N. Vishnu Prasad, Vikas Joshi, S. Umesh", "title": "Modified SPLICE and its Extension to Non-Stereo Data for Noise Robust\n  Speech Recognition", "comments": "Submitted to Automatic Speech Recognition and Understanding (ASRU)\n  2013 Workshop", "journal-ref": null, "doi": "10.1109/ASRU.2013.6707725", "report-no": null, "categories": "cs.LG cs.CV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, a modification to the training process of the popular SPLICE\nalgorithm has been proposed for noise robust speech recognition. The\nmodification is based on feature correlations, and enables this stereo-based\nalgorithm to improve the performance in all noise conditions, especially in\nunseen cases. Further, the modified framework is extended to work for\nnon-stereo datasets where clean and noisy training utterances, but not stereo\ncounterparts, are required. Finally, an MLLR-based computationally efficient\nrun-time noise adaptation method in SPLICE framework has been proposed. The\nmodified SPLICE shows 8.6% absolute improvement over SPLICE in Test C of\nAurora-2 database, and 2.93% overall. Non-stereo method shows 10.37% and 6.93%\nabsolute improvements over Aurora-2 and Aurora-4 baseline models respectively.\nRun-time adaptation shows 9.89% absolute improvement in modified framework as\ncompared to SPLICE for Test C, and 4.96% overall w.r.t. standard MLLR\nadaptation on HMMs.\n", "versions": [{"version": "v1", "created": "Mon, 15 Jul 2013 18:39:10 GMT"}], "update_date": "2014-02-12", "authors_parsed": [["Kumar", "D. S. Pavan", ""], ["Prasad", "N. Vishnu", ""], ["Joshi", "Vikas", ""], ["Umesh", "S.", ""]]}, {"id": "1307.4145", "submitter": "Jie  Wang", "authors": "Jie Wang, Jiayu Zhou, Jun Liu, Peter Wonka, Jieping Ye", "title": "A Safe Screening Rule for Sparse Logistic Regression", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The l1-regularized logistic regression (or sparse logistic regression) is a\nwidely used method for simultaneous classification and feature selection.\nAlthough many recent efforts have been devoted to its efficient implementation,\nits application to high dimensional data still poses significant challenges. In\nthis paper, we present a fast and effective sparse logistic regression\nscreening rule (Slores) to identify the 0 components in the solution vector,\nwhich may lead to a substantial reduction in the number of features to be\nentered to the optimization. An appealing feature of Slores is that the data\nset needs to be scanned only once to run the screening and its computational\ncost is negligible compared to that of solving the sparse logistic regression\nproblem. Moreover, Slores is independent of solvers for sparse logistic\nregression, thus Slores can be integrated with any existing solver to improve\nthe efficiency. We have evaluated Slores using high-dimensional data sets from\ndifferent applications. Extensive experimental results demonstrate that Slores\noutperforms the existing state-of-the-art screening rules and the efficiency of\nsolving sparse logistic regression is improved by one magnitude in general.\n", "versions": [{"version": "v1", "created": "Tue, 16 Jul 2013 02:03:51 GMT"}, {"version": "v2", "created": "Thu, 18 Jul 2013 22:57:16 GMT"}], "update_date": "2013-07-22", "authors_parsed": [["Wang", "Jie", ""], ["Zhou", "Jiayu", ""], ["Liu", "Jun", ""], ["Wonka", "Peter", ""], ["Ye", "Jieping", ""]]}, {"id": "1307.4156", "submitter": "Jie  Wang", "authors": "Jie Wang, Jun Liu, Jieping Ye", "title": "Efficient Mixed-Norm Regularization: Algorithms and Safe Screening\n  Methods", "comments": "arXiv admin note: text overlap with arXiv:1009.4766", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Sparse learning has recently received increasing attention in many areas\nincluding machine learning, statistics, and applied mathematics. The mixed-norm\nregularization based on the l1q norm with q>1 is attractive in many\napplications of regression and classification in that it facilitates group\nsparsity in the model. The resulting optimization problem is, however,\nchallenging to solve due to the inherent structure of the mixed-norm\nregularization. Existing work deals with special cases with q=1, 2, infinity,\nand they cannot be easily extended to the general case. In this paper, we\npropose an efficient algorithm based on the accelerated gradient method for\nsolving the general l1q-regularized problem. One key building block of the\nproposed algorithm is the l1q-regularized Euclidean projection (EP_1q). Our\ntheoretical analysis reveals the key properties of EP_1q and illustrates why\nEP_1q for the general q is significantly more challenging to solve than the\nspecial cases. Based on our theoretical analysis, we develop an efficient\nalgorithm for EP_1q by solving two zero finding problems. To further improve\nthe efficiency of solving large dimensional mixed-norm regularized problems, we\npropose a screening method which is able to quickly identify the inactive\ngroups, i.e., groups that have 0 components in the solution. This may lead to\nsubstantial reduction in the number of groups to be entered to the\noptimization. An appealing feature of our screening method is that the data set\nneeds to be scanned only once to run the screening. Compared to that of solving\nthe mixed-norm regularized problems, the computational cost of our screening\ntest is negligible. The key of the proposed screening method is an accurate\nsensitivity analysis of the dual optimal solution when the regularization\nparameter varies. Experimental results demonstrate the efficiency of the\nproposed algorithm.\n", "versions": [{"version": "v1", "created": "Tue, 16 Jul 2013 03:09:13 GMT"}], "update_date": "2013-07-17", "authors_parsed": [["Wang", "Jie", ""], ["Liu", "Jun", ""], ["Ye", "Jieping", ""]]}, {"id": "1307.4502", "submitter": "Weiyu Xu", "authors": "Weiyu Xu, Myung Cho", "title": "Universally Elevating the Phase Transition Performance of Compressed\n  Sensing: Non-Isometric Matrices are Not Necessarily Bad Matrices", "comments": "6pages, 2 figures. arXiv admin note: substantial text overlap with\n  arXiv:1010.2236, arXiv:1004.0402", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IT math.IT math.OC stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In compressed sensing problems, $\\ell_1$ minimization or Basis Pursuit was\nknown to have the best provable phase transition performance of recoverable\nsparsity among polynomial-time algorithms. It is of great theoretical and\npractical interest to find alternative polynomial-time algorithms which perform\nbetter than $\\ell_1$ minimization. \\cite{Icassp reweighted l_1}, \\cite{Isit\nreweighted l_1}, \\cite{XuScaingLaw} and \\cite{iterativereweightedjournal} have\nshown that a two-stage re-weighted $\\ell_1$ minimization algorithm can boost\nthe phase transition performance for signals whose nonzero elements follow an\namplitude probability density function (pdf) $f(\\cdot)$ whose $t$-th derivative\n$f^{t}(0) \\neq 0$ for some integer $t \\geq 0$. However, for signals whose\nnonzero elements are strictly suspended from zero in distribution (for example,\nconstant-modulus, only taking values `$+d$' or `$-d$' for some nonzero real\nnumber $d$), no polynomial-time signal recovery algorithms were known to\nprovide better phase transition performance than plain $\\ell_1$ minimization,\nespecially for dense sensing matrices. In this paper, we show that a\npolynomial-time algorithm can universally elevate the phase-transition\nperformance of compressed sensing, compared with $\\ell_1$ minimization, even\nfor signals with constant-modulus nonzero elements. Contrary to conventional\nwisdoms that compressed sensing matrices are desired to be isometric, we show\nthat non-isometric matrices are not necessarily bad sensing matrices. In this\npaper, we also provide a framework for recovering sparse signals when sensing\nmatrices are not isometric.\n", "versions": [{"version": "v1", "created": "Wed, 17 Jul 2013 04:59:24 GMT"}], "update_date": "2013-07-19", "authors_parsed": [["Xu", "Weiyu", ""], ["Cho", "Myung", ""]]}, {"id": "1307.4514", "submitter": "Aur\\'elien Bellet", "authors": "Aur\\'elien Bellet", "title": "Supervised Metric Learning with Generalization Guarantees", "comments": "PhD thesis defended on December 11, 2012 (Laboratoire Hubert Curien,\n  University of Saint-Etienne)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The crucial importance of metrics in machine learning algorithms has led to\nan increasing interest in optimizing distance and similarity functions, an area\nof research known as metric learning. When data consist of feature vectors, a\nlarge body of work has focused on learning a Mahalanobis distance. Less work\nhas been devoted to metric learning from structured objects (such as strings or\ntrees), most of it focusing on optimizing a notion of edit distance. We\nidentify two important limitations of current metric learning approaches.\nFirst, they allow to improve the performance of local algorithms such as\nk-nearest neighbors, but metric learning for global algorithms (such as linear\nclassifiers) has not been studied so far. Second, the question of the\ngeneralization ability of metric learning methods has been largely ignored. In\nthis thesis, we propose theoretical and algorithmic contributions that address\nthese limitations. Our first contribution is the derivation of a new kernel\nfunction built from learned edit probabilities. Our second contribution is a\nnovel framework for learning string and tree edit similarities inspired by the\nrecent theory of (e,g,t)-good similarity functions. Using uniform stability\narguments, we establish theoretical guarantees for the learned similarity that\ngive a bound on the generalization error of a linear classifier built from that\nsimilarity. In our third contribution, we extend these ideas to metric learning\nfrom feature vectors by proposing a bilinear similarity learning method that\nefficiently optimizes the (e,g,t)-goodness. Generalization guarantees are\nderived for our approach, highlighting that our method minimizes a tighter\nbound on the generalization error of the classifier. Our last contribution is a\nframework for establishing generalization bounds for a large class of existing\nmetric learning algorithms based on a notion of algorithmic robustness.\n", "versions": [{"version": "v1", "created": "Wed, 17 Jul 2013 06:42:00 GMT"}, {"version": "v2", "created": "Tue, 23 Jul 2013 17:42:26 GMT"}], "update_date": "2019-01-25", "authors_parsed": [["Bellet", "Aur\u00e9lien", ""]]}, {"id": "1307.4564", "submitter": "Nicol\\`o Cesa-Bianchi", "authors": "Noga Alon, Nicol\\`o Cesa-Bianchi, Claudio Gentile, Yishay Mansour", "title": "From Bandits to Experts: A Tale of Domination and Independence", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider the partial observability model for multi-armed bandits,\nintroduced by Mannor and Shamir. Our main result is a characterization of\nregret in the directed observability model in terms of the dominating and\nindependence numbers of the observability graph. We also show that in the\nundirected case, the learner can achieve optimal regret without even accessing\nthe observability graph before selecting an action. Both results are shown\nusing variants of the Exp3 algorithm operating on the observability graph in a\ntime-efficient manner.\n", "versions": [{"version": "v1", "created": "Wed, 17 Jul 2013 10:24:00 GMT"}], "update_date": "2013-07-18", "authors_parsed": [["Alon", "Noga", ""], ["Cesa-Bianchi", "Nicol\u00f2", ""], ["Gentile", "Claudio", ""], ["Mansour", "Yishay", ""]]}, {"id": "1307.4653", "submitter": "Massimiliano Pontil", "authors": "Bernardino Romera-Paredes and Massimiliano Pontil", "title": "A New Convex Relaxation for Tensor Completion", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG math.OC stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study the problem of learning a tensor from a set of linear measurements.\nA prominent methodology for this problem is based on a generalization of trace\nnorm regularization, which has been used extensively for learning low rank\nmatrices, to the tensor setting. In this paper, we highlight some limitations\nof this approach and propose an alternative convex relaxation on the Euclidean\nball. We then describe a technique to solve the associated regularization\nproblem, which builds upon the alternating direction method of multipliers.\nExperiments on one synthetic dataset and two real datasets indicate that the\nproposed method improves significantly over tensor trace norm regularization in\nterms of estimation error, while remaining computationally tractable.\n", "versions": [{"version": "v1", "created": "Wed, 17 Jul 2013 14:38:47 GMT"}], "update_date": "2013-07-18", "authors_parsed": [["Romera-Paredes", "Bernardino", ""], ["Pontil", "Massimiliano", ""]]}, {"id": "1307.4847", "submitter": "Zheng Wen", "authors": "Zheng Wen and Benjamin Van Roy", "title": "Efficient Reinforcement Learning in Deterministic Systems with Value\n  Function Generalization", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI cs.SY stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider the problem of reinforcement learning over episodes of a\nfinite-horizon deterministic system and as a solution propose optimistic\nconstraint propagation (OCP), an algorithm designed to synthesize efficient\nexploration and value function generalization. We establish that when the true\nvalue function lies within a given hypothesis class, OCP selects optimal\nactions over all but at most K episodes, where K is the eluder dimension of the\ngiven hypothesis class. We establish further efficiency and asymptotic\nperformance guarantees that apply even if the true value function does not lie\nin the given hypothesis class, for the special case where the hypothesis class\nis the span of pre-specified indicator functions over disjoint sets. We also\ndiscuss the computational complexity of OCP and present computational results\ninvolving two illustrative examples.\n", "versions": [{"version": "v1", "created": "Thu, 18 Jul 2013 07:22:39 GMT"}, {"version": "v2", "created": "Tue, 1 Oct 2013 23:05:18 GMT"}, {"version": "v3", "created": "Sun, 8 May 2016 20:10:21 GMT"}, {"version": "v4", "created": "Wed, 6 Jul 2016 23:56:50 GMT"}], "update_date": "2016-07-08", "authors_parsed": [["Wen", "Zheng", ""], ["Van Roy", "Benjamin", ""]]}, {"id": "1307.4891", "submitter": "Reinhard Heckel", "authors": "Reinhard Heckel and Helmut B\\\"olcskei", "title": "Robust Subspace Clustering via Thresholding", "comments": "final version, to appear in the IEEE Transactions on Information\n  Theory", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.IT cs.LG math.IT", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The problem of clustering noisy and incompletely observed high-dimensional\ndata points into a union of low-dimensional subspaces and a set of outliers is\nconsidered. The number of subspaces, their dimensions, and their orientations\nare assumed unknown. We propose a simple low-complexity subspace clustering\nalgorithm, which applies spectral clustering to an adjacency matrix obtained by\nthresholding the correlations between data points. In other words, the\nadjacency matrix is constructed from the nearest neighbors of each data point\nin spherical distance. A statistical performance analysis shows that the\nalgorithm exhibits robustness to additive noise and succeeds even when the\nsubspaces intersect. Specifically, our results reveal an explicit tradeoff\nbetween the affinity of the subspaces and the tolerable noise level. We\nfurthermore prove that the algorithm succeeds even when the data points are\nincompletely observed with the number of missing entries allowed to be (up to a\nlog-factor) linear in the ambient dimension. We also propose a simple scheme\nthat provably detects outliers, and we present numerical results on real and\nsynthetic data.\n", "versions": [{"version": "v1", "created": "Thu, 18 Jul 2013 10:08:47 GMT"}, {"version": "v2", "created": "Wed, 9 Apr 2014 13:23:04 GMT"}, {"version": "v3", "created": "Wed, 25 Jun 2014 13:18:37 GMT"}, {"version": "v4", "created": "Fri, 21 Aug 2015 13:53:51 GMT"}], "update_date": "2015-08-24", "authors_parsed": [["Heckel", "Reinhard", ""], ["B\u00f6lcskei", "Helmut", ""]]}, {"id": "1307.5118", "submitter": "Tingting Zhao Tingting Zhao", "authors": "Syogo Mori, Voot Tangkaratt, Tingting Zhao, Jun Morimoto, and Masashi\n  Sugiyama", "title": "Model-Based Policy Gradients with Parameter-Based Exploration by\n  Least-Squares Conditional Density Estimation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The goal of reinforcement learning (RL) is to let an agent learn an optimal\ncontrol policy in an unknown environment so that future expected rewards are\nmaximized. The model-free RL approach directly learns the policy based on data\nsamples. Although using many samples tends to improve the accuracy of policy\nlearning, collecting a large number of samples is often expensive in practice.\nOn the other hand, the model-based RL approach first estimates the transition\nmodel of the environment and then learns the policy based on the estimated\ntransition model. Thus, if the transition model is accurately learned from a\nsmall amount of data, the model-based approach can perform better than the\nmodel-free approach. In this paper, we propose a novel model-based RL method by\ncombining a recently proposed model-free policy search method called policy\ngradients with parameter-based exploration and the state-of-the-art transition\nmodel estimator called least-squares conditional density estimation. Through\nexperiments, we demonstrate the practical usefulness of the proposed method.\n", "versions": [{"version": "v1", "created": "Fri, 19 Jul 2013 03:00:39 GMT"}], "update_date": "2013-07-22", "authors_parsed": [["Mori", "Syogo", ""], ["Tangkaratt", "Voot", ""], ["Zhao", "Tingting", ""], ["Morimoto", "Jun", ""], ["Sugiyama", "Masashi", ""]]}, {"id": "1307.5161", "submitter": "Xavier Boix", "authors": "Gemma Roig, Xavier Boix, Luc Van Gool", "title": "Random Binary Mappings for Kernel Learning and Efficient SVM", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Support Vector Machines (SVMs) are powerful learners that have led to\nstate-of-the-art results in various computer vision problems. SVMs suffer from\nvarious drawbacks in terms of selecting the right kernel, which depends on the\nimage descriptors, as well as computational and memory efficiency. This paper\nintroduces a novel kernel, which serves such issues well. The kernel is learned\nby exploiting a large amount of low-complex, randomized binary mappings of the\ninput feature. This leads to an efficient SVM, while also alleviating the task\nof kernel selection. We demonstrate the capabilities of our kernel on 6\nstandard vision benchmarks, in which we combine several common image\ndescriptors, namely histograms (Flowers17 and Daimler), attribute-like\ndescriptors (UCI, OSR, and a-VOC08), and Sparse Quantization (ImageNet).\nResults show that our kernel learning adapts well to the different descriptors\ntypes, achieving the performance of the kernels specifically tuned for each\nimage descriptor, and with similar evaluation cost as efficient SVM methods.\n", "versions": [{"version": "v1", "created": "Fri, 19 Jul 2013 08:47:32 GMT"}, {"version": "v2", "created": "Fri, 28 Mar 2014 08:49:17 GMT"}], "update_date": "2014-03-31", "authors_parsed": [["Roig", "Gemma", ""], ["Boix", "Xavier", ""], ["Van Gool", "Luc", ""]]}, {"id": "1307.5302", "submitter": "Dino Sejdinovic", "authors": "Dino Sejdinovic, Heiko Strathmann, Maria Lomeli Garcia, Christophe\n  Andrieu, Arthur Gretton", "title": "Kernel Adaptive Metropolis-Hastings", "comments": "Proceedings of the 31st International Conference on Machine Learning,\n  Beijing, China, 2014; JMLR: W&CP volume 32(2)", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A Kernel Adaptive Metropolis-Hastings algorithm is introduced, for the\npurpose of sampling from a target distribution with strongly nonlinear support.\nThe algorithm embeds the trajectory of the Markov chain into a reproducing\nkernel Hilbert space (RKHS), such that the feature space covariance of the\nsamples informs the choice of proposal. The procedure is computationally\nefficient and straightforward to implement, since the RKHS moves can be\nintegrated out analytically: our proposal distribution in the original space is\na normal distribution whose mean and covariance depend on where the current\nsample lies in the support of the target distribution, and adapts to its local\ncovariance structure. Furthermore, the procedure requires neither gradients nor\nany other higher order information about the target, making it particularly\nattractive for contexts such as Pseudo-Marginal MCMC. Kernel Adaptive\nMetropolis-Hastings outperforms competing fixed and adaptive samplers on\nmultivariate, highly nonlinear target distributions, arising in both real-world\nand synthetic examples. Code may be downloaded at\nhttps://github.com/karlnapf/kameleon-mcmc.\n", "versions": [{"version": "v1", "created": "Fri, 19 Jul 2013 18:26:34 GMT"}, {"version": "v2", "created": "Thu, 13 Feb 2014 18:06:06 GMT"}, {"version": "v3", "created": "Thu, 12 Jun 2014 22:30:05 GMT"}], "update_date": "2014-06-16", "authors_parsed": [["Sejdinovic", "Dino", ""], ["Strathmann", "Heiko", ""], ["Garcia", "Maria Lomeli", ""], ["Andrieu", "Christophe", ""], ["Gretton", "Arthur", ""]]}, {"id": "1307.5339", "submitter": "Kean Ming Tan", "authors": "Kean Ming Tan, Daniela Witten, and Ali Shojaie", "title": "The Cluster Graphical Lasso for improved estimation of Gaussian\n  graphical models", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider the task of estimating a Gaussian graphical model in the\nhigh-dimensional setting. The graphical lasso, which involves maximizing the\nGaussian log likelihood subject to an l1 penalty, is a well-studied approach\nfor this task. We begin by introducing a surprising connection between the\ngraphical lasso and hierarchical clustering: the graphical lasso in effect\nperforms a two-step procedure, in which (1) single linkage hierarchical\nclustering is performed on the variables in order to identify connected\ncomponents, and then (2) an l1-penalized log likelihood is maximized on the\nsubset of variables within each connected component. In other words, the\ngraphical lasso determines the connected components of the estimated network\nvia single linkage clustering. Unfortunately, single linkage clustering is\nknown to perform poorly in certain settings. Therefore, we propose the cluster\ngraphical lasso, which involves clustering the features using an alternative to\nsingle linkage clustering, and then performing the graphical lasso on the\nsubset of variables within each cluster. We establish model selection\nconsistency for this technique, and demonstrate its improved performance\nrelative to the graphical lasso in a simulation study, as well as in\napplications to an equities data set, a university webpage data set, and a gene\nexpression data set.\n", "versions": [{"version": "v1", "created": "Fri, 19 Jul 2013 21:19:11 GMT"}], "update_date": "2013-07-23", "authors_parsed": [["Tan", "Kean Ming", ""], ["Witten", "Daniela", ""], ["Shojaie", "Ali", ""]]}, {"id": "1307.5381", "submitter": "Sang-Yun Oh", "authors": "Kshitij Khare, Sang-Yun Oh, Bala Rajaratnam", "title": "A convex pseudo-likelihood framework for high dimensional partial\n  correlation estimation with convergence guarantees", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.CO stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Sparse high dimensional graphical model selection is a topic of much interest\nin modern day statistics. A popular approach is to apply l1-penalties to either\n(1) parametric likelihoods, or, (2) regularized regression/pseudo-likelihoods,\nwith the latter having the distinct advantage that they do not explicitly\nassume Gaussianity. As none of the popular methods proposed for solving\npseudo-likelihood based objective functions have provable convergence\nguarantees, it is not clear if corresponding estimators exist or are even\ncomputable, or if they actually yield correct partial correlation graphs. This\npaper proposes a new pseudo-likelihood based graphical model selection method\nthat aims to overcome some of the shortcomings of current methods, but at the\nsame time retain all their respective strengths. In particular, we introduce a\nnovel framework that leads to a convex formulation of the partial covariance\nregression graph problem, resulting in an objective function comprised of\nquadratic forms. The objective is then optimized via a coordinate-wise\napproach. The specific functional form of the objective function facilitates\nrigorous convergence analysis leading to convergence guarantees; an important\nproperty that cannot be established using standard results, when the dimension\nis larger than the sample size, as is often the case in high dimensional\napplications. These convergence guarantees ensure that estimators are\nwell-defined under very general conditions, and are always computable. In\naddition, the approach yields estimators that have good large sample properties\nand also respect symmetry. Furthermore, application to simulated/real data,\ntiming comparisons and numerical convergence is demonstrated. We also present a\nnovel unifying framework that places all graphical pseudo-likelihood methods as\nspecial cases of a more general formulation, leading to important insights.\n", "versions": [{"version": "v1", "created": "Sat, 20 Jul 2013 07:01:20 GMT"}, {"version": "v2", "created": "Wed, 26 Mar 2014 05:27:44 GMT"}, {"version": "v3", "created": "Thu, 14 Aug 2014 21:15:28 GMT"}], "update_date": "2014-08-18", "authors_parsed": [["Khare", "Kshitij", ""], ["Oh", "Sang-Yun", ""], ["Rajaratnam", "Bala", ""]]}, {"id": "1307.5449", "submitter": "Yonatan Gur", "authors": "O. Besbes, Y. Gur, and A. Zeevi", "title": "Non-stationary Stochastic Optimization", "comments": null, "journal-ref": null, "doi": "10.1287/opre.2015.1408", "report-no": null, "categories": "math.PR cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider a non-stationary variant of a sequential stochastic optimization\nproblem, in which the underlying cost functions may change along the horizon.\nWe propose a measure, termed variation budget, that controls the extent of said\nchange, and study how restrictions on this budget impact achievable\nperformance. We identify sharp conditions under which it is possible to achieve\nlong-run-average optimality and more refined performance measures such as rate\noptimality that fully characterize the complexity of such problems. In doing\nso, we also establish a strong connection between two rather disparate strands\nof literature: adversarial online convex optimization; and the more traditional\nstochastic approximation paradigm (couched in a non-stationary setting). This\nconnection is the key to deriving well performing policies in the latter, by\nleveraging structure of optimal policies in the former. Finally, tight bounds\non the minimax regret allow us to quantify the \"price of non-stationarity,\"\nwhich mathematically captures the added complexity embedded in a temporally\nchanging environment versus a stationary one.\n", "versions": [{"version": "v1", "created": "Sat, 20 Jul 2013 18:46:01 GMT"}, {"version": "v2", "created": "Mon, 22 Dec 2014 22:45:18 GMT"}], "update_date": "2019-06-07", "authors_parsed": [["Besbes", "O.", ""], ["Gur", "Y.", ""], ["Zeevi", "A.", ""]]}, {"id": "1307.5494", "submitter": "Laura Balzano", "authors": "Laura Balzano and Stephen J. Wright", "title": "On GROUSE and Incremental SVD", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NA cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  GROUSE (Grassmannian Rank-One Update Subspace Estimation) is an incremental\nalgorithm for identifying a subspace of Rn from a sequence of vectors in this\nsubspace, where only a subset of components of each vector is revealed at each\niteration. Recent analysis has shown that GROUSE converges locally at an\nexpected linear rate, under certain assumptions. GROUSE has a similar flavor to\nthe incremental singular value decomposition algorithm, which updates the SVD\nof a matrix following addition of a single column. In this paper, we modify the\nincremental SVD approach to handle missing data, and demonstrate that this\nmodified approach is equivalent to GROUSE, for a certain choice of an\nalgorithmic parameter.\n", "versions": [{"version": "v1", "created": "Sun, 21 Jul 2013 03:47:16 GMT"}], "update_date": "2013-07-23", "authors_parsed": [["Balzano", "Laura", ""], ["Wright", "Stephen J.", ""]]}, {"id": "1307.5558", "submitter": "Paul McNicholas", "authors": "Paula M. Murray, Paul D. McNicholas and Ryan P. Browne", "title": "Mixtures of Common Skew-t Factor Analyzers", "comments": null, "journal-ref": null, "doi": "10.1002/sta4.43", "report-no": null, "categories": "stat.ME stat.AP stat.CO stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A mixture of common skew-t factor analyzers model is introduced for\nmodel-based clustering of high-dimensional data. By assuming common component\nfactor loadings, this model allows clustering to be performed in the presence\nof a large number of mixture components or when the number of dimensions is too\nlarge to be well-modelled by the mixtures of factor analyzers model or a\nvariant thereof. Furthermore, assuming that the component densities follow a\nskew-t distribution allows robust clustering of skewed data. The alternating\nexpectation-conditional maximization algorithm is employed for parameter\nestimation. We demonstrate excellent clustering performance when our model is\napplied to real and simulated data.This paper marks the first time that skewed\ncommon factors have been used.\n", "versions": [{"version": "v1", "created": "Sun, 21 Jul 2013 19:18:39 GMT"}, {"version": "v2", "created": "Mon, 26 Aug 2013 15:56:34 GMT"}, {"version": "v3", "created": "Fri, 30 Aug 2013 21:28:57 GMT"}], "update_date": "2014-05-05", "authors_parsed": [["Murray", "Paula M.", ""], ["McNicholas", "Paul D.", ""], ["Browne", "Ryan P.", ""]]}, {"id": "1307.5599", "submitter": "Naresh Kumar Mallenahalli Prof. Dr.", "authors": "M. Naresh Kumar", "title": "Performance comparison of State-of-the-art Missing Value Imputation\n  Algorithms on Some Bench mark Datasets", "comments": "17 pages, 6 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Decision making from data involves identifying a set of attributes that\ncontribute to effective decision making through computational intelligence. The\npresence of missing values greatly influences the selection of right set of\nattributes and this renders degradation in classification accuracies of the\nclassifiers. As missing values are quite common in data collection phase during\nfield experiments or clinical trails appropriate handling would improve the\nclassifier performance. In this paper we present a review of recently developed\nmissing value imputation algorithms and compare their performance on some bench\nmark datasets.\n", "versions": [{"version": "v1", "created": "Mon, 22 Jul 2013 06:50:21 GMT"}], "update_date": "2013-07-23", "authors_parsed": [["Kumar", "M. Naresh", ""]]}, {"id": "1307.5601", "submitter": "Zhihua Zhang", "authors": "Zhihua Zhang, Shibo Zhao, Zebang Shen, and Shuchang Zhou", "title": "Kinetic Energy Plus Penalty Functions for Sparse Estimation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper we propose and study a family of sparsity-inducing penalty\nfunctions. Since the penalty functions are related to the kinetic energy in\nspecial relativity, we call them \\emph{kinetic energy plus} (KEP) functions. We\nconstruct the KEP function by using the concave conjugate of a\n$\\chi^2$-distance function and present several novel insights into the KEP\nfunction with $q=1$. In particular, we derive a thresholding operator based on\nthe KEP function, and prove its mathematical properties and asymptotic\nproperties in sparsity modeling. Moreover, we show that a coordinate descent\nalgorithm is especially appropriate for the KEP function. Additionally, we\ndiscuss the relationship of KEP with the penalty functions $\\ell_{1/2}$ and\nMCP. The theoretical and empirical analysis validates that the KEP function is\neffective and efficient in high-dimensional data modeling.\n", "versions": [{"version": "v1", "created": "Mon, 22 Jul 2013 06:55:13 GMT"}, {"version": "v2", "created": "Mon, 21 Oct 2013 13:36:42 GMT"}, {"version": "v3", "created": "Sun, 6 Jul 2014 05:09:35 GMT"}], "update_date": "2014-07-08", "authors_parsed": [["Zhang", "Zhihua", ""], ["Zhao", "Shibo", ""], ["Shen", "Zebang", ""], ["Zhou", "Shuchang", ""]]}, {"id": "1307.5870", "submitter": "Cun Mu", "authors": "Cun Mu, Bo Huang, John Wright, Donald Goldfarb", "title": "Square Deal: Lower Bounds and Improved Relaxations for Tensor Recovery", "comments": "Slight modifications are made in this second version (mainly, Lemma\n  5)", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recovering a low-rank tensor from incomplete information is a recurring\nproblem in signal processing and machine learning. The most popular convex\nrelaxation of this problem minimizes the sum of the nuclear norms of the\nunfoldings of the tensor. We show that this approach can be substantially\nsuboptimal: reliably recovering a $K$-way tensor of length $n$ and Tucker rank\n$r$ from Gaussian measurements requires $\\Omega(r n^{K-1})$ observations. In\ncontrast, a certain (intractable) nonconvex formulation needs only $O(r^K +\nnrK)$ observations. We introduce a very simple, new convex relaxation, which\npartially bridges this gap. Our new formulation succeeds with $O(r^{\\lfloor K/2\n\\rfloor}n^{\\lceil K/2 \\rceil})$ observations. While these results pertain to\nGaussian measurements, simulations strongly suggest that the new norm also\noutperforms the sum of nuclear norms for tensor completion from a random subset\nof entries.\n  Our lower bound for the sum-of-nuclear-norms model follows from a new result\non recovering signals with multiple sparse structures (e.g. sparse, low rank),\nwhich perhaps surprisingly demonstrates the significant suboptimality of the\ncommonly used recovery approach via minimizing the sum of individual sparsity\ninducing norms (e.g. $l_1$, nuclear norm). Our new formulation for low-rank\ntensor recovery however opens the possibility in reducing the sample complexity\nby exploiting several structures jointly.\n", "versions": [{"version": "v1", "created": "Mon, 22 Jul 2013 20:23:29 GMT"}, {"version": "v2", "created": "Thu, 15 Aug 2013 05:59:52 GMT"}], "update_date": "2013-08-16", "authors_parsed": [["Mu", "Cun", ""], ["Huang", "Bo", ""], ["Wright", "John", ""], ["Goldfarb", "Donald", ""]]}, {"id": "1307.5944", "submitter": "Eric Hall", "authors": "Eric C. Hall and Rebecca M. Willett", "title": "Online Optimization in Dynamic Environments", "comments": "arXiv admin note: text overlap with arXiv:1301.1254", "journal-ref": "IEEE Journal of Selected Topics in Signal Processing - Signal\n  Processing for Big Data, vol. 9, no 4. 2015", "doi": null, "report-no": null, "categories": "stat.ML cs.LG math.OC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  High-velocity streams of high-dimensional data pose significant \"big data\"\nanalysis challenges across a range of applications and settings. Online\nlearning and online convex programming play a significant role in the rapid\nrecovery of important or anomalous information from these large datastreams.\nWhile recent advances in online learning have led to novel and rapidly\nconverging algorithms, these methods are unable to adapt to nonstationary\nenvironments arising in real-world problems. This paper describes a dynamic\nmirror descent framework which addresses this challenge, yielding low\ntheoretical regret bounds and accurate, adaptive, and computationally efficient\nalgorithms which are applicable to broad classes of problems. The methods are\ncapable of learning and adapting to an underlying and possibly time-varying\ndynamical model. Empirical results in the context of dynamic texture analysis,\nsolar flare detection, sequential compressed sensing of a dynamic scene,\ntraffic surveillance,tracking self-exciting point processes and network\nbehavior in the Enron email corpus support the core theoretical findings.\n", "versions": [{"version": "v1", "created": "Tue, 23 Jul 2013 04:13:44 GMT"}, {"version": "v2", "created": "Thu, 3 Jul 2014 21:21:17 GMT"}, {"version": "v3", "created": "Tue, 19 Jan 2016 17:14:35 GMT"}], "update_date": "2016-01-20", "authors_parsed": [["Hall", "Eric C.", ""], ["Willett", "Rebecca M.", ""]]}, {"id": "1307.6134", "submitter": "Paul Reverdy", "authors": "Paul Reverdy, Vaibhav Srivastava, Naomi E. Leonard", "title": "Modeling Human Decision-making in Generalized Gaussian Multi-armed\n  Bandits", "comments": "25 pages. Appendix G included in this version details minor\n  modifications that correct for an oversight in the previously-published\n  proofs. The remainder of the text reflects the previously-published version", "journal-ref": "Proceedings of the IEEE, vol. 102, iss. 4, p. 544-571, 2014", "doi": null, "report-no": null, "categories": "cs.LG math.OC stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a formal model of human decision-making in explore-exploit tasks\nusing the context of multi-armed bandit problems, where the decision-maker must\nchoose among multiple options with uncertain rewards. We address the standard\nmulti-armed bandit problem, the multi-armed bandit problem with transition\ncosts, and the multi-armed bandit problem on graphs. We focus on the case of\nGaussian rewards in a setting where the decision-maker uses Bayesian inference\nto estimate the reward values. We model the decision-maker's prior knowledge\nwith the Bayesian prior on the mean reward. We develop the upper credible limit\n(UCL) algorithm for the standard multi-armed bandit problem and show that this\ndeterministic algorithm achieves logarithmic cumulative expected regret, which\nis optimal performance for uninformative priors. We show how good priors and\ngood assumptions on the correlation structure among arms can greatly enhance\ndecision-making performance, even over short time horizons. We extend to the\nstochastic UCL algorithm and draw several connections to human decision-making\nbehavior. We present empirical data from human experiments and show that human\nperformance is efficiently captured by the stochastic UCL algorithm with\nappropriate parameters. For the multi-armed bandit problem with transition\ncosts and the multi-armed bandit problem on graphs, we generalize the UCL\nalgorithm to the block UCL algorithm and the graphical block UCL algorithm,\nrespectively. We show that these algorithms also achieve logarithmic cumulative\nexpected regret and require a sub-logarithmic expected number of transitions\namong arms. We further illustrate the performance of these algorithms with\nnumerical examples. NB: Appendix G included in this version details minor\nmodifications that correct for an oversight in the previously-published proofs.\nThe remainder of the text reflects the published work.\n", "versions": [{"version": "v1", "created": "Tue, 23 Jul 2013 16:05:13 GMT"}, {"version": "v2", "created": "Thu, 13 Feb 2014 17:28:13 GMT"}, {"version": "v3", "created": "Fri, 14 Feb 2014 19:10:11 GMT"}, {"version": "v4", "created": "Fri, 19 Apr 2019 16:14:11 GMT"}, {"version": "v5", "created": "Fri, 20 Dec 2019 15:27:31 GMT"}], "update_date": "2019-12-23", "authors_parsed": [["Reverdy", "Paul", ""], ["Srivastava", "Vaibhav", ""], ["Leonard", "Naomi E.", ""]]}, {"id": "1307.6143", "submitter": "Niko Br\\\"ummer", "authors": "Niko Brummer", "title": "Generative, Fully Bayesian, Gaussian, Openset Pattern Classifier", "comments": "Research Report, BOSARIS 2012 Speaker Recognition Workshop", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This report works out the details of a closed-form, fully Bayesian,\nmulticlass, openset, generative pattern classifier using multivariate Gaussian\nlikelihoods, with conjugate priors. The generative model has a common\nwithin-class covariance, which is proportional to the between-class covariance\nin the conjugate prior. The scalar proportionality constant is the only plugin\nparameter. All other model parameters are intergated out in closed form. An\nexpression is given for the model evidence, which can be used to make plugin\nestimates for the proportionality constant. Pattern recognition is done via the\npredictive likeihoods of classes for which training data is available, as well\nas a predicitve likelihood for any as yet unseen class.\n", "versions": [{"version": "v1", "created": "Tue, 23 Jul 2013 16:33:00 GMT"}, {"version": "v2", "created": "Wed, 24 Jul 2013 13:26:08 GMT"}], "update_date": "2013-07-25", "authors_parsed": [["Brummer", "Niko", ""]]}, {"id": "1307.6417", "submitter": "Andreas Mayr", "authors": "Andreas Mayr and Matthias Schmid", "title": "Boosting the concordance index for survival data - a unified framework\n  to derive and evaluate biomarker combinations", "comments": "revised manuscript - added simulation study, additional results", "journal-ref": "PloS ONE 2014, 9(1): e84483", "doi": "10.1371/journal.pone.0084483", "report-no": null, "categories": "stat.AP stat.ME stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The development of molecular signatures for the prediction of time-to-event\noutcomes is a methodologically challenging task in bioinformatics and\nbiostatistics. Although there are numerous approaches for the derivation of\nmarker combinations and their evaluation, the underlying methodology often\nsuffers from the problem that different optimization criteria are mixed during\nthe feature selection, estimation and evaluation steps. This might result in\nmarker combinations that are only suboptimal regarding the evaluation criterion\nof interest. To address this issue, we propose a unified framework to derive\nand evaluate biomarker combinations. Our approach is based on the concordance\nindex for time-to-event data, which is a non-parametric measure to quantify the\ndiscrimatory power of a prediction rule. Specifically, we propose a\ncomponent-wise boosting algorithm that results in linear biomarker combinations\nthat are optimal with respect to a smoothed version of the concordance index.\nWe investigate the performance of our algorithm in a large-scale simulation\nstudy and in two molecular data sets for the prediction of survival in breast\ncancer patients. Our numerical results show that the new approach is not only\nmethodologically sound but can also lead to a higher discriminatory power than\ntraditional approaches for the derivation of gene signatures.\n", "versions": [{"version": "v1", "created": "Wed, 24 Jul 2013 13:51:16 GMT"}, {"version": "v2", "created": "Fri, 25 Oct 2013 14:14:42 GMT"}], "update_date": "2014-01-09", "authors_parsed": [["Mayr", "Andreas", ""], ["Schmid", "Matthias", ""]]}, {"id": "1307.6515", "submitter": "Sivaraman Balakrishnan", "authors": "Sivaraman Balakrishnan, Srivatsan Narayanan, Alessandro Rinaldo, Aarti\n  Singh, Larry Wasserman", "title": "Cluster Trees on Manifolds", "comments": "28 pages, 3 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper we investigate the problem of estimating the cluster tree for a\ndensity $f$ supported on or near a smooth $d$-dimensional manifold $M$\nisometrically embedded in $\\mathbb{R}^D$. We analyze a modified version of a\n$k$-nearest neighbor based algorithm recently proposed by Chaudhuri and\nDasgupta. The main results of this paper show that under mild assumptions on\n$f$ and $M$, we obtain rates of convergence that depend on $d$ only but not on\nthe ambient dimension $D$. We also show that similar (albeit non-algorithmic)\nresults can be obtained for kernel density estimators. We sketch a construction\nof a sample complexity lower bound instance for a natural class of manifold\noblivious clustering algorithms. We further briefly consider the known manifold\ncase and show that in this case a spatially adaptive algorithm achieves better\nrates.\n", "versions": [{"version": "v1", "created": "Wed, 24 Jul 2013 18:17:53 GMT"}], "update_date": "2013-07-25", "authors_parsed": [["Balakrishnan", "Sivaraman", ""], ["Narayanan", "Srivatsan", ""], ["Rinaldo", "Alessandro", ""], ["Singh", "Aarti", ""], ["Wasserman", "Larry", ""]]}, {"id": "1307.6522", "submitter": "Mu Zhu", "authors": "Mu Zhu", "title": "When is the majority-vote classifier beneficial?", "comments": "Submitted to \"The American Statistician\", January 2013; revised, July\n  2013", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.ML stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In his seminal work, Schapire (1990) proved that weak classifiers could be\nimproved to achieve arbitrarily high accuracy, but he never implied that a\nsimple majority-vote mechanism could always do the trick. By comparing the\nasymptotic misclassification error of the majority-vote classifier with the\naverage individual error, we discover an interesting phase-transition\nphenomenon. For binary classification with equal prior probabilities, our\nresult implies that, for the majority-vote mechanism to work, the collection of\nweak classifiers must meet the minimum requirement of having an average true\npositive rate of at least 50% and an average false positive rate of at most\n50%.\n", "versions": [{"version": "v1", "created": "Wed, 24 Jul 2013 18:33:51 GMT"}], "update_date": "2013-07-25", "authors_parsed": [["Zhu", "Mu", ""]]}, {"id": "1307.6616", "submitter": "Shaobo Lin", "authors": "Shaobo Lin, Chen Xu, Jingshan Zeng, Jian Fang", "title": "Does generalization performance of $l^q$ regularization learning depend\n  on $q$? A negative example", "comments": "35 pages, 3 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://creativecommons.org/licenses/by/3.0/", "abstract": "  $l^q$-regularization has been demonstrated to be an attractive technique in\nmachine learning and statistical modeling. It attempts to improve the\ngeneralization (prediction) capability of a machine (model) through\nappropriately shrinking its coefficients. The shape of a $l^q$ estimator\ndiffers in varying choices of the regularization order $q$. In particular,\n$l^1$ leads to the LASSO estimate, while $l^{2}$ corresponds to the smooth\nridge regression. This makes the order $q$ a potential tuning parameter in\napplications. To facilitate the use of $l^{q}$-regularization, we intend to\nseek for a modeling strategy where an elaborative selection on $q$ is\navoidable. In this spirit, we place our investigation within a general\nframework of $l^{q}$-regularized kernel learning under a sample dependent\nhypothesis space (SDHS). For a designated class of kernel functions, we show\nthat all $l^{q}$ estimators for $0< q < \\infty$ attain similar generalization\nerror bounds. These estimated bounds are almost optimal in the sense that up to\na logarithmic factor, the upper and lower bounds are asymptotically identical.\nThis finding tentatively reveals that, in some modeling contexts, the choice of\n$q$ might not have a strong impact in terms of the generalization capability.\nFrom this perspective, $q$ can be arbitrarily specified, or specified merely by\nother no generalization criteria like smoothness, computational complexity,\nsparsity, etc..\n", "versions": [{"version": "v1", "created": "Thu, 25 Jul 2013 00:48:04 GMT"}], "update_date": "2013-07-26", "authors_parsed": [["Lin", "Shaobo", ""], ["Xu", "Chen", ""], ["Zeng", "Jingshan", ""], ["Fang", "Jian", ""]]}, {"id": "1307.6769", "submitter": "Tamara Broderick", "authors": "Tamara Broderick, Nicholas Boyd, Andre Wibisono, Ashia C. Wilson,\n  Michael I. Jordan", "title": "Streaming Variational Bayes", "comments": "25 pages, 3 figures, 1 table", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present SDA-Bayes, a framework for (S)treaming, (D)istributed,\n(A)synchronous computation of a Bayesian posterior. The framework makes\nstreaming updates to the estimated posterior according to a user-specified\napproximation batch primitive. We demonstrate the usefulness of our framework,\nwith variational Bayes (VB) as the primitive, by fitting the latent Dirichlet\nallocation model to two large-scale document collections. We demonstrate the\nadvantages of our algorithm over stochastic variational inference (SVI) by\ncomparing the two after a single pass through a known amount of data---a case\nwhere SVI may be applied---and in the streaming setting, where SVI does not\napply.\n", "versions": [{"version": "v1", "created": "Thu, 25 Jul 2013 15:03:40 GMT"}, {"version": "v2", "created": "Wed, 20 Nov 2013 23:29:01 GMT"}], "update_date": "2013-11-22", "authors_parsed": [["Broderick", "Tamara", ""], ["Boyd", "Nicholas", ""], ["Wibisono", "Andre", ""], ["Wilson", "Ashia C.", ""], ["Jordan", "Michael I.", ""]]}, {"id": "1307.6887", "submitter": "Mohammad Gheshlaghi Azar", "authors": "Mohammad Gheshlaghi Azar and Alessandro Lazaric and Emma Brunskill", "title": "Sequential Transfer in Multi-armed Bandit with Finite Set of Models", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Learning from prior tasks and transferring that experience to improve future\nperformance is critical for building lifelong learning agents. Although results\nin supervised and reinforcement learning show that transfer may significantly\nimprove the learning performance, most of the literature on transfer is focused\non batch learning tasks. In this paper we study the problem of\n\\textit{sequential transfer in online learning}, notably in the multi-armed\nbandit framework, where the objective is to minimize the cumulative regret over\na sequence of tasks by incrementally transferring knowledge from prior tasks.\nWe introduce a novel bandit algorithm based on a method-of-moments approach for\nthe estimation of the possible tasks and derive regret bounds for it.\n", "versions": [{"version": "v1", "created": "Thu, 25 Jul 2013 22:17:12 GMT"}], "update_date": "2013-07-29", "authors_parsed": [["Azar", "Mohammad Gheshlaghi", ""], ["Lazaric", "Alessandro", ""], ["Brunskill", "Emma", ""]]}, {"id": "1307.7024", "submitter": "Shiliang Sun", "authors": "Shiliang Sun", "title": "Multi-view Laplacian Support Vector Machines", "comments": "Lecture Notes in Computer Science, 2011, 7121: 209-222", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a new approach, multi-view Laplacian support vector machines\n(SVMs), for semi-supervised learning under the multi-view scenario. It\nintegrates manifold regularization and multi-view regularization into the usual\nformulation of SVMs and is a natural extension of SVMs from supervised learning\nto multi-view semi-supervised learning. The function optimization problem in a\nreproducing kernel Hilbert space is converted to an optimization in a\nfinite-dimensional Euclidean space. After providing a theoretical bound for the\ngeneralization performance of the proposed method, we further give a\nformulation of the empirical Rademacher complexity which affects the bound\nsignificantly. From this bound and the empirical Rademacher complexity, we can\ngain insights into the roles played by different regularization terms to the\ngeneralization performance. Experimental results on synthetic and real-world\ndata sets are presented, which validate the effectiveness of the proposed\nmulti-view Laplacian SVMs approach.\n", "versions": [{"version": "v1", "created": "Fri, 26 Jul 2013 13:02:14 GMT"}], "update_date": "2013-07-29", "authors_parsed": [["Sun", "Shiliang", ""]]}, {"id": "1307.7028", "submitter": "Shiliang Sun", "authors": "Shiliang Sun", "title": "Infinite Mixtures of Multivariate Gaussian Processes", "comments": "Proceedings of the International Conference on Machine Learning and\n  Cybernetics, 2013, pages 1011-1016", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper presents a new model called infinite mixtures of multivariate\nGaussian processes, which can be used to learn vector-valued functions and\napplied to multitask learning. As an extension of the single multivariate\nGaussian process, the mixture model has the advantages of modeling multimodal\ndata and alleviating the computationally cubic complexity of the multivariate\nGaussian process. A Dirichlet process prior is adopted to allow the (possibly\ninfinite) number of mixture components to be automatically inferred from\ntraining data, and Markov chain Monte Carlo sampling techniques are used for\nparameter and latent variable inference. Preliminary experimental results on\nmultivariate regression show the feasibility of the proposed model.\n", "versions": [{"version": "v1", "created": "Fri, 26 Jul 2013 13:24:31 GMT"}], "update_date": "2013-07-29", "authors_parsed": [["Sun", "Shiliang", ""]]}, {"id": "1307.7306", "submitter": "Kristjan Greenewald", "authors": "Kristjan Greenewald, Theodoros Tsiligkaridis, Alfred O Hero III", "title": "Kronecker Sum Decompositions of Space-Time Data", "comments": "5 pages, 8 figures, accepted to CAMSAP 2013", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper we consider the use of the space vs. time Kronecker product\ndecomposition in the estimation of covariance matrices for spatio-temporal\ndata. This decomposition imposes lower dimensional structure on the estimated\ncovariance matrix, thus reducing the number of samples required for estimation.\nTo allow a smooth tradeoff between the reduction in the number of parameters\n(to reduce estimation variance) and the accuracy of the covariance\napproximation (affecting estimation bias), we introduce a diagonally loaded\nmodification of the sum of kronecker products representation [1]. We derive a\nCramer-Rao bound (CRB) on the minimum attainable mean squared predictor\ncoefficient estimation error for unbiased estimators of Kronecker structured\ncovariance matrices. We illustrate the accuracy of the diagonally loaded\nKronecker sum decomposition by applying it to video data of human activity.\n", "versions": [{"version": "v1", "created": "Sat, 27 Jul 2013 20:43:21 GMT"}, {"version": "v2", "created": "Thu, 10 Oct 2013 14:20:39 GMT"}], "update_date": "2013-10-11", "authors_parsed": [["Greenewald", "Kristjan", ""], ["Tsiligkaridis", "Theodoros", ""], ["Hero", "Alfred O", "III"]]}, {"id": "1307.7577", "submitter": "Jun Liu", "authors": "Jun Liu, Zheng Zhao, Jie Wang, Jieping Ye", "title": "Safe Screening With Variational Inequalities and Its Application to\n  LASSO", "comments": "Accepted by International Conference on Machine Learning 2014", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Sparse learning techniques have been routinely used for feature selection as\nthe resulting model usually has a small number of non-zero entries. Safe\nscreening, which eliminates the features that are guaranteed to have zero\ncoefficients for a certain value of the regularization parameter, is a\ntechnique for improving the computational efficiency. Safe screening is gaining\nincreasing attention since 1) solving sparse learning formulations usually has\na high computational cost especially when the number of features is large and\n2) one needs to try several regularization parameters to select a suitable\nmodel. In this paper, we propose an approach called \"Sasvi\" (Safe screening\nwith variational inequalities). Sasvi makes use of the variational inequality\nthat provides the sufficient and necessary optimality condition for the dual\nproblem. Several existing approaches for Lasso screening can be casted as\nrelaxed versions of the proposed Sasvi, thus Sasvi provides a stronger safe\nscreening rule. We further study the monotone properties of Sasvi for Lasso,\nbased on which a sure removal regularization parameter can be identified for\neach feature. Experimental results on both synthetic and real data sets are\nreported to demonstrate the effectiveness of the proposed Sasvi for Lasso\nscreening.\n", "versions": [{"version": "v1", "created": "Mon, 29 Jul 2013 13:45:58 GMT"}, {"version": "v2", "created": "Wed, 30 Oct 2013 15:00:04 GMT"}, {"version": "v3", "created": "Mon, 12 May 2014 19:46:39 GMT"}], "update_date": "2014-05-13", "authors_parsed": [["Liu", "Jun", ""], ["Zhao", "Zheng", ""], ["Wang", "Jie", ""], ["Ye", "Jieping", ""]]}, {"id": "1307.7666", "submitter": "Sivaraman Balakrishnan", "authors": "Sivaraman Balakrishnan, Alessandro Rinaldo, Aarti Singh and Larry\n  Wasserman", "title": "Tight Lower Bounds for Homology Inference", "comments": "5 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.CG math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The homology groups of a manifold are important topological invariants that\nprovide an algebraic summary of the manifold. These groups contain rich\ntopological information, for instance, about the connected components, holes,\ntunnels and sometimes the dimension of the manifold. In earlier work, we have\nconsidered the statistical problem of estimating the homology of a manifold\nfrom noiseless samples and from noisy samples under several different noise\nmodels. We derived upper and lower bounds on the minimax risk for this problem.\nIn this note we revisit the noiseless case. In previous work we used Le Cam's\nlemma to establish a lower bound that differed from the upper bound of Niyogi,\nSmale and Weinberger by a polynomial factor in the condition number.\n  In this note we use a different construction based on the direct analysis of\nthe likelihood ratio test to show that the upper bound of Niyogi, Smale and\nWeinberger is in fact tight, thus establishing rate optimal asymptotic minimax\nbounds for the problem. The techniques we use here extend in a straightforward\nway to the noisy settings considered in our earlier work.\n", "versions": [{"version": "v1", "created": "Mon, 29 Jul 2013 17:57:33 GMT"}], "update_date": "2013-07-30", "authors_parsed": [["Balakrishnan", "Sivaraman", ""], ["Rinaldo", "Alessandro", ""], ["Singh", "Aarti", ""], ["Wasserman", "Larry", ""]]}, {"id": "1307.7852", "submitter": "Jingdong Wang", "authors": "Jingdong Wang, Jing Wang, Gang Zeng, Zhuowen Tu, Rui Gan, and Shipeng\n  Li", "title": "Scalable $k$-NN graph construction", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG stat.ML", "license": "http://creativecommons.org/licenses/by-nc-sa/3.0/", "abstract": "  The $k$-NN graph has played a central role in increasingly popular\ndata-driven techniques for various learning and vision tasks; yet, finding an\nefficient and effective way to construct $k$-NN graphs remains a challenge,\nespecially for large-scale high-dimensional data. In this paper, we propose a\nnew approach to construct approximate $k$-NN graphs with emphasis in:\nefficiency and accuracy. We hierarchically and randomly divide the data points\ninto subsets and build an exact neighborhood graph over each subset, achieving\na base approximate neighborhood graph; we then repeat this process for several\ntimes to generate multiple neighborhood graphs, which are combined to yield a\nmore accurate approximate neighborhood graph. Furthermore, we propose a\nneighborhood propagation scheme to further enhance the accuracy. We show both\ntheoretical and empirical accuracy and efficiency of our approach to $k$-NN\ngraph construction and demonstrate significant speed-up in dealing with large\nscale visual data.\n", "versions": [{"version": "v1", "created": "Tue, 30 Jul 2013 07:33:31 GMT"}], "update_date": "2013-07-31", "authors_parsed": [["Wang", "Jingdong", ""], ["Wang", "Jing", ""], ["Zeng", "Gang", ""], ["Tu", "Zhuowen", ""], ["Gan", "Rui", ""], ["Li", "Shipeng", ""]]}, {"id": "1307.7981", "submitter": "Niko Br\\\"ummer", "authors": "Niko Br\\\"ummer and George Doddington", "title": "Likelihood-ratio calibration using prior-weighted proper scoring rules", "comments": "Accepted, Interspeech 2013", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Prior-weighted logistic regression has become a standard tool for calibration\nin speaker recognition. Logistic regression is the optimization of the expected\nvalue of the logarithmic scoring rule. We generalize this via a parametric\nfamily of proper scoring rules. Our theoretical analysis shows how different\nmembers of this family induce different relative weightings over a spectrum of\napplications of which the decision thresholds range from low to high. Special\nattention is given to the interaction between prior weighting and proper\nscoring rule parameters. Experiments on NIST SRE'12 suggest that for\napplications with low false-alarm rate requirements, scoring rules tailored to\nemphasize higher score thresholds may give better accuracy than logistic\nregression.\n", "versions": [{"version": "v1", "created": "Tue, 30 Jul 2013 13:59:13 GMT"}], "update_date": "2013-07-31", "authors_parsed": [["Br\u00fcmmer", "Niko", ""], ["Doddington", "George", ""]]}, {"id": "1307.7993", "submitter": "Weiguang Wang", "authors": "Weiguang Wang, Yingbin Liang, Eric P. Xing", "title": "Sharp Threshold for Multivariate Multi-Response Linear Regression via\n  Block Regularized Lasso", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we investigate a multivariate multi-response (MVMR) linear\nregression problem, which contains multiple linear regression models with\ndifferently distributed design matrices, and different regression and output\nvectors. The goal is to recover the support union of all regression vectors\nusing $l_1/l_2$-regularized Lasso. We characterize sufficient and necessary\nconditions on sample complexity \\emph{as a sharp threshold} to guarantee\nsuccessful recovery of the support union. Namely, if the sample size is above\nthe threshold, then $l_1/l_2$-regularized Lasso correctly recovers the support\nunion; and if the sample size is below the threshold, $l_1/l_2$-regularized\nLasso fails to recover the support union. In particular, the threshold\nprecisely captures the impact of the sparsity of regression vectors and the\nstatistical properties of the design matrices on sample complexity. Therefore,\nthe threshold function also captures the advantages of joint support union\nrecovery using multi-task Lasso over individual support recovery using\nsingle-task Lasso.\n", "versions": [{"version": "v1", "created": "Tue, 30 Jul 2013 14:24:52 GMT"}], "update_date": "2013-07-31", "authors_parsed": [["Wang", "Weiguang", ""], ["Liang", "Yingbin", ""], ["Xing", "Eric P.", ""]]}, {"id": "1307.8136", "submitter": "Brian Kent", "authors": "Brian P. Kent, Alessandro Rinaldo, Timothy Verstynen", "title": "DeBaCl: A Python Package for Interactive DEnsity-BAsed CLustering", "comments": "28 pages, 9 figures, for associated software see\n  https://github.com/CoAxLab/DeBaCl", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME cs.LG stat.ML", "license": "http://creativecommons.org/licenses/by/3.0/", "abstract": "  The level set tree approach of Hartigan (1975) provides a probabilistically\nbased and highly interpretable encoding of the clustering behavior of a\ndataset. By representing the hierarchy of data modes as a dendrogram of the\nlevel sets of a density estimator, this approach offers many advantages for\nexploratory analysis and clustering, especially for complex and\nhigh-dimensional data. Several R packages exist for level set tree estimation,\nbut their practical usefulness is limited by computational inefficiency,\nabsence of interactive graphical capabilities and, from a theoretical\nperspective, reliance on asymptotic approximations. To make it easier for\npractitioners to capture the advantages of level set trees, we have written the\nPython package DeBaCl for DEnsity-BAsed CLustering. In this article we\nillustrate how DeBaCl's level set tree estimates can be used for difficult\nclustering tasks and interactive graphical data analysis. The package is\nintended to promote the practical use of level set trees through improvements\nin computational efficiency and a high degree of user customization. In\naddition, the flexible algorithms implemented in DeBaCl enjoy finite sample\naccuracy, as demonstrated in recent literature on density clustering. Finally,\nwe show the level set tree framework can be easily extended to deal with\nfunctional data.\n", "versions": [{"version": "v1", "created": "Tue, 30 Jul 2013 20:19:26 GMT"}], "update_date": "2013-08-01", "authors_parsed": [["Kent", "Brian P.", ""], ["Rinaldo", "Alessandro", ""], ["Verstynen", "Timothy", ""]]}, {"id": "1307.8229", "submitter": "Mengjie Chen", "authors": "Mengjie Chen, Chao Gao, Hongyu Zhao", "title": "Posterior Contraction Rates of the Phylogenetic Indian Buffet Processes", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML math.ST q-bio.QM stat.AP stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  By expressing prior distributions as general stochastic processes,\nnonparametric Bayesian methods provide a flexible way to incorporate prior\nknowledge and constrain the latent structure in statistical inference. The\nIndian buffet process (IBP) is such an example that can be used to define a\nprior distribution on infinite binary features, where the exchangeability among\nsubjects is assumed. The phylogenetic Indian buffet process (pIBP), a\nderivative of IBP, enables the modeling of non-exchangeability among subjects\nthrough a stochastic process on a rooted tree, which is similar to that used in\nphylogenetics, to describe relationships among the subjects. In this paper, we\nstudy the theoretical properties of IBP and pIBP under a binary factor model.\nWe establish the posterior contraction rates for both IBP and pIBP and\nsubstantiate the theoretical results through simulation studies. This is the\nfirst work addressing the frequentist property of the posterior behaviors of\nIBP and pIBP. We also demonstrated its practical usefulness by applying pIBP\nprior to a real data example arising in the field of cancer genomics where the\nexchangeability among subjects is violated.\n", "versions": [{"version": "v1", "created": "Wed, 31 Jul 2013 05:56:20 GMT"}, {"version": "v2", "created": "Tue, 19 May 2015 20:02:07 GMT"}], "update_date": "2015-05-21", "authors_parsed": [["Chen", "Mengjie", ""], ["Gao", "Chao", ""], ["Zhao", "Hongyu", ""]]}, {"id": "1307.8333", "submitter": "Stan Hatko", "authors": "Stan Hatko", "title": "Borel Isomorphic Dimensionality Reduction of Data and Supervised\n  Learning", "comments": "Winter 2013 Honours research project done under the supervision of\n  Dr. Vladimir Pestov at the University of Ottawa; 45 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this project we further investigate the idea of reducing the\ndimensionality of datasets using a Borel isomorphism with the purpose of\nsubsequently applying supervised learning algorithms, as originally suggested\nby my supervisor V. Pestov (in 2011 Dagstuhl preprint). Any consistent learning\nalgorithm, for example kNN, retains universal consistency after a Borel\nisomorphism is applied. A series of concrete examples of Borel isomorphisms\nthat reduce the number of dimensions in a dataset is provided, based on\nmultiplying the data by orthogonal matrices before the dimensionality reducing\nBorel isomorphism is applied. We test the accuracy of the resulting classifier\nin a lower dimensional space with various data sets. Working with a phoneme\nvoice recognition dataset, of dimension 256 with 5 classes (phonemes), we show\nthat a Borel isomorphic reduction to dimension 16 leads to a minimal drop in\naccuracy. In conclusion, we discuss further prospects of the method.\n", "versions": [{"version": "v1", "created": "Mon, 29 Jul 2013 03:09:49 GMT"}], "update_date": "2013-08-01", "authors_parsed": [["Hatko", "Stan", ""]]}, {"id": "1307.8371", "submitter": "Pranjal Awasthi", "authors": "Pranjal Awasthi, Maria Florina Balcan, Philip M. Long", "title": "The Power of Localization for Efficiently Learning Linear Separators\n  with Noise", "comments": "Contains improved label complexity analysis communicated to us by\n  Steve Hanneke", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CC cs.DS stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We introduce a new approach for designing computationally efficient learning\nalgorithms that are tolerant to noise, and demonstrate its effectiveness by\ndesigning algorithms with improved noise tolerance guarantees for learning\nlinear separators.\n  We consider both the malicious noise model and the adversarial label noise\nmodel. For malicious noise, where the adversary can corrupt both the label and\nthe features, we provide a polynomial-time algorithm for learning linear\nseparators in $\\Re^d$ under isotropic log-concave distributions that can\ntolerate a nearly information-theoretically optimal noise rate of $\\eta =\n\\Omega(\\epsilon)$. For the adversarial label noise model, where the\ndistribution over the feature vectors is unchanged, and the overall probability\nof a noisy label is constrained to be at most $\\eta$, we also give a\npolynomial-time algorithm for learning linear separators in $\\Re^d$ under\nisotropic log-concave distributions that can handle a noise rate of $\\eta =\n\\Omega\\left(\\epsilon\\right)$.\n  We show that, in the active learning model, our algorithms achieve a label\ncomplexity whose dependence on the error parameter $\\epsilon$ is\npolylogarithmic. This provides the first polynomial-time active learning\nalgorithm for learning linear separators in the presence of malicious noise or\nadversarial label noise.\n", "versions": [{"version": "v1", "created": "Wed, 31 Jul 2013 16:11:26 GMT"}, {"version": "v2", "created": "Mon, 12 Aug 2013 18:51:51 GMT"}, {"version": "v3", "created": "Mon, 23 Sep 2013 21:49:27 GMT"}, {"version": "v4", "created": "Wed, 13 Nov 2013 19:18:31 GMT"}, {"version": "v5", "created": "Mon, 16 Dec 2013 17:20:36 GMT"}, {"version": "v6", "created": "Fri, 3 Jan 2014 17:20:00 GMT"}, {"version": "v7", "created": "Fri, 7 Mar 2014 17:15:52 GMT"}, {"version": "v8", "created": "Wed, 12 Oct 2016 17:42:40 GMT"}, {"version": "v9", "created": "Sun, 3 Jun 2018 18:22:37 GMT"}], "update_date": "2018-06-05", "authors_parsed": [["Awasthi", "Pranjal", ""], ["Balcan", "Maria Florina", ""], ["Long", "Philip M.", ""]]}, {"id": "1307.8430", "submitter": "Bryan Conroy", "authors": "Bryan R. Conroy, Jennifer M. Walz, Brian Cheung, Paul Sajda", "title": "Fast Simultaneous Training of Generalized Linear Models (FaSTGLZ)", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present an efficient algorithm for simultaneously training sparse\ngeneralized linear models across many related problems, which may arise from\nbootstrapping, cross-validation and nonparametric permutation testing. Our\napproach leverages the redundancies across problems to obtain significant\ncomputational improvements relative to solving the problems sequentially by a\nconventional algorithm. We demonstrate our fast simultaneous training of\ngeneralized linear models (FaSTGLZ) algorithm on a number of real-world\ndatasets, and we run otherwise computationally intensive bootstrapping and\npermutation test analyses that are typically necessary for obtaining\nstatistically rigorous classification results and meaningful interpretation.\nCode is freely available at http://liinc.bme.columbia.edu/fastglz.\n", "versions": [{"version": "v1", "created": "Wed, 31 Jul 2013 19:18:11 GMT"}], "update_date": "2013-08-01", "authors_parsed": [["Conroy", "Bryan R.", ""], ["Walz", "Jennifer M.", ""], ["Cheung", "Brian", ""], ["Sajda", "Paul", ""]]}]