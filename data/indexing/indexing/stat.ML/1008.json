[{"id": "1008.0204", "submitter": "Guido F.  Montufar", "authors": "Guido Montufar", "title": "Mixture decompositions of exponential families using a decomposition of\n  their sample spaces", "comments": "17 pages, 2 figures", "journal-ref": "Kybernetika, Volume 49 (2013), Number 1", "doi": null, "report-no": null, "categories": "math.ST stat.ML stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study the problem of finding the smallest $m$ such that every element of\nan exponential family can be written as a mixture of $m$ elements of another\nexponential family. We propose an approach based on coverings and packings of\nthe face lattice of the corresponding convex support polytopes and results from\ncoding theory. We show that $m=q^{N-1}$ is the smallest number for which any\ndistribution of $N$ $q$-ary variables can be written as mixture of $m$\nindependent $q$-ary variables. Furthermore, we show that any distribution of\n$N$ binary variables is a mixture of $m = 2^{N-(k+1)}(1+ 1/(2^k-1))$ elements\nof the $k$-interaction exponential family.\n", "versions": [{"version": "v1", "created": "Sun, 1 Aug 2010 20:24:02 GMT"}, {"version": "v2", "created": "Sat, 11 Dec 2010 16:08:08 GMT"}, {"version": "v3", "created": "Thu, 27 Oct 2011 14:25:41 GMT"}, {"version": "v4", "created": "Mon, 25 Feb 2013 22:24:28 GMT"}], "update_date": "2013-02-27", "authors_parsed": [["Montufar", "Guido", ""]]}, {"id": "1008.0706", "submitter": "Allen Lavoie", "authors": "Allen Lavoie and Mukkai Krishnamoorthy", "title": "Algorithmic Detection of Computer Generated Text", "comments": null, "journal-ref": null, "doi": null, "report-no": "10-07", "categories": "stat.ML cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Computer generated academic papers have been used to expose a lack of\nthorough human review at several computer science conferences. We assess the\nproblem of classifying such documents. After identifying and evaluating several\nquantifiable features of academic papers, we apply methods from machine\nlearning to build a binary classifier. In tests with two hundred papers, the\nresulting classifier correctly labeled papers either as human written or as\ncomputer generated with no false classifications of computer generated papers\nas human and a 2% false classification rate for human papers as computer\ngenerated. We believe generalizations of these features are applicable to\nsimilar classification problems. While most current text-based spam detection\ntechniques focus on the keyword-based classification of email messages, a new\ngeneration of unsolicited computer-generated advertisements masquerade as\nlegitimate postings in online groups, message boards and social news sites. Our\nresults show that taking the formatting and contextual clues offered by these\nenvironments into account may be of central importance when selecting features\nwith which to identify such unwanted postings.\n", "versions": [{"version": "v1", "created": "Wed, 4 Aug 2010 06:00:21 GMT"}], "update_date": "2010-08-05", "authors_parsed": [["Lavoie", "Allen", ""], ["Krishnamoorthy", "Mukkai", ""]]}, {"id": "1008.2028", "submitter": "Daphne Koller", "authors": "Suchi Saria, Daphne Koller and Anna Penn", "title": "Discovering shared and individual latent structure in multiple time\n  series", "comments": "Additional supplementary section in tex file", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.AI stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper proposes a nonparametric Bayesian method for exploratory data\nanalysis and feature construction in continuous time series. Our method focuses\non understanding shared features in a set of time series that exhibit\nsignificant individual variability. Our method builds on the framework of\nlatent Diricihlet allocation (LDA) and its extension to hierarchical Dirichlet\nprocesses, which allows us to characterize each series as switching between\nlatent ``topics'', where each topic is characterized as a distribution over\n``words'' that specify the series dynamics. However, unlike standard\napplications of LDA, we discover the words as we learn the model. We apply this\nmodel to the task of tracking the physiological signals of premature infants;\nour model obtains clinically significant insights as well as useful features\nfor supervised learning tasks.\n", "versions": [{"version": "v1", "created": "Thu, 12 Aug 2010 00:41:23 GMT"}], "update_date": "2010-08-13", "authors_parsed": [["Saria", "Suchi", ""], ["Koller", "Daphne", ""], ["Penn", "Anna", ""]]}, {"id": "1008.2277", "submitter": "Jose M. Pe\\~na", "authors": "Jose M. Pe\\~na", "title": "Faithfulness in Chain Graphs: The Gaussian Case", "comments": null, "journal-ref": "Proceedings of the 14th International Conference on Artificial\n  Intelligence and Statistics (AISTATS 2011), 588-599", "doi": null, "report-no": null, "categories": "stat.ML cs.AI math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper deals with chain graphs under the classic\nLauritzen-Wermuth-Frydenberg interpretation. We prove that the regular Gaussian\ndistributions that factorize with respect to a chain graph $G$ with $d$\nparameters have positive Lebesgue measure with respect to $\\mathbb{R}^d$,\nwhereas those that factorize with respect to $G$ but are not faithful to it\nhave zero Lebesgue measure with respect to $\\mathbb{R}^d$. This means that, in\nthe measure-theoretic sense described, almost all the regular Gaussian\ndistributions that factorize with respect to $G$ are faithful to it.\n", "versions": [{"version": "v1", "created": "Fri, 13 Aug 2010 10:03:48 GMT"}], "update_date": "2012-06-27", "authors_parsed": [["Pe\u00f1a", "Jose M.", ""]]}, {"id": "1008.2514", "submitter": "Gert De Cooman", "authors": "Gert de Cooman, Filip Hermans, Alessandro Antonucci and Marco Zaffalon", "title": "Epistemic irrelevance in credal nets: the case of imprecise Markov trees", "comments": "29 pages, 5 figures, 1 table", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI math.PR stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We focus on credal nets, which are graphical models that generalise Bayesian\nnets to imprecise probability. We replace the notion of strong independence\ncommonly used in credal nets with the weaker notion of epistemic irrelevance,\nwhich is arguably more suited for a behavioural theory of probability. Focusing\non directed trees, we show how to combine the given local uncertainty models in\nthe nodes of the graph into a global model, and we use this to construct and\njustify an exact message-passing algorithm that computes updated beliefs for a\nvariable in the tree. The algorithm, which is linear in the number of nodes, is\nformulated entirely in terms of coherent lower previsions, and is shown to\nsatisfy a number of rationality requirements. We supply examples of the\nalgorithm's operation, and report an application to on-line character\nrecognition that illustrates the advantages of our approach for prediction. We\ncomment on the perspectives, opened by the availability, for the first time, of\na truly efficient algorithm based on epistemic irrelevance.\n", "versions": [{"version": "v1", "created": "Sun, 15 Aug 2010 12:26:24 GMT"}], "update_date": "2010-08-17", "authors_parsed": [["de Cooman", "Gert", ""], ["Hermans", "Filip", ""], ["Antonucci", "Alessandro", ""], ["Zaffalon", "Marco", ""]]}, {"id": "1008.2743", "submitter": "Gautam Pendse", "authors": "Gautam V. Pendse", "title": "PMOG: The projected mixture of Gaussians model with application to blind\n  source separation", "comments": "46 pages, 9 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.AI stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We extend the mixtures of Gaussians (MOG) model to the projected mixture of\nGaussians (PMOG) model. In the PMOG model, we assume that q dimensional input\ndata points z_i are projected by a q dimensional vector w into 1-D variables\nu_i. The projected variables u_i are assumed to follow a 1-D MOG model. In the\nPMOG model, we maximize the likelihood of observing u_i to find both the model\nparameters for the 1-D MOG as well as the projection vector w. First, we derive\nan EM algorithm for estimating the PMOG model. Next, we show how the PMOG model\ncan be applied to the problem of blind source separation (BSS). In contrast to\nconventional BSS where an objective function based on an approximation to\ndifferential entropy is minimized, PMOG based BSS simply minimizes the\ndifferential entropy of projected sources by fitting a flexible MOG model in\nthe projected 1-D space while simultaneously optimizing the projection vector\nw. The advantage of PMOG over conventional BSS algorithms is the more flexible\nfitting of non-Gaussian source densities without assuming near-Gaussianity (as\nin conventional BSS) and still retaining computational feasibility.\n", "versions": [{"version": "v1", "created": "Mon, 16 Aug 2010 19:26:17 GMT"}], "update_date": "2010-08-17", "authors_parsed": [["Pendse", "Gautam V.", ""]]}, {"id": "1008.2908", "submitter": "Giuseppe Jurman", "authors": "Giuseppe Jurman and Cesare Furlanello", "title": "A unifying view for performance measures in multi-class prediction", "comments": null, "journal-ref": "PLoS ONE 7(8): e41882 (2012)", "doi": "10.1371/journal.pone.0041882", "report-no": null, "categories": "stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In the last few years, many different performance measures have been\nintroduced to overcome the weakness of the most natural metric, the Accuracy.\nAmong them, Matthews Correlation Coefficient has recently gained popularity\namong researchers not only in machine learning but also in several application\nfields such as bioinformatics. Nonetheless, further novel functions are being\nproposed in literature. We show that Confusion Entropy, a recently introduced\nclassifier performance measure for multi-class problems, has a strong\n(monotone) relation with the multi-class generalization of a classical metric,\nthe Matthews Correlation Coefficient. Computational evidence in support of the\nclaim is provided, together with an outline of the theoretical explanation.\n", "versions": [{"version": "v1", "created": "Tue, 17 Aug 2010 14:47:31 GMT"}], "update_date": "2012-08-20", "authors_parsed": [["Jurman", "Giuseppe", ""], ["Furlanello", "Cesare", ""]]}, {"id": "1008.3043", "submitter": "Karin Schnass", "authors": "Massimo Fornasier, Karin Schnass, Jan Vybiral", "title": "Learning Functions of Few Arbitrary Linear Parameters in High Dimensions", "comments": "31 pages, this version was accepted to Foundations of Computational\n  Mathematics, the final publication will be available on\n  http://www.springerlink.com", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.NA cs.CC cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Let us assume that $f$ is a continuous function defined on the unit ball of\n$\\mathbb R^d$, of the form $f(x) = g (A x)$, where $A$ is a $k \\times d$ matrix\nand $g$ is a function of $k$ variables for $k \\ll d$. We are given a budget $m\n\\in \\mathbb N$ of possible point evaluations $f(x_i)$, $i=1,...,m$, of $f$,\nwhich we are allowed to query in order to construct a uniform approximating\nfunction. Under certain smoothness and variation assumptions on the function\n$g$, and an {\\it arbitrary} choice of the matrix $A$, we present in this paper\n  1. a sampling choice of the points $\\{x_i\\}$ drawn at random for each\nfunction approximation;\n  2. algorithms (Algorithm 1 and Algorithm 2) for computing the approximating\nfunction, whose complexity is at most polynomial in the dimension $d$ and in\nthe number $m$ of points.\n  Due to the arbitrariness of $A$, the choice of the sampling points will be\naccording to suitable random distributions and our results hold with\noverwhelming probability. Our approach uses tools taken from the {\\it\ncompressed sensing} framework, recent Chernoff bounds for sums of\npositive-semidefinite matrices, and classical stability bounds for invariant\nsubspaces of singular value decompositions.\n", "versions": [{"version": "v1", "created": "Wed, 18 Aug 2010 08:36:21 GMT"}, {"version": "v2", "created": "Tue, 17 Jan 2012 18:52:44 GMT"}], "update_date": "2012-01-18", "authors_parsed": [["Fornasier", "Massimo", ""], ["Schnass", "Karin", ""], ["Vybiral", "Jan", ""]]}, {"id": "1008.3572", "submitter": "Sayan Mukherjee", "authors": "Paul Bendich and Sayan Mukherjee and Bei Wang", "title": "Towards Stratification Learning through Homology Inference", "comments": "48 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.GT stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A topological approach to stratification learning is developed for point\ncloud data drawn from a stratified space. Given such data, our objective is to\ninfer which points belong to the same strata. First we define a multi-scale\nnotion of a stratified space, giving a stratification for each radius level. We\nthen use methods derived from kernel and cokernel persistent homology to\ncluster the data points into different strata, and we prove a result which\nguarantees the correctness of our clustering, given certain topological\nconditions; some geometric intuition for these topological conditions is also\nprovided. Our correctness result is then given a probabilistic flavor: we give\nbounds on the minimum number of sample points required to infer, with\nprobability, which points belong to the same strata. Finally, we give an\nexplicit algorithm for the clustering, prove its correctness, and apply it to\nsome simulated data.\n", "versions": [{"version": "v1", "created": "Fri, 20 Aug 2010 20:46:39 GMT"}], "update_date": "2010-08-24", "authors_parsed": [["Bendich", "Paul", ""], ["Mukherjee", "Sayan", ""], ["Wang", "Bei", ""]]}, {"id": "1008.3585", "submitter": "Fionn Murtagh", "authors": "Fionn Murtagh", "title": "Ultrametric and Generalized Ultrametric in Computational Logic and in\n  Data Analysis", "comments": "19 pp., 5 figures, 3 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LO cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Following a review of metric, ultrametric and generalized ultrametric, we\nreview their application in data analysis. We show how they allow us to explore\nboth geometry and topology of information, starting with measured data. Some\nthemes are then developed based on the use of metric, ultrametric and\ngeneralized ultrametric in logic. In particular we study approximation chains\nin an ultrametric or generalized ultrametric context. Our aim in this work is\nto extend the scope of data analysis by facilitating reasoning based on the\ndata analysis; and to show how quantitative and qualitative data analysis can\nbe incorporated into logic programming.\n", "versions": [{"version": "v1", "created": "Fri, 20 Aug 2010 23:07:54 GMT"}], "update_date": "2010-08-24", "authors_parsed": [["Murtagh", "Fionn", ""]]}, {"id": "1008.3951", "submitter": "Jiheng Wang", "authors": "Jiheng Wang, Guangzhe Fan and Zhou Wang", "title": "A Simple CW-SSIM Kernel-based Nearest Neighbor Method for Handwritten\n  Digit Classification", "comments": "16 pages, 11 figures, 1 table and 3 equations", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a simple kernel based nearest neighbor approach for handwritten\ndigit classification. The \"distance\" here is actually a kernel defining the\nsimilarity between two images. We carefully study the effects of different\nnumber of neighbors and weight schemes and report the results. With only a few\nnearest neighbors (or most similar images) to vote, the test set error rate on\nMNIST database could reach about 1.5%-2.0%, which is very close to many\nadvanced models.\n", "versions": [{"version": "v1", "created": "Tue, 24 Aug 2010 02:47:00 GMT"}, {"version": "v2", "created": "Wed, 1 Sep 2010 14:20:43 GMT"}, {"version": "v3", "created": "Fri, 3 Sep 2010 08:16:47 GMT"}], "update_date": "2010-09-06", "authors_parsed": [["Wang", "Jiheng", ""], ["Fan", "Guangzhe", ""], ["Wang", "Zhou", ""]]}, {"id": "1008.3952", "submitter": "Jiheng Wang", "authors": "Fang Yang, Jiheng Wang and Guangzhe Fan", "title": "Kernel induced random survival forests", "comments": "16 Pages, 7 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Kernel Induced Random Survival Forests (KIRSF) is a statistical learning\nalgorithm which aims to improve prediction accuracy for survival data. As in\nRandom Survival Forests (RSF), Cumulative Hazard Function is predicted for each\nindividual in the test set. Prediction error is estimated using Harrell's\nconcordance index (C index) [Harrell et al. (1982)]. The C-index can be\ninterpreted as a misclassification probability and does not depend on a single\nfixed time for evaluation. The C-index also specifically accounts for\ncensoring. By utilizing kernel functions, KIRSF achieves better results than\nRSF in many situations. In this report, we show how to incorporate kernel\nfunctions into RSF. We test the performance of KIRSF and compare our method to\nRSF. We find that the KIRSF's performance is better than RSF in many occasions.\n", "versions": [{"version": "v1", "created": "Tue, 24 Aug 2010 02:54:03 GMT"}], "update_date": "2010-08-25", "authors_parsed": [["Yang", "Fang", ""], ["Wang", "Jiheng", ""], ["Fan", "Guangzhe", ""]]}, {"id": "1008.4000", "submitter": "Dacheng Tao", "authors": "Tianyi Zhou, Dacheng Tao, Xindong Wu", "title": "NESVM: a Fast Gradient Method for Support Vector Machines", "comments": "10 pages, 11 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://creativecommons.org/licenses/by-nc-sa/3.0/", "abstract": "  Support vector machines (SVMs) are invaluable tools for many practical\napplications in artificial intelligence, e.g., classification and event\nrecognition. However, popular SVM solvers are not sufficiently efficient for\napplications with a great deal of samples as well as a large number of\nfeatures. In this paper, thus, we present NESVM, a fast gradient SVM solver\nthat can optimize various SVM models, e.g., classical SVM, linear programming\nSVM and least square SVM. Compared against SVM-Perf\n\\cite{SVM_Perf}\\cite{PerfML} (its convergence rate in solving the dual SVM is\nupper bounded by $\\mathcal O(1/\\sqrt{k})$, wherein $k$ is the number of\niterations.) and Pegasos \\cite{Pegasos} (online SVM that converges at rate\n$\\mathcal O(1/k)$ for the primal SVM), NESVM achieves the optimal convergence\nrate at $\\mathcal O(1/k^{2})$ and a linear time complexity. In particular,\nNESVM smoothes the non-differentiable hinge loss and $\\ell_1$-norm in the\nprimal SVM. Then the optimal gradient method without any line search is adopted\nto solve the optimization. In each iteration round, the current gradient and\nhistorical gradients are combined to determine the descent direction, while the\nLipschitz constant determines the step size. Only two matrix-vector\nmultiplications are required in each iteration round. Therefore, NESVM is more\nefficient than existing SVM solvers. In addition, NESVM is available for both\nlinear and nonlinear kernels. We also propose \"homotopy NESVM\" to accelerate\nNESVM by dynamically decreasing the smooth parameter and using the continuation\nmethod. Our experiments on census income categorization, indoor/outdoor scene\nclassification, event recognition and scene recognition suggest the efficiency\nand the effectiveness of NESVM. The MATLAB code of NESVM will be available on\nour website for further assessment.\n", "versions": [{"version": "v1", "created": "Tue, 24 Aug 2010 10:02:01 GMT"}], "update_date": "2011-10-02", "authors_parsed": [["Zhou", "Tianyi", ""], ["Tao", "Dacheng", ""], ["Wu", "Xindong", ""]]}, {"id": "1008.4182", "submitter": "James P. Crutchfield", "authors": "Nicholas F. Travers and James P. Crutchfield", "title": "Exact Synchronization for Finite-State Sources", "comments": "9 pages, 6 figures; now includes analytical calculation of the\n  synchronization rate; updates and corrections added", "journal-ref": null, "doi": "10.1007/s10955-011-0342-4", "report-no": null, "categories": "nlin.CD cs.IT math.DS math.IT stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We analyze how an observer synchronizes to the internal state of a\nfinite-state information source, using the epsilon-machine causal\nrepresentation. Here, we treat the case of exact synchronization, when it is\npossible for the observer to synchronize completely after a finite number of\nobservations. The more difficult case of strictly asymptotic synchronization is\ntreated in a sequel. In both cases, we find that an observer, on average, will\nsynchronize to the source state exponentially fast and that, as a result, the\naverage accuracy in an observer's predictions of the source output approaches\nits optimal level exponentially fast as well. Additionally, we show here how to\nanalytically calculate the synchronization rate for exact epsilon-machines and\nprovide an efficient polynomial-time algorithm to test epsilon-machines for\nexactness.\n", "versions": [{"version": "v1", "created": "Wed, 25 Aug 2010 00:59:08 GMT"}, {"version": "v2", "created": "Tue, 26 Oct 2010 02:24:56 GMT"}, {"version": "v3", "created": "Mon, 3 Jan 2011 22:22:20 GMT"}], "update_date": "2015-05-19", "authors_parsed": [["Travers", "Nicholas F.", ""], ["Crutchfield", "James P.", ""]]}, {"id": "1008.4220", "submitter": "Francis Bach", "authors": "Francis Bach (INRIA Rocquencourt, LIENS)", "title": "Structured sparsity-inducing norms through submodular functions", "comments": null, "journal-ref": "NIPS, Canada (2010)", "doi": null, "report-no": null, "categories": "cs.LG math.OC stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Sparse methods for supervised learning aim at finding good linear predictors\nfrom as few variables as possible, i.e., with small cardinality of their\nsupports. This combinatorial selection problem is often turned into a convex\noptimization problem by replacing the cardinality function by its convex\nenvelope (tightest convex lower bound), in this case the L1-norm. In this\npaper, we investigate more general set-functions than the cardinality, that may\nincorporate prior knowledge or structural constraints which are common in many\napplications: namely, we show that for nondecreasing submodular set-functions,\nthe corresponding convex envelope can be obtained from its \\lova extension, a\ncommon tool in submodular analysis. This defines a family of polyhedral norms,\nfor which we provide generic algorithmic tools (subgradients and proximal\noperators) and theoretical results (conditions for support recovery or\nhigh-dimensional inference). By selecting specific submodular functions, we can\ngive a new interpretation to known norms, such as those based on\nrank-statistics or grouped norms with potentially overlapping groups; we also\ndefine new norms, in particular ones that can be used as non-factorial priors\nfor supervised learning.\n", "versions": [{"version": "v1", "created": "Wed, 25 Aug 2010 07:28:08 GMT"}, {"version": "v2", "created": "Wed, 22 Sep 2010 03:11:25 GMT"}, {"version": "v3", "created": "Fri, 12 Nov 2010 14:51:23 GMT"}], "update_date": "2010-11-15", "authors_parsed": [["Bach", "Francis", "", "INRIA Rocquencourt, LIENS"]]}, {"id": "1008.4973", "submitter": "Nabin Malakar", "authors": "N. K. Malakar and K. H. Knuth", "title": "Entropy-Based Search Algorithm for Experimental Design", "comments": "8 pages, 3 figures. To appear in the proceedings of MaxEnt 2010, held\n  in Chamonix, France", "journal-ref": null, "doi": "10.1063/1.3573612", "report-no": null, "categories": "stat.ML cs.LG physics.comp-ph physics.data-an", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The scientific method relies on the iterated processes of inference and\ninquiry. The inference phase consists of selecting the most probable models\nbased on the available data; whereas the inquiry phase consists of using what\nis known about the models to select the most relevant experiment. Optimizing\ninquiry involves searching the parameterized space of experiments to select the\nexperiment that promises, on average, to be maximally informative. In the case\nwhere it is important to learn about each of the model parameters, the\nrelevance of an experiment is quantified by Shannon entropy of the distribution\nof experimental outcomes predicted by a probable set of models. If the set of\npotential experiments is described by many parameters, we must search this\nhigh-dimensional entropy space. Brute force search methods will be slow and\ncomputationally expensive. We present an entropy-based search algorithm, called\nnested entropy sampling, to select the most informative experiment for\nefficient experimental design. This algorithm is inspired by Skilling's nested\nsampling algorithm used in inference and borrows the concept of a rising\nthreshold while a set of experiment samples are maintained. We demonstrate that\nthis algorithm not only selects highly relevant experiments, but also is more\nefficient than brute force search. Such entropic search techniques promise to\ngreatly benefit autonomous experimental design.\n", "versions": [{"version": "v1", "created": "Sun, 29 Aug 2010 23:37:19 GMT"}], "update_date": "2015-05-19", "authors_parsed": [["Malakar", "N. K.", ""], ["Knuth", "K. H.", ""]]}, {"id": "1008.4988", "submitter": "Heng Luo", "authors": "Heng Luo, Ruimin Shen and Cahngyong Niu", "title": "Sparse Group Restricted Boltzmann Machines", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Since learning is typically very slow in Boltzmann machines, there is a need\nto restrict connections within hidden layers. However, the resulting states of\nhidden units exhibit statistical dependencies. Based on this observation, we\npropose using $l_1/l_2$ regularization upon the activation possibilities of\nhidden units in restricted Boltzmann machines to capture the loacal\ndependencies among hidden units. This regularization not only encourages hidden\nunits of many groups to be inactive given observed data but also makes hidden\nunits within a group compete with each other for modeling observed data. Thus,\nthe $l_1/l_2$ regularization on RBMs yields sparsity at both the group and the\nhidden unit levels. We call RBMs trained with the regularizer \\emph{sparse\ngroup} RBMs. The proposed sparse group RBMs are applied to three tasks:\nmodeling patches of natural images, modeling handwritten digits and pretaining\na deep networks for a classification task. Furthermore, we illustrate the\nregularizer can also be applied to deep Boltzmann machines, which lead to\nsparse group deep Boltzmann machines. When adapted to the MNIST data set, a\ntwo-layer sparse group Boltzmann machine achieves an error rate of $0.84\\%$,\nwhich is, to our knowledge, the best published result on the\npermutation-invariant version of the MNIST task.\n", "versions": [{"version": "v1", "created": "Mon, 30 Aug 2010 02:44:19 GMT"}], "update_date": "2010-08-31", "authors_parsed": [["Luo", "Heng", ""], ["Shen", "Ruimin", ""], ["Niu", "Cahngyong", ""]]}, {"id": "1008.5071", "submitter": "Gael Varoquaux", "authors": "Ga\\\"el Varoquaux (LNAO, INRIA Saclay - Ile de France), Alexandre\n  Gramfort (LNAO, INRIA Saclay - Ile de France), Jean Baptiste Poline (LNAO),\n  Bertrand Thirion (LNAO, INRIA Saclay - Ile de France)", "title": "Brain covariance selection: better individual functional connectivity\n  models using population prior", "comments": "in Advances in Neural Information Processing Systems, Vancouver :\n  Canada (2010)", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML q-bio.NC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Spontaneous brain activity, as observed in functional neuroimaging, has been\nshown to display reproducible structure that expresses brain architecture and\ncarries markers of brain pathologies. An important view of modern neuroscience\nis that such large-scale structure of coherent activity reflects modularity\nproperties of brain connectivity graphs. However, to date, there has been no\ndemonstration that the limited and noisy data available in spontaneous activity\nobservations could be used to learn full-brain probabilistic models that\ngeneralize to new data. Learning such models entails two main challenges: i)\nmodeling full brain connectivity is a difficult estimation problem that faces\nthe curse of dimensionality and ii) variability between subjects, coupled with\nthe variability of functional signals between experimental runs, makes the use\nof multiple datasets challenging. We describe subject-level brain functional\nconnectivity structure as a multivariate Gaussian process and introduce a new\nstrategy to estimate it from group data, by imposing a common structure on the\ngraphical model in the population. We show that individual models learned from\nfunctional Magnetic Resonance Imaging (fMRI) data using this population prior\ngeneralize better to unseen data than models based on alternative\nregularization schemes. To our knowledge, this is the first report of a\ncross-validated model of spontaneous brain activity. Finally, we use the\nestimated graphical model to explore the large-scale characteristics of\nfunctional architecture and show for the first time that known cognitive\nnetworks appear as the integrated communities of functional connectivity graph.\n", "versions": [{"version": "v1", "created": "Mon, 30 Aug 2010 12:52:36 GMT"}, {"version": "v2", "created": "Tue, 21 Sep 2010 06:02:33 GMT"}, {"version": "v3", "created": "Sat, 30 Oct 2010 16:21:30 GMT"}, {"version": "v4", "created": "Fri, 12 Nov 2010 05:55:55 GMT"}], "update_date": "2010-11-15", "authors_parsed": [["Varoquaux", "Ga\u00ebl", "", "LNAO, INRIA Saclay - Ile de France"], ["Gramfort", "Alexandre", "", "LNAO, INRIA Saclay - Ile de France"], ["Poline", "Jean Baptiste", "", "LNAO"], ["Thirion", "Bertrand", "", "LNAO, INRIA Saclay - Ile de France"]]}, {"id": "1008.5090", "submitter": "Francesco Dinuzzo", "authors": "Francesco Dinuzzo", "title": "Fixed-point and coordinate descent algorithms for regularized kernel\n  methods", "comments": null, "journal-ref": null, "doi": "10.1109/TNN.2011.2164096", "report-no": null, "categories": "cs.LG math.OC stat.CO stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we study two general classes of optimization algorithms for\nkernel methods with convex loss function and quadratic norm regularization, and\nanalyze their convergence. The first approach, based on fixed-point iterations,\nis simple to implement and analyze, and can be easily parallelized. The second,\nbased on coordinate descent, exploits the structure of additively separable\nloss functions to compute solutions of line searches in closed form. Instances\nof these general classes of algorithms are already incorporated into state of\nthe art machine learning software for large scale problems. We start from a\nsolution characterization of the regularized problem, obtained using\nsub-differential calculus and resolvents of monotone operators, that holds for\ngeneral convex loss functions regardless of differentiability. The two\nmethodologies described in the paper can be regarded as instances of non-linear\nJacobi and Gauss-Seidel algorithms, and are both well-suited to solve large\nscale problems.\n", "versions": [{"version": "v1", "created": "Mon, 30 Aug 2010 14:39:57 GMT"}], "update_date": "2013-07-02", "authors_parsed": [["Dinuzzo", "Francesco", ""]]}, {"id": "1008.5209", "submitter": "Julien Mairal", "authors": "Julien Mairal (INRIA Rocquencourt, LIENS), Rodolphe Jenatton (INRIA\n  Rocquencourt, LIENS), Guillaume Obozinski (INRIA Rocquencourt, LIENS),\n  Francis Bach (INRIA Rocquencourt, LIENS)", "title": "Network Flow Algorithms for Structured Sparsity", "comments": "accepted for publication in Adv. Neural Information Processing\n  Systems, 2010", "journal-ref": null, "doi": null, "report-no": "RR-7372", "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider a class of learning problems that involve a structured\nsparsity-inducing norm defined as the sum of $\\ell_\\infty$-norms over groups of\nvariables. Whereas a lot of effort has been put in developing fast optimization\nmethods when the groups are disjoint or embedded in a specific hierarchical\nstructure, we address here the case of general overlapping groups. To this end,\nwe show that the corresponding optimization problem is related to network flow\noptimization. More precisely, the proximal problem associated with the norm we\nconsider is dual to a quadratic min-cost flow problem. We propose an efficient\nprocedure which computes its solution exactly in polynomial time. Our algorithm\nscales up to millions of variables, and opens up a whole new range of\napplications for structured sparse models. We present several experiments on\nimage and video data, demonstrating the applicability and scalability of our\napproach for various problems.\n", "versions": [{"version": "v1", "created": "Tue, 31 Aug 2010 03:39:49 GMT"}], "update_date": "2010-09-02", "authors_parsed": [["Mairal", "Julien", "", "INRIA Rocquencourt, LIENS"], ["Jenatton", "Rodolphe", "", "INRIA\n  Rocquencourt, LIENS"], ["Obozinski", "Guillaume", "", "INRIA Rocquencourt, LIENS"], ["Bach", "Francis", "", "INRIA Rocquencourt, LIENS"]]}, {"id": "1008.5211", "submitter": "Mladen Kolar", "authors": "Mladen Kolar, John Lafferty, Larry Wasserman", "title": "Union Support Recovery in Multi-task Learning", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We sharply characterize the performance of different penalization schemes for\nthe problem of selecting the relevant variables in the multi-task setting.\nPrevious work focuses on the regression problem where conditions on the design\nmatrix complicate the analysis. A clearer and simpler picture emerges by\nstudying the Normal means model. This model, often used in the field of\nstatistics, is a simplified model that provides a laboratory for studying\ncomplex procedures.\n", "versions": [{"version": "v1", "created": "Tue, 31 Aug 2010 03:49:26 GMT"}], "update_date": "2010-09-01", "authors_parsed": [["Kolar", "Mladen", ""], ["Lafferty", "John", ""], ["Wasserman", "Larry", ""]]}, {"id": "1008.5386", "submitter": "Ricardo Silva", "authors": "Ricardo Silva and Charles Blundell and Yee Whye Teh", "title": "Mixed Cumulative Distribution Networks", "comments": "11 pages, 4 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Directed acyclic graphs (DAGs) are a popular framework to express\nmultivariate probability distributions. Acyclic directed mixed graphs (ADMGs)\nare generalizations of DAGs that can succinctly capture much richer sets of\nconditional independencies, and are especially useful in modeling the effects\nof latent variables implicitly. Unfortunately there are currently no good\nparameterizations of general ADMGs. In this paper, we apply recent work on\ncumulative distribution networks and copulas to propose one one general\nconstruction for ADMG models. We consider a simple parameter estimation\napproach, and report some encouraging experimental results.\n", "versions": [{"version": "v1", "created": "Tue, 31 Aug 2010 18:51:43 GMT"}], "update_date": "2010-09-01", "authors_parsed": [["Silva", "Ricardo", ""], ["Blundell", "Charles", ""], ["Teh", "Yee Whye", ""]]}]