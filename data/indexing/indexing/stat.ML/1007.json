[{"id": "1007.0499", "submitter": "Ali Shojaie", "authors": "Ali Shojaie and George Michailidis", "title": "Discovering Graphical Granger Causality Using the Truncating Lasso\n  Penalty", "comments": "12 pages, 4 figures, 1 table", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML q-bio.MN", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Components of biological systems interact with each other in order to carry\nout vital cell functions. Such information can be used to improve estimation\nand inference, and to obtain better insights into the underlying cellular\nmechanisms. Discovering regulatory interactions among genes is therefore an\nimportant problem in systems biology. Whole-genome expression data over time\nprovides an opportunity to determine how the expression levels of genes are\naffected by changes in transcription levels of other genes, and can therefore\nbe used to discover regulatory interactions among genes.\n  In this paper, we propose a novel penalization method, called truncating\nlasso, for estimation of causal relationships from time-course gene expression\ndata. The proposed penalty can correctly determine the order of the underlying\ntime series, and improves the performance of the lasso-type estimators.\nMoreover, the resulting estimate provides information on the time lag between\nactivation of transcription factors and their effects on regulated genes. We\nprovide an efficient algorithm for estimation of model parameters, and show\nthat the proposed method can consistently discover causal relationships in the\nlarge $p$, small $n$ setting. The performance of the proposed model is\nevaluated favorably in simulated, as well as real, data examples. The proposed\ntruncating lasso method is implemented in the R-package grangerTlasso and is\navailable at http://www.stat.lsa.umich.edu/~shojaie.\n", "versions": [{"version": "v1", "created": "Sat, 3 Jul 2010 15:35:50 GMT"}], "update_date": "2010-07-06", "authors_parsed": [["Shojaie", "Ali", ""], ["Michailidis", "George", ""]]}, {"id": "1007.0549", "submitter": "Larry Wasserman", "authors": "Christopher Genovese, Marco Perone-Pacifico, Isabella Verdinelli and\n  Larry Wasserman", "title": "Minimax Manifold Estimation", "comments": "journal submission, revision with some errors corrected", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We find the minimax rate of convergence in Hausdorff distance for estimating\na manifold M of dimension d embedded in R^D given a noisy sample from the\nmanifold. We assume that the manifold satisfies a smoothness condition and that\nthe noise distribution has compact support. We show that the optimal rate of\nconvergence is n^{-2/(2+d)}. Thus, the minimax rate depends only on the\ndimension of the manifold, not on the dimension of the space in which M is\nembedded.\n", "versions": [{"version": "v1", "created": "Sun, 4 Jul 2010 13:11:40 GMT"}, {"version": "v2", "created": "Tue, 23 Nov 2010 17:21:02 GMT"}, {"version": "v3", "created": "Wed, 28 Sep 2011 18:14:13 GMT"}], "update_date": "2011-09-29", "authors_parsed": [["Genovese", "Christopher", ""], ["Perone-Pacifico", "Marco", ""], ["Verdinelli", "Isabella", ""], ["Wasserman", "Larry", ""]]}, {"id": "1007.0563", "submitter": "Divyanshu Vats", "authors": "Divyanshu Vats and Jose M. F. Moura", "title": "Graphical Models as Block-Tree Graphs", "comments": "29 pages. Correction to version 1", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.IT math.IT math.PR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We introduce block-tree graphs as a framework for deriving efficient\nalgorithms on graphical models. We define block-tree graphs as a\ntree-structured graph where each node is a cluster of nodes such that the\nclusters in the graph are disjoint. This differs from junction-trees, where two\nclusters connected by an edge always have at least one common node. When\ncompared to junction-trees, we show that constructing block-tree graphs is\nfaster, and finding optimal block-tree graphs has a much smaller search space.\nApplying our block-tree graph framework to graphical models, we show that, for\nsome graphs, e.g., grid graphs, using block-tree graphs for inference is\ncomputationally more efficient than using junction-trees. For graphical models\nwith boundary conditions, the block-tree graph framework transforms the\nboundary valued problem into an initial value problem. For Gaussian graphical\nmodels, the block-tree graph framework leads to a linear state-space\nrepresentation. Since exact inference in graphical models can be\ncomputationally intractable, we propose to use spanning block-trees to derive\napproximate inference algorithms. Experimental results show the improved\nperformance in using spanning block-trees versus using spanning trees for\napproximate estimation over Gaussian graphical models.\n", "versions": [{"version": "v1", "created": "Sun, 4 Jul 2010 15:36:41 GMT"}, {"version": "v2", "created": "Sat, 13 Nov 2010 18:12:51 GMT"}], "update_date": "2010-11-16", "authors_parsed": [["Vats", "Divyanshu", ""], ["Moura", "Jose M. F.", ""]]}, {"id": "1007.0832", "submitter": "Francois Bavaud", "authors": "Fran\\c{c}ois Bavaud", "title": "Euclidean Distances, soft and spectral Clustering on Weighted Graphs", "comments": "accepted for presentation (and further publication) at the ECML PKDD\n  2010 conference", "journal-ref": "ECML PKDD 2010, J.L. Balc\\'azar et al. (Eds.): ECML PKDD 2010,\n  Part I, LNAI 6321, pp. 103-118", "doi": "10.1007/978-3-642-15880-3", "report-no": null, "categories": "stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We define a class of Euclidean distances on weighted graphs, enabling to\nperform thermodynamic soft graph clustering. The class can be constructed form\nthe \"raw coordinates\" encountered in spectral clustering, and can be extended\nby means of higher-dimensional embeddings (Schoenberg transformations).\nGeographical flow data, properly conditioned, illustrate the procedure as well\nas visualization aspects.\n", "versions": [{"version": "v1", "created": "Tue, 6 Jul 2010 08:36:21 GMT"}], "update_date": "2010-09-15", "authors_parsed": [["Bavaud", "Fran\u00e7ois", ""]]}, {"id": "1007.1075", "submitter": "Ulrike von Luxburg", "authors": "Ulrike von Luxburg", "title": "Clustering Stability: An Overview", "comments": null, "journal-ref": "Foundations and Trends in Machine Learning, Vol. 2, No. 3, p.\n  235-274, 2010", "doi": "10.1561/2200000008", "report-no": null, "categories": "stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A popular method for selecting the number of clusters is based on stability\narguments: one chooses the number of clusters such that the corresponding\nclustering results are \"most stable\". In recent years, a series of papers has\nanalyzed the behavior of this method from a theoretical point of view. However,\nthe results are very technical and difficult to interpret for non-experts. In\nthis paper we give a high-level overview about the existing literature on\nclustering stability. In addition to presenting the results in a slightly\ninformal but accessible way, we relate them to each other and discuss their\ndifferent implications.\n", "versions": [{"version": "v1", "created": "Wed, 7 Jul 2010 08:31:17 GMT"}], "update_date": "2010-07-08", "authors_parsed": [["von Luxburg", "Ulrike", ""]]}, {"id": "1007.1684", "submitter": "Karl Rohe", "authors": "Karl Rohe, Sourav Chatterjee, Bin Yu", "title": "Spectral clustering and the high-dimensional stochastic blockmodel", "comments": "Published in at http://dx.doi.org/10.1214/11-AOS887 the Annals of\n  Statistics (http://www.imstat.org/aos/) by the Institute of Mathematical\n  Statistics (http://www.imstat.org)", "journal-ref": "Annals of Statistics 2011, Vol. 39, No. 4, 1878-1915", "doi": "10.1214/11-AOS887", "report-no": "IMS-AOS-AOS887", "categories": "stat.ML math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Networks or graphs can easily represent a diverse set of data sources that\nare characterized by interacting units or actors. Social networks, representing\npeople who communicate with each other, are one example. Communities or\nclusters of highly connected actors form an essential feature in the structure\nof several empirical networks. Spectral clustering is a popular and\ncomputationally feasible method to discover these communities. The stochastic\nblockmodel [Social Networks 5 (1983) 109--137] is a social network model with\nwell-defined communities; each node is a member of one community. For a network\ngenerated from the Stochastic Blockmodel, we bound the number of nodes\n\"misclustered\" by spectral clustering. The asymptotic results in this paper are\nthe first clustering results that allow the number of clusters in the model to\ngrow with the number of nodes, hence the name high-dimensional. In order to\nstudy spectral clustering under the stochastic blockmodel, we first show that\nunder the more general latent space model, the eigenvectors of the normalized\ngraph Laplacian asymptotically converge to the eigenvectors of a \"population\"\nnormalized graph Laplacian. Aside from the implication for spectral clustering,\nthis provides insight into a graph visualization technique. Our method of\nstudying the eigenvectors of random matrices is original.\n", "versions": [{"version": "v1", "created": "Fri, 9 Jul 2010 23:19:06 GMT"}, {"version": "v2", "created": "Thu, 30 Dec 2010 23:08:38 GMT"}, {"version": "v3", "created": "Tue, 13 Dec 2011 11:25:27 GMT"}], "update_date": "2011-12-14", "authors_parsed": [["Rohe", "Karl", ""], ["Chatterjee", "Sourav", ""], ["Yu", "Bin", ""]]}, {"id": "1007.2450", "submitter": "Sergey Plis", "authors": "Sergey M. Plis and Terran Lane and Vince D. Calhoun", "title": "Directional Statistics on Permutations", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML", "license": "http://creativecommons.org/licenses/by/3.0/", "abstract": "  Distributions over permutations arise in applications ranging from\nmulti-object tracking to ranking of instances. The difficulty of dealing with\nthese distributions is caused by the size of their domain, which is factorial\nin the number of considered entities ($n!$). It makes the direct definition of\na multinomial distribution over permutation space impractical for all but a\nvery small $n$. In this work we propose an embedding of all $n!$ permutations\nfor a given $n$ in a surface of a hypersphere defined in\n$\\mathbbm{R}^{(n-1)^2}$. As a result of the embedding, we acquire ability to\ndefine continuous distributions over a hypersphere with all the benefits of\ndirectional statistics. We provide polynomial time projections between the\ncontinuous hypersphere representation and the $n!$-element permutation space.\nThe framework provides a way to use continuous directional probability\ndensities and the methods developed thereof for establishing densities over\npermutations. As a demonstration of the benefits of the framework we derive an\ninference procedure for a state-space model over permutations. We demonstrate\nthe approach with applications.\n", "versions": [{"version": "v1", "created": "Wed, 14 Jul 2010 22:59:19 GMT"}], "update_date": "2010-07-16", "authors_parsed": [["Plis", "Sergey M.", ""], ["Lane", "Terran", ""], ["Calhoun", "Vince D.", ""]]}, {"id": "1007.2964", "submitter": "Andrew Nobel", "authors": "Terrence M. Adams and Andrew B. Nobel", "title": "The Gap Dimension and Uniform Laws of Large Numbers for Ergodic\n  Processes", "comments": "24 pages, submitted for publication", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.PR math.ST stat.ML stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Let F be a family of Borel measurable functions on a complete separable\nmetric space. The gap (or fat-shattering) dimension of F is a combinatorial\nquantity that measures the extent to which functions f in F can separate finite\nsets of points at a predefined resolution gamma > 0. We establish a connection\nbetween the gap dimension of F and the uniform convergence of its sample\naverages under ergodic sampling. In particular, we show that if the gap\ndimension of F at resolution gamma > 0 is finite, then for every ergodic\nprocess the sample averages of functions in F are eventually within 10 gamma of\ntheir limiting expectations uniformly over the class F. If the gap dimension of\nF is finite for every resolution gamma > 0 then the sample averages of\nfunctions in F converge uniformly to their limiting expectations. We assume\nonly that F is uniformly bounded and countable (or countably approximable). No\nsmoothness conditions are placed on F, and no assumptions beyond ergodicity are\nplaced on the sampling processes. Our results extend existing work for i.i.d.\nprocesses.\n", "versions": [{"version": "v1", "created": "Sat, 17 Jul 2010 23:41:15 GMT"}], "update_date": "2016-11-25", "authors_parsed": [["Adams", "Terrence M.", ""], ["Nobel", "Andrew B.", ""]]}, {"id": "1007.3098", "submitter": "Yiyuan She", "authors": "Yiyuan She", "title": "Reduced Rank Vector Generalized Linear Models for Feature Extraction", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Supervised linear feature extraction can be achieved by fitting a reduced\nrank multivariate model. This paper studies rank penalized and rank constrained\nvector generalized linear models. From the perspective of thresholding rules,\nwe build a framework for fitting singular value penalized models and use it for\nfeature extraction. Through solving the rank constraint form of the problem, we\npropose progressive feature space reduction for fast computation in high\ndimensions with little performance loss. A novel projective cross-validation is\nproposed for parameter tuning in such nonconvex setups. Real data applications\nare given to show the power of the methodology in supervised dimension\nreduction and feature extraction.\n", "versions": [{"version": "v1", "created": "Mon, 19 Jul 2010 09:30:19 GMT"}, {"version": "v2", "created": "Tue, 20 Dec 2011 03:43:06 GMT"}, {"version": "v3", "created": "Wed, 9 May 2012 23:07:29 GMT"}], "update_date": "2012-05-11", "authors_parsed": [["She", "Yiyuan", ""]]}, {"id": "1007.3564", "submitter": "Dacheng Tao", "authors": "Tianyi Zhou, Dacheng Tao, Xindong Wu", "title": "Manifold Elastic Net: A Unified Framework for Sparse Dimension Reduction", "comments": "33 pages, 12 figures", "journal-ref": "Journal of Data Mining and Knowledge Discovery, 2010", "doi": "10.1007/s10618-010-0182-x", "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  It is difficult to find the optimal sparse solution of a manifold learning\nbased dimensionality reduction algorithm. The lasso or the elastic net\npenalized manifold learning based dimensionality reduction is not directly a\nlasso penalized least square problem and thus the least angle regression (LARS)\n(Efron et al. \\cite{LARS}), one of the most popular algorithms in sparse\nlearning, cannot be applied. Therefore, most current approaches take indirect\nways or have strict settings, which can be inconvenient for applications. In\nthis paper, we proposed the manifold elastic net or MEN for short. MEN\nincorporates the merits of both the manifold learning based dimensionality\nreduction and the sparse learning based dimensionality reduction. By using a\nseries of equivalent transformations, we show MEN is equivalent to the lasso\npenalized least square problem and thus LARS is adopted to obtain the optimal\nsparse solution of MEN. In particular, MEN has the following advantages for\nsubsequent classification: 1) the local geometry of samples is well preserved\nfor low dimensional data representation, 2) both the margin maximization and\nthe classification error minimization are considered for sparse projection\ncalculation, 3) the projection matrix of MEN improves the parsimony in\ncomputation, 4) the elastic net penalty reduces the over-fitting problem, and\n5) the projection matrix of MEN can be interpreted psychologically and\nphysiologically. Experimental evidence on face recognition over various popular\ndatasets suggests that MEN is superior to top level dimensionality reduction\nalgorithms.\n", "versions": [{"version": "v1", "created": "Wed, 21 Jul 2010 05:50:47 GMT"}, {"version": "v2", "created": "Sat, 24 Jul 2010 03:48:30 GMT"}, {"version": "v3", "created": "Tue, 27 Jul 2010 03:01:09 GMT"}], "update_date": "2010-07-28", "authors_parsed": [["Zhou", "Tianyi", ""], ["Tao", "Dacheng", ""], ["Wu", "Xindong", ""]]}, {"id": "1007.3622", "submitter": "Alexey Koloydenko", "authors": "J\\\"uri Lember and Alexey A. Koloydenko", "title": "A generalized risk approach to path inference based on hidden Markov\n  models", "comments": "Section 5: corrected denominators of the scaled beta variables (pp.\n  27-30), => corrections in claims 1, 3, Prop. 12, bottom of Table 1. Decoder\n  (49), Corol. 14 are generalized to handle 0 probabilities. Notation is more\n  closely aligned with (Bishop, 2006). Details are inserted in eqn-s (43); the\n  positivity assumption in Prop. 11 is explicit. Fixed typing errors in\n  equation (41), Example 2", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Motivated by the unceasing interest in hidden Markov models (HMMs), this\npaper re-examines hidden path inference in these models, using primarily a\nrisk-based framework. While the most common maximum a posteriori (MAP), or\nViterbi, path estimator and the minimum error, or Posterior Decoder (PD), have\nlong been around, other path estimators, or decoders, have been either only\nhinted at or applied more recently and in dedicated applications generally\nunfamiliar to the statistical learning community. Over a decade ago, however, a\nfamily of algorithmically defined decoders aiming to hybridize the two standard\nones was proposed (Brushe et al., 1998). The present paper gives a careful\nanalysis of this hybridization approach, identifies several problems and issues\nwith it and other previously proposed approaches, and proposes practical\nresolutions of those. Furthermore, simple modifications of the classical\ncriteria for hidden path recognition are shown to lead to a new class of\ndecoders. Dynamic programming algorithms to compute these decoders in the usual\nforward-backward manner are presented. A particularly interesting subclass of\nsuch estimators can be also viewed as hybrids of the MAP and PD estimators.\nSimilar to previously proposed MAP-PD hybrids, the new class is parameterized\nby a small number of tunable parameters. Unlike their algorithmic predecessors,\nthe new risk-based decoders are more clearly interpretable, and, most\nimportantly, work \"out of the box\" in practice, which is demonstrated on some\nreal bioinformatics tasks and data. Some further generalizations and\napplications are discussed in conclusion.\n", "versions": [{"version": "v1", "created": "Wed, 21 Jul 2010 11:44:30 GMT"}, {"version": "v2", "created": "Fri, 20 Jul 2012 16:31:20 GMT"}, {"version": "v3", "created": "Wed, 14 Nov 2012 16:16:43 GMT"}, {"version": "v4", "created": "Tue, 16 Apr 2013 11:58:17 GMT"}], "update_date": "2013-04-17", "authors_parsed": [["Lember", "J\u00fcri", ""], ["Koloydenko", "Alexey A.", ""]]}, {"id": "1007.3884", "submitter": "Cassio P. de Campos", "authors": "Cassio P. de Campos", "title": "New Results for the MAP Problem in Bayesian Networks", "comments": "A couple of typos were fixed, as well as the notation in part of\n  section 4, which was misleading. Theoretical and empirical results have not\n  changed", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.CC stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper presents new results for the (partial) maximum a posteriori (MAP)\nproblem in Bayesian networks, which is the problem of querying the most\nprobable state configuration of some of the network variables given evidence.\nFirst, it is demonstrated that the problem remains hard even in networks with\nvery simple topology, such as binary polytrees and simple trees (including the\nNaive Bayes structure). Such proofs extend previous complexity results for the\nproblem. Inapproximability results are also derived in the case of trees if the\nnumber of states per variable is not bounded. Although the problem is shown to\nbe hard and inapproximable even in very simple scenarios, a new exact algorithm\nis described that is empirically fast in networks of bounded treewidth and\nbounded number of states per variable. The same algorithm is used as basis of a\nFully Polynomial Time Approximation Scheme for MAP under such assumptions.\nApproximation schemes were generally thought to be impossible for this problem,\nbut we show otherwise for classes of networks that are important in practice.\nThe algorithms are extensively tested using some well-known networks as well as\nrandom generated cases to show their effectiveness.\n", "versions": [{"version": "v1", "created": "Thu, 22 Jul 2010 13:38:17 GMT"}, {"version": "v2", "created": "Thu, 29 Jul 2010 13:54:46 GMT"}], "update_date": "2010-07-30", "authors_parsed": [["de Campos", "Cassio P.", ""]]}, {"id": "1007.4037", "submitter": "Andrew Nobel", "authors": "Terrence M. Adams and Andrew B. Nobel", "title": "Uniform Approximation and Bracketing Properties of VC classes", "comments": "10 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.PR math.ST stat.ML stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We show that the sets in a family with finite VC dimension can be uniformly\napproximated within a given error by a finite partition. Immediate corollaries\ninclude the fact that VC classes have finite bracketing numbers, satisfy\nuniform laws of averages under strong dependence, and exhibit uniform mixing.\nOur results are based on recent work concerning uniform laws of averages for VC\nclasses under ergodic sampling.\n", "versions": [{"version": "v1", "created": "Fri, 23 Jul 2010 02:52:08 GMT"}], "update_date": "2010-07-26", "authors_parsed": [["Adams", "Terrence M.", ""], ["Nobel", "Andrew B.", ""]]}, {"id": "1007.4062", "submitter": "Robert Hable", "authors": "Andreas Christmann, Robert Hable", "title": "Support Vector Machines for Additive Models: Consistency and Robustness", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Support vector machines (SVMs) are special kernel based methods and belong to\nthe most successful learning methods since more than a decade. SVMs can\ninformally be described as a kind of regularized M-estimators for functions and\nhave demonstrated their usefulness in many complicated real-life problems.\nDuring the last years a great part of the statistical research on SVMs has\nconcentrated on the question how to design SVMs such that they are universally\nconsistent and statistically robust for nonparametric classification or\nnonparametric regression purposes. In many applications, some qualitative prior\nknowledge of the distribution P or of the unknown function f to be estimated is\npresent or the prediction function with a good interpretability is desired,\nsuch that a semiparametric model or an additive model is of interest.\n  In this paper we mainly address the question how to design SVMs by choosing\nthe reproducing kernel Hilbert space (RKHS) or its corresponding kernel to\nobtain consistent and statistically robust estimators in additive models. We\ngive an explicit construction of kernels - and thus of their RKHSs - which\nleads in combination with a Lipschitz continuous loss function to consistent\nand statistically robust SMVs for additive models. Examples are quantile\nregression based on the pinball loss function, regression based on the\nepsilon-insensitive loss function, and classification based on the hinge loss\nfunction.\n", "versions": [{"version": "v1", "created": "Fri, 23 Jul 2010 07:57:04 GMT"}], "update_date": "2010-07-26", "authors_parsed": [["Christmann", "Andreas", ""], ["Hable", "Robert", ""]]}, {"id": "1007.4134", "submitter": "Svebor Karaman", "authors": "Svebor Karaman (LaBRI), Jenny Benois-Pineau (LaBRI), R\\'emi M\\'egret\n  (IMS), Vladislavs Dovgalecs (IMS), Jean-Fran\\c{c}ois Dartigues, Yann\n  Ga\\\"estel (ISPED)", "title": "Human Daily Activities Indexing in Videos from Wearable Cameras for\n  Monitoring of Patients with Dementia Diseases", "comments": null, "journal-ref": "ICPR 2010, Istanbul : Turquie (2010)", "doi": "10.1109/ICPR.2010.999", "report-no": null, "categories": "cs.MM stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Our research focuses on analysing human activities according to a known\nbehaviorist scenario, in case of noisy and high dimensional collected data. The\ndata come from the monitoring of patients with dementia diseases by wearable\ncameras. We define a structural model of video recordings based on a Hidden\nMarkov Model. New spatio-temporal features, color features and localization\nfeatures are proposed as observations. First results in recognition of\nactivities are promising.\n", "versions": [{"version": "v1", "created": "Fri, 23 Jul 2010 13:59:42 GMT"}], "update_date": "2014-05-15", "authors_parsed": [["Karaman", "Svebor", "", "LaBRI"], ["Benois-Pineau", "Jenny", "", "LaBRI"], ["M\u00e9gret", "R\u00e9mi", "", "IMS"], ["Dovgalecs", "Vladislavs", "", "IMS"], ["Dartigues", "Jean-Fran\u00e7ois", "", "ISPED"], ["Ga\u00ebstel", "Yann", "", "ISPED"]]}, {"id": "1007.5354", "submitter": "James P. Crutchfield", "authors": "James P. Crutchfield, Christopher J. Ellison, Ryan G. James, John R.\n  Mahoney", "title": "Synchronization and Control in Intrinsic and Designed Computation: An\n  Information-Theoretic Analysis of Competing Models of Stochastic Computation", "comments": "25 pages, 13 figures, 1 table", "journal-ref": null, "doi": "10.1063/1.3489888", "report-no": null, "categories": "cond-mat.stat-mech cs.IT math.IT nlin.CD stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We adapt tools from information theory to analyze how an observer comes to\nsynchronize with the hidden states of a finitary, stationary stochastic\nprocess. We show that synchronization is determined by both the process's\ninternal organization and by an observer's model of it. We analyze these\ncomponents using the convergence of state-block and block-state entropies,\ncomparing them to the previously known convergence properties of the Shannon\nblock entropy. Along the way, we introduce a hierarchy of information\nquantifiers as derivatives and integrals of these entropies, which parallels a\nsimilar hierarchy introduced for block entropy. We also draw out the duality\nbetween synchronization properties and a process's controllability. The tools\nlead to a new classification of a process's alternative representations in\nterms of minimality, synchronizability, and unifilarity.\n", "versions": [{"version": "v1", "created": "Fri, 30 Jul 2010 00:37:42 GMT"}], "update_date": "2015-05-19", "authors_parsed": [["Crutchfield", "James P.", ""], ["Ellison", "Christopher J.", ""], ["James", "Ryan G.", ""], ["Mahoney", "John R.", ""]]}]