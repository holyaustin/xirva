[{"id": "1101.0240", "submitter": "Andrew Wilson", "authors": "Andrew Gordon Wilson, Zoubin Ghahramani", "title": "Generalised Wishart Processes", "comments": "14 pages, 4 figures, 1 table. Submitted for publication", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME math.PR q-fin.CP q-fin.ST stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We introduce a stochastic process with Wishart marginals: the generalised\nWishart process (GWP). It is a collection of positive semi-definite random\nmatrices indexed by any arbitrary dependent variable. We use it to model\ndynamic (e.g. time varying) covariance matrices. Unlike existing models, it can\ncapture a diverse class of covariance structures, it can easily handle missing\ndata, the dependent variable can readily include covariates other than time,\nand it scales well with dimension; there is no need for free parameters, and\noptional parameters are easy to interpret. We describe how to construct the\nGWP, introduce general procedures for inference and predictions, and show that\nit outperforms its main competitor, multivariate GARCH, even on financial data\nthat especially suits GARCH. We also show how to predict the mean of a\nmultivariate process while accounting for dynamic correlations.\n", "versions": [{"version": "v1", "created": "Fri, 31 Dec 2010 12:25:01 GMT"}], "update_date": "2011-01-04", "authors_parsed": [["Wilson", "Andrew Gordon", ""], ["Ghahramani", "Zoubin", ""]]}, {"id": "1101.0316", "submitter": "Amit Mishra", "authors": "Amit Kumar Mishra and Bernard Mulgrew", "title": "Bistatic SAR ATR", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  With the present revival of interest in bistatic radar systems, research in\nthat area has gained momentum. Given some of the strategic advantages for a\nbistatic configuration, and tech- nological advances in the past few years,\nlarge-scale implementation of the bistatic systems is a scope for the near\nfuture. If the bistatic systems are to replace the monostatic systems (at least\npar- tially), then all the existing usages of a monostatic system should be\nmanageable in a bistatic system. A detailed investigation of the possibilities\nof an automatic target recognition (ATR) facil- ity in a bistatic radar system\nis presented. Because of the lack of data, experiments were carried out on\nsimulated data. Still, the results are positive and make a positive case for\nthe introduction of the bistatic configuration. First, it was found that,\ncontrary to the popular expectation that the bistatic ATR performance might be\nsubstantially worse than the monostatic ATR performance, the bistatic ATR\nperformed fairly well (though not better than the monostatic ATR). Second, the\nATR per- formance does not deteriorate substantially with increasing bistatic\nangle. Last, the polarimetric data from bistatic scattering were found to have\ndistinct information, contrary to expert opinions. Along with these results,\nsuggestions were also made about how to stabilise the bistatic-ATR per-\nformance with changing bistatic angle. Finally, a new fast and robust ATR\nalgorithm (developed in the present work) has been presented.\n", "versions": [{"version": "v1", "created": "Sat, 1 Jan 2011 04:36:35 GMT"}], "update_date": "2011-01-04", "authors_parsed": [["Mishra", "Amit Kumar", ""], ["Mulgrew", "Bernard", ""]]}, {"id": "1101.0317", "submitter": "Amit Mishra", "authors": "Amit Kumar Mishra and Bernard Mulgrew", "title": "Generation of SAR Image for Real-life Objects using General Purpose EM\n  Simulators", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In the applications related to airborne radars, simulation has always played\nan important role. This is mainly because of the two fold reason of the\nunavailability of desired data and the difficulty associated with the\ncollection of data under controlled environment. A simple example will be\nregarding the collection of pure multipolar radar data. Even after phenomenal\ndevelopment in the field of radar hardware design and signal processing, till\nnow the collection of pure multipolar data is a challenge for the radar system\ndesigners. Till very recently, the power of computer simulation of radar signal\nreturn was available to a very selected few. This was because of the heavy cost\nassociated with some of the main line electro magnetic (EM) simulators for\nradar signal simulation, and secondly because many such EM simulators are for\nrestricted marketting. However, because of the fast progress made in the field\nof EM simulation, many of the current generic EM simulators can be used to\nsimulate radar returns from realistic targets. The current article expounds the\nsteps towards generating a synthetic aperture radar (SAR) image database of\nground targets, using a eneric EM g simulator. It also demonstrates by the help\nof some example images, the quality of the SAR mage generated i using a general\npurpose EM simulator.\n", "versions": [{"version": "v1", "created": "Sat, 1 Jan 2011 04:42:07 GMT"}], "update_date": "2011-01-04", "authors_parsed": [["Mishra", "Amit Kumar", ""], ["Mulgrew", "Bernard", ""]]}, {"id": "1101.0434", "submitter": "Stephane Chretien", "authors": "St\\'ephane Chr\\'etien and S\\'ebastien Darses", "title": "Sparse recovery with unknown variance: a LASSO-type approach", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.ML stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We address the issue of estimating the regression vector $\\beta$ in the\ngeneric $s$-sparse linear model $y = X\\beta+z$, with $\\beta\\in\\R^{p}$,\n$y\\in\\R^{n}$, $z\\sim\\mathcal N(0,\\sg^2 I)$ and $p> n$ when the variance\n$\\sg^{2}$ is unknown. We study two LASSO-type methods that jointly estimate\n$\\beta$ and the variance. These estimators are minimizers of the $\\ell_1$\npenalized least-squares functional, where the relaxation parameter is tuned\naccording to two different strategies. In the first strategy, the relaxation\nparameter is of the order $\\ch{\\sigma} \\sqrt{\\log p}$, where $\\ch{\\sigma}^2$ is\nthe empirical variance. %The resulting optimization problem can be solved by\nrunning only a few successive LASSO instances with %recursive updating of the\nrelaxation parameter. In the second strategy, the relaxation parameter is\nchosen so as to enforce a trade-off between the fidelity and the penalty terms\nat optimality. For both estimators, our assumptions are similar to the ones\nproposed by Cand\\`es and Plan in {\\it Ann. Stat. (2009)}, for the case where\n$\\sg^{2}$ is known. We prove that our estimators ensure exact recovery of the\nsupport and sign pattern of $\\beta$ with high probability. We present\nsimulations results showing that the first estimator enjoys nearly the same\nperformances in practice as the standard LASSO (known variance case) for a wide\nrange of the signal to noise ratio. Our second estimator is shown to outperform\nboth in terms of false detection, when the signal to noise ratio is low.\n", "versions": [{"version": "v1", "created": "Sun, 2 Jan 2011 21:40:26 GMT"}, {"version": "v2", "created": "Tue, 15 Mar 2011 22:57:33 GMT"}, {"version": "v3", "created": "Thu, 12 May 2011 22:30:22 GMT"}, {"version": "v4", "created": "Sat, 28 Jan 2012 00:06:14 GMT"}, {"version": "v5", "created": "Mon, 5 Nov 2012 15:37:51 GMT"}], "update_date": "2012-11-06", "authors_parsed": [["Chr\u00e9tien", "St\u00e9phane", ""], ["Darses", "S\u00e9bastien", ""]]}, {"id": "1101.0632", "submitter": "Doug Speed", "authors": "Doug Speed, Simon Tavar\\'e", "title": "Sparse Partitioning: Nonlinear regression with binary or tertiary\n  predictors, with application to association studies", "comments": "Published in at http://dx.doi.org/10.1214/10-AOAS411 the Annals of\n  Applied Statistics (http://www.imstat.org/aoas/) by the Institute of\n  Mathematical Statistics (http://www.imstat.org)", "journal-ref": "Annals of Applied Statistics 2011, Vol. 5, No. 2A, 873-893", "doi": "10.1214/10-AOAS411", "report-no": "IMS-AOAS-AOAS411", "categories": "q-bio.QM stat.AP stat.ME stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper presents Sparse Partitioning, a Bayesian method for identifying\npredictors that either individually or in combination with others affect a\nresponse variable. The method is designed for regression problems involving\nbinary or tertiary predictors and allows the number of predictors to exceed the\nsize of the sample, two properties which make it well suited for association\nstudies. Sparse Partitioning differs from other regression methods by placing\nno restrictions on how the predictors may influence the response. To compensate\nfor this generality, Sparse Partitioning implements a novel way of exploring\nthe model space. It searches for high posterior probability partitions of the\npredictor set, where each partition defines groups of predictors that jointly\ninfluence the response. The result is a robust method that requires no prior\nknowledge of the true predictor--response relationship. Testing on simulated\ndata suggests Sparse Partitioning will typically match the performance of an\nexisting method on a data set which obeys the existing method's model\nassumptions. When these assumptions are violated, Sparse Partitioning will\ngenerally offer superior performance.\n", "versions": [{"version": "v1", "created": "Tue, 4 Jan 2011 00:43:09 GMT"}, {"version": "v2", "created": "Tue, 30 Aug 2011 06:11:26 GMT"}], "update_date": "2011-08-31", "authors_parsed": [["Speed", "Doug", ""], ["Tavar\u00e9", "Simon", ""]]}, {"id": "1101.0673", "submitter": "Marco Cuturi", "authors": "Marco Cuturi, Arnaud Doucet", "title": "Autoregressive Kernels For Time Series", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose in this work a new family of kernels for variable-length time\nseries. Our work builds upon the vector autoregressive (VAR) model for\nmultivariate stochastic processes: given a multivariate time series x, we\nconsider the likelihood function p_{\\theta}(x) of different parameters \\theta\nin the VAR model as features to describe x. To compare two time series x and\nx', we form the product of their features p_{\\theta}(x) p_{\\theta}(x') which is\nintegrated out w.r.t \\theta using a matrix normal-inverse Wishart prior. Among\nother properties, this kernel can be easily computed when the dimension d of\nthe time series is much larger than the lengths of the considered time series x\nand x'. It can also be generalized to time series taking values in arbitrary\nstate spaces, as long as the state space itself is endowed with a kernel\n\\kappa. In that case, the kernel between x and x' is a a function of the Gram\nmatrices produced by \\kappa on observations and subsequences of observations\nenumerated in x and x'. We describe a computationally efficient implementation\nof this generalization that uses low-rank matrix factorization techniques.\nThese kernels are compared to other known kernels using a set of benchmark\nclassification tasks carried out with support vector machines.\n", "versions": [{"version": "v1", "created": "Tue, 4 Jan 2011 08:44:14 GMT"}], "update_date": "2011-01-05", "authors_parsed": [["Cuturi", "Marco", ""], ["Doucet", "Arnaud", ""]]}, {"id": "1101.1057", "submitter": "Sebastien Gerchinovitz", "authors": "S\\'ebastien Gerchinovitz (DMA, INRIA Paris - Rocquencourt)", "title": "Sparsity regret bounds for individual sequences in online linear\n  regression", "comments": "Published in Journal of Machine Learning Research at\n  http://www.jmlr.org/papers/volume14/gerchinovitz13a/gerchinovitz13a.pdf", "journal-ref": "Journal of Machine Learning Research 14 (2011) 729-769", "doi": null, "report-no": "RR-7504", "categories": "stat.ML cs.LG math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider the problem of online linear regression on arbitrary\ndeterministic sequences when the ambient dimension d can be much larger than\nthe number of time rounds T. We introduce the notion of sparsity regret bound,\nwhich is a deterministic online counterpart of recent risk bounds derived in\nthe stochastic setting under a sparsity scenario. We prove such regret bounds\nfor an online-learning algorithm called SeqSEW and based on exponential\nweighting and data-driven truncation. In a second part we apply a\nparameter-free version of this algorithm to the stochastic setting (regression\nmodel with random design). This yields risk bounds of the same flavor as in\nDalalyan and Tsybakov (2011) but which solve two questions left open therein.\nIn particular our risk bounds are adaptive (up to a logarithmic factor) to the\nunknown variance of the noise if the latter is Gaussian. We also address the\nregression model with fixed design.\n", "versions": [{"version": "v1", "created": "Wed, 5 Jan 2011 19:43:37 GMT"}, {"version": "v2", "created": "Sun, 18 Mar 2012 06:52:57 GMT"}, {"version": "v3", "created": "Fri, 12 Apr 2013 10:33:39 GMT"}], "update_date": "2013-04-17", "authors_parsed": [["Gerchinovitz", "S\u00e9bastien", "", "DMA, INRIA Paris - Rocquencourt"]]}, {"id": "1101.1715", "submitter": "Jose M. Pe\\~na", "authors": "Jose M. Pe\\~na", "title": "Finding Consensus Bayesian Network Structures", "comments": "Changes from v3 to v4: Section 1 has been extended with more\n  motivation and a review of the literature. Theorem 3 proves that CONSENSUS is\n  not only NP-hard but NP-complete. A flaw in Theorem 4 has been fixed. The\n  proof of Theorem 5 has been re-written from scratch. Now, it is\n  self-contained, i.e. it does not rely upon the algorithm by Chickering (2004)", "journal-ref": "Journal of Artificial Intelligence Research, 42, 661-687, 2011", "doi": null, "report-no": null, "categories": "stat.ML cs.AI math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Suppose that multiple experts (or learning algorithms) provide us with\nalternative Bayesian network (BN) structures over a domain, and that we are\ninterested in combining them into a single consensus BN structure.\nSpecifically, we are interested in that the consensus BN structure only\nrepresents independences all the given BN structures agree upon and that it has\nas few parameters associated as possible. In this paper, we prove that there\nmay exist several non-equivalent consensus BN structures and that finding one\nof them is NP-hard. Thus, we decide to resort to heuristics to find an\napproximated consensus BN structure. In this paper, we consider the heuristic\nproposed in\n\\citep{MatzkevichandAbramson1992,MatzkevichandAbramson1993a,MatzkevichandAbramson1993b}.\nThis heuristic builds upon two algorithms, called Methods A and B, for\nefficiently deriving the minimal directed independence map of a BN structure\nrelative to a given node ordering. Methods A and B are claimed to be correct\nalthough no proof is provided (a proof is just sketched). In this paper, we\nshow that Methods A and B are not correct and propose a correction of them.\n", "versions": [{"version": "v1", "created": "Mon, 10 Jan 2011 07:41:50 GMT"}, {"version": "v2", "created": "Mon, 24 Jan 2011 16:54:15 GMT"}, {"version": "v3", "created": "Sat, 26 Feb 2011 22:15:07 GMT"}, {"version": "v4", "created": "Mon, 11 Jul 2011 15:57:34 GMT"}], "update_date": "2015-03-17", "authors_parsed": [["Pe\u00f1a", "Jose M.", ""]]}, {"id": "1101.2017", "submitter": "Emily Fox", "authors": "Emily Fox and David Dunson", "title": "Bayesian Nonparametric Covariance Regression", "comments": "33 pages, 8 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Although there is a rich literature on methods for allowing the variance in a\nunivariate regression model to vary with predictors, time and other factors,\nrelatively little has been done in the multivariate case. Our focus is on\ndeveloping a class of nonparametric covariance regression models, which allow\nan unknown p x p covariance matrix to change flexibly with predictors. The\nproposed modeling framework induces a prior on a collection of covariance\nmatrices indexed by predictors through priors for predictor-dependent loadings\nmatrices in a factor model. In particular, the predictor-dependent loadings are\ncharacterized as a sparse combination of a collection of unknown dictionary\nfunctions (e.g, Gaussian process random functions). The induced covariance is\nthen a regularized quadratic function of these dictionary elements. Our\nproposed framework leads to a highly-flexible, but computationally tractable\nformulation with simple conjugate posterior updates that can readily handle\nmissing data. Theoretical properties are discussed and the methods are\nillustrated through simulations studies and an application to the Google Flu\nTrends data.\n", "versions": [{"version": "v1", "created": "Tue, 11 Jan 2011 04:25:21 GMT"}, {"version": "v2", "created": "Tue, 8 Feb 2011 23:44:49 GMT"}], "update_date": "2015-03-17", "authors_parsed": [["Fox", "Emily", ""], ["Dunson", "David", ""]]}, {"id": "1101.2374", "submitter": "Charles Bouveyron", "authors": "Charles Bouveyron and Camille Brunet", "title": "Simultaneous model-based clustering and visualization in the Fisher\n  discriminative subspace", "comments": null, "journal-ref": "Statistics and Computing, 2011", "doi": "10.1007/s11222-011-9249-9", "report-no": null, "categories": "stat.ME stat.AP stat.CO stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Clustering in high-dimensional spaces is nowadays a recurrent problem in many\nscientific domains but remains a difficult task from both the clustering\naccuracy and the result understanding points of view. This paper presents a\ndiscriminative latent mixture (DLM) model which fits the data in a latent\northonormal discriminative subspace with an intrinsic dimension lower than the\ndimension of the original space. By constraining model parameters within and\nbetween groups, a family of 12 parsimonious DLM models is exhibited which\nallows to fit onto various situations. An estimation algorithm, called the\nFisher-EM algorithm, is also proposed for estimating both the mixture\nparameters and the discriminative subspace. Experiments on simulated and real\ndatasets show that the proposed approach performs better than existing\nclustering methods while providing a useful representation of the clustered\ndata. The method is as well applied to the clustering of mass spectrometry\ndata.\n", "versions": [{"version": "v1", "created": "Wed, 12 Jan 2011 14:40:06 GMT"}, {"version": "v2", "created": "Tue, 19 Apr 2011 08:21:44 GMT"}], "update_date": "2011-04-20", "authors_parsed": [["Bouveyron", "Charles", ""], ["Brunet", "Camille", ""]]}, {"id": "1101.2489", "submitter": "Shohei Shimizu", "authors": "Shohei Shimizu, Takanori Inazumi, Yasuhiro Sogawa, Aapo Hyvarinen,\n  Yoshinobu Kawahara, Takashi Washio, Patrik O. Hoyer, Kenneth Bollen", "title": "DirectLiNGAM: A direct method for learning a linear non-Gaussian\n  structural equation model", "comments": "A revised version of this was accepted in Journal of Machine Learning\n  Research", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Structural equation models and Bayesian networks have been widely used to\nanalyze causal relations between continuous variables. In such frameworks,\nlinear acyclic models are typically used to model the data-generating process\nof variables. Recently, it was shown that use of non-Gaussianity identifies the\nfull structure of a linear acyclic model, i.e., a causal ordering of variables\nand their connection strengths, without using any prior knowledge on the\nnetwork structure, which is not the case with conventional methods. However,\nexisting estimation methods are based on iterative search algorithms and may\nnot converge to a correct solution in a finite number of steps. In this paper,\nwe propose a new direct method to estimate a causal ordering and connection\nstrengths based on non-Gaussianity.\n  In contrast to the previous methods, our algorithm requires no algorithmic\nparameters and is guaranteed to converge to the right solution within a small\nfixed number of steps if the data strictly follows the model.\n", "versions": [{"version": "v1", "created": "Thu, 13 Jan 2011 03:58:47 GMT"}, {"version": "v2", "created": "Fri, 14 Jan 2011 03:01:03 GMT"}, {"version": "v3", "created": "Thu, 7 Apr 2011 09:08:19 GMT"}], "update_date": "2011-04-08", "authors_parsed": [["Shimizu", "Shohei", ""], ["Inazumi", "Takanori", ""], ["Sogawa", "Yasuhiro", ""], ["Hyvarinen", "Aapo", ""], ["Kawahara", "Yoshinobu", ""], ["Washio", "Takashi", ""], ["Hoyer", "Patrik O.", ""], ["Bollen", "Kenneth", ""]]}, {"id": "1101.3462", "submitter": "Nicolas Dobigeon", "authors": "Olivier Besson, Nicolas Dobigeon and Jean-Yves Tourneret", "title": "Minimum mean square distance estimation of a subspace", "comments": null, "journal-ref": null, "doi": "10.1109/TSP.2011.2166548", "report-no": null, "categories": "stat.ME stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider the problem of subspace estimation in a Bayesian setting. Since\nwe are operating in the Grassmann manifold, the usual approach which consists\nof minimizing the mean square error (MSE) between the true subspace $U$ and its\nestimate $\\hat{U}$ may not be adequate as the MSE is not the natural metric in\nthe Grassmann manifold. As an alternative, we propose to carry out subspace\nestimation by minimizing the mean square distance (MSD) between $U$ and its\nestimate, where the considered distance is a natural metric in the Grassmann\nmanifold, viz. the distance between the projection matrices. We show that the\nresulting estimator is no longer the posterior mean of $U$ but entails\ncomputing the principal eigenvectors of the posterior mean of $U U^{T}$.\nDerivation of the MMSD estimator is carried out in a few illustrative examples\nincluding a linear Gaussian model for the data and a Bingham or von Mises\nFisher prior distribution for $U$. In all scenarios, posterior distributions\nare derived and the MMSD estimator is obtained either analytically or\nimplemented via a Markov chain Monte Carlo simulation method. The method is\nshown to provide accurate estimates even when the number of samples is lower\nthan the dimension of $U$. An application to hyperspectral imagery is finally\ninvestigated.\n", "versions": [{"version": "v1", "created": "Tue, 18 Jan 2011 14:33:22 GMT"}], "update_date": "2015-05-27", "authors_parsed": [["Besson", "Olivier", ""], ["Dobigeon", "Nicolas", ""], ["Tourneret", "Jean-Yves", ""]]}, {"id": "1101.3501", "submitter": "Adam D. Bull", "authors": "Adam D. Bull", "title": "Convergence rates of efficient global optimization algorithms", "comments": null, "journal-ref": "Journal of Machine Learning Research 12:2879-2904, 2011", "doi": null, "report-no": null, "categories": "stat.ML math.OC math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Efficient global optimization is the problem of minimizing an unknown\nfunction f, using as few evaluations f(x) as possible. It can be considered as\na continuum-armed bandit problem, with noiseless data and simple regret.\nExpected improvement is perhaps the most popular method for solving this\nproblem; the algorithm performs well in experiments, but little is known about\nits theoretical properties. Implementing expected improvement requires a choice\nof Gaussian process prior, which determines an associated space of functions,\nits reproducing-kernel Hilbert space (RKHS). When the prior is fixed, expected\nimprovement is known to converge on the minimum of any function in the RKHS. We\nbegin by providing convergence rates for this procedure. The rates are optimal\nfor functions of low smoothness, and we modify the algorithm to attain optimal\nrates for smoother functions. For practitioners, however, these results are\nsomewhat misleading. Priors are typically not held fixed, but depend on\nparameters estimated from the data. For standard estimators, we show this\nprocedure may never discover the minimum of f. We then propose alternative\nestimators, chosen to minimize the constants in the rate of convergence, and\nshow these estimators retain the convergence rates of a fixed prior.\n", "versions": [{"version": "v1", "created": "Tue, 18 Jan 2011 17:04:18 GMT"}, {"version": "v2", "created": "Tue, 15 Feb 2011 15:06:22 GMT"}, {"version": "v3", "created": "Sat, 22 Oct 2011 10:35:49 GMT"}], "update_date": "2013-02-19", "authors_parsed": [["Bull", "Adam D.", ""]]}, {"id": "1101.3594", "submitter": "Donghui Yan", "authors": "Donghui Yan, Peng Gong, Aiyou Chen and Liheng Zhong", "title": "Classification under Data Contamination with Application to Remote\n  Sensing Image Mis-registration", "comments": "23 pages, 10 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This work is motivated by the problem of image mis-registration in remote\nsensing and we are interested in determining the resulting loss in the accuracy\nof pattern classification. A statistical formulation is given where we propose\nto use data contamination to model and understand the phenomenon of image\nmis-registration. This model is widely applicable to many other types of errors\nas well, for example, measurement errors and gross errors etc. The impact of\ndata contamination on classification is studied under a statistical learning\ntheoretical framework. A closed-form asymptotic bound is established for the\nresulting loss in classification accuracy, which is less than\n$\\epsilon/(1-\\epsilon)$ for data contamination of an amount of $\\epsilon$. Our\nbound is sharper than similar bounds in the domain adaptation literature and,\nunlike such bounds, it applies to classifiers with an infinite\nVapnik-Chervonekis (VC) dimension. Extensive simulations have been conducted on\nboth synthetic and real datasets under various types of data contamination,\nincluding label flipping, feature swapping and the replacement of feature\nvalues with data generated from a random source such as a Gaussian or Cauchy\ndistribution. Our simulation results show that the bound we derive is fairly\ntight.\n", "versions": [{"version": "v1", "created": "Wed, 19 Jan 2011 00:41:43 GMT"}, {"version": "v2", "created": "Thu, 5 Jan 2012 18:04:10 GMT"}], "update_date": "2015-03-17", "authors_parsed": [["Yan", "Donghui", ""], ["Gong", "Peng", ""], ["Chen", "Aiyou", ""], ["Zhong", "Liheng", ""]]}, {"id": "1101.3712", "submitter": "Alexander Sch\\\"onhuth", "authors": "Alexander Sch\\\"onhuth", "title": "Generic identification of binary-valued hidden Markov processes", "comments": "28 pages", "journal-ref": "Journal of Algebraic Statistics, 5(1), 72-99, 2014", "doi": null, "report-no": null, "categories": "math.ST math.AG stat.ML stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The generic identification problem is to decide whether a stochastic process\n$(X_t)$ is a hidden Markov process and if yes to infer its parameters for all\nbut a subset of parametrizations that form a lower-dimensional subvariety in\nparameter space. Partial answers so far available depend on extra assumptions\non the processes, which are usually centered around stationarity. Here we\npresent a general solution for binary-valued hidden Markov processes. Our\napproach is rooted in algebraic statistics hence it is geometric in nature. We\nfind that the algebraic varieties associated with the probability distributions\nof binary-valued hidden Markov processes are zero sets of determinantal\nequations which draws a connection to well-studied objects from algebra. As a\nconsequence, our solution allows for algorithmic implementation based on\nelementary (linear) algebraic routines.\n", "versions": [{"version": "v1", "created": "Wed, 19 Jan 2011 15:57:58 GMT"}, {"version": "v2", "created": "Mon, 24 Jan 2011 16:27:38 GMT"}, {"version": "v3", "created": "Fri, 11 Feb 2011 14:03:15 GMT"}, {"version": "v4", "created": "Fri, 2 Sep 2011 12:58:30 GMT"}, {"version": "v5", "created": "Mon, 6 Feb 2012 11:40:00 GMT"}, {"version": "v6", "created": "Tue, 22 Oct 2013 15:20:17 GMT"}], "update_date": "2015-01-14", "authors_parsed": [["Sch\u00f6nhuth", "Alexander", ""]]}, {"id": "1101.4179", "submitter": "Herv\\'e Cardot", "authors": "Herv\\'e Cardot, Peggy C\\'enac, Jean-Marie Monnez", "title": "A fast and recursive algorithm for clustering large datasets with\n  $k$-medians", "comments": "Under revision for Computational Statistics and Data Analysis", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.CO stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Clustering with fast algorithms large samples of high dimensional data is an\nimportant challenge in computational statistics. Borrowing ideas from MacQueen\n(1967) who introduced a sequential version of the $k$-means algorithm, a new\nclass of recursive stochastic gradient algorithms designed for the $k$-medians\nloss criterion is proposed. By their recursive nature, these algorithms are\nvery fast and are well adapted to deal with large samples of data that are\nallowed to arrive sequentially. It is proved that the stochastic gradient\nalgorithm converges almost surely to the set of stationary points of the\nunderlying loss criterion. A particular attention is paid to the averaged\nversions, which are known to have better performances, and a data-driven\nprocedure that allows automatic selection of the value of the descent step is\nproposed.\n  The performance of the averaged sequential estimator is compared on a\nsimulation study, both in terms of computation speed and accuracy of the\nestimations, with more classical partitioning techniques such as $k$-means,\ntrimmed $k$-means and PAM (partitioning around medoids). Finally, this new\nonline clustering technique is illustrated on determining television audience\nprofiles with a sample of more than 5000 individual television audiences\nmeasured every minute over a period of 24 hours.\n", "versions": [{"version": "v1", "created": "Fri, 21 Jan 2011 16:48:51 GMT"}, {"version": "v2", "created": "Mon, 18 Jul 2011 10:54:46 GMT"}, {"version": "v3", "created": "Tue, 18 Oct 2011 12:19:17 GMT"}], "update_date": "2015-03-17", "authors_parsed": [["Cardot", "Herv\u00e9", ""], ["C\u00e9nac", "Peggy", ""], ["Monnez", "Jean-Marie", ""]]}, {"id": "1101.4388", "submitter": "Guohui Song", "authors": "Guohui Song, Haizhang Zhang, Fred J. Hickernell", "title": "Reproducing Kernel Banach Spaces with the l1 Norm", "comments": "28 pages, an extra section was added", "journal-ref": "Appl. Comput. Harmon. Anal., 34:96-116, 2013", "doi": "10.1016/j.acha.2012.03.009", "report-no": null, "categories": "stat.ML cs.LG math.FA", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Targeting at sparse learning, we construct Banach spaces B of functions on an\ninput space X with the properties that (1) B possesses an l1 norm in the sense\nthat it is isometrically isomorphic to the Banach space of integrable functions\non X with respect to the counting measure; (2) point evaluations are continuous\nlinear functionals on B and are representable through a bilinear form with a\nkernel function; (3) regularized learning schemes on B satisfy the linear\nrepresenter theorem. Examples of kernel functions admissible for the\nconstruction of such spaces are given.\n", "versions": [{"version": "v1", "created": "Sun, 23 Jan 2011 16:57:03 GMT"}, {"version": "v2", "created": "Wed, 26 Jan 2011 04:56:10 GMT"}, {"version": "v3", "created": "Wed, 28 Mar 2012 18:46:47 GMT"}], "update_date": "2015-01-16", "authors_parsed": [["Song", "Guohui", ""], ["Zhang", "Haizhang", ""], ["Hickernell", "Fred J.", ""]]}, {"id": "1101.4439", "submitter": "Haizhang Zhang", "authors": "Guohui Song, Haizhang Zhang", "title": "Reproducing Kernel Banach Spaces with the l1 Norm II: Error Analysis for\n  Regularized Least Square Regression", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG math.FA", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A typical approach in estimating the learning rate of a regularized learning\nscheme is to bound the approximation error by the sum of the sampling error,\nthe hypothesis error and the regularization error. Using a reproducing kernel\nspace that satisfies the linear representer theorem brings the advantage of\ndiscarding the hypothesis error from the sum automatically. Following this\ndirection, we illustrate how reproducing kernel Banach spaces with the l1 norm\ncan be applied to improve the learning rate estimate of l1-regularization in\nmachine learning.\n", "versions": [{"version": "v1", "created": "Mon, 24 Jan 2011 03:39:57 GMT"}, {"version": "v2", "created": "Thu, 27 Jan 2011 14:45:29 GMT"}], "update_date": "2011-01-28", "authors_parsed": [["Song", "Guohui", ""], ["Zhang", "Haizhang", ""]]}, {"id": "1101.4657", "submitter": "Peter Orbanz", "authors": "Peter Orbanz", "title": "Projective Limit Random Probabilities on Polish Spaces", "comments": "20 pages, 3 figures. Published in the Electronic Journal of\n  Statistics by the Institute of Mathematical Statistics", "journal-ref": "Electronic Journal of Statistics 2011, Vol. 5, 1354-1373", "doi": "10.1214/11-EJS641", "report-no": null, "categories": "math.ST stat.ML stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A pivotal problem in Bayesian nonparametrics is the construction of prior\ndistributions on the space M(V) of probability measures on a given domain V. In\nprinciple, such distributions on the infinite-dimensional space M(V) can be\nconstructed from their finite-dimensional marginals---the most prominent\nexample being the construction of the Dirichlet process from finite-dimensional\nDirichlet distributions. This approach is both intuitive and applicable to the\nconstruction of arbitrary distributions on M(V), but also hamstrung by a number\nof technical difficulties. We show how these difficulties can be resolved if\nthe domain V is a Polish topological space, and give a representation theorem\ndirectly applicable to the construction of any probability distribution on M(V)\nwhose first moment measure is well-defined. The proof draws on a projective\nlimit theorem of Bochner, and on properties of set functions on Polish spaces\nto establish countable additivity of the resulting random probabilities.\n", "versions": [{"version": "v1", "created": "Mon, 24 Jan 2011 21:03:11 GMT"}, {"version": "v2", "created": "Mon, 4 Apr 2011 18:26:39 GMT"}, {"version": "v3", "created": "Wed, 19 Oct 2011 11:00:46 GMT"}], "update_date": "2011-10-20", "authors_parsed": [["Orbanz", "Peter", ""]]}, {"id": "1101.5008", "submitter": "Anne-Claire Haury", "authors": "Anne-Claire Haury (CBIO), Pierre Gestraud, Jean-Philippe Vert (CBIO)", "title": "The influence of feature selection methods on accuracy, stability and\n  interpretability of molecular signatures", "comments": null, "journal-ref": "PLoS ONE (2011) 6(12): e28210", "doi": "10.1371/journal.pone.0028210", "report-no": null, "categories": "q-bio.QM stat.AP stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Motivation: Biomarker discovery from high-dimensional data is a crucial\nproblem with enormous applications in biology and medicine. It is also\nextremely challenging from a statistical viewpoint, but surprisingly few\nstudies have investigated the relative strengths and weaknesses of the plethora\nof existing feature selection methods. Methods: We compare 32 feature selection\nmethods on 4 public gene expression datasets for breast cancer prognosis, in\nterms of predictive performance, stability and functional interpretability of\nthe signatures they produce. Results: We observe that the feature selection\nmethod has a significant influence on the accuracy, stability and\ninterpretability of signatures. Simple filter methods generally outperform more\ncomplex embedded or wrapper methods, and ensemble feature selection has\ngenerally no positive effect. Overall a simple Student's t-test seems to\nprovide the best results. Availability: Code and data are publicly available at\nhttp://cbio.ensmp.fr/~ahaury/.\n", "versions": [{"version": "v1", "created": "Wed, 26 Jan 2011 09:04:05 GMT"}, {"version": "v2", "created": "Thu, 23 Jun 2011 07:17:10 GMT"}], "update_date": "2012-09-17", "authors_parsed": [["Haury", "Anne-Claire", "", "CBIO"], ["Gestraud", "Pierre", "", "CBIO"], ["Vert", "Jean-Philippe", "", "CBIO"]]}, {"id": "1101.5184", "submitter": "Marco Scutari", "authors": "Marco Scutari and Adriana Brogini", "title": "Bayesian Network Structure Learning with Permutation Tests", "comments": "13 pages, 4 figures. Presented at the Conference 'Statistics for\n  Complex Problems', Padova, June 15, 2010", "journal-ref": "Communications in Statistics - Theory and Methods 2012, 42(16-17):\n  3233-3243", "doi": "10.1080/03610926.2011.593284", "report-no": null, "categories": "stat.ML stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In literature there are several studies on the performance of Bayesian\nnetwork structure learning algorithms. The focus of these studies is almost\nalways the heuristics the learning algorithms are based on, i.e. the\nmaximisation algorithms (in score-based algorithms) or the techniques for\nlearning the dependencies of each variable (in constraint-based algorithms). In\nthis paper we investigate how the use of permutation tests instead of\nparametric ones affects the performance of Bayesian network structure learning\nfrom discrete data. Shrinkage tests are also covered to provide a broad\noverview of the techniques developed in current literature.\n", "versions": [{"version": "v1", "created": "Thu, 27 Jan 2011 00:12:18 GMT"}, {"version": "v2", "created": "Sat, 26 Mar 2011 09:41:51 GMT"}, {"version": "v3", "created": "Sun, 26 Aug 2012 18:12:19 GMT"}], "update_date": "2012-08-28", "authors_parsed": [["Scutari", "Marco", ""], ["Brogini", "Adriana", ""]]}, {"id": "1101.5435", "submitter": "Daniel Ting", "authors": "Daniel Ting, Ling Huang, and Michael Jordan", "title": "An Analysis of the Convergence of Graph Laplacians", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Existing approaches to analyzing the asymptotics of graph Laplacians\ntypically assume a well-behaved kernel function with smoothness assumptions. We\nremove the smoothness assumption and generalize the analysis of graph\nLaplacians to include previously unstudied graphs including kNN graphs. We also\nintroduce a kernel-free framework to analyze graph constructions with shrinking\nneighborhoods in general and apply it to analyze locally linear embedding\n(LLE). We also describe how for a given limiting Laplacian operator desirable\nproperties such as a convergent spectrum and sparseness can be achieved\nchoosing the appropriate graph construction.\n", "versions": [{"version": "v1", "created": "Fri, 28 Jan 2011 03:32:01 GMT"}], "update_date": "2011-01-31", "authors_parsed": [["Ting", "Daniel", ""], ["Huang", "Ling", ""], ["Jordan", "Michael", ""]]}, {"id": "1101.5734", "submitter": "Yilun Chen", "authors": "Yilun Chen, Alfred O. Hero III", "title": "Recursive $\\ell_{1,\\infty}$ Group lasso", "comments": "8 pages, double column, 6 figures", "journal-ref": null, "doi": "10.1109/TSP.2012.2192924", "report-no": null, "categories": "stat.ME stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We introduce a recursive adaptive group lasso algorithm for real-time\npenalized least squares prediction that produces a time sequence of optimal\nsparse predictor coefficient vectors. At each time index the proposed algorithm\ncomputes an exact update of the optimal $\\ell_{1,\\infty}$-penalized recursive\nleast squares (RLS) predictor. Each update minimizes a convex but\nnondifferentiable function optimization problem. We develop an online homotopy\nmethod to reduce the computational complexity. Numerical simulations\ndemonstrate that the proposed algorithm outperforms the $\\ell_1$ regularized\nRLS algorithm for a group sparse system identification problem and has lower\nimplementation complexity than direct group lasso solvers.\n", "versions": [{"version": "v1", "created": "Sat, 29 Jan 2011 23:55:49 GMT"}], "update_date": "2015-05-27", "authors_parsed": [["Chen", "Yilun", ""], ["Hero", "Alfred O.", "III"]]}, {"id": "1101.5919", "submitter": "Leo Lahti", "authors": "Leo Lahti, Samuel Myllykangas, Sakari Knuutila and Samuel Kaski", "title": "Dependency detection with similarity constraints", "comments": "9 pages, 3 figures. Appeared in proceedings of the 2009 IEEE\n  International Workshop on Machine Learning for Signal Processing XIX\n  (MLSP'09). Implementation of the method available at\n  http://bioconductor.org/packages/devel/bioc/html/pint.html", "journal-ref": "In T{\\\"u}lay Adali, Jocelyn Chanussot, Christian Jutten, and Jan\n  Larsen, editors, Proceedings of the 2009 IEEE International Workshop on\n  Machine Learning for Signal Processing XIX, pages 89--94. IEEE, Piscataway,\n  NJ, USA, 2009", "doi": "10.1109/MLSP.2009.5306192", "report-no": null, "categories": "stat.ML q-bio.GN", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Unsupervised two-view learning, or detection of dependencies between two\npaired data sets, is typically done by some variant of canonical correlation\nanalysis (CCA). CCA searches for a linear projection for each view, such that\nthe correlations between the projections are maximized. The solution is\ninvariant to any linear transformation of either or both of the views; for\ntasks with small sample size such flexibility implies overfitting, which is\neven worse for more flexible nonparametric or kernel-based dependency discovery\nmethods. We develop variants which reduce the degrees of freedom by assuming\nconstraints on similarity of the projections in the two views. A particular\nexample is provided by a cancer gene discovery application where chromosomal\ndistance affects the dependencies between gene copy number and activity levels.\nSimilarity constraints are shown to improve detection performance of known\ncancer genes.\n", "versions": [{"version": "v1", "created": "Mon, 31 Jan 2011 11:38:32 GMT"}], "update_date": "2016-11-18", "authors_parsed": [["Lahti", "Leo", ""], ["Myllykangas", "Samuel", ""], ["Knuutila", "Sakari", ""], ["Kaski", "Samuel", ""]]}]