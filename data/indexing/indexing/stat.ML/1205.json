[{"id": "1205.0047", "submitter": "Soummya Kar", "authors": "Soummya Kar, Jose' M.F. Moura and H. Vincent Poor", "title": "$QD$-Learning: A Collaborative Distributed Strategy for Multi-Agent\n  Reinforcement Learning Through Consensus + Innovations", "comments": "Submitted to the IEEE Transactions on Signal Processing, 33 pages", "journal-ref": null, "doi": "10.1109/TSP.2013.2241057", "report-no": null, "categories": "stat.ML cs.LG cs.MA math.OC math.PR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The paper considers a class of multi-agent Markov decision processes (MDPs),\nin which the network agents respond differently (as manifested by the\ninstantaneous one-stage random costs) to a global controlled state and the\ncontrol actions of a remote controller. The paper investigates a distributed\nreinforcement learning setup with no prior information on the global state\ntransition and local agent cost statistics. Specifically, with the agents'\nobjective consisting of minimizing a network-averaged infinite horizon\ndiscounted cost, the paper proposes a distributed version of $Q$-learning,\n$\\mathcal{QD}$-learning, in which the network agents collaborate by means of\nlocal processing and mutual information exchange over a sparse (possibly\nstochastic) communication network to achieve the network goal. Under the\nassumption that each agent is only aware of its local online cost data and the\ninter-agent communication network is \\emph{weakly} connected, the proposed\ndistributed scheme is almost surely (a.s.) shown to yield asymptotically the\ndesired value function and the optimal stationary control policy at each\nnetwork agent. The analytical techniques developed in the paper to address the\nmixed time-scale stochastic dynamics of the \\emph{consensus + innovations}\nform, which arise as a result of the proposed interactive distributed scheme,\nare of independent interest.\n", "versions": [{"version": "v1", "created": "Mon, 30 Apr 2012 22:48:37 GMT"}, {"version": "v2", "created": "Thu, 25 Oct 2012 01:59:10 GMT"}], "update_date": "2015-06-04", "authors_parsed": [["Kar", "Soummya", ""], ["Moura", "Jose' M. F.", ""], ["Poor", "H. Vincent", ""]]}, {"id": "1205.0079", "submitter": "Julien Mairal", "authors": "Julien Mairal and Bin Yu", "title": "Complexity Analysis of the Lasso Regularization Path", "comments": "To appear in the proceedings of 29th International Conference on\n  Machine Learning (ICML 2012)", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG math.OC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The regularization path of the Lasso can be shown to be piecewise linear,\nmaking it possible to \"follow\" and explicitly compute the entire path. We\nanalyze in this paper this popular strategy, and prove that its worst case\ncomplexity is exponential in the number of variables. We then oppose this\npessimistic result to an (optimistic) approximate analysis: We show that an\napproximate path with at most O(1/sqrt(epsilon)) linear segments can always be\nobtained, where every point on the path is guaranteed to be optimal up to a\nrelative epsilon-duality gap. We complete our theoretical analysis with a\npractical algorithm to compute these approximate paths.\n", "versions": [{"version": "v1", "created": "Tue, 1 May 2012 03:37:13 GMT"}, {"version": "v2", "created": "Sat, 19 May 2012 21:06:21 GMT"}], "update_date": "2012-05-22", "authors_parsed": [["Mairal", "Julien", ""], ["Yu", "Bin", ""]]}, {"id": "1205.0288", "submitter": "Arash Afkanpour", "authors": "Arash Afkanpour, Andr\\'as Gy\\\"orgy, Csaba Szepesv\\'ari, Michael\n  Bowling", "title": "A Randomized Mirror Descent Algorithm for Large Scale Multiple Kernel\n  Learning", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider the problem of simultaneously learning to linearly combine a very\nlarge number of kernels and learn a good predictor based on the learnt kernel.\nWhen the number of kernels $d$ to be combined is very large, multiple kernel\nlearning methods whose computational cost scales linearly in $d$ are\nintractable. We propose a randomized version of the mirror descent algorithm to\novercome this issue, under the objective of minimizing the group $p$-norm\npenalized empirical risk. The key to achieve the required exponential speed-up\nis the computationally efficient construction of low-variance estimates of the\ngradient. We propose importance sampling based estimates, and find that the\nideal distribution samples a coordinate with a probability proportional to the\nmagnitude of the corresponding gradient. We show the surprising result that in\nthe case of learning the coefficients of a polynomial kernel, the combinatorial\nstructure of the base kernels to be combined allows the implementation of\nsampling from this distribution to run in $O(\\log(d))$ time, making the total\ncomputational cost of the method to achieve an $\\epsilon$-optimal solution to\nbe $O(\\log(d)/\\epsilon^2)$, thereby allowing our method to operate for very\nlarge values of $d$. Experiments with simulated and real data confirm that the\nnew algorithm is computationally more efficient than its state-of-the-art\nalternatives.\n", "versions": [{"version": "v1", "created": "Tue, 1 May 2012 23:42:57 GMT"}, {"version": "v2", "created": "Mon, 7 Jan 2013 17:42:46 GMT"}], "update_date": "2015-03-20", "authors_parsed": [["Afkanpour", "Arash", ""], ["Gy\u00f6rgy", "Andr\u00e1s", ""], ["Szepesv\u00e1ri", "Csaba", ""], ["Bowling", "Michael", ""]]}, {"id": "1205.0310", "submitter": "James Scott", "authors": "Nicholas G. Polson, James G. Scott, Jesse Windle", "title": "Bayesian inference for logistic models using Polya-Gamma latent\n  variables", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.CO stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a new data-augmentation strategy for fully Bayesian inference in\nmodels with binomial likelihoods. The approach appeals to a new class of\nPolya-Gamma distributions, which are constructed in detail. A variety of\nexamples are presented to show the versatility of the method, including\nlogistic regression, negative binomial regression, nonlinear mixed-effects\nmodels, and spatial models for count data. In each case, our data-augmentation\nstrategy leads to simple, effective methods for posterior inference that: (1)\ncircumvent the need for analytic approximations, numerical integration, or\nMetropolis-Hastings; and (2) outperform other known data-augmentation\nstrategies, both in ease of use and in computational efficiency. All methods,\nincluding an efficient sampler for the Polya-Gamma distribution, are\nimplemented in the R package BayesLogit.\n  In the technical supplement appended to the end of the paper, we provide\nfurther details regarding the generation of Polya-Gamma random variables; the\nempirical benchmarks reported in the main manuscript; and the extension of the\nbasic data-augmentation framework to contingency tables and multinomial\noutcomes.\n", "versions": [{"version": "v1", "created": "Wed, 2 May 2012 02:52:37 GMT"}, {"version": "v2", "created": "Wed, 20 Feb 2013 20:20:15 GMT"}, {"version": "v3", "created": "Mon, 22 Jul 2013 16:19:08 GMT"}], "update_date": "2015-03-20", "authors_parsed": [["Polson", "Nicholas G.", ""], ["Scott", "James G.", ""], ["Windle", "Jesse", ""]]}, {"id": "1205.0411", "submitter": "Dino Sejdinovic", "authors": "Dino Sejdinovic, Arthur Gretton, Bharath Sriperumbudur, Kenji Fukumizu", "title": "Hypothesis testing using pairwise distances and associated kernels (with\n  Appendix)", "comments": "Appearing in Proceedings of the 29th International Conference on\n  Machine Learning, Edinburgh, Scotland, UK, 2012", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ME stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We provide a unifying framework linking two classes of statistics used in\ntwo-sample and independence testing: on the one hand, the energy distances and\ndistance covariances from the statistics literature; on the other, distances\nbetween embeddings of distributions to reproducing kernel Hilbert spaces\n(RKHS), as established in machine learning. The equivalence holds when energy\ndistances are computed with semimetrics of negative type, in which case a\nkernel may be defined such that the RKHS distance between distributions\ncorresponds exactly to the energy distance. We determine the class of\nprobability distributions for which kernels induced by semimetrics are\ncharacteristic (that is, for which embeddings of the distributions to an RKHS\nare injective). Finally, we investigate the performance of this family of\nkernels in two-sample and independence tests: we show in particular that the\nenergy distance most commonly employed in statistics is just one member of a\nparametric family of kernels, and that other choices from this family can yield\nmore powerful tests.\n", "versions": [{"version": "v1", "created": "Wed, 2 May 2012 12:49:19 GMT"}, {"version": "v2", "created": "Mon, 21 May 2012 23:29:06 GMT"}], "update_date": "2015-03-20", "authors_parsed": [["Sejdinovic", "Dino", ""], ["Gretton", "Arthur", ""], ["Sriperumbudur", "Bharath", ""], ["Fukumizu", "Kenji", ""]]}, {"id": "1205.0793", "submitter": "Jennifer Listgarten", "authors": "Jennifer Listgarten, Christoph Lippert, Eun Yong Kang, Jing Xiang,\n  Carl M. Kadie and David Heckerman", "title": "A powerful and efficient set test for genetic markers that handles\n  confounders", "comments": "* denotes equal contributions", "journal-ref": null, "doi": "10.1093/bioinformatics/btt177", "report-no": null, "categories": "q-bio.GN stat.AP stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Approaches for testing sets of variants, such as a set of rare or common\nvariants within a gene or pathway, for association with complex traits are\nimportant. In particular, set tests allow for aggregation of weak signal within\na set, can capture interplay among variants, and reduce the burden of multiple\nhypothesis testing. Until now, these approaches did not address confounding by\nfamily relatedness and population structure, a problem that is becoming more\nimportant as larger data sets are used to increase power.\n  Results: We introduce a new approach for set tests that handles confounders.\nOur model is based on the linear mixed model and uses two random effects-one to\ncapture the set association signal and one to capture confounders. We also\nintroduce a computational speedup for two-random-effects models that makes this\napproach feasible even for extremely large cohorts. Using this model with both\nthe likelihood ratio test and score test, we find that the former yields more\npower while controlling type I error. Application of our approach to richly\nstructured GAW14 data demonstrates that our method successfully corrects for\npopulation structure and family relatedness, while application of our method to\na 15,000 individual Crohn's disease case-control cohort demonstrates that it\nadditionally recovers genes not recoverable by univariate analysis.\n  Availability: A Python-based library implementing our approach is available\nat http://mscompbio.codeplex.com\n", "versions": [{"version": "v1", "created": "Thu, 3 May 2012 19:05:38 GMT"}, {"version": "v2", "created": "Wed, 26 Sep 2012 16:49:49 GMT"}, {"version": "v3", "created": "Mon, 8 Apr 2013 04:30:32 GMT"}], "update_date": "2013-05-28", "authors_parsed": [["Listgarten", "Jennifer", ""], ["Lippert", "Christoph", ""], ["Kang", "Eun Yong", ""], ["Xiang", "Jing", ""], ["Kadie", "Carl M.", ""], ["Heckerman", "David", ""]]}, {"id": "1205.0953", "submitter": "Martin Slawski", "authors": "Martin Slawski and Matthias Hein", "title": "Non-negative least squares for high-dimensional linear models:\n  consistency and sparse recovery without regularization", "comments": "major revision", "journal-ref": "Electronic Journal of Statistics, 7(0):3004-3056, 2013", "doi": null, "report-no": null, "categories": "math.ST stat.ML stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Least squares fitting is in general not useful for high-dimensional linear\nmodels, in which the number of predictors is of the same or even larger order\nof magnitude than the number of samples. Theory developed in recent years has\ncoined a paradigm according to which sparsity-promoting regularization is\nregarded as a necessity in such setting. Deviating from this paradigm, we show\nthat non-negativity constraints on the regression coefficients may be similarly\neffective as explicit regularization if the design matrix has additional\nproperties, which are met in several applications of non-negative least squares\n(NNLS). We show that for these designs, the performance of NNLS with regard to\nprediction and estimation is comparable to that of the lasso. We argue further\nthat in specific cases, NNLS may have a better $\\ell_{\\infty}$-rate in\nestimation and hence also advantages with respect to support recovery when\ncombined with thresholding. From a practical point of view, NNLS does not\ndepend on a regularization parameter and is hence easier to use.\n", "versions": [{"version": "v1", "created": "Fri, 4 May 2012 13:34:39 GMT"}, {"version": "v2", "created": "Wed, 12 Feb 2014 16:18:40 GMT"}], "update_date": "2014-02-13", "authors_parsed": [["Slawski", "Martin", ""], ["Hein", "Matthias", ""]]}, {"id": "1205.1053", "submitter": "Dongwoo Kim", "authors": "Dongwoo Kim, Yeonseung Chung, Alice Oh", "title": "Variable Selection for Latent Dirichlet Allocation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In latent Dirichlet allocation (LDA), topics are multinomial distributions\nover the entire vocabulary. However, the vocabulary usually contains many words\nthat are not relevant in forming the topics. We adopt a variable selection\nmethod widely used in statistical modeling as a dimension reduction tool and\ncombine it with LDA. In this variable selection model for LDA (vsLDA), topics\nare multinomial distributions over a subset of the vocabulary, and by excluding\nwords that are not informative for finding the latent topic structure of the\ncorpus, vsLDA finds topics that are more robust and discriminative. We compare\nthree models, vsLDA, LDA with symmetric priors, and LDA with asymmetric priors,\non heldout likelihood, MCMC chain consistency, and document classification. The\nperformance of vsLDA is better than symmetric LDA for likelihood and\nclassification, better than asymmetric LDA for consistency and classification,\nand about the same in the other comparisons.\n", "versions": [{"version": "v1", "created": "Fri, 4 May 2012 03:14:18 GMT"}], "update_date": "2012-05-08", "authors_parsed": [["Kim", "Dongwoo", ""], ["Chung", "Yeonseung", ""], ["Oh", "Alice", ""]]}, {"id": "1205.1181", "submitter": "Anne-Claire Haury", "authors": "Anne-Claire Haury (CBIO), Fantine Mordelet, Paola Vera-Licona,\n  Jean-Philippe Vert (CBIO)", "title": "TIGRESS: Trustful Inference of Gene REgulation using Stability Selection", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML q-bio.QM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Inferring the structure of gene regulatory networks (GRN) from gene\nexpression data has many applications, from the elucidation of complex\nbiological processes to the identification of potential drug targets. It is\nhowever a notoriously difficult problem, for which the many existing methods\nreach limited accuracy. In this paper, we formulate GRN inference as a sparse\nregression problem and investigate the performance of a popular feature\nselection method, least angle regression (LARS) combined with stability\nselection. We introduce a novel, robust and accurate scoring technique for\nstability selection, which improves the performance of feature selection with\nLARS. The resulting method, which we call TIGRESS (Trustful Inference of Gene\nREgulation using Stability Selection), was ranked among the top methods in the\nDREAM5 gene network reconstruction challenge. We investigate in depth the\ninfluence of the various parameters of the method and show that a fine\nparameter tuning can lead to significant improvements and state-of-the-art\nperformance for GRN inference. TIGRESS reaches state-of-the-art performance on\nbenchmark data. This study confirms the potential of feature selection\ntechniques for GRN inference. Code and data are available on\nhttp://cbio.ensmp.fr/~ahaury. Running TIGRESS online is possible on\nGenePattern: http://www.broadinstitute.org/cancer/software/genepattern/.\n", "versions": [{"version": "v1", "created": "Sun, 6 May 2012 05:47:36 GMT"}], "update_date": "2012-05-08", "authors_parsed": [["Haury", "Anne-Claire", "", "CBIO"], ["Mordelet", "Fantine", "", "CBIO"], ["Vera-Licona", "Paola", "", "CBIO"], ["Vert", "Jean-Philippe", "", "CBIO"]]}, {"id": "1205.1240", "submitter": "Guillaume Obozinski", "authors": "Guillaume Obozinski (INRIA Paris - Rocquencourt, LIENS), Francis Bach\n  (INRIA Paris - Rocquencourt, LIENS)", "title": "Convex Relaxation for Combinatorial Penalties", "comments": "35 page", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we propose an unifying view of several recently proposed\nstructured sparsity-inducing norms. We consider the situation of a model\nsimultaneously (a) penalized by a set- function de ned on the support of the\nunknown parameter vector which represents prior knowledge on supports, and (b)\nregularized in Lp-norm. We show that the natural combinatorial optimization\nproblems obtained may be relaxed into convex optimization problems and\nintroduce a notion, the lower combinatorial envelope of a set-function, that\ncharacterizes the tightness of our relaxations. We moreover establish links\nwith norms based on latent representations including the latent group Lasso and\nblock-coding, and with norms obtained from submodular functions.\n", "versions": [{"version": "v1", "created": "Sun, 6 May 2012 19:54:33 GMT"}], "update_date": "2012-05-08", "authors_parsed": [["Obozinski", "Guillaume", "", "INRIA Paris - Rocquencourt, LIENS"], ["Bach", "Francis", "", "INRIA Paris - Rocquencourt, LIENS"]]}, {"id": "1205.1245", "submitter": "Martin Vincent", "authors": "Martin Vincent, Niels Richard Hansen", "title": "Sparse group lasso and high dimensional multinomial classification", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The sparse group lasso optimization problem is solved using a coordinate\ngradient descent algorithm. The algorithm is applicable to a broad class of\nconvex loss functions. Convergence of the algorithm is established, and the\nalgorithm is used to investigate the performance of the multinomial sparse\ngroup lasso classifier. On three different real data examples the multinomial\ngroup lasso clearly outperforms multinomial lasso in terms of achieved\nclassification error rate and in terms of including fewer features for the\nclassification. The run-time of our sparse group lasso implementation is of the\nsame order of magnitude as the multinomial lasso algorithm implemented in the R\npackage glmnet. Our implementation scales well with the problem size. One of\nthe high dimensional examples considered is a 50 class classification problem\nwith 10k features, which amounts to estimating 500k parameters. The\nimplementation is available as the R package msgl.\n", "versions": [{"version": "v1", "created": "Sun, 6 May 2012 20:18:13 GMT"}, {"version": "v2", "created": "Wed, 6 Feb 2013 09:36:02 GMT"}], "update_date": "2013-02-07", "authors_parsed": [["Vincent", "Martin", ""], ["Hansen", "Niels Richard", ""]]}, {"id": "1205.1287", "submitter": "Zhilin Zhang", "authors": "Zhilin Zhang, Tzyy-Ping Jung, Scott Makeig, Bhaskar D. Rao", "title": "Compressed Sensing for Energy-Efficient Wireless Telemonitoring of\n  Noninvasive Fetal ECG via Block Sparse Bayesian Learning", "comments": "The code and the data can be downloaded from the first author's\n  homepage: http://sites.google.com/site/researchbyzhang/bsbl, or\n  http://dsp.ucsd.edu/~zhilin/BSBL.html", "journal-ref": null, "doi": "10.1109/TBME.2012.2226175", "report-no": null, "categories": "stat.ML cs.LG stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Fetal ECG (FECG) telemonitoring is an important branch in telemedicine. The\ndesign of a telemonitoring system via a wireless body-area network with low\nenergy consumption for ambulatory use is highly desirable. As an emerging\ntechnique, compressed sensing (CS) shows great promise in\ncompressing/reconstructing data with low energy consumption. However, due to\nsome specific characteristics of raw FECG recordings such as non-sparsity and\nstrong noise contamination, current CS algorithms generally fail in this\napplication.\n  This work proposes to use the block sparse Bayesian learning (BSBL) framework\nto compress/reconstruct non-sparse raw FECG recordings. Experimental results\nshow that the framework can reconstruct the raw recordings with high quality.\nEspecially, the reconstruction does not destroy the interdependence relation\namong the multichannel recordings. This ensures that the independent component\nanalysis decomposition of the reconstructed recordings has high fidelity.\nFurthermore, the framework allows the use of a sparse binary sensing matrix\nwith much fewer nonzero entries to compress recordings. Particularly, each\ncolumn of the matrix can contain only two nonzero entries. This shows the\nframework, compared to other algorithms such as current CS algorithms and\nwavelet algorithms, can greatly reduce code execution in CPU in the data\ncompression stage.\n", "versions": [{"version": "v1", "created": "Mon, 7 May 2012 06:15:15 GMT"}, {"version": "v2", "created": "Mon, 9 Jul 2012 16:04:28 GMT"}, {"version": "v3", "created": "Sun, 29 Jul 2012 12:41:03 GMT"}, {"version": "v4", "created": "Wed, 19 Sep 2012 20:05:41 GMT"}, {"version": "v5", "created": "Sun, 23 Sep 2012 22:29:43 GMT"}, {"version": "v6", "created": "Fri, 19 Oct 2012 12:24:39 GMT"}, {"version": "v7", "created": "Sun, 2 Nov 2014 05:38:50 GMT"}], "update_date": "2015-03-20", "authors_parsed": [["Zhang", "Zhilin", ""], ["Jung", "Tzyy-Ping", ""], ["Makeig", "Scott", ""], ["Rao", "Bhaskar D.", ""]]}, {"id": "1205.1406", "submitter": "Emile Richard", "authors": "Emile Richard, Pierre-Andre Savalle, Nicolas Vayatis", "title": "Graph Prediction in a Low-Rank and Autoregressive Setting", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study the problem of prediction for evolving graph data. We formulate the\nproblem as the minimization of a convex objective encouraging sparsity and\nlow-rank of the solution, that reflect natural graph properties. The convex\nformulation allows to obtain oracle inequalities and efficient solvers. We\nprovide empirical results for our algorithm and comparison with competing\nmethods, and point out two open questions related to compressed sensing and\nalgebra of low-rank and sparse matrices.\n", "versions": [{"version": "v1", "created": "Mon, 7 May 2012 14:24:24 GMT"}, {"version": "v2", "created": "Wed, 9 May 2012 11:22:58 GMT"}], "update_date": "2012-05-10", "authors_parsed": [["Richard", "Emile", ""], ["Savalle", "Pierre-Andre", ""], ["Vayatis", "Nicolas", ""]]}, {"id": "1205.1482", "submitter": "Charles-Alban Deledalle", "authors": "Charles-Alban Deledalle (CEREMADE), Samuel Vaiter (CEREMADE), Gabriel\n  Peyr\\'e (CEREMADE), Jalal Fadili (GREYC), Charles Dossal (IMB)", "title": "Risk estimation for matrix recovery with spectral regularization", "comments": "This version is an update of our original paper presented at\n  ICML'2012 workshop on Sparsity, Dictionaries and Projections in Machine\n  Learning and Signal Processing", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.OC cs.IT cs.LG math.IT math.ST stat.ML stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we develop an approach to recursively estimate the quadratic\nrisk for matrix recovery problems regularized with spectral functions. Toward\nthis end, in the spirit of the SURE theory, a key step is to compute the (weak)\nderivative and divergence of a solution with respect to the observations. As\nsuch a solution is not available in closed form, but rather through a proximal\nsplitting algorithm, we propose to recursively compute the divergence from the\nsequence of iterates. A second challenge that we unlocked is the computation of\nthe (weak) derivative of the proximity operator of a spectral function. To show\nthe potential applicability of our approach, we exemplify it on a matrix\ncompletion problem to objectively and automatically select the regularization\nparameter.\n", "versions": [{"version": "v1", "created": "Mon, 7 May 2012 18:55:04 GMT"}, {"version": "v2", "created": "Tue, 23 Oct 2012 10:44:04 GMT"}, {"version": "v3", "created": "Thu, 1 Nov 2012 20:28:03 GMT"}], "update_date": "2012-11-07", "authors_parsed": [["Deledalle", "Charles-Alban", "", "CEREMADE"], ["Vaiter", "Samuel", "", "CEREMADE"], ["Peyr\u00e9", "Gabriel", "", "CEREMADE"], ["Fadili", "Jalal", "", "GREYC"], ["Dossal", "Charles", "", "IMB"]]}, {"id": "1205.1496", "submitter": "Jing Qian", "authors": "Jing Qian, Venkatesh Saligrama, Manqi Zhao", "title": "Graph-based Learning with Unbalanced Clusters", "comments": "21 pages, 7 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Graph construction is a crucial step in spectral clustering (SC) and\ngraph-based semi-supervised learning (SSL). Spectral methods applied on\nstandard graphs such as full-RBF, $\\epsilon$-graphs and $k$-NN graphs can lead\nto poor performance in the presence of proximal and unbalanced data. This is\nbecause spectral methods based on minimizing RatioCut or normalized cut on\nthese graphs tend to put more importance on balancing cluster sizes over\nreducing cut values. We propose a novel graph construction technique and show\nthat the RatioCut solution on this new graph is able to handle proximal and\nunbalanced data. Our method is based on adaptively modulating the neighborhood\ndegrees in a $k$-NN graph, which tends to sparsify neighborhoods in low density\nregions. Our method adapts to data with varying levels of unbalancedness and\ncan be naturally used for small cluster detection. We justify our ideas through\nlimit cut analysis. Unsupervised and semi-supervised experiments on synthetic\nand real data sets demonstrate the superiority of our method.\n", "versions": [{"version": "v1", "created": "Mon, 7 May 2012 19:55:31 GMT"}, {"version": "v2", "created": "Tue, 8 May 2012 18:27:52 GMT"}], "update_date": "2012-05-09", "authors_parsed": [["Qian", "Jing", ""], ["Saligrama", "Venkatesh", ""], ["Zhao", "Manqi", ""]]}, {"id": "1205.1782", "submitter": "Marek Petrik", "authors": "Marek Petrik", "title": "Approximate Dynamic Programming By Minimizing Distributionally Robust\n  Bounds", "comments": "In Proceedings of International Conference on Machine Learning, 2012", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Approximate dynamic programming is a popular method for solving large Markov\ndecision processes. This paper describes a new class of approximate dynamic\nprogramming (ADP) methods- distributionally robust ADP-that address the curse\nof dimensionality by minimizing a pessimistic bound on the policy loss. This\napproach turns ADP into an optimization problem, for which we derive new\nmathematical program formulations and analyze its properties. DRADP improves on\nthe theoretical guarantees of existing ADP methods-it guarantees convergence\nand L1 norm based error bounds. The empirical evaluation of DRADP shows that\nthe theoretical guarantees translate well into good performance on benchmark\nproblems.\n", "versions": [{"version": "v1", "created": "Tue, 8 May 2012 19:22:43 GMT"}, {"version": "v2", "created": "Mon, 21 May 2012 16:30:22 GMT"}], "update_date": "2012-05-22", "authors_parsed": [["Petrik", "Marek", ""]]}, {"id": "1205.1828", "submitter": "Jascha Sohl-Dickstein", "authors": "Jascha Sohl-Dickstein", "title": "The Natural Gradient by Analogy to Signal Whitening, and Recipes and\n  Tricks for its Use", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The natural gradient allows for more efficient gradient descent by removing\ndependencies and biases inherent in a function's parameterization. Several\npapers present the topic thoroughly and precisely. It remains a very difficult\nidea to get your head around however. The intent of this note is to provide\nsimple intuition for the natural gradient and its use. We review how an ill\nconditioned parameter space can undermine learning, introduce the natural\ngradient by analogy to the more widely understood concept of signal whitening,\nand present tricks and specific prescriptions for applying the natural gradient\nto learning problems.\n", "versions": [{"version": "v1", "created": "Tue, 8 May 2012 21:12:03 GMT"}], "update_date": "2012-05-10", "authors_parsed": [["Sohl-Dickstein", "Jascha", ""]]}, {"id": "1205.1989", "submitter": "Seunghak Lee", "authors": "Seunghak Lee and Eric P. Xing", "title": "Structured Input-Output Lasso, with Application to eQTL Mapping, and a\n  Thresholding Algorithm for Fast Estimation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML q-bio.GN q-bio.QM stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider the problem of learning a high-dimensional multi-task regression\nmodel, under sparsity constraints induced by presence of grouping structures on\nthe input covariates and on the output predictors. This problem is primarily\nmotivated by expression quantitative trait locus (eQTL) mapping, of which the\ngoal is to discover genetic variations in the genome (inputs) that influence\nthe expression levels of multiple co-expressed genes (outputs), either\nepistatically, or pleiotropically, or both. A structured input-output lasso\n(SIOL) model based on an intricate l1/l2-norm penalty over the regression\ncoefficient matrix is employed to enable discovery of complex sparse\ninput/output relationships; and a highly efficient new optimization algorithm\ncalled hierarchical group thresholding (HiGT) is developed to solve the\nresultant non-differentiable, non-separable, and ultra high-dimensional\noptimization problem. We show on both simulation and on a yeast eQTL dataset\nthat our model leads to significantly better recovery of the structured sparse\nrelationships between the inputs and the outputs, and our algorithm\nsignificantly outperforms other optimization techniques under the same model.\nAdditionally, we propose a novel approach for efficiently and effectively\ndetecting input interactions by exploiting the prior knowledge available from\nbiological experiments.\n", "versions": [{"version": "v1", "created": "Wed, 9 May 2012 14:14:22 GMT"}], "update_date": "2012-05-10", "authors_parsed": [["Lee", "Seunghak", ""], ["Xing", "Eric P.", ""]]}, {"id": "1205.2056", "submitter": "Ryan Rossi", "authors": "Ryan Rossi, Brian Gallagher, Jennifer Neville, Keith Henderson", "title": "Dynamic Behavioral Mixed-Membership Model for Large Evolving Networks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.SI cs.LG physics.soc-ph stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The majority of real-world networks are dynamic and extremely large (e.g.,\nInternet Traffic, Twitter, Facebook, ...). To understand the structural\nbehavior of nodes in these large dynamic networks, it may be necessary to model\nthe dynamics of behavioral roles representing the main connectivity patterns\nover time. In this paper, we propose a dynamic behavioral mixed-membership\nmodel (DBMM) that captures the roles of nodes in the graph and how they evolve\nover time. Unlike other node-centric models, our model is scalable for\nanalyzing large dynamic networks. In addition, DBMM is flexible,\nparameter-free, has no functional form or parameterization, and is\ninterpretable (identifies explainable patterns). The performance results\nindicate our approach can be applied to very large networks while the\nexperimental results show that our model uncovers interesting patterns\nunderlying the dynamics of these networks.\n", "versions": [{"version": "v1", "created": "Wed, 9 May 2012 18:20:32 GMT"}], "update_date": "2015-03-20", "authors_parsed": [["Rossi", "Ryan", ""], ["Gallagher", "Brian", ""], ["Neville", "Jennifer", ""], ["Henderson", "Keith", ""]]}, {"id": "1205.2106", "submitter": "Lingsong Zhang Lingsong Zhang", "authors": "Lingsong Zhang and Zhengyuan Zhu", "title": "Spatial Multiresolution Cluster Detection Method", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.CO stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A novel multi-resolution cluster detection (MCD) method is proposed to\nidentify irregularly shaped clusters in space. Multi-scale test statistic on a\nsingle cell is derived based on likelihood ratio statistic for Bernoulli\nsequence, Poisson sequence and Normal sequence. A neighborhood variability\nmeasure is defined to select the optimal test threshold. The MCD method is\ncompared with single scale testing methods controlling for false discovery rate\nand the spatial scan statistics using simulation and f-MRI data. The MCD method\nis shown to be more effective for discovering irregularly shaped clusters, and\nthe implementation of this method does not require heavy computation, making it\nsuitable for cluster detection for large spatial data.\n", "versions": [{"version": "v1", "created": "Wed, 9 May 2012 21:15:08 GMT"}], "update_date": "2012-05-11", "authors_parsed": [["Zhang", "Lingsong", ""], ["Zhu", "Zhengyuan", ""]]}, {"id": "1205.2171", "submitter": "Hachem Kadri", "authors": "Hachem Kadri (INRIA Lille - Nord Europe), Mohammad Ghavamzadeh (INRIA\n  Lille - Nord Europe), Philippe Preux (INRIA Lille - Nord Europe)", "title": "A Generalized Kernel Approach to Structured Output Learning", "comments": "in International Conference on Machine Learning (ICML), Jun 2013,\n  Atlanta, United States. 2013", "journal-ref": null, "doi": null, "report-no": "RR-7956", "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study the problem of structured output learning from a regression\nperspective. We first provide a general formulation of the kernel dependency\nestimation (KDE) problem using operator-valued kernels. We show that some of\nthe existing formulations of this problem are special cases of our framework.\nWe then propose a covariance-based operator-valued kernel that allows us to\ntake into account the structure of the kernel feature space. This kernel\noperates on the output space and encodes the interactions between the outputs\nwithout any reference to the input space. To address this issue, we introduce a\nvariant of our KDE method based on the conditional covariance operator that in\naddition to the correlation between the outputs takes into account the effects\nof the input variables. Finally, we evaluate the performance of our KDE\napproach using both covariance and conditional covariance kernels on two\nstructured output problems, and compare it to the state-of-the-art kernel-based\nstructured output regression methods.\n", "versions": [{"version": "v1", "created": "Thu, 10 May 2012 07:01:00 GMT"}, {"version": "v2", "created": "Wed, 15 Jul 2015 18:42:11 GMT"}], "update_date": "2015-07-16", "authors_parsed": [["Kadri", "Hachem", "", "INRIA Lille - Nord Europe"], ["Ghavamzadeh", "Mohammad", "", "INRIA\n  Lille - Nord Europe"], ["Preux", "Philippe", "", "INRIA Lille - Nord Europe"]]}, {"id": "1205.2172", "submitter": "Mohamed Khalil El Mahrsi", "authors": "Mohamed Khalil El Mahrsi (LTCI), Fabrice Rossi (SAMM)", "title": "Modularity-Based Clustering for Network-Constrained Trajectories", "comments": "20-th European Symposium on Artificial Neural Networks, Computational\n  Intelligence and Machine Learning (ESANN 2012), Bruges : Belgium (2012)", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG physics.data-an", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a novel clustering approach for moving object trajectories that\nare constrained by an underlying road network. The approach builds a similarity\ngraph based on these trajectories then uses modularity-optimization hiearchical\ngraph clustering to regroup trajectories with similar profiles. Our\nexperimental study shows the superiority of the proposed approach over classic\nhierarchical clustering and gives a brief insight to visualization of the\nclustering results.\n", "versions": [{"version": "v1", "created": "Thu, 10 May 2012 07:02:20 GMT"}, {"version": "v2", "created": "Fri, 5 Oct 2012 06:22:14 GMT"}], "update_date": "2012-10-08", "authors_parsed": [["Mahrsi", "Mohamed Khalil El", "", "LTCI"], ["Rossi", "Fabrice", "", "SAMM"]]}, {"id": "1205.2282", "submitter": "Fabrice Rossi", "authors": "Matthieu Durut (LTCI), Beno\\^it Patra (LSTA), Fabrice Rossi (SAMM)", "title": "A Discussion on Parallelization Schemes for Stochastic Vector\n  Quantization Algorithms", "comments": null, "journal-ref": "20-th European Symposium on Artificial Neural Networks,\n  Computational Intelligence and Machine Learning (ESANN 2012), Bruges :\n  Belgium (2012)", "doi": null, "report-no": null, "categories": "stat.ML cs.DC cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper studies parallelization schemes for stochastic Vector Quantization\nalgorithms in order to obtain time speed-ups using distributed resources. We\nshow that the most intuitive parallelization scheme does not lead to better\nperformances than the sequential algorithm. Another distributed scheme is\ntherefore introduced which obtains the expected speed-ups. Then, it is improved\nto fit implementation on distributed architectures where communications are\nslow and inter-machines synchronization too costly. The schemes are tested with\nsimulated distributed architectures and, for the last one, with Microsoft\nWindows Azure platform obtaining speed-ups up to 32 Virtual Machines.\n", "versions": [{"version": "v1", "created": "Thu, 10 May 2012 14:44:31 GMT"}], "update_date": "2012-05-14", "authors_parsed": [["Durut", "Matthieu", "", "LTCI"], ["Patra", "Beno\u00eet", "", "LSTA"], ["Rossi", "Fabrice", "", "SAMM"]]}, {"id": "1205.2334", "submitter": "Yong Zhang", "authors": "Zhaosong Lu and Yong Zhang", "title": "Sparse Approximation via Penalty Decomposition Methods", "comments": "31 pages, 3 figures and 9 tables. arXiv admin note: substantial text\n  overlap with arXiv:1008.5372", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG math.OC stat.CO stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper we consider sparse approximation problems, that is, general\n$l_0$ minimization problems with the $l_0$-\"norm\" of a vector being a part of\nconstraints or objective function. In particular, we first study the\nfirst-order optimality conditions for these problems. We then propose penalty\ndecomposition (PD) methods for solving them in which a sequence of penalty\nsubproblems are solved by a block coordinate descent (BCD) method. Under some\nsuitable assumptions, we establish that any accumulation point of the sequence\ngenerated by the PD methods satisfies the first-order optimality conditions of\nthe problems. Furthermore, for the problems in which the $l_0$ part is the only\nnonconvex part, we show that such an accumulation point is a local minimizer of\nthe problems. In addition, we show that any accumulation point of the sequence\ngenerated by the BCD method is a saddle point of the penalty subproblem.\nMoreover, for the problems in which the $l_0$ part is the only nonconvex part,\nwe establish that such an accumulation point is a local minimizer of the\npenalty subproblem. Finally, we test the performance of our PD methods by\napplying them to sparse logistic regression, sparse inverse covariance\nselection, and compressed sensing problems. The computational results\ndemonstrate that our methods generally outperform the existing methods in terms\nof solution quality and/or speed.\n", "versions": [{"version": "v1", "created": "Thu, 10 May 2012 18:25:06 GMT"}, {"version": "v2", "created": "Wed, 30 May 2012 00:49:30 GMT"}], "update_date": "2012-05-31", "authors_parsed": [["Lu", "Zhaosong", ""], ["Zhang", "Yong", ""]]}, {"id": "1205.2382", "submitter": "Mete Ozay", "authors": "Mete Ozay, Ilke \\\"Oztekin, Uygar \\\"Oztekin, Fatos T. Yarman Vural", "title": "Mesh Learning for Classifying Cognitive Processes", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NE cs.AI cs.CV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A relatively recent advance in cognitive neuroscience has been multi-voxel\npattern analysis (MVPA), which enables researchers to decode brain states\nand/or the type of information represented in the brain during a cognitive\noperation. MVPA methods utilize machine learning algorithms to distinguish\namong types of information or cognitive states represented in the brain, based\non distributed patterns of neural activity. In the current investigation, we\npropose a new approach for representation of neural data for pattern analysis,\nnamely a Mesh Learning Model. In this approach, at each time instant, a star\nmesh is formed around each voxel, such that the voxel corresponding to the\ncenter node is surrounded by its p-nearest neighbors. The arc weights of each\nmesh are estimated from the voxel intensity values by least squares method. The\nestimated arc weights of all the meshes, called Mesh Arc Descriptors (MADs),\nare then used to train a classifier, such as Neural Networks, k-Nearest\nNeighbor, Na\\\"ive Bayes and Support Vector Machines. The proposed Mesh Model\nwas tested on neuroimaging data acquired via functional magnetic resonance\nimaging (fMRI) during a recognition memory experiment using categorized word\nlists, employing a previously established experimental paradigm (\\\"Oztekin &\nBadre, 2011). Results suggest that the proposed Mesh Learning approach can\nprovide an effective algorithm for pattern analysis of brain activity during\ncognitive processing.\n", "versions": [{"version": "v1", "created": "Thu, 10 May 2012 20:22:17 GMT"}, {"version": "v2", "created": "Fri, 2 Nov 2012 17:44:03 GMT"}, {"version": "v3", "created": "Thu, 5 Feb 2015 21:50:00 GMT"}], "update_date": "2015-02-09", "authors_parsed": [["Ozay", "Mete", ""], ["\u00d6ztekin", "Ilke", ""], ["\u00d6ztekin", "Uygar", ""], ["Vural", "Fatos T. Yarman", ""]]}, {"id": "1205.2536", "submitter": "Jonas Peters", "authors": "Jonas Peters and Peter B\\\"uhlmann", "title": "Identifiability of Gaussian structural equation models with equal error\n  variances", "comments": null, "journal-ref": "Biometrika 2014, Vol. 101, No. 1, 219-228", "doi": "10.1093/biomet/ast043", "report-no": null, "categories": "stat.ML math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider structural equation models in which variables can be written as a\nfunction of their parents and noise terms, which are assumed to be jointly\nindependent. Corresponding to each structural equation model, there is a\ndirected acyclic graph describing the relationships between the variables. In\nGaussian structural equation models with linear functions, the graph can be\nidentified from the joint distribution only up to Markov equivalence classes,\nassuming faithfulness. In this work, we prove full identifiability if all noise\nvariables have the same variances: the directed acyclic graph can be recovered\nfrom the joint Gaussian distribution. Our result has direct implications for\ncausal inference: if the data follow a Gaussian structural equation model with\nequal error variances and assuming that all variables are observed, the causal\nstructure can be inferred from observational data only. We propose a\nstatistical method and an algorithm that exploit our theoretical findings.\n", "versions": [{"version": "v1", "created": "Fri, 11 May 2012 14:24:15 GMT"}, {"version": "v2", "created": "Tue, 4 Jun 2013 13:33:59 GMT"}, {"version": "v3", "created": "Wed, 28 Aug 2013 13:53:40 GMT"}], "update_date": "2014-06-03", "authors_parsed": [["Peters", "Jonas", ""], ["B\u00fchlmann", "Peter", ""]]}, {"id": "1205.2599", "submitter": "Kun Zhang", "authors": "Kun Zhang, Aapo Hyvarinen", "title": "On the Identifiability of the Post-Nonlinear Causal Model", "comments": "Appears in Proceedings of the Twenty-Fifth Conference on Uncertainty\n  in Artificial Intelligence (UAI2009)", "journal-ref": null, "doi": null, "report-no": "UAI-P-2009-PG-647-655", "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  By taking into account the nonlinear effect of the cause, the inner noise\neffect, and the measurement distortion effect in the observed variables, the\npost-nonlinear (PNL) causal model has demonstrated its excellent performance in\ndistinguishing the cause from effect. However, its identifiability has not been\nproperly addressed, and how to apply it in the case of more than two variables\nis also a problem. In this paper, we conduct a systematic investigation on its\nidentifiability in the two-variable case. We show that this model is\nidentifiable in most cases; by enumerating all possible situations in which the\nmodel is not identifiable, we provide sufficient conditions for its\nidentifiability. Simulations are given to support the theoretical results.\nMoreover, in the case of more than two variables, we show that the whole causal\nstructure can be found by applying the PNL causal model to each structure in\nthe Markov equivalent class and testing if the disturbance is independent of\nthe direct causes for each variable. In this way the exhaustive search over all\npossible causal structures is avoided.\n", "versions": [{"version": "v1", "created": "Wed, 9 May 2012 18:49:04 GMT"}], "update_date": "2012-05-14", "authors_parsed": [["Zhang", "Kun", ""], ["Hyvarinen", "Aapo", ""]]}, {"id": "1205.2604", "submitter": "David Wingate", "authors": "David Wingate, Noah Goodman, Daniel Roy, Joshua Tenenbaum", "title": "The Infinite Latent Events Model", "comments": "Appears in Proceedings of the Twenty-Fifth Conference on Uncertainty\n  in Artificial Intelligence (UAI2009)", "journal-ref": null, "doi": null, "report-no": "UAI-P-2009-PG-607-614", "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present the Infinite Latent Events Model, a nonparametric hierarchical\nBayesian distribution over infinite dimensional Dynamic Bayesian Networks with\nbinary state representations and noisy-OR-like transitions. The distribution\ncan be used to learn structure in discrete timeseries data by simultaneously\ninferring a set of latent events, which events fired at each timestep, and how\nthose events are causally linked. We illustrate the model on a sound\nfactorization task, a network topology identification task, and a video game\ntask.\n", "versions": [{"version": "v1", "created": "Wed, 9 May 2012 18:43:56 GMT"}], "update_date": "2012-05-14", "authors_parsed": [["Wingate", "David", ""], ["Goodman", "Noah", ""], ["Roy", "Daniel", ""], ["Tenenbaum", "Joshua", ""]]}, {"id": "1205.2605", "submitter": "Max Welling", "authors": "Max Welling", "title": "Herding Dynamic Weights for Partially Observed Random Field Models", "comments": "Appears in Proceedings of the Twenty-Fifth Conference on Uncertainty\n  in Artificial Intelligence (UAI2009)", "journal-ref": null, "doi": null, "report-no": "UAI-P-2009-PG-599-606", "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Learning the parameters of a (potentially partially observable) random field\nmodel is intractable in general. Instead of focussing on a single optimal\nparameter value we propose to treat parameters as dynamical quantities. We\nintroduce an algorithm to generate complex dynamics for parameters and (both\nvisible and hidden) state vectors. We show that under certain conditions\naverages computed over trajectories of the proposed dynamical system converge\nto averages computed over the data. Our \"herding dynamics\" does not require\nexpensive operations such as exponentiation and is fully deterministic.\n", "versions": [{"version": "v1", "created": "Wed, 9 May 2012 18:42:06 GMT"}], "update_date": "2012-05-14", "authors_parsed": [["Welling", "Max", ""]]}, {"id": "1205.2608", "submitter": "Christopher M. Vigorito", "authors": "Christopher M. Vigorito", "title": "Temporal-Difference Networks for Dynamical Systems with Continuous\n  Observations and Actions", "comments": "Appears in Proceedings of the Twenty-Fifth Conference on Uncertainty\n  in Artificial Intelligence (UAI2009)", "journal-ref": null, "doi": null, "report-no": "UAI-P-2009-PG-575-582", "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Temporal-difference (TD) networks are a class of predictive state\nrepresentations that use well-established TD methods to learn models of\npartially observable dynamical systems. Previous research with TD networks has\ndealt only with dynamical systems with finite sets of observations and actions.\nWe present an algorithm for learning TD network representations of dynamical\nsystems with continuous observations and actions. Our results show that the\nalgorithm is capable of learning accurate and robust models of several noisy\ncontinuous dynamical systems. The algorithm presented here is the first fully\nincremental method for learning a predictive representation of a continuous\ndynamical system.\n", "versions": [{"version": "v1", "created": "Wed, 9 May 2012 18:38:39 GMT"}], "update_date": "2012-05-14", "authors_parsed": [["Vigorito", "Christopher M.", ""]]}, {"id": "1205.2609", "submitter": "Nakul Verma", "authors": "Nakul Verma, Samory Kpotufe, Sanjoy Dasgupta", "title": "Which Spatial Partition Trees are Adaptive to Intrinsic Dimension?", "comments": "Appears in Proceedings of the Twenty-Fifth Conference on Uncertainty\n  in Artificial Intelligence (UAI2009)", "journal-ref": null, "doi": null, "report-no": "UAI-P-2009-PG-565-574", "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recent theory work has found that a special type of spatial partition tree -\ncalled a random projection tree - is adaptive to the intrinsic dimension of the\ndata from which it is built. Here we examine this same question, with a\ncombination of theory and experiments, for a broader class of trees that\nincludes k-d trees, dyadic trees, and PCA trees. Our motivation is to get a\nfeel for (i) the kind of intrinsic low dimensional structure that can be\nempirically verified, (ii) the extent to which a spatial partition can exploit\nsuch structure, and (iii) the implications for standard statistical tasks such\nas regression, vector quantization, and nearest neighbor search.\n", "versions": [{"version": "v1", "created": "Wed, 9 May 2012 18:37:50 GMT"}], "update_date": "2012-05-14", "authors_parsed": [["Verma", "Nakul", ""], ["Kpotufe", "Samory", ""], ["Dasgupta", "Sanjoy", ""]]}, {"id": "1205.2612", "submitter": "Jin Tian", "authors": "Jin Tian, Ru He", "title": "Computing Posterior Probabilities of Structural Features in Bayesian\n  Networks", "comments": "Appears in Proceedings of the Twenty-Fifth Conference on Uncertainty\n  in Artificial Intelligence (UAI2009)", "journal-ref": null, "doi": null, "report-no": "UAI-P-2009-PG-538-547", "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study the problem of learning Bayesian network structures from data.\nKoivisto and Sood (2004) and Koivisto (2006) presented algorithms that can\ncompute the exact marginal posterior probability of a subnetwork, e.g., a\nsingle edge, in O(n2n) time and the posterior probabilities for all n(n-1)\npotential edges in O(n2n) total time, assuming that the number of parents per\nnode or the indegree is bounded by a constant. One main drawback of their\nalgorithms is the requirement of a special structure prior that is non uniform\nand does not respect Markov equivalence. In this paper, we develop an algorithm\nthat can compute the exact posterior probability of a subnetwork in O(3n) time\nand the posterior probabilities for all n(n-1) potential edges in O(n3n) total\ntime. Our algorithm also assumes a bounded indegree but allows general\nstructure priors. We demonstrate the applicability of the algorithm on several\ndata sets with up to 20 variables.\n", "versions": [{"version": "v1", "created": "Wed, 9 May 2012 18:33:52 GMT"}], "update_date": "2012-05-14", "authors_parsed": [["Tian", "Jin", ""], ["He", "Ru", ""]]}, {"id": "1205.2614", "submitter": "Graham W Taylor", "authors": "Graham W Taylor, Geoffrey E. Hinton", "title": "Products of Hidden Markov Models: It Takes N>1 to Tango", "comments": "Appears in Proceedings of the Twenty-Fifth Conference on Uncertainty\n  in Artificial Intelligence (UAI2009)", "journal-ref": null, "doi": null, "report-no": "UAI-P-2009-PG-522-529", "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Products of Hidden Markov Models(PoHMMs) are an interesting class of\ngenerative models which have received little attention since their\nintroduction. This maybe in part due to their more computationally expensive\ngradient-based learning algorithm,and the intractability of computing the log\nlikelihood of sequences under the model. In this paper, we demonstrate how the\npartition function can be estimated reliably via Annealed Importance Sampling.\nWe perform experiments using contrastive divergence learning on rainfall data\nand data captured from pairs of people dancing. Our results suggest that\nadvances in learning and evaluation for undirected graphical models and recent\nincreases in available computing power make PoHMMs worth considering for\ncomplex time-series modeling tasks.\n", "versions": [{"version": "v1", "created": "Wed, 9 May 2012 18:30:23 GMT"}], "update_date": "2012-05-14", "authors_parsed": [["Taylor", "Graham W", ""], ["Hinton", "Geoffrey E.", ""]]}, {"id": "1205.2617", "submitter": "Mark Schmidt", "authors": "Mark Schmidt, Kevin Murphy", "title": "Modeling Discrete Interventional Data using Directed Cyclic Graphical\n  Models", "comments": "Appears in Proceedings of the Twenty-Fifth Conference on Uncertainty\n  in Artificial Intelligence (UAI2009)", "journal-ref": null, "doi": null, "report-no": "UAI-P-2009-PG-487-495", "categories": "stat.ML cs.LG stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We outline a representation for discrete multivariate distributions in terms\nof interventional potential functions that are globally normalized. This\nrepresentation can be used to model the effects of interventions, and the\nindependence properties encoded in this model can be represented as a directed\ngraph that allows cycles. In addition to discussing inference and sampling with\nthis representation, we give an exponential family parametrization that allows\nparameter estimation to be stated as a convex optimization problem; we also\ngive a convex relaxation of the task of simultaneous parameter and structure\nlearning using group l1-regularization. The model is evaluated on simulated\ndata and intracellular flow cytometry data.\n", "versions": [{"version": "v1", "created": "Wed, 9 May 2012 18:26:23 GMT"}], "update_date": "2012-05-14", "authors_parsed": [["Schmidt", "Mark", ""], ["Murphy", "Kevin", ""]]}, {"id": "1205.2618", "submitter": "Steffen Rendle", "authors": "Steffen Rendle, Christoph Freudenthaler, Zeno Gantner, Lars\n  Schmidt-Thieme", "title": "BPR: Bayesian Personalized Ranking from Implicit Feedback", "comments": "Appears in Proceedings of the Twenty-Fifth Conference on Uncertainty\n  in Artificial Intelligence (UAI2009)", "journal-ref": null, "doi": null, "report-no": "UAI-P-2009-PG-452-461", "categories": "cs.IR cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Item recommendation is the task of predicting a personalized ranking on a set\nof items (e.g. websites, movies, products). In this paper, we investigate the\nmost common scenario with implicit feedback (e.g. clicks, purchases). There are\nmany methods for item recommendation from implicit feedback like matrix\nfactorization (MF) or adaptive knearest-neighbor (kNN). Even though these\nmethods are designed for the item prediction task of personalized ranking, none\nof them is directly optimized for ranking. In this paper we present a generic\noptimization criterion BPR-Opt for personalized ranking that is the maximum\nposterior estimator derived from a Bayesian analysis of the problem. We also\nprovide a generic learning algorithm for optimizing models with respect to\nBPR-Opt. The learning method is based on stochastic gradient descent with\nbootstrap sampling. We show how to apply our method to two state-of-the-art\nrecommender models: matrix factorization and adaptive kNN. Our experiments\nindicate that for the task of personalized ranking our optimization method\noutperforms the standard learning techniques for MF and kNN. The results show\nthe importance of optimizing models for the right criterion.\n", "versions": [{"version": "v1", "created": "Wed, 9 May 2012 18:25:09 GMT"}], "update_date": "2012-05-14", "authors_parsed": [["Rendle", "Steffen", ""], ["Freudenthaler", "Christoph", ""], ["Gantner", "Zeno", ""], ["Schmidt-Thieme", "Lars", ""]]}, {"id": "1205.2622", "submitter": "Sara Mostafavi", "authors": "Sara Mostafavi, Quaid Morris", "title": "Using the Gene Ontology Hierarchy when Predicting Gene Function", "comments": "Appears in Proceedings of the Twenty-Fifth Conference on Uncertainty\n  in Artificial Intelligence (UAI2009)", "journal-ref": null, "doi": null, "report-no": "UAI-P-2009-PG-419-427", "categories": "cs.LG cs.CE stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The problem of multilabel classification when the labels are related through\na hierarchical categorization scheme occurs in many application domains such as\ncomputational biology. For example, this problem arises naturally when trying\nto automatically assign gene function using a controlled vocabularies like Gene\nOntology. However, most existing approaches for predicting gene functions solve\nindependent classification problems to predict genes that are involved in a\ngiven function category, independently of the rest. Here, we propose two simple\nmethods for incorporating information about the hierarchical nature of the\ncategorization scheme. In the first method, we use information about a gene's\nprevious annotation to set an initial prior on its label. In a second approach,\nwe extend a graph-based semi-supervised learning algorithm for predicting gene\nfunction in a hierarchy. We show that we can efficiently solve this problem by\nsolving a linear system of equations. We compare these approaches with a\nprevious label reconciliation-based approach. Results show that using the\nhierarchy information directly, compared to using reconciliation methods,\nimproves gene function prediction.\n", "versions": [{"version": "v1", "created": "Wed, 9 May 2012 17:26:42 GMT"}], "update_date": "2012-05-14", "authors_parsed": [["Mostafavi", "Sara", ""], ["Morris", "Quaid", ""]]}, {"id": "1205.2623", "submitter": "Thomas P. Minka", "authors": "Thomas P. Minka, Rongjing Xiang, Yuan (Alan) Qi", "title": "Virtual Vector Machine for Bayesian Online Classification", "comments": "Appears in Proceedings of the Twenty-Fifth Conference on Uncertainty\n  in Artificial Intelligence (UAI2009)", "journal-ref": null, "doi": null, "report-no": "UAI-P-2009-PG-411-418", "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In a typical online learning scenario, a learner is required to process a\nlarge data stream using a small memory buffer. Such a requirement is usually in\nconflict with a learner's primary pursuit of prediction accuracy. To address\nthis dilemma, we introduce a novel Bayesian online classi cation algorithm,\ncalled the Virtual Vector Machine. The virtual vector machine allows you to\nsmoothly trade-off prediction accuracy with memory size. The virtual vector\nmachine summarizes the information contained in the preceding data stream by a\nGaussian distribution over the classi cation weights plus a constant number of\nvirtual data points. The virtual data points are designed to add extra\nnon-Gaussian information about the classi cation weights. To maintain the\nconstant number of virtual points, the virtual vector machine adds the current\nreal data point into the virtual point set, merges two most similar virtual\npoints into a new virtual point or deletes a virtual point that is far from the\ndecision boundary. The information lost in this process is absorbed into the\nGaussian distribution. The extra information provided by the virtual points\nleads to improved predictive accuracy over previous online classification\nalgorithms.\n", "versions": [{"version": "v1", "created": "Wed, 9 May 2012 17:24:52 GMT"}], "update_date": "2012-05-14", "authors_parsed": [["Minka", "Thomas P.", "", "Alan"], ["Xiang", "Rongjing", "", "Alan"], ["Yuan", "", "", "Alan"], ["Qi", "", ""]]}, {"id": "1205.2626", "submitter": "Benjamin Marlin", "authors": "Benjamin Marlin, Mark Schmidt, Kevin Murphy", "title": "Group Sparse Priors for Covariance Estimation", "comments": "Appears in Proceedings of the Twenty-Fifth Conference on Uncertainty\n  in Artificial Intelligence (UAI2009)", "journal-ref": null, "doi": null, "report-no": "UAI-P-2009-PG-383-392", "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recently it has become popular to learn sparse Gaussian graphical models\n(GGMs) by imposing l1 or group l1,2 penalties on the elements of the precision\nmatrix. Thispenalized likelihood approach results in a tractable convex\noptimization problem. In this paper, we reinterpret these results as performing\nMAP estimation under a novel prior which we call the group l1 and l1,2\npositivedefinite matrix distributions. This enables us to build a hierarchical\nmodel in which the l1 regularization terms vary depending on which group the\nentries are assigned to, which in turn allows us to learn block structured\nsparse GGMs with unknown group assignments. Exact inference in this\nhierarchical model is intractable, due to the need to compute the normalization\nconstant of these matrix distributions. However, we derive upper bounds on the\npartition functions, which lets us use fast variational inference (optimizing a\nlower bound on the joint posterior). We show that on two real world data sets\n(motion capture and financial data), our method which infers the block\nstructure outperforms a method that uses a fixed block structure, which in turn\noutperforms baseline methods that ignore block structure.\n", "versions": [{"version": "v1", "created": "Wed, 9 May 2012 17:19:05 GMT"}], "update_date": "2012-05-14", "authors_parsed": [["Marlin", "Benjamin", ""], ["Schmidt", "Mark", ""], ["Murphy", "Kevin", ""]]}, {"id": "1205.2627", "submitter": "Yi Mao", "authors": "Yi Mao, Guy Lebanon", "title": "Domain Knowledge Uncertainty and Probabilistic Parameter Constraints", "comments": "Appears in Proceedings of the Twenty-Fifth Conference on Uncertainty\n  in Artificial Intelligence (UAI2009)", "journal-ref": null, "doi": null, "report-no": "UAI-P-2009-PG-375-382", "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Incorporating domain knowledge into the modeling process is an effective way\nto improve learning accuracy. However, as it is provided by humans, domain\nknowledge can only be specified with some degree of uncertainty. We propose to\nexplicitly model such uncertainty through probabilistic constraints over the\nparameter space. In contrast to hard parameter constraints, our approach is\neffective also when the domain knowledge is inaccurate and generally results in\nsuperior modeling accuracy. We focus on generative and conditional modeling\nwhere the parameters are assigned a Dirichlet or Gaussian prior and demonstrate\nthe framework with experiments on both synthetic and real-world data.\n", "versions": [{"version": "v1", "created": "Wed, 9 May 2012 17:17:33 GMT"}], "update_date": "2012-05-14", "authors_parsed": [["Mao", "Yi", ""], ["Lebanon", "Guy", ""]]}, {"id": "1205.2628", "submitter": "Yishay Mansour", "authors": "Yishay Mansour, Mehryar Mohri, Afshin Rostamizadeh", "title": "Multiple Source Adaptation and the Renyi Divergence", "comments": "Appears in Proceedings of the Twenty-Fifth Conference on Uncertainty\n  in Artificial Intelligence (UAI2009)", "journal-ref": null, "doi": null, "report-no": "UAI-P-2009-PG-367-374", "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper presents a novel theoretical study of the general problem of\nmultiple source adaptation using the notion of Renyi divergence. Our results\nbuild on our previous work [12], but significantly broaden the scope of that\nwork in several directions. We extend previous multiple source loss guarantees\nbased on distribution weighted combinations to arbitrary target distributions\nP, not necessarily mixtures of the source distributions, analyze both known and\nunknown target distribution cases, and prove a lower bound. We further extend\nour bounds to deal with the case where the learner receives an approximate\ndistribution for each source instead of the exact one, and show that similar\nloss guarantees can be achieved depending on the divergence between the\napproximate and true distributions. We also analyze the case where the labeling\nfunctions of the source domains are somewhat different. Finally, we report the\nresults of experiments with both an artificial data set and a sentiment\nanalysis task, showing the performance benefits of the distribution weighted\ncombinations and the quality of our bounds based on the Renyi divergence.\n", "versions": [{"version": "v1", "created": "Wed, 9 May 2012 17:15:45 GMT"}], "update_date": "2012-05-14", "authors_parsed": [["Mansour", "Yishay", ""], ["Mohri", "Mehryar", ""], ["Rostamizadeh", "Afshin", ""]]}, {"id": "1205.2629", "submitter": "Siwei Lyu", "authors": "Siwei Lyu", "title": "Interpretation and Generalization of Score Matching", "comments": "Appears in Proceedings of the Twenty-Fifth Conference on Uncertainty\n  in Artificial Intelligence (UAI2009)", "journal-ref": null, "doi": null, "report-no": "UAI-P-2009-PG-359-366", "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Score matching is a recently developed parameter learning method that is\nparticularly effective to complicated high dimensional density models with\nintractable partition functions. In this paper, we study two issues that have\nnot been completely resolved for score matching. First, we provide a formal\nlink between maximum likelihood and score matching. Our analysis shows that\nscore matching finds model parameters that are more robust with noisy training\ndata. Second, we develop a generalization of score matching. Based on this\ngeneralization, we further demonstrate an extension of score matching to models\nof discrete data.\n", "versions": [{"version": "v1", "created": "Wed, 9 May 2012 17:14:10 GMT"}], "update_date": "2012-05-14", "authors_parsed": [["Lyu", "Siwei", ""]]}, {"id": "1205.2631", "submitter": "Jun Liu", "authors": "Jun Liu, Shuiwang Ji, Jieping Ye", "title": "Multi-Task Feature Learning Via Efficient l2,1-Norm Minimization", "comments": "Appears in Proceedings of the Twenty-Fifth Conference on Uncertainty\n  in Artificial Intelligence (UAI2009)", "journal-ref": null, "doi": null, "report-no": "UAI-P-2009-PG-339-348", "categories": "cs.LG cs.CV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The problem of joint feature selection across a group of related tasks has\napplications in many areas including biomedical informatics and computer\nvision. We consider the l2,1-norm regularized regression model for joint\nfeature selection from multiple tasks, which can be derived in the\nprobabilistic framework by assuming a suitable prior from the exponential\nfamily. One appealing feature of the l2,1-norm regularization is that it\nencourages multiple predictors to share similar sparsity patterns. However, the\nresulting optimization problem is challenging to solve due to the\nnon-smoothness of the l2,1-norm regularization. In this paper, we propose to\naccelerate the computation by reformulating it as two equivalent smooth convex\noptimization problems which are then solved via the Nesterov's method-an\noptimal first-order black-box method for smooth convex optimization. A key\nbuilding block in solving the reformulations is the Euclidean projection. We\nshow that the Euclidean projection for the first reformulation can be\nanalytically computed, while the Euclidean projection for the second one can be\ncomputed in linear time. Empirical evaluations on several data sets verify the\nefficiency of the proposed algorithms.\n", "versions": [{"version": "v1", "created": "Wed, 9 May 2012 17:09:42 GMT"}], "update_date": "2012-05-14", "authors_parsed": [["Liu", "Jun", ""], ["Ji", "Shuiwang", ""], ["Ye", "Jieping", ""]]}, {"id": "1205.2632", "submitter": "Ping Li", "authors": "Ping Li", "title": "Improving Compressed Counting", "comments": "Appears in Proceedings of the Twenty-Fifth Conference on Uncertainty\n  in Artificial Intelligence (UAI2009)", "journal-ref": null, "doi": null, "report-no": "UAI-P-2009-PG-329-338", "categories": "cs.DS cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Compressed Counting (CC) [22] was recently proposed for estimating the ath\nfrequency moments of data streams, where 0 < a <= 2. CC can be used for\nestimating Shannon entropy, which can be approximated by certain functions of\nthe ath frequency moments as a -> 1. Monitoring Shannon entropy for anomaly\ndetection (e.g., DDoS attacks) in large networks is an important task. This\npaper presents a new algorithm for improving CC. The improvement is most\nsubstantial when a -> 1--. For example, when a = 0:99, the new algorithm\nreduces the estimation variance roughly by 100-fold. This new algorithm would\nmake CC considerably more practical for estimating Shannon entropy.\nFurthermore, the new algorithm is statistically optimal when a = 0.5.\n", "versions": [{"version": "v1", "created": "Wed, 9 May 2012 15:49:12 GMT"}], "update_date": "2012-05-14", "authors_parsed": [["Li", "Ping", ""]]}, {"id": "1205.2640", "submitter": "Dominik Janzing", "authors": "Dominik Janzing, Jonas Peters, Joris Mooij, Bernhard Schoelkopf", "title": "Identifying confounders using additive noise models", "comments": "Appears in Proceedings of the Twenty-Fifth Conference on Uncertainty\n  in Artificial Intelligence (UAI2009)", "journal-ref": null, "doi": null, "report-no": "UAI-P-2009-PG-249-257", "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a method for inferring the existence of a latent common cause\n('confounder') of two observed random variables. The method assumes that the\ntwo effects of the confounder are (possibly nonlinear) functions of the\nconfounder plus independent, additive noise. We discuss under which conditions\nthe model is identifiable (up to an arbitrary reparameterization of the\nconfounder) from the joint distribution of the effects. We state and prove a\ntheoretical result that provides evidence for the conjecture that the model is\ngenerically identifiable under suitable technical conditions. In addition, we\npropose a practical method to estimate the confounder from a finite i.i.d.\nsample of the effects and illustrate that the method works well on both\nsimulated and real-world data.\n", "versions": [{"version": "v1", "created": "Wed, 9 May 2012 15:31:59 GMT"}], "update_date": "2012-05-14", "authors_parsed": [["Janzing", "Dominik", ""], ["Peters", "Jonas", ""], ["Mooij", "Joris", ""], ["Schoelkopf", "Bernhard", ""]]}, {"id": "1205.2641", "submitter": "Patrik O. Hoyer", "authors": "Patrik O. Hoyer, Antti Hyttinen", "title": "Bayesian Discovery of Linear Acyclic Causal Models", "comments": "Appears in Proceedings of the Twenty-Fifth Conference on Uncertainty\n  in Artificial Intelligence (UAI2009)", "journal-ref": null, "doi": null, "report-no": "UAI-P-2009-PG-240-248", "categories": "stat.ML cs.LG stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Methods for automated discovery of causal relationships from\nnon-interventional data have received much attention recently. A widely used\nand well understood model family is given by linear acyclic causal models\n(recursive structural equation models). For Gaussian data both constraint-based\nmethods (Spirtes et al., 1993; Pearl, 2000) (which output a single equivalence\nclass) and Bayesian score-based methods (Geiger and Heckerman, 1994) (which\nassign relative scores to the equivalence classes) are available. On the\ncontrary, all current methods able to utilize non-Gaussianity in the data\n(Shimizu et al., 2006; Hoyer et al., 2008) always return only a single graph or\na single equivalence class, and so are fundamentally unable to express the\ndegree of certainty attached to that output. In this paper we develop a\nBayesian score-based approach able to take advantage of non-Gaussianity when\nestimating linear acyclic causal models, and we empirically demonstrate that,\nat least on very modest size networks, its accuracy is as good as or better\nthan existing methods. We provide a complete code package (in R) which\nimplements all algorithms and performs all of the analysis provided in the\npaper, and hope that this will further the application of these methods to\nsolving causal inference problems.\n", "versions": [{"version": "v1", "created": "Wed, 9 May 2012 15:30:07 GMT"}], "update_date": "2012-05-14", "authors_parsed": [["Hoyer", "Patrik O.", ""], ["Hyttinen", "Antti", ""]]}, {"id": "1205.2643", "submitter": "Matthias Hoffman", "authors": "Matthias Hoffman, Hendrik Kueck, Nando de Freitas, Arnaud Doucet", "title": "New inference strategies for solving Markov Decision Processes using\n  reversible jump MCMC", "comments": "Appears in Proceedings of the Twenty-Fifth Conference on Uncertainty\n  in Artificial Intelligence (UAI2009)", "journal-ref": null, "doi": null, "report-no": "UAI-P-2009-PG-223-231", "categories": "cs.LG cs.SY math.OC stat.CO stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper we build on previous work which uses inferences techniques, in\nparticular Markov Chain Monte Carlo (MCMC) methods, to solve parameterized\ncontrol problems. We propose a number of modifications in order to make this\napproach more practical in general, higher-dimensional spaces. We first\nintroduce a new target distribution which is able to incorporate more reward\ninformation from sampled trajectories. We also show how to break strong\ncorrelations between the policy parameters and sampled trajectories in order to\nsample more freely. Finally, we show how to incorporate these techniques in a\nprincipled manner to obtain estimates of the optimal policy.\n", "versions": [{"version": "v1", "created": "Wed, 9 May 2012 15:26:47 GMT"}], "update_date": "2012-05-14", "authors_parsed": [["Hoffman", "Matthias", ""], ["Kueck", "Hendrik", ""], ["de Freitas", "Nando", ""], ["Doucet", "Arnaud", ""]]}, {"id": "1205.2648", "submitter": "Yu Fan", "authors": "Yu Fan, Christian R. Shelton", "title": "Learning Continuous-Time Social Network Dynamics", "comments": "Appears in Proceedings of the Twenty-Fifth Conference on Uncertainty\n  in Artificial Intelligence (UAI2009)", "journal-ref": null, "doi": null, "report-no": "UAI-P-2009-PG-161-168", "categories": "cs.SI cs.LG physics.soc-ph stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We demonstrate that a number of sociology models for social network dynamics\ncan be viewed as continuous time Bayesian networks (CTBNs). A sampling-based\napproximate inference method for CTBNs can be used as the basis of an\nexpectation-maximization procedure that achieves better accuracy in estimating\nthe parameters of the model than the standard method of moments\nalgorithmfromthe sociology literature. We extend the existing social network\nmodels to allow for indirect and asynchronous observations of the links. A\nMarkov chain Monte Carlo sampling algorithm for this new model permits\nestimation and inference. We provide results on both a synthetic network (for\nverification) and real social network data.\n", "versions": [{"version": "v1", "created": "Wed, 9 May 2012 15:13:59 GMT"}], "update_date": "2012-05-14", "authors_parsed": [["Fan", "Yu", ""], ["Shelton", "Christian R.", ""]]}, {"id": "1205.2650", "submitter": "Finale Doshi-Velez", "authors": "Finale Doshi-Velez, Zoubin Ghahramani", "title": "Correlated Non-Parametric Latent Feature Models", "comments": "Appears in Proceedings of the Twenty-Fifth Conference on Uncertainty\n  in Artificial Intelligence (UAI2009)", "journal-ref": null, "doi": null, "report-no": "UAI-P-2009-PG-143-150", "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We are often interested in explaining data through a set of hidden factors or\nfeatures. When the number of hidden features is unknown, the Indian Buffet\nProcess (IBP) is a nonparametric latent feature model that does not bound the\nnumber of active features in dataset. However, the IBP assumes that all latent\nfeatures are uncorrelated, making it inadequate for many realworld problems. We\nintroduce a framework for correlated nonparametric feature models, generalising\nthe IBP. We use this framework to generate several specific models and\ndemonstrate applications on realworld datasets.\n", "versions": [{"version": "v1", "created": "Wed, 9 May 2012 15:09:51 GMT"}], "update_date": "2012-05-14", "authors_parsed": [["Doshi-Velez", "Finale", ""], ["Ghahramani", "Zoubin", ""]]}, {"id": "1205.2653", "submitter": "Corinna Cortes", "authors": "Corinna Cortes, Mehryar Mohri, Afshin Rostamizadeh", "title": "L2 Regularization for Learning Kernels", "comments": "Appears in Proceedings of the Twenty-Fifth Conference on Uncertainty\n  in Artificial Intelligence (UAI2009)", "journal-ref": null, "doi": null, "report-no": "UAI-P-2009-PG-109-116", "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The choice of the kernel is critical to the success of many learning\nalgorithms but it is typically left to the user. Instead, the training data can\nbe used to learn the kernel by selecting it out of a given family, such as that\nof non-negative linear combinations of p base kernels, constrained by a trace\nor L1 regularization. This paper studies the problem of learning kernels with\nthe same family of kernels but with an L2 regularization instead, and for\nregression problems. We analyze the problem of learning kernels with ridge\nregression. We derive the form of the solution of the optimization problem and\ngive an efficient iterative algorithm for computing that solution. We present a\nnovel theoretical analysis of the problem based on stability and give learning\nbounds for orthogonal kernels that contain only an additive term O(pp/m) when\ncompared to the standard kernel ridge regression stability bound. We also\nreport the results of experiments indicating that L1 regularization can lead to\nmodest improvements for a small number of kernels, but to performance\ndegradations in larger-scale cases. In contrast, L2 regularization never\ndegrades performance and in fact achieves significant improvements with a large\nnumber of kernels.\n", "versions": [{"version": "v1", "created": "Wed, 9 May 2012 15:01:22 GMT"}], "update_date": "2012-05-14", "authors_parsed": [["Cortes", "Corinna", ""], ["Mohri", "Mehryar", ""], ["Rostamizadeh", "Afshin", ""]]}, {"id": "1205.2656", "submitter": "David M. Bradley", "authors": "David M. Bradley, J Andrew Bagnell", "title": "Convex Coding", "comments": "Appears in Proceedings of the Twenty-Fifth Conference on Uncertainty\n  in Artificial Intelligence (UAI2009)", "journal-ref": null, "doi": null, "report-no": "UAI-P-2009-PG-83-90", "categories": "cs.LG cs.IT math.IT stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Inspired by recent work on convex formulations of clustering (Lashkari &\nGolland, 2008; Nowozin & Bakir, 2008) we investigate a new formulation of the\nSparse Coding Problem (Olshausen & Field, 1997). In sparse coding we attempt to\nsimultaneously represent a sequence of data-vectors sparsely (i.e. sparse\napproximation (Tropp et al., 2006)) in terms of a 'code' defined by a set of\nbasis elements, while also finding a code that enables such an approximation.\nAs existing alternating optimization procedures for sparse coding are\ntheoretically prone to severe local minima problems, we propose a convex\nrelaxation of the sparse coding problem and derive a boosting-style algorithm,\nthat (Nowozin & Bakir, 2008) serves as a convex 'master problem' which calls a\n(potentially non-convex) sub-problem to identify the next code element to add.\nFinally, we demonstrate the properties of our boosted coding algorithm on an\nimage denoising task.\n", "versions": [{"version": "v1", "created": "Wed, 9 May 2012 14:54:51 GMT"}], "update_date": "2012-05-14", "authors_parsed": [["Bradley", "David M.", ""], ["Bagnell", "J Andrew", ""]]}, {"id": "1205.2657", "submitter": "Jordan Boyd-Graber", "authors": "Jordan Boyd-Graber, David Blei", "title": "Multilingual Topic Models for Unaligned Text", "comments": "Appears in Proceedings of the Twenty-Fifth Conference on Uncertainty\n  in Artificial Intelligence (UAI2009)", "journal-ref": null, "doi": null, "report-no": "UAI-P-2009-PG-75-82", "categories": "cs.CL cs.IR cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We develop the multilingual topic model for unaligned text (MuTo), a\nprobabilistic model of text that is designed to analyze corpora composed of\ndocuments in two languages. From these documents, MuTo uses stochastic EM to\nsimultaneously discover both a matching between the languages and multilingual\nlatent topics. We demonstrate that MuTo is able to find shared topics on\nreal-world multilingual corpora, successfully pairing related documents across\nlanguages. MuTo provides a new framework for creating multilingual topic models\nwithout needing carefully curated parallel corpora and allows applications\nbuilt using the topic model formalism to be applied to a much wider class of\ncorpora.\n", "versions": [{"version": "v1", "created": "Wed, 9 May 2012 14:53:11 GMT"}], "update_date": "2012-05-14", "authors_parsed": [["Boyd-Graber", "Jordan", ""], ["Blei", "David", ""]]}, {"id": "1205.2658", "submitter": "Alexandre Bouchard-Cote", "authors": "Alexandre Bouchard-Cote, Michael I. Jordan", "title": "Optimization of Structured Mean Field Objectives", "comments": "Appears in Proceedings of the Twenty-Fifth Conference on Uncertainty\n  in Artificial Intelligence (UAI2009)", "journal-ref": null, "doi": null, "report-no": "UAI-P-2009-PG-67-74", "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In intractable, undirected graphical models, an intuitive way of creating\nstructured mean field approximations is to select an acyclic tractable\nsubgraph. We show that the hardness of computing the objective function and\ngradient of the mean field objective qualitatively depends on a simple graph\nproperty. If the tractable subgraph has this property- we call such subgraphs\nv-acyclic-a very fast block coordinate ascent algorithm is possible. If not,\noptimization is harder, but we show a new algorithm based on the construction\nof an auxiliary exponential family that can be used to make inference possible\nin this case as well. We discuss the advantages and disadvantages of each\nregime and compare the algorithms empirically.\n", "versions": [{"version": "v1", "created": "Wed, 9 May 2012 14:51:42 GMT"}], "update_date": "2012-05-14", "authors_parsed": [["Bouchard-Cote", "Alexandre", ""], ["Jordan", "Michael I.", ""]]}, {"id": "1205.2660", "submitter": "Kedar Bellare", "authors": "Kedar Bellare, Gregory Druck, Andrew McCallum", "title": "Alternating Projections for Learning with Expectation Constraints", "comments": "Appears in Proceedings of the Twenty-Fifth Conference on Uncertainty\n  in Artificial Intelligence (UAI2009)", "journal-ref": null, "doi": null, "report-no": "UAI-P-2009-PG-43-50", "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present an objective function for learning with unlabeled data that\nutilizes auxiliary expectation constraints. We optimize this objective function\nusing a procedure that alternates between information and moment projections.\nOur method provides an alternate interpretation of the posterior regularization\nframework (Graca et al., 2008), maintains uncertainty during optimization\nunlike constraint-driven learning (Chang et al., 2007), and is more efficient\nthan generalized expectation criteria (Mann & McCallum, 2008). Applications of\nthis framework include minimally supervised learning, semisupervised learning,\nand learning with constraints that are more expressive than the underlying\nmodel. In experiments, we demonstrate comparable accuracy to generalized\nexpectation criteria for minimally supervised learning, and use expressive\nstructural constraints to guide semi-supervised learning, providing a 3%-6%\nimprovement over stateof-the-art constraint-driven learning.\n", "versions": [{"version": "v1", "created": "Wed, 9 May 2012 14:48:34 GMT"}], "update_date": "2012-05-14", "authors_parsed": [["Bellare", "Kedar", ""], ["Druck", "Gregory", ""], ["McCallum", "Andrew", ""]]}, {"id": "1205.2662", "submitter": "Arthur Asuncion", "authors": "Arthur Asuncion, Max Welling, Padhraic Smyth, Yee Whye Teh", "title": "On Smoothing and Inference for Topic Models", "comments": "Appears in Proceedings of the Twenty-Fifth Conference on Uncertainty\n  in Artificial Intelligence (UAI2009)", "journal-ref": null, "doi": null, "report-no": "UAI-P-2009-PG-27-34", "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Latent Dirichlet analysis, or topic modeling, is a flexible latent variable\nframework for modeling high-dimensional sparse count data. Various learning\nalgorithms have been developed in recent years, including collapsed Gibbs\nsampling, variational inference, and maximum a posteriori estimation, and this\nvariety motivates the need for careful empirical comparisons. In this paper, we\nhighlight the close connections between these approaches. We find that the main\ndifferences are attributable to the amount of smoothing applied to the counts.\nWhen the hyperparameters are optimized, the differences in performance among\nthe algorithms diminish significantly. The ability of these algorithms to\nachieve solutions of comparable accuracy gives us the freedom to select\ncomputationally efficient approaches. Using the insights gained from this\ncomparative study, we show how accurate topic models can be learned in several\nseconds on text corpora with thousands of documents.\n", "versions": [{"version": "v1", "created": "Wed, 9 May 2012 14:43:32 GMT"}], "update_date": "2012-05-14", "authors_parsed": [["Asuncion", "Arthur", ""], ["Welling", "Max", ""], ["Smyth", "Padhraic", ""], ["Teh", "Yee Whye", ""]]}, {"id": "1205.3109", "submitter": "Arthur Guez", "authors": "Arthur Guez and David Silver and Peter Dayan", "title": "Efficient Bayes-Adaptive Reinforcement Learning using Sample-Based\n  Search", "comments": "14 pages, 7 figures, includes supplementary material. Advances in\n  Neural Information Processing Systems (NIPS) 2012", "journal-ref": "(2012) Advances in Neural Information Processing Systems 25, pages\n  1034-1042", "doi": null, "report-no": null, "categories": "cs.LG cs.AI stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Bayesian model-based reinforcement learning is a formally elegant approach to\nlearning optimal behaviour under model uncertainty, trading off exploration and\nexploitation in an ideal way. Unfortunately, finding the resulting\nBayes-optimal policies is notoriously taxing, since the search space becomes\nenormous. In this paper we introduce a tractable, sample-based method for\napproximate Bayes-optimal planning which exploits Monte-Carlo tree search. Our\napproach outperformed prior Bayesian model-based RL algorithms by a significant\nmargin on several well-known benchmark problems -- because it avoids expensive\napplications of Bayes rule within the search tree by lazily sampling models\nfrom the current beliefs. We illustrate the advantages of our approach by\nshowing it working in an infinite state space domain which is qualitatively out\nof reach of almost all previous work in Bayesian exploration.\n", "versions": [{"version": "v1", "created": "Mon, 14 May 2012 17:20:29 GMT"}, {"version": "v2", "created": "Sat, 13 Oct 2012 15:19:09 GMT"}, {"version": "v3", "created": "Thu, 3 Jan 2013 14:44:59 GMT"}, {"version": "v4", "created": "Wed, 18 Dec 2013 11:45:49 GMT"}], "update_date": "2015-03-20", "authors_parsed": [["Guez", "Arthur", ""], ["Silver", "David", ""], ["Dayan", "Peter", ""]]}, {"id": "1205.3181", "submitter": "Sebastien Bubeck", "authors": "S\\'ebastien Bubeck, Tengyao Wang, Nitin Viswanathan", "title": "Multiple Identifications in Multi-Armed Bandits", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study the problem of identifying the top $m$ arms in a multi-armed bandit\ngame. Our proposed solution relies on a new algorithm based on successive\nrejects of the seemingly bad arms, and successive accepts of the good ones.\nThis algorithmic contribution allows to tackle other multiple identifications\nsettings that were previously out of reach. In particular we show that this\nidea of successive accepts and rejects applies to the multi-bandit best arm\nidentification problem.\n", "versions": [{"version": "v1", "created": "Mon, 14 May 2012 20:10:04 GMT"}], "update_date": "2012-05-16", "authors_parsed": [["Bubeck", "S\u00e9bastien", ""], ["Wang", "Tengyao", ""], ["Viswanathan", "Nitin", ""]]}, {"id": "1205.3193", "submitter": "Joonseok Lee", "authors": "Joonseok Lee, Mingxuan Sun, Guy Lebanon", "title": "A Comparative Study of Collaborative Filtering Algorithms", "comments": "27 pages, 12 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Collaborative filtering is a rapidly advancing research area. Every year\nseveral new techniques are proposed and yet it is not clear which of the\ntechniques work best and under what conditions. In this paper we conduct a\nstudy comparing several collaborative filtering techniques -- both classic and\nrecent state-of-the-art -- in a variety of experimental contexts. Specifically,\nwe report conclusions controlling for number of items, number of users,\nsparsity level, performance criteria, and computational complexity. Our\nconclusions identify what algorithms work well and in what conditions, and\ncontribute to both industrial deployment collaborative filtering algorithms and\nto the research community.\n", "versions": [{"version": "v1", "created": "Mon, 14 May 2012 21:08:05 GMT"}], "update_date": "2012-05-16", "authors_parsed": [["Lee", "Joonseok", ""], ["Sun", "Mingxuan", ""], ["Lebanon", "Guy", ""]]}, {"id": "1205.3217", "submitter": "Mauricio Sadinle", "authors": "Mauricio Sadinle, Stephen E. Fienberg", "title": "A Generalized Fellegi-Sunter Framework for Multiple Record Linkage With\n  Application to Homicide Record Systems", "comments": "Several changes with respect to previous version. Accepted in the\n  Journal of the American Statistical Association", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP stat.ME stat.ML stat.OT", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a probabilistic method for linking multiple datafiles. This task\nis not trivial in the absence of unique identifiers for the individuals\nrecorded. This is a common scenario when linking census data to coverage\nmeasurement surveys for census coverage evaluation, and in general when\nmultiple record-systems need to be integrated for posterior analysis. Our\nmethod generalizes the Fellegi-Sunter theory for linking records from two\ndatafiles and its modern implementations. The multiple record linkage goal is\nto classify the record K-tuples coming from K datafiles according to the\ndifferent matching patterns. Our method incorporates the transitivity of\nagreement in the computation of the data used to model matching probabilities.\nWe use a mixture model to fit matching probabilities via maximum likelihood\nusing the EM algorithm. We present a method to decide the record K-tuples\nmembership to the subsets of matching patterns and we prove its optimality. We\napply our method to the integration of three Colombian homicide record systems\nand we perform a simulation study in order to explore the performance of the\nmethod under measurement error and different scenarios. The proposed method\nworks well and opens some directions for future research.\n", "versions": [{"version": "v1", "created": "Mon, 14 May 2012 23:01:28 GMT"}, {"version": "v2", "created": "Wed, 6 Feb 2013 18:58:00 GMT"}], "update_date": "2013-02-07", "authors_parsed": [["Sadinle", "Mauricio", ""], ["Fienberg", "Stephen E.", ""]]}, {"id": "1205.3234", "submitter": "Keisuke Yamazaki", "authors": "Keisuke Yamazaki", "title": "Asymptotic Accuracy of Bayes Estimation for Latent Variables with\n  Redundancy", "comments": "35 pages, 2 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Hierarchical parametric models consisting of observable and latent variables\nare widely used for unsupervised learning tasks. For example, a mixture model\nis a representative hierarchical model for clustering. From the statistical\npoint of view, the models can be regular or singular due to the distribution of\ndata. In the regular case, the models have the identifiability; there is\none-to-one relation between a probability density function for the model\nexpression and the parameter. The Fisher information matrix is positive\ndefinite, and the estimation accuracy of both observable and latent variables\nhas been studied. In the singular case, on the other hand, the models are not\nidentifiable and the Fisher matrix is not positive definite. Conventional\nstatistical analysis based on the inverse Fisher matrix is not applicable.\nRecently, an algebraic geometrical analysis has been developed and is used to\nelucidate the Bayes estimation of observable variables. The present paper\napplies this analysis to latent-variable estimation and determines its\ntheoretical performance. Our results clarify behavior of the convergence of the\nposterior distribution. It is found that the posterior of the\nobservable-variable estimation can be different from the one in the\nlatent-variable estimation. Because of the difference, the Markov chain Monte\nCarlo method based on the parameter and the latent variable cannot construct\nthe desired posterior distribution.\n", "versions": [{"version": "v1", "created": "Tue, 15 May 2012 01:53:26 GMT"}, {"version": "v2", "created": "Wed, 23 May 2012 02:18:21 GMT"}, {"version": "v3", "created": "Wed, 30 Jan 2013 05:00:42 GMT"}, {"version": "v4", "created": "Mon, 13 May 2013 08:26:31 GMT"}, {"version": "v5", "created": "Thu, 23 Jan 2014 07:21:22 GMT"}], "update_date": "2014-01-24", "authors_parsed": [["Yamazaki", "Keisuke", ""]]}, {"id": "1205.3997", "submitter": "Pedro Alejandro Ortega", "authors": "Pedro A. Ortega and Daniel A. Braun", "title": "Free Energy and the Generalized Optimality Equations for Sequential\n  Decision Making", "comments": "10 pages, 2 figures", "journal-ref": "European Workshop on Reinforcement Learning 2012", "doi": null, "report-no": null, "categories": "stat.ML cs.AI cs.GT cs.SY", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The free energy functional has recently been proposed as a variational\nprinciple for bounded rational decision-making, since it instantiates a natural\ntrade-off between utility gains and information processing costs that can be\naxiomatically derived. Here we apply the free energy principle to general\ndecision trees that include both adversarial and stochastic environments. We\nderive generalized sequential optimality equations that not only include the\nBellman optimality equations as a limit case, but also lead to well-known\ndecision-rules such as Expectimax, Minimax and Expectiminimax. We show how\nthese decision-rules can be derived from a single free energy principle that\nassigns a resource parameter to each node in the decision tree. These resource\nparameters express a concrete computational cost that can be measured as the\namount of samples that are needed from the distribution that belongs to each\nnode. The free energy principle therefore provides the normative basis for\ngeneralized optimality equations that account for both adversarial and\nstochastic environments.\n", "versions": [{"version": "v1", "created": "Thu, 17 May 2012 18:06:45 GMT"}], "update_date": "2012-05-18", "authors_parsed": [["Ortega", "Pedro A.", ""], ["Braun", "Daniel A.", ""]]}, {"id": "1205.4120", "submitter": "Hao Wang", "authors": "Hao Wang", "title": "Two New Algorithms for Solving Covariance Graphical Lasso Based on\n  Coordinate Descent and ECM", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.CO stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Covariance graphical lasso applies a lasso penalty on the elements of the\ncovariance matrix. This method is useful because it not only produces sparse\nestimation of covariance matrix but also discovers marginal independence\nstructures by generating zeros in the covariance matrix. We propose and explore\ntwo new algorithms for solving the covariance graphical lasso problem. Our new\nalgorithms are based on coordinate descent and ECM. We show that these two\nalgorithms are more attractive than the only existing competing algorithm of\nBien and Tibshirani (2011) in terms of simplicity, speed and stability. We also\ndiscuss convergence properties of our algorithms.\n", "versions": [{"version": "v1", "created": "Fri, 18 May 2012 09:33:29 GMT"}], "update_date": "2012-05-21", "authors_parsed": [["Wang", "Hao", ""]]}, {"id": "1205.4159", "submitter": "Wray Buntine", "authors": "Changyou Chen, Wray Buntine and Nan Ding", "title": "Theory of Dependent Hierarchical Normalized Random Measures", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG math.ST stat.ML stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper presents theory for Normalized Random Measures (NRMs), Normalized\nGeneralized Gammas (NGGs), a particular kind of NRM, and Dependent Hierarchical\nNRMs which allow networks of dependent NRMs to be analysed. These have been\nused, for instance, for time-dependent topic modelling. In this paper, we first\nintroduce some mathematical background of completely random measures (CRMs) and\ntheir construction from Poisson processes, and then introduce NRMs and NGGs.\nSlice sampling is also introduced for posterior inference. The dependency\noperators in Poisson processes and for the corresponding CRMs and NRMs is then\nintroduced and Posterior inference for the NGG presented. Finally, we give\ndependency and composition results when applying these operators to NRMs so\nthey can be used in a network with hierarchical and dependent relations.\n", "versions": [{"version": "v1", "created": "Fri, 18 May 2012 13:56:17 GMT"}, {"version": "v2", "created": "Fri, 25 May 2012 05:32:57 GMT"}], "update_date": "2012-05-28", "authors_parsed": [["Chen", "Changyou", ""], ["Buntine", "Wray", ""], ["Ding", "Nan", ""]]}, {"id": "1205.4217", "submitter": "Nathaniel Korda", "authors": "Emilie Kaufmann, Nathaniel Korda and R\\'emi Munos", "title": "Thompson Sampling: An Asymptotically Optimal Finite Time Analysis", "comments": "15 pages, 2 figures, submitted to ALT (Algorithmic Learning Theory)", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The question of the optimality of Thompson Sampling for solving the\nstochastic multi-armed bandit problem had been open since 1933. In this paper\nwe answer it positively for the case of Bernoulli rewards by providing the\nfirst finite-time analysis that matches the asymptotic rate given in the Lai\nand Robbins lower bound for the cumulative regret. The proof is accompanied by\na numerical comparison with other optimal policies, experiments that have been\nlacking in the literature until now for the Bernoulli case.\n", "versions": [{"version": "v1", "created": "Fri, 18 May 2012 19:00:51 GMT"}, {"version": "v2", "created": "Thu, 19 Jul 2012 13:59:13 GMT"}], "update_date": "2012-07-20", "authors_parsed": [["Kaufmann", "Emilie", ""], ["Korda", "Nathaniel", ""], ["Munos", "R\u00e9mi", ""]]}, {"id": "1205.4343", "submitter": "Andres Munoz", "authors": "Mehryar Mohri and Andres Munoz Medina", "title": "New Analysis and Algorithm for Learning with Drifting Distributions", "comments": "15 pages, 2 figures to be published in volume 7568 of the Lecture\n  Notes in Computer Science series", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a new analysis of the problem of learning with drifting\ndistributions in the batch setting using the notion of discrepancy. We prove\nlearning bounds based on the Rademacher complexity of the hypothesis set and\nthe discrepancy of distributions both for a drifting PAC scenario and a\ntracking scenario. Our bounds are always tighter and in some cases\nsubstantially improve upon previous ones based on the $L_1$ distance. We also\npresent a generalization of the standard on-line to batch conversion to the\ndrifting scenario in terms of the discrepancy and arbitrary convex combinations\nof hypotheses. We introduce a new algorithm exploiting these learning\nguarantees, which we show can be formulated as a simple QP. Finally, we report\nthe results of preliminary experiments demonstrating the benefits of this\nalgorithm.\n", "versions": [{"version": "v1", "created": "Sat, 19 May 2012 16:09:15 GMT"}, {"version": "v2", "created": "Sun, 26 Aug 2012 00:15:55 GMT"}], "update_date": "2012-08-28", "authors_parsed": [["Mohri", "Mehryar", ""], ["Medina", "Andres Munoz", ""]]}, {"id": "1205.4377", "submitter": "Kirill Trapeznikov", "authors": "Kirill Trapeznikov, Venkatesh Saligrama, David Castanon", "title": "Multi-Stage Classifier Design", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In many classification systems, sensing modalities have different acquisition\ncosts. It is often {\\it unnecessary} to use every modality to classify a\nmajority of examples. We study a multi-stage system in a prediction time cost\nreduction setting, where the full data is available for training, but for a\ntest example, measurements in a new modality can be acquired at each stage for\nan additional cost. We seek decision rules to reduce the average measurement\nacquisition cost. We formulate an empirical risk minimization problem (ERM) for\na multi-stage reject classifier, wherein the stage $k$ classifier either\nclassifies a sample using only the measurements acquired so far or rejects it\nto the next stage where more attributes can be acquired for a cost. To solve\nthe ERM problem, we show that the optimal reject classifier at each stage is a\ncombination of two binary classifiers, one biased towards positive examples and\nthe other biased towards negative examples. We use this parameterization to\nconstruct stage-by-stage global surrogate risk, develop an iterative algorithm\nin the boosting framework and present convergence and generalization results.\nWe test our work on synthetic, medical and explosives detection datasets. Our\nresults demonstrate that substantial cost reduction without a significant\nsacrifice in accuracy is achievable.\n", "versions": [{"version": "v1", "created": "Sun, 20 May 2012 03:15:13 GMT"}, {"version": "v2", "created": "Tue, 29 Jan 2013 16:54:30 GMT"}], "update_date": "2013-01-30", "authors_parsed": [["Trapeznikov", "Kirill", ""], ["Saligrama", "Venkatesh", ""], ["Castanon", "David", ""]]}, {"id": "1205.4471", "submitter": "Zhilin Zhang", "authors": "Bhaskar D. Rao, Zhilin Zhang, Yuzhe Jin", "title": "Sparse Signal Recovery in the Presence of Intra-Vector and Inter-Vector\n  Correlation", "comments": "Invited review paper of 2012 International Conference on Signal\n  Processing and Communications (SPCOM 2012)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IT cs.LG math.IT stat.ME stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This work discusses the problem of sparse signal recovery when there is\ncorrelation among the values of non-zero entries. We examine intra-vector\ncorrelation in the context of the block sparse model and inter-vector\ncorrelation in the context of the multiple measurement vector model, as well as\ntheir combination. Algorithms based on the sparse Bayesian learning are\npresented and the benefits of incorporating correlation at the algorithm level\nare discussed. The impact of correlation on the limits of support recovery is\nalso discussed highlighting the different impact intra-vector and inter-vector\ncorrelations have on such limits.\n", "versions": [{"version": "v1", "created": "Sun, 20 May 2012 23:56:17 GMT"}], "update_date": "2012-05-22", "authors_parsed": [["Rao", "Bhaskar D.", ""], ["Zhang", "Zhilin", ""], ["Jin", "Yuzhe", ""]]}, {"id": "1205.4476", "submitter": "Deniz Akdemir", "authors": "Deniz Akdemir and Nicolas Heslot", "title": "Soft Rule Ensembles for Statistical Learning", "comments": "arXiv admin note: text overlap with arXiv:1112.3699", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this article supervised learning problems are solved using soft rule\nensembles. We first review the importance sampling learning ensembles (ISLE)\napproach that is useful for generating hard rules. The soft rules are then\nobtained with logistic regression from the corresponding hard rules. In order\nto deal with the perfect separation problem related to the logistic regression,\nFirth's bias corrected likelihood is used. Various examples and simulation\nresults show that soft rule ensembles can improve predictive performance over\nhard rule ensembles.\n", "versions": [{"version": "v1", "created": "Mon, 21 May 2012 01:46:04 GMT"}, {"version": "v2", "created": "Thu, 23 Aug 2012 16:14:45 GMT"}, {"version": "v3", "created": "Fri, 22 Feb 2013 17:03:20 GMT"}], "update_date": "2015-03-20", "authors_parsed": [["Akdemir", "Deniz", ""], ["Heslot", "Nicolas", ""]]}, {"id": "1205.4481", "submitter": "Hua Ouyang", "authors": "Hua Ouyang, Alexander Gray", "title": "Stochastic Smoothing for Nonsmooth Minimizations: Accelerating SGD by\n  Exploiting Structure", "comments": "Full length version of ICML'12 with all proofs. In this version, a\n  bug in proving Theorem 6 is fixed. We'd like to thank Dr. Francesco Orabona\n  for pointing it out", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.CO stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this work we consider the stochastic minimization of nonsmooth convex loss\nfunctions, a central problem in machine learning. We propose a novel algorithm\ncalled Accelerated Nonsmooth Stochastic Gradient Descent (ANSGD), which\nexploits the structure of common nonsmooth loss functions to achieve optimal\nconvergence rates for a class of problems including SVMs. It is the first\nstochastic algorithm that can achieve the optimal O(1/t) rate for minimizing\nnonsmooth loss functions (with strong convexity). The fast rates are confirmed\nby empirical comparisons, in which ANSGD significantly outperforms previous\nsubgradient descent algorithms including SGD.\n", "versions": [{"version": "v1", "created": "Mon, 21 May 2012 03:29:17 GMT"}, {"version": "v2", "created": "Mon, 2 Jul 2012 14:53:38 GMT"}, {"version": "v3", "created": "Wed, 25 Jul 2012 15:15:42 GMT"}, {"version": "v4", "created": "Mon, 1 Oct 2012 16:55:06 GMT"}], "update_date": "2012-10-02", "authors_parsed": [["Ouyang", "Hua", ""], ["Gray", "Alexander", ""]]}, {"id": "1205.4546", "submitter": "Myunghwan Kim", "authors": "Myunghwan Kim and Jure Leskovec", "title": "Latent Multi-group Membership Graph Model", "comments": "10 pages, 4 figures, 4 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.SI physics.soc-ph stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We develop the Latent Multi-group Membership Graph (LMMG) model, a model of\nnetworks with rich node feature structure. In the LMMG model, each node belongs\nto multiple groups and each latent group models the occurrence of links as well\nas the node feature structure. The LMMG can be used to summarize the network\nstructure, to predict links between the nodes, and to predict missing features\nof a node. We derive efficient inference and learning algorithms and evaluate\nthe predictive performance of the LMMG on several social and document network\ndatasets.\n", "versions": [{"version": "v1", "created": "Mon, 21 May 2012 09:56:10 GMT"}], "update_date": "2012-05-22", "authors_parsed": [["Kim", "Myunghwan", ""], ["Leskovec", "Jure", ""]]}, {"id": "1205.4591", "submitter": "Georg M. Goerg", "authors": "Georg M. Goerg", "title": "Forecastable Component Analysis (ForeCA)", "comments": "10 pages, 4 figures; ICML 2013", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  I introduce Forecastable Component Analysis (ForeCA), a novel dimension\nreduction technique for temporally dependent signals. Based on a new\nforecastability measure, ForeCA finds an optimal transformation to separate a\nmultivariate time series into a forecastable and an orthogonal white noise\nspace. I present a converging algorithm with a fast eigenvector solution.\nApplications to financial and macro-economic time series show that ForeCA can\nsuccessfully discover informative structure, which can be used for forecasting\nas well as classification. The R package ForeCA\n(http://cran.r-project.org/web/packages/ForeCA/index.html) accompanies this\nwork and is publicly available on CRAN.\n", "versions": [{"version": "v1", "created": "Mon, 21 May 2012 13:17:46 GMT"}, {"version": "v2", "created": "Wed, 12 Dec 2012 04:35:24 GMT"}, {"version": "v3", "created": "Sat, 4 May 2013 19:33:05 GMT"}], "update_date": "2013-05-07", "authors_parsed": [["Goerg", "Georg M.", ""]]}, {"id": "1205.4656", "submitter": "Steffen Gr\\\"unew\\\"alder", "authors": "Steffen Gr\\\"unew\\\"alder, Guy Lever, Luca Baldassarre, Sam Patterson,\n  Arthur Gretton, Massimilano Pontil", "title": "Conditional mean embeddings as regressors - supplementary", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We demonstrate an equivalence between reproducing kernel Hilbert space (RKHS)\nembeddings of conditional distributions and vector-valued regressors. This\nconnection introduces a natural regularized loss function which the RKHS\nembeddings minimise, providing an intuitive understanding of the embeddings and\na justification for their use. Furthermore, the equivalence allows the\napplication of vector-valued regression methods and results to the problem of\nlearning conditional distributions. Using this link we derive a sparse version\nof the embedding by considering alternative formulations. Further, by applying\nconvergence results for vector-valued regression to the embedding problem we\nderive minimax convergence rates which are O(\\log(n)/n) -- compared to current\nstate of the art rates of O(n^{-1/4}) -- and are valid under milder and more\nintuitive assumptions. These minimax upper rates coincide with lower rates up\nto a logarithmic factor, showing that the embedding method achieves nearly\noptimal rates. We study our sparse embedding algorithm in a reinforcement\nlearning task where the algorithm shows significant improvement in sparsity\nover an incomplete Cholesky decomposition.\n", "versions": [{"version": "v1", "created": "Mon, 21 May 2012 16:43:02 GMT"}, {"version": "v2", "created": "Tue, 24 Jul 2012 13:15:47 GMT"}], "update_date": "2012-07-25", "authors_parsed": [["Gr\u00fcnew\u00e4lder", "Steffen", ""], ["Lever", "Guy", ""], ["Baldassarre", "Luca", ""], ["Patterson", "Sam", ""], ["Gretton", "Arthur", ""], ["Pontil", "Massimilano", ""]]}, {"id": "1205.4770", "submitter": "James Sharpnack", "authors": "Mladen Kolar, James Sharpnack", "title": "Variance function estimation in high-dimensions", "comments": "Appearing in Proceedings of the 29 th International Conference on\n  Machine Learning, Edinburgh, Scotland, UK, 2012", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider the high-dimensional heteroscedastic regression model, where the\nmean and the log variance are modeled as a linear combination of input\nvariables. Existing literature on high-dimensional linear regres- sion models\nhas largely ignored non-constant error variances, even though they commonly\noccur in a variety of applications ranging from biostatis- tics to finance. In\nthis paper we study a class of non-convex penalized pseudolikelihood estimators\nfor both the mean and variance parameters. We show that the Heteroscedastic\nIterative Penalized Pseudolikelihood Optimizer (HIPPO) achieves the oracle\nproperty, that is, we prove that the rates of convergence are the same as if\nthe true model was known. We demonstrate numerical properties of the procedure\non a simulation study and real world data.\n", "versions": [{"version": "v1", "created": "Mon, 21 May 2012 23:35:49 GMT"}], "update_date": "2012-05-23", "authors_parsed": [["Kolar", "Mladen", ""], ["Sharpnack", "James", ""]]}, {"id": "1205.5012", "submitter": "Jason Lee", "authors": "Jason D. Lee and Trevor J. Hastie", "title": "Learning Mixed Graphical Models", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.CV cs.LG math.OC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider the problem of learning the structure of a pairwise graphical\nmodel over continuous and discrete variables. We present a new pairwise model\nfor graphical models with both continuous and discrete variables that is\namenable to structure learning. In previous work, authors have considered\nstructure learning of Gaussian graphical models and structure learning of\ndiscrete models. Our approach is a natural generalization of these two lines of\nwork to the mixed case. The penalization scheme involves a novel symmetric use\nof the group-lasso norm and follows naturally from a particular parametrization\nof the model.\n", "versions": [{"version": "v1", "created": "Tue, 22 May 2012 19:20:07 GMT"}, {"version": "v2", "created": "Mon, 11 Jun 2012 06:46:47 GMT"}, {"version": "v3", "created": "Wed, 3 Jul 2013 23:06:52 GMT"}], "update_date": "2013-07-05", "authors_parsed": [["Lee", "Jason D.", ""], ["Hastie", "Trevor J.", ""]]}, {"id": "1205.5050", "submitter": "Jacob Bien", "authors": "Jacob Bien, Jonathan Taylor, Robert Tibshirani", "title": "A lasso for hierarchical interactions", "comments": "Published in at http://dx.doi.org/10.1214/13-AOS1096 the Annals of\n  Statistics (http://www.imstat.org/aos/) by the Institute of Mathematical\n  Statistics (http://www.imstat.org)", "journal-ref": "Annals of Statistics 2013, Vol. 41, No. 3, 1111-1141", "doi": "10.1214/13-AOS1096", "report-no": "IMS-AOS-AOS1096", "categories": "stat.ME math.ST stat.ML stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We add a set of convex constraints to the lasso to produce sparse interaction\nmodels that honor the hierarchy restriction that an interaction only be\nincluded in a model if one or both variables are marginally important. We give\na precise characterization of the effect of this hierarchy constraint, prove\nthat hierarchy holds with probability one and derive an unbiased estimate for\nthe degrees of freedom of our estimator. A bound on this estimate reveals the\namount of fitting \"saved\" by the hierarchy constraint. We distinguish between\nparameter sparsity - the number of nonzero coefficients - and practical\nsparsity - the number of raw variables one must measure to make a new\nprediction. Hierarchy focuses on the latter, which is more closely tied to\nimportant data collection concerns such as cost, time and effort. We develop an\nalgorithm, available in the R package hierNet, and perform an empirical study\nof our method.\n", "versions": [{"version": "v1", "created": "Tue, 22 May 2012 20:33:35 GMT"}, {"version": "v2", "created": "Thu, 31 Jan 2013 20:26:31 GMT"}, {"version": "v3", "created": "Wed, 19 Jun 2013 10:13:53 GMT"}], "update_date": "2013-06-20", "authors_parsed": [["Bien", "Jacob", ""], ["Taylor", "Jonathan", ""], ["Tibshirani", "Robert", ""]]}, {"id": "1205.5075", "submitter": "Shuo Xiang", "authors": "Shuo Xiang, Xiaotong Shen, Jieping Ye", "title": "Efficient Sparse Group Feature Selection via Nonconvex Optimization", "comments": "Accepted by the 30th International Conference on Machine Learning\n  (ICML 2013)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Sparse feature selection has been demonstrated to be effective in handling\nhigh-dimensional data. While promising, most of the existing works use convex\nmethods, which may be suboptimal in terms of the accuracy of feature selection\nand parameter estimation. In this paper, we expand a nonconvex paradigm to\nsparse group feature selection, which is motivated by applications that require\nidentifying the underlying group structure and performing feature selection\nsimultaneously. The main contributions of this article are twofold: (1)\nstatistically, we introduce a nonconvex sparse group feature selection model\nwhich can reconstruct the oracle estimator. Therefore, consistent feature\nselection and parameter estimation can be achieved; (2) computationally, we\npropose an efficient algorithm that is applicable to large-scale problems.\nNumerical results suggest that the proposed nonconvex method compares favorably\nagainst its competitors on synthetic data and real-world applications, thus\nachieving desired goal of delivering high performance.\n", "versions": [{"version": "v1", "created": "Wed, 23 May 2012 00:02:01 GMT"}, {"version": "v2", "created": "Fri, 18 Jan 2013 21:06:49 GMT"}], "update_date": "2013-01-22", "authors_parsed": [["Xiang", "Shuo", ""], ["Shen", "Xiaotong", ""], ["Ye", "Jieping", ""]]}, {"id": "1205.5819", "submitter": "Damjan Kalajdzievski", "authors": "Damjan Kalajdzievski", "title": "Measurability Aspects of the Compactness Theorem for Sample Compression\n  Schemes", "comments": "Latex 2e, 64 pages, 1 figure. This is an M.Sc. thesis defended on\n  July 4'th 2012 at the University of Ottawa, Canada, under the supervision of\n  Dr. V. Pestov, and with examiners Dr. J. Levy and Dr. S. Zilles", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  It was proved in 1998 by Ben-David and Litman that a concept space has a\nsample compression scheme of size d if and only if every finite subspace has a\nsample compression scheme of size d. In the compactness theorem, measurability\nof the hypotheses of the created sample compression scheme is not guaranteed;\nat the same time measurability of the hypotheses is a necessary condition for\nlearnability. In this thesis we discuss when a sample compression scheme,\ncreated from com- pression schemes on finite subspaces via the compactness\ntheorem, have measurable hypotheses. We show that if X is a standard Borel\nspace with a d-maximum and universally separable concept class C, then (X,C)\nhas a sample compression scheme of size d with universally Borel measurable\nhypotheses. Additionally we introduce a new variant of compression scheme\ncalled a copy sample compression scheme.\n", "versions": [{"version": "v1", "created": "Fri, 25 May 2012 20:38:55 GMT"}, {"version": "v2", "created": "Tue, 17 Jul 2012 04:35:11 GMT"}], "update_date": "2015-03-20", "authors_parsed": [["Kalajdzievski", "Damjan", ""]]}, {"id": "1205.5868", "submitter": "Kei Hirose", "authors": "Kei Hirose, Michio Yamamoto", "title": "Sparse estimation via nonconcave penalized likelihood in a factor\n  analysis model", "comments": "23 pages, 2 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.CO stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider the problem of sparse estimation in a factor analysis model. A\ntraditional estimation procedure in use is the following two-step approach: the\nmodel is estimated by maximum likelihood method and then a rotation technique\nis utilized to find sparse factor loadings. However, the maximum likelihood\nestimates cannot be obtained when the number of variables is much larger than\nthe number of observations. Furthermore, even if the maximum likelihood\nestimates are available, the rotation technique does not often produce a\nsufficiently sparse solution. In order to handle these problems, this paper\nintroduces a penalized likelihood procedure that imposes a nonconvex penalty on\nthe factor loadings. We show that the penalized likelihood procedure can be\nviewed as a generalization of the traditional two-step approach, and the\nproposed methodology can produce sparser solutions than the rotation technique.\nA new algorithm via the EM algorithm along with coordinate descent is\nintroduced to compute the entire solution path, which permits the application\nto a wide variety of convex and nonconvex penalties. Monte Carlo simulations\nare conducted to investigate the performance of our modeling strategy. A real\ndata example is also given to illustrate our procedure.\n", "versions": [{"version": "v1", "created": "Sat, 26 May 2012 10:04:27 GMT"}, {"version": "v2", "created": "Tue, 29 May 2012 14:28:46 GMT"}, {"version": "v3", "created": "Fri, 15 Mar 2013 11:41:15 GMT"}], "update_date": "2013-03-18", "authors_parsed": [["Hirose", "Kei", ""], ["Yamamoto", "Michio", ""]]}, {"id": "1205.5920", "submitter": "Nam Lee", "authors": "Nam H. Lee and Jordan Yoder and Minh Tang and Carey E Priebe", "title": "On latent position inference from doubly stochastic messaging activities", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML math.ST stat.ME stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We model messaging activities as a hierarchical doubly stochastic point\nprocess with three main levels, and develop an iterative algorithm for\ninferring actors' relative latent positions from a stream of messaging activity\ndata. Each of the message-exchanging actors is modeled as a process in a latent\nspace. The actors' latent positions are assumed to be influenced by the\ndistribution of a much larger population over the latent space. Each actor's\nmovement in the latent space is modeled as being governed by two parameters\nthat we call confidence and visibility, in addition to dependence on the\npopulation distribution. The messaging frequency between a pair of actors is\nassumed to be inversely proportional to the distance between their latent\npositions. Our inference algorithm is based on a projection approach to an\nonline filtering problem. The algorithm associates each actor with a\nprobability density-valued process, and each probability density is assumed to\nbe a mixture of basis functions. For efficient numerical experiments, we\nfurther develop our algorithm for the case where the basis functions are\nobtained by translating and scaling a standard Gaussian density.\n", "versions": [{"version": "v1", "created": "Sat, 26 May 2012 22:30:58 GMT"}, {"version": "v2", "created": "Tue, 29 May 2012 00:52:45 GMT"}, {"version": "v3", "created": "Wed, 6 Feb 2013 21:04:11 GMT"}, {"version": "v4", "created": "Thu, 14 Feb 2013 03:19:13 GMT"}, {"version": "v5", "created": "Thu, 25 Apr 2013 15:02:17 GMT"}], "update_date": "2013-04-26", "authors_parsed": [["Lee", "Nam H.", ""], ["Yoder", "Jordan", ""], ["Tang", "Minh", ""], ["Priebe", "Carey E", ""]]}, {"id": "1205.6031", "submitter": "Xin Guo", "authors": "Wen-Jun Shen, Hau-San Wong, Quan-Wu Xiao, Xin Guo, Stephen Smale", "title": "Towards a Mathematical Foundation of Immunology and Amino Acid Chains", "comments": "updated on June 25, 2012", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG q-bio.GN", "license": "http://creativecommons.org/licenses/by-nc-sa/3.0/", "abstract": "  We attempt to set a mathematical foundation of immunology and amino acid\nchains. To measure the similarities of these chains, a kernel on strings is\ndefined using only the sequence of the chains and a good amino acid\nsubstitution matrix (e.g. BLOSUM62). The kernel is used in learning machines to\npredict binding affinities of peptides to human leukocyte antigens DR (HLA-DR)\nmolecules. On both fixed allele (Nielsen and Lund 2009) and pan-allele (Nielsen\net.al. 2010) benchmark databases, our algorithm achieves the state-of-the-art\nperformance. The kernel is also used to define a distance on an HLA-DR allele\nset based on which a clustering analysis precisely recovers the serotype\nclassifications assigned by WHO (Nielsen and Lund 2009, and Marsh et.al. 2010).\nThese results suggest that our kernel relates well the chain structure of both\npeptides and HLA-DR molecules to their biological functions, and that it offers\na simple, powerful and promising methodology to immunology and amino acid chain\nstudies.\n", "versions": [{"version": "v1", "created": "Mon, 28 May 2012 05:47:52 GMT"}, {"version": "v2", "created": "Mon, 25 Jun 2012 04:45:37 GMT"}], "update_date": "2012-06-26", "authors_parsed": [["Shen", "Wen-Jun", ""], ["Wong", "Hau-San", ""], ["Xiao", "Quan-Wu", ""], ["Guo", "Xin", ""], ["Smale", "Stephen", ""]]}, {"id": "1205.6210", "submitter": "Christian Sigg", "authors": "Christian D. Sigg and Tomas Dikk and Joachim M. Buhmann", "title": "Learning Dictionaries with Bounded Self-Coherence", "comments": "4 pages, 2 figures; IEEE Signal Processing Letters, vol. 19, no. 12,\n  2012", "journal-ref": null, "doi": "10.1109/LSP.2012.2223757", "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Sparse coding in learned dictionaries has been established as a successful\napproach for signal denoising, source separation and solving inverse problems\nin general. A dictionary learning method adapts an initial dictionary to a\nparticular signal class by iteratively computing an approximate factorization\nof a training data matrix into a dictionary and a sparse coding matrix. The\nlearned dictionary is characterized by two properties: the coherence of the\ndictionary to observations of the signal class, and the self-coherence of the\ndictionary atoms. A high coherence to the signal class enables the sparse\ncoding of signal observations with a small approximation error, while a low\nself-coherence of the atoms guarantees atom recovery and a more rapid residual\nerror decay rate for the sparse coding algorithm. The two goals of high signal\ncoherence and low self-coherence are typically in conflict, therefore one seeks\na trade-off between them, depending on the application. We present a dictionary\nlearning method with an effective control over the self-coherence of the\ntrained dictionary, enabling a trade-off between maximizing the sparsity of\ncodings and approximating an equiangular tight frame.\n", "versions": [{"version": "v1", "created": "Mon, 28 May 2012 20:06:45 GMT"}, {"version": "v2", "created": "Wed, 17 Oct 2012 09:20:15 GMT"}], "update_date": "2012-10-18", "authors_parsed": [["Sigg", "Christian D.", ""], ["Dikk", "Tomas", ""], ["Buhmann", "Joachim M.", ""]]}, {"id": "1205.6326", "submitter": "Iain Murray", "authors": "Krzysztof Chalupka, Christopher K. I. Williams and Iain Murray", "title": "A Framework for Evaluating Approximation Methods for Gaussian Process\n  Regression", "comments": "19 pages, 4 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Gaussian process (GP) predictors are an important component of many Bayesian\napproaches to machine learning. However, even a straightforward implementation\nof Gaussian process regression (GPR) requires O(n^2) space and O(n^3) time for\na dataset of n examples. Several approximation methods have been proposed, but\nthere is a lack of understanding of the relative merits of the different\napproximations, and in what situations they are most useful. We recommend\nassessing the quality of the predictions obtained as a function of the compute\ntime taken, and comparing to standard baselines (e.g., Subset of Data and\nFITC). We empirically investigate four different approximation algorithms on\nfour different prediction problems, and make our code available to encourage\nfuture comparisons.\n", "versions": [{"version": "v1", "created": "Tue, 29 May 2012 10:59:30 GMT"}, {"version": "v2", "created": "Mon, 5 Nov 2012 17:39:32 GMT"}], "update_date": "2012-11-06", "authors_parsed": [["Chalupka", "Krzysztof", ""], ["Williams", "Christopher K. I.", ""], ["Murray", "Iain", ""]]}, {"id": "1205.6523", "submitter": "Jana Gevertz", "authors": "Chamont Wang, Jana Gevertz, Chaur-Chin Chen, Leonardo Auslender", "title": "Finding Important Genes from High-Dimensional Data: An Appraisal of\n  Statistical Tests and Machine-Learning Approaches", "comments": "36 pages, 9 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG q-bio.QM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Over the past decades, statisticians and machine-learning researchers have\ndeveloped literally thousands of new tools for the reduction of\nhigh-dimensional data in order to identify the variables most responsible for a\nparticular trait. These tools have applications in a plethora of settings,\nincluding data analysis in the fields of business, education, forensics, and\nbiology (such as microarray, proteomics, brain imaging), to name a few.\n  In the present work, we focus our investigation on the limitations and\npotential misuses of certain tools in the analysis of the benchmark colon\ncancer data (2,000 variables; Alon et al., 1999) and the prostate cancer data\n(6,033 variables; Efron, 2010, 2008). Our analysis demonstrates that models\nthat produce 100% accuracy measures often select different sets of genes and\ncannot stand the scrutiny of parameter estimates and model stability.\n  Furthermore, we created a host of simulation datasets and \"artificial\ndiseases\" to evaluate the reliability of commonly used statistical and data\nmining tools. We found that certain widely used models can classify the data\nwith 100% accuracy without using any of the variables responsible for the\ndisease. With moderate sample size and suitable pre-screening, stochastic\ngradient boosting will be shown to be a superior model for gene selection and\nvariable screening from high-dimensional datasets.\n", "versions": [{"version": "v1", "created": "Wed, 30 May 2012 01:23:01 GMT"}], "update_date": "2012-05-31", "authors_parsed": [["Wang", "Chamont", ""], ["Gevertz", "Jana", ""], ["Chen", "Chaur-Chin", ""], ["Auslender", "Leonardo", ""]]}, {"id": "1205.7009", "submitter": "Yaojia Zhu", "authors": "Yaojia Zhu and Xiaoran Yan and Cristopher Moore", "title": "Oriented and Degree-generated Block Models: Generating and Inferring\n  Communities with Inhomogeneous Degree Distributions", "comments": "22 pages, 3 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.SI cond-mat.stat-mech physics.soc-ph stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The stochastic block model is a powerful tool for inferring community\nstructure from network topology. However, it predicts a Poisson degree\ndistribution within each community, while most real-world networks have a\nheavy-tailed degree distribution. The degree-corrected block model can\naccommodate arbitrary degree distributions within communities. But since it\ntakes the vertex degrees as parameters rather than generating them, it cannot\nuse them to help it classify the vertices, and its natural generalization to\ndirected graphs cannot even use the orientations of the edges. In this paper,\nwe present variants of the block model with the best of both worlds: they can\nuse vertex degrees and edge orientations in the classification process, while\ntolerating heavy-tailed degree distributions within communities. We show that\nfor some networks, including synthetic networks and networks of word\nadjacencies in English text, these new block models achieve a higher accuracy\nthan either standard or degree-corrected block models.\n", "versions": [{"version": "v1", "created": "Thu, 31 May 2012 14:36:44 GMT"}], "update_date": "2012-06-01", "authors_parsed": [["Zhu", "Yaojia", ""], ["Yan", "Xiaoran", ""], ["Moore", "Cristopher", ""]]}]