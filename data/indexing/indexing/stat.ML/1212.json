[{"id": "1212.0139", "submitter": "Alexandre Chotard", "authors": "Alexandre Chotard (INRIA Saclay - Ile de France, LRI), Anne Auger\n  (INRIA Saclay - Ile de France), Nikolaus Hansen (INRIA Saclay - Ile de\n  France)", "title": "Cumulative Step-size Adaptation on Linear Functions", "comments": "arXiv admin note: substantial text overlap with arXiv:1206.1208", "journal-ref": "PPSN 2012 (2012) 72-81", "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The CSA-ES is an Evolution Strategy with Cumulative Step size Adaptation,\nwhere the step size is adapted measuring the length of a so-called cumulative\npath. The cumulative path is a combination of the previous steps realized by\nthe algorithm, where the importance of each step decreases with time. This\narticle studies the CSA-ES on composites of strictly increasing functions with\naffine linear functions through the investigation of its underlying Markov\nchains. Rigorous results on the change and the variation of the step size are\nderived with and without cumulation. The step-size diverges geometrically fast\nin most cases. Furthermore, the influence of the cumulation parameter is\nstudied.\n", "versions": [{"version": "v1", "created": "Sat, 1 Dec 2012 17:46:34 GMT"}], "update_date": "2013-02-19", "authors_parsed": [["Chotard", "Alexandre", "", "INRIA Saclay - Ile de France, LRI"], ["Auger", "Anne", "", "INRIA Saclay - Ile de France"], ["Hansen", "Nikolaus", "", "INRIA Saclay - Ile de\n  France"]]}, {"id": "1212.0171", "submitter": "Nicholas Ruozzi", "authors": "Nicholas Ruozzi and Sekhar Tatikonda", "title": "Message-Passing Algorithms for Quadratic Minimization", "comments": null, "journal-ref": "Journal of Machine Learning Research. 14 (Aug) :2287-2314, 2013", "doi": null, "report-no": null, "categories": "cs.IT cs.LG math.IT stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Gaussian belief propagation (GaBP) is an iterative algorithm for computing\nthe mean of a multivariate Gaussian distribution, or equivalently, the minimum\nof a multivariate positive definite quadratic function. Sufficient conditions,\nsuch as walk-summability, that guarantee the convergence and correctness of\nGaBP are known, but GaBP may fail to converge to the correct solution given an\narbitrary positive definite quadratic function. As was observed in previous\nwork, the GaBP algorithm fails to converge if the computation trees produced by\nthe algorithm are not positive definite. In this work, we will show that the\nfailure modes of the GaBP algorithm can be understood via graph covers, and we\nprove that a parameterized generalization of the min-sum algorithm can be used\nto ensure that the computation trees remain positive definite whenever the\ninput matrix is positive definite. We demonstrate that the resulting algorithm\nis closely related to other iterative schemes for quadratic minimization such\nas the Gauss-Seidel and Jacobi algorithms. Finally, we observe, empirically,\nthat there always exists a choice of parameters such that the above\ngeneralization of the GaBP algorithm converges.\n", "versions": [{"version": "v1", "created": "Sun, 2 Dec 2012 00:34:04 GMT"}], "update_date": "2014-01-07", "authors_parsed": [["Ruozzi", "Nicholas", ""], ["Tatikonda", "Sekhar", ""]]}, {"id": "1212.0388", "submitter": "Loc Tran H", "authors": "Loc Tran", "title": "Hypergraph and protein function prediction with gene expression data", "comments": "12 pages, 1 figure. arXiv admin note: substantial text overlap with\n  arXiv:1211.4289", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG q-bio.QM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Most network-based protein (or gene) function prediction methods are based on\nthe assumption that the labels of two adjacent proteins in the network are\nlikely to be the same. However, assuming the pairwise relationship between\nproteins or genes is not complete, the information a group of genes that show\nvery similar patterns of expression and tend to have similar functions (i.e.\nthe functional modules) is missed. The natural way overcoming the information\nloss of the above assumption is to represent the gene expression data as the\nhypergraph. Thus, in this paper, the three un-normalized, random walk, and\nsymmetric normalized hypergraph Laplacian based semi-supervised learning\nmethods applied to hypergraph constructed from the gene expression data in\norder to predict the functions of yeast proteins are introduced. Experiment\nresults show that the average accuracy performance measures of these three\nhypergraph Laplacian based semi-supervised learning methods are the same.\nHowever, their average accuracy performance measures of these three methods are\nmuch greater than the average accuracy performance measures of un-normalized\ngraph Laplacian based semi-supervised learning method (i.e. the baseline method\nof this paper) applied to gene co-expression network created from the gene\nexpression data.\n", "versions": [{"version": "v1", "created": "Mon, 3 Dec 2012 13:53:39 GMT"}], "update_date": "2012-12-04", "authors_parsed": [["Tran", "Loc", ""]]}, {"id": "1212.0451", "submitter": "Sirisha Rambhatla", "authors": "Sirisha Rambhatla and Jarvis D. Haupt", "title": "Semi-blind Source Separation via Sparse Representations and Online\n  Dictionary Learning", "comments": "5 pages, In Proceedings of the 47th Asilomar Conference on Signals\n  Systems and Computers, 2013", "journal-ref": null, "doi": "10.1109/ACSSC.2013.6810587", "report-no": null, "categories": "cs.SD stat.AP stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This work examines a semi-blind single-channel source separation problem. Our\nspecific aim is to separate one source whose local structure is approximately\nknown, from another a priori unspecified background source, given only a single\nlinear combination of the two sources. We propose a separation technique based\non local sparse approximations along the lines of recent efforts in sparse\nrepresentations and dictionary learning. A key feature of our procedure is the\nonline learning of dictionaries (using only the data itself) to sparsely model\nthe background source, which facilitates its separation from the\npartially-known source. Our approach is applicable to source separation\nproblems in various application domains; here, we demonstrate the performance\nof our proposed approach via simulation on a stylized audio source separation\ntask.\n", "versions": [{"version": "v1", "created": "Mon, 3 Dec 2012 17:06:41 GMT"}, {"version": "v2", "created": "Sun, 25 Jan 2015 02:31:09 GMT"}], "update_date": "2015-01-27", "authors_parsed": [["Rambhatla", "Sirisha", ""], ["Haupt", "Jarvis D.", ""]]}, {"id": "1212.0463", "submitter": "Daniel McDonald", "authors": "Daniel J. McDonald and Cosma Rohilla Shalizi and Mark Schervish", "title": "Nonparametric risk bounds for time-series forecasting", "comments": "34 pages, 3 figures", "journal-ref": "Journal of Machine Learning Research. (2017). Vol 18. p. 1-40", "doi": null, "report-no": null, "categories": "math.ST cs.LG stat.ML stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We derive generalization error bounds for traditional time-series forecasting\nmodels. Our results hold for many standard forecasting tools including\nautoregressive models, moving average models, and, more generally, linear\nstate-space models. These non-asymptotic bounds need only weak assumptions on\nthe data-generating process, yet allow forecasters to select among competing\nmodels and to guarantee, with high probability, that their chosen model will\nperform well. We motivate our techniques with and apply them to standard\neconomic and financial forecasting tools---a GARCH model for predicting equity\nvolatility and a dynamic stochastic general equilibrium model (DSGE), the\nstandard tool in macroeconomic forecasting. We demonstrate in particular how\nour techniques can aid forecasters and policy makers in choosing models which\nbehave well under uncertainty and mis-specification.\n", "versions": [{"version": "v1", "created": "Mon, 3 Dec 2012 17:42:45 GMT"}, {"version": "v2", "created": "Sat, 10 Sep 2016 20:05:05 GMT"}], "update_date": "2017-07-25", "authors_parsed": [["McDonald", "Daniel J.", ""], ["Shalizi", "Cosma Rohilla", ""], ["Schervish", "Mark", ""]]}, {"id": "1212.0467", "submitter": "Praneeth Netrapalli", "authors": "Prateek Jain, Praneeth Netrapalli and Sujay Sanghavi", "title": "Low-rank Matrix Completion using Alternating Minimization", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG math.OC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Alternating minimization represents a widely applicable and empirically\nsuccessful approach for finding low-rank matrices that best fit the given data.\nFor example, for the problem of low-rank matrix completion, this method is\nbelieved to be one of the most accurate and efficient, and formed a major\ncomponent of the winning entry in the Netflix Challenge.\n  In the alternating minimization approach, the low-rank target matrix is\nwritten in a bi-linear form, i.e. $X = UV^\\dag$; the algorithm then alternates\nbetween finding the best $U$ and the best $V$. Typically, each alternating step\nin isolation is convex and tractable. However the overall problem becomes\nnon-convex and there has been almost no theoretical understanding of when this\napproach yields a good result.\n  In this paper we present first theoretical analysis of the performance of\nalternating minimization for matrix completion, and the related problem of\nmatrix sensing. For both these problems, celebrated recent results have shown\nthat they become well-posed and tractable once certain (now standard)\nconditions are imposed on the problem. We show that alternating minimization\nalso succeeds under similar conditions. Moreover, compared to existing results,\nour paper shows that alternating minimization guarantees faster (in particular,\ngeometric) convergence to the true matrix, while allowing a simpler analysis.\n", "versions": [{"version": "v1", "created": "Mon, 3 Dec 2012 17:57:50 GMT"}], "update_date": "2012-12-04", "authors_parsed": [["Jain", "Prateek", ""], ["Netrapalli", "Praneeth", ""], ["Sanghavi", "Sujay", ""]]}, {"id": "1212.0478", "submitter": "Po-Ling Loh", "authors": "Po-Ling Loh, Martin J. Wainwright", "title": "Structure estimation for discrete graphical models: Generalized\n  covariance matrices and their inverses", "comments": "Published in at http://dx.doi.org/10.1214/13-AOS1162 the Annals of\n  Statistics (http://www.imstat.org/aos/) by the Institute of Mathematical\n  Statistics (http://www.imstat.org)", "journal-ref": "Annals of Statistics 2013, Vol. 41, No. 6, 3022-3049", "doi": "10.1214/13-AOS1162", "report-no": "IMS-AOS-AOS1162", "categories": "stat.ML math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We investigate the relationship between the structure of a discrete graphical\nmodel and the support of the inverse of a generalized covariance matrix. We\nshow that for certain graph structures, the support of the inverse covariance\nmatrix of indicator variables on the vertices of a graph reflects the\nconditional independence structure of the graph. Our work extends results that\nhave previously been established only in the context of multivariate Gaussian\ngraphical models, thereby addressing an open question about the significance of\nthe inverse covariance matrix of a non-Gaussian distribution. The proof\nexploits a combination of ideas from the geometry of exponential families,\njunction tree theory and convex analysis. These population-level results have\nvarious consequences for graph selection methods, both known and novel,\nincluding a novel method for structure estimation for missing or corrupted\nobservations. We provide nonasymptotic guarantees for such methods and\nillustrate the sharpness of these predictions via simulations.\n", "versions": [{"version": "v1", "created": "Mon, 3 Dec 2012 18:20:35 GMT"}, {"version": "v2", "created": "Mon, 6 Jan 2014 07:01:20 GMT"}], "update_date": "2014-01-07", "authors_parsed": [["Loh", "Po-Ling", ""], ["Wainwright", "Martin J.", ""]]}, {"id": "1212.0634", "submitter": "Shifeng Xiong Doc", "authors": "Shifeng Xiong", "title": "Better subset regression", "comments": "24 pages, 1 figure", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME math.ST stat.CO stat.ML stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  To find efficient screening methods for high dimensional linear regression\nmodels, this paper studies the relationship between model fitting and screening\nperformance. Under a sparsity assumption, we show that a subset that includes\nthe true submodel always yields smaller residual sum of squares (i.e., has\nbetter model fitting) than all that do not in a general asymptotic setting.\nThis indicates that, for screening important variables, we could follow a\n\"better fitting, better screening\" rule, i.e., pick a \"better\" subset that has\nbetter model fitting. To seek such a better subset, we consider the\noptimization problem associated with best subset regression. An EM algorithm,\ncalled orthogonalizing subset screening, and its accelerating version are\nproposed for searching for the best subset. Although the two algorithms cannot\nguarantee that a subset they yield is the best, their monotonicity property\nmakes the subset have better model fitting than initial subsets generated by\npopular screening methods, and thus the subset can have better screening\nperformance asymptotically. Simulation results show that our methods are very\ncompetitive in high dimensional variable screening even for finite sample\nsizes.\n", "versions": [{"version": "v1", "created": "Tue, 4 Dec 2012 07:49:48 GMT"}, {"version": "v2", "created": "Tue, 19 Mar 2013 02:58:03 GMT"}], "update_date": "2013-03-20", "authors_parsed": [["Xiong", "Shifeng", ""]]}, {"id": "1212.0695", "submitter": "Emanuele Frandi", "authors": "Emanuele Frandi, Ricardo Nanculef, Maria Grazia Gasparo, Stefano Lodi,\n  Claudio Sartori", "title": "Training Support Vector Machines Using Frank-Wolfe Optimization Methods", "comments": null, "journal-ref": "International Journal on Pattern Recognition and Artificial\n  Intelligence, 27(3), 2013", "doi": "10.1142/S0218001413600033", "report-no": null, "categories": "cs.LG cs.CV math.OC stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Training a Support Vector Machine (SVM) requires the solution of a quadratic\nprogramming problem (QP) whose computational complexity becomes prohibitively\nexpensive for large scale datasets. Traditional optimization methods cannot be\ndirectly applied in these cases, mainly due to memory restrictions.\n  By adopting a slightly different objective function and under mild conditions\non the kernel used within the model, efficient algorithms to train SVMs have\nbeen devised under the name of Core Vector Machines (CVMs). This framework\nexploits the equivalence of the resulting learning problem with the task of\nbuilding a Minimal Enclosing Ball (MEB) problem in a feature space, where data\nis implicitly embedded by a kernel function.\n  In this paper, we improve on the CVM approach by proposing two novel methods\nto build SVMs based on the Frank-Wolfe algorithm, recently revisited as a fast\nmethod to approximate the solution of a MEB problem. In contrast to CVMs, our\nalgorithms do not require to compute the solutions of a sequence of\nincreasingly complex QPs and are defined by using only analytic optimization\nsteps. Experiments on a large collection of datasets show that our methods\nscale better than CVMs in most cases, sometimes at the price of a slightly\nlower accuracy. As CVMs, the proposed methods can be easily extended to machine\nlearning problems other than binary classification. However, effective\nclassifiers are also obtained using kernels which do not satisfy the condition\nrequired by CVMs and can thus be used for a wider set of problems.\n", "versions": [{"version": "v1", "created": "Tue, 4 Dec 2012 12:05:31 GMT"}], "update_date": "2014-01-29", "authors_parsed": [["Frandi", "Emanuele", ""], ["Nanculef", "Ricardo", ""], ["Gasparo", "Maria Grazia", ""], ["Lodi", "Stefano", ""], ["Sartori", "Claudio", ""]]}, {"id": "1212.0873", "submitter": "Peter Richtarik", "authors": "Peter Richt\\'arik and Martin Tak\\'a\\v{c}", "title": "Parallel Coordinate Descent Methods for Big Data Optimization", "comments": "43 pages, 8 tables, 6 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.OC cs.AI stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this work we show that randomized (block) coordinate descent methods can\nbe accelerated by parallelization when applied to the problem of minimizing the\nsum of a partially separable smooth convex function and a simple separable\nconvex function. The theoretical speedup, as compared to the serial method, and\nreferring to the number of iterations needed to approximately solve the problem\nwith high probability, is a simple expression depending on the number of\nparallel processors and a natural and easily computable measure of separability\nof the smooth component of the objective function. In the worst case, when no\ndegree of separability is present, there may be no speedup; in the best case,\nwhen the problem is separable, the speedup is equal to the number of\nprocessors. Our analysis also works in the mode when the number of blocks being\nupdated at each iteration is random, which allows for modeling situations with\nbusy or unreliable processors. We show that our algorithm is able to solve a\nLASSO problem involving a matrix with 20 billion nonzeros in 2 hours on a large\nmemory node with 24 cores.\n", "versions": [{"version": "v1", "created": "Tue, 4 Dec 2012 21:10:37 GMT"}, {"version": "v2", "created": "Mon, 25 Nov 2013 21:14:58 GMT"}], "update_date": "2013-11-27", "authors_parsed": [["Richt\u00e1rik", "Peter", ""], ["Tak\u00e1\u010d", "Martin", ""]]}, {"id": "1212.0912", "submitter": "Aleksandr Aravkin", "authors": "Aleksandr Y. Aravkin and Tristan van Leeuwen and Ning Tu", "title": "Sparse seismic imaging using variable projection", "comments": "5 pages, 4 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.OC stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider an important class of signal processing problems where the signal\nof interest is known to be sparse, and can be recovered from data given\nauxiliary information about how the data was generated. For example, a sparse\nGreen's function may be recovered from seismic experimental data using sparsity\noptimization when the source signature is known. Unfortunately, in practice\nthis information is often missing, and must be recovered from data along with\nthe signal using deconvolution techniques.\n  In this paper, we present a novel methodology to simultaneously solve for the\nsparse signal and auxiliary parameters using a recently proposed variable\nprojection technique. Our main contribution is to combine variable projection\nwith sparsity promoting optimization, obtaining an efficient algorithm for\nlarge-scale sparse deconvolution problems. We demonstrate the algorithm on a\nseismic imaging example.\n", "versions": [{"version": "v1", "created": "Wed, 5 Dec 2012 01:12:03 GMT"}], "update_date": "2012-12-06", "authors_parsed": [["Aravkin", "Aleksandr Y.", ""], ["van Leeuwen", "Tristan", ""], ["Tu", "Ning", ""]]}, {"id": "1212.0945", "submitter": "Allon G. Percus", "authors": "Cristina Garcia-Cardona, Arjuna Flenner and Allon G. Percus", "title": "Multiclass Diffuse Interface Models for Semi-Supervised Learning on\n  Graphs", "comments": "9 pages, to appear in Proceedings of the 2nd International Conference\n  on Pattern Recognition Applications and Methods (ICPRAM 2013)", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG math.ST physics.data-an stat.TH", "license": "http://creativecommons.org/licenses/publicdomain/", "abstract": "  We present a graph-based variational algorithm for multiclass classification\nof high-dimensional data, motivated by total variation techniques. The energy\nfunctional is based on a diffuse interface model with a periodic potential. We\naugment the model by introducing an alternative measure of smoothness that\npreserves symmetry among the class labels. Through this modification of the\nstandard Laplacian, we construct an efficient multiclass method that allows for\nsharp transitions between classes. The experimental results demonstrate that\nour approach is competitive with the state of the art among other graph-based\nalgorithms.\n", "versions": [{"version": "v1", "created": "Wed, 5 Dec 2012 07:13:54 GMT"}], "update_date": "2012-12-06", "authors_parsed": [["Garcia-Cardona", "Cristina", ""], ["Flenner", "Arjuna", ""], ["Percus", "Allon G.", ""]]}, {"id": "1212.0960", "submitter": "Hyun Joon Jung", "authors": "Hyun Joon Jung and Matthew Lease", "title": "Evaluating Classifiers Without Expert Labels", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.IR stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper considers the challenge of evaluating a set of classifiers, as\ndone in shared task evaluations like the KDD Cup or NIST TREC, without expert\nlabels. While expert labels provide the traditional cornerstone for evaluating\nstatistical learners, limited or expensive access to experts represents a\npractical bottleneck. Instead, we seek methodology for estimating performance\nof the classifiers which is more scalable than expert labeling yet preserves\nhigh correlation with evaluation based on expert labels. We consider both: 1)\nusing only labels automatically generated by the classifiers (blind\nevaluation); and 2) using labels obtained via crowdsourcing. While\ncrowdsourcing methods are lauded for scalability, using such data for\nevaluation raises serious concerns given the prevalence of label noise. In\nregard to blind evaluation, two broad strategies are investigated: combine &\nscore and score & combine methods infer a single pseudo-gold label set by\naggregating classifier labels; classifiers are then evaluated based on this\nsingle pseudo-gold label set. On the other hand, score & combine methods: 1)\nsample multiple label sets from classifier outputs, 2) evaluate classifiers on\neach label set, and 3) average classifier performance across label sets. When\nadditional crowd labels are also collected, we investigate two alternative\navenues for exploiting them: 1) direct evaluation of classifiers; or 2)\nsupervision of combine & score methods. To assess generality of our techniques,\nclassifier performance is measured using four common classification metrics,\nwith statistical significance tests. Finally, we measure both score and rank\ncorrelations between estimated classifier performance vs. actual performance\naccording to expert judgments. Rigorous evaluation of classifiers from the TREC\n2011 Crowdsourcing Track shows reliable evaluation can be achieved without\nreliance on expert labels.\n", "versions": [{"version": "v1", "created": "Wed, 5 Dec 2012 08:15:36 GMT"}], "update_date": "2012-12-06", "authors_parsed": [["Jung", "Hyun Joon", ""], ["Lease", "Matthew", ""]]}, {"id": "1212.0967", "submitter": "Sameer Singh", "authors": "Sameer Singh and Thore Graepel", "title": "Compiling Relational Database Schemata into Probabilistic Graphical\n  Models", "comments": "NIPS 2012 Workshop on Probabilistic Programming", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.DB cs.LG stat.ML", "license": "http://creativecommons.org/licenses/by-nc-sa/3.0/", "abstract": "  Instead of requiring a domain expert to specify the probabilistic\ndependencies of the data, in this work we present an approach that uses the\nrelational DB schema to automatically construct a Bayesian graphical model for\na database. This resulting model contains customized distributions for columns,\nlatent variables that cluster the data, and factors that reflect and represent\nthe foreign key links. Experiments demonstrate the accuracy of the model and\nthe scalability of inference on synthetic and real-world data.\n", "versions": [{"version": "v1", "created": "Wed, 5 Dec 2012 08:52:33 GMT"}], "update_date": "2012-12-07", "authors_parsed": [["Singh", "Sameer", ""], ["Graepel", "Thore", ""]]}, {"id": "1212.0975", "submitter": "Arya Iranmehr", "authors": "Hamed Masnadi-Shirazi, Nuno Vasconcelos and Arya Iranmehr", "title": "Cost-Sensitive Support Vector Machines", "comments": "32 pages, 4 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A new procedure for learning cost-sensitive SVM(CS-SVM) classifiers is\nproposed. The SVM hinge loss is extended to the cost sensitive setting, and the\nCS-SVM is derived as the minimizer of the associated risk. The extension of the\nhinge loss draws on recent connections between risk minimization and\nprobability elicitation. These connections are generalized to cost-sensitive\nclassification, in a manner that guarantees consistency with the cost-sensitive\nBayes risk, and associated Bayes decision rule. This ensures that optimal\ndecision rules, under the new hinge loss, implement the Bayes-optimal\ncost-sensitive classification boundary. Minimization of the new hinge loss is\nshown to be a generalization of the classic SVM optimization problem, and can\nbe solved by identical procedures. The dual problem of CS-SVM is carefully\nscrutinized by means of regularization theory and sensitivity analysis and the\nCS-SVM algorithm is substantiated. The proposed algorithm is also extended to\ncost-sensitive learning with example dependent costs. The minimum cost\nsensitive risk is proposed as the performance measure and is connected to ROC\nanalysis through vector optimization. The resulting algorithm avoids the\nshortcomings of previous approaches to cost-sensitive SVM design, and is shown\nto have superior experimental performance on a large number of cost sensitive\nand imbalanced datasets.\n", "versions": [{"version": "v1", "created": "Wed, 5 Dec 2012 09:24:11 GMT"}, {"version": "v2", "created": "Sun, 15 Feb 2015 11:17:57 GMT"}], "update_date": "2015-02-17", "authors_parsed": [["Masnadi-Shirazi", "Hamed", ""], ["Vasconcelos", "Nuno", ""], ["Iranmehr", "Arya", ""]]}, {"id": "1212.1100", "submitter": "Jim Smith Dr", "authors": "J. E. Smith, P. Caleb-Solly, M. A. Tahir, D. Sannen, H. van-Brussel", "title": "Making Early Predictions of the Accuracy of Machine Learning\n  Applications", "comments": "35 pagers, 12 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The accuracy of machine learning systems is a widely studied research topic.\nEstablished techniques such as cross-validation predict the accuracy on unseen\ndata of the classifier produced by applying a given learning method to a given\ntraining data set. However, they do not predict whether incurring the cost of\nobtaining more data and undergoing further training will lead to higher\naccuracy. In this paper we investigate techniques for making such early\npredictions. We note that when a machine learning algorithm is presented with a\ntraining set the classifier produced, and hence its error, will depend on the\ncharacteristics of the algorithm, on training set's size, and also on its\nspecific composition. In particular we hypothesise that if a number of\nclassifiers are produced, and their observed error is decomposed into bias and\nvariance terms, then although these components may behave differently, their\nbehaviour may be predictable.\n  We test our hypothesis by building models that, given a measurement taken\nfrom the classifier created from a limited number of samples, predict the\nvalues that would be measured from the classifier produced when the full data\nset is presented. We create separate models for bias, variance and total error.\nOur models are built from the results of applying ten different machine\nlearning algorithms to a range of data sets, and tested with \"unseen\"\nalgorithms and datasets. We analyse the results for various numbers of initial\ntraining samples, and total dataset sizes. Results show that our predictions\nare very highly correlated with the values observed after undertaking the extra\ntraining. Finally we consider the more complex case where an ensemble of\nheterogeneous classifiers is trained, and show how we can accurately estimate\nan upper bound on the accuracy achievable after further training.\n", "versions": [{"version": "v1", "created": "Wed, 5 Dec 2012 17:07:39 GMT"}], "update_date": "2012-12-06", "authors_parsed": [["Smith", "J. E.", ""], ["Caleb-Solly", "P.", ""], ["Tahir", "M. A.", ""], ["Sannen", "D.", ""], ["van-Brussel", "H.", ""]]}, {"id": "1212.1108", "submitter": "Luis Ortiz", "authors": "Joshua Belanich and Luis E. Ortiz", "title": "On the Convergence Properties of Optimal AdaBoost", "comments": "66 pp, 7 figs, 1 table; Change - presentation; dominated and\n  effective weak-classifiers; experiments with dec. stumps in real-world data:\n  reduction in #effective and unique stumps, may explain \"resistance to\n  overfitting;\" log-growth #unique stumps with #rounds -> \"AdaBoost-cycles\n  Conjecture\" likely false in general; and new generalization bounds; submitted\n  to MLJ 4/10/15", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  AdaBoost is one of the most popular machine-learning algorithms. It is simple\nto implement and often found very effective by practitioners, while still being\nmathematically elegant and theoretically sound. AdaBoost's behavior in\npractice, and in particular the test-error behavior, has puzzled many eminent\nresearchers for over a decade: It seems to defy our general intuition in\nmachine learning regarding the fundamental trade-off between model complexity\nand generalization performance. In this paper, we establish the convergence of\n\"Optimal AdaBoost,\" a term coined by Rudin, Daubechies, and Schapire in 2004.\nWe prove the convergence, with the number of rounds, of the classifier itself,\nits generalization error, and its resulting margins for fixed data sets, under\ncertain reasonable conditions. More generally, we prove that the time/per-round\naverage of almost any function of the example weights converges. Our approach\nis to frame AdaBoost as a dynamical system, to provide sufficient conditions\nfor the existence of an invariant measure, and to employ tools from ergodic\ntheory. Unlike previous work, we do not assume AdaBoost cycles; actually, we\npresent empirical evidence against it on real-world datasets. Our main\ntheoretical results hold under a weaker condition. We show sufficient empirical\nevidence that Optimal AdaBoost always met the condition on every real-world\ndataset we tried. Our results formally ground future convergence-rate analyses,\nand may even provide opportunities for slight algorithmic modifications to\noptimize the generalization ability of AdaBoost classifiers, thus reducing a\npractitioner's burden of deciding how long to run the algorithm.\n", "versions": [{"version": "v1", "created": "Wed, 5 Dec 2012 17:29:59 GMT"}, {"version": "v2", "created": "Sat, 11 Apr 2015 16:18:43 GMT"}], "update_date": "2015-04-14", "authors_parsed": [["Belanich", "Joshua", ""], ["Ortiz", "Luis E.", ""]]}, {"id": "1212.1131", "submitter": "Lior Rokach", "authors": "Gilad Katz, Guy Shani, Bracha Shapira, Lior Rokach", "title": "Using Wikipedia to Boost SVD Recommender Systems", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.IR stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Singular Value Decomposition (SVD) has been used successfully in recent years\nin the area of recommender systems. In this paper we present how this model can\nbe extended to consider both user ratings and information from Wikipedia. By\nmapping items to Wikipedia pages and quantifying their similarity, we are able\nto use this information in order to improve recommendation accuracy, especially\nwhen the sparsity is high. Another advantage of the proposed approach is the\nfact that it can be easily integrated into any other SVD implementation,\nregardless of additional parameters that may have been added to it. Preliminary\nexperimental results on the MovieLens dataset are encouraging.\n", "versions": [{"version": "v1", "created": "Wed, 5 Dec 2012 19:03:39 GMT"}], "update_date": "2012-12-06", "authors_parsed": [["Katz", "Gilad", ""], ["Shani", "Guy", ""], ["Shapira", "Bracha", ""], ["Rokach", "Lior", ""]]}, {"id": "1212.1143", "submitter": "Jake Bouvrie", "authors": "Jake Bouvrie and Mauro Maggioni", "title": "Multiscale Markov Decision Problems: Compression, Solution, and Transfer\n  Learning", "comments": "86 pages, 15 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.SY math.OC stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Many problems in sequential decision making and stochastic control often have\nnatural multiscale structure: sub-tasks are assembled together to accomplish\ncomplex goals. Systematically inferring and leveraging hierarchical structure,\nparticularly beyond a single level of abstraction, has remained a longstanding\nchallenge. We describe a fast multiscale procedure for repeatedly compressing,\nor homogenizing, Markov decision processes (MDPs), wherein a hierarchy of\nsub-problems at different scales is automatically determined. Coarsened MDPs\nare themselves independent, deterministic MDPs, and may be solved using\nexisting algorithms. The multiscale representation delivered by this procedure\ndecouples sub-tasks from each other and can lead to substantial improvements in\nconvergence rates both locally within sub-problems and globally across\nsub-problems, yielding significant computational savings. A second fundamental\naspect of this work is that these multiscale decompositions yield new transfer\nopportunities across different problems, where solutions of sub-tasks at\ndifferent levels of the hierarchy may be amenable to transfer to new problems.\nLocalized transfer of policies and potential operators at arbitrary scales is\nemphasized. Finally, we demonstrate compression and transfer in a collection of\nillustrative domains, including examples involving discrete and continuous\nstatespaces.\n", "versions": [{"version": "v1", "created": "Wed, 5 Dec 2012 20:09:11 GMT"}], "update_date": "2012-12-06", "authors_parsed": [["Bouvrie", "Jake", ""], ["Maggioni", "Mauro", ""]]}, {"id": "1212.1180", "submitter": "Mark Kon", "authors": "Mark A. Kon and Leszek Plaskota", "title": "On Some Integrated Approaches to Inference", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present arguments for the formulation of unified approach to different\nstandard continuous inference methods from partial information. It is claimed\nthat an explicit partition of information into a priori (prior knowledge) and a\nposteriori information (data) is an important way of standardizing inference\napproaches so that they can be compared on a normative scale, and so that\nnotions of optimal algorithms become farther-reaching. The inference methods\nconsidered include neural network approaches, information-based complexity, and\nMonte Carlo, spline, and regularization methods. The model is an extension of\ncurrently used continuous complexity models, with a class of algorithms in the\nform of optimization methods, in which an optimization functional (involving\nthe data) is minimized. This extends the family of current approaches in\ncontinuous complexity theory, which include the use of interpolatory algorithms\nin worst and average case settings.\n", "versions": [{"version": "v1", "created": "Wed, 5 Dec 2012 21:19:35 GMT"}], "update_date": "2012-12-07", "authors_parsed": [["Kon", "Mark A.", ""], ["Plaskota", "Leszek", ""]]}, {"id": "1212.1182", "submitter": "Minh Tang", "authors": "Minh Tang, Daniel L. Sussman, Carey E. Priebe", "title": "Universally consistent vertex classification for latent positions graphs", "comments": "Published in at http://dx.doi.org/10.1214/13-AOS1112 the Annals of\n  Statistics (http://www.imstat.org/aos/) by the Institute of Mathematical\n  Statistics (http://www.imstat.org)", "journal-ref": "Annals of Statistics 2013, Vol. 41, No. 3, 1406-1430", "doi": "10.1214/13-AOS1112", "report-no": "IMS-AOS-AOS1112", "categories": "stat.ML math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this work we show that, using the eigen-decomposition of the adjacency\nmatrix, we can consistently estimate feature maps for latent position graphs\nwith positive definite link function $\\kappa$, provided that the latent\npositions are i.i.d. from some distribution F. We then consider the\nexploitation task of vertex classification where the link function $\\kappa$\nbelongs to the class of universal kernels and class labels are observed for a\nnumber of vertices tending to infinity and that the remaining vertices are to\nbe classified. We show that minimization of the empirical $\\varphi$-risk for\nsome convex surrogate $\\varphi$ of 0-1 loss over a class of linear classifiers\nwith increasing complexities yields a universally consistent classifier, that\nis, a classification rule with error converging to Bayes optimal for any\ndistribution F.\n", "versions": [{"version": "v1", "created": "Wed, 5 Dec 2012 21:24:11 GMT"}, {"version": "v2", "created": "Wed, 13 Mar 2013 22:55:09 GMT"}, {"version": "v3", "created": "Tue, 13 Aug 2013 07:38:44 GMT"}], "update_date": "2013-08-14", "authors_parsed": [["Tang", "Minh", ""], ["Sussman", "Daniel L.", ""], ["Priebe", "Carey E.", ""]]}, {"id": "1212.1263", "submitter": "Mark Kon", "authors": "Mark A. Kon", "title": "On the probabilistic continuous complexity conjecture", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper we prove the probabilistic continuous complexity conjecture. In\ncontinuous complexity theory, this states that the complexity of solving a\ncontinuous problem with probability approaching 1 converges (in this limit) to\nthe complexity of solving the same problem in its worst case. We prove the\nconjecture holds if and only if space of problem elements is uniformly convex.\nThe non-uniformly convex case has a striking counterexample in the problem of\nidentifying a Brownian path in Wiener space, where it is shown that\nprobabilistic complexity converges to only half of the worst case complexity in\nthis limit.\n", "versions": [{"version": "v1", "created": "Thu, 6 Dec 2012 09:17:39 GMT"}], "update_date": "2012-12-07", "authors_parsed": [["Kon", "Mark A.", ""]]}, {"id": "1212.1384", "submitter": "Jos\\'e Enrique Chac\\'on", "authors": "Jos\\'e E. Chac\\'on", "title": "Clusters and water flows: a novel approach to modal clustering through\n  Morse theory", "comments": "25 pages, 7 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST math.DG stat.ML stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The problem of finding groups in data (cluster analysis) has been extensively\nstudied by researchers from the fields of Statistics and Computer Science,\namong others. However, despite its popularity it is widely recognized that the\ninvestigation of some theoretical aspects of clustering has been relatively\nsparse. One of the main reasons for this lack of theoretical results is surely\nthe fact that, unlike the situation with other statistical problems as\nregression or classification, for some of the cluster methodologies it is quite\ndifficult to specify a population goal to which the data-based clustering\nalgorithms should try to get close. This paper aims to provide some insight\ninto the theoretical foundations of the usual nonparametric approach to\nclustering, which understands clusters as regions of high density, by\npresenting an explicit formulation for the ideal population clustering.\n", "versions": [{"version": "v1", "created": "Thu, 6 Dec 2012 17:20:43 GMT"}, {"version": "v2", "created": "Thu, 10 Jan 2013 19:17:33 GMT"}], "update_date": "2013-10-10", "authors_parsed": [["Chac\u00f3n", "Jos\u00e9 E.", ""]]}, {"id": "1212.1496", "submitter": "Massimiliano Pontil", "authors": "Andreas Maurer and Massimiliano Pontil", "title": "Excess risk bounds for multitask learning with trace norm regularization", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Trace norm regularization is a popular method of multitask learning. We give\nexcess risk bounds with explicit dependence on the number of tasks, the number\nof examples per task and properties of the data distribution. The bounds are\nindependent of the dimension of the input space, which may be infinite as in\nthe case of reproducing kernel Hilbert spaces. A byproduct of the proof are\nbounds on the expected norm of sums of random positive semidefinite matrices\nwith subexponential moments.\n", "versions": [{"version": "v1", "created": "Thu, 6 Dec 2012 23:06:32 GMT"}, {"version": "v2", "created": "Mon, 14 Jan 2013 17:55:24 GMT"}], "update_date": "2013-01-15", "authors_parsed": [["Maurer", "Andreas", ""], ["Pontil", "Massimiliano", ""]]}, {"id": "1212.1524", "submitter": "Ludovic Arnold", "authors": "Ludovic Arnold and Yann Ollivier", "title": "Layer-wise learning of deep generative models", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NE cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  When using deep, multi-layered architectures to build generative models of\ndata, it is difficult to train all layers at once. We propose a layer-wise\ntraining procedure admitting a performance guarantee compared to the global\noptimum. It is based on an optimistic proxy of future performance, the best\nlatent marginal. We interpret auto-encoders in this setting as generative\nmodels, by showing that they train a lower bound of this criterion. We test the\nnew learning procedure against a state of the art method (stacked RBMs), and\nfind it to improve performance. Both theory and experiments highlight the\nimportance, when training deep architectures, of using an inference model (from\ndata to hidden variables) richer than the generative model (from hidden\nvariables to data).\n", "versions": [{"version": "v1", "created": "Fri, 7 Dec 2012 03:14:50 GMT"}, {"version": "v2", "created": "Sat, 16 Feb 2013 13:24:46 GMT"}], "update_date": "2013-02-19", "authors_parsed": [["Arnold", "Ludovic", ""], ["Ollivier", "Yann", ""]]}, {"id": "1212.1666", "submitter": "Ilkka Kivim\\\"aki", "authors": "Ilkka Kivim\\\"aki, Masashi Shimbo, Marco Saerens", "title": "Developments in the theory of randomized shortest paths with a\n  comparison of graph node distances", "comments": "30 pages, 4 figures, 3 tables", "journal-ref": null, "doi": "10.1016/j.physa.2013.09.016", "report-no": null, "categories": "stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  There have lately been several suggestions for parametrized distances on a\ngraph that generalize the shortest path distance and the commute time or\nresistance distance. The need for developing such distances has risen from the\nobservation that the above-mentioned common distances in many situations fail\nto take into account the global structure of the graph. In this article, we\ndevelop the theory of one family of graph node distances, known as the\nrandomized shortest path dissimilarity, which has its foundation in statistical\nphysics. We show that the randomized shortest path dissimilarity can be easily\ncomputed in closed form for all pairs of nodes of a graph. Moreover, we come up\nwith a new definition of a distance measure that we call the free energy\ndistance. The free energy distance can be seen as an upgrade of the randomized\nshortest path dissimilarity as it defines a metric, in addition to which it\nsatisfies the graph-geodetic property. The derivation and computation of the\nfree energy distance are also straightforward. We then make a comparison\nbetween a set of generalized distances that interpolate between the shortest\npath distance and the commute time, or resistance distance. This comparison\nfocuses on the applicability of the distances in graph node clustering and\nclassification. The comparison, in general, shows that the parametrized\ndistances perform well in the tasks. In particular, we see that the results\nobtained with the free energy distance are among the best in all the\nexperiments.\n", "versions": [{"version": "v1", "created": "Fri, 7 Dec 2012 17:51:17 GMT"}, {"version": "v2", "created": "Thu, 3 Oct 2013 10:46:02 GMT"}], "update_date": "2015-03-17", "authors_parsed": [["Kivim\u00e4ki", "Ilkka", ""], ["Shimbo", "Masashi", ""], ["Saerens", "Marco", ""]]}, {"id": "1212.1707", "submitter": "Ramji Venkataramanan", "authors": "Ramji Venkataramanan, Tuhin Sarkar, Sekhar Tatikonda", "title": "Lossy Compression via Sparse Linear Regression: Computationally\n  Efficient Encoding and Decoding", "comments": "14 pages, to appear in IEEE Transactions on Information Theory", "journal-ref": "IEEE Transactions on Information Theory, vol. 60, no. 6, pp.\n  3265-3278, June 2014", "doi": "10.1109/TIT.2014.2314676", "report-no": null, "categories": "cs.IT math.IT stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose computationally efficient encoders and decoders for lossy\ncompression using a Sparse Regression Code. The codebook is defined by a design\nmatrix and codewords are structured linear combinations of columns of this\nmatrix. The proposed encoding algorithm sequentially chooses columns of the\ndesign matrix to successively approximate the source sequence. It is shown to\nachieve the optimal distortion-rate function for i.i.d Gaussian sources under\nthe squared-error distortion criterion. For a given rate, the parameters of the\ndesign matrix can be varied to trade off distortion performance with encoding\ncomplexity. An example of such a trade-off as a function of the block length n\nis the following. With computational resource (space or time) per source sample\nof O((n/\\log n)^2), for a fixed distortion-level above the Gaussian\ndistortion-rate function, the probability of excess distortion decays\nexponentially in n. The Sparse Regression Code is robust in the following\nsense: for any ergodic source, the proposed encoder achieves the optimal\ndistortion-rate function of an i.i.d Gaussian source with the same variance.\nSimulations show that the encoder has good empirical performance, especially at\nlow and moderate rates.\n", "versions": [{"version": "v1", "created": "Fri, 7 Dec 2012 20:54:26 GMT"}, {"version": "v2", "created": "Fri, 28 Mar 2014 15:15:06 GMT"}], "update_date": "2014-05-20", "authors_parsed": [["Venkataramanan", "Ramji", ""], ["Sarkar", "Tuhin", ""], ["Tatikonda", "Sekhar", ""]]}, {"id": "1212.1780", "submitter": "Charanpal Dhanjal", "authors": "Charanpal Dhanjal (LIP6), Nicolas Baskiotis (LIP6), St\\'ephan\n  Cl\\'emen\\c{c}on (LTCI), Nicolas Usunier (LIP6)", "title": "An Empirical Comparison of V-fold Penalisation and Cross Validation for\n  Model Selection in Distribution-Free Regression", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Model selection is a crucial issue in machine-learning and a wide variety of\npenalisation methods (with possibly data dependent complexity penalties) have\nrecently been introduced for this purpose. However their empirical performance\nis generally not well documented in the literature. It is the goal of this\npaper to investigate to which extent such recent techniques can be successfully\nused for the tuning of both the regularisation and kernel parameters in support\nvector regression (SVR) and the complexity measure in regression trees (CART).\nThis task is traditionally solved via V-fold cross-validation (VFCV), which\ngives efficient results for a reasonable computational cost. A disadvantage\nhowever of VFCV is that the procedure is known to provide an asymptotically\nsuboptimal risk estimate as the number of examples tends to infinity. Recently,\na penalisation procedure called V-fold penalisation has been proposed to\nimprove on VFCV, supported by theoretical arguments. Here we report on an\nextensive set of experiments comparing V-fold penalisation and VFCV for\nSVR/CART calibration on several benchmark datasets. We highlight cases in which\nVFCV and V-fold penalisation provide poor estimates of the risk respectively\nand introduce a modified penalisation technique to reduce the estimation error.\n", "versions": [{"version": "v1", "created": "Sat, 8 Dec 2012 11:34:10 GMT"}], "update_date": "2012-12-11", "authors_parsed": [["Dhanjal", "Charanpal", "", "LIP6"], ["Baskiotis", "Nicolas", "", "LIP6"], ["Cl\u00e9men\u00e7on", "St\u00e9phan", "", "LTCI"], ["Usunier", "Nicolas", "", "LIP6"]]}, {"id": "1212.1824", "submitter": "Ohad Shamir", "authors": "Ohad Shamir and Tong Zhang", "title": "Stochastic Gradient Descent for Non-smooth Optimization: Convergence\n  Results and Optimal Averaging Schemes", "comments": "To appear in ICML 2013", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG math.OC stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Stochastic Gradient Descent (SGD) is one of the simplest and most popular\nstochastic optimization methods. While it has already been theoretically\nstudied for decades, the classical analysis usually required non-trivial\nsmoothness assumptions, which do not apply to many modern applications of SGD\nwith non-smooth objective functions such as support vector machines. In this\npaper, we investigate the performance of SGD without such smoothness\nassumptions, as well as a running average scheme to convert the SGD iterates to\na solution with optimal optimization accuracy. In this framework, we prove that\nafter T rounds, the suboptimality of the last SGD iterate scales as\nO(log(T)/\\sqrt{T}) for non-smooth convex objective functions, and O(log(T)/T)\nin the non-smooth strongly convex case. To the best of our knowledge, these are\nthe first bounds of this kind, and almost match the minimax-optimal rates\nobtainable by appropriate averaging schemes. We also propose a new and simple\naveraging scheme, which not only attains optimal rates, but can also be easily\ncomputed on-the-fly (in contrast, the suffix averaging scheme proposed in\nRakhlin et al. (2011) is not as simple to implement). Finally, we provide some\nexperimental illustrations.\n", "versions": [{"version": "v1", "created": "Sat, 8 Dec 2012 18:22:42 GMT"}, {"version": "v2", "created": "Fri, 28 Dec 2012 10:58:48 GMT"}], "update_date": "2013-01-01", "authors_parsed": [["Shamir", "Ohad", ""], ["Zhang", "Tong", ""]]}, {"id": "1212.2002", "submitter": "Simon Lacoste-Julien", "authors": "Simon Lacoste-Julien, Mark Schmidt, Francis Bach", "title": "A simpler approach to obtaining an O(1/t) convergence rate for the\n  projected stochastic subgradient method", "comments": "8 pages, 6 figures. Changes with previous version: Added reference to\n  concurrently submitted work arXiv:1212.1824v1; clarifications added; typos\n  corrected; title changed to 'subgradient method' as 'subgradient descent' is\n  misnomer", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG math.OC stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this note, we present a new averaging technique for the projected\nstochastic subgradient method. By using a weighted average with a weight of t+1\nfor each iterate w_t at iteration t, we obtain the convergence rate of O(1/t)\nwith both an easy proof and an easy implementation. The new scheme is compared\nempirically to existing techniques, with similar performance behavior.\n", "versions": [{"version": "v1", "created": "Mon, 10 Dec 2012 09:22:06 GMT"}, {"version": "v2", "created": "Thu, 20 Dec 2012 20:55:23 GMT"}], "update_date": "2012-12-21", "authors_parsed": [["Lacoste-Julien", "Simon", ""], ["Schmidt", "Mark", ""], ["Bach", "Francis", ""]]}, {"id": "1212.2126", "submitter": "Tamara Broderick", "authors": "Tamara Broderick, Brian Kulis, Michael I. Jordan", "title": "MAD-Bayes: MAP-based Asymptotic Derivations from Bayes", "comments": "13 pages, 3 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The classical mixture of Gaussians model is related to K-means via\nsmall-variance asymptotics: as the covariances of the Gaussians tend to zero,\nthe negative log-likelihood of the mixture of Gaussians model approaches the\nK-means objective, and the EM algorithm approaches the K-means algorithm. Kulis\n& Jordan (2012) used this observation to obtain a novel K-means-like algorithm\nfrom a Gibbs sampler for the Dirichlet process (DP) mixture. We instead\nconsider applying small-variance asymptotics directly to the posterior in\nBayesian nonparametric models. This framework is independent of any specific\nBayesian inference algorithm, and it has the major advantage that it\ngeneralizes immediately to a range of models beyond the DP mixture. To\nillustrate, we apply our framework to the feature learning setting, where the\nbeta process and Indian buffet process provide an appropriate Bayesian\nnonparametric prior. We obtain a novel objective function that goes beyond\nclustering to learn (and penalize new) groupings for which we relax the mutual\nexclusivity and exhaustivity assumptions of clustering. We demonstrate several\nother algorithms, all of which are scalable and simple to implement. Empirical\nresults demonstrate the benefits of the new framework.\n", "versions": [{"version": "v1", "created": "Mon, 10 Dec 2012 16:42:44 GMT"}, {"version": "v2", "created": "Fri, 15 Feb 2013 23:48:57 GMT"}], "update_date": "2013-02-19", "authors_parsed": [["Broderick", "Tamara", ""], ["Kulis", "Brian", ""], ["Jordan", "Michael I.", ""]]}, {"id": "1212.2136", "submitter": "Boris Flach", "authors": "Boris Flach", "title": "A class of random fields on complete graphs with tractable partition\n  function", "comments": "accepted for publication in IEEE TPAMI (short paper)", "journal-ref": null, "doi": "10.1109/TPAMI.2013.99", "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The aim of this short note is to draw attention to a method by which the\npartition function and marginal probabilities for a certain class of random\nfields on complete graphs can be computed in polynomial time. This class\nincludes Ising models with homogeneous pairwise potentials but arbitrary\n(inhomogeneous) unary potentials. Similarly, the partition function and\nmarginal probabilities can be computed in polynomial time for random fields on\ncomplete bipartite graphs, provided they have homogeneous pairwise potentials.\nWe expect that these tractable classes of large scale random fields can be very\nuseful for the evaluation of approximation algorithms by providing exact error\nestimates.\n", "versions": [{"version": "v1", "created": "Mon, 10 Dec 2012 17:12:51 GMT"}, {"version": "v2", "created": "Tue, 18 Jun 2013 12:03:42 GMT"}], "update_date": "2013-06-19", "authors_parsed": [["Flach", "Boris", ""]]}, {"id": "1212.2340", "submitter": "Emilie Morvant", "authors": "Pascal Germain, Amaury Habrard (LAHC), Fran\\c{c}ois Laviolette, Emilie\n  Morvant (LIF)", "title": "PAC-Bayesian Learning and Domain Adaptation", "comments": "https://sites.google.com/site/multitradeoffs2012/", "journal-ref": "Multi-Trade-offs in Machine Learning, NIPS 2012 Workshop, Lake\n  Tahoe : United States (2012)", "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In machine learning, Domain Adaptation (DA) arises when the distribution gen-\nerating the test (target) data differs from the one generating the learning\n(source) data. It is well known that DA is an hard task even under strong\nassumptions, among which the covariate-shift where the source and target\ndistributions diverge only in their marginals, i.e. they have the same labeling\nfunction. Another popular approach is to consider an hypothesis class that\nmoves closer the two distributions while implying a low-error for both tasks.\nThis is a VC-dim approach that restricts the complexity of an hypothesis class\nin order to get good generalization. Instead, we propose a PAC-Bayesian\napproach that seeks for suitable weights to be given to each hypothesis in\norder to build a majority vote. We prove a new DA bound in the PAC-Bayesian\ncontext. This leads us to design the first DA-PAC-Bayesian algorithm based on\nthe minimization of the proposed bound. Doing so, we seek for a \\rho-weighted\nmajority vote that takes into account a trade-off between three quantities. The\nfirst two quantities being, as usual in the PAC-Bayesian approach, (a) the\ncomplexity of the majority vote (measured by a Kullback-Leibler divergence) and\n(b) its empirical risk (measured by the \\rho-average errors on the source\nsample). The third quantity is (c) the capacity of the majority vote to\ndistinguish some structural difference between the source and target samples.\n", "versions": [{"version": "v1", "created": "Tue, 11 Dec 2012 09:03:17 GMT"}], "update_date": "2012-12-12", "authors_parsed": [["Germain", "Pascal", "", "LAHC"], ["Habrard", "Amaury", "", "LAHC"], ["Laviolette", "Fran\u00e7ois", "", "LIF"], ["Morvant", "Emilie", "", "LIF"]]}, {"id": "1212.2442", "submitter": "Craig Boutilier", "authors": "Craig Boutilier, Richard S. Zemel, Benjamin Marlin", "title": "Active Collaborative Filtering", "comments": "Appears in Proceedings of the Nineteenth Conference on Uncertainty in\n  Artificial Intelligence (UAI2003)", "journal-ref": null, "doi": null, "report-no": "UAI-P-2003-PG-98-106", "categories": "cs.IR cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Collaborative filtering (CF) allows the preferences of multiple users to be\npooled to make recommendations regarding unseen products. We consider in this\npaper the problem of online and interactive CF: given the current ratings\nassociated with a user, what queries (new ratings) would most improve the\nquality of the recommendations made? We cast this terms of expected value of\ninformation (EVOI); but the online computational cost of computing optimal\nqueries is prohibitive. We show how offline prototyping and computation of\nbounds on EVOI can be used to dramatically reduce the required online\ncomputation. The framework we develop is general, but we focus on derivations\nand empirical study in the specific case of the multiple-cause vector\nquantization model.\n", "versions": [{"version": "v1", "created": "Fri, 19 Oct 2012 15:04:12 GMT"}], "update_date": "2012-12-12", "authors_parsed": [["Boutilier", "Craig", ""], ["Zemel", "Richard S.", ""], ["Marlin", "Benjamin", ""]]}, {"id": "1212.2447", "submitter": "Christopher M. Bishop", "authors": "Christopher M. Bishop, Markus Svensen", "title": "Bayesian Hierarchical Mixtures of Experts", "comments": "Appears in Proceedings of the Nineteenth Conference on Uncertainty in\n  Artificial Intelligence (UAI2003)", "journal-ref": null, "doi": null, "report-no": "UAI-P-2003-PG-57-64", "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The Hierarchical Mixture of Experts (HME) is a well-known tree-based model\nfor regression and classification, based on soft probabilistic splits. In its\noriginal formulation it was trained by maximum likelihood, and is therefore\nprone to over-fitting. Furthermore the maximum likelihood framework offers no\nnatural metric for optimizing the complexity and structure of the tree.\nPrevious attempts to provide a Bayesian treatment of the HME model have relied\neither on ad-hoc local Gaussian approximations or have dealt with related\nmodels representing the joint distribution of both input and output variables.\nIn this paper we describe a fully Bayesian treatment of the HME model based on\nvariational inference. By combining local and global variational methods we\nobtain a rigourous lower bound on the marginal probability of the data under\nthe model. This bound is optimized during the training phase, and its resulting\nvalue can be used for model order selection. We present results using this\napproach for a data set describing robot arm kinematics.\n", "versions": [{"version": "v1", "created": "Fri, 19 Oct 2012 15:03:51 GMT"}], "update_date": "2012-12-12", "authors_parsed": [["Bishop", "Christopher M.", ""], ["Svensen", "Markus", ""]]}, {"id": "1212.2460", "submitter": "Gal Elidan", "authors": "Gal Elidan, Nir Friedman", "title": "The Information Bottleneck EM Algorithm", "comments": "Appears in Proceedings of the Nineteenth Conference on Uncertainty in\n  Artificial Intelligence (UAI2003)", "journal-ref": null, "doi": null, "report-no": "UAI-P-2003-PG-200-208", "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Learning with hidden variables is a central challenge in probabilistic\ngraphical models that has important implications for many real-life problems.\nThe classical approach is using the Expectation Maximization (EM) algorithm.\nThis algorithm, however, can get trapped in local maxima. In this paper we\nexplore a new approach that is based on the Information Bottleneck principle.\nIn this approach, we view the learning problem as a tradeoff between two\ninformation theoretic objectives. The first is to make the hidden variables\nuninformative about the identity of specific instances. The second is to make\nthe hidden variables informative about the observed attributes. By exploring\ndifferent tradeoffs between these two objectives, we can gradually converge on\na high-scoring solution. As we show, the resulting, Information Bottleneck\nExpectation Maximization (IB-EM) algorithm, manages to find solutions that are\nsuperior to standard EM methods.\n", "versions": [{"version": "v1", "created": "Fri, 19 Oct 2012 15:05:02 GMT"}], "update_date": "2012-12-12", "authors_parsed": [["Elidan", "Gal", ""], ["Friedman", "Nir", ""]]}, {"id": "1212.2462", "submitter": "Mathias Drton", "authors": "Mathias Drton, Thomas S. Richardson", "title": "A New Algorithm for Maximum Likelihood Estimation in Gaussian Graphical\n  Models for Marginal Independence", "comments": "Appears in Proceedings of the Nineteenth Conference on Uncertainty in\n  Artificial Intelligence (UAI2003)", "journal-ref": null, "doi": null, "report-no": "UAI-P-2003-PG-184-191", "categories": "stat.ME cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Graphical models with bi-directed edges (<->) represent marginal\nindependence: the absence of an edge between two vertices indicates that the\ncorresponding variables are marginally independent. In this paper, we consider\nmaximum likelihood estimation in the case of continuous variables with a\nGaussian joint distribution, sometimes termed a covariance graph model. We\npresent a new fitting algorithm which exploits standard regression techniques\nand establish its convergence properties. Moreover, we contrast our procedure\nto existing estimation methods.\n", "versions": [{"version": "v1", "created": "Fri, 19 Oct 2012 15:04:52 GMT"}], "update_date": "2012-12-12", "authors_parsed": [["Drton", "Mathias", ""], ["Richardson", "Thomas S.", ""]]}, {"id": "1212.2464", "submitter": "Denver Dash", "authors": "Denver Dash, Marek J. Druzdzel", "title": "A Robust Independence Test for Constraint-Based Learning of Causal\n  Structure", "comments": "Appears in Proceedings of the Nineteenth Conference on Uncertainty in\n  Artificial Intelligence (UAI2003)", "journal-ref": null, "doi": null, "report-no": "UAI-P-2003-PG-167-174", "categories": "cs.AI cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Constraint-based (CB) learning is a formalism for learning a causal network\nwith a database D by performing a series of conditional-independence tests to\ninfer structural information. This paper considers a new test of independence\nthat combines ideas from Bayesian learning, Bayesian network inference, and\nclassical hypothesis testing to produce a more reliable and robust test. The\nnew test can be calculated in the same asymptotic time and space required for\nthe standard tests such as the chi-squared test, but it allows the\nspecification of a prior distribution over parameters and can be used when the\ndatabase is incomplete. We prove that the test is correct, and we demonstrate\nempirically that, when used with a CB causal discovery algorithm with\nnoninformative priors, it recovers structural features more reliably and it\nproduces networks with smaller KL-Divergence, especially as the number of nodes\nincreases or the number of records decreases. Another benefit is the dramatic\nreduction in the probability that a CB algorithm will stall during the search,\nproviding a remedy for an annoying problem plaguing CB learning when the\ndatabase is small.\n", "versions": [{"version": "v1", "created": "Fri, 19 Oct 2012 15:04:44 GMT"}], "update_date": "2012-12-12", "authors_parsed": [["Dash", "Denver", ""], ["Druzdzel", "Marek J.", ""]]}, {"id": "1212.2466", "submitter": "Adrian Corduneanu", "authors": "Adrian Corduneanu, Tommi S. Jaakkola", "title": "On Information Regularization", "comments": "Appears in Proceedings of the Nineteenth Conference on Uncertainty in\n  Artificial Intelligence (UAI2003)", "journal-ref": null, "doi": null, "report-no": "UAI-P-2003-PG-151-158", "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We formulate a principle for classification with the knowledge of the\nmarginal distribution over the data points (unlabeled data). The principle is\ncast in terms of Tikhonov style regularization where the regularization penalty\narticulates the way in which the marginal density should constrain otherwise\nunrestricted conditional distributions. Specifically, the regularization\npenalty penalizes any information introduced between the examples and labels\nbeyond what is provided by the available labeled examples. The work extends\nSzummer and Jaakkola's information regularization (NIPS 2002) to multiple\ndimensions, providing a regularizer independent of the covering of the space\nused in the derivation. We show in addition how the information regularizer can\nbe used as a measure of complexity of the classification task with unlabeled\ndata and prove a relevant sample-complexity bound. We illustrate the\nregularization principle in practice by restricting the class of conditional\ndistributions to be logistic regression models and constructing the\nregularization penalty from a finite set of unlabeled examples.\n", "versions": [{"version": "v1", "created": "Fri, 19 Oct 2012 15:04:36 GMT"}], "update_date": "2012-12-12", "authors_parsed": [["Corduneanu", "Adrian", ""], ["Jaakkola", "Tommi S.", ""]]}, {"id": "1212.2468", "submitter": "David Maxwell Chickering", "authors": "David Maxwell Chickering, Christopher Meek, David Heckerman", "title": "Large-Sample Learning of Bayesian Networks is NP-Hard", "comments": "Appears in Proceedings of the Nineteenth Conference on Uncertainty in\n  Artificial Intelligence (UAI2003)", "journal-ref": null, "doi": null, "report-no": "UAI-P-2003-PG-124-133", "categories": "cs.LG cs.AI stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we provide new complexity results for algorithms that learn\ndiscrete-variable Bayesian networks from data. Our results apply whenever the\nlearning algorithm uses a scoring criterion that favors the simplest model able\nto represent the generative distribution exactly. Our results therefore hold\nwhenever the learning algorithm uses a consistent scoring criterion and is\napplied to a sufficiently large dataset. We show that identifying high-scoring\nstructures is hard, even when we are given an independence oracle, an inference\noracle, and/or an information oracle. Our negative results also apply to the\nlearning of discrete-variable Bayesian networks in which each node has at most\nk parents, for all k > 3.\n", "versions": [{"version": "v1", "created": "Fri, 19 Oct 2012 15:04:28 GMT"}], "update_date": "2012-12-12", "authors_parsed": [["Chickering", "David Maxwell", ""], ["Meek", "Christopher", ""], ["Heckerman", "David", ""]]}, {"id": "1212.2470", "submitter": "Hei Chan", "authors": "Hei Chan, Adnan Darwiche", "title": "Reasoning about Bayesian Network Classifiers", "comments": "Appears in Proceedings of the Nineteenth Conference on Uncertainty in\n  Artificial Intelligence (UAI2003)", "journal-ref": null, "doi": null, "report-no": "UAI-P-2003-PG-107-115", "categories": "cs.LG cs.AI stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Bayesian network classifiers are used in many fields, and one common class of\nclassifiers are naive Bayes classifiers. In this paper, we introduce an\napproach for reasoning about Bayesian network classifiers in which we\nexplicitly convert them into Ordered Decision Diagrams (ODDs), which are then\nused to reason about the properties of these classifiers. Specifically, we\npresent an algorithm for converting any naive Bayes classifier into an ODD, and\nwe show theoretically and experimentally that this algorithm can give us an ODD\nthat is tractable in size even given an intractable number of instances. Since\nODDs are tractable representations of classifiers, our algorithm allows us to\nefficiently test the equivalence of two naive Bayes classifiers and\ncharacterize discrepancies between them. We also show a number of additional\nresults including a count of distinct classifiers that can be induced by\nchanging some CPT in a naive Bayes classifier, and the range of allowable\nchanges to a CPT which keeps the current classifier unchanged.\n", "versions": [{"version": "v1", "created": "Fri, 19 Oct 2012 15:04:17 GMT"}], "update_date": "2012-12-12", "authors_parsed": [["Chan", "Hei", ""], ["Darwiche", "Adnan", ""]]}, {"id": "1212.2472", "submitter": "Daniel J. Lizotte", "authors": "Daniel J. Lizotte, Omid Madani, Russell Greiner", "title": "Budgeted Learning of Naive-Bayes Classifiers", "comments": "Appears in Proceedings of the Nineteenth Conference on Uncertainty in\n  Artificial Intelligence (UAI2003)", "journal-ref": null, "doi": null, "report-no": "UAI-P-2003-PG-378-385", "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Frequently, acquiring training data has an associated cost. We consider the\nsituation where the learner may purchase data during training, subject TO a\nbudget. IN particular, we examine the CASE WHERE each feature label has an\nassociated cost, AND the total cost OF ALL feature labels acquired during\ntraining must NOT exceed the budget.This paper compares methods FOR choosing\nwhich feature label TO purchase next, given the budget AND the CURRENT belief\nstate OF naive Bayes model parameters.Whereas active learning has traditionally\nfocused ON myopic(greedy) strategies FOR query selection, this paper presents a\ntractable method FOR incorporating knowledge OF the budget INTO the decision\nmaking process, which improves performance.\n", "versions": [{"version": "v1", "created": "Fri, 19 Oct 2012 15:06:36 GMT"}], "update_date": "2012-12-12", "authors_parsed": [["Lizotte", "Daniel J.", ""], ["Madani", "Omid", ""], ["Greiner", "Russell", ""]]}, {"id": "1212.2474", "submitter": "Guy Lebanon", "authors": "Guy Lebanon", "title": "Learning Riemannian Metrics", "comments": "Appears in Proceedings of the Nineteenth Conference on Uncertainty in\n  Artificial Intelligence (UAI2003)", "journal-ref": null, "doi": null, "report-no": "UAI-P-2003-PG-362-369", "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a solution to the problem of estimating a Riemannian metric\nassociated with a given differentiable manifold. The metric learning problem is\nbased on minimizing the relative volume of a given set of points. We derive the\ndetails for a family of metrics on the multinomial simplex. The resulting\nmetric has applications in text classification and bears some similarity to\nTFIDF representation of text documents.\n", "versions": [{"version": "v1", "created": "Fri, 19 Oct 2012 15:06:27 GMT"}], "update_date": "2012-12-12", "authors_parsed": [["Lebanon", "Guy", ""]]}, {"id": "1212.2480", "submitter": "Tom Heskes", "authors": "Tom Heskes, Kees Albers, Hilbert Kappen", "title": "Approximate Inference and Constrained Optimization", "comments": "Appears in Proceedings of the Nineteenth Conference on Uncertainty in\n  Artificial Intelligence (UAI2003)", "journal-ref": null, "doi": null, "report-no": "UAI-P-2003-PG-313-320", "categories": "cs.LG cs.AI stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Loopy and generalized belief propagation are popular algorithms for\napproximate inference in Markov random fields and Bayesian networks. Fixed\npoints of these algorithms correspond to extrema of the Bethe and Kikuchi free\nenergy. However, belief propagation does not always converge, which explains\nthe need for approaches that explicitly minimize the Kikuchi/Bethe free energy,\nsuch as CCCP and UPS. Here we describe a class of algorithms that solves this\ntypically nonconvex constrained minimization of the Kikuchi free energy through\na sequence of convex constrained minimizations of upper bounds on the Kikuchi\nfree energy. Intuitively one would expect tighter bounds to lead to faster\nalgorithms, which is indeed convincingly demonstrated in our simulations.\nSeveral ideas are applied to obtain tight convex bounds that yield dramatic\nspeed-ups over CCCP.\n", "versions": [{"version": "v1", "created": "Fri, 19 Oct 2012 15:06:00 GMT"}], "update_date": "2012-12-12", "authors_parsed": [["Heskes", "Tom", ""], ["Albers", "Kees", ""], ["Kappen", "Hilbert", ""]]}, {"id": "1212.2483", "submitter": "Amir Globerson", "authors": "Amir Globerson, Gal Chechik, Naftali Tishby", "title": "Sufficient Dimensionality Reduction with Irrelevant Statistics", "comments": "Appears in Proceedings of the Nineteenth Conference on Uncertainty in\n  Artificial Intelligence (UAI2003)", "journal-ref": null, "doi": null, "report-no": "UAI-P-2003-PG-281-288", "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The problem of finding a reduced dimensionality representation of categorical\nvariables while preserving their most relevant characteristics is fundamental\nfor the analysis of complex data. Specifically, given a co-occurrence matrix of\ntwo variables, one often seeks a compact representation of one variable which\npreserves information about the other variable. We have recently introduced\n``Sufficient Dimensionality Reduction' [GT-2003], a method that extracts\ncontinuous reduced dimensional features whose measurements (i.e., expectation\nvalues) capture maximal mutual information among the variables. However, such\nmeasurements often capture information that is irrelevant for a given task.\nWidely known examples are illumination conditions, which are irrelevant as\nfeatures for face recognition, writing style which is irrelevant as a feature\nfor content classification, and intonation which is irrelevant as a feature for\nspeech recognition. Such irrelevance cannot be deduced apriori, since it\ndepends on the details of the task, and is thus inherently ill defined in the\npurely unsupervised case. Separating relevant from irrelevant features can be\nachieved using additional side data that contains such irrelevant structures.\nThis approach was taken in [CT-2002], extending the information bottleneck\nmethod, which uses clustering to compress the data. Here we use this\nside-information framework to identify features whose measurements are\nmaximally informative for the original data set, but carry as little\ninformation as possible on a side data set. In statistical terms this can be\nunderstood as extracting statistics which are maximally sufficient for the\noriginal dataset, while simultaneously maximally ancillary for the side\ndataset. We formulate this tradeoff as a constrained optimization problem and\ncharacterize its solutions. We then derive a gradient descent algorithm for\nthis problem, which is based on the Generalized Iterative Scaling method for\nfinding maximum entropy distributions. The method is demonstrated on synthetic\ndata, as well as on real face recognition datasets, and is shown to outperform\nstandard methods such as oriented PCA.\n", "versions": [{"version": "v1", "created": "Fri, 19 Oct 2012 15:05:46 GMT"}], "update_date": "2012-12-12", "authors_parsed": [["Globerson", "Amir", ""], ["Chechik", "Gal", ""], ["Tishby", "Naftali", ""]]}, {"id": "1212.2487", "submitter": "Eibe Frank", "authors": "Eibe Frank, Mark Hall, Bernhard Pfahringer", "title": "Locally Weighted Naive Bayes", "comments": "Appears in Proceedings of the Nineteenth Conference on Uncertainty in\n  Artificial Intelligence (UAI2003)", "journal-ref": null, "doi": null, "report-no": "UAI-P-2003-PG-249-256", "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Despite its simplicity, the naive Bayes classifier has surprised machine\nlearning researchers by exhibiting good performance on a variety of learning\nproblems. Encouraged by these results, researchers have looked to overcome\nnaive Bayes primary weakness - attribute independence - and improve the\nperformance of the algorithm. This paper presents a locally weighted version of\nnaive Bayes that relaxes the independence assumption by learning local models\nat prediction time. Experimental results show that locally weighted naive Bayes\nrarely degrades accuracy compared to standard naive Bayes and, in many cases,\nimproves accuracy dramatically. The main advantage of this method compared to\nother techniques for enhancing naive Bayes is its conceptual and computational\nsimplicity.\n", "versions": [{"version": "v1", "created": "Fri, 19 Oct 2012 15:05:29 GMT"}], "update_date": "2012-12-12", "authors_parsed": [["Frank", "Eibe", ""], ["Hall", "Mark", ""], ["Pfahringer", "Bernhard", ""]]}, {"id": "1212.2488", "submitter": "Ari Frank", "authors": "Ari Frank, Dan Geiger, Zohar Yakhini", "title": "A Distance-Based Branch and Bound Feature Selection Algorithm", "comments": "Appears in Proceedings of the Nineteenth Conference on Uncertainty in\n  Artificial Intelligence (UAI2003)", "journal-ref": null, "doi": null, "report-no": "UAI-P-2003-PG-241-248", "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  There is no known efficient method for selecting k Gaussian features from n\nwhich achieve the lowest Bayesian classification error. We show an example of\nhow greedy algorithms faced with this task are led to give results that are not\noptimal. This motivates us to propose a more robust approach. We present a\nBranch and Bound algorithm for finding a subset of k independent Gaussian\nfeatures which minimizes the naive Bayesian classification error. Our algorithm\nuses additive monotonic distance measures to produce bounds for the Bayesian\nclassification error in order to exclude many feature subsets from evaluation,\nwhile still returning an optimal solution. We test our method on synthetic data\nas well as data obtained from gene expression profiling.\n", "versions": [{"version": "v1", "created": "Fri, 19 Oct 2012 15:05:25 GMT"}], "update_date": "2012-12-12", "authors_parsed": [["Frank", "Ari", ""], ["Geiger", "Dan", ""], ["Yakhini", "Zohar", ""]]}, {"id": "1212.2490", "submitter": "Ruslan R Salakhutdinov", "authors": "Ruslan R Salakhutdinov, Sam T Roweis, Zoubin Ghahramani", "title": "On the Convergence of Bound Optimization Algorithms", "comments": "Appears in Proceedings of the Nineteenth Conference on Uncertainty in\n  Artificial Intelligence (UAI2003)", "journal-ref": null, "doi": null, "report-no": "UAI-P-2003-PG-509-516", "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Many practitioners who use the EM algorithm complain that it is sometimes\nslow. When does this happen, and what can be done about it? In this paper, we\nstudy the general class of bound optimization algorithms - including\nExpectation-Maximization, Iterative Scaling and CCCP - and their relationship\nto direct optimization algorithms such as gradient-based methods for parameter\nlearning. We derive a general relationship between the updates performed by\nbound optimization methods and those of gradient and second-order methods and\nidentify analytic conditions under which bound optimization algorithms exhibit\nquasi-Newton behavior, and conditions under which they possess poor,\nfirst-order convergence. Based on this analysis, we consider several specific\nalgorithms, interpret and analyze their convergence properties and provide some\nrecipes for preprocessing input to these algorithms to yield faster convergence\nbehavior. We report empirical results supporting our analysis and showing that\nsimple data preprocessing can result in dramatically improved performance of\nbound optimizers in practice.\n", "versions": [{"version": "v1", "created": "Fri, 19 Oct 2012 15:07:56 GMT"}], "update_date": "2012-12-12", "authors_parsed": [["Salakhutdinov", "Ruslan R", ""], ["Roweis", "Sam T", ""], ["Ghahramani", "Zoubin", ""]]}, {"id": "1212.2491", "submitter": "Dmitry Rusakov", "authors": "Dmitry Rusakov, Dan Geiger", "title": "Automated Analytic Asymptotic Evaluation of the Marginal Likelihood for\n  Latent Models", "comments": "Appears in Proceedings of the Nineteenth Conference on Uncertainty in\n  Artificial Intelligence (UAI2003)", "journal-ref": null, "doi": null, "report-no": "UAI-P-2003-PG-501-508", "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present and implement two algorithms for analytic asymptotic evaluation of\nthe marginal likelihood of data given a Bayesian network with hidden nodes. As\nshown by previous work, this evaluation is particularly hard for latent\nBayesian network models, namely networks that include hidden variables, where\nasymptotic approximation deviates from the standard BIC score. Our algorithms\nsolve two central difficulties in asymptotic evaluation of marginal likelihood\nintegrals, namely, evaluation of regular dimensionality drop for latent\nBayesian network models and computation of non-standard approximation formulas\nfor singular statistics for these models. The presented algorithms are\nimplemented in Matlab and Maple and their usage is demonstrated for marginal\nlikelihood approximations for Bayesian networks with hidden variables.\n", "versions": [{"version": "v1", "created": "Fri, 19 Oct 2012 15:07:51 GMT"}], "update_date": "2012-12-12", "authors_parsed": [["Rusakov", "Dmitry", ""], ["Geiger", "Dan", ""]]}, {"id": "1212.2494", "submitter": "Romer Rosales", "authors": "Romer Rosales, Brendan J. Frey", "title": "Learning Generative Models of Similarity Matrices", "comments": "Appears in Proceedings of the Nineteenth Conference on Uncertainty in\n  Artificial Intelligence (UAI2003)", "journal-ref": null, "doi": null, "report-no": "UAI-P-2003-PG-485-492", "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We describe a probabilistic (generative) view of affinity matrices along with\ninference algorithms for a subclass of problems associated with data\nclustering. This probabilistic view is helpful in understanding different\nmodels and algorithms that are based on affinity functions OF the data. IN\nparticular, we show how(greedy) inference FOR a specific probabilistic model IS\nequivalent TO the spectral clustering algorithm.It also provides a framework\nFOR developing new algorithms AND extended models. AS one CASE, we present new\ngenerative data clustering models that allow us TO infer the underlying\ndistance measure suitable for the clustering problem at hand. These models seem\nto perform well in a larger class of problems for which other clustering\nalgorithms (including spectral clustering) usually fail. Experimental\nevaluation was performed in a variety point data sets, showing excellent\nperformance.\n", "versions": [{"version": "v1", "created": "Fri, 19 Oct 2012 15:07:42 GMT"}], "update_date": "2012-12-12", "authors_parsed": [["Rosales", "Romer", ""], ["Frey", "Brendan J.", ""]]}, {"id": "1212.2498", "submitter": "Uri Nodelman", "authors": "Uri Nodelman, Christian R. Shelton, Daphne Koller", "title": "Learning Continuous Time Bayesian Networks", "comments": "Appears in Proceedings of the Nineteenth Conference on Uncertainty in\n  Artificial Intelligence (UAI2003)", "journal-ref": null, "doi": null, "report-no": "UAI-P-2003-PG-451-458", "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Continuous time Bayesian networks (CTBNs) describe structured stochastic\nprocesses with finitely many states that evolve over continuous time. A CTBN is\na directed (possibly cyclic) dependency graph over a set of variables, each of\nwhich represents a finite state continuous time Markov process whose transition\nmodel is a function of its parents. We address the problem of learning\nparameters and structure of a CTBN from fully observed data. We define a\nconjugate prior for CTBNs, and show how it can be used both for Bayesian\nparameter estimation and as the basis of a Bayesian score for structure\nlearning. Because acyclicity is not a constraint in CTBNs, we can show that the\nstructure learning problem is significantly easier, both in theory and in\npractice, than structure learning for dynamic Bayesian networks (DBNs).\nFurthermore, as CTBNs can tailor the parameters and dependency structure to the\ndifferent time granularities of the evolution of different variables, they can\nprovide a better fit to continuous-time processes than DBNs with a fixed time\ngranularity.\n", "versions": [{"version": "v1", "created": "Fri, 19 Oct 2012 15:07:23 GMT"}], "update_date": "2012-12-12", "authors_parsed": [["Nodelman", "Uri", ""], ["Shelton", "Christian R.", ""], ["Koller", "Daphne", ""]]}, {"id": "1212.2500", "submitter": "Jens D. Nielsen", "authors": "Jens D. Nielsen, Tomas Kocka, Jose M. Pena", "title": "On Local Optima in Learning Bayesian Networks", "comments": "Appears in Proceedings of the Nineteenth Conference on Uncertainty in\n  Artificial Intelligence (UAI2003)", "journal-ref": null, "doi": null, "report-no": "UAI-P-2003-PG-435-442", "categories": "cs.LG cs.AI stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper proposes and evaluates the k-greedy equivalence search algorithm\n(KES) for learning Bayesian networks (BNs) from complete data. The main\ncharacteristic of KES is that it allows a trade-off between greediness and\nrandomness, thus exploring different good local optima. When greediness is set\nat maximum, KES corresponds to the greedy equivalence search algorithm (GES).\nWhen greediness is kept at minimum, we prove that under mild assumptions KES\nasymptotically returns any inclusion optimal BN with nonzero probability.\nExperimental results for both synthetic and real data are reported showing that\nKES often finds a better local optima than GES. Moreover, we use KES to\nexperimentally confirm that the number of different local optima is often huge.\n", "versions": [{"version": "v1", "created": "Fri, 19 Oct 2012 15:07:12 GMT"}], "update_date": "2012-12-12", "authors_parsed": [["Nielsen", "Jens D.", ""], ["Kocka", "Tomas", ""], ["Pena", "Jose M.", ""]]}, {"id": "1212.2503", "submitter": "Christopher Meek", "authors": "Christopher Meek, David Maxwell Chickering", "title": "Practically Perfect", "comments": "Appears in Proceedings of the Nineteenth Conference on Uncertainty in\n  Artificial Intelligence (UAI2003)", "journal-ref": null, "doi": null, "report-no": "UAI-P-2003-PG-411-416", "categories": "cs.AI stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The property of perfectness plays an important role in the theory of Bayesian\nnetworks. First, the existence of perfect distributions for arbitrary sets of\nvariables and directed acyclic graphs implies that various methods for reading\nindependence from the structure of the graph (e.g., Pearl, 1988; Lauritzen,\nDawid, Larsen & Leimer, 1990) are complete. Second, the asymptotic reliability\nof various search methods is guaranteed under the assumption that the\ngenerating distribution is perfect (e.g., Spirtes, Glymour & Scheines, 2000;\nChickering & Meek, 2002). We provide a lower-bound on the probability of\nsampling a non-perfect distribution when using a fixed number of bits to\nrepresent the parameters of the Bayesian network. This bound approaches zero\nexponentially fast as one increases the number of bits used to represent the\nparameters. This result implies that perfect distributions with fixed-length\nrepresentations exist. We also provide a lower-bound on the number of bits\nneeded to guarantee that a distribution sampled from a uniform Dirichlet\ndistribution is perfect with probability greater than 1/2. This result is\nuseful for constructing randomized reductions for hardness proofs.\n", "versions": [{"version": "v1", "created": "Fri, 19 Oct 2012 15:06:57 GMT"}], "update_date": "2012-12-12", "authors_parsed": [["Meek", "Christopher", ""], ["Chickering", "David Maxwell", ""]]}, {"id": "1212.2504", "submitter": "Andrew McCallum", "authors": "Andrew McCallum", "title": "Efficiently Inducing Features of Conditional Random Fields", "comments": "Appears in Proceedings of the Nineteenth Conference on Uncertainty in\n  Artificial Intelligence (UAI2003)", "journal-ref": null, "doi": null, "report-no": "UAI-P-2003-PG-403-410", "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Conditional Random Fields (CRFs) are undirected graphical models, a special\ncase of which correspond to conditionally-trained finite state machines. A key\nadvantage of these models is their great flexibility to include a wide array of\noverlapping, multi-granularity, non-independent features of the input. In face\nof this freedom, an important question that remains is, what features should be\nused? This paper presents a feature induction method for CRFs. Founded on the\nprinciple of constructing only those feature conjunctions that significantly\nincrease log-likelihood, the approach is based on that of Della Pietra et al\n[1997], but altered to work with conditional rather than joint probabilities,\nand with additional modifications for providing tractability specifically for a\nsequence model. In comparison with traditional approaches, automated feature\ninduction offers both improved accuracy and more than an order of magnitude\nreduction in feature count; it enables the use of richer, higher-order Markov\nmodels, and offers more freedom to liberally guess about which atomic features\nmay be relevant to a task. The induction method applies to linear-chain CRFs,\nas well as to more arbitrary CRF structures, also known as Relational Markov\nNetworks [Taskar & Koller, 2002]. We present experimental results on a named\nentity extraction task.\n", "versions": [{"version": "v1", "created": "Fri, 19 Oct 2012 15:06:52 GMT"}], "update_date": "2012-12-12", "authors_parsed": [["McCallum", "Andrew", ""]]}, {"id": "1212.2508", "submitter": "Kai Yu", "authors": "Kai Yu, Anton Schwaighofer, Volker Tresp, Wei-Ying Ma, HongJiang Zhang", "title": "Collaborative Ensemble Learning: Combining Collaborative and\n  Content-Based Information Filtering via Hierarchical Bayes", "comments": "Appears in Proceedings of the Nineteenth Conference on Uncertainty in\n  Artificial Intelligence (UAI2003)", "journal-ref": null, "doi": null, "report-no": "UAI-P-2003-PG-616-623", "categories": "cs.LG cs.IR stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Collaborative filtering (CF) and content-based filtering (CBF) have widely\nbeen used in information filtering applications. Both approaches have their\nstrengths and weaknesses which is why researchers have developed hybrid\nsystems. This paper proposes a novel approach to unify CF and CBF in a\nprobabilistic framework, named collaborative ensemble learning. It uses\nprobabilistic SVMs to model each user's profile (as CBF does).At the prediction\nphase, it combines a society OF users profiles, represented by their respective\nSVM models, to predict an active users preferences(the CF idea).The combination\nscheme is embedded in a probabilistic framework and retains an intuitive\nexplanation.Moreover, collaborative ensemble learning does not require a global\ntraining stage and thus can incrementally incorporate new data.We report\nresults based on two data sets. For the Reuters-21578 text data set, we\nsimulate user ratings under the assumption that each user is interested in only\none category. In the second experiment, we use users' opinions on a set of 642\nart images that were collected through a web-based survey. For both data sets,\ncollaborative ensemble achieved excellent performance in terms of\nrecommendation accuracy.\n", "versions": [{"version": "v1", "created": "Fri, 19 Oct 2012 15:08:51 GMT"}], "update_date": "2012-12-12", "authors_parsed": [["Yu", "Kai", ""], ["Schwaighofer", "Anton", ""], ["Tresp", "Volker", ""], ["Ma", "Wei-Ying", ""], ["Zhang", "HongJiang", ""]]}, {"id": "1212.2510", "submitter": "Chen-Hsiang Yeang", "authors": "Chen-Hsiang Yeang, Martin Szummer", "title": "Markov Random Walk Representations with Continuous Distributions", "comments": "Appears in Proceedings of the Nineteenth Conference on Uncertainty in\n  Artificial Intelligence (UAI2003)", "journal-ref": null, "doi": null, "report-no": "UAI-P-2003-PG-600-607", "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Representations based on random walks can exploit discrete data distributions\nfor clustering and classification. We extend such representations from discrete\nto continuous distributions. Transition probabilities are now calculated using\na diffusion equation with a diffusion coefficient that inversely depends on the\ndata density. We relate this diffusion equation to a path integral and derive\nthe corresponding path probability measure. The framework is useful for\nincorporating continuous data densities and prior knowledge.\n", "versions": [{"version": "v1", "created": "Fri, 19 Oct 2012 15:08:42 GMT"}], "update_date": "2012-12-12", "authors_parsed": [["Yeang", "Chen-Hsiang", ""], ["Szummer", "Martin", ""]]}, {"id": "1212.2511", "submitter": "Keisuke Yamazaki", "authors": "Keisuke Yamazaki, Sumio Watanbe", "title": "Stochastic complexity of Bayesian networks", "comments": "Appears in Proceedings of the Nineteenth Conference on Uncertainty in\n  Artificial Intelligence (UAI2003)", "journal-ref": null, "doi": null, "report-no": "UAI-P-2003-PG-592-599", "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Bayesian networks are now being used in enormous fields, for example,\ndiagnosis of a system, data mining, clustering and so on. In spite of their\nwide range of applications, the statistical properties have not yet been\nclarified, because the models are nonidentifiable and non-regular. In a\nBayesian network, the set of its parameter for a smaller model is an analytic\nset with singularities in the space of large ones. Because of these\nsingularities, the Fisher information matrices are not positive definite. In\nother words, the mathematical foundation for learning was not constructed. In\nrecent years, however, we have developed a method to analyze non-regular models\nusing algebraic geometry. This method revealed the relation between the models\nsingularities and its statistical properties. In this paper, applying this\nmethod to Bayesian networks with latent variables, we clarify the order of the\nstochastic complexities.Our result claims that the upper bound of those is\nsmaller than the dimension of the parameter space. This means that the Bayesian\ngeneralization error is also far smaller than that of regular model, and that\nSchwarzs model selection criterion BIC needs to be improved for Bayesian\nnetworks.\n", "versions": [{"version": "v1", "created": "Fri, 19 Oct 2012 15:08:38 GMT"}], "update_date": "2012-12-12", "authors_parsed": [["Yamazaki", "Keisuke", ""], ["Watanbe", "Sumio", ""]]}, {"id": "1212.2512", "submitter": "Eric P. Xing", "authors": "Eric P. Xing, Michael I. Jordan, Stuart Russell", "title": "A Generalized Mean Field Algorithm for Variational Inference in\n  Exponential Families", "comments": "Appears in Proceedings of the Nineteenth Conference on Uncertainty in\n  Artificial Intelligence (UAI2003)", "journal-ref": null, "doi": null, "report-no": "UAI-P-2003-PG-583-591", "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The mean field methods, which entail approximating intractable probability\ndistributions variationally with distributions from a tractable family, enjoy\nhigh efficiency, guaranteed convergence, and provide lower bounds on the true\nlikelihood. But due to requirement for model-specific derivation of the\noptimization equations and unclear inference quality in various models, it is\nnot widely used as a generic approximate inference algorithm. In this paper, we\ndiscuss a generalized mean field theory on variational approximation to a broad\nclass of intractable distributions using a rich set of tractable distributions\nvia constrained optimization over distribution spaces. We present a class of\ngeneralized mean field (GMF) algorithms for approximate inference in complex\nexponential family models, which entails limiting the optimization over the\nclass of cluster-factorizable distributions. GMF is a generic method requiring\nno model-specific derivations. It factors a complex model into a set of\ndisjoint variable clusters, and uses a set of canonical fix-point equations to\niteratively update the cluster distributions, and converge to locally optimal\ncluster marginals that preserve the original dependency structure within each\ncluster, hence, fully decomposed the overall inference problem. We empirically\nanalyzed the effect of different tractable family (clusters of different\ngranularity) on inference quality, and compared GMF with BP on several\ncanonical models. Possible extension to higher-order MF approximation is also\ndiscussed.\n", "versions": [{"version": "v1", "created": "Fri, 19 Oct 2012 15:08:33 GMT"}], "update_date": "2012-12-12", "authors_parsed": [["Xing", "Eric P.", ""], ["Jordan", "Michael I.", ""], ["Russell", "Stuart", ""]]}, {"id": "1212.2513", "submitter": "Max Welling", "authors": "Max Welling, Richard S. Zemel, Geoffrey E. Hinton", "title": "Efficient Parametric Projection Pursuit Density Estimation", "comments": "Appears in Proceedings of the Nineteenth Conference on Uncertainty in\n  Artificial Intelligence (UAI2003)", "journal-ref": null, "doi": null, "report-no": "UAI-P-2003-PG-575-582", "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Product models of low dimensional experts are a powerful way to avoid the\ncurse of dimensionality. We present the ``under-complete product of experts'\n(UPoE), where each expert models a one dimensional projection of the data. The\nUPoE is fully tractable and may be interpreted as a parametric probabilistic\nmodel for projection pursuit. Its ML learning rules are identical to the\napproximate learning rules proposed before for under-complete ICA. We also\nderive an efficient sequential learning algorithm and discuss its relationship\nto projection pursuit density estimation and feature induction algorithms for\nadditive random field models.\n", "versions": [{"version": "v1", "created": "Fri, 19 Oct 2012 15:08:28 GMT"}], "update_date": "2012-12-12", "authors_parsed": [["Welling", "Max", ""], ["Zemel", "Richard S.", ""], ["Hinton", "Geoffrey E.", ""]]}, {"id": "1212.2514", "submitter": "Shaojun Wang", "authors": "Shaojun Wang, Dale Schuurmans, Fuchun Peng, Yunxin Zhao", "title": "Boltzmann Machine Learning with the Latent Maximum Entropy Principle", "comments": "Appears in Proceedings of the Nineteenth Conference on Uncertainty in\n  Artificial Intelligence (UAI2003)", "journal-ref": null, "doi": null, "report-no": "UAI-P-2003-PG-567-574", "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a new statistical learning paradigm for Boltzmann machines based\non a new inference principle we have proposed: the latent maximum entropy\nprinciple (LME). LME is different both from Jaynes maximum entropy principle\nand from standard maximum likelihood estimation.We demonstrate the LME\nprinciple BY deriving new algorithms for Boltzmann machine parameter\nestimation, and show how robust and fast new variant of the EM algorithm can be\ndeveloped.Our experiments show that estimation based on LME generally yields\nbetter results than maximum likelihood estimation, particularly when inferring\nhidden units from small amounts of data.\n", "versions": [{"version": "v1", "created": "Fri, 19 Oct 2012 15:08:24 GMT"}], "update_date": "2012-12-12", "authors_parsed": [["Wang", "Shaojun", ""], ["Schuurmans", "Dale", ""], ["Peng", "Fuchun", ""], ["Zhao", "Yunxin", ""]]}, {"id": "1212.2516", "submitter": "Ricardo Silva", "authors": "Ricardo Silva, Richard Scheines, Clark Glymour, Peter L. Spirtes", "title": "Learning Measurement Models for Unobserved Variables", "comments": "Appears in Proceedings of the Nineteenth Conference on Uncertainty in\n  Artificial Intelligence (UAI2003)", "journal-ref": null, "doi": null, "report-no": "UAI-P-2003-PG-543-550", "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Observed associations in a database may be due in whole or part to variations\nin unrecorded (latent) variables. Identifying such variables and their causal\nrelationships with one another is a principal goal in many scientific and\npractical domains. Previous work shows that, given a partition of observed\nvariables such that members of a class share only a single latent common cause,\nstandard search algorithms for causal Bayes nets can infer structural relations\nbetween latent variables. We introduce an algorithm for discovering such\npartitions when they exist. Uniquely among available procedures, the algorithm\nis (asymptotically) correct under standard assumptions in causal Bayes net\nsearch algorithms, requires no prior knowledge of the number of latent\nvariables, and does not depend on the mathematical form of the relationships\namong the latent variables. We evaluate the algorithm on a variety of simulated\ndata sets.\n", "versions": [{"version": "v1", "created": "Fri, 19 Oct 2012 15:08:15 GMT"}], "update_date": "2012-12-12", "authors_parsed": [["Silva", "Ricardo", ""], ["Scheines", "Richard", ""], ["Glymour", "Clark", ""], ["Spirtes", "Peter L.", ""]]}, {"id": "1212.2517", "submitter": "Eran Segal", "authors": "Eran Segal, Dana Pe'er, Aviv Regev, Daphne Koller, Nir Friedman", "title": "Learning Module Networks", "comments": "Appears in Proceedings of the Nineteenth Conference on Uncertainty in\n  Artificial Intelligence (UAI2003)", "journal-ref": null, "doi": null, "report-no": "UAI-P-2003-PG-525-534", "categories": "cs.LG cs.CE stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Methods for learning Bayesian network structure can discover dependency\nstructure between observed variables, and have been shown to be useful in many\napplications. However, in domains that involve a large number of variables, the\nspace of possible network structures is enormous, making it difficult, for both\ncomputational and statistical reasons, to identify a good model. In this paper,\nwe consider a solution to this problem, suitable for domains where many\nvariables have similar behavior. Our method is based on a new class of models,\nwhich we call module networks. A module network explicitly represents the\nnotion of a module - a set of variables that have the same parents in the\nnetwork and share the same conditional probability distribution. We define the\nsemantics of module networks, and describe an algorithm that learns a module\nnetwork from data. The algorithm learns both the partitioning of the variables\ninto modules and the dependency structure between the variables. We evaluate\nour algorithm on synthetic data, and on real data in the domains of gene\nexpression and the stock market. Our results show that module networks\ngeneralize better than Bayesian networks, and that the learned module network\nstructure reveals regularities that are obscured in learned Bayesian networks.\n", "versions": [{"version": "v1", "created": "Fri, 19 Oct 2012 15:08:06 GMT"}], "update_date": "2012-12-12", "authors_parsed": [["Segal", "Eran", ""], ["Pe'er", "Dana", ""], ["Regev", "Aviv", ""], ["Koller", "Daphne", ""], ["Friedman", "Nir", ""]]}, {"id": "1212.2573", "submitter": "K. S. Sesh Kumar", "authors": "K. S. Sesh Kumar (LIENS, INRIA Paris - Rocquencourt), Francis Bach\n  (LIENS, INRIA Paris - Rocquencourt)", "title": "Convex Relaxations for Learning Bounded Treewidth Decomposable Graphs", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.DS stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider the problem of learning the structure of undirected graphical\nmodels with bounded treewidth, within the maximum likelihood framework. This is\nan NP-hard problem and most approaches consider local search techniques. In\nthis paper, we pose it as a combinatorial optimization problem, which is then\nrelaxed to a convex optimization problem that involves searching over the\nforest and hyperforest polytopes with special structures, independently. A\nsupergradient method is used to solve the dual problem, with a run-time\ncomplexity of $O(k^3 n^{k+2} \\log n)$ for each iteration, where $n$ is the\nnumber of variables and $k$ is a bound on the treewidth. We compare our\napproach to state-of-the-art methods on synthetic datasets and classical\nbenchmarks, showing the gains of the novel convex approach.\n", "versions": [{"version": "v1", "created": "Tue, 11 Dec 2012 18:22:31 GMT"}], "update_date": "2012-12-12", "authors_parsed": [["Kumar", "K. S. Sesh", "", "LIENS, INRIA Paris - Rocquencourt"], ["Bach", "Francis", "", "LIENS, INRIA Paris - Rocquencourt"]]}, {"id": "1212.2686", "submitter": "Ian Goodfellow", "authors": "Ian Goodfellow, Aaron Courville, Yoshua Bengio", "title": "Joint Training of Deep Boltzmann Machines", "comments": "4 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We introduce a new method for training deep Boltzmann machines jointly. Prior\nmethods require an initial learning pass that trains the deep Boltzmann machine\ngreedily, one layer at a time, or do not perform well on classifi- cation\ntasks.\n", "versions": [{"version": "v1", "created": "Wed, 12 Dec 2012 01:59:27 GMT"}], "update_date": "2012-12-13", "authors_parsed": [["Goodfellow", "Ian", ""], ["Courville", "Aaron", ""], ["Bengio", "Yoshua", ""]]}, {"id": "1212.2767", "submitter": "Ioannis Psorakis", "authors": "Ioannis Psorakis, Iead Rezek, Zach Frankel, Stephen J. Roberts", "title": "Bayesian one-mode projection for dynamic bipartite graphs", "comments": "11 pages, 5 figures", "journal-ref": null, "doi": null, "report-no": "PARG 12-12(1)", "categories": "stat.ML cond-mat.stat-mech cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a Bayesian methodology for one-mode projecting a bipartite network\nthat is being observed across a series of discrete time steps. The resulting\none mode network captures the uncertainty over the presence/absence of each\nlink and provides a probability distribution over its possible weight values.\nAdditionally, the incorporation of prior knowledge over previous states makes\nthe resulting network less sensitive to noise and missing observations that\nusually take place during the data collection process. The methodology consists\nof computationally inexpensive update rules and is scalable to large problems,\nvia an appropriate distributed implementation.\n", "versions": [{"version": "v1", "created": "Wed, 12 Dec 2012 10:55:27 GMT"}], "update_date": "2012-12-13", "authors_parsed": [["Psorakis", "Ioannis", ""], ["Rezek", "Iead", ""], ["Frankel", "Zach", ""], ["Roberts", "Stephen J.", ""]]}, {"id": "1212.2784", "submitter": "Elvira Romano Dr", "authors": "Elvira Romano, Antonio Balzanella", "title": "Clustering of functional boxplots for multiple streaming time series", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper we introduce a micro-clustering strategy for Functional\nBoxplots. The aim is to summarize a set of streaming time series splitted in\nnon overlapping windows. It is a two step strategy which performs at first, an\non-line summarization by means of functional data structures, named Functional\nBoxplot micro-clusters; then it reveals the final summarization by processing,\noff-line, the functional data structures. Our main contribute consists in\nproviding a new definition of micro-cluster based on Functional Boxplots and,\nin defining a proximity measure which allows to compare and update them. This\nallows to get a finer graphical summarization of the streaming time series by\nfive functional basic statistics of data. The obtained synthesis will be able\nto keep track of the dynamic evolution of the multiple streams.\n", "versions": [{"version": "v1", "created": "Wed, 12 Dec 2012 11:54:11 GMT"}], "update_date": "2012-12-13", "authors_parsed": [["Romano", "Elvira", ""], ["Balzanella", "Antonio", ""]]}, {"id": "1212.2834", "submitter": "Mehrdad Yaghoobi Vaighan", "authors": "Mehrdad Yaghoobi, Laurent Daudet, Michael E. Davies", "title": "Dictionary Subselection Using an Overcomplete Joint Sparsity Model", "comments": "the title previously was \"Optimal Dictionary Selection Using an\n  Overcomplete Joint Sparsity Model\"", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG math.OC stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Many natural signals exhibit a sparse representation, whenever a suitable\ndescribing model is given. Here, a linear generative model is considered, where\nmany sparsity-based signal processing techniques rely on such a simplified\nmodel. As this model is often unknown for many classes of the signals, we need\nto select such a model based on the domain knowledge or using some exemplar\nsignals. This paper presents a new exemplar based approach for the linear model\n(called the dictionary) selection, for such sparse inverse problems. The\nproblem of dictionary selection, which has also been called the dictionary\nlearning in this setting, is first reformulated as a joint sparsity model. The\njoint sparsity model here differs from the standard joint sparsity model as it\nconsiders an overcompleteness in the representation of each signal, within the\nrange of selected subspaces. The new dictionary selection paradigm is examined\nwith some synthetic and realistic simulations.\n", "versions": [{"version": "v1", "created": "Wed, 12 Dec 2012 15:02:20 GMT"}, {"version": "v2", "created": "Mon, 10 Jun 2013 09:31:45 GMT"}], "update_date": "2013-06-11", "authors_parsed": [["Yaghoobi", "Mehrdad", ""], ["Daudet", "Laurent", ""], ["Davies", "Michael E.", ""]]}, {"id": "1212.2991", "submitter": "Shawn Hershey", "authors": "Shawn Hershey, Jeff Bernstein, Bill Bradley, Andrew Schweitzer, Noah\n  Stein, Theo Weber, Ben Vigoda", "title": "Accelerating Inference: towards a full Language, Compiler and Hardware\n  stack", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.SE cs.AI stat.ML", "license": "http://creativecommons.org/licenses/by/3.0/", "abstract": "  We introduce Dimple, a fully open-source API for probabilistic modeling.\nDimple allows the user to specify probabilistic models in the form of graphical\nmodels, Bayesian networks, or factor graphs, and performs inference (by\nautomatically deriving an inference engine from a variety of algorithms) on the\nmodel. Dimple also serves as a compiler for GP5, a hardware accelerator for\ninference.\n", "versions": [{"version": "v1", "created": "Wed, 12 Dec 2012 21:40:23 GMT"}], "update_date": "2012-12-14", "authors_parsed": [["Hershey", "Shawn", ""], ["Bernstein", "Jeff", ""], ["Bradley", "Bill", ""], ["Schweitzer", "Andrew", ""], ["Stein", "Noah", ""], ["Weber", "Theo", ""], ["Vigoda", "Ben", ""]]}, {"id": "1212.3214", "submitter": "Yupeng Cun", "authors": "Yupeng Cun, Holger Fr\\\"ohlich", "title": "Integrating Prior Knowledge Into Prognostic Biomarker Discovery based on\n  Network Structure", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "q-bio.GN stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Background: Predictive, stable and interpretable gene signatures are\ngenerally seen as an important step towards a better personalized medicine.\nDuring the last decade various methods have been proposed for that purpose.\nHowever, one important obstacle for making gene signatures a standard tool in\nclinics is the typical low reproducibility of these signatures combined with\nthe difficulty to achieve a clear biological interpretation. For that purpose\nin the last years there has been a growing interest in approaches that try to\nintegrate information from molecular interaction networks. Results: We propose\na novel algorithm, called FrSVM, which integrates protein-protein interaction\nnetwork information into gene selection for prognostic biomarker discovery. Our\nmethod is a simple filter based approach, which focuses on central genes with\nlarge differences in their expression. Compared to several other competing\nmethods our algorithm reveals a significantly better prediction performance and\nhigher signature stability. More- over, obtained gene lists are highly enriched\nwith known disease genes and drug targets. We extendd our approach further by\nintegrating information on candidate disease genes and targets of disease\nassociated Transcript Factors (TFs).\n", "versions": [{"version": "v1", "created": "Thu, 13 Dec 2012 16:44:55 GMT"}, {"version": "v2", "created": "Mon, 27 May 2013 13:47:19 GMT"}], "update_date": "2013-05-28", "authors_parsed": [["Cun", "Yupeng", ""], ["Fr\u00f6hlich", "Holger", ""]]}, {"id": "1212.3276", "submitter": "Sivan Sabato", "authors": "Sivan Sabato and Shai Shalev-Shwartz and Nathan Srebro and Daniel Hsu\n  and Tong Zhang", "title": "Learning Sparse Low-Threshold Linear Classifiers", "comments": null, "journal-ref": "Journal of Machine Learning Research, 16(Jul):1275-1304, 2015", "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider the problem of learning a non-negative linear classifier with a\n$1$-norm of at most $k$, and a fixed threshold, under the hinge-loss. This\nproblem generalizes the problem of learning a $k$-monotone disjunction. We\nprove that we can learn efficiently in this setting, at a rate which is linear\nin both $k$ and the size of the threshold, and that this is the best possible\nrate. We provide an efficient online learning algorithm that achieves the\noptimal rate, and show that in the batch case, empirical risk minimization\nachieves this rate as well. The rates we show are tighter than the uniform\nconvergence rate, which grows with $k^2$.\n", "versions": [{"version": "v1", "created": "Thu, 13 Dec 2012 19:20:21 GMT"}, {"version": "v2", "created": "Sun, 6 Jul 2014 02:55:23 GMT"}, {"version": "v3", "created": "Mon, 18 Apr 2016 09:17:36 GMT"}], "update_date": "2016-04-19", "authors_parsed": [["Sabato", "Sivan", ""], ["Shalev-Shwartz", "Shai", ""], ["Srebro", "Nathan", ""], ["Hsu", "Daniel", ""], ["Zhang", "Tong", ""]]}, {"id": "1212.3850", "submitter": "Nima Noorshams", "authors": "Nima Noorshams and Martin J. Wainwright", "title": "Belief Propagation for Continuous State Spaces: Stochastic\n  Message-Passing with Quantitative Guarantees", "comments": "Portions of the results were presented at the International Symposium\n  on Information Theory 2012. The results were also submitted to the Journal of\n  Machine Learning Research on December 16th 2012", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IT cs.LG math.IT stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The sum-product or belief propagation (BP) algorithm is a widely used\nmessage-passing technique for computing approximate marginals in graphical\nmodels. We introduce a new technique, called stochastic orthogonal series\nmessage-passing (SOSMP), for computing the BP fixed point in models with\ncontinuous random variables. It is based on a deterministic approximation of\nthe messages via orthogonal series expansion, and a stochastic approximation\nvia Monte Carlo estimates of the integral updates of the basis coefficients. We\nprove that the SOSMP iterates converge to a \\delta-neighborhood of the unique\nBP fixed point for any tree-structured graph, and for any graphs with cycles in\nwhich the BP updates satisfy a contractivity condition. In addition, we\ndemonstrate how to choose the number of basis coefficients as a function of the\ndesired approximation accuracy \\delta and smoothness of the compatibility\nfunctions. We illustrate our theory with both simulated examples and in\napplication to optical flow estimation.\n", "versions": [{"version": "v1", "created": "Sun, 16 Dec 2012 23:22:56 GMT"}], "update_date": "2012-12-18", "authors_parsed": [["Noorshams", "Nima", ""], ["Wainwright", "Martin J.", ""]]}, {"id": "1212.3900", "submitter": "Liangjie Hong", "authors": "Liangjie Hong", "title": "A Tutorial on Probabilistic Latent Semantic Analysis", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this tutorial, I will discuss the details about how Probabilistic Latent\nSemantic Analysis (PLSA) is formalized and how different learning algorithms\nare proposed to learn the model.\n", "versions": [{"version": "v1", "created": "Mon, 17 Dec 2012 06:49:14 GMT"}, {"version": "v2", "created": "Fri, 21 Dec 2012 19:55:53 GMT"}], "update_date": "2012-12-24", "authors_parsed": [["Hong", "Liangjie", ""]]}, {"id": "1212.4093", "submitter": "David Choi", "authors": "David Choi, Patrick J. Wolfe", "title": "Co-clustering separately exchangeable network data", "comments": "Published in at http://dx.doi.org/10.1214/13-AOS1173 the Annals of\n  Statistics (http://www.imstat.org/aos/) by the Institute of Mathematical\n  Statistics (http://www.imstat.org)", "journal-ref": "Annals of Statistics 2014, Vol. 42, No. 1, 29-63", "doi": "10.1214/13-AOS1173", "report-no": "IMS-AOS-AOS1173", "categories": "math.ST cs.SI math.CO stat.ML stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This article establishes the performance of stochastic blockmodels in\naddressing the co-clustering problem of partitioning a binary array into\nsubsets, assuming only that the data are generated by a nonparametric process\nsatisfying the condition of separate exchangeability. We provide oracle\ninequalities with rate of convergence $\\mathcal{O}_P(n^{-1/4})$ corresponding\nto profile likelihood maximization and mean-square error minimization, and show\nthat the blockmodel can be interpreted in this setting as an optimal\npiecewise-constant approximation to the generative nonparametric model. We also\nshow for large sample sizes that the detection of co-clusters in such data\nindicates with high probability the existence of co-clusters of equal size and\nasymptotically equivalent connectivity in the underlying generative process.\n", "versions": [{"version": "v1", "created": "Mon, 17 Dec 2012 18:31:53 GMT"}, {"version": "v2", "created": "Tue, 18 Dec 2012 03:33:51 GMT"}, {"version": "v3", "created": "Sun, 26 May 2013 22:20:43 GMT"}, {"version": "v4", "created": "Fri, 20 Sep 2013 11:12:57 GMT"}, {"version": "v5", "created": "Thu, 16 Jan 2014 11:52:25 GMT"}], "update_date": "2014-01-17", "authors_parsed": [["Choi", "David", ""], ["Wolfe", "Patrick J.", ""]]}, {"id": "1212.4137", "submitter": "Martin Tak\\'a\\v{c}", "authors": "Peter Richt\\'arik, Majid Jahani, Selin Damla Ahipa\\c{s}ao\\u{g}lu,\n  Martin Tak\\'a\\v{c}", "title": "Alternating Maximization: Unifying Framework for 8 Sparse PCA\n  Formulations and Efficient Parallel Codes", "comments": "29 pages, 9 tables, 7 figures (the paper is accompanied by a release\n  of the open-source code '24am')", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG math.OC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Given a multivariate data set, sparse principal component analysis (SPCA)\naims to extract several linear combinations of the variables that together\nexplain the variance in the data as much as possible, while controlling the\nnumber of nonzero loadings in these combinations. In this paper we consider 8\ndifferent optimization formulations for computing a single sparse loading\nvector; these are obtained by combining the following factors: we employ two\nnorms for measuring variance (L2, L1) and two sparsity-inducing norms (L0, L1),\nwhich are used in two different ways (constraint, penalty). Three of our\nformulations, notably the one with L0 constraint and L1 variance, have not been\nconsidered in the literature. We give a unifying reformulation which we propose\nto solve via a natural alternating maximization (AM) method. We show the the AM\nmethod is nontrivially equivalent to GPower (Journ\\'{e}e et al; JMLR\n11:517--553, 2010) for all our formulations. Besides this, we provide 24\nefficient parallel SPCA implementations: 3 codes (multi-core, GPU and cluster)\nfor each of the 8 problems. Parallelism in the methods is aimed at i) speeding\nup computations (our GPU code can be 100 times faster than an efficient serial\ncode written in C++), ii) obtaining solutions explaining more variance and iii)\ndealing with big data problems (our cluster code is able to solve a 357 GB\nproblem in about a minute).\n", "versions": [{"version": "v1", "created": "Mon, 17 Dec 2012 20:53:35 GMT"}, {"version": "v2", "created": "Thu, 7 May 2020 00:50:36 GMT"}], "update_date": "2020-05-08", "authors_parsed": [["Richt\u00e1rik", "Peter", ""], ["Jahani", "Majid", ""], ["Ahipa\u015fao\u011flu", "Selin Damla", ""], ["Tak\u00e1\u010d", "Martin", ""]]}, {"id": "1212.4174", "submitter": "Mahantesh Halappanavar", "authors": "Chad Scherrer, Ambuj Tewari, Mahantesh Halappanavar, David Haglin", "title": "Feature Clustering for Accelerating Parallel Coordinate Descent", "comments": "Accepted for publication in the proceedings of NIPS (Neural\n  Information Processing Systems Foundations) 2012, Lake Tahoe, Nevada", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.DC cs.LG math.OC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Large-scale L1-regularized loss minimization problems arise in\nhigh-dimensional applications such as compressed sensing and high-dimensional\nsupervised learning, including classification and regression problems.\nHigh-performance algorithms and implementations are critical to efficiently\nsolving these problems. Building upon previous work on coordinate descent\nalgorithms for L1-regularized problems, we introduce a novel family of\nalgorithms called block-greedy coordinate descent that includes, as special\ncases, several existing algorithms such as SCD, Greedy CD, Shotgun, and\nThread-Greedy. We give a unified convergence analysis for the family of\nblock-greedy algorithms. The analysis suggests that block-greedy coordinate\ndescent can better exploit parallelism if features are clustered so that the\nmaximum inner product between features in different blocks is small. Our\ntheoretical convergence analysis is supported with experimental re- sults using\ndata from diverse real-world applications. We hope that algorithmic approaches\nand convergence analysis we provide will not only advance the field, but will\nalso encourage researchers to systematically explore the design space of\nalgorithms for solving large-scale L1-regularization problems.\n", "versions": [{"version": "v1", "created": "Mon, 17 Dec 2012 21:43:31 GMT"}], "update_date": "2012-12-19", "authors_parsed": [["Scherrer", "Chad", ""], ["Tewari", "Ambuj", ""], ["Halappanavar", "Mahantesh", ""], ["Haglin", "David", ""]]}, {"id": "1212.4269", "submitter": "Morteza Ibrahimi", "authors": "Morteza Ibrahimi, Andrea Montanari and George S Moore", "title": "Accelerated Time-of-Flight Mass Spectrometry", "comments": "14 pages, 18 figures. This paper is submitted to IEEE Transaction on\n  Signal Processing", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.OC cs.CE stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study a simple modification to the conventional time of flight mass\nspectrometry (TOFMS) where a \\emph{variable} and (pseudo)-\\emph{random} pulsing\nrate is used which allows for traces from different pulses to overlap. This\nmodification requires little alteration to the currently employed hardware.\nHowever, it requires a reconstruction method to recover the spectrum from\nhighly aliased traces. We propose and demonstrate an efficient algorithm that\ncan process massive TOFMS data using computational resources that can be\nconsidered modest with today's standards. This approach can be used to improve\nduty cycle, speed, and mass resolving power of TOFMS at the same time. We\nexpect this to extend the applicability of TOFMS to new domains.\n", "versions": [{"version": "v1", "created": "Tue, 18 Dec 2012 08:40:55 GMT"}, {"version": "v2", "created": "Sun, 28 Jul 2013 22:45:01 GMT"}], "update_date": "2013-07-30", "authors_parsed": [["Ibrahimi", "Morteza", ""], ["Montanari", "Andrea", ""], ["Moore", "George S", ""]]}, {"id": "1212.4347", "submitter": "Bonggun Shin", "authors": "Bonggun Shin, Alice Oh", "title": "Bayesian Group Nonnegative Matrix Factorization for EEG Analysis", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a generative model of a group EEG analysis, based on appropriate\nkernel assumptions on EEG data. We derive the variational inference update rule\nusing various approximation techniques. The proposed model outperforms the\ncurrent state-of-the-art algorithms in terms of common pattern extraction. The\nvalidity of the proposed model is tested on the BCI competition dataset.\n", "versions": [{"version": "v1", "created": "Tue, 18 Dec 2012 13:35:38 GMT"}], "update_date": "2012-12-19", "authors_parsed": [["Shin", "Bonggun", ""], ["Oh", "Alice", ""]]}, {"id": "1212.4507", "submitter": "Joe Staines", "authors": "Joe Staines and David Barber", "title": "Variational Optimization", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG cs.NA", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We discuss a general technique that can be used to form a differentiable\nbound on the optima of non-differentiable or discrete objective functions. We\nform a unified description of these methods and consider under which\ncircumstances the bound is concave. In particular we consider two concrete\napplications of the method, namely sparse learning and support vector\nclassification.\n", "versions": [{"version": "v1", "created": "Tue, 18 Dec 2012 21:06:10 GMT"}, {"version": "v2", "created": "Thu, 20 Dec 2012 18:49:18 GMT"}], "update_date": "2012-12-21", "authors_parsed": [["Staines", "Joe", ""], ["Barber", "David", ""]]}, {"id": "1212.4562", "submitter": "Mark Kon", "authors": "Mark A. Kon", "title": "A complexity analysis of statistical learning algorithms", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We apply information-based complexity analysis to support vector machine\n(SVM) algorithms, with the goal of a comprehensive continuous algorithmic\nanalysis of such algorithms. This involves complexity measures in which some\nhigher order operations (e.g., certain optimizations) are considered primitive\nfor the purposes of measuring complexity. We consider classes of information\noperators and algorithms made up of scaled families, and investigate the\nutility of scaling the complexities to minimize error. We look at the division\nof statistical learning into information and algorithmic components, at the\ncomplexities of each, and at applications to support vector machine (SVM) and\nmore general machine learning algorithms. We give applications to SVM\nalgorithms graded into linear and higher order components, and give an example\nin biomedical informatics.\n", "versions": [{"version": "v1", "created": "Wed, 19 Dec 2012 03:06:13 GMT"}], "update_date": "2012-12-20", "authors_parsed": [["Kon", "Mark A.", ""]]}, {"id": "1212.4569", "submitter": "Mark Kon", "authors": "Yue Fan, Louise Raphael and Mark Kon", "title": "Feature vector regularization in machine learning", "comments": "31 pages, one figure", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Problems in machine learning (ML) can involve noisy input data, and ML\nclassification methods have reached limiting accuracies when based on standard\nML data sets consisting of feature vectors and their classes. Greater accuracy\nwill require incorporation of prior structural information on data into\nlearning. We study methods to regularize feature vectors (unsupervised\nregularization methods), analogous to supervised regularization for estimating\nfunctions in ML. We study regularization (denoising) of ML feature vectors\nusing Tikhonov and other regularization methods for functions on ${\\bf R}^n$. A\nfeature vector ${\\bf x}=(x_1,\\ldots,x_n)=\\{x_q\\}_{q=1}^n$ is viewed as a\nfunction of its index $q$, and smoothed using prior information on its\nstructure. This can involve a penalty functional on feature vectors analogous\nto those in statistical learning, or use of proximity (e.g. graph) structure on\nthe set of indices. Such feature vector regularization inherits a property from\nfunction denoising on ${\\bf R}^n$, in that accuracy is non-monotonic in the\ndenoising (regularization) parameter $\\alpha$. Under some assumptions about the\nnoise level and the data structure, we show that the best reconstruction\naccuracy also occurs at a finite positive $\\alpha$ in index spaces with graph\nstructures. We adapt two standard function denoising methods used on ${\\bf\nR}^n$, local averaging and kernel regression. In general the index space can be\nany discrete set with a notion of proximity, e.g. a metric space, a subset of\n${\\bf R}^n$, or a graph/network, with feature vectors as functions with some\nnotion of continuity. We show this improves feature vector recovery, and thus\nthe subsequent classification or regression done on them. We give an example in\ngene expression analysis for cancer classification with the genome as an index\nspace and network structure based protein-protein interactions.\n", "versions": [{"version": "v1", "created": "Wed, 19 Dec 2012 03:48:24 GMT"}, {"version": "v2", "created": "Mon, 30 Dec 2013 09:21:07 GMT"}], "update_date": "2013-12-31", "authors_parsed": [["Fan", "Yue", ""], ["Raphael", "Louise", ""], ["Kon", "Mark", ""]]}, {"id": "1212.4775", "submitter": "Mario Frank", "authors": "Mario Frank, Joachim M. Buhmann, David Basin", "title": "Role Mining with Probabilistic Models", "comments": "accepted for publication at ACM Transactions on Information and\n  System Security (TISSEC)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CR cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Role mining tackles the problem of finding a role-based access control (RBAC)\nconfiguration, given an access-control matrix assigning users to access\npermissions as input. Most role mining approaches work by constructing a large\nset of candidate roles and use a greedy selection strategy to iteratively pick\na small subset such that the differences between the resulting RBAC\nconfiguration and the access control matrix are minimized. In this paper, we\nadvocate an alternative approach that recasts role mining as an inference\nproblem rather than a lossy compression problem. Instead of using combinatorial\nalgorithms to minimize the number of roles needed to represent the\naccess-control matrix, we derive probabilistic models to learn the RBAC\nconfiguration that most likely underlies the given matrix.\n  Our models are generative in that they reflect the way that permissions are\nassigned to users in a given RBAC configuration. We additionally model how\nuser-permission assignments that conflict with an RBAC configuration emerge and\nwe investigate the influence of constraints on role hierarchies and on the\nnumber of assignments. In experiments with access-control matrices from\nreal-world enterprises, we compare our proposed models with other role mining\nmethods. Our results show that our probabilistic models infer roles that\ngeneralize well to new system users for a wide variety of data, while other\nmodels' generalization abilities depend on the dataset given.\n", "versions": [{"version": "v1", "created": "Wed, 19 Dec 2012 18:12:34 GMT"}, {"version": "v2", "created": "Thu, 3 Jan 2013 17:27:55 GMT"}, {"version": "v3", "created": "Fri, 4 Jan 2013 22:24:15 GMT"}], "update_date": "2013-01-08", "authors_parsed": [["Frank", "Mario", ""], ["Buhmann", "Joachim M.", ""], ["Basin", "David", ""]]}, {"id": "1212.4777", "submitter": "Ankur Moitra", "authors": "Sanjeev Arora, Rong Ge, Yoni Halpern, David Mimno, Ankur Moitra, David\n  Sontag, Yichen Wu, Michael Zhu", "title": "A Practical Algorithm for Topic Modeling with Provable Guarantees", "comments": "26 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.DS stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Topic models provide a useful method for dimensionality reduction and\nexploratory data analysis in large text corpora. Most approaches to topic model\ninference have been based on a maximum likelihood objective. Efficient\nalgorithms exist that approximate this objective, but they have no provable\nguarantees. Recently, algorithms have been introduced that provide provable\nbounds, but these algorithms are not practical because they are inefficient and\nnot robust to violations of model assumptions. In this paper we present an\nalgorithm for topic model inference that is both provable and practical. The\nalgorithm produces results comparable to the best MCMC implementations while\nrunning orders of magnitude faster.\n", "versions": [{"version": "v1", "created": "Wed, 19 Dec 2012 18:14:51 GMT"}], "update_date": "2012-12-20", "authors_parsed": [["Arora", "Sanjeev", ""], ["Ge", "Rong", ""], ["Halpern", "Yoni", ""], ["Mimno", "David", ""], ["Moitra", "Ankur", ""], ["Sontag", "David", ""], ["Wu", "Yichen", ""], ["Zhu", "Michael", ""]]}, {"id": "1212.4794", "submitter": "Tiago Peixoto", "authors": "Tiago P. Peixoto", "title": "Parsimonious module inference in large networks", "comments": "5 pages, 4 figures + Supplemental Material", "journal-ref": "Phys. Rev. Lett. 110, 148701 (2013)", "doi": "10.1103/PhysRevLett.110.148701", "report-no": null, "categories": "physics.data-an physics.soc-ph stat.ML", "license": "http://creativecommons.org/licenses/by-nc-sa/3.0/", "abstract": "  We investigate the detectability of modules in large networks when the number\nof modules is not known in advance. We employ the minimum description length\n(MDL) principle which seeks to minimize the total amount of information\nrequired to describe the network, and avoid overfitting. According to this\ncriterion, we obtain general bounds on the detectability of any prescribed\nblock structure, given the number of nodes and edges in the sampled network. We\nalso obtain that the maximum number of detectable blocks scales as $\\sqrt{N}$,\nwhere $N$ is the number of nodes in the network, for a fixed average degree\n$<k>$. We also show that the simplicity of the MDL approach yields an efficient\nmultilevel Monte Carlo inference algorithm with a complexity of $O(\\tau N\\log\nN)$, if the number of blocks is unknown, and $O(\\tau N)$ if it is known, where\n$\\tau$ is the mixing time of the Markov chain. We illustrate the application of\nthe method on a large network of actors and films with over $10^6$ edges, and a\ndissortative, bipartite block structure.\n", "versions": [{"version": "v1", "created": "Wed, 19 Dec 2012 19:10:54 GMT"}, {"version": "v2", "created": "Mon, 31 Dec 2012 16:24:08 GMT"}, {"version": "v3", "created": "Fri, 5 Apr 2013 16:39:05 GMT"}, {"version": "v4", "created": "Mon, 8 Apr 2013 09:37:48 GMT"}], "update_date": "2013-04-09", "authors_parsed": [["Peixoto", "Tiago P.", ""]]}, {"id": "1212.4799", "submitter": "Cameron Freer", "authors": "Cameron E. Freer and Daniel M. Roy and Joshua B. Tenenbaum", "title": "Towards common-sense reasoning via conditional simulation: legacies of\n  Turing in Artificial Intelligence", "comments": "51 pages, 6 figures, 1 table. Slight typographical fixes", "journal-ref": "Turing's Legacy: Developments from Turing's Ideas in Logic, ed.\n  Rod Downey, ASL Lecture Notes in Logic 42, Cambridge University Press, 2014", "doi": "10.1017/CBO9781107338579.007", "report-no": null, "categories": "cs.AI math.LO stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The problem of replicating the flexibility of human common-sense reasoning\nhas captured the imagination of computer scientists since the early days of\nAlan Turing's foundational work on computation and the philosophy of artificial\nintelligence. In the intervening years, the idea of cognition as computation\nhas emerged as a fundamental tenet of Artificial Intelligence (AI) and\ncognitive science. But what kind of computation is cognition?\n  We describe a computational formalism centered around a probabilistic Turing\nmachine called QUERY, which captures the operation of probabilistic\nconditioning via conditional simulation. Through several examples and analyses,\nwe demonstrate how the QUERY abstraction can be used to cast common-sense\nreasoning as probabilistic inference in a statistical model of our observations\nand the uncertain structure of the world that generated that experience. This\nformulation is a recent synthesis of several research programs in AI and\ncognitive science, but it also represents a surprising convergence of several\nof Turing's pioneering insights in AI, the foundations of computation, and\nstatistics.\n", "versions": [{"version": "v1", "created": "Wed, 19 Dec 2012 19:20:57 GMT"}, {"version": "v2", "created": "Wed, 9 Oct 2013 15:13:44 GMT"}], "update_date": "2015-11-24", "authors_parsed": [["Freer", "Cameron E.", ""], ["Roy", "Daniel M.", ""], ["Tenenbaum", "Joshua B.", ""]]}, {"id": "1212.4871", "submitter": "Ramin  Norousi", "authors": "Ramin Norousi, Stephan Wickles, Christoph Leidig, Thomas Becker,\n  Volker J. Schmid, Roland Beckmann, Achim Tresch", "title": "Automatic post-picking using MAPPOS improves particle image detection\n  from Cryo-EM micrographs", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Cryo-electron microscopy (cryo-EM) studies using single particle\nreconstruction are extensively used to reveal structural information on\nmacromolecular complexes. Aiming at the highest achievable resolution, state of\nthe art electron microscopes automatically acquire thousands of high-quality\nmicrographs. Particles are detected on and boxed out from each micrograph using\nfully- or semi-automated approaches. However, the obtained particles still\nrequire laborious manual post-picking classification, which is one major\nbottleneck for single particle analysis of large datasets. We introduce MAPPOS,\na supervised post-picking strategy for the classification of boxed particle\nimages, as additional strategy adding to the already efficient automated\nparticle picking routines. MAPPOS employs machine learning techniques to train\na robust classifier from a small number of characteristic image features. In\norder to accurately quantify the performance of MAPPOS we used simulated\nparticle and non-particle images. In addition, we verified our method by\napplying it to an experimental cryo-EM dataset and comparing the results to the\nmanual classification of the same dataset. Comparisons between MAPPOS and\nmanual post-picking classification by several human experts demonstrated that\nmerely a few hundred sample images are sufficient for MAPPOS to classify an\nentire dataset with a human-like performance. MAPPOS was shown to greatly\naccelerate the throughput of large datasets by reducing the manual workload by\norders of magnitude while maintaining a reliable identification of non-particle\nimages.\n", "versions": [{"version": "v1", "created": "Wed, 19 Dec 2012 22:17:18 GMT"}], "update_date": "2012-12-21", "authors_parsed": [["Norousi", "Ramin", ""], ["Wickles", "Stephan", ""], ["Leidig", "Christoph", ""], ["Becker", "Thomas", ""], ["Schmid", "Volker J.", ""], ["Beckmann", "Roland", ""], ["Tresch", "Achim", ""]]}, {"id": "1212.4906", "submitter": "James Dowty", "authors": "James G. Dowty", "title": "SMML estimators for 1-dimensional continuous data", "comments": "10 pages, 2 tables and 1 figure", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IT math.IT math.ST stat.ML stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A method is given for calculating the strict minimum message length (SMML)\nestimator for 1-dimensional exponential families with continuous sufficient\nstatistics. A set of $n$ equations are found that the $n$ cut-points of the\nSMML estimator must satisfy. These equations can be solved using Newton's\nmethod and this approach is used to produce new results and to replicate\nresults that C. S. Wallace obtained using his boundary rules for the SMML\nestimator. A rigorous proof is also given that, despite being composed of step\nfunctions, the posterior probability corresponding to the SMML estimator is a\ncontinuous function of the data.\n", "versions": [{"version": "v1", "created": "Thu, 20 Dec 2012 01:54:03 GMT"}], "update_date": "2012-12-21", "authors_parsed": [["Dowty", "James G.", ""]]}, {"id": "1212.5156", "submitter": "Christopher R. Genovese", "authors": "Christopher R. Genovese, Marco Perone-Pacifico, Isabella Verdinelli,\n  Larry Wasserman", "title": "Nonparametric ridge estimation", "comments": "Published in at http://dx.doi.org/10.1214/14-AOS1218 the Annals of\n  Statistics (http://www.imstat.org/aos/) by the Institute of Mathematical\n  Statistics (http://www.imstat.org)", "journal-ref": "Annals of Statistics, Vol. 42, No. 4, 1511-1545 (2014)", "doi": "10.1214/14-AOS1218", "report-no": "IMS-AOS-AOS1218", "categories": "math.ST cs.LG stat.ML stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study the problem of estimating the ridges of a density function. Ridge\nestimation is an extension of mode finding and is useful for understanding the\nstructure of a density. It can also be used to find hidden structure in point\ncloud data. We show that, under mild regularity conditions, the ridges of the\nkernel density estimator consistently estimate the ridges of the true density.\nWhen the data are noisy measurements of a manifold, we show that the ridges are\nclose and topologically similar to the hidden manifold. To find the estimated\nridges in practice, we adapt the modified mean-shift algorithm proposed by\nOzertem and Erdogmus [J. Mach. Learn. Res. 12 (2011) 1249-1286]. Some numerical\nexperiments verify that the algorithm is accurate.\n", "versions": [{"version": "v1", "created": "Thu, 20 Dec 2012 17:41:23 GMT"}, {"version": "v2", "created": "Thu, 21 Aug 2014 12:10:31 GMT"}, {"version": "v3", "created": "Thu, 28 Aug 2014 08:28:48 GMT"}], "update_date": "2014-08-29", "authors_parsed": [["Genovese", "Christopher R.", ""], ["Perone-Pacifico", "Marco", ""], ["Verdinelli", "Isabella", ""], ["Wasserman", "Larry", ""]]}, {"id": "1212.5203", "submitter": "Michael Larsen", "authors": "Michael D. Larsen", "title": "An Experiment with Hierarchical Bayesian Record Linkage", "comments": "14 pages, 0 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.AP stat.CO stat.ME stat.ML stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In record linkage (RL), or exact file matching, the goal is to identify the\nlinks between entities with information on two or more files. RL is an\nimportant activity in areas including counting the population, enhancing survey\nframes and data, and conducting epidemiological and follow-up studies. RL is\nchallenging when files are very large, no accurate personal identification (ID)\nnumber is present on all files for all units, and some information is recorded\nwith error. Without an unique ID number one must rely on comparisons of names,\naddresses, dates, and other information to find the links. Latent class models\ncan be used to automatically score the value of information for determining\nmatch status. Data for fitting models come from comparisons made within groups\nof units that pass initial file blocking requirements. Data distributions can\nvary across blocks. This article examines the use of prior information and\nhierarchical latent class models in the context of RL.\n", "versions": [{"version": "v1", "created": "Thu, 20 Dec 2012 19:37:25 GMT"}], "update_date": "2012-12-21", "authors_parsed": [["Larsen", "Michael D.", ""]]}, {"id": "1212.5332", "submitter": "Yingying Fan", "authors": "Yingying Fan, Jiashun Jin, Zhigang Yao", "title": "Optimal classification in sparse Gaussian graphic model", "comments": "Published in at http://dx.doi.org/10.1214/13-AOS1163 the Annals of\n  Statistics (http://www.imstat.org/aos/) by the Institute of Mathematical\n  Statistics (http://www.imstat.org)", "journal-ref": "Annals of Statistics 2013, Vol. 41, No. 5, 2537-2571", "doi": "10.1214/13-AOS1163", "report-no": "IMS-AOS-AOS1163", "categories": "stat.ML math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Consider a two-class classification problem where the number of features is\nmuch larger than the sample size. The features are masked by Gaussian noise\nwith mean zero and covariance matrix $\\Sigma$, where the precision matrix\n$\\Omega=\\Sigma^{-1}$ is unknown but is presumably sparse. The useful features,\nalso unknown, are sparse and each contributes weakly (i.e., rare and weak) to\nthe classification decision. By obtaining a reasonably good estimate of\n$\\Omega$, we formulate the setting as a linear regression model. We propose a\ntwo-stage classification method where we first select features by the method of\nInnovated Thresholding (IT), and then use the retained features and Fisher's\nLDA for classification. In this approach, a crucial problem is how to set the\nthreshold of IT. We approach this problem by adapting the recent innovation of\nHigher Criticism Thresholding (HCT). We find that when useful features are rare\nand weak, the limiting behavior of HCT is essentially just as good as the\nlimiting behavior of ideal threshold, the threshold one would choose if the\nunderlying distribution of the signals is known (if only). Somewhat\nsurprisingly, when $\\Omega$ is sufficiently sparse, its off-diagonal\ncoordinates usually do not have a major influence over the classification\ndecision. Compared to recent work in the case where $\\Omega$ is the identity\nmatrix [Proc. Natl. Acad. Sci. USA 105 (2008) 14790-14795; Philos. Trans. R.\nSoc. Lond. Ser. A Math. Phys. Eng. Sci. 367 (2009) 4449-4470], the current\nsetting is much more general, which needs a new approach and much more\nsophisticated analysis. One key component of the analysis is the intimate\nrelationship between HCT and Fisher's separation. Another key component is the\ntight large-deviation bounds for empirical processes for data with\nunconventional correlation structures, where graph theory on vertex coloring\nplays an important role.\n", "versions": [{"version": "v1", "created": "Fri, 21 Dec 2012 04:25:12 GMT"}, {"version": "v2", "created": "Wed, 20 Nov 2013 09:33:09 GMT"}], "update_date": "2013-11-21", "authors_parsed": [["Fan", "Yingying", ""], ["Jin", "Jiashun", ""], ["Yao", "Zhigang", ""]]}, {"id": "1212.5423", "submitter": "Shameem Puthiya Parambath Mr.", "authors": "Shameem A Puthiya Parambath", "title": "Topic Extraction and Bundling of Related Scientific Articles", "comments": "NeSeFo 2012", "journal-ref": null, "doi": null, "report-no": "NeSeFo 2012", "categories": "cs.IR cs.DL stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Automatic classification of scientific articles based on common\ncharacteristics is an interesting problem with many applications in digital\nlibrary and information retrieval systems. Properly organized articles can be\nuseful for automatic generation of taxonomies in scientific writings, textual\nsummarization, efficient information retrieval etc. Generating article bundles\nfrom a large number of input articles, based on the associated features of the\narticles is tedious and computationally expensive task. In this report we\npropose an automatic two-step approach for topic extraction and bundling of\nrelated articles from a set of scientific articles in real-time. For topic\nextraction, we make use of Latent Dirichlet Allocation (LDA) topic modeling\ntechniques and for bundling, we make use of hierarchical agglomerative\nclustering techniques.\n  We run experiments to validate our bundling semantics and compare it with\nexisting models in use. We make use of an online crowdsourcing marketplace\nprovided by Amazon called Amazon Mechanical Turk to carry out experiments. We\nexplain our experimental setup and empirical results in detail and show that\nour method is advantageous over existing ones.\n", "versions": [{"version": "v1", "created": "Fri, 21 Dec 2012 13:25:00 GMT"}, {"version": "v2", "created": "Fri, 1 May 2015 15:30:45 GMT"}], "update_date": "2015-05-04", "authors_parsed": [["Parambath", "Shameem A Puthiya", ""]]}, {"id": "1212.5637", "submitter": "Claudio Gentile", "authors": "Nicolo' Cesa-Bianchi, Claudio Gentile, Fabio Vitale, Giovanni Zappella", "title": "Random Spanning Trees and the Prediction of Weighted Graphs", "comments": "Appeared in ICML 2010", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We investigate the problem of sequentially predicting the binary labels on\nthe nodes of an arbitrary weighted graph. We show that, under a suitable\nparametrization of the problem, the optimal number of prediction mistakes can\nbe characterized (up to logarithmic factors) by the cutsize of a random\nspanning tree of the graph. The cutsize is induced by the unknown adversarial\nlabeling of the graph nodes. In deriving our characterization, we obtain a\nsimple randomized algorithm achieving in expectation the optimal mistake bound\non any polynomially connected weighted graph. Our algorithm draws a random\nspanning tree of the original graph and then predicts the nodes of this tree in\nconstant expected amortized time and linear space. Experiments on real-world\ndatasets show that our method compares well to both global (Perceptron) and\nlocal (label propagation) methods, while being generally faster in practice.\n", "versions": [{"version": "v1", "created": "Fri, 21 Dec 2012 23:51:21 GMT"}], "update_date": "2012-12-27", "authors_parsed": [["Cesa-Bianchi", "Nicolo'", ""], ["Gentile", "Claudio", ""], ["Vitale", "Fabio", ""], ["Zappella", "Giovanni", ""]]}, {"id": "1212.5760", "submitter": "Paul McNicholas", "authors": "Yuhong Wei and Paul D. McNicholas", "title": "Mixture Model Averaging for Clustering", "comments": null, "journal-ref": null, "doi": "10.1007/s11634-014-0182-6", "report-no": null, "categories": "stat.ME stat.CO stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In mixture model-based clustering applications, it is common to fit several\nmodels from a family and report clustering results from only the `best' one. In\nsuch circumstances, selection of this best model is achieved using a model\nselection criterion, most often the Bayesian information criterion. Rather than\nthrow away all but the best model, we average multiple models that are in some\nsense close to the best one, thereby producing a weighted average of clustering\nresults. Two (weighted) averaging approaches are considered: averaging the\ncomponent membership probabilities and averaging models. In both cases, Occam's\nwindow is used to determine closeness to the best model and weights are\ncomputed within a Bayesian model averaging paradigm. In some cases, we need to\nmerge components before averaging; we introduce a method for merging mixture\ncomponents based on the adjusted Rand index. The effectiveness of our\nmodel-based clustering averaging approaches is illustrated using a family of\nGaussian mixture models on real and simulated data.\n", "versions": [{"version": "v1", "created": "Sun, 23 Dec 2012 04:29:13 GMT"}, {"version": "v2", "created": "Mon, 24 Jun 2013 14:26:16 GMT"}, {"version": "v3", "created": "Sat, 26 Jul 2014 20:36:39 GMT"}], "update_date": "2017-10-09", "authors_parsed": [["Wei", "Yuhong", ""], ["McNicholas", "Paul D.", ""]]}, {"id": "1212.5921", "submitter": "Miguel \\'A. Carreira-Perpi\\~n\\'an", "authors": "Miguel \\'A. Carreira-Perpi\\~n\\'an and Weiran Wang", "title": "Distributed optimization of deeply nested systems", "comments": "21 pages, 7 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.NE math.OC stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In science and engineering, intelligent processing of complex signals such as\nimages, sound or language is often performed by a parameterized hierarchy of\nnonlinear processing layers, sometimes biologically inspired. Hierarchical\nsystems (or, more generally, nested systems) offer a way to generate complex\nmappings using simple stages. Each layer performs a different operation and\nachieves an ever more sophisticated representation of the input, as, for\nexample, in an deep artificial neural network, an object recognition cascade in\ncomputer vision or a speech front-end processing. Joint estimation of the\nparameters of all the layers and selection of an optimal architecture is widely\nconsidered to be a difficult numerical nonconvex optimization problem,\ndifficult to parallelize for execution in a distributed computation\nenvironment, and requiring significant human expert effort, which leads to\nsuboptimal systems in practice. We describe a general mathematical strategy to\nlearn the parameters and, to some extent, the architecture of nested systems,\ncalled the method of auxiliary coordinates (MAC). This replaces the original\nproblem involving a deeply nested function with a constrained problem involving\na different function in an augmented space without nesting. The constrained\nproblem may be solved with penalty-based methods using alternating optimization\nover the parameters and the auxiliary coordinates. MAC has provable\nconvergence, is easy to implement reusing existing algorithms for single\nlayers, can be parallelized trivially and massively, applies even when\nparameter derivatives are not available or not desirable, and is competitive\nwith state-of-the-art nonlinear optimizers even in the serial computation\nsetting, often providing reasonable models within a few iterations.\n", "versions": [{"version": "v1", "created": "Mon, 24 Dec 2012 14:45:25 GMT"}], "update_date": "2012-12-27", "authors_parsed": [["Carreira-Perpi\u00f1\u00e1n", "Miguel \u00c1.", ""], ["Wang", "Weiran", ""]]}, {"id": "1212.5932", "submitter": "Leo Lahti", "authors": "Leo Lahti, Aurora Torrente, Laura L. Elo, Alvis Brazma, Johan Rung", "title": "Fully scalable online-preprocessing algorithm for short oligonucleotide\n  microarray atlases", "comments": "20 pages, 3 figures, 1 supplementary PDF", "journal-ref": "Leo Lahti, Aurora Torrente, Laura L. Elo, Alvis Brazma, Johan\n  Rung. A fully scalable online pre-processing algorithm for short\n  oligonucleotide microarray atlases. Nucleic Acids Research, Online April 5,\n  2013", "doi": "10.1093/nar/gkt229", "report-no": null, "categories": "q-bio.QM cs.CE cs.LG q-bio.GN stat.AP stat.ML", "license": "http://creativecommons.org/licenses/by/3.0/", "abstract": "  Accumulation of standardized data collections is opening up novel\nopportunities for holistic characterization of genome function. The limited\nscalability of current preprocessing techniques has, however, formed a\nbottleneck for full utilization of contemporary microarray collections. While\nshort oligonucleotide arrays constitute a major source of genome-wide profiling\ndata, scalable probe-level preprocessing algorithms have been available only\nfor few measurement platforms based on pre-calculated model parameters from\nrestricted reference training sets. To overcome these key limitations, we\nintroduce a fully scalable online-learning algorithm that provides tools to\nprocess large microarray atlases including tens of thousands of arrays. Unlike\nthe alternatives, the proposed algorithm scales up in linear time with respect\nto sample size and is readily applicable to all short oligonucleotide\nplatforms. This is the only available preprocessing algorithm that can learn\nprobe-level parameters based on sequential hyperparameter updates at small,\nconsecutive batches of data, thus circumventing the extensive memory\nrequirements of the standard approaches and opening up novel opportunities to\ntake full advantage of contemporary microarray data collections. Moreover,\nusing the most comprehensive data collections to estimate probe-level effects\ncan assist in pinpointing individual probes affected by various biases and\nprovide new tools to guide array design and quality control. The implementation\nis freely available in R/Bioconductor at\nhttp://www.bioconductor.org/packages/devel/bioc/html/RPA.html\n", "versions": [{"version": "v1", "created": "Mon, 24 Dec 2012 16:41:08 GMT"}, {"version": "v2", "created": "Thu, 27 Dec 2012 11:23:39 GMT"}], "update_date": "2013-04-09", "authors_parsed": [["Lahti", "Leo", ""], ["Torrente", "Aurora", ""], ["Elo", "Laura L.", ""], ["Brazma", "Alvis", ""], ["Rung", "Johan", ""]]}, {"id": "1212.6018", "submitter": "Gordon J Ross", "authors": "Gordon J. Ross, Niall M. Adams, Dimitris K. Tasoulis, David J. Hand", "title": "Exponentially Weighted Moving Average Charts for Detecting Concept Drift", "comments": null, "journal-ref": "Pattern Recognition Letters, 33(2) 191-198, 2012", "doi": "10.1016/j.patrec.2011.08.019", "report-no": null, "categories": "stat.ML cs.LG stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Classifying streaming data requires the development of methods which are\ncomputationally efficient and able to cope with changes in the underlying\ndistribution of the stream, a phenomenon known in the literature as concept\ndrift. We propose a new method for detecting concept drift which uses an\nExponentially Weighted Moving Average (EWMA) chart to monitor the\nmisclassification rate of an streaming classifier. Our approach is modular and\ncan hence be run in parallel with any underlying classifier to provide an\nadditional layer of concept drift detection. Moreover our method is\ncomputationally efficient with overhead O(1) and works in a fully online manner\nwith no need to store data points in memory. Unlike many existing approaches to\nconcept drift detection, our method allows the rate of false positive\ndetections to be controlled and kept constant over time.\n", "versions": [{"version": "v1", "created": "Tue, 25 Dec 2012 11:01:48 GMT"}], "update_date": "2012-12-27", "authors_parsed": [["Ross", "Gordon J.", ""], ["Adams", "Niall M.", ""], ["Tasoulis", "Dimitris K.", ""], ["Hand", "David J.", ""]]}, {"id": "1212.6110", "submitter": "Makiko Konoshima", "authors": "Makiko Konoshima and Yui Noma", "title": "Hyperplane Arrangements and Locality-Sensitive Hashing with Lift", "comments": "9 pages, 7 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.IR stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Locality-sensitive hashing converts high-dimensional feature vectors, such as\nimage and speech, into bit arrays and allows high-speed similarity calculation\nwith the Hamming distance. There is a hashing scheme that maps feature vectors\nto bit arrays depending on the signs of the inner products between feature\nvectors and the normal vectors of hyperplanes placed in the feature space. This\nhashing can be seen as a discretization of the feature space by hyperplanes. If\nlabels for data are given, one can determine the hyperplanes by using learning\nalgorithms. However, many proposed learning methods do not consider the\nhyperplanes' offsets. Not doing so decreases the number of partitioned regions,\nand the correlation between Hamming distances and Euclidean distances becomes\nsmall. In this paper, we propose a lift map that converts learning algorithms\nwithout the offsets to the ones that take into account the offsets. With this\nmethod, the learning methods without the offsets give the discretizations of\nspaces as if it takes into account the offsets. For the proposed method, we\ninput several high-dimensional feature data sets and studied the relationship\nbetween the statistical characteristics of data, the number of hyperplanes, and\nthe effect of the proposed method.\n", "versions": [{"version": "v1", "created": "Wed, 26 Dec 2012 02:14:41 GMT"}], "update_date": "2012-12-27", "authors_parsed": [["Konoshima", "Makiko", ""], ["Noma", "Yui", ""]]}, {"id": "1212.6246", "submitter": "Radford M. Neal", "authors": "Chunyi Wang and Radford M. Neal", "title": "Gaussian Process Regression with Heteroscedastic or Non-Gaussian\n  Residuals", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Gaussian Process (GP) regression models typically assume that residuals are\nGaussian and have the same variance for all observations. However, applications\nwith input-dependent noise (heteroscedastic residuals) frequently arise in\npractice, as do applications in which the residuals do not have a Gaussian\ndistribution. In this paper, we propose a GP Regression model with a latent\nvariable that serves as an additional unobserved covariate for the regression.\nThis model (which we call GPLC) allows for heteroscedasticity since it allows\nthe function to have a changing partial derivative with respect to this\nunobserved covariate. With a suitable covariance function, our GPLC model can\nhandle (a) Gaussian residuals with input-dependent variance, or (b)\nnon-Gaussian residuals with input-dependent variance, or (c) Gaussian residuals\nwith constant variance. We compare our model, using synthetic datasets, with a\nmodel proposed by Goldberg, Williams and Bishop (1998), which we refer to as\nGPLV, which only deals with case (a), as well as a standard GP model which can\nhandle only case (c). Markov Chain Monte Carlo methods are developed for both\nmodelsl. Experiments show that when the data is heteroscedastic, both GPLC and\nGPLV give better results (smaller mean squared error and negative\nlog-probability density) than standard GP regression. In addition, when the\nresidual are Gaussian, our GPLC model is generally nearly as good as GPLV,\nwhile when the residuals are non-Gaussian, our GPLC model is better than GPLV.\n", "versions": [{"version": "v1", "created": "Wed, 26 Dec 2012 20:45:48 GMT"}], "update_date": "2012-12-27", "authors_parsed": [["Wang", "Chunyi", ""], ["Neal", "Radford M.", ""]]}, {"id": "1212.6316", "submitter": "Nathalie Villa-Vialaneix", "authors": "Madalina Olteanu (SAMM), Nathalie Villa-Vialaneix (SAMM), Marie\n  Cottrell (SAMM)", "title": "On-line relational SOM for dissimilarity data", "comments": "WSOM 2012, Santiago : Chile (2012)", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In some applications and in order to address real world situations better,\ndata may be more complex than simple vectors. In some examples, they can be\nknown through their pairwise dissimilarities only. Several variants of the Self\nOrganizing Map algorithm were introduced to generalize the original algorithm\nto this framework. Whereas median SOM is based on a rough representation of the\nprototypes, relational SOM allows representing these prototypes by a virtual\ncombination of all elements in the data set. However, this latter approach\nsuffers from two main drawbacks. First, its complexity can be large. Second,\nonly a batch version of this algorithm has been studied so far and it often\nprovides results having a bad topographic organization. In this article, an\non-line version of relational SOM is described and justified. The algorithm is\ntested on several datasets, including categorical data and graphs, and compared\nwith the batch version and with other SOM algorithms for non vector data.\n", "versions": [{"version": "v1", "created": "Thu, 27 Dec 2012 07:07:06 GMT"}], "update_date": "2013-01-03", "authors_parsed": [["Olteanu", "Madalina", "", "SAMM"], ["Villa-Vialaneix", "Nathalie", "", "SAMM"], ["Cottrell", "Marie", "", "SAMM"]]}, {"id": "1212.6323", "submitter": "Pili Hu", "authors": "Pili Hu and Wing Cheong Lau", "title": "Localized Algorithm of Community Detection on Large-Scale Decentralized\n  Social Networks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.SI physics.soc-ph stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Despite the overwhelming success of the existing Social Networking Services\n(SNS), their centralized ownership and control have led to serious concerns in\nuser privacy, censorship vulnerability and operational robustness of these\nservices. To overcome these limitations, Distributed Social Networks (DSN) have\nrecently been proposed and implemented. Under these new DSN architectures, no\nsingle party possesses the full knowledge of the entire social network. While\nthis approach solves the above problems, the lack of global knowledge for the\nDSN nodes makes it much more challenging to support some common but critical\nSNS services like friends discovery and community detection. In this paper, we\ntackle the problem of community detection for a given user under the constraint\nof limited local topology information as imposed by common DSN architectures.\nBy considering the Personalized Page Rank (PPR) approach as an ink spilling\nprocess, we justify its applicability for decentralized community detection\nusing limited local topology information.Our proposed PPR-based solution has a\nwide range of applications such as friends recommendation, targeted\nadvertisement, automated social relationship labeling and sybil defense. Using\ndata collected from a large-scale SNS in practice, we demonstrate our adapted\nversion of PPR can significantly outperform the basic PR as well as two other\ncommonly used heuristics. The inclusion of a few manually labeled friends in\nthe Escape Vector (EV) can boost the performance considerably (64.97% relative\nimprovement in terms of Area Under the ROC Curve (AUC)).\n", "versions": [{"version": "v1", "created": "Thu, 27 Dec 2012 07:59:01 GMT"}], "update_date": "2013-01-01", "authors_parsed": [["Hu", "Pili", ""], ["Lau", "Wing Cheong", ""]]}, {"id": "1212.6659", "submitter": "Raphael Pelossof", "authors": "Raphael Pelossof and Zhiliang Ying", "title": "Focus of Attention for Linear Predictors", "comments": "9 pages, 4 figures. arXiv admin note: substantial text overlap with\n  arXiv:1105.0382", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.AI cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a method to stop the evaluation of a prediction process when the\nresult of the full evaluation is obvious. This trait is highly desirable in\nprediction tasks where a predictor evaluates all its features for every example\nin large datasets. We observe that some examples are easier to classify than\nothers, a phenomenon which is characterized by the event when most of the\nfeatures agree on the class of an example. By stopping the feature evaluation\nwhen encountering an easy- to-classify example, the predictor can achieve\nsubstantial gains in computation. Our method provides a natural attention\nmechanism for linear predictors where the predictor concentrates most of its\ncomputation on hard-to-classify examples and quickly discards easy-to-classify\nones. By modifying a linear prediction algorithm such as an SVM or AdaBoost to\ninclude our attentive method we prove that the average number of features\ncomputed is O(sqrt(n log 1/sqrt(delta))) where n is the original number of\nfeatures, and delta is the error rate incurred due to early stopping. We\ndemonstrate the effectiveness of Attentive Prediction on MNIST, Real-sim,\nGisette, and synthetic datasets.\n", "versions": [{"version": "v1", "created": "Sat, 29 Dec 2012 20:23:48 GMT"}], "update_date": "2013-01-01", "authors_parsed": [["Pelossof", "Raphael", ""], ["Ying", "Zhiliang", ""]]}, {"id": "1212.6788", "submitter": "Zuofeng Shang", "authors": "Zuofeng Shang, Guang Cheng", "title": "Local and global asymptotic inference in smoothing spline models", "comments": "Published in at http://dx.doi.org/10.1214/13-AOS1164 the Annals of\n  Statistics (http://www.imstat.org/aos/) by the Institute of Mathematical\n  Statistics (http://www.imstat.org)", "journal-ref": "Annals of Statistics 2013, Vol. 41, No. 5, 2608-2638", "doi": "10.1214/13-AOS1164", "report-no": "IMS-AOS-AOS1164", "categories": "math.ST stat.ML stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This article studies local and global inference for smoothing spline\nestimation in a unified asymptotic framework. We first introduce a new\ntechnical tool called functional Bahadur representation, which significantly\ngeneralizes the traditional Bahadur representation in parametric models, that\nis, Bahadur [Ann. Inst. Statist. Math. 37 (1966) 577-580]. Equipped with this\ntool, we develop four interconnected procedures for inference: (i) pointwise\nconfidence interval; (ii) local likelihood ratio testing; (iii) simultaneous\nconfidence band; (iv) global likelihood ratio testing. In particular, our\nconfidence intervals are proved to be asymptotically valid at any point in the\nsupport, and they are shorter on average than the Bayesian confidence intervals\nproposed by Wahba [J. R. Stat. Soc. Ser. B Stat. Methodol. 45 (1983) 133-150]\nand Nychka [J. Amer. Statist. Assoc. 83 (1988) 1134-1143]. We also discuss a\nversion of the Wilks phenomenon arising from local/global likelihood ratio\ntesting. It is also worth noting that our simultaneous confidence bands are the\nfirst ones applicable to general quasi-likelihood models. Furthermore, issues\nrelating to optimality and efficiency are carefully addressed. As a by-product,\nwe discover a surprising relationship between periodic and nonperiodic\nsmoothing splines in terms of inference.\n", "versions": [{"version": "v1", "created": "Sun, 30 Dec 2012 22:34:28 GMT"}, {"version": "v2", "created": "Wed, 28 Aug 2013 03:55:58 GMT"}, {"version": "v3", "created": "Tue, 26 Nov 2013 11:56:08 GMT"}], "update_date": "2013-11-27", "authors_parsed": [["Shang", "Zuofeng", ""], ["Cheng", "Guang", ""]]}, {"id": "1212.6846", "submitter": "Sagar Kale", "authors": "Sagar Kale", "title": "Maximizing a Nonnegative, Monotone, Submodular Function Constrained to\n  Matchings", "comments": "Withdrawn because the main result is implied by a more general result\n  about p-independence-system (which generalize matchings) in the paper by\n  Calinescu, Chekuri, Pal, and Vondrak, Maximizing a Monotone Submodular\n  Function Subject to a Matroid Constraint, SIAM J. Comput., 2011, Vol 40, No\n  6, pp. 1740-1766", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.AI cs.CC cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Submodular functions have many applications. Matchings have many\napplications. The bitext word alignment problem can be modeled as the problem\nof maximizing a nonnegative, monotone, submodular function constrained to\nmatchings in a complete bipartite graph where each vertex corresponds to a word\nin the two input sentences and each edge represents a potential word-to-word\ntranslation. We propose a more general problem of maximizing a nonnegative,\nmonotone, submodular function defined on the edge set of a complete graph\nconstrained to matchings; we call this problem the CSM-Matching problem.\nCSM-Matching also generalizes the maximum-weight matching problem, which has a\npolynomial-time algorithm; however, we show that it is NP-hard to approximate\nCSM-Matching within a factor of e/(e-1) by reducing the max k-cover problem to\nit. Our main result is a simple, greedy, 3-approximation algorithm for\nCSM-Matching. Then we reduce CSM-Matching to maximizing a nonnegative,\nmonotone, submodular function over two matroids, i.e., CSM-2-Matroids.\nCSM-2-Matroids has a (2+epsilon)-approximation algorithm - called LSV2. We show\nthat we can find a (4+epsilon)-approximate solution to CSM-Matching using LSV2.\nWe extend this approach to similar problems.\n", "versions": [{"version": "v1", "created": "Mon, 31 Dec 2012 09:32:51 GMT"}, {"version": "v2", "created": "Thu, 10 Jan 2013 21:20:45 GMT"}], "update_date": "2013-01-14", "authors_parsed": [["Kale", "Sagar", ""]]}, {"id": "1212.6936", "submitter": "David Luengo", "authors": "David Luengo, Javier Via, Sandra Monzon, Tom Trigano and Antonio\n  Artes-Rodriguez", "title": "Blind Analysis of EGM Signals: Sparsity-Aware Formulation", "comments": "29 pages, 1 figure", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This technical note considers the problems of blind sparse learning and\ninference of electrogram (EGM) signals under atrial fibrillation (AF)\nconditions. First of all we introduce a mathematical model for the observed\nsignals that takes into account the multiple foci typically appearing inside\nthe heart during AF. Then we propose a reconstruction model based on a fixed\ndictionary and discuss several alternatives for choosing the dictionary. In\norder to obtain a sparse solution that takes into account the biological\nrestrictions of the problem, a first alternative is using LASSO regularization\nfollowed by a post-processing stage that removes low amplitude coefficients\nviolating the refractory period characteristic of cardiac cells. As an\nalternative we propose a novel regularization term, called cross products LASSO\n(CP-LASSO), that is able to incorporate the biological constraints directly\ninto the optimization problem. Unfortunately, the resulting problem is\nnon-convex, but we show how it can be solved efficiently in an approximated way\nmaking use of successive convex approximations (SCA). Finally, spectral\nanalysis is performed on the clean activation sequence obtained from the sparse\nlearning stage in order to estimate the number of latent foci and their\nfrequencies. Simulations on synthetic and real data are provided to validate\nthe proposed approach.\n", "versions": [{"version": "v1", "created": "Mon, 31 Dec 2012 17:56:08 GMT"}], "update_date": "2013-01-01", "authors_parsed": [["Luengo", "David", ""], ["Via", "Javier", ""], ["Monzon", "Sandra", ""], ["Trigano", "Tom", ""], ["Artes-Rodriguez", "Antonio", ""]]}]