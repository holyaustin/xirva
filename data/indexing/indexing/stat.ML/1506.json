[{"id": "1506.00053", "submitter": "Panagiotis Tsilifis", "authors": "Panagiotis Tsilifis, Roger G. Ghanem and Paris Hajali", "title": "Efficient Bayesian experimentation using an expected information gain\n  lower bound", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML physics.geo-ph stat.CO stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Experimental design is crucial for inference where limitations in the data\ncollection procedure are present due to cost or other restrictions. Optimal\nexperimental designs determine parameters that in some appropriate sense make\nthe data the most informative possible. In a Bayesian setting this is\ntranslated to updating to the best possible posterior. Information theoretic\narguments have led to the formation of the expected information gain as a\ndesign criterion. This can be evaluated mainly by Monte Carlo sampling and\nmaximized by using stochastic approximation methods, both known for being\ncomputationally expensive tasks. We propose a framework where a lower bound of\nthe expected information gain is used as an alternative design criterion. In\naddition to alleviating the computational burden, this also addresses issues\nconcerning estimation bias. The problem of permeability inference in a large\ncontaminated area is used to demonstrate the validity of our approach where we\nemploy the massively parallel version of the multiphase multicomponent\nsimulator TOUGH2 to simulate contaminant transport and a Polynomial Chaos\napproximation of the forward model that further accelerates the objective\nfunction evaluations. The proposed methodology is demonstrated to a setting\nwhere field measurements are available.\n", "versions": [{"version": "v1", "created": "Sat, 30 May 2015 01:17:38 GMT"}, {"version": "v2", "created": "Thu, 10 Mar 2016 18:34:41 GMT"}], "update_date": "2016-03-11", "authors_parsed": [["Tsilifis", "Panagiotis", ""], ["Ghanem", "Roger G.", ""], ["Hajali", "Paris", ""]]}, {"id": "1506.00054", "submitter": "Justin Kinney", "authors": "Gurinder S. Atwal and Justin B. Kinney", "title": "Learning quantitative sequence-function relationships from massively\n  parallel experiments", "comments": "35 pages, 8 figures. Revised manuscript currently under review for\n  publication", "journal-ref": null, "doi": "10.1007/s10955-015-1398-3", "report-no": null, "categories": "q-bio.QM math.ST physics.bio-ph physics.data-an stat.ML stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A fundamental aspect of biological information processing is the ubiquity of\nsequence-function relationships -- functions that map the sequence of DNA, RNA,\nor protein to a biochemically relevant activity. Most sequence-function\nrelationships in biology are quantitative, but only recently have experimental\ntechniques for effectively measuring these relationships been developed. The\nadvent of such \"massively parallel\" experiments presents an exciting\nopportunity for the concepts and methods of statistical physics to inform the\nstudy of biological systems. After reviewing these recent experimental\nadvances, we focus on the problem of how to infer parametric models of\nsequence-function relationships from the data produced by these experiments.\nSpecifically, we retrace and extend recent theoretical work showing that\ninference based on mutual information, not the standard likelihood-based\napproach, is often necessary for accurately learning the parameters of these\nmodels. Closely connected with this result is the emergence of \"diffeomorphic\nmodes\" -- directions in parameter space that are far less constrained by data\nthan likelihood-based inference would suggest. Analogous to Goldstone modes in\nphysics, diffeomorphic modes arise from an arbitrarily broken symmetry of the\ninference problem. An analytically tractable model of a massively parallel\nexperiment is then described, providing an explicit demonstration of these\nfundamental aspects of statistical inference. This paper concludes with an\noutlook on the theoretical and computational challenges currently facing\nstudies of quantitative sequence-function relationships.\n", "versions": [{"version": "v1", "created": "Sat, 30 May 2015 01:20:59 GMT"}, {"version": "v2", "created": "Tue, 22 Sep 2015 15:47:20 GMT"}], "update_date": "2016-03-23", "authors_parsed": [["Atwal", "Gurinder S.", ""], ["Kinney", "Justin B.", ""]]}, {"id": "1506.00059", "submitter": "Martin Arjovsky", "authors": "Martin Arjovsky", "title": "Saddle-free Hessian-free Optimization", "comments": "NIPS 2016 Workshop on Nonconvex Optimization for Machine Learning:\n  Theory and Practice", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NA stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Nonconvex optimization problems such as the ones in training deep neural\nnetworks suffer from a phenomenon called saddle point proliferation. This means\nthat there are a vast number of high error saddle points present in the loss\nfunction. Second order methods have been tremendously successful and widely\nadopted in the convex optimization community, while their usefulness in deep\nlearning remains limited. This is due to two problems: computational complexity\nand the methods being driven towards the high error saddle points. We introduce\na novel algorithm specially designed to solve these two issues, providing a\ncrucial first step to take the widely known advantages of Newton's method to\nthe nonconvex optimization community, especially in high dimensional settings.\n", "versions": [{"version": "v1", "created": "Sat, 30 May 2015 02:42:21 GMT"}, {"version": "v2", "created": "Wed, 12 Oct 2016 18:59:41 GMT"}, {"version": "v3", "created": "Sat, 5 Nov 2016 22:37:12 GMT"}], "update_date": "2016-11-08", "authors_parsed": [["Arjovsky", "Martin", ""]]}, {"id": "1506.00102", "submitter": "Pau Bellot", "authors": "Pau Bellot, Patrick E. Meyer", "title": "Efficient combination of pairswise feature networks", "comments": "JMLR: Workshop and Conference Proceedings, 2014 Connectomics (ECML\n  2014)", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper presents a novel method for the reconstruction of a neural network\nconnectivity using calcium fluorescence data. We introduce a fast unsupervised\nmethod to integrate different networks that reconstructs structural\nconnectivity from neuron activity. Our method improves the state-of-the-art\nreconstruction method General Transfer Entropy (GTE). We are able to better\neliminate indirect links, improving therefore the quality of the network via a\nnormalization and ensemble process of GTE and three new informative features.\nThe approach is based on a simple combination of networks, which is remarkably\nfast. The performance of our approach is benchmarked on simulated time series\nprovided at the connectomics challenge and also submitted at the public\ncompetition.\n", "versions": [{"version": "v1", "created": "Sat, 30 May 2015 10:31:31 GMT"}], "update_date": "2015-06-02", "authors_parsed": [["Bellot", "Pau", ""], ["Meyer", "Patrick E.", ""]]}, {"id": "1506.00308", "submitter": "Ardavan Saeedi", "authors": "Ardavan Saeedi, Vlad Firoiu, Vikash Mansinghka", "title": "Automatic Inference for Inverting Software Simulators via Probabilistic\n  Programming", "comments": "ICML 2014 AutoML Workshop", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Models of complex systems are often formalized as sequential software\nsimulators: computationally intensive programs that iteratively build up\nprobable system configurations given parameters and initial conditions. These\nsimulators enable modelers to capture effects that are difficult to\ncharacterize analytically or summarize statistically. However, in many\nreal-world applications, these simulations need to be inverted to match the\nobserved data. This typically requires the custom design, derivation and\nimplementation of sophisticated inversion algorithms. Here we give a framework\nfor inverting a broad class of complex software simulators via probabilistic\nprogramming and automatic inference, using under 20 lines of probabilistic\ncode. Our approach is based on a formulation of inversion as approximate\ninference in a simple sequential probabilistic model. We implement four\ninference strategies, including Metropolis-Hastings, a sequentialized\nMetropolis-Hastings scheme, and a particle Markov chain Monte Carlo scheme,\nrequiring 4 or fewer lines of probabilistic code each. We demonstrate our\nframework by applying it to invert a real geological software simulator from\nthe oil and gas industry.\n", "versions": [{"version": "v1", "created": "Sun, 31 May 2015 23:53:23 GMT"}], "update_date": "2015-06-02", "authors_parsed": [["Saeedi", "Ardavan", ""], ["Firoiu", "Vlad", ""], ["Mansinghka", "Vikash", ""]]}, {"id": "1506.00323", "submitter": "Anastasia Podosinnikova", "authors": "Anastasia Podosinnikova, Simon Setzer, and Matthias Hein", "title": "Robust PCA: Optimization of the Robust Reconstruction Error over the\n  Stiefel Manifold", "comments": "long version of GCPR 2014 paper", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  It is well known that Principal Component Analysis (PCA) is strongly affected\nby outliers and a lot of effort has been put into robustification of PCA. In\nthis paper we present a new algorithm for robust PCA minimizing the trimmed\nreconstruction error. By directly minimizing over the Stiefel manifold, we\navoid deflation as often used by projection pursuit methods. In distinction to\nother methods for robust PCA, our method has no free parameter and is\ncomputationally very efficient. We illustrate the performance on various\ndatasets including an application to background modeling and subtraction. Our\nmethod performs better or similar to current state-of-the-art methods while\nbeing faster.\n", "versions": [{"version": "v1", "created": "Mon, 1 Jun 2015 01:57:15 GMT"}], "update_date": "2015-06-02", "authors_parsed": [["Podosinnikova", "Anastasia", ""], ["Setzer", "Simon", ""], ["Hein", "Matthias", ""]]}, {"id": "1506.00327", "submitter": "Zhiguang Wang", "authors": "Zhiguang Wang and Tim Oates", "title": "Imaging Time-Series to Improve Classification and Imputation", "comments": "Accepted by IJCAI-2015 ML track", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.NE stat.ML", "license": "http://creativecommons.org/licenses/by/3.0/", "abstract": "  Inspired by recent successes of deep learning in computer vision, we propose\na novel framework for encoding time series as different types of images,\nnamely, Gramian Angular Summation/Difference Fields (GASF/GADF) and Markov\nTransition Fields (MTF). This enables the use of techniques from computer\nvision for time series classification and imputation. We used Tiled\nConvolutional Neural Networks (tiled CNNs) on 20 standard datasets to learn\nhigh-level features from the individual and compound GASF-GADF-MTF images. Our\napproaches achieve highly competitive results when compared to nine of the\ncurrent best time series classification approaches. Inspired by the bijection\nproperty of GASF on 0/1 rescaled data, we train Denoised Auto-encoders (DA) on\nthe GASF images of four standard and one synthesized compound dataset. The\nimputation MSE on test data is reduced by 12.18%-48.02% when compared to using\nthe raw data. An analysis of the features and weights learned via tiled CNNs\nand DAs explains why the approaches work.\n", "versions": [{"version": "v1", "created": "Mon, 1 Jun 2015 02:17:06 GMT"}], "update_date": "2015-06-02", "authors_parsed": [["Wang", "Zhiguang", ""], ["Oates", "Tim", ""]]}, {"id": "1506.00354", "submitter": "Yasser Roudi", "authors": "Yasser Roudi and Graham Taylor", "title": "Learning with hidden variables", "comments": "revised version accepted in Current Opinion in Neurobiology", "journal-ref": "Current Opinion in Neurobiology (2015), 35: 110-118", "doi": "10.1016/j.conb.2015.07.006", "report-no": null, "categories": "q-bio.NC cond-mat.dis-nn cs.LG cs.NE stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Learning and inferring features that generate sensory input is a task\ncontinuously performed by cortex. In recent years, novel algorithms and\nlearning rules have been proposed that allow neural network models to learn\nsuch features from natural images, written text, audio signals, etc. These\nnetworks usually involve deep architectures with many layers of hidden neurons.\nHere we review recent advancements in this area emphasizing, amongst other\nthings, the processing of dynamical inputs by networks with hidden nodes and\nthe role of single neuron models. These points and the questions they arise can\nprovide conceptual advancements in understanding of learning in the cortex and\nthe relationship between machine learning approaches to learning with hidden\nnodes and those in cortical circuits.\n", "versions": [{"version": "v1", "created": "Mon, 1 Jun 2015 05:36:19 GMT"}, {"version": "v2", "created": "Fri, 24 Jul 2015 20:37:48 GMT"}], "update_date": "2021-04-13", "authors_parsed": [["Roudi", "Yasser", ""], ["Taylor", "Graham", ""]]}, {"id": "1506.00552", "submitter": "Julie Nutini", "authors": "Julie Nutini, Mark Schmidt, Issam H. Laradji, Michael Friedlander,\n  Hoyt Koepke", "title": "Coordinate Descent Converges Faster with the Gauss-Southwell Rule Than\n  Random Selection", "comments": "ICML 2015. v2: Updated the Gauss-Southwell-q result in Section 8 and\n  Appendix H, to remove the part depending on mu_1 (the proof had an error).\n  Added Section 8.1, which discusses conditions under which a rate depending on\n  mu_1 does hold", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.OC cs.LG stat.CO stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  There has been significant recent work on the theory and application of\nrandomized coordinate descent algorithms, beginning with the work of Nesterov\n[SIAM J. Optim., 22(2), 2012], who showed that a random-coordinate selection\nrule achieves the same convergence rate as the Gauss-Southwell selection rule.\nThis result suggests that we should never use the Gauss-Southwell rule, as it\nis typically much more expensive than random selection. However, the empirical\nbehaviours of these algorithms contradict this theoretical result: in\napplications where the computational costs of the selection rules are\ncomparable, the Gauss-Southwell selection rule tends to perform substantially\nbetter than random coordinate selection. We give a simple analysis of the\nGauss-Southwell rule showing that---except in extreme cases---its convergence\nrate is faster than choosing random coordinates. Further, in this work we (i)\nshow that exact coordinate optimization improves the convergence rate for\ncertain sparse problems, (ii) propose a Gauss-Southwell-Lipschitz rule that\ngives an even faster convergence rate given knowledge of the Lipschitz\nconstants of the partial derivatives, (iii) analyze the effect of approximate\nGauss-Southwell rules, and (iv) analyze proximal-gradient variants of the\nGauss-Southwell rule.\n", "versions": [{"version": "v1", "created": "Mon, 1 Jun 2015 16:04:37 GMT"}, {"version": "v2", "created": "Sun, 28 Oct 2018 17:11:00 GMT"}], "update_date": "2018-10-30", "authors_parsed": [["Nutini", "Julie", ""], ["Schmidt", "Mark", ""], ["Laradji", "Issam H.", ""], ["Friedlander", "Michael", ""], ["Koepke", "Hoyt", ""]]}, {"id": "1506.00553", "submitter": "Giles Hooker", "authors": "Giles Hooker and Lucas Mentch", "title": "Bootstrap Bias Corrections for Ensemble Methods", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper examines the use of a residual bootstrap for bias correction in\nmachine learning regression methods. Accounting for bias is an important\nobstacle in recent efforts to develop statistical inference for machine\nlearning methods. We demonstrate empirically that the proposed bootstrap bias\ncorrection can lead to substantial improvements in both bias and predictive\naccuracy. In the context of ensembles of trees, we show that this correction\ncan be approximated at only double the cost of training the original ensemble\nwithout introducing additional variance. Our method is shown to improve\ntest-set accuracy over random forests by up to 70\\% on example problems from\nthe UCI repository.\n", "versions": [{"version": "v1", "created": "Mon, 1 Jun 2015 16:06:19 GMT"}], "update_date": "2015-06-02", "authors_parsed": [["Hooker", "Giles", ""], ["Mentch", "Lucas", ""]]}, {"id": "1506.00619", "submitter": "Bart van Merri\\\"enboer", "authors": "Bart van Merri\\\"enboer, Dzmitry Bahdanau, Vincent Dumoulin, Dmitriy\n  Serdyuk, David Warde-Farley, Jan Chorowski, Yoshua Bengio", "title": "Blocks and Fuel: Frameworks for deep learning", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.NE stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We introduce two Python frameworks to train neural networks on large\ndatasets: Blocks and Fuel. Blocks is based on Theano, a linear algebra compiler\nwith CUDA-support. It facilitates the training of complex neural network models\nby providing parametrized Theano operations, attaching metadata to Theano's\nsymbolic computational graph, and providing an extensive set of utilities to\nassist training the networks, e.g. training algorithms, logging, monitoring,\nvisualization, and serialization. Fuel provides a standard format for machine\nlearning datasets. It allows the user to easily iterate over large datasets,\nperforming many types of pre-processing on the fly.\n", "versions": [{"version": "v1", "created": "Mon, 1 Jun 2015 19:28:27 GMT"}], "update_date": "2015-06-02", "authors_parsed": [["van Merri\u00ebnboer", "Bart", ""], ["Bahdanau", "Dzmitry", ""], ["Dumoulin", "Vincent", ""], ["Serdyuk", "Dmitriy", ""], ["Warde-Farley", "David", ""], ["Chorowski", "Jan", ""], ["Bengio", "Yoshua", ""]]}, {"id": "1506.00673", "submitter": "Rahul Agarwal", "authors": "Rahul Agarwal, Pierre Sacre, and Sridevi V. Sarma", "title": "Mutual Dependence: A Novel Method for Computing Dependencies Between\n  Random Variables", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.ML stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In data science, it is often required to estimate dependencies between\ndifferent data sources. These dependencies are typically calculated using\nPearson's correlation, distance correlation, and/or mutual information.\nHowever, none of these measures satisfy all the Granger's axioms for an \"ideal\nmeasure\". One such ideal measure, proposed by Granger himself, calculates the\nBhattacharyya distance between the joint probability density function (pdf) and\nthe product of marginal pdfs. We call this measure the mutual dependence.\nHowever, to date this measure has not been directly computable from data. In\nthis paper, we use our recently introduced maximum likelihood non-parametric\nestimator for band-limited pdfs, to compute the mutual dependence directly from\nthe data. We construct the estimator of mutual dependence and compare its\nperformance to standard measures (Pearson's and distance correlation) for\ndifferent known pdfs by computing convergence rates, computational complexity,\nand the ability to capture nonlinear dependencies. Our mutual dependence\nestimator requires fewer samples to converge to theoretical values, is faster\nto compute, and captures more complex dependencies than standard measures.\n", "versions": [{"version": "v1", "created": "Mon, 1 Jun 2015 20:53:45 GMT"}], "update_date": "2015-06-03", "authors_parsed": [["Agarwal", "Rahul", ""], ["Sacre", "Pierre", ""], ["Sarma", "Sridevi V.", ""]]}, {"id": "1506.00745", "submitter": "Paul Wiggins Dr", "authors": "Colin H. LaMont and Paul A. Wiggins", "title": "An objective prior that unifies objective Bayes and information-based\n  inference", "comments": "7 pages, 1 figure (+ minor corrections)", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG physics.data-an", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  There are three principle paradigms of statistical inference: (i) Bayesian,\n(ii) information-based and (iii) frequentist inference. We describe an\nobjective prior (the weighting or $w$-prior) which unifies objective Bayes and\ninformation-based inference. The $w$-prior is chosen to make the marginal\nprobability an unbiased estimator of the predictive performance of the model.\nThis definition has several other natural interpretations. From the perspective\nof the information content of the prior, the $w$-prior is both uniformly and\nmaximally uninformative. The $w$-prior can also be understood to result in a\nuniform density of distinguishable models in parameter space. Finally we\ndemonstrate the the $w$-prior is equivalent to the Akaike Information Criterion\n(AIC) for regular models in the asymptotic limit. The $w$-prior appears to be\ngenerically applicable to statistical inference and is free of {\\it ad hoc}\nregularization. The mechanism for suppressing complexity is analogous to AIC:\nmodel complexity reduces model predictivity. We expect this new objective-Bayes\napproach to inference to be widely-applicable to machine-learning problems\nincluding singular models.\n", "versions": [{"version": "v1", "created": "Tue, 2 Jun 2015 04:35:12 GMT"}, {"version": "v2", "created": "Thu, 25 Jun 2015 03:03:51 GMT"}], "update_date": "2015-06-26", "authors_parsed": [["LaMont", "Colin H.", ""], ["Wiggins", "Paul A.", ""]]}, {"id": "1506.00779", "submitter": "Junpei Komiyama", "authors": "Junpei Komiyama, Junya Honda, Hiroshi Nakagawa", "title": "Optimal Regret Analysis of Thompson Sampling in Stochastic Multi-armed\n  Bandit Problem with Multiple Plays", "comments": "Appeared in ICML2015. Fixed the evaluation of term (B) in Lemma 3.\n  Replaced \\tilde{\\mu}->\\theta", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We discuss a multiple-play multi-armed bandit (MAB) problem in which several\narms are selected at each round. Recently, Thompson sampling (TS), a randomized\nalgorithm with a Bayesian spirit, has attracted much attention for its\nempirically excellent performance, and it is revealed to have an optimal regret\nbound in the standard single-play MAB problem. In this paper, we propose the\nmultiple-play Thompson sampling (MP-TS) algorithm, an extension of TS to the\nmultiple-play MAB problem, and discuss its regret analysis. We prove that MP-TS\nfor binary rewards has the optimal regret upper bound that matches the regret\nlower bound provided by Anantharam et al. (1987). Therefore, MP-TS is the first\ncomputationally efficient algorithm with optimal regret. A set of computer\nsimulations was also conducted, which compared MP-TS with state-of-the-art\nalgorithms. We also propose a modification of MP-TS, which is shown to have\nbetter empirical performance.\n", "versions": [{"version": "v1", "created": "Tue, 2 Jun 2015 07:42:16 GMT"}, {"version": "v2", "created": "Tue, 24 May 2016 12:21:19 GMT"}, {"version": "v3", "created": "Wed, 20 Mar 2019 18:10:22 GMT"}], "update_date": "2019-03-22", "authors_parsed": [["Komiyama", "Junpei", ""], ["Honda", "Junya", ""], ["Nakagawa", "Hiroshi", ""]]}, {"id": "1506.00852", "submitter": "Ulrike von Luxburg", "authors": "Mehdi S. M. Sajjadi, Morteza Alamgir, Ulrike von Luxburg", "title": "Peer Grading in a Course on Algorithms and Data Structures: Machine\n  Learning Algorithms do not Improve over Simple Baselines", "comments": "Published at the Third Annual ACM Conference on Learning at Scale L@S", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Peer grading is the process of students reviewing each others' work, such as\nhomework submissions, and has lately become a popular mechanism used in massive\nopen online courses (MOOCs). Intrigued by this idea, we used it in a course on\nalgorithms and data structures at the University of Hamburg. Throughout the\nwhole semester, students repeatedly handed in submissions to exercises, which\nwere then evaluated both by teaching assistants and by a peer grading\nmechanism, yielding a large dataset of teacher and peer grades. We applied\ndifferent statistical and machine learning methods to aggregate the peer grades\nin order to come up with accurate final grades for the submissions (supervised\nand unsupervised, methods based on numeric scores and ordinal rankings).\nSurprisingly, none of them improves over the baseline of using the mean peer\ngrade as the final grade. We discuss a number of possible explanations for\nthese results and present a thorough analysis of the generated dataset.\n", "versions": [{"version": "v1", "created": "Tue, 2 Jun 2015 12:03:30 GMT"}, {"version": "v2", "created": "Wed, 10 Feb 2016 14:49:19 GMT"}], "update_date": "2016-02-11", "authors_parsed": [["Sajjadi", "Mehdi S. M.", ""], ["Alamgir", "Morteza", ""], ["von Luxburg", "Ulrike", ""]]}, {"id": "1506.00898", "submitter": "Akshay Krishnamurthy", "authors": "Martin Azizyan, Akshay Krishnamurthy, Aarti Singh", "title": "Extreme Compressive Sampling for Covariance Estimation", "comments": null, "journal-ref": "IEEE Transactions on Information Theory (Volume: 64, Issue: 12,\n  Dec. 2018)", "doi": null, "report-no": null, "categories": "stat.ML cs.IT math.IT", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper studies the problem of estimating the covariance of a collection\nof vectors using only highly compressed measurements of each vector. An\nestimator based on back-projections of these compressive samples is proposed\nand analyzed. A distribution-free analysis shows that by observing just a\nsingle linear measurement of each vector, one can consistently estimate the\ncovariance matrix, in both infinity and spectral norm, and this same analysis\nleads to precise rates of convergence in both norms. Via information-theoretic\ntechniques, lower bounds showing that this estimator is minimax-optimal for\nboth infinity and spectral norm estimation problems are established. These\nresults are also specialized to give matching upper and lower bounds for\nestimating the population covariance of a collection of Gaussian vectors, again\nin the compressive measurement model. The analysis conducted in this paper\nshows that the effective sample complexity for this problem is scaled by a\nfactor of $m^2/d^2$ where $m$ is the compression dimension and $d$ is the\nambient dimension. Applications to subspace learning (Principal Components\nAnalysis) and learning over distributed sensor networks are also discussed.\n", "versions": [{"version": "v1", "created": "Tue, 2 Jun 2015 14:32:28 GMT"}, {"version": "v2", "created": "Wed, 28 Oct 2015 14:07:12 GMT"}, {"version": "v3", "created": "Tue, 15 Jan 2019 00:42:18 GMT"}], "update_date": "2019-01-16", "authors_parsed": [["Azizyan", "Martin", ""], ["Krishnamurthy", "Akshay", ""], ["Singh", "Aarti", ""]]}, {"id": "1506.00976", "submitter": "Gautier Marti", "authors": "Gautier Marti, Philippe Very and Philippe Donnat", "title": "Toward a generic representation of random variables for machine learning", "comments": "submitted to Pattern Recognition Letters", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper presents a pre-processing and a distance which improve the\nperformance of machine learning algorithms working on independent and\nidentically distributed stochastic processes. We introduce a novel\nnon-parametric approach to represent random variables which splits apart\ndependency and distribution without losing any information. We also propound an\nassociated metric leveraging this representation and its statistical estimate.\nBesides experiments on synthetic datasets, the benefits of our contribution is\nillustrated through the example of clustering financial time series, for\ninstance prices from the credit default swaps market. Results are available on\nthe website www.datagrapple.com and an IPython Notebook tutorial is available\nat www.datagrapple.com/Tech for reproducible research.\n", "versions": [{"version": "v1", "created": "Tue, 2 Jun 2015 17:58:48 GMT"}, {"version": "v2", "created": "Thu, 3 Sep 2015 19:23:30 GMT"}], "update_date": "2015-09-04", "authors_parsed": [["Marti", "Gautier", ""], ["Very", "Philippe", ""], ["Donnat", "Philippe", ""]]}, {"id": "1506.01094", "submitter": "Kelvin Guu", "authors": "Kelvin Guu, John Miller, Percy Liang", "title": "Traversing Knowledge Graphs in Vector Space", "comments": "2015 Conference on Empirical Methods on Natural Language Processing\n  (EMNLP)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI cs.DB stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Path queries on a knowledge graph can be used to answer compositional\nquestions such as \"What languages are spoken by people living in Lisbon?\".\nHowever, knowledge graphs often have missing facts (edges) which disrupts path\nqueries. Recent models for knowledge base completion impute missing facts by\nembedding knowledge graphs in vector spaces. We show that these models can be\nrecursively applied to answer path queries, but that they suffer from cascading\nerrors. This motivates a new \"compositional\" training objective, which\ndramatically improves all models' ability to answer path queries, in some cases\nmore than doubling accuracy. On a standard knowledge base completion task, we\nalso demonstrate that compositional training acts as a novel form of structural\nregularization, reliably improving performance across all base models (reducing\nerrors by up to 43%) and achieving new state-of-the-art results.\n", "versions": [{"version": "v1", "created": "Wed, 3 Jun 2015 00:38:25 GMT"}, {"version": "v2", "created": "Wed, 19 Aug 2015 05:16:24 GMT"}], "update_date": "2015-08-20", "authors_parsed": [["Guu", "Kelvin", ""], ["Miller", "John", ""], ["Liang", "Percy", ""]]}, {"id": "1506.01110", "submitter": "Bokai Cao", "authors": "Bokai Cao, Hucheng Zhou, Guoqiang Li and Philip S. Yu", "title": "Multi-View Factorization Machines", "comments": "WSDM 2016", "journal-ref": null, "doi": "10.1145/2835776.2835777", "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  For a learning task, data can usually be collected from different sources or\nbe represented from multiple views. For example, laboratory results from\ndifferent medical examinations are available for disease diagnosis, and each of\nthem can only reflect the health state of a person from a particular\naspect/view. Therefore, different views provide complementary information for\nlearning tasks. An effective integration of the multi-view information is\nexpected to facilitate the learning performance. In this paper, we propose a\ngeneral predictor, named multi-view machines (MVMs), that can effectively\ninclude all the possible interactions between features from multiple views. A\njoint factorization is embedded for the full-order interaction parameters which\nallows parameter estimation under sparsity. Moreover, MVMs can work in\nconjunction with different loss functions for a variety of machine learning\ntasks. A stochastic gradient descent method is presented to learn the MVM\nmodel. We further illustrate the advantages of MVMs through comparison with\nother methods for multi-view classification, including support vector machines\n(SVMs), support tensor machines (STMs) and factorization machines (FMs).\n", "versions": [{"version": "v1", "created": "Wed, 3 Jun 2015 03:06:42 GMT"}, {"version": "v2", "created": "Fri, 23 Mar 2018 21:18:28 GMT"}], "update_date": "2018-03-28", "authors_parsed": [["Cao", "Bokai", ""], ["Zhou", "Hucheng", ""], ["Li", "Guoqiang", ""], ["Yu", "Philip S.", ""]]}, {"id": "1506.01113", "submitter": "Conrado Miranda", "authors": "Conrado Silva Miranda, Fernando Jos\\'e Von Zuben", "title": "Multi-Objective Optimization for Self-Adjusting Weighted Gradient in\n  Machine Learning Tasks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Much of the focus in machine learning research is placed in creating new\narchitectures and optimization methods, but the overall loss function is seldom\nquestioned. This paper interprets machine learning from a multi-objective\noptimization perspective, showing the limitations of the default linear\ncombination of loss functions over a data set and introducing the hypervolume\nindicator as an alternative. It is shown that the gradient of the hypervolume\nis defined by a self-adjusting weighted mean of the individual loss gradients,\nmaking it similar to the gradient of a weighted mean loss but without requiring\nthe weights to be defined a priori. This enables an inner boosting-like\nbehavior, where the current model is used to automatically place higher weights\non samples with higher losses but without requiring the use of multiple models.\nResults on a denoising autoencoder show that the new formulation is able to\nachieve better mean loss than the direct optimization of the mean loss,\nproviding evidence to the conjecture that self-adjusting the weights creates a\nsmoother loss surface.\n", "versions": [{"version": "v1", "created": "Wed, 3 Jun 2015 03:53:18 GMT"}, {"version": "v2", "created": "Tue, 21 Jul 2015 01:54:13 GMT"}], "update_date": "2015-07-22", "authors_parsed": [["Miranda", "Conrado Silva", ""], ["Von Zuben", "Fernando Jos\u00e9", ""]]}, {"id": "1506.01286", "submitter": "Toby Hocking", "authors": "Toby Dylan Hocking, Guillaume Bourque", "title": "PeakSegJoint: fast supervised peak detection via joint segmentation of\n  multiple count data samples", "comments": "11 pages, 5 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML q-bio.GN", "license": "http://creativecommons.org/licenses/by-nc-sa/3.0/", "abstract": "  Joint peak detection is a central problem when comparing samples in genomic\ndata analysis, but current algorithms for this task are unsupervised and\nlimited to at most 2 sample types. We propose PeakSegJoint, a new constrained\nmaximum likelihood segmentation model for any number of sample types. To select\nthe number of peaks in the segmentation, we propose a supervised penalty\nlearning model. To infer the parameters of these two models, we propose to use\na discrete optimization heuristic for the segmentation, and convex optimization\nfor the penalty learning. In comparisons with state-of-the-art peak detection\nalgorithms, PeakSegJoint achieves similar accuracy, faster speeds, and a more\ninterpretable model with overlapping peaks that occur in exactly the same\npositions across all samples.\n", "versions": [{"version": "v1", "created": "Wed, 3 Jun 2015 15:42:09 GMT"}], "update_date": "2015-06-04", "authors_parsed": [["Hocking", "Toby Dylan", ""], ["Bourque", "Guillaume", ""]]}, {"id": "1506.01326", "submitter": "Philipp Hennig PhD", "authors": "Philipp Hennig and Michael A Osborne and Mark Girolami", "title": "Probabilistic Numerics and Uncertainty in Computations", "comments": "Author Generated Postprint. 17 pages, 4 Figures, 1 Table", "journal-ref": null, "doi": "10.1098/rspa.2015.0142", "report-no": null, "categories": "math.NA cs.AI cs.LG stat.CO stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We deliver a call to arms for probabilistic numerical methods: algorithms for\nnumerical tasks, including linear algebra, integration, optimization and\nsolving differential equations, that return uncertainties in their\ncalculations. Such uncertainties, arising from the loss of precision induced by\nnumerical calculation with limited time or hardware, are important for much\ncontemporary science and industry. Within applications such as climate science\nand astrophysics, the need to make decisions on the basis of computations with\nlarge and complex data has led to a renewed focus on the management of\nnumerical uncertainty. We describe how several seminal classic numerical\nmethods can be interpreted naturally as probabilistic inference. We then show\nthat the probabilistic view suggests new algorithms that can flexibly be\nadapted to suit application specifics, while delivering improved empirical\nperformance. We provide concrete illustrations of the benefits of probabilistic\nnumeric algorithms on real scientific problems from astrometry and astronomical\nimaging, while highlighting open problems with these new algorithms. Finally,\nwe describe how probabilistic numerical methods provide a coherent framework\nfor identifying the uncertainty in calculations performed with a combination of\nnumerical algorithms (e.g. both numerical optimisers and differential equation\nsolvers), potentially allowing the diagnosis (and control) of error sources in\ncomputations.\n", "versions": [{"version": "v1", "created": "Wed, 3 Jun 2015 17:45:01 GMT"}], "update_date": "2016-02-17", "authors_parsed": [["Hennig", "Philipp", ""], ["Osborne", "Michael A", ""], ["Girolami", "Mark", ""]]}, {"id": "1506.01338", "submitter": "Hossein Keshavarz", "authors": "Hossein Keshavarz, Clayton Scott, XuanLong Nguyen", "title": "Optimal change point detection in Gaussian processes", "comments": "42 pages, 2 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST cs.IT cs.LG math.IT stat.ML stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study the problem of detecting a change in the mean of one-dimensional\nGaussian process data. This problem is investigated in the setting of\nincreasing domain (customarily employed in time series analysis) and in the\nsetting of fixed domain (typically arising in spatial data analysis). We\npropose a detection method based on the generalized likelihood ratio test\n(GLRT), and show that our method achieves nearly asymptotically optimal rate in\nthe minimax sense, in both settings. The salient feature of the proposed method\nis that it exploits in an efficient way the data dependence captured by the\nGaussian process covariance structure. When the covariance is not known, we\npropose the plug-in GLRT method and derive conditions under which the method\nremains asymptotically near optimal. By contrast, the standard CUSUM method,\nwhich does not account for the covariance structure, is shown to be\nasymptotically optimal only in the increasing domain. Our algorithms and\naccompanying theory are applicable to a wide variety of covariance structures,\nincluding the Matern class, the powered exponential class, and others. The\nplug-in GLRT method is shown to perform well for maximum likelihood estimators\nwith a dense covariance matrix.\n", "versions": [{"version": "v1", "created": "Wed, 3 Jun 2015 18:05:30 GMT"}, {"version": "v2", "created": "Sat, 8 Apr 2017 00:07:03 GMT"}], "update_date": "2017-04-11", "authors_parsed": [["Keshavarz", "Hossein", ""], ["Scott", "Clayton", ""], ["Nguyen", "XuanLong", ""]]}, {"id": "1506.01349", "submitter": "Peter Frazier", "authors": "Peter I. Frazier and Jialei Wang", "title": "Bayesian optimization for materials design", "comments": null, "journal-ref": null, "doi": "10.1007/978-3-319-23871-5_3", "report-no": null, "categories": "stat.ML math.OC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We introduce Bayesian optimization, a technique developed for optimizing\ntime-consuming engineering simulations and for fitting machine learning models\non large datasets. Bayesian optimization guides the choice of experiments\nduring materials design and discovery to find good material designs in as few\nexperiments as possible. We focus on the case when materials designs are\nparameterized by a low-dimensional vector. Bayesian optimization is built on a\nstatistical technique called Gaussian process regression, which allows\npredicting the performance of a new design based on previously tested designs.\nAfter providing a detailed introduction to Gaussian process regression, we\nintroduce two Bayesian optimization methods: expected improvement, for design\nproblems with noise-free evaluations; and the knowledge-gradient method, which\ngeneralizes expected improvement and may be used in design problems with noisy\nevaluations. Both methods are derived using a value-of-information analysis,\nand enjoy one-step Bayes-optimality.\n", "versions": [{"version": "v1", "created": "Wed, 3 Jun 2015 18:56:19 GMT"}], "update_date": "2017-11-22", "authors_parsed": [["Frazier", "Peter I.", ""], ["Wang", "Jialei", ""]]}, {"id": "1506.01351", "submitter": "Jeffrey Regier", "authors": "Jeffrey Regier, Andrew Miller, Jon McAuliffe, Ryan Adams, Matt\n  Hoffman, Dustin Lang, David Schlegel, Prabhat", "title": "Celeste: Variational inference for a generative model of astronomical\n  images", "comments": "in the Proceedings of the 32nd International Conference on Machine\n  Learning (2015)", "journal-ref": null, "doi": null, "report-no": null, "categories": "astro-ph.IM stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a new, fully generative model of optical telescope image sets,\nalong with a variational procedure for inference. Each pixel intensity is\ntreated as a Poisson random variable, with a rate parameter dependent on latent\nproperties of stars and galaxies. Key latent properties are themselves random,\nwith scientific prior distributions constructed from large ancillary data sets.\nWe check our approach on synthetic images. We also run it on images from a\nmajor sky survey, where it exceeds the performance of the current\nstate-of-the-art method for locating celestial bodies and measuring their\ncolors.\n", "versions": [{"version": "v1", "created": "Wed, 3 Jun 2015 19:03:28 GMT"}], "update_date": "2015-06-04", "authors_parsed": [["Regier", "Jeffrey", ""], ["Miller", "Andrew", ""], ["McAuliffe", "Jon", ""], ["Adams", "Ryan", ""], ["Hoffman", "Matt", ""], ["Lang", "Dustin", ""], ["Schlegel", "David", ""], ["Prabhat", "", ""]]}, {"id": "1506.01418", "submitter": "Umut \\c{S}im\\c{s}ekli", "authors": "Umut \\c{S}im\\c{s}ekli, Hazal Koptagel, Hakan G\\\"ulda\\c{s}, A. Taylan\n  Cemgil, Figen \\\"Oztoprak, \\c{S}. \\.Ilker Birbil", "title": "Parallel Stochastic Gradient Markov Chain Monte Carlo for Matrix\n  Factorisation Models", "comments": "10 pages, 6 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  For large matrix factorisation problems, we develop a distributed Markov\nChain Monte Carlo (MCMC) method based on stochastic gradient Langevin dynamics\n(SGLD) that we call Parallel SGLD (PSGLD). PSGLD has very favourable scaling\nproperties with increasing data size and is comparable in terms of\ncomputational requirements to optimisation methods based on stochastic gradient\ndescent. PSGLD achieves high performance by exploiting the conditional\nindependence structure of the MF models to sub-sample data in a systematic\nmanner as to allow parallelisation and distributed computation. We provide a\nconvergence proof of the algorithm and verify its superior performance on\nvarious architectures such as Graphics Processing Units, shared memory\nmulti-core systems and multi-computer clusters.\n", "versions": [{"version": "v1", "created": "Wed, 3 Jun 2015 22:13:35 GMT"}, {"version": "v2", "created": "Mon, 28 Sep 2015 20:14:34 GMT"}], "update_date": "2015-09-30", "authors_parsed": [["\u015eim\u015fekli", "Umut", ""], ["Koptagel", "Hazal", ""], ["G\u00fclda\u015f", "Hakan", ""], ["Cemgil", "A. Taylan", ""], ["\u00d6ztoprak", "Figen", ""], ["Birbil", "\u015e. \u0130lker", ""]]}, {"id": "1506.01490", "submitter": "Chun-Liang Li", "authors": "Chun-Liang Li, Hsuan-Tien Lin, Chi-Jen Lu", "title": "Rivalry of Two Families of Algorithms for Memory-Restricted Streaming\n  PCA", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study the problem of recovering the subspace spanned by the first $k$\nprincipal components of $d$-dimensional data under the streaming setting, with\na memory bound of $O(kd)$. Two families of algorithms are known for this\nproblem. The first family is based on the framework of stochastic gradient\ndescent. Nevertheless, the convergence rate of the family can be seriously\naffected by the learning rate of the descent steps and deserves more serious\nstudy. The second family is based on the power method over blocks of data, but\nsetting the block size for its existing algorithms is not an easy task. In this\npaper, we analyze the convergence rate of a representative algorithm with\ndecayed learning rate (Oja and Karhunen, 1985) in the first family for the\ngeneral $k>1$ case. Moreover, we propose a novel algorithm for the second\nfamily that sets the block sizes automatically and dynamically with faster\nconvergence rate. We then conduct empirical studies that fairly compare the two\nfamilies on real-world data. The studies reveal the advantages and\ndisadvantages of these two families.\n", "versions": [{"version": "v1", "created": "Thu, 4 Jun 2015 07:36:57 GMT"}, {"version": "v2", "created": "Mon, 12 Oct 2015 02:19:30 GMT"}], "update_date": "2015-10-13", "authors_parsed": [["Li", "Chun-Liang", ""], ["Lin", "Hsuan-Tien", ""], ["Lu", "Chi-Jen", ""]]}, {"id": "1506.01520", "submitter": "Brendan van Rooyen", "authors": "Brendan van Rooyen, Aditya Krishna Menon, Robert C. Williamson", "title": "An Average Classification Algorithm", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Many classification algorithms produce a classifier that is a weighted\naverage of kernel evaluations. When working with a high or infinite dimensional\nkernel, it is imperative for speed of evaluation and storage issues that as few\ntraining samples as possible are used in the kernel expansion. Popular existing\napproaches focus on altering standard learning algorithms, such as the Support\nVector Machine, to induce sparsity, as well as post-hoc procedures for sparse\napproximations. Here we adopt the latter approach. We begin with a very simple\nclassifier, given by the kernel mean $$ f(x) = \\frac{1}{n}\n\\sum\\limits_{i=i}^{n} y_i K(x_i,x) $$ We then find a sparse approximation to\nthis kernel mean via herding. The result is an accurate, easily parallelized\nalgorithm for learning classifiers.\n", "versions": [{"version": "v1", "created": "Thu, 4 Jun 2015 09:38:23 GMT"}, {"version": "v2", "created": "Fri, 5 Jun 2015 22:29:36 GMT"}, {"version": "v3", "created": "Tue, 15 Dec 2015 22:47:53 GMT"}], "update_date": "2015-12-17", "authors_parsed": [["van Rooyen", "Brendan", ""], ["Menon", "Aditya Krishna", ""], ["Williamson", "Robert C.", ""]]}, {"id": "1506.01567", "submitter": "Felix Abramovich", "authors": "Felix Abramovich and Marianna Pensky", "title": "Classification with many classes: challenges and pluses", "comments": "25 pages, 3 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.ME stat.ML stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The objective of the paper is to study accuracy of multi-class classification\nin high-dimensional setting, where the number of classes is also large (\"large\n$L$, large $p$, small $n$\" model). While this problem arises in many practical\napplications and many techniques have been recently developed for its solution,\nto the best of our knowledge nobody provided a rigorous theoretical analysis of\nthis important setup. The purpose of the present paper is to fill in this gap.\n  We consider one of the most common settings, classification of\nhigh-dimensional normal vectors where, unlike standard assumptions, the number\nof classes could be large. We derive non-asymptotic conditions on effects of\nsignificant features, and the low and the upper bounds for distances between\nclasses required for successful feature selection and classification with a\ngiven accuracy. Furthermore, we study an asymptotic setup where the number of\nclasses is diverging with the dimension of feature space and while the number\nof samples per class is possibly limited. We point out on an interesting and,\nat first glance, somewhat counter-intuitive phenomenon that a large number of\nclasses may be a \"blessing\" rather than a \"curse\" since, in certain settings,\nthe precision of classification can improve as the number of classes grows.\nThis is due to more accurate feature selection since even weaker significant\nfeatures, which are not sufficiently strong to be manifested in a coarse\nclassification, being shared across the classes, have a stronger impact as the\nnumber of classes increases. We supplement our theoretical investigation by a\nsimulation study and a real data example where we again observe the above\nphenomenon.\n", "versions": [{"version": "v1", "created": "Thu, 4 Jun 2015 12:57:19 GMT"}, {"version": "v2", "created": "Wed, 29 Mar 2017 07:32:26 GMT"}, {"version": "v3", "created": "Sun, 12 Nov 2017 10:48:37 GMT"}, {"version": "v4", "created": "Wed, 17 Jul 2019 12:25:41 GMT"}], "update_date": "2019-07-18", "authors_parsed": [["Abramovich", "Felix", ""], ["Pensky", "Marianna", ""]]}, {"id": "1506.01709", "submitter": "H\\'ector P. Mart\\'inez", "authors": "Vincent E. Farrugia, H\\'ector P. Mart\\'inez, Georgios N. Yannakakis", "title": "The Preference Learning Toolbox", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.IR cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Preference learning (PL) is a core area of machine learning that handles\ndatasets with ordinal relations. As the number of generated data of ordinal\nnature is increasing, the importance and role of the PL field becomes central\nwithin machine learning research and practice. This paper introduces an open\nsource, scalable, efficient and accessible preference learning toolbox that\nsupports the key phases of the data training process incorporating various\npopular data preprocessing, feature selection and preference learning methods.\n", "versions": [{"version": "v1", "created": "Thu, 4 Jun 2015 19:58:56 GMT"}], "update_date": "2015-06-05", "authors_parsed": [["Farrugia", "Vincent E.", ""], ["Mart\u00ednez", "H\u00e9ctor P.", ""], ["Yannakakis", "Georgios N.", ""]]}, {"id": "1506.01744", "submitter": "Kevin Chen", "authors": "Chicheng Zhang, Jimin Song, Kevin C Chen, Kamalika Chaudhuri", "title": "Spectral Learning of Large Structured HMMs for Comparative Epigenomics", "comments": "27 pages, 3 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG math.ST q-bio.GN stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We develop a latent variable model and an efficient spectral algorithm\nmotivated by the recent emergence of very large data sets of chromatin marks\nfrom multiple human cell types. A natural model for chromatin data in one cell\ntype is a Hidden Markov Model (HMM); we model the relationship between multiple\ncell types by connecting their hidden states by a fixed tree of known\nstructure. The main challenge with learning parameters of such models is that\niterative methods such as EM are very slow, while naive spectral methods result\nin time and space complexity exponential in the number of cell types. We\nexploit properties of the tree structure of the hidden states to provide\nspectral algorithms that are more computationally efficient for current\nbiological datasets. We provide sample complexity bounds for our algorithm and\nevaluate it experimentally on biological data from nine human cell types.\nFinally, we show that beyond our specific model, some of our algorithmic ideas\ncan be applied to other graphical models.\n", "versions": [{"version": "v1", "created": "Thu, 4 Jun 2015 22:57:28 GMT"}], "update_date": "2015-06-09", "authors_parsed": [["Zhang", "Chicheng", ""], ["Song", "Jimin", ""], ["Chen", "Kevin C", ""], ["Chaudhuri", "Kamalika", ""]]}, {"id": "1506.01782", "submitter": "Xiangyu Wang", "authors": "Xiangyu Wang and Chenlei Leng", "title": "High-dimensional Ordinary Least-squares Projection for Screening\n  Variables", "comments": "To appear in JRSS-B", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME math.ST stat.ML stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Variable selection is a challenging issue in statistical applications when\nthe number of predictors $p$ far exceeds the number of observations $n$. In\nthis ultra-high dimensional setting, the sure independence screening (SIS)\nprocedure was introduced to significantly reduce the dimensionality by\npreserving the true model with overwhelming probability, before a refined\nsecond stage analysis. However, the aforementioned sure screening property\nstrongly relies on the assumption that the important variables in the model\nhave large marginal correlations with the response, which rarely holds in\nreality. To overcome this, we propose a novel and simple screening technique\ncalled the high-dimensional ordinary least-squares projection (HOLP). We show\nthat HOLP possesses the sure screening property and gives consistent variable\nselection without the strong correlation assumption, and has a low\ncomputational complexity. A ridge type HOLP procedure is also discussed.\nSimulation study shows that HOLP performs competitively compared to many other\nmarginal correlation based methods. An application to a mammalian eye disease\ndata illustrates the attractiveness of HOLP.\n", "versions": [{"version": "v1", "created": "Fri, 5 Jun 2015 05:39:38 GMT"}], "update_date": "2015-06-08", "authors_parsed": [["Wang", "Xiangyu", ""], ["Leng", "Chenlei", ""]]}, {"id": "1506.01900", "submitter": "Yossi Arjevani", "authors": "Yossi Arjevani and Ohad Shamir", "title": "Communication Complexity of Distributed Convex Learning and Optimization", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG math.OC stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study the fundamental limits to communication-efficient distributed\nmethods for convex learning and optimization, under different assumptions on\nthe information available to individual machines, and the types of functions\nconsidered. We identify cases where existing algorithms are already worst-case\noptimal, as well as cases where room for further improvement is still possible.\nAmong other things, our results indicate that without similarity between the\nlocal objective functions (due to statistical data similarity or otherwise)\nmany communication rounds may be required, even if the machines have unbounded\ncomputational power.\n", "versions": [{"version": "v1", "created": "Fri, 5 Jun 2015 13:24:17 GMT"}, {"version": "v2", "created": "Wed, 28 Oct 2015 19:02:22 GMT"}], "update_date": "2015-10-29", "authors_parsed": [["Arjevani", "Yossi", ""], ["Shamir", "Ohad", ""]]}, {"id": "1506.01911", "submitter": "Lionel Pigou", "authors": "Lionel Pigou, A\\\"aron van den Oord, Sander Dieleman, Mieke Van\n  Herreweghe, Joni Dambre", "title": "Beyond Temporal Pooling: Recurrence and Temporal Convolutions for\n  Gesture Recognition in Video", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI cs.LG cs.NE stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recent studies have demonstrated the power of recurrent neural networks for\nmachine translation, image captioning and speech recognition. For the task of\ncapturing temporal structure in video, however, there still remain numerous\nopen research questions. Current research suggests using a simple temporal\nfeature pooling strategy to take into account the temporal aspect of video. We\ndemonstrate that this method is not sufficient for gesture recognition, where\ntemporal information is more discriminative compared to general video\nclassification tasks. We explore deep architectures for gesture recognition in\nvideo and propose a new end-to-end trainable neural network architecture\nincorporating temporal convolutions and bidirectional recurrence. Our main\ncontributions are twofold; first, we show that recurrence is crucial for this\ntask; second, we show that adding temporal convolutions leads to significant\nimprovements. We evaluate the different approaches on the Montalbano gesture\nrecognition dataset, where we achieve state-of-the-art results.\n", "versions": [{"version": "v1", "created": "Fri, 5 Jun 2015 13:43:01 GMT"}, {"version": "v2", "created": "Mon, 9 Nov 2015 16:20:26 GMT"}, {"version": "v3", "created": "Wed, 10 Feb 2016 16:50:29 GMT"}], "update_date": "2016-02-11", "authors_parsed": [["Pigou", "Lionel", ""], ["Oord", "A\u00e4ron van den", ""], ["Dieleman", "Sander", ""], ["Van Herreweghe", "Mieke", ""], ["Dambre", "Joni", ""]]}, {"id": "1506.01972", "submitter": "Zeyuan Allen-Zhu", "authors": "Zeyuan Allen-Zhu, Yang Yuan", "title": "Improved SVRG for Non-Strongly-Convex or Sum-of-Non-Convex Objectives", "comments": "improved writing and included more experiments in this version", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.DS math.OC stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Many classical algorithms are found until several years later to outlive the\nconfines in which they were conceived, and continue to be relevant in\nunforeseen settings. In this paper, we show that SVRG is one such method: being\noriginally designed for strongly convex objectives, it is also very robust in\nnon-strongly convex or sum-of-non-convex settings.\n  More precisely, we provide new analysis to improve the state-of-the-art\nrunning times in both settings by either applying SVRG or its novel variant.\nSince non-strongly convex objectives include important examples such as Lasso\nor logistic regression, and sum-of-non-convex objectives include famous\nexamples such as stochastic PCA and is even believed to be related to training\ndeep neural nets, our results also imply better performances in these\napplications.\n", "versions": [{"version": "v1", "created": "Fri, 5 Jun 2015 17:00:43 GMT"}, {"version": "v2", "created": "Fri, 5 Feb 2016 20:55:39 GMT"}, {"version": "v3", "created": "Fri, 27 May 2016 19:14:20 GMT"}], "update_date": "2016-05-30", "authors_parsed": [["Allen-Zhu", "Zeyuan", ""], ["Yuan", "Yang", ""]]}, {"id": "1506.02080", "submitter": "Ruben Martinez-Cantin", "authors": "Ruben Martinez-Cantin", "title": "Local Nonstationarity for Efficient Bayesian Optimization", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Bayesian optimization has shown to be a fundamental global optimization\nalgorithm in many applications: ranging from automatic machine learning,\nrobotics, reinforcement learning, experimental design, simulations, etc. The\nmost popular and effective Bayesian optimization relies on a surrogate model in\nthe form of a Gaussian process due to its flexibility to represent a prior over\nfunction. However, many algorithms and setups relies on the stationarity\nassumption of the Gaussian process. In this paper, we present a novel\nnonstationary strategy for Bayesian optimization that is able to outperform the\nstate of the art in Bayesian optimization both in stationary and nonstationary\nproblems.\n", "versions": [{"version": "v1", "created": "Fri, 5 Jun 2015 22:36:15 GMT"}], "update_date": "2015-06-09", "authors_parsed": [["Martinez-Cantin", "Ruben", ""]]}, {"id": "1506.02085", "submitter": "Min Xu", "authors": "Min Xu, Rudy Setiono", "title": "Gene selection for cancer classification using a hybrid of univariate\n  and multivariate feature selection methods", "comments": null, "journal-ref": "Applied Genomics and Proteomics. 2003:2(2)79-91", "doi": null, "report-no": null, "categories": "q-bio.QM cs.CE cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Various approaches to gene selection for cancer classification based on\nmicroarray data can be found in the literature and they may be grouped into two\ncategories: univariate methods and multivariate methods. Univariate methods\nlook at each gene in the data in isolation from others. They measure the\ncontribution of a particular gene to the classification without considering the\npresence of the other genes. In contrast, multivariate methods measure the\nrelative contribution of a gene to the classification by taking the other genes\nin the data into consideration. Multivariate methods select fewer genes in\ngeneral. However, the selection process of multivariate methods may be\nsensitive to the presence of irrelevant genes, noises in the expression and\noutliers in the training data. At the same time, the computational cost of\nmultivariate methods is high. To overcome the disadvantages of the two types of\napproaches, we propose a hybrid method to obtain gene sets that are small and\nhighly discriminative.\n  We devise our hybrid method from the univariate Maximum Likelihood method\n(LIK) and the multivariate Recursive Feature Elimination method (RFE). We\nanalyze the properties of these methods and systematically test the\neffectiveness of our proposed method on two cancer microarray datasets. Our\nexperiments on a leukemia dataset and a small, round blue cell tumors dataset\ndemonstrate the effectiveness of our hybrid method. It is able to discover sets\nconsisting of fewer genes than those reported in the literature and at the same\ntime achieve the same or better prediction accuracy.\n", "versions": [{"version": "v1", "created": "Fri, 5 Jun 2015 23:29:06 GMT"}], "update_date": "2015-06-18", "authors_parsed": [["Xu", "Min", ""], ["Setiono", "Rudy", ""]]}, {"id": "1506.02087", "submitter": "Min Xu", "authors": "Min Xu", "title": "Global Gene Expression Analysis Using Machine Learning Methods", "comments": "Author's master thesis (National University of Singapore, May 2003).\n  Adviser: Rudy Setiono", "journal-ref": null, "doi": null, "report-no": null, "categories": "q-bio.QM cs.CE cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Microarray is a technology to quantitatively monitor the expression of large\nnumber of genes in parallel. It has become one of the main tools for global\ngene expression analysis in molecular biology research in recent years. The\nlarge amount of expression data generated by this technology makes the study of\ncertain complex biological problems possible and machine learning methods are\nplaying a crucial role in the analysis process. At present, many machine\nlearning methods have been or have the potential to be applied to major areas\nof gene expression analysis. These areas include clustering, classification,\ndynamic modeling and reverse engineering.\n  In this thesis, we focus our work on using machine learning methods to solve\nthe classification problems arising from microarray data. We first identify the\nmajor types of the classification problems; then apply several machine learning\nmethods to solve the problems and perform systematic tests on real and\nartificial datasets. We propose improvement to existing methods. Specifically,\nwe develop a multivariate and a hybrid feature selection method to obtain high\nclassification performance for high dimension classification problems. Using\nthe hybrid feature selection method, we are able to identify small sets of\nfeatures that give predictive accuracy that is as good as that from other\nmethods which require many more features.\n", "versions": [{"version": "v1", "created": "Fri, 5 Jun 2015 23:37:20 GMT"}], "update_date": "2015-06-18", "authors_parsed": [["Xu", "Min", ""]]}, {"id": "1506.02107", "submitter": "Jie Ding", "authors": "Jie Ding, Mohammad Noshad, and Vahid Tarokh", "title": "Data-Driven Learning of the Number of States in Multi-State\n  Autoregressive Models", "comments": "This paper will appear in the Proceedings of 53rd Annual Allerton\n  Conference on Communication, Control, and Computing, 2015", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this work, we consider the class of multi-state autoregressive processes\nthat can be used to model non-stationary time-series of interest. In order to\ncapture different autoregressive (AR) states underlying an observed time\nseries, it is crucial to select the appropriate number of states. We propose a\nnew model selection technique based on the Gap statistics, which uses a null\nreference distribution on the stable AR filters to check whether adding a new\nAR state significantly improves the performance of the model. To that end, we\ndefine a new distance measure between AR filters based on mean squared\nprediction error (MSPE), and propose an efficient method to generate random\nstable filters that are uniformly distributed in the coefficient space.\nNumerical results are provided to evaluate the performance of the proposed\napproach.\n", "versions": [{"version": "v1", "created": "Sat, 6 Jun 2015 02:47:24 GMT"}, {"version": "v2", "created": "Wed, 16 Sep 2015 15:54:13 GMT"}, {"version": "v3", "created": "Sat, 10 Oct 2015 00:25:31 GMT"}], "update_date": "2015-10-13", "authors_parsed": [["Ding", "Jie", ""], ["Noshad", "Mohammad", ""], ["Tarokh", "Vahid", ""]]}, {"id": "1506.02108", "submitter": "Chunhua Shen", "authors": "Guosheng Lin, Chunhua Shen, Ian Reid, Anton van den Hengel", "title": "Deeply Learning the Messages in Message Passing Inference", "comments": "11 pages. Appearing in Proc. The Twenty-ninth Annual Conference on\n  Neural Information Processing Systems (NIPS), 2015, Montreal, Canada", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Deep structured output learning shows great promise in tasks like semantic\nimage segmentation. We proffer a new, efficient deep structured model learning\nscheme, in which we show how deep Convolutional Neural Networks (CNNs) can be\nused to estimate the messages in message passing inference for structured\nprediction with Conditional Random Fields (CRFs). With such CNN message\nestimators, we obviate the need to learn or evaluate potential functions for\nmessage calculation. This confers significant efficiency for learning, since\notherwise when performing structured learning for a CRF with CNN potentials it\nis necessary to undertake expensive inference for every stochastic gradient\niteration. The network output dimension for message estimation is the same as\nthe number of classes, in contrast to the network output for general CNN\npotential functions in CRFs, which is exponential in the order of the\npotentials. Hence CNN message learning has fewer network parameters and is more\nscalable for cases that a large number of classes are involved. We apply our\nmethod to semantic image segmentation on the PASCAL VOC 2012 dataset. We\nachieve an intersection-over-union score of 73.4 on its test set, which is the\nbest reported result for methods using the VOC training images alone. This\nimpressive performance demonstrates the effectiveness and usefulness of our CNN\nmessage learning method.\n", "versions": [{"version": "v1", "created": "Sat, 6 Jun 2015 02:52:38 GMT"}, {"version": "v2", "created": "Wed, 10 Jun 2015 06:49:06 GMT"}, {"version": "v3", "created": "Tue, 8 Sep 2015 04:29:45 GMT"}], "update_date": "2015-09-09", "authors_parsed": [["Lin", "Guosheng", ""], ["Shen", "Chunhua", ""], ["Reid", "Ian", ""], ["Hengel", "Anton van den", ""]]}, {"id": "1506.02142", "submitter": "Yarin Gal", "authors": "Yarin Gal, Zoubin Ghahramani", "title": "Dropout as a Bayesian Approximation: Representing Model Uncertainty in\n  Deep Learning", "comments": "12 pages, 6 figures; fixed a mistake with standard error and added a\n  new table with updated results (marked \"Update [October 2016]\"); Published in\n  ICML 2016", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Deep learning tools have gained tremendous attention in applied machine\nlearning. However such tools for regression and classification do not capture\nmodel uncertainty. In comparison, Bayesian models offer a mathematically\ngrounded framework to reason about model uncertainty, but usually come with a\nprohibitive computational cost. In this paper we develop a new theoretical\nframework casting dropout training in deep neural networks (NNs) as approximate\nBayesian inference in deep Gaussian processes. A direct result of this theory\ngives us tools to model uncertainty with dropout NNs -- extracting information\nfrom existing models that has been thrown away so far. This mitigates the\nproblem of representing uncertainty in deep learning without sacrificing either\ncomputational complexity or test accuracy. We perform an extensive study of the\nproperties of dropout's uncertainty. Various network architectures and\nnon-linearities are assessed on tasks of regression and classification, using\nMNIST as an example. We show a considerable improvement in predictive\nlog-likelihood and RMSE compared to existing state-of-the-art methods, and\nfinish by using dropout's uncertainty in deep reinforcement learning.\n", "versions": [{"version": "v1", "created": "Sat, 6 Jun 2015 12:30:43 GMT"}, {"version": "v2", "created": "Thu, 27 Aug 2015 13:39:15 GMT"}, {"version": "v3", "created": "Sun, 27 Sep 2015 15:15:31 GMT"}, {"version": "v4", "created": "Sat, 31 Oct 2015 19:45:05 GMT"}, {"version": "v5", "created": "Wed, 25 May 2016 18:48:52 GMT"}, {"version": "v6", "created": "Tue, 4 Oct 2016 16:50:26 GMT"}], "update_date": "2016-10-05", "authors_parsed": [["Gal", "Yarin", ""], ["Ghahramani", "Zoubin", ""]]}, {"id": "1506.02155", "submitter": "Zoltan Szabo", "authors": "Bharath K. Sriperumbudur and Zoltan Szabo", "title": "Optimal Rates for Random Fourier Features", "comments": "To appear at NIPS-2015", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST cs.LG math.FA stat.ML stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Kernel methods represent one of the most powerful tools in machine learning\nto tackle problems expressed in terms of function values and derivatives due to\ntheir capability to represent and model complex relations. While these methods\nshow good versatility, they are computationally intensive and have poor\nscalability to large data as they require operations on Gram matrices. In order\nto mitigate this serious computational limitation, recently randomized\nconstructions have been proposed in the literature, which allow the application\nof fast linear algorithms. Random Fourier features (RFF) are among the most\npopular and widely applied constructions: they provide an easily computable,\nlow-dimensional feature representation for shift-invariant kernels. Despite the\npopularity of RFFs, very little is understood theoretically about their\napproximation quality. In this paper, we provide a detailed finite-sample\ntheoretical analysis about the approximation quality of RFFs by (i)\nestablishing optimal (in terms of the RFF dimension, and growing set size)\nperformance guarantees in uniform norm, and (ii) presenting guarantees in $L^r$\n($1\\le r<\\infty$) norms. We also propose an RFF approximation to derivatives of\na kernel with a theoretical study on its approximation quality.\n", "versions": [{"version": "v1", "created": "Sat, 6 Jun 2015 14:37:01 GMT"}, {"version": "v2", "created": "Wed, 4 Nov 2015 22:58:57 GMT"}], "update_date": "2015-11-06", "authors_parsed": [["Sriperumbudur", "Bharath K.", ""], ["Szabo", "Zoltan", ""]]}, {"id": "1506.02157", "submitter": "Yarin Gal", "authors": "Yarin Gal, Zoubin Ghahramani", "title": "Dropout as a Bayesian Approximation: Appendix", "comments": "20 pages, 1 figure; ICML proceedings version", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We show that a neural network with arbitrary depth and non-linearities, with\ndropout applied before every weight layer, is mathematically equivalent to an\napproximation to a well known Bayesian model. This interpretation might offer\nan explanation to some of dropout's key properties, such as its robustness to\nover-fitting. Our interpretation allows us to reason about uncertainty in deep\nlearning, and allows the introduction of the Bayesian machinery into existing\ndeep learning frameworks in a principled way.\n  This document is an appendix for the main paper \"Dropout as a Bayesian\nApproximation: Representing Model Uncertainty in Deep Learning\" by Gal and\nGhahramani, 2015.\n", "versions": [{"version": "v1", "created": "Sat, 6 Jun 2015 14:42:06 GMT"}, {"version": "v2", "created": "Thu, 27 Aug 2015 13:43:41 GMT"}, {"version": "v3", "created": "Sun, 27 Sep 2015 16:07:19 GMT"}, {"version": "v4", "created": "Sat, 31 Oct 2015 20:51:22 GMT"}, {"version": "v5", "created": "Wed, 25 May 2016 18:46:56 GMT"}], "update_date": "2016-05-26", "authors_parsed": [["Gal", "Yarin", ""], ["Ghahramani", "Zoubin", ""]]}, {"id": "1506.02158", "submitter": "Yarin Gal", "authors": "Yarin Gal, Zoubin Ghahramani", "title": "Bayesian Convolutional Neural Networks with Bernoulli Approximate\n  Variational Inference", "comments": "12 pages, 3 figures, ICLR format, updated with reviewer comments", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Convolutional neural networks (CNNs) work well on large datasets. But\nlabelled data is hard to collect, and in some applications larger amounts of\ndata are not available. The problem then is how to use CNNs with small data --\nas CNNs overfit quickly. We present an efficient Bayesian CNN, offering better\nrobustness to over-fitting on small data than traditional approaches. This is\nby placing a probability distribution over the CNN's kernels. We approximate\nour model's intractable posterior with Bernoulli variational distributions,\nrequiring no additional model parameters.\n  On the theoretical side, we cast dropout network training as approximate\ninference in Bayesian neural networks. This allows us to implement our model\nusing existing tools in deep learning with no increase in time complexity,\nwhile highlighting a negative result in the field. We show a considerable\nimprovement in classification accuracy compared to standard techniques and\nimprove on published state-of-the-art results for CIFAR-10.\n", "versions": [{"version": "v1", "created": "Sat, 6 Jun 2015 14:43:40 GMT"}, {"version": "v2", "created": "Thu, 27 Aug 2015 13:30:17 GMT"}, {"version": "v3", "created": "Sun, 27 Sep 2015 13:34:58 GMT"}, {"version": "v4", "created": "Mon, 2 Nov 2015 14:33:59 GMT"}, {"version": "v5", "created": "Mon, 30 Nov 2015 21:22:15 GMT"}, {"version": "v6", "created": "Mon, 18 Jan 2016 20:42:07 GMT"}], "update_date": "2016-01-19", "authors_parsed": [["Gal", "Yarin", ""], ["Ghahramani", "Zoubin", ""]]}, {"id": "1506.02169", "submitter": "Kyle S. Cranmer", "authors": "Kyle Cranmer, Juan Pavez, Gilles Louppe", "title": "Approximating Likelihood Ratios with Calibrated Discriminative\n  Classifiers", "comments": "35 pages, 5 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP physics.data-an stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In many fields of science, generalized likelihood ratio tests are established\ntools for statistical inference. At the same time, it has become increasingly\ncommon that a simulator (or generative model) is used to describe complex\nprocesses that tie parameters $\\theta$ of an underlying theory and measurement\napparatus to high-dimensional observations $\\mathbf{x}\\in \\mathbb{R}^p$.\nHowever, simulator often do not provide a way to evaluate the likelihood\nfunction for a given observation $\\mathbf{x}$, which motivates a new class of\nlikelihood-free inference algorithms. In this paper, we show that likelihood\nratios are invariant under a specific class of dimensionality reduction maps\n$\\mathbb{R}^p \\mapsto \\mathbb{R}$. As a direct consequence, we show that\ndiscriminative classifiers can be used to approximate the generalized\nlikelihood ratio statistic when only a generative model for the data is\navailable. This leads to a new machine learning-based approach to\nlikelihood-free inference that is complementary to Approximate Bayesian\nComputation, and which does not require a prior on the model parameters.\nExperimental results on artificial problems with known exact likelihoods\nillustrate the potential of the proposed method.\n", "versions": [{"version": "v1", "created": "Sat, 6 Jun 2015 16:59:23 GMT"}, {"version": "v2", "created": "Fri, 18 Mar 2016 16:47:11 GMT"}], "update_date": "2016-03-21", "authors_parsed": [["Cranmer", "Kyle", ""], ["Pavez", "Juan", ""], ["Louppe", "Gilles", ""]]}, {"id": "1506.02194", "submitter": "Patrick Rebeschini", "authors": "Patrick Rebeschini and Amin Karbasi", "title": "Fast Mixing for Discrete Point Processes", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We investigate the systematic mechanism for designing fast mixing Markov\nchain Monte Carlo algorithms to sample from discrete point processes under the\nDobrushin uniqueness condition for Gibbs measures. Discrete point processes are\ndefined as probability distributions $\\mu(S)\\propto \\exp(\\beta f(S))$ over all\nsubsets $S\\in 2^V$ of a finite set $V$ through a bounded set function\n$f:2^V\\rightarrow \\mathbb{R}$ and a parameter $\\beta>0$. A subclass of discrete\npoint processes characterized by submodular functions (which include\nlog-submodular distributions, submodular point processes, and determinantal\npoint processes) has recently gained a lot of interest in machine learning and\nshown to be effective for modeling diversity and coverage. We show that if the\nset function (not necessarily submodular) displays a natural notion of decay of\ncorrelation, then, for $\\beta$ small enough, it is possible to design fast\nmixing Markov chain Monte Carlo methods that yield error bounds on marginal\napproximations that do not depend on the size of the set $V$. The sufficient\nconditions that we derive involve a control on the (discrete) Hessian of set\nfunctions, a quantity that has not been previously considered in the\nliterature. We specialize our results for submodular functions, and we discuss\ncanonical examples where the Hessian can be easily controlled.\n", "versions": [{"version": "v1", "created": "Sat, 6 Jun 2015 21:19:21 GMT"}], "update_date": "2015-06-09", "authors_parsed": [["Rebeschini", "Patrick", ""], ["Karbasi", "Amin", ""]]}, {"id": "1506.02196", "submitter": "Patrick L. Combettes", "authors": "Michel Barlaud, Wafa Belhajali, Patrick L. Combettes, and Lionel\n  Fillatre", "title": "Classification and regression using an outer approximation\n  projection-gradient method", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.ML stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper deals with sparse feature selection and grouping for\nclassification and regression. The classification or regression problems under\nconsideration consists in minimizing a convex empirical risk function subject\nto an $\\ell^1$ constraint, a pairwise $\\ell^\\infty$ constraint, or a pairwise\n$\\ell^1$ constraint. Existing work, such as the Lasso formulation, has focused\nmainly on Lagrangian penalty approximations, which often require ad hoc or\ncomputationally expensive procedures to determine the penalization parameter.\nWe depart from this approach and address the constrained problem directly via a\nsplitting method. The structure of the method is that of the classical\ngradient-projection algorithm, which alternates a gradient step on the\nobjective and a projection step onto the lower level set modeling the\nconstraint. The novelty of our approach is that the projection step is\nimplemented via an outer approximation scheme in which the constraint set is\napproximated by a sequence of simple convex sets consisting of the intersection\nof two half-spaces. Convergence of the iterates generated by the algorithm is\nestablished for a general smooth convex minimization problem with inequality\nconstraints. Experiments on both synthetic and biological data show that our\nmethod outperforms penalty methods.\n", "versions": [{"version": "v1", "created": "Sat, 6 Jun 2015 21:47:02 GMT"}, {"version": "v2", "created": "Sat, 16 Jan 2016 03:33:13 GMT"}, {"version": "v3", "created": "Wed, 21 Sep 2016 01:05:21 GMT"}, {"version": "v4", "created": "Thu, 23 Mar 2017 19:08:37 GMT"}], "update_date": "2017-03-27", "authors_parsed": [["Barlaud", "Michel", ""], ["Belhajali", "Wafa", ""], ["Combettes", "Patrick L.", ""], ["Fillatre", "Lionel", ""]]}, {"id": "1506.02222", "submitter": "Xiangyu Wang", "authors": "Xiangyu Wang, David Dunson and Chenlei Leng", "title": "No penalty no tears: Least squares in high-dimensional linear models", "comments": "Added results for non-sparse models; Added results for elliptical\n  distribution; Added simulations for adaptive lasso", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME cs.LG math.ST stat.ML stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Ordinary least squares (OLS) is the default method for fitting linear models,\nbut is not applicable for problems with dimensionality larger than the sample\nsize. For these problems, we advocate the use of a generalized version of OLS\nmotivated by ridge regression, and propose two novel three-step algorithms\ninvolving least squares fitting and hard thresholding. The algorithms are\nmethodologically simple to understand intuitively, computationally easy to\nimplement efficiently, and theoretically appealing for choosing models\nconsistently. Numerical exercises comparing our methods with penalization-based\napproaches in simulations and data analyses illustrate the great potential of\nthe proposed algorithms.\n", "versions": [{"version": "v1", "created": "Sun, 7 Jun 2015 05:45:24 GMT"}, {"version": "v2", "created": "Wed, 10 Jun 2015 03:31:06 GMT"}, {"version": "v3", "created": "Fri, 9 Oct 2015 21:30:39 GMT"}, {"version": "v4", "created": "Mon, 23 Nov 2015 09:21:37 GMT"}, {"version": "v5", "created": "Thu, 16 Jun 2016 07:13:40 GMT"}], "update_date": "2016-06-17", "authors_parsed": [["Wang", "Xiangyu", ""], ["Dunson", "David", ""], ["Leng", "Chenlei", ""]]}, {"id": "1506.02227", "submitter": "Peter Richtarik", "authors": "Dominik Csiba and Peter Richt\\'arik", "title": "Primal Method for ERM with Flexible Mini-batching Schemes and Non-convex\n  Losses", "comments": "13 pages, 3 figures, 2 algorithms", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.OC cs.DS cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this work we develop a new algorithm for regularized empirical risk\nminimization. Our method extends recent techniques of Shalev-Shwartz [02/2015],\nwhich enable a dual-free analysis of SDCA, to arbitrary mini-batching schemes.\nMoreover, our method is able to better utilize the information in the data\ndefining the ERM problem. For convex loss functions, our complexity results\nmatch those of QUARTZ, which is a primal-dual method also allowing for\narbitrary mini-batching schemes. The advantage of a dual-free analysis comes\nfrom the fact that it guarantees convergence even for non-convex loss\nfunctions, as long as the average loss is convex. We illustrate through\nexperiments the utility of being able to design arbitrary mini-batching\nschemes.\n", "versions": [{"version": "v1", "created": "Sun, 7 Jun 2015 06:45:55 GMT"}], "update_date": "2015-06-09", "authors_parsed": [["Csiba", "Dominik", ""], ["Richt\u00e1rik", "Peter", ""]]}, {"id": "1506.02236", "submitter": "Yves-Laurent Kom Samo", "authors": "Yves-Laurent Kom Samo and Stephen Roberts", "title": "Generalized Spectral Kernels", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  In this paper we propose a family of tractable kernels that is dense in the\nfamily of bounded positive semi-definite functions (i.e. can approximate any\nbounded kernel with arbitrary precision). We start by discussing the case of\nstationary kernels, and propose a family of spectral kernels that extends\nexisting approaches such as spectral mixture kernels and sparse spectrum\nkernels. Our extension has two primary advantages. Firstly, unlike existing\nspectral approaches that yield infinite differentiability, the kernels we\nintroduce allow learning the degree of differentiability of the latent function\nin Gaussian process (GP) models and functions in the reproducing kernel Hilbert\nspace (RKHS) in other kernel methods. Secondly, we show that some of the\nkernels we propose require fewer parameters than existing spectral kernels for\nthe same accuracy, thereby leading to faster and more robust inference.\nFinally, we generalize our approach and propose a flexible and tractable family\nof spectral kernels that we prove can approximate any continuous bounded\nnonstationary kernel.\n", "versions": [{"version": "v1", "created": "Sun, 7 Jun 2015 08:41:58 GMT"}, {"version": "v2", "created": "Fri, 9 Oct 2015 21:17:06 GMT"}], "update_date": "2015-10-13", "authors_parsed": [["Samo", "Yves-Laurent Kom", ""], ["Roberts", "Stephen", ""]]}, {"id": "1506.02239", "submitter": "Yves-Laurent Kom Samo", "authors": "Yves-Laurent Kom Samo and Stephen Roberts", "title": "String Gaussian Process Kernels", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML", "license": "http://creativecommons.org/licenses/by-nc-sa/3.0/", "abstract": "  We introduce a new class of nonstationary kernels, which we derive as\ncovariance functions of a novel family of stochastic processes we refer to as\nstring Gaussian processes (string GPs). We construct string GPs to allow for\nmultiple types of local patterns in the data, while ensuring a mild global\nregularity condition. In this paper, we illustrate the efficacy of the approach\nusing synthetic data and demonstrate that the model outperforms competing\napproaches on well studied, real-life datasets that exhibit nonstationary\nfeatures.\n", "versions": [{"version": "v1", "created": "Sun, 7 Jun 2015 09:04:57 GMT"}], "update_date": "2015-06-09", "authors_parsed": [["Samo", "Yves-Laurent Kom", ""], ["Roberts", "Stephen", ""]]}, {"id": "1506.02256", "submitter": "Zhiyuan Tang", "authors": "Zhiyuan Tang, Dong Wang, Yiqiao Pan, Zhiyong Zhang", "title": "Knowledge Transfer Pre-training", "comments": "arXiv admin note: text overlap with arXiv:1505.04630", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.NE stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Pre-training is crucial for learning deep neural networks. Most of existing\npre-training methods train simple models (e.g., restricted Boltzmann machines)\nand then stack them layer by layer to form the deep structure. This layer-wise\npre-training has found strong theoretical foundation and broad empirical\nsupport. However, it is not easy to employ such method to pre-train models\nwithout a clear multi-layer structure,e.g., recurrent neural networks (RNNs).\nThis paper presents a new pre-training approach based on knowledge transfer\nlearning. In contrast to the layer-wise approach which trains model components\nincrementally, the new approach trains the entire model as a whole but with an\neasier objective function. This is achieved by utilizing soft targets produced\nby a prior trained model (teacher model). Compared to the conventional\nlayer-wise methods, this new method does not care about the model structure, so\ncan be used to pre-train very complex models. Experiments on a speech\nrecognition task demonstrated that with this approach, complex RNNs can be well\ntrained with a weaker deep neural network (DNN) model. Furthermore, the new\nmethod can be combined with conventional layer-wise pre-training to deliver\nadditional gains.\n", "versions": [{"version": "v1", "created": "Sun, 7 Jun 2015 11:55:33 GMT"}], "update_date": "2015-06-09", "authors_parsed": [["Tang", "Zhiyuan", ""], ["Wang", "Dong", ""], ["Pan", "Yiqiao", ""], ["Zhang", "Zhiyong", ""]]}, {"id": "1506.02267", "submitter": "Andreas Svensson", "authors": "Andreas Svensson, Arno Solin, Simo S\\\"arkk\\\"a, Thomas B. Sch\\\"on", "title": "Computationally Efficient Bayesian Learning of Gaussian Process State\n  Space Models", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.CO stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Gaussian processes allow for flexible specification of prior assumptions of\nunknown dynamics in state space models. We present a procedure for efficient\nBayesian learning in Gaussian process state space models, where the\nrepresentation is formed by projecting the problem onto a set of approximate\neigenfunctions derived from the prior covariance structure. Learning under this\nfamily of models can be conducted using a carefully crafted particle MCMC\nalgorithm. This scheme is computationally efficient and yet allows for a fully\nBayesian treatment of the problem. Compared to conventional system\nidentification tools or existing learning methods, we show competitive\nperformance and reliable quantification of uncertainties in the model.\n", "versions": [{"version": "v1", "created": "Sun, 7 Jun 2015 13:57:51 GMT"}, {"version": "v2", "created": "Fri, 15 Apr 2016 07:39:12 GMT"}], "update_date": "2016-04-18", "authors_parsed": [["Svensson", "Andreas", ""], ["Solin", "Arno", ""], ["S\u00e4rkk\u00e4", "Simo", ""], ["Sch\u00f6n", "Thomas B.", ""]]}, {"id": "1506.02278", "submitter": "Yen-Chi Chen", "authors": "Yen-Chi Chen, Christopher R. Genovese, Shirley Ho, Larry Wasserman", "title": "Optimal Ridge Detection using Coverage Risk", "comments": "16 pages, 4 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We introduce the concept of coverage risk as an error measure for density\nridge estimation. The coverage risk generalizes the mean integrated square\nerror to set estimation. We propose two risk estimators for the coverage risk\nand we show that we can select tuning parameters by minimizing the estimated\nrisk. We study the rate of convergence for coverage risk and prove consistency\nof the risk estimators. We apply our method to three simulated datasets and to\ncosmology data. In all the examples, the proposed method successfully recover\nthe underlying density structure.\n", "versions": [{"version": "v1", "created": "Sun, 7 Jun 2015 15:52:36 GMT"}], "update_date": "2015-06-09", "authors_parsed": [["Chen", "Yen-Chi", ""], ["Genovese", "Christopher R.", ""], ["Ho", "Shirley", ""], ["Wasserman", "Larry", ""]]}, {"id": "1506.02344", "submitter": "Anastasios Kyrillidis", "authors": "Megasthenis Asteris, Anastasios Kyrillidis, Alexandros G. Dimakis,\n  Han-Gyol Yi and, Bharath Chandrasekaran", "title": "Stay on path: PCA along graph paths", "comments": "12 pages, 5 figures, In Proceedings of International Conference on\n  Machine Learning (ICML) 2015", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.IT cs.LG math.IT math.OC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We introduce a variant of (sparse) PCA in which the set of feasible support\nsets is determined by a graph. In particular, we consider the following\nsetting: given a directed acyclic graph $G$ on $p$ vertices corresponding to\nvariables, the non-zero entries of the extracted principal component must\ncoincide with vertices lying along a path in $G$.\n  From a statistical perspective, information on the underlying network may\npotentially reduce the number of observations required to recover the\npopulation principal component. We consider the canonical estimator which\noptimally exploits the prior knowledge by solving a non-convex quadratic\nmaximization on the empirical covariance. We introduce a simple network and\nanalyze the estimator under the spiked covariance model. We show that side\ninformation potentially improves the statistical complexity.\n  We propose two algorithms to approximate the solution of the constrained\nquadratic maximization, and recover a component with the desired properties. We\nempirically evaluate our schemes on synthetic and real datasets.\n", "versions": [{"version": "v1", "created": "Mon, 8 Jun 2015 03:37:36 GMT"}, {"version": "v2", "created": "Fri, 19 Jun 2015 02:27:49 GMT"}], "update_date": "2015-06-22", "authors_parsed": [["Asteris", "Megasthenis", ""], ["Kyrillidis", "Anastasios", ""], ["Dimakis", "Alexandros G.", ""], ["and", "Han-Gyol Yi", ""], ["Chandrasekaran", "Bharath", ""]]}, {"id": "1506.02348", "submitter": "Kamalika Chaudhuri", "authors": "Kamalika Chaudhuri and Sham Kakade and Praneeth Netrapalli and Sujay\n  Sanghavi", "title": "Convergence Rates of Active Learning for Maximum Likelihood Estimation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  An active learner is given a class of models, a large set of unlabeled\nexamples, and the ability to interactively query labels of a subset of these\nexamples; the goal of the learner is to learn a model in the class that fits\nthe data well.\n  Previous theoretical work has rigorously characterized label complexity of\nactive learning, but most of this work has focused on the PAC or the agnostic\nPAC model. In this paper, we shift our attention to a more general setting --\nmaximum likelihood estimation. Provided certain conditions hold on the model\nclass, we provide a two-stage active learning algorithm for this problem. The\nconditions we require are fairly general, and cover the widely popular class of\nGeneralized Linear Models, which in turn, include models for binary and\nmulti-class classification, regression, and conditional random fields.\n  We provide an upper bound on the label requirement of our algorithm, and a\nlower bound that matches it up to lower order terms. Our analysis shows that\nunlike binary classification in the realizable case, just a single extra round\nof interaction is sufficient to achieve near-optimal performance in maximum\nlikelihood estimation. On the empirical side, the recent work in\n~\\cite{Zhang12} and~\\cite{Zhang14} (on active linear and logistic regression)\nshows the promise of this approach.\n", "versions": [{"version": "v1", "created": "Mon, 8 Jun 2015 04:05:43 GMT"}], "update_date": "2015-06-09", "authors_parsed": [["Chaudhuri", "Kamalika", ""], ["Kakade", "Sham", ""], ["Netrapalli", "Praneeth", ""], ["Sanghavi", "Sujay", ""]]}, {"id": "1506.02351", "submitter": "Junbo Zhao", "authors": "Junbo Zhao, Michael Mathieu, Ross Goroshin, Yann LeCun", "title": "Stacked What-Where Auto-encoders", "comments": "Workshop track - ICLR 2016", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a novel architecture, the \"stacked what-where auto-encoders\"\n(SWWAE), which integrates discriminative and generative pathways and provides a\nunified approach to supervised, semi-supervised and unsupervised learning\nwithout relying on sampling during training. An instantiation of SWWAE uses a\nconvolutional net (Convnet) (LeCun et al. (1998)) to encode the input, and\nemploys a deconvolutional net (Deconvnet) (Zeiler et al. (2010)) to produce the\nreconstruction. The objective function includes reconstruction terms that\ninduce the hidden states in the Deconvnet to be similar to those of the\nConvnet. Each pooling layer produces two sets of variables: the \"what\" which\nare fed to the next layer, and its complementary variable \"where\" that are fed\nto the corresponding layer in the generative decoder.\n", "versions": [{"version": "v1", "created": "Mon, 8 Jun 2015 04:45:33 GMT"}, {"version": "v2", "created": "Fri, 12 Jun 2015 02:38:59 GMT"}, {"version": "v3", "created": "Tue, 23 Jun 2015 02:59:39 GMT"}, {"version": "v4", "created": "Sat, 4 Jul 2015 23:36:39 GMT"}, {"version": "v5", "created": "Wed, 11 Nov 2015 04:06:00 GMT"}, {"version": "v6", "created": "Sun, 15 Nov 2015 00:23:14 GMT"}, {"version": "v7", "created": "Tue, 17 Nov 2015 20:36:18 GMT"}, {"version": "v8", "created": "Sun, 14 Feb 2016 21:09:22 GMT"}], "update_date": "2016-02-16", "authors_parsed": [["Zhao", "Junbo", ""], ["Mathieu", "Michael", ""], ["Goroshin", "Ross", ""], ["LeCun", "Yann", ""]]}, {"id": "1506.02371", "submitter": "Viktoriya Krakovna", "authors": "Viktoriya Krakovna, Jiong Du, Jun S. Liu", "title": "Interpretable Selection and Visualization of Features and Interactions\n  Using Bayesian Forests", "comments": "R package: github.com/vkrakovna/sbfc", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  It is becoming increasingly important for machine learning methods to make\npredictions that are interpretable as well as accurate. In many practical\napplications, it is of interest which features and feature interactions are\nrelevant to the prediction task. We present a novel method, Selective Bayesian\nForest Classifier, that strikes a balance between predictive power and\ninterpretability by simultaneously performing classification, feature\nselection, feature interaction detection and visualization. It builds\nparsimonious yet flexible models using tree-structured Bayesian networks, and\nsamples an ensemble of such models using Markov chain Monte Carlo. We build in\nfeature selection by dividing the trees into two groups according to their\nrelevance to the outcome of interest. Our method performs competitively on\nclassification and feature selection benchmarks in low and high dimensions, and\nincludes a visualization tool that provides insight into relevant features and\ninteractions.\n", "versions": [{"version": "v1", "created": "Mon, 8 Jun 2015 07:11:23 GMT"}, {"version": "v2", "created": "Sun, 11 Oct 2015 05:16:33 GMT"}, {"version": "v3", "created": "Tue, 13 Oct 2015 17:52:26 GMT"}, {"version": "v4", "created": "Sun, 7 Feb 2016 19:53:24 GMT"}], "update_date": "2016-02-09", "authors_parsed": [["Krakovna", "Viktoriya", ""], ["Du", "Jiong", ""], ["Liu", "Jun S.", ""]]}, {"id": "1506.02396", "submitter": "Zhimin Peng", "authors": "Zhimin Peng, Yangyang Xu, Ming Yan, Wotao Yin", "title": "ARock: an Algorithmic Framework for Asynchronous Parallel Coordinate\n  Updates", "comments": "updated the linear convergence proofs", "journal-ref": "SIAM Journal on Scientific Computing, 38 (2016), A2851-A2879", "doi": "10.1137/15M1024950", "report-no": null, "categories": "math.OC cs.DC stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Finding a fixed point to a nonexpansive operator, i.e., $x^*=Tx^*$, abstracts\nmany problems in numerical linear algebra, optimization, and other areas of\nscientific computing. To solve fixed-point problems, we propose ARock, an\nalgorithmic framework in which multiple agents (machines, processors, or cores)\nupdate $x$ in an asynchronous parallel fashion. Asynchrony is crucial to\nparallel computing since it reduces synchronization wait, relaxes communication\nbottleneck, and thus speeds up computing significantly. At each step of ARock,\nan agent updates a randomly selected coordinate $x_i$ based on possibly\nout-of-date information on $x$. The agents share $x$ through either global\nmemory or communication. If writing $x_i$ is atomic, the agents can read and\nwrite $x$ without memory locks.\n  Theoretically, we show that if the nonexpansive operator $T$ has a fixed\npoint, then with probability one, ARock generates a sequence that converges to\na fixed points of $T$. Our conditions on $T$ and step sizes are weaker than\ncomparable work. Linear convergence is also obtained.\n  We propose special cases of ARock for linear systems, convex optimization,\nmachine learning, as well as distributed and decentralized consensus problems.\nNumerical experiments of solving sparse logistic regression problems are\npresented.\n", "versions": [{"version": "v1", "created": "Mon, 8 Jun 2015 08:31:53 GMT"}, {"version": "v2", "created": "Thu, 9 Jul 2015 05:57:35 GMT"}, {"version": "v3", "created": "Sun, 12 Jul 2015 04:05:15 GMT"}, {"version": "v4", "created": "Mon, 25 Jan 2016 20:06:08 GMT"}, {"version": "v5", "created": "Fri, 27 May 2016 03:55:31 GMT"}], "update_date": "2016-10-03", "authors_parsed": [["Peng", "Zhimin", ""], ["Xu", "Yangyang", ""], ["Yan", "Ming", ""], ["Yin", "Wotao", ""]]}, {"id": "1506.02428", "submitter": "Purushottam Kar", "authors": "Kush Bhatia and Prateek Jain and Purushottam Kar", "title": "Robust Regression via Hard Thresholding", "comments": "24 pages, 3 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study the problem of Robust Least Squares Regression (RLSR) where several\nresponse variables can be adversarially corrupted. More specifically, for a\ndata matrix X \\in R^{p x n} and an underlying model w*, the response vector is\ngenerated as y = X'w* + b where b \\in R^n is the corruption vector supported\nover at most C.n coordinates. Existing exact recovery results for RLSR focus\nsolely on L1-penalty based convex formulations and impose relatively strict\nmodel assumptions such as requiring the corruptions b to be selected\nindependently of X.\n  In this work, we study a simple hard-thresholding algorithm called TORRENT\nwhich, under mild conditions on X, can recover w* exactly even if b corrupts\nthe response variables in an adversarial manner, i.e. both the support and\nentries of b are selected adversarially after observing X and w*. Our results\nhold under deterministic assumptions which are satisfied if X is sampled from\nany sub-Gaussian distribution. Finally unlike existing results that apply only\nto a fixed w*, generated independently of X, our results are universal and hold\nfor any w* \\in R^p.\n  Next, we propose gradient descent-based extensions of TORRENT that can scale\nefficiently to large scale problems, such as high dimensional sparse recovery\nand prove similar recovery guarantees for these extensions. Empirically we find\nTORRENT, and more so its extensions, offering significantly faster recovery\nthan the state-of-the-art L1 solvers. For instance, even on moderate-sized\ndatasets (with p = 50K) with around 40% corrupted responses, a variant of our\nproposed method called TORRENT-HYB is more than 20x faster than the best L1\nsolver.\n", "versions": [{"version": "v1", "created": "Mon, 8 Jun 2015 10:13:53 GMT"}], "update_date": "2015-06-09", "authors_parsed": [["Bhatia", "Kush", ""], ["Jain", "Prateek", ""], ["Kar", "Purushottam", ""]]}, {"id": "1506.02494", "submitter": "Christina Heinze", "authors": "Dominik Rothenh\\\"ausler, Christina Heinze, Jonas Peters and Nicolai\n  Meinshausen", "title": "backShift: Learning causal cyclic graphs from unknown shift\n  interventions", "comments": null, "journal-ref": "Advances in Neural Information Processing Systems 28 (2015)\n  1513-1521", "doi": null, "report-no": null, "categories": "stat.ME stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a simple method to learn linear causal cyclic models in the\npresence of latent variables. The method relies on equilibrium data of the\nmodel recorded under a specific kind of interventions (\"shift interventions\").\nThe location and strength of these interventions do not have to be known and\ncan be estimated from the data. Our method, called backShift, only uses second\nmoments of the data and performs simple joint matrix diagonalization, applied\nto differences between covariance matrices. We give a sufficient and necessary\ncondition for identifiability of the system, which is fulfilled almost surely\nunder some quite general assumptions if and only if there are at least three\ndistinct experimental settings, one of which can be pure observational data. We\ndemonstrate the performance on some simulated data and applications in flow\ncytometry and financial time series. The code is made available as R-package\nbackShift.\n", "versions": [{"version": "v1", "created": "Mon, 8 Jun 2015 13:41:22 GMT"}, {"version": "v2", "created": "Wed, 17 Jun 2015 11:12:23 GMT"}, {"version": "v3", "created": "Wed, 18 Nov 2015 12:13:03 GMT"}], "update_date": "2016-01-11", "authors_parsed": [["Rothenh\u00e4usler", "Dominik", ""], ["Heinze", "Christina", ""], ["Peters", "Jonas", ""], ["Meinshausen", "Nicolai", ""]]}, {"id": "1506.02510", "submitter": "Onur Dikmen", "authors": "Onur Dikmen", "title": "Learning Mixtures of Ising Models using Pseudolikelihood", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Maximum pseudolikelihood method has been among the most important methods for\nlearning parameters of statistical physics models, such as Ising models. In\nthis paper, we study how pseudolikelihood can be derived for learning\nparameters of a mixture of Ising models. The performance of the proposed\napproach is demonstrated for Ising and Potts models on both synthetic and real\ndata.\n", "versions": [{"version": "v1", "created": "Mon, 8 Jun 2015 14:00:32 GMT"}], "update_date": "2015-06-09", "authors_parsed": [["Dikmen", "Onur", ""]]}, {"id": "1506.02520", "submitter": "Tianwen Wei", "authors": "Stephane Chretien and Tianwen Wei", "title": "Convex recovery of tensors using nuclear norm penalization", "comments": "To appear in proceedings LVA/ICA 2015 at Czech Republic", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The subdifferential of convex functions of the singular spectrum of real\nmatrices has been widely studied in matrix analysis, optimization and automatic\ncontrol theory. Convex analysis and optimization over spaces of tensors is now\ngaining much interest due to its potential applications to signal processing,\nstatistics and engineering. The goal of this paper is to present an\napplications to the problem of low rank tensor recovery based on linear random\nmeasurement by extending the results of Tropp to the tensors setting.\n", "versions": [{"version": "v1", "created": "Mon, 8 Jun 2015 14:33:04 GMT"}], "update_date": "2015-06-09", "authors_parsed": [["Chretien", "Stephane", ""], ["Wei", "Tianwen", ""]]}, {"id": "1506.02530", "submitter": "Martin Tak\\'a\\v{c}", "authors": "Chenxin Ma, Rachael Tappenden, Martin Tak\\'a\\v{c}", "title": "Linear Convergence of the Randomized Feasible Descent Method Under the\n  Weak Strong Convexity Assumption", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper we generalize the framework of the feasible descent method\n(FDM) to a randomized (R-FDM) and a coordinate-wise random feasible descent\nmethod (RC-FDM) framework. We show that the famous SDCA algorithm for\noptimizing the SVM dual problem, or the stochastic coordinate descent method\nfor the LASSO problem, fits into the framework of RC-FDM. We prove linear\nconvergence for both R-FDM and RC-FDM under the weak strong convexity\nassumption. Moreover, we show that the duality gap converges linearly for\nRC-FDM, which implies that the duality gap also converges linearly for SDCA\napplied to the SVM dual problem.\n", "versions": [{"version": "v1", "created": "Mon, 8 Jun 2015 14:54:08 GMT"}], "update_date": "2015-06-09", "authors_parsed": [["Ma", "Chenxin", ""], ["Tappenden", "Rachael", ""], ["Tak\u00e1\u010d", "Martin", ""]]}, {"id": "1506.02544", "submitter": "Youssef  Mroueh", "authors": "Youssef Mroueh, Stephen Voinea, Tomaso Poggio", "title": "Learning with Group Invariant Features: A Kernel Perspective", "comments": "NIPS 2015", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We analyze in this paper a random feature map based on a theory of invariance\nI-theory introduced recently. More specifically, a group invariant signal\nsignature is obtained through cumulative distributions of group transformed\nrandom projections. Our analysis bridges invariant feature learning with kernel\nmethods, as we show that this feature map defines an expected Haar integration\nkernel that is invariant to the specified group action. We show how this\nnon-linear random feature map approximates this group invariant kernel\nuniformly on a set of $N$ points. Moreover, we show that it defines a function\nspace that is dense in the equivalent Invariant Reproducing Kernel Hilbert\nSpace. Finally, we quantify error rates of the convergence of the empirical\nrisk minimization, as well as the reduction in the sample complexity of a\nlearning algorithm using such an invariant representation for signal\nclassification, in a classical supervised learning setting.\n", "versions": [{"version": "v1", "created": "Mon, 8 Jun 2015 15:19:30 GMT"}, {"version": "v2", "created": "Fri, 4 Dec 2015 20:49:25 GMT"}], "update_date": "2015-12-07", "authors_parsed": [["Mroueh", "Youssef", ""], ["Voinea", "Stephen", ""], ["Poggio", "Tomaso", ""]]}, {"id": "1506.02550", "submitter": "Junpei Komiyama", "authors": "Junpei Komiyama, Junya Honda, Hisashi Kashima, Hiroshi Nakagawa", "title": "Regret Lower Bound and Optimal Algorithm in Dueling Bandit Problem", "comments": "26 pages, 10 figures, to appear in COLT2015 (ver.3: revised related\n  work (RUCB))", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study the $K$-armed dueling bandit problem, a variation of the standard\nstochastic bandit problem where the feedback is limited to relative comparisons\nof a pair of arms. We introduce a tight asymptotic regret lower bound that is\nbased on the information divergence. An algorithm that is inspired by the\nDeterministic Minimum Empirical Divergence algorithm (Honda and Takemura, 2010)\nis proposed, and its regret is analyzed. The proposed algorithm is found to be\nthe first one with a regret upper bound that matches the lower bound.\nExperimental comparisons of dueling bandit algorithms show that the proposed\nalgorithm significantly outperforms existing ones.\n", "versions": [{"version": "v1", "created": "Mon, 8 Jun 2015 15:28:39 GMT"}, {"version": "v2", "created": "Tue, 16 Jun 2015 10:19:53 GMT"}, {"version": "v3", "created": "Mon, 29 Jun 2015 12:35:58 GMT"}], "update_date": "2015-06-30", "authors_parsed": [["Komiyama", "Junpei", ""], ["Honda", "Junya", ""], ["Kashima", "Hisashi", ""], ["Nakagawa", "Hiroshi", ""]]}, {"id": "1506.02554", "submitter": "Christina Heinze", "authors": "Christina Heinze, Brian McWilliams, Nicolai Meinshausen", "title": "DUAL-LOCO: Distributing Statistical Estimation Using Random Projections", "comments": "13 pages", "journal-ref": "Proceedings of the 19th International Conference on Artificial\n  Intelligence and Statistics, 51, 2016, 12 pages", "doi": null, "report-no": null, "categories": "stat.ML cs.DC cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present DUAL-LOCO, a communication-efficient algorithm for distributed\nstatistical estimation. DUAL-LOCO assumes that the data is distributed\naccording to the features rather than the samples. It requires only a single\nround of communication where low-dimensional random projections are used to\napproximate the dependences between features available to different workers. We\nshow that DUAL-LOCO has bounded approximation error which only depends weakly\non the number of workers. We compare DUAL-LOCO against a state-of-the-art\ndistributed optimization method on a variety of real world datasets and show\nthat it obtains better speedups while retaining good accuracy.\n", "versions": [{"version": "v1", "created": "Mon, 8 Jun 2015 15:35:24 GMT"}, {"version": "v2", "created": "Fri, 8 Jan 2016 16:44:27 GMT"}], "update_date": "2016-08-04", "authors_parsed": [["Heinze", "Christina", ""], ["McWilliams", "Brian", ""], ["Meinshausen", "Nicolai", ""]]}, {"id": "1506.02557", "submitter": "Diederik P Kingma M.Sc.", "authors": "Diederik P. Kingma, Tim Salimans, Max Welling", "title": "Variational Dropout and the Local Reparameterization Trick", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We investigate a local reparameterizaton technique for greatly reducing the\nvariance of stochastic gradients for variational Bayesian inference (SGVB) of a\nposterior over model parameters, while retaining parallelizability. This local\nreparameterization translates uncertainty about global parameters into local\nnoise that is independent across datapoints in the minibatch. Such\nparameterizations can be trivially parallelized and have variance that is\ninversely proportional to the minibatch size, generally leading to much faster\nconvergence. Additionally, we explore a connection with dropout: Gaussian\ndropout objectives correspond to SGVB with local reparameterization, a\nscale-invariant prior and proportionally fixed posterior variance. Our method\nallows inference of more flexibly parameterized posteriors; specifically, we\npropose variational dropout, a generalization of Gaussian dropout where the\ndropout rates are learned, often leading to better models. The method is\ndemonstrated through several experiments.\n", "versions": [{"version": "v1", "created": "Mon, 8 Jun 2015 15:37:56 GMT"}, {"version": "v2", "created": "Sun, 20 Dec 2015 16:07:38 GMT"}], "update_date": "2015-12-22", "authors_parsed": [["Kingma", "Diederik P.", ""], ["Salimans", "Tim", ""], ["Welling", "Max", ""]]}, {"id": "1506.02564", "submitter": "Heiko Strathmann", "authors": "Heiko Strathmann, Dino Sejdinovic, Samuel Livingstone, Zoltan Szabo,\n  Arthur Gretton", "title": "Gradient-free Hamiltonian Monte Carlo with Efficient Kernel Exponential\n  Families", "comments": "20 pages, 7 figures", "journal-ref": "Advances in Neural Information Processing Systems 28, 2015", "doi": null, "report-no": null, "categories": "stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose Kernel Hamiltonian Monte Carlo (KMC), a gradient-free adaptive\nMCMC algorithm based on Hamiltonian Monte Carlo (HMC). On target densities\nwhere classical HMC is not an option due to intractable gradients, KMC\nadaptively learns the target's gradient structure by fitting an exponential\nfamily model in a Reproducing Kernel Hilbert Space. Computational costs are\nreduced by two novel efficient approximations to this gradient. While being\nasymptotically exact, KMC mimics HMC in terms of sampling efficiency, and\noffers substantial mixing improvements over state-of-the-art gradient free\nsamplers. We support our claims with experimental studies on both toy and\nreal-world applications, including Approximate Bayesian Computation and\nexact-approximate MCMC.\n", "versions": [{"version": "v1", "created": "Mon, 8 Jun 2015 15:55:30 GMT"}, {"version": "v2", "created": "Tue, 24 Nov 2015 12:34:29 GMT"}], "update_date": "2015-11-25", "authors_parsed": [["Strathmann", "Heiko", ""], ["Sejdinovic", "Dino", ""], ["Livingstone", "Samuel", ""], ["Szabo", "Zoltan", ""], ["Gretton", "Arthur", ""]]}, {"id": "1506.02565", "submitter": "Seungjin Choi", "authors": "Yong-Deok Kim, Taewoong Jang, Bohyung Han, and Seungjin Choi", "title": "Learning to Select Pre-Trained Deep Representations with Bayesian\n  Evidence Framework", "comments": "Appearing in CVPR-2016 (oral presentation)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a Bayesian evidence framework to facilitate transfer learning from\npre-trained deep convolutional neural networks (CNNs). Our framework is\nformulated on top of a least squares SVM (LS-SVM) classifier, which is simple\nand fast in both training and testing, and achieves competitive performance in\npractice. The regularization parameters in LS-SVM is estimated automatically\nwithout grid search and cross-validation by maximizing evidence, which is a\nuseful measure to select the best performing CNN out of multiple candidates for\ntransfer learning; the evidence is optimized efficiently by employing Aitken's\ndelta-squared process, which accelerates convergence of fixed point update. The\nproposed Bayesian evidence framework also provides a good solution to identify\nthe best ensemble of heterogeneous CNNs through a greedy algorithm. Our\nBayesian evidence framework for transfer learning is tested on 12 visual\nrecognition datasets and illustrates the state-of-the-art performance\nconsistently in terms of prediction accuracy and modeling efficiency.\n", "versions": [{"version": "v1", "created": "Mon, 8 Jun 2015 15:56:26 GMT"}, {"version": "v2", "created": "Tue, 9 Jun 2015 18:57:35 GMT"}, {"version": "v3", "created": "Thu, 24 Dec 2015 03:40:28 GMT"}, {"version": "v4", "created": "Mon, 25 Apr 2016 01:35:31 GMT"}], "update_date": "2016-04-26", "authors_parsed": [["Kim", "Yong-Deok", ""], ["Jang", "Taewoong", ""], ["Han", "Bohyung", ""], ["Choi", "Seungjin", ""]]}, {"id": "1506.02617", "submitter": "Behnam Neyshabur", "authors": "Behnam Neyshabur, Ruslan Salakhutdinov, Nathan Srebro", "title": "Path-SGD: Path-Normalized Optimization in Deep Neural Networks", "comments": "12 pages, 5 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV cs.NE stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We revisit the choice of SGD for training deep neural networks by\nreconsidering the appropriate geometry in which to optimize the weights. We\nargue for a geometry invariant to rescaling of weights that does not affect the\noutput of the network, and suggest Path-SGD, which is an approximate steepest\ndescent method with respect to a path-wise regularizer related to max-norm\nregularization. Path-SGD is easy and efficient to implement and leads to\nempirical gains over SGD and AdaGrad.\n", "versions": [{"version": "v1", "created": "Mon, 8 Jun 2015 19:01:33 GMT"}], "update_date": "2015-06-09", "authors_parsed": [["Neyshabur", "Behnam", ""], ["Salakhutdinov", "Ruslan", ""], ["Srebro", "Nathan", ""]]}, {"id": "1506.02620", "submitter": "Ching-pei Lee", "authors": "Ching-pei Lee, Kai-Wei Chang, Shyam Upadhyay, Dan Roth", "title": "Distributed Training of Structured SVM", "comments": "NIPS Workshop on Optimization for Machine Learning, 2015", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.DC cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Training structured prediction models is time-consuming. However, most\nexisting approaches only use a single machine, thus, the advantage of computing\npower and the capacity for larger data sets of multiple machines have not been\nexploited. In this work, we propose an efficient algorithm for distributedly\ntraining structured support vector machines based on a distributed\nblock-coordinate descent method. Both theoretical and experimental results\nindicate that our method is efficient.\n", "versions": [{"version": "v1", "created": "Mon, 8 Jun 2015 19:12:24 GMT"}, {"version": "v2", "created": "Sun, 14 Feb 2016 12:15:45 GMT"}], "update_date": "2016-02-16", "authors_parsed": [["Lee", "Ching-pei", ""], ["Chang", "Kai-Wei", ""], ["Upadhyay", "Shyam", ""], ["Roth", "Dan", ""]]}, {"id": "1506.02633", "submitter": "Antonio Rieser", "authors": "Antonio Rieser", "title": "A Topological Approach to Spectral Clustering", "comments": "21 Pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  We propose two related unsupervised clustering algorithms which, for input,\ntake data assumed to be sampled from a uniform distribution supported on a\nmetric space $X$, and output a clustering of the data based on the selection of\na topological model for the connected components of $X$. Both algorithms work\nby selecting a graph on the samples from a natural one-parameter family of\ngraphs, using a geometric criterion in the first case and an information\ntheoretic criterion in the second. The estimated connected components of $X$\nare identified with the kernel of the associated graph Laplacian, which allows\nthe algorithm to work without requiring the number of expected clusters or\nother auxiliary data as input.\n", "versions": [{"version": "v1", "created": "Mon, 8 Jun 2015 19:39:37 GMT"}, {"version": "v2", "created": "Sat, 25 Jul 2020 16:37:29 GMT"}], "update_date": "2020-07-28", "authors_parsed": [["Rieser", "Antonio", ""]]}, {"id": "1506.02681", "submitter": "Francois-Xavier Briol", "authors": "Fran\\c{c}ois-Xavier Briol, Chris J. Oates, Mark Girolami, Michael A.\n  Osborne", "title": "Frank-Wolfe Bayesian Quadrature: Probabilistic Integration with\n  Theoretical Guarantees", "comments": null, "journal-ref": "Advances in Neural Information Processing Systems 28, 1162--1170,\n  2015", "doi": null, "report-no": null, "categories": "stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  There is renewed interest in formulating integration as an inference problem,\nmotivated by obtaining a full distribution over numerical error that can be\npropagated through subsequent computation. Current methods, such as Bayesian\nQuadrature, demonstrate impressive empirical performance but lack theoretical\nanalysis. An important challenge is to reconcile these probabilistic\nintegrators with rigorous convergence guarantees. In this paper, we present the\nfirst probabilistic integrator that admits such theoretical treatment, called\nFrank-Wolfe Bayesian Quadrature (FWBQ). Under FWBQ, convergence to the true\nvalue of the integral is shown to be exponential and posterior contraction\nrates are proven to be superexponential. In simulations, FWBQ is competitive\nwith state-of-the-art methods and out-performs alternatives based on\nFrank-Wolfe optimisation. Our approach is applied to successfully quantify\nnumerical error in the solution to a challenging model choice problem in\ncellular biology.\n", "versions": [{"version": "v1", "created": "Mon, 8 Jun 2015 20:17:30 GMT"}, {"version": "v2", "created": "Thu, 29 Oct 2015 23:52:51 GMT"}, {"version": "v3", "created": "Sun, 6 Dec 2015 22:05:30 GMT"}], "update_date": "2017-08-01", "authors_parsed": [["Briol", "Fran\u00e7ois-Xavier", ""], ["Oates", "Chris J.", ""], ["Girolami", "Mark", ""], ["Osborne", "Michael A.", ""]]}, {"id": "1506.02686", "submitter": "George Monta\\~nez", "authors": "George D. Montanez, Cosma Rohilla Shalizi", "title": "The LICORS Cabinet: Nonparametric Algorithms for Spatio-temporal\n  Prediction", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Spatio-temporal data is intrinsically high dimensional, so unsupervised\nmodeling is only feasible if we can exploit structure in the process. When the\ndynamics are local in both space and time, this structure can be exploited by\nsplitting the global field into many lower-dimensional \"light cones\". We review\nlight cone decompositions for predictive state reconstruction, introducing\nthree simple light cone algorithms. These methods allow for tractable inference\nof spatio-temporal data, such as full-frame video. The algorithms make few\nassumptions on the underlying process yet have good predictive performance and\ncan provide distributions over spatio-temporal data, enabling sophisticated\nprobabilistic inference.\n", "versions": [{"version": "v1", "created": "Mon, 8 Jun 2015 20:26:08 GMT"}, {"version": "v2", "created": "Wed, 14 Sep 2016 14:20:11 GMT"}], "update_date": "2016-09-15", "authors_parsed": [["Montanez", "George D.", ""], ["Shalizi", "Cosma Rohilla", ""]]}, {"id": "1506.02690", "submitter": "Zhiguang Wang", "authors": "Zhiguang Wang, Tim Oates, James Lo", "title": "Adaptive Normalized Risk-Averting Training For Deep Neural Networks", "comments": "AAAI 2016, 0.39%~0.4% ER on MNIST with single 32-32-256-10 ConvNets,\n  code available at https://github.com/cauchyturing/ANRAE", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.NE stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper proposes a set of new error criteria and learning approaches,\nAdaptive Normalized Risk-Averting Training (ANRAT), to attack the non-convex\noptimization problem in training deep neural networks (DNNs). Theoretically, we\ndemonstrate its effectiveness on global and local convexity lower-bounded by\nthe standard $L_p$-norm error. By analyzing the gradient on the convexity index\n$\\lambda$, we explain the reason why to learn $\\lambda$ adaptively using\ngradient descent works. In practice, we show how this method improves training\nof deep neural networks to solve visual recognition tasks on the MNIST and\nCIFAR-10 datasets. Without using pretraining or other tricks, we obtain results\ncomparable or superior to those reported in recent literature on the same tasks\nusing standard ConvNets + MSE/cross entropy. Performance on deep/shallow\nmultilayer perceptrons and Denoised Auto-encoders is also explored. ANRAT can\nbe combined with other quasi-Newton training methods, innovative network\nvariants, regularization techniques and other specific tricks in DNNs. Other\nthan unsupervised pretraining, it provides a new perspective to address the\nnon-convex optimization problem in DNNs.\n", "versions": [{"version": "v1", "created": "Mon, 8 Jun 2015 20:42:12 GMT"}, {"version": "v2", "created": "Fri, 7 Aug 2015 14:53:46 GMT"}, {"version": "v3", "created": "Thu, 9 Jun 2016 04:10:22 GMT"}], "update_date": "2016-06-10", "authors_parsed": [["Wang", "Zhiguang", ""], ["Oates", "Tim", ""], ["Lo", "James", ""]]}, {"id": "1506.02699", "submitter": "Subhadeep Paul", "authors": "Subhadeep Paul and Yuguo Chen", "title": "Community detection in multi-relational data with restricted multi-layer\n  stochastic blockmodel", "comments": "55 pages, 9 figures", "journal-ref": "Electron. J. Statist. Volume 10, Number 2 (2016), 3807-3870", "doi": "10.1214/16-EJS1211", "report-no": null, "categories": "stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In recent years there has been an increased interest in statistical analysis\nof data with multiple types of relations among a set of entities. Such\nmulti-relational data can be represented as multi-layer graphs where the set of\nvertices represents the entities and multiple types of edges represent the\ndifferent relations among them. For community detection in multi-layer graphs,\nwe consider two random graph models, the multi-layer stochastic blockmodel\n(MLSBM) and a model with a restricted parameter space, the restricted\nmulti-layer stochastic blockmodel (RMLSBM). We derive consistency results for\ncommunity assignments of the maximum likelihood estimators (MLEs) in both\nmodels where MLSBM is assumed to be the true model, and either the number of\nnodes or the number of types of edges or both grow. We compare MLEs in the two\nmodels with other baseline approaches, such as separate modeling of layers,\naggregating the layers and majority voting. RMLSBM is shown to have advantage\nover MLSBM when either the growth rate of the number of communities is high or\nthe growth rate of the average degree of the component graphs in the\nmulti-graph is low. We also derive minimax rates of error and sharp thresholds\nfor achieving consistency of community detection in both models, which are then\nused to compare the multi-layer models with a baseline model, the aggregate\nstochastic block model. The simulation studies and real data applications\nconfirm the superior performance of the multi-layer approaches in comparison to\nthe baseline procedures.\n", "versions": [{"version": "v1", "created": "Mon, 8 Jun 2015 20:53:33 GMT"}, {"version": "v2", "created": "Thu, 21 Jan 2016 23:27:44 GMT"}], "update_date": "2017-04-27", "authors_parsed": [["Paul", "Subhadeep", ""], ["Chen", "Yuguo", ""]]}, {"id": "1506.02761", "submitter": "Shihao Ji", "authors": "Shihao Ji, Hyokun Yun, Pinar Yanardag, Shin Matsushima, and S. V. N.\n  Vishwanathan", "title": "WordRank: Learning Word Embeddings via Robust Ranking", "comments": "Conference on Empirical Methods in Natural Language Processing\n  (EMNLP), November 1-5, 2016, Austin, Texas, USA", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Embedding words in a vector space has gained a lot of attention in recent\nyears. While state-of-the-art methods provide efficient computation of word\nsimilarities via a low-dimensional matrix embedding, their motivation is often\nleft unclear. In this paper, we argue that word embedding can be naturally\nviewed as a ranking problem due to the ranking nature of the evaluation\nmetrics. Then, based on this insight, we propose a novel framework WordRank\nthat efficiently estimates word representations via robust ranking, in which\nthe attention mechanism and robustness to noise are readily achieved via the\nDCG-like ranking losses. The performance of WordRank is measured in word\nsimilarity and word analogy benchmarks, and the results are compared to the\nstate-of-the-art word embedding techniques. Our algorithm is very competitive\nto the state-of-the- arts on large corpora, while outperforms them by a\nsignificant margin when the training set is limited (i.e., sparse and noisy).\nWith 17 million tokens, WordRank performs almost as well as existing methods\nusing 7.2 billion tokens on a popular word similarity benchmark. Our multi-node\ndistributed implementation of WordRank is publicly available for general usage.\n", "versions": [{"version": "v1", "created": "Tue, 9 Jun 2015 03:08:06 GMT"}, {"version": "v2", "created": "Sat, 6 Feb 2016 06:02:56 GMT"}, {"version": "v3", "created": "Tue, 23 Feb 2016 06:40:14 GMT"}, {"version": "v4", "created": "Tue, 27 Sep 2016 21:11:56 GMT"}], "update_date": "2016-09-29", "authors_parsed": [["Ji", "Shihao", ""], ["Yun", "Hyokun", ""], ["Yanardag", "Pinar", ""], ["Matsushima", "Shin", ""], ["Vishwanathan", "S. V. N.", ""]]}, {"id": "1506.02784", "submitter": "Song Liu Dr.", "authors": "Song Liu, Kenji Fukumizu", "title": "Estimating Posterior Ratio for Classification: Transfer Learning from\n  Probabilistic Perspective", "comments": "Revision Comments: The proofs were corrected from a few mistakes. The\n  title and the introduction was changed. We have also re-run a few experiments", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Transfer learning assumes classifiers of similar tasks share certain\nparameter structures. Unfortunately, modern classifiers uses sophisticated\nfeature representations with huge parameter spaces which lead to costly\ntransfer. Under the impression that changes from one classifier to another\nshould be ``simple'', an efficient transfer learning criteria that only learns\nthe ``differences'' is proposed in this paper. We train a \\emph{posterior\nratio} which turns out to minimizes the upper-bound of the target learning\nrisk. The model of posterior ratio does not have to share the same parameter\nspace with the source classifier at all so it can be easily modelled and\nefficiently trained. The resulting classifier therefore is obtained by simply\nmultiplying the existing probabilistic-classifier with the learned posterior\nratio.\n", "versions": [{"version": "v1", "created": "Tue, 9 Jun 2015 05:38:17 GMT"}, {"version": "v2", "created": "Wed, 10 Jun 2015 03:17:57 GMT"}, {"version": "v3", "created": "Mon, 19 Oct 2015 05:16:55 GMT"}], "update_date": "2015-10-20", "authors_parsed": [["Liu", "Song", ""], ["Fukumizu", "Kenji", ""]]}, {"id": "1506.02785", "submitter": "Danica J. Sutherland", "authors": "Danica J. Sutherland and Jeff Schneider", "title": "On the Error of Random Fourier Features", "comments": "Published at UAI 2015", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Kernel methods give powerful, flexible, and theoretically grounded approaches\nto solving many problems in machine learning. The standard approach, however,\nrequires pairwise evaluations of a kernel function, which can lead to\nscalability issues for very large datasets. Rahimi and Recht (2007) suggested a\npopular approach to handling this problem, known as random Fourier features.\nThe quality of this approximation, however, is not well understood. We improve\nthe uniform error bound of that paper, as well as giving novel understandings\nof the embedding's variance, approximation error, and use in some machine\nlearning methods. We also point out that surprisingly, of the two main variants\nof those features, the more widely used is strictly higher-variance for the\nGaussian kernel and has worse bounds.\n", "versions": [{"version": "v1", "created": "Tue, 9 Jun 2015 05:39:02 GMT"}], "update_date": "2021-04-08", "authors_parsed": [["Sutherland", "Danica J.", ""], ["Schneider", "Jeff", ""]]}, {"id": "1506.02903", "submitter": "Daniel Hsu", "authors": "Daniel Hsu, Aryeh Kontorovich, Csaba Szepesv\\'ari", "title": "Mixing Time Estimation in Reversible Markov Chains from a Single Sample\n  Path", "comments": "28 pages; minor clarification in Appendix A concerning lower bounds", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This article provides the first procedure for computing a fully\ndata-dependent interval that traps the mixing time $t_{\\text{mix}}$ of a finite\nreversible ergodic Markov chain at a prescribed confidence level. The interval\nis computed from a single finite-length sample path from the Markov chain, and\ndoes not require the knowledge of any parameters of the chain. This stands in\ncontrast to previous approaches, which either only provide point estimates, or\nrequire a reset mechanism, or additional prior knowledge. The interval is\nconstructed around the relaxation time $t_{\\text{relax}}$, which is strongly\nrelated to the mixing time, and the width of the interval converges to zero\nroughly at a $\\sqrt{n}$ rate, where $n$ is the length of the sample path. Upper\nand lower bounds are given on the number of samples required to achieve\nconstant-factor multiplicative accuracy. The lower bounds indicate that, unless\nfurther restrictions are placed on the chain, no procedure can achieve this\naccuracy level before seeing each state at least $\\Omega(t_{\\text{relax}})$\ntimes on the average. Finally, future directions of research are identified.\n", "versions": [{"version": "v1", "created": "Tue, 9 Jun 2015 13:30:13 GMT"}, {"version": "v2", "created": "Sun, 1 Nov 2015 00:09:24 GMT"}, {"version": "v3", "created": "Tue, 3 Nov 2015 04:27:46 GMT"}], "update_date": "2015-11-04", "authors_parsed": [["Hsu", "Daniel", ""], ["Kontorovich", "Aryeh", ""], ["Szepesv\u00e1ri", "Csaba", ""]]}, {"id": "1506.02914", "submitter": "Eric Tramel", "authors": "Marylou Gabri\\'e and Eric W. Tramel and Florent Krzakala", "title": "Training Restricted Boltzmann Machines via the Thouless-Anderson-Palmer\n  Free Energy", "comments": "8 pages, 7 figures, demo online at\n  http://www.lps.ens.fr/~krzakala/WASP.html", "journal-ref": "Advances in Neural Information Processing Systems (NIPS 2015) 28,\n  pages 640--648", "doi": null, "report-no": null, "categories": "cond-mat.dis-nn cs.LG cs.NE stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Restricted Boltzmann machines are undirected neural networks which have been\nshown to be effective in many applications, including serving as\ninitializations for training deep multi-layer neural networks. One of the main\nreasons for their success is the existence of efficient and practical\nstochastic algorithms, such as contrastive divergence, for unsupervised\ntraining. We propose an alternative deterministic iterative procedure based on\nan improved mean field method from statistical physics known as the\nThouless-Anderson-Palmer approach. We demonstrate that our algorithm provides\nperformance equal to, and sometimes superior to, persistent contrastive\ndivergence, while also providing a clear and easy to evaluate objective\nfunction. We believe that this strategy can be easily generalized to other\nmodels as well as to more accurate higher-order approximations, paving the way\nfor systematic improvements in training Boltzmann machines with hidden units.\n", "versions": [{"version": "v1", "created": "Tue, 9 Jun 2015 14:02:02 GMT"}, {"version": "v2", "created": "Mon, 15 Jun 2015 08:30:06 GMT"}], "update_date": "2016-02-11", "authors_parsed": [["Gabri\u00e9", "Marylou", ""], ["Tramel", "Eric W.", ""], ["Krzakala", "Florent", ""]]}, {"id": "1506.02975", "submitter": "Vincent Zhao", "authors": "Vincent Zhao, Steven W. Zucker", "title": "Stagewise Learning for Sparse Clustering of Discretely-Valued Data", "comments": "9 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG q-bio.QM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The performance of EM in learning mixtures of product distributions often\ndepends on the initialization. This can be problematic in crowdsourcing and\nother applications, e.g. when a small number of 'experts' are diluted by a\nlarge number of noisy, unreliable participants. We develop a new EM algorithm\nthat is driven by these experts. In a manner that differs from other\napproaches, we start from a single mixture class. The algorithm then develops\nthe set of 'experts' in a stagewise fashion based on a mutual information\ncriterion. At each stage EM operates on this subset of the players, effectively\nregularizing the E rather than the M step. Experiments show that stagewise EM\noutperforms other initialization techniques for crowdsourcing and neurosciences\napplications, and can guide a full EM to results comparable to those obtained\nknowing the exact distribution.\n", "versions": [{"version": "v1", "created": "Tue, 9 Jun 2015 16:00:21 GMT"}, {"version": "v2", "created": "Sat, 28 May 2016 02:38:42 GMT"}], "update_date": "2016-05-31", "authors_parsed": [["Zhao", "Vincent", ""], ["Zucker", "Steven W.", ""]]}, {"id": "1506.03016", "submitter": "Atsushi Nitanda", "authors": "Atsushi Nitanda", "title": "Accelerated Stochastic Gradient Descent for Minimizing Finite Sums", "comments": "[v2] corrected citation to proxSVRG, corrected typos in Figure\n  1(option2) and 3(R4 -> R3)", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose an optimization method for minimizing the finite sums of smooth\nconvex functions. Our method incorporates an accelerated gradient descent (AGD)\nand a stochastic variance reduction gradient (SVRG) in a mini-batch setting.\nUnlike SVRG, our method can be directly applied to non-strongly and strongly\nconvex problems. We show that our method achieves a lower overall complexity\nthan the recently proposed methods that supports non-strongly convex problems.\nMoreover, this method has a fast rate of convergence for strongly convex\nproblems. Our experiments show the effectiveness of our method.\n", "versions": [{"version": "v1", "created": "Tue, 9 Jun 2015 17:38:32 GMT"}, {"version": "v2", "created": "Wed, 10 Jun 2015 16:25:39 GMT"}], "update_date": "2015-06-11", "authors_parsed": [["Nitanda", "Atsushi", ""]]}, {"id": "1506.03039", "submitter": "Jack Gorham", "authors": "Jackson Gorham and Lester Mackey", "title": "Measuring Sample Quality with Stein's Method", "comments": "17 pages, 6 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG math.PR stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  To improve the efficiency of Monte Carlo estimation, practitioners are\nturning to biased Markov chain Monte Carlo procedures that trade off asymptotic\nexactness for computational speed. The reasoning is sound: a reduction in\nvariance due to more rapid sampling can outweigh the bias introduced. However,\nthe inexactness creates new challenges for sampler and parameter selection,\nsince standard measures of sample quality like effective sample size do not\naccount for asymptotic bias. To address these challenges, we introduce a new\ncomputable quality measure based on Stein's method that quantifies the maximum\ndiscrepancy between sample and target expectations over a large class of test\nfunctions. We use our tool to compare exact, biased, and deterministic sample\nsequences and illustrate applications to hyperparameter selection, convergence\nrate assessment, and quantifying bias-variance tradeoffs in posterior\ninference.\n", "versions": [{"version": "v1", "created": "Tue, 9 Jun 2015 18:48:58 GMT"}, {"version": "v2", "created": "Fri, 19 Jun 2015 05:15:18 GMT"}, {"version": "v3", "created": "Sat, 12 Sep 2015 23:31:21 GMT"}, {"version": "v4", "created": "Mon, 11 Jan 2016 03:47:27 GMT"}, {"version": "v5", "created": "Mon, 6 Mar 2017 18:59:16 GMT"}, {"version": "v6", "created": "Tue, 1 Jan 2019 03:07:44 GMT"}], "update_date": "2019-01-03", "authors_parsed": [["Gorham", "Jackson", ""], ["Mackey", "Lester", ""]]}, {"id": "1506.03041", "submitter": "Diana Borsa", "authors": "Diana Borsa, Thore Graepel, Andrew Gordon", "title": "The Wreath Process: A totally generative model of geometric shape based\n  on nested symmetries", "comments": "10 pages(double-column), 60+ figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider the problem of modelling noisy but highly symmetric shapes that\ncan be viewed as hierarchies of whole-part relationships in which higher level\nobjects are composed of transformed collections of lower level objects. To this\nend, we propose the stochastic wreath process, a fully generative probabilistic\nmodel of drawings. Following Leyton's \"Generative Theory of Shape\", we\nrepresent shapes as sequences of transformation groups composed through a\nwreath product.\n  This representation emphasizes the maximization of transfer --- the idea that\nthe most compact and meaningful representation of a given shape is achieved by\nmaximizing the re-use of existing building blocks or parts.\n  The proposed stochastic wreath process extends Leyton's theory by defining a\nprobability distribution over geometric shapes in terms of noise processes that\nare aligned with the generative group structure of the shape. We propose an\ninference scheme for recovering the generative history of given images in terms\nof the wreath process using reversible jump Markov chain Monte Carlo methods\nand Approximate Bayesian Computation. In the context of sketching we\ndemonstrate the feasibility and limitations of this approach on model-generated\nand real data.\n", "versions": [{"version": "v1", "created": "Tue, 9 Jun 2015 18:56:43 GMT"}], "update_date": "2015-06-10", "authors_parsed": [["Borsa", "Diana", ""], ["Graepel", "Thore", ""], ["Gordon", "Andrew", ""]]}, {"id": "1506.03072", "submitter": "Vijay Kumar", "authors": "Vijay Kumar and Dan Levy", "title": "Clustering by transitive propagation", "comments": "13 pages + 2 appendices, figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cond-mat.stat-mech stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a global optimization algorithm for clustering data given the\nratio of likelihoods that each pair of data points is in the same cluster or in\ndifferent clusters. To define a clustering solution in terms of pairwise\nrelationships, a necessary and sufficient condition is that belonging to the\nsame cluster satisfies transitivity. We define a global objective function\nbased on pairwise likelihood ratios and a transitivity constraint over all\ntriples, assigning an equal prior probability to all clustering solutions. We\nmaximize the objective function by implementing max-sum message passing on the\ncorresponding factor graph to arrive at an O(N^3) algorithm. Lastly, we\ndemonstrate an application inspired by mutational sequencing for decoding\nrandom binary words transmitted through a noisy channel.\n", "versions": [{"version": "v1", "created": "Tue, 9 Jun 2015 20:00:26 GMT"}], "update_date": "2015-06-11", "authors_parsed": [["Kumar", "Vijay", ""], ["Levy", "Dan", ""]]}, {"id": "1506.03074", "submitter": "Maxim Rabinovich", "authors": "Maxim Rabinovich, Elaine Angelino, Michael I. Jordan", "title": "Variational consensus Monte Carlo", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Practitioners of Bayesian statistics have long depended on Markov chain Monte\nCarlo (MCMC) to obtain samples from intractable posterior distributions.\nUnfortunately, MCMC algorithms are typically serial, and do not scale to the\nlarge datasets typical of modern machine learning. The recently proposed\nconsensus Monte Carlo algorithm removes this limitation by partitioning the\ndata and drawing samples conditional on each partition in parallel (Scott et\nal, 2013). A fixed aggregation function then combines these samples, yielding\napproximate posterior samples. We introduce variational consensus Monte Carlo\n(VCMC), a variational Bayes algorithm that optimizes over aggregation functions\nto obtain samples from a distribution that better approximates the target. The\nresulting objective contains an intractable entropy term; we therefore derive a\nrelaxation of the objective and show that the relaxed problem is blockwise\nconcave under mild conditions. We illustrate the advantages of our algorithm on\nthree inference tasks from the literature, demonstrating both the superior\nquality of the posterior approximation and the moderate overhead of the\noptimization step. Our algorithm achieves a relative error reduction (measured\nagainst serial MCMC) of up to 39% compared to consensus Monte Carlo on the task\nof estimating 300-dimensional probit regression parameter expectations;\nsimilarly, it achieves an error reduction of 92% on the task of estimating\ncluster comembership probabilities in a Gaussian mixture model with 8\ncomponents in 8 dimensions. Furthermore, these gains come at moderate cost\ncompared to the runtime of serial MCMC, achieving near-ideal speedup in some\ninstances.\n", "versions": [{"version": "v1", "created": "Tue, 9 Jun 2015 20:00:48 GMT"}], "update_date": "2015-06-11", "authors_parsed": [["Rabinovich", "Maxim", ""], ["Angelino", "Elaine", ""], ["Jordan", "Michael I.", ""]]}, {"id": "1506.03101", "submitter": "Bo Dai", "authors": "Bo Dai, Niao He, Hanjun Dai, Le Song", "title": "Provable Bayesian Inference via Particle Mirror Descent", "comments": "38 pages, 26 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.CO stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Bayesian methods are appealing in their flexibility in modeling complex data\nand ability in capturing uncertainty in parameters. However, when Bayes' rule\ndoes not result in tractable closed-form, most approximate inference algorithms\nlack either scalability or rigorous guarantees. To tackle this challenge, we\npropose a simple yet provable algorithm, \\emph{Particle Mirror Descent} (PMD),\nto iteratively approximate the posterior density. PMD is inspired by stochastic\nfunctional mirror descent where one descends in the density space using a small\nbatch of data points at each iteration, and by particle filtering where one\nuses samples to approximate a function. We prove result of the first kind that,\nwith $m$ particles, PMD provides a posterior density estimator that converges\nin terms of $KL$-divergence to the true posterior in rate $O(1/\\sqrt{m})$. We\ndemonstrate competitive empirical performances of PMD compared to several\napproximate inference algorithms in mixture models, logistic regression, sparse\nGaussian processes and latent Dirichlet allocation on large scale datasets.\n", "versions": [{"version": "v1", "created": "Tue, 9 Jun 2015 20:57:37 GMT"}, {"version": "v2", "created": "Tue, 3 May 2016 19:06:18 GMT"}, {"version": "v3", "created": "Thu, 5 May 2016 22:56:13 GMT"}], "update_date": "2016-05-09", "authors_parsed": [["Dai", "Bo", ""], ["He", "Niao", ""], ["Dai", "Hanjun", ""], ["Song", "Le", ""]]}, {"id": "1506.03134", "submitter": "Oriol Vinyals", "authors": "Oriol Vinyals, Meire Fortunato, Navdeep Jaitly", "title": "Pointer Networks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.CG cs.LG cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We introduce a new neural architecture to learn the conditional probability\nof an output sequence with elements that are discrete tokens corresponding to\npositions in an input sequence. Such problems cannot be trivially addressed by\nexistent approaches such as sequence-to-sequence and Neural Turing Machines,\nbecause the number of target classes in each step of the output depends on the\nlength of the input, which is variable. Problems such as sorting variable sized\nsequences, and various combinatorial optimization problems belong to this\nclass. Our model solves the problem of variable size output dictionaries using\na recently proposed mechanism of neural attention. It differs from the previous\nattention attempts in that, instead of using attention to blend hidden units of\nan encoder to a context vector at each decoder step, it uses attention as a\npointer to select a member of the input sequence as the output. We call this\narchitecture a Pointer Net (Ptr-Net). We show Ptr-Nets can be used to learn\napproximate solutions to three challenging geometric problems -- finding planar\nconvex hulls, computing Delaunay triangulations, and the planar Travelling\nSalesman Problem -- using training examples alone. Ptr-Nets not only improve\nover sequence-to-sequence with input attention, but also allow us to generalize\nto variable size output dictionaries. We show that the learnt models generalize\nbeyond the maximum lengths they were trained on. We hope our results on these\ntasks will encourage a broader exploration of neural learning for discrete\nproblems.\n", "versions": [{"version": "v1", "created": "Tue, 9 Jun 2015 23:38:16 GMT"}, {"version": "v2", "created": "Mon, 2 Jan 2017 10:25:29 GMT"}], "update_date": "2017-01-03", "authors_parsed": [["Vinyals", "Oriol", ""], ["Fortunato", "Meire", ""], ["Jaitly", "Navdeep", ""]]}, {"id": "1506.03137", "submitter": "Tselil Schramm", "authors": "Tselil Schramm and Benjamin Weitz", "title": "Symmetric Tensor Completion from Multilinear Entries and Learning\n  Product Mixtures over the Hypercube", "comments": "Removed adversarial matrix completion algorithm after discovering\n  that our matrix completion results can be derived from prior work", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We give an algorithm for completing an order-$m$ symmetric low-rank tensor\nfrom its multilinear entries in time roughly proportional to the number of\ntensor entries. We apply our tensor completion algorithm to the problem of\nlearning mixtures of product distributions over the hypercube, obtaining new\nalgorithmic results. If the centers of the product distribution are linearly\nindependent, then we recover distributions with as many as $\\Omega(n)$ centers\nin polynomial time and sample complexity. In the general case, we recover\ndistributions with as many as $\\tilde\\Omega(n)$ centers in quasi-polynomial\ntime, answering an open problem of Feldman et al. (SIAM J. Comp.) for the\nspecial case of distributions with incoherent bias vectors.\n  Our main algorithmic tool is the iterated application of a low-rank matrix\ncompletion algorithm for matrices with adversarially missing entries.\n", "versions": [{"version": "v1", "created": "Tue, 9 Jun 2015 23:53:42 GMT"}, {"version": "v2", "created": "Thu, 9 Jul 2015 22:54:20 GMT"}, {"version": "v3", "created": "Mon, 23 Nov 2015 21:32:41 GMT"}], "update_date": "2015-11-25", "authors_parsed": [["Schramm", "Tselil", ""], ["Weitz", "Benjamin", ""]]}, {"id": "1506.03159", "submitter": "Dustin Tran", "authors": "Dustin Tran, David M. Blei, Edoardo M. Airoldi", "title": "Copula variational inference", "comments": "Appears in Neural Information Processing Systems, 2015", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG stat.CO stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We develop a general variational inference method that preserves dependency\namong the latent variables. Our method uses copulas to augment the families of\ndistributions used in mean-field and structured approximations. Copulas model\nthe dependency that is not captured by the original variational distribution,\nand thus the augmented variational family guarantees better approximations to\nthe posterior. With stochastic optimization, inference on the augmented\ndistribution is scalable. Furthermore, our strategy is generic: it can be\napplied to any inference procedure that currently uses the mean-field or\nstructured approach. Copula variational inference has many advantages: it\nreduces bias; it is less sensitive to local optima; it is less sensitive to\nhyperparameters; and it helps characterize and interpret the dependency among\nthe latent variables.\n", "versions": [{"version": "v1", "created": "Wed, 10 Jun 2015 04:14:22 GMT"}, {"version": "v2", "created": "Sat, 31 Oct 2015 06:52:07 GMT"}], "update_date": "2015-11-03", "authors_parsed": [["Tran", "Dustin", ""], ["Blei", "David M.", ""], ["Airoldi", "Edoardo M.", ""]]}, {"id": "1506.03164", "submitter": "Fangjian Guo", "authors": "Xiangyu Wang, Fangjian Guo, Katherine A. Heller and David B. Dunson", "title": "Parallelizing MCMC with Random Partition Trees", "comments": "25 pages, 9 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The modern scale of data has brought new challenges to Bayesian inference. In\nparticular, conventional MCMC algorithms are computationally very expensive for\nlarge data sets. A promising approach to solve this problem is embarrassingly\nparallel MCMC (EP-MCMC), which first partitions the data into multiple subsets\nand runs independent sampling algorithms on each subset. The subset posterior\ndraws are then aggregated via some combining rules to obtain the final\napproximation. Existing EP-MCMC algorithms are limited by approximation\naccuracy and difficulty in resampling. In this article, we propose a new\nEP-MCMC algorithm PART that solves these problems. The new algorithm applies\nrandom partition trees to combine the subset posterior draws, which is\ndistribution-free, easy to resample from and can adapt to multiple scales. We\nprovide theoretical justification and extensive experiments illustrating\nempirical performance.\n", "versions": [{"version": "v1", "created": "Wed, 10 Jun 2015 05:05:45 GMT"}, {"version": "v2", "created": "Mon, 26 Oct 2015 18:13:55 GMT"}], "update_date": "2015-10-27", "authors_parsed": [["Wang", "Xiangyu", ""], ["Guo", "Fangjian", ""], ["Heller", "Katherine A.", ""], ["Dunson", "David B.", ""]]}, {"id": "1506.03208", "submitter": "Eric Nalisnick", "authors": "Eric Nalisnick, Anima Anandkumar, Padhraic Smyth", "title": "A Scale Mixture Perspective of Multiplicative Noise in Neural Networks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Corrupting the input and hidden layers of deep neural networks (DNNs) with\nmultiplicative noise, often drawn from the Bernoulli distribution (or\n'dropout'), provides regularization that has significantly contributed to deep\nlearning's success. However, understanding how multiplicative corruptions\nprevent overfitting has been difficult due to the complexity of a DNN's\nfunctional form. In this paper, we show that when a Gaussian prior is placed on\na DNN's weights, applying multiplicative noise induces a Gaussian scale\nmixture, which can be reparameterized to circumvent the problematic likelihood\nfunction. Analysis can then proceed by using a type-II maximum likelihood\nprocedure to derive a closed-form expression revealing how regularization\nevolves as a function of the network's weights. Results show that\nmultiplicative noise forces weights to become either sparse or invariant to\nrescaling. We find our analysis has implications for model compression as it\nnaturally reveals a weight pruning rule that starkly contrasts with the\ncommonly used signal-to-noise ratio (SNR). While the SNR prunes weights with\nlarge variances, seeing them as noisy, our approach recognizes their robustness\nand retains them. We empirically demonstrate our approach has a strong\nadvantage over the SNR heuristic and is competitive to retraining with soft\ntargets produced from a teacher model.\n", "versions": [{"version": "v1", "created": "Wed, 10 Jun 2015 08:33:05 GMT"}], "update_date": "2015-06-11", "authors_parsed": [["Nalisnick", "Eric", ""], ["Anandkumar", "Anima", ""], ["Smyth", "Padhraic", ""]]}, {"id": "1506.03271", "submitter": "Gergely Neu", "authors": "Gergely Neu", "title": "Explore no more: Improved high-probability regret bounds for\n  non-stochastic bandits", "comments": "To appear at NIPS 2015", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This work addresses the problem of regret minimization in non-stochastic\nmulti-armed bandit problems, focusing on performance guarantees that hold with\nhigh probability. Such results are rather scarce in the literature since\nproving them requires a large deal of technical effort and significant\nmodifications to the standard, more intuitive algorithms that come only with\nguarantees that hold on expectation. One of these modifications is forcing the\nlearner to sample arms from the uniform distribution at least\n$\\Omega(\\sqrt{T})$ times over $T$ rounds, which can adversely affect\nperformance if many of the arms are suboptimal. While it is widely conjectured\nthat this property is essential for proving high-probability regret bounds, we\nshow in this paper that it is possible to achieve such strong results without\nthis undesirable exploration component. Our result relies on a simple and\nintuitive loss-estimation strategy called Implicit eXploration (IX) that allows\na remarkably clean analysis. To demonstrate the flexibility of our technique,\nwe derive several improved high-probability bounds for various extensions of\nthe standard multi-armed bandit framework. Finally, we conduct a simple\nexperiment that illustrates the robustness of our implicit exploration\ntechnique.\n", "versions": [{"version": "v1", "created": "Wed, 10 Jun 2015 12:19:21 GMT"}, {"version": "v2", "created": "Thu, 16 Jul 2015 12:59:46 GMT"}, {"version": "v3", "created": "Tue, 3 Nov 2015 08:42:39 GMT"}], "update_date": "2015-11-04", "authors_parsed": [["Neu", "Gergely", ""]]}, {"id": "1506.03338", "submitter": "Shixiang Gu", "authors": "Shixiang Gu and Zoubin Ghahramani and Richard E. Turner", "title": "Neural Adaptive Sequential Monte Carlo", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Sequential Monte Carlo (SMC), or particle filtering, is a popular class of\nmethods for sampling from an intractable target distribution using a sequence\nof simpler intermediate distributions. Like other importance sampling-based\nmethods, performance is critically dependent on the proposal distribution: a\nbad proposal can lead to arbitrarily inaccurate estimates of the target\ndistribution. This paper presents a new method for automatically adapting the\nproposal using an approximation of the Kullback-Leibler divergence between the\ntrue posterior and the proposal distribution. The method is very flexible,\napplicable to any parameterized proposal distribution and it supports online\nand batch variants. We use the new framework to adapt powerful proposal\ndistributions with rich parameterizations based upon neural networks leading to\nNeural Adaptive Sequential Monte Carlo (NASMC). Experiments indicate that NASMC\nsignificantly improves inference in a non-linear state space model\noutperforming adaptive proposal methods including the Extended Kalman and\nUnscented Particle Filters. Experiments also indicate that improved inference\ntranslates into improved parameter learning when NASMC is used as a subroutine\nof Particle Marginal Metropolis Hastings. Finally we show that NASMC is able to\ntrain a latent variable recurrent neural network (LV-RNN) achieving results\nthat compete with the state-of-the-art for polymorphic music modelling. NASMC\ncan be seen as bridging the gap between adaptive SMC methods and the recent\nwork in scalable, black-box variational inference.\n", "versions": [{"version": "v1", "created": "Wed, 10 Jun 2015 14:52:09 GMT"}, {"version": "v2", "created": "Thu, 11 Jun 2015 07:11:56 GMT"}, {"version": "v3", "created": "Mon, 16 Nov 2015 21:28:48 GMT"}], "update_date": "2015-11-18", "authors_parsed": [["Gu", "Shixiang", ""], ["Ghahramani", "Zoubin", ""], ["Turner", "Richard E.", ""]]}, {"id": "1506.03374", "submitter": "Shipra Agrawal", "authors": "Shipra Agrawal and Nikhil R. Devanur and Lihong Li", "title": "An efficient algorithm for contextual bandits with knapsacks, and an\n  extension to concave objectives", "comments": "Extended abstract appeared in COLT 2016", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider a contextual version of multi-armed bandit problem with global\nknapsack constraints. In each round, the outcome of pulling an arm is a scalar\nreward and a resource consumption vector, both dependent on the context, and\nthe global knapsack constraints require the total consumption for each resource\nto be below some pre-fixed budget. The learning agent competes with an\narbitrary set of context-dependent policies. This problem was introduced by\nBadanidiyuru et al. (2014), who gave a computationally inefficient algorithm\nwith near-optimal regret bounds for it. We give a computationally efficient\nalgorithm for this problem with slightly better regret bounds, by generalizing\nthe approach of Agarwal et al. (2014) for the non-constrained version of the\nproblem. The computational time of our algorithm scales logarithmically in the\nsize of the policy space. This answers the main open question of Badanidiyuru\net al. (2014). We also extend our results to a variant where there are no\nknapsack constraints but the objective is an arbitrary Lipschitz concave\nfunction of the sum of outcome vectors.\n", "versions": [{"version": "v1", "created": "Wed, 10 Jun 2015 16:14:19 GMT"}, {"version": "v2", "created": "Sat, 9 Jul 2016 05:46:06 GMT"}], "update_date": "2016-07-12", "authors_parsed": [["Agrawal", "Shipra", ""], ["Devanur", "Nikhil R.", ""], ["Li", "Lihong", ""]]}, {"id": "1506.03378", "submitter": "Lihong Li", "authors": "Che-Yu Liu and Lihong Li", "title": "On the Prior Sensitivity of Thompson Sampling", "comments": "Appears in the 27th International Conference on Algorithmic Learning\n  Theory (ALT), 2016", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The empirically successful Thompson Sampling algorithm for stochastic bandits\nhas drawn much interest in understanding its theoretical properties. One\nimportant benefit of the algorithm is that it allows domain knowledge to be\nconveniently encoded as a prior distribution to balance exploration and\nexploitation more effectively. While it is generally believed that the\nalgorithm's regret is low (high) when the prior is good (bad), little is known\nabout the exact dependence. In this paper, we fully characterize the\nalgorithm's worst-case dependence of regret on the choice of prior, focusing on\na special yet representative case. These results also provide insights into the\ngeneral sensitivity of the algorithm to the choice of priors. In particular,\nwith $p$ being the prior probability mass of the true reward-generating model,\nwe prove $O(\\sqrt{T/p})$ and $O(\\sqrt{(1-p)T})$ regret upper bounds for the\nbad- and good-prior cases, respectively, as well as \\emph{matching} lower\nbounds. Our proofs rely on the discovery of a fundamental property of Thompson\nSampling and make heavy use of martingale theory, both of which appear novel in\nthe literature, to the best of our knowledge.\n", "versions": [{"version": "v1", "created": "Wed, 10 Jun 2015 16:22:26 GMT"}, {"version": "v2", "created": "Thu, 21 Jul 2016 01:43:09 GMT"}], "update_date": "2016-07-22", "authors_parsed": [["Liu", "Che-Yu", ""], ["Li", "Lihong", ""]]}, {"id": "1506.03382", "submitter": "Xiaodong Li", "authors": "T. Tony Cai, Xiaodong Li, and Zongming Ma", "title": "Optimal Rates of Convergence for Noisy Sparse Phase Retrieval via\n  Thresholded Wirtinger Flow", "comments": "28 pages, 4 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST cs.IT math.IT math.NA stat.ML stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper considers the noisy sparse phase retrieval problem: recovering a\nsparse signal $x \\in \\mathbb{R}^p$ from noisy quadratic measurements $y_j =\n(a_j' x )^2 + \\epsilon_j$, $j=1, \\ldots, m$, with independent sub-exponential\nnoise $\\epsilon_j$. The goals are to understand the effect of the sparsity of\n$x$ on the estimation precision and to construct a computationally feasible\nestimator to achieve the optimal rates. Inspired by the Wirtinger Flow [12]\nproposed for noiseless and non-sparse phase retrieval, a novel thresholded\ngradient descent algorithm is proposed and it is shown to adaptively achieve\nthe minimax optimal rates of convergence over a wide range of sparsity levels\nwhen the $a_j$'s are independent standard Gaussian random vectors, provided\nthat the sample size is sufficiently large compared to the sparsity of $x$.\n", "versions": [{"version": "v1", "created": "Wed, 10 Jun 2015 16:32:33 GMT"}], "update_date": "2015-06-11", "authors_parsed": [["Cai", "T. Tony", ""], ["Li", "Xiaodong", ""], ["Ma", "Zongming", ""]]}, {"id": "1506.03410", "submitter": "Benjamin Falk", "authors": "Tyler M. Tomita, James Browne, Cencheng Shen, Jaewon Chung, Jesse L.\n  Patsolic, Benjamin Falk, Jason Yim, Carey E. Priebe, Randal Burns, Mauro\n  Maggioni, Joshua T. Vogelstein", "title": "Sparse Projection Oblique Randomer Forests", "comments": "31 pages; submitted to Journal of Machine Learning Research for\n  review", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Decision forests, including Random Forests and Gradient Boosting Trees, have\nrecently demonstrated state-of-the-art performance in a variety of machine\nlearning settings. Decision forests are typically ensembles of axis-aligned\ndecision trees; that is, trees that split only along feature dimensions. In\ncontrast, many recent extensions to decision forests are based on axis-oblique\nsplits. Unfortunately, these extensions forfeit one or more of the favorable\nproperties of decision forests based on axis-aligned splits, such as robustness\nto many noise dimensions, interpretability, or computational efficiency. We\nintroduce yet another decision forest, called \"Sparse Projection Oblique\nRandomer Forests\" (SPORF). SPORF uses very sparse random projections, i.e.,\nlinear combinations of a small subset of features. SPORF significantly improves\naccuracy over existing state-of-the-art algorithms on a standard benchmark\nsuite for classification with >100 problems of varying dimension, sample size,\nand number of classes. To illustrate how SPORF addresses the limitations of\nboth axis-aligned and existing oblique decision forest methods, we conduct\nextensive simulated experiments. SPORF typically yields improved performance\nover existing decision forests, while mitigating computational efficiency and\nscalability and maintaining interpretability. SPORF can easily be incorporated\ninto other ensemble methods such as boosting to obtain potentially similar\ngains.\n", "versions": [{"version": "v1", "created": "Wed, 10 Jun 2015 17:55:51 GMT"}, {"version": "v2", "created": "Wed, 4 May 2016 18:14:39 GMT"}, {"version": "v3", "created": "Mon, 19 Mar 2018 21:51:16 GMT"}, {"version": "v4", "created": "Wed, 10 Oct 2018 00:36:04 GMT"}, {"version": "v5", "created": "Mon, 30 Sep 2019 21:35:19 GMT"}, {"version": "v6", "created": "Thu, 3 Oct 2019 14:04:32 GMT"}], "update_date": "2019-10-04", "authors_parsed": [["Tomita", "Tyler M.", ""], ["Browne", "James", ""], ["Shen", "Cencheng", ""], ["Chung", "Jaewon", ""], ["Patsolic", "Jesse L.", ""], ["Falk", "Benjamin", ""], ["Yim", "Jason", ""], ["Priebe", "Carey E.", ""], ["Burns", "Randal", ""], ["Maggioni", "Mauro", ""], ["Vogelstein", "Joshua T.", ""]]}, {"id": "1506.03412", "submitter": "Vamsi Ithapu", "authors": "Vamsi K. Ithapu, Sathya Ravi, Vikas Singh", "title": "Convergence rates for pretraining and dropout: Guiding learning\n  parameters using network structure", "comments": "This manuscript is now superseded by arXiv:1511.05297 and the\n  corresponding accepted paper in 54th Allerton Conference on Communication,\n  Control and Computing (2017)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV cs.NE math.OC stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Unsupervised pretraining and dropout have been well studied, especially with\nrespect to regularization and output consistency. However, our understanding\nabout the explicit convergence rates of the parameter estimates, and their\ndependence on the learning (like denoising and dropout rate) and structural\n(like depth and layer lengths) aspects of the network is less mature. An\ninteresting question in this context is to ask if the network structure could\n\"guide\" the choices of such learning parameters. In this work, we explore these\ngaps between network structure, the learning mechanisms and their interaction\nwith parameter convergence rates. We present a way to address these issues\nbased on the backpropagation convergence rates for general nonconvex objectives\nusing first-order information. We then incorporate two learning mechanisms into\nthis general framework -- denoising autoencoder and dropout, and subsequently\nderive the convergence rates of deep networks. Building upon these bounds, we\nprovide insights into the choices of learning parameters and network sizes that\nachieve certain levels of convergence accuracy. The results derived here\nsupport existing empirical observations, and we also conduct a set of\nexperiments to evaluate them.\n", "versions": [{"version": "v1", "created": "Wed, 10 Jun 2015 17:59:57 GMT"}, {"version": "v2", "created": "Tue, 17 Nov 2015 04:52:53 GMT"}, {"version": "v3", "created": "Wed, 22 Feb 2017 17:32:07 GMT"}], "update_date": "2017-02-23", "authors_parsed": [["Ithapu", "Vamsi K.", ""], ["Ravi", "Sathya", ""], ["Singh", "Vikas", ""]]}, {"id": "1506.03431", "submitter": "Alp Kucukelbir", "authors": "Alp Kucukelbir, Rajesh Ranganath, Andrew Gelman, David M. Blei", "title": "Automatic Variational Inference in Stan", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Variational inference is a scalable technique for approximate Bayesian\ninference. Deriving variational inference algorithms requires tedious\nmodel-specific calculations; this makes it difficult to automate. We propose an\nautomatic variational inference algorithm, automatic differentiation\nvariational inference (ADVI). The user only provides a Bayesian model and a\ndataset; nothing else. We make no conjugacy assumptions and support a broad\nclass of models. The algorithm automatically determines an appropriate\nvariational family and optimizes the variational objective. We implement ADVI\nin Stan (code available now), a probabilistic programming framework. We compare\nADVI to MCMC sampling across hierarchical generalized linear models,\nnonconjugate matrix factorization, and a mixture model. We train the mixture\nmodel on a quarter million images. With ADVI we can use variational inference\non any model we write in Stan.\n", "versions": [{"version": "v1", "created": "Wed, 10 Jun 2015 19:19:59 GMT"}, {"version": "v2", "created": "Fri, 12 Jun 2015 18:17:55 GMT"}], "update_date": "2015-06-15", "authors_parsed": [["Kucukelbir", "Alp", ""], ["Ranganath", "Rajesh", ""], ["Gelman", "Andrew", ""], ["Blei", "David M.", ""]]}, {"id": "1506.03478", "submitter": "Lucas Theis", "authors": "Lucas Theis and Matthias Bethge", "title": "Generative Image Modeling Using Spatial LSTMs", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Modeling the distribution of natural images is challenging, partly because of\nstrong statistical dependencies which can extend over hundreds of pixels.\nRecurrent neural networks have been successful in capturing long-range\ndependencies in a number of problems but only recently have found their way\ninto generative image models. We here introduce a recurrent image model based\non multi-dimensional long short-term memory units which are particularly suited\nfor image modeling due to their spatial structure. Our model scales to images\nof arbitrary size and its likelihood is computationally tractable. We find that\nit outperforms the state of the art in quantitative comparisons on several\nimage datasets and produces promising results when used for texture synthesis\nand inpainting.\n", "versions": [{"version": "v1", "created": "Wed, 10 Jun 2015 20:56:14 GMT"}, {"version": "v2", "created": "Fri, 18 Sep 2015 08:06:06 GMT"}], "update_date": "2015-09-21", "authors_parsed": [["Theis", "Lucas", ""], ["Bethge", "Matthias", ""]]}, {"id": "1506.03486", "submitter": "Akshay Balsubramani", "authors": "Akshay Balsubramani, Aaditya Ramdas", "title": "Sequential Nonparametric Testing with the Law of the Iterated Logarithm", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG math.ST stat.ME stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a new algorithmic framework for sequential hypothesis testing with\ni.i.d. data, which includes A/B testing, nonparametric two-sample testing, and\nindependence testing as special cases. It is novel in several ways: (a) it\ntakes linear time and constant space to compute on the fly, (b) it has the same\npower guarantee as a non-sequential version of the test with the same\ncomputational constraints up to a small factor, and (c) it accesses only as\nmany samples as are required - its stopping time adapts to the unknown\ndifficulty of the problem. All our test statistics are constructed to be\nzero-mean martingales under the null hypothesis, and the rejection threshold is\ngoverned by a uniform non-asymptotic law of the iterated logarithm (LIL). For\nthe case of nonparametric two-sample mean testing, we also provide a finite\nsample power analysis, and the first non-asymptotic stopping time calculations\nfor this class of problems. We verify our predictions for type I and II errors\nand stopping times using simulations.\n", "versions": [{"version": "v1", "created": "Wed, 10 Jun 2015 21:28:38 GMT"}, {"version": "v2", "created": "Wed, 2 Mar 2016 02:11:19 GMT"}], "update_date": "2016-03-03", "authors_parsed": [["Balsubramani", "Akshay", ""], ["Ramdas", "Aaditya", ""]]}, {"id": "1506.03489", "submitter": "Rachel Cummings", "authors": "Rachel Cummings, Stratis Ioannidis, and Katrina Ligett", "title": "Truthful Linear Regression", "comments": "To appear in Proceedings of the 28th Annual Conference on Learning\n  Theory (COLT 2015)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.GT cs.DS stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider the problem of fitting a linear model to data held by individuals\nwho are concerned about their privacy. Incentivizing most players to truthfully\nreport their data to the analyst constrains our design to mechanisms that\nprovide a privacy guarantee to the participants; we use differential privacy to\nmodel individuals' privacy losses. This immediately poses a problem, as\ndifferentially private computation of a linear model necessarily produces a\nbiased estimation, and existing approaches to design mechanisms to elicit data\nfrom privacy-sensitive individuals do not generalize well to biased estimators.\nWe overcome this challenge through an appropriate design of the computation and\npayment scheme.\n", "versions": [{"version": "v1", "created": "Wed, 10 Jun 2015 21:39:44 GMT"}], "update_date": "2015-06-12", "authors_parsed": [["Cummings", "Rachel", ""], ["Ioannidis", "Stratis", ""], ["Ligett", "Katrina", ""]]}, {"id": "1506.03493", "submitter": "Aaron Schein", "authors": "Aaron Schein, John Paisley, David M. Blei, Hanna Wallach", "title": "Bayesian Poisson Tensor Factorization for Inferring Multilateral\n  Relations from Sparse Dyadic Event Counts", "comments": "To appear in Proceedings of the 21st ACM SIGKDD Conference of\n  Knowledge Discovery and Data Mining (KDD 2015)", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.AI cs.LG cs.SI stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a Bayesian tensor factorization model for inferring latent group\nstructures from dynamic pairwise interaction patterns. For decades, political\nscientists have collected and analyzed records of the form \"country $i$ took\naction $a$ toward country $j$ at time $t$\"---known as dyadic events---in order\nto form and test theories of international relations. We represent these event\ndata as a tensor of counts and develop Bayesian Poisson tensor factorization to\ninfer a low-dimensional, interpretable representation of their salient\npatterns. We demonstrate that our model's predictive performance is better than\nthat of standard non-negative tensor factorization methods. We also provide a\ncomparison of our variational updates to their maximum likelihood counterparts.\nIn doing so, we identify a better way to form point estimates of the latent\nfactors than that typically used in Bayesian Poisson matrix factorization.\nFinally, we showcase our model as an exploratory analysis tool for political\nscientists. We show that the inferred latent factor matrices capture\ninterpretable multilateral relations that both conform to and inform our\nknowledge of international affairs.\n", "versions": [{"version": "v1", "created": "Wed, 10 Jun 2015 21:49:31 GMT"}], "update_date": "2015-06-12", "authors_parsed": [["Schein", "Aaron", ""], ["Paisley", "John", ""], ["Blei", "David M.", ""], ["Wallach", "Hanna", ""]]}, {"id": "1506.03498", "submitter": "Alaa Saade", "authors": "Alaa Saade, Florent Krzakala and Lenka Zdeborov\\'a", "title": "Matrix Completion from Fewer Entries: Spectral Detectability and Rank\n  Estimation", "comments": "NIPS Conference 2015", "journal-ref": "Advances in Neural Information Processing Systems (NIPS 2015) 28,\n  pages 1261--1269", "doi": null, "report-no": null, "categories": "cond-mat.dis-nn cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The completion of low rank matrices from few entries is a task with many\npractical applications. We consider here two aspects of this problem:\ndetectability, i.e. the ability to estimate the rank $r$ reliably from the\nfewest possible random entries, and performance in achieving small\nreconstruction error. We propose a spectral algorithm for these two tasks\ncalled MaCBetH (for Matrix Completion with the Bethe Hessian). The rank is\nestimated as the number of negative eigenvalues of the Bethe Hessian matrix,\nand the corresponding eigenvectors are used as initial condition for the\nminimization of the discrepancy between the estimated matrix and the revealed\nentries. We analyze the performance in a random matrix setting using results\nfrom the statistical mechanics of the Hopfield neural network, and show in\nparticular that MaCBetH efficiently detects the rank $r$ of a large $n\\times m$\nmatrix from $C(r)r\\sqrt{nm}$ entries, where $C(r)$ is a constant close to $1$.\nWe also evaluate the corresponding root-mean-square error empirically and show\nthat MaCBetH compares favorably to other existing approaches.\n", "versions": [{"version": "v1", "created": "Wed, 10 Jun 2015 22:46:02 GMT"}, {"version": "v2", "created": "Tue, 30 Jun 2015 17:15:13 GMT"}, {"version": "v3", "created": "Thu, 28 Jan 2016 10:16:56 GMT"}], "update_date": "2016-02-11", "authors_parsed": [["Saade", "Alaa", ""], ["Krzakala", "Florent", ""], ["Zdeborov\u00e1", "Lenka", ""]]}, {"id": "1506.03504", "submitter": "Philip Bachman", "authors": "Philip Bachman and Doina Precup", "title": "Data Generation as Sequential Decision Making", "comments": "Accepted for publication at Advances in Neural Information Processing\n  Systems (NIPS) 2015", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We connect a broad class of generative models through their shared reliance\non sequential decision making. Motivated by this view, we develop extensions to\nan existing model, and then explore the idea further in the context of data\nimputation -- perhaps the simplest setting in which to investigate the relation\nbetween unconditional and conditional generative modelling. We formulate data\nimputation as an MDP and develop models capable of representing effective\npolicies for it. We construct the models using neural networks and train them\nusing a form of guided policy search. Our models generate predictions through\nan iterative process of feedback and refinement. We show that this approach can\nlearn effective policies for imputation problems of varying difficulty and\nacross multiple datasets.\n", "versions": [{"version": "v1", "created": "Wed, 10 Jun 2015 23:17:24 GMT"}, {"version": "v2", "created": "Sun, 1 Nov 2015 00:31:11 GMT"}, {"version": "v3", "created": "Tue, 3 Nov 2015 01:16:31 GMT"}], "update_date": "2015-11-04", "authors_parsed": [["Bachman", "Philip", ""], ["Precup", "Doina", ""]]}, {"id": "1506.03509", "submitter": "Furong Huang", "authors": "Furong Huang, Animashree Anandkumar", "title": "Convolutional Dictionary Learning through Tensor Factorization", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Tensor methods have emerged as a powerful paradigm for consistent learning of\nmany latent variable models such as topic models, independent component\nanalysis and dictionary learning. Model parameters are estimated via CP\ndecomposition of the observed higher order input moments. However, in many\ndomains, additional invariances such as shift invariances exist, enforced via\nmodels such as convolutional dictionary learning. In this paper, we develop\nnovel tensor decomposition algorithms for parameter estimation of convolutional\nmodels. Our algorithm is based on the popular alternating least squares method,\nbut with efficient projections onto the space of stacked circulant matrices.\nOur method is embarrassingly parallel and consists of simple operations such as\nfast Fourier transforms and matrix multiplications. Our algorithm converges to\nthe dictionary much faster and more accurately compared to the alternating\nminimization over filters and activation maps.\n", "versions": [{"version": "v1", "created": "Wed, 10 Jun 2015 23:48:18 GMT"}, {"version": "v2", "created": "Fri, 12 Jun 2015 22:46:18 GMT"}, {"version": "v3", "created": "Thu, 18 Jun 2015 21:04:28 GMT"}], "update_date": "2015-06-22", "authors_parsed": [["Huang", "Furong", ""], ["Anandkumar", "Animashree", ""]]}, {"id": "1506.03521", "submitter": "Samet Oymak", "authors": "Samet Oymak, Benjamin Recht, Mahdi Soltanolkotabi", "title": "Isometric sketching of any set via the Restricted Isometry Property", "comments": "17 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IT cs.DS math.IT math.PR math.ST stat.ML stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper we show that for the purposes of dimensionality reduction\ncertain class of structured random matrices behave similarly to random Gaussian\nmatrices. This class includes several matrices for which matrix-vector multiply\ncan be computed in log-linear time, providing efficient dimensionality\nreduction of general sets. In particular, we show that using such matrices any\nset from high dimensions can be embedded into lower dimensions with near\noptimal distortion. We obtain our results by connecting dimensionality\nreduction of any set to dimensionality reduction of sparse vectors via a\nchaining argument.\n", "versions": [{"version": "v1", "created": "Thu, 11 Jun 2015 00:49:51 GMT"}, {"version": "v2", "created": "Tue, 6 Oct 2015 20:09:41 GMT"}], "update_date": "2015-10-08", "authors_parsed": [["Oymak", "Samet", ""], ["Recht", "Benjamin", ""], ["Soltanolkotabi", "Mahdi", ""]]}, {"id": "1506.03620", "submitter": "Martin Genzel", "authors": "Tim Conrad, Martin Genzel, Nada Cvetkovic, Niklas Wulkow, Alexander\n  Leichtle, Jan Vybiral, Gitta Kutyniok, Christof Sch\\\"utte", "title": "Sparse Proteomics Analysis - A compressed sensing-based approach for\n  feature selection and classification of high-dimensional proteomics mass\n  spectrometry data", "comments": null, "journal-ref": "BMC Bioinform. 18 (2017), 160", "doi": "10.1186/s12859-017-1565-4", "report-no": null, "categories": "math.ST stat.ML stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Background: High-throughput proteomics techniques, such as mass spectrometry\n(MS)-based approaches, produce very high-dimensional data-sets. In a clinical\nsetting one is often interested in how mass spectra differ between patients of\ndifferent classes, for example spectra from healthy patients vs. spectra from\npatients having a particular disease. Machine learning algorithms are needed to\n(a) identify these discriminating features and (b) classify unknown spectra\nbased on this feature set. Since the acquired data is usually noisy, the\nalgorithms should be robust against noise and outliers, while the identified\nfeature set should be as small as possible.\n  Results: We present a new algorithm, Sparse Proteomics Analysis (SPA), based\non the theory of compressed sensing that allows us to identify a minimal\ndiscriminating set of features from mass spectrometry data-sets. We show (1)\nhow our method performs on artificial and real-world data-sets, (2) that its\nperformance is competitive with standard (and widely used) algorithms for\nanalyzing proteomics data, and (3) that it is robust against random and\nsystematic noise. We further demonstrate the applicability of our algorithm to\ntwo previously published clinical data-sets.\n", "versions": [{"version": "v1", "created": "Thu, 11 Jun 2015 10:55:49 GMT"}, {"version": "v2", "created": "Tue, 6 Sep 2016 09:10:55 GMT"}, {"version": "v3", "created": "Sat, 26 Nov 2016 09:10:24 GMT"}], "update_date": "2017-06-14", "authors_parsed": [["Conrad", "Tim", ""], ["Genzel", "Martin", ""], ["Cvetkovic", "Nada", ""], ["Wulkow", "Niklas", ""], ["Leichtle", "Alexander", ""], ["Vybiral", "Jan", ""], ["Kutyniok", "Gitta", ""], ["Sch\u00fctte", "Christof", ""]]}, {"id": "1506.03662", "submitter": "Simon Lacoste-Julien", "authors": "Thomas Hofmann, Aurelien Lucchi, Simon Lacoste-Julien, Brian\n  McWilliams", "title": "Variance Reduced Stochastic Gradient Descent with Neighbors", "comments": "Appears in: Advances in Neural Information Processing Systems 28\n  (NIPS 2015). 13 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG math.OC stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Stochastic Gradient Descent (SGD) is a workhorse in machine learning, yet its\nslow convergence can be a computational bottleneck. Variance reduction\ntechniques such as SAG, SVRG and SAGA have been proposed to overcome this\nweakness, achieving linear convergence. However, these methods are either based\non computations of full gradients at pivot points, or on keeping per data point\ncorrections in memory. Therefore speed-ups relative to SGD may need a minimal\nnumber of epochs in order to materialize. This paper investigates algorithms\nthat can exploit neighborhood structure in the training data to share and\nre-use information about past stochastic gradients across data points, which\noffers advantages in the transient optimization phase. As a side-product we\nprovide a unified convergence analysis for a family of variance reduction\nalgorithms, which we call memorization algorithms. We provide experimental\nresults supporting our theory.\n", "versions": [{"version": "v1", "created": "Thu, 11 Jun 2015 13:14:33 GMT"}, {"version": "v2", "created": "Thu, 5 Nov 2015 12:30:49 GMT"}, {"version": "v3", "created": "Tue, 17 Nov 2015 22:00:11 GMT"}, {"version": "v4", "created": "Fri, 26 Feb 2016 19:55:56 GMT"}], "update_date": "2016-02-29", "authors_parsed": [["Hofmann", "Thomas", ""], ["Lucchi", "Aurelien", ""], ["Lacoste-Julien", "Simon", ""], ["McWilliams", "Brian", ""]]}, {"id": "1506.03693", "submitter": "Edward Meeds", "authors": "Edward Meeds and Max Welling", "title": "Optimization Monte Carlo: Efficient and Embarrassingly Parallel\n  Likelihood-Free Inference", "comments": "NIPS 2015 camera ready", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We describe an embarrassingly parallel, anytime Monte Carlo method for\nlikelihood-free models. The algorithm starts with the view that the\nstochasticity of the pseudo-samples generated by the simulator can be\ncontrolled externally by a vector of random numbers u, in such a way that the\noutcome, knowing u, is deterministic. For each instantiation of u we run an\noptimization procedure to minimize the distance between summary statistics of\nthe simulator and the data. After reweighing these samples using the prior and\nthe Jacobian (accounting for the change of volume in transforming from the\nspace of summary statistics to the space of parameters) we show that this\nweighted ensemble represents a Monte Carlo estimate of the posterior\ndistribution. The procedure can be run embarrassingly parallel (each node\nhandling one sample) and anytime (by allocating resources to the worst\nperforming sample). The procedure is validated on six experiments.\n", "versions": [{"version": "v1", "created": "Thu, 11 Jun 2015 14:45:30 GMT"}, {"version": "v2", "created": "Wed, 2 Dec 2015 18:58:09 GMT"}], "update_date": "2015-12-03", "authors_parsed": [["Meeds", "Edward", ""], ["Welling", "Max", ""]]}, {"id": "1506.03705", "submitter": "Youssef  Mroueh", "authors": "Youssef Mroueh, Steven Rennie, Vaibhava Goel", "title": "Random Maxout Features", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we propose and study random maxout features, which are\nconstructed by first projecting the input data onto sets of randomly generated\nvectors with Gaussian elements, and then outputing the maximum projection value\nfor each set. We show that the resulting random feature map, when used in\nconjunction with linear models, allows for the locally linear estimation of the\nfunction of interest in classification tasks, and for the locally linear\nembedding of points when used for dimensionality reduction or data\nvisualization. We derive generalization bounds for learning that assess the\nerror in approximating locally linear functions by linear functions in the\nmaxout feature space, and empirically evaluate the efficacy of the approach on\nthe MNIST and TIMIT classification tasks.\n", "versions": [{"version": "v1", "created": "Thu, 11 Jun 2015 15:19:47 GMT"}, {"version": "v2", "created": "Fri, 12 Jun 2015 19:18:28 GMT"}], "update_date": "2015-06-15", "authors_parsed": [["Mroueh", "Youssef", ""], ["Rennie", "Steven", ""], ["Goel", "Vaibhava", ""]]}, {"id": "1506.03736", "submitter": "Joseph  Salmon", "authors": "Eugene Ndiaye, Olivier Fercoq, Alexandre Gramfort, Joseph Salmon", "title": "GAP Safe screening rules for sparse multi-task and multi-class models", "comments": "in Proceedings of the 29-th Conference on Neural Information\n  Processing Systems (NIPS), 2015", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG math.OC stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  High dimensional regression benefits from sparsity promoting regularizations.\nScreening rules leverage the known sparsity of the solution by ignoring some\nvariables in the optimization, hence speeding up solvers. When the procedure is\nproven not to discard features wrongly the rules are said to be \\emph{safe}. In\nthis paper we derive new safe rules for generalized linear models regularized\nwith $\\ell_1$ and $\\ell_1/\\ell_2$ norms. The rules are based on duality gap\ncomputations and spherical safe regions whose diameters converge to zero. This\nallows to discard safely more variables, in particular for low regularization\nparameters. The GAP Safe rule can cope with any iterative solver and we\nillustrate its performance on coordinate descent for multi-task Lasso, binary\nand multinomial logistic regression, demonstrating significant speed ups on all\ntested datasets with respect to previous safe rules.\n", "versions": [{"version": "v1", "created": "Thu, 11 Jun 2015 16:25:36 GMT"}, {"version": "v2", "created": "Wed, 18 Nov 2015 10:07:20 GMT"}], "update_date": "2015-11-19", "authors_parsed": [["Ndiaye", "Eugene", ""], ["Fercoq", "Olivier", ""], ["Gramfort", "Alexandre", ""], ["Salmon", "Joseph", ""]]}, {"id": "1506.03762", "submitter": "Thibaut Le Gouic", "authors": "Thibaut Le Gouic (I2M, CS-HSE)", "title": "Recovering metric from full ordinal information", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Given a geodesic space (E, d), we show that full ordinal knowledge on the\nmetric d-i.e. knowledge of the function D d : (w, x, y, z) $\\rightarrow$ 1\nd(w,x)$\\le$d(y,z) , determines uniquely-up to a constant factor-the metric d.\nFor a subspace En of n points of E, converging in Hausdorff distance to E, we\nconstruct a metric dn on En, based only on the knowledge of D d on En and\nestablish a sharp upper bound of the Gromov-Hausdorff distance between (En, dn)\nand (E, d).\n", "versions": [{"version": "v1", "created": "Thu, 11 Jun 2015 18:11:48 GMT"}, {"version": "v2", "created": "Fri, 12 Jun 2015 08:11:29 GMT"}, {"version": "v3", "created": "Fri, 12 Feb 2016 19:12:46 GMT"}, {"version": "v4", "created": "Sat, 29 Dec 2018 17:50:38 GMT"}], "update_date": "2019-01-01", "authors_parsed": [["Gouic", "Thibaut Le", "", "I2M, CS-HSE"]]}, {"id": "1506.03767", "submitter": "Oren Rippel", "authors": "Oren Rippel, Jasper Snoek and Ryan P. Adams", "title": "Spectral Representations for Convolutional Neural Networks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Discrete Fourier transforms provide a significant speedup in the computation\nof convolutions in deep learning. In this work, we demonstrate that, beyond its\nadvantages for efficient computation, the spectral domain also provides a\npowerful representation in which to model and train convolutional neural\nnetworks (CNNs).\n  We employ spectral representations to introduce a number of innovations to\nCNN design. First, we propose spectral pooling, which performs dimensionality\nreduction by truncating the representation in the frequency domain. This\napproach preserves considerably more information per parameter than other\npooling strategies and enables flexibility in the choice of pooling output\ndimensionality. This representation also enables a new form of stochastic\nregularization by randomized modification of resolution. We show that these\nmethods achieve competitive results on classification and approximation tasks,\nwithout using any dropout or max-pooling.\n  Finally, we demonstrate the effectiveness of complex-coefficient spectral\nparameterization of convolutional filters. While this leaves the underlying\nmodel unchanged, it results in a representation that greatly facilitates\noptimization. We observe on a variety of popular CNN configurations that this\nleads to significantly faster convergence during training.\n", "versions": [{"version": "v1", "created": "Thu, 11 Jun 2015 18:23:18 GMT"}], "update_date": "2015-06-12", "authors_parsed": [["Rippel", "Oren", ""], ["Snoek", "Jasper", ""], ["Adams", "Ryan P.", ""]]}, {"id": "1506.03768", "submitter": "Ye Wang", "authors": "Ye Wang, David B. Dunson", "title": "Probabilistic Curve Learning: Coulomb Repulsion and the Electrostatic\n  Gaussian Process", "comments": "16 pages, 6 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Learning of low dimensional structure in multidimensional data is a canonical\nproblem in machine learning. One common approach is to suppose that the\nobserved data are close to a lower-dimensional smooth manifold. There are a\nrich variety of manifold learning methods available, which allow mapping of\ndata points to the manifold. However, there is a clear lack of probabilistic\nmethods that allow learning of the manifold along with the generative\ndistribution of the observed data. The best attempt is the Gaussian process\nlatent variable model (GP-LVM), but identifiability issues lead to poor\nperformance. We solve these issues by proposing a novel Coulomb repulsive\nprocess (Corp) for locations of points on the manifold, inspired by physical\nmodels of electrostatic interactions among particles. Combining this process\nwith a GP prior for the mapping function yields a novel electrostatic GP\n(electroGP) process. Focusing on the simple case of a one-dimensional manifold,\nwe develop efficient inference algorithms, and illustrate substantially\nimproved performance in a variety of experiments including filling in missing\nframes in video.\n", "versions": [{"version": "v1", "created": "Thu, 11 Jun 2015 18:27:19 GMT"}], "update_date": "2015-06-12", "authors_parsed": [["Wang", "Ye", ""], ["Dunson", "David B.", ""]]}, {"id": "1506.03784", "submitter": "M{\\aa}ns Magnusson", "authors": "M{\\aa}ns Magnusson, Leif Jonsson, Mattias Villani, David Broman", "title": "Sparse Partially Collapsed MCMC for Parallel Inference in Topic Models", "comments": "Accepted for publication in Journal of Computational and Graphical\n  Statistics", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Topic models, and more specifically the class of Latent Dirichlet Allocation\n(LDA), are widely used for probabilistic modeling of text. MCMC sampling from\nthe posterior distribution is typically performed using a collapsed Gibbs\nsampler. We propose a parallel sparse partially collapsed Gibbs sampler and\ncompare its speed and efficiency to state-of-the-art samplers for topic models\non five well-known text corpora of differing sizes and properties. In\nparticular, we propose and compare two different strategies for sampling the\nparameter block with latent topic indicators. The experiments show that the\nincrease in statistical inefficiency from only partial collapsing is smaller\nthan commonly assumed, and can be more than compensated by the speedup from\nparallelization and sparsity on larger corpora. We also prove that the\npartially collapsed samplers scale well with the size of the corpus. The\nproposed algorithm is fast, efficient, exact, and can be used in more modeling\nsituations than the ordinary collapsed sampler.\n", "versions": [{"version": "v1", "created": "Thu, 11 Jun 2015 19:16:01 GMT"}, {"version": "v2", "created": "Wed, 19 Oct 2016 21:22:53 GMT"}, {"version": "v3", "created": "Tue, 15 Aug 2017 05:36:07 GMT"}], "update_date": "2017-08-16", "authors_parsed": [["Magnusson", "M\u00e5ns", ""], ["Jonsson", "Leif", ""], ["Villani", "Mattias", ""], ["Broman", "David", ""]]}, {"id": "1506.03805", "submitter": "Balaji Lakshminarayanan", "authors": "Balaji Lakshminarayanan, Daniel M. Roy and Yee Whye Teh", "title": "Mondrian Forests for Large-Scale Regression when Uncertainty Matters", "comments": "Proceedings of the 19th International Conference on Artificial\n  Intelligence and Statistics (AISTATS) 2016, Cadiz, Spain. JMLR: W&CP volume\n  51", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Many real-world regression problems demand a measure of the uncertainty\nassociated with each prediction. Standard decision forests deliver efficient\nstate-of-the-art predictive performance, but high-quality uncertainty estimates\nare lacking. Gaussian processes (GPs) deliver uncertainty estimates, but\nscaling GPs to large-scale data sets comes at the cost of approximating the\nuncertainty estimates. We extend Mondrian forests, first proposed by\nLakshminarayanan et al. (2014) for classification problems, to the large-scale\nnon-parametric regression setting. Using a novel hierarchical Gaussian prior\nthat dovetails with the Mondrian forest framework, we obtain principled\nuncertainty estimates, while still retaining the computational advantages of\ndecision forests. Through a combination of illustrative examples, real-world\nlarge-scale datasets, and Bayesian optimization benchmarks, we demonstrate that\nMondrian forests outperform approximate GPs on large-scale regression tasks and\ndeliver better-calibrated uncertainty assessments than decision-forest-based\nmethods.\n", "versions": [{"version": "v1", "created": "Thu, 11 Jun 2015 19:55:02 GMT"}, {"version": "v2", "created": "Thu, 15 Oct 2015 18:10:07 GMT"}, {"version": "v3", "created": "Wed, 20 Apr 2016 11:43:13 GMT"}, {"version": "v4", "created": "Fri, 27 May 2016 11:15:55 GMT"}], "update_date": "2016-05-30", "authors_parsed": [["Lakshminarayanan", "Balaji", ""], ["Roy", "Daniel M.", ""], ["Teh", "Yee Whye", ""]]}, {"id": "1506.03850", "submitter": "Trevor Hastie", "authors": "Alexandra Chouldechova and Trevor Hastie", "title": "Generalized Additive Model Selection", "comments": "23 pages, 10 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We introduce GAMSEL (Generalized Additive Model Selection), a penalized\nlikelihood approach for fitting sparse generalized additive models in high\ndimension. Our method interpolates between null, linear and additive models by\nallowing the effect of each variable to be estimated as being either zero,\nlinear, or a low-complexity curve, as determined by the data. We present a\nblockwise coordinate descent procedure for efficiently optimizing the penalized\nlikelihood objective over a dense grid of the tuning parameter, producing a\nregularization path of additive models. We demonstrate the performance of our\nmethod on both real and simulated data examples, and compare it with existing\ntechniques for additive model selection.\n", "versions": [{"version": "v1", "created": "Thu, 11 Jun 2015 21:47:52 GMT"}, {"version": "v2", "created": "Wed, 17 Jun 2015 01:16:23 GMT"}], "update_date": "2015-06-18", "authors_parsed": [["Chouldechova", "Alexandra", ""], ["Hastie", "Trevor", ""]]}, {"id": "1506.03852", "submitter": "Shell Hu", "authors": "Shell X. Hu, Christopher K. I. Williams and Sinisa Todorovic", "title": "Tree-Cut for Probabilistic Image Segmentation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper presents a new probabilistic generative model for image\nsegmentation, i.e. the task of partitioning an image into homogeneous regions.\nOur model is grounded on a mid-level image representation, called a region\ntree, in which regions are recursively split into subregions until superpixels\nare reached. Given the region tree, image segmentation is formalized as\nsampling cuts in the tree from the model. Inference for the cuts is exact, and\nformulated using dynamic programming. Our tree-cut model can be tuned to sample\nsegmentations at a particular scale of interest out of many possible multiscale\nimage segmentations. This generalizes the common notion that there should be\nonly one correct segmentation per image. Also, it allows moving beyond the\nstandard single-scale evaluation, where the segmentation result for an image is\naveraged against the corresponding set of coarse and fine human annotations, to\nconduct a scale-specific evaluation. Our quantitative results are comparable to\nthose of the leading gPb-owt-ucm method, with the notable advantage that we\nadditionally produce a distribution over all possible tree-consistent\nsegmentations of the image.\n", "versions": [{"version": "v1", "created": "Thu, 11 Jun 2015 21:55:06 GMT"}], "update_date": "2015-06-15", "authors_parsed": [["Hu", "Shell X.", ""], ["Williams", "Christopher K. I.", ""], ["Todorovic", "Sinisa", ""]]}, {"id": "1506.03877", "submitter": "J\\\"org Bornschein", "authors": "Jorg Bornschein and Samira Shabanian and Asja Fischer and Yoshua\n  Bengio", "title": "Bidirectional Helmholtz Machines", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Efficient unsupervised training and inference in deep generative models\nremains a challenging problem. One basic approach, called Helmholtz machine,\ninvolves training a top-down directed generative model together with a\nbottom-up auxiliary model used for approximate inference. Recent results\nindicate that better generative models can be obtained with better approximate\ninference procedures. Instead of improving the inference procedure, we here\npropose a new model which guarantees that the top-down and bottom-up\ndistributions can efficiently invert each other. We achieve this by\ninterpreting both the top-down and the bottom-up directed models as approximate\ninference distributions and by defining the model distribution to be the\ngeometric mean of these two. We present a lower-bound for the likelihood of\nthis model and we show that optimizing this bound regularizes the model so that\nthe Bhattacharyya distance between the bottom-up and top-down approximate\ndistributions is minimized. This approach results in state of the art\ngenerative models which prefer significantly deeper architectures while it\nallows for orders of magnitude more efficient approximate inference.\n", "versions": [{"version": "v1", "created": "Fri, 12 Jun 2015 00:08:20 GMT"}, {"version": "v2", "created": "Tue, 7 Jul 2015 19:08:07 GMT"}, {"version": "v3", "created": "Fri, 20 Nov 2015 18:48:00 GMT"}, {"version": "v4", "created": "Mon, 4 Jan 2016 00:07:47 GMT"}, {"version": "v5", "created": "Wed, 25 May 2016 02:54:26 GMT"}], "update_date": "2016-05-26", "authors_parsed": [["Bornschein", "Jorg", ""], ["Shabanian", "Samira", ""], ["Fischer", "Asja", ""], ["Bengio", "Yoshua", ""]]}, {"id": "1506.03880", "submitter": "Ciar\\'an M. Lee", "authors": "Ciar\\'an M. Lee, Robert W. Spekkens", "title": "Causal inference via algebraic geometry: feasibility tests for\n  functional causal structures with two binary observed variables", "comments": "Accepted for publication in Journal of Causal Inference. Revised and\n  updated in response to referee feedback. 16+5 pages, 26+2 figures. Comments\n  welcome", "journal-ref": "Journal of Causal Inference, Volume 5, Issue 2, 2017", "doi": "10.1515/jci-2016-0013", "report-no": null, "categories": "stat.ML quant-ph", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We provide a scheme for inferring causal relations from uncontrolled\nstatistical data based on tools from computational algebraic geometry, in\nparticular, the computation of Groebner bases. We focus on causal structures\ncontaining just two observed variables, each of which is binary. We consider\nthe consequences of imposing different restrictions on the number and\ncardinality of latent variables and of assuming different functional\ndependences of the observed variables on the latent ones (in particular, the\nnoise need not be additive). We provide an inductive scheme for classifying\nfunctional causal structures into distinct observational equivalence classes.\nFor each observational equivalence class, we provide a procedure for deriving\nconstraints on the joint distribution that are necessary and sufficient\nconditions for it to arise from a model in that class. We also demonstrate how\nthis sort of approach provides a means of determining which causal parameters\nare identifiable and how to solve for these. Prospects for expanding the scope\nof our scheme, in particular to the problem of quantum causal inference, are\nalso discussed.\n", "versions": [{"version": "v1", "created": "Fri, 12 Jun 2015 00:51:44 GMT"}, {"version": "v2", "created": "Sat, 18 Feb 2017 18:53:59 GMT"}], "update_date": "2017-10-18", "authors_parsed": [["Lee", "Ciar\u00e1n M.", ""], ["Spekkens", "Robert W.", ""]]}, {"id": "1506.03942", "submitter": "Longfei Lu", "authors": "Longfei Lu", "title": "Optimal $\\gamma$ and $C$ for $\\epsilon$-Support Vector Regression with\n  RBF Kernels", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The objective of this study is to investigate the efficient determination of\n$C$ and $\\gamma$ for Support Vector Regression with RBF or mahalanobis kernel\nbased on numerical and statistician considerations, which indicates the\nconnection between $C$ and kernels and demonstrates that the deviation of\ngeometric distance of neighbour observation in mapped space effects the predict\naccuracy of $\\epsilon$-SVR. We determinate the arrange of $\\gamma$ & $C$ and\npropose our method to choose their best values.\n", "versions": [{"version": "v1", "created": "Fri, 12 Jun 2015 09:03:50 GMT"}], "update_date": "2015-06-15", "authors_parsed": [["Lu", "Longfei", ""]]}, {"id": "1506.03958", "submitter": "Clemens Hage", "authors": "Clemens Hage and Martin Kleinsteuber", "title": "Robust Structured Low-Rank Approximation on the Grassmannian", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Over the past years Robust PCA has been established as a standard tool for\nreliable low-rank approximation of matrices in the presence of outliers.\nRecently, the Robust PCA approach via nuclear norm minimization has been\nextended to matrices with linear structures which appear in applications such\nas system identification and data series analysis. At the same time it has been\nshown how to control the rank of a structured approximation via matrix\nfactorization approaches. The drawbacks of these methods either lie in the lack\nof robustness against outliers or in their static nature of repeated\nbatch-processing. We present a Robust Structured Low-Rank Approximation method\non the Grassmannian that on the one hand allows for fast re-initialization in\nan online setting due to subspace identification with manifolds, and that is\nrobust against outliers due to a smooth approximation of the $\\ell_p$-norm cost\nfunction on the other hand. The method is evaluated in online time series\nforecasting tasks on simulated and real-world data.\n", "versions": [{"version": "v1", "created": "Fri, 12 Jun 2015 09:48:04 GMT"}], "update_date": "2015-06-15", "authors_parsed": [["Hage", "Clemens", ""], ["Kleinsteuber", "Martin", ""]]}, {"id": "1506.04000", "submitter": "Maurizio Filippone", "authors": "James Hensman, Alexander G. de G. Matthews, Maurizio Filippone, Zoubin\n  Ghahramani", "title": "MCMC for Variationally Sparse Gaussian Processes", "comments": "16 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Gaussian process (GP) models form a core part of probabilistic machine\nlearning. Considerable research effort has been made into attacking three\nissues with GP models: how to compute efficiently when the number of data is\nlarge; how to approximate the posterior when the likelihood is not Gaussian and\nhow to estimate covariance function parameter posteriors. This paper\nsimultaneously addresses these, using a variational approximation to the\nposterior which is sparse in support of the function but otherwise free-form.\nThe result is a Hybrid Monte-Carlo sampling scheme which allows for a\nnon-Gaussian approximation over the function values and covariance parameters\nsimultaneously, with efficient computations based on inducing-point sparse GPs.\nCode to replicate each experiment in this paper will be available shortly.\n", "versions": [{"version": "v1", "created": "Fri, 12 Jun 2015 12:24:35 GMT"}], "update_date": "2015-06-15", "authors_parsed": [["Hensman", "James", ""], ["Matthews", "Alexander G. de G.", ""], ["Filippone", "Maurizio", ""], ["Ghahramani", "Zoubin", ""]]}, {"id": "1506.04088", "submitter": "Ryan Giordano", "authors": "Ryan Giordano, Tamara Broderick, Michael Jordan", "title": "Linear Response Methods for Accurate Covariance Estimates from Mean\n  Field Variational Bayes", "comments": "21 pages. arXiv admin note: substantial text overlap with\n  arXiv:1502.07685", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Mean field variational Bayes (MFVB) is a popular posterior approximation\nmethod due to its fast runtime on large-scale data sets. However, it is well\nknown that a major failing of MFVB is that it underestimates the uncertainty of\nmodel variables (sometimes severely) and provides no information about model\nvariable covariance.\n  We generalize linear response methods from statistical physics to deliver\naccurate uncertainty estimates for model variables---both for individual\nvariables and coherently across variables. We call our method linear response\nvariational Bayes (LRVB). When the MFVB posterior approximation is in the\nexponential family, LRVB has a simple, analytic form, even for non-conjugate\nmodels. Indeed, we make no assumptions about the form of the true posterior. We\ndemonstrate the accuracy and scalability of our method on a range of models for\nboth simulated and real data.\n", "versions": [{"version": "v1", "created": "Fri, 12 Jun 2015 18:03:01 GMT"}, {"version": "v2", "created": "Wed, 23 Dec 2015 19:28:21 GMT"}], "update_date": "2015-12-24", "authors_parsed": [["Giordano", "Ryan", ""], ["Broderick", "Tamara", ""], ["Jordan", "Michael", ""]]}, {"id": "1506.04091", "submitter": "James Ridgway", "authors": "Pierre Alquier and James Ridgway and Nicolas Chopin", "title": "On the properties of variational approximations of Gibbs posteriors", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The PAC-Bayesian approach is a powerful set of techniques to derive non-\nasymptotic risk bounds for random estimators. The corresponding optimal\ndistribution of estimators, usually called the Gibbs posterior, is\nunfortunately intractable. One may sample from it using Markov chain Monte\nCarlo, but this is often too slow for big datasets. We consider instead\nvariational approximations of the Gibbs posterior, which are fast to compute.\nWe undertake a general study of the properties of such approximations. Our main\nfinding is that such a variational approximation has often the same rate of\nconvergence as the original PAC-Bayesian procedure it approximates. We\nspecialise our results to several learning tasks (classification, ranking,\nmatrix completion),discuss how to implement a variational approximation in each\ncase, and illustrate the good properties of said approximation on real\ndatasets.\n", "versions": [{"version": "v1", "created": "Fri, 12 Jun 2015 18:06:30 GMT"}, {"version": "v2", "created": "Mon, 15 Jun 2015 08:11:01 GMT"}], "update_date": "2015-06-16", "authors_parsed": [["Alquier", "Pierre", ""], ["Ridgway", "James", ""], ["Chopin", "Nicolas", ""]]}, {"id": "1506.04093", "submitter": "Zhanxing Zhu", "authors": "Zhanxing Zhu and Amos J. Storkey", "title": "Adaptive Stochastic Primal-Dual Coordinate Descent for Separable Saddle\n  Point Problems", "comments": "Accepted by ECML/PKDD2015", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider a generic convex-concave saddle point problem with separable\nstructure, a form that covers a wide-ranged machine learning applications.\nUnder this problem structure, we follow the framework of primal-dual updates\nfor saddle point problems, and incorporate stochastic block coordinate descent\nwith adaptive stepsize into this framework. We theoretically show that our\nproposal of adaptive stepsize potentially achieves a sharper linear convergence\nrate compared with the existing methods. Additionally, since we can select\n\"mini-batch\" of block coordinates to update, our method is also amenable to\nparallel processing for large-scale data. We apply the proposed method to\nregularized empirical risk minimization and show that it performs comparably\nor, more often, better than state-of-the-art methods on both synthetic and\nreal-world data sets.\n", "versions": [{"version": "v1", "created": "Fri, 12 Jun 2015 18:08:37 GMT"}], "update_date": "2015-06-15", "authors_parsed": [["Zhu", "Zhanxing", ""], ["Storkey", "Amos J.", ""]]}, {"id": "1506.04132", "submitter": "Yingzhen Li", "authors": "Yingzhen Li, Jose Miguel Hernandez-Lobato, Richard E. Turner", "title": "Stochastic Expectation Propagation", "comments": "Published at NIPS 2015. 18 pages including supplementary", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Expectation propagation (EP) is a deterministic approximation algorithm that\nis often used to perform approximate Bayesian parameter learning. EP\napproximates the full intractable posterior distribution through a set of local\napproximations that are iteratively refined for each datapoint. EP can offer\nanalytic and computational advantages over other approximations, such as\nVariational Inference (VI), and is the method of choice for a number of models.\nThe local nature of EP appears to make it an ideal candidate for performing\nBayesian learning on large models in large-scale dataset settings. However, EP\nhas a crucial limitation in this context: the number of approximating factors\nneeds to increase with the number of data-points, N, which often entails a\nprohibitively large memory overhead. This paper presents an extension to EP,\ncalled stochastic expectation propagation (SEP), that maintains a global\nposterior approximation (like VI) but updates it in a local way (like EP).\nExperiments on a number of canonical learning problems using synthetic and\nreal-world datasets indicate that SEP performs almost as well as full EP, but\nreduces the memory consumption by a factor of $N$. SEP is therefore ideally\nsuited to performing approximate Bayesian learning in the large model, large\ndataset setting.\n", "versions": [{"version": "v1", "created": "Fri, 12 Jun 2015 19:51:06 GMT"}, {"version": "v2", "created": "Wed, 18 Nov 2015 10:52:17 GMT"}], "update_date": "2015-11-19", "authors_parsed": [["Li", "Yingzhen", ""], ["Hernandez-Lobato", "Jose Miguel", ""], ["Turner", "Richard E.", ""]]}, {"id": "1506.04135", "submitter": "Fabrice Rossi", "authors": "Arnaud De Myttenaere (SAMM, Viadeo), Boris Golden (Viadeo),\n  B\\'en\\'edicte Le Grand (CRI), Fabrice Rossi (SAMM)", "title": "Reducing offline evaluation bias of collaborative filtering algorithms", "comments": "European Symposium on Artificial Neural Networks, Computational\n  Intelligence and Machine Learning (ESANN), Apr 2015, Bruges, Belgium.\n  pp.137-142, 2015, Proceedings of the 23-th European Symposium on Artificial\n  Neural Networks, Computational Intelligence and Machine Learning (ESANN 2015)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recommendation systems have been integrated into the majority of large online\nsystems to filter and rank information according to user profiles. It thus\ninfluences the way users interact with the system and, as a consequence, bias\nthe evaluation of the performance of a recommendation algorithm computed using\nhistorical data (via offline evaluation). This paper presents a new application\nof a weighted offline evaluation to reduce this bias for collaborative\nfiltering algorithms.\n", "versions": [{"version": "v1", "created": "Fri, 12 Jun 2015 19:57:27 GMT"}], "update_date": "2015-06-15", "authors_parsed": [["De Myttenaere", "Arnaud", "", "SAMM, Viadeo"], ["Golden", "Boris", "", "Viadeo"], ["Grand", "B\u00e9n\u00e9dicte Le", "", "CRI"], ["Rossi", "Fabrice", "", "SAMM"]]}, {"id": "1506.04138", "submitter": "Fabrice Rossi", "authors": "Marco Corneli (SAMM), Pierre Latouche (SAMM), Fabrice Rossi (SAMM)", "title": "Exact ICL maximization in a non-stationary time extension of the latent\n  block model for dynamic networks", "comments": "European Symposium on Artificial Neural Networks, Computational\n  Intelligence and Machine Learning (ESANN), Apr 2015, Bruges, Belgium.\n  pp.225-230, 2015, Proceedings of the 23-th European Symposium on Artificial\n  Neural Networks, Computational Intelligence and Machine Learning (ESANN 2015)", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The latent block model (LBM) is a flexible probabilistic tool to describe\ninteractions between node sets in bipartite networks, but it does not account\nfor interactions of time varying intensity between nodes in unknown classes. In\nthis paper we propose a non stationary temporal extension of the LBM that\nclusters simultaneously the two node sets of a bipartite network and constructs\nclasses of time intervals on which interactions are stationary. The number of\nclusters as well as the membership to classes are obtained by maximizing the\nexact complete-data integrated likelihood relying on a greedy search approach.\nExperiments on simulated and real data are carried out in order to assess the\nproposed methodology.\n", "versions": [{"version": "v1", "created": "Fri, 12 Jun 2015 19:59:17 GMT"}], "update_date": "2015-06-15", "authors_parsed": [["Corneli", "Marco", "", "SAMM"], ["Latouche", "Pierre", "", "SAMM"], ["Rossi", "Fabrice", "", "SAMM"]]}, {"id": "1506.04147", "submitter": "Jacob Andreas", "authors": "Jacob Andreas, Maxim Rabinovich, Dan Klein, Michael I. Jordan", "title": "On the accuracy of self-normalized log-linear models", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.CL cs.LG stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Calculation of the log-normalizer is a major computational obstacle in\napplications of log-linear models with large output spaces. The problem of fast\nnormalizer computation has therefore attracted significant attention in the\ntheoretical and applied machine learning literature. In this paper, we analyze\na recently proposed technique known as \"self-normalization\", which introduces a\nregularization term in training to penalize log normalizers for deviating from\nzero. This makes it possible to use unnormalized model scores as approximate\nprobabilities. Empirical evidence suggests that self-normalization is extremely\neffective, but a theoretical understanding of why it should work, and how\ngenerally it can be applied, is largely lacking. We prove generalization bounds\non the estimated variance of normalizers and upper bounds on the loss in\naccuracy due to self-normalization, describe classes of input distributions\nthat self-normalize easily, and construct explicit examples of high-variance\ninput distributions. Our theoretical results make predictions about the\ndifficulty of fitting self-normalized models to several classes of\ndistributions, and we conclude with empirical validation of these predictions.\n", "versions": [{"version": "v1", "created": "Fri, 12 Jun 2015 20:00:29 GMT"}, {"version": "v2", "created": "Thu, 18 Jun 2015 15:22:50 GMT"}], "update_date": "2015-06-19", "authors_parsed": [["Andreas", "Jacob", ""], ["Rabinovich", "Maxim", ""], ["Klein", "Dan", ""], ["Jordan", "Michael I.", ""]]}, {"id": "1506.04158", "submitter": "Emilie Kaufmann", "authors": "Emilie Kaufmann (SEQUEL, CRIStAL, CNRS), Thomas Bonald (LTCI, LINCS),\n  Marc Lelarge (LINCS, DYOGENE)", "title": "A Spectral Algorithm with Additive Clustering for the Recovery of\n  Overlapping Communities in Networks", "comments": "Journal of Theoretical Computer Science (TCS), Elsevier, A Para\\^itre", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper presents a novel spectral algorithm with additive clustering\ndesigned to identify overlapping communities in networks. The algorithm is\nbased on geometric properties of the spectrum of the expected adjacency matrix\nin a random graph model that we call stochastic blockmodel with overlap (SBMO).\nAn adaptive version of the algorithm, that does not require the knowledge of\nthe number of hidden communities, is proved to be consistent under the SBMO\nwhen the degrees in the graph are (slightly more than) logarithmic. The\nalgorithm is shown to perform well on simulated data and on real-world graphs\nwith known overlapping communities.\n", "versions": [{"version": "v1", "created": "Fri, 12 Jun 2015 20:08:54 GMT"}, {"version": "v2", "created": "Fri, 20 May 2016 13:12:34 GMT"}, {"version": "v3", "created": "Mon, 6 Nov 2017 07:45:58 GMT"}], "update_date": "2017-11-07", "authors_parsed": [["Kaufmann", "Emilie", "", "SEQUEL, CRIStAL, CNRS"], ["Bonald", "Thomas", "", "LTCI, LINCS"], ["Lelarge", "Marc", "", "LINCS, DYOGENE"]]}, {"id": "1506.04176", "submitter": "Fabrice Rossi", "authors": "Arnaud De Myttenaere (SAMM), Boris Golden (Viadeo), B\\'en\\'edicte Le\n  Grand (CRI), Fabrice Rossi (SAMM)", "title": "Using the Mean Absolute Percentage Error for Regression Models", "comments": "European Symposium on Artificial Neural Networks, Computational\n  Intelligence and Machine Learning (ESANN), Apr 2015, Bruges, Belgium. 2015,\n  Proceedings of the 23-th European Symposium on Artificial Neural Networks,\n  Computational Intelligence and Machine Learning (ESANN 2015)", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study in this paper the consequences of using the Mean Absolute Percentage\nError (MAPE) as a measure of quality for regression models. We show that\nfinding the best model under the MAPE is equivalent to doing weighted Mean\nAbsolute Error (MAE) regression. We show that universal consistency of\nEmpirical Risk Minimization remains possible using the MAPE instead of the MAE.\n", "versions": [{"version": "v1", "created": "Fri, 12 Jun 2015 20:38:57 GMT"}], "update_date": "2015-06-16", "authors_parsed": [["De Myttenaere", "Arnaud", "", "SAMM"], ["Golden", "Boris", "", "Viadeo"], ["Grand", "B\u00e9n\u00e9dicte Le", "", "CRI"], ["Rossi", "Fabrice", "", "SAMM"]]}, {"id": "1506.04177", "submitter": "Fabrice Rossi", "authors": "Tsirizo Rabenoro (SAMM), J\\'er\\^ome Lacaille, Marie Cottrell (SAMM),\n  Fabrice Rossi (SAMM)", "title": "Search Strategies for Binary Feature Selection for a Naive Bayes\n  Classifier", "comments": null, "journal-ref": "European Symposium on Artificial Neural Networks, Computational\n  Intelligence and Machine Learning (ESANN), Apr 2015, Bruges, Belgium.\n  pp.291-296, 2015, Proceedings of the 23-th European Symposium on Artificial\n  Neural Networks, Computational Intelligence and Machine Learning (ESANN 2015)", "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We compare in this paper several feature selection methods for the Naive\nBayes Classifier (NBC) when the data under study are described by a large\nnumber of redundant binary indicators. Wrapper approaches guided by the NBC\nestimation of the classification error probability out-perform filter\napproaches while retaining a reasonable computational cost.\n", "versions": [{"version": "v1", "created": "Fri, 12 Jun 2015 20:39:31 GMT"}], "update_date": "2015-06-16", "authors_parsed": [["Rabenoro", "Tsirizo", "", "SAMM"], ["Lacaille", "J\u00e9r\u00f4me", "", "SAMM"], ["Cottrell", "Marie", "", "SAMM"], ["Rossi", "Fabrice", "", "SAMM"]]}, {"id": "1506.04209", "submitter": "Kejun Huang", "authors": "Kejun Huang, Nicholas D. Sidiropoulos, Athanasios P. Liavas", "title": "A Flexible and Efficient Algorithmic Framework for Constrained Matrix\n  and Tensor Factorization", "comments": null, "journal-ref": null, "doi": "10.1109/TSP.2016.2576427", "report-no": null, "categories": "stat.ML cs.LG math.OC stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a general algorithmic framework for constrained matrix and tensor\nfactorization, which is widely used in signal processing and machine learning.\nThe new framework is a hybrid between alternating optimization (AO) and the\nalternating direction method of multipliers (ADMM): each matrix factor is\nupdated in turn, using ADMM, hence the name AO-ADMM. This combination can\nnaturally accommodate a great variety of constraints on the factor matrices,\nand almost all possible loss measures for the fitting. Computation caching and\nwarm start strategies are used to ensure that each update is evaluated\nefficiently, while the outer AO framework exploits recent developments in block\ncoordinate descent (BCD)-type methods which help ensure that every limit point\nis a stationary point, as well as faster and more robust convergence in\npractice. Three special cases are studied in detail: non-negative matrix/tensor\nfactorization, constrained matrix/tensor completion, and dictionary learning.\nExtensive simulations and experiments with real data are used to showcase the\neffectiveness and broad applicability of the proposed framework.\n", "versions": [{"version": "v1", "created": "Sat, 13 Jun 2015 01:42:05 GMT"}, {"version": "v2", "created": "Wed, 28 Oct 2015 16:24:52 GMT"}], "update_date": "2016-08-24", "authors_parsed": [["Huang", "Kejun", ""], ["Sidiropoulos", "Nicholas D.", ""], ["Liavas", "Athanasios P.", ""]]}, {"id": "1506.04322", "submitter": "Nesreen Ahmed", "authors": "Nesreen K. Ahmed, Jennifer Neville, Ryan A. Rossi, Nick Duffield, and\n  Theodore L. Willke", "title": "Graphlet Decomposition: Framework, Algorithms, and Applications", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.SI cs.DC cs.IR stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  From social science to biology, numerous applications often rely on graphlets\nfor intuitive and meaningful characterization of networks at both the global\nmacro-level as well as the local micro-level. While graphlets have witnessed a\ntremendous success and impact in a variety of domains, there has yet to be a\nfast and efficient approach for computing the frequencies of these subgraph\npatterns. However, existing methods are not scalable to large networks with\nmillions of nodes and edges, which impedes the application of graphlets to new\nproblems that require large-scale network analysis. To address these problems,\nwe propose a fast, efficient, and parallel algorithm for counting graphlets of\nsize k={3,4}-nodes that take only a fraction of the time to compute when\ncompared with the current methods used. The proposed graphlet counting\nalgorithms leverages a number of proven combinatorial arguments for different\ngraphlets. For each edge, we count a few graphlets, and with these counts along\nwith the combinatorial arguments, we obtain the exact counts of others in\nconstant time. On a large collection of 300+ networks from a variety of\ndomains, our graphlet counting strategies are on average 460x faster than\ncurrent methods. This brings new opportunities to investigate the use of\ngraphlets on much larger networks and newer applications as we show in the\nexperiments. To the best of our knowledge, this paper provides the largest\ngraphlet computations to date as well as the largest systematic investigation\non over 300+ networks from a variety of domains.\n", "versions": [{"version": "v1", "created": "Sat, 13 Jun 2015 21:32:12 GMT"}, {"version": "v2", "created": "Mon, 15 Feb 2016 23:23:31 GMT"}], "update_date": "2016-02-17", "authors_parsed": [["Ahmed", "Nesreen K.", ""], ["Neville", "Jennifer", ""], ["Rossi", "Ryan A.", ""], ["Duffield", "Nick", ""], ["Willke", "Theodore L.", ""]]}, {"id": "1506.04389", "submitter": "\\\"Omer Deniz Akyildiz", "authors": "\\\"Omer Deniz Aky{\\i}ld{\\i}z", "title": "Online Matrix Factorization via Broyden Updates", "comments": "Submitted. Little revisions on acknowledgements, and added references", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we propose an online algorithm to compute matrix\nfactorizations. Proposed algorithm updates the dictionary matrix and associated\ncoefficients using a single observation at each time. The algorithm performs\nlow-rank updates to dictionary matrix. We derive the algorithm by defining a\nsimple objective function to minimize whenever an observation is arrived. We\nextend the algorithm further for handling missing data. We also provide a\nmini-batch extension which enables to compute the matrix factorization on big\ndatasets. We demonstrate the efficiency of our algorithm on a real dataset and\ngive comparisons with well-known algorithms such as stochastic gradient matrix\nfactorization and nonnegative matrix factorization (NMF).\n", "versions": [{"version": "v1", "created": "Sun, 14 Jun 2015 13:19:38 GMT"}, {"version": "v2", "created": "Fri, 26 Jun 2015 07:11:17 GMT"}], "update_date": "2015-06-29", "authors_parsed": [["Aky\u0131ld\u0131z", "\u00d6mer Deniz", ""]]}, {"id": "1506.04416", "submitter": "Anoop Korattikara", "authors": "Anoop Korattikara, Vivek Rathod, Kevin Murphy, Max Welling", "title": "Bayesian Dark Knowledge", "comments": "final version submitted to NIPS 2015", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider the problem of Bayesian parameter estimation for deep neural\nnetworks, which is important in problem settings where we may have little data,\nand/ or where we need accurate posterior predictive densities, e.g., for\napplications involving bandits or active learning. One simple approach to this\nis to use online Monte Carlo methods, such as SGLD (stochastic gradient\nLangevin dynamics). Unfortunately, such a method needs to store many copies of\nthe parameters (which wastes memory), and needs to make predictions using many\nversions of the model (which wastes time).\n  We describe a method for \"distilling\" a Monte Carlo approximation to the\nposterior predictive density into a more compact form, namely a single deep\nneural network. We compare to two very recent approaches to Bayesian neural\nnetworks, namely an approach based on expectation propagation [Hernandez-Lobato\nand Adams, 2015] and an approach based on variational Bayes [Blundell et al.,\n2015]. Our method performs better than both of these, is much simpler to\nimplement, and uses less computation at test time.\n", "versions": [{"version": "v1", "created": "Sun, 14 Jun 2015 16:22:16 GMT"}, {"version": "v2", "created": "Tue, 7 Jul 2015 18:38:47 GMT"}, {"version": "v3", "created": "Fri, 6 Nov 2015 23:51:30 GMT"}], "update_date": "2015-11-10", "authors_parsed": [["Korattikara", "Anoop", ""], ["Rathod", "Vivek", ""], ["Murphy", "Kevin", ""], ["Welling", "Max", ""]]}, {"id": "1506.04448", "submitter": "Yining Wang", "authors": "Yining Wang, Hsiao-Yu Tung, Alexander Smola and Animashree Anandkumar", "title": "Fast and Guaranteed Tensor Decomposition via Sketching", "comments": "29 pages. Appeared in Proceedings of Advances in Neural Information\n  Processing Systems (NIPS), held at Montreal, Canada in 2015", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Tensor CANDECOMP/PARAFAC (CP) decomposition has wide applications in\nstatistical learning of latent variable models and in data mining. In this\npaper, we propose fast and randomized tensor CP decomposition algorithms based\non sketching. We build on the idea of count sketches, but introduce many novel\nideas which are unique to tensors. We develop novel methods for randomized\ncomputation of tensor contractions via FFTs, without explicitly forming the\ntensors. Such tensor contractions are encountered in decomposition methods such\nas tensor power iterations and alternating least squares. We also design novel\ncolliding hashes for symmetric tensors to further save time in computing the\nsketches. We then combine these sketching ideas with existing whitening and\ntensor power iterative techniques to obtain the fastest algorithm on both\nsparse and dense tensors. The quality of approximation under our method does\nnot depend on properties such as sparsity, uniformity of elements, etc. We\napply the method for topic modeling and obtain competitive results.\n", "versions": [{"version": "v1", "created": "Sun, 14 Jun 2015 23:07:38 GMT"}, {"version": "v2", "created": "Tue, 20 Oct 2015 14:45:41 GMT"}], "update_date": "2015-10-21", "authors_parsed": [["Wang", "Yining", ""], ["Tung", "Hsiao-Yu", ""], ["Smola", "Alexander", ""], ["Anandkumar", "Animashree", ""]]}, {"id": "1506.04513", "submitter": "Matus Telgarsky", "authors": "Matus Telgarsky and Miroslav Dud\\'ik and Robert Schapire", "title": "Convex Risk Minimization and Conditional Probability Estimation", "comments": "To appear, COLT 2015", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper proves, in very general settings, that convex risk minimization is\na procedure to select a unique conditional probability model determined by the\nclassification problem. Unlike most previous work, we give results that are\ngeneral enough to include cases in which no minimum exists, as occurs\ntypically, for instance, with standard boosting algorithms. Concretely, we\nfirst show that any sequence of predictors minimizing convex risk over the\nsource distribution will converge to this unique model when the class of\npredictors is linear (but potentially of infinite dimension). Secondly, we show\nthe same result holds for \\emph{empirical} risk minimization whenever this\nclass of predictors is finite dimensional, where the essential technical\ncontribution is a norm-free generalization bound.\n", "versions": [{"version": "v1", "created": "Mon, 15 Jun 2015 08:41:39 GMT"}], "update_date": "2015-06-16", "authors_parsed": [["Telgarsky", "Matus", ""], ["Dud\u00edk", "Miroslav", ""], ["Schapire", "Robert", ""]]}, {"id": "1506.04573", "submitter": "Emilie Morvant", "authors": "Pascal Germain (SIERRA), Amaury Habrard (LaHC), Fran\\c{c}ois\n  Laviolette, Emilie Morvant (LaHC)", "title": "A New PAC-Bayesian Perspective on Domain Adaptation", "comments": "Published at ICML 2016", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study the issue of PAC-Bayesian domain adaptation: We want to learn, from\na source domain, a majority vote model dedicated to a target one. Our\ntheoretical contribution brings a new perspective by deriving an upper-bound on\nthe target risk where the distributions' divergence---expressed as a\nratio---controls the trade-off between a source error measure and the target\nvoters' disagreement. Our bound suggests that one has to focus on regions where\nthe source data is informative.From this result, we derive a PAC-Bayesian\ngeneralization bound, and specialize it to linear classifiers. Then, we infer a\nlearning algorithmand perform experiments on real data.\n", "versions": [{"version": "v1", "created": "Mon, 15 Jun 2015 12:46:45 GMT"}, {"version": "v2", "created": "Mon, 21 Sep 2015 10:49:00 GMT"}, {"version": "v3", "created": "Mon, 14 Mar 2016 19:44:22 GMT"}, {"version": "v4", "created": "Tue, 26 Jul 2016 10:29:33 GMT"}], "update_date": "2016-07-27", "authors_parsed": [["Germain", "Pascal", "", "SIERRA"], ["Habrard", "Amaury", "", "LaHC"], ["Laviolette", "Fran\u00e7ois", "", "LaHC"], ["Morvant", "Emilie", "", "LaHC"]]}, {"id": "1506.04696", "submitter": "Yi-An Ma", "authors": "Yi-An Ma, Tianqi Chen and Emily B. Fox", "title": "A Complete Recipe for Stochastic Gradient MCMC", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.ME stat.ML stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Many recent Markov chain Monte Carlo (MCMC) samplers leverage continuous\ndynamics to define a transition kernel that efficiently explores a target\ndistribution. In tandem, a focus has been on devising scalable variants that\nsubsample the data and use stochastic gradients in place of full-data gradients\nin the dynamic simulations. However, such stochastic gradient MCMC samplers\nhave lagged behind their full-data counterparts in terms of the complexity of\ndynamics considered since proving convergence in the presence of the stochastic\ngradient noise is non-trivial. Even with simple dynamics, significant physical\nintuition is often required to modify the dynamical system to account for the\nstochastic gradient noise. In this paper, we provide a general recipe for\nconstructing MCMC samplers--including stochastic gradient versions--based on\ncontinuous Markov processes specified via two matrices. We constructively prove\nthat the framework is complete. That is, any continuous Markov process that\nprovides samples from the target distribution can be written in our framework.\nWe show how previous continuous-dynamic samplers can be trivially \"reinvented\"\nin our framework, avoiding the complicated sampler-specific proofs. We likewise\nuse our recipe to straightforwardly propose a new state-adaptive sampler:\nstochastic gradient Riemann Hamiltonian Monte Carlo (SGRHMC). Our experiments\non simulated data and a streaming Wikipedia analysis demonstrate that the\nproposed SGRHMC sampler inherits the benefits of Riemann HMC, with the\nscalability of stochastic gradient methods.\n", "versions": [{"version": "v1", "created": "Mon, 15 Jun 2015 18:32:37 GMT"}, {"version": "v2", "created": "Sun, 1 Nov 2015 00:18:32 GMT"}], "update_date": "2015-11-03", "authors_parsed": [["Ma", "Yi-An", ""], ["Chen", "Tianqi", ""], ["Fox", "Emily B.", ""]]}, {"id": "1506.04725", "submitter": "Kacper Chwialkowski", "authors": "Kacper Chwialkowski and Aaditya Ramdas and Dino Sejdinovic and Arthur\n  Gretton", "title": "Fast Two-Sample Testing with Analytic Representations of Probability\n  Measures", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a class of nonparametric two-sample tests with a cost linear in\nthe sample size. Two tests are given, both based on an ensemble of distances\nbetween analytic functions representing each of the distributions. The first\ntest uses smoothed empirical characteristic functions to represent the\ndistributions, the second uses distribution embeddings in a reproducing kernel\nHilbert space. Analyticity implies that differences in the distributions may be\ndetected almost surely at a finite number of randomly chosen\nlocations/frequencies. The new tests are consistent against a larger class of\nalternatives than the previous linear-time tests based on the (non-smoothed)\nempirical characteristic functions, while being much faster than the current\nstate-of-the-art quadratic-time kernel-based or energy distance-based tests.\nExperiments on artificial benchmarks and on challenging real-world testing\nproblems demonstrate that our tests give a better power/time tradeoff than\ncompeting approaches, and in some cases, better outright power than even the\nmost expensive quadratic-time tests. This performance advantage is retained\neven in high dimensions, and in cases where the difference in distributions is\nnot observable with low order statistics.\n", "versions": [{"version": "v1", "created": "Mon, 15 Jun 2015 19:42:45 GMT"}], "update_date": "2015-06-16", "authors_parsed": [["Chwialkowski", "Kacper", ""], ["Ramdas", "Aaditya", ""], ["Sejdinovic", "Dino", ""], ["Gretton", "Arthur", ""]]}, {"id": "1506.04744", "submitter": "Cristian Danescu-Niculescu-Mizil", "authors": "Vlad Niculae, Srijan Kumar, Jordan Boyd-Graber, Cristian\n  Danescu-Niculescu-Mizil", "title": "Linguistic Harbingers of Betrayal: A Case Study on an Online Strategy\n  Game", "comments": "To appear at ACL 2015. 10pp, 4 fig. Data and other info available at\n  http://vene.ro/betrayal/", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI cs.SI physics.soc-ph stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Interpersonal relations are fickle, with close friendships often dissolving\ninto enmity. In this work, we explore linguistic cues that presage such\ntransitions by studying dyadic interactions in an online strategy game where\nplayers form alliances and break those alliances through betrayal. We\ncharacterize friendships that are unlikely to last and examine temporal\npatterns that foretell betrayal.\n  We reveal that subtle signs of imminent betrayal are encoded in the\nconversational patterns of the dyad, even if the victim is not aware of the\nrelationship's fate. In particular, we find that lasting friendships exhibit a\nform of balance that manifests itself through language. In contrast, sudden\nchanges in the balance of certain conversational attributes---such as positive\nsentiment, politeness, or focus on future planning---signal impending betrayal.\n", "versions": [{"version": "v1", "created": "Mon, 15 Jun 2015 20:00:29 GMT"}], "update_date": "2015-06-17", "authors_parsed": [["Niculae", "Vlad", ""], ["Kumar", "Srijan", ""], ["Boyd-Graber", "Jordan", ""], ["Danescu-Niculescu-Mizil", "Cristian", ""]]}, {"id": "1506.04838", "submitter": "Zeyuan Allen-Zhu", "authors": "Zeyuan Allen-Zhu and Zhenyu Liao and Lorenzo Orecchia", "title": "Spectral Sparsification and Regret Minimization Beyond Matrix\n  Multiplicative Updates", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.DS math.OC stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we provide a novel construction of the linear-sized spectral\nsparsifiers of Batson, Spielman and Srivastava [BSS14]. While previous\nconstructions required $\\Omega(n^4)$ running time [BSS14, Zou12], our\nsparsification routine can be implemented in almost-quadratic running time\n$O(n^{2+\\varepsilon})$.\n  The fundamental conceptual novelty of our work is the leveraging of a strong\nconnection between sparsification and a regret minimization problem over\ndensity matrices. This connection was known to provide an interpretation of the\nrandomized sparsifiers of Spielman and Srivastava [SS11] via the application of\nmatrix multiplicative weight updates (MWU) [CHS11, Vis14]. In this paper, we\nexplain how matrix MWU naturally arises as an instance of the\nFollow-the-Regularized-Leader framework and generalize this approach to yield a\nlarger class of updates. This new class allows us to accelerate the\nconstruction of linear-sized spectral sparsifiers, and give novel insights on\nthe motivation behind Batson, Spielman and Srivastava [BSS14].\n", "versions": [{"version": "v1", "created": "Tue, 16 Jun 2015 05:31:57 GMT"}], "update_date": "2015-06-17", "authors_parsed": [["Allen-Zhu", "Zeyuan", ""], ["Liao", "Zhenyu", ""], ["Orecchia", "Lorenzo", ""]]}, {"id": "1506.04855", "submitter": "Wojciech Kot{\\l}owski", "authors": "Wojciech Kot{\\l}owski, Manfred K. Warmuth", "title": "PCA with Gaussian perturbations", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Most of machine learning deals with vector parameters. Ideally we would like\nto take higher order information into account and make use of matrix or even\ntensor parameters. However the resulting algorithms are usually inefficient.\nHere we address on-line learning with matrix parameters. It is often easy to\nobtain online algorithm with good generalization performance if you\neigendecompose the current parameter matrix in each trial (at a cost of\n$O(n^3)$ per trial). Ideally we want to avoid the decompositions and spend\n$O(n^2)$ per trial, i.e. linear time in the size of the matrix data. There is a\ncore trade-off between the running time and the generalization performance,\nhere measured by the regret of the on-line algorithm (total gain of the best\noff-line predictor minus the total gain of the on-line algorithm). We focus on\nthe key matrix problem of rank $k$ Principal Component Analysis in\n$\\mathbb{R}^n$ where $k \\ll n$. There are $O(n^3)$ algorithms that achieve the\noptimum regret but require eigendecompositions. We develop a simple algorithm\nthat needs $O(kn^2)$ per trial whose regret is off by a small factor of\n$O(n^{1/4})$. The algorithm is based on the Follow the Perturbed Leader\nparadigm. It replaces full eigendecompositions at each trial by the problem\nfinding $k$ principal components of the current covariance matrix that is\nperturbed by Gaussian noise.\n", "versions": [{"version": "v1", "created": "Tue, 16 Jun 2015 07:04:19 GMT"}, {"version": "v2", "created": "Thu, 23 Jul 2015 20:22:27 GMT"}], "update_date": "2015-07-27", "authors_parsed": [["Kot\u0142owski", "Wojciech", ""], ["Warmuth", "Manfred K.", ""]]}, {"id": "1506.05011", "submitter": "Theofanis Karaletsos", "authors": "Theofanis Karaletsos, Serge Belongie, Gunnar R\\\"atsch", "title": "Bayesian representation learning with oracle constraints", "comments": "16 pages, publishes in ICLR 16", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Representation learning systems typically rely on massive amounts of labeled\ndata in order to be trained to high accuracy. Recently, high-dimensional\nparametric models like neural networks have succeeded in building rich\nrepresentations using either compressive, reconstructive or supervised\ncriteria. However, the semantic structure inherent in observations is\noftentimes lost in the process. Human perception excels at understanding\nsemantics but cannot always be expressed in terms of labels. Thus,\n\\emph{oracles} or \\emph{human-in-the-loop systems}, for example crowdsourcing,\nare often employed to generate similarity constraints using an implicit\nsimilarity function encoded in human perception. In this work we propose to\ncombine \\emph{generative unsupervised feature learning} with a\n\\emph{probabilistic treatment of oracle information like triplets} in order to\ntransfer implicit privileged oracle knowledge into explicit nonlinear Bayesian\nlatent factor models of the observations. We use a fast variational algorithm\nto learn the joint model and demonstrate applicability to a well-known image\ndataset. We show how implicit triplet information can provide rich information\nto learn representations that outperform previous metric learning approaches as\nwell as generative models without this side-information in a variety of\npredictive tasks. In addition, we illustrate that the proposed approach\ncompartmentalizes the latent spaces semantically which allows interpretation of\nthe latent variables.\n", "versions": [{"version": "v1", "created": "Tue, 16 Jun 2015 15:54:59 GMT"}, {"version": "v2", "created": "Sat, 21 Nov 2015 05:24:01 GMT"}, {"version": "v3", "created": "Fri, 8 Jan 2016 04:47:21 GMT"}, {"version": "v4", "created": "Tue, 1 Mar 2016 23:36:04 GMT"}], "update_date": "2016-03-03", "authors_parsed": [["Karaletsos", "Theofanis", ""], ["Belongie", "Serge", ""], ["R\u00e4tsch", "Gunnar", ""]]}, {"id": "1506.05173", "submitter": "Saurabh Paul", "authors": "Saurabh Paul, Petros Drineas", "title": "Feature Selection for Ridge Regression with Provable Guarantees", "comments": "To appear in Neural Computation. A shorter version of this paper\n  appeared at ECML-PKDD 2014 under the title \"Deterministic Feature Selection\n  for Regularized Least Squares Classification.\"", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.IT cs.LG math.IT", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We introduce single-set spectral sparsification as a deterministic sampling\nbased feature selection technique for regularized least squares classification,\nwhich is the classification analogue to ridge regression. The method is\nunsupervised and gives worst-case guarantees of the generalization power of the\nclassification function after feature selection with respect to the\nclassification function obtained using all features. We also introduce\nleverage-score sampling as an unsupervised randomized feature selection method\nfor ridge regression. We provide risk bounds for both single-set spectral\nsparsification and leverage-score sampling on ridge regression in the fixed\ndesign setting and show that the risk in the sampled space is comparable to the\nrisk in the full-feature space. We perform experiments on synthetic and\nreal-world datasets, namely a subset of TechTC-300 datasets, to support our\ntheory. Experimental results indicate that the proposed methods perform better\nthan the existing feature selection methods.\n", "versions": [{"version": "v1", "created": "Wed, 17 Jun 2015 00:05:04 GMT"}, {"version": "v2", "created": "Sat, 5 Dec 2015 18:27:38 GMT"}], "update_date": "2015-12-08", "authors_parsed": [["Paul", "Saurabh", ""], ["Drineas", "Petros", ""]]}, {"id": "1506.05215", "submitter": "Ying Sun", "authors": "Ying Sun, Prabhu Babu, and Daniel P. Palomar", "title": "Robust Estimation of Structured Covariance Matrix for Heavy-Tailed\n  Elliptical Distributions", "comments": null, "journal-ref": null, "doi": "10.1109/TSP.2016.2546222", "report-no": null, "categories": "stat.AP stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper considers the problem of robustly estimating a structured\ncovariance matrix with an elliptical underlying distribution with known mean.\nIn applications where the covariance matrix naturally possesses a certain\nstructure, taking the prior structure information into account in the\nestimation procedure is beneficial to improve the estimation accuracy. We\npropose incorporating the prior structure information into Tyler's M-estimator\nand formulate the problem as minimizing the cost function of Tyler's estimator\nunder the prior structural constraint. First, the estimation under a general\nconvex structural constraint is introduced with an efficient algorithm for\nfinding the estimator derived based on the majorization minimization (MM)\nalgorithm framework. Then, the algorithm is tailored to several special\nstructures that enjoy a wide range of applications in signal processing related\nfields, namely, sum of rank-one matrices, Toeplitz, and banded Toeplitz\nstructure. In addition, two types of non-convex structures, i.e., the Kronecker\nstructure and the spiked covariance structure, are also discussed, where it is\nshown that simple algorithms can be derived under the guidelines of MM.\nNumerical results show that the proposed estimator achieves a smaller\nestimation error than the benchmark estimators at a lower computational cost.\n", "versions": [{"version": "v1", "created": "Wed, 17 Jun 2015 06:17:26 GMT"}], "update_date": "2016-06-29", "authors_parsed": [["Sun", "Ying", ""], ["Babu", "Prabhu", ""], ["Palomar", "Daniel P.", ""]]}, {"id": "1506.05244", "submitter": "Thomas E Bartlett", "authors": "Thomas E. Bartlett and Alexey Zaikin", "title": "Detection of Epigenomic Network Community Oncomarkers", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP q-bio.GN q-bio.MN stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper we propose network methodology to infer prognostic cancer\nbiomarkers based on the epigenetic pattern DNA methylation. Epigenetic\nprocesses such as DNA methylation reflect environmental risk factors, and are\nincreasingly recognised for their fundamental role in diseases such as cancer.\nDNA methylation is a gene-regulatory pattern, and hence provides a means by\nwhich to assess genomic regulatory interactions. Network models are a natural\nway to represent and analyse groups of such interactions. The utility of\nnetwork models also increases as the quantity of data and number of variables\nincrease, making them increasingly relevant to large-scale genomic studies. We\npropose methodology to infer prognostic genomic networks from a DNA\nmethylation-based measure of genomic interaction and association. We then show\nhow to identify prognostic biomarkers from such networks, which we term\n`network community oncomarkers'. We illustrate the power of our proposed\nmethodology in the context of a large publicly available breast cancer dataset.\n", "versions": [{"version": "v1", "created": "Wed, 17 Jun 2015 08:48:27 GMT"}, {"version": "v2", "created": "Tue, 28 Jul 2015 11:23:58 GMT"}, {"version": "v3", "created": "Mon, 1 Aug 2016 18:10:17 GMT"}], "update_date": "2016-08-02", "authors_parsed": [["Bartlett", "Thomas E.", ""], ["Zaikin", "Alexey", ""]]}, {"id": "1506.05439", "submitter": "Charlie Frogner", "authors": "Charlie Frogner, Chiyuan Zhang, Hossein Mobahi, Mauricio Araya-Polo,\n  Tomaso Poggio", "title": "Learning with a Wasserstein Loss", "comments": "NIPS 2015; v3 updates Algorithm 1 and Equations 6, 8", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Learning to predict multi-label outputs is challenging, but in many problems\nthere is a natural metric on the outputs that can be used to improve\npredictions. In this paper we develop a loss function for multi-label learning,\nbased on the Wasserstein distance. The Wasserstein distance provides a natural\nnotion of dissimilarity for probability measures. Although optimizing with\nrespect to the exact Wasserstein distance is costly, recent work has described\na regularized approximation that is efficiently computed. We describe an\nefficient learning algorithm based on this regularization, as well as a novel\nextension of the Wasserstein distance from probability measures to unnormalized\nmeasures. We also describe a statistical learning bound for the loss. The\nWasserstein loss can encourage smoothness of the predictions with respect to a\nchosen metric on the output space. We demonstrate this property on a real-data\ntag prediction problem, using the Yahoo Flickr Creative Commons dataset,\noutperforming a baseline that doesn't use the metric.\n", "versions": [{"version": "v1", "created": "Wed, 17 Jun 2015 19:36:41 GMT"}, {"version": "v2", "created": "Fri, 6 Nov 2015 03:46:05 GMT"}, {"version": "v3", "created": "Wed, 30 Dec 2015 01:08:11 GMT"}], "update_date": "2015-12-31", "authors_parsed": [["Frogner", "Charlie", ""], ["Zhang", "Chiyuan", ""], ["Mobahi", "Hossein", ""], ["Araya-Polo", "Mauricio", ""], ["Poggio", "Tomaso", ""]]}, {"id": "1506.05446", "submitter": "Weijie Su", "authors": "Weijie Su, Junyang Qian, Linxi Liu", "title": "Communication-Efficient False Discovery Rate Control via Knockoff\n  Aggregation", "comments": "Generalized to the case that linear models can have different\n  parameters; changed title; updated references", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The false discovery rate (FDR)---the expected fraction of spurious\ndiscoveries among all the discoveries---provides a popular statistical\nassessment of the reproducibility of scientific studies in various disciplines.\nIn this work, we introduce a new method for controlling the FDR in\nmeta-analysis of many decentralized linear models. Our method targets the\nscenario where many research groups---possibly the number of which is\nrandom---are independently testing a common set of hypotheses and then sending\nsummary statistics to a coordinating center in an online manner. Built on the\nknockoffs framework introduced by Barber and Candes (2015), our procedure\nstarts by applying the knockoff filter to each linear model and then aggregates\nthe summary statistics via one-shot communication in a novel way. This method\ngives exact FDR control non-asymptotically without any knowledge of the noise\nvariances or making any assumption about sparsity of the signal. In certain\nsettings, it has a communication complexity that is optimal up to a logarithmic\nfactor.\n", "versions": [{"version": "v1", "created": "Wed, 17 Jun 2015 19:56:59 GMT"}, {"version": "v2", "created": "Mon, 9 Nov 2015 18:24:26 GMT"}], "update_date": "2015-11-10", "authors_parsed": [["Su", "Weijie", ""], ["Qian", "Junyang", ""], ["Liu", "Linxi", ""]]}, {"id": "1506.05555", "submitter": "Cheng Zhang", "authors": "Cheng Zhang, Babak Shahbaba and Hongkai Zhao", "title": "Hamiltonian Monte Carlo Acceleration Using Surrogate Functions with\n  Random Bases", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.CO stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  For big data analysis, high computational cost for Bayesian methods often\nlimits their applications in practice. In recent years, there have been many\nattempts to improve computational efficiency of Bayesian inference. Here we\npropose an efficient and scalable computational technique for a\nstate-of-the-art Markov Chain Monte Carlo (MCMC) methods, namely, Hamiltonian\nMonte Carlo (HMC). The key idea is to explore and exploit the structure and\nregularity in parameter space for the underlying probabilistic model to\nconstruct an effective approximation of its geometric properties. To this end,\nwe build a surrogate function to approximate the target distribution using\nproperly chosen random bases and an efficient optimization process. The\nresulting method provides a flexible, scalable, and efficient sampling\nalgorithm, which converges to the correct target distribution. We show that by\nchoosing the basis functions and optimization process differently, our method\ncan be related to other approaches for the construction of surrogate functions\nsuch as generalized additive models or Gaussian process models. Experiments\nbased on simulated and real data show that our approach leads to substantially\nmore efficient sampling algorithms compared to existing state-of-the art\nmethods.\n", "versions": [{"version": "v1", "created": "Thu, 18 Jun 2015 06:25:59 GMT"}, {"version": "v2", "created": "Fri, 19 Jun 2015 17:36:39 GMT"}, {"version": "v3", "created": "Fri, 18 Sep 2015 22:17:16 GMT"}, {"version": "v4", "created": "Wed, 4 May 2016 23:23:16 GMT"}, {"version": "v5", "created": "Mon, 17 Apr 2017 22:05:56 GMT"}], "update_date": "2017-04-19", "authors_parsed": [["Zhang", "Cheng", ""], ["Shahbaba", "Babak", ""], ["Zhao", "Hongkai", ""]]}, {"id": "1506.05600", "submitter": "Ridho Rahmadi", "authors": "Ridho Rahmadi, Perry Groot, Marianne Heins, Hans Knoop, Tom Heskes\n  (The OPTIMISTIC consortium)", "title": "Causality on Cross-Sectional Data: Stable Specification Search in\n  Constrained Structural Equation Modeling", "comments": null, "journal-ref": "Applied.Soft.Comp. 52 (2017) 687-698", "doi": "10.1016/j.asoc.2016.10.003", "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Causal modeling has long been an attractive topic for many researchers and in\nrecent decades there has seen a surge in theoretical development and discovery\nalgorithms. Generally discovery algorithms can be divided into two approaches:\nconstraint-based and score-based. The constraint-based approach is able to\ndetect common causes of the observed variables but the use of independence\ntests makes it less reliable. The score-based approach produces a result that\nis easier to interpret as it also measures the reliability of the inferred\ncausal relationships, but it is unable to detect common confounders of the\nobserved variables. A drawback of both score-based and constrained-based\napproaches is the inherent instability in structure estimation. With finite\nsamples small changes in the data can lead to completely different optimal\nstructures. The present work introduces a new hypothesis-free score-based\ncausal discovery algorithm, called stable specification search, that is robust\nfor finite samples based on recent advances in stability selection using\nsubsampling and selection algorithms. Structure search is performed over\nStructural Equation Models. Our approach uses exploratory search but allows\nincorporation of prior background knowledge. We validated our approach on one\nsimulated data set, which we compare to the known ground truth, and two\nreal-world data sets for Chronic Fatigue Syndrome and Attention Deficit\nHyperactivity Disorder, which we compare to earlier medical studies. The\nresults on the simulated data set show significant improvement over alternative\napproaches and the results on the real-word data sets show consistency with the\nhypothesis driven models constructed by medical experts.\n", "versions": [{"version": "v1", "created": "Thu, 18 Jun 2015 09:33:10 GMT"}, {"version": "v2", "created": "Wed, 18 May 2016 09:42:37 GMT"}, {"version": "v3", "created": "Thu, 14 Jul 2016 14:30:55 GMT"}], "update_date": "2017-02-06", "authors_parsed": [["Rahmadi", "Ridho", "", "The OPTIMISTIC consortium"], ["Groot", "Perry", "", "The OPTIMISTIC consortium"], ["Heins", "Marianne", "", "The OPTIMISTIC consortium"], ["Knoop", "Hans", "", "The OPTIMISTIC consortium"], ["Heskes", "Tom", "", "The OPTIMISTIC consortium"]]}, {"id": "1506.05666", "submitter": "Hiroaki Sasaki", "authors": "Hiroaki Sasaki, Michael U. Gutmann, Hayaru Shouno, Aapo Hyv\\\"arinen", "title": "Simultaneous Estimation of Non-Gaussian Components and their Correlation\n  Structure", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The statistical dependencies which independent component analysis (ICA)\ncannot remove often provide rich information beyond the linear independent\ncomponents. It would thus be very useful to estimate the dependency structure\nfrom data. While such models have been proposed, they usually concentrated on\nhigher-order correlations such as energy (square) correlations. Yet, linear\ncorrelations are a most fundamental and informative form of dependency in many\nreal data sets. Linear correlations are usually completely removed by ICA and\nrelated methods, so they can only be analyzed by developing new methods which\nexplicitly allow for linearly correlated components. In this paper, we propose\na probabilistic model of linear non-Gaussian components which are allowed to\nhave both linear and energy correlations. The precision matrix of the linear\ncomponents is assumed to be randomly generated by a higher-order process and\nexplicitly parametrized by a parameter matrix. The estimation of the parameter\nmatrix is shown to be particularly simple because using score matching, the\nobjective function is a quadratic form. Using simulations with artificial data,\nwe demonstrate that the proposed method improves identifiability of\nnon-Gaussian components by simultaneously learning their correlation structure.\nApplications on simulated complex cells with natural image input, as well as\nspectrograms of natural audio data show that the method finds new kinds of\ndependencies between the components.\n", "versions": [{"version": "v1", "created": "Thu, 18 Jun 2015 13:18:15 GMT"}, {"version": "v2", "created": "Thu, 27 Jul 2017 05:58:05 GMT"}], "update_date": "2017-07-28", "authors_parsed": [["Sasaki", "Hiroaki", ""], ["Gutmann", "Michael U.", ""], ["Shouno", "Hayaru", ""], ["Hyv\u00e4rinen", "Aapo", ""]]}, {"id": "1506.05692", "submitter": "Maxime Gasse", "authors": "Maxime Gasse (DM2L), Alex Aussem (DM2L), Haytham Elghazel (DM2L)", "title": "A hybrid algorithm for Bayesian network structure learning with\n  application to multi-label learning", "comments": "arXiv admin note: text overlap with arXiv:1101.5184 by other authors", "journal-ref": "Expert Systems with Applications, Elsevier, 2014, 41 (15),\n  pp.6755-6772", "doi": "10.1016/j.eswa.2014.04.032", "report-no": null, "categories": "stat.ML cs.AI cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a novel hybrid algorithm for Bayesian network structure learning,\ncalled H2PC. It first reconstructs the skeleton of a Bayesian network and then\nperforms a Bayesian-scoring greedy hill-climbing search to orient the edges.\nThe algorithm is based on divide-and-conquer constraint-based subroutines to\nlearn the local structure around a target variable. We conduct two series of\nexperimental comparisons of H2PC against Max-Min Hill-Climbing (MMHC), which is\ncurrently the most powerful state-of-the-art algorithm for Bayesian network\nstructure learning. First, we use eight well-known Bayesian network benchmarks\nwith various data sizes to assess the quality of the learned structure returned\nby the algorithms. Our extensive experiments show that H2PC outperforms MMHC in\nterms of goodness of fit to new data and quality of the network structure with\nrespect to the true dependence structure of the data. Second, we investigate\nH2PC's ability to solve the multi-label learning problem. We provide\ntheoretical results to characterize and identify graphically the so-called\nminimal label powersets that appear as irreducible factors in the joint\ndistribution under the faithfulness condition. The multi-label learning problem\nis then decomposed into a series of multi-class classification problems, where\neach multi-class variable encodes a label powerset. H2PC is shown to compare\nfavorably to MMHC in terms of global classification accuracy over ten\nmulti-label data sets covering different application domains. Overall, our\nexperiments support the conclusions that local structural learning with H2PC in\nthe form of local neighborhood induction is a theoretically well-motivated and\nempirically effective learning framework that is well suited to multi-label\nlearning. The source code (in R) of H2PC as well as all data sets used for the\nempirical tests are publicly available.\n", "versions": [{"version": "v1", "created": "Thu, 18 Jun 2015 14:24:19 GMT"}], "update_date": "2015-06-19", "authors_parsed": [["Gasse", "Maxime", "", "DM2L"], ["Aussem", "Alex", "", "DM2L"], ["Elghazel", "Haytham", "", "DM2L"]]}, {"id": "1506.05776", "submitter": "Ping Ren", "authors": "Ping Ren", "title": "A tree augmented naive Bayesian network experiment for breast cancer\n  prediction", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML q-bio.QM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In order to investigate the breast cancer prediction problem on the aging\npopulation with the grades of DCIS, we conduct a tree augmented naive Bayesian\nnetwork experiment trained and tested on a large clinical dataset including\nconsecutive diagnostic mammography examinations, consequent biopsy outcomes and\nrelated cancer registry records in the population of women across all ages. The\naggregated results of our ten-fold cross validation method recommend a biopsy\nthreshold higher than 2% for the aging population.\n", "versions": [{"version": "v1", "created": "Thu, 18 Jun 2015 19:22:42 GMT"}], "update_date": "2015-06-19", "authors_parsed": [["Ren", "Ping", ""]]}, {"id": "1506.05822", "submitter": "Jakob Runge", "authors": "Jakob Runge, Reik V. Donner, and J\\\"urgen Kurths", "title": "Optimal model-free prediction from multivariate time series", "comments": "14 pages, 9 figures", "journal-ref": "Phys. Rev. E 91, 052909, May 2015", "doi": "10.1103/PhysRevE.91.052909", "report-no": null, "categories": "stat.ML stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Forecasting a time series from multivariate predictors constitutes a\nchallenging problem, especially using model-free approaches. Most techniques,\nsuch as nearest-neighbor prediction, quickly suffer from the curse of\ndimensionality and overfitting for more than a few predictors which has limited\ntheir application mostly to the univariate case. Therefore, selection\nstrategies are needed that harness the available information as efficiently as\npossible. Since often the right combination of predictors matters, ideally all\nsubsets of possible predictors should be tested for their predictive power, but\nthe exponentially growing number of combinations makes such an approach\ncomputationally prohibitive. Here a prediction scheme that overcomes this\nstrong limitation is introduced utilizing a causal pre-selection step which\ndrastically reduces the number of possible predictors to the most predictive\nset of causal drivers making a globally optimal search scheme tractable. The\ninformation-theoretic optimality is derived and practical selection criteria\nare discussed. As demonstrated for multivariate nonlinear stochastic delay\nprocesses, the optimal scheme can even be less computationally expensive than\ncommonly used sub-optimal schemes like forward selection. The method suggests a\ngeneral framework to apply the optimal model-free approach to select variables\nand subsequently fit a model to further improve a prediction or learn\nstatistical dependencies. The performance of this framework is illustrated on a\nclimatological index of El Ni\\~no Southern Oscillation.\n", "versions": [{"version": "v1", "created": "Thu, 18 Jun 2015 21:17:11 GMT"}], "update_date": "2015-06-22", "authors_parsed": [["Runge", "Jakob", ""], ["Donner", "Reik V.", ""], ["Kurths", "J\u00fcrgen", ""]]}, {"id": "1506.05843", "submitter": "Scott Linderman", "authors": "Scott W. Linderman, Matthew J. Johnson, and Ryan P. Adams", "title": "Dependent Multinomial Models Made Easy: Stick Breaking with the\n  P\\'olya-Gamma Augmentation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Many practical modeling problems involve discrete data that are best\nrepresented as draws from multinomial or categorical distributions. For\nexample, nucleotides in a DNA sequence, children's names in a given state and\nyear, and text documents are all commonly modeled with multinomial\ndistributions. In all of these cases, we expect some form of dependency between\nthe draws: the nucleotide at one position in the DNA strand may depend on the\npreceding nucleotides, children's names are highly correlated from year to\nyear, and topics in text may be correlated and dynamic. These dependencies are\nnot naturally captured by the typical Dirichlet-multinomial formulation. Here,\nwe leverage a logistic stick-breaking representation and recent innovations in\nP\\'olya-gamma augmentation to reformulate the multinomial distribution in terms\nof latent variables with jointly Gaussian likelihoods, enabling us to take\nadvantage of a host of Bayesian inference techniques for Gaussian models with\nminimal overhead.\n", "versions": [{"version": "v1", "created": "Thu, 18 Jun 2015 22:45:16 GMT"}], "update_date": "2015-06-22", "authors_parsed": [["Linderman", "Scott W.", ""], ["Johnson", "Matthew J.", ""], ["Adams", "Ryan P.", ""]]}, {"id": "1506.05855", "submitter": "Paul Wiggins Dr", "authors": "Colin H. LaMont and Paul A. Wiggins", "title": "Information-based inference for singular models and finite sample sizes:\n  A frequentist information criterion", "comments": "30 Pages, 6 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG physics.data-an", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In the information-based paradigm of inference, model selection is performed\nby selecting the candidate model with the best estimated predictive\nperformance. The success of this approach depends on the accuracy of the\nestimate of the predictive complexity. In the large-sample-size limit of a\nregular model, the predictive performance is well estimated by the Akaike\nInformation Criterion (AIC). However, this approximation can either\nsignificantly under or over-estimating the complexity in a wide range of\nimportant applications where models are either non-regular or\nfinite-sample-size corrections are significant. We introduce an improved\napproximation for the complexity that is used to define a new information\ncriterion: the Frequentist Information Criterion (QIC). QIC extends the\napplicability of information-based inference to the finite-sample-size regime\nof regular models and to singular models. We demonstrate the power and the\ncomparative advantage of QIC in a number of example analyses.\n", "versions": [{"version": "v1", "created": "Fri, 19 Jun 2015 00:53:40 GMT"}, {"version": "v2", "created": "Fri, 8 Jan 2016 22:13:52 GMT"}, {"version": "v3", "created": "Thu, 24 Mar 2016 21:42:30 GMT"}, {"version": "v4", "created": "Wed, 16 Aug 2017 20:36:48 GMT"}, {"version": "v5", "created": "Fri, 8 Jun 2018 16:22:44 GMT"}], "update_date": "2018-06-11", "authors_parsed": [["LaMont", "Colin H.", ""], ["Wiggins", "Paul A.", ""]]}, {"id": "1506.05860", "submitter": "Shaobo Han", "authors": "Shaobo Han, Xuejun Liao, David B. Dunson, Lawrence Carin", "title": "Variational Gaussian Copula Inference", "comments": "Appearing in Proceedings of the 19th International Conference on\n  Artificial Intelligence and Statistics (AISTATS) 2016, Cadiz, Spain. JMLR:\n  W&CP volume 51", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We utilize copulas to constitute a unified framework for constructing and\noptimizing variational proposals in hierarchical Bayesian models. For models\nwith continuous and non-Gaussian hidden variables, we propose a semiparametric\nand automated variational Gaussian copula approach, in which the parametric\nGaussian copula family is able to preserve multivariate posterior dependence,\nand the nonparametric transformations based on Bernstein polynomials provide\nample flexibility in characterizing the univariate marginal posteriors.\n", "versions": [{"version": "v1", "created": "Fri, 19 Jun 2015 01:49:46 GMT"}, {"version": "v2", "created": "Wed, 16 Mar 2016 01:26:51 GMT"}, {"version": "v3", "created": "Wed, 18 May 2016 15:16:28 GMT"}], "update_date": "2016-05-19", "authors_parsed": [["Han", "Shaobo", ""], ["Liao", "Xuejun", ""], ["Dunson", "David B.", ""], ["Carin", "Lawrence", ""]]}, {"id": "1506.05900", "submitter": "Hassan Ashtiani", "authors": "Hassan Ashtiani, Shai Ben-David", "title": "Representation Learning for Clustering: A Statistical Framework", "comments": "To be published in Proceedings of UAI 2015", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We address the problem of communicating domain knowledge from a user to the\ndesigner of a clustering algorithm. We propose a protocol in which the user\nprovides a clustering of a relatively small random sample of a data set. The\nalgorithm designer then uses that sample to come up with a data representation\nunder which $k$-means clustering results in a clustering (of the full data set)\nthat is aligned with the user's clustering. We provide a formal statistical\nmodel for analyzing the sample complexity of learning a clustering\nrepresentation with this paradigm. We then introduce a notion of capacity of a\nclass of possible representations, in the spirit of the VC-dimension, showing\nthat classes of representations that have finite such dimension can be\nsuccessfully learned with sample size error bounds, and end our discussion with\nan analysis of that dimension for classes of representations induced by linear\nembeddings.\n", "versions": [{"version": "v1", "created": "Fri, 19 Jun 2015 08:18:59 GMT"}], "update_date": "2015-06-22", "authors_parsed": [["Ashtiani", "Hassan", ""], ["Ben-David", "Shai", ""]]}, {"id": "1506.05934", "submitter": "Thibaut Lienart", "authors": "Thibaut Lienart, Yee Whye Teh and Arnaud Doucet", "title": "Expectation Particle Belief Propagation", "comments": "submitted to NIPS 2015", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.CO cs.AI stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose an original particle-based implementation of the Loopy Belief\nPropagation (LPB) algorithm for pairwise Markov Random Fields (MRF) on a\ncontinuous state space. The algorithm constructs adaptively efficient proposal\ndistributions approximating the local beliefs at each note of the MRF. This is\nachieved by considering proposal distributions in the exponential family whose\nparameters are updated iterately in an Expectation Propagation (EP) framework.\nThe proposed particle scheme provides consistent estimation of the LBP\nmarginals as the number of particles increases. We demonstrate that it provides\nmore accurate results than the Particle Belief Propagation (PBP) algorithm of\nIhler and McAllester (2009) at a fraction of the computational cost and is\nadditionally more robust empirically. The computational complexity of our\nalgorithm at each iteration is quadratic in the number of particles. We also\npropose an accelerated implementation with sub-quadratic computational\ncomplexity which still provides consistent estimates of the loopy BP marginal\ndistributions and performs almost as well as the original procedure.\n", "versions": [{"version": "v1", "created": "Fri, 19 Jun 2015 09:34:21 GMT"}], "update_date": "2015-06-22", "authors_parsed": [["Lienart", "Thibaut", ""], ["Teh", "Yee Whye", ""], ["Doucet", "Arnaud", ""]]}, {"id": "1506.05936", "submitter": "Shiwei Lan", "authors": "Shiwei Lan and Babak Shahbaba", "title": "Sampling constrained probability distributions using Spherical\n  Augmentation", "comments": "41 pages, 13 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.CO stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Statistical models with constrained probability distributions are abundant in\nmachine learning. Some examples include regression models with norm constraints\n(e.g., Lasso), probit, many copula models, and latent Dirichlet allocation\n(LDA). Bayesian inference involving probability distributions confined to\nconstrained domains could be quite challenging for commonly used sampling\nalgorithms. In this paper, we propose a novel augmentation technique that\nhandles a wide range of constraints by mapping the constrained domain to a\nsphere in the augmented space. By moving freely on the surface of this sphere,\nsampling algorithms handle constraints implicitly and generate proposals that\nremain within boundaries when mapped back to the original space. Our proposed\nmethod, called {Spherical Augmentation}, provides a mathematically natural and\ncomputationally efficient framework for sampling from constrained probability\ndistributions. We show the advantages of our method over state-of-the-art\nsampling algorithms, such as exact Hamiltonian Monte Carlo, using several\nexamples including truncated Gaussian distributions, Bayesian Lasso, Bayesian\nbridge regression, reconstruction of quantized stationary Gaussian process, and\nLDA for topic modeling.\n", "versions": [{"version": "v1", "created": "Fri, 19 Jun 2015 09:44:53 GMT"}], "update_date": "2015-06-22", "authors_parsed": [["Lan", "Shiwei", ""], ["Shahbaba", "Babak", ""]]}, {"id": "1506.05950", "submitter": "Tapio Pahikkala", "authors": "Tapio Pahikkala, Markus Viljanen, Antti Airola, Willem Waegeman", "title": "Spectral Analysis of Symmetric and Anti-Symmetric Pairwise Kernels", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider the problem of learning regression functions from pairwise data\nwhen there exists prior knowledge that the relation to be learned is symmetric\nor anti-symmetric. Such prior knowledge is commonly enforced by symmetrizing or\nanti-symmetrizing pairwise kernel functions. Through spectral analysis, we show\nthat these transformations reduce the kernel's effective dimension. Further, we\nprovide an analysis of the approximation properties of the resulting kernels,\nand bound the regularization bias of the kernels in terms of the corresponding\nbias of the original kernel.\n", "versions": [{"version": "v1", "created": "Fri, 19 Jun 2015 10:24:01 GMT"}], "update_date": "2015-06-22", "authors_parsed": [["Pahikkala", "Tapio", ""], ["Viljanen", "Markus", ""], ["Airola", "Antti", ""], ["Waegeman", "Willem", ""]]}, {"id": "1506.05967", "submitter": "Masaaki Imaizumi", "authors": "Masaaki Imaizumi, Kohei Hayashi", "title": "Doubly Decomposing Nonparametric Tensor Regression", "comments": "21 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Nonparametric extension of tensor regression is proposed. Nonlinearity in a\nhigh-dimensional tensor space is broken into simple local functions by\nincorporating low-rank tensor decomposition. Compared to naive nonparametric\napproaches, our formulation considerably improves the convergence rate of\nestimation while maintaining consistency with the same function class under\nspecific conditions. To estimate local functions, we develop a Bayesian\nestimator with the Gaussian process prior. Experimental results show its\ntheoretical properties and high performance in terms of predicting a summary\nstatistic of a real complex network.\n", "versions": [{"version": "v1", "created": "Fri, 19 Jun 2015 12:03:52 GMT"}, {"version": "v2", "created": "Mon, 29 Jun 2015 11:25:34 GMT"}, {"version": "v3", "created": "Tue, 8 Mar 2016 09:57:31 GMT"}], "update_date": "2016-03-09", "authors_parsed": [["Imaizumi", "Masaaki", ""], ["Hayashi", "Kohei", ""]]}, {"id": "1506.05985", "submitter": "Xavier Bresson", "authors": "Xavier Bresson and Thomas Laurent and James von Brecht", "title": "Enhanced Lasso Recovery on Graph", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This work aims at recovering signals that are sparse on graphs. Compressed\nsensing offers techniques for signal recovery from a few linear measurements\nand graph Fourier analysis provides a signal representation on graph. In this\npaper, we leverage these two frameworks to introduce a new Lasso recovery\nalgorithm on graphs. More precisely, we present a non-convex, non-smooth\nalgorithm that outperforms the standard convex Lasso technique. We carry out\nnumerical experiments on three benchmark graph datasets.\n", "versions": [{"version": "v1", "created": "Fri, 19 Jun 2015 12:59:18 GMT"}], "update_date": "2015-06-22", "authors_parsed": [["Bresson", "Xavier", ""], ["Laurent", "Thomas", ""], ["von Brecht", "James", ""]]}, {"id": "1506.06040", "submitter": "Esin Karahan", "authors": "Esin Karahan, Pedro A. Rojas-Lopez, Maria L. Bringas-Vega, Pedro A.\n  Valdes-Hernandez, Pedro A. Valdes-Sosa", "title": "Tensor Analysis and Fusion of Multimodal Brain Images", "comments": "23 pages, 15 figures, submitted to Proceedings of the IEEE", "journal-ref": null, "doi": "10.1109/JPROC.2015.2455028", "report-no": null, "categories": "stat.ME cs.NA stat.AP stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Current high-throughput data acquisition technologies probe dynamical systems\nwith different imaging modalities, generating massive data sets at different\nspatial and temporal resolutions posing challenging problems in multimodal data\nfusion. A case in point is the attempt to parse out the brain structures and\nnetworks that underpin human cognitive processes by analysis of different\nneuroimaging modalities (functional MRI, EEG, NIRS etc.). We emphasize that the\nmultimodal, multi-scale nature of neuroimaging data is well reflected by a\nmulti-way (tensor) structure where the underlying processes can be summarized\nby a relatively small number of components or \"atoms\". We introduce\nMarkov-Penrose diagrams - an integration of Bayesian DAG and tensor network\nnotation in order to analyze these models. These diagrams not only clarify\nmatrix and tensor EEG and fMRI time/frequency analysis and inverse problems,\nbut also help understand multimodal fusion via Multiway Partial Least Squares\nand Coupled Matrix-Tensor Factorization. We show here, for the first time, that\nGranger causal analysis of brain networks is a tensor regression problem, thus\nallowing the atomic decomposition of brain networks. Analysis of EEG and fMRI\nrecordings shows the potential of the methods and suggests their use in other\nscientific domains.\n", "versions": [{"version": "v1", "created": "Fri, 19 Jun 2015 15:03:22 GMT"}], "update_date": "2016-11-18", "authors_parsed": [["Karahan", "Esin", ""], ["Rojas-Lopez", "Pedro A.", ""], ["Bringas-Vega", "Maria L.", ""], ["Valdes-Hernandez", "Pedro A.", ""], ["Valdes-Sosa", "Pedro A.", ""]]}, {"id": "1506.06068", "submitter": "Teng Qiu", "authors": "Teng Qiu, Yongjie Li", "title": "A general framework for the IT-based clustering methods", "comments": "17 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG stat.ML", "license": "http://creativecommons.org/licenses/by-nc-sa/3.0/", "abstract": "  Previously, we proposed a physically inspired rule to organize the data\npoints in a sparse yet effective structure, called the in-tree (IT) graph,\nwhich is able to capture a wide class of underlying cluster structures in the\ndatasets, especially for the density-based datasets. Although there are some\nredundant edges or lines between clusters requiring to be removed by computer,\nthis IT graph has a big advantage compared with the k-nearest-neighborhood\n(k-NN) or the minimal spanning tree (MST) graph, in that the redundant edges in\nthe IT graph are much more distinguishable and thus can be easily determined by\nseveral methods previously proposed by us.\n  In this paper, we propose a general framework to re-construct the IT graph,\nbased on an initial neighborhood graph, such as the k-NN or MST, etc, and the\ncorresponding graph distances. For this general framework, our previous way of\nconstructing the IT graph turns out to be a special case of it. This general\nframework 1) can make the IT graph capture a wider class of underlying cluster\nstructures in the datasets, especially for the manifolds, and 2) should be more\neffective to cluster the sparse or graph-based datasets.\n", "versions": [{"version": "v1", "created": "Fri, 19 Jun 2015 16:03:31 GMT"}], "update_date": "2015-06-22", "authors_parsed": [["Qiu", "Teng", ""], ["Li", "Yongjie", ""]]}, {"id": "1506.06081", "submitter": "Qinqing Zheng", "authors": "Qinqing Zheng, John Lafferty", "title": "A Convergent Gradient Descent Algorithm for Rank Minimization and\n  Semidefinite Programming from Random Linear Measurements", "comments": "Fix a minor error in Appendix E", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a simple, scalable, and fast gradient descent algorithm to\noptimize a nonconvex objective for the rank minimization problem and a closely\nrelated family of semidefinite programs. With $O(r^3 \\kappa^2 n \\log n)$ random\nmeasurements of a positive semidefinite $n \\times n$ matrix of rank $r$ and\ncondition number $\\kappa$, our method is guaranteed to converge linearly to the\nglobal optimum.\n", "versions": [{"version": "v1", "created": "Fri, 19 Jun 2015 16:41:08 GMT"}, {"version": "v2", "created": "Thu, 8 Oct 2015 22:46:11 GMT"}, {"version": "v3", "created": "Thu, 24 Mar 2016 16:27:51 GMT"}], "update_date": "2016-03-25", "authors_parsed": [["Zheng", "Qinqing", ""], ["Lafferty", "John", ""]]}, {"id": "1506.06100", "submitter": "Guillaume Bouchard", "authors": "Guillaume Bouchard, Balaji Lakshminarayanan", "title": "Approximate Inference with the Variational Holder Bound", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG math.FA", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We introduce the Variational Holder (VH) bound as an alternative to\nVariational Bayes (VB) for approximate Bayesian inference. Unlike VB which\ntypically involves maximization of a non-convex lower bound with respect to the\nvariational parameters, the VH bound involves minimization of a convex upper\nbound to the intractable integral with respect to the variational parameters.\nMinimization of the VH bound is a convex optimization problem; hence the VH\nmethod can be applied using off-the-shelf convex optimization algorithms and\nthe approximation error of the VH bound can also be analyzed using tools from\nconvex optimization literature. We present experiments on the task of\nintegrating a truncated multivariate Gaussian distribution and compare our\nmethod to VB, EP and a state-of-the-art numerical integration method for this\nproblem.\n", "versions": [{"version": "v1", "created": "Fri, 19 Jun 2015 18:00:40 GMT"}], "update_date": "2015-06-22", "authors_parsed": [["Bouchard", "Guillaume", ""], ["Lakshminarayanan", "Balaji", ""]]}, {"id": "1506.06129", "submitter": "Paul Wiggins Dr", "authors": "Paul A. Wiggins", "title": "A simple application of FIC to model selection", "comments": "7 Pages, 1 figure, & Appendix. arXiv admin note: text overlap with\n  arXiv:1506.05855", "journal-ref": null, "doi": null, "report-no": null, "categories": "physics.data-an cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We have recently proposed a new information-based approach to model\nselection, the Frequentist Information Criterion (FIC), that reconciles\ninformation-based and frequentist inference. The purpose of this current paper\nis to provide a simple example of the application of this criterion and a\ndemonstration of the natural emergence of model complexities with both AIC-like\n($N^0$) and BIC-like ($\\log N$) scaling with observation number $N$. The\napplication developed is deliberately simplified to make the analysis\nanalytically tractable.\n", "versions": [{"version": "v1", "created": "Fri, 19 Jun 2015 00:39:43 GMT"}], "update_date": "2015-06-23", "authors_parsed": [["Wiggins", "Paul A.", ""]]}, {"id": "1506.06179", "submitter": "Aaron Clauset", "authors": "Amir Ghasemian and Pan Zhang and Aaron Clauset and Cristopher Moore\n  and Leto Peel", "title": "Detectability thresholds and optimal algorithms for community structure\n  in dynamic networks", "comments": "9 pages, 3 figures", "journal-ref": "Phys. Rev. X 6, 031005 (2016)", "doi": "10.1103/PhysRevX.6.031005", "report-no": null, "categories": "stat.ML cond-mat.dis-nn cs.LG cs.SI physics.data-an", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study the fundamental limits on learning latent community structure in\ndynamic networks. Specifically, we study dynamic stochastic block models where\nnodes change their community membership over time, but where edges are\ngenerated independently at each time step. In this setting (which is a special\ncase of several existing models), we are able to derive the detectability\nthreshold exactly, as a function of the rate of change and the strength of the\ncommunities. Below this threshold, we claim that no algorithm can identify the\ncommunities better than chance. We then give two algorithms that are optimal in\nthe sense that they succeed all the way down to this limit. The first uses\nbelief propagation (BP), which gives asymptotically optimal accuracy, and the\nsecond is a fast spectral clustering algorithm, based on linearizing the BP\nequations. We verify our analytic and algorithmic results via numerical\nsimulation, and close with a brief discussion of extensions and open questions.\n", "versions": [{"version": "v1", "created": "Fri, 19 Jun 2015 23:07:51 GMT"}], "update_date": "2016-07-20", "authors_parsed": [["Ghasemian", "Amir", ""], ["Zhang", "Pan", ""], ["Clauset", "Aaron", ""], ["Moore", "Cristopher", ""], ["Peel", "Leto", ""]]}, {"id": "1506.06272", "submitter": "Fei Sha", "authors": "Junqi Jin, Kun Fu, Runpeng Cui, Fei Sha and Changshui Zhang", "title": "Aligning where to see and what to tell: image caption with region-based\n  attention and scene factorization", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recent progress on automatic generation of image captions has shown that it\nis possible to describe the most salient information conveyed by images with\naccurate and meaningful sentences. In this paper, we propose an image caption\nsystem that exploits the parallel structures between images and sentences. In\nour model, the process of generating the next word, given the previously\ngenerated ones, is aligned with the visual perception experience where the\nattention shifting among the visual regions imposes a thread of visual\nordering. This alignment characterizes the flow of \"abstract meaning\", encoding\nwhat is semantically shared by both the visual scene and the text description.\nOur system also makes another novel modeling contribution by introducing\nscene-specific contexts that capture higher-level semantic information encoded\nin an image. The contexts adapt language models for word generation to specific\nscene types. We benchmark our system and contrast to published results on\nseveral popular datasets. We show that using either region-based attention or\nscene-specific contexts improves systems without those components. Furthermore,\ncombining these two modeling ingredients attains the state-of-the-art\nperformance.\n", "versions": [{"version": "v1", "created": "Sat, 20 Jun 2015 17:25:38 GMT"}], "update_date": "2015-06-23", "authors_parsed": [["Jin", "Junqi", ""], ["Fu", "Kun", ""], ["Cui", "Runpeng", ""], ["Sha", "Fei", ""], ["Zhang", "Changshui", ""]]}, {"id": "1506.06318", "submitter": "Shang-Tse Chen", "authors": "Shang-Tse Chen, Maria-Florina Balcan, Duen Horng Chau", "title": "Communication Efficient Distributed Agnostic Boosting", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider the problem of learning from distributed data in the agnostic\nsetting, i.e., in the presence of arbitrary forms of noise. Our main\ncontribution is a general distributed boosting-based procedure for learning an\narbitrary concept space, that is simultaneously noise tolerant, communication\nefficient, and computationally efficient. This improves significantly over\nprior works that were either communication efficient only in noise-free\nscenarios or computationally prohibitive. Empirical results on large synthetic\nand real-world datasets demonstrate the effectiveness and scalability of the\nproposed approach.\n", "versions": [{"version": "v1", "created": "Sun, 21 Jun 2015 04:35:42 GMT"}, {"version": "v2", "created": "Fri, 18 Nov 2016 16:55:03 GMT"}], "update_date": "2016-11-21", "authors_parsed": [["Chen", "Shang-Tse", ""], ["Balcan", "Maria-Florina", ""], ["Chau", "Duen Horng", ""]]}, {"id": "1506.06422", "submitter": "Justin Eldridge", "authors": "Justin Eldridge, Mikhail Belkin, Yusu Wang", "title": "Beyond Hartigan Consistency: Merge Distortion Metric for Hierarchical\n  Clustering", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Hierarchical clustering is a popular method for analyzing data which\nassociates a tree to a dataset. Hartigan consistency has been used extensively\nas a framework to analyze such clustering algorithms from a statistical point\nof view. Still, as we show in the paper, a tree which is Hartigan consistent\nwith a given density can look very different than the correct limit tree.\nSpecifically, Hartigan consistency permits two types of undesirable\nconfigurations which we term over-segmentation and improper nesting. Moreover,\nHartigan consistency is a limit property and does not directly quantify\ndifference between trees.\n  In this paper we identify two limit properties, separation and minimality,\nwhich address both over-segmentation and improper nesting and together imply\n(but are not implied by) Hartigan consistency. We proceed to introduce a merge\ndistortion metric between hierarchical clusterings and show that convergence in\nour distance implies both separation and minimality. We also prove that uniform\nseparation and minimality imply convergence in the merge distortion metric.\nFurthermore, we show that our merge distortion metric is stable under\nperturbations of the density.\n  Finally, we demonstrate applicability of these concepts by proving\nconvergence results for two clustering algorithms. First, we show convergence\n(and hence separation and minimality) of the recent robust single linkage\nalgorithm of Chaudhuri and Dasgupta (2010). Second, we provide convergence\nresults on manifolds for topological split tree clustering.\n", "versions": [{"version": "v1", "created": "Sun, 21 Jun 2015 23:19:37 GMT"}, {"version": "v2", "created": "Mon, 13 Jul 2015 18:01:07 GMT"}], "update_date": "2015-07-14", "authors_parsed": [["Eldridge", "Justin", ""], ["Belkin", "Mikhail", ""], ["Wang", "Yusu", ""]]}, {"id": "1506.06438", "submitter": "Christopher De Sa", "authors": "Christopher De Sa, Ce Zhang, Kunle Olukotun, Christopher R\\'e", "title": "Taming the Wild: A Unified Analysis of Hogwild!-Style Algorithms", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG math.OC stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Stochastic gradient descent (SGD) is a ubiquitous algorithm for a variety of\nmachine learning problems. Researchers and industry have developed several\ntechniques to optimize SGD's runtime performance, including asynchronous\nexecution and reduced precision. Our main result is a martingale-based analysis\nthat enables us to capture the rich noise models that may arise from such\ntechniques. Specifically, we use our new analysis in three ways: (1) we derive\nconvergence rates for the convex case (Hogwild!) with relaxed assumptions on\nthe sparsity of the problem; (2) we analyze asynchronous SGD algorithms for\nnon-convex matrix problems including matrix completion; and (3) we design and\nanalyze an asynchronous SGD algorithm, called Buckwild!, that uses\nlower-precision arithmetic. We show experimentally that our algorithms run\nefficiently for a variety of problems on modern hardware.\n", "versions": [{"version": "v1", "created": "Mon, 22 Jun 2015 01:48:39 GMT"}, {"version": "v2", "created": "Fri, 2 Oct 2015 22:46:57 GMT"}], "update_date": "2015-10-06", "authors_parsed": [["De Sa", "Christopher", ""], ["Zhang", "Ce", ""], ["Olukotun", "Kunle", ""], ["R\u00e9", "Christopher", ""]]}, {"id": "1506.06472", "submitter": "Peter Sadowski", "authors": "Pierre Baldi and Peter Sadowski", "title": "A Theory of Local Learning, the Learning Channel, and the Optimality of\n  Backpropagation", "comments": null, "journal-ref": "Neural Networks, vol. 83, pp. 51-74, Nov. 2016", "doi": "10.1016/j.neunet.2016.07.006", "report-no": null, "categories": "cs.LG cs.NE stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In a physical neural system, where storage and processing are intimately\nintertwined, the rules for adjusting the synaptic weights can only depend on\nvariables that are available locally, such as the activity of the pre- and\npost-synaptic neurons, resulting in local learning rules. A systematic\nframework for studying the space of local learning rules is obtained by first\nspecifying the nature of the local variables, and then the functional form that\nties them together into each learning rule. Such a framework enables also the\nsystematic discovery of new learning rules and exploration of relationships\nbetween learning rules and group symmetries. We study polynomial local learning\nrules stratified by their degree and analyze their behavior and capabilities in\nboth linear and non-linear units and networks. Stacking local learning rules in\ndeep feedforward networks leads to deep local learning. While deep local\nlearning can learn interesting representations, it cannot learn complex\ninput-output functions, even when targets are available for the top layer.\nLearning complex input-output functions requires local deep learning where\ntarget information is communicated to the deep layers through a backward\nlearning channel. The nature of the communicated information about the targets\nand the structure of the learning channel partition the space of learning\nalgorithms. We estimate the learning channel capacity associated with several\nalgorithms and show that backpropagation outperforms them by simultaneously\nmaximizing the information rate and minimizing the computational cost, even in\nrecurrent networks. The theory clarifies the concept of Hebbian learning,\nestablishes the power and limitations of local learning rules, introduces the\nlearning channel which enables a formal analysis of the optimality of\nbackpropagation, and explains the sparsity of the space of learning rules\ndiscovered so far.\n", "versions": [{"version": "v1", "created": "Mon, 22 Jun 2015 05:16:57 GMT"}, {"version": "v2", "created": "Fri, 21 Oct 2016 23:24:25 GMT"}], "update_date": "2016-10-25", "authors_parsed": [["Baldi", "Pierre", ""], ["Sadowski", "Peter", ""]]}, {"id": "1506.06573", "submitter": "Akshay Balsubramani", "authors": "Akshay Balsubramani", "title": "PAC-Bayes Iterated Logarithm Bounds for Martingale Mixtures", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG math.PR stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We give tight concentration bounds for mixtures of martingales that are\nsimultaneously uniform over (a) mixture distributions, in a PAC-Bayes sense;\nand (b) all finite times. These bounds are proved in terms of the martingale\nvariance, extending classical Bernstein inequalities, and sharpening and\nsimplifying prior work.\n", "versions": [{"version": "v1", "created": "Mon, 22 Jun 2015 12:47:07 GMT"}], "update_date": "2015-06-23", "authors_parsed": [["Balsubramani", "Akshay", ""]]}, {"id": "1506.06646", "submitter": "Tadahiro Taniguchi", "authors": "Tadahiro Taniguchi, Ryo Nakashima, and Shogo Nagasaka", "title": "Nonparametric Bayesian Double Articulation Analyzer for Direct Language\n  Acquisition from Continuous Speech Signals", "comments": "15 pages, 7 figures, Draft submitted to IEEE Transactions on\n  Autonomous Mental Development (TAMD)", "journal-ref": null, "doi": "10.1109/TCDS.2016.2550591", "report-no": null, "categories": "cs.AI cs.CL cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Human infants can discover words directly from unsegmented speech signals\nwithout any explicitly labeled data. In this paper, we develop a novel machine\nlearning method called nonparametric Bayesian double articulation analyzer\n(NPB-DAA) that can directly acquire language and acoustic models from observed\ncontinuous speech signals. For this purpose, we propose an integrative\ngenerative model that combines a language model and an acoustic model into a\nsingle generative model called the \"hierarchical Dirichlet process hidden\nlanguage model\" (HDP-HLM). The HDP-HLM is obtained by extending the\nhierarchical Dirichlet process hidden semi-Markov model (HDP-HSMM) proposed by\nJohnson et al. An inference procedure for the HDP-HLM is derived using the\nblocked Gibbs sampler originally proposed for the HDP-HSMM. This procedure\nenables the simultaneous and direct inference of language and acoustic models\nfrom continuous speech signals. Based on the HDP-HLM and its inference\nprocedure, we developed a novel double articulation analyzer. By assuming\nHDP-HLM as a generative model of observed time series data, and by inferring\nlatent variables of the model, the method can analyze latent double\narticulation structure, i.e., hierarchically organized latent words and\nphonemes, of the data in an unsupervised manner. The novel unsupervised double\narticulation analyzer is called NPB-DAA.\n  The NPB-DAA can automatically estimate double articulation structure embedded\nin speech signals. We also carried out two evaluation experiments using\nsynthetic data and actual human continuous speech signals representing Japanese\nvowel sequences. In the word acquisition and phoneme categorization tasks, the\nNPB-DAA outperformed a conventional double articulation analyzer (DAA) and\nbaseline automatic speech recognition system whose acoustic model was trained\nin a supervised manner.\n", "versions": [{"version": "v1", "created": "Mon, 22 Jun 2015 15:21:57 GMT"}, {"version": "v2", "created": "Wed, 9 Mar 2016 15:59:07 GMT"}], "update_date": "2016-11-17", "authors_parsed": [["Taniguchi", "Tadahiro", ""], ["Nakashima", "Ryo", ""], ["Nagasaka", "Shogo", ""]]}, {"id": "1506.06650", "submitter": "Syed Awais Wahab Shah", "authors": "Syed A. W. Shah, Karim Abed-Meraim and Tareq Y. Al-Naffouri", "title": "Blind Source Separation Algorithms Using Hyperbolic and Givens Rotations\n  for High-Order QAM Constellations", "comments": "13 pages, 11 figures, submitted to IEEE Trans. Signal Process,\n  updated: New algorithms added", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IT math.IT stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper addresses the problem of blind demixing of instantaneous mixtures\nin a multiple-input multiple-output communication system. The main objective is\nto present efficient blind source separation (BSS) algorithms dedicated to\nmoderate or high-order QAM constellations. Four new iterative batch BSS\nalgorithms are presented dealing with the multimodulus (MM) and alphabet\nmatched (AM) criteria. For the optimization of these cost functions, iterative\nmethods of Givens and hyperbolic rotations are used. A pre-whitening operation\nis also utilized to reduce the complexity of design problem. It is noticed that\nthe designed algorithms using Givens rotations gives satisfactory performance\nonly for large number of samples. However, for small number of samples, the\nalgorithms designed by combining both Givens and hyperbolic rotations\ncompensate for the ill-whitening that occurs in this case and thus improves the\nperformance. Two algorithms dealing with the MM criterion are presented for\nmoderate order QAM signals such as 16-QAM. The other two dealing with the AM\ncriterion are presented for high-order QAM signals. These methods are finally\ncompared with the state of art batch BSS algorithms in terms of\nsignal-to-interference and noise ratio, symbol error rate and convergence rate.\nSimulation results show that the proposed methods outperform the contemporary\nbatch BSS algorithms.\n", "versions": [{"version": "v1", "created": "Mon, 22 Jun 2015 15:26:49 GMT"}, {"version": "v2", "created": "Wed, 29 Jun 2016 18:09:55 GMT"}], "update_date": "2016-06-30", "authors_parsed": [["Shah", "Syed A. W.", ""], ["Abed-Meraim", "Karim", ""], ["Al-Naffouri", "Tareq Y.", ""]]}, {"id": "1506.06707", "submitter": "Faicel Chamroukhi", "authors": "Faicel Chamroukhi", "title": "Non-Normal Mixtures of Experts", "comments": "61 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Mixture of Experts (MoE) is a popular framework for modeling heterogeneity in\ndata for regression, classification and clustering. For continuous data which\nwe consider here in the context of regression and cluster analysis, MoE usually\nuse normal experts, that is, expert components following the Gaussian\ndistribution. However, for a set of data containing a group or groups of\nobservations with asymmetric behavior, heavy tails or atypical observations,\nthe use of normal experts may be unsuitable and can unduly affect the fit of\nthe MoE model. In this paper, we introduce new non-normal mixture of experts\n(NNMoE) which can deal with these issues regarding possibly skewed,\nheavy-tailed data and with outliers. The proposed models are the skew-normal\nMoE and the robust $t$ MoE and skew $t$ MoE, respectively named SNMoE, TMoE and\nSTMoE. We develop dedicated expectation-maximization (EM) and expectation\nconditional maximization (ECM) algorithms to estimate the parameters of the\nproposed models by monotonically maximizing the observed data log-likelihood.\nWe describe how the presented models can be used in prediction and in\nmodel-based clustering of regression data. Numerical experiments carried out on\nsimulated data show the effectiveness and the robustness of the proposed models\nin terms modeling non-linear regression functions as well as in model-based\nclustering. Then, to show their usefulness for practical applications, the\nproposed models are applied to the real-world data of tone perception for\nmusical data analysis, and the one of temperature anomalies for the analysis of\nclimate change data.\n", "versions": [{"version": "v1", "created": "Mon, 22 Jun 2015 18:12:36 GMT"}, {"version": "v2", "created": "Sun, 28 Jun 2015 14:18:03 GMT"}], "update_date": "2015-06-30", "authors_parsed": [["Chamroukhi", "Faicel", ""]]}, {"id": "1506.06840", "submitter": "Sashank J. Reddi", "authors": "Sashank J. Reddi, Ahmed Hefny, Suvrit Sra, Barnab\\'as P\\'oczos, Alex\n  Smola", "title": "On Variance Reduction in Stochastic Gradient Descent and its\n  Asynchronous Variants", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study optimization algorithms based on variance reduction for stochastic\ngradient descent (SGD). Remarkable recent progress has been made in this\ndirection through development of algorithms like SAG, SVRG, SAGA. These\nalgorithms have been shown to outperform SGD, both theoretically and\nempirically. However, asynchronous versions of these algorithms---a crucial\nrequirement for modern large-scale applications---have not been studied. We\nbridge this gap by presenting a unifying framework for many variance reduction\ntechniques. Subsequently, we propose an asynchronous algorithm grounded in our\nframework, and prove its fast convergence. An important consequence of our\ngeneral approach is that it yields asynchronous versions of variance reduction\nalgorithms such as SVRG and SAGA as a byproduct. Our method achieves near\nlinear speedup in sparse settings common to machine learning. We demonstrate\nthe empirical performance of our method through a concrete realization of\nasynchronous SVRG.\n", "versions": [{"version": "v1", "created": "Tue, 23 Jun 2015 01:57:19 GMT"}, {"version": "v2", "created": "Mon, 25 Jan 2016 01:12:06 GMT"}], "update_date": "2016-01-26", "authors_parsed": [["Reddi", "Sashank J.", ""], ["Hefny", "Ahmed", ""], ["Sra", "Suvrit", ""], ["P\u00f3czos", "Barnab\u00e1s", ""], ["Smola", "Alex", ""]]}, {"id": "1506.06962", "submitter": "Fabrice Rossi", "authors": "Pierre Latouche (SAMM), Fabrice Rossi (SAMM)", "title": "Graphs in machine learning: an introduction", "comments": null, "journal-ref": "European Symposium on Artificial Neural Networks, Computational\n  Intelligence and Machine Learning (ESANN), Apr 2015, Bruges, Belgium.\n  pp.207-218, 2015, Proceedings of the 23-th European Symposium on Artificial\n  Neural Networks, Computational Intelligence and Machine Learning (ESANN 2015)", "doi": null, "report-no": null, "categories": "stat.ML cs.LG cs.SI physics.soc-ph", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Graphs are commonly used to characterise interactions between objects of\ninterest. Because they are based on a straightforward formalism, they are used\nin many scientific fields from computer science to historical sciences. In this\npaper, we give an introduction to some methods relying on graphs for learning.\nThis includes both unsupervised and supervised methods. Unsupervised learning\nalgorithms usually aim at visualising graphs in latent spaces and/or clustering\nthe nodes. Both focus on extracting knowledge from graph topologies. While most\nexisting techniques are only applicable to static graphs, where edges do not\nevolve through time, recent developments have shown that they could be extended\nto deal with evolving networks. In a supervised context, one generally aims at\ninferring labels or numerical values attached to nodes using both the graph\nand, when they are available, node characteristics. Balancing the two sources\nof information can be challenging, especially as they can disagree locally or\nglobally. In both contexts, supervised and un-supervised, data can be\nrelational (augmented with one or several global graphs) as described above, or\ngraph valued. In this latter case, each object of interest is given as a full\ngraph (possibly completed by other characteristics). In this context, natural\ntasks include graph clustering (as in producing clusters of graphs rather than\nclusters of nodes in a single graph), graph classification, etc. 1 Real\nnetworks One of the first practical studies on graphs can be dated back to the\noriginal work of Moreno [51] in the 30s. Since then, there has been a growing\ninterest in graph analysis associated with strong developments in the modelling\nand the processing of these data. Graphs are now used in many scientific\nfields. In Biology [54, 2, 7], for instance, metabolic networks can describe\npathways of biochemical reactions [41], while in social sciences networks are\nused to represent relation ties between actors [66, 56, 36, 34]. Other examples\ninclude powergrids [71] and the web [75]. Recently, networks have also been\nconsidered in other areas such as geography [22] and history [59, 39]. In\nmachine learning, networks are seen as powerful tools to model problems in\norder to extract information from data and for prediction purposes. This is the\nobject of this paper. For more complete surveys, we refer to [28, 62, 49, 45].\nIn this section, we introduce notations and highlight properties shared by most\nreal networks. In Section 2, we then consider methods aiming at extracting\ninformation from a unique network. We will particularly focus on clustering\nmethods where the goal is to find clusters of vertices. Finally, in Section 3,\ntechniques that take a series of networks into account, where each network is\n", "versions": [{"version": "v1", "created": "Tue, 23 Jun 2015 12:12:45 GMT"}], "update_date": "2015-06-24", "authors_parsed": [["Latouche", "Pierre", "", "SAMM"], ["Rossi", "Fabrice", "", "SAMM"]]}, {"id": "1506.06972", "submitter": "Gergo Barta", "authors": "Gergo Barta, Gyula Borbely, Gabor Nagy, Sandor Kazi, Tamas Henk", "title": "GEFCOM 2014 - Probabilistic Electricity Price Forecasting", "comments": "10 pages, 5 figures, KES-IDT 2015 conference. The final publication\n  is available at Springer via http://dx.doi.org/10.1007/978-3-319-19857-6_7", "journal-ref": null, "doi": "10.1007/978-3-319-19857-6_7", "report-no": null, "categories": "stat.ML cs.CE cs.LG stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Energy price forecasting is a relevant yet hard task in the field of\nmulti-step time series forecasting. In this paper we compare a well-known and\nestablished method, ARMA with exogenous variables with a relatively new\ntechnique Gradient Boosting Regression. The method was tested on data from\nGlobal Energy Forecasting Competition 2014 with a year long rolling window\nforecast. The results from the experiment reveal that a multi-model approach is\nsignificantly better performing in terms of error metrics. Gradient Boosting\ncan deal with seasonality and auto-correlation out-of-the box and achieve lower\nrate of normalized mean absolute error on real-world data.\n", "versions": [{"version": "v1", "created": "Tue, 23 Jun 2015 12:27:50 GMT"}], "update_date": "2015-06-24", "authors_parsed": [["Barta", "Gergo", ""], ["Borbely", "Gyula", ""], ["Nagy", "Gabor", ""], ["Kazi", "Sandor", ""], ["Henk", "Tamas", ""]]}, {"id": "1506.06975", "submitter": "Johan Dahlin", "authors": "Johan Dahlin, Mattias Villani and Thomas B. Sch\\\"on", "title": "Bayesian optimisation for fast approximate inference in state-space\n  models with intractable likelihoods", "comments": "24 pages, 7 figures. Submitted to journal for review", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.CO q-fin.RM stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider the problem of approximate Bayesian parameter inference in\nnon-linear state-space models with intractable likelihoods. Sequential Monte\nCarlo with approximate Bayesian computations (SMC-ABC) is one approach to\napproximate the likelihood in this type of models. However, such approximations\ncan be noisy and computationally costly which hinders efficient implementations\nusing standard methods based on optimisation and Monte Carlo methods. We\npropose a computationally efficient novel method based on the combination of\nGaussian process optimisation and SMC-ABC to create a Laplace approximation of\nthe intractable posterior. We exemplify the proposed algorithm for inference in\nstochastic volatility models with both synthetic and real-world data as well as\nfor estimating the Value-at-Risk for two portfolios using a copula model. We\ndocument speed-ups of between one and two orders of magnitude compared to\nstate-of-the-art algorithms for posterior inference.\n", "versions": [{"version": "v1", "created": "Tue, 23 Jun 2015 13:04:03 GMT"}, {"version": "v2", "created": "Tue, 20 Dec 2016 07:54:19 GMT"}, {"version": "v3", "created": "Tue, 13 Jun 2017 13:20:12 GMT"}], "update_date": "2017-06-14", "authors_parsed": [["Dahlin", "Johan", ""], ["Villani", "Mattias", ""], ["Sch\u00f6n", "Thomas B.", ""]]}, {"id": "1506.07216", "submitter": "Tengyu Ma", "authors": "Mark Braverman, Ankit Garg, Tengyu Ma, Huy L. Nguyen, and David P.\n  Woodruff", "title": "Communication Lower Bounds for Statistical Estimation Problems via a\n  Distributed Data Processing Inequality", "comments": "To appear at STOC 2016. Fixed typos in theorem 4.5 and incorporated\n  reviewers' suggestions", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CC cs.IT math.IT stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study the tradeoff between the statistical error and communication cost of\ndistributed statistical estimation problems in high dimensions. In the\ndistributed sparse Gaussian mean estimation problem, each of the $m$ machines\nreceives $n$ data points from a $d$-dimensional Gaussian distribution with\nunknown mean $\\theta$ which is promised to be $k$-sparse. The machines\ncommunicate by message passing and aim to estimate the mean $\\theta$. We\nprovide a tight (up to logarithmic factors) tradeoff between the estimation\nerror and the number of bits communicated between the machines. This directly\nleads to a lower bound for the distributed \\textit{sparse linear regression}\nproblem: to achieve the statistical minimax error, the total communication is\nat least $\\Omega(\\min\\{n,d\\}m)$, where $n$ is the number of observations that\neach machine receives and $d$ is the ambient dimension. These lower results\nimprove upon [Sha14,SD'14] by allowing multi-round iterative communication\nmodel. We also give the first optimal simultaneous protocol in the dense case\nfor mean estimation.\n  As our main technique, we prove a \\textit{distributed data processing\ninequality}, as a generalization of usual data processing inequalities, which\nmight be of independent interest and useful for other problems.\n", "versions": [{"version": "v1", "created": "Wed, 24 Jun 2015 01:01:41 GMT"}, {"version": "v2", "created": "Sun, 22 Nov 2015 23:37:03 GMT"}, {"version": "v3", "created": "Tue, 10 May 2016 00:58:29 GMT"}], "update_date": "2016-05-11", "authors_parsed": [["Braverman", "Mark", ""], ["Garg", "Ankit", ""], ["Ma", "Tengyu", ""], ["Nguyen", "Huy L.", ""], ["Woodruff", "David P.", ""]]}, {"id": "1506.07251", "submitter": "Kevin Vervier", "authors": "K\\'evin Vervier (CBIO), Pierre Mah\\'e, Jean-Baptiste Veyrieras,\n  Jean-Philippe Vert (CBIO)", "title": "Benchmark of structured machine learning methods for microbial\n  identification from mass-spectrometry data", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG q-bio.QM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Microbial identification is a central issue in microbiology, in particular in\nthe fields of infectious diseases diagnosis and industrial quality control. The\nconcept of species is tightly linked to the concept of biological and clinical\nclassification where the proximity between species is generally measured in\nterms of evolutionary distances and/or clinical phenotypes. Surprisingly, the\ninformation provided by this well-known hierarchical structure is rarely used\nby machine learning-based automatic microbial identification systems.\nStructured machine learning methods were recently proposed for taking into\naccount the structure embedded in a hierarchy and using it as additional a\npriori information, and could therefore allow to improve microbial\nidentification systems. We test and compare several state-of-the-art machine\nlearning methods for microbial identification on a new Matrix-Assisted Laser\nDesorption/Ionization Time-of-Flight mass spectrometry (MALDI-TOF MS) dataset.\nWe include in the benchmark standard and structured methods, that leverage the\nknowledge of the underlying hierarchical structure in the learning process. Our\nresults show that although some methods perform better than others, structured\nmethods do not consistently perform better than their \"flat\" counterparts. We\npostulate that this is partly due to the fact that standard methods already\nreach a high level of accuracy in this context, and that they mainly confuse\nspecies close to each other in the tree, a case where using the known hierarchy\nis not helpful.\n", "versions": [{"version": "v1", "created": "Wed, 24 Jun 2015 06:13:15 GMT"}], "update_date": "2015-06-25", "authors_parsed": [["Vervier", "K\u00e9vin", "", "CBIO"], ["Mah\u00e9", "Pierre", "", "CBIO"], ["Veyrieras", "Jean-Baptiste", "", "CBIO"], ["Vert", "Jean-Philippe", "", "CBIO"]]}, {"id": "1506.07365", "submitter": "Manuel Watter", "authors": "Manuel Watter, Jost Tobias Springenberg, Joschka Boedecker, Martin\n  Riedmiller", "title": "Embed to Control: A Locally Linear Latent Dynamics Model for Control\n  from Raw Images", "comments": "Final NIPS version", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We introduce Embed to Control (E2C), a method for model learning and control\nof non-linear dynamical systems from raw pixel images. E2C consists of a deep\ngenerative model, belonging to the family of variational autoencoders, that\nlearns to generate image trajectories from a latent space in which the dynamics\nis constrained to be locally linear. Our model is derived directly from an\noptimal control formulation in latent space, supports long-term prediction of\nimage sequences and exhibits strong performance on a variety of complex control\nproblems.\n", "versions": [{"version": "v1", "created": "Wed, 24 Jun 2015 13:48:51 GMT"}, {"version": "v2", "created": "Thu, 25 Jun 2015 21:08:02 GMT"}, {"version": "v3", "created": "Fri, 20 Nov 2015 14:49:18 GMT"}], "update_date": "2015-11-23", "authors_parsed": [["Watter", "Manuel", ""], ["Springenberg", "Jost Tobias", ""], ["Boedecker", "Joschka", ""], ["Riedmiller", "Martin", ""]]}, {"id": "1506.07405", "submitter": "Dejiao Zhang", "authors": "Dejiao Zhang, Laura Balzano", "title": "Global Convergence of a Grassmannian Gradient Descent Algorithm for\n  Subspace Estimation", "comments": "23 pages, 10 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NA math.NA stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  It has been observed in a variety of contexts that gradient descent methods\nhave great success in solving low-rank matrix factorization problems, despite\nthe relevant problem formulation being non-convex. We tackle a particular\ninstance of this scenario, where we seek the $d$-dimensional subspace spanned\nby a streaming data matrix. We apply the natural first order incremental\ngradient descent method, constraining the gradient method to the Grassmannian.\nIn this paper, we propose an adaptive step size scheme that is greedy for the\nnoiseless case, that maximizes the improvement of our metric of convergence at\neach data index $t$, and yields an expected improvement for the noisy case. We\nshow that, with noise-free data, this method converges from any random\ninitialization to the global minimum of the problem. For noisy data, we provide\nthe expected convergence rate of the proposed algorithm per iteration.\n", "versions": [{"version": "v1", "created": "Wed, 24 Jun 2015 14:59:27 GMT"}, {"version": "v2", "created": "Mon, 25 Apr 2016 18:31:20 GMT"}, {"version": "v3", "created": "Sat, 25 Jun 2016 01:13:37 GMT"}], "update_date": "2016-06-28", "authors_parsed": [["Zhang", "Dejiao", ""], ["Balzano", "Laura", ""]]}, {"id": "1506.07477", "submitter": "Jiatao Gu", "authors": "Jiatao Gu and Victor O.K. Li", "title": "Efficient Learning for Undirected Topic Models", "comments": "Accepted by ACL-IJCNLP 2015 short paper. 6 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CL cs.IR stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Replicated Softmax model, a well-known undirected topic model, is powerful in\nextracting semantic representations of documents. Traditional learning\nstrategies such as Contrastive Divergence are very inefficient. This paper\nprovides a novel estimator to speed up the learning based on Noise Contrastive\nEstimate, extended for documents of variant lengths and weighted inputs.\nExperiments on two benchmarks show that the new estimator achieves great\nlearning efficiency and high accuracy on document retrieval and classification.\n", "versions": [{"version": "v1", "created": "Wed, 24 Jun 2015 17:27:28 GMT"}], "update_date": "2015-06-25", "authors_parsed": [["Gu", "Jiatao", ""], ["Li", "Victor O. K.", ""]]}, {"id": "1506.07503", "submitter": "Jan Chorowski", "authors": "Jan Chorowski, Dzmitry Bahdanau, Dmitriy Serdyuk, Kyunghyun Cho,\n  Yoshua Bengio", "title": "Attention-Based Models for Speech Recognition", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.LG cs.NE stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recurrent sequence generators conditioned on input data through an attention\nmechanism have recently shown very good performance on a range of tasks in-\ncluding machine translation, handwriting synthesis and image caption gen-\neration. We extend the attention-mechanism with features needed for speech\nrecognition. We show that while an adaptation of the model used for machine\ntranslation in reaches a competitive 18.7% phoneme error rate (PER) on the\nTIMIT phoneme recognition task, it can only be applied to utterances which are\nroughly as long as the ones it was trained on. We offer a qualitative\nexplanation of this failure and propose a novel and generic method of adding\nlocation-awareness to the attention mechanism to alleviate this issue. The new\nmethod yields a model that is robust to long inputs and achieves 18% PER in\nsingle utterances and 20% in 10-times longer (repeated) utterances. Finally, we\npropose a change to the at- tention mechanism that prevents it from\nconcentrating too much on single frames, which further reduces PER to 17.6%\nlevel.\n", "versions": [{"version": "v1", "created": "Wed, 24 Jun 2015 19:10:33 GMT"}], "update_date": "2015-06-25", "authors_parsed": [["Chorowski", "Jan", ""], ["Bahdanau", "Dzmitry", ""], ["Serdyuk", "Dmitriy", ""], ["Cho", "Kyunghyun", ""], ["Bengio", "Yoshua", ""]]}, {"id": "1506.07504", "submitter": "Maja Rudolph", "authors": "Maja R. Rudolph, Joseph G. Ellis, and David M. Blei", "title": "Objective Variables for Probabilistic Revenue Maximization in\n  Second-Price Auctions with Reserve", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.AI cs.GT cs.LG stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Many online companies sell advertisement space in second-price auctions with\nreserve. In this paper, we develop a probabilistic method to learn a profitable\nstrategy to set the reserve price. We use historical auction data with features\nto fit a predictor of the best reserve price. This problem is delicate - the\nstructure of the auction is such that a reserve price set too high is much\nworse than a reserve price set too low. To address this we develop objective\nvariables, a new framework for combining probabilistic modeling with optimal\ndecision-making. Objective variables are \"hallucinated observations\" that\ntransform the revenue maximization task into a regularized maximum likelihood\nestimation problem, which we solve with an EM algorithm. This framework enables\na variety of prediction mechanisms to set the reserve price. As examples, we\nstudy objective variable methods with regression, kernelized regression, and\nneural networks on simulated and real data. Our methods outperform previous\napproaches both in terms of scalability and profit.\n", "versions": [{"version": "v1", "created": "Wed, 24 Jun 2015 19:20:18 GMT"}], "update_date": "2015-06-25", "authors_parsed": [["Rudolph", "Maja R.", ""], ["Ellis", "Joseph G.", ""], ["Blei", "David M.", ""]]}, {"id": "1506.07512", "submitter": "Roy Frostig", "authors": "Roy Frostig, Rong Ge, Sham M. Kakade, Aaron Sidford", "title": "Un-regularizing: approximate proximal point and faster stochastic\n  algorithms for empirical risk minimization", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.DS cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We develop a family of accelerated stochastic algorithms that minimize sums\nof convex functions. Our algorithms improve upon the fastest running time for\nempirical risk minimization (ERM), and in particular linear least-squares\nregression, across a wide range of problem settings. To achieve this, we\nestablish a framework based on the classical proximal point algorithm. Namely,\nwe provide several algorithms that reduce the minimization of a strongly convex\nfunction to approximate minimizations of regularizations of the function. Using\nthese results, we accelerate recent fast stochastic algorithms in a black-box\nfashion. Empirically, we demonstrate that the resulting algorithms exhibit\nnotions of stability that are advantageous in practice. Both in theory and in\npractice, the provided algorithms reap the computational benefits of adding a\nlarge strongly convex regularization term, without incurring a corresponding\nbias to the original problem.\n", "versions": [{"version": "v1", "created": "Wed, 24 Jun 2015 19:53:45 GMT"}], "update_date": "2015-06-25", "authors_parsed": [["Frostig", "Roy", ""], ["Ge", "Rong", ""], ["Kakade", "Sham M.", ""], ["Sidford", "Aaron", ""]]}, {"id": "1506.07540", "submitter": "Benjamin Haeffele", "authors": "Benjamin D. Haeffele and Rene Vidal", "title": "Global Optimality in Tensor Factorization, Deep Learning, and Beyond", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NA cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Techniques involving factorization are found in a wide range of applications\nand have enjoyed significant empirical success in many fields. However, common\nto a vast majority of these problems is the significant disadvantage that the\nassociated optimization problems are typically non-convex due to a multilinear\nform or other convexity destroying transformation. Here we build on ideas from\nconvex relaxations of matrix factorizations and present a very general\nframework which allows for the analysis of a wide range of non-convex\nfactorization problems - including matrix factorization, tensor factorization,\nand deep neural network training formulations. We derive sufficient conditions\nto guarantee that a local minimum of the non-convex optimization problem is a\nglobal minimum and show that if the size of the factorized variables is large\nenough then from any initialization it is possible to find a global minimizer\nusing a purely local descent algorithm. Our framework also provides a partial\ntheoretical justification for the increasingly common use of Rectified Linear\nUnits (ReLUs) in deep neural networks and offers guidance on deep network\narchitectures and regularization strategies to facilitate efficient\noptimization.\n", "versions": [{"version": "v1", "created": "Wed, 24 Jun 2015 20:08:47 GMT"}], "update_date": "2015-06-26", "authors_parsed": [["Haeffele", "Benjamin D.", ""], ["Vidal", "Rene", ""]]}, {"id": "1506.07609", "submitter": "Vikas Garg", "authors": "Vikas K. Garg, Cynthia Rudin, and Tommi Jaakkola", "title": "CRAFT: ClusteR-specific Assorted Feature selecTion", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a framework for clustering with cluster-specific feature\nselection. The framework, CRAFT, is derived from asymptotic log posterior\nformulations of nonparametric MAP-based clustering models. CRAFT handles\nassorted data, i.e., both numeric and categorical data, and the underlying\nobjective functions are intuitively appealing. The resulting algorithm is\nsimple to implement and scales nicely, requires minimal parameter tuning,\nobviates the need to specify the number of clusters a priori, and compares\nfavorably with other methods on real datasets.\n", "versions": [{"version": "v1", "created": "Thu, 25 Jun 2015 04:14:49 GMT"}], "update_date": "2015-06-26", "authors_parsed": [["Garg", "Vikas K.", ""], ["Rudin", "Cynthia", ""], ["Jaakkola", "Tommi", ""]]}, {"id": "1506.07611", "submitter": "Brian Baingana Mr", "authors": "Brian Baingana and Georgios B. Giannakis", "title": "Joint community and anomaly tracking in dynamic networks", "comments": "13 pages", "journal-ref": null, "doi": "10.1109/TSP.2015.2510971", "report-no": null, "categories": "stat.ML cs.SI physics.soc-ph", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Most real-world networks exhibit community structure, a phenomenon\ncharacterized by existence of node clusters whose intra-edge connectivity is\nstronger than edge connectivities between nodes belonging to different\nclusters. In addition to facilitating a better understanding of network\nbehavior, community detection finds many practical applications in diverse\nsettings. Communities in online social networks are indicative of shared\nfunctional roles, or affiliation to a common socio-economic status, the\nknowledge of which is vital for targeted advertisement. In buyer-seller\nnetworks, community detection facilitates better product recommendations.\nUnfortunately, reliability of community assignments is hindered by anomalous\nuser behavior often observed as unfair self-promotion, or \"fake\"\nhighly-connected accounts created to promote fraud. The present paper advocates\na novel approach for jointly tracking communities while detecting such\nanomalous nodes in time-varying networks. By postulating edge creation as the\nresult of mutual community participation by node pairs, a dynamic factor model\nwith anomalous memberships captured through a sparse outlier matrix is put\nforth. Efficient tracking algorithms suitable for both online and decentralized\noperation are developed. Experiments conducted on both synthetic and real\nnetwork time series successfully unveil underlying communities and anomalous\nnodes.\n", "versions": [{"version": "v1", "created": "Thu, 25 Jun 2015 04:40:14 GMT"}], "update_date": "2016-04-20", "authors_parsed": [["Baingana", "Brian", ""], ["Giannakis", "Georgios B.", ""]]}, {"id": "1506.07613", "submitter": "Sobhan Naderi Parizi", "authors": "Sobhan Naderi Parizi, Kun He, Reza Aghajani, Stan Sclaroff, Pedro\n  Felzenszwalb", "title": "Generalized Majorization-Minimization", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.IT cs.LG math.IT stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Non-convex optimization is ubiquitous in machine learning.\nMajorization-Minimization (MM) is a powerful iterative procedure for optimizing\nnon-convex functions that works by optimizing a sequence of bounds on the\nfunction. In MM, the bound at each iteration is required to \\emph{touch} the\nobjective function at the optimizer of the previous bound. We show that this\ntouching constraint is unnecessary and overly restrictive. We generalize MM by\nrelaxing this constraint, and propose a new optimization framework, named\nGeneralized Majorization-Minimization (G-MM), that is more flexible. For\ninstance, G-MM can incorporate application-specific biases into the\noptimization procedure without changing the objective function. We derive G-MM\nalgorithms for several latent variable models and show empirically that they\nconsistently outperform their MM counterparts in optimizing non-convex\nobjectives. In particular, G-MM algorithms appear to be less sensitive to\ninitialization.\n", "versions": [{"version": "v1", "created": "Thu, 25 Jun 2015 04:56:50 GMT"}, {"version": "v2", "created": "Thu, 28 Jul 2016 04:47:13 GMT"}, {"version": "v3", "created": "Fri, 17 May 2019 17:13:53 GMT"}], "update_date": "2019-05-20", "authors_parsed": [["Parizi", "Sobhan Naderi", ""], ["He", "Kun", ""], ["Aghajani", "Reza", ""], ["Sclaroff", "Stan", ""], ["Felzenszwalb", "Pedro", ""]]}, {"id": "1506.07615", "submitter": "Hongyang Zhang", "authors": "Hongyang Zhang, Zhouchen Lin, Chao Zhang", "title": "Completing Low-Rank Matrices with Corrupted Samples from Few\n  Coefficients in General Basis", "comments": "To appear in IEEE Transactions on Information Theory", "journal-ref": null, "doi": "10.1109/TIT.2016.2573311", "report-no": null, "categories": "cs.IT cs.LG cs.NA math.IT math.NA stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Subspace recovery from corrupted and missing data is crucial for various\napplications in signal processing and information theory. To complete missing\nvalues and detect column corruptions, existing robust Matrix Completion (MC)\nmethods mostly concentrate on recovering a low-rank matrix from few corrupted\ncoefficients w.r.t. standard basis, which, however, does not apply to more\ngeneral basis, e.g., Fourier basis. In this paper, we prove that the range\nspace of an $m\\times n$ matrix with rank $r$ can be exactly recovered from few\ncoefficients w.r.t. general basis, though $r$ and the number of corrupted\nsamples are both as high as $O(\\min\\{m,n\\}/\\log^3 (m+n))$. Our model covers\nprevious ones as special cases, and robust MC can recover the intrinsic matrix\nwith a higher rank. Moreover, we suggest a universal choice of the\nregularization parameter, which is $\\lambda=1/\\sqrt{\\log n}$. By our\n$\\ell_{2,1}$ filtering algorithm, which has theoretical guarantees, we can\nfurther reduce the computational cost of our model. As an application, we also\nfind that the solutions to extended robust Low-Rank Representation and to our\nextended robust MC are mutually expressible, so both our theory and algorithm\ncan be applied to the subspace clustering problem with missing values under\ncertain conditions. Experiments verify our theories.\n", "versions": [{"version": "v1", "created": "Thu, 25 Jun 2015 05:11:44 GMT"}, {"version": "v2", "created": "Mon, 23 May 2016 17:59:24 GMT"}], "update_date": "2016-11-18", "authors_parsed": [["Zhang", "Hongyang", ""], ["Lin", "Zhouchen", ""], ["Zhang", "Chao", ""]]}, {"id": "1506.07677", "submitter": "Suvrit Sra", "authors": "Reshad Hosseini and Suvrit Sra", "title": "Manifold Optimization for Gaussian Mixture Models", "comments": "19 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG math.OC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We take a new look at parameter estimation for Gaussian Mixture Models\n(GMMs). In particular, we propose using \\emph{Riemannian manifold optimization}\nas a powerful counterpart to Expectation Maximization (EM). An out-of-the-box\ninvocation of manifold optimization, however, fails spectacularly: it converges\nto the same solution but vastly slower. Driven by intuition from manifold\nconvexity, we then propose a reparamerization that has remarkable empirical\nconsequences. It makes manifold optimization not only match EM---a highly\nencouraging result in itself given the poor record nonlinear programming\nmethods have had against EM so far---but also outperform EM in many practical\nsettings, while displaying much less variability in running times. We further\nhighlight the strengths of manifold optimization by developing a somewhat tuned\nmanifold LBFGS method that proves even more competitive and reliable than\nexisting manifold optimization tools. We hope that our results encourage a\nwider consideration of manifold optimization for parameter estimation problems.\n", "versions": [{"version": "v1", "created": "Thu, 25 Jun 2015 09:40:51 GMT"}], "update_date": "2015-06-26", "authors_parsed": [["Hosseini", "Reshad", ""], ["Sra", "Suvrit", ""]]}, {"id": "1506.07721", "submitter": "Kazuto Fukuchi", "authors": "Kazuto Fukuchi and Jun Sakuma", "title": "Fairness-Aware Learning with Restriction of Universal Dependency using\n  f-Divergences", "comments": "15 pages, 2 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Fairness-aware learning is a novel framework for classification tasks. Like\nregular empirical risk minimization (ERM), it aims to learn a classifier with a\nlow error rate, and at the same time, for the predictions of the classifier to\nbe independent of sensitive features, such as gender, religion, race, and\nethnicity. Existing methods can achieve low dependencies on given samples, but\nthis is not guaranteed on unseen samples. The existing fairness-aware learning\nalgorithms employ different dependency measures, and each algorithm is\nspecifically designed for a particular one. Such diversity makes it difficult\nto theoretically analyze and compare them. In this paper, we propose a general\nframework for fairness-aware learning that uses f-divergences and that covers\nmost of the dependency measures employed in the existing methods. We introduce\na way to estimate the f-divergences that allows us to give a unified analysis\nfor the upper bound of the estimation error; this bound is tighter than that of\nthe existing convergence rate analysis of the divergence estimation. With our\ndivergence estimate, we propose a fairness-aware learning algorithm, and\nperform a theoretical analysis of its generalization error. Our analysis\nreveals that, under mild assumptions and even with enforcement of fairness, the\ngeneralization error of our method is $O(\\sqrt{1/n})$, which is the same as\nthat of the regular ERM. In addition, and more importantly, we show that, for\nany f-divergence, the upper bound of the estimation error of the divergence is\n$O(\\sqrt{1/n})$. This indicates that our fairness-aware learning algorithm\nguarantees low dependencies on unseen samples for any dependency measure\nrepresented by an f-divergence.\n", "versions": [{"version": "v1", "created": "Thu, 25 Jun 2015 12:24:43 GMT"}], "update_date": "2015-06-26", "authors_parsed": [["Fukuchi", "Kazuto", ""], ["Sakuma", "Jun", ""]]}, {"id": "1506.07840", "submitter": "Gal Mishne", "authors": "Gal Mishne, Uri Shaham, Alexander Cloninger and Israel Cohen", "title": "Diffusion Nets", "comments": "24 pages, 12 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG math.CA", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Non-linear manifold learning enables high-dimensional data analysis, but\nrequires out-of-sample-extension methods to process new data points. In this\npaper, we propose a manifold learning algorithm based on deep learning to\ncreate an encoder, which maps a high-dimensional dataset and its\nlow-dimensional embedding, and a decoder, which takes the embedded data back to\nthe high-dimensional space. Stacking the encoder and decoder together\nconstructs an autoencoder, which we term a diffusion net, that performs\nout-of-sample-extension as well as outlier detection. We introduce new neural\nnet constraints for the encoder, which preserves the local geometry of the\npoints, and we prove rates of convergence for the encoder. Also, our approach\nis efficient in both computational complexity and memory requirements, as\nopposed to previous methods that require storage of all training points in both\nthe high-dimensional and the low-dimensional spaces to calculate the\nout-of-sample-extension and the pre-image.\n", "versions": [{"version": "v1", "created": "Thu, 25 Jun 2015 18:13:49 GMT"}], "update_date": "2015-06-26", "authors_parsed": [["Mishne", "Gal", ""], ["Shaham", "Uri", ""], ["Cloninger", "Alexander", ""], ["Cohen", "Israel", ""]]}, {"id": "1506.07868", "submitter": "Chris White", "authors": "Chris D. White, Sujay Sanghavi, and Rachel Ward", "title": "The local convexity of solving systems of quadratic equations", "comments": "36 pages, 3 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.NA math.OC stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper considers the recovery of a rank $r$ positive semidefinite matrix\n$X X^T\\in\\mathbb{R}^{n\\times n}$ from $m$ scalar measurements of the form $y_i\n:= a_i^T X X^T a_i$ (i.e., quadratic measurements of $X$). Such problems arise\nin a variety of applications, including covariance sketching of\nhigh-dimensional data streams, quadratic regression, quantum state tomography,\namong others. A natural approach to this problem is to minimize the loss\nfunction $f(U) = \\sum_i (y_i - a_i^TUU^Ta_i)^2$ which has an entire manifold of\nsolutions given by $\\{XO\\}_{O\\in\\mathcal{O}_r}$ where $\\mathcal{O}_r$ is the\northogonal group of $r\\times r$ orthogonal matrices; this is {\\it non-convex}\nin the $n\\times r$ matrix $U$, but methods like gradient descent are simple and\neasy to implement (as compared to semidefinite relaxation approaches).\n  In this paper we show that once we have $m \\geq C nr \\log^2(n)$ samples from\nisotropic gaussian $a_i$, with high probability {\\em (a)} this function admits\na dimension-independent region of {\\em local strong convexity} on lines\nperpendicular to the solution manifold, and {\\em (b)} with an additional\npolynomial factor of $r$ samples, a simple spectral initialization will land\nwithin the region of convexity with high probability. Together, this implies\nthat gradient descent with initialization (but no re-sampling) will converge\nlinearly to the correct $X$, up to an orthogonal transformation. We believe\nthat this general technique (local convexity reachable by spectral\ninitialization) should prove applicable to a broader class of nonconvex\noptimization problems.\n", "versions": [{"version": "v1", "created": "Thu, 25 Jun 2015 19:44:51 GMT"}, {"version": "v2", "created": "Fri, 26 Jun 2015 02:58:27 GMT"}, {"version": "v3", "created": "Sat, 18 Jul 2015 19:05:28 GMT"}, {"version": "v4", "created": "Fri, 7 Aug 2015 14:44:20 GMT"}, {"version": "v5", "created": "Wed, 1 Jun 2016 12:13:06 GMT"}], "update_date": "2016-06-02", "authors_parsed": [["White", "Chris D.", ""], ["Sanghavi", "Sujay", ""], ["Ward", "Rachel", ""]]}, {"id": "1506.07902", "submitter": "Akshay Krishnamurthy", "authors": "Akshay Krishnamurthy", "title": "Minimax Structured Normal Means Inference", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.IT math.IT", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We provide a unified treatment of a broad class of noisy structure recovery\nproblems, known as structured normal means problems. In this setting, the goal\nis to identify, from a finite collection of Gaussian distributions with\ndifferent means, the distribution that produced some observed data. Recent work\nhas studied several special cases including sparse vectors, biclusters, and\ngraph-based structures. We establish nearly matching upper and lower bounds on\nthe minimax probability of error for any structured normal means problem, and\nwe derive an optimality certificate for the maximum likelihood estimator, which\ncan be applied to many instantiations. We also consider an experimental design\nsetting, where we generalize our minimax bounds and derive an algorithm for\ncomputing a design strategy with a certain optimality property. We show that\nour results give tight minimax bounds for many structure recovery problems and\nconsider some consequences for interactive sampling.\n", "versions": [{"version": "v1", "created": "Thu, 25 Jun 2015 21:35:21 GMT"}, {"version": "v2", "created": "Tue, 26 Jan 2016 02:31:50 GMT"}], "update_date": "2016-01-27", "authors_parsed": [["Krishnamurthy", "Akshay", ""]]}, {"id": "1506.07925", "submitter": "Alexander Volfovsky", "authors": "Daniel L. Sussman, Alexander Volfovsky, Edoardo M. Airoldi", "title": "Analyzing statistical and computational tradeoffs of estimation\n  procedures", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.CO stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The recent explosion in the amount and dimensionality of data has exacerbated\nthe need of trading off computational and statistical efficiency carefully, so\nthat inference is both tractable and meaningful. We propose a framework that\nprovides an explicit opportunity for practitioners to specify how much\nstatistical risk they are willing to accept for a given computational cost, and\nleads to a theoretical risk-computation frontier for any given inference\nproblem. We illustrate the tradeoff between risk and computation and illustrate\nthe frontier in three distinct settings. First, we derive analytic forms for\nthe risk of estimating parameters in the classical setting of estimating the\nmean and variance for normally distributed data and for the more general\nsetting of parameters of an exponential family. The second example concentrates\non computationally constrained Hodges-Lehmann estimators. We conclude with an\nevaluation of risk associated with early termination of iterative matrix\ninversion algorithms in the context of linear regression.\n", "versions": [{"version": "v1", "created": "Thu, 25 Jun 2015 23:57:34 GMT"}], "update_date": "2015-06-29", "authors_parsed": [["Sussman", "Daniel L.", ""], ["Volfovsky", "Alexander", ""], ["Airoldi", "Edoardo M.", ""]]}, {"id": "1506.07930", "submitter": "Saeid Amiri", "authors": "Saeid Amiri, Bertrand Clarke and Jennifer Clarke", "title": "Clustering categorical data via ensembling dissimilarity matrices", "comments": null, "journal-ref": null, "doi": "10.1080/10618600.2017.1305278", "report-no": null, "categories": "stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a technique for clustering categorical data by generating many\ndissimilarity matrices and averaging over them. We begin by demonstrating our\ntechnique on low dimensional categorical data and comparing it to several other\ntechniques that have been proposed. Then we give conditions under which our\nmethod should yield good results in general. Our method extends to high\ndimensional categorical data of equal lengths by ensembling over many choices\nof explanatory variables. In this context we compare our method with two other\nmethods. Finally, we extend our method to high dimensional categorical data\nvectors of unequal length by using alignment techniques to equalize the\nlengths. We give examples to show that our method continues to provide good\nresults, in particular, better in the context of genome sequences than\nclusterings suggested by phylogenetic trees.\n", "versions": [{"version": "v1", "created": "Fri, 26 Jun 2015 00:48:17 GMT"}], "update_date": "2017-09-20", "authors_parsed": [["Amiri", "Saeid", ""], ["Clarke", "Bertrand", ""], ["Clarke", "Jennifer", ""]]}, {"id": "1506.07944", "submitter": "Vivien Seguy", "authors": "Vivien Seguy, Marco Cuturi", "title": "Principal Geodesic Analysis for Probability Measures under the Optimal\n  Transport Metric", "comments": "9 pages, 8 figures. To appear in Advances in Neural Information\n  Processing Systems (NIPS) 2015", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Given a family of probability measures in P(X), the space of probability\nmeasures on a Hilbert space X, our goal in this paper is to highlight one ore\nmore curves in P(X) that summarize efficiently that family. We propose to study\nthis problem under the optimal transport (Wasserstein) geometry, using curves\nthat are restricted to be geodesic segments under that metric. We show that\nconcepts that play a key role in Euclidean PCA, such as data centering or\northogonality of principal directions, find a natural equivalent in the optimal\ntransport geometry, using Wasserstein means and differential geometry. The\nimplementation of these ideas is, however, computationally challenging. To\nachieve scalable algorithms that can handle thousands of measures, we propose\nto use a relaxed definition for geodesics and regularized optimal transport\ndistances. The interest of our approach is demonstrated on images seen either\nas shapes or color histograms.\n", "versions": [{"version": "v1", "created": "Fri, 26 Jun 2015 02:37:26 GMT"}, {"version": "v2", "created": "Sun, 22 Nov 2015 12:55:01 GMT"}], "update_date": "2015-11-24", "authors_parsed": [["Seguy", "Vivien", ""], ["Cuturi", "Marco", ""]]}, {"id": "1506.07947", "submitter": "Sewoong Oh", "authors": "Sewoong Oh, Kiran K. Thekumparampil, and Jiaming Xu", "title": "Collaboratively Learning Preferences from Ordinal Data", "comments": "38 pages 2 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.IT math.IT stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In applications such as recommendation systems and revenue management, it is\nimportant to predict preferences on items that have not been seen by a user or\npredict outcomes of comparisons among those that have never been compared. A\npopular discrete choice model of multinomial logit model captures the structure\nof the hidden preferences with a low-rank matrix. In order to predict the\npreferences, we want to learn the underlying model from noisy observations of\nthe low-rank matrix, collected as revealed preferences in various forms of\nordinal data. A natural approach to learn such a model is to solve a convex\nrelaxation of nuclear norm minimization. We present the convex relaxation\napproach in two contexts of interest: collaborative ranking and bundled choice\nmodeling. In both cases, we show that the convex relaxation is minimax optimal.\nWe prove an upper bound on the resulting error with finite samples, and provide\na matching information-theoretic lower bound.\n", "versions": [{"version": "v1", "created": "Fri, 26 Jun 2015 03:06:27 GMT"}], "update_date": "2015-06-29", "authors_parsed": [["Oh", "Sewoong", ""], ["Thekumparampil", "Kiran K.", ""], ["Xu", "Jiaming", ""]]}, {"id": "1506.07959", "submitter": "Shaohua Li", "authors": "Shaohua Li, Ryohei Fujimaki, Chunyan Miao", "title": "Factorized Asymptotic Bayesian Inference for Factorial Hidden Markov\n  Models", "comments": "9 pages, 3 figures, 2 appendix pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Factorial hidden Markov models (FHMMs) are powerful tools of modeling\nsequential data. Learning FHMMs yields a challenging simultaneous model\nselection issue, i.e., selecting the number of multiple Markov chains and the\ndimensionality of each chain. Our main contribution is to address this model\nselection issue by extending Factorized Asymptotic Bayesian (FAB) inference to\nFHMMs. First, we offer a better approximation of marginal log-likelihood than\nthe previous FAB inference. Our key idea is to integrate out transition\nprobabilities, yet still apply the Laplace approximation to emission\nprobabilities. Second, we prove that if there are two very similar hidden\nstates in an FHMM, i.e. one is redundant, then FAB will almost surely shrink\nand eliminate one of them, making the model parsimonious. Experimental results\nshow that FAB for FHMMs significantly outperforms state-of-the-art\nnonparametric Bayesian iFHMM and Variational FHMM in model selection accuracy,\nwith competitive held-out perplexity.\n", "versions": [{"version": "v1", "created": "Fri, 26 Jun 2015 05:24:30 GMT"}], "update_date": "2015-06-29", "authors_parsed": [["Li", "Shaohua", ""], ["Fujimaki", "Ryohei", ""], ["Miao", "Chunyan", ""]]}, {"id": "1506.07997", "submitter": "Ichiro Takeuchi Prof.", "authors": "S. Suzumura, K. Nakagawa, K. Tsuda, I. Takeuchi", "title": "An Efficient Post-Selection Inference on High-Order Interaction Models", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Finding statistically significant high-order interaction features in\npredictive modeling is important but challenging task. The difficulty lies in\nthe fact that, for a recent applications with high-dimensional covariates, the\nnumber of possible high-order interaction features would be extremely large.\nIdentifying statistically significant features from such a huge pool of\ncandidates would be highly challenging both in computational and statistical\nsenses. To work with this problem, we consider a two stage algorithm where we\nfirst select a set of high-order interaction features by marginal screening,\nand then make statistical inferences on the regression model fitted only with\nthe selected features. Such statistical inferences are called post-selection\ninference (PSI), and receiving an increasing attention in the literature. One\nof the seminal recent advancements in PSI literature is the works by Lee et al.\nwhere the authors presented an algorithmic framework for computing exact\nsampling distributions in PSI. A main challenge when applying their approach to\nour high-order interaction models is to cope with the fact that PSI in general\ndepends not only on the selected features but also on the unselected features,\nmaking it hard to apply to our extremely high-dimensional high-order\ninteraction models. The goal of this paper is to overcome this difficulty by\nintroducing a novel efficient method for PSI. Our key idea is to exploit the\nunderlying tree structure among high-order interaction features, and to develop\na pruning method of the tree which enables us to quickly identify a group of\nunselected features that are guaranteed to have no influence on PSI. The\nexperimental results indicate that the proposed method allows us to reliably\nidentify statistically significant high-order interaction features with\nreasonable computational cost.\n", "versions": [{"version": "v1", "created": "Fri, 26 Jun 2015 08:52:37 GMT"}], "update_date": "2015-06-29", "authors_parsed": [["Suzumura", "S.", ""], ["Nakagawa", "K.", ""], ["Tsuda", "K.", ""], ["Takeuchi", "I.", ""]]}, {"id": "1506.08002", "submitter": "Ichiro Takeuchi Prof.", "authors": "Kazuya Nakagawa, Shinya Suzumura, Masayuki Karasuyama, Koji Tsuda,\n  Ichiro Takeuchi", "title": "Safe Feature Pruning for Sparse High-Order Interaction Models", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Taking into account high-order interactions among covariates is valuable in\nmany practical regression problems. This is, however, computationally\nchallenging task because the number of high-order interaction features to be\nconsidered would be extremely large unless the number of covariates is\nsufficiently small. In this paper, we propose a novel efficient algorithm for\nLASSO-based sparse learning of such high-order interaction models. Our basic\nstrategy for reducing the number of features is to employ the idea of recently\nproposed safe feature screening (SFS) rule. An SFS rule has a property that, if\na feature satisfies the rule, then the feature is guaranteed to be non-active\nin the LASSO solution, meaning that it can be safely screened-out prior to the\nLASSO training process. If a large number of features can be screened-out\nbefore training the LASSO, the computational cost and the memory requirment can\nbe dramatically reduced. However, applying such an SFS rule to each of the\nextremely large number of high-order interaction features would be\ncomputationally infeasible. Our key idea for solving this computational issue\nis to exploit the underlying tree structure among high-order interaction\nfeatures. Specifically, we introduce a pruning condition called safe feature\npruning (SFP) rule which has a property that, if the rule is satisfied in a\ncertain node of the tree, then all the high-order interaction features\ncorresponding to its descendant nodes can be guaranteed to be non-active at the\noptimal solution. Our algorithm is extremely efficient, making it possible to\nwork, e.g., with 3rd order interactions of 10,000 original covariates, where\nthe number of possible high-order interaction features is greater than 10^{12}.\n", "versions": [{"version": "v1", "created": "Fri, 26 Jun 2015 09:08:26 GMT"}], "update_date": "2015-06-29", "authors_parsed": [["Nakagawa", "Kazuya", ""], ["Suzumura", "Shinya", ""], ["Karasuyama", "Masayuki", ""], ["Tsuda", "Koji", ""], ["Takeuchi", "Ichiro", ""]]}, {"id": "1506.08009", "submitter": "Francois Petitjean Ph.D.", "authors": "Francois Petitjean, Tao Li, Nikolaj Tatti, Geoffrey I. Webb", "title": "Skopus: Mining top-k sequential patterns under leverage", "comments": null, "journal-ref": "Data Mining and Knowledge Discovery, September 2016, Volume 30,\n  Issue 5, pp 1086-1111", "doi": "10.1007/s10618-016-0467-9", "report-no": null, "categories": "cs.AI cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper presents a framework for exact discovery of the top-k sequential\npatterns under Leverage. It combines (1) a novel definition of the expected\nsupport for a sequential pattern - a concept on which most interestingness\nmeasures directly rely - with (2) SkOPUS: a new branch-and-bound algorithm for\nthe exact discovery of top-k sequential patterns under a given measure of\ninterest. Our interestingness measure employs the partition approach. A pattern\nis interesting to the extent that it is more frequent than can be explained by\nassuming independence between any of the pairs of patterns from which it can be\ncomposed. The larger the support compared to the expectation under\nindependence, the more interesting is the pattern. We build on these two\nelements to exactly extract the k sequential patterns with highest leverage,\nconsistent with our definition of expected support. We conduct experiments on\nboth synthetic data with known patterns and real-world datasets; both\nexperiments confirm the consistency and relevance of our approach with regard\nto the state of the art. This article was published in Data Mining and\nKnowledge Discovery and is accessible at\nhttp://dx.doi.org/10.1007/s10618-016-0467-9.\n", "versions": [{"version": "v1", "created": "Fri, 26 Jun 2015 09:36:10 GMT"}, {"version": "v2", "created": "Wed, 6 Jan 2016 04:48:08 GMT"}, {"version": "v3", "created": "Mon, 8 May 2017 23:59:16 GMT"}, {"version": "v4", "created": "Mon, 5 Feb 2018 01:26:34 GMT"}], "update_date": "2018-02-06", "authors_parsed": [["Petitjean", "Francois", ""], ["Li", "Tao", ""], ["Tatti", "Nikolaj", ""], ["Webb", "Geoffrey I.", ""]]}, {"id": "1506.08105", "submitter": "Parthan Kasarapu Mr", "authors": "Parthan Kasarapu", "title": "Modelling of directional data using Kent distributions", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The modelling of data on a spherical surface requires the consideration of\ndirectional probability distributions. To model asymmetrically distributed data\non a three-dimensional sphere, Kent distributions are often used. The moment\nestimates of the parameters are typically used in modelling tasks involving\nKent distributions. However, these lack a rigorous statistical treatment. The\nfocus of the paper is to introduce a Bayesian estimation of the parameters of\nthe Kent distribution which has not been carried out in the literature, partly\nbecause of its complex mathematical form. We employ the Bayesian\ninformation-theoretic paradigm of Minimum Message Length (MML) to bridge this\ngap and derive reliable estimators. The inferred parameters are subsequently\nused in mixture modelling of Kent distributions. The problem of inferring the\nsuitable number of mixture components is also addressed using the MML\ncriterion. We demonstrate the superior performance of the derived MML-based\nparameter estimates against the traditional estimators. We apply the MML\nprinciple to infer mixtures of Kent distributions to model empirical data\ncorresponding to protein conformations. We demonstrate the effectiveness of\nKent models to act as improved descriptors of protein structural data as\ncompared to commonly used von Mises-Fisher distributions.\n", "versions": [{"version": "v1", "created": "Fri, 26 Jun 2015 15:03:33 GMT"}], "update_date": "2015-06-29", "authors_parsed": [["Kasarapu", "Parthan", ""]]}, {"id": "1506.08126", "submitter": "Dragomir", "authors": "Dragomir Radev and Amanda Stent and Joel Tetreault and Aasish Pappu\n  and Aikaterini Iliakopoulou and Agustin Chanfreau and Paloma de Juan and\n  Jordi Vallmitjana and Alejandro Jaimes and Rahul Jha and Bob Mankoff", "title": "Humor in Collective Discourse: Unsupervised Funniness Detection in the\n  New Yorker Cartoon Caption Contest", "comments": "10 pages, in submission", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI cs.MM stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The New Yorker publishes a weekly captionless cartoon. More than 5,000\nreaders submit captions for it. The editors select three of them and ask the\nreaders to pick the funniest one. We describe an experiment that compares a\ndozen automatic methods for selecting the funniest caption. We show that\nnegative sentiment, human-centeredness, and lexical centrality most strongly\nmatch the funniest captions, followed by positive sentiment. These results are\nuseful for understanding humor and also in the design of more engaging\nconversational agents in text and multimodal (vision+text) systems. As part of\nthis work, a large set of cartoons and captions is being made available to the\ncommunity.\n", "versions": [{"version": "v1", "created": "Fri, 26 Jun 2015 15:48:10 GMT"}], "update_date": "2015-06-29", "authors_parsed": [["Radev", "Dragomir", ""], ["Stent", "Amanda", ""], ["Tetreault", "Joel", ""], ["Pappu", "Aasish", ""], ["Iliakopoulou", "Aikaterini", ""], ["Chanfreau", "Agustin", ""], ["de Juan", "Paloma", ""], ["Vallmitjana", "Jordi", ""], ["Jaimes", "Alejandro", ""], ["Jha", "Rahul", ""], ["Mankoff", "Bob", ""]]}, {"id": "1506.08170", "submitter": "Zhuang Ma", "authors": "Zhuang Ma, Yichao Lu, Dean Foster", "title": "Finding Linear Structure in Large Datasets with Scalable Canonical\n  Correlation Analysis", "comments": "Appearing in International Conference on Machine Learning (ICML) 2015", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Canonical Correlation Analysis (CCA) is a widely used spectral technique for\nfinding correlation structures in multi-view datasets. In this paper, we tackle\nthe problem of large scale CCA, where classical algorithms, usually requiring\ncomputing the product of two huge matrices and huge matrix decomposition, are\ncomputationally and storage expensive. We recast CCA from a novel perspective\nand propose a scalable and memory efficient Augmented Approximate Gradient\n(AppGrad) scheme for finding top $k$ dimensional canonical subspace which only\ninvolves large matrix multiplying a thin matrix of width $k$ and small matrix\ndecomposition of dimension $k\\times k$. Further, AppGrad achieves optimal\nstorage complexity $O(k(p_1+p_2))$, compared with classical algorithms which\nusually require $O(p_1^2+p_2^2)$ space to store two dense whitening matrices.\nThe proposed scheme naturally generalizes to stochastic optimization regime,\nespecially efficient for huge datasets where batch algorithms are prohibitive.\nThe online property of stochastic AppGrad is also well suited to the streaming\nscenario, where data comes sequentially. To the best of our knowledge, it is\nthe first stochastic algorithm for CCA. Experiments on four real data sets are\nprovided to show the effectiveness of the proposed methods.\n", "versions": [{"version": "v1", "created": "Fri, 26 Jun 2015 17:51:57 GMT"}], "update_date": "2015-06-29", "authors_parsed": [["Ma", "Zhuang", ""], ["Lu", "Yichao", ""], ["Foster", "Dean", ""]]}, {"id": "1506.08180", "submitter": "Amar Shah", "authors": "Amar Shah and David A. Knowles and Zoubin Ghahramani", "title": "An Empirical Study of Stochastic Variational Algorithms for the Beta\n  Bernoulli Process", "comments": "ICML, 12 pages. Volume 37: Proceedings of The 32nd International\n  Conference on Machine Learning, 2015", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG stat.AP stat.CO stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Stochastic variational inference (SVI) is emerging as the most promising\ncandidate for scaling inference in Bayesian probabilistic models to large\ndatasets. However, the performance of these methods has been assessed primarily\nin the context of Bayesian topic models, particularly latent Dirichlet\nallocation (LDA). Deriving several new algorithms, and using synthetic, image\nand genomic datasets, we investigate whether the understanding gleaned from LDA\napplies in the setting of sparse latent factor models, specifically beta\nprocess factor analysis (BPFA). We demonstrate that the big picture is\nconsistent: using Gibbs sampling within SVI to maintain certain posterior\ndependencies is extremely effective. However, we find that different posterior\ndependencies are important in BPFA relative to LDA. Particularly,\napproximations able to model intra-local variable dependence perform best.\n", "versions": [{"version": "v1", "created": "Fri, 26 Jun 2015 18:55:11 GMT"}], "update_date": "2015-06-29", "authors_parsed": [["Shah", "Amar", ""], ["Knowles", "David A.", ""], ["Ghahramani", "Zoubin", ""]]}, {"id": "1506.08272", "submitter": "Xiangru Lian", "authors": "Xiangru Lian, Yijun Huang, Yuncheng Li, Ji Liu", "title": "Asynchronous Parallel Stochastic Gradient for Nonconvex Optimization", "comments": "33 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.OC cs.NA stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Asynchronous parallel implementations of stochastic gradient (SG) have been\nbroadly used in solving deep neural network and received many successes in\npractice recently. However, existing theories cannot explain their convergence\nand speedup properties, mainly due to the nonconvexity of most deep learning\nformulations and the asynchronous parallel mechanism. To fill the gaps in\ntheory and provide theoretical supports, this paper studies two asynchronous\nparallel implementations of SG: one is on the computer network and the other is\non the shared memory system. We establish an ergodic convergence rate\n$O(1/\\sqrt{K})$ for both algorithms and prove that the linear speedup is\nachievable if the number of workers is bounded by $\\sqrt{K}$ ($K$ is the total\nnumber of iterations). Our results generalize and improve existing analysis for\nconvex minimization.\n", "versions": [{"version": "v1", "created": "Sat, 27 Jun 2015 08:41:50 GMT"}, {"version": "v2", "created": "Thu, 29 Oct 2015 15:39:14 GMT"}, {"version": "v3", "created": "Sat, 20 May 2017 07:35:06 GMT"}, {"version": "v4", "created": "Sat, 10 Jun 2017 05:04:18 GMT"}, {"version": "v5", "created": "Thu, 18 Apr 2019 18:25:04 GMT"}], "update_date": "2019-04-22", "authors_parsed": [["Lian", "Xiangru", ""], ["Huang", "Yijun", ""], ["Li", "Yuncheng", ""], ["Liu", "Ji", ""]]}, {"id": "1506.08301", "submitter": "Yilun Wang", "authors": "Yilun Wang, Zhiqiang Li, Yifeng Wang, Xiaona Wang, Junjie Zheng,\n  Xujuan Duan, Huafu Chen", "title": "A Novel Approach for Stable Selection of Informative Redundant Features\n  from High Dimensional fMRI Data", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Feature selection is among the most important components because it not only\nhelps enhance the classification accuracy, but also or even more important\nprovides potential biomarker discovery. However, traditional multivariate\nmethods is likely to obtain unstable and unreliable results in case of an\nextremely high dimensional feature space and very limited training samples,\nwhere the features are often correlated or redundant. In order to improve the\nstability, generalization and interpretations of the discovered potential\nbiomarker and enhance the robustness of the resultant classifier, the redundant\nbut informative features need to be also selected. Therefore we introduced a\nnovel feature selection method which combines a recent implementation of the\nstability selection approach and the elastic net approach. The advantage in\nterms of better control of false discoveries and missed discoveries of our\napproach, and the resulted better interpretability of the obtained potential\nbiomarker is verified in both synthetic and real fMRI experiments. In addition,\nwe are among the first to demonstrate the robustness of feature selection\nbenefiting from the incorporation of stability selection and also among the\nfirst to demonstrate the possible unrobustness of the classical univariate\ntwo-sample t-test method. Specifically, we show the robustness of our feature\nselection results in existence of noisy (wrong) training labels, as well as the\nrobustness of the resulted classifier based on our feature selection results in\nthe existence of data variation, demonstrated by a multi-center\nattention-deficit/hyperactivity disorder (ADHD) fMRI data.\n", "versions": [{"version": "v1", "created": "Sat, 27 Jun 2015 15:28:31 GMT"}, {"version": "v2", "created": "Wed, 25 May 2016 02:37:39 GMT"}], "update_date": "2016-05-26", "authors_parsed": [["Wang", "Yilun", ""], ["Li", "Zhiqiang", ""], ["Wang", "Yifeng", ""], ["Wang", "Xiaona", ""], ["Zheng", "Junjie", ""], ["Duan", "Xujuan", ""], ["Chen", "Huafu", ""]]}, {"id": "1506.08387", "submitter": "Tomohiko Mizutani", "authors": "Tomohiko Mizutani", "title": "Robustness Analysis of Preconditioned Successive Projection Algorithm\n  for General Form of Separable NMF Problem", "comments": "19 pages", "journal-ref": "Linear Algebra and its Applications, 497, pp. 1-22, 2016", "doi": "10.1016/j.laa.2016.02.016", "report-no": null, "categories": "stat.ML math.OC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The successive projection algorithm (SPA) has been known to work well for\nseparable nonnegative matrix factorization (NMF) problems arising in\napplications, such as topic extraction from documents and endmember detection\nin hyperspectral images. One of the reasons is in that the algorithm is robust\nto noise. Gillis and Vavasis showed in [SIAM J. Optim., 25(1), pp. 677-698,\n2015] that a preconditioner can further enhance its noise robustness. The proof\nrested on the condition that the dimension $d$ and factorization rank $r$ in\nthe separable NMF problem coincide with each other. However, it may be\nunrealistic to expect that the condition holds in separable NMF problems\nappearing in actual applications; in such problems, $d$ is usually greater than\n$r$. This paper shows, without the condition $d=r$, that the preconditioned SPA\nis robust to noise.\n", "versions": [{"version": "v1", "created": "Sun, 28 Jun 2015 12:10:27 GMT"}, {"version": "v2", "created": "Thu, 28 Jan 2016 08:35:07 GMT"}], "update_date": "2016-03-03", "authors_parsed": [["Mizutani", "Tomohiko", ""]]}, {"id": "1506.08448", "submitter": "Dennis Forster", "authors": "Dennis Forster, Abdul-Saboor Sheikh and J\\\"org L\\\"ucke", "title": "Neural Simpletrons - Minimalistic Directed Generative Networks for\n  Learning with Few Labels", "comments": null, "journal-ref": "Neural Computation Volume 30, Issue 8, August 2018, p.2113-2174", "doi": "10.1162/neco_a_01100", "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Classifiers for the semi-supervised setting often combine strong supervised\nmodels with additional learning objectives to make use of unlabeled data. This\nresults in powerful though very complex models that are hard to train and that\ndemand additional labels for optimal parameter tuning, which are often not\ngiven when labeled data is very sparse. We here study a minimalistic\nmulti-layer generative neural network for semi-supervised learning in a form\nand setting as similar to standard discriminative networks as possible. Based\non normalized Poisson mixtures, we derive compact and local learning and neural\nactivation rules. Learning and inference in the network can be scaled using\nstandard deep learning tools for parallelized GPU implementation. With the\nsingle objective of likelihood optimization, both labeled and unlabeled data\nare naturally incorporated into learning. Empirical evaluations on standard\nbenchmarks show, that for datasets with few labels the derived minimalistic\nnetwork improves on all classical deep learning approaches and is competitive\nwith their recent variants without the need of additional labels for parameter\ntuning. Furthermore, we find that the studied network is the best performing\nmonolithic (`non-hybrid') system for few labels, and that it can be applied in\nthe limit of very few labels, where no other system has been reported to\noperate so far.\n", "versions": [{"version": "v1", "created": "Sun, 28 Jun 2015 20:25:15 GMT"}, {"version": "v2", "created": "Fri, 23 Oct 2015 12:35:00 GMT"}, {"version": "v3", "created": "Tue, 3 May 2016 13:52:20 GMT"}, {"version": "v4", "created": "Fri, 18 Nov 2016 15:08:51 GMT"}], "update_date": "2019-05-01", "authors_parsed": [["Forster", "Dennis", ""], ["Sheikh", "Abdul-Saboor", ""], ["L\u00fccke", "J\u00f6rg", ""]]}, {"id": "1506.08473", "submitter": "Majid Janzamin", "authors": "Majid Janzamin and Hanie Sedghi and Anima Anandkumar", "title": "Beating the Perils of Non-Convexity: Guaranteed Training of Neural\n  Networks using Tensor Methods", "comments": "The tensor decomposition analysis is expanded, and the analysis of\n  ridge regression is added for recovering the parameters of last layer of\n  neural network", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.NE stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Training neural networks is a challenging non-convex optimization problem,\nand backpropagation or gradient descent can get stuck in spurious local optima.\nWe propose a novel algorithm based on tensor decomposition for guaranteed\ntraining of two-layer neural networks. We provide risk bounds for our proposed\nmethod, with a polynomial sample complexity in the relevant parameters, such as\ninput dimension and number of neurons. While learning arbitrary target\nfunctions is NP-hard, we provide transparent conditions on the function and the\ninput for learnability. Our training method is based on tensor decomposition,\nwhich provably converges to the global optimum, under a set of mild\nnon-degeneracy conditions. It consists of simple embarrassingly parallel linear\nand multi-linear operations, and is competitive with standard stochastic\ngradient descent (SGD), in terms of computational complexity. Thus, we propose\na computationally efficient method with guaranteed risk bounds for training\nneural networks with one hidden layer.\n", "versions": [{"version": "v1", "created": "Sun, 28 Jun 2015 23:19:49 GMT"}, {"version": "v2", "created": "Mon, 24 Aug 2015 19:31:24 GMT"}, {"version": "v3", "created": "Tue, 12 Jan 2016 03:19:42 GMT"}], "update_date": "2016-01-13", "authors_parsed": [["Janzamin", "Majid", ""], ["Sedghi", "Hanie", ""], ["Anandkumar", "Anima", ""]]}, {"id": "1506.08499", "submitter": "Yipeng Liu Dr.", "authors": "Yipeng Liu, Maarten De Vos, Sabine Van Huffel", "title": "Compressed Sensing of Multi-Channel EEG Signals: The Simultaneous\n  Cosparsity and Low Rank Optimization", "comments": "11 pages, 3 figures; accepted by IEEE Transactions on Biomedical\n  Engineering", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IT math.IT stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Goal: This paper deals with the problems that some EEG signals have no good\nsparse representation and single channel processing is not computationally\nefficient in compressed sensing of multi-channel EEG signals. Methods: An\noptimization model with L0 norm and Schatten-0 norm is proposed to enforce\ncosparsity and low rank structures in the reconstructed multi-channel EEG\nsignals. Both convex relaxation and global consensus optimization with\nalternating direction method of multipliers are used to compute the\noptimization model. Results: The performance of multi-channel EEG signal\nreconstruction is improved in term of both accuracy and computational\ncomplexity. Conclusion: The proposed method is a better candidate than previous\nsparse signal recovery methods for compressed sensing of EEG signals.\nSignificance: The proposed method enables successful compressed sensing of EEG\nsignals even when the signals have no good sparse representation. Using\ncompressed sensing would much reduce the power consumption of wireless EEG\nsystem.\n", "versions": [{"version": "v1", "created": "Mon, 29 Jun 2015 03:51:14 GMT"}], "update_date": "2015-06-30", "authors_parsed": [["Liu", "Yipeng", ""], ["De Vos", "Maarten", ""], ["Van Huffel", "Sabine", ""]]}, {"id": "1506.08511", "submitter": "Min Xu", "authors": "Min Xu", "title": "Integrative analysis of gene expression and phenotype data", "comments": "Author's PhD thesis (University of Southern California, August 2009).\n  Adviser: Xianghong Zhou", "journal-ref": null, "doi": null, "report-no": null, "categories": "q-bio.QM q-bio.GN q-bio.MN stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The linking genotype to phenotype is the fundamental aim of modern genetics.\nWe focus on study of links between gene expression data and phenotype data\nthrough integrative analysis. We propose three approaches.\n  1) The inherent complexity of phenotypes makes high-throughput phenotype\nprofiling a very difficult and laborious process. We propose a method of\nautomated multi-dimensional profiling which uses gene expression similarity.\nLarge-scale analysis show that our method can provide robust profiling that\nreveals different phenotypic aspects of samples. This profiling technique is\nalso capable of interpolation and extrapolation beyond the phenotype\ninformation given in training data. It can be used in many applications,\nincluding facilitating experimental design and detecting confounding factors.\n  2) Phenotype association analysis problems are complicated by small sample\nsize and high dimensionality. Consequently, phenotype-associated gene subsets\nobtained from training data are very sensitive to selection of training\nsamples, and the constructed sample phenotype classifiers tend to have poor\ngeneralization properties. To eliminate these obstacles, we propose a novel\napproach that generates sequences of increasingly discriminative gene cluster\ncombinations. Our experiments on both simulated and real datasets show robust\nand accurate classification performance.\n  3) Many complex phenotypes, such as cancer, are the product of not only gene\nexpression, but also gene interaction. We propose an integrative approach to\nfind gene network modules that activate under different phenotype conditions.\nUsing our method, we discovered cancer subtype-specific network modules, as\nwell as the ways in which these modules coordinate. In particular, we detected\na breast-cancer specific tumor suppressor network module with a hub gene,\nPDGFRL, which may play an important role in this module.\n", "versions": [{"version": "v1", "created": "Mon, 29 Jun 2015 05:26:29 GMT"}], "update_date": "2015-06-30", "authors_parsed": [["Xu", "Min", ""]]}, {"id": "1506.08536", "submitter": "Luca Citi", "authors": "Luca Citi", "title": "A simple yet efficient algorithm for multiple kernel learning under\n  elastic-net constraints", "comments": "18 pages, one figure, updated version of the paper draft", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This papers introduces an algorithm for the solution of multiple kernel\nlearning (MKL) problems with elastic-net constraints on the kernel weights. The\nalgorithm compares very favourably in terms of time and space complexity to\nexisting approaches and can be implemented with simple code that does not rely\non external libraries (except a conventional SVM solver).\n", "versions": [{"version": "v1", "created": "Mon, 29 Jun 2015 08:11:52 GMT"}, {"version": "v2", "created": "Wed, 13 Jan 2016 23:01:42 GMT"}, {"version": "v3", "created": "Fri, 5 Apr 2019 09:37:11 GMT"}], "update_date": "2019-04-08", "authors_parsed": [["Citi", "Luca", ""]]}, {"id": "1506.08544", "submitter": "Stephane Robin", "authors": "Nathalie Peyrard and Marie-Jos\\'ee Cros and Simon de Givry and Alain\n  Franc and St\\'ephane Robin and R\\'egis Sabbadin and Thomas Schiex and\n  Matthieu Vignes", "title": "Exact and approximate inference in graphical models: variable\n  elimination and beyond", "comments": "47 pages, 3 tables, 12 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.AI cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Probabilistic graphical models offer a powerful framework to account for the\ndependence structure between variables, which is represented as a graph.\nHowever, the dependence between variables may render inference tasks\nintractable. In this paper we review techniques exploiting the graph structure\nfor exact inference, borrowed from optimisation and computer science. They are\nbuilt on the principle of variable elimination whose complexity is dictated in\nan intricate way by the order in which variables are eliminated. The so-called\ntreewidth of the graph characterises this algorithmic complexity: low-treewidth\ngraphs can be processed efficiently. The first message that we illustrate is\ntherefore the idea that for inference in graphical model, the number of\nvariables is not the limiting factor, and it is worth checking for the\ntreewidth before turning to approximate methods. We show how algorithms\nproviding an upper bound of the treewidth can be exploited to derive a 'good'\nelimination order enabling to perform exact inference. The second message is\nthat when the treewidth is too large, algorithms for approximate inference\nlinked to the principle of variable elimination, such as loopy belief\npropagation and variational approaches, can lead to accurate results while\nbeing much less time consuming than Monte-Carlo approaches. We illustrate the\ntechniques reviewed in this article on benchmarks of inference problems in\ngenetic linkage analysis and computer vision, as well as on hidden variables\nrestoration in coupled Hidden Markov Models.\n", "versions": [{"version": "v1", "created": "Mon, 29 Jun 2015 08:45:11 GMT"}, {"version": "v2", "created": "Mon, 12 Mar 2018 07:45:08 GMT"}], "update_date": "2018-03-13", "authors_parsed": [["Peyrard", "Nathalie", ""], ["Cros", "Marie-Jos\u00e9e", ""], ["de Givry", "Simon", ""], ["Franc", "Alain", ""], ["Robin", "St\u00e9phane", ""], ["Sabbadin", "R\u00e9gis", ""], ["Schiex", "Thomas", ""], ["Vignes", "Matthieu", ""]]}, {"id": "1506.08621", "submitter": "Lennart Gulikers", "authors": "Lennart Gulikers, Marc Lelarge, Laurent Massouli\\'e", "title": "A spectral method for community detection in moderately-sparse\n  degree-corrected stochastic block models", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.PR cs.LG cs.SI stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider community detection in Degree-Corrected Stochastic Block Models\n(DC-SBM). We propose a spectral clustering algorithm based on a suitably\nnormalized adjacency matrix. We show that this algorithm consistently recovers\nthe block-membership of all but a vanishing fraction of nodes, in the regime\nwhere the lowest degree is of order log$(n)$ or higher. Recovery succeeds even\nfor very heterogeneous degree-distributions. The used algorithm does not rely\non parameters as input. In particular, it does not need to know the number of\ncommunities.\n", "versions": [{"version": "v1", "created": "Mon, 29 Jun 2015 13:44:54 GMT"}, {"version": "v2", "created": "Wed, 13 Apr 2016 14:50:53 GMT"}, {"version": "v3", "created": "Tue, 7 Feb 2017 12:31:41 GMT"}], "update_date": "2017-02-09", "authors_parsed": [["Gulikers", "Lennart", ""], ["Lelarge", "Marc", ""], ["Massouli\u00e9", "Laurent", ""]]}, {"id": "1506.08669", "submitter": "Tzu-Kuo Huang", "authors": "Tzu-Kuo Huang, Alekh Agarwal, Daniel J. Hsu, John Langford, Robert E.\n  Schapire", "title": "Efficient and Parsimonious Agnostic Active Learning", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We develop a new active learning algorithm for the streaming setting\nsatisfying three important properties: 1) It provably works for any classifier\nrepresentation and classification problem including those with severe noise. 2)\nIt is efficiently implementable with an ERM oracle. 3) It is more aggressive\nthan all previous approaches satisfying 1 and 2. To do this we create an\nalgorithm based on a newly defined optimization problem and analyze it. We also\nconduct the first experimental analysis of all efficient agnostic active\nlearning algorithms, evaluating their strengths and weaknesses in different\nsettings.\n", "versions": [{"version": "v1", "created": "Mon, 29 Jun 2015 15:02:55 GMT"}, {"version": "v2", "created": "Wed, 6 Jan 2016 02:49:21 GMT"}, {"version": "v3", "created": "Thu, 7 Jan 2016 18:27:33 GMT"}], "update_date": "2016-01-08", "authors_parsed": [["Huang", "Tzu-Kuo", ""], ["Agarwal", "Alekh", ""], ["Hsu", "Daniel J.", ""], ["Langford", "John", ""], ["Schapire", "Robert E.", ""]]}, {"id": "1506.08690", "submitter": "Gergo Barta", "authors": "Gabor Nagy and Gergo Barta and Tamas Henk", "title": "Portfolio optimization using local linear regression ensembles in\n  RapidMiner", "comments": "RCOMM 2012: Rapidminer Community Meeting and Conference", "journal-ref": null, "doi": null, "report-no": null, "categories": "q-fin.PM cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper we implement a Local Linear Regression Ensemble Committee\n(LOLREC) to predict 1-day-ahead returns of 453 assets form the S&P500. The\nestimates and the historical returns of the committees are used to compute the\nweights of the portfolio from the 453 stock. The proposed method outperforms\nbenchmark portfolio selection strategies that optimize the growth rate of the\ncapital. We investigate the effect of algorithm parameter m: the number of\nselected stocks on achieved average annual yields. Results suggest the\nalgorithm's practical usefulness in everyday trading.\n", "versions": [{"version": "v1", "created": "Mon, 29 Jun 2015 15:42:39 GMT"}], "update_date": "2015-06-30", "authors_parsed": [["Nagy", "Gabor", ""], ["Barta", "Gergo", ""], ["Henk", "Tamas", ""]]}, {"id": "1506.08700", "submitter": "Xavier Bouthillier", "authors": "Xavier Bouthillier, Kishore Konda, Pascal Vincent, Roland Memisevic", "title": "Dropout as data augmentation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Dropout is typically interpreted as bagging a large number of models sharing\nparameters. We show that using dropout in a network can also be interpreted as\na kind of data augmentation in the input space without domain knowledge. We\npresent an approach to projecting the dropout noise within a network back into\nthe input space, thereby generating augmented versions of the training data,\nand we show that training a deterministic network on the augmented samples\nyields similar results. Finally, we propose a new dropout noise scheme based on\nour observations and show that it improves dropout results without adding\nsignificant computational cost.\n", "versions": [{"version": "v1", "created": "Mon, 29 Jun 2015 15:55:45 GMT"}, {"version": "v2", "created": "Thu, 19 Nov 2015 21:48:20 GMT"}, {"version": "v3", "created": "Thu, 26 Nov 2015 18:29:08 GMT"}, {"version": "v4", "created": "Fri, 8 Jan 2016 02:56:04 GMT"}], "update_date": "2016-01-11", "authors_parsed": [["Bouthillier", "Xavier", ""], ["Konda", "Kishore", ""], ["Vincent", "Pascal", ""], ["Memisevic", "Roland", ""]]}, {"id": "1506.08760", "submitter": "Gautam Dasarathy", "authors": "Gautam Dasarathy, Robert Nowak, Xiaojin Zhu", "title": "S2: An Efficient Graph Based Active Learning Algorithm with Application\n  to Nonparametric Classification", "comments": "A version of this paper appears in the Conference on Learning Theory\n  (COLT) 2015", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper investigates the problem of active learning for binary label\nprediction on a graph. We introduce a simple and label-efficient algorithm\ncalled S2 for this task. At each step, S2 selects the vertex to be labeled\nbased on the structure of the graph and all previously gathered labels.\nSpecifically, S2 queries for the label of the vertex that bisects the *shortest\nshortest* path between any pair of oppositely labeled vertices. We present a\ntheoretical estimate of the number of queries S2 needs in terms of a novel\nparametrization of the complexity of binary functions on graphs. We also\npresent experimental results demonstrating the performance of S2 on both real\nand synthetic data. While other graph-based active learning algorithms have\nshown promise in practice, our algorithm is the first with both good\nperformance and theoretical guarantees. Finally, we demonstrate the\nimplications of the S2 algorithm to the theory of nonparametric active\nlearning. In particular, we show that S2 achieves near minimax optimal excess\nrisk for an important class of nonparametric classification problems.\n", "versions": [{"version": "v1", "created": "Mon, 29 Jun 2015 18:03:25 GMT"}], "update_date": "2015-06-30", "authors_parsed": [["Dasarathy", "Gautam", ""], ["Nowak", "Robert", ""], ["Zhu", "Xiaojin", ""]]}, {"id": "1506.08776", "submitter": "Avinava Dubey", "authors": "Junier Oliva, Avinava Dubey, Andrew G. Wilson, Barnabas Poczos, Jeff\n  Schneider, Eric P. Xing", "title": "Bayesian Nonparametric Kernel-Learning", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Kernel methods are ubiquitous tools in machine learning. However, there is\noften little reason for the common practice of selecting a kernel a priori.\nEven if a universal approximating kernel is selected, the quality of the finite\nsample estimator may be greatly affected by the choice of kernel. Furthermore,\nwhen directly applying kernel methods, one typically needs to compute a $N\n\\times N$ Gram matrix of pairwise kernel evaluations to work with a dataset of\n$N$ instances. The computation of this Gram matrix precludes the direct\napplication of kernel methods on large datasets, and makes kernel learning\nespecially difficult. In this paper we introduce Bayesian nonparmetric\nkernel-learning (BaNK), a generic, data-driven framework for scalable learning\nof kernels. BaNK places a nonparametric prior on the spectral distribution of\nrandom frequencies allowing it to both learn kernels and scale to large\ndatasets. We show that this framework can be used for large scale regression\nand classification tasks. Furthermore, we show that BaNK outperforms several\nother scalable approaches for kernel learning on a variety of real world\ndatasets.\n", "versions": [{"version": "v1", "created": "Mon, 29 Jun 2015 18:48:47 GMT"}, {"version": "v2", "created": "Tue, 30 Jan 2018 01:34:39 GMT"}], "update_date": "2018-01-31", "authors_parsed": [["Oliva", "Junier", ""], ["Dubey", "Avinava", ""], ["Wilson", "Andrew G.", ""], ["Poczos", "Barnabas", ""], ["Schneider", "Jeff", ""], ["Xing", "Eric P.", ""]]}, {"id": "1506.08826", "submitter": "Yen-Chi Chen", "authors": "Yen-Chi Chen, Christopher R. Genovese, Larry Wasserman", "title": "Statistical Inference using the Morse-Smale Complex", "comments": "45 pages, 13 figures. Accepted to Electronic Journal of Statistics", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.ME stat.ML stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The Morse-Smale complex of a function $f$ decomposes the sample space into\ncells where $f$ is increasing or decreasing. When applied to nonparametric\ndensity estimation and regression, it provides a way to represent, visualize,\nand compare multivariate functions. In this paper, we present some statistical\nresults on estimating Morse-Smale complexes. This allows us to derive new\nresults for two existing methods: mode clustering and Morse-Smale regression.\nWe also develop two new methods based on the Morse-Smale complex: a\nvisualization technique for multivariate functions and a two-sample,\nmultivariate hypothesis test.\n", "versions": [{"version": "v1", "created": "Mon, 29 Jun 2015 20:00:40 GMT"}, {"version": "v2", "created": "Tue, 4 Apr 2017 02:18:58 GMT"}], "update_date": "2017-04-05", "authors_parsed": [["Chen", "Yen-Chi", ""], ["Genovese", "Christopher R.", ""], ["Wasserman", "Larry", ""]]}, {"id": "1506.08858", "submitter": "Louis-Francois Arsenault", "authors": "Louis-Fran\\c{c}ois Arsenault, O. Anatole von Lilienfeld, and Andrew J.\n  Millis", "title": "Machine learning for many-body physics: efficient solution of dynamical\n  mean-field theory", "comments": "main text 5 pages, supplemental 5 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cond-mat.str-el stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Machine learning methods for solving the equations of dynamical mean-field\ntheory are developed. The method is demonstrated on the three dimensional\nHubbard model. The key technical issues are defining a mapping of an input\nfunction to an output function, and distinguishing metallic from insulating\nsolutions. Both metallic and Mott insulator solutions can be predicted. The\nvalidity of the machine learning scheme is assessed by comparing predictions of\nfull correlation functions, of quasi-particle weight and particle density to\nvalues directly computed. The results indicate that with modest further\ndevelopment, machine learning approach may be an attractive computational\nefficient option for real materials predictions for strongly correlated\nsystems.\n", "versions": [{"version": "v1", "created": "Mon, 29 Jun 2015 20:36:53 GMT"}], "update_date": "2015-07-01", "authors_parsed": [["Arsenault", "Louis-Fran\u00e7ois", ""], ["von Lilienfeld", "O. Anatole", ""], ["Millis", "Andrew J.", ""]]}, {"id": "1506.08910", "submitter": "Ravi Ganti", "authors": "Ravi Ganti, Nikhil Rao, Rebecca M. Willett and Robert Nowak", "title": "Learning Single Index Models in High Dimensions", "comments": "16 pages, 2 figures, 1 table", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Single Index Models (SIMs) are simple yet flexible semi-parametric models for\nclassification and regression. Response variables are modeled as a nonlinear,\nmonotonic function of a linear combination of features. Estimation in this\ncontext requires learning both the feature weights, and the nonlinear function.\nWhile methods have been described to learn SIMs in the low dimensional regime,\na method that can efficiently learn SIMs in high dimensions has not been\nforthcoming. We propose three variants of a computationally and statistically\nefficient algorithm for SIM inference in high dimensions. We establish excess\nrisk bounds for the proposed algorithms and experimentally validate the\nadvantages that our SIM learning methods provide relative to Generalized Linear\nModel (GLM) and low dimensional SIM based learning methods.\n", "versions": [{"version": "v1", "created": "Tue, 30 Jun 2015 00:45:25 GMT"}], "update_date": "2015-07-01", "authors_parsed": [["Ganti", "Ravi", ""], ["Rao", "Nikhil", ""], ["Willett", "Rebecca M.", ""], ["Nowak", "Robert", ""]]}, {"id": "1506.09016", "submitter": "Th\\'eo Trouillon", "authors": "Guillaume Bouchard, Th\\'eo Trouillon, Julien Perez, Adrien Gaidon", "title": "Online Learning to Sample", "comments": "Update: removed convergence theorem and proof as there is an error.\n  Submitted to UAI 2016", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV cs.NA math.OC stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Stochastic Gradient Descent (SGD) is one of the most widely used techniques\nfor online optimization in machine learning. In this work, we accelerate SGD by\nadaptively learning how to sample the most useful training examples at each\ntime step. First, we show that SGD can be used to learn the best possible\nsampling distribution of an importance sampling estimator. Second, we show that\nthe sampling distribution of an SGD algorithm can be estimated online by\nincrementally minimizing the variance of the gradient. The resulting algorithm\n- called Adaptive Weighted SGD (AW-SGD) - maintains a set of parameters to\noptimize, as well as a set of parameters to sample learning examples. We show\nthat AWSGD yields faster convergence in three different applications: (i) image\nclassification with deep features, where the sampling of images depends on\ntheir labels, (ii) matrix factorization, where rows and columns are not sampled\nuniformly, and (iii) reinforcement learning, where the optimized and\nexploration policies are estimated at the same time, where our approach\ncorresponds to an off-policy gradient algorithm.\n", "versions": [{"version": "v1", "created": "Tue, 30 Jun 2015 10:08:35 GMT"}, {"version": "v2", "created": "Tue, 15 Mar 2016 16:08:56 GMT"}], "update_date": "2016-03-16", "authors_parsed": [["Bouchard", "Guillaume", ""], ["Trouillon", "Th\u00e9o", ""], ["Perez", "Julien", ""], ["Gaidon", "Adrien", ""]]}, {"id": "1506.09039", "submitter": "Yutian Chen", "authors": "Yutian Chen, Zoubin Ghahramani", "title": "Scalable Discrete Sampling as a Multi-Armed Bandit Problem", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Drawing a sample from a discrete distribution is one of the building\ncomponents for Monte Carlo methods. Like other sampling algorithms, discrete\nsampling suffers from the high computational burden in large-scale inference\nproblems. We study the problem of sampling a discrete random variable with a\nhigh degree of dependency that is typical in large-scale Bayesian inference and\ngraphical models, and propose an efficient approximate solution with a\nsubsampling approach. We make a novel connection between the discrete sampling\nand Multi-Armed Bandits problems with a finite reward population and provide\nthree algorithms with theoretical guarantees. Empirical evaluations show the\nrobustness and efficiency of the approximate algorithms in both synthetic and\nreal-world large-scale problems.\n", "versions": [{"version": "v1", "created": "Tue, 30 Jun 2015 11:20:45 GMT"}, {"version": "v2", "created": "Tue, 9 Feb 2016 14:21:05 GMT"}, {"version": "v3", "created": "Wed, 27 Apr 2016 21:09:43 GMT"}], "update_date": "2016-04-29", "authors_parsed": [["Chen", "Yutian", ""], ["Ghahramani", "Zoubin", ""]]}, {"id": "1506.09068", "submitter": "Shaohua Li", "authors": "Shaohua Li", "title": "On the Equivalence of Factorized Information Criterion Regularization\n  and the Chinese Restaurant Process Prior", "comments": "3 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Factorized Information Criterion (FIC) is a recently developed information\ncriterion, based on which a novel model selection methodology, namely\nFactorized Asymptotic Bayesian (FAB) Inference, has been developed and\nsuccessfully applied to various hierarchical Bayesian models. The Dirichlet\nProcess (DP) prior, and one of its well known representations, the Chinese\nRestaurant Process (CRP), derive another line of model selection methods. FIC\ncan be viewed as a prior distribution over the latent variable configurations.\nUnder this view, we prove that when the parameter dimensionality $D_{c}=2$, FIC\nis equivalent to CRP. We argue that when $D_{c}>2$, FIC avoids an inherent\nproblem of DP/CRP, i.e. the data likelihood will dominate the impact of the\nprior, and thus the model selection capability will weaken as $D_{c}$\nincreases. However, FIC overestimates the data likelihood. As a result, FIC may\nbe overly biased towards models with less components. We propose a natural\ngeneralization of FIC, which finds a middle ground between CRP and FIC, and may\nyield more accurate model selection results than FIC.\n", "versions": [{"version": "v1", "created": "Tue, 30 Jun 2015 13:02:15 GMT"}, {"version": "v2", "created": "Wed, 1 Jul 2015 03:01:50 GMT"}], "update_date": "2015-07-02", "authors_parsed": [["Li", "Shaohua", ""]]}, {"id": "1506.09153", "submitter": "Gunnar R\\\"atsch", "authors": "Christian Widmer, Marius Kloft, Vipin T Sreedharan, Gunnar R\\\"atsch", "title": "Framework for Multi-task Multiple Kernel Learning and Applications in\n  Genome Analysis", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.CE cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a general regularization-based framework for Multi-task learning\n(MTL), in which the similarity between tasks can be learned or refined using\n$\\ell_p$-norm Multiple Kernel learning (MKL). Based on this very general\nformulation (including a general loss function), we derive the corresponding\ndual formulation using Fenchel duality applied to Hermitian matrices. We show\nthat numerous established MTL methods can be derived as special cases from\nboth, the primal and dual of our formulation. Furthermore, we derive a modern\ndual-coordinate descend optimization strategy for the hinge-loss variant of our\nformulation and provide convergence bounds for our algorithm. As a special\ncase, we implement in C++ a fast LibLinear-style solver for $\\ell_p$-norm MKL.\nIn the experimental section, we analyze various aspects of our algorithm such\nas predictive performance and ability to reconstruct task relationships on\nbiologically inspired synthetic data, where we have full control over the\nunderlying ground truth. We also experiment on a new dataset from the domain of\ncomputational biology that we collected for the purpose of this paper. It\nconcerns the prediction of transcription start sites (TSS) over nine organisms,\nwhich is a crucial task in gene finding. Our solvers including all discussed\nspecial cases are made available as open-source software as part of the SHOGUN\nmachine learning toolbox (available at \\url{http://shogun.ml}).\n", "versions": [{"version": "v1", "created": "Tue, 30 Jun 2015 16:52:27 GMT"}], "update_date": "2015-07-01", "authors_parsed": [["Widmer", "Christian", ""], ["Kloft", "Marius", ""], ["Sreedharan", "Vipin T", ""], ["R\u00e4tsch", "Gunnar", ""]]}]