[{"id": "1409.0031", "submitter": "Eric Hall", "authors": "Eric C. Hall and Rebecca M. Willett", "title": "Tracking Dynamic Point Processes on Networks", "comments": null, "journal-ref": "IEEE Transaction on Information Theory, Vol. 62, No. 7, 2016", "doi": null, "report-no": null, "categories": "stat.ML cs.IT cs.SI math.IT", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Cascading chains of events are a salient feature of many real-world social,\nbiological, and financial networks. In social networks, social reciprocity\naccounts for retaliations in gang interactions, proxy wars in nation-state\nconflicts, or Internet memes shared via social media. Neuron spikes stimulate\nor inhibit spike activity in other neurons. Stock market shocks can trigger a\ncontagion of volatility throughout a financial network. In these and other\nexamples, only individual events associated with network nodes are observed,\nusually without knowledge of the underlying dynamic relationships between\nnodes. This paper addresses the challenge of tracking how events within such\nnetworks stimulate or influence future events. The proposed approach is an\nonline learning framework well-suited to streaming data, using a multivariate\nHawkes point process model to encapsulate autoregressive features of observed\nevents within the social network. Recent work on online learning in dynamic\nenvironments is leveraged not only to exploit the dynamics within the\nunderlying network, but also to track the network structure as it evolves.\nRegret bounds and experimental results demonstrate that the proposed method\nperforms nearly as well as an oracle or batch algorithm.\n", "versions": [{"version": "v1", "created": "Fri, 29 Aug 2014 20:37:09 GMT"}, {"version": "v2", "created": "Wed, 3 Sep 2014 23:58:39 GMT"}, {"version": "v3", "created": "Fri, 1 Jul 2016 20:56:02 GMT"}], "update_date": "2016-07-05", "authors_parsed": [["Hall", "Eric C.", ""], ["Willett", "Rebecca M.", ""]]}, {"id": "1409.0107", "submitter": "Marco Congedo", "authors": "Alexandre Barachant and Marco Congedo", "title": "A Plug&Play P300 BCI Using Information Geometry", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.HC stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper presents a new classification methods for Event Related Potentials\n(ERP) based on an Information geometry framework. Through a new estimation of\ncovariance matrices, this work extend the use of Riemannian geometry, which was\npreviously limited to SMR-based BCI, to the problem of classification of ERPs.\nAs compared to the state-of-the-art, this new method increases performance,\nreduces the number of data needed for the calibration and features good\ngeneralisation across sessions and subjects. This method is illustrated on data\nrecorded with the P300-based game brain invaders. Finally, an online and\nadaptive implementation is described, where the BCI is initialized with generic\nparameters derived from a database and continuously adapt to the individual,\nallowing the user to play the game without any calibration while keeping a high\naccuracy.\n", "versions": [{"version": "v1", "created": "Sat, 30 Aug 2014 12:17:15 GMT"}], "update_date": "2014-09-02", "authors_parsed": [["Barachant", "Alexandre", ""], ["Congedo", "Marco", ""]]}, {"id": "1409.0272", "submitter": "Andre Goncalves", "authors": "Andre R. Goncalves, Puja Das, Soumyadeep Chatterjee, Vidyashankar\n  Sivakumar, Fernando J. Von Zuben, Arindam Banerjee", "title": "Multi-task Sparse Structure Learning", "comments": "23rd ACM International Conference on Information and Knowledge\n  Management - CIKM 2014", "journal-ref": null, "doi": "10.1145/2661829.2662091", "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Multi-task learning (MTL) aims to improve generalization performance by\nlearning multiple related tasks simultaneously. While sometimes the underlying\ntask relationship structure is known, often the structure needs to be estimated\nfrom data at hand. In this paper, we present a novel family of models for MTL,\napplicable to regression and classification problems, capable of learning the\nstructure of task relationships. In particular, we consider a joint estimation\nproblem of the task relationship structure and the individual task parameters,\nwhich is solved using alternating minimization. The task relationship structure\nlearning component builds on recent advances in structure learning of Gaussian\ngraphical models based on sparse estimators of the precision (inverse\ncovariance) matrix. We illustrate the effectiveness of the proposed model on a\nvariety of synthetic and benchmark datasets for regression and classification.\nWe also consider the problem of combining climate model outputs for better\nprojections of future climate, with focus on temperature in South America, and\nshow that the proposed model outperforms several existing methods for the\nproblem.\n", "versions": [{"version": "v1", "created": "Mon, 1 Sep 2014 00:33:38 GMT"}, {"version": "v2", "created": "Tue, 2 Sep 2014 00:33:35 GMT"}], "update_date": "2017-01-31", "authors_parsed": [["Goncalves", "Andre R.", ""], ["Das", "Puja", ""], ["Chatterjee", "Soumyadeep", ""], ["Sivakumar", "Vidyashankar", ""], ["Von Zuben", "Fernando J.", ""], ["Banerjee", "Arindam", ""]]}, {"id": "1409.0473", "submitter": "Dzmitry Bahdanau", "authors": "Dzmitry Bahdanau and Kyunghyun Cho and Yoshua Bengio", "title": "Neural Machine Translation by Jointly Learning to Align and Translate", "comments": "Accepted at ICLR 2015 as oral presentation", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.LG cs.NE stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Neural machine translation is a recently proposed approach to machine\ntranslation. Unlike the traditional statistical machine translation, the neural\nmachine translation aims at building a single neural network that can be\njointly tuned to maximize the translation performance. The models proposed\nrecently for neural machine translation often belong to a family of\nencoder-decoders and consists of an encoder that encodes a source sentence into\na fixed-length vector from which a decoder generates a translation. In this\npaper, we conjecture that the use of a fixed-length vector is a bottleneck in\nimproving the performance of this basic encoder-decoder architecture, and\npropose to extend this by allowing a model to automatically (soft-)search for\nparts of a source sentence that are relevant to predicting a target word,\nwithout having to form these parts as a hard segment explicitly. With this new\napproach, we achieve a translation performance comparable to the existing\nstate-of-the-art phrase-based system on the task of English-to-French\ntranslation. Furthermore, qualitative analysis reveals that the\n(soft-)alignments found by the model agree well with our intuition.\n", "versions": [{"version": "v1", "created": "Mon, 1 Sep 2014 16:33:02 GMT"}, {"version": "v2", "created": "Thu, 4 Sep 2014 18:32:00 GMT"}, {"version": "v3", "created": "Tue, 7 Oct 2014 18:10:39 GMT"}, {"version": "v4", "created": "Fri, 19 Dec 2014 21:39:11 GMT"}, {"version": "v5", "created": "Sun, 22 Mar 2015 17:08:39 GMT"}, {"version": "v6", "created": "Fri, 24 Apr 2015 13:25:33 GMT"}, {"version": "v7", "created": "Thu, 19 May 2016 21:53:22 GMT"}], "update_date": "2016-05-23", "authors_parsed": [["Bahdanau", "Dzmitry", ""], ["Cho", "Kyunghyun", ""], ["Bengio", "Yoshua", ""]]}, {"id": "1409.0578", "submitter": "Sebastian Vollmer", "authors": "Yee Whye Teh, Alexandre Thi\\'ery, Sebastian Vollmer", "title": "Consistency and fluctuations for stochastic gradient Langevin dynamics", "comments": "35 pages, 5 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Applying standard Markov chain Monte Carlo (MCMC) algorithms to large data\nsets is computationally expensive. Both the calculation of the acceptance\nprobability and the creation of informed proposals usually require an iteration\nthrough the whole data set. The recently proposed stochastic gradient Langevin\ndynamics (SGLD) method circumvents this problem by generating proposals which\nare only based on a subset of the data, by skipping the accept-reject step and\nby using decreasing step-sizes sequence $(\\delta_m)_{m \\geq 0}$.\n  %Under appropriate Lyapunov conditions, We provide in this article a rigorous\nmathematical framework for analysing this algorithm. We prove that, under\nverifiable assumptions, the algorithm is consistent, satisfies a central limit\ntheorem (CLT) and its asymptotic bias-variance decomposition can be\ncharacterized by an explicit functional of the step-sizes sequence\n$(\\delta_m)_{m \\geq 0}$. We leverage this analysis to give practical\nrecommendations for the notoriously difficult tuning of this algorithm: it is\nasymptotically optimal to use a step-size sequence of the type $\\delta_m \\asymp\nm^{-1/3}$, leading to an algorithm whose mean squared error (MSE) decreases at\nrate $\\mathcal{O}(m^{-1/3})$\n", "versions": [{"version": "v1", "created": "Mon, 1 Sep 2014 23:00:35 GMT"}, {"version": "v2", "created": "Fri, 12 Jun 2015 08:00:46 GMT"}], "update_date": "2015-06-15", "authors_parsed": [["Teh", "Yee Whye", ""], ["Thi\u00e9ry", "Alexandre", ""], ["Vollmer", "Sebastian", ""]]}, {"id": "1409.0585", "submitter": "KyungHyun Cho", "authors": "Li Yao and Sherjil Ozair and Kyunghyun Cho and Yoshua Bengio", "title": "On the Equivalence Between Deep NADE and Generative Stochastic Networks", "comments": "ECML/PKDD 2014", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Neural Autoregressive Distribution Estimators (NADEs) have recently been\nshown as successful alternatives for modeling high dimensional multimodal\ndistributions. One issue associated with NADEs is that they rely on a\nparticular order of factorization for $P(\\mathbf{x})$. This issue has been\nrecently addressed by a variant of NADE called Orderless NADEs and its deeper\nversion, Deep Orderless NADE. Orderless NADEs are trained based on a criterion\nthat stochastically maximizes $P(\\mathbf{x})$ with all possible orders of\nfactorizations. Unfortunately, ancestral sampling from deep NADE is very\nexpensive, corresponding to running through a neural net separately predicting\neach of the visible variables given some others. This work makes a connection\nbetween this criterion and the training criterion for Generative Stochastic\nNetworks (GSNs). It shows that training NADEs in this way also trains a GSN,\nwhich defines a Markov chain associated with the NADE model. Based on this\nconnection, we show an alternative way to sample from a trained Orderless NADE\nthat allows to trade-off computing time and quality of the samples: a 3 to\n10-fold speedup (taking into account the waste due to correlations between\nconsecutive samples of the chain) can be obtained without noticeably reducing\nthe quality of the samples. This is achieved using a novel sampling procedure\nfor GSNs called annealed GSN sampling, similar to tempering methods that\ncombines fast mixing (obtained thanks to steps at high noise levels) with\naccurate samples (obtained thanks to steps at low noise levels).\n", "versions": [{"version": "v1", "created": "Tue, 2 Sep 2014 01:22:42 GMT"}], "update_date": "2014-09-03", "authors_parsed": [["Yao", "Li", ""], ["Ozair", "Sherjil", ""], ["Cho", "Kyunghyun", ""], ["Bengio", "Yoshua", ""]]}, {"id": "1409.0745", "submitter": "Hongyang Zhang", "authors": "Hongyang Zhang and Ruben H. Zamar", "title": "Multi-rank Sparse Hierarchical Clustering", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  There has been a surge in the number of large and flat data sets - data sets\ncontaining a large number of features and a relatively small number of\nobservations - due to the growing ability to collect and store information in\nmedical research and other fields. Hierarchical clustering is a widely used\nclustering tool. In hierarchical clustering, large and flat data sets may allow\nfor a better coverage of clustering features (features that help explain the\ntrue underlying clusters) but, such data sets usually include a large fraction\nof noise features (non-clustering features) that may hide the underlying\nclusters. Witten and Tibshirani (2010) proposed a sparse hierarchical\nclustering framework to cluster the observations using an adaptively chosen\nsubset of the features, however, we show that this framework has some\nlimitations when the data sets contain clustering features with complex\nstructure. In this paper, we propose the Multi-rank sparse hierarchical\nclustering (MrSHC). We show that, using simulation studies and real data\nexamples, MrSHC produces superior feature selection and clustering performance\ncomparing to the classical (of-the-shelf) hierarchical clustering and the\nexisting sparse hierarchical clustering framework.\n", "versions": [{"version": "v1", "created": "Tue, 2 Sep 2014 15:10:33 GMT"}, {"version": "v2", "created": "Mon, 3 Jul 2017 19:06:40 GMT"}], "update_date": "2017-07-05", "authors_parsed": [["Zhang", "Hongyang", ""], ["Zamar", "Ruben H.", ""]]}, {"id": "1409.0791", "submitter": "Jian Yang", "authors": "Jian Yang, Liqiu Meng", "title": "Feature Selection in Conditional Random Fields for Map Matching of GPS\n  Trajectories", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.AI cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Map matching of the GPS trajectory serves the purpose of recovering the\noriginal route on a road network from a sequence of noisy GPS observations. It\nis a fundamental technique to many Location Based Services. However, map\nmatching of a low sampling rate on urban road network is still a challenging\ntask. In this paper, the characteristics of Conditional Random Fields with\nregard to inducing many contextual features and feature selection are explored\nfor the map matching of the GPS trajectories at a low sampling rate.\nExperiments on a taxi trajectory dataset show that our method may achieve\ncompetitive results along with the success of reducing model complexity for\ncomputation-limited applications.\n", "versions": [{"version": "v1", "created": "Tue, 2 Sep 2014 16:52:53 GMT"}], "update_date": "2014-09-03", "authors_parsed": [["Yang", "Jian", ""], ["Meng", "Liqiu", ""]]}, {"id": "1409.0797", "submitter": "Jian Yang", "authors": "Jian Yang and Liqiu Meng", "title": "Feature Engineering for Map Matching of Low-Sampling-Rate GPS\n  Trajectories in Road Network", "comments": "ECML/PKDD14 workshop on Machine Learning for Urban Sensor\n  Data(SenseML)", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Map matching of GPS trajectories from a sequence of noisy observations serves\nthe purpose of recovering the original routes in a road network. In this work\nin progress, we attempt to share our experience of feature construction in a\nspatial database by reporting our ongoing experiment of feature extrac-tion in\nConditional Random Fields (CRFs) for map matching. Our preliminary results are\nobtained from real-world taxi GPS trajectories.\n", "versions": [{"version": "v1", "created": "Tue, 2 Sep 2014 17:15:58 GMT"}], "update_date": "2014-09-03", "authors_parsed": [["Yang", "Jian", ""], ["Meng", "Liqiu", ""]]}, {"id": "1409.0934", "submitter": "Takafumi Kanamori Dr.", "authors": "Takafumi Kanamori, Shuhei Fujiwara, Akiko Takeda", "title": "Breakdown Point of Robust Support Vector Machine", "comments": "27 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The support vector machine (SVM) is one of the most successful learning\nmethods for solving classification problems. Despite its popularity, SVM has a\nserious drawback, that is sensitivity to outliers in training samples. The\npenalty on misclassification is defined by a convex loss called the hinge loss,\nand the unboundedness of the convex loss causes the sensitivity to outliers. To\ndeal with outliers, robust variants of SVM have been proposed, such as the\nrobust outlier detection algorithm and an SVM with a bounded loss called the\nramp loss. In this paper, we propose a robust variant of SVM and investigate\nits robustness in terms of the breakdown point. The breakdown point is a\nrobustness measure that is the largest amount of contamination such that the\nestimated classifier still gives information about the non-contaminated data.\nThe main contribution of this paper is to show an exact evaluation of the\nbreakdown point for the robust SVM. For learning parameters such as the\nregularization parameter in our algorithm, we derive a simple formula that\nguarantees the robustness of the classifier. When the learning parameters are\ndetermined with a grid search using cross validation, our formula works to\nreduce the number of candidate search points. The robustness of the proposed\nmethod is confirmed in numerical experiments. We show that the statistical\nproperties of the robust SVM are well explained by a theoretical analysis of\nthe breakdown point.\n", "versions": [{"version": "v1", "created": "Wed, 3 Sep 2014 01:39:34 GMT"}], "update_date": "2014-09-04", "authors_parsed": [["Kanamori", "Takafumi", ""], ["Fujiwara", "Shuhei", ""], ["Takeda", "Akiko", ""]]}, {"id": "1409.0940", "submitter": "Haim Avron", "authors": "Vikas Sindhwani and Haim Avron", "title": "High-performance Kernel Machines with Implicit Distributed Optimization\n  and Randomization", "comments": "Work presented at MMDS 2014 (June 2014) and JSM 2014", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.DC cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In order to fully utilize \"big data\", it is often required to use \"big\nmodels\". Such models tend to grow with the complexity and size of the training\ndata, and do not make strong parametric assumptions upfront on the nature of\nthe underlying statistical dependencies. Kernel methods fit this need well, as\nthey constitute a versatile and principled statistical methodology for solving\na wide range of non-parametric modelling problems. However, their high\ncomputational costs (in storage and time) pose a significant barrier to their\nwidespread adoption in big data applications.\n  We propose an algorithmic framework and high-performance implementation for\nmassive-scale training of kernel-based statistical models, based on combining\ntwo key technical ingredients: (i) distributed general purpose convex\noptimization, and (ii) the use of randomization to improve the scalability of\nkernel methods. Our approach is based on a block-splitting variant of the\nAlternating Directions Method of Multipliers, carefully reconfigured to handle\nvery large random feature matrices, while exploiting hybrid parallelism\ntypically found in modern clusters of multicore machines. Our implementation\nsupports a variety of statistical learning tasks by enabling several loss\nfunctions, regularization schemes, kernels, and layers of randomized\napproximations for both dense and sparse datasets, in a highly extensible\nframework. We evaluate the ability of our framework to learn models on data\nfrom applications, and provide a comparison against existing sequential and\nparallel libraries.\n", "versions": [{"version": "v1", "created": "Wed, 3 Sep 2014 02:28:51 GMT"}, {"version": "v2", "created": "Tue, 23 Dec 2014 21:38:15 GMT"}, {"version": "v3", "created": "Thu, 16 Apr 2015 18:06:53 GMT"}], "update_date": "2015-04-17", "authors_parsed": [["Sindhwani", "Vikas", ""], ["Avron", "Haim", ""]]}, {"id": "1409.1062", "submitter": "Fanhua Shang", "authors": "Fanhua Shang, Yuanyuan Liu, Hanghang Tong, James Cheng, Hong Cheng", "title": "Structured Low-Rank Matrix Factorization with Missing and Grossly\n  Corrupted Observations", "comments": "28 pages, 9 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recovering low-rank and sparse matrices from incomplete or corrupted\nobservations is an important problem in machine learning, statistics,\nbioinformatics, computer vision, as well as signal and image processing. In\ntheory, this problem can be solved by the natural convex joint/mixed\nrelaxations (i.e., l_{1}-norm and trace norm) under certain conditions.\nHowever, all current provable algorithms suffer from superlinear per-iteration\ncost, which severely limits their applicability to large-scale problems. In\nthis paper, we propose a scalable, provable structured low-rank matrix\nfactorization method to recover low-rank and sparse matrices from missing and\ngrossly corrupted data, i.e., robust matrix completion (RMC) problems, or\nincomplete and grossly corrupted measurements, i.e., compressive principal\ncomponent pursuit (CPCP) problems. Specifically, we first present two\nsmall-scale matrix trace norm regularized bilinear structured factorization\nmodels for RMC and CPCP problems, in which repetitively calculating SVD of a\nlarge-scale matrix is replaced by updating two much smaller factor matrices.\nThen, we apply the alternating direction method of multipliers (ADMM) to\nefficiently solve the RMC problems. Finally, we provide the convergence\nanalysis of our algorithm, and extend it to address general CPCP problems.\nExperimental results verified both the efficiency and effectiveness of our\nmethod compared with the state-of-the-art methods.\n", "versions": [{"version": "v1", "created": "Wed, 3 Sep 2014 12:36:25 GMT"}], "update_date": "2014-09-04", "authors_parsed": [["Shang", "Fanhua", ""], ["Liu", "Yuanyuan", ""], ["Tong", "Hanghang", ""], ["Cheng", "James", ""], ["Cheng", "Hong", ""]]}, {"id": "1409.1257", "submitter": "KyungHyun Cho", "authors": "Jean Pouget-Abadie and Dzmitry Bahdanau and Bart van Merrienboer and\n  Kyunghyun Cho and Yoshua Bengio", "title": "Overcoming the Curse of Sentence Length for Neural Machine Translation\n  using Automatic Segmentation", "comments": "Eighth Workshop on Syntax, Semantics and Structure in Statistical\n  Translation (SSST-8)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.LG cs.NE stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The authors of (Cho et al., 2014a) have shown that the recently introduced\nneural network translation systems suffer from a significant drop in\ntranslation quality when translating long sentences, unlike existing\nphrase-based translation systems. In this paper, we propose a way to address\nthis issue by automatically segmenting an input sentence into phrases that can\nbe easily translated by the neural network translation model. Once each segment\nhas been independently translated by the neural machine translation model, the\ntranslated clauses are concatenated to form a final translation. Empirical\nresults show a significant improvement in translation quality for long\nsentences.\n", "versions": [{"version": "v1", "created": "Wed, 3 Sep 2014 21:00:49 GMT"}, {"version": "v2", "created": "Tue, 7 Oct 2014 18:09:37 GMT"}], "update_date": "2014-10-08", "authors_parsed": [["Pouget-Abadie", "Jean", ""], ["Bahdanau", "Dzmitry", ""], ["van Merrienboer", "Bart", ""], ["Cho", "Kyunghyun", ""], ["Bengio", "Yoshua", ""]]}, {"id": "1409.1259", "submitter": "KyungHyun Cho", "authors": "Kyunghyun Cho and Bart van Merrienboer and Dzmitry Bahdanau and Yoshua\n  Bengio", "title": "On the Properties of Neural Machine Translation: Encoder-Decoder\n  Approaches", "comments": "Eighth Workshop on Syntax, Semantics and Structure in Statistical\n  Translation (SSST-8)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Neural machine translation is a relatively new approach to statistical\nmachine translation based purely on neural networks. The neural machine\ntranslation models often consist of an encoder and a decoder. The encoder\nextracts a fixed-length representation from a variable-length input sentence,\nand the decoder generates a correct translation from this representation. In\nthis paper, we focus on analyzing the properties of the neural machine\ntranslation using two models; RNN Encoder--Decoder and a newly proposed gated\nrecursive convolutional neural network. We show that the neural machine\ntranslation performs relatively well on short sentences without unknown words,\nbut its performance degrades rapidly as the length of the sentence and the\nnumber of unknown words increase. Furthermore, we find that the proposed gated\nrecursive convolutional network learns a grammatical structure of a sentence\nautomatically.\n", "versions": [{"version": "v1", "created": "Wed, 3 Sep 2014 21:03:41 GMT"}, {"version": "v2", "created": "Tue, 7 Oct 2014 18:08:30 GMT"}], "update_date": "2014-10-08", "authors_parsed": [["Cho", "Kyunghyun", ""], ["van Merrienboer", "Bart", ""], ["Bahdanau", "Dzmitry", ""], ["Bengio", "Yoshua", ""]]}, {"id": "1409.1320", "submitter": "Wei Ping", "authors": "Wei Ping, Qiang Liu, Alexander Ihler", "title": "Marginal Structured SVM with Hidden Variables", "comments": "Accepted by the 31st International Conference on Machine Learning\n  (ICML 2014). 12 pages version with supplement", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this work, we propose the marginal structured SVM (MSSVM) for structured\nprediction with hidden variables. MSSVM properly accounts for the uncertainty\nof hidden variables, and can significantly outperform the previously proposed\nlatent structured SVM (LSSVM; Yu & Joachims (2009)) and other state-of-art\nmethods, especially when that uncertainty is large. Our method also results in\na smoother objective function, making gradient-based optimization of MSSVMs\nconverge significantly faster than for LSSVMs. We also show that our method\nconsistently outperforms hidden conditional random fields (HCRFs; Quattoni et\nal. (2007)) on both simulated and real-world datasets. Furthermore, we propose\na unified framework that includes both our and several other existing methods\nas special cases, and provides insights into the comparison of different models\nin practice.\n", "versions": [{"version": "v1", "created": "Thu, 4 Sep 2014 05:06:34 GMT"}, {"version": "v2", "created": "Fri, 5 Sep 2014 21:13:36 GMT"}], "update_date": "2014-09-09", "authors_parsed": [["Ping", "Wei", ""], ["Liu", "Qiang", ""], ["Ihler", "Alexander", ""]]}, {"id": "1409.1403", "submitter": "Vladimir Temlyakov", "authors": "D. Bazarkhanov and V. Temlyakov", "title": "Nonlinear tensor product approximation of functions", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML math.NA", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We are interested in approximation of a multivariate function\n$f(x_1,\\dots,x_d)$ by linear combinations of products $u^1(x_1)\\cdots u^d(x_d)$\nof univariate functions $u^i(x_i)$, $i=1,\\dots,d$. In the case $d=2$ it is a\nclassical problem of bilinear approximation. In the case of approximation in\nthe $L_2$ space the bilinear approximation problem is closely related to the\nproblem of singular value decomposition (also called Schmidt expansion) of the\ncorresponding integral operator with the kernel $f(x_1,x_2)$. There are known\nresults on the rate of decay of errors of best bilinear approximation in $L_p$\nunder different smoothness assumptions on $f$. The problem of multilinear\napproximation (nonlinear tensor product approximation) in the case $d\\ge 3$ is\nmore difficult and much less studied than the bilinear approximation problem.\nWe will present results on best multilinear approximation in $L_p$ under mixed\nsmoothness assumption on $f$.\n", "versions": [{"version": "v1", "created": "Thu, 4 Sep 2014 11:12:48 GMT"}], "update_date": "2014-09-05", "authors_parsed": [["Bazarkhanov", "D.", ""], ["Temlyakov", "V.", ""]]}, {"id": "1409.1458", "submitter": "Martin Jaggi", "authors": "Martin Jaggi, Virginia Smith, Martin Tak\\'a\\v{c}, Jonathan Terhorst,\n  Sanjay Krishnan, Thomas Hofmann, Michael I. Jordan", "title": "Communication-Efficient Distributed Dual Coordinate Ascent", "comments": "NIPS 2014 version, including proofs. Published in Advances in Neural\n  Information Processing Systems 27 (NIPS 2014)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG math.OC stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Communication remains the most significant bottleneck in the performance of\ndistributed optimization algorithms for large-scale machine learning. In this\npaper, we propose a communication-efficient framework, CoCoA, that uses local\ncomputation in a primal-dual setting to dramatically reduce the amount of\nnecessary communication. We provide a strong convergence rate analysis for this\nclass of algorithms, as well as experiments on real-world distributed datasets\nwith implementations in Spark. In our experiments, we find that as compared to\nstate-of-the-art mini-batch versions of SGD and SDCA algorithms, CoCoA\nconverges to the same .001-accurate solution quality on average 25x as quickly.\n", "versions": [{"version": "v1", "created": "Thu, 4 Sep 2014 14:59:35 GMT"}, {"version": "v2", "created": "Mon, 29 Sep 2014 16:07:32 GMT"}], "update_date": "2014-09-30", "authors_parsed": [["Jaggi", "Martin", ""], ["Smith", "Virginia", ""], ["Tak\u00e1\u010d", "Martin", ""], ["Terhorst", "Jonathan", ""], ["Krishnan", "Sanjay", ""], ["Hofmann", "Thomas", ""], ["Jordan", "Michael I.", ""]]}, {"id": "1409.1576", "submitter": "Amir Hajian", "authors": "Amir Hajian, Marcelo Alvarez, J. Richard Bond", "title": "Machine Learning Etudes in Astrophysics: Selection Functions for Mock\n  Cluster Catalogs", "comments": "Matches version to appear in JCAP. Discussions expanded, 2 figures\n  added. 16 pages, 8 figures", "journal-ref": null, "doi": "10.1088/1475-7516/2015/01/038", "report-no": null, "categories": "astro-ph.CO astro-ph.IM cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Making mock simulated catalogs is an important component of astrophysical\ndata analysis. Selection criteria for observed astronomical objects are often\ntoo complicated to be derived from first principles. However the existence of\nan observed group of objects is a well-suited problem for machine learning\nclassification. In this paper we use one-class classifiers to learn the\nproperties of an observed catalog of clusters of galaxies from ROSAT and to\npick clusters from mock simulations that resemble the observed ROSAT catalog.\nWe show how this method can be used to study the cross-correlations of thermal\nSunya'ev-Zeldovich signals with number density maps of X-ray selected cluster\ncatalogs. The method reduces the bias due to hand-tuning the selection function\nand is readily scalable to large catalogs with a high-dimensional space of\nastrophysical features.\n", "versions": [{"version": "v1", "created": "Thu, 4 Sep 2014 20:00:05 GMT"}, {"version": "v2", "created": "Wed, 21 Jan 2015 21:11:42 GMT"}], "update_date": "2015-06-22", "authors_parsed": [["Hajian", "Amir", ""], ["Alvarez", "Marcelo", ""], ["Bond", "J. Richard", ""]]}, {"id": "1409.1976", "submitter": "Quan Zhou", "authors": "Quan Zhou, Wenlin Chen, Shiji Song, Jacob R. Gardner, Kilian Q.\n  Weinberger, Yixin Chen", "title": "A Reduction of the Elastic Net to Support Vector Machines with an\n  Application to GPU Computing", "comments": "10 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The past years have witnessed many dedicated open-source projects that built\nand maintain implementations of Support Vector Machines (SVM), parallelized for\nGPU, multi-core CPUs and distributed systems. Up to this point, no comparable\neffort has been made to parallelize the Elastic Net, despite its popularity in\nmany high impact applications, including genetics, neuroscience and systems\nbiology. The first contribution in this paper is of theoretical nature. We\nestablish a tight link between two seemingly different algorithms and prove\nthat Elastic Net regression can be reduced to SVM with squared hinge loss\nclassification. Our second contribution is to derive a practical algorithm\nbased on this reduction. The reduction enables us to utilize prior efforts in\nspeeding up and parallelizing SVMs to obtain a highly optimized and parallel\nsolver for the Elastic Net and Lasso. With a simple wrapper, consisting of only\n11 lines of MATLAB code, we obtain an Elastic Net implementation that naturally\nutilizes GPU and multi-core CPUs. We demonstrate on twelve real world data\nsets, that our algorithm yields identical results as the popular (and highly\noptimized) glmnet implementation but is one or several orders of magnitude\nfaster.\n", "versions": [{"version": "v1", "created": "Sat, 6 Sep 2014 03:12:39 GMT"}], "update_date": "2014-09-09", "authors_parsed": [["Zhou", "Quan", ""], ["Chen", "Wenlin", ""], ["Song", "Shiji", ""], ["Gardner", "Jacob R.", ""], ["Weinberger", "Kilian Q.", ""], ["Chen", "Yixin", ""]]}, {"id": "1409.2045", "submitter": "Aryan Mokhtari", "authors": "Aryan Mokhtari and Alejandro Ribeiro", "title": "Global Convergence of Online Limited Memory BFGS", "comments": "37 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.OC cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Global convergence of an online (stochastic) limited memory version of the\nBroyden-Fletcher- Goldfarb-Shanno (BFGS) quasi-Newton method for solving\noptimization problems with stochastic objectives that arise in large scale\nmachine learning is established. Lower and upper bounds on the Hessian\neigenvalues of the sample functions are shown to suffice to guarantee that the\ncurvature approximation matrices have bounded determinants and traces, which,\nin turn, permits establishing convergence to optimal arguments with probability\n1. Numerical experiments on support vector machines with synthetic data\nshowcase reductions in convergence time relative to stochastic gradient descent\nalgorithms as well as reductions in storage and computation relative to other\nonline quasi-Newton methods. Experimental evaluation on a search engine\nadvertising problem corroborates that these advantages also manifest in\npractical applications.\n", "versions": [{"version": "v1", "created": "Sat, 6 Sep 2014 18:51:17 GMT"}], "update_date": "2014-09-09", "authors_parsed": [["Mokhtari", "Aryan", ""], ["Ribeiro", "Alejandro", ""]]}, {"id": "1409.2232", "submitter": "Jim Jing-Yan Wang", "authors": "Jim Jing-Yan Wang, Xuefeng Cui, Ge Yu, Lili Guo, Xin Gao", "title": "When coding meets ranking: A joint framework based on local learning", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Sparse coding, which represents a data point as a sparse reconstruction code\nwith regard to a dictionary, has been a popular data representation method.\nMeanwhile, in database retrieval problems, learning the ranking scores from\ndata points plays an important role. Up to now, these two problems have always\nbeen considered separately, assuming that data coding and ranking are two\nindependent and irrelevant problems. However, is there any internal\nrelationship between sparse coding and ranking score learning? If yes, how to\nexplore and make use of this internal relationship? In this paper, we try to\nanswer these questions by developing the first joint sparse coding and ranking\nscore learning algorithm. To explore the local distribution in the sparse code\nspace, and also to bridge coding and ranking problems, we assume that in the\nneighborhood of each data point, the ranking scores can be approximated from\nthe corresponding sparse codes by a local linear function. By considering the\nlocal approximation error of ranking scores, the reconstruction error and\nsparsity of sparse coding, and the query information provided by the user, we\nconstruct a unified objective function for learning of sparse codes, the\ndictionary and ranking scores. We further develop an iterative algorithm to\nsolve this optimization problem.\n", "versions": [{"version": "v1", "created": "Mon, 8 Sep 2014 08:10:37 GMT"}, {"version": "v2", "created": "Wed, 2 Nov 2016 07:33:51 GMT"}], "update_date": "2016-11-03", "authors_parsed": [["Wang", "Jim Jing-Yan", ""], ["Cui", "Xuefeng", ""], ["Yu", "Ge", ""], ["Guo", "Lili", ""], ["Gao", "Xin", ""]]}, {"id": "1409.2287", "submitter": "Andreas Damianou Mr", "authors": "Andreas C. Damianou, Michalis K. Titsias, Neil D. Lawrence", "title": "Variational Inference for Uncertainty on the Inputs of Gaussian Process\n  Models", "comments": "51 pages (of which 10 is Appendix), 19 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.AI cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The Gaussian process latent variable model (GP-LVM) provides a flexible\napproach for non-linear dimensionality reduction that has been widely applied.\nHowever, the current approach for training GP-LVMs is based on maximum\nlikelihood, where the latent projection variables are maximized over rather\nthan integrated out. In this paper we present a Bayesian method for training\nGP-LVMs by introducing a non-standard variational inference framework that\nallows to approximately integrate out the latent variables and subsequently\ntrain a GP-LVM by maximizing an analytic lower bound on the exact marginal\nlikelihood. We apply this method for learning a GP-LVM from iid observations\nand for learning non-linear dynamical systems where the observations are\ntemporally correlated. We show that a benefit of the variational Bayesian\nprocedure is its robustness to overfitting and its ability to automatically\nselect the dimensionality of the nonlinear latent space. The resulting\nframework is generic, flexible and easy to extend for other purposes, such as\nGaussian process regression with uncertain inputs and semi-supervised Gaussian\nprocesses. We demonstrate our method on synthetic data and standard machine\nlearning benchmarks, as well as challenging real world datasets, including high\nresolution video data.\n", "versions": [{"version": "v1", "created": "Mon, 8 Sep 2014 10:47:23 GMT"}], "update_date": "2014-09-09", "authors_parsed": [["Damianou", "Andreas C.", ""], ["Titsias", "Michalis K.", ""], ["Lawrence", "Neil D.", ""]]}, {"id": "1409.2552", "submitter": "Yan Li", "authors": "Yan Li", "title": "Sparse Additive Model using Symmetric Nonnegative Definite Smoothers", "comments": "This is a term project report and has been withdrawn by the authors;\n  arXiv admin note: author list has been modified due to misrepresentation of\n  authorship", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We introduce a new algorithm, called adaptive sparse backfitting algorithm,\nfor solving high dimensional Sparse Additive Model (SpAM) utilizing symmetric,\nnon-negative definite smoothers. Unlike the previous sparse backfitting\nalgorithm, our method is essentially a block coordinate descent algorithm that\nguarantees to converge to the optimal solution. It bridges the gap between the\npopulation backfitting algorithm and that of the data version. We also prove\nvariable selection consistency under suitable conditions. Numerical studies on\nboth synthesis and real data are conducted to show that adaptive sparse\nbackfitting algorithm outperforms previous sparse backfitting algorithm in\nfitting and predicting high dimensional nonparametric models.\n", "versions": [{"version": "v1", "created": "Mon, 8 Sep 2014 23:47:51 GMT"}, {"version": "v2", "created": "Mon, 29 Sep 2014 20:16:07 GMT"}, {"version": "v3", "created": "Fri, 3 Oct 2014 01:55:04 GMT"}], "update_date": "2014-11-13", "authors_parsed": [["Li", "Yan", ""]]}, {"id": "1409.2558", "submitter": "Ting Kei Pong", "authors": "Xiaojun Chen, Zhaosong Lu, Ting Kei Pong", "title": "Penalty methods for a class of non-Lipschitz optimization problems", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.OC stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider a class of constrained optimization problems with a possibly\nnonconvex non-Lipschitz objective and a convex feasible set being the\nintersection of a polyhedron and a possibly degenerate ellipsoid. Such problems\nhave a wide range of applications in data science, where the objective is used\nfor inducing sparsity in the solutions while the constraint set models the\nnoise tolerance and incorporates other prior information for data fitting. To\nsolve this class of constrained optimization problems, a common approach is the\npenalty method. However, there is little theory on exact penalization for\nproblems with nonconvex and non-Lipschitz objective functions. In this paper,\nwe study the existence of exact penalty parameters regarding local minimizers,\nstationary points and $\\epsilon$-minimizers under suitable assumptions.\nMoreover, we discuss a penalty method whose subproblems are solved via a\nnonmonotone proximal gradient method with a suitable update scheme for the\npenalty parameters, and prove the convergence of the algorithm to a KKT point\nof the constrained problem. Preliminary numerical results demonstrate the\nefficiency of the penalty method for finding sparse solutions of\nunderdetermined linear systems.\n", "versions": [{"version": "v1", "created": "Tue, 9 Sep 2014 00:41:56 GMT"}, {"version": "v2", "created": "Sat, 27 Jun 2015 12:20:30 GMT"}, {"version": "v3", "created": "Thu, 7 Apr 2016 02:14:59 GMT"}], "update_date": "2016-04-08", "authors_parsed": [["Chen", "Xiaojun", ""], ["Lu", "Zhaosong", ""], ["Pong", "Ting Kei", ""]]}, {"id": "1409.2574", "submitter": "John Hershey", "authors": "John R. Hershey, Jonathan Le Roux, Felix Weninger", "title": "Deep Unfolding: Model-Based Inspiration of Novel Deep Architectures", "comments": "Added sections on reducing belief propagation to network activation\n  functions, and on conversion between conventional network parameters and BP\n  potentials for binary MRFs. Some bugs and typos were also fixed, and notation\n  made a bit clearer", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.NE stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Model-based methods and deep neural networks have both been tremendously\nsuccessful paradigms in machine learning. In model-based methods, problem\ndomain knowledge can be built into the constraints of the model, typically at\nthe expense of difficulties during inference. In contrast, deterministic deep\nneural networks are constructed in such a way that inference is\nstraightforward, but their architectures are generic and it is unclear how to\nincorporate knowledge. This work aims to obtain the advantages of both\napproaches. To do so, we start with a model-based approach and an associated\ninference algorithm, and \\emph{unfold} the inference iterations as layers in a\ndeep network. Rather than optimizing the original model, we \\emph{untie} the\nmodel parameters across layers, in order to create a more powerful network. The\nresulting architecture can be trained discriminatively to perform accurate\ninference within a fixed network size. We show how this framework allows us to\ninterpret conventional networks as mean-field inference in Markov random\nfields, and to obtain new architectures by instead using belief propagation as\nthe inference algorithm. We then show its application to a non-negative matrix\nfactorization model that incorporates the problem-domain knowledge that sound\nsources are additive. Deep unfolding of this model yields a new kind of\nnon-negative deep neural network, that can be trained using a multiplicative\nbackpropagation-style update algorithm. We present speech enhancement\nexperiments showing that our approach is competitive with conventional neural\nnetworks despite using far fewer parameters.\n", "versions": [{"version": "v1", "created": "Tue, 9 Sep 2014 02:31:11 GMT"}, {"version": "v2", "created": "Thu, 11 Sep 2014 22:59:52 GMT"}, {"version": "v3", "created": "Wed, 8 Oct 2014 23:50:51 GMT"}, {"version": "v4", "created": "Thu, 20 Nov 2014 01:52:53 GMT"}], "update_date": "2014-11-21", "authors_parsed": [["Hershey", "John R.", ""], ["Roux", "Jonathan Le", ""], ["Weninger", "Felix", ""]]}, {"id": "1409.2617", "submitter": "Ahmed Hefny", "authors": "Sashank Reddi, Ahmed Hefny, Carlton Downey, Avinava Dubey, Suvrit Sra", "title": "Large-scale randomized-coordinate descent methods with non-separable\n  linear constraints", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.OC stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We develop randomized (block) coordinate descent (CD) methods for linearly\nconstrained convex optimization. Unlike most CD methods, we do not assume the\nconstraints to be separable, but let them be coupled linearly. To our\nknowledge, ours is the first CD method that allows linear coupling constraints,\nwithout making the global iteration complexity have an exponential dependence\non the number of constraints. We present algorithms and analysis for four key\nproblem scenarios: (i) smooth; (ii) smooth + nonsmooth separable; (iii)\nasynchronous parallel; and (iv) stochastic. We illustrate empirical behavior of\nour algorithms by simulation experiments.\n", "versions": [{"version": "v1", "created": "Tue, 9 Sep 2014 07:05:19 GMT"}, {"version": "v2", "created": "Sun, 21 Sep 2014 00:10:38 GMT"}, {"version": "v3", "created": "Fri, 3 Oct 2014 11:37:51 GMT"}, {"version": "v4", "created": "Wed, 4 Mar 2015 00:44:38 GMT"}, {"version": "v5", "created": "Wed, 10 Jun 2015 04:52:35 GMT"}], "update_date": "2015-06-11", "authors_parsed": [["Reddi", "Sashank", ""], ["Hefny", "Ahmed", ""], ["Downey", "Carlton", ""], ["Dubey", "Avinava", ""], ["Sra", "Suvrit", ""]]}, {"id": "1409.2620", "submitter": "Suyog Gupta", "authors": "Suyog Gupta, Vikas Sindhwani, Kailash Gopalakrishnan", "title": "Learning Machines Implemented on Non-Deterministic Hardware", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper highlights new opportunities for designing large-scale machine\nlearning systems as a consequence of blurring traditional boundaries that have\nallowed algorithm designers and application-level practitioners to stay -- for\nthe most part -- oblivious to the details of the underlying hardware-level\nimplementations. The hardware/software co-design methodology advocated here\nhinges on the deployment of compute-intensive machine learning kernels onto\ncompute platforms that trade-off determinism in the computation for improvement\nin speed and/or energy efficiency. To achieve this, we revisit digital\nstochastic circuits for approximating matrix computations that are ubiquitous\nin machine learning algorithms. Theoretical and empirical evaluation is\nundertaken to assess the impact of the hardware-induced computational noise on\nalgorithm performance. As a proof-of-concept, a stochastic hardware simulator\nis employed for training deep neural networks for image recognition problems.\n", "versions": [{"version": "v1", "created": "Tue, 9 Sep 2014 07:35:40 GMT"}], "update_date": "2014-09-10", "authors_parsed": [["Gupta", "Suyog", ""], ["Sindhwani", "Vikas", ""], ["Gopalakrishnan", "Kailash", ""]]}, {"id": "1409.2655", "submitter": "Lester Mackey", "authors": "Lester Mackey and Jordan Bryan and Man Yue Mo", "title": "Weighted Classification Cascades for Optimizing Discovery Significance\n  in the HiggsML Challenge", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We introduce a minorization-maximization approach to optimizing common\nmeasures of discovery significance in high energy physics. The approach\nalternates between solving a weighted binary classification problem and\nupdating class weights in a simple, closed-form manner. Moreover, an argument\nbased on convex duality shows that an improvement in weighted classification\nerror on any round yields a commensurate improvement in discovery significance.\nWe complement our derivation with experimental results from the 2014 Higgs\nboson machine learning challenge.\n", "versions": [{"version": "v1", "created": "Tue, 9 Sep 2014 09:52:59 GMT"}, {"version": "v2", "created": "Wed, 10 Sep 2014 13:48:21 GMT"}, {"version": "v3", "created": "Sat, 13 Dec 2014 23:00:52 GMT"}, {"version": "v4", "created": "Sun, 14 Jun 2015 22:25:40 GMT"}, {"version": "v5", "created": "Thu, 10 Sep 2015 06:57:59 GMT"}], "update_date": "2015-09-11", "authors_parsed": [["Mackey", "Lester", ""], ["Bryan", "Jordan", ""], ["Mo", "Man Yue", ""]]}, {"id": "1409.2713", "submitter": "Henrik Nyman", "authors": "Henrik Nyman, Johan Pensar, Timo Koski, Jukka Corander", "title": "Context-specific independence in graphical log-linear models", "comments": "18 pages, 5 figures. arXiv admin note: text overlap with\n  arXiv:1309.6415", "journal-ref": null, "doi": "10.1007/s00180-015-0606-6", "report-no": null, "categories": "stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Log-linear models are the popular workhorses of analyzing contingency tables.\nA log-linear parameterization of an interaction model can be more expressive\nthan a direct parameterization based on probabilities, leading to a powerful\nway of defining restrictions derived from marginal, conditional and\ncontext-specific independence. However, parameter estimation is often simpler\nunder a direct parameterization, provided that the model enjoys certain\ndecomposability properties. Here we introduce a cyclical projection algorithm\nfor obtaining maximum likelihood estimates of log-linear parameters under an\narbitrary context-specific graphical log-linear model, which needs not satisfy\ncriteria of decomposability. We illustrate that lifting the restriction of\ndecomposability makes the models more expressive, such that additional\ncontext-specific independencies embedded in real data can be identified. It is\nalso shown how a context-specific graphical model can correspond to a\nnon-hierarchical log-linear parameterization with a concise interpretation.\nThis observation can pave way to further development of non-hierarchical\nlog-linear models, which have been largely neglected due to their believed lack\nof interpretability.\n", "versions": [{"version": "v1", "created": "Tue, 9 Sep 2014 12:33:56 GMT"}], "update_date": "2015-08-06", "authors_parsed": [["Nyman", "Henrik", ""], ["Pensar", "Johan", ""], ["Koski", "Timo", ""], ["Corander", "Jukka", ""]]}, {"id": "1409.2802", "submitter": "William March", "authors": "William B. March and George Biros", "title": "Far-Field Compression for Fast Kernel Summation Methods in High\n  Dimensions", "comments": "43 pages, 21 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider fast kernel summations in high dimensions: given a large set of\npoints in $d$ dimensions (with $d \\gg 3$) and a pair-potential function (the\n{\\em kernel} function), we compute a weighted sum of all pairwise kernel\ninteractions for each point in the set. Direct summation is equivalent to a\n(dense) matrix-vector multiplication and scales quadratically with the number\nof points. Fast kernel summation algorithms reduce this cost to log-linear or\nlinear complexity.\n  Treecodes and Fast Multipole Methods (FMMs) deliver tremendous speedups by\nconstructing approximate representations of interactions of points that are far\nfrom each other. In algebraic terms, these representations correspond to\nlow-rank approximations of blocks of the overall interaction matrix. Existing\napproaches require an excessive number of kernel evaluations with increasing\n$d$ and number of points in the dataset.\n  To address this issue, we use a randomized algebraic approach in which we\nfirst sample the rows of a block and then construct its approximate, low-rank\ninterpolative decomposition. We examine the feasibility of this approach\ntheoretically and experimentally. We provide a new theoretical result showing a\ntighter bound on the reconstruction error from uniformly sampling rows than the\nexisting state-of-the-art. We demonstrate that our sampling approach is\ncompetitive with existing (but prohibitively expensive) methods from the\nliterature. We also construct kernel matrices for the Laplacian, Gaussian, and\npolynomial kernels -- all commonly used in physics and data analysis. We\nexplore the numerical properties of blocks of these matrices, and show that\nthey are amenable to our approach. Depending on the data set, our randomized\nalgorithm can successfully compute low rank approximations in high dimensions.\nWe report results for data sets with ambient dimensions from four to 1,000.\n", "versions": [{"version": "v1", "created": "Tue, 9 Sep 2014 16:28:40 GMT"}, {"version": "v2", "created": "Fri, 13 Feb 2015 00:30:28 GMT"}], "update_date": "2015-02-16", "authors_parsed": [["March", "William B.", ""], ["Biros", "George", ""]]}, {"id": "1409.2824", "submitter": "Ulrich Paquet", "authors": "Ulrich Paquet and Noam Koenigstein and Ole Winther", "title": "Scalable Bayesian Modelling of Paired Symbols", "comments": "15 pages, 6 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a novel, scalable and Bayesian approach to modelling the\noccurrence of pairs of symbols (i,j) drawn from a large vocabulary. Observed\npairs are assumed to be generated by a simple popularity based selection\nprocess followed by censoring using a preference function. By basing inference\non the well-founded principle of variational bounding, and using new\nsite-independent bounds, we show how a scalable inference procedure can be\nobtained for large data sets. State of the art results are presented on\nreal-world movie viewing data.\n", "versions": [{"version": "v1", "created": "Tue, 9 Sep 2014 17:42:55 GMT"}, {"version": "v2", "created": "Wed, 10 Sep 2014 09:30:37 GMT"}], "update_date": "2014-09-11", "authors_parsed": [["Paquet", "Ulrich", ""], ["Koenigstein", "Noam", ""], ["Winther", "Ole", ""]]}, {"id": "1409.2848", "submitter": "Ohad Shamir", "authors": "Ohad Shamir", "title": "A Stochastic PCA and SVD Algorithm with an Exponential Convergence Rate", "comments": "Fixed a minor bug in the proof of lemma 1 (which does not affect the\n  result)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.NA math.OC stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We describe and analyze a simple algorithm for principal component analysis\nand singular value decomposition, VR-PCA, which uses computationally cheap\nstochastic iterations, yet converges exponentially fast to the optimal\nsolution. In contrast, existing algorithms suffer either from slow convergence,\nor computationally intensive iterations whose runtime scales with the data\nsize. The algorithm builds on a recent variance-reduced stochastic gradient\ntechnique, which was previously analyzed for strongly convex optimization,\nwhereas here we apply it to an inherently non-convex problem, using a very\ndifferent analysis.\n", "versions": [{"version": "v1", "created": "Tue, 9 Sep 2014 19:31:52 GMT"}, {"version": "v2", "created": "Sun, 25 Jan 2015 08:28:03 GMT"}, {"version": "v3", "created": "Sun, 19 Apr 2015 14:19:08 GMT"}, {"version": "v4", "created": "Sun, 26 Apr 2015 12:20:12 GMT"}, {"version": "v5", "created": "Fri, 31 Jul 2015 04:41:42 GMT"}], "update_date": "2015-08-03", "authors_parsed": [["Shamir", "Ohad", ""]]}, {"id": "1409.2944", "submitter": "Hao Wang", "authors": "Hao Wang and Naiyan Wang and Dit-Yan Yeung", "title": "Collaborative Deep Learning for Recommender Systems", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CL cs.IR cs.NE stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Collaborative filtering (CF) is a successful approach commonly used by many\nrecommender systems. Conventional CF-based methods use the ratings given to\nitems by users as the sole source of information for learning to make\nrecommendation. However, the ratings are often very sparse in many\napplications, causing CF-based methods to degrade significantly in their\nrecommendation performance. To address this sparsity problem, auxiliary\ninformation such as item content information may be utilized. Collaborative\ntopic regression (CTR) is an appealing recent method taking this approach which\ntightly couples the two components that learn from two different sources of\ninformation. Nevertheless, the latent representation learned by CTR may not be\nvery effective when the auxiliary information is very sparse. To address this\nproblem, we generalize recent advances in deep learning from i.i.d. input to\nnon-i.i.d. (CF-based) input and propose in this paper a hierarchical Bayesian\nmodel called collaborative deep learning (CDL), which jointly performs deep\nrepresentation learning for the content information and collaborative filtering\nfor the ratings (feedback) matrix. Extensive experiments on three real-world\ndatasets from different domains show that CDL can significantly advance the\nstate of the art.\n", "versions": [{"version": "v1", "created": "Wed, 10 Sep 2014 03:05:22 GMT"}, {"version": "v2", "created": "Thu, 18 Jun 2015 09:23:37 GMT"}], "update_date": "2015-06-22", "authors_parsed": [["Wang", "Hao", ""], ["Wang", "Naiyan", ""], ["Yeung", "Dit-Yan", ""]]}, {"id": "1409.3257", "submitter": "Lin Xiao", "authors": "Yuchen Zhang and Lin Xiao", "title": "Stochastic Primal-Dual Coordinate Method for Regularized Empirical Risk\n  Minimization", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.OC stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider a generic convex optimization problem associated with regularized\nempirical risk minimization of linear predictors. The problem structure allows\nus to reformulate it as a convex-concave saddle point problem. We propose a\nstochastic primal-dual coordinate (SPDC) method, which alternates between\nmaximizing over a randomly chosen dual variable and minimizing over the primal\nvariable. An extrapolation step on the primal variable is performed to obtain\naccelerated convergence rate. We also develop a mini-batch version of the SPDC\nmethod which facilitates parallel computing, and an extension with weighted\nsampling probabilities on the dual variables, which has a better complexity\nthan uniform sampling on unnormalized data. Both theoretically and empirically,\nwe show that the SPDC method has comparable or better performance than several\nstate-of-the-art optimization methods.\n", "versions": [{"version": "v1", "created": "Wed, 10 Sep 2014 21:25:22 GMT"}, {"version": "v2", "created": "Wed, 9 Sep 2015 05:37:23 GMT"}], "update_date": "2015-09-10", "authors_parsed": [["Zhang", "Yuchen", ""], ["Xiao", "Lin", ""]]}, {"id": "1409.3518", "submitter": "Do-kyum Kim", "authors": "Do-kyum Kim, Geoffrey M. Voelker, Lawrence K. Saul", "title": "Topic Modeling of Hierarchical Corpora", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.IR cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study the problem of topic modeling in corpora whose documents are\norganized in a multi-level hierarchy. We explore a parametric approach to this\nproblem, assuming that the number of topics is known or can be estimated by\ncross-validation. The models we consider can be viewed as special\n(finite-dimensional) instances of hierarchical Dirichlet processes (HDPs). For\nthese models we show that there exists a simple variational approximation for\nprobabilistic inference. The approximation relies on a previously unexploited\ninequality that handles the conditional dependence between Dirichlet latent\nvariables in adjacent levels of the model's hierarchy. We compare our approach\nto existing implementations of nonparametric HDPs. On several benchmarks we\nfind that our approach is faster than Gibbs sampling and able to learn more\npredictive models than existing variational methods. Finally, we demonstrate\nthe large-scale viability of our approach on two newly available corpora from\nresearchers in computer security---one with 350,000 documents and over 6,000\ninternal subcategories, the other with a five-level deep hierarchy.\n", "versions": [{"version": "v1", "created": "Thu, 11 Sep 2014 18:00:59 GMT"}, {"version": "v2", "created": "Mon, 13 Apr 2015 06:29:46 GMT"}], "update_date": "2015-04-14", "authors_parsed": [["Kim", "Do-kyum", ""], ["Voelker", "Geoffrey M.", ""], ["Saul", "Lawrence K.", ""]]}, {"id": "1409.3660", "submitter": "Fei-Yun Zhu", "authors": "Feiyun Zhu, Bin Fan, Xinliang Zhu, Ying Wang, Shiming Xiang and\n  Chunhong Pan", "title": "10,000+ Times Accelerated Robust Subset Selection (ARSS)", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Subset selection from massive data with noised information is increasingly\npopular for various applications. This problem is still highly challenging as\ncurrent methods are generally slow in speed and sensitive to outliers. To\naddress the above two issues, we propose an accelerated robust subset selection\n(ARSS) method. Specifically in the subset selection area, this is the first\nattempt to employ the $\\ell_{p}(0<p\\leq1)$-norm based measure for the\nrepresentation loss, preventing large errors from dominating our objective. As\na result, the robustness against outlier elements is greatly enhanced.\nActually, data size is generally much larger than feature length, i.e. $N\\gg\nL$. Based on this observation, we propose a speedup solver (via ALM and\nequivalent derivations) to highly reduce the computational cost, theoretically\nfrom $O(N^{4})$ to $O(N{}^{2}L)$. Extensive experiments on ten benchmark\ndatasets verify that our method not only outperforms state of the art methods,\nbut also runs 10,000+ times faster than the most related method.\n", "versions": [{"version": "v1", "created": "Fri, 12 Sep 2014 07:18:17 GMT"}, {"version": "v2", "created": "Fri, 19 Sep 2014 02:49:19 GMT"}, {"version": "v3", "created": "Mon, 13 Oct 2014 07:58:57 GMT"}, {"version": "v4", "created": "Mon, 17 Nov 2014 14:39:31 GMT"}], "update_date": "2014-11-18", "authors_parsed": [["Zhu", "Feiyun", ""], ["Fan", "Bin", ""], ["Zhu", "Xinliang", ""], ["Wang", "Ying", ""], ["Xiang", "Shiming", ""], ["Pan", "Chunhong", ""]]}, {"id": "1409.3768", "submitter": "Sang-Yun Oh", "authors": "Sang-Yun Oh, Onkar Dalal, Kshitij Khare, Bala Rajaratnam", "title": "Optimization Methods for Sparse Pseudo-Likelihood Graphical Model\n  Selection", "comments": "NIPS accepted version", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.CO cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Sparse high dimensional graphical model selection is a popular topic in\ncontemporary machine learning. To this end, various useful approaches have been\nproposed in the context of $\\ell_1$-penalized estimation in the Gaussian\nframework. Though many of these inverse covariance estimation approaches are\ndemonstrably scalable and have leveraged recent advances in convex\noptimization, they still depend on the Gaussian functional form. To address\nthis gap, a convex pseudo-likelihood based partial correlation graph estimation\nmethod (CONCORD) has been recently proposed. This method uses coordinate-wise\nminimization of a regression based pseudo-likelihood, and has been shown to\nhave robust model selection properties in comparison with the Gaussian\napproach. In direct contrast to the parallel work in the Gaussian setting\nhowever, this new convex pseudo-likelihood framework has not leveraged the\nextensive array of methods that have been proposed in the machine learning\nliterature for convex optimization. In this paper, we address this crucial gap\nby proposing two proximal gradient methods (CONCORD-ISTA and CONCORD-FISTA) for\nperforming $\\ell_1$-regularized inverse covariance matrix estimation in the\npseudo-likelihood framework. We present timing comparisons with coordinate-wise\nminimization and demonstrate that our approach yields tremendous payoffs for\n$\\ell_1$-penalized partial correlation graph estimation outside the Gaussian\nsetting, thus yielding the fastest and most scalable approach for such\nproblems. We undertake a theoretical analysis of our approach and rigorously\ndemonstrate convergence, and also derive rates thereof.\n", "versions": [{"version": "v1", "created": "Fri, 12 Sep 2014 15:25:07 GMT"}], "update_date": "2014-09-15", "authors_parsed": [["Oh", "Sang-Yun", ""], ["Dalal", "Onkar", ""], ["Khare", "Kshitij", ""], ["Rajaratnam", "Bala", ""]]}, {"id": "1409.3881", "submitter": "Michael Bloodgood", "authors": "Michael Bloodgood and K. Vijay-Shanker", "title": "An Approach to Reducing Annotation Costs for BioNLP", "comments": "2 pages, 1 figure, 5 tables; appeared in Proceedings of the Workshop\n  on Current Trends in Biomedical Natural Language Processing at ACL\n  (Association for Computational Linguistics) 2008", "journal-ref": "In Proceedings of the Workshop on Current Trends in Biomedical\n  Natural Language Processing, pages 104-105, Columbus, Ohio, June 2008.\n  Association for Computational Linguistics", "doi": null, "report-no": null, "categories": "cs.CL cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  There is a broad range of BioNLP tasks for which active learning (AL) can\nsignificantly reduce annotation costs and a specific AL algorithm we have\ndeveloped is particularly effective in reducing annotation costs for these\ntasks. We have previously developed an AL algorithm called ClosestInitPA that\nworks best with tasks that have the following characteristics: redundancy in\ntraining material, burdensome annotation costs, Support Vector Machines (SVMs)\nwork well for the task, and imbalanced datasets (i.e. when set up as a binary\nclassification problem, one class is substantially rarer than the other). Many\nBioNLP tasks have these characteristics and thus our AL algorithm is a natural\napproach to apply to BioNLP tasks.\n", "versions": [{"version": "v1", "created": "Fri, 12 Sep 2014 22:40:38 GMT"}], "update_date": "2014-09-16", "authors_parsed": [["Bloodgood", "Michael", ""], ["Vijay-Shanker", "K.", ""]]}, {"id": "1409.3912", "submitter": "Takafumi Kanamori Dr.", "authors": "Kota Matsui, Wataru Kumagai, Takafumi Kanamori", "title": "Parallel Distributed Block Coordinate Descent Methods based on Pairwise\n  Comparison Oracle", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper provides a block coordinate descent algorithm to solve\nunconstrained optimization problems. In our algorithm, computation of function\nvalues or gradients is not required. Instead, pairwise comparison of function\nvalues is used. Our algorithm consists of two steps; one is the direction\nestimate step and the other is the search step. Both steps require only\npairwise comparison of function values, which tells us only the order of\nfunction values over two points. In the direction estimate step, a Newton type\nsearch direction is estimated. A computation method like block coordinate\ndescent methods is used with the pairwise comparison. In the search step, a\nnumerical solution is updated along the estimated direction. The computation in\nthe direction estimate step can be easily parallelized, and thus, the algorithm\nworks efficiently to find the minimizer of the objective function. Also, we\nshow an upper bound of the convergence rate. In numerical experiments, we show\nthat our method efficiently finds the optimal solution compared to some\nexisting methods based on the pairwise comparison.\n", "versions": [{"version": "v1", "created": "Sat, 13 Sep 2014 05:45:32 GMT"}], "update_date": "2014-09-16", "authors_parsed": [["Matsui", "Kota", ""], ["Kumagai", "Wataru", ""], ["Kanamori", "Takafumi", ""]]}, {"id": "1409.4005", "submitter": "Robert Nowak", "authors": "Mario A. T. Figueiredo and Robert D. Nowak", "title": "Sparse Estimation with Strongly Correlated Variables using Ordered\n  Weighted L1 Regularization", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper studies ordered weighted L1 (OWL) norm regularization for sparse\nestimation problems with strongly correlated variables. We prove sufficient\nconditions for clustering based on the correlation/colinearity of variables\nusing the OWL norm, of which the so-called OSCAR is a particular case. Our\nresults extend previous ones for OSCAR in several ways: for the squared error\nloss, our conditions hold for the more general OWL norm and under weaker\nassumptions; we also establish clustering conditions for the absolute error\nloss, which is, as far as we know, a novel result. Furthermore, we characterize\nthe statistical performance of OWL norm regularization for generative models in\nwhich certain clusters of regression variables are strongly (even perfectly)\ncorrelated, but variables in different clusters are uncorrelated. We show that\nif the true p-dimensional signal generating the data involves only s of the\nclusters, then O(s log p) samples suffice to accurately estimate the signal,\nregardless of the number of coefficients within the clusters. The estimation of\ns-sparse signals with completely independent variables requires just as many\nmeasurements. In other words, using the OWL we pay no price (in terms of the\nnumber of measurements) for the presence of strongly correlated variables.\n", "versions": [{"version": "v1", "created": "Sun, 14 Sep 2014 02:33:16 GMT"}], "update_date": "2014-09-16", "authors_parsed": [["Figueiredo", "Mario A. T.", ""], ["Nowak", "Robert D.", ""]]}, {"id": "1409.4011", "submitter": "David Duvenaud", "authors": "Kevin Swersky, David Duvenaud, Jasper Snoek, Frank Hutter, Michael A.\n  Osborne", "title": "Raiders of the Lost Architecture: Kernels for Bayesian Optimization in\n  Conditional Parameter Spaces", "comments": "6 pages, 3 figures. Appeared in the NIPS 2013 workshop on Bayesian\n  optimization", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML", "license": "http://creativecommons.org/licenses/by-nc-sa/3.0/", "abstract": "  In practical Bayesian optimization, we must often search over structures with\ndiffering numbers of parameters. For instance, we may wish to search over\nneural network architectures with an unknown number of layers. To relate\nperformance data gathered for different architectures, we define a new kernel\nfor conditional parameter spaces that explicitly includes information about\nwhich parameters are relevant in a given structure. We show that this kernel\nimproves model quality and Bayesian optimization results over several simpler\nbaseline kernels.\n", "versions": [{"version": "v1", "created": "Sun, 14 Sep 2014 04:01:37 GMT"}], "update_date": "2014-09-16", "authors_parsed": [["Swersky", "Kevin", ""], ["Duvenaud", "David", ""], ["Snoek", "Jasper", ""], ["Hutter", "Frank", ""], ["Osborne", "Michael A.", ""]]}, {"id": "1409.4044", "submitter": "Alain Tapp", "authors": "Alain Tapp", "title": "A new approach in machine learning", "comments": "Preliminary report", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this technical report we presented a novel approach to machine learning.\nOnce the new framework is presented, we will provide a simple and yet very\npowerful learning algorithm which will be benchmark on various dataset.\n  The framework we proposed is based on booleen circuits; more specifically the\nclassifier produced by our algorithm have that form. Using bits and boolean\ngates instead of real numbers and multiplication enable the the learning\nalgorithm and classifier to use very efficient boolean vector operations. This\nenable both the learning algorithm and classifier to be extremely efficient.\nThe accuracy of the classifier we obtain with our framework compares very\nfavorably those produced by conventional techniques, both in terms of\nefficiency and accuracy.\n", "versions": [{"version": "v1", "created": "Sun, 14 Sep 2014 10:25:23 GMT"}], "update_date": "2014-09-16", "authors_parsed": [["Tapp", "Alain", ""]]}, {"id": "1409.4141", "submitter": "Harold Soh", "authors": "Harold Soh", "title": "Probabilistic Network Metrics: Variational Bayesian Network Centrality", "comments": "17 pages, 6 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Network metrics form a fundamental part of the network analysis toolbox. Used\nto quantitatively measure different aspects of the network, these metrics can\ngive insights into the underlying network structure and function. In this work,\nwe connect network metrics to modern probabilistic machine learning. We focus\non the centrality metric, which is used a wide variety of applications from web\nsearch to gene-analysis. First, we formulate an eigenvector-based Bayesian\ncentrality model for determining node importance. Compared to existing methods,\nour probabilistic model allows for the assimilation of multiple edge weight\nobservations, the inclusion of priors and the extraction of uncertainties. To\nenable tractable inference, we develop a variational lower bound (VBC) that is\ndemonstrated to be effective on a variety of networks (two synthetic and five\nreal-world graphs). We then bridge this model to sparse Gaussian processes. The\nsparse variational Bayesian centrality Gaussian process (VBC-GP) learns a\nmapping between node attributes to latent centrality and hence, is capable of\npredicting centralities from node features and can potentially represent a\nlarge number of nodes using only a limited number of inducing inputs.\nExperiments show that the VBC-GP learns high-quality mappings and compares\nfavorably to a two-step baseline, i.e., a full GP trained on the node\nattributes and pre-computed centralities. Finally, we present two case-studies\nusing the VBC-GP: first, to ascertain relevant features in a taxi transport\nnetwork and second, to distribute a limited number of vaccines to mitigate the\nseverity of a viral outbreak.\n", "versions": [{"version": "v1", "created": "Mon, 15 Sep 2014 02:16:49 GMT"}, {"version": "v2", "created": "Wed, 3 Jun 2015 00:17:58 GMT"}], "update_date": "2015-06-04", "authors_parsed": [["Soh", "Harold", ""]]}, {"id": "1409.4320", "submitter": "Wing-Kin Ma", "authors": "Xiao Fu, Wing-Kin Ma, Tsung-Han Chan, and Jos\\'e M. Bioucas-Dias", "title": "Self-Dictionary Sparse Regression for Hyperspectral Unmixing: Greedy\n  Pursuit and Pure Pixel Search are Related", "comments": null, "journal-ref": null, "doi": "10.1109/JSTSP.2015.2410763", "report-no": null, "categories": "stat.ML cs.IT math.IT math.OC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper considers a recently emerged hyperspectral unmixing formulation\nbased on sparse regression of a self-dictionary multiple measurement vector\n(SD-MMV) model, wherein the measured hyperspectral pixels are used as the\ndictionary. Operating under the pure pixel assumption, this SD-MMV formalism is\nspecial in that it allows simultaneous identification of the endmember spectral\nsignatures and the number of endmembers. Previous SD-MMV studies mainly focus\non convex relaxations. In this study, we explore the alternative of greedy\npursuit, which generally provides efficient and simple algorithms. In\nparticular, we design a greedy SD-MMV algorithm using simultaneous orthogonal\nmatching pursuit. Intriguingly, the proposed greedy algorithm is shown to be\nclosely related to some existing pure pixel search algorithms, especially, the\nsuccessive projection algorithm (SPA). Thus, a link between SD-MMV and pure\npixel search is revealed. We then perform exact recovery analyses, and prove\nthat the proposed greedy algorithm is robust to noise---including its\nidentification of the (unknown) number of endmembers---under a sufficiently low\nnoise level. The identification performance of the proposed greedy algorithm is\ndemonstrated through both synthetic and real-data experiments.\n", "versions": [{"version": "v1", "created": "Mon, 15 Sep 2014 16:48:31 GMT"}, {"version": "v2", "created": "Tue, 3 Mar 2015 10:24:32 GMT"}], "update_date": "2015-10-28", "authors_parsed": [["Fu", "Xiao", ""], ["Ma", "Wing-Kin", ""], ["Chan", "Tsung-Han", ""], ["Bioucas-Dias", "Jos\u00e9 M.", ""]]}, {"id": "1409.4327", "submitter": "Dinesh Jayaraman", "authors": "Dinesh Jayaraman and Kristen Grauman", "title": "Zero Shot Recognition with Unreliable Attributes", "comments": "NIPS 2014", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In principle, zero-shot learning makes it possible to train a recognition\nmodel simply by specifying the category's attributes. For example, with\nclassifiers for generic attributes like \\emph{striped} and \\emph{four-legged},\none can construct a classifier for the zebra category by enumerating which\nproperties it possesses---even without providing zebra training images. In\npractice, however, the standard zero-shot paradigm suffers because attribute\npredictions in novel images are hard to get right. We propose a novel random\nforest approach to train zero-shot models that explicitly accounts for the\nunreliability of attribute predictions. By leveraging statistics about each\nattribute's error tendencies, our method obtains more robust discriminative\nmodels for the unseen classes. We further devise extensions to handle the\nfew-shot scenario and unreliable attribute descriptions. On three datasets, we\ndemonstrate the benefit for visual category learning with zero or few training\nexamples, a critical domain for rare categories or categories defined on the\nfly.\n", "versions": [{"version": "v1", "created": "Mon, 15 Sep 2014 16:56:07 GMT"}, {"version": "v2", "created": "Tue, 29 Mar 2016 19:33:17 GMT"}], "update_date": "2016-03-30", "authors_parsed": [["Jayaraman", "Dinesh", ""], ["Grauman", "Kristen", ""]]}, {"id": "1409.4366", "submitter": "David Lopez-Paz", "authors": "David Lopez-Paz, Krikamol Muandet, Benjamin Recht", "title": "The Randomized Causation Coefficient", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We are interested in learning causal relationships between pairs of random\nvariables, purely from observational data. To effectively address this task,\nthe state-of-the-art relies on strong assumptions regarding the mechanisms\nmapping causes to effects, such as invertibility or the existence of additive\nnoise, which only hold in limited situations. On the contrary, this short paper\nproposes to learn how to perform causal inference directly from data, and\nwithout the need of feature engineering. In particular, we pose causality as a\nkernel mean embedding classification problem, where inputs are samples from\narbitrary probability distributions on pairs of random variables, and labels\nare types of causal relationships. We validate the performance of our method on\nsynthetic and real-world data against the state-of-the-art. Moreover, we\nsubmitted our algorithm to the ChaLearn's \"Fast Causation Coefficient\nChallenge\" competition, with which we won the fastest code prize and ranked\nthird in the overall leaderboard.\n", "versions": [{"version": "v1", "created": "Mon, 15 Sep 2014 18:23:47 GMT"}], "update_date": "2014-09-16", "authors_parsed": [["Lopez-Paz", "David", ""], ["Muandet", "Krikamol", ""], ["Recht", "Benjamin", ""]]}, {"id": "1409.4566", "submitter": "Ethem Alpaydin", "authors": "Olcay Taner Yildiz, Ethem Alpaydin", "title": "Multivariate Comparison of Classification Algorithms", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Statistical tests that compare classification algorithms are univariate and\nuse a single performance measure, e.g., misclassification error, $F$ measure,\nAUC, and so on. In multivariate tests, comparison is done using multiple\nmeasures simultaneously. For example, error is the sum of false positives and\nfalse negatives and a univariate test on error cannot make a distinction\nbetween these two sources, but a 2-variate test can. Similarly, instead of\ncombining precision and recall in $F$ measure, we can have a 2-variate test on\n(precision, recall). We use Hotelling's multivariate $T^2$ test for comparing\ntwo algorithms, and when we have three or more algorithms we use the\nmultivariate analysis of variance (MANOVA) followed by pairwise post hoc tests.\nIn our experiments, we see that multivariate tests have higher power than\nunivariate tests, that is, they can detect differences that univariate tests\ncannot. We also discuss how multivariate analysis allows us to automatically\nextract performance measures that best distinguish the behavior of multiple\nalgorithms.\n", "versions": [{"version": "v1", "created": "Tue, 16 Sep 2014 10:18:12 GMT"}], "update_date": "2014-09-17", "authors_parsed": [["Yildiz", "Olcay Taner", ""], ["Alpaydin", "Ethem", ""]]}, {"id": "1409.4573", "submitter": "Daniel Hern\\'andez-Lobato", "authors": "Daniel Hern\\'andez-Lobato and Pablo Morales-Mombiela and David\n  Lopez-Paz and Alberto Su\\'arez", "title": "Non-linear Causal Inference using Gaussianity Measures", "comments": "35 pages, 9 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We provide theoretical and empirical evidence for a type of asymmetry between\ncauses and effects that is present when these are related via linear models\ncontaminated with additive non-Gaussian noise. Assuming that the causes and the\neffects have the same distribution, we show that the distribution of the\nresiduals of a linear fit in the anti-causal direction is closer to a Gaussian\nthan the distribution of the residuals in the causal direction. This\nGaussianization effect is characterized by reduction of the magnitude of the\nhigh-order cumulants and by an increment of the differential entropy of the\nresiduals. The problem of non-linear causal inference is addressed by\nperforming an embedding in an expanded feature space, in which the relation\nbetween causes and effects can be assumed to be linear. The effectiveness of a\nmethod to discriminate between causes and effects based on this type of\nasymmetry is illustrated in a variety of experiments using different measures\nof Gaussianity. The proposed method is shown to be competitive with\nstate-of-the-art techniques for causal inference.\n", "versions": [{"version": "v1", "created": "Tue, 16 Sep 2014 10:45:25 GMT"}, {"version": "v2", "created": "Wed, 17 Jun 2015 14:19:41 GMT"}, {"version": "v3", "created": "Sun, 21 Feb 2016 16:48:18 GMT"}], "update_date": "2016-02-23", "authors_parsed": [["Hern\u00e1ndez-Lobato", "Daniel", ""], ["Morales-Mombiela", "Pablo", ""], ["Lopez-Paz", "David", ""], ["Su\u00e1rez", "Alberto", ""]]}, {"id": "1409.4617", "submitter": "Ronald Hochreiter", "authors": "Ronald Hochreiter and Christoph Waldhauser", "title": "The Role of Emotions in Propagating Brands in Social Networks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.SI cs.CL stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A key aspect of word of mouth marketing are emotions. Emotions in texts help\npropagating messages in conventional advertising. In word of mouth scenarios,\nemotions help to engage consumers and incite to propagate the message further.\nWhile the function of emotions in offline marketing in general and word of\nmouth marketing in particular is rather well understood, online marketing can\nonly offer a limited view on the function of emotions. In this contribution we\nseek to close this gap. We therefore investigate how emotions function in\nsocial media. To do so, we collected more than 30,000 brand marketing messages\nfrom the Google+ social networking site. Using state of the art computational\nlinguistics classifiers, we compute the sentiment of these messages. Starting\nout with Poisson regression-based baseline models, we seek to replicate earlier\nfindings using this large data set. We extend upon earlier research by\ncomputing multi-level mixed effects models that compare the function of\nemotions across different industries. We find that while the well known notion\nof activating emotions propagating messages holds in general for our data as\nwell. But there are significant differences between the observed industries.\n", "versions": [{"version": "v1", "created": "Tue, 16 Sep 2014 13:03:51 GMT"}], "update_date": "2014-09-17", "authors_parsed": [["Hochreiter", "Ronald", ""], ["Waldhauser", "Christoph", ""]]}, {"id": "1409.4747", "submitter": "Fabrice Rossi", "authors": "Tsirizo Rabenoro (SAMM), J\\'er\\^ome Lacaille, Marie Cottrell (SAMM),\n  Fabrice Rossi (SAMM)", "title": "Anomaly Detection Based on Indicators Aggregation", "comments": "International Joint Conference on Neural Networks (IJCNN 2014),\n  Beijing : China (2014). arXiv admin note: substantial text overlap with\n  arXiv:1407.0880", "journal-ref": null, "doi": "10.1109/IJCNN.2014.6889841", "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Automatic anomaly detection is a major issue in various areas. Beyond mere\ndetection, the identification of the source of the problem that produced the\nanomaly is also essential. This is particularly the case in aircraft engine\nhealth monitoring where detecting early signs of failure (anomalies) and\nhelping the engine owner to implement efficiently the adapted maintenance\noperations (fixing the source of the anomaly) are of crucial importance to\nreduce the costs attached to unscheduled maintenance. This paper introduces a\ngeneral methodology that aims at classifying monitoring signals into normal\nones and several classes of abnormal ones. The main idea is to leverage expert\nknowledge by generating a very large number of binary indicators. Each\nindicator corresponds to a fully parametrized anomaly detector built from\nparametric anomaly scores designed by experts. A feature selection method is\nused to keep only the most discriminant indicators which are used at inputs of\na Naive Bayes classifier. This give an interpretable classifier based on\ninterpretable anomaly detectors whose parameters have been optimized indirectly\nby the selection process. The proposed methodology is evaluated on simulated\ndata designed to reproduce some of the anomaly types observed in real world\nengines.\n", "versions": [{"version": "v1", "created": "Tue, 16 Sep 2014 19:44:31 GMT"}], "update_date": "2016-08-10", "authors_parsed": [["Rabenoro", "Tsirizo", "", "SAMM"], ["Lacaille", "J\u00e9r\u00f4me", "", "SAMM"], ["Cottrell", "Marie", "", "SAMM"], ["Rossi", "Fabrice", "", "SAMM"]]}, {"id": "1409.4757", "submitter": "Katsuhiko Ishiguro", "authors": "Katsuhiko Ishiguro, Issei Sato, Naonori Ueda", "title": "Collapsed Variational Bayes Inference of Infinite Relational Model", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The Infinite Relational Model (IRM) is a probabilistic model for relational\ndata clustering that partitions objects into clusters based on observed\nrelationships. This paper presents Averaged CVB (ACVB) solutions for IRM,\nconvergence-guaranteed and practically useful fast Collapsed Variational Bayes\n(CVB) inferences. We first derive ordinary CVB and CVB0 for IRM based on the\nlower bound maximization. CVB solutions yield deterministic iterative\nprocedures for inferring IRM given the truncated number of clusters. Our\nproposal includes CVB0 updates of hyperparameters including the concentration\nparameter of the Dirichlet Process, which has not been studied in the\nliterature. To make the CVB more practically useful, we further study the CVB\ninference in two aspects. First, we study the convergence issues and develop a\nconvergence-guaranteed algorithm for any CVB-based inferences called ACVB,\nwhich enables automatic convergence detection and frees non-expert\npractitioners from difficult and costly manual monitoring of inference\nprocesses. Second, we present a few techniques for speeding up IRM inferences.\nIn particular, we describe the linear time inference of CVB0, allowing the IRM\nfor larger relational data uses. The ACVB solutions of IRM showed comparable or\nbetter performance compared to existing inference methods in experiments, and\nprovide deterministic, faster, and easier convergence detection.\n", "versions": [{"version": "v1", "created": "Tue, 16 Sep 2014 08:06:36 GMT"}], "update_date": "2014-09-18", "authors_parsed": [["Ishiguro", "Katsuhiko", ""], ["Sato", "Issei", ""], ["Ueda", "Naonori", ""]]}, {"id": "1409.4835", "submitter": "Michael Bloodgood", "authors": "Michael Bloodgood and K. Vijay-Shanker", "title": "Taking into Account the Differences between Actively and Passively\n  Acquired Data: The Case of Active Learning with Support Vector Machines for\n  Imbalanced Datasets", "comments": "4 pages, 5 figures; appeared in Proceedings of Human Language\n  Technologies: The 2009 Annual Conference of the North American Chapter of the\n  Association for Computational Linguistics, Companion Volume: Short Papers,\n  pages 137-140, Boulder, Colorado, June 2009. Association for Computational\n  Linguistics", "journal-ref": "Proceedings of HLT: The 2009 Annual Conference of the North\n  American Chapter of the Association for Computational Linguistics, Short\n  Papers, pages 137-140, Boulder, Colorado, June 2009. Association for\n  Computational Linguistics", "doi": null, "report-no": null, "categories": "cs.LG cs.CL stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Actively sampled data can have very different characteristics than passively\nsampled data. Therefore, it's promising to investigate using different\ninference procedures during AL than are used during passive learning (PL). This\ngeneral idea is explored in detail for the focused case of AL with\ncost-weighted SVMs for imbalanced data, a situation that arises for many HLT\ntasks. The key idea behind the proposed InitPA method for addressing imbalance\nis to base cost models during AL on an estimate of overall corpus imbalance\ncomputed via a small unbiased sample rather than the imbalance in the labeled\ntraining data, which is the leading method used during PL.\n", "versions": [{"version": "v1", "created": "Wed, 17 Sep 2014 00:00:11 GMT"}], "update_date": "2014-09-18", "authors_parsed": [["Bloodgood", "Michael", ""], ["Vijay-Shanker", "K.", ""]]}, {"id": "1409.4928", "submitter": "Christophe Sch\\\"ulke", "authors": "Ang\\'elique Dr\\'emeau, Christophe Sch\\\"ulke, Yingying Xu, Devavrat\n  Shah", "title": "Statistical inference with probabilistic graphical models", "comments": "Chapter of \"Statistical Physics, Optimization, Inference, and\n  Message-Passing Algorithms\", Eds.: F. Krzakala, F. Ricci-Tersenghi, L.\n  Zdeborova, R. Zecchina, E. W. Tramel, L. F. Cugliandolo (Oxford University\n  Press, to appear)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  These are notes from the lecture of Devavrat Shah given at the autumn school\n\"Statistical Physics, Optimization, Inference, and Message-Passing Algorithms\",\nthat took place in Les Houches, France from Monday September 30th, 2013, till\nFriday October 11th, 2013. The school was organized by Florent Krzakala from\nUPMC & ENS Paris, Federico Ricci-Tersenghi from La Sapienza Roma, Lenka\nZdeborova from CEA Saclay & CNRS, and Riccardo Zecchina from Politecnico\nTorino. This lecture of Devavrat Shah (MIT) covers the basics of inference and\nlearning. It explains how inference problems are represented within structures\nknown as graphical models. The theoretical basis of the belief propagation\nalgorithm is then explained and derived. This lecture sets the stage for\ngeneralizations and applications of message passing algorithms.\n", "versions": [{"version": "v1", "created": "Wed, 17 Sep 2014 09:52:36 GMT"}], "update_date": "2014-09-18", "authors_parsed": [["Dr\u00e9meau", "Ang\u00e9lique", ""], ["Sch\u00fclke", "Christophe", ""], ["Xu", "Yingying", ""], ["Shah", "Devavrat", ""]]}, {"id": "1409.4936", "submitter": "Anthony Bagnall Dr", "authors": "Anthony Bagnall and Reda Younsi", "title": "Ensembles of Random Sphere Cover Classifiers", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose and evaluate alternative ensemble schemes for a new instance based\nlearning classifier, the Randomised Sphere Cover (RSC) classifier. RSC fuses\ninstances into spheres, then bases classification on distance to spheres rather\nthan distance to instances. The randomised nature of RSC makes it ideal for use\nin ensembles. We propose two ensemble methods tailored to the RSC classifier;\n$\\alpha \\beta$RSE, an ensemble based on instance resampling and $\\alpha$RSSE, a\nsubspace ensemble. We compare $\\alpha \\beta$RSE and $\\alpha$RSSE to tree based\nensembles on a set of UCI datasets and demonstrates that RSC ensembles perform\nsignificantly better than some of these ensembles, and not significantly worse\nthan the others. We demonstrate via a case study on six gene expression data\nsets that $\\alpha$RSSE can outperform other subspace ensemble methods on high\ndimensional data when used in conjunction with an attribute filter. Finally, we\nperform a set of Bias/Variance decomposition experiments to analyse the source\nof improvement in comparison to a base classifier.\n", "versions": [{"version": "v1", "created": "Wed, 17 Sep 2014 10:18:34 GMT"}], "update_date": "2014-09-18", "authors_parsed": [["Bagnall", "Anthony", ""], ["Younsi", "Reda", ""]]}, {"id": "1409.5009", "submitter": "Ming Yuan", "authors": "Luwan Zhang, Grace Wahba and Ming Yuan", "title": "Distance Shrinkage and Euclidean Embedding via Regularized Kernel\n  Estimation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML math.ST stat.ME stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Although recovering an Euclidean distance matrix from noisy observations is a\ncommon problem in practice, how well this could be done remains largely\nunknown. To fill in this void, we study a simple distance matrix estimate based\nupon the so-called regularized kernel estimate. We show that such an estimate\ncan be characterized as simply applying a constant amount of shrinkage to all\nobserved pairwise distances. This fact allows us to establish risk bounds for\nthe estimate implying that the true distances can be estimated consistently in\nan average sense as the number of objects increases. In addition, such a\ncharacterization suggests an efficient algorithm to compute the distance matrix\nestimator, as an alternative to the usual second order cone programming known\nnot to scale well for large problems. Numerical experiments and an application\nin visualizing the diversity of Vpu protein sequences from a recent HIV-1 study\nfurther demonstrate the practical merits of the proposed method.\n", "versions": [{"version": "v1", "created": "Wed, 17 Sep 2014 14:42:28 GMT"}], "update_date": "2014-09-18", "authors_parsed": [["Zhang", "Luwan", ""], ["Wahba", "Grace", ""], ["Yuan", "Ming", ""]]}, {"id": "1409.5103", "submitter": "Harry van Zanten", "authors": "Alisa Kirichenko and Harry van Zanten", "title": "Optimality of Poisson processes intensity learning with Gaussian\n  processes", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.ML stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper we provide theoretical support for the so-called \"Sigmoidal\nGaussian Cox Process\" approach to learning the intensity of an inhomogeneous\nPoisson process on a $d$-dimensional domain. This method was proposed by Adams,\nMurray and MacKay (ICML, 2009), who developed a tractable computational\napproach and showed in simulation and real data experiments that it can work\nquite satisfactorily. The results presented in the present paper provide\ntheoretical underpinning of the method. In particular, we show how to tune the\npriors on the hyper parameters of the model in order for the procedure to\nautomatically adapt to the degree of smoothness of the unknown intensity and to\nachieve optimal convergence rates.\n", "versions": [{"version": "v1", "created": "Wed, 17 Sep 2014 19:06:50 GMT"}, {"version": "v2", "created": "Mon, 2 Mar 2015 12:02:17 GMT"}], "update_date": "2015-03-03", "authors_parsed": [["Kirichenko", "Alisa", ""], ["van Zanten", "Harry", ""]]}, {"id": "1409.5165", "submitter": "Michael Bloodgood", "authors": "Michael Bloodgood and K. Vijay-Shanker", "title": "A Method for Stopping Active Learning Based on Stabilizing Predictions\n  and the Need for User-Adjustable Stopping", "comments": "9 pages, 3 figures, 5 tables; appeared in Proceedings of the\n  Thirteenth Conference on Computational Natural Language Learning\n  (CoNLL-2009), June 2009", "journal-ref": "In Proceedings of the Thirteenth Conference on Computational\n  Natural Language Learning (CoNLL-2009), pages 39-47, Boulder, Colorado, June\n  2009. Association for Computational Linguistics", "doi": null, "report-no": null, "categories": "cs.LG cs.CL stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A survey of existing methods for stopping active learning (AL) reveals the\nneeds for methods that are: more widely applicable; more aggressive in saving\nannotations; and more stable across changing datasets. A new method for\nstopping AL based on stabilizing predictions is presented that addresses these\nneeds. Furthermore, stopping methods are required to handle a broad range of\ndifferent annotation/performance tradeoff valuations. Despite this, the\nexisting body of work is dominated by conservative methods with little (if any)\nattention paid to providing users with control over the behavior of stopping\nmethods. The proposed method is shown to fill a gap in the level of\naggressiveness available for stopping AL and supports providing users with\ncontrol over stopping behavior.\n", "versions": [{"version": "v1", "created": "Wed, 17 Sep 2014 23:28:59 GMT"}], "update_date": "2014-09-19", "authors_parsed": [["Bloodgood", "Michael", ""], ["Vijay-Shanker", "K.", ""]]}, {"id": "1409.5178", "submitter": "Yu Nishiyama", "authors": "Yu Nishiyama and Motonobu Kanagawa and Arthur Gretton and Kenji\n  Fukumizu", "title": "Model-based Kernel Sum Rule: Kernel Bayesian Inference with\n  Probabilistic Models", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Kernel Bayesian inference is a principled approach to nonparametric inference\nin probabilistic graphical models, where probabilistic relationships between\nvariables are learned from data in a nonparametric manner. Various algorithms\nof kernel Bayesian inference have been developed by combining kernelized basic\nprobabilistic operations such as the kernel sum rule and kernel Bayes' rule.\nHowever, the current framework is fully nonparametric, and it does not allow a\nuser to flexibly combine nonparametric and model-based inferences. This is\ninefficient when there are good probabilistic models (or simulation models)\navailable for some parts of a graphical model; this is in particular true in\nscientific fields where \"models\" are the central topic of study. Our\ncontribution in this paper is to introduce a novel approach, termed the {\\em\nmodel-based kernel sum rule} (Mb-KSR), to combine a probabilistic model and\nkernel Bayesian inference. By combining the Mb-KSR with the existing kernelized\nprobabilistic rules, one can develop various algorithms for hybrid (i.e.,\nnonparametric and model-based) inferences. As an illustrative example, we\nconsider Bayesian filtering in a state space model, where typically there\nexists an accurate probabilistic model for the state transition process. We\npropose a novel filtering method that combines model-based inference for the\nstate transition process and data-driven, nonparametric inference for the\nobservation generating process. We empirically validate our approach with\nsynthetic and real-data experiments, the latter being the problem of\nvision-based mobile robot localization in robotics, which illustrates the\neffectiveness of the proposed hybrid approach.\n", "versions": [{"version": "v1", "created": "Thu, 18 Sep 2014 02:14:00 GMT"}, {"version": "v2", "created": "Thu, 5 Apr 2018 09:30:22 GMT"}, {"version": "v3", "created": "Wed, 17 Apr 2019 07:01:05 GMT"}], "update_date": "2019-04-18", "authors_parsed": [["Nishiyama", "Yu", ""], ["Kanagawa", "Motonobu", ""], ["Gretton", "Arthur", ""], ["Fukumizu", "Kenji", ""]]}, {"id": "1409.5185", "submitter": "Zhuowen Tu", "authors": "Chen-Yu Lee, Saining Xie, Patrick Gallagher, Zhengyou Zhang, Zhuowen\n  Tu", "title": "Deeply-Supervised Nets", "comments": "Patent disclosure, UCSD Docket No. SD2014-313, filed on May 22, 2014", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.CV cs.LG cs.NE", "license": "http://creativecommons.org/licenses/by-nc-sa/3.0/", "abstract": "  Our proposed deeply-supervised nets (DSN) method simultaneously minimizes\nclassification error while making the learning process of hidden layers direct\nand transparent. We make an attempt to boost the classification performance by\nstudying a new formulation in deep networks. Three aspects in convolutional\nneural networks (CNN) style architectures are being looked at: (1) transparency\nof the intermediate layers to the overall classification; (2)\ndiscriminativeness and robustness of learned features, especially in the early\nlayers; (3) effectiveness in training due to the presence of the exploding and\nvanishing gradients. We introduce \"companion objective\" to the individual\nhidden layers, in addition to the overall objective at the output layer (a\ndifferent strategy to layer-wise pre-training). We extend techniques from\nstochastic gradient methods to analyze our algorithm. The advantage of our\nmethod is evident and our experimental result on benchmark datasets shows\nsignificant performance gain over existing methods (e.g. all state-of-the-art\nresults on MNIST, CIFAR-10, CIFAR-100, and SVHN).\n", "versions": [{"version": "v1", "created": "Thu, 18 Sep 2014 04:08:25 GMT"}, {"version": "v2", "created": "Thu, 25 Sep 2014 05:03:06 GMT"}], "update_date": "2017-04-26", "authors_parsed": [["Lee", "Chen-Yu", ""], ["Xie", "Saining", ""], ["Gallagher", "Patrick", ""], ["Zhang", "Zhengyou", ""], ["Tu", "Zhuowen", ""]]}, {"id": "1409.5191", "submitter": "Jascha Sohl-Dickstein", "authors": "Jascha Sohl-Dickstein, Mayur Mudigonda, Michael R. DeWeese", "title": "Hamiltonian Monte Carlo Without Detailed Balance", "comments": "Accepted conference submission to ICML 2014 and also featured in a\n  special edition of JMLR. Since updated to include additional literature\n  citations", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.CO stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a method for performing Hamiltonian Monte Carlo that largely\neliminates sample rejection for typical hyperparameters. In situations that\nwould normally lead to rejection, instead a longer trajectory is computed until\na new state is reached that can be accepted. This is achieved using Markov\nchain transitions that satisfy the fixed point equation, but do not satisfy\ndetailed balance. The resulting algorithm significantly suppresses the random\nwalk behavior and wasted function evaluations that are typically the\nconsequence of update rejection. We demonstrate a greater than factor of two\nimprovement in mixing time on three test problems. We release the source code\nas Python and MATLAB packages.\n", "versions": [{"version": "v1", "created": "Thu, 18 Sep 2014 05:01:07 GMT"}, {"version": "v2", "created": "Wed, 3 Dec 2014 07:36:42 GMT"}, {"version": "v3", "created": "Wed, 17 Jun 2015 04:31:19 GMT"}, {"version": "v4", "created": "Wed, 24 Jun 2015 06:43:37 GMT"}, {"version": "v5", "created": "Fri, 25 Mar 2016 21:42:22 GMT"}], "update_date": "2016-03-29", "authors_parsed": [["Sohl-Dickstein", "Jascha", ""], ["Mudigonda", "Mayur", ""], ["DeWeese", "Michael R.", ""]]}, {"id": "1409.5391", "submitter": "Ashley Petersen", "authors": "Ashley Petersen, Daniela Witten, and Noah Simon", "title": "Fused Lasso Additive Model", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider the problem of predicting an outcome variable using $p$\ncovariates that are measured on $n$ independent observations, in the setting in\nwhich flexible and interpretable fits are desirable. We propose the fused lasso\nadditive model (FLAM), in which each additive function is estimated to be\npiecewise constant with a small number of adaptively-chosen knots. FLAM is the\nsolution to a convex optimization problem, for which a simple algorithm with\nguaranteed convergence to the global optimum is provided. FLAM is shown to be\nconsistent in high dimensions, and an unbiased estimator of its degrees of\nfreedom is proposed. We evaluate the performance of FLAM in a simulation study\nand on two data sets.\n", "versions": [{"version": "v1", "created": "Thu, 18 Sep 2014 18:02:17 GMT"}], "update_date": "2014-09-19", "authors_parsed": [["Petersen", "Ashley", ""], ["Witten", "Daniela", ""], ["Simon", "Noah", ""]]}, {"id": "1409.5402", "submitter": "John Canny", "authors": "Huasha Zhao and Biye Jiang and John Canny", "title": "SAME but Different: Fast and High-Quality Gibbs Parameter Estimation", "comments": "10 pages, 5 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Gibbs sampling is a workhorse for Bayesian inference but has several\nlimitations when used for parameter estimation, and is often much slower than\nnon-sampling inference methods. SAME (State Augmentation for Marginal\nEstimation) \\cite{Doucet99,Doucet02} is an approach to MAP parameter estimation\nwhich gives improved parameter estimates over direct Gibbs sampling. SAME can\nbe viewed as cooling the posterior parameter distribution and allows annealed\nsearch for the MAP parameters, often yielding very high quality (lower loss)\nestimates. But it does so at the expense of additional samples per iteration\nand generally slower performance. On the other hand, SAME dramatically\nincreases the parallelism in the sampling schedule, and is an excellent match\nfor modern (SIMD) hardware. In this paper we explore the application of SAME to\ngraphical model inference on modern hardware. We show that combining SAME with\nfactored sample representation (or approximation) gives throughput competitive\nwith the fastest symbolic methods, but with potentially better quality. We\ndescribe experiments on Latent Dirichlet Allocation, achieving speeds similar\nto the fastest reported methods (online Variational Bayes) and lower\ncross-validated loss than other LDA implementations. The method is simple to\nimplement and should be applicable to many other models.\n", "versions": [{"version": "v1", "created": "Thu, 18 Sep 2014 18:31:50 GMT"}], "update_date": "2014-09-19", "authors_parsed": [["Zhao", "Huasha", ""], ["Jiang", "Biye", ""], ["Canny", "John", ""]]}, {"id": "1409.5557", "submitter": "Eric Tramel", "authors": "Eric W. Tramel and Santhosh Kumar and Andrei Giurgiu and Andrea\n  Montanari", "title": "Statistical Estimation: From Denoising to Sparse Regression and Hidden\n  Cliques", "comments": "Chapter of \"Statistical Physics, Optimization, Inference, and\n  Message-Passing Algorithms\", Eds.: F. Krzakala, F. Ricci-Tersenghi, L.\n  Zdeborova, R. Zecchina, E. W. Tramel, L. F. Cugliandolo (Oxford University\n  Press, to appear)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IT math.IT stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  These notes review six lectures given by Prof. Andrea Montanari on the topic\nof statistical estimation for linear models. The first two lectures cover the\nprinciples of signal recovery from linear measurements in terms of minimax\nrisk. Subsequent lectures demonstrate the application of these principles to\nseveral practical problems in science and engineering. Specifically, these\ntopics include denoising of error-laden signals, recovery of compressively\nsensed signals, reconstruction of low-rank matrices, and also the discovery of\nhidden cliques within large networks.\n", "versions": [{"version": "v1", "created": "Fri, 19 Sep 2014 09:00:20 GMT"}], "update_date": "2014-09-22", "authors_parsed": [["Tramel", "Eric W.", ""], ["Kumar", "Santhosh", ""], ["Giurgiu", "Andrei", ""], ["Montanari", "Andrea", ""]]}, {"id": "1409.5743", "submitter": "Matteo Rucco", "authors": "Matteo Rucco, David M. S. Rodrigues, Emanuela Merelli, Jeffrey H.\n  Johnson, Lorenzo Falsetti, Cinzia Nitti and Aldo Salvi", "title": "Neural Hypernetwork Approach for Pulmonary Embolism diagnosis", "comments": "16 pages, 6 figures, 5 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "physics.med-ph cs.LG physics.data-an q-bio.QM stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This work introduces an integrative approach based on Q-analysis with machine\nlearning. The new approach, called Neural Hypernetwork, has been applied to a\ncase study of pulmonary embolism diagnosis. The objective of the application of\nneural hyper-network to pulmonary embolism (PE) is to improve diagnose for\nreducing the number of CT-angiography needed. Hypernetworks, based on\ntopological simplicial complex, generalize the concept of two-relation to\nmany-body relation. Furthermore, Hypernetworks provide a significant\ngeneralization of network theory, enabling the integration of relational\nstructure, logic and analytic dynamics. Another important results is that\nQ-analysis stays close to the data, while other approaches manipulate data,\nprojecting them into metric spaces or applying some filtering functions to\nhighlight the intrinsic relations. A pulmonary embolism (PE) is a blockage of\nthe main artery of the lung or one of its branches, frequently fatal. Our study\nuses data on 28 diagnostic features of 1,427 people considered to be at risk of\nPE. The resulting neural hypernetwork correctly recognized 94% of those\ndeveloping a PE. This is better than previous results that have been obtained\nwith other methods (statistical selection of features, partial least squares\nregression, topological data analysis in a metric space).\n", "versions": [{"version": "v1", "created": "Fri, 19 Sep 2014 18:04:11 GMT"}, {"version": "v2", "created": "Mon, 13 Oct 2014 14:22:48 GMT"}], "update_date": "2014-10-14", "authors_parsed": [["Rucco", "Matteo", ""], ["Rodrigues", "David M. S.", ""], ["Merelli", "Emanuela", ""], ["Johnson", "Jeffrey H.", ""], ["Falsetti", "Lorenzo", ""], ["Nitti", "Cinzia", ""], ["Salvi", "Aldo", ""]]}, {"id": "1409.5834", "submitter": "Tim Roughgarden", "authors": "Amir Globerson and Tim Roughgarden and David Sontag and Cafer Yildirim", "title": "Tight Error Bounds for Structured Prediction", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.DS stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Structured prediction tasks in machine learning involve the simultaneous\nprediction of multiple labels. This is typically done by maximizing a score\nfunction on the space of labels, which decomposes as a sum of pairwise\nelements, each depending on two specific labels. Intuitively, the more pairwise\nterms are used, the better the expected accuracy. However, there is currently\nno theoretical account of this intuition. This paper takes a significant step\nin this direction.\n  We formulate the problem as classifying the vertices of a known graph\n$G=(V,E)$, where the vertices and edges of the graph are labelled and correlate\nsemi-randomly with the ground truth. We show that the prospects for achieving\nlow expected Hamming error depend on the structure of the graph $G$ in\ninteresting ways. For example, if $G$ is a very poor expander, like a path,\nthen large expected Hamming error is inevitable. Our main positive result shows\nthat, for a wide class of graphs including 2D grid graphs common in machine\nvision applications, there is a polynomial-time algorithm with small and\ninformation-theoretically near-optimal expected error. Our results provide a\nfirst step toward a theoretical justification for the empirical success of the\nefficient approximate inference algorithms that are used for structured\nprediction in models where exact inference is intractable.\n", "versions": [{"version": "v1", "created": "Fri, 19 Sep 2014 23:51:09 GMT"}], "update_date": "2014-09-23", "authors_parsed": [["Globerson", "Amir", ""], ["Roughgarden", "Tim", ""], ["Sontag", "David", ""], ["Yildirim", "Cafer", ""]]}, {"id": "1409.5937", "submitter": "Jiashi Feng", "authors": "Jiashi Feng, Huan Xu, Shie Mannor", "title": "Distributed Robust Learning", "comments": "18 pages, 2 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a framework for distributed robust statistical learning on {\\em\nbig contaminated data}. The Distributed Robust Learning (DRL) framework can\nreduce the computational time of traditional robust learning methods by several\norders of magnitude. We analyze the robustness property of DRL, showing that\nDRL not only preserves the robustness of the base robust learning method, but\nalso tolerates contaminations on a constant fraction of results from computing\nnodes (node failures). More precisely, even in presence of the most adversarial\noutlier distribution over computing nodes, DRL still achieves a breakdown point\nof at least $ \\lambda^*/2 $, where $ \\lambda^* $ is the break down point of\ncorresponding centralized algorithm. This is in stark contrast with naive\ndivision-and-averaging implementation, which may reduce the breakdown point by\na factor of $ k $ when $ k $ computing nodes are used. We then specialize the\nDRL framework for two concrete cases: distributed robust principal component\nanalysis and distributed robust regression. We demonstrate the efficiency and\nthe robustness advantages of DRL through comprehensive simulations and\npredicting image tags on a large-scale image set.\n", "versions": [{"version": "v1", "created": "Sun, 21 Sep 2014 03:53:20 GMT"}, {"version": "v2", "created": "Sat, 7 Feb 2015 15:55:06 GMT"}], "update_date": "2015-02-10", "authors_parsed": [["Feng", "Jiashi", ""], ["Xu", "Huan", ""], ["Mannor", "Shie", ""]]}, {"id": "1409.6041", "submitter": "Muhammad Ghifary", "authors": "Muhammad Ghifary and W. Bastiaan Kleijn and Mengjie Zhang", "title": "Domain Adaptive Neural Networks for Object Recognition", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI cs.LG cs.NE stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a simple neural network model to deal with the domain adaptation\nproblem in object recognition. Our model incorporates the Maximum Mean\nDiscrepancy (MMD) measure as a regularization in the supervised learning to\nreduce the distribution mismatch between the source and target domains in the\nlatent space. From experiments, we demonstrate that the MMD regularization is\nan effective tool to provide good domain adaptation models on both SURF\nfeatures and raw image pixels of a particular image data set. We also show that\nour proposed model, preceded by the denoising auto-encoder pretraining,\nachieves better performance than recent benchmark models on the same data sets.\nThis work represents the first study of MMD measure in the context of neural\nnetworks.\n", "versions": [{"version": "v1", "created": "Sun, 21 Sep 2014 20:42:00 GMT"}], "update_date": "2016-07-28", "authors_parsed": [["Ghifary", "Muhammad", ""], ["Kleijn", "W. Bastiaan", ""], ["Zhang", "Mengjie", ""]]}, {"id": "1409.6045", "submitter": "Paul Honeine", "authors": "Paul Honeine", "title": "Analyzing sparse dictionaries for online learning with kernels", "comments": "10 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.CV cs.IT cs.LG math.IT", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Many signal processing and machine learning methods share essentially the\nsame linear-in-the-parameter model, with as many parameters as available\nsamples as in kernel-based machines. Sparse approximation is essential in many\ndisciplines, with new challenges emerging in online learning with kernels. To\nthis end, several sparsity measures have been proposed in the literature to\nquantify sparse dictionaries and constructing relevant ones, the most prolific\nones being the distance, the approximation, the coherence and the Babel\nmeasures. In this paper, we analyze sparse dictionaries based on these\nmeasures. By conducting an eigenvalue analysis, we show that these sparsity\nmeasures share many properties, including the linear independence condition and\ninducing a well-posed optimization problem. Furthermore, we prove that there\nexists a quasi-isometry between the parameter (i.e., dual) space and the\ndictionary's induced feature space.\n", "versions": [{"version": "v1", "created": "Sun, 21 Sep 2014 21:46:19 GMT"}], "update_date": "2014-09-23", "authors_parsed": [["Honeine", "Paul", ""]]}, {"id": "1409.6046", "submitter": "Paul Honeine", "authors": "Paul Honeine", "title": "Approximation errors of online sparsification criteria", "comments": "10 pages", "journal-ref": null, "doi": "10.1109/TSP.2015.2442960", "report-no": null, "categories": "stat.ML cs.CV cs.IT cs.LG cs.NE math.IT", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Many machine learning frameworks, such as resource-allocating networks,\nkernel-based methods, Gaussian processes, and radial-basis-function networks,\nrequire a sparsification scheme in order to address the online learning\nparadigm. For this purpose, several online sparsification criteria have been\nproposed to restrict the model definition on a subset of samples. The most\nknown criterion is the (linear) approximation criterion, which discards any\nsample that can be well represented by the already contributing samples, an\noperation with excessive computational complexity. Several computationally\nefficient sparsification criteria have been introduced in the literature, such\nas the distance, the coherence and the Babel criteria. In this paper, we\nprovide a framework that connects these sparsification criteria to the issue of\napproximating samples, by deriving theoretical bounds on the approximation\nerrors. Moreover, we investigate the error of approximating any feature, by\nproposing upper-bounds on the approximation error for each of the\naforementioned sparsification criteria. Two classes of features are described\nin detail, the empirical mean and the principal axes in the kernel principal\ncomponent analysis.\n", "versions": [{"version": "v1", "created": "Sun, 21 Sep 2014 21:53:08 GMT"}], "update_date": "2015-10-28", "authors_parsed": [["Honeine", "Paul", ""]]}, {"id": "1409.6086", "submitter": "Yu-Xiang Wang", "authors": "Yu-Xiang Wang and Veeranjaneyulu Sadhanala and Wei Dai and Willie\n  Neiswanger and Suvrit Sra and Eric P. Xing", "title": "Parallel and Distributed Block-Coordinate Frank-Wolfe Algorithms", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML math.OC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We develop parallel and distributed Frank-Wolfe algorithms; the former on\nshared memory machines with mini-batching, and the latter in a delayed update\nframework. Whenever possible, we perform computations asynchronously, which\nhelps attain speedups on multicore machines as well as in distributed\nenvironments. Moreover, instead of worst-case bounded delays, our methods only\ndepend (mildly) on \\emph{expected} delays, allowing them to be robust to\nstragglers and faulty worker threads. Our algorithms assume block-separable\nconstraints, and subsume the recent Block-Coordinate Frank-Wolfe (BCFW)\nmethod~\\citep{lacoste2013block}. Our analysis reveals problem-dependent\nquantities that govern the speedups of our methods over BCFW. We present\nexperiments on structural SVM and Group Fused Lasso, obtaining significant\nspeedups over competing state-of-the-art (and synchronous) methods.\n", "versions": [{"version": "v1", "created": "Mon, 22 Sep 2014 06:49:22 GMT"}, {"version": "v2", "created": "Sat, 13 Feb 2016 04:44:24 GMT"}], "update_date": "2016-02-16", "authors_parsed": [["Wang", "Yu-Xiang", ""], ["Sadhanala", "Veeranjaneyulu", ""], ["Dai", "Wei", ""], ["Neiswanger", "Willie", ""], ["Sra", "Suvrit", ""], ["Xing", "Eric P.", ""]]}, {"id": "1409.6111", "submitter": "Xiaochuan Zhao", "authors": "Xiaochuan Zhao and Ali H. Sayed", "title": "Distributed Clustering and Learning Over Networks", "comments": "47 pages, 6 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.OC cs.LG cs.MA cs.SY stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Distributed processing over networks relies on in-network processing and\ncooperation among neighboring agents. Cooperation is beneficial when agents\nshare a common objective. However, in many applications agents may belong to\ndifferent clusters that pursue different objectives. Then, indiscriminate\ncooperation will lead to undesired results. In this work, we propose an\nadaptive clustering and learning scheme that allows agents to learn which\nneighbors they should cooperate with and which other neighbors they should\nignore. In doing so, the resulting algorithm enables the agents to identify\ntheir clusters and to attain improved learning and estimation accuracy over\nnetworks. We carry out a detailed mean-square analysis and assess the error\nprobabilities of Types I and II, i.e., false alarm and mis-detection, for the\nclustering mechanism. Among other results, we establish that these\nprobabilities decay exponentially with the step-sizes so that the probability\nof correct clustering can be made arbitrarily close to one.\n", "versions": [{"version": "v1", "created": "Mon, 22 Sep 2014 08:47:11 GMT"}], "update_date": "2014-09-23", "authors_parsed": [["Zhao", "Xiaochuan", ""], ["Sayed", "Ali H.", ""]]}, {"id": "1409.6179", "submitter": "Andre Manoel", "authors": "Jack Raymond, Andre Manoel, Manfred Opper", "title": "Expectation Propagation", "comments": "Chapter of \"Statistical Physics, Optimization, Inference, and\n  Message-Passing Algorithms\", Eds.: F. Krzakala, F. Ricci-Tersenghi, L.\n  Zdeborova, R. Zecchina, E. W. Tramel, L. F. Cugliandolo (Oxford University\n  Press, to appear)", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Variational inference is a powerful concept that underlies many iterative\napproximation algorithms; expectation propagation, mean-field methods and\nbelief propagations were all central themes at the school that can be perceived\nfrom this unifying framework. The lectures of Manfred Opper introduce the\narchetypal example of Expectation Propagation, before establishing the\nconnection with the other approximation methods. Corrections by expansion about\nthe expectation propagation are then explained. Finally some advanced inference\ntopics and applications are explored in the final sections.\n", "versions": [{"version": "v1", "created": "Mon, 22 Sep 2014 14:17:15 GMT"}], "update_date": "2014-09-23", "authors_parsed": [["Raymond", "Jack", ""], ["Manoel", "Andre", ""], ["Opper", "Manfred", ""]]}, {"id": "1409.6805", "submitter": "Siting Ren", "authors": "Siting Ren, Sheng Gao", "title": "Improving Cross-domain Recommendation through Probabilistic\n  Cluster-level Latent Factor Model--Extended Version", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Cross-domain recommendation has been proposed to transfer user behavior\npattern by pooling together the rating data from multiple domains to alleviate\nthe sparsity problem appearing in single rating domains. However, previous\nmodels only assume that multiple domains share a latent common rating pattern\nbased on the user-item co-clustering. To capture diversities among different\ndomains, we propose a novel Probabilistic Cluster-level Latent Factor (PCLF)\nmodel to improve the cross-domain recommendation performance. Experiments on\nseveral real world datasets demonstrate that our proposed model outperforms the\nstate-of-the-art methods for the cross-domain recommendation task.\n", "versions": [{"version": "v1", "created": "Wed, 24 Sep 2014 02:55:31 GMT"}], "update_date": "2014-09-26", "authors_parsed": [["Ren", "Siting", ""], ["Gao", "Sheng", ""]]}, {"id": "1409.6833", "submitter": "John Lafferty", "authors": "Yuancheng Zhu and John Lafferty", "title": "Quantized Estimation of Gaussian Sequence Models in Euclidean Balls", "comments": "Appearing at NIPS 2014", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.ML stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A central result in statistical theory is Pinsker's theorem, which\ncharacterizes the minimax rate in the normal means model of nonparametric\nestimation. In this paper, we present an extension to Pinsker's theorem where\nestimation is carried out under storage or communication constraints. In\nparticular, we place limits on the number of bits used to encode an estimator,\nand analyze the excess risk in terms of this constraint, the signal size, and\nthe noise level. We give sharp upper and lower bounds for the case of a\nEuclidean ball, which establishes the Pareto-optimal minimax tradeoff between\nstorage and risk in this setting.\n", "versions": [{"version": "v1", "created": "Wed, 24 Sep 2014 05:35:12 GMT"}], "update_date": "2014-09-25", "authors_parsed": [["Zhu", "Yuancheng", ""], ["Lafferty", "John", ""]]}, {"id": "1409.6981", "submitter": "Faicel Chamroukhi", "authors": "Faicel Chamroukhi", "title": "Unsupervised learning of regression mixture models with unknown number\n  of components", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Regression mixture models are widely studied in statistics, machine learning\nand data analysis. Fitting regression mixtures is challenging and is usually\nperformed by maximum likelihood by using the expectation-maximization (EM)\nalgorithm. However, it is well-known that the initialization is crucial for EM.\nIf the initialization is inappropriately performed, the EM algorithm may lead\nto unsatisfactory results. The EM algorithm also requires the number of\nclusters to be given a priori; the problem of selecting the number of mixture\ncomponents requires using model selection criteria to choose one from a set of\npre-estimated candidate models. We propose a new fully unsupervised algorithm\nto learn regression mixture models with unknown number of components. The\ndeveloped unsupervised learning approach consists in a penalized maximum\nlikelihood estimation carried out by a robust expectation-maximization (EM)\nalgorithm for fitting polynomial, spline and B-spline regressions mixtures. The\nproposed learning approach is fully unsupervised: 1) it simultaneously infers\nthe model parameters and the optimal number of the regression mixture\ncomponents from the data as the learning proceeds, rather than in a two-fold\nscheme as in standard model-based clustering using afterward model selection\ncriteria, and 2) it does not require accurate initialization unlike the\nstandard EM for regression mixtures. The developed approach is applied to curve\nclustering problems. Numerical experiments on simulated data show that the\nproposed robust EM algorithm performs well and provides accurate results in\nterms of robustness with regard initialization and retrieving the optimal\npartition with the actual number of clusters. An application to real data in\nthe framework of functional data clustering, confirms the benefit of the\nproposed approach for practical applications.\n", "versions": [{"version": "v1", "created": "Wed, 24 Sep 2014 14:55:00 GMT"}], "update_date": "2014-09-25", "authors_parsed": [["Chamroukhi", "Faicel", ""]]}, {"id": "1409.7074", "submitter": "Charles Fisher", "authors": "Charles K. Fisher", "title": "Variational Pseudolikelihood for Regularized Ising Inference", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cond-mat.stat-mech cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  I propose a variational approach to maximum pseudolikelihood inference of the\nIsing model. The variational algorithm is more computationally efficient, and\ndoes a better job predicting out-of-sample correlations than $L_2$ regularized\nmaximum pseudolikelihood inference as well as mean field and isolated spin pair\napproximations with pseudocount regularization. The key to the approach is a\nvariational energy that regularizes the inference problem by shrinking the\ncouplings towards zero, while still allowing some large couplings to explain\nstrong correlations. The utility of the variational pseudolikelihood approach\nis illustrated by training an Ising model to represent the letters A-J using\nsamples of letters from different computer fonts.\n", "versions": [{"version": "v1", "created": "Wed, 24 Sep 2014 20:01:15 GMT"}], "update_date": "2014-09-26", "authors_parsed": [["Fisher", "Charles K.", ""]]}, {"id": "1409.7085", "submitter": "Michael Bloodgood", "authors": "Kathryn Baker, Michael Bloodgood, Chris Callison-Burch, Bonnie J.\n  Dorr, Nathaniel W. Filardo, Lori Levin, Scott Miller and Christine Piatko", "title": "Semantically-Informed Syntactic Machine Translation: A Tree-Grafting\n  Approach", "comments": "10 pages, 7 figures, 3 tables; appeared in Proceedings of the Ninth\n  Conference of the Association for Machine Translation in the Americas (AMTA),\n  October 2010", "journal-ref": "In Proceedings of the Ninth Conference of the Association for\n  Machine Translation in the Americas (AMTA), Denver, Colorado, October 2010", "doi": null, "report-no": null, "categories": "cs.CL cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We describe a unified and coherent syntactic framework for supporting a\nsemantically-informed syntactic approach to statistical machine translation.\nSemantically enriched syntactic tags assigned to the target-language training\ntexts improved translation quality. The resulting system significantly\noutperformed a linguistically naive baseline model (Hiero), and reached the\nhighest scores yet reported on the NIST 2009 Urdu-English translation task.\nThis finding supports the hypothesis (posed by many researchers in the MT\ncommunity, e.g., in DARPA GALE) that both syntactic and semantic information\nare critical for improving translation quality---and further demonstrates that\nlarge gains can be achieved for low-resource languages with different word\norder than English.\n", "versions": [{"version": "v1", "created": "Wed, 24 Sep 2014 20:16:49 GMT"}], "update_date": "2014-09-26", "authors_parsed": [["Baker", "Kathryn", ""], ["Bloodgood", "Michael", ""], ["Callison-Burch", "Chris", ""], ["Dorr", "Bonnie J.", ""], ["Filardo", "Nathaniel W.", ""], ["Levin", "Lori", ""], ["Miller", "Scott", ""], ["Piatko", "Christine", ""]]}, {"id": "1409.7134", "submitter": "Charles Zheng", "authors": "Charles Zheng, Franco Pestilli, Ariel Rokem", "title": "Deconvolution of High-Dimensional Mixtures via Boosting, with\n  Application to Diffusion-Weighted MRI of Human Brain", "comments": "NIPS 2014, accepted", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Diffusion-weighted magnetic resonance imaging (DWI) and fiber tractography\nare the only methods to measure the structure of the white matter in the living\nhuman brain. The diffusion signal has been modelled as the combined\ncontribution from many individual fascicles of nerve fibers passing through\neach location in the white matter. Typically, this is done via basis pursuit,\nbut estimation of the exact directions is limited due to discretization. The\ndifficulties inherent in modeling DWI data are shared by many other problems\ninvolving fitting non-parametric mixture models. Ekanadaham et al. proposed an\napproach, continuous basis pursuit, to overcome discretization error in the\n1-dimensional case (e.g., spike-sorting). Here, we propose a more general\nalgorithm that fits mixture models of any dimensionality without\ndiscretization. Our algorithm uses the principles of L2-boost, together with\nrefitting of the weights and pruning of the parameters. The addition of these\nsteps to L2-boost both accelerates the algorithm and assures its accuracy. We\nrefer to the resulting algorithm as elastic basis pursuit, or EBP, since it\nexpands and contracts the active set of kernels as needed. We show that in\ncontrast to existing approaches to fitting mixtures, our boosting framework (1)\nenables the selection of the optimal bias-variance tradeoff along the solution\npath, and (2) scales with high-dimensional problems. In simulations of DWI, we\nfind that EBP yields better parameter estimates than a non-negative least\nsquares (NNLS) approach, or the standard model used in DWI, the tensor model,\nwhich serves as the basis for diffusion tensor imaging (DTI). We demonstrate\nthe utility of the method in DWI data acquired in parts of the brain containing\ncrossings of multiple fascicles of nerve fibers.\n", "versions": [{"version": "v1", "created": "Thu, 25 Sep 2014 01:05:06 GMT"}, {"version": "v2", "created": "Fri, 26 Sep 2014 16:51:38 GMT"}], "update_date": "2014-09-29", "authors_parsed": [["Zheng", "Charles", ""], ["Pestilli", "Franco", ""], ["Rokem", "Ariel", ""]]}, {"id": "1409.7193", "submitter": "Goran Marjanovic", "authors": "Goran Marjanovic, Magnus O. Ulfarsson, Alfred O. Hero III", "title": "MIST: L0 Sparse Linear Regression with Momentum", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Significant attention has been given to minimizing a penalized least squares\ncriterion for estimating sparse solutions to large linear systems of equations.\nThe penalty is responsible for inducing sparsity and the natural choice is the\nso-called $l_0$ norm. In this paper we develop a Momentumized Iterative\nShrinkage Thresholding (MIST) algorithm for minimizing the resulting non-convex\ncriterion and prove its convergence to a local minimizer. Simulations on large\ndata sets show superior performance of the proposed method to other methods.\n", "versions": [{"version": "v1", "created": "Thu, 25 Sep 2014 09:28:54 GMT"}, {"version": "v2", "created": "Thu, 19 Mar 2015 03:04:20 GMT"}], "update_date": "2015-03-20", "authors_parsed": [["Marjanovic", "Goran", ""], ["Ulfarsson", "Magnus O.", ""], ["Hero", "Alfred O.", "III"]]}, {"id": "1409.7287", "submitter": "Andreas Svensson", "authors": "Andreas Svensson, Thomas B. Sch\\\"on, Fredrik Lindsten", "title": "Identification of jump Markov linear models using particle filters", "comments": "Accepted to 53rd IEEE International Conference on Decision and\n  Control (CDC), 2014 (Los Angeles, CA, USA)", "journal-ref": "Proc. of IEEE 53rd Conference on Decision and Control (CDC),\n  pp.6504,6509, 15-17 Dec. 2014 (Los Angeles, CA, USA)", "doi": "10.1109/CDC.2014.7040409", "report-no": null, "categories": "stat.CO math.OC stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Jump Markov linear models consists of a finite number of linear state space\nmodels and a discrete variable encoding the jumps (or switches) between the\ndifferent linear models. Identifying jump Markov linear models makes for a\nchallenging problem lacking an analytical solution. We derive a new expectation\nmaximization (EM) type algorithm that produce maximum likelihood estimates of\nthe model parameters. Our development hinges upon recent progress in combining\nparticle filters with Markov chain Monte Carlo methods in solving the nonlinear\nstate smoothing problem inherent in the EM formulation. Key to our development\nis that we exploit a conditionally linear Gaussian substructure in the model,\nallowing for an efficient algorithm.\n", "versions": [{"version": "v1", "created": "Thu, 25 Sep 2014 15:18:40 GMT"}], "update_date": "2015-02-17", "authors_parsed": [["Svensson", "Andreas", ""], ["Sch\u00f6n", "Thomas B.", ""], ["Lindsten", "Fredrik", ""]]}, {"id": "1409.7458", "submitter": "Jiantao Jiao", "authors": "Jiantao Jiao, Kartik Venkat, Yanjun Han, Tsachy Weissman", "title": "Beyond Maximum Likelihood: from Theory to Practice", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME cs.DS cs.IT math.IT stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Maximum likelihood is the most widely used statistical estimation technique.\nRecent work by the authors introduced a general methodology for the\nconstruction of estimators for functionals in parametric models, and\ndemonstrated improvements - both in theory and in practice - over the maximum\nlikelihood estimator (MLE), particularly in high dimensional scenarios\ninvolving parameter dimension comparable to or larger than the number of\nsamples. This approach to estimation, building on results from approximation\ntheory, is shown to yield minimax rate-optimal estimators for a wide class of\nfunctionals, implementable with modest computational requirements. In a\nnutshell, a message of this recent work is that, for a wide class of\nfunctionals, the performance of these essentially optimal estimators with $n$\nsamples is comparable to that of the MLE with $n \\ln n$ samples.\n  In the present paper, we highlight the applicability of the aforementioned\nmethodology to statistical problems beyond functional estimation, and show that\nit can yield substantial gains. For example, we demonstrate that for learning\ntree-structured graphical models, our approach achieves a significant reduction\nof the required data size compared with the classical Chow--Liu algorithm,\nwhich is an implementation of the MLE, to achieve the same accuracy. The key\nstep in improving the Chow--Liu algorithm is to replace the empirical mutual\ninformation with the estimator for mutual information proposed by the authors.\nFurther, applying the same replacement approach to classical Bayesian network\nclassification, the resulting classifiers uniformly outperform the previous\nclassifiers on 26 widely used datasets.\n", "versions": [{"version": "v1", "created": "Fri, 26 Sep 2014 01:45:34 GMT"}], "update_date": "2014-09-29", "authors_parsed": [["Jiao", "Jiantao", ""], ["Venkat", "Kartik", ""], ["Han", "Yanjun", ""], ["Weissman", "Tsachy", ""]]}, {"id": "1409.7461", "submitter": "Ozan \\.Irsoy", "authors": "Ozan \\.Irsoy, Ethem Alpayd{\\i}n", "title": "Autoencoder Trees", "comments": "9 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We discuss an autoencoder model in which the encoding and decoding functions\nare implemented by decision trees. We use the soft decision tree where internal\nnodes realize soft multivariate splits given by a gating function and the\noverall output is the average of all leaves weighted by the gating values on\ntheir path. The encoder tree takes the input and generates a lower dimensional\nrepresentation in the leaves and the decoder tree takes this and reconstructs\nthe original input. Exploiting the continuity of the trees, autoencoder trees\nare trained with stochastic gradient descent. On handwritten digit and news\ndata, we see that the autoencoder trees yield good reconstruction error\ncompared to traditional autoencoder perceptrons. We also see that the\nautoencoder tree captures hierarchical representations at different\ngranularities of the data on its different levels and the leaves capture the\nlocalities in the input space.\n", "versions": [{"version": "v1", "created": "Fri, 26 Sep 2014 02:27:04 GMT"}], "update_date": "2014-09-29", "authors_parsed": [["\u0130rsoy", "Ozan", ""], ["Alpayd\u0131n", "Ethem", ""]]}, {"id": "1409.7480", "submitter": "Mohamed Elhoseiny Mohamed Elhoseiny", "authors": "Mohamed Elhoseiny, Ahmed Elgammal", "title": "Generalized Twin Gaussian Processes using Sharma-Mittal Divergence", "comments": "This work got accepted for Publication in the Machine Learning\n  Journal 2015. The work is scheduled for presentation at ECML-PKDD 2015\n  journal track papers", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  There has been a growing interest in mutual information measures due to their\nwide range of applications in Machine Learning and Computer Vision. In this\npaper, we present a generalized structured regression framework based on\nShama-Mittal divergence, a relative entropy measure, which is introduced to the\nMachine Learning community in this work. Sharma-Mittal (SM) divergence is a\ngeneralized mutual information measure for the widely used R\\'enyi, Tsallis,\nBhattacharyya, and Kullback-Leibler (KL) relative entropies. Specifically, we\nstudy Sharma-Mittal divergence as a cost function in the context of the Twin\nGaussian Processes (TGP)~\\citep{Bo:2010}, which generalizes over the\nKL-divergence without computational penalty. We show interesting properties of\nSharma-Mittal TGP (SMTGP) through a theoretical analysis, which covers missing\ninsights in the traditional TGP formulation. However, we generalize this theory\nbased on SM-divergence instead of KL-divergence which is a special case.\nExperimentally, we evaluated the proposed SMTGP framework on several datasets.\nThe results show that SMTGP reaches better predictions than KL-based TGP, since\nit offers a bigger class of models through its parameters that we learn from\nthe data.\n", "versions": [{"version": "v1", "created": "Fri, 26 Sep 2014 06:46:38 GMT"}, {"version": "v2", "created": "Wed, 1 Oct 2014 13:32:50 GMT"}, {"version": "v3", "created": "Fri, 3 Oct 2014 03:54:41 GMT"}, {"version": "v4", "created": "Mon, 6 Oct 2014 03:47:51 GMT"}, {"version": "v5", "created": "Mon, 1 Jun 2015 06:30:29 GMT"}], "update_date": "2015-06-02", "authors_parsed": [["Elhoseiny", "Mohamed", ""], ["Elgammal", "Ahmed", ""]]}, {"id": "1409.7489", "submitter": "Jisun An", "authors": "Jisun An, Daniele Quercia, Jon Crowcroft", "title": "Recommending Investors for Crowdfunding Projects", "comments": "Published in Proc. of WWW 2014", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.SI cs.CY cs.HC physics.soc-ph stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  To bring their innovative ideas to market, those embarking in new ventures\nhave to raise money, and, to do so, they have often resorted to banks and\nventure capitalists. Nowadays, they have an additional option: that of\ncrowdfunding. The name refers to the idea that funds come from a network of\npeople on the Internet who are passionate about supporting others' projects.\nOne of the most popular crowdfunding sites is Kickstarter. In it, creators post\ndescriptions of their projects and advertise them on social media sites (mainly\nTwitter), while investors look for projects to support. The most common reason\nfor project failure is the inability of founders to connect with a sufficient\nnumber of investors, and that is mainly because hitherto there has not been any\nautomatic way of matching creators and investors. We thus set out to propose\ndifferent ways of recommending investors found on Twitter for specific\nKickstarter projects. We do so by conducting hypothesis-driven analyses of\npledging behavior and translate the corresponding findings into different\nrecommendation strategies. The best strategy achieves, on average, 84% of\naccuracy in predicting a list of potential investors' Twitter accounts for any\ngiven project. Our findings also produced key insights about the whys and\nwherefores of investors deciding to support innovative efforts.\n", "versions": [{"version": "v1", "created": "Fri, 26 Sep 2014 07:57:28 GMT"}, {"version": "v2", "created": "Sun, 12 Oct 2014 11:23:36 GMT"}], "update_date": "2014-10-14", "authors_parsed": [["An", "Jisun", ""], ["Quercia", "Daniele", ""], ["Crowcroft", "Jon", ""]]}, {"id": "1409.7495", "submitter": "Yaroslav Ganin", "authors": "Yaroslav Ganin, Victor Lempitsky", "title": "Unsupervised Domain Adaptation by Backpropagation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Top-performing deep architectures are trained on massive amounts of labeled\ndata. In the absence of labeled data for a certain task, domain adaptation\noften provides an attractive option given that labeled data of similar nature\nbut from a different domain (e.g. synthetic images) are available. Here, we\npropose a new approach to domain adaptation in deep architectures that can be\ntrained on large amount of labeled data from the source domain and large amount\nof unlabeled data from the target domain (no labeled target-domain data is\nnecessary).\n  As the training progresses, the approach promotes the emergence of \"deep\"\nfeatures that are (i) discriminative for the main learning task on the source\ndomain and (ii) invariant with respect to the shift between the domains. We\nshow that this adaptation behaviour can be achieved in almost any feed-forward\nmodel by augmenting it with few standard layers and a simple new gradient\nreversal layer. The resulting augmented architecture can be trained using\nstandard backpropagation.\n  Overall, the approach can be implemented with little effort using any of the\ndeep-learning packages. The method performs very well in a series of image\nclassification experiments, achieving adaptation effect in the presence of big\ndomain shifts and outperforming previous state-of-the-art on Office datasets.\n", "versions": [{"version": "v1", "created": "Fri, 26 Sep 2014 08:22:21 GMT"}, {"version": "v2", "created": "Fri, 27 Feb 2015 14:54:37 GMT"}], "update_date": "2015-03-02", "authors_parsed": [["Ganin", "Yaroslav", ""], ["Lempitsky", "Victor", ""]]}, {"id": "1409.7552", "submitter": "Johannes Kulick", "authors": "Johannes Kulick, Robert Lieck and Marc Toussaint", "title": "The Advantage of Cross Entropy over Entropy in Iterative Information\n  Gathering", "comments": "24 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Gathering the most information by picking the least amount of data is a\ncommon task in experimental design or when exploring an unknown environment in\nreinforcement learning and robotics. A widely used measure for quantifying the\ninformation contained in some distribution of interest is its entropy. Greedily\nminimizing the expected entropy is therefore a standard method for choosing\nsamples in order to gain strong beliefs about the underlying random variables.\nWe show that this approach is prone to temporally getting stuck in local optima\ncorresponding to wrongly biased beliefs. We suggest instead maximizing the\nexpected cross entropy between old and new belief, which aims at challenging\nrefutable beliefs and thereby avoids these local optima. We show that both\ncriteria are closely related and that their difference can be traced back to\nthe asymmetry of the Kullback-Leibler divergence. In illustrative examples as\nwell as simulated and real-world experiments we demonstrate the advantage of\ncross entropy over simple entropy for practical applications.\n", "versions": [{"version": "v1", "created": "Fri, 26 Sep 2014 12:25:33 GMT"}, {"version": "v2", "created": "Wed, 16 Sep 2015 10:03:11 GMT"}], "update_date": "2015-09-17", "authors_parsed": [["Kulick", "Johannes", ""], ["Lieck", "Robert", ""], ["Toussaint", "Marc", ""]]}, {"id": "1409.7591", "submitter": "Arun Maiya", "authors": "Arun S. Maiya and Robert M. Rolfe", "title": "Topic Similarity Networks: Visual Analytics for Large Document Sets", "comments": "9 pages; 2014 IEEE International Conference on Big Data (IEEE BigData\n  2014)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.HC cs.IR cs.SI stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We investigate ways in which to improve the interpretability of LDA topic\nmodels by better analyzing and visualizing their outputs. We focus on examining\nwhat we refer to as topic similarity networks: graphs in which nodes represent\nlatent topics in text collections and links represent similarity among topics.\nWe describe efficient and effective approaches to both building and labeling\nsuch networks. Visualizations of topic models based on these networks are shown\nto be a powerful means of exploring, characterizing, and summarizing large\ncollections of unstructured text documents. They help to \"tease out\"\nnon-obvious connections among different sets of documents and provide insights\ninto how topics form larger themes. We demonstrate the efficacy and\npracticality of these approaches through two case studies: 1) NSF grants for\nbasic research spanning a 14 year period and 2) the entire English portion of\nWikipedia.\n", "versions": [{"version": "v1", "created": "Fri, 26 Sep 2014 15:11:57 GMT"}], "update_date": "2014-09-29", "authors_parsed": [["Maiya", "Arun S.", ""], ["Rolfe", "Robert M.", ""]]}, {"id": "1409.7672", "submitter": "Dennis Leung", "authors": "Dennis Leung, Mathias Drton", "title": "Order-invariant prior specification in Bayesian factor analysis", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In (exploratory) factor analysis, the loading matrix is identified only up to\northogonal rotation. For identifiability, one thus often takes the loading\nmatrix to be lower triangular with positive diagonal entries. In Bayesian\ninference, a standard practice is then to specify a prior under which the\nloadings are independent, the off-diagonal loadings are normally distributed,\nand the diagonal loadings follow a truncated normal distribution. This prior\nspecification, however, depends in an important way on how the variables and\nassociated rows of the loading matrix are ordered. We show how a minor\nmodification of the approach allows one to compute with the identifiable lower\ntriangular loading matrix but maintain invariance properties under reordering\nof the variables.\n", "versions": [{"version": "v1", "created": "Fri, 26 Sep 2014 19:18:43 GMT"}], "update_date": "2014-09-29", "authors_parsed": [["Leung", "Dennis", ""], ["Drton", "Mathias", ""]]}, {"id": "1409.8185", "submitter": "Theodoros Tsiligkaridis", "authors": "Theodoros Tsiligkaridis, Keith W. Forsythe", "title": "Adaptive Low-Complexity Sequential Inference for Dirichlet Process\n  Mixture Models", "comments": "25 pages, To appear in Advances in Neural Information Processing\n  Systems (NIPS) 2015", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We develop a sequential low-complexity inference procedure for Dirichlet\nprocess mixtures of Gaussians for online clustering and parameter estimation\nwhen the number of clusters are unknown a-priori. We present an easily\ncomputable, closed form parametric expression for the conditional likelihood,\nin which hyperparameters are recursively updated as a function of the streaming\ndata assuming conjugate priors. Motivated by large-sample asymptotics, we\npropose a novel adaptive low-complexity design for the Dirichlet process\nconcentration parameter and show that the number of classes grow at most at a\nlogarithmic rate. We further prove that in the large-sample limit, the\nconditional likelihood and data predictive distribution become asymptotically\nGaussian. We demonstrate through experiments on synthetic and real data sets\nthat our approach is superior to other online state-of-the-art methods.\n", "versions": [{"version": "v1", "created": "Mon, 29 Sep 2014 16:47:44 GMT"}, {"version": "v2", "created": "Thu, 5 Feb 2015 15:55:06 GMT"}, {"version": "v3", "created": "Fri, 11 Sep 2015 20:07:31 GMT"}], "update_date": "2015-09-15", "authors_parsed": [["Tsiligkaridis", "Theodoros", ""], ["Forsythe", "Keith W.", ""]]}, {"id": "1409.8276", "submitter": "Beyza Ermis Ms", "authors": "Beyza Ermis, A. Taylan Cemgil", "title": "A Bayesian Tensor Factorization Model via Variational Inference for Link\n  Prediction", "comments": "arXiv admin note: substantial text overlap with arXiv:1409.8083", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.NA stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Probabilistic approaches for tensor factorization aim to extract meaningful\nstructure from incomplete data by postulating low rank constraints. Recently,\nvariational Bayesian (VB) inference techniques have successfully been applied\nto large scale models. This paper presents full Bayesian inference via VB on\nboth single and coupled tensor factorization models. Our method can be run even\nfor very large models and is easily implemented. It exhibits better prediction\nperformance than existing approaches based on maximum likelihood on several\nreal-world datasets for missing link prediction problem.\n", "versions": [{"version": "v1", "created": "Mon, 29 Sep 2014 12:29:21 GMT"}], "update_date": "2014-10-01", "authors_parsed": [["Ermis", "Beyza", ""], ["Cemgil", "A. Taylan", ""]]}, {"id": "1409.8327", "submitter": "Alessandro Chiuso", "authors": "Giulia Prando and Alessandro Chiuso and Gianluigi Pillonetto", "title": "Bayesian and regularization approaches to multivariable linear system\n  identification: the role of rank penalties", "comments": "to appear in IEEE Conference on Decision and Control, 2014", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.SY cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recent developments in linear system identification have proposed the use of\nnon-parameteric methods, relying on regularization strategies, to handle the\nso-called bias/variance trade-off. This paper introduces an impulse response\nestimator which relies on an $\\ell_2$-type regularization including a\nrank-penalty derived using the log-det heuristic as a smooth approximation to\nthe rank function. This allows to account for different properties of the\nestimated impulse response (e.g. smoothness and stability) while also\npenalizing high-complexity models. This also allows to account and enforce\ncoupling between different input-output channels in MIMO systems. According to\nthe Bayesian paradigm, the parameters defining the relative weight of the two\nregularization terms as well as the structure of the rank penalty are estimated\noptimizing the marginal likelihood. Once these hyperameters have been\nestimated, the impulse response estimate is available in closed form.\nExperiments show that the proposed method is superior to the estimator relying\non the \"classic\" $\\ell_2$-regularization alone as well as those based in atomic\nand nuclear norm.\n", "versions": [{"version": "v1", "created": "Mon, 29 Sep 2014 21:08:54 GMT"}], "update_date": "2014-10-01", "authors_parsed": [["Prando", "Giulia", ""], ["Chiuso", "Alessandro", ""], ["Pillonetto", "Gianluigi", ""]]}, {"id": "1409.8359", "submitter": "Swayambhoo Jain", "authors": "Swayambhoo Jain, Seung-Jun Kim and Georgios B. Giannakis", "title": "Backhaul-Constrained Multi-Cell Cooperation Leveraging Sparsity and\n  Spectral Clustering", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IT cs.NI math.IT stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Multi-cell cooperative processing with limited backhaul traffic is studied\nfor cellular uplinks. Aiming at reduced backhaul overhead, a\nsparsity-regularized multi-cell receive-filter design problem is formulated.\nBoth unstructured distributed cooperation as well as clustered cooperation, in\nwhich base station groups are formed for tight cooperation, are considered.\nDynamic clustered cooperation, where the sparse equalizer and the cooperation\nclusters are jointly determined, is solved via alternating minimization based\non spectral clustering and group-sparse regression. Furthermore, decentralized\nimplementations of both unstructured and clustered cooperation schemes are\ndeveloped for scalability, robustness and computational efficiency. Extensive\nnumerical tests verify the efficacy of the proposed methods.\n", "versions": [{"version": "v1", "created": "Tue, 30 Sep 2014 01:37:11 GMT"}, {"version": "v2", "created": "Wed, 14 Oct 2015 22:22:16 GMT"}], "update_date": "2015-10-16", "authors_parsed": [["Jain", "Swayambhoo", ""], ["Kim", "Seung-Jun", ""], ["Giannakis", "Georgios B.", ""]]}, {"id": "1409.8428", "submitter": "Ohad Shamir", "authors": "Noga Alon, Nicol\\`o Cesa-Bianchi, Claudio Gentile, Shie Mannor, Yishay\n  Mansour and Ohad Shamir", "title": "Nonstochastic Multi-Armed Bandits with Graph-Structured Feedback", "comments": "Preliminary versions of parts of this paper appeared in [1,20], and\n  also as arXiv papers arXiv:1106.2436 and arXiv:1307.4564", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present and study a partial-information model of online learning, where a\ndecision maker repeatedly chooses from a finite set of actions, and observes\nsome subset of the associated losses. This naturally models several situations\nwhere the losses of different actions are related, and knowing the loss of one\naction provides information on the loss of other actions. Moreover, it\ngeneralizes and interpolates between the well studied full-information setting\n(where all losses are revealed) and the bandit setting (where only the loss of\nthe action chosen by the player is revealed). We provide several algorithms\naddressing different variants of our setting, and provide tight regret bounds\ndepending on combinatorial properties of the information feedback structure.\n", "versions": [{"version": "v1", "created": "Tue, 30 Sep 2014 08:29:13 GMT"}], "update_date": "2014-10-01", "authors_parsed": [["Alon", "Noga", ""], ["Cesa-Bianchi", "Nicol\u00f2", ""], ["Gentile", "Claudio", ""], ["Mannor", "Shie", ""], ["Mansour", "Yishay", ""], ["Shamir", "Ohad", ""]]}, {"id": "1409.8437", "submitter": "Ingo Steinwart", "authors": "Ingo Steinwart", "title": "Fully adaptive density-based clustering", "comments": "Published at http://dx.doi.org/10.1214/15-AOS1331 in the Annals of\n  Statistics (http://www.imstat.org/aos/) by the Institute of Mathematical\n  Statistics (http://www.imstat.org)", "journal-ref": "Annals of Statistics 2015, Vol. 43, No. 5, 2132-2167", "doi": "10.1214/15-AOS1331", "report-no": "IMS-AOS-AOS1331", "categories": "stat.ME stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The clusters of a distribution are often defined by the connected components\nof a density level set. However, this definition depends on the user-specified\nlevel. We address this issue by proposing a simple, generic algorithm, which\nuses an almost arbitrary level set estimator to estimate the smallest level at\nwhich there are more than one connected components. In the case where this\nalgorithm is fed with histogram-based level set estimates, we provide a finite\nsample analysis, which is then used to show that the algorithm consistently\nestimates both the smallest level and the corresponding connected components.\nWe further establish rates of convergence for the two estimation problems, and\nlast but not least, we present a simple, yet adaptive strategy for determining\nthe width-parameter of the involved density estimator in a data-depending way.\n", "versions": [{"version": "v1", "created": "Tue, 30 Sep 2014 08:54:28 GMT"}, {"version": "v2", "created": "Thu, 5 Mar 2015 15:47:40 GMT"}, {"version": "v3", "created": "Fri, 23 Oct 2015 09:22:00 GMT"}, {"version": "v4", "created": "Wed, 28 Oct 2015 07:31:20 GMT"}], "update_date": "2015-10-29", "authors_parsed": [["Steinwart", "Ingo", ""]]}, {"id": "1409.8444", "submitter": "Ting Kei Pong", "authors": "Guoyin Li, Ting Kei Pong", "title": "Douglas-Rachford splitting for nonconvex optimization with application\n  to nonconvex feasibility problems", "comments": "To appear in Mathematical Programming", "journal-ref": null, "doi": "10.1007/s10107-015-0963-5", "report-no": null, "categories": "math.OC stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We adapt the Douglas-Rachford (DR) splitting method to solve nonconvex\nfeasibility problems by studying this method for a class of nonconvex\noptimization problem. While the convergence properties of the method for convex\nproblems have been well studied, far less is known in the nonconvex setting. In\nthis paper, for the direct adaptation of the method to minimize the sum of a\nproper closed function $g$ and a smooth function $f$ with a Lipschitz\ncontinuous gradient, we show that if the step-size parameter is smaller than a\ncomputable threshold and the sequence generated has a cluster point, then it\ngives a stationary point of the optimization problem. Convergence of the whole\nsequence and a local convergence rate are also established under the additional\nassumption that $f$ and $g$ are semi-algebraic. We also give simple sufficient\nconditions guaranteeing the boundedness of the sequence generated. We then\napply our nonconvex DR splitting method to finding a point in the intersection\nof a closed convex set $C$ and a general closed set $D$ by minimizing the\nsquared distance to $C$ subject to $D$. We show that if either set is bounded\nand the step-size parameter is smaller than a computable threshold, then the\nsequence generated from the DR splitting method is actually bounded.\nConsequently, the sequence generated will have cluster points that are\nstationary for an optimization problem, and the whole sequence is convergent\nunder an additional assumption that $C$ and $D$ are semi-algebraic. We achieve\nthese results based on a new merit function constructed particularly for the DR\nsplitting method. Our preliminary numerical results indicate that our DR\nsplitting method usually outperforms the alternating projection method in\nfinding a sparse solution of a linear system, in terms of both the solution\nquality and the number of iterations taken.\n", "versions": [{"version": "v1", "created": "Tue, 30 Sep 2014 09:12:29 GMT"}, {"version": "v2", "created": "Fri, 23 Jan 2015 13:06:20 GMT"}, {"version": "v3", "created": "Mon, 4 May 2015 11:49:43 GMT"}, {"version": "v4", "created": "Wed, 6 May 2015 04:58:59 GMT"}, {"version": "v5", "created": "Thu, 5 Nov 2015 06:31:11 GMT"}], "update_date": "2015-11-17", "authors_parsed": [["Li", "Guoyin", ""], ["Pong", "Ting Kei", ""]]}, {"id": "1409.8485", "submitter": "An Zeng", "authors": "Hao Liao, An Zeng, Yi-Cheng Zhang", "title": "Predicting missing links via correlation between nodes", "comments": "7 pages, 3 figures, 2 tables. arXiv admin note: text overlap with\n  arXiv:1010.0725 by other authors", "journal-ref": null, "doi": null, "report-no": null, "categories": "physics.soc-ph cs.SI physics.data-an stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  As a fundamental problem in many different fields, link prediction aims to\nestimate the likelihood of an existing link between two nodes based on the\nobserved information. Since this problem is related to many applications\nranging from uncovering missing data to predicting the evolution of networks,\nlink prediction has been intensively investigated recently and many methods\nhave been proposed so far. The essential challenge of link prediction is to\nestimate the similarity between nodes. Most of the existing methods are based\non the common neighbor index and its variants. In this paper, we propose to\ncalculate the similarity between nodes by the correlation coefficient. This\nmethod is found to be very effective when applied to calculate similarity based\non high order paths. We finally fuse the correlation-based method with the\nresource allocation method, and find that the combined method can substantially\noutperform the existing methods, especially in sparse networks.\n", "versions": [{"version": "v1", "created": "Tue, 30 Sep 2014 11:12:58 GMT"}], "update_date": "2014-10-01", "authors_parsed": [["Liao", "Hao", ""], ["Zeng", "An", ""], ["Zhang", "Yi-Cheng", ""]]}, {"id": "1409.8576", "submitter": "Huseyin Ozkan", "authors": "Huseyin Ozkan, Ozgun S. Pelvan and Suleyman S. Kozat", "title": "Data Imputation through the Identification of Local Anomalies", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We introduce a comprehensive and statistical framework in a model free\nsetting for a complete treatment of localized data corruptions due to severe\nnoise sources, e.g., an occluder in the case of a visual recording. Within this\nframework, we propose i) a novel algorithm to efficiently separate, i.e.,\ndetect and localize, possible corruptions from a given suspicious data instance\nand ii) a Maximum A Posteriori (MAP) estimator to impute the corrupted data. As\na generalization to Euclidean distance, we also propose a novel distance\nmeasure, which is based on the ranked deviations among the data attributes and\nempirically shown to be superior in separating the corruptions. Our algorithm\nfirst splits the suspicious instance into parts through a binary partitioning\ntree in the space of data attributes and iteratively tests those parts to\ndetect local anomalies using the nominal statistics extracted from an\nuncorrupted (clean) reference data set. Once each part is labeled as anomalous\nvs normal, the corresponding binary patterns over this tree that characterize\ncorruptions are identified and the affected attributes are imputed. Under a\ncertain conditional independency structure assumed for the binary patterns, we\nanalytically show that the false alarm rate of the introduced algorithm in\ndetecting the corruptions is independent of the data and can be directly set\nwithout any parameter tuning. The proposed framework is tested over several\nwell-known machine learning data sets with synthetically generated corruptions;\nand experimentally shown to produce remarkable improvements in terms of\nclassification purposes with strong corruption separation capabilities. Our\nexperiments also indicate that the proposed algorithms outperform the typical\napproaches and are robust to varying training phase conditions.\n", "versions": [{"version": "v1", "created": "Tue, 30 Sep 2014 14:55:07 GMT"}], "update_date": "2014-10-02", "authors_parsed": [["Ozkan", "Huseyin", ""], ["Pelvan", "Ozgun S.", ""], ["Kozat", "Suleyman S.", ""]]}, {"id": "1409.8606", "submitter": "Shahin Shahrampour", "authors": "Shahin Shahrampour, Alexander Rakhlin, Ali Jadbabaie", "title": "Distributed Detection : Finite-time Analysis and Impact of Network\n  Topology", "comments": "29 pages, 5 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.OC cs.LG cs.SI stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper addresses the problem of distributed detection in multi-agent\nnetworks. Agents receive private signals about an unknown state of the world.\nThe underlying state is globally identifiable, yet informative signals may be\ndispersed throughout the network. Using an optimization-based framework, we\ndevelop an iterative local strategy for updating individual beliefs. In\ncontrast to the existing literature which focuses on asymptotic learning, we\nprovide a finite-time analysis. Furthermore, we introduce a Kullback-Leibler\ncost to compare the efficiency of the algorithm to its centralized counterpart.\nOur bounds on the cost are expressed in terms of network size, spectral gap,\ncentrality of each agent and relative entropy of agents' signal structures. A\nkey observation is that distributing more informative signals to central agents\nresults in a faster learning rate. Furthermore, optimizing the weights, we can\nspeed up learning by improving the spectral gap. We also quantify the effect of\nlink failures on learning speed in symmetric networks. We finally provide\nnumerical simulations which verify our theoretical results.\n", "versions": [{"version": "v1", "created": "Tue, 30 Sep 2014 15:49:59 GMT"}], "update_date": "2014-10-01", "authors_parsed": [["Shahrampour", "Shahin", ""], ["Rakhlin", "Alexander", ""], ["Jadbabaie", "Ali", ""]]}, {"id": "1409.8630", "submitter": "Daniel Andr\\'es D\\'iaz Pach\\'on", "authors": "Daniel A D\\'iaz-Pach\\'on, Jean-Eudes Dazard, J. Sunil Rao", "title": "Unsupervised Bump Hunting Using Principal Components", "comments": "24 pages, 9 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Principal Components Analysis is a widely used technique for dimension\nreduction and characterization of variability in multivariate populations. Our\ninterest lies in studying when and why the rotation to principal components can\nbe used effectively within a response-predictor set relationship in the context\nof mode hunting. Specifically focusing on the Patient Rule Induction Method\n(PRIM), we first develop a fast version of this algorithm (fastPRIM) under\nnormality which facilitates the theoretical studies to follow. Using basic\ngeometrical arguments, we then demonstrate how the PC rotation of the predictor\nspace alone can in fact generate improved mode estimators. Simulation results\nare used to illustrate our findings.\n", "versions": [{"version": "v1", "created": "Tue, 30 Sep 2014 17:06:47 GMT"}], "update_date": "2014-10-01", "authors_parsed": [["D\u00edaz-Pach\u00f3n", "Daniel A", ""], ["Dazard", "Jean-Eudes", ""], ["Rao", "J. Sunil", ""]]}]