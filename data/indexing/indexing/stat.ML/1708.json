[{"id": "1708.00049", "submitter": "Richard Phillips", "authors": "Richard L. Phillips, Kyu Hyun Chang and Sorelle A. Friedler", "title": "Interpretable Active Learning", "comments": "13 pages, 8 figures, presented at 2018 Conference on Fairness,\n  Accountability, and Transparency (FAT*), New York, New York, USA. Proceedings\n  of the 1st Conference on Fairness, Accountability and Transparency, PMLR\n  81:49-61, 2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Active learning has long been a topic of study in machine learning. However,\nas increasingly complex and opaque models have become standard practice, the\nprocess of active learning, too, has become more opaque. There has been little\ninvestigation into interpreting what specific trends and patterns an active\nlearning strategy may be exploring. This work expands on the Local\nInterpretable Model-agnostic Explanations framework (LIME) to provide\nexplanations for active learning recommendations. We demonstrate how LIME can\nbe used to generate locally faithful explanations for an active learning\nstrategy, and how these explanations can be used to understand how different\nmodels and datasets explore a problem space over time. In order to quantify the\nper-subgroup differences in how an active learning strategy queries spatial\nregions, we introduce a notion of uncertainty bias (based on disparate impact)\nto measure the discrepancy in the confidence for a model's predictions between\none subgroup and another. Using the uncertainty bias measure, we show that our\nquery explanations accurately reflect the subgroup focus of the active learning\nqueries, allowing for an interpretable explanation of what is being learned as\npoints with similar sources of uncertainty have their uncertainty bias\nresolved. We demonstrate that this technique can be applied to track\nuncertainty bias over user-defined clusters or automatically generated clusters\nbased on the source of uncertainty.\n", "versions": [{"version": "v1", "created": "Mon, 31 Jul 2017 19:46:59 GMT"}, {"version": "v2", "created": "Sun, 24 Jun 2018 02:45:50 GMT"}], "update_date": "2018-06-26", "authors_parsed": [["Phillips", "Richard L.", ""], ["Chang", "Kyu Hyun", ""], ["Friedler", "Sorelle A.", ""]]}, {"id": "1708.00069", "submitter": "Peng Zheng", "authors": "Peng Zheng, Aleksandr Y. Aravkin, Karthikeyan Natesan Ramamurthy and\n  Jayaraman Jayaraman Thiagarajan", "title": "Learning Robust Representations for Computer Vision", "comments": "8 pages, 7 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Unsupervised learning techniques in computer vision often require learning\nlatent representations, such as low-dimensional linear and non-linear\nsubspaces. Noise and outliers in the data can frustrate these approaches by\nobscuring the latent spaces.\n  Our main goal is deeper understanding and new development of robust\napproaches for representation learning. We provide a new interpretation for\nexisting robust approaches and present two specific contributions: a new robust\nPCA approach, which can separate foreground features from dynamic background,\nand a novel robust spectral clustering method, that can cluster facial images\nwith high accuracy. Both contributions show superior performance to standard\nmethods on real-world test sets.\n", "versions": [{"version": "v1", "created": "Mon, 31 Jul 2017 20:50:01 GMT"}], "update_date": "2017-08-02", "authors_parsed": [["Zheng", "Peng", ""], ["Aravkin", "Aleksandr Y.", ""], ["Ramamurthy", "Karthikeyan Natesan", ""], ["Thiagarajan", "Jayaraman Jayaraman", ""]]}, {"id": "1708.00075", "submitter": "Cyril Zhang", "authors": "Elad Hazan, Karan Singh, Cyril Zhang", "title": "Efficient Regret Minimization in Non-Convex Games", "comments": "Published as a conference paper at ICML 2017", "journal-ref": null, "doi": null, "report-no": "PMLR 70:1433-1441", "categories": "cs.LG cs.GT stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider regret minimization in repeated games with non-convex loss\nfunctions. Minimizing the standard notion of regret is computationally\nintractable. Thus, we define a natural notion of regret which permits efficient\noptimization and generalizes offline guarantees for convergence to an\napproximate local optimum. We give gradient-based methods that achieve optimal\nregret, which in turn guarantee convergence to equilibrium in this framework.\n", "versions": [{"version": "v1", "created": "Mon, 31 Jul 2017 21:23:29 GMT"}], "update_date": "2017-11-06", "authors_parsed": [["Hazan", "Elad", ""], ["Singh", "Karan", ""], ["Zhang", "Cyril", ""]]}, {"id": "1708.00077", "submitter": "Ekaterina Lobacheva Ms", "authors": "Ekaterina Lobacheva, Nadezhda Chirkova, Dmitry Vetrov", "title": "Bayesian Sparsification of Recurrent Neural Networks", "comments": "Published in Workshop on Learning to Generate Natural Language, ICML,\n  2017", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.CL cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recurrent neural networks show state-of-the-art results in many text analysis\ntasks but often require a lot of memory to store their weights. Recently\nproposed Sparse Variational Dropout eliminates the majority of the weights in a\nfeed-forward neural network without significant loss of quality. We apply this\ntechnique to sparsify recurrent neural networks. To account for recurrent\nspecifics we also rely on Binary Variational Dropout for RNN. We report 99.5%\nsparsity level on sentiment analysis task without a quality drop and up to 87%\nsparsity level on language modeling task with slight loss of accuracy.\n", "versions": [{"version": "v1", "created": "Mon, 31 Jul 2017 21:33:42 GMT"}], "update_date": "2017-08-02", "authors_parsed": [["Lobacheva", "Ekaterina", ""], ["Chirkova", "Nadezhda", ""], ["Vetrov", "Dmitry", ""]]}, {"id": "1708.00102", "submitter": "Lucas Lehnert", "authors": "Lucas Lehnert, Stefanie Tellex, and Michael L. Littman", "title": "Advantages and Limitations of using Successor Features for Transfer in\n  Reinforcement Learning", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  One question central to Reinforcement Learning is how to learn a feature\nrepresentation that supports algorithm scaling and re-use of learned\ninformation from different tasks. Successor Features approach this problem by\nlearning a feature representation that satisfies a temporal constraint. We\npresent an implementation of an approach that decouples the feature\nrepresentation from the reward function, making it suitable for transferring\nknowledge between domains. We then assess the advantages and limitations of\nusing Successor Features for transfer.\n", "versions": [{"version": "v1", "created": "Mon, 31 Jul 2017 23:36:18 GMT"}], "update_date": "2017-08-02", "authors_parsed": [["Lehnert", "Lucas", ""], ["Tellex", "Stefanie", ""], ["Littman", "Michael L.", ""]]}, {"id": "1708.00112", "submitter": "Benjamin Lengerich", "authors": "Benjamin J. Lengerich, Andrew L. Maas, Christopher Potts", "title": "Retrofitting Distributional Embeddings to Knowledge Graphs with\n  Functional Relations", "comments": "COLING 2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.CL cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Knowledge graphs are a versatile framework to encode richly structured data\nrelationships, but it can be challenging to combine these graphs with\nunstructured data. Methods for retrofitting pre-trained entity representations\nto the structure of a knowledge graph typically assume that entities are\nembedded in a connected space and that relations imply similarity. However,\nuseful knowledge graphs often contain diverse entities and relations (with\npotentially disjoint underlying corpora) which do not accord with these\nassumptions. To overcome these limitations, we present Functional Retrofitting,\na framework that generalizes current retrofitting methods by explicitly\nmodeling pairwise relations. Our framework can directly incorporate a variety\nof pairwise penalty functions previously developed for knowledge graph\ncompletion. Further, it allows users to encode, learn, and extract information\nabout relation semantics. We present both linear and neural instantiations of\nthe framework. Functional Retrofitting significantly outperforms existing\nretrofitting methods on complex knowledge graphs and loses no accuracy on\nsimpler graphs (in which relations do imply similarity). Finally, we\ndemonstrate the utility of the framework by predicting new drug--disease\ntreatment pairs in a large, complex health knowledge graph.\n", "versions": [{"version": "v1", "created": "Tue, 1 Aug 2017 00:23:03 GMT"}, {"version": "v2", "created": "Wed, 25 Oct 2017 18:13:26 GMT"}, {"version": "v3", "created": "Sat, 16 Jun 2018 13:01:49 GMT"}], "update_date": "2018-06-19", "authors_parsed": [["Lengerich", "Benjamin J.", ""], ["Maas", "Andrew L.", ""], ["Potts", "Christopher", ""]]}, {"id": "1708.00122", "submitter": "Jessica Rudd", "authors": "Jessica M. Rudd", "title": "Application of Support Vector Machine Modeling and Graph Theory Metrics\n  for Disease Classification", "comments": "6 pages, 9 figures, 3 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Disease classification is a crucial element of biomedical research. Recent\nstudies have demonstrated that machine learning techniques, such as Support\nVector Machine (SVM) modeling, produce similar or improved predictive\ncapabilities in comparison to the traditional method of Logistic Regression. In\naddition, it has been found that social network metrics can provide useful\npredictive information for disease modeling. In this study, we combine\nsimulated social network metrics with SVM to predict diabetes in a sample of\ndata from the Behavioral Risk Factor Surveillance System. In this dataset,\nLogistic Regression outperformed SVM with ROC index of 81.8 and 81.7 for models\nwith and without graph metrics, respectively. SVM with a polynomial kernel had\nROC index of 72.9 and 75.6 for models with and without graph metrics,\nrespectively. Although this did not perform as well as Logistic Regression, the\nresults are consistent with previous studies utilizing SVM to classify\ndiabetes.\n", "versions": [{"version": "v1", "created": "Tue, 1 Aug 2017 01:25:03 GMT"}], "update_date": "2017-08-02", "authors_parsed": [["Rudd", "Jessica M.", ""]]}, {"id": "1708.00132", "submitter": "Masaaki Imaizumi", "authors": "Masaaki Imaizumi, Takanori Maehara, Kohei Hayashi", "title": "On Tensor Train Rank Minimization: Statistical Efficiency and Scalable\n  Algorithm", "comments": "24 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Tensor train (TT) decomposition provides a space-efficient representation for\nhigher-order tensors. Despite its advantage, we face two crucial limitations\nwhen we apply the TT decomposition to machine learning problems: the lack of\nstatistical theory and of scalable algorithms. In this paper, we address the\nlimitations. First, we introduce a convex relaxation of the TT decomposition\nproblem and derive its error bound for the tensor completion task. Next, we\ndevelop an alternating optimization method with a randomization technique, in\nwhich the time complexity is as efficient as the space complexity is. In\nexperiments, we numerically confirm the derived bounds and empirically\ndemonstrate the performance of our method with a real higher-order tensor.\n", "versions": [{"version": "v1", "created": "Tue, 1 Aug 2017 02:18:41 GMT"}, {"version": "v2", "created": "Wed, 2 Aug 2017 01:23:18 GMT"}], "update_date": "2017-08-03", "authors_parsed": [["Imaizumi", "Masaaki", ""], ["Maehara", "Takanori", ""], ["Hayashi", "Kohei", ""]]}, {"id": "1708.00146", "submitter": "Quanming Yao", "authors": "Quanming Yao, James T.Kwok, Taifeng Wang and Tie-Yan Liu", "title": "Large-Scale Low-Rank Matrix Learning with Nonconvex Regularizers", "comments": "Accepted by TPAMI in 2018 (extension of ICDM-2015 conference paper\n  arXiv:1512.00984)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Low-rank modeling has many important applications in computer vision and\nmachine learning. While the matrix rank is often approximated by the convex\nnuclear norm, the use of nonconvex low-rank regularizers has demonstrated\nbetter empirical performance. However, the resulting optimization problem is\nmuch more challenging. Recent state-of-the-art requires an expensive full SVD\nin each iteration. In this paper, we show that for many commonly-used nonconvex\nlow-rank regularizers, a cutoff can be derived to automatically threshold the\nsingular values obtained from the proximal operator. This allows such operator\nbeing efficiently approximated by power method. Based on it, we develop a\nproximal gradient algorithm (and its accelerated variant) with inexact proximal\nsplitting and prove that a convergence rate of O(1/T) where T is the number of\niterations is guaranteed. Furthermore, we show the proposed algorithm can be\nwell parallelized, which achieves nearly linear speedup w.r.t the number of\nthreads. Extensive experiments are performed on matrix completion and robust\nprincipal component analysis, which shows a significant speedup over the\nstate-of-the-art. Moreover, the matrix solution obtained is more accurate and\nhas a lower rank than that of the nuclear norm regularizer.\n", "versions": [{"version": "v1", "created": "Tue, 1 Aug 2017 03:21:55 GMT"}, {"version": "v2", "created": "Sat, 23 Sep 2017 03:33:27 GMT"}, {"version": "v3", "created": "Mon, 23 Jul 2018 17:10:20 GMT"}], "update_date": "2018-07-25", "authors_parsed": [["Yao", "Quanming", ""], ["Kwok", "James T.", ""], ["Wang", "Taifeng", ""], ["Liu", "Tie-Yan", ""]]}, {"id": "1708.00185", "submitter": "Boyan Zhang", "authors": "Mingyuan Bai, Boyan Zhang, and Junbin Gao", "title": "Tensorial Recurrent Neural Networks for Longitudinal Data Analysis", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Traditional Recurrent Neural Networks assume vectorized data as inputs.\nHowever many data from modern science and technology come in certain structures\nsuch as tensorial time series data. To apply the recurrent neural networks for\nthis type of data, a vectorisation process is necessary, while such a\nvectorisation leads to the loss of the precise information of the spatial or\nlongitudinal dimensions. In addition, such a vectorized data is not an optimum\nsolution for learning the representation of the longitudinal data. In this\npaper, we propose a new variant of tensorial neural networks which directly\ntake tensorial time series data as inputs. We call this new variant as\nTensorial Recurrent Neural Network (TRNN). The proposed TRNN is based on tensor\nTucker decomposition.\n", "versions": [{"version": "v1", "created": "Tue, 1 Aug 2017 07:14:36 GMT"}], "update_date": "2017-08-02", "authors_parsed": [["Bai", "Mingyuan", ""], ["Zhang", "Boyan", ""], ["Gao", "Junbin", ""]]}, {"id": "1708.00253", "submitter": "Matja\\v{z} Kukar PhD", "authors": "Gregor Gun\\v{c}ar, Matja\\v{z} Kukar, Mateja Notar, Miran Brvar, Peter\n  \\v{C}ernel\\v{c}, Manca Notar, Marko Notar", "title": "Application of machine learning for hematological diagnosis", "comments": "15 pages, 6 figures", "journal-ref": null, "doi": "10.1038/s41598-017-18564-8", "report-no": null, "categories": "stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Quick and accurate medical diagnosis is crucial for the successful treatment\nof a disease. Using machine learning algorithms, we have built two models to\npredict a hematologic disease, based on laboratory blood test results. In one\npredictive model, we used all available blood test parameters and in the other\na reduced set, which is usually measured upon patient admittance. Both models\nproduced good results, with a prediction accuracy of 0.88 and 0.86, when\nconsidering the list of five most probable diseases, and 0.59 and 0.57, when\nconsidering only the most probable disease. Models did not differ significantly\nfrom each other, which indicates that a reduced set of parameters contains a\nrelevant fingerprint of a disease, expanding the utility of the model for\ngeneral practitioner's use and indicating that there is more information in the\nblood test results than physicians recognize. In the clinical test we showed\nthat the accuracy of our predictive models was on a par with the ability of\nhematology specialists. Our study is the first to show that a machine learning\npredictive model based on blood tests alone, can be successfully applied to\npredict hematologic diseases and could open up unprecedented possibilities in\nmedical diagnosis.\n", "versions": [{"version": "v1", "created": "Tue, 1 Aug 2017 11:15:52 GMT"}], "update_date": "2020-06-09", "authors_parsed": [["Gun\u010dar", "Gregor", ""], ["Kukar", "Matja\u017e", ""], ["Notar", "Mateja", ""], ["Brvar", "Miran", ""], ["\u010cernel\u010d", "Peter", ""], ["Notar", "Manca", ""], ["Notar", "Marko", ""]]}, {"id": "1708.00257", "submitter": "Teng Zhang", "authors": "Teng Zhang and Yi Yang", "title": "Robust PCA by Manifold Optimization", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Robust PCA is a widely used statistical procedure to recover a underlying\nlow-rank matrix with grossly corrupted observations. This work considers the\nproblem of robust PCA as a nonconvex optimization problem on the manifold of\nlow-rank matrices, and proposes two algorithms (for two versions of\nretractions) based on manifold optimization. It is shown that, with a proper\ndesigned initialization, the proposed algorithms are guaranteed to converge to\nthe underlying low-rank matrix linearly. Compared with a previous work based on\nthe Burer-Monterio decomposition of low-rank matrices, the proposed algorithms\nreduce the dependence on the conditional number of the underlying low-rank\nmatrix theoretically. Simulations and real data examples confirm the\ncompetitive performance of our method.\n", "versions": [{"version": "v1", "created": "Tue, 1 Aug 2017 11:39:21 GMT"}, {"version": "v2", "created": "Mon, 7 Aug 2017 05:00:54 GMT"}, {"version": "v3", "created": "Fri, 1 Sep 2017 07:00:36 GMT"}], "update_date": "2017-09-04", "authors_parsed": [["Zhang", "Teng", ""], ["Yang", "Yi", ""]]}, {"id": "1708.00260", "submitter": "Hae Beom Lee", "authors": "Hae Beom Lee, Eunho Yang, Sung Ju Hwang", "title": "Deep Asymmetric Multi-task Feature Learning", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose Deep Asymmetric Multitask Feature Learning (Deep-AMTFL) which can\nlearn deep representations shared across multiple tasks while effectively\npreventing negative transfer that may happen in the feature sharing process.\nSpecifically, we introduce an asymmetric autoencoder term that allows reliable\npredictors for the easy tasks to have high contribution to the feature learning\nwhile suppressing the influences of unreliable predictors for more difficult\ntasks. This allows the learning of less noisy representations, and enables\nunreliable predictors to exploit knowledge from the reliable predictors via the\nshared latent features. Such asymmetric knowledge transfer through shared\nfeatures is also more scalable and efficient than inter-task asymmetric\ntransfer. We validate our Deep-AMTFL model on multiple benchmark datasets for\nmultitask learning and image classification, on which it significantly\noutperforms existing symmetric and asymmetric multitask learning models, by\neffectively preventing negative transfer in deep feature learning.\n", "versions": [{"version": "v1", "created": "Tue, 1 Aug 2017 11:44:33 GMT"}, {"version": "v2", "created": "Wed, 13 Sep 2017 19:05:45 GMT"}, {"version": "v3", "created": "Sat, 30 Jun 2018 21:19:59 GMT"}], "update_date": "2018-07-03", "authors_parsed": [["Lee", "Hae Beom", ""], ["Yang", "Eunho", ""], ["Hwang", "Sung Ju", ""]]}, {"id": "1708.00308", "submitter": "Igor Melnyk", "authors": "Ramesh Nallapati, Igor Melnyk, Abhishek Kumar and Bowen Zhou", "title": "SenGen: Sentence Generating Neural Variational Topic Model", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a new topic model that generates documents by sampling a topic for\none whole sentence at a time, and generating the words in the sentence using an\nRNN decoder that is conditioned on the topic of the sentence. We argue that\nthis novel formalism will help us not only visualize and model the topical\ndiscourse structure in a document better, but also potentially lead to more\ninterpretable topics since we can now illustrate topics by sampling\nrepresentative sentences instead of bag of words or phrases. We present a\nvariational auto-encoder approach for learning in which we use a factorized\nvariational encoder that independently models the posterior over topical\nmixture vectors of documents using a feed-forward network, and the posterior\nover topic assignments to sentences using an RNN. Our preliminary experiments\non two different datasets indicate early promise, but also expose many\nchallenges that remain to be addressed.\n", "versions": [{"version": "v1", "created": "Tue, 1 Aug 2017 13:31:24 GMT"}], "update_date": "2017-08-03", "authors_parsed": [["Nallapati", "Ramesh", ""], ["Melnyk", "Igor", ""], ["Kumar", "Abhishek", ""], ["Zhou", "Bowen", ""]]}, {"id": "1708.00430", "submitter": "Jelena Bradic", "authors": "Yinchu Zhu and Jelena Bradic", "title": "Breaking the curse of dimensionality in regression", "comments": "51 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME cs.IT math.IT math.ST stat.ML stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Models with many signals, high-dimensional models, often impose structures on\nthe signal strengths. The common assumption is that only a few signals are\nstrong and most of the signals are zero or close (collectively) to zero.\nHowever, such a requirement might not be valid in many real-life applications.\nIn this article, we are interested in conducting large-scale inference in\nmodels that might have signals of mixed strengths. The key challenge is that\nthe signals that are not under testing might be collectively non-negligible\n(although individually small) and cannot be accurately learned. This article\ndevelops a new class of tests that arise from a moment matching formulation. A\nvirtue of these moment-matching statistics is their ability to borrow strength\nacross features, adapt to the sparsity size and exert adjustment for testing\ngrowing number of hypothesis. GRoup-level Inference of Parameter, GRIP, test\nharvests effective sparsity structures with hypothesis formulation for an\nefficient multiple testing procedure. Simulated data showcase that GRIPs error\ncontrol is far better than the alternative methods. We develop a minimax\ntheory, demonstrating optimality of GRIP for a broad range of models, including\nthose where the model is a mixture of a sparse and high-dimensional dense\nsignals.\n", "versions": [{"version": "v1", "created": "Tue, 1 Aug 2017 17:39:00 GMT"}], "update_date": "2017-08-02", "authors_parsed": [["Zhu", "Yinchu", ""], ["Bradic", "Jelena", ""]]}, {"id": "1708.00489", "submitter": "Ozan Sener", "authors": "Ozan Sener, Silvio Savarese", "title": "Active Learning for Convolutional Neural Networks: A Core-Set Approach", "comments": "ICLR 2018 Paper", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Convolutional neural networks (CNNs) have been successfully applied to many\nrecognition and learning tasks using a universal recipe; training a deep model\non a very large dataset of supervised examples. However, this approach is\nrather restrictive in practice since collecting a large set of labeled images\nis very expensive. One way to ease this problem is coming up with smart ways\nfor choosing images to be labelled from a very large collection (ie. active\nlearning).\n  Our empirical study suggests that many of the active learning heuristics in\nthe literature are not effective when applied to CNNs in batch setting.\nInspired by these limitations, we define the problem of active learning as\ncore-set selection, ie. choosing set of points such that a model learned over\nthe selected subset is competitive for the remaining data points. We further\npresent a theoretical result characterizing the performance of any selected\nsubset using the geometry of the datapoints. As an active learning algorithm,\nwe choose the subset which is expected to yield best result according to our\ncharacterization. Our experiments show that the proposed method significantly\noutperforms existing approaches in image classification experiments by a large\nmargin.\n", "versions": [{"version": "v1", "created": "Tue, 1 Aug 2017 19:50:53 GMT"}, {"version": "v2", "created": "Fri, 27 Oct 2017 20:30:22 GMT"}, {"version": "v3", "created": "Wed, 21 Feb 2018 10:55:00 GMT"}, {"version": "v4", "created": "Fri, 1 Jun 2018 10:17:23 GMT"}], "update_date": "2018-06-04", "authors_parsed": [["Sener", "Ozan", ""], ["Savarese", "Silvio", ""]]}, {"id": "1708.00495", "submitter": "Brett Israelsen", "authors": "Brett W Israelsen", "title": "\"I can assure you [$\\ldots$] that it's going to be all right\" -- A\n  definition, case for, and survey of algorithmic assurances in human-autonomy\n  trust relationships", "comments": "Copy submitted to area exam committee", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CY cs.AI cs.HC stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  As technology become more advanced, those who design, use and are otherwise\naffected by it want to know that it will perform correctly, and understand why\nit does what it does, and how to use it appropriately. In essence they want to\nbe able to trust the systems that are being designed. In this survey we present\nassurances that are the method by which users can understand how to trust this\ntechnology. Trust between humans and autonomy is reviewed, and the implications\nfor the design of assurances are highlighted. A survey of research that has\nbeen performed with respect to assurances is presented, and several key ideas\nare extracted in order to refine the definition of assurances. Several\ndirections for future research are identified and discussed.\n", "versions": [{"version": "v1", "created": "Tue, 1 Aug 2017 20:13:18 GMT"}, {"version": "v2", "created": "Mon, 4 Sep 2017 18:39:22 GMT"}], "update_date": "2017-09-07", "authors_parsed": [["Israelsen", "Brett W", ""]]}, {"id": "1708.00524", "submitter": "Bjarke Felbo", "authors": "Bjarke Felbo, Alan Mislove, Anders S{\\o}gaard, Iyad Rahwan and Sune\n  Lehmann", "title": "Using millions of emoji occurrences to learn any-domain representations\n  for detecting sentiment, emotion and sarcasm", "comments": "Accepted at EMNLP 2017. Please include EMNLP in any citations. Minor\n  changes from the EMNLP camera-ready version. 9 pages + references and\n  supplementary material", "journal-ref": "Proceedings of the 2017 Conference on Empirical Methods in Natural\n  Language Processing", "doi": "10.18653/v1/D17-1169", "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  NLP tasks are often limited by scarcity of manually annotated data. In social\nmedia sentiment analysis and related tasks, researchers have therefore used\nbinarized emoticons and specific hashtags as forms of distant supervision. Our\npaper shows that by extending the distant supervision to a more diverse set of\nnoisy labels, the models can learn richer representations. Through emoji\nprediction on a dataset of 1246 million tweets containing one of 64 common\nemojis we obtain state-of-the-art performance on 8 benchmark datasets within\nsentiment, emotion and sarcasm detection using a single pretrained model. Our\nanalyses confirm that the diversity of our emotional labels yield a performance\nimprovement over previous distant supervision approaches.\n", "versions": [{"version": "v1", "created": "Tue, 1 Aug 2017 21:28:42 GMT"}, {"version": "v2", "created": "Sat, 7 Oct 2017 19:21:48 GMT"}], "update_date": "2019-11-19", "authors_parsed": [["Felbo", "Bjarke", ""], ["Mislove", "Alan", ""], ["S\u00f8gaard", "Anders", ""], ["Rahwan", "Iyad", ""], ["Lehmann", "Sune", ""]]}, {"id": "1708.00549", "submitter": "Luke Vilnis", "authors": "Xiang Li, Luke Vilnis, Andrew McCallum", "title": "Improved Representation Learning for Predicting Commonsense Ontologies", "comments": "4 pages, ICML 2017 DeepStruct Workshop", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recent work in learning ontologies (hierarchical and partially-ordered\nstructures) has leveraged the intrinsic geometry of spaces of learned\nrepresentations to make predictions that automatically obey complex structural\nconstraints. We explore two extensions of one such model, the order-embedding\nmodel for hierarchical relation learning, with an aim towards improved\nperformance on text data for commonsense knowledge representation. Our first\nmodel jointly learns ordering relations and non-hierarchical knowledge in the\nform of raw text. Our second extension exploits the partial order structure of\nthe training data to find long-distance triplet constraints among embeddings\nwhich are poorly enforced by the pairwise training procedure. We find that both\nincorporating free text and augmented training constraints improve over the\noriginal order-embedding model and other strong baselines.\n", "versions": [{"version": "v1", "created": "Tue, 1 Aug 2017 23:05:57 GMT"}], "update_date": "2017-08-03", "authors_parsed": [["Li", "Xiang", ""], ["Vilnis", "Luke", ""], ["McCallum", "Andrew", ""]]}, {"id": "1708.00588", "submitter": "Maziar Raissi", "authors": "Maziar Raissi and George Em Karniadakis", "title": "Hidden Physics Models: Machine Learning of Nonlinear Partial\n  Differential Equations", "comments": null, "journal-ref": null, "doi": "10.1016/j.jcp.2017.11.039", "report-no": null, "categories": "cs.AI cs.LG math.AP stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  While there is currently a lot of enthusiasm about \"big data\", useful data is\nusually \"small\" and expensive to acquire. In this paper, we present a new\nparadigm of learning partial differential equations from {\\em small} data. In\nparticular, we introduce \\emph{hidden physics models}, which are essentially\ndata-efficient learning machines capable of leveraging the underlying laws of\nphysics, expressed by time dependent and nonlinear partial differential\nequations, to extract patterns from high-dimensional data generated from\nexperiments. The proposed methodology may be applied to the problem of\nlearning, system identification, or data-driven discovery of partial\ndifferential equations. Our framework relies on Gaussian processes, a powerful\ntool for probabilistic inference over functions, that enables us to strike a\nbalance between model complexity and data fitting. The effectiveness of the\nproposed approach is demonstrated through a variety of canonical problems,\nspanning a number of scientific domains, including the Navier-Stokes,\nSchr\\\"odinger, Kuramoto-Sivashinsky, and time dependent linear fractional\nequations. The methodology provides a promising new direction for harnessing\nthe long-standing developments of classical methods in applied mathematics and\nmathematical physics to design learning machines with the ability to operate in\ncomplex domains without requiring large quantities of data.\n", "versions": [{"version": "v1", "created": "Wed, 2 Aug 2017 03:28:54 GMT"}, {"version": "v2", "created": "Mon, 21 Aug 2017 22:39:46 GMT"}], "update_date": "2018-01-17", "authors_parsed": [["Raissi", "Maziar", ""], ["Karniadakis", "George Em", ""]]}, {"id": "1708.00598", "submitter": "Minhyeok Lee", "authors": "Minhyeok Lee and Junhee Seok", "title": "Controllable Generative Adversarial Network", "comments": "A fully revised version of this paper is published in IEEE Access.\n  Please refer to https://doi.org/10.1109/ACCESS.2019.2899108", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recently introduced generative adversarial network (GAN) has been shown\nnumerous promising results to generate realistic samples. The essential task of\nGAN is to control the features of samples generated from a random distribution.\nWhile the current GAN structures, such as conditional GAN, successfully\ngenerate samples with desired major features, they often fail to produce\ndetailed features that bring specific differences among samples. To overcome\nthis limitation, here we propose a controllable GAN (ControlGAN) structure. By\nseparating a feature classifier from a discriminator, the generator of\nControlGAN is designed to learn generating synthetic samples with the specific\ndetailed features. Evaluated with multiple image datasets, ControlGAN shows a\npower to generate improved samples with well-controlled features. Furthermore,\nwe demonstrate that ControlGAN can generate intermediate features and opposite\nfeatures for interpolated and extrapolated input labels that are not used in\nthe training process. It implies that ControlGAN can significantly contribute\nto the variety of generated samples.\n", "versions": [{"version": "v1", "created": "Wed, 2 Aug 2017 04:17:59 GMT"}, {"version": "v2", "created": "Tue, 12 Sep 2017 10:37:48 GMT"}, {"version": "v3", "created": "Wed, 18 Apr 2018 06:21:20 GMT"}, {"version": "v4", "created": "Tue, 1 May 2018 22:39:24 GMT"}, {"version": "v5", "created": "Sat, 30 Mar 2019 08:00:54 GMT"}], "update_date": "2019-04-02", "authors_parsed": [["Lee", "Minhyeok", ""], ["Seok", "Junhee", ""]]}, {"id": "1708.00601", "submitter": "Jonathan Jiang", "authors": "Jonathan Q. Jiang, Michael K. Ng", "title": "Exact Tensor Completion from Sparsely Corrupted Observations via Convex\n  Optimization", "comments": "36 pages, 9 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV cs.IT math.IT stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper conducts a rigorous analysis for provable estimation of\nmultidimensional arrays, in particular third-order tensors, from a random\nsubset of its corrupted entries. Our study rests heavily on a recently proposed\ntensor algebraic framework in which we can obtain tensor singular value\ndecomposition (t-SVD) that is similar to the SVD for matrices, and define a new\nnotion of tensor rank referred to as the tubal rank. We prove that by simply\nsolving a convex program, which minimizes a weighted combination of tubal\nnuclear norm, a convex surrogate for the tubal rank, and the $\\ell_1$-norm, one\ncan recover an incoherent tensor exactly with overwhelming probability,\nprovided that its tubal rank is not too large and that the corruptions are\nreasonably sparse. Interestingly, our result includes the recovery guarantees\nfor the problems of tensor completion (TC) and tensor principal component\nanalysis (TRPCA) under the same algebraic setup as special cases. An\nalternating direction method of multipliers (ADMM) algorithm is presented to\nsolve this optimization problem. Numerical experiments verify our theory and\nreal-world applications demonstrate the effectiveness of our algorithm.\n", "versions": [{"version": "v1", "created": "Wed, 2 Aug 2017 04:45:42 GMT"}], "update_date": "2017-08-03", "authors_parsed": [["Jiang", "Jonathan Q.", ""], ["Ng", "Michael K.", ""]]}, {"id": "1708.00689", "submitter": "Marco Scutari", "authors": "Marco Scutari", "title": "Dirichlet Bayesian Network Scores and the Maximum Relative Entropy\n  Principle", "comments": "20 pages, 4 figures; extended version submitted to Behaviormetrika", "journal-ref": "Journal of Machine Learning Research (73, Proceedings Track, AMBN\n  2017), 8-20; extended version in Behaviormetrika, 45(2), 337-362", "doi": null, "report-no": null, "categories": "math.ST stat.ME stat.ML stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A classic approach for learning Bayesian networks from data is to identify a\nmaximum a posteriori (MAP) network structure. In the case of discrete Bayesian\nnetworks, MAP networks are selected by maximising one of several possible\nBayesian Dirichlet (BD) scores; the most famous is the Bayesian Dirichlet\nequivalent uniform (BDeu) score from Heckerman et al (1995). The key properties\nof BDeu arise from its uniform prior over the parameters of each local\ndistribution in the network, which makes structure learning computationally\nefficient; it does not require the elicitation of prior knowledge from experts;\nand it satisfies score equivalence.\n  In this paper we will review the derivation and the properties of BD scores,\nand of BDeu in particular, and we will link them to the corresponding entropy\nestimates to study them from an information theoretic perspective. To this end,\nwe will work in the context of the foundational work of Giffin and Caticha\n(2007), who showed that Bayesian inference can be framed as a particular case\nof the maximum relative entropy principle. We will use this connection to show\nthat BDeu should not be used for structure learning from sparse data, since it\nviolates the maximum relative entropy principle; and that it is also\nproblematic from a more classic Bayesian model selection perspective, because\nit produces Bayes factors that are sensitive to the value of its only\nhyperparameter. Using a large simulation study, we found in our previous work\n(Scutari, 2016) that the Bayesian Dirichlet sparse (BDs) score seems to provide\nbetter accuracy in structure learning; in this paper we further show that BDs\ndoes not suffer from the issues above, and we recommend to use it for sparse\ndata instead of BDeu. Finally, will show that these issues are in fact\ndifferent aspects of the same problem and a consequence of the distributional\nassumptions of the prior.\n", "versions": [{"version": "v1", "created": "Wed, 2 Aug 2017 10:30:21 GMT"}, {"version": "v2", "created": "Wed, 16 Aug 2017 21:29:36 GMT"}, {"version": "v3", "created": "Thu, 24 Aug 2017 22:28:50 GMT"}, {"version": "v4", "created": "Thu, 25 Jan 2018 09:27:57 GMT"}, {"version": "v5", "created": "Mon, 5 Mar 2018 11:34:07 GMT"}], "update_date": "2018-12-13", "authors_parsed": [["Scutari", "Marco", ""]]}, {"id": "1708.00707", "submitter": "Henri Vuollekoski", "authors": "Jarno Lintusaari, Henri Vuollekoski, Antti Kangasr\\\"a\\\"asi\\\"o, Kusti\n  Skyt\\'en, Marko J\\\"arvenp\\\"a\\\"a, Pekka Marttinen, Michael U. Gutmann, Aki\n  Vehtari, Jukka Corander, and Samuel Kaski", "title": "ELFI: Engine for Likelihood-Free Inference", "comments": null, "journal-ref": "Journal of Machine Learning Research, 19(16):1-7, 2018.\n  http://jmlr.org/papers/v19/17-374.html", "doi": null, "report-no": null, "categories": "stat.ML cs.MS stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Engine for Likelihood-Free Inference (ELFI) is a Python software library for\nperforming likelihood-free inference (LFI). ELFI provides a convenient syntax\nfor arranging components in LFI, such as priors, simulators, summaries or\ndistances, to a network called ELFI graph. The components can be implemented in\na wide variety of languages. The stand-alone ELFI graph can be used with any of\nthe available inference methods without modifications. A central method\nimplemented in ELFI is Bayesian Optimization for Likelihood-Free Inference\n(BOLFI), which has recently been shown to accelerate likelihood-free inference\nup to several orders of magnitude by surrogate-modelling the distance. ELFI\nalso has an inbuilt support for output data storing for reuse and analysis, and\nsupports parallelization of computation from multiple cores up to a cluster\nenvironment. ELFI is designed to be extensible and provides interfaces for\nwidening its functionality. This makes the adding of new inference methods to\nELFI straightforward and automatically compatible with the inbuilt features.\n", "versions": [{"version": "v1", "created": "Wed, 2 Aug 2017 11:39:11 GMT"}, {"version": "v2", "created": "Fri, 25 May 2018 13:04:57 GMT"}, {"version": "v3", "created": "Thu, 5 Jul 2018 08:34:27 GMT"}], "update_date": "2018-10-15", "authors_parsed": [["Lintusaari", "Jarno", ""], ["Vuollekoski", "Henri", ""], ["Kangasr\u00e4\u00e4si\u00f6", "Antti", ""], ["Skyt\u00e9n", "Kusti", ""], ["J\u00e4rvenp\u00e4\u00e4", "Marko", ""], ["Marttinen", "Pekka", ""], ["Gutmann", "Michael U.", ""], ["Vehtari", "Aki", ""], ["Corander", "Jukka", ""], ["Kaski", "Samuel", ""]]}, {"id": "1708.00754", "submitter": "Indre Zliobaite", "authors": "Indre Zliobaite", "title": "Fairness-aware machine learning: a perspective", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.CY cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Algorithms learned from data are increasingly used for deciding many aspects\nin our life: from movies we see, to prices we pay, or medicine we get. Yet\nthere is growing evidence that decision making by inappropriately trained\nalgorithms may unintentionally discriminate people. For example, in automated\nmatching of candidate CVs with job descriptions, algorithms may capture and\npropagate ethnicity related biases. Several repairs for selected algorithms\nhave already been proposed, but the underlying mechanisms how such\ndiscrimination happens from the computational perspective are not yet\nscientifically understood. We need to develop theoretical understanding how\nalgorithms may become discriminatory, and establish fundamental machine\nlearning principles for prevention. We need to analyze machine learning process\nas a whole to systematically explain the roots of discrimination occurrence,\nwhich will allow to devise global machine learning optimization criteria for\nguaranteed prevention, as opposed to pushing empirical constraints into\nexisting algorithms case-by-case. As a result, the state-of-the-art will\nadvance from heuristic repairing, to proactive and theoretically supported\nprevention. This is needed not only because law requires to protect vulnerable\npeople. Penetration of big data initiatives will only increase, and computer\nscience needs to provide solid explanations and accountability to the public,\nbefore public concerns lead to unnecessarily restrictive regulations against\nmachine learning.\n", "versions": [{"version": "v1", "created": "Wed, 2 Aug 2017 14:14:49 GMT"}], "update_date": "2017-08-03", "authors_parsed": [["Zliobaite", "Indre", ""]]}, {"id": "1708.00768", "submitter": "Odalric-Ambrym Maillard", "authors": "Audrey Durand, Odalric-Ambrym Maillard, Joelle Pineau", "title": "Streaming kernel regression with provably adaptive mean, variance, and\n  regularization", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider the problem of streaming kernel regression, when the observations\narrive sequentially and the goal is to recover the underlying mean function,\nassumed to belong to an RKHS. The variance of the noise is not assumed to be\nknown. In this context, we tackle the problem of tuning the regularization\nparameter adaptively at each time step, while maintaining tight confidence\nbounds estimates on the value of the mean function at each point. To this end,\nwe first generalize existing results for finite-dimensional linear regression\nwith fixed regularization and known variance to the kernel setup with a\nregularization parameter allowed to be a measurable function of past\nobservations. Then, using appropriate self-normalized inequalities we build\nupper and lower bound estimates for the variance, leading to Bersntein-like\nconcentration bounds. The later is used in order to define the adaptive\nregularization. The bounds resulting from our technique are valid uniformly\nover all observation points and all time steps, and are compared against the\nliterature with numerical experiments. Finally, the potential of these tools is\nillustrated by an application to kernelized bandits, where we revisit the\nKernel UCB and Kernel Thompson Sampling procedures, and show the benefits of\nthe novel adaptive kernel tuning strategy.\n", "versions": [{"version": "v1", "created": "Wed, 2 Aug 2017 14:38:48 GMT"}], "update_date": "2017-08-03", "authors_parsed": [["Durand", "Audrey", ""], ["Maillard", "Odalric-Ambrym", ""], ["Pineau", "Joelle", ""]]}, {"id": "1708.00847", "submitter": "Piotr Zwiernik", "authors": "Piotr Zwiernik", "title": "Latent tree models", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.ML stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Latent tree models are graphical models defined on trees, in which only a\nsubset of variables is observed. They were first discussed by Judea Pearl as\ntree-decomposable distributions to generalise star-decomposable distributions\nsuch as the latent class model. Latent tree models, or their submodels, are\nwidely used in: phylogenetic analysis, network tomography, computer vision,\ncausal modeling, and data clustering. They also contain other well-known\nclasses of models like hidden Markov models, Brownian motion tree model, the\nIsing model on a tree, and many popular models used in phylogenetics. This\narticle offers a concise introduction to the theory of latent tree models. We\nemphasise the role of tree metrics in the structural description of this model\nclass, in designing learning algorithms, and in understanding fundamental\nlimits of what and when can be learned.\n", "versions": [{"version": "v1", "created": "Wed, 2 Aug 2017 17:43:29 GMT"}], "update_date": "2017-08-03", "authors_parsed": [["Zwiernik", "Piotr", ""]]}, {"id": "1708.00909", "submitter": "Joshua Glaser", "authors": "Joshua I. Glaser, Ari S. Benjamin, Raeed H. Chowdhury, Matthew G.\n  Perich, Lee E. Miller, Konrad P. Kording", "title": "Machine learning for neural decoding", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "q-bio.NC cs.LG stat.ML", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Despite rapid advances in machine learning tools, the majority of neural\ndecoding approaches still use traditional methods. Modern machine learning\ntools, which are versatile and easy to use, have the potential to significantly\nimprove decoding performance. This tutorial describes how to effectively apply\nthese algorithms for typical decoding problems. We provide descriptions, best\npractices, and code for applying common machine learning methods, including\nneural networks and gradient boosting. We also provide detailed comparisons of\nthe performance of various methods at the task of decoding spiking activity in\nmotor cortex, somatosensory cortex, and hippocampus. Modern methods,\nparticularly neural networks and ensembles, significantly outperform\ntraditional approaches, such as Wiener and Kalman filters. Improving the\nperformance of neural decoding algorithms allows neuroscientists to better\nunderstand the information contained in a neural population and can help\nadvance engineering applications such as brain machine interfaces.\n", "versions": [{"version": "v1", "created": "Wed, 2 Aug 2017 19:53:22 GMT"}, {"version": "v2", "created": "Fri, 4 May 2018 16:58:31 GMT"}, {"version": "v3", "created": "Fri, 20 Sep 2019 02:46:47 GMT"}, {"version": "v4", "created": "Fri, 3 Jul 2020 15:25:31 GMT"}], "update_date": "2020-07-06", "authors_parsed": [["Glaser", "Joshua I.", ""], ["Benjamin", "Ari S.", ""], ["Chowdhury", "Raeed H.", ""], ["Perich", "Matthew G.", ""], ["Miller", "Lee E.", ""], ["Kording", "Konrad P.", ""]]}, {"id": "1708.00955", "submitter": "Matias Quiroz", "authors": "Khue-Dung Dang, Matias Quiroz, Robert Kohn, Minh-Ngoc Tran, Mattias\n  Villani", "title": "Hamiltonian Monte Carlo with Energy Conserving Subsampling", "comments": "Includes an experiment on the scalability of the method. Text has\n  been revised too", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.CO stat.ME stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Hamiltonian Monte Carlo (HMC) samples efficiently from high-dimensional\nposterior distributions with proposed parameter draws obtained by iterating on\na discretized version of the Hamiltonian dynamics. The iterations make HMC\ncomputationally costly, especially in problems with large datasets, since it is\nnecessary to compute posterior densities and their derivatives with respect to\nthe parameters. Naively computing the Hamiltonian dynamics on a subset of the\ndata causes HMC to lose its key ability to generate distant parameter proposals\nwith high acceptance probability. The key insight in our article is that\nefficient subsampling HMC for the parameters is possible if both the dynamics\nand the acceptance probability are computed from the same data subsample in\neach complete HMC iteration. We show that this is possible to do in a\nprincipled way in a HMC-within-Gibbs framework where the subsample is updated\nusing a pseudo marginal MH step and the parameters are then updated using an\nHMC step, based on the current subsample. We show that our subsampling methods\nare fast and compare favorably to two popular sampling algorithms that utilize\ngradient estimates from data subsampling. We also explore the current\nlimitations of subsampling HMC algorithms by varying the quality of the\nvariance reducing control variates used in the estimators of the posterior\ndensity and its gradients.\n", "versions": [{"version": "v1", "created": "Wed, 2 Aug 2017 23:19:07 GMT"}, {"version": "v2", "created": "Wed, 5 Dec 2018 01:33:52 GMT"}, {"version": "v3", "created": "Thu, 2 May 2019 00:02:16 GMT"}], "update_date": "2019-05-03", "authors_parsed": [["Dang", "Khue-Dung", ""], ["Quiroz", "Matias", ""], ["Kohn", "Robert", ""], ["Tran", "Minh-Ngoc", ""], ["Villani", "Mattias", ""]]}, {"id": "1708.00983", "submitter": "Aliasghar Mortazi", "authors": "Aliasghar Mortazi, Jeremy Burt, Ulas Bagci", "title": "Multi-Planar Deep Segmentation Networks for Cardiac Substructures from\n  MRI and CT", "comments": "The paper is accepted to STACOM 2017", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Non-invasive detection of cardiovascular disorders from radiology scans\nrequires quantitative image analysis of the heart and its substructures. There\nare well-established measurements that radiologists use for diseases assessment\nsuch as ejection fraction, volume of four chambers, and myocardium mass. These\nmeasurements are derived as outcomes of precise segmentation of the heart and\nits substructures. The aim of this paper is to provide such measurements\nthrough an accurate image segmentation algorithm that automatically delineates\nseven substructures of the heart from MRI and/or CT scans. Our proposed method\nis based on multi-planar deep convolutional neural networks (CNN) with an\nadaptive fusion strategy where we automatically utilize complementary\ninformation from different planes of the 3D scans for improved delineations.\nFor CT and MRI, we have separately designed three CNNs (the same architectural\nconfiguration) for three planes, and have trained the networks from scratch for\nvoxel-wise labeling for the following cardiac structures: myocardium of left\nventricle (Myo), left atrium (LA), left ventricle (LV), right atrium (RA),\nright ventricle (RV), ascending aorta (Ao), and main pulmonary artery (PA). We\nhave evaluated the proposed method with 4-fold-cross validation on the\nmulti-modality whole heart segmentation challenge (MM-WHS 2017) dataset. The\nprecision and dice index of 0.93 and 0.90, and 0.87 and 0.85 were achieved for\nCT and MR images, respectively. While a CT volume was segmented about 50\nseconds, an MRI scan was segmented around 17 seconds with the GPUs/CUDA\nimplementation.\n", "versions": [{"version": "v1", "created": "Thu, 3 Aug 2017 03:30:03 GMT"}], "update_date": "2017-08-04", "authors_parsed": [["Mortazi", "Aliasghar", ""], ["Burt", "Jeremy", ""], ["Bagci", "Ulas", ""]]}, {"id": "1708.00994", "submitter": "Katri Pulliyakode Saishankar", "authors": "Saishankar Katri Pulliyakode and Sheetal Kalyani", "title": "Reinforcement learning techniques for Outer Loop Link Adaptation in\n  4G/5G systems", "comments": "There is a patent submission based on this work", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.IT math.IT", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Wireless systems perform rate adaptation to transmit at highest possible\ninstantaneous rates. Rate adaptation has been increasingly granular over\ngenerations of wireless systems. The base-station uses SINR and packet decode\nfeedback called acknowledgement/no acknowledgement (ACK/NACK) to perform rate\nadaptation. SINR is used for rate anchoring called inner look adaptation and\nACK/NACK is used for fine offset adjustments called Outer Loop Link Adaptation\n(OLLA). We cast the OLLA as a reinforcement learning problem of the class of\nMulti-Armed Bandits (MAB) where the different offset values are the arms of the\nbandit. In OLLA, as the offset values increase, the probability of packet error\nalso increase, and every user equipment (UE) has a desired Block Error Rate\n(BLER) to meet certain Quality of Service (QoS) requirements. For this MAB we\npropose a binary search based algorithm which achieves a Probably Approximately\nCorrect (PAC) solution making use of bounds from large deviation theory and\nconfidence bounds. In addition to this we also discuss how a Thompson sampling\nor UCB based method will not help us meet the target objectives. Finally,\nsimulation results are provided on an LTE system simulator and thereby prove\nthe efficacy of our proposed algorithm.\n", "versions": [{"version": "v1", "created": "Thu, 3 Aug 2017 04:33:57 GMT"}], "update_date": "2017-08-04", "authors_parsed": [["Pulliyakode", "Saishankar Katri", ""], ["Kalyani", "Sheetal", ""]]}, {"id": "1708.01012", "submitter": "Fan Zhou", "authors": "Fan Zhou and Guojing Cong", "title": "On the convergence properties of a $K$-step averaging stochastic\n  gradient descent algorithm for nonconvex optimization", "comments": null, "journal-ref": null, "doi": "10.24963/ijcai.2018/447", "report-no": null, "categories": "cs.LG cs.DC stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Despite their popularity, the practical performance of asynchronous\nstochastic gradient descent methods (ASGD) for solving large scale machine\nlearning problems are not as good as theoretical results indicate. We adopt and\nanalyze a synchronous K-step averaging stochastic gradient descent algorithm\nwhich we call K-AVG. We establish the convergence results of K-AVG for\nnonconvex objectives and explain why the K-step delay is necessary and leads to\nbetter performance than traditional parallel stochastic gradient descent which\nis a special case of K-AVG with $K=1$. We also show that K-AVG scales better\nthan ASGD. Another advantage of K-AVG over ASGD is that it allows larger\nstepsizes. On a cluster of $128$ GPUs, K-AVG is faster than ASGD\nimplementations and achieves better accuracies and faster convergence for\n\\cifar dataset.\n", "versions": [{"version": "v1", "created": "Thu, 3 Aug 2017 06:18:36 GMT"}, {"version": "v2", "created": "Tue, 28 Nov 2017 20:28:04 GMT"}, {"version": "v3", "created": "Wed, 16 May 2018 22:38:30 GMT"}], "update_date": "2019-11-05", "authors_parsed": [["Zhou", "Fan", ""], ["Cong", "Guojing", ""]]}, {"id": "1708.01104", "submitter": "Andreas Holzinger", "authors": "Andreas Holzinger, Markus Plass, Katharina Holzinger, Gloria Cerasela\n  Crisan, Camelia-M. Pintea, Vasile Palade", "title": "A glass-box interactive machine learning approach for solving NP-hard\n  problems with the human-in-the-loop", "comments": "26 pages, 5 figures", "journal-ref": "CREAT.MATH.INFORM. 28(2) (2019) 121-134", "doi": null, "report-no": null, "categories": "cs.AI stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The goal of Machine Learning to automatically learn from data, extract\nknowledge and to make decisions without any human intervention. Such automatic\n(aML) approaches show impressive success. Recent results even demonstrate\nintriguingly that deep learning applied for automatic classification of skin\nlesions is on par with the performance of dermatologists, yet outperforms the\naverage. As human perception is inherently limited, such approaches can\ndiscover patterns, e.g. that two objects are similar, in arbitrarily\nhigh-dimensional spaces what no human is able to do. Humans can deal only with\nlimited amounts of data, whilst big data is beneficial for aML; however, in\nhealth informatics, we are often confronted with a small number of data sets,\nwhere aML suffer of insufficient training samples and many problems are\ncomputationally hard. Here, interactive machine learning (iML) may be of help,\nwhere a human-in-the-loop contributes to reduce the complexity of NP-hard\nproblems. A further motivation for iML is that standard black-box approaches\nlack transparency, hence do not foster trust and acceptance of ML among\nend-users. Rising legal and privacy aspects, e.g. with the new European General\nData Protection Regulations, make black-box approaches difficult to use,\nbecause they often are not able to explain why a decision has been made. In\nthis paper, we present some experiments to demonstrate the effectiveness of the\nhuman-in-the-loop approach, particularly in opening the black-box to a\nglass-box and thus enabling a human directly to interact with an learning\nalgorithm. We selected the Ant Colony Optimization framework, and applied it on\nthe Traveling Salesman Problem, which is a good example, due to its relevance\nfor health informatics, e.g. for the study of protein folding. From studies of\nhow humans extract so much from so little data, fundamental ML-research also\nmay benefit.\n", "versions": [{"version": "v1", "created": "Thu, 3 Aug 2017 11:33:10 GMT"}], "update_date": "2021-03-16", "authors_parsed": [["Holzinger", "Andreas", ""], ["Plass", "Markus", ""], ["Holzinger", "Katharina", ""], ["Crisan", "Gloria Cerasela", ""], ["Pintea", "Camelia-M.", ""], ["Palade", "Vasile", ""]]}, {"id": "1708.01198", "submitter": "Jithin George", "authors": "Jithin Donny George, Ronan Keane and Conor Zellmer", "title": "Estimating speech from lip dynamics", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV stat.CO stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The goal of this project is to develop a limited lip reading algorithm for a\nsubset of the English language. We consider a scenario in which no audio\ninformation is available. The raw video is processed and the position of the\nlips in each frame is extracted. We then prepare the lip data for processing\nand classify the lips into visemes and phonemes. Hidden Markov Models are used\nto predict the words the speaker is saying based on the sequences of classified\nphonemes and visemes. The GRID audiovisual sentence corpus [10][11] database is\nused for our study.\n", "versions": [{"version": "v1", "created": "Thu, 3 Aug 2017 16:23:13 GMT"}], "update_date": "2017-08-04", "authors_parsed": [["George", "Jithin Donny", ""], ["Keane", "Ronan", ""], ["Zellmer", "Conor", ""]]}, {"id": "1708.01206", "submitter": "Andrey Kormilitzin", "authors": "Andrey Kormilitzin, Kate E.A. Saunders, Paul J. Harrison, John R.\n  Geddes, Terry Lyons", "title": "Detecting early signs of depressive and manic episodes in patients with\n  bipolar disorder using the signature-based model", "comments": "12 pages, 3 tables, 10 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recurrent major mood episodes and subsyndromal mood instability cause\nsubstantial disability in patients with bipolar disorder. Early identification\nof mood episodes enabling timely mood stabilisation is an important clinical\ngoal. Recent technological advances allow the prospective reporting of mood in\nreal time enabling more accurate, efficient data capture. The complex nature of\nthese data streams in combination with challenge of deriving meaning from\nmissing data mean pose a significant analytic challenge. The signature method\nis derived from stochastic analysis and has the ability to capture important\nproperties of complex ordered time series data. To explore whether the onset of\nepisodes of mania and depression can be identified using self-reported mood\ndata.\n", "versions": [{"version": "v1", "created": "Thu, 3 Aug 2017 16:48:35 GMT"}], "update_date": "2017-08-04", "authors_parsed": [["Kormilitzin", "Andrey", ""], ["Saunders", "Kate E. A.", ""], ["Harrison", "Paul J.", ""], ["Geddes", "John R.", ""], ["Lyons", "Terry", ""]]}, {"id": "1708.01289", "submitter": "Jules Pondard", "authors": "Valentin Thomas, Jules Pondard, Emmanuel Bengio, Marc Sarfati,\n  Philippe Beaudoin, Marie-Jean Meurs, Joelle Pineau, Doina Precup, Yoshua\n  Bengio", "title": "Independently Controllable Factors", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  It has been postulated that a good representation is one that disentangles\nthe underlying explanatory factors of variation. However, it remains an open\nquestion what kind of training framework could potentially achieve that.\nWhereas most previous work focuses on the static setting (e.g., with images),\nwe postulate that some of the causal factors could be discovered if the learner\nis allowed to interact with its environment. The agent can experiment with\ndifferent actions and observe their effects. More specifically, we hypothesize\nthat some of these factors correspond to aspects of the environment which are\nindependently controllable, i.e., that there exists a policy and a learnable\nfeature for each such aspect of the environment, such that this policy can\nyield changes in that feature with minimal changes to other features that\nexplain the statistical variations in the observed data. We propose a specific\nobjective function to find such factors and verify experimentally that it can\nindeed disentangle independently controllable aspects of the environment\nwithout any extrinsic reward signal.\n", "versions": [{"version": "v1", "created": "Thu, 3 Aug 2017 19:32:33 GMT"}, {"version": "v2", "created": "Fri, 25 Aug 2017 22:18:11 GMT"}], "update_date": "2017-08-29", "authors_parsed": [["Thomas", "Valentin", ""], ["Pondard", "Jules", ""], ["Bengio", "Emmanuel", ""], ["Sarfati", "Marc", ""], ["Beaudoin", "Philippe", ""], ["Meurs", "Marie-Jean", ""], ["Pineau", "Joelle", ""], ["Precup", "Doina", ""], ["Bengio", "Yoshua", ""]]}, {"id": "1708.01383", "submitter": "Bicheng Ying", "authors": "Bicheng Ying and Kun Yuan and Ali H. Sayed", "title": "Variance-Reduced Stochastic Learning under Random Reshuffling", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG math.OC stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Several useful variance-reduced stochastic gradient algorithms, such as SVRG,\nSAGA, Finito, and SAG, have been proposed to minimize empirical risks with\nlinear convergence properties to the exact minimizer. The existing convergence\nresults assume uniform data sampling with replacement. However, it has been\nobserved in related works that random reshuffling can deliver superior\nperformance over uniform sampling and, yet, no formal proofs or guarantees of\nexact convergence exist for variance-reduced algorithms under random\nreshuffling. This paper makes two contributions. First, it resolves this open\nissue and provides the first theoretical guarantee of linear convergence under\nrandom reshuffling for SAGA; the argument is also adaptable to other\nvariance-reduced algorithms. Second, under random reshuffling, the paper\nproposes a new amortized variance-reduced gradient (AVRG) algorithm with\nconstant storage requirements compared to SAGA and with balanced gradient\ncomputations compared to SVRG. AVRG is also shown analytically to converge\nlinearly.\n", "versions": [{"version": "v1", "created": "Fri, 4 Aug 2017 05:43:33 GMT"}, {"version": "v2", "created": "Sun, 20 Aug 2017 04:40:53 GMT"}, {"version": "v3", "created": "Fri, 16 Feb 2018 15:35:32 GMT"}], "update_date": "2018-02-19", "authors_parsed": [["Ying", "Bicheng", ""], ["Yuan", "Kun", ""], ["Sayed", "Ali H.", ""]]}, {"id": "1708.01384", "submitter": "Kun Yuan", "authors": "Kun Yuan, Bicheng Ying, Jiageng Liu, and Ali H. Sayed", "title": "Variance-Reduced Stochastic Learning by Networked Agents under Random\n  Reshuffling", "comments": "23 pages, 12 figures, submitted for publication", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG math.OC stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A new amortized variance-reduced gradient (AVRG) algorithm was developed in\n\\cite{ying2017convergence}, which has constant storage requirement in\ncomparison to SAGA and balanced gradient computations in comparison to SVRG.\nOne key advantage of the AVRG strategy is its amenability to decentralized\nimplementations. In this work, we show how AVRG can be extended to the network\ncase where multiple learning agents are assumed to be connected by a graph\ntopology. In this scenario, each agent observes data that is spatially\ndistributed and all agents are only allowed to communicate with direct\nneighbors. Moreover, the amount of data observed by the individual agents may\ndiffer drastically. For such situations, the balanced gradient computation\nproperty of AVRG becomes a real advantage in reducing idle time caused by\nunbalanced local data storage requirements, which is characteristic of other\nreduced-variance gradient algorithms. The resulting diffusion-AVRG algorithm is\nshown to have linear convergence to the exact solution, and is much more memory\nefficient than other alternative algorithms. In addition, we propose a\nmini-batch strategy to balance the communication and computation efficiency for\ndiffusion-AVRG. When a proper batch size is employed, it is observed in\nsimulations that diffusion-AVRG is more computationally efficient than exact\ndiffusion or EXTRA while maintaining almost the same communication efficiency.\n", "versions": [{"version": "v1", "created": "Fri, 4 Aug 2017 05:43:38 GMT"}, {"version": "v2", "created": "Sat, 23 Dec 2017 03:50:42 GMT"}, {"version": "v3", "created": "Tue, 29 May 2018 15:19:44 GMT"}], "update_date": "2018-05-30", "authors_parsed": [["Yuan", "Kun", ""], ["Ying", "Bicheng", ""], ["Liu", "Jiageng", ""], ["Sayed", "Ali H.", ""]]}, {"id": "1708.01432", "submitter": "Tiago Peixoto", "authors": "Tiago P. Peixoto", "title": "Nonparametric weighted stochastic block models", "comments": "19 pages, 11 figures. Code is freely available as part of graph-tool\n  at https://graph-tool.skewed.de . See also the HOWTO at\n  https://graph-tool.skewed.de/static/doc/demos/inference/inference.html", "journal-ref": "Phys. Rev. E 97, 012306 (2018)", "doi": "10.1103/PhysRevE.97.012306", "report-no": null, "categories": "stat.ML physics.data-an physics.soc-ph", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  We present a Bayesian formulation of weighted stochastic block models that\ncan be used to infer the large-scale modular structure of weighted networks,\nincluding their hierarchical organization. Our method is nonparametric, and\nthus does not require the prior knowledge of the number of groups or other\ndimensions of the model, which are instead inferred from data. We give a\ncomprehensive treatment of different kinds of edge weights (i.e. continuous or\ndiscrete, signed or unsigned, bounded or unbounded), as well as arbitrary\nweight transformations, and describe an unsupervised model selection approach\nto choose the best network description. We illustrate the application of our\nmethod to a variety of empirical weighted networks, such as global migrations,\nvoting patterns in congress, and neural connections in the human brain.\n", "versions": [{"version": "v1", "created": "Fri, 4 Aug 2017 09:42:32 GMT"}, {"version": "v2", "created": "Wed, 20 Sep 2017 09:51:37 GMT"}, {"version": "v3", "created": "Thu, 18 Jan 2018 05:16:06 GMT"}, {"version": "v4", "created": "Fri, 19 Jan 2018 03:58:26 GMT"}], "update_date": "2018-01-24", "authors_parsed": [["Peixoto", "Tiago P.", ""]]}, {"id": "1708.01440", "submitter": "Giulia Bert\\`o", "authors": "Emanuele Olivetti, Giulia Bert\\`o, Pietro Gori, Nusrat Sharmin, Paolo\n  Avesani", "title": "Comparison of Distances for Supervised Segmentation of White Matter\n  Tractography", "comments": null, "journal-ref": null, "doi": "10.1109/PRNI.2017.7981502", "report-no": null, "categories": "stat.ML q-bio.QM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Tractograms are mathematical representations of the main paths of axons\nwithin the white matter of the brain, from diffusion MRI data. Such\nrepresentations are in the form of polylines, called streamlines, and one\nstreamline approximates the common path of tens of thousands of axons. The\nanalysis of tractograms is a task of interest in multiple fields, like\nneurosurgery and neurology. A basic building block of many pipelines of\nanalysis is the definition of a distance function between streamlines. Multiple\ndistance functions have been proposed in the literature, and different authors\nuse different distances, usually without a specific reason other than invoking\nthe \"common practice\". To this end, in this work we want to test such common\npractices, in order to obtain factual reasons for choosing one distance over\nanother. For these reasons, in this work we compare many streamline distance\nfunctions available in the literature. We focus on the common task of automatic\nbundle segmentation and we adopt the recent approach of supervised segmentation\nfrom expert-based examples. Using the HCP dataset, we compare several distances\nobtaining guidelines on the choice of which distance function one should use\nfor supervised bundle segmentation.\n", "versions": [{"version": "v1", "created": "Fri, 4 Aug 2017 10:18:19 GMT"}], "update_date": "2017-08-07", "authors_parsed": [["Olivetti", "Emanuele", ""], ["Bert\u00f2", "Giulia", ""], ["Gori", "Pietro", ""], ["Sharmin", "Nusrat", ""], ["Avesani", "Paolo", ""]]}, {"id": "1708.01519", "submitter": "Mehran Safayani", "authors": "Mehran Safayani and Saeid Momenzadeh", "title": "A Latent Variable Model for Two-Dimensional Canonical Correlation\n  Analysis and its Variational Inference", "comments": null, "journal-ref": null, "doi": "10.1007/s00500-020-04906-8", "report-no": null, "categories": "cs.CV cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Describing the dimension reduction (DR) techniques by means of probabilistic\nmodels has recently been given special attention. Probabilistic models, in\naddition to a better interpretability of the DR methods, provide a framework\nfor further extensions of such algorithms. One of the new approaches to the\nprobabilistic DR methods is to preserving the internal structure of data. It is\nmeant that it is not necessary that the data first be converted from the matrix\nor tensor format to the vector format in the process of dimensionality\nreduction. In this paper, a latent variable model for matrix-variate data for\ncanonical correlation analysis (CCA) is proposed. Since in general there is not\nany analytical maximum likelihood solution for this model, we present two\napproaches for learning the parameters. The proposed methods are evaluated\nusing the synthetic data in terms of convergence and quality of mappings. Also,\nreal data set is employed for assessing the proposed methods with several\nprobabilistic and none-probabilistic CCA based approaches. The results confirm\nthe superiority of the proposed methods with respect to the competing\nalgorithms. Moreover, this model can be considered as a framework for further\nextensions.\n", "versions": [{"version": "v1", "created": "Fri, 4 Aug 2017 14:16:25 GMT"}], "update_date": "2020-05-12", "authors_parsed": [["Safayani", "Mehran", ""], ["Momenzadeh", "Saeid", ""]]}, {"id": "1708.01529", "submitter": "Michalis Titsias", "authors": "Michalis K. Titsias", "title": "Learning Model Reparametrizations: Implicit Variational Inference by\n  Fitting MCMC distributions", "comments": "16 pages, 6 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We introduce a new algorithm for approximate inference that combines\nreparametrization, Markov chain Monte Carlo and variational methods. We\nconstruct a very flexible implicit variational distribution synthesized by an\narbitrary Markov chain Monte Carlo operation and a deterministic transformation\nthat can be optimized using the reparametrization trick. Unlike current methods\nfor implicit variational inference, our method avoids the computation of log\ndensity ratios and therefore it is easily applicable to arbitrary continuous\nand differentiable models. We demonstrate the proposed algorithm for fitting\nbanana-shaped distributions and for training variational autoencoders.\n", "versions": [{"version": "v1", "created": "Fri, 4 Aug 2017 14:50:03 GMT"}], "update_date": "2017-08-07", "authors_parsed": [["Titsias", "Michalis K.", ""]]}, {"id": "1708.01648", "submitter": "Chuhang Zou", "authors": "Chuhang Zou, Ersin Yumer, Jimei Yang, Duygu Ceylan, Derek Hoiem", "title": "3D-PRNN: Generating Shape Primitives with Recurrent Neural Networks", "comments": "ICCV 2017", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The success of various applications including robotics, digital content\ncreation, and visualization demand a structured and abstract representation of\nthe 3D world from limited sensor data. Inspired by the nature of human\nperception of 3D shapes as a collection of simple parts, we explore such an\nabstract shape representation based on primitives. Given a single depth image\nof an object, we present 3D-PRNN, a generative recurrent neural network that\nsynthesizes multiple plausible shapes composed of a set of primitives. Our\ngenerative model encodes symmetry characteristics of common man-made objects,\npreserves long-range structural coherence, and describes objects of varying\ncomplexity with a compact representation. We also propose a method based on\nGaussian Fields to generate a large scale dataset of primitive-based shape\nrepresentations to train our network. We evaluate our approach on a wide range\nof examples and show that it outperforms nearest-neighbor based shape retrieval\nmethods and is on-par with voxel-based generative models while using a\nsignificantly reduced parameter space.\n", "versions": [{"version": "v1", "created": "Fri, 4 Aug 2017 19:30:13 GMT"}], "update_date": "2017-08-08", "authors_parsed": [["Zou", "Chuhang", ""], ["Yumer", "Ersin", ""], ["Yang", "Jimei", ""], ["Ceylan", "Duygu", ""], ["Hoiem", "Derek", ""]]}, {"id": "1708.01666", "submitter": "Jie Cao", "authors": "Yang Jiang, Zeyang Dou, Qun Hao, Jie Cao, Kun Gao, Xi Chen", "title": "An Effective Training Method For Deep Convolutional Neural Network", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we propose the nonlinearity generation method to speed up and\nstabilize the training of deep convolutional neural networks. The proposed\nmethod modifies a family of activation functions as nonlinearity generators\n(NGs). NGs make the activation functions linear symmetric for their inputs to\nlower model capacity, and automatically introduce nonlinearity to enhance the\ncapacity of the model during training. The proposed method can be considered an\nunusual form of regularization: the model parameters are obtained by training a\nrelatively low-capacity model, that is relatively easy to optimize at the\nbeginning, with only a few iterations, and these parameters are reused for the\ninitialization of a higher-capacity model. We derive the upper and lower bounds\nof variance of the weight variation, and show that the initial symmetric\nstructure of NGs helps stabilize training. We evaluate the proposed method on\ndifferent frameworks of convolutional neural networks over two object\nrecognition benchmark tasks (CIFAR-10 and CIFAR-100). Experimental results\nshowed that the proposed method allows us to (1) speed up the convergence of\ntraining, (2) allow for less careful weight initialization, (3) improve or at\nleast maintain the performance of the model at negligible extra computational\ncost, and (4) easily train a very deep model.\n", "versions": [{"version": "v1", "created": "Mon, 31 Jul 2017 23:19:03 GMT"}, {"version": "v2", "created": "Sun, 13 Aug 2017 14:41:04 GMT"}, {"version": "v3", "created": "Mon, 21 Aug 2017 15:45:11 GMT"}, {"version": "v4", "created": "Tue, 10 Oct 2017 08:58:03 GMT"}, {"version": "v5", "created": "Tue, 17 Oct 2017 15:53:20 GMT"}], "update_date": "2017-10-18", "authors_parsed": [["Jiang", "Yang", ""], ["Dou", "Zeyang", ""], ["Hao", "Qun", ""], ["Cao", "Jie", ""], ["Gao", "Kun", ""], ["Chen", "Xi", ""]]}, {"id": "1708.01677", "submitter": "Martin Gerlach", "authors": "Martin Gerlach and Tiago P. Peixoto and Eduardo G. Altmann", "title": "A network approach to topic models", "comments": "22 pages, 10 figures, code available at https://topsbm.github.io/", "journal-ref": "Science Advances 4, eaaq1360 (2018)", "doi": "10.1126/sciadv.aaq1360", "report-no": null, "categories": "stat.ML cs.CL physics.data-an physics.soc-ph", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  One of the main computational and scientific challenges in the modern age is\nto extract useful information from unstructured texts. Topic models are one\npopular machine-learning approach which infers the latent topical structure of\na collection of documents. Despite their success --- in particular of its most\nwidely used variant called Latent Dirichlet Allocation (LDA) --- and numerous\napplications in sociology, history, and linguistics, topic models are known to\nsuffer from severe conceptual and practical problems, e.g. a lack of\njustification for the Bayesian priors, discrepancies with statistical\nproperties of real texts, and the inability to properly choose the number of\ntopics. Here we obtain a fresh view on the problem of identifying topical\nstructures by relating it to the problem of finding communities in complex\nnetworks. This is achieved by representing text corpora as bipartite networks\nof documents and words. By adapting existing community-detection methods --\nusing a stochastic block model (SBM) with non-parametric priors -- we obtain a\nmore versatile and principled framework for topic modeling (e.g., it\nautomatically detects the number of topics and hierarchically clusters both the\nwords and documents). The analysis of artificial and real corpora demonstrates\nthat our SBM approach leads to better topic models than LDA in terms of\nstatistical model selection. More importantly, our work shows how to formally\nrelate methods from community detection and topic modeling, opening the\npossibility of cross-fertilization between these two fields.\n", "versions": [{"version": "v1", "created": "Fri, 4 Aug 2017 22:35:50 GMT"}, {"version": "v2", "created": "Thu, 19 Jul 2018 13:14:31 GMT"}], "update_date": "2018-07-20", "authors_parsed": [["Gerlach", "Martin", ""], ["Peixoto", "Tiago P.", ""], ["Altmann", "Eduardo G.", ""]]}, {"id": "1708.01715", "submitter": "Oleksii Kuchaiev", "authors": "Oleksii Kuchaiev, Boris Ginsburg", "title": "Training Deep AutoEncoders for Collaborative Filtering", "comments": "5 pages, 6 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper proposes a novel model for the rating prediction task in\nrecommender systems which significantly outperforms previous state-of-the art\nmodels on a time-split Netflix data set. Our model is based on deep autoencoder\nwith 6 layers and is trained end-to-end without any layer-wise pre-training. We\nempirically demonstrate that: a) deep autoencoder models generalize much better\nthan the shallow ones, b) non-linear activation functions with negative parts\nare crucial for training deep models, and c) heavy use of regularization\ntechniques such as dropout is necessary to prevent over-fiting. We also propose\na new training algorithm based on iterative output re-feeding to overcome\nnatural sparseness of collaborate filtering. The new algorithm significantly\nspeeds up training and improves model performance. Our code is available at\nhttps://github.com/NVIDIA/DeepRecommender\n", "versions": [{"version": "v1", "created": "Sat, 5 Aug 2017 06:31:50 GMT"}, {"version": "v2", "created": "Wed, 16 Aug 2017 23:45:27 GMT"}, {"version": "v3", "created": "Tue, 10 Oct 2017 22:31:59 GMT"}], "update_date": "2017-10-12", "authors_parsed": [["Kuchaiev", "Oleksii", ""], ["Ginsburg", "Boris", ""]]}, {"id": "1708.01729", "submitter": "Zhiming Zhou", "authors": "Zhiming Zhou, Weinan Zhang, Jun Wang", "title": "Inception Score, Label Smoothing, Gradient Vanishing and -log(D(x))\n  Alternative", "comments": "An advanced version is included in arXiv:1703.02000 \"Activation\n  Maximization Generative Adversarial Nets\"", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI cs.CV stat.ML", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  In this article, we mathematically study several GAN related topics,\nincluding Inception score, label smoothing, gradient vanishing and the\n-log(D(x)) alternative.\n  ---\n  An advanced version is included in arXiv:1703.02000 \"Activation Maximization\nGenerative Adversarial Nets\".\n  Please refer Section 6 in 1703.02000 for detailed analysis on Inception\nScore, and refer its appendix for the discussions on Label Smoothing, Gradient\nVanishing and -log(D(x)) Alternative.\n", "versions": [{"version": "v1", "created": "Sat, 5 Aug 2017 08:15:07 GMT"}, {"version": "v2", "created": "Sat, 3 Mar 2018 02:30:32 GMT"}, {"version": "v3", "created": "Sat, 30 Jun 2018 07:02:11 GMT"}], "update_date": "2018-07-03", "authors_parsed": [["Zhou", "Zhiming", ""], ["Zhang", "Weinan", ""], ["Wang", "Jun", ""]]}, {"id": "1708.01733", "submitter": "Francesco Locatello", "authors": "Francesco Locatello, Rajiv Khanna, Joydeep Ghosh, Gunnar R\\\"atsch", "title": "Boosting Variational Inference: an Optimization Perspective", "comments": null, "journal-ref": "AISTATS 2018", "doi": null, "report-no": null, "categories": "cs.LG cs.AI stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Variational inference is a popular technique to approximate a possibly\nintractable Bayesian posterior with a more tractable one. Recently, boosting\nvariational inference has been proposed as a new paradigm to approximate the\nposterior by a mixture of densities by greedily adding components to the\nmixture. However, as is the case with many other variational inference\nalgorithms, its theoretical properties have not been studied. In the present\nwork, we study the convergence properties of this approach from a modern\noptimization viewpoint by establishing connections to the classic Frank-Wolfe\nalgorithm. Our analyses yields novel theoretical insights regarding the\nsufficient conditions for convergence, explicit rates, and algorithmic\nsimplifications. Since a lot of focus in previous works for variational\ninference has been on tractability, our work is especially important as a much\nneeded attempt to bridge the gap between probabilistic models and their\ncorresponding theoretical properties.\n", "versions": [{"version": "v1", "created": "Sat, 5 Aug 2017 08:42:11 GMT"}, {"version": "v2", "created": "Wed, 7 Mar 2018 13:04:35 GMT"}], "update_date": "2018-03-08", "authors_parsed": [["Locatello", "Francesco", ""], ["Khanna", "Rajiv", ""], ["Ghosh", "Joydeep", ""], ["R\u00e4tsch", "Gunnar", ""]]}, {"id": "1708.01746", "submitter": "Lev Utkin", "authors": "Lev V. Utkin and Irina L. Utkina", "title": "A simple genome-wide association study algorithm", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "q-bio.QM stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A computationally simple genome-wide association study (GWAS) algorithm for\nestimating the main and epistatic effects of markers or single nucleotide\npolymorphisms (SNPs) is proposed. It is based on the intuitive assumption that\nchanges of alleles corresponding to important SNPs in a pair of individuals\nlead to large difference of phenotype values of these individuals. The\nalgorithm is based on considering pairs of individuals instead of SNPs or pairs\nof SNPs. The main advantage of the algorithm is that it weakly depends on the\nnumber of SNPs in a genotype matrix. It mainly depends on the number of\nindividuals, which is typically very small in comparison with the number of\nSNPs. Numerical experiments with real data sets illustrate the proposed\nalgorithm.\n", "versions": [{"version": "v1", "created": "Sat, 5 Aug 2017 10:47:21 GMT"}], "update_date": "2017-08-08", "authors_parsed": [["Utkin", "Lev V.", ""], ["Utkina", "Irina L.", ""]]}, {"id": "1708.01772", "submitter": "Nikolai Slavov", "authors": "Dmitry Malioutov, Tianchi Chen, Jacob Jaffe, Edoardo Airoldi, Steven\n  Carr, Bogdan Budnik and Nikolai Slavov", "title": "Quantifying homologous proteins and proteoforms", "comments": null, "journal-ref": "Molecular & Cellular Proteomics, 2018", "doi": "10.1074/mcp.TIR118.000947", "report-no": "mcp.TIR118.000947", "categories": "q-bio.QM q-bio.GN stat.AP stat.ME stat.ML", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Many proteoforms - arising from alternative splicing, post-translational\nmodifications (PTMs), or paralogous genes - have distinct biological functions,\nsuch as histone PTM proteoforms. However, their quantification by existing\nbottom-up mass-spectrometry (MS) methods is undermined by peptide-specific\nbiases. To avoid these biases, we developed and implemented a first-principles\nmodel (HIquant) for quantifying proteoform stoichiometries. We characterized\nwhen MS data allow inferring proteoform stoichiometries by HIquant, derived an\nalgorithm for optimal inference, and demonstrated experimentally high accuracy\nin quantifying fractional PTM occupancy without using external standards, even\nin the challenging case of the histone modification code.\n  HIquant server is implemented at:\nhttps://web.northeastern.edu/slavov/2014_HIquant/\n", "versions": [{"version": "v1", "created": "Sat, 5 Aug 2017 13:52:12 GMT"}], "update_date": "2018-10-29", "authors_parsed": [["Malioutov", "Dmitry", ""], ["Chen", "Tianchi", ""], ["Jaffe", "Jacob", ""], ["Airoldi", "Edoardo", ""], ["Carr", "Steven", ""], ["Budnik", "Bogdan", ""], ["Slavov", "Nikolai", ""]]}, {"id": "1708.01799", "submitter": "Chen-Yu Wei", "authors": "Haipeng Luo and Chen-Yu Wei and Alekh Agarwal and John Langford", "title": "Efficient Contextual Bandits in Non-stationary Worlds", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Most contextual bandit algorithms minimize regret against the best fixed\npolicy, a questionable benchmark for non-stationary environments that are\nubiquitous in applications. In this work, we develop several efficient\ncontextual bandit algorithms for non-stationary environments by equipping\nexisting methods for i.i.d. problems with sophisticated statistical tests so as\nto dynamically adapt to a change in distribution.\n  We analyze various standard notions of regret suited to non-stationary\nenvironments for these algorithms, including interval regret, switching regret,\nand dynamic regret. When competing with the best policy at each time, one of\nour algorithms achieves regret $\\mathcal{O}(\\sqrt{ST})$ if there are $T$ rounds\nwith $S$ stationary periods, or more generally\n$\\mathcal{O}(\\Delta^{1/3}T^{2/3})$ where $\\Delta$ is some non-stationarity\nmeasure. These results almost match the optimal guarantees achieved by an\ninefficient baseline that is a variant of the classic Exp4 algorithm. The\ndynamic regret result is also the first one for efficient and fully adversarial\ncontextual bandit.\n  Furthermore, while the results above require tuning a parameter based on the\nunknown quantity $S$ or $\\Delta$, we also develop a parameter free algorithm\nachieving regret $\\min\\{S^{1/4}T^{3/4}, \\Delta^{1/5}T^{4/5}\\}$. This improves\nand generalizes the best existing result $\\Delta^{0.18}T^{0.82}$ by Karnin and\nAnava (2016) which only holds for the two-armed bandit problem.\n", "versions": [{"version": "v1", "created": "Sat, 5 Aug 2017 18:21:31 GMT"}, {"version": "v2", "created": "Tue, 20 Feb 2018 06:58:43 GMT"}, {"version": "v3", "created": "Thu, 7 Jun 2018 17:26:07 GMT"}, {"version": "v4", "created": "Wed, 3 Apr 2019 18:51:43 GMT"}], "update_date": "2019-04-05", "authors_parsed": [["Luo", "Haipeng", ""], ["Wei", "Chen-Yu", ""], ["Agarwal", "Alekh", ""], ["Langford", "John", ""]]}, {"id": "1708.01810", "submitter": "Shing Chan", "authors": "Shing Chan and Ahmed H. Elsheikh", "title": "Parametrization and generation of geological models with generative\n  adversarial networks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML physics.comp-ph physics.data-an physics.geo-ph", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  One of the main challenges in the parametrization of geological models is the\nability to capture complex geological structures often observed in the\nsubsurface. In recent years, generative adversarial networks (GAN) were\nproposed as an efficient method for the generation and parametrization of\ncomplex data, showing state-of-the-art performances in challenging computer\nvision tasks such as reproducing natural images (handwritten digits, human\nfaces, etc.). In this work, we study the application of Wasserstein GAN for the\nparametrization of geological models. The effectiveness of the method is\nassessed for uncertainty propagation tasks using several test cases involving\ndifferent permeability patterns and subsurface flow problems. Results show that\nGANs are able to generate samples that preserve the multipoint statistical\nfeatures of the geological models both visually and quantitatively. The\ngenerated samples reproduce both the geological structures and the flow\nstatistics of the reference geology.\n", "versions": [{"version": "v1", "created": "Sat, 5 Aug 2017 19:07:06 GMT"}, {"version": "v2", "created": "Tue, 9 Apr 2019 08:01:07 GMT"}], "update_date": "2019-04-10", "authors_parsed": [["Chan", "Shing", ""], ["Elsheikh", "Ahmed H.", ""]]}, {"id": "1708.01867", "submitter": "Felix Leibfried", "authors": "Felix Leibfried, Jordi Grau-Moya and Haitham Bou-Ammar", "title": "An Information-Theoretic Optimality Principle for Deep Reinforcement\n  Learning", "comments": "Presented at the NIPS Deep Reinforcement Learning Workshop, Montreal,\n  Canada, 2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We methodologically address the problem of Q-value overestimation in deep\nreinforcement learning to handle high-dimensional state spaces efficiently. By\nadapting concepts from information theory, we introduce an intrinsic penalty\nsignal encouraging reduced Q-value estimates. The resultant algorithm\nencompasses a wide range of learning outcomes containing deep Q-networks as a\nspecial case. Different learning outcomes can be demonstrated by tuning a\nLagrange multiplier accordingly. We furthermore propose a novel scheduling\nscheme for this Lagrange multiplier to ensure efficient and robust learning. In\nexperiments on Atari, our algorithm outperforms other algorithms (e.g. deep and\ndouble deep Q-networks) in terms of both game-play performance and sample\ncomplexity. These results remain valid under the recently proposed dueling\narchitecture.\n", "versions": [{"version": "v1", "created": "Sun, 6 Aug 2017 09:23:22 GMT"}, {"version": "v2", "created": "Wed, 8 Nov 2017 16:27:50 GMT"}, {"version": "v3", "created": "Thu, 8 Feb 2018 14:07:53 GMT"}, {"version": "v4", "created": "Thu, 6 Sep 2018 09:27:21 GMT"}, {"version": "v5", "created": "Tue, 20 Nov 2018 11:55:21 GMT"}], "update_date": "2018-11-21", "authors_parsed": [["Leibfried", "Felix", ""], ["Grau-Moya", "Jordi", ""], ["Bou-Ammar", "Haitham", ""]]}, {"id": "1708.01886", "submitter": "Hamid Eghbal-zadeh", "authors": "Hamid Eghbal-zadeh, Gerhard Widmer", "title": "Probabilistic Generative Adversarial Networks", "comments": "Submitted to NIPS 2017", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We introduce the Probabilistic Generative Adversarial Network (PGAN), a new\nGAN variant based on a new kind of objective function. The central idea is to\nintegrate a probabilistic model (a Gaussian Mixture Model, in our case) into\nthe GAN framework which supports a new kind of loss function (based on\nlikelihood rather than classification loss), and at the same time gives a\nmeaningful measure of the quality of the outputs generated by the network.\nExperiments with MNIST show that the model learns to generate realistic images,\nand at the same time computes likelihoods that are correlated with the quality\nof the generated images. We show that PGAN is better able to cope with\ninstability problems that are usually observed in the GAN training procedure.\nWe investigate this from three aspects: the probability landscape of the\ndiscriminator, gradients of the generator, and the perfect discriminator\nproblem.\n", "versions": [{"version": "v1", "created": "Sun, 6 Aug 2017 13:09:59 GMT"}], "update_date": "2017-08-08", "authors_parsed": [["Eghbal-zadeh", "Hamid", ""], ["Widmer", "Gerhard", ""]]}, {"id": "1708.01911", "submitter": "Thomas Kurbiel", "authors": "Thomas Kurbiel and Shahrzad Khaleghian", "title": "Training of Deep Neural Networks based on Distance Measures using\n  RMSProp", "comments": "6 pages, 14 figure", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The vanishing gradient problem was a major obstacle for the success of deep\nlearning. In recent years it was gradually alleviated through multiple\ndifferent techniques. However the problem was not really overcome in a\nfundamental way, since it is inherent to neural networks with activation\nfunctions based on dot products. In a series of papers, we are going to analyze\nalternative neural network structures which are not based on dot products. In\nthis first paper, we revisit neural networks built up of layers based on\ndistance measures and Gaussian activation functions. These kinds of networks\nwere only sparsely used in the past since they are hard to train when using\nplain stochastic gradient descent methods. We show that by using Root Mean\nSquare Propagation (RMSProp) it is possible to efficiently learn multi-layer\nneural networks. Furthermore we show that when appropriately initialized these\nkinds of neural networks suffer much less from the vanishing and exploding\ngradient problem than traditional neural networks even for deep networks.\n", "versions": [{"version": "v1", "created": "Sun, 6 Aug 2017 17:10:38 GMT"}], "update_date": "2017-08-08", "authors_parsed": [["Kurbiel", "Thomas", ""], ["Khaleghian", "Shahrzad", ""]]}, {"id": "1708.01945", "submitter": "Shusen Wang", "authors": "Miles E. Lopes and Shusen Wang and Michael W. Mahoney", "title": "A Bootstrap Method for Error Estimation in Randomized Matrix\n  Multiplication", "comments": null, "journal-ref": "Journal of Machine Learning Research, 20(39): 1-40, 2019", "doi": null, "report-no": null, "categories": "stat.ML cs.LG cs.NA", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In recent years, randomized methods for numerical linear algebra have\nreceived growing interest as a general approach to large-scale problems.\nTypically, the essential ingredient of these methods is some form of randomized\ndimension reduction, which accelerates computations, but also creates random\napproximation error. In this way, the dimension reduction step encodes a\ntradeoff between cost and accuracy. However, the exact numerical relationship\nbetween cost and accuracy is typically unknown, and consequently, it may be\ndifficult for the user to precisely know (1) how accurate a given solution is,\nor (2) how much computation is needed to achieve a given level of accuracy. In\nthe current paper, we study randomized matrix multiplication (sketching) as a\nprototype setting for addressing these general problems. As a solution, we\ndevelop a bootstrap method for \\emph{directly estimating} the accuracy as a\nfunction of the reduced dimension (as opposed to deriving worst-case bounds on\nthe accuracy in terms of the reduced dimension). From a computational\nstandpoint, the proposed method does not substantially increase the cost of\nstandard sketching methods, and this is made possible by an \"extrapolation\"\ntechnique. In addition, we provide both theoretical and empirical results to\ndemonstrate the effectiveness of the proposed method.\n", "versions": [{"version": "v1", "created": "Sun, 6 Aug 2017 22:20:13 GMT"}, {"version": "v2", "created": "Thu, 4 Apr 2019 01:20:24 GMT"}], "update_date": "2019-04-05", "authors_parsed": [["Lopes", "Miles E.", ""], ["Wang", "Shusen", ""], ["Mahoney", "Michael W.", ""]]}, {"id": "1708.01947", "submitter": "Wesley Tansey", "authors": "Wesley Tansey, Jesse Thomason, James G. Scott", "title": "Interpretable Low-Dimensional Regression via Data-Adaptive Smoothing", "comments": "4 pages, 1 figure presented at 2017 ICML Workshop on Human\n  Interpretability in Machine Learning (WHI 2017), Sydney, NSW, Australia", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider the problem of estimating a regression function in the common\nsituation where the number of features is small, where interpretability of the\nmodel is a high priority, and where simple linear or additive models fail to\nprovide adequate performance. To address this problem, we present Maximum\nVariance Total Variation denoising (MVTV), an approach that is conceptually\nrelated both to CART and to the more recent CRISP algorithm, a state-of-the-art\nalternative method for interpretable nonlinear regression. MVTV divides the\nfeature space into blocks of constant value and fits the value of all blocks\njointly via a convex optimization routine. Our method is fully data-adaptive,\nin that it incorporates highly robust routines for tuning all hyperparameters\nautomatically. We compare our approach against CART and CRISP via both a\ncomplexity-accuracy tradeoff metric and a human study, demonstrating that that\nMVTV is a more powerful and interpretable method.\n", "versions": [{"version": "v1", "created": "Sun, 6 Aug 2017 22:31:26 GMT"}], "update_date": "2017-08-08", "authors_parsed": [["Tansey", "Wesley", ""], ["Thomason", "Jesse", ""], ["Scott", "James G.", ""]]}, {"id": "1708.01955", "submitter": "Morgan A. Schmitz", "authors": "Morgan A. Schmitz, Matthieu Heitz, Nicolas Bonneel, Fred Maurice\n  Ngol\\`e Mboula, David Coeurjolly, Marco Cuturi, Gabriel Peyr\\'e, Jean-Luc\n  Starck", "title": "Wasserstein Dictionary Learning: Optimal Transport-based unsupervised\n  non-linear dictionary learning", "comments": "Published in SIAM SIIMS. 46 pages, 24 figures", "journal-ref": "SIAM Journal on Imaging Sciences 11(1) (2018) 643-678", "doi": "10.1137/17M1140431", "report-no": null, "categories": "stat.ML cs.GR math.OC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper introduces a new nonlinear dictionary learning method for\nhistograms in the probability simplex. The method leverages optimal transport\ntheory, in the sense that our aim is to reconstruct histograms using so-called\ndisplacement interpolations (a.k.a. Wasserstein barycenters) between dictionary\natoms; such atoms are themselves synthetic histograms in the probability\nsimplex. Our method simultaneously estimates such atoms, and, for each\ndatapoint, the vector of weights that can optimally reconstruct it as an\noptimal transport barycenter of such atoms. Our method is computationally\ntractable thanks to the addition of an entropic regularization to the usual\noptimal transportation problem, leading to an approximation scheme that is\nefficient, parallel and simple to differentiate. Both atoms and weights are\nlearned using a gradient-based descent method. Gradients are obtained by\nautomatic differentiation of the generalized Sinkhorn iterations that yield\nbarycenters with entropic smoothing. Because of its formulation relying on\nWasserstein barycenters instead of the usual matrix product between dictionary\nand codes, our method allows for nonlinear relationships between atoms and the\nreconstruction of input data. We illustrate its application in several\ndifferent image processing settings.\n", "versions": [{"version": "v1", "created": "Mon, 7 Aug 2017 01:00:40 GMT"}, {"version": "v2", "created": "Tue, 9 Jan 2018 17:07:57 GMT"}, {"version": "v3", "created": "Thu, 15 Mar 2018 13:15:44 GMT"}], "update_date": "2018-03-16", "authors_parsed": [["Schmitz", "Morgan A.", ""], ["Heitz", "Matthieu", ""], ["Bonneel", "Nicolas", ""], ["Mboula", "Fred Maurice Ngol\u00e8", ""], ["Coeurjolly", "David", ""], ["Cuturi", "Marco", ""], ["Peyr\u00e9", "Gabriel", ""], ["Starck", "Jean-Luc", ""]]}, {"id": "1708.01960", "submitter": "Qiang Wu", "authors": "Zhengchu Guo, Lei Shi and Qiang Wu", "title": "Learning Theory of Distributed Regression with Bias Corrected\n  Regularization Kernel Network", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Distributed learning is an effective way to analyze big data. In distributed\nregression, a typical approach is to divide the big data into multiple blocks,\napply a base regression algorithm on each of them, and then simply average the\noutput functions learnt from these blocks. Since the average process will\ndecrease the variance, not the bias, bias correction is expected to improve the\nlearning performance if the base regression algorithm is a biased one.\nRegularization kernel network is an effective and widely used method for\nnonlinear regression analysis. In this paper we will investigate a bias\ncorrected version of regularization kernel network. We derive the error bounds\nwhen it is applied to a single data set and when it is applied as a base\nalgorithm in distributed regression. We show that, under certain appropriate\nconditions, the optimal learning rates can be reached in both situations.\n", "versions": [{"version": "v1", "created": "Mon, 7 Aug 2017 01:54:56 GMT"}], "update_date": "2017-08-08", "authors_parsed": [["Guo", "Zhengchu", ""], ["Shi", "Lei", ""], ["Wu", "Qiang", ""]]}, {"id": "1708.01977", "submitter": "Xinkun Nie", "authors": "Xinkun Nie, Xiaoying Tian, Jonathan Taylor, James Zou", "title": "Why Adaptively Collected Data Have Negative Bias and How to Correct for\n  It", "comments": "Accepted to the 21st International Conference on Artificial\n  Intelligence and Statistics (AISTATS) 2018, Lanzarote, Spain", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  From scientific experiments to online A/B testing, the previously observed\ndata often affects how future experiments are performed, which in turn affects\nwhich data will be collected. Such adaptivity introduces complex correlations\nbetween the data and the collection procedure. In this paper, we prove that\nwhen the data collection procedure satisfies natural conditions, then sample\nmeans of the data have systematic \\emph{negative} biases. As an example,\nconsider an adaptive clinical trial where additional data points are more\nlikely to be tested for treatments that show initial promise. Our surprising\nresult implies that the average observed treatment effects would underestimate\nthe true effects of each treatment. We quantitatively analyze the magnitude and\nbehavior of this negative bias in a variety of settings. We also propose a\nnovel debiasing algorithm based on selective inference techniques. In\nexperiments, our method can effectively reduce bias and estimation error.\n", "versions": [{"version": "v1", "created": "Mon, 7 Aug 2017 03:30:05 GMT"}, {"version": "v2", "created": "Sat, 30 Dec 2017 20:45:47 GMT"}], "update_date": "2018-01-03", "authors_parsed": [["Nie", "Xinkun", ""], ["Tian", "Xiaoying", ""], ["Taylor", "Jonathan", ""], ["Zou", "James", ""]]}, {"id": "1708.01986", "submitter": "Takeshi Ise", "authors": "Takeshi Ise, Mari Minagawa, and Masanori Onishi", "title": "Identifying 3 moss species by deep learning, using the \"chopped picture\"\n  method", "comments": "7 pages, 5 figures, 1 table", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In general, object identification tends not to work well on ambiguous,\namorphous objects such as vegetation. In this study, we developed a simple but\neffective approach to identify ambiguous objects and applied the method to\nseveral moss species. As a result, the model correctly classified test images\nwith accuracy more than 90%. Using this approach will help progress in computer\nvision studies.\n", "versions": [{"version": "v1", "created": "Mon, 7 Aug 2017 04:37:23 GMT"}, {"version": "v2", "created": "Tue, 8 Aug 2017 01:38:37 GMT"}], "update_date": "2017-08-09", "authors_parsed": [["Ise", "Takeshi", ""], ["Minagawa", "Mari", ""], ["Onishi", "Masanori", ""]]}, {"id": "1708.02059", "submitter": "Xinyue Shen", "authors": "Xinyue Shen, Yuantao Gu", "title": "Nonconvex Sparse Logistic Regression with Weakly Convex Regularization", "comments": null, "journal-ref": null, "doi": "10.1109/TSP.2018.2824289", "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this work we propose to fit a sparse logistic regression model by a weakly\nconvex regularized nonconvex optimization problem. The idea is based on the\nfinding that a weakly convex function as an approximation of the $\\ell_0$\npseudo norm is able to better induce sparsity than the commonly used $\\ell_1$\nnorm. For a class of weakly convex sparsity inducing functions, we prove the\nnonconvexity of the corresponding sparse logistic regression problem, and study\nits local optimality conditions and the choice of the regularization parameter\nto exclude trivial solutions. Despite the nonconvexity, a method based on\nproximal gradient descent is used to solve the general weakly convex sparse\nlogistic regression, and its convergence behavior is studied theoretically.\nThen the general framework is applied to a specific weakly convex function, and\na necessary and sufficient local optimality condition is provided. The solution\nmethod is instantiated in this case as an iterative firm-shrinkage algorithm,\nand its effectiveness is demonstrated in numerical experiments by both randomly\ngenerated and real datasets.\n", "versions": [{"version": "v1", "created": "Mon, 7 Aug 2017 10:19:16 GMT"}], "update_date": "2018-05-23", "authors_parsed": [["Shen", "Xinyue", ""], ["Gu", "Yuantao", ""]]}, {"id": "1708.02105", "submitter": "Wei Hu", "authors": "Zeyuan Allen-Zhu, Elad Hazan, Wei Hu, Yuanzhi Li", "title": "Linear Convergence of a Frank-Wolfe Type Algorithm over Trace-Norm Balls", "comments": "In NIPS 2017", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.DS math.OC stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a rank-$k$ variant of the classical Frank-Wolfe algorithm to solve\nconvex optimization over a trace-norm ball. Our algorithm replaces the top\nsingular-vector computation ($1$-SVD) in Frank-Wolfe with a top-$k$\nsingular-vector computation ($k$-SVD), which can be done by repeatedly applying\n$1$-SVD $k$ times. Alternatively, our algorithm can be viewed as a rank-$k$\nrestricted version of projected gradient descent. We show that our algorithm\nhas a linear convergence rate when the objective function is smooth and\nstrongly convex, and the optimal solution has rank at most $k$. This improves\nthe convergence rate and the total time complexity of the Frank-Wolfe method\nand its variants.\n", "versions": [{"version": "v1", "created": "Mon, 7 Aug 2017 13:07:20 GMT"}, {"version": "v2", "created": "Thu, 19 Oct 2017 23:33:37 GMT"}, {"version": "v3", "created": "Thu, 9 Nov 2017 02:16:15 GMT"}], "update_date": "2017-11-10", "authors_parsed": [["Allen-Zhu", "Zeyuan", ""], ["Hazan", "Elad", ""], ["Hu", "Wei", ""], ["Li", "Yuanzhi", ""]]}, {"id": "1708.02122", "submitter": "Colleen Farrelly", "authors": "Colleen M. Farrelly", "title": "KNN Ensembles for Tweedie Regression: The Power of Multiscale\n  Neighborhoods", "comments": "17 pages, 11 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Very few K-nearest-neighbor (KNN) ensembles exist, despite the efficacy of\nthis approach in regression, classification, and outlier detection. Those that\ndo exist focus on bagging features, rather than varying k or bagging\nobservations; it is unknown whether varying k or bagging observations can\nimprove prediction. Given recent studies from topological data analysis,\nvarying k may function like multiscale topological methods, providing stability\nand better prediction, as well as increased ensemble diversity.\n  This paper explores 7 KNN ensemble algorithms combining bagged features,\nbagged observations, and varied k to understand how each of these contribute to\nmodel fit. Specifically, these algorithms are tested on Tweedie regression\nproblems through simulations and 6 real datasets; results are compared to\nstate-of-the-art machine learning models including extreme learning machines,\nrandom forest, boosted regression, and Morse-Smale regression.\n  Results on simulations suggest gains from varying k above and beyond bagging\nfeatures or samples, as well as the robustness of KNN ensembles to the curse of\ndimensionality. KNN regression ensembles perform favorably against\nstate-of-the-art algorithms and dramatically improve performance over KNN\nregression. Further, real dataset results suggest varying k is a good strategy\nin general (particularly for difficult Tweedie regression problems) and that\nKNN regression ensembles often outperform state-of-the-art methods.\n  These results for k-varying ensembles echo recent theoretical results in\ntopological data analysis, where multidimensional filter functions and\nmultiscale coverings provide stability and performance gains over\nsingle-dimensional filters and single-scale covering. This opens up the\npossibility of leveraging multiscale neighborhoods and multiple measures of\nlocal geometry in ensemble methods.\n", "versions": [{"version": "v1", "created": "Sat, 29 Jul 2017 21:45:37 GMT"}], "update_date": "2017-08-08", "authors_parsed": [["Farrelly", "Colleen M.", ""]]}, {"id": "1708.02183", "submitter": "Yi Ding", "authors": "Yi Ding, Risi Kondor, Jonathan Eskreis-Winkler", "title": "Multiresolution Kernel Approximation for Gaussian Process Regression", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Gaussian process regression generally does not scale to beyond a few\nthousands data points without applying some sort of kernel approximation\nmethod. Most approximations focus on the high eigenvalue part of the spectrum\nof the kernel matrix, $K$, which leads to bad performance when the length scale\nof the kernel is small. In this paper we introduce Multiresolution Kernel\nApproximation (MKA), the first true broad bandwidth kernel approximation\nalgorithm. Important points about MKA are that it is memory efficient, and it\nis a direct method, which means that it also makes it easy to approximate\n$K^{-1}$ and $\\mathop{\\textrm{det}}(K)$.\n", "versions": [{"version": "v1", "created": "Mon, 7 Aug 2017 16:09:24 GMT"}, {"version": "v2", "created": "Sun, 5 Nov 2017 15:48:32 GMT"}, {"version": "v3", "created": "Tue, 30 Jan 2018 02:18:05 GMT"}], "update_date": "2018-01-31", "authors_parsed": [["Ding", "Yi", ""], ["Kondor", "Risi", ""], ["Eskreis-Winkler", "Jonathan", ""]]}, {"id": "1708.02230", "submitter": "Richard Everitt", "authors": "Richard G. Everitt and Paulina A. Rowi\\'nska", "title": "Delayed acceptance ABC-SMC", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.CO physics.data-an stat.AP stat.ME stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Approximate Bayesian computation (ABC) is now an established technique for\nstatistical inference used in cases where the likelihood function is\ncomputationally expensive or not available. It relies on the use of a~model\nthat is specified in the form of a~simulator, and approximates the likelihood\nat a~parameter value $\\theta$ by simulating auxiliary data sets $x$ and\nevaluating the distance of $x$ from the true data $y$. However, ABC is not\ncomputationally feasible in cases where using the simulator for each $\\theta$\nis very expensive. This paper investigates this situation in cases where\na~cheap, but approximate, simulator is available. The approach is to employ\ndelayed acceptance Markov chain Monte Carlo (MCMC) within an ABC sequential\nMonte Carlo (SMC) sampler in order to, in a~first stage of the kernel, use the\ncheap simulator to rule out parts of the parameter space that are not worth\nexploring, so that the ``true'' simulator is only run (in the second stage of\nthe kernel) where there is a~reasonable chance of accepting proposed values of\n$\\theta$. We show that this approach can be used quite automatically, with few\ntuning parameters. Applications to stochastic differential equation models and\nlatent doubly intractable distributions are presented.\n", "versions": [{"version": "v1", "created": "Mon, 7 Aug 2017 17:52:04 GMT"}, {"version": "v2", "created": "Sun, 31 May 2020 14:56:49 GMT"}], "update_date": "2020-06-02", "authors_parsed": [["Everitt", "Richard G.", ""], ["Rowi\u0144ska", "Paulina A.", ""]]}, {"id": "1708.02286", "submitter": "Yu Cheng", "authors": "Shuangjie Xu, Yu Cheng, Kang Gu, Yang Yang, Shiyu Chang, Pan Zhou", "title": "Jointly Attentive Spatial-Temporal Pooling Networks for Video-based\n  Person Re-Identification", "comments": "To appear in ICCV 2017", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Person Re-Identification (person re-id) is a crucial task as its applications\nin visual surveillance and human-computer interaction. In this work, we present\na novel joint Spatial and Temporal Attention Pooling Network (ASTPN) for\nvideo-based person re-identification, which enables the feature extractor to be\naware of the current input video sequences, in a way that interdependency from\nthe matching items can directly influence the computation of each other's\nrepresentation. Specifically, the spatial pooling layer is able to select\nregions from each frame, while the attention temporal pooling performed can\nselect informative frames over the sequence, both pooling guided by the\ninformation from distance matching. Experiments are conduced on the iLIDS-VID,\nPRID-2011 and MARS datasets and the results demonstrate that this approach\noutperforms existing state-of-art methods. We also analyze how the joint\npooling in both dimensions can boost the person re-id performance more\neffectively than using either of them separately.\n", "versions": [{"version": "v1", "created": "Thu, 3 Aug 2017 02:35:17 GMT"}, {"version": "v2", "created": "Fri, 29 Sep 2017 14:41:58 GMT"}], "update_date": "2017-10-02", "authors_parsed": [["Xu", "Shuangjie", ""], ["Cheng", "Yu", ""], ["Gu", "Kang", ""], ["Yang", "Yang", ""], ["Chang", "Shiyu", ""], ["Zhou", "Pan", ""]]}, {"id": "1708.02363", "submitter": "Ilias Flaounas", "authors": "Ilias Flaounas", "title": "Beyond the technical challenges for deploying Machine Learning solutions\n  in a software company", "comments": "Human in the Loop Machine Learning Workshop, International Conference\n  on Machine Learning, Sydney, Australia, 2017", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC cs.AI cs.SE stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recently software development companies started to embrace Machine Learning\n(ML) techniques for introducing a series of advanced functionality in their\nproducts such as personalisation of the user experience, improved search,\ncontent recommendation and automation. The technical challenges for tackling\nthese problems are heavily researched in literature. A less studied area is a\npragmatic approach to the role of humans in a complex modern industrial\nenvironment where ML based systems are developed. Key stakeholders affect the\nsystem from inception and up to operation and maintenance. Product managers\nwant to embed \"smart\" experiences for their users and drive the decisions on\nwhat should be built next; software engineers are challenged to build or\nutilise ML software tools that require skills that are well outside of their\ncomfort zone; legal and risk departments may influence design choices and data\naccess; operations teams are requested to maintain ML systems which are\nnon-stationary in their nature and change behaviour over time; and finally ML\npractitioners should communicate with all these stakeholders to successfully\nbuild a reliable system. This paper discusses some of the challenges we faced\nin Atlassian as we started investing more in the ML space.\n", "versions": [{"version": "v1", "created": "Tue, 8 Aug 2017 03:59:09 GMT"}], "update_date": "2017-08-09", "authors_parsed": [["Flaounas", "Ilias", ""]]}, {"id": "1708.02455", "submitter": "Linxiao Yang", "authors": "Linxiao Yang, Jun Fang, Huiping Duan, Hongbin Li and Bing Zeng", "title": "Fast Low-Rank Bayesian Matrix Completion with Hierarchical Gaussian\n  Prior Models", "comments": null, "journal-ref": null, "doi": "10.1109/TSP.2018.2816575", "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The problem of low rank matrix completion is considered in this paper. To\nexploit the underlying low-rank structure of the data matrix, we propose a\nhierarchical Gaussian prior model, where columns of the low-rank matrix are\nassumed to follow a Gaussian distribution with zero mean and a common precision\nmatrix, and a Wishart distribution is specified as a hyperprior over the\nprecision matrix. We show that such a hierarchical Gaussian prior has the\npotential to encourage a low-rank solution. Based on the proposed hierarchical\nprior model, a variational Bayesian method is developed for matrix completion,\nwhere the generalized approximate massage passing (GAMP) technique is embedded\ninto the variational Bayesian inference in order to circumvent cumbersome\nmatrix inverse operations. Simulation results show that our proposed method\ndemonstrates superiority over existing state-of-the-art matrix completion\nmethods.\n", "versions": [{"version": "v1", "created": "Tue, 8 Aug 2017 11:59:10 GMT"}, {"version": "v2", "created": "Sat, 26 Aug 2017 14:19:54 GMT"}], "update_date": "2018-05-09", "authors_parsed": [["Yang", "Linxiao", ""], ["Fang", "Jun", ""], ["Duan", "Huiping", ""], ["Li", "Hongbin", ""], ["Zeng", "Bing", ""]]}, {"id": "1708.02497", "submitter": "Janne Lepp\\\"a-aho", "authors": "Janne Lepp\\\"a-aho, Santeri R\\\"ais\\\"anen, Xiao Yang, Teemu Roos", "title": "Learning non-parametric Markov networks with mutual information", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.IT math.IT stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a method for learning Markov network structures for continuous\ndata without invoking any assumptions about the distribution of the variables.\nThe method makes use of previous work on a non-parametric estimator for mutual\ninformation which is used to create a non-parametric test for multivariate\nconditional independence. This independence test is then combined with an\nefficient constraint-based algorithm for learning the graph structure. The\nperformance of the method is evaluated on several synthetic data sets and it is\nshown to learn considerably more accurate structures than competing methods\nwhen the dependencies between the variables involve non-linearities.\n", "versions": [{"version": "v1", "created": "Tue, 8 Aug 2017 14:10:55 GMT"}], "update_date": "2017-08-09", "authors_parsed": [["Lepp\u00e4-aho", "Janne", ""], ["R\u00e4is\u00e4nen", "Santeri", ""], ["Yang", "Xiao", ""], ["Roos", "Teemu", ""]]}, {"id": "1708.02511", "submitter": "Gabriel Huang", "authors": "Gabriel Huang, Hugo Berard, Ahmed Touati, Gauthier Gidel, Pascal\n  Vincent, Simon Lacoste-Julien", "title": "Parametric Adversarial Divergences are Good Task Losses for Generative\n  Modeling", "comments": "22 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Generative modeling of high dimensional data like images is a notoriously\ndifficult and ill-defined problem. In particular, how to evaluate a learned\ngenerative model is unclear. In this position paper, we argue that adversarial\nlearning, pioneered with generative adversarial networks (GANs), provides an\ninteresting framework to implicitly define more meaningful task losses for\ngenerative modeling tasks, such as for generating \"visually realistic\" images.\nWe refer to those task losses as parametric adversarial divergences and we give\ntwo main reasons why we think parametric divergences are good learning\nobjectives for generative modeling. Additionally, we unify the processes of\nchoosing a good structured loss (in structured prediction) and choosing a\ndiscriminator architecture (in generative modeling) using statistical decision\ntheory; we are then able to formalize and quantify the intuition that \"weaker\"\nlosses are easier to learn from, in a specific setting. Finally, we propose two\nnew challenging tasks to evaluate parametric and nonparametric divergences: a\nqualitative task of generating very high-resolution digits, and a quantitative\ntask of learning data that satisfies high-level algebraic constraints. We use\ntwo common divergences to train a generator and show that the parametric\ndivergence outperforms the nonparametric divergence on both the qualitative and\nthe quantitative task.\n", "versions": [{"version": "v1", "created": "Tue, 8 Aug 2017 15:01:55 GMT"}, {"version": "v2", "created": "Tue, 21 Nov 2017 00:13:56 GMT"}, {"version": "v3", "created": "Wed, 27 Jun 2018 19:58:51 GMT"}], "update_date": "2018-06-29", "authors_parsed": [["Huang", "Gabriel", ""], ["Berard", "Hugo", ""], ["Touati", "Ahmed", ""], ["Gidel", "Gauthier", ""], ["Vincent", "Pascal", ""], ["Lacoste-Julien", "Simon", ""]]}, {"id": "1708.02544", "submitter": "Farnood Salehi", "authors": "Farnood Salehi, L. Elisa Celis and Patrick Thiran", "title": "Stochastic Optimization with Bandit Sampling", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI math.OC stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Many stochastic optimization algorithms work by estimating the gradient of\nthe cost function on the fly by sampling datapoints uniformly at random from a\ntraining set. However, the estimator might have a large variance, which\ninadvertently slows down the convergence rate of the algorithms. One way to\nreduce this variance is to sample the datapoints from a carefully selected\nnon-uniform distribution. In this work, we propose a novel non-uniform sampling\napproach that uses the multi-armed bandit framework. Theoretically, we show\nthat our algorithm asymptotically approximates the optimal variance within a\nfactor of 3. Empirically, we show that using this datapoint-selection technique\nresults in a significant reduction in the convergence time and variance of\nseveral stochastic optimization algorithms such as SGD, SVRG and SAGA. This\napproach for sampling datapoints is general, and can be used in conjunction\nwith any algorithm that uses an unbiased gradient estimation -- we expect it to\nhave broad applicability beyond the specific examples explored in this work.\n", "versions": [{"version": "v1", "created": "Tue, 8 Aug 2017 16:15:26 GMT"}, {"version": "v2", "created": "Wed, 9 Aug 2017 13:20:18 GMT"}], "update_date": "2017-08-10", "authors_parsed": [["Salehi", "Farnood", ""], ["Celis", "L. Elisa", ""], ["Thiran", "Patrick", ""]]}, {"id": "1708.02556", "submitter": "Tu Dinh Nguyen", "authors": "Quan Hoang, Tu Dinh Nguyen, Trung Le and Dinh Phung", "title": "Multi-Generator Generative Adversarial Nets", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a new approach to train the Generative Adversarial Nets (GANs)\nwith a mixture of generators to overcome the mode collapsing problem. The main\nintuition is to employ multiple generators, instead of using a single one as in\nthe original GAN. The idea is simple, yet proven to be extremely effective at\ncovering diverse data modes, easily overcoming the mode collapse and delivering\nstate-of-the-art results. A minimax formulation is able to establish among a\nclassifier, a discriminator, and a set of generators in a similar spirit with\nGAN. Generators create samples that are intended to come from the same\ndistribution as the training data, whilst the discriminator determines whether\nsamples are true data or generated by generators, and the classifier specifies\nwhich generator a sample comes from. The distinguishing feature is that\ninternal samples are created from multiple generators, and then one of them\nwill be randomly selected as final output similar to the mechanism of a\nprobabilistic mixture model. We term our method Mixture GAN (MGAN). We develop\ntheoretical analysis to prove that, at the equilibrium, the Jensen-Shannon\ndivergence (JSD) between the mixture of generators' distributions and the\nempirical data distribution is minimal, whilst the JSD among generators'\ndistributions is maximal, hence effectively avoiding the mode collapse. By\nutilizing parameter sharing, our proposed model adds minimal computational cost\nto the standard GAN, and thus can also efficiently scale to large-scale\ndatasets. We conduct extensive experiments on synthetic 2D data and natural\nimage databases (CIFAR-10, STL-10 and ImageNet) to demonstrate the superior\nperformance of our MGAN in achieving state-of-the-art Inception scores over\nlatest baselines, generating diverse and appealing recognizable objects at\ndifferent resolutions, and specializing in capturing different types of objects\nby generators.\n", "versions": [{"version": "v1", "created": "Tue, 8 Aug 2017 16:48:35 GMT"}, {"version": "v2", "created": "Wed, 9 Aug 2017 01:00:49 GMT"}, {"version": "v3", "created": "Tue, 19 Sep 2017 03:24:03 GMT"}, {"version": "v4", "created": "Fri, 27 Oct 2017 23:54:26 GMT"}], "update_date": "2017-10-31", "authors_parsed": [["Hoang", "Quan", ""], ["Nguyen", "Tu Dinh", ""], ["Le", "Trung", ""], ["Phung", "Dinh", ""]]}, {"id": "1708.02581", "submitter": "Damian Straszak", "authors": "Damian Straszak and Nisheeth K. Vishnoi", "title": "Belief Propagation, Bethe Approximation and Polynomials", "comments": "Invited to Allerton 2017", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.DS cs.IT math.IT stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Factor graphs are important models for succinctly representing probability\ndistributions in machine learning, coding theory, and statistical physics.\nSeveral computational problems, such as computing marginals and partition\nfunctions, arise naturally when working with factor graphs. Belief propagation\nis a widely deployed iterative method for solving these problems. However,\ndespite its significant empirical success, not much is known about the\ncorrectness and efficiency of belief propagation.\n  Bethe approximation is an optimization-based framework for approximating\npartition functions. While it is known that the stationary points of the Bethe\napproximation coincide with the fixed points of belief propagation, in general,\nthe relation between the Bethe approximation and the partition function is not\nwell understood. It has been observed that for a few classes of factor graphs,\nthe Bethe approximation always gives a lower bound to the partition function,\nwhich distinguishes them from the general case, where neither a lower bound,\nnor an upper bound holds universally. This has been rigorously proved for\npermanents and for attractive graphical models.\n  Here we consider bipartite normal factor graphs and show that if the local\nconstraints satisfy a certain analytic property, the Bethe approximation is a\nlower bound to the partition function. We arrive at this result by viewing\nfactor graphs through the lens of polynomials. In this process, we reformulate\nthe Bethe approximation as a polynomial optimization problem. Our sufficient\ncondition for the lower bound property to hold is inspired by recent\ndevelopments in the theory of real stable polynomials. We believe that this way\nof viewing factor graphs and its connection to real stability might lead to a\nbetter understanding of belief propagation and factor graphs in general.\n", "versions": [{"version": "v1", "created": "Tue, 8 Aug 2017 17:56:15 GMT"}], "update_date": "2017-08-09", "authors_parsed": [["Straszak", "Damian", ""], ["Vishnoi", "Nisheeth K.", ""]]}, {"id": "1708.02582", "submitter": "Taesik Na", "authors": "Taesik Na, Jong Hwan Ko, and Saibal Mukhopadhyay", "title": "Cascade Adversarial Machine Learning Regularized with a Unified\n  Embedding", "comments": "16 pages, 9 figures, International Conference on Learning\n  Representations (ICLR) 2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Injecting adversarial examples during training, known as adversarial\ntraining, can improve robustness against one-step attacks, but not for unknown\niterative attacks. To address this challenge, we first show iteratively\ngenerated adversarial images easily transfer between networks trained with the\nsame strategy. Inspired by this observation, we propose cascade adversarial\ntraining, which transfers the knowledge of the end results of adversarial\ntraining. We train a network from scratch by injecting iteratively generated\nadversarial images crafted from already defended networks in addition to\none-step adversarial images from the network being trained. We also propose to\nutilize embedding space for both classification and low-level (pixel-level)\nsimilarity learning to ignore unknown pixel level perturbation. During\ntraining, we inject adversarial images without replacing their corresponding\nclean images and penalize the distance between the two embeddings (clean and\nadversarial). Experimental results show that cascade adversarial training\ntogether with our proposed low-level similarity learning efficiently enhances\nthe robustness against iterative attacks, but at the expense of decreased\nrobustness against one-step attacks. We show that combining those two\ntechniques can also improve robustness under the worst case black box attack\nscenario.\n", "versions": [{"version": "v1", "created": "Tue, 8 Aug 2017 17:58:40 GMT"}, {"version": "v2", "created": "Sun, 25 Feb 2018 08:37:28 GMT"}, {"version": "v3", "created": "Sat, 17 Mar 2018 06:09:58 GMT"}], "update_date": "2018-03-20", "authors_parsed": [["Na", "Taesik", ""], ["Ko", "Jong Hwan", ""], ["Mukhopadhyay", "Saibal", ""]]}, {"id": "1708.02620", "submitter": "Pin-Yu Chen", "authors": "Pin-Yu Chen and Alfred O. Hero", "title": "Multilayer Spectral Graph Clustering via Convex Layer Aggregation:\n  Theory and Algorithms", "comments": "Published at IEEE Transactions on Signal and Information Processing\n  over Networks", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.SI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Multilayer graphs are commonly used for representing different relations\nbetween entities and handling heterogeneous data processing tasks. Non-standard\nmultilayer graph clustering methods are needed for assigning clusters to a\ncommon multilayer node set and for combining information from each layer. This\npaper presents a multilayer spectral graph clustering (SGC) framework that\nperforms convex layer aggregation. Under a multilayer signal plus noise model,\nwe provide a phase transition analysis of clustering reliability. Moreover, we\nuse the phase transition criterion to propose a multilayer iterative model\norder selection algorithm (MIMOSA) for multilayer SGC, which features automated\ncluster assignment and layer weight adaptation, and provides statistical\nclustering reliability guarantees. Numerical simulations on synthetic\nmultilayer graphs verify the phase transition analysis, and experiments on\nreal-world multilayer graphs show that MIMOSA is competitive or better than\nother clustering methods.\n", "versions": [{"version": "v1", "created": "Tue, 8 Aug 2017 19:40:09 GMT"}], "update_date": "2017-08-10", "authors_parsed": [["Chen", "Pin-Yu", ""], ["Hero", "Alfred O.", ""]]}, {"id": "1708.02635", "submitter": "Doyup Lee", "authors": "Doyup Lee", "title": "Anomaly Detection in Multivariate Non-stationary Time Series for\n  Automatic DBMS Diagnosis", "comments": "8 pages", "journal-ref": null, "doi": "10.1109/ICMLA.2017.0-126", "report-no": null, "categories": "stat.ML cs.LG stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Anomaly detection in database management systems (DBMSs) is difficult because\nof increasing number of statistics (stat) and event metrics in big data system.\nIn this paper, I propose an automatic DBMS diagnosis system that detects\nanomaly periods with abnormal DB stat metrics and finds causal events in the\nperiods. Reconstruction error from deep autoencoder and statistical process\ncontrol approach are applied to detect time period with anomalies. Related\nevents are found using time series similarity measures between events and\nabnormal stat metrics. After training deep autoencoder with DBMS metric data,\nefficacy of anomaly detection is investigated from other DBMSs containing\nanomalies. Experiment results show effectiveness of proposed model, especially,\nbatch temporal normalization layer. Proposed model is used for publishing\nautomatic DBMS diagnosis reports in order to determine DBMS configuration and\nSQL tuning.\n", "versions": [{"version": "v1", "created": "Tue, 8 Aug 2017 20:04:19 GMT"}, {"version": "v2", "created": "Mon, 9 Oct 2017 23:54:22 GMT"}], "update_date": "2018-01-26", "authors_parsed": [["Lee", "Doyup", ""]]}, {"id": "1708.02663", "submitter": "Mohamed Amine Bouhlel Dr", "authors": "Mohamed Amine Bouhlel and Joaquim R. R. A. Martins", "title": "Gradient-enhanced kriging for high-dimensional problems", "comments": "27 pages, 5 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Surrogate models provide a low computational cost alternative to evaluating\nexpensive functions. The construction of accurate surrogate models with large\nnumbers of independent variables is currently prohibitive because it requires a\nlarge number of function evaluations. Gradient-enhanced kriging has the\npotential to reduce the number of function evaluations for the desired accuracy\nwhen efficient gradient computation, such as an adjoint method, is available.\nHowever, current gradient-enhanced kriging methods do not scale well with the\nnumber of sampling points due to the rapid growth in the size of the\ncorrelation matrix where new information is added for each sampling point in\neach direction of the design space. They do not scale well with the number of\nindependent variables either due to the increase in the number of\nhyperparameters that needs to be estimated. To address this issue, we develop a\nnew gradient-enhanced surrogate model approach that drastically reduced the\nnumber of hyperparameters through the use of the partial-least squares method\nthat maintains accuracy. In addition, this method is able to control the size\nof the correlation matrix by adding only relevant points defined through the\ninformation provided by the partial-least squares method. To validate our\nmethod, we compare the global accuracy of the proposed method with conventional\nkriging surrogate models on two analytic functions with up to 100 dimensions,\nas well as engineering problems of varied complexity with up to 15 dimensions.\nWe show that the proposed method requires fewer sampling points than\nconventional methods to obtain the desired accuracy, or provides more accuracy\nfor a fixed budget of sampling points. In some cases, we get over 3 times more\naccurate models than a bench of surrogate models from the literature, and also\nover 3200 times faster than standard gradient-enhanced kriging models.\n", "versions": [{"version": "v1", "created": "Tue, 8 Aug 2017 21:58:49 GMT"}], "update_date": "2017-08-10", "authors_parsed": [["Bouhlel", "Mohamed Amine", ""], ["Martins", "Joaquim R. R. A.", ""]]}, {"id": "1708.02666", "submitter": "Kush Varshney", "authors": "Been Kim, Dmitry M. Malioutov, Kush R. Varshney, Adrian Weller", "title": "Proceedings of the 2017 ICML Workshop on Human Interpretability in\n  Machine Learning (WHI 2017)", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This is the Proceedings of the 2017 ICML Workshop on Human Interpretability\nin Machine Learning (WHI 2017), which was held in Sydney, Australia, August 10,\n2017. Invited speakers were Tony Jebara, Pang Wei Koh, and David Sontag.\n", "versions": [{"version": "v1", "created": "Tue, 8 Aug 2017 22:21:11 GMT"}], "update_date": "2017-08-10", "authors_parsed": [["Kim", "Been", ""], ["Malioutov", "Dmitry M.", ""], ["Varshney", "Kush R.", ""], ["Weller", "Adrian", ""]]}, {"id": "1708.02691", "submitter": "Boris Hanin", "authors": "Boris Hanin", "title": "Universal Function Approximation by Deep Neural Nets with Bounded Width\n  and ReLU Activations", "comments": "v3. Theorem 3 removed. Comments Welcome. 9p", "journal-ref": "Mathematics 2019, 7(10), 992", "doi": "10.3390/math7100992", "report-no": null, "categories": "stat.ML cs.CG cs.LG math.FA math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This article concerns the expressive power of depth in neural nets with ReLU\nactivations and bounded width. We are particularly interested in the following\nquestions: what is the minimal width $w_{\\text{min}}(d)$ so that ReLU nets of\nwidth $w_{\\text{min}}(d)$ (and arbitrary depth) can approximate any continuous\nfunction on the unit cube $[0,1]^d$ aribitrarily well? For ReLU nets near this\nminimal width, what can one say about the depth necessary to approximate a\ngiven function? Our approach to this paper is based on the observation that,\ndue to the convexity of the ReLU activation, ReLU nets are particularly\nwell-suited for representing convex functions. In particular, we prove that\nReLU nets with width $d+1$ can approximate any continuous convex function of\n$d$ variables arbitrarily well. These results then give quantitative depth\nestimates for the rate of approximation of any continuous scalar function on\nthe $d$-dimensional cube $[0,1]^d$ by ReLU nets with width $d+3.$\n", "versions": [{"version": "v1", "created": "Wed, 9 Aug 2017 01:37:21 GMT"}, {"version": "v2", "created": "Mon, 21 Aug 2017 22:40:55 GMT"}, {"version": "v3", "created": "Wed, 20 Dec 2017 17:38:26 GMT"}], "update_date": "2019-10-22", "authors_parsed": [["Hanin", "Boris", ""]]}, {"id": "1708.02735", "submitter": "Stanislav Fort", "authors": "Stanislav Fort", "title": "Gaussian Prototypical Networks for Few-Shot Learning on Omniglot", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV cs.NE stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a novel architecture for $k$-shot classification on the Omniglot\ndataset. Building on prototypical networks, we extend their architecture to\nwhat we call Gaussian prototypical networks. Prototypical networks learn a map\nbetween images and embedding vectors, and use their clustering for\nclassification. In our model, a part of the encoder output is interpreted as a\nconfidence region estimate about the embedding point, and expressed as a\nGaussian covariance matrix. Our network then constructs a direction and class\ndependent distance metric on the embedding space, using uncertainties of\nindividual data points as weights. We show that Gaussian prototypical networks\nare a preferred architecture over vanilla prototypical networks with an\nequivalent number of parameters. We report state-of-the-art performance in\n1-shot and 5-shot classification both in 5-way and 20-way regime (for 5-shot\n5-way, we are comparable to previous state-of-the-art) on the Omniglot dataset.\nWe explore artificially down-sampling a fraction of images in the training set,\nwhich improves our performance even further. We therefore hypothesize that\nGaussian prototypical networks might perform better in less homogeneous,\nnoisier datasets, which are commonplace in real world applications.\n", "versions": [{"version": "v1", "created": "Wed, 9 Aug 2017 06:53:31 GMT"}], "update_date": "2017-08-10", "authors_parsed": [["Fort", "Stanislav", ""]]}, {"id": "1708.02867", "submitter": "Mostafa Shehata", "authors": "Mostafa A. Shehata, Mohammad Nassef and Amr A. Badr", "title": "Simulated Annealing with Levy Distribution for Fast Matrix\n  Factorization-Based Collaborative Filtering", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.IR stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Matrix factorization is one of the best approaches for collaborative\nfiltering, because of its high accuracy in presenting users and items latent\nfactors. The main disadvantages of matrix factorization are its complexity, and\nbeing very hard to be parallelized, specially with very large matrices. In this\npaper, we introduce a new method for collaborative filtering based on Matrix\nFactorization by combining simulated annealing with levy distribution. By using\nthis method, good solutions are achieved in acceptable time with low\ncomputations, compared to other methods like stochastic gradient descent,\nalternating least squares, and weighted non-negative matrix factorization.\n", "versions": [{"version": "v1", "created": "Wed, 9 Aug 2017 15:14:54 GMT"}], "update_date": "2017-08-10", "authors_parsed": [["Shehata", "Mostafa A.", ""], ["Nassef", "Mohammad", ""], ["Badr", "Amr A.", ""]]}, {"id": "1708.02883", "submitter": "Wing-Kin Ma", "authors": "Chia-Hsiang Lin, Ruiyuan Wu, Wing-Kin Ma, Chong-Yung Chi, and Yue Wang", "title": "Maximum Volume Inscribed Ellipsoid: A New Simplex-Structured Matrix\n  Factorization Framework via Facet Enumeration and Convex Optimization", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Consider a structured matrix factorization model where one factor is\nrestricted to have its columns lying in the unit simplex. This\nsimplex-structured matrix factorization (SSMF) model and the associated\nfactorization techniques have spurred much interest in research topics over\ndifferent areas, such as hyperspectral unmixing in remote sensing, topic\ndiscovery in machine learning, to name a few. In this paper we develop a new\ntheoretical SSMF framework whose idea is to study a maximum volume ellipsoid\ninscribed in the convex hull of the data points. This maximum volume inscribed\nellipsoid (MVIE) idea has not been attempted in prior literature, and we show a\nsufficient condition under which the MVIE framework guarantees exact recovery\nof the factors. The sufficient recovery condition we show for MVIE is much more\nrelaxed than that of separable non-negative matrix factorization (or pure-pixel\nsearch); coincidentally it is also identical to that of minimum volume\nenclosing simplex, which is known to be a powerful SSMF framework for\nnon-separable problem instances. We also show that MVIE can be practically\nimplemented by performing facet enumeration and then by solving a convex\noptimization problem. The potential of the MVIE framework is illustrated by\nnumerical results.\n", "versions": [{"version": "v1", "created": "Wed, 9 Aug 2017 15:48:58 GMT"}, {"version": "v2", "created": "Tue, 23 Jan 2018 08:41:32 GMT"}, {"version": "v3", "created": "Thu, 21 Jun 2018 07:36:24 GMT"}], "update_date": "2018-06-22", "authors_parsed": [["Lin", "Chia-Hsiang", ""], ["Wu", "Ruiyuan", ""], ["Ma", "Wing-Kin", ""], ["Chi", "Chong-Yung", ""], ["Wang", "Yue", ""]]}, {"id": "1708.02900", "submitter": "Vincent Wong", "authors": "Vincent Wong and Yaneer Bar-Yam", "title": "How Do People Differ? A Social Media Approach", "comments": "13 pages, 4 figures", "journal-ref": null, "doi": null, "report-no": "New England Complex Systems Institute 2017-08-01", "categories": "physics.soc-ph cs.SI nlin.AO physics.data-an stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Research from a variety of fields including psychology and linguistics have\nfound correlations and patterns in personal attributes and behavior, but\nefforts to understand the broader heterogeneity in human behavior have not yet\nintegrated these approaches and perspectives with a cohesive methodology. Here\nwe extract patterns in behavior and relate those patterns together in a\nhigh-dimensional picture. We use dimension reduction to analyze word usage in\ntext data from the online discussion platform Reddit. We find that pronouns can\nbe used to characterize the space of the two most prominent dimensions that\ncapture the greatest differences in word usage, even though pronouns were not\nincluded in the determination of those dimensions. These patterns overlap with\npatterns of topics of discussion to reveal relationships between pronouns and\ntopics that can describe the user population. This analysis corroborates\nfindings from past research that have identified word use differences across\npopulations and synthesizes them relative to one another. We believe this is a\nstep toward understanding how differences between people are related to each\nother.\n", "versions": [{"version": "v1", "created": "Wed, 9 Aug 2017 16:30:37 GMT"}], "update_date": "2017-08-10", "authors_parsed": [["Wong", "Vincent", ""], ["Bar-Yam", "Yaneer", ""]]}, {"id": "1708.02918", "submitter": "Volker Tresp", "authors": "Volker Tresp and Yunpu Ma", "title": "The Tensor Memory Hypothesis", "comments": "Presented at MLINI-2016 workshop, 2016 (arXiv:1701.01437) Report-no:\n  MLINI/2016/06", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.LG q-bio.NC stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We discuss memory models which are based on tensor decompositions using\nlatent representations of entities and events. We show how episodic memory and\nsemantic memory can be realized and discuss how new memory traces can be\ngenerated from sensory input: Existing memories are the basis for perception\nand new memories are generated via perception. We relate our mathematical\napproach to the hippocampal memory indexing theory. We describe the first\ndetailed mathematical models for the complete processing pipeline from sensory\ninput and its semantic decoding, i.e., perception, to the formation of episodic\nand semantic memories and their declarative semantic decodings. Our main\nhypothesis is that perception includes an active semantic decoding process,\nwhich relies on latent representations of entities and predicates, and that\nepisodic and semantic memories depend on the same decoding process. We\ncontribute to the debate between the leading memory consolidation theories,\ni.e., the standard consolidation theory (SCT) and the multiple trace theory\n(MTT). The latter is closely related to the complementary learning systems\n(CLS) framework. In particular, we show explicitly how episodic memory can\nteach the neocortex to form a semantic memory, which is a core issue in MTT and\nCLS.\n", "versions": [{"version": "v1", "created": "Wed, 9 Aug 2017 17:22:18 GMT"}, {"version": "v2", "created": "Mon, 28 Aug 2017 14:20:57 GMT"}], "update_date": "2017-08-29", "authors_parsed": [["Tresp", "Volker", ""], ["Ma", "Yunpu", ""]]}, {"id": "1708.02949", "submitter": "Benjamin Nachman", "authors": "Eric M. Metodiev, Benjamin Nachman, Jesse Thaler", "title": "Classification without labels: Learning from mixed samples in high\n  energy physics", "comments": "18 pages, 5 figures; v2: intro extended and references added; v3:\n  additional discussion to match JHEP version", "journal-ref": "JHEP 10 (2017) 174", "doi": "10.1007/JHEP10(2017)174", "report-no": "MIT--CTP 4922", "categories": "hep-ph hep-ex stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Modern machine learning techniques can be used to construct powerful models\nfor difficult collider physics problems. In many applications, however, these\nmodels are trained on imperfect simulations due to a lack of truth-level\ninformation in the data, which risks the model learning artifacts of the\nsimulation. In this paper, we introduce the paradigm of classification without\nlabels (CWoLa) in which a classifier is trained to distinguish statistical\nmixtures of classes, which are common in collider physics. Crucially, neither\nindividual labels nor class proportions are required, yet we prove that the\noptimal classifier in the CWoLa paradigm is also the optimal classifier in the\ntraditional fully-supervised case where all label information is available.\nAfter demonstrating the power of this method in an analytical toy example, we\nconsider a realistic benchmark for collider physics: distinguishing quark-\nversus gluon-initiated jets using mixed quark/gluon training samples. More\ngenerally, CWoLa can be applied to any classification problem where labels or\nclass proportions are unknown or simulations are unreliable, but statistical\nmixtures of the classes are available.\n", "versions": [{"version": "v1", "created": "Wed, 9 Aug 2017 18:00:06 GMT"}, {"version": "v2", "created": "Fri, 8 Sep 2017 04:22:03 GMT"}, {"version": "v3", "created": "Sat, 18 Nov 2017 19:21:43 GMT"}], "update_date": "2017-11-21", "authors_parsed": [["Metodiev", "Eric M.", ""], ["Nachman", "Benjamin", ""], ["Thaler", "Jesse", ""]]}, {"id": "1708.02975", "submitter": "Daniel Hsu", "authors": "Daniel Hsu", "title": "Anomaly Detection on Graph Time Series", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.NE stat.ML", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  In this paper, we use variational recurrent neural network to investigate the\nanomaly detection problem on graph time series. The temporal correlation is\nmodeled by the combination of recurrent neural network (RNN) and variational\ninference (VI), while the spatial information is captured by the graph\nconvolutional network. In order to incorporate external factors, we use feature\nextractor to augment the transition of latent variables, which can learn the\ninfluence of external factors. With the target function as accumulative ELBO,\nit is easy to extend this model to on-line method. The experimental study on\ntraffic flow data shows the detection capability of the proposed method.\n", "versions": [{"version": "v1", "created": "Wed, 9 Aug 2017 19:15:56 GMT"}, {"version": "v2", "created": "Wed, 1 Nov 2017 23:50:17 GMT"}], "update_date": "2017-11-03", "authors_parsed": [["Hsu", "Daniel", ""]]}, {"id": "1708.02979", "submitter": "Andrei Turkin", "authors": "Andrei Turkin", "title": "Tikhonov Regularization for Long Short-Term Memory Networks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.NE stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  It is a well-known fact that adding noise to the input data often improves\nnetwork performance. While the dropout technique may be a cause of memory loss,\nwhen it is applied to recurrent connections, Tikhonov regularization, which can\nbe regarded as the training with additive noise, avoids this issue naturally,\nthough it implies regularizer derivation for different architectures. In case\nof feedforward neural networks this is straightforward, while for networks with\nrecurrent connections and complicated layers it leads to some difficulties. In\nthis paper, a Tikhonov regularizer is derived for Long-Short Term Memory (LSTM)\nnetworks. Although it is independent of time for simplicity, it considers\ninteraction between weights of the LSTM unit, which in theory makes it possible\nto regularize the unit with complicated dependences by using only one parameter\nthat measures the input data perturbation. The regularizer that is proposed in\nthis paper has three parameters: one to control the regularization process, and\nother two to maintain computation stability while the network is being trained.\nThe theory developed in this paper can be applied to get such regularizers for\ndifferent recurrent neural networks with Hadamard products and Lipschitz\ncontinuous functions.\n", "versions": [{"version": "v1", "created": "Wed, 9 Aug 2017 19:34:26 GMT"}], "update_date": "2017-08-11", "authors_parsed": [["Turkin", "Andrei", ""]]}, {"id": "1708.02999", "submitter": "Mohammadreza Soltani", "authors": "Mohammadreza Soltani and Chinmay Hegde", "title": "Demixing Structured Superposition Signals from Periodic and Aperiodic\n  Nonlinear Observations", "comments": "arXiv admin note: substantial text overlap with arXiv:1701.06597", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider the demixing problem of two (or more) structured high-dimensional\nvectors from a limited number of nonlinear observations where this nonlinearity\nis due to either a periodic or an aperiodic function. We study certain families\nof structured superposition models, and propose a method which provably\nrecovers the components given (nearly) $m = \\mathcal{O}(s)$ samples where $s$\ndenotes the sparsity level of the underlying components. This strictly improves\nupon previous nonlinear demixing techniques and asymptotically matches the best\npossible sample complexity. We also provide a range of simulations to\nillustrate the performance of the proposed algorithms.\n", "versions": [{"version": "v1", "created": "Tue, 8 Aug 2017 16:16:17 GMT"}], "update_date": "2017-08-11", "authors_parsed": [["Soltani", "Mohammadreza", ""], ["Hegde", "Chinmay", ""]]}, {"id": "1708.03020", "submitter": "Yining Wang", "authors": "Xi Chen, Yining Wang, Yu-Xiang Wang", "title": "Non-stationary Stochastic Optimization under $L_{p,q}$-Variation\n  Measures", "comments": "38 pages, 3 figures. Revised version", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider a non-stationary sequential stochastic optimization problem, in\nwhich the underlying cost functions change over time under a variation budget\nconstraint. We propose an $L_{p,q}$-variation functional to quantify the\nchange, which yields less variation for dynamic function sequences whose\nchanges are constrained to short time periods or small subsets of input domain.\nUnder the $L_{p,q}$-variation constraint, we derive both upper and matching\nlower regret bounds for smooth and strongly convex function sequences, which\ngeneralize previous results in Besbes et al. (2015). Furthermore, we provide an\nupper bound for general convex function sequences with noisy gradient feedback,\nwhich matches the optimal rate as $p\\to\\infty$. Our results reveal some\nsurprising phenomena under this general variation functional, such as the curse\nof dimensionality of the function domain. The key technical novelties in our\nanalysis include affinity lemmas that characterize the distance of the\nminimizers of two convex functions with bounded Lp difference, and a cubic\nspline based construction that attains matching lower bounds.\n", "versions": [{"version": "v1", "created": "Wed, 9 Aug 2017 21:36:38 GMT"}, {"version": "v2", "created": "Thu, 31 Aug 2017 18:45:41 GMT"}, {"version": "v3", "created": "Fri, 11 May 2018 16:04:56 GMT"}], "update_date": "2018-05-14", "authors_parsed": [["Chen", "Xi", ""], ["Wang", "Yining", ""], ["Wang", "Yu-Xiang", ""]]}, {"id": "1708.03027", "submitter": "Rongrong Zhang", "authors": "Rongrong Zhang, Wei Deng, Michael Yu Zhu", "title": "Using Deep Neural Networks to Automate Large Scale Statistical Analysis\n  for Big Data Applications", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Statistical analysis (SA) is a complex process to deduce population\nproperties from analysis of data. It usually takes a well-trained analyst to\nsuccessfully perform SA, and it becomes extremely challenging to apply SA to\nbig data applications. We propose to use deep neural networks to automate the\nSA process. In particular, we propose to construct convolutional neural\nnetworks (CNNs) to perform automatic model selection and parameter estimation,\ntwo most important SA tasks. We refer to the resulting CNNs as the neural model\nselector and the neural model estimator, respectively, which can be properly\ntrained using labeled data systematically generated from candidate models.\nSimulation study shows that both the selector and estimator demonstrate\nexcellent performances. The idea and proposed framework can be further extended\nto automate the entire SA process and have the potential to revolutionize how\nSA is performed in big data analytics.\n", "versions": [{"version": "v1", "created": "Wed, 9 Aug 2017 22:34:36 GMT"}], "update_date": "2017-08-11", "authors_parsed": [["Zhang", "Rongrong", ""], ["Deng", "Wei", ""], ["Zhu", "Michael Yu", ""]]}, {"id": "1708.03046", "submitter": "Weijie J. Su", "authors": "Weijie J. Su", "title": "When Is the First Spurious Variable Selected by Sequential Regression\n  Procedures?", "comments": "Accepted by Biometrika", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.ML stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Applied statisticians use sequential regression procedures to produce a\nranking of explanatory variables and, in settings of low correlations between\nvariables and strong true effect sizes, expect that variables at the very top\nof this ranking are truly relevant to the response. In a regime of certain\nsparsity levels, however, three examples of sequential procedures--forward\nstepwise, the lasso, and least angle regression--are shown to include the first\nspurious variable unexpectedly early. We derive a rigorous, sharp prediction of\nthe rank of the first spurious variable for these three procedures,\ndemonstrating that the first spurious variable occurs earlier and earlier as\nthe regression coefficients become denser. This counterintuitive phenomenon\npersists for statistically independent Gaussian random designs and an\narbitrarily large magnitude of the true effects. We gain a better understanding\nof the phenomenon by identifying the underlying cause and then leverage the\ninsights to introduce a simple visualization tool termed the double-ranking\ndiagram to improve on sequential methods. As a byproduct of these findings, we\nobtain the first provable result certifying the exact equivalence between the\nlasso and least angle regression in the early stages of solution paths beyond\northogonal designs. This equivalence can seamlessly carry over many important\nmodel selection results concerning the lasso to least angle regression.\n", "versions": [{"version": "v1", "created": "Thu, 10 Aug 2017 01:56:16 GMT"}, {"version": "v2", "created": "Wed, 11 Jul 2018 16:02:59 GMT"}], "update_date": "2018-07-12", "authors_parsed": [["Su", "Weijie J.", ""]]}, {"id": "1708.03052", "submitter": "Lee Gao", "authors": "Lee Gao, Ronghuo Zheng", "title": "Communication-Free Parallel Supervised Topic Models", "comments": "8 pages, 7 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CL cs.IR stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Embarrassingly (communication-free) parallel Markov chain Monte Carlo (MCMC)\nmethods are commonly used in learning graphical models. However, MCMC cannot be\ndirectly applied in learning topic models because of the quasi-ergodicity\nproblem caused by multimodal distribution of topics. In this paper, we develop\nan embarrassingly parallel MCMC algorithm for sLDA. Our algorithm works by\nswitching the order of sampled topics combination and labeling variable\nprediction in sLDA, in which it overcomes the quasi-ergodicity problem because\nhigh-dimension topics that follow a multimodal distribution are projected into\none-dimension document labels that follow a unimodal distribution. Our\nempirical experiments confirm that the out-of-sample prediction performance\nusing our embarrassingly parallel algorithm is comparable to non-parallel sLDA\nwhile the computation time is significantly reduced.\n", "versions": [{"version": "v1", "created": "Thu, 10 Aug 2017 02:03:52 GMT"}], "update_date": "2017-08-11", "authors_parsed": [["Gao", "Lee", ""], ["Zheng", "Ronghuo", ""]]}, {"id": "1708.03131", "submitter": "Daniil Ryabko", "authors": "Daniil Ryabko", "title": "Hypotheses testing on infinite random graphs", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.IT math.IT math.ST stat.ML stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Drawing on some recent results that provide the formalism necessary to\ndefinite stationarity for infinite random graphs, this paper initiates the\nstudy of statistical and learning questions pertaining to these objects.\nSpecifically, a criterion for the existence of a consistent test for complex\nhypotheses is presented, generalizing the corresponding results on time series.\nAs an application, it is shown how one can test that a tree has the Markov\nproperty, or, more generally, to estimate its memory.\n", "versions": [{"version": "v1", "created": "Thu, 10 Aug 2017 09:11:31 GMT"}], "update_date": "2017-08-11", "authors_parsed": [["Ryabko", "Daniil", ""]]}, {"id": "1708.03218", "submitter": "Farhad Pourkamali-Anaraki", "authors": "Farhad Pourkamali-Anaraki, Stephen Becker", "title": "Improved Fixed-Rank Nystr\\\"om Approximation via QR Decomposition:\n  Practical and Theoretical Aspects", "comments": "Accepted in Neurocomputing. arXiv admin note: text overlap with\n  arXiv:1612.06470", "journal-ref": "Neurocomputing, 2019", "doi": "10.1016/j.neucom.2019.06.070", "report-no": null, "categories": "stat.ML cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The Nystrom method is a popular technique that uses a small number of\nlandmark points to compute a fixed-rank approximation of large kernel matrices\nthat arise in machine learning problems. In practice, to ensure high quality\napproximations, the number of landmark points is chosen to be greater than the\ntarget rank. However, for simplicity the standard Nystrom method uses a\nsub-optimal procedure for rank reduction. In this paper, we examine the\ndrawbacks of the standard Nystrom method in terms of poor performance and lack\nof theoretical guarantees. To address these issues, we present an efficient\nmodification for generating improved fixed-rank Nystrom approximations.\nTheoretical analysis and numerical experiments are provided to demonstrate the\nadvantages of the modified method over the standard Nystrom method. Overall,\nthe aim of this paper is to convince researchers to use the modified method, as\nit has nearly identical computational complexity, is easy to code, has greatly\nimproved accuracy in many cases, and is optimal in a sense that we make\nprecise.\n", "versions": [{"version": "v1", "created": "Tue, 8 Aug 2017 23:52:53 GMT"}, {"version": "v2", "created": "Mon, 2 Dec 2019 15:11:19 GMT"}], "update_date": "2019-12-04", "authors_parsed": [["Pourkamali-Anaraki", "Farhad", ""], ["Becker", "Stephen", ""]]}, {"id": "1708.03229", "submitter": "Yanshuai Cao", "authors": "Yanshuai Cao, Luyu Wang", "title": "Automatic Selection of t-SNE Perplexity", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.LG stat.AP stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  t-Distributed Stochastic Neighbor Embedding (t-SNE) is one of the most widely\nused dimensionality reduction methods for data visualization, but it has a\nperplexity hyperparameter that requires manual selection. In practice, proper\ntuning of t-SNE perplexity requires users to understand the inner working of\nthe method as well as to have hands-on experience. We propose a model selection\nobjective for t-SNE perplexity that requires negligible extra computation\nbeyond that of the t-SNE itself. We empirically validate that the perplexity\nsettings found by our approach are consistent with preferences elicited from\nhuman experts across a number of datasets. The similarities of our approach to\nBayesian information criteria (BIC) and minimum description length (MDL) are\nalso analyzed.\n", "versions": [{"version": "v1", "created": "Thu, 10 Aug 2017 14:19:20 GMT"}], "update_date": "2017-08-11", "authors_parsed": [["Cao", "Yanshuai", ""], ["Wang", "Luyu", ""]]}, {"id": "1708.03288", "submitter": "Peter Radchenko", "authors": "Rahul Mazumder and Peter Radchenko and Antoine Dedieu", "title": "Subset Selection with Shrinkage: Sparse Linear Modeling when the SNR is\n  low", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME math.OC math.ST stat.CO stat.ML stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study a seemingly unexpected and relatively less understood overfitting\naspect of a fundamental tool in sparse linear modeling - best subset selection,\nwhich minimizes the residual sum of squares subject to a constraint on the\nnumber of nonzero coefficients. While the best subset selection procedure is\noften perceived as the \"gold standard\" in sparse learning when the signal to\nnoise ratio (SNR) is high, its predictive performance deteriorates when the SNR\nis low. In particular, it is outperformed by continuous shrinkage methods, such\nas ridge regression and the Lasso. We investigate the behavior of best subset\nselection in the high-noise regimes and propose an alternative approach based\non a regularized version of the least-squares criterion. Our proposed\nestimators (a) mitigate, to a large extent, the poor predictive performance of\nbest subset selection in the high-noise regimes; and (b) perform favorably,\nwhile generally delivering substantially sparser models, relative to the best\npredictive models available via ridge regression and the Lasso. We conduct an\nextensive theoretical analysis of the predictive properties of the proposed\napproach and provide justification for its superior predictive performance\nrelative to best subset selection when the noise-level is high. Our estimators\ncan be expressed as solutions to mixed integer second order conic optimization\nproblems and, hence, are amenable to modern computational tools from\nmathematical optimization.\n", "versions": [{"version": "v1", "created": "Thu, 10 Aug 2017 16:28:39 GMT"}, {"version": "v2", "created": "Fri, 12 Jun 2020 18:30:37 GMT"}, {"version": "v3", "created": "Wed, 2 Jun 2021 04:23:46 GMT"}], "update_date": "2021-06-03", "authors_parsed": [["Mazumder", "Rahul", ""], ["Radchenko", "Peter", ""], ["Dedieu", "Antoine", ""]]}, {"id": "1708.03392", "submitter": "Marinka Zitnik", "authors": "Marinka Zitnik and Blaz Zupan", "title": "Jumping across biomedical contexts using compressive data fusion", "comments": "In Proceedings of the 24th International Conference on Intelligent\n  Systems for Molecular Biology (ISMB), 2016", "journal-ref": "Bioinformatics, 32 (12): i90-i100 (2016)", "doi": null, "report-no": null, "categories": "cs.LG q-bio.MN stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Motivation: The rapid growth of diverse biological data allows us to consider\ninteractions between a variety of objects, such as genes, chemicals, molecular\nsignatures, diseases, pathways and environmental exposures. Often, any pair of\nobjects--such as a gene and a disease--can be related in different ways, for\nexample, directly via gene-disease associations or indirectly via functional\nannotations, chemicals and pathways. Different ways of relating these objects\ncarry different semantic meanings. However, traditional methods disregard these\nsemantics and thus cannot fully exploit their value in data modeling.\n  Results: We present Medusa, an approach to detect size-k modules of objects\nthat, taken together, appear most significant to another set of objects. Medusa\noperates on large-scale collections of heterogeneous data sets and explicitly\ndistinguishes between diverse data semantics. It advances research along two\ndimensions: it builds on collective matrix factorization to derive different\nsemantics, and it formulates the growing of the modules as a submodular\noptimization program. Medusa is flexible in choosing or combining semantic\nmeanings and provides theoretical guarantees about detection quality. In a\nsystematic study on 310 complex diseases, we show the effectiveness of Medusa\nin associating genes with diseases and detecting disease modules. We\ndemonstrate that in predicting gene-disease associations Medusa compares\nfavorably to methods that ignore diverse semantic meanings. We find that the\nutility of different semantics depends on disease categories and that, overall,\nMedusa recovers disease modules more accurately when combining different\nsemantics.\n", "versions": [{"version": "v1", "created": "Thu, 10 Aug 2017 21:39:54 GMT"}], "update_date": "2017-08-14", "authors_parsed": [["Zitnik", "Marinka", ""], ["Zupan", "Blaz", ""]]}, {"id": "1708.03498", "submitter": "Sjoerd van Steenkiste", "authors": "Klaus Greff, Sjoerd van Steenkiste, J\\\"urgen Schmidhuber", "title": "Neural Expectation Maximization", "comments": "Accepted to NIPS 2017", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.NE stat.ML", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Many real world tasks such as reasoning and physical interaction require\nidentification and manipulation of conceptual entities. A first step towards\nsolving these tasks is the automated discovery of distributed symbol-like\nrepresentations. In this paper, we explicitly formalize this problem as\ninference in a spatial mixture model where each component is parametrized by a\nneural network. Based on the Expectation Maximization framework we then derive\na differentiable clustering method that simultaneously learns how to group and\nrepresent individual entities. We evaluate our method on the (sequential)\nperceptual grouping task and find that it is able to accurately recover the\nconstituent objects. We demonstrate that the learned representations are useful\nfor next-step prediction.\n", "versions": [{"version": "v1", "created": "Fri, 11 Aug 2017 10:17:23 GMT"}, {"version": "v2", "created": "Sat, 4 Nov 2017 15:14:47 GMT"}], "update_date": "2017-11-07", "authors_parsed": [["Greff", "Klaus", ""], ["van Steenkiste", "Sjoerd", ""], ["Schmidhuber", "J\u00fcrgen", ""]]}, {"id": "1708.03665", "submitter": "Stephen Edwards", "authors": "Dominique T. Shipmon, Jason M. Gurevitch, Paolo M. Piselli and Stephen\n  T. Edwards", "title": "Time Series Anomaly Detection; Detection of anomalous drops with limited\n  features and sparse examples in noisy highly periodic data", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Google uses continuous streams of data from industry partners in order to\ndeliver accurate results to users. Unexpected drops in traffic can be an\nindication of an underlying issue and may be an early warning that remedial\naction may be necessary. Detecting such drops is non-trivial because streams\nare variable and noisy, with roughly regular spikes (in many different shapes)\nin traffic data. We investigated the question of whether or not we can predict\nanomalies in these data streams. Our goal is to utilize Machine Learning and\nstatistical approaches to classify anomalous drops in periodic, but noisy,\ntraffic patterns. Since we do not have a large body of labeled examples to\ndirectly apply supervised learning for anomaly classification, we approached\nthe problem in two parts. First we used TensorFlow to train our various models\nincluding DNNs, RNNs, and LSTMs to perform regression and predict the expected\nvalue in the time series. Secondly we created anomaly detection rules that\ncompared the actual values to predicted values. Since the problem requires\nfinding sustained anomalies, rather than just short delays or momentary\ninactivity in the data, our two detection methods focused on continuous\nsections of activity rather than just single points. We tried multiple\ncombinations of our models and rules and found that using the intersection of\nour two anomaly detection methods proved to be an effective method of detecting\nanomalies on almost all of our models. In the process we also found that not\nall data fell within our experimental assumptions, as one data stream had no\nperiodicity, and therefore no time based model could predict it.\n", "versions": [{"version": "v1", "created": "Fri, 11 Aug 2017 19:04:53 GMT"}], "update_date": "2017-08-15", "authors_parsed": [["Shipmon", "Dominique T.", ""], ["Gurevitch", "Jason M.", ""], ["Piselli", "Paolo M.", ""], ["Edwards", "Stephen T.", ""]]}, {"id": "1708.03704", "submitter": "Alan Mosca", "authors": "Alan Mosca, George D Magoulas", "title": "Deep Incremental Boosting", "comments": null, "journal-ref": "Christoph Benzm\\\"uller, Geoff Sutcliffe and Raul Rojas (editors).\n  GCAI 2016. 2nd Global Conference on Artificial Intelligence, vol 41, pages\n  293--302", "doi": null, "report-no": null, "categories": "stat.ML cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper introduces Deep Incremental Boosting, a new technique derived from\nAdaBoost, specifically adapted to work with Deep Learning methods, that reduces\nthe required training time and improves generalisation. We draw inspiration\nfrom Transfer of Learning approaches to reduce the start-up time to training\neach incremental Ensemble member. We show a set of experiments that outlines\nsome preliminary results on some common Deep Learning datasets and discuss the\npotential improvements Deep Incremental Boosting brings to traditional Ensemble\nmethods in Deep Learning.\n", "versions": [{"version": "v1", "created": "Fri, 11 Aug 2017 21:05:58 GMT"}], "update_date": "2017-08-15", "authors_parsed": [["Mosca", "Alan", ""], ["Magoulas", "George D", ""]]}, {"id": "1708.03731", "submitter": "Jan N. van Rijn PhD", "authors": "Bernd Bischl, Giuseppe Casalicchio, Matthias Feurer, Frank Hutter,\n  Michel Lang, Rafael G. Mantovani, Jan N. van Rijn, Joaquin Vanschoren", "title": "OpenML Benchmarking Suites", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Machine learning research depends on objectively interpretable, comparable,\nand reproducible algorithm benchmarks. Therefore, we advocate the use of\ncurated, comprehensive suites of machine learning tasks to standardize the\nsetup, execution, and reporting of benchmarks. We enable this through software\ntools that help to create and leverage these benchmarking suites. These are\nseamlessly integrated into the OpenML platform, and accessible through\ninterfaces in Python, Java, and R. OpenML benchmarking suites are (a) easy to\nuse through standardized data formats, APIs, and client libraries; (b)\nmachine-readable, with extensive meta-information on the included datasets; and\n(c) allow benchmarks to be shared and reused in future studies. We also present\na first, carefully curated and practical benchmarking suite for classification:\nthe OpenML Curated Classification benchmarking suite 2018 (OpenML-CC18).\n", "versions": [{"version": "v1", "created": "Fri, 11 Aug 2017 23:28:48 GMT"}, {"version": "v2", "created": "Tue, 24 Sep 2019 16:02:48 GMT"}], "update_date": "2019-09-25", "authors_parsed": [["Bischl", "Bernd", ""], ["Casalicchio", "Giuseppe", ""], ["Feurer", "Matthias", ""], ["Hutter", "Frank", ""], ["Lang", "Michel", ""], ["Mantovani", "Rafael G.", ""], ["van Rijn", "Jan N.", ""], ["Vanschoren", "Joaquin", ""]]}, {"id": "1708.03735", "submitter": "Anirbit Mukherjee", "authors": "Akshay Rangamani, Anirbit Mukherjee, Amitabh Basu, Tejaswini\n  Ganapathy, Ashish Arora, Sang Chin and Trac D. Tran", "title": "Sparse Coding and Autoencoders", "comments": "In this new version of the paper with a small change in the\n  distributional assumptions we are actually able to prove the asymptotic\n  criticality of a neighbourhood of the ground truth dictionary for even just\n  the standard squared loss of the ReLU autoencoder (unlike the regularized\n  loss in the older version)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG math.OC stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In \"Dictionary Learning\" one tries to recover incoherent matrices $A^* \\in\n\\mathbb{R}^{n \\times h}$ (typically overcomplete and whose columns are assumed\nto be normalized) and sparse vectors $x^* \\in \\mathbb{R}^h$ with a small\nsupport of size $h^p$ for some $0 <p < 1$ while having access to observations\n$y \\in \\mathbb{R}^n$ where $y = A^*x^*$. In this work we undertake a rigorous\nanalysis of whether gradient descent on the squared loss of an autoencoder can\nsolve the dictionary learning problem. The \"Autoencoder\" architecture we\nconsider is a $\\mathbb{R}^n \\rightarrow \\mathbb{R}^n$ mapping with a single\nReLU activation layer of size $h$.\n  Under very mild distributional assumptions on $x^*$, we prove that the norm\nof the expected gradient of the standard squared loss function is\nasymptotically (in sparse code dimension) negligible for all points in a small\nneighborhood of $A^*$. This is supported with experimental evidence using\nsynthetic data. We also conduct experiments to suggest that $A^*$ is a local\nminimum. Along the way we prove that a layer of ReLU gates can be set up to\nautomatically recover the support of the sparse codes. This property holds\nindependent of the loss function. We believe that it could be of independent\ninterest.\n", "versions": [{"version": "v1", "created": "Sat, 12 Aug 2017 01:02:47 GMT"}, {"version": "v2", "created": "Fri, 20 Oct 2017 18:07:53 GMT"}], "update_date": "2017-10-24", "authors_parsed": [["Rangamani", "Akshay", ""], ["Mukherjee", "Anirbit", ""], ["Basu", "Amitabh", ""], ["Ganapathy", "Tejaswini", ""], ["Arora", "Ashish", ""], ["Chin", "Sang", ""], ["Tran", "Trac D.", ""]]}, {"id": "1708.03741", "submitter": "Hao Yu", "authors": "Hao Yu and Michael J. Neely and Xiaohan Wei", "title": "Online Convex Optimization with Stochastic Constraints", "comments": "This paper extends our own ArXiv reports arXiv:1604.02218 (by\n  considering more general stochastic functional constraints) and\n  arXiv:1702.04783 (by relaxing a deterministic Slater-type assumption to a\n  weaker stochastic Slater assumption; refining proofs; and providing high\n  probability performance guarantees). See Introduction section (especially\n  footnotes 1 and 2) for more details of distinctions", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.OC stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper considers online convex optimization (OCO) with stochastic\nconstraints, which generalizes Zinkevich's OCO over a known simple fixed set by\nintroducing multiple stochastic functional constraints that are i.i.d.\ngenerated at each round and are disclosed to the decision maker only after the\ndecision is made. This formulation arises naturally when decisions are\nrestricted by stochastic environments or deterministic environments with noisy\nobservations. It also includes many important problems as special cases, such\nas OCO with long term constraints, stochastic constrained convex optimization,\nand deterministic constrained convex optimization. To solve this problem, this\npaper proposes a new algorithm that achieves $O(\\sqrt{T})$ expected regret and\nconstraint violations and $O(\\sqrt{T}\\log(T))$ high probability regret and\nconstraint violations. Experiments on a real-world data center scheduling\nproblem further verify the performance of the new algorithm.\n", "versions": [{"version": "v1", "created": "Sat, 12 Aug 2017 04:11:04 GMT"}], "update_date": "2017-08-15", "authors_parsed": [["Yu", "Hao", ""], ["Neely", "Michael J.", ""], ["Wei", "Xiaohan", ""]]}, {"id": "1708.03788", "submitter": "Daniel Smilkov", "authors": "Daniel Smilkov, Shan Carter, D. Sculley, Fernanda B. Vi\\'egas, Martin\n  Wattenberg", "title": "Direct-Manipulation Visualization of Deep Networks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.HC stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The recent successes of deep learning have led to a wave of interest from\nnon-experts. Gaining an understanding of this technology, however, is\ndifficult. While the theory is important, it is also helpful for novices to\ndevelop an intuitive feel for the effect of different hyperparameters and\nstructural variations. We describe TensorFlow Playground, an interactive, open\nsourced visualization that allows users to experiment via direct manipulation\nrather than coding, enabling them to quickly build an intuition about neural\nnets.\n", "versions": [{"version": "v1", "created": "Sat, 12 Aug 2017 15:36:26 GMT"}], "update_date": "2017-08-15", "authors_parsed": [["Smilkov", "Daniel", ""], ["Carter", "Shan", ""], ["Sculley", "D.", ""], ["Vi\u00e9gas", "Fernanda B.", ""], ["Wattenberg", "Martin", ""]]}, {"id": "1708.03845", "submitter": "Daniel Chicharro", "authors": "Daniel Chicharro", "title": "Quantifying multivariate redundancy with maximum entropy decompositions\n  of mutual information", "comments": "New Section 4A formalizes the maximum entropy decomposition\n  assumptions, showing for the multivariate case that the maximum entropy\n  measures provide bounds or match the actual synergy, unique, and redundancy\n  measures(proofs in new Appendix). New Section 5 addresses the decomposition\n  interpretability when some assumptions are not fulfilled, e.g. if a\n  nonnegative decomposition cannot be constructed", "journal-ref": null, "doi": null, "report-no": null, "categories": "physics.data-an q-bio.NC stat.ML", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Williams and Beer (2010) proposed a nonnegative mutual information\ndecomposition, based on the construction of redundancy lattices, which allows\nseparating the information that a set of variables contains about a target\nvariable into nonnegative components interpretable as the unique information of\nsome variables not provided by others as well as redundant and synergistic\ncomponents. However, the definition of multivariate measures of redundancy that\ncomply with nonnegativity and conform to certain axioms that capture\nconceptually desirable properties of redundancy has proven to be elusive. We\nhere present a procedure to determine nonnegative multivariate redundancy\nmeasures, within the maximum entropy framework. In particular, we generalize\nexisting bivariate maximum entropy measures of redundancy and unique\ninformation, defining measures of the redundant information that a group of\nvariables has about a target, and of the unique redundant information that a\ngroup of variables has about a target that is not redundant with information\nfrom another group. The two key ingredients for this approach are: First, the\nidentification of a type of constraints on entropy maximization that allows\nisolating components of redundancy and unique redundancy by mirroring them to\nsynergy components. Second, the construction of rooted tree-based\ndecompositions of the mutual information, which conform to the axioms of the\nredundancy lattice by the local implementation at each tree node of binary\nunfoldings of the information using hierarchically related maximum entropy\nconstraints. Altogether, the proposed measures quantify the different\nmultivariate redundancy contributions of a nonnegative mutual information\ndecomposition consistent with the redundancy lattice.\n", "versions": [{"version": "v1", "created": "Sun, 13 Aug 2017 03:29:57 GMT"}, {"version": "v2", "created": "Tue, 3 Apr 2018 18:14:23 GMT"}], "update_date": "2018-04-05", "authors_parsed": [["Chicharro", "Daniel", ""]]}, {"id": "1708.03914", "submitter": "Almog Lahav", "authors": "Almog Lahav, Ronen Talmon and Yuval Kluger", "title": "Mahalanonbis Distance Informed by Clustering", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A fundamental question in data analysis, machine learning and signal\nprocessing is how to compare between data points. The choice of the distance\nmetric is specifically challenging for high-dimensional data sets, where the\nproblem of meaningfulness is more prominent (e.g. the Euclidean distance\nbetween images). In this paper, we propose to exploit a property of\nhigh-dimensional data that is usually ignored - which is the structure stemming\nfrom the relationships between the coordinates. Specifically we show that\norganizing similar coordinates in clusters can be exploited for the\nconstruction of the Mahalanobis distance between samples. When the observable\nsamples are generated by a nonlinear transformation of hidden variables, the\nMahalanobis distance allows the recovery of the Euclidean distances in the\nhidden space.We illustrate the advantage of our approach on a synthetic example\nwhere the discovery of clusters of correlated coordinates improves the\nestimation of the principal directions of the samples. Our method was applied\nto real data of gene expression for lung adenocarcinomas (lung cancer). By\nusing the proposed metric we found a partition of subjects to risk groups with\na good separation between their Kaplan-Meier survival plot.\n", "versions": [{"version": "v1", "created": "Sun, 13 Aug 2017 14:30:03 GMT"}], "update_date": "2017-08-15", "authors_parsed": [["Lahav", "Almog", ""], ["Talmon", "Ronen", ""], ["Kluger", "Yuval", ""]]}, {"id": "1708.03951", "submitter": "Anirudh Kamath", "authors": "Anirudh Kamath, Aditya Singh, Raj Ramnani, Ayush Vyas, Jay Shenoy", "title": "Optimization of Ensemble Supervised Learning Algorithms for Increased\n  Sensitivity, Specificity, and AUC of Population-Based Colorectal Cancer\n  Screenings", "comments": "7 pages, 3 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.AI q-bio.QM", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Over 150,000 new people in the United States are diagnosed with colorectal\ncancer each year. Nearly a third die from it (American Cancer Society). The\nonly approved noninvasive diagnosis tools currently involve fecal blood count\ntests (FOBTs) or stool DNA tests. Fecal blood count tests take only five\nminutes and are available over the counter for as low as \\$15. They are highly\nspecific, yet not nearly as sensitive, yielding a high percentage (25%) of\nfalse negatives (Colon Cancer Alliance). Moreover, FOBT results are far too\ngeneralized, meaning that a positive result could mean much more than just\ncolorectal cancer, and could just as easily mean hemorrhoids, anal fissure,\nproctitis, Crohn's disease, diverticulosis, ulcerative colitis, rectal ulcer,\nrectal prolapse, ischemic colitis, angiodysplasia, rectal trauma, proctitis\nfrom radiation therapy, and others. Stool DNA tests, the modern benchmark for\nCRC screening, have a much higher sensitivity and specificity, but also cost\n\\$600, take two weeks to process, and are not for high-risk individuals or\npeople with a history of polyps. To yield a cheap and effective CRC screening\nalternative, a unique ensemble-based classification algorithm is put in place\nthat considers the FIT result, BMI, smoking history, and diabetic status of\npatients. This method is tested under ten-fold cross validation to have a .95\nAUC, 92% specificity, 89% sensitivity, .88 F1, and 90% precision. Once\nclinically validated, this test promises to be cheaper, faster, and potentially\nmore accurate when compared to a stool DNA test.\n", "versions": [{"version": "v1", "created": "Sun, 13 Aug 2017 18:48:58 GMT"}, {"version": "v2", "created": "Tue, 15 Aug 2017 02:49:02 GMT"}], "update_date": "2017-08-16", "authors_parsed": [["Kamath", "Anirudh", ""], ["Singh", "Aditya", ""], ["Ramnani", "Raj", ""], ["Vyas", "Ayush", ""], ["Shenoy", "Jay", ""]]}, {"id": "1708.03995", "submitter": "Prathusha Kameswara Sarma", "authors": "Prathusha Kameswara Sarma, Bill Sethares", "title": "Sentiment Analysis by Joint Learning of Word Embeddings and Classifier", "comments": "10 pages. Under submission", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Word embeddings are representations of individual words of a text document in\na vector space and they are often use- ful for performing natural language pro-\ncessing tasks. Current state of the art al- gorithms for learning word\nembeddings learn vector representations from large corpora of text documents in\nan unsu- pervised fashion. This paper introduces SWESA (Supervised Word\nEmbeddings for Sentiment Analysis), an algorithm for sentiment analysis via\nword embeddings. SWESA leverages document label infor- mation to learn vector\nrepresentations of words from a modest corpus of text doc- uments by solving an\noptimization prob- lem that minimizes a cost function with respect to both word\nembeddings as well as classification accuracy. Analysis re- veals that SWESA\nprovides an efficient way of estimating the dimension of the word embeddings\nthat are to be learned. Experiments on several real world data sets show that\nSWESA has superior per- formance when compared to previously suggested\napproaches to word embeddings and sentiment analysis tasks.\n", "versions": [{"version": "v1", "created": "Mon, 14 Aug 2017 02:40:20 GMT"}], "update_date": "2018-05-15", "authors_parsed": [["Sarma", "Prathusha Kameswara", ""], ["Sethares", "Bill", ""]]}, {"id": "1708.03999", "submitter": "Huan Zhang", "authors": "Pin-Yu Chen, Huan Zhang, Yash Sharma, Jinfeng Yi, Cho-Jui Hsieh", "title": "ZOO: Zeroth Order Optimization based Black-box Attacks to Deep Neural\n  Networks without Training Substitute Models", "comments": "Accepted by 10th ACM Workshop on Artificial Intelligence and Security\n  (AISEC) with the 24th ACM Conference on Computer and Communications Security\n  (CCS)", "journal-ref": null, "doi": "10.1145/3128572.3140448", "report-no": null, "categories": "stat.ML cs.CR cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Deep neural networks (DNNs) are one of the most prominent technologies of our\ntime, as they achieve state-of-the-art performance in many machine learning\ntasks, including but not limited to image classification, text mining, and\nspeech processing. However, recent research on DNNs has indicated\never-increasing concern on the robustness to adversarial examples, especially\nfor security-critical tasks such as traffic sign identification for autonomous\ndriving. Studies have unveiled the vulnerability of a well-trained DNN by\ndemonstrating the ability of generating barely noticeable (to both human and\nmachines) adversarial images that lead to misclassification. Furthermore,\nresearchers have shown that these adversarial images are highly transferable by\nsimply training and attacking a substitute model built upon the target model,\nknown as a black-box attack to DNNs.\n  Similar to the setting of training substitute models, in this paper we\npropose an effective black-box attack that also only has access to the input\n(images) and the output (confidence scores) of a targeted DNN. However,\ndifferent from leveraging attack transferability from substitute models, we\npropose zeroth order optimization (ZOO) based attacks to directly estimate the\ngradients of the targeted DNN for generating adversarial examples. We use\nzeroth order stochastic coordinate descent along with dimension reduction,\nhierarchical attack and importance sampling techniques to efficiently attack\nblack-box models. By exploiting zeroth order optimization, improved attacks to\nthe targeted DNN can be accomplished, sparing the need for training substitute\nmodels and avoiding the loss in attack transferability. Experimental results on\nMNIST, CIFAR10 and ImageNet show that the proposed ZOO attack is as effective\nas the state-of-the-art white-box attack and significantly outperforms existing\nblack-box attacks via substitute models.\n", "versions": [{"version": "v1", "created": "Mon, 14 Aug 2017 03:48:03 GMT"}, {"version": "v2", "created": "Thu, 2 Nov 2017 04:18:44 GMT"}], "update_date": "2017-11-03", "authors_parsed": [["Chen", "Pin-Yu", ""], ["Zhang", "Huan", ""], ["Sharma", "Yash", ""], ["Yi", "Jinfeng", ""], ["Hsieh", "Cho-Jui", ""]]}, {"id": "1708.04106", "submitter": "Guorui Zhou", "authors": "Guorui Zhou, Ying Fan, Runpeng Cui, Weijie Bian, Xiaoqiang Zhu, Kun\n  Gai", "title": "Rocket Launching: A Universal and Efficient Framework for Training\n  Well-performing Light Net", "comments": "10 pages, AAAI2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Models applied on real time response task, like click-through rate (CTR)\nprediction model, require high accuracy and rigorous response time. Therefore,\ntop-performing deep models of high depth and complexity are not well suited for\nthese applications with the limitations on the inference time. In order to\nfurther improve the neural networks' performance given the time and\ncomputational limitations, we propose an approach that exploits a cumbersome\nnet to help train the lightweight net for prediction. We dub the whole process\nrocket launching, where the cumbersome booster net is used to guide the\nlearning of the target light net throughout the whole training process. We\nanalyze different loss functions aiming at pushing the light net to behave\nsimilarly to the booster net, and adopt the loss with best performance in our\nexperiments. We use one technique called gradient block to improve the\nperformance of the light net and booster net further. Experiments on benchmark\ndatasets and real-life industrial advertisement data present that our light\nmodel can get performance only previously achievable with more complex models.\n", "versions": [{"version": "v1", "created": "Mon, 14 Aug 2017 13:06:15 GMT"}, {"version": "v2", "created": "Wed, 16 Aug 2017 14:22:27 GMT"}, {"version": "v3", "created": "Thu, 15 Mar 2018 03:35:52 GMT"}], "update_date": "2018-03-16", "authors_parsed": [["Zhou", "Guorui", ""], ["Fan", "Ying", ""], ["Cui", "Runpeng", ""], ["Bian", "Weijie", ""], ["Zhu", "Xiaoqiang", ""], ["Gai", "Kun", ""]]}, {"id": "1708.04232", "submitter": "Arash Rahnama", "authors": "Arash Rahnama, Abdullah Alchihabi, Vijay Gupta, Panos Antsaklis, Fatos\n  T. Yarman Vural", "title": "Encoding Multi-Resolution Brain Networks Using Unsupervised Deep\n  Learning", "comments": "6 pages, 3 figures, submitted to The 17th annual IEEE International\n  Conference on BioInformatics and BioEngineering", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.CV q-bio.NC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The main goal of this study is to extract a set of brain networks in multiple\ntime-resolutions to analyze the connectivity patterns among the anatomic\nregions for a given cognitive task. We suggest a deep architecture which learns\nthe natural groupings of the connectivity patterns of human brain in multiple\ntime-resolutions. The suggested architecture is tested on task data set of\nHuman Connectome Project (HCP) where we extract multi-resolution networks, each\nof which corresponds to a cognitive task. At the first level of this\narchitecture, we decompose the fMRI signal into multiple sub-bands using\nwavelet decompositions. At the second level, for each sub-band, we estimate a\nbrain network extracted from short time windows of the fMRI signal. At the\nthird level, we feed the adjacency matrices of each mesh network at each\ntime-resolution into an unsupervised deep learning algorithm, namely, a Stacked\nDe- noising Auto-Encoder (SDAE). The outputs of the SDAE provide a compact\nconnectivity representation for each time window at each sub-band of the fMRI\nsignal. We concatenate the learned representations of all sub-bands at each\nwindow and cluster them by a hierarchical algorithm to find the natural\ngroupings among the windows. We observe that each cluster represents a\ncognitive task with a performance of 93% Rand Index and 71% Adjusted Rand\nIndex. We visualize the mean values and the precisions of the networks at each\ncomponent of the cluster mixture. The mean brain networks at cluster centers\nshow the variations among cognitive tasks and the precision of each cluster\nshows the within cluster variability of networks, across the subjects.\n", "versions": [{"version": "v1", "created": "Sun, 13 Aug 2017 01:43:11 GMT"}], "update_date": "2017-08-16", "authors_parsed": [["Rahnama", "Arash", ""], ["Alchihabi", "Abdullah", ""], ["Gupta", "Vijay", ""], ["Antsaklis", "Panos", ""], ["Vural", "Fatos T. Yarman", ""]]}, {"id": "1708.04312", "submitter": "Andres G. Abad", "authors": "Andres G. Abad and Luis I. Reyes-Castro", "title": "Collaborative Filtering using Denoising Auto-Encoders for Market Basket\n  Data", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recommender systems (RS) help users navigate large sets of items in the\nsearch for \"interesting\" ones. One approach to RS is Collaborative Filtering\n(CF), which is based on the idea that similar users are interested in similar\nitems. Most model-based approaches to CF seek to train a\nmachine-learning/data-mining model based on sparse data; the model is then used\nto provide recommendations. While most of the proposed approaches are effective\nfor small-size situations, the combinatorial nature of the problem makes it\nimpractical for medium-to-large instances. In this work we present a novel\napproach to CF that works by training a Denoising Auto-Encoder (DAE) on\ncorrupted baskets, i.e., baskets from which one or more items have been\nremoved. The DAE is then forced to learn to reconstruct the original basket\ngiven its corrupted input. Due to recent advancements in optimization and other\ntechnologies for training neural-network models (such as DAE), the proposed\nmethod results in a scalable and practical approach to CF. The contribution of\nthis work is twofold: (1) to identify missing items in observed baskets and,\nthus, directly providing a CF model; and, (2) to construct a generative model\nof baskets which may be used, for instance, in simulation analysis or as part\nof a more complex analytical method.\n", "versions": [{"version": "v1", "created": "Mon, 14 Aug 2017 20:32:35 GMT"}], "update_date": "2017-08-16", "authors_parsed": [["Abad", "Andres G.", ""], ["Reyes-Castro", "Luis I.", ""]]}, {"id": "1708.04357", "submitter": "Trang Pham", "authors": "Trang Pham, Truyen Tran, Hoa Dam, Svetha Venkatesh", "title": "Graph Classification via Deep Learning with Virtual Nodes", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Learning representation for graph classification turns a variable-size graph\ninto a fixed-size vector (or matrix). Such a representation works nicely with\nalgebraic manipulations. Here we introduce a simple method to augment an\nattributed graph with a virtual node that is bidirectionally connected to all\nexisting nodes. The virtual node represents the latent aspects of the graph,\nwhich are not immediately available from the attributes and local connectivity\nstructures. The expanded graph is then put through any node representation\nmethod. The representation of the virtual node is then the representation of\nthe entire graph. In this paper, we use the recently introduced Column Network\nfor the expanded graph, resulting in a new end-to-end graph classification\nmodel dubbed Virtual Column Network (VCN). The model is validated on two tasks:\n(i) predicting bio-activity of chemical compounds, and (ii) finding software\nvulnerability from source code. Results demonstrate that VCN is competitive\nagainst well-established rivals.\n", "versions": [{"version": "v1", "created": "Mon, 14 Aug 2017 23:47:02 GMT"}], "update_date": "2017-08-16", "authors_parsed": [["Pham", "Trang", ""], ["Tran", "Truyen", ""], ["Dam", "Hoa", ""], ["Venkatesh", "Svetha", ""]]}, {"id": "1708.04403", "submitter": "Zhi-Hua Zhou", "authors": "Wei Wang and Zhi-Hua Zhou", "title": "Theoretical Foundation of Co-Training and Disagreement-Based Algorithms", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Disagreement-based approaches generate multiple classifiers and exploit the\ndisagreement among them with unlabeled data to improve learning performance.\nCo-training is a representative paradigm of them, which trains two classifiers\nseparately on two sufficient and redundant views; while for the applications\nwhere there is only one view, several successful variants of co-training with\ntwo different classifiers on single-view data instead of two views have been\nproposed. For these disagreement-based approaches, there are several important\nissues which still are unsolved, in this article we present theoretical\nanalyses to address these issues, which provides a theoretical foundation of\nco-training and disagreement-based approaches.\n", "versions": [{"version": "v1", "created": "Tue, 15 Aug 2017 06:00:33 GMT"}], "update_date": "2017-08-16", "authors_parsed": [["Wang", "Wei", ""], ["Zhou", "Zhi-Hua", ""]]}, {"id": "1708.04465", "submitter": "Jos van der Westhuizen", "authors": "David Janz, Jos van der Westhuizen, Jos\\'e Miguel Hern\\'andez-Lobato", "title": "Actively Learning what makes a Discrete Sequence Valid", "comments": "6 pages, 2 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Deep learning techniques have been hugely successful for traditional\nsupervised and unsupervised machine learning problems. In large part, these\ntechniques solve continuous optimization problems. Recently however, discrete\ngenerative deep learning models have been successfully used to efficiently\nsearch high-dimensional discrete spaces. These methods work by representing\ndiscrete objects as sequences, for which powerful sequence-based deep models\ncan be employed. Unfortunately, these techniques are significantly hindered by\nthe fact that these generative models often produce invalid sequences. As a\nstep towards solving this problem, we propose to learn a deep recurrent\nvalidator model. Given a partial sequence, our model learns the probability of\nthat sequence occurring as the beginning of a full valid sequence. Thus this\nidentifies valid versus invalid sequences and crucially it also provides\ninsight about how individual sequence elements influence the validity of\ndiscrete objects. To learn this model we propose an approach inspired by\nseminal work in Bayesian active learning. On a synthetic dataset, we\ndemonstrate the ability of our model to distinguish valid and invalid\nsequences. We believe this is a key step toward learning generative models that\nfaithfully produce valid discrete objects.\n", "versions": [{"version": "v1", "created": "Tue, 15 Aug 2017 11:52:35 GMT"}], "update_date": "2017-08-16", "authors_parsed": [["Janz", "David", ""], ["van der Westhuizen", "Jos", ""], ["Hern\u00e1ndez-Lobato", "Jos\u00e9 Miguel", ""]]}, {"id": "1708.04527", "submitter": "Martin Copenhaver", "authors": "Dimitris Bertsimas, Martin S. Copenhaver, and Rahul Mazumder", "title": "The Trimmed Lasso: Sparsity and Robustness", "comments": "32 pages (excluding appendix); 4 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME math.OC math.ST stat.CO stat.ML stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Nonconvex penalty methods for sparse modeling in linear regression have been\na topic of fervent interest in recent years. Herein, we study a family of\nnonconvex penalty functions that we call the trimmed Lasso and that offers\nexact control over the desired level of sparsity of estimators. We analyze its\nstructural properties and in doing so show the following:\n  1) Drawing parallels between robust statistics and robust optimization, we\nshow that the trimmed-Lasso-regularized least squares problem can be viewed as\na generalized form of total least squares under a specific model of\nuncertainty. In contrast, this same model of uncertainty, viewed instead\nthrough a robust optimization lens, leads to the convex SLOPE (or OWL) penalty.\n  2) Further, in relating the trimmed Lasso to commonly used sparsity-inducing\npenalty functions, we provide a succinct characterization of the connection\nbetween trimmed-Lasso- like approaches and penalty functions that are\ncoordinate-wise separable, showing that the trimmed penalties subsume existing\ncoordinate-wise separable penalties, with strict containment in general.\n  3) Finally, we describe a variety of exact and heuristic algorithms, both\nexisting and new, for trimmed Lasso regularized estimation problems. We include\na comparison between the different approaches and an accompanying\nimplementation of the algorithms.\n", "versions": [{"version": "v1", "created": "Tue, 15 Aug 2017 14:56:28 GMT"}], "update_date": "2017-08-16", "authors_parsed": [["Bertsimas", "Dimitris", ""], ["Copenhaver", "Martin S.", ""], ["Mazumder", "Rahul", ""]]}, {"id": "1708.04529", "submitter": "Yuya Yoshikawa", "authors": "Yuya Yoshikawa", "title": "Learning from Noisy Label Distributions", "comments": "Accepted in ICANN2017", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we consider a novel machine learning problem, that is,\nlearning a classifier from noisy label distributions. In this problem, each\ninstance with a feature vector belongs to at least one group. Then, instead of\nthe true label of each instance, we observe the label distribution of the\ninstances associated with a group, where the label distribution is distorted by\nan unknown noise. Our goals are to (1) estimate the true label of each\ninstance, and (2) learn a classifier that predicts the true label of a new\ninstance. We propose a probabilistic model that considers true label\ndistributions of groups and parameters that represent the noise as hidden\nvariables. The model can be learned based on a variational Bayesian method. In\nnumerical experiments, we show that the proposed model outperforms existing\nmethods in terms of the estimation of the true labels of instances.\n", "versions": [{"version": "v1", "created": "Fri, 11 Aug 2017 03:25:46 GMT"}], "update_date": "2017-08-17", "authors_parsed": [["Yoshikawa", "Yuya", ""]]}, {"id": "1708.04557", "submitter": "Slava Mikhaylov", "authors": "Alexander Herzog and Slava J. Mikhaylov", "title": "Database of Parliamentary Speeches in Ireland, 1919-2013", "comments": "The database is made available on the Harvard Dataverse at\n  http://dx.doi.org/10.7910/DVN/6MZN76", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.SI stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a database of parliamentary debates that contains the complete\nrecord of parliamentary speeches from D\\'ail \\'Eireann, the lower house and\nprincipal chamber of the Irish parliament, from 1919 to 2013. In addition, the\ndatabase contains background information on all TDs (Teachta D\\'ala, members of\nparliament), such as their party affiliations, constituencies and office\npositions. The current version of the database includes close to 4.5 million\nspeeches from 1,178 TDs. The speeches were downloaded from the official\nparliament website and further processed and parsed with a Python script.\nBackground information on TDs was collected from the member database of the\nparliament website. Data on cabinet positions (ministers and junior ministers)\nwas collected from the official website of the government. A record linkage\nalgorithm and human coders were used to match TDs and ministers.\n", "versions": [{"version": "v1", "created": "Tue, 15 Aug 2017 15:34:33 GMT"}], "update_date": "2017-08-16", "authors_parsed": [["Herzog", "Alexander", ""], ["Mikhaylov", "Slava J.", ""]]}, {"id": "1708.04622", "submitter": "Alan Morningstar", "authors": "Alan Morningstar and Roger G. Melko", "title": "Deep Learning the Ising Model Near Criticality", "comments": "16 pages, 8 figures, 1 table", "journal-ref": "J. Mach. Learn. Res. 18, 5975 (2018)", "doi": null, "report-no": null, "categories": "cond-mat.dis-nn cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  It is well established that neural networks with deep architectures perform\nbetter than shallow networks for many tasks in machine learning. In statistical\nphysics, while there has been recent interest in representing physical data\nwith generative modelling, the focus has been on shallow neural networks. A\nnatural question to ask is whether deep neural networks hold any advantage over\nshallow networks in representing such data. We investigate this question by\nusing unsupervised, generative graphical models to learn the probability\ndistribution of a two-dimensional Ising system. Deep Boltzmann machines, deep\nbelief networks, and deep restricted Boltzmann networks are trained on thermal\nspin configurations from this system, and compared to the shallow architecture\nof the restricted Boltzmann machine. We benchmark the models, focussing on the\naccuracy of generating energetic observables near the phase transition, where\nthese quantities are most difficult to approximate. Interestingly, after\ntraining the generative networks, we observe that the accuracy essentially\ndepends only on the number of neurons in the first hidden layer of the network,\nand not on other model details such as network depth or model type. This is\nevidence that shallow networks are more efficient than deep networks at\nrepresenting physical probability distributions associated with Ising systems\nnear criticality.\n", "versions": [{"version": "v1", "created": "Tue, 15 Aug 2017 18:00:01 GMT"}], "update_date": "2019-03-06", "authors_parsed": [["Morningstar", "Alan", ""], ["Melko", "Roger G.", ""]]}, {"id": "1708.04636", "submitter": "David Hallac", "authors": "David Hallac, Abhijit Sharang, Rainer Stahlmann, Andreas Lamprecht,\n  Markus Huber, Martin Roehder, Rok Sosic, Jure Leskovec", "title": "Driver Identification Using Automobile Sensor Data from a Single Turn", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC cs.SI stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  As automotive electronics continue to advance, cars are becoming more and\nmore reliant on sensors to perform everyday driving operations. These sensors\nare omnipresent and help the car navigate, reduce accidents, and provide\ncomfortable rides. However, they can also be used to learn about the drivers\nthemselves. In this paper, we propose a method to predict, from sensor data\ncollected at a single turn, the identity of a driver out of a given set of\nindividuals. We cast the problem in terms of time series classification, where\nour dataset contains sensor readings at one turn, repeated several times by\nmultiple drivers. We build a classifier to find unique patterns in each\nindividual's driving style, which are visible in the data even on such a short\nroad segment. To test our approach, we analyze a new dataset collected by AUDI\nAG and Audi Electronics Venture, where a fleet of test vehicles was equipped\nwith automotive data loggers storing all sensor readings on real roads. We show\nthat turns are particularly well-suited for detecting variations across\ndrivers, especially when compared to straightaways. We then focus on the 12\nmost frequently made turns in the dataset, which include rural, urban, highway\non-ramps, and more, obtaining accurate identification results and learning\nuseful insights about driver behavior in a variety of settings.\n", "versions": [{"version": "v1", "created": "Fri, 9 Jun 2017 17:15:00 GMT"}], "update_date": "2017-08-17", "authors_parsed": [["Hallac", "David", ""], ["Sharang", "Abhijit", ""], ["Stahlmann", "Rainer", ""], ["Lamprecht", "Andreas", ""], ["Huber", "Markus", ""], ["Roehder", "Martin", ""], ["Sosic", "Rok", ""], ["Leskovec", "Jure", ""]]}, {"id": "1708.04649", "submitter": "Ping Wang", "authors": "Ping Wang, Yan Li, Chandan K. Reddy", "title": "Machine Learning for Survival Analysis: A Survey", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Accurately predicting the time of occurrence of an event of interest is a\ncritical problem in longitudinal data analysis. One of the main challenges in\nthis context is the presence of instances whose event outcomes become\nunobservable after a certain time point or when some instances do not\nexperience any event during the monitoring period. Such a phenomenon is called\ncensoring which can be effectively handled using survival analysis techniques.\nTraditionally, statistical approaches have been widely developed in the\nliterature to overcome this censoring issue. In addition, many machine learning\nalgorithms are adapted to effectively handle survival data and tackle other\nchallenging problems that arise in real-world data. In this survey, we provide\na comprehensive and structured review of the representative statistical methods\nalong with the machine learning techniques used in survival analysis and\nprovide a detailed taxonomy of the existing methods. We also discuss several\ntopics that are closely related to survival analysis and illustrate several\nsuccessful applications in various real-world application domains. We hope that\nthis paper will provide a more thorough understanding of the recent advances in\nsurvival analysis and offer some guidelines on applying these approaches to\nsolve new problems that arise in applications with censored data.\n", "versions": [{"version": "v1", "created": "Tue, 15 Aug 2017 19:03:36 GMT"}], "update_date": "2017-12-26", "authors_parsed": [["Wang", "Ping", ""], ["Li", "Yan", ""], ["Reddy", "Chandan K.", ""]]}, {"id": "1708.04680", "submitter": "Marcus Bloice", "authors": "Marcus D. Bloice, Christof Stocker, Andreas Holzinger", "title": "Augmentor: An Image Augmentation Library for Machine Learning", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The generation of artificial data based on existing observations, known as\ndata augmentation, is a technique used in machine learning to improve model\naccuracy, generalisation, and to control overfitting. Augmentor is a software\npackage, available in both Python and Julia versions, that provides a high\nlevel API for the expansion of image data using a stochastic, pipeline-based\napproach which effectively allows for images to be sampled from a distribution\nof augmented images at runtime. Augmentor provides methods for most standard\naugmentation practices as well as several advanced features such as\nlabel-preserving, randomised elastic distortions, and provides many helper\nfunctions for typical augmentation tasks used in machine learning.\n", "versions": [{"version": "v1", "created": "Fri, 11 Aug 2017 11:19:44 GMT"}], "update_date": "2017-08-18", "authors_parsed": [["Bloice", "Marcus D.", ""], ["Stocker", "Christof", ""], ["Holzinger", "Andreas", ""]]}, {"id": "1708.04692", "submitter": "Anton Osokin", "authors": "Anton Osokin, Anatole Chessel, Rafael E. Carazo Salas and Federico\n  Vaggi", "title": "GANs for Biological Image Synthesis", "comments": "The paper appearing at the International Conference on Computer\n  Vision (ICCV) 2017 + its supplementary materials", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we propose a novel application of Generative Adversarial\nNetworks (GAN) to the synthesis of cells imaged by fluorescence microscopy.\nCompared to natural images, cells tend to have a simpler and more geometric\nglobal structure that facilitates image generation. However, the correlation\nbetween the spatial pattern of different fluorescent proteins reflects\nimportant biological functions, and synthesized images have to capture these\nrelationships to be relevant for biological applications. We adapt GANs to the\ntask at hand and propose new models with casual dependencies between image\nchannels that can generate multi-channel images, which would be impossible to\nobtain experimentally. We evaluate our approach using two independent\ntechniques and compare it against sensible baselines. Finally, we demonstrate\nthat by interpolating across the latent space we can mimic the known changes in\nprotein localization that occur through time during the cell cycle, allowing us\nto predict temporal evolution from static images.\n", "versions": [{"version": "v1", "created": "Tue, 15 Aug 2017 21:04:11 GMT"}, {"version": "v2", "created": "Tue, 12 Sep 2017 09:18:24 GMT"}], "update_date": "2017-09-13", "authors_parsed": [["Osokin", "Anton", ""], ["Chessel", "Anatole", ""], ["Salas", "Rafael E. Carazo", ""], ["Vaggi", "Federico", ""]]}, {"id": "1708.04729", "submitter": "Yizhe Zhang", "authors": "Yizhe Zhang, Dinghan Shen, Guoyin Wang, Zhe Gan, Ricardo Henao,\n  Lawrence Carin", "title": "Deconvolutional Paragraph Representation Learning", "comments": "Accepted by NIPS 2017", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Learning latent representations from long text sequences is an important\nfirst step in many natural language processing applications. Recurrent Neural\nNetworks (RNNs) have become a cornerstone for this challenging task. However,\nthe quality of sentences during RNN-based decoding (reconstruction) decreases\nwith the length of the text. We propose a sequence-to-sequence, purely\nconvolutional and deconvolutional autoencoding framework that is free of the\nabove issue, while also being computationally efficient. The proposed method is\nsimple, easy to implement and can be leveraged as a building block for many\napplications. We show empirically that compared to RNNs, our framework is\nbetter at reconstructing and correcting long paragraphs. Quantitative\nevaluation on semi-supervised text classification and summarization tasks\ndemonstrate the potential for better utilization of long unlabeled text data.\n", "versions": [{"version": "v1", "created": "Wed, 16 Aug 2017 00:52:32 GMT"}, {"version": "v2", "created": "Fri, 8 Sep 2017 17:13:13 GMT"}, {"version": "v3", "created": "Fri, 22 Sep 2017 15:20:27 GMT"}], "update_date": "2017-09-25", "authors_parsed": [["Zhang", "Yizhe", ""], ["Shen", "Dinghan", ""], ["Wang", "Guoyin", ""], ["Gan", "Zhe", ""], ["Henao", "Ricardo", ""], ["Carin", "Lawrence", ""]]}, {"id": "1708.04733", "submitter": "Minh Trung Le", "authors": "Trung Le, Hung Vu, Tu Dinh Nguyen, Dinh Phung", "title": "Geometric Enclosing Networks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Training model to generate data has increasingly attracted research attention\nand become important in modern world applications. We propose in this paper a\nnew geometry-based optimization approach to address this problem. Orthogonal to\ncurrent state-of-the-art density-based approaches, most notably VAE and GAN, we\npresent a fresh new idea that borrows the principle of minimal enclosing ball\nto train a generator G\\left(\\bz\\right) in such a way that both training and\ngenerated data, after being mapped to the feature space, are enclosed in the\nsame sphere. We develop theory to guarantee that the mapping is bijective so\nthat its inverse from feature space to data space results in expressive\nnonlinear contours to describe the data manifold, hence ensuring data generated\nare also lying on the data manifold learned from training data. Our model\nenjoys a nice geometric interpretation, hence termed Geometric Enclosing\nNetworks (GEN), and possesses some key advantages over its rivals, namely\nsimple and easy-to-control optimization formulation, avoidance of mode\ncollapsing and efficiently learn data manifold representation in a completely\nunsupervised manner. We conducted extensive experiments on synthesis and\nreal-world datasets to illustrate the behaviors, strength and weakness of our\nproposed GEN, in particular its ability to handle multi-modal data and quality\nof generated data.\n", "versions": [{"version": "v1", "created": "Wed, 16 Aug 2017 01:10:49 GMT"}, {"version": "v2", "created": "Thu, 17 Aug 2017 04:58:35 GMT"}], "update_date": "2017-08-18", "authors_parsed": [["Le", "Trung", ""], ["Vu", "Hung", ""], ["Nguyen", "Tu Dinh", ""], ["Phung", "Dinh", ""]]}, {"id": "1708.04753", "submitter": "Yun Yang", "authors": "Yun Yang, Anirban Bhattacharya, Debdeep Pati", "title": "Frequentist coverage and sup-norm convergence rate in Gaussian process\n  regression", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.CO stat.ML stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Gaussian process (GP) regression is a powerful interpolation technique due to\nits flexibility in capturing non-linearity. In this paper, we provide a general\nframework for understanding the frequentist coverage of point-wise and\nsimultaneous Bayesian credible sets in GP regression. As an intermediate\nresult, we develop a Bernstein von-Mises type result under supremum norm in\nrandom design GP regression. Identifying both the mean and covariance function\nof the posterior distribution of the Gaussian process as regularized\n$M$-estimators, we show that the sampling distribution of the posterior mean\nfunction and the centered posterior distribution can be respectively\napproximated by two population level GPs. By developing a comparison inequality\nbetween two GPs, we provide exact characterization of frequentist coverage\nprobabilities of Bayesian point-wise credible intervals and simultaneous\ncredible bands of the regression function. Our results show that inference\nbased on GP regression tends to be conservative; when the prior is\nunder-smoothed, the resulting credible intervals and bands have minimax-optimal\nsizes, with their frequentist coverage converging to a non-degenerate value\nbetween their nominal level and one. As a byproduct of our theory, we show that\nthe GP regression also yields minimax-optimal posterior contraction rate\nrelative to the supremum norm, which provides a positive evidence to the long\nstanding problem on optimal supremum norm contraction rate in GP regression.\n", "versions": [{"version": "v1", "created": "Wed, 16 Aug 2017 03:07:54 GMT"}], "update_date": "2017-08-17", "authors_parsed": [["Yang", "Yun", ""], ["Bhattacharya", "Anirban", ""], ["Pati", "Debdeep", ""]]}, {"id": "1708.04757", "submitter": "Hossein Soleimani", "authors": "Hossein Soleimani, James Hensman, Suchi Saria", "title": "Scalable Joint Models for Reliable Uncertainty-Aware Event Prediction", "comments": "To appear in IEEE Transaction on Pattern Analysis and Machine\n  Intelligence", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.AI cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Missing data and noisy observations pose significant challenges for reliably\npredicting events from irregularly sampled multivariate time series\n(longitudinal) data. Imputation methods, which are typically used for\ncompleting the data prior to event prediction, lack a principled mechanism to\naccount for the uncertainty due to missingness. Alternatively, state-of-the-art\njoint modeling techniques can be used for jointly modeling the longitudinal and\nevent data and compute event probabilities conditioned on the longitudinal\nobservations. These approaches, however, make strong parametric assumptions and\ndo not easily scale to multivariate signals with many observations. Our\nproposed approach consists of several key innovations. First, we develop a\nflexible and scalable joint model based upon sparse multiple-output Gaussian\nprocesses. Unlike state-of-the-art joint models, the proposed model can explain\nhighly challenging structure including non-Gaussian noise while scaling to\nlarge data. Second, we derive an optimal policy for predicting events using the\ndistribution of the event occurrence estimated by the joint model. The derived\npolicy trades-off the cost of a delayed detection versus incorrect assessments\nand abstains from making decisions when the estimated event probability does\nnot satisfy the derived confidence criteria. Experiments on a large dataset\nshow that the proposed framework significantly outperforms state-of-the-art\ntechniques in event prediction.\n", "versions": [{"version": "v1", "created": "Wed, 16 Aug 2017 03:27:25 GMT"}], "update_date": "2017-08-17", "authors_parsed": [["Soleimani", "Hossein", ""], ["Hensman", "James", ""], ["Saria", "Suchi", ""]]}, {"id": "1708.04764", "submitter": "Yuantao Gu", "authors": "Yanxi Chen, Gen Li and Yuantao Gu", "title": "Active Orthogonal Matching Pursuit for Sparse Subspace Clustering", "comments": "14 pages, 5 figures, 1 table", "journal-ref": null, "doi": "10.1109/LSP.2017.2741509", "report-no": null, "categories": "cs.LG cs.CV cs.IT math.IT stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Sparse Subspace Clustering (SSC) is a state-of-the-art method for clustering\nhigh-dimensional data points lying in a union of low-dimensional subspaces.\nHowever, while $\\ell_1$ optimization-based SSC algorithms suffer from high\ncomputational complexity, other variants of SSC, such as Orthogonal Matching\nPursuit-based SSC (OMP-SSC), lose clustering accuracy in pursuit of improving\ntime efficiency. In this letter, we propose a novel Active OMP-SSC, which\nimproves clustering accuracy of OMP-SSC by adaptively updating data points and\nrandomly dropping data points in the OMP process, while still enjoying the low\ncomputational complexity of greedy pursuit algorithms. We provide heuristic\nanalysis of our approach, and explain how these two active steps achieve a\nbetter tradeoff between connectivity and separation. Numerical results on both\nsynthetic data and real-world data validate our analyses and show the\nadvantages of the proposed active algorithm.\n", "versions": [{"version": "v1", "created": "Wed, 16 Aug 2017 04:15:37 GMT"}], "update_date": "2018-02-14", "authors_parsed": [["Chen", "Yanxi", ""], ["Li", "Gen", ""], ["Gu", "Yuantao", ""]]}, {"id": "1708.04781", "submitter": "Yichi Zhou", "authors": "Yichi Zhou, Jun Zhu, Jingwei Zhuo", "title": "Racing Thompson: an Efficient Algorithm for Thompson Sampling with\n  Non-conjugate Priors", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Thompson sampling has impressive empirical performance for many multi-armed\nbandit problems. But current algorithms for Thompson sampling only work for the\ncase of conjugate priors since these algorithms require to infer the posterior,\nwhich is often computationally intractable when the prior is not conjugate. In\nthis paper, we propose a novel algorithm for Thompson sampling which only\nrequires to draw samples from a tractable distribution, so our algorithm is\nefficient even when the prior is non-conjugate. To do this, we reformulate\nThompson sampling as an optimization problem via the Gumbel-Max trick. After\nthat we construct a set of random variables and our goal is to identify the one\nwith highest mean. Finally, we solve it with techniques in best arm\nidentification.\n", "versions": [{"version": "v1", "created": "Wed, 16 Aug 2017 06:20:40 GMT"}], "update_date": "2017-08-17", "authors_parsed": [["Zhou", "Yichi", ""], ["Zhu", "Jun", ""], ["Zhuo", "Jingwei", ""]]}, {"id": "1708.04788", "submitter": "Aswin Raghavan", "authors": "Aswin Raghavan, Mohamed Amer, Sek Chai, Graham Taylor", "title": "BitNet: Bit-Regularized Deep Neural Networks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a novel optimization strategy for training neural networks which\nwe call \"BitNet\". The parameters of neural networks are usually unconstrained\nand have a dynamic range dispersed over all real values. Our key idea is to\nlimit the expressive power of the network by dynamically controlling the range\nand set of values that the parameters can take. We formulate this idea using a\nnovel end-to-end approach that circumvents the discrete parameter space by\noptimizing a relaxed continuous and differentiable upper bound of the typical\nclassification loss function. The approach can be interpreted as a\nregularization inspired by the Minimum Description Length (MDL) principle. For\neach layer of the network, our approach optimizes real-valued translation and\nscaling factors and arbitrary precision integer-valued parameters (weights). We\nempirically compare BitNet to an equivalent unregularized model on the MNIST\nand CIFAR-10 datasets. We show that BitNet converges faster to a superior\nquality solution. Additionally, the resulting model has significant savings in\nmemory due to the use of integer-valued parameters.\n", "versions": [{"version": "v1", "created": "Wed, 16 Aug 2017 06:51:23 GMT"}, {"version": "v2", "created": "Tue, 26 Jun 2018 07:52:53 GMT"}, {"version": "v3", "created": "Fri, 16 Nov 2018 20:20:25 GMT"}], "update_date": "2018-11-20", "authors_parsed": [["Raghavan", "Aswin", ""], ["Amer", "Mohamed", ""], ["Chai", "Sek", ""], ["Taylor", "Graham", ""]]}, {"id": "1708.04801", "submitter": "Daning Cheng", "authors": "Cheng Daning, Li Shigang and Zhang Yunquan", "title": "Weighted parallel SGD for distributed unbalanced-workload training\n  system", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Stochastic gradient descent (SGD) is a popular stochastic optimization method\nin machine learning. Traditional parallel SGD algorithms, e.g., SimuParallel\nSGD, often require all nodes to have the same performance or to consume equal\nquantities of data. However, these requirements are difficult to satisfy when\nthe parallel SGD algorithms run in a heterogeneous computing environment;\nlow-performance nodes will exert a negative influence on the final result. In\nthis paper, we propose an algorithm called weighted parallel SGD (WP-SGD).\nWP-SGD combines weighted model parameters from different nodes in the system to\nproduce the final output. WP-SGD makes use of the reduction in standard\ndeviation to compensate for the loss from the inconsistency in performance of\nnodes in the cluster, which means that WP-SGD does not require that all nodes\nconsume equal quantities of data. We also analyze the theoretical feasibility\nof running two other parallel SGD algorithms combined with WP-SGD in a\nheterogeneous environment. The experimental results show that WP-SGD\nsignificantly outperforms the traditional parallel SGD algorithms on\ndistributed training systems with an unbalanced workload.\n", "versions": [{"version": "v1", "created": "Wed, 16 Aug 2017 08:29:23 GMT"}], "update_date": "2017-08-17", "authors_parsed": [["Daning", "Cheng", ""], ["Shigang", "Li", ""], ["Yunquan", "Zhang", ""]]}, {"id": "1708.04825", "submitter": "Allan M. M. Leal", "authors": "Allan M. M. Leal, Dmitrii A. Kulik and Martin O. Saar", "title": "Ultra-Fast Reactive Transport Simulations When Chemical Reactions Meet\n  Machine Learning: Chemical Equilibrium", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.OC physics.chem-ph physics.flu-dyn stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  During reactive transport modeling, the computational cost associated with\nchemical reaction calculations is often 10-100 times higher than that of\ntransport calculations. Most of these costs results from chemical equilibrium\ncalculations that are performed at least once in every mesh cell and at every\ntime step of the simulation. Calculating chemical equilibrium is an iterative\nprocess, where each iteration is in general so computationally expensive that\neven if every calculation converged in a single iteration, the resulting\nspeedup would not be significant. Thus, rather than proposing a fast-converging\nnumerical method for solving chemical equilibrium equations, we present a\nmachine learning method that enables new equilibrium states to be quickly and\naccurately estimated, whenever a previous equilibrium calculation with similar\ninput conditions has been performed. We demonstrate the use of this smart\nchemical equilibrium method in a reactive transport modeling example and show\nthat, even at early simulation times, the majority of all equilibrium\ncalculations are quickly predicted and, after some time steps, the\nmachine-learning-accelerated chemical solver has been fully trained to rapidly\nperform all subsequent equilibrium calculations, resulting in speedups of\nalmost two orders of magnitude. We remark that our new on-demand machine\nlearning method can be applied to any case in which a massive number of\nsequential/parallel evaluations of a computationally expensive function $f$\nneeds to be done, $y=f(x)$. We remark, that, in contrast to traditional machine\nlearning algorithms, our on-demand training approach does not require a\nstatistics-based training phase before the actual simulation of interest\ncommences. The introduced on-demand training scheme requires, however, the\nfirst-order derivatives $\\partial f/\\partial x$ for later smart predictions.\n", "versions": [{"version": "v1", "created": "Wed, 16 Aug 2017 09:48:21 GMT"}], "update_date": "2017-08-17", "authors_parsed": [["Leal", "Allan M. M.", ""], ["Kulik", "Dmitrii A.", ""], ["Saar", "Martin O.", ""]]}, {"id": "1708.04887", "submitter": "Jelena Bradic", "authors": "Jelena Bradic, Gerda Claeskens, Thomas Gueuning", "title": "Fixed effects testing in high-dimensional linear mixed models", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME cs.LG math.ST stat.ML stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Many scientific and engineering challenges -- ranging from pharmacokinetic\ndrug dosage allocation and personalized medicine to marketing mix (4Ps)\nrecommendations -- require an understanding of the unobserved heterogeneity in\norder to develop the best decision making-processes. In this paper, we develop\na hypothesis test and the corresponding p-value for testing for the\nsignificance of the homogeneous structure in linear mixed models. A robust\nmatching moment construction is used for creating a test that adapts to the\nsize of the model sparsity. When unobserved heterogeneity at a cluster level is\nconstant, we show that our test is both consistent and unbiased even when the\ndimension of the model is extremely high. Our theoretical results rely on a new\nfamily of adaptive sparse estimators of the fixed effects that do not require\nconsistent estimation of the random effects. Moreover, our inference results do\nnot require consistent model selection. We showcase that moment matching can be\nextended to nonlinear mixed effects models and to generalized linear mixed\neffects models. In numerical and real data experiments, we find that the\ndeveloped method is extremely accurate, that it adapts to the size of the\nunderlying model and is decidedly powerful in the presence of irrelevant\ncovariates.\n", "versions": [{"version": "v1", "created": "Mon, 14 Aug 2017 21:48:46 GMT"}], "update_date": "2017-08-17", "authors_parsed": [["Bradic", "Jelena", ""], ["Claeskens", "Gerda", ""], ["Gueuning", "Thomas", ""]]}, {"id": "1708.04922", "submitter": "Michael Motro", "authors": "Michael Motro, Joydeep Ghosh, Chandra Bhat", "title": "Optimal Alarms for Vehicular Collision Detection", "comments": null, "journal-ref": null, "doi": "10.1109/IVS.2017.7995732", "report-no": null, "categories": "stat.ML cs.RO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  An important application of intelligent vehicles is advance detection of\ndangerous events such as collisions. This problem is framed as a problem of\noptimal alarm choice given predictive models for vehicle location and motion.\nTechniques for real-time collision detection are surveyed and grouped into\nthree classes: random Monte Carlo sampling, faster deterministic\napproximations, and machine learning models trained by simulation. Theoretical\nguarantees on the performance of these collision detection techniques are\nprovided where possible, and empirical analysis is provided for two example\nscenarios. Results validate Monte Carlo sampling as a robust solution despite\nits simplicity.\n", "versions": [{"version": "v1", "created": "Wed, 16 Aug 2017 15:09:09 GMT"}], "update_date": "2017-08-17", "authors_parsed": [["Motro", "Michael", ""], ["Ghosh", "Joydeep", ""], ["Bhat", "Chandra", ""]]}, {"id": "1708.04970", "submitter": "Daniel Ting", "authors": "Daniel Ting", "title": "Adaptive Threshold Sampling and Estimation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Sampling is a fundamental problem in both computer science and statistics. A\nnumber of issues arise when designing a method based on sampling. These include\nstatistical considerations such as constructing a good sampling design and\nensuring there are good, tractable estimators for the quantities of interest as\nwell as computational considerations such as designing fast algorithms for\nstreaming data and ensuring the sample fits within memory constraints.\nUnfortunately, existing sampling methods are only able to address all of these\nissues in limited scenarios.\n  We develop a framework that can be used to address these issues in a broad\nrange of scenarios. In particular, it addresses the problem of drawing and\nusing samples under some memory budget constraint. This problem can be\nchallenging since the memory budget forces samples to be drawn\nnon-independently and consequently, makes computation of resulting estimators\ndifficult.\n  At the core of the framework is the notion of a data adaptive thresholding\nscheme where the threshold effectively allows one to treat the non-independent\nsample as if it were drawn independently. We provide sufficient conditions for\na thresholding scheme to allow this and provide ways to build and compose such\nschemes.\n  Furthermore, we provide fast algorithms to efficiently sample under these\nthresholding schemes.\n", "versions": [{"version": "v1", "created": "Wed, 16 Aug 2017 16:45:47 GMT"}], "update_date": "2017-08-17", "authors_parsed": [["Ting", "Daniel", ""]]}, {"id": "1708.04975", "submitter": "Eric Laloy", "authors": "Eric Laloy, Romain H\\'erault, Diederik Jacques, Niklas Linde", "title": "Training-image based geostatistical inversion using a spatial generative\n  adversarial neural network", "comments": null, "journal-ref": "Water Resources Research, 54, 381-406, 2018", "doi": "10.1002/2017WR022148", "report-no": null, "categories": "stat.ML cs.CV physics.geo-ph", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Probabilistic inversion within a multiple-point statistics framework is often\ncomputationally prohibitive for high-dimensional problems. To partly address\nthis, we introduce and evaluate a new training-image based inversion approach\nfor complex geologic media. Our approach relies on a deep neural network of the\ngenerative adversarial network (GAN) type. After training using a training\nimage (TI), our proposed spatial GAN (SGAN) can quickly generate 2D and 3D\nunconditional realizations. A key characteristic of our SGAN is that it defines\na (very) low-dimensional parameterization, thereby allowing for efficient\nprobabilistic inversion using state-of-the-art Markov chain Monte Carlo (MCMC)\nmethods. In addition, available direct conditioning data can be incorporated\nwithin the inversion. Several 2D and 3D categorical TIs are first used to\nanalyze the performance of our SGAN for unconditional geostatistical\nsimulation. Training our deep network can take several hours. After training,\nrealizations containing a few millions of pixels/voxels can be produced in a\nmatter of seconds. This makes it especially useful for simulating many\nthousands of realizations (e.g., for MCMC inversion) as the relative cost of\nthe training per realization diminishes with the considered number of\nrealizations. Synthetic inversion case studies involving 2D steady-state flow\nand 3D transient hydraulic tomography with and without direct conditioning data\nare used to illustrate the effectiveness of our proposed SGAN-based inversion.\nFor the 2D case, the inversion rapidly explores the posterior model\ndistribution. For the 3D case, the inversion recovers model realizations that\nfit the data close to the target level and visually resemble the true model\nwell.\n", "versions": [{"version": "v1", "created": "Wed, 16 Aug 2017 17:04:52 GMT"}, {"version": "v2", "created": "Tue, 8 Jan 2019 14:44:42 GMT"}], "update_date": "2019-01-09", "authors_parsed": [["Laloy", "Eric", ""], ["H\u00e9rault", "Romain", ""], ["Jacques", "Diederik", ""], ["Linde", "Niklas", ""]]}, {"id": "1708.05033", "submitter": "Pratik Gajane", "authors": "Pratik Gajane, Tanguy Urvoy, Emilie Kaufmann", "title": "Corrupt Bandits for Preserving Local Privacy", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study a variant of the stochastic multi-armed bandit (MAB) problem in\nwhich the rewards are corrupted. In this framework, motivated by privacy\npreservation in online recommender systems, the goal is to maximize the sum of\nthe (unobserved) rewards, based on the observation of transformation of these\nrewards through a stochastic corruption process with known parameters. We\nprovide a lower bound on the expected regret of any bandit algorithm in this\ncorrupted setting. We devise a frequentist algorithm, KLUCB-CF, and a Bayesian\nalgorithm, TS-CF and give upper bounds on their regret. We also provide the\nappropriate corruption parameters to guarantee a desired level of local privacy\nand analyze how this impacts the regret. Finally, we present some experimental\nresults that confirm our analysis.\n", "versions": [{"version": "v1", "created": "Wed, 16 Aug 2017 18:32:24 GMT"}, {"version": "v2", "created": "Thu, 2 Nov 2017 23:13:14 GMT"}], "update_date": "2017-11-06", "authors_parsed": [["Gajane", "Pratik", ""], ["Urvoy", "Tanguy", ""], ["Kaufmann", "Emilie", ""]]}, {"id": "1708.05070", "submitter": "Randal Olson", "authors": "Randal S. Olson, William La Cava, Zairah Mustahsan, Akshay Varik,\n  Jason H. Moore", "title": "Data-driven Advice for Applying Machine Learning to Bioinformatics\n  Problems", "comments": "12 pages, 5 figures, 4 tables. To be published in the proceedings of\n  PSB 2018. Randal S. Olson and William La Cava contributed equally as co-first\n  authors", "journal-ref": null, "doi": null, "report-no": null, "categories": "q-bio.QM cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  As the bioinformatics field grows, it must keep pace not only with new data\nbut with new algorithms. Here we contribute a thorough analysis of 13\nstate-of-the-art, commonly used machine learning algorithms on a set of 165\npublicly available classification problems in order to provide data-driven\nalgorithm recommendations to current researchers. We present a number of\nstatistical and visual comparisons of algorithm performance and quantify the\neffect of model selection and algorithm tuning for each algorithm and dataset.\nThe analysis culminates in the recommendation of five algorithms with\nhyperparameters that maximize classifier performance across the tested\nproblems, as well as general guidelines for applying machine learning to\nsupervised classification problems.\n", "versions": [{"version": "v1", "created": "Tue, 8 Aug 2017 21:41:48 GMT"}, {"version": "v2", "created": "Sun, 7 Jan 2018 19:08:53 GMT"}], "update_date": "2018-01-09", "authors_parsed": [["Olson", "Randal S.", ""], ["La Cava", "William", ""], ["Mustahsan", "Zairah", ""], ["Varik", "Akshay", ""], ["Moore", "Jason H.", ""]]}, {"id": "1708.05094", "submitter": "Patrick McDermott", "authors": "Patrick L. McDermott and Christopher K. Wikle", "title": "An Ensemble Quadratic Echo State Network for Nonlinear Spatio-Temporal\n  Forecasting", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Spatio-temporal data and processes are prevalent across a wide variety of\nscientific disciplines. These processes are often characterized by nonlinear\ntime dynamics that include interactions across multiple scales of spatial and\ntemporal variability. The data sets associated with many of these processes are\nincreasing in size due to advances in automated data measurement, management,\nand numerical simulator output. Non- linear spatio-temporal models have only\nrecently seen interest in statistics, but there are many classes of such models\nin the engineering and geophysical sciences. Tradi- tionally, these models are\nmore heuristic than those that have been presented in the statistics\nliterature, but are often intuitive and quite efficient computationally. We\nshow here that with fairly simple, but important, enhancements, the echo state\nnet- work (ESN) machine learning approach can be used to generate long-lead\nforecasts of nonlinear spatio-temporal processes, with reasonable uncertainty\nquantification, and at only a fraction of the computational expense of a\ntraditional parametric nonlinear spatio-temporal models.\n", "versions": [{"version": "v1", "created": "Wed, 16 Aug 2017 22:08:25 GMT"}], "update_date": "2017-08-18", "authors_parsed": [["McDermott", "Patrick L.", ""], ["Wikle", "Christopher K.", ""]]}, {"id": "1708.05106", "submitter": "Arin Chaudhuri", "authors": "Arin Chaudhuri, Deovrat Kakde, Carol Sadek, Laura Gonzalez, Seunghyun\n  Kong", "title": "The Mean and Median Criterion for Automatic Kernel Bandwidth Selection\n  for Support Vector Data Description", "comments": null, "journal-ref": null, "doi": "10.1109/ICDMW.2017.116", "report-no": null, "categories": "cs.LG cs.AI stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Support vector data description (SVDD) is a popular technique for detecting\nanomalies. The SVDD classifier partitions the whole space into an inlier\nregion, which consists of the region near the training data, and an outlier\nregion, which consists of points away from the training data. The computation\nof the SVDD classifier requires a kernel function, and the Gaussian kernel is a\ncommon choice for the kernel function. The Gaussian kernel has a bandwidth\nparameter, whose value is important for good results. A small bandwidth leads\nto overfitting, and the resulting SVDD classifier overestimates the number of\nanomalies. A large bandwidth leads to underfitting, and the classifier fails to\ndetect many anomalies. In this paper we present a new automatic, unsupervised\nmethod for selecting the Gaussian kernel bandwidth. The selected value can be\ncomputed quickly, and it is competitive with existing bandwidth selection\nmethods.\n", "versions": [{"version": "v1", "created": "Wed, 16 Aug 2017 23:38:35 GMT"}, {"version": "v2", "created": "Mon, 21 Aug 2017 23:12:34 GMT"}], "update_date": "2018-07-23", "authors_parsed": [["Chaudhuri", "Arin", ""], ["Kakde", "Deovrat", ""], ["Sadek", "Carol", ""], ["Gonzalez", "Laura", ""], ["Kong", "Seunghyun", ""]]}, {"id": "1708.05123", "submitter": "Ruoxi Wang", "authors": "Ruoxi Wang, Bin Fu, Gang Fu, Mingliang Wang", "title": "Deep & Cross Network for Ad Click Predictions", "comments": "In Proceedings of AdKDD and TargetAd, Halifax, NS, Canada, August,\n  14, 2017, 7 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Feature engineering has been the key to the success of many prediction\nmodels. However, the process is non-trivial and often requires manual feature\nengineering or exhaustive searching. DNNs are able to automatically learn\nfeature interactions; however, they generate all the interactions implicitly,\nand are not necessarily efficient in learning all types of cross features. In\nthis paper, we propose the Deep & Cross Network (DCN) which keeps the benefits\nof a DNN model, and beyond that, it introduces a novel cross network that is\nmore efficient in learning certain bounded-degree feature interactions. In\nparticular, DCN explicitly applies feature crossing at each layer, requires no\nmanual feature engineering, and adds negligible extra complexity to the DNN\nmodel. Our experimental results have demonstrated its superiority over the\nstate-of-art algorithms on the CTR prediction dataset and dense classification\ndataset, in terms of both model accuracy and memory usage.\n", "versions": [{"version": "v1", "created": "Thu, 17 Aug 2017 03:28:04 GMT"}], "update_date": "2017-08-18", "authors_parsed": [["Wang", "Ruoxi", ""], ["Fu", "Bin", ""], ["Fu", "Gang", ""], ["Wang", "Mingliang", ""]]}, {"id": "1708.05200", "submitter": "Arash Mehrjou", "authors": "Arash Mehrjou", "title": "Towards life cycle identification of malaria parasites using machine\n  learning and Riemannian geometry", "comments": "22 pages, 8 Figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Malaria is a serious infectious disease that is responsible for over half\nmillion deaths yearly worldwide. The major cause of these mortalities is late\nor inaccurate diagnosis. Manual microscopy is currently considered as the\ndominant diagnostic method for malaria. However, it is time consuming and prone\nto human errors. The aim of this paper is to automate the diagnosis process and\nminimize the human intervention. We have developed the hardware and software\nfor a cost-efficient malaria diagnostic system. This paper describes the\nmanufactured hardware and also proposes novel software to handle parasite\ndetection and life-stage identification. A motorized microscope is developed to\ntake images from Giemsa-stained blood smears. A patch-based unsupervised\nstatistical clustering algorithm is proposed which offers a novel method for\nclassification of different regions within blood images. The proposed method\nprovides better robustness against different imaging settings. The core of the\nproposed algorithm is a model called Mixture of Independent Component Analysis.\nA manifold based optimization method is proposed that facilitates the\napplication of the model for high dimensional data usually acquired in medical\nmicroscopy. The method was tested on 600 blood slides with various imaging\nconditions. The speed of the method is higher than current supervised systems\nwhile its accuracy is comparable to or better than them.\n", "versions": [{"version": "v1", "created": "Thu, 17 Aug 2017 10:55:05 GMT"}], "update_date": "2017-08-18", "authors_parsed": [["Mehrjou", "Arash", ""]]}, {"id": "1708.05207", "submitter": "Jamie Hayes", "authors": "Jamie Hayes and George Danezis", "title": "Learning Universal Adversarial Perturbations with Generative Models", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CR cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Neural networks are known to be vulnerable to adversarial examples, inputs\nthat have been intentionally perturbed to remain visually similar to the source\ninput, but cause a misclassification. It was recently shown that given a\ndataset and classifier, there exists so called universal adversarial\nperturbations, a single perturbation that causes a misclassification when\napplied to any input. In this work, we introduce universal adversarial\nnetworks, a generative network that is capable of fooling a target classifier\nwhen it's generated output is added to a clean sample from a dataset. We show\nthat this technique improves on known universal adversarial attacks.\n", "versions": [{"version": "v1", "created": "Thu, 17 Aug 2017 11:25:39 GMT"}, {"version": "v2", "created": "Thu, 21 Dec 2017 17:26:10 GMT"}, {"version": "v3", "created": "Fri, 5 Jan 2018 14:12:48 GMT"}], "update_date": "2018-01-08", "authors_parsed": [["Hayes", "Jamie", ""], ["Danezis", "George", ""]]}, {"id": "1708.05239", "submitter": "Christopher Nemeth", "authors": "Christopher Nemeth, Fredrik Lindsten, Maurizio Filippone and James\n  Hensman", "title": "Pseudo-extended Markov chain Monte Carlo", "comments": "Advances in Neural Information Processing Systems 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.CO stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Sampling from posterior distributions using Markov chain Monte Carlo (MCMC)\nmethods can require an exhaustive number of iterations, particularly when the\nposterior is multi-modal as the MCMC sampler can become trapped in a local mode\nfor a large number of iterations. In this paper, we introduce the\npseudo-extended MCMC method as a simple approach for improving the mixing of\nthe MCMC sampler for multi-modal posterior distributions. The pseudo-extended\nmethod augments the state-space of the posterior using pseudo-samples as\nauxiliary variables. On the extended space, the modes of the posterior are\nconnected, which allows the MCMC sampler to easily move between well-separated\nposterior modes. We demonstrate that the pseudo-extended approach delivers\nimproved MCMC sampling over the Hamiltonian Monte Carlo algorithm on\nmulti-modal posteriors, including Boltzmann machines and models with\nsparsity-inducing priors.\n", "versions": [{"version": "v1", "created": "Thu, 17 Aug 2017 12:45:07 GMT"}, {"version": "v2", "created": "Thu, 14 Feb 2019 12:03:37 GMT"}, {"version": "v3", "created": "Tue, 29 Oct 2019 17:13:57 GMT"}], "update_date": "2019-10-30", "authors_parsed": [["Nemeth", "Christopher", ""], ["Lindsten", "Fredrik", ""], ["Filippone", "Maurizio", ""], ["Hensman", "James", ""]]}, {"id": "1708.05254", "submitter": "Bharath Sriperumbudur", "authors": "Ingo Steinwart, Bharath K. Sriperumbudur, Philipp Thomann", "title": "Adaptive Clustering Using Kernel Density Estimators", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We derive and analyze a generic, recursive algorithm for estimating all\nsplits in a finite cluster tree as well as the corresponding clusters. We\nfurther investigate statistical properties of this generic clustering algorithm\nwhen it receives level set estimates from a kernel density estimator. In\nparticular, we derive finite sample guarantees, consistency, rates of\nconvergence, and an adaptive data-driven strategy for choosing the kernel\nbandwidth. For these results we do not need continuity assumptions on the\ndensity such as H\\\"{o}lder continuity, but only require intuitive geometric\nassumptions of non-parametric nature.\n", "versions": [{"version": "v1", "created": "Thu, 17 Aug 2017 13:19:16 GMT"}, {"version": "v2", "created": "Sat, 17 Aug 2019 02:19:24 GMT"}], "update_date": "2019-08-20", "authors_parsed": [["Steinwart", "Ingo", ""], ["Sriperumbudur", "Bharath K.", ""], ["Thomann", "Philipp", ""]]}, {"id": "1708.05257", "submitter": "Christoph Carl Kling", "authors": "Christoph Carl Kling", "title": "Auxiliary Variables for Multi-Dirichlet Priors", "comments": "Technical Report", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Bayesian models that mix multiple Dirichlet prior parameters, called\nMulti-Dirichlet priors (MD) in this paper, are gaining popularity. Inferring\nmixing weights and parameters of mixed prior distributions seems tricky, as\nsums over Dirichlet parameters complicate the joint distribution of model\nparameters.\n  This paper shows a novel auxiliary variable scheme which helps to simplify\nthe inference for models involving hierarchical MDs and MDPs. Using this\nscheme, it is easy to derive fully collapsed inference schemes which allow for\nan efficient inference.\n", "versions": [{"version": "v1", "created": "Thu, 17 Aug 2017 13:24:36 GMT"}], "update_date": "2017-08-18", "authors_parsed": [["Kling", "Christoph Carl", ""]]}, {"id": "1708.05258", "submitter": "Pascal Kerschke", "authors": "Pascal Kerschke", "title": "Comprehensive Feature-Based Landscape Analysis of Continuous and\n  Constrained Optimization Problems Using the R-Package flacco", "comments": "30 pages, 15 figures, currently under review at Journal of\n  Statistical Software, further information on flacco can be found here:\n  https://github.com/kerschke/flacco", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Choosing the best-performing optimizer(s) out of a portfolio of optimization\nalgorithms is usually a difficult and complex task. It gets even worse, if the\nunderlying functions are unknown, i.e., so-called Black-Box problems, and\nfunction evaluations are considered to be expensive. In the case of continuous\nsingle-objective optimization problems, Exploratory Landscape Analysis (ELA) -\na sophisticated and effective approach for characterizing the landscapes of\nsuch problems by means of numerical values before actually performing the\noptimization task itself - is advantageous. Unfortunately, until now it has\nbeen quite complicated to compute multiple ELA features simultaneously, as the\ncorresponding code has been - if at all - spread across multiple platforms or\nat least across several packages within these platforms.\n  This article presents a broad summary of existing ELA approaches and\nintroduces flacco, an R-package for feature-based landscape analysis of\ncontinuous and constrained optimization problems. Although its functions\nneither solve the optimization problem itself nor the related \"Algorithm\nSelection Problem (ASP)\", it offers easy access to an essential ingredient of\nthe ASP by providing a wide collection of ELA features on a single platform -\neven within a single package. In addition, flacco provides multiple\nvisualization techniques, which enhance the understanding of some of these\nnumerical features, and thereby make certain landscape properties more\ncomprehensible. On top of that, we will introduce the package's build-in, as\nwell as web-hosted and hence platform-independent, graphical user interface\n(GUI), which facilitates the usage of the package - especially for people who\nare not familiar with R - making it a very convenient toolbox when working\ntowards algorithm selection of continuous single-objective optimization\nproblems.\n", "versions": [{"version": "v1", "created": "Thu, 17 Aug 2017 13:27:21 GMT"}], "update_date": "2017-08-18", "authors_parsed": [["Kerschke", "Pascal", ""]]}, {"id": "1708.05357", "submitter": "Celestine D\\\"unner", "authors": "Celestine D\\\"unner, Thomas Parnell, Martin Jaggi", "title": "Efficient Use of Limited-Memory Accelerators for Linear Learning on\n  Heterogeneous Systems", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.DC math.OC stat.ML", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  We propose a generic algorithmic building block to accelerate training of\nmachine learning models on heterogeneous compute systems. Our scheme allows to\nefficiently employ compute accelerators such as GPUs and FPGAs for the training\nof large-scale machine learning models, when the training data exceeds their\nmemory capacity. Also, it provides adaptivity to any system's memory hierarchy\nin terms of size and processing speed. Our technique is built upon novel\ntheoretical insights regarding primal-dual coordinate methods, and uses duality\ngap information to dynamically decide which part of the data should be made\navailable for fast processing. To illustrate the power of our approach we\ndemonstrate its performance for training of generalized linear models on a\nlarge-scale dataset exceeding the memory size of a modern GPU, showing an\norder-of-magnitude speedup over existing approaches.\n", "versions": [{"version": "v1", "created": "Thu, 17 Aug 2017 16:33:20 GMT"}, {"version": "v2", "created": "Tue, 7 Nov 2017 10:47:36 GMT"}], "update_date": "2017-11-08", "authors_parsed": [["D\u00fcnner", "Celestine", ""], ["Parnell", "Thomas", ""], ["Jaggi", "Martin", ""]]}, {"id": "1708.05446", "submitter": "Feiyun Zhu", "authors": "Feiyun Zhu, Xinliang Zhu, Sheng Wang, Jiawen Yao, Junzhou Huang", "title": "Robust Contextual Bandit via the Capped-$\\ell_{2}$ norm", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper considers the actor-critic contextual bandit for the mobile health\n(mHealth) intervention. The state-of-the-art decision-making methods in mHealth\ngenerally assume that the noise in the dynamic system follows the Gaussian\ndistribution. Those methods use the least-square-based algorithm to estimate\nthe expected reward, which is prone to the existence of outliers. To deal with\nthe issue of outliers, we propose a novel robust actor-critic contextual bandit\nmethod for the mHealth intervention. In the critic updating, the\ncapped-$\\ell_{2}$ norm is used to measure the approximation error, which\nprevents outliers from dominating our objective. A set of weights could be\nachieved from the critic updating. Considering them gives a weighted objective\nfor the actor updating. It provides the badly noised sample in the critic\nupdating with zero weights for the actor updating. As a result, the robustness\nof both actor-critic updating is enhanced. There is a key parameter in the\ncapped-$\\ell_{2}$ norm. We provide a reliable method to properly set it by\nmaking use of one of the most fundamental definitions of outliers in\nstatistics. Extensive experiment results demonstrate that our method can\nachieve almost identical results compared with the state-of-the-art methods on\nthe dataset without outliers and dramatically outperform them on the datasets\nnoised by outliers.\n", "versions": [{"version": "v1", "created": "Thu, 17 Aug 2017 21:44:36 GMT"}], "update_date": "2017-08-21", "authors_parsed": [["Zhu", "Feiyun", ""], ["Zhu", "Xinliang", ""], ["Wang", "Sheng", ""], ["Yao", "Jiawen", ""], ["Huang", "Junzhou", ""]]}, {"id": "1708.05472", "submitter": "Braxton Osting", "authors": "Braxton Osting and Todd Harry Reeb", "title": "Consistency of Dirichlet Partitions", "comments": "23 pages, 3 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST math.OC stat.ML stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A Dirichlet $k$-partition of a domain $U \\subseteq \\mathbb{R}^d$ is a\ncollection of $k$ pairwise disjoint open subsets such that the sum of their\nfirst Laplace-Dirichlet eigenvalues is minimal. A discrete version of Dirichlet\npartitions has been posed on graphs with applications in data analysis. Both\nversions admit variational formulations: solutions are characterized by\nminimizers of the Dirichlet energy of mappings from $U$ into a singular space\n$\\Sigma_k \\subseteq \\mathbb{R}^k$. In this paper, we extend results of N.\\\nGarc\\'ia Trillos and D.\\ Slep\\v{c}ev to show that there exist solutions of the\ncontinuum problem arising as limits to solutions of a sequence of discrete\nproblems. Specifically, a sequence of points $\\{x_i\\}_{i \\in \\mathbb{N}}$ from\n$U$ is sampled i.i.d.\\ with respect to a given probability measure $\\nu$ on $U$\nand for all $n \\in \\mathbb{N}$, a geometric graph $G_n$ is constructed from the\nfirst $n$ points $x_1, x_2, \\ldots, x_n$ and the pairwise distances between the\npoints. With probability one with respect to the choice of points $\\{x_i\\}_{i\n\\in \\mathbb{N}}$, we show that as $n \\to \\infty$ the discrete Dirichlet\nenergies for functions $G_n \\to \\Sigma_k$ $\\Gamma$-converge to (a scalar\nmultiple of) the continuum Dirichlet energy for functions $U \\to \\Sigma_k$ with\nrespect to a metric coming from the theory of optimal transport. This, along\nwith a compactness property for the aforementioned energies that we prove,\nimplies the convergence of minimizers. When $\\nu$ is the uniform distribution,\nour results also imply the statistical consistency statement that Dirichlet\npartitions of geometric graphs converge to partitions of the sampled space in\nthe Hausdorff sense.\n", "versions": [{"version": "v1", "created": "Fri, 18 Aug 2017 00:19:17 GMT"}], "update_date": "2017-08-21", "authors_parsed": [["Osting", "Braxton", ""], ["Reeb", "Todd Harry", ""]]}, {"id": "1708.05487", "submitter": "Shaogao Lv", "authors": "Shaogao Lv and Heng Lian", "title": "Debiased distributed learning for sparse partial linear models in high\n  dimensions", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Although various distributed machine learning schemes have been proposed\nrecently for pure linear models and fully nonparametric models, little\nattention has been paid on distributed optimization for semi-paramemetric\nmodels with multiple-level structures (e.g. sparsity, linearity and\nnonlinearity). To address these issues, the current paper proposes a new\ncommunication-efficient distributed learning algorithm for partially sparse\nlinear models with an increasing number of features. The proposed method is\nbased on the classical divide and conquer strategy for handing big data and\neach sub-method defined on each subsample consists of a debiased estimation of\nthe double-regularized least squares approach. With the proposed method, we\ntheoretically prove that our global parametric estimator can achieve optimal\nparametric rate in our semi-parametric model given an appropriate partition on\nthe total data. Specially, the choice of data partition relies on the\nunderlying smoothness of the nonparametric component, but it is adaptive to the\nsparsity parameter. Even under the non-distributed setting, we develop a new\nand easily-read proof for optimal estimation of the parametric error in high\ndimensional partial linear model. Finally, several simulated experiments are\nimplemented to indicate comparable empirical performance of our debiased\ntechnique under the distributed setting.\n", "versions": [{"version": "v1", "created": "Fri, 18 Aug 2017 02:35:30 GMT"}, {"version": "v2", "created": "Sun, 3 Nov 2019 04:06:28 GMT"}], "update_date": "2019-11-05", "authors_parsed": [["Lv", "Shaogao", ""], ["Lian", "Heng", ""]]}, {"id": "1708.05512", "submitter": "Sanping Zhou", "authors": "Sanping Zhou, Jinjun Wang, Rui Shi, Qiqi Hou, Yihong Gong, Nanning\n  Zheng", "title": "Large Margin Learning in Set to Set Similarity Comparison for Person\n  Re-identification", "comments": "Accepted by IEEE Transactions on Multimedia", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Person re-identification (Re-ID) aims at matching images of the same person\nacross disjoint camera views, which is a challenging problem in multimedia\nanalysis, multimedia editing and content-based media retrieval communities. The\nmajor challenge lies in how to preserve similarity of the same person across\nvideo footages with large appearance variations, while discriminating different\nindividuals. To address this problem, conventional methods usually consider the\npairwise similarity between persons by only measuring the point to point (P2P)\ndistance. In this paper, we propose to use deep learning technique to model a\nnovel set to set (S2S) distance, in which the underline objective focuses on\npreserving the compactness of intra-class samples for each camera view, while\nmaximizing the margin between the intra-class set and inter-class set. The S2S\ndistance metric is consisted of three terms, namely the class-identity term,\nthe relative distance term and the regularization term. The class-identity term\nkeeps the intra-class samples within each camera view gathering together, the\nrelative distance term maximizes the distance between the intra-class class set\nand inter-class set across different camera views, and the regularization term\nsmoothness the parameters of deep convolutional neural network (CNN). As a\nresult, the final learned deep model can effectively find out the matched\ntarget to the probe object among various candidates in the video gallery by\nlearning discriminative and stable feature representations. Using the CUHK01,\nCUHK03, PRID2011 and Market1501 benchmark datasets, we extensively conducted\ncomparative evaluations to demonstrate the advantages of our method over the\nstate-of-the-art approaches.\n", "versions": [{"version": "v1", "created": "Fri, 18 Aug 2017 05:19:01 GMT"}], "update_date": "2017-08-21", "authors_parsed": [["Zhou", "Sanping", ""], ["Wang", "Jinjun", ""], ["Shi", "Rui", ""], ["Hou", "Qiqi", ""], ["Gong", "Yihong", ""], ["Zheng", "Nanning", ""]]}, {"id": "1708.05569", "submitter": "Francesco Tudisco", "authors": "Francesco Tudisco, Pedro Mercado and Matthias Hein", "title": "Community detection in networks via nonlinear modularity eigenvectors", "comments": null, "journal-ref": "SIAM J. Applied Mathematics, 78:2393--2419, 2018", "doi": "10.1137/17M1144143", "report-no": null, "categories": "cs.SI math.OC stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Revealing a community structure in a network or dataset is a central problem\narising in many scientific areas. The modularity function $Q$ is an established\nmeasure quantifying the quality of a community, being identified as a set of\nnodes having high modularity. In our terminology, a set of nodes with positive\nmodularity is called a \\textit{module} and a set that maximizes $Q$ is thus\ncalled \\textit{leading module}. Finding a leading module in a network is an\nimportant task, however the dimension of real-world problems makes the\nmaximization of $Q$ unfeasible. This poses the need of approximation techniques\nwhich are typically based on a linear relaxation of $Q$, induced by the\nspectrum of the modularity matrix $M$. In this work we propose a nonlinear\nrelaxation which is instead based on the spectrum of a nonlinear modularity\noperator $\\mathcal M$. We show that extremal eigenvalues of $\\mathcal M$\nprovide an exact relaxation of the modularity measure $Q$, however at the price\nof being more challenging to be computed than those of $M$. Thus we extend the\nwork made on nonlinear Laplacians, by proposing a computational scheme, named\n\\textit{generalized RatioDCA}, to address such extremal eigenvalues. We show\nmonotonic ascent and convergence of the method. We finally apply the new method\nto several synthetic and real-world data sets, showing both effectiveness of\nthe model and performance of the method.\n", "versions": [{"version": "v1", "created": "Fri, 18 Aug 2017 11:43:21 GMT"}, {"version": "v2", "created": "Wed, 12 Sep 2018 10:38:07 GMT"}], "update_date": "2018-09-13", "authors_parsed": [["Tudisco", "Francesco", ""], ["Mercado", "Pedro", ""], ["Hein", "Matthias", ""]]}, {"id": "1708.05573", "submitter": "Soumendu Sundar Mukherjee", "authors": "Soumendu Sundar Mukherjee, Purnamrita Sarkar, and Peter J. Bickel", "title": "Two provably consistent divide and conquer clustering algorithms for\n  large networks", "comments": "41 pages, comments are most welcome", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML math.ST stat.CO stat.ME stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this article, we advance divide-and-conquer strategies for solving the\ncommunity detection problem in networks. We propose two algorithms which\nperform clustering on a number of small subgraphs and finally patches the\nresults into a single clustering. The main advantage of these algorithms is\nthat they bring down significantly the computational cost of traditional\nalgorithms, including spectral clustering, semi-definite programs, modularity\nbased methods, likelihood based methods etc., without losing on accuracy and\neven improving accuracy at times. These algorithms are also, by nature,\nparallelizable. Thus, exploiting the facts that most traditional algorithms are\naccurate and the corresponding optimization problems are much simpler in small\nproblems, our divide-and-conquer methods provide an omnibus recipe for scaling\ntraditional algorithms up to large networks. We prove consistency of these\nalgorithms under various subgraph selection procedures and perform extensive\nsimulations and real-data analysis to understand the advantages of the\ndivide-and-conquer approach in various settings.\n", "versions": [{"version": "v1", "created": "Fri, 18 Aug 2017 12:09:10 GMT"}], "update_date": "2017-08-21", "authors_parsed": [["Mukherjee", "Soumendu Sundar", ""], ["Sarkar", "Purnamrita", ""], ["Bickel", "Peter J.", ""]]}, {"id": "1708.05594", "submitter": "Tu Dinh Nguyen", "authors": "Tu Dinh Nguyen, Truyen Tran, Dinh Phung, Svetha Venkatesh", "title": "Statistical Latent Space Approach for Mixed Data Modelling and\n  Applications", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The analysis of mixed data has been raising challenges in statistics and\nmachine learning. One of two most prominent challenges is to develop new\nstatistical techniques and methodologies to effectively handle mixed data by\nmaking the data less heterogeneous with minimum loss of information. The other\nchallenge is that such methods must be able to apply in large-scale tasks when\ndealing with huge amount of mixed data. To tackle these challenges, we\nintroduce parameter sharing and balancing extensions to our recent model, the\nmixed-variate restricted Boltzmann machine (MV.RBM) which can transform\nheterogeneous data into homogeneous representation. We also integrate\nstructured sparsity and distance metric learning into RBM-based models. Our\nproposed methods are applied in various applications including latent patient\nprofile modelling in medical data analysis and representation learning for\nimage retrieval. The experimental results demonstrate the models perform better\nthan baseline methods in medical data and outperform state-of-the-art rivals in\nimage dataset.\n", "versions": [{"version": "v1", "created": "Fri, 18 Aug 2017 13:17:57 GMT"}], "update_date": "2017-08-21", "authors_parsed": [["Nguyen", "Tu Dinh", ""], ["Tran", "Truyen", ""], ["Phung", "Dinh", ""], ["Venkatesh", "Svetha", ""]]}, {"id": "1708.05629", "submitter": "Ying Wei", "authors": "Ying Wei, Yu Zhang, Qiang Yang", "title": "Learning to Transfer", "comments": "12 pages, 8 figures, conference", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Transfer learning borrows knowledge from a source domain to facilitate\nlearning in a target domain. Two primary issues to be addressed in transfer\nlearning are what and how to transfer. For a pair of domains, adopting\ndifferent transfer learning algorithms results in different knowledge\ntransferred between them. To discover the optimal transfer learning algorithm\nthat maximally improves the learning performance in the target domain,\nresearchers have to exhaustively explore all existing transfer learning\nalgorithms, which is computationally intractable. As a trade-off, a sub-optimal\nalgorithm is selected, which requires considerable expertise in an ad-hoc way.\nMeanwhile, it is widely accepted in educational psychology that human beings\nimprove transfer learning skills of deciding what to transfer through\nmeta-cognitive reflection on inductive transfer learning practices. Motivated\nby this, we propose a novel transfer learning framework known as Learning to\nTransfer (L2T) to automatically determine what and how to transfer are the best\nby leveraging previous transfer learning experiences. We establish the L2T\nframework in two stages: 1) we first learn a reflection function encrypting\ntransfer learning skills from experiences; and 2) we infer what and how to\ntransfer for a newly arrived pair of domains by optimizing the reflection\nfunction. Extensive experiments demonstrate the L2T's superiority over several\nstate-of-the-art transfer learning algorithms and its effectiveness on\ndiscovering more transferable knowledge.\n", "versions": [{"version": "v1", "created": "Fri, 18 Aug 2017 14:36:29 GMT"}], "update_date": "2017-08-21", "authors_parsed": [["Wei", "Ying", ""], ["Zhang", "Yu", ""], ["Yang", "Qiang", ""]]}, {"id": "1708.05712", "submitter": "Colleen Farrelly", "authors": "Colleen M. Farrelly", "title": "Extensions of Morse-Smale Regression with Application to Actuarial\n  Science", "comments": "14 pages, 10 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The problem of subgroups is ubiquitous in scientific research (ex. disease\nheterogeneity, spatial distributions in ecology...), and piecewise regression\nis one way to deal with this phenomenon. Morse-Smale regression offers a way to\npartition the regression function based on level sets of a defined function and\nthat function's basins of attraction. This topologically-based piecewise\nregression algorithm has shown promise in its initial applications, but the\ncurrent implementation in the literature has been limited to elastic net and\ngeneralized linear regression. It is possible that nonparametric methods, such\nas random forest or conditional inference trees, may provide better prediction\nand insight through modeling interaction terms and other nonlinear\nrelationships between predictors and a given outcome.\n  This study explores the use of several machine learning algorithms within a\nMorse-Smale piecewise regression framework, including boosted regression with\nlinear baselearners, homotopy-based LASSO, conditional inference trees, random\nforest, and a wide neural network framework called extreme learning machines.\nSimulations on Tweedie regression problems with varying Tweedie parameter and\ndispersion suggest that many machine learning approaches to Morse-Smale\npiecewise regression improve the original algorithm's performance, particularly\nfor outcomes with lower dispersion and linear or a mix of linear and nonlinear\npredictor relationships. On a real actuarial problem, several of these new\nalgorithms perform as good as or better than the original Morse-Smale\nregression algorithm, and most provide information on the nature of predictor\nrelationships within each partition to provide insight into differences between\ndataset partitions.\n", "versions": [{"version": "v1", "created": "Thu, 17 Aug 2017 22:33:35 GMT"}], "update_date": "2017-08-22", "authors_parsed": [["Farrelly", "Colleen M.", ""]]}, {"id": "1708.05715", "submitter": "Tahereh Mazaheri", "authors": "Patrick Chao, Tahereh Mazaheri, Bo Sun, Nicholas B. Weingartner and\n  Zohar Nussinov", "title": "The Stochastic Replica Approach to Machine Learning: Stability and\n  Parameter Optimization", "comments": "30 pages, 42 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cond-mat.stat-mech physics.data-an", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We introduce a statistical physics inspired supervised machine learning\nalgorithm for classification and regression problems. The method is based on\nthe invariances or stability of predicted results when known data is\nrepresented as expansions in terms of various stochastic functions. The\nalgorithm predicts the classification/regression values of new data by\ncombining (via voting) the outputs of these numerous linear expansions in\nrandomly chosen functions. The few parameters (typically only one parameter is\nused in all studied examples) that this model has may be automatically\noptimized. The algorithm has been tested on 10 diverse training data sets of\nvarious types and feature space dimensions. It has been shown to consistently\nexhibit high accuracy and readily allow for optimization of parameters, while\nsimultaneously avoiding pitfalls of existing algorithms such as those\nassociated with class imbalance. We very briefly speculate on whether spatial\ncoordinates in physical theories may be viewed as emergent \"features\" that\nenable a robust machine learning type description of data with generic low\norder smooth functions.\n", "versions": [{"version": "v1", "created": "Fri, 18 Aug 2017 18:00:00 GMT"}, {"version": "v2", "created": "Fri, 13 Oct 2017 18:00:02 GMT"}, {"version": "v3", "created": "Fri, 16 Nov 2018 05:14:33 GMT"}], "update_date": "2018-11-19", "authors_parsed": [["Chao", "Patrick", ""], ["Mazaheri", "Tahereh", ""], ["Sun", "Bo", ""], ["Weingartner", "Nicholas B.", ""], ["Nussinov", "Zohar", ""]]}, {"id": "1708.05757", "submitter": "John Dabiri", "authors": "Kristy L. Schlueter-Kuck and John O. Dabiri", "title": "Identification of individual coherent sets associated with flow\n  trajectories using Coherent Structure Coloring", "comments": "In press at Chaos", "journal-ref": null, "doi": "10.1063/1.4993862", "report-no": null, "categories": "physics.flu-dyn math.DS stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a method for identifying the coherent structures associated with\nindividual Lagrangian flow trajectories even where only sparse particle\ntrajectory data is available. The method, based on techniques in spectral graph\ntheory, uses the Coherent Structure Coloring vector and associated eigenvectors\nto analyze the distance in higher-dimensional eigenspace between a selected\nreference trajectory and other tracer trajectories in the flow. By analyzing\nthis distance metric in a hierarchical clustering, the coherent structure of\nwhich the reference particle is a member can be identified. This algorithm is\nproven successful in identifying coherent structures of varying complexities in\ncanonical unsteady flows. Additionally, the method is able to assess the\nrelative coherence of the associated structure in comparison to the surrounding\nflow. Although the method is demonstrated here in the context of fluid flow\nkinematics, the generality of the approach allows for its potential application\nto other unsupervised clustering problems in dynamical systems such as neuronal\nactivity, gene expression, or social networks.\n", "versions": [{"version": "v1", "created": "Fri, 18 Aug 2017 20:35:54 GMT"}], "update_date": "2017-10-11", "authors_parsed": [["Schlueter-Kuck", "Kristy L.", ""], ["Dabiri", "John O.", ""]]}, {"id": "1708.05768", "submitter": "Gal Mishne", "authors": "Gal Mishne, Ronen Talmon, Israel Cohen, Ronald R. Coifman and Yuval\n  Kluger", "title": "Data-Driven Tree Transforms and Metrics", "comments": "16 pages, 5 figures. Accepted to IEEE Transactions on Signal and\n  Information Processing over Networks", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG q-bio.QM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider the analysis of high dimensional data given in the form of a\nmatrix with columns consisting of observations and rows consisting of features.\nOften the data is such that the observations do not reside on a regular grid,\nand the given order of the features is arbitrary and does not convey a notion\nof locality. Therefore, traditional transforms and metrics cannot be used for\ndata organization and analysis. In this paper, our goal is to organize the data\nby defining an appropriate representation and metric such that they respect the\nsmoothness and structure underlying the data. We also aim to generalize the\njoint clustering of observations and features in the case the data does not\nfall into clear disjoint groups. For this purpose, we propose multiscale\ndata-driven transforms and metrics based on trees. Their construction is\nimplemented in an iterative refinement procedure that exploits the\nco-dependencies between features and observations. Beyond the organization of a\nsingle dataset, our approach enables us to transfer the organization learned\nfrom one dataset to another and to integrate several datasets together. We\npresent an application to breast cancer gene expression analysis: learning\nmetrics on the genes to cluster the tumor samples into cancer sub-types and\nvalidating the joint organization of both the genes and the samples. We\ndemonstrate that using our approach to combine information from multiple gene\nexpression cohorts, acquired by different profiling technologies, improves the\nclustering of tumor samples.\n", "versions": [{"version": "v1", "created": "Fri, 18 Aug 2017 21:32:34 GMT"}], "update_date": "2017-08-22", "authors_parsed": [["Mishne", "Gal", ""], ["Talmon", "Ronen", ""], ["Cohen", "Israel", ""], ["Coifman", "Ronald R.", ""], ["Kluger", "Yuval", ""]]}, {"id": "1708.05789", "submitter": "Kumar Sricharan", "authors": "Kumar Sricharan, Raja Bala, Matthew Shreve, Hui Ding, Kumar Saketh,\n  Jin Sun", "title": "Semi-supervised Conditional GANs", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We introduce a new model for building conditional generative models in a\nsemi-supervised setting to conditionally generate data given attributes by\nadapting the GAN framework. The proposed semi-supervised GAN (SS-GAN) model\nuses a pair of stacked discriminators to learn the marginal distribution of the\ndata, and the conditional distribution of the attributes given the data\nrespectively. In the semi-supervised setting, the marginal distribution (which\nis often harder to learn) is learned from the labeled + unlabeled data, and the\nconditional distribution is learned purely from the labeled data. Our\nexperimental results demonstrate that this model performs significantly better\ncompared to existing semi-supervised conditional GAN models.\n", "versions": [{"version": "v1", "created": "Sat, 19 Aug 2017 01:05:02 GMT"}], "update_date": "2017-08-22", "authors_parsed": [["Sricharan", "Kumar", ""], ["Bala", "Raja", ""], ["Shreve", "Matthew", ""], ["Ding", "Hui", ""], ["Saketh", "Kumar", ""], ["Sun", "Jin", ""]]}, {"id": "1708.05840", "submitter": "Disha Shrivastava", "authors": "Disha Shrivastava, Santanu Chaudhury and Dr. Jayadeva", "title": "A Data and Model-Parallel, Distributed and Scalable Framework for\n  Training of Deep Networks in Apache Spark", "comments": "12 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.AI cs.CV cs.DC cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Training deep networks is expensive and time-consuming with the training\nperiod increasing with data size and growth in model parameters. In this paper,\nwe provide a framework for distributed training of deep networks over a cluster\nof CPUs in Apache Spark. The framework implements both Data Parallelism and\nModel Parallelism making it suitable to use for deep networks which require\nhuge training data and model parameters which are too big to fit into the\nmemory of a single machine. It can be scaled easily over a cluster of cheap\ncommodity hardware to attain significant speedup and obtain better results\nmaking it quite economical as compared to farm of GPUs and supercomputers. We\nhave proposed a new algorithm for training of deep networks for the case when\nthe network is partitioned across the machines (Model Parallelism) along with\ndetailed cost analysis and proof of convergence of the same. We have developed\nimplementations for Fully-Connected Feedforward Networks, Convolutional Neural\nNetworks, Recurrent Neural Networks and Long Short-Term Memory architectures.\nWe present the results of extensive simulations demonstrating the speedup and\naccuracy obtained by our framework for different sizes of the data and model\nparameters with variation in the number of worker cores/partitions; thereby\nshowing that our proposed framework can achieve significant speedup (upto 11X\nfor CNN) and is also quite scalable.\n", "versions": [{"version": "v1", "created": "Sat, 19 Aug 2017 13:17:58 GMT"}], "update_date": "2017-08-22", "authors_parsed": [["Shrivastava", "Disha", ""], ["Chaudhury", "Santanu", ""], ["Jayadeva", "Dr.", ""]]}, {"id": "1708.05866", "submitter": "Kai Arulkumaran", "authors": "Kai Arulkumaran, Marc Peter Deisenroth, Miles Brundage, Anil Anthony\n  Bharath", "title": "A Brief Survey of Deep Reinforcement Learning", "comments": "IEEE Signal Processing Magazine, Special Issue on Deep Learning for\n  Image Understanding (arXiv extended version)", "journal-ref": null, "doi": "10.1109/MSP.2017.2743240", "report-no": null, "categories": "cs.LG cs.AI cs.CV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Deep reinforcement learning is poised to revolutionise the field of AI and\nrepresents a step towards building autonomous systems with a higher level\nunderstanding of the visual world. Currently, deep learning is enabling\nreinforcement learning to scale to problems that were previously intractable,\nsuch as learning to play video games directly from pixels. Deep reinforcement\nlearning algorithms are also applied to robotics, allowing control policies for\nrobots to be learned directly from camera inputs in the real world. In this\nsurvey, we begin with an introduction to the general field of reinforcement\nlearning, then progress to the main streams of value-based and policy-based\nmethods. Our survey will cover central algorithms in deep reinforcement\nlearning, including the deep $Q$-network, trust region policy optimisation, and\nasynchronous advantage actor-critic. In parallel, we highlight the unique\nadvantages of deep neural networks, focusing on visual understanding via\nreinforcement learning. To conclude, we describe several current areas of\nresearch within the field.\n", "versions": [{"version": "v1", "created": "Sat, 19 Aug 2017 15:55:31 GMT"}, {"version": "v2", "created": "Thu, 28 Sep 2017 21:51:43 GMT"}], "update_date": "2017-11-15", "authors_parsed": [["Arulkumaran", "Kai", ""], ["Deisenroth", "Marc Peter", ""], ["Brundage", "Miles", ""], ["Bharath", "Anil Anthony", ""]]}, {"id": "1708.05894", "submitter": "Joseph Futoma", "authors": "Joseph Futoma, Sanjay Hariharan, Mark Sendak, Nathan Brajer, Meredith\n  Clement, Armando Bedoya, Cara O'Brien, Katherine Heller", "title": "An Improved Multi-Output Gaussian Process RNN with Real-Time Validation\n  for Early Sepsis Detection", "comments": "Presented at Machine Learning for Healthcare 2017, Boston, MA", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML stat.AP stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Sepsis is a poorly understood and potentially life-threatening complication\nthat can occur as a result of infection. Early detection and treatment improves\npatient outcomes, and as such it poses an important challenge in medicine. In\nthis work, we develop a flexible classifier that leverages streaming lab\nresults, vitals, and medications to predict sepsis before it occurs. We model\npatient clinical time series with multi-output Gaussian processes, maintaining\nuncertainty about the physiological state of a patient while also imputing\nmissing values. The mean function takes into account the effects of medications\nadministered on the trajectories of the physiological variables. Latent\nfunction values from the Gaussian process are then fed into a deep recurrent\nneural network to classify patient encounters as septic or not, and the overall\nmodel is trained end-to-end using back-propagation. We train and validate our\nmodel on a large dataset of 18 months of heterogeneous inpatient stays from the\nDuke University Health System, and develop a new \"real-time\" validation scheme\nfor simulating the performance of our model as it will actually be used. Our\nproposed method substantially outperforms clinical baselines, and improves on a\nprevious related model for detecting sepsis. Our model's predictions will be\ndisplayed in a real-time analytics dashboard to be used by a sepsis rapid\nresponse team to help detect and improve treatment of sepsis.\n", "versions": [{"version": "v1", "created": "Sat, 19 Aug 2017 20:14:07 GMT"}], "update_date": "2017-08-22", "authors_parsed": [["Futoma", "Joseph", ""], ["Hariharan", "Sanjay", ""], ["Sendak", "Mark", ""], ["Brajer", "Nathan", ""], ["Clement", "Meredith", ""], ["Bedoya", "Armando", ""], ["O'Brien", "Cara", ""], ["Heller", "Katherine", ""]]}, {"id": "1708.05917", "submitter": "Peter Mills", "authors": "Peter Mills", "title": "Accelerating Kernel Classifiers Through Borders Mapping", "comments": "37 pages; 8 figures; 7 tables. List of symbols included. Corrections:\n  parameters table was from the previous revision; volume and page numbers\n  added", "journal-ref": "Journal of Real-Time Image Processing 17, 313-327(2020)", "doi": "10.1007/s11554-018-0769-9", "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Support vector machines (SVM) and other kernel techniques represent a family\nof powerful statistical classification methods with high accuracy and broad\napplicability. Because they use all or a significant portion of the training\ndata, however, they can be slow, especially for large problems. Piecewise\nlinear classifiers are similarly versatile, yet have the additional advantages\nof simplicity, ease of interpretation and, if the number of component linear\nclassifiers is not too large, speed. Here we show how a simple, piecewise\nlinear classifier can be trained from a kernel-based classifier in order to\nimprove the classification speed. The method works by finding the root of the\ndifference in conditional probabilities between pairs of opposite classes to\nbuild up a representation of the decision boundary. When tested on 17 different\ndatasets, it succeeded in improving the classification speed of a SVM for 12 of\nthem by up to two orders-of-magnitude. Of these, two were less accurate than a\nsimple, linear classifier. The method is best suited to problems with continuum\nfeatures data and smooth probability functions. Because the component linear\nclassifiers are built up individually from an existing classifier, rather than\nthrough a simultaneous optimization procedure, the classifier is also fast to\ntrain.\n", "versions": [{"version": "v1", "created": "Sun, 20 Aug 2017 00:24:17 GMT"}, {"version": "v2", "created": "Sun, 3 Dec 2017 15:42:49 GMT"}, {"version": "v3", "created": "Wed, 25 Jul 2018 18:12:59 GMT"}, {"version": "v4", "created": "Thu, 31 Oct 2019 02:22:16 GMT"}, {"version": "v5", "created": "Mon, 5 Apr 2021 02:24:54 GMT"}], "update_date": "2021-04-06", "authors_parsed": [["Mills", "Peter", ""]]}, {"id": "1708.05929", "submitter": "Leman Akoglu", "authors": "Meghanath Macha and Leman Akoglu", "title": "Explaining Anomalies in Groups with Characterizing Subspace Rules", "comments": "31 pages, 6 figures, 9 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Anomaly detection has numerous applications and has been studied vastly. We\nconsider a complementary problem that has a much sparser literature: anomaly\ndescription. Interpretation of anomalies is crucial for practitioners for\nsense-making, troubleshooting, and planning actions. To this end, we present a\nnew approach called x-PACS (for eXplaining Patterns of Anomalies with\nCharacterizing Subspaces), which \"reverse-engineers\" the known anomalies by\nidentifying (1) the groups (or patterns) that they form, and (2) the\ncharacterizing subspace and feature rules that separate each anomalous pattern\nfrom normal instances. Explaining anomalies in groups not only saves analyst\ntime and gives insight into various types of anomalies, but also draws\nattention to potentially critical, repeating anomalies.\n  In developing x-PACS, we first construct a desiderata for the anomaly\ndescription problem. From a descriptive data mining perspective, our method\nexhibits five desired properties in our desiderata. Namely, it can unearth\nanomalous patterns (i) of multiple different types, (ii) hidden in arbitrary\nsubspaces of a high dimensional space, (iii) interpretable by the analysts,\n(iv) different from normal patterns of the data, and finally (v) succinct,\nproviding the shortest data description. Furthermore, x-PACS is highly\nparallelizable and scales linearly in terms of data size.\n  No existing work on anomaly description satisfies all of these properties\nsimultaneously. While not our primary goal, the anomalous patterns we find\nserve as interpretable \"signatures\" and can be used for detection. We show the\neffectiveness of x-PACS in explanation as well as detection on real-world\ndatasets as compared to state-of-the-art.\n", "versions": [{"version": "v1", "created": "Sun, 20 Aug 2017 03:41:32 GMT"}, {"version": "v2", "created": "Wed, 11 Oct 2017 16:24:40 GMT"}, {"version": "v3", "created": "Thu, 16 Nov 2017 15:27:28 GMT"}, {"version": "v4", "created": "Wed, 2 May 2018 21:36:12 GMT"}], "update_date": "2018-05-04", "authors_parsed": [["Macha", "Meghanath", ""], ["Akoglu", "Leman", ""]]}, {"id": "1708.05932", "submitter": "Marco Mondelli", "authors": "Marco Mondelli and Andrea Montanari", "title": "Fundamental Limits of Weak Recovery with Applications to Phase Retrieval", "comments": "63 pages, 3 figures, presented at COLT'18 and accepted at Foundations\n  of Computational Mathematics", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.IT math.IT", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In phase retrieval we want to recover an unknown signal $\\boldsymbol\nx\\in\\mathbb C^d$ from $n$ quadratic measurements of the form $y_i =\n|\\langle{\\boldsymbol a}_i,{\\boldsymbol x}\\rangle|^2+w_i$ where $\\boldsymbol\na_i\\in \\mathbb C^d$ are known sensing vectors and $w_i$ is measurement noise.\nWe ask the following weak recovery question: what is the minimum number of\nmeasurements $n$ needed to produce an estimator $\\hat{\\boldsymbol\nx}(\\boldsymbol y)$ that is positively correlated with the signal $\\boldsymbol\nx$? We consider the case of Gaussian vectors $\\boldsymbol a_i$. We prove that -\nin the high-dimensional limit - a sharp phase transition takes place, and we\nlocate the threshold in the regime of vanishingly small noise. For $n\\le\nd-o(d)$ no estimator can do significantly better than random and achieve a\nstrictly positive correlation. For $n\\ge d+o(d)$ a simple spectral estimator\nachieves a positive correlation. Surprisingly, numerical simulations with the\nsame spectral estimator demonstrate promising performance with realistic\nsensing matrices. Spectral methods are used to initialize non-convex\noptimization algorithms in phase retrieval, and our approach can boost the\nperformance in this setting as well.\n  Our impossibility result is based on classical information-theory arguments.\nThe spectral algorithm computes the leading eigenvector of a weighted empirical\ncovariance matrix. We obtain a sharp characterization of the spectral\nproperties of this random matrix using tools from free probability and\ngeneralizing a recent result by Lu and Li. Both the upper and lower bound\ngeneralize beyond phase retrieval to measurements $y_i$ produced according to a\ngeneralized linear model. As a byproduct of our analysis, we compare the\nthreshold of the proposed spectral method with that of a message passing\nalgorithm.\n", "versions": [{"version": "v1", "created": "Sun, 20 Aug 2017 04:27:52 GMT"}, {"version": "v2", "created": "Thu, 28 Sep 2017 21:32:45 GMT"}, {"version": "v3", "created": "Thu, 26 Jul 2018 02:06:28 GMT"}], "update_date": "2018-07-27", "authors_parsed": [["Mondelli", "Marco", ""], ["Montanari", "Andrea", ""]]}, {"id": "1708.05963", "submitter": "Dmitry Ignatov", "authors": "Artem M. Grachev, Dmitry I. Ignatov, Andrey V. Savchenko", "title": "Neural Networks Compression for Language Modeling", "comments": "Keywords: LSTM, RNN, language modeling, low-rank factorization,\n  pruning, quantization. Published by Springer in the LNCS series, 7th\n  International Conference on Pattern Recognition and Machine Intelligence,\n  2017", "journal-ref": null, "doi": "10.1007/978-3-319-69900-4_44", "report-no": null, "categories": "stat.ML cs.CL cs.LG cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we consider several compression techniques for the language\nmodeling problem based on recurrent neural networks (RNNs). It is known that\nconventional RNNs, e.g, LSTM-based networks in language modeling, are\ncharacterized with either high space complexity or substantial inference time.\nThis problem is especially crucial for mobile applications, in which the\nconstant interaction with the remote server is inappropriate. By using the Penn\nTreebank (PTB) dataset we compare pruning, quantization, low-rank\nfactorization, tensor train decomposition for LSTM networks in terms of model\nsize and suitability for fast inference.\n", "versions": [{"version": "v1", "created": "Sun, 20 Aug 2017 13:37:06 GMT"}], "update_date": "2019-04-09", "authors_parsed": [["Grachev", "Artem M.", ""], ["Ignatov", "Dmitry I.", ""], ["Savchenko", "Andrey V.", ""]]}, {"id": "1708.05978", "submitter": "Linbo Qiao", "authors": "Tianyi Lin and Linbo Qiao and Teng Zhang and Jiashi Feng and Bofeng\n  Zhang", "title": "Stochastic Primal-Dual Proximal ExtraGradient Descent for Compositely\n  Regularized Optimization", "comments": null, "journal-ref": null, "doi": "10.1016/j.neucom.2017.07.066", "report-no": null, "categories": "cs.LG math.OC stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider a wide range of regularized stochastic minimization problems with\ntwo regularization terms, one of which is composed with a linear function. This\noptimization model abstracts a number of important applications in artificial\nintelligence and machine learning, such as fused Lasso, fused logistic\nregression, and a class of graph-guided regularized minimization. The\ncomputational challenges of this model are in two folds. On one hand, the\nclosed-form solution of the proximal mapping associated with the composed\nregularization term or the expected objective function is not available. On the\nother hand, the calculation of the full gradient of the expectation in the\nobjective is very expensive when the number of input data samples is\nconsiderably large. To address these issues, we propose a stochastic variant of\nextra-gradient type methods, namely \\textsf{Stochastic Primal-Dual Proximal\nExtraGradient descent (SPDPEG)}, and analyze its convergence property for both\nconvex and strongly convex objectives. For general convex objectives, the\nuniformly average iterates generated by \\textsf{SPDPEG} converge in expectation\nwith $O(1/\\sqrt{t})$ rate. While for strongly convex objectives, the uniformly\nand non-uniformly average iterates generated by \\textsf{SPDPEG} converge with\n$O(\\log(t)/t)$ and $O(1/t)$ rates, respectively. The order of the rate of the\nproposed algorithm is known to match the best convergence rate for first-order\nstochastic algorithms. Experiments on fused logistic regression and\ngraph-guided regularized logistic regression problems show that the proposed\nalgorithm performs very efficiently and consistently outperforms other\ncompeting algorithms.\n", "versions": [{"version": "v1", "created": "Sun, 20 Aug 2017 14:53:12 GMT"}, {"version": "v2", "created": "Sun, 27 Aug 2017 17:37:03 GMT"}, {"version": "v3", "created": "Tue, 12 Dec 2017 13:48:30 GMT"}, {"version": "v4", "created": "Thu, 1 Feb 2018 08:00:24 GMT"}], "update_date": "2018-02-02", "authors_parsed": [["Lin", "Tianyi", ""], ["Qiao", "Linbo", ""], ["Zhang", "Teng", ""], ["Feng", "Jiashi", ""], ["Zhang", "Bofeng", ""]]}, {"id": "1708.06020", "submitter": "Luke Taylor", "authors": "Luke Taylor, Geoff Nitschke", "title": "Improving Deep Learning using Generic Data Augmentation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Deep artificial neural networks require a large corpus of training data in\norder to effectively learn, where collection of such training data is often\nexpensive and laborious. Data augmentation overcomes this issue by artificially\ninflating the training set with label preserving transformations. Recently\nthere has been extensive use of generic data augmentation to improve\nConvolutional Neural Network (CNN) task performance. This study benchmarks\nvarious popular data augmentation schemes to allow researchers to make informed\ndecisions as to which training methods are most appropriate for their data\nsets. Various geometric and photometric schemes are evaluated on a\ncoarse-grained data set using a relatively simple CNN. Experimental results,\nrun using 4-fold cross-validation and reported in terms of Top-1 and Top-5\naccuracy, indicate that cropping in geometric augmentation significantly\nincreases CNN task performance.\n", "versions": [{"version": "v1", "created": "Sun, 20 Aug 2017 21:16:59 GMT"}], "update_date": "2017-08-22", "authors_parsed": [["Taylor", "Luke", ""], ["Nitschke", "Geoff", ""]]}, {"id": "1708.06040", "submitter": "Tongzhou Wang", "authors": "Tongzhou Wang, Yi Wu, David A. Moore, Stuart J. Russell", "title": "Meta-Learning MCMC Proposals", "comments": "32nd Conference on Neural Information Processing Systems (NeurIPS\n  2018), Montreal, Canada", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Effective implementations of sampling-based probabilistic inference often\nrequire manually constructed, model-specific proposals. Inspired by recent\nprogresses in meta-learning for training learning agents that can generalize to\nunseen environments, we propose a meta-learning approach to building effective\nand generalizable MCMC proposals. We parametrize the proposal as a neural\nnetwork to provide fast approximations to block Gibbs conditionals. The learned\nneural proposals generalize to occurrences of common structural motifs across\ndifferent models, allowing for the construction of a library of learned\ninference primitives that can accelerate inference on unseen models with no\nmodel-specific training required. We explore several applications including\nopen-universe Gaussian mixture models, in which our learned proposals\noutperform a hand-tuned sampler, and a real-world named entity recognition\ntask, in which our sampler yields higher final F1 scores than classical\nsingle-site Gibbs sampling.\n", "versions": [{"version": "v1", "created": "Mon, 21 Aug 2017 00:44:32 GMT"}, {"version": "v2", "created": "Sun, 3 Dec 2017 18:47:50 GMT"}, {"version": "v3", "created": "Thu, 14 Dec 2017 04:32:39 GMT"}, {"version": "v4", "created": "Tue, 27 Nov 2018 12:09:11 GMT"}, {"version": "v5", "created": "Tue, 1 Jan 2019 06:47:06 GMT"}], "update_date": "2019-01-03", "authors_parsed": [["Wang", "Tongzhou", ""], ["Wu", "Yi", ""], ["Moore", "David A.", ""], ["Russell", "Stuart J.", ""]]}, {"id": "1708.06077", "submitter": "Waheed Bajwa", "authors": "Talal Ahmed and Waheed U. Bajwa", "title": "ExSIS: Extended Sure Independence Screening for Ultrahigh-dimensional\n  Linear Models", "comments": "30 pages; 3 figures and 1 table; preprint of a journal publication", "journal-ref": "EURASIP J. Signal Processing, vol. 159, pp. 33-48, Jun. 2019", "doi": "10.1016/j.sigpro.2019.01.018", "report-no": null, "categories": "math.ST cs.IT math.IT stat.ML stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Statistical inference can be computationally prohibitive in\nultrahigh-dimensional linear models. Correlation-based variable screening, in\nwhich one leverages marginal correlations for removal of irrelevant variables\nfrom the model prior to statistical inference, can be used to overcome this\nchallenge. Prior works on correlation-based variable screening either impose\nstatistical priors on the linear model or assume specific post-screening\ninference methods. This paper first extends the analysis of correlation-based\nvariable screening to arbitrary linear models and post-screening inference\ntechniques. In particular, (i) it shows that a condition---termed the screening\ncondition---is sufficient for successful correlation-based screening of linear\nmodels, and (ii) it provides insights into the dependence of marginal\ncorrelation-based screening on different problem parameters. Numerical\nexperiments confirm that these insights are not mere artifacts of analysis;\nrather, they are reflective of the challenges associated with marginal\ncorrelation-based variable screening. Second, the paper explicitly derives the\nscreening condition for arbitrary (random or deterministic) linear models and,\nin the process, it establishes that---under appropriate conditions---it is\npossible to reduce the dimension of an ultrahigh-dimensional, arbitrary linear\nmodel to almost the sample size even when the number of active variables scales\nalmost linearly with the sample size. Third, it specializes the screening\ncondition to sub-Gaussian linear models and contrasts the final results to\nthose existing in the literature. This specialization formally validates the\nclaim that the main result of this paper generalizes existing ones on\ncorrelation-based screening.\n", "versions": [{"version": "v1", "created": "Mon, 21 Aug 2017 03:41:14 GMT"}, {"version": "v2", "created": "Sat, 4 Jul 2020 18:48:20 GMT"}], "update_date": "2020-07-07", "authors_parsed": [["Ahmed", "Talal", ""], ["Bajwa", "Waheed U.", ""]]}, {"id": "1708.06235", "submitter": "Joao Vieira", "authors": "Joao Vieira, Erik Leitinger, Muris Sarajlic, Xuhong Li and Fredrik\n  Tufvesson", "title": "Deep Convolutional Neural Networks for Massive MIMO Fingerprint-Based\n  Positioning", "comments": "Accepted in the IEEE International Symposium on Personal, Indoor and\n  Mobile Radio Communications (PIMRC) 2017", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.IT math.IT", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper provides an initial investigation on the application of\nconvolutional neural networks (CNNs) for fingerprint-based positioning using\nmeasured massive MIMO channels. When represented in appropriate domains,\nmassive MIMO channels have a sparse structure which can be efficiently learned\nby CNNs for positioning purposes. We evaluate the positioning accuracy of\nstate-of-the-art CNNs with channel fingerprints generated from a channel model\nwith a rich clustered structure: the COST 2100 channel model. We find that\nmoderately deep CNNs can achieve fractional-wavelength positioning accuracies,\nprovided that an enough representative data set is available for training.\n", "versions": [{"version": "v1", "created": "Mon, 21 Aug 2017 14:10:31 GMT"}], "update_date": "2017-08-22", "authors_parsed": [["Vieira", "Joao", ""], ["Leitinger", "Erik", ""], ["Sarajlic", "Muris", ""], ["Li", "Xuhong", ""], ["Tufvesson", "Fredrik", ""]]}, {"id": "1708.06243", "submitter": "Ge Wang", "authors": "Fenglei Fan, Wenxiang Cong, Ge Wang", "title": "General Backpropagation Algorithm for Training Second-order Neural\n  Networks", "comments": "5 pages, 7 figures, 19 references", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The artificial neural network is a popular framework in machine learning. To\nempower individual neurons, we recently suggested that the current type of\nneurons could be upgraded to 2nd order counterparts, in which the linear\noperation between inputs to a neuron and the associated weights is replaced\nwith a nonlinear quadratic operation. A single 2nd order neurons already has a\nstrong nonlinear modeling ability, such as implementing basic fuzzy logic\noperations. In this paper, we develop a general backpropagation (BP) algorithm\nto train the network consisting of 2nd-order neurons. The numerical studies are\nperformed to verify of the generalized BP algorithm.\n", "versions": [{"version": "v1", "created": "Thu, 17 Aug 2017 21:42:22 GMT"}], "update_date": "2017-08-22", "authors_parsed": [["Fan", "Fenglei", ""], ["Cong", "Wenxiang", ""], ["Wang", "Ge", ""]]}, {"id": "1708.06246", "submitter": "Garima Gupta", "authors": "Karamjit Singh, Garima Gupta, Vartika Tewari and Gautam Shroff", "title": "Comparative Benchmarking of Causal Discovery Techniques", "comments": "arXiv admin note: text overlap with arXiv:1506.07669,\n  arXiv:1611.03977 by other authors", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper we present a comprehensive view of prominent causal discovery\nalgorithms, categorized into two main categories (1) assuming acyclic and no\nlatent variables, and (2) allowing both cycles and latent variables, along with\nexperimental results comparing them from three perspectives: (a) structural\naccuracy, (b) standard predictive accuracy, and (c) accuracy of counterfactual\ninference. For (b) and (c) we train causal Bayesian networks with structures as\npredicted by each causal discovery technique to carry out counterfactual or\nstandard predictive inference. We compare causal algorithms on two pub- licly\navailable and one simulated datasets having different sample sizes: small,\nmedium and large. Experiments show that structural accuracy of a technique does\nnot necessarily correlate with higher accuracy of inferencing tasks. Fur- ther,\nsurveyed structure learning algorithms do not perform well in terms of\nstructural accuracy in case of datasets having large number of variables.\n", "versions": [{"version": "v1", "created": "Fri, 18 Aug 2017 04:18:30 GMT"}, {"version": "v2", "created": "Tue, 12 Sep 2017 04:02:48 GMT"}], "update_date": "2017-09-13", "authors_parsed": [["Singh", "Karamjit", ""], ["Gupta", "Garima", ""], ["Tewari", "Vartika", ""], ["Shroff", "Gautam", ""]]}, {"id": "1708.06250", "submitter": "Biswa Sengupta", "authors": "Biswa Sengupta and Yu Qian", "title": "Pillar Networks++: Distributed non-parametric deep and wide networks", "comments": "arXiv admin note: substantial text overlap with arXiv:1707.06923", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.NE stat.CO stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In recent work, it was shown that combining multi-kernel based support vector\nmachines (SVMs) can lead to near state-of-the-art performance on an action\nrecognition dataset (HMDB-51 dataset). This was 0.4\\% lower than frameworks\nthat used hand-crafted features in addition to the deep convolutional feature\nextractors. In the present work, we show that combining distributed Gaussian\nProcesses with multi-stream deep convolutional neural networks (CNN) alleviate\nthe need to augment a neural network with hand-crafted features. In contrast to\nprior work, we treat each deep neural convolutional network as an expert\nwherein the individual predictions (and their respective uncertainties) are\ncombined into a Product of Experts (PoE) framework.\n", "versions": [{"version": "v1", "created": "Fri, 18 Aug 2017 07:51:43 GMT"}], "update_date": "2017-11-21", "authors_parsed": [["Sengupta", "Biswa", ""], ["Qian", "Yu", ""]]}, {"id": "1708.06347", "submitter": "Colleen Farrelly", "authors": "Colleen M. Farrelly", "title": "Deep vs. Diverse Architectures for Classification Problems", "comments": "Paper done as part of R&D project at Kaplan University, submitted to\n  GCAI 2017", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This study compares various superlearner and deep learning architectures\n(machine-learning-based and neural-network-based) for classification problems\nacross several simulated and industrial datasets to assess performance and\ncomputational efficiency, as both methods have nice theoretical convergence\nproperties. Superlearner formulations outperform other methods at small to\nmoderate sample sizes (500-2500) on nonlinear and mixed linear/nonlinear\npredictor relationship datasets, while deep neural networks perform well on\nlinear predictor relationship datasets of all sizes. This suggests faster\nconvergence of the superlearner compared to deep neural network architectures\non many messy classification problems for real-world data.\n  Superlearners also yield interpretable models, allowing users to examine\nimportant signals in the data; in addition, they offer flexible formulation,\nwhere users can retain good performance with low-computational-cost base\nalgorithms.\n  K-nearest-neighbor (KNN) regression demonstrates improvements using the\nsuperlearner framework, as well; KNN superlearners consistently outperform deep\narchitectures and KNN regression, suggesting that superlearners may be better\nable to capture local and global geometric features through utilizing a variety\nof algorithms to probe the data space.\n", "versions": [{"version": "v1", "created": "Mon, 21 Aug 2017 15:31:44 GMT"}], "update_date": "2017-08-23", "authors_parsed": [["Farrelly", "Colleen M.", ""]]}, {"id": "1708.06374", "submitter": "Shai Shalev-Shwartz", "authors": "Shai Shalev-Shwartz, Shaked Shammah, Amnon Shashua", "title": "On a Formal Model of Safe and Scalable Self-driving Cars", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.RO cs.AI stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In recent years, car makers and tech companies have been racing towards self\ndriving cars. It seems that the main parameter in this race is who will have\nthe first car on the road. The goal of this paper is to add to the equation two\nadditional crucial parameters. The first is standardization of safety assurance\n--- what are the minimal requirements that every self-driving car must satisfy,\nand how can we verify these requirements. The second parameter is scalability\n--- engineering solutions that lead to unleashed costs will not scale to\nmillions of cars, which will push interest in this field into a niche academic\ncorner, and drive the entire field into a \"winter of autonomous driving\". In\nthe first part of the paper we propose a white-box, interpretable, mathematical\nmodel for safety assurance, which we call Responsibility-Sensitive Safety\n(RSS). In the second part we describe a design of a system that adheres to our\nsafety assurance requirements and is scalable to millions of cars.\n", "versions": [{"version": "v1", "created": "Mon, 21 Aug 2017 18:22:19 GMT"}, {"version": "v2", "created": "Sun, 8 Oct 2017 04:06:22 GMT"}, {"version": "v3", "created": "Tue, 12 Dec 2017 17:15:39 GMT"}, {"version": "v4", "created": "Mon, 18 Dec 2017 07:10:24 GMT"}, {"version": "v5", "created": "Thu, 15 Mar 2018 04:39:41 GMT"}, {"version": "v6", "created": "Sat, 27 Oct 2018 09:10:37 GMT"}], "update_date": "2018-10-30", "authors_parsed": [["Shalev-Shwartz", "Shai", ""], ["Shammah", "Shaked", ""], ["Shashua", "Amnon", ""]]}, {"id": "1708.06401", "submitter": "Swapnil Mishra", "authors": "Marian-Andrei Rizoiu, Young Lee, Swapnil Mishra, Lexing Xie", "title": "A Tutorial on Hawkes Processes for Events in Social Media", "comments": "Some typos corrected and references added", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.SI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This chapter provides an accessible introduction for point processes, and\nespecially Hawkes processes, for modeling discrete, inter-dependent events over\ncontinuous time. We start by reviewing the definitions and the key concepts in\npoint processes. We then introduce the Hawkes process, its event intensity\nfunction, as well as schemes for event simulation and parameter estimation. We\nalso describe a practical example drawn from social media data - we show how to\nmodel retweet cascades using a Hawkes self-exciting process. We presents a\ndesign of the memory kernel, and results on estimating parameters and\npredicting popularity. The code and sample event data are available as an\nonline appendix\n", "versions": [{"version": "v1", "created": "Mon, 21 Aug 2017 20:13:16 GMT"}, {"version": "v2", "created": "Mon, 9 Oct 2017 08:46:28 GMT"}], "update_date": "2017-10-10", "authors_parsed": [["Rizoiu", "Marian-Andrei", ""], ["Lee", "Young", ""], ["Mishra", "Swapnil", ""], ["Xie", "Lexing", ""]]}, {"id": "1708.06438", "submitter": "Mattia Desana", "authors": "Mattia Desana and Christoph Schn\\\"orr", "title": "Sum-Product Graphical Models", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper introduces a new probabilistic architecture called Sum-Product\nGraphical Model (SPGM). SPGMs combine traits from Sum-Product Networks (SPNs)\nand Graphical Models (GMs): Like SPNs, SPGMs always enable tractable inference\nusing a class of models that incorporate context specific independence. Like\nGMs, SPGMs provide a high-level model interpretation in terms of conditional\nindependence assumptions and corresponding factorizations. Thus, the new\narchitecture represents a class of probability distributions that combines, for\nthe first time, the semantics of graphical models with the evaluation\nefficiency of SPNs. We also propose a novel algorithm for learning both the\nstructure and the parameters of SPGMs. A comparative empirical evaluation\ndemonstrates competitive performances of our approach in density estimation.\n", "versions": [{"version": "v1", "created": "Mon, 21 Aug 2017 22:23:20 GMT"}], "update_date": "2017-08-23", "authors_parsed": [["Desana", "Mattia", ""], ["Schn\u00f6rr", "Christoph", ""]]}, {"id": "1708.06539", "submitter": "Ratneel Deo Vikash", "authors": "Ratneel Vikash Deo, Rohitash Chandra and Anuraganand Sharma", "title": "Stacked transfer learning for tropical cyclone intensity prediction", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Tropical cyclone wind-intensity prediction is a challenging task considering\ndrastic changes climate patterns over the last few decades. In order to develop\nrobust prediction models, one needs to consider different characteristics of\ncyclones in terms of spatial and temporal characteristics. Transfer learning\nincorporates knowledge from a related source dataset to compliment a target\ndatasets especially in cases where there is lack or data. Stacking is a form of\nensemble learning focused for improving generalization that has been recently\nused for transfer learning problems which is referred to as transfer stacking.\nIn this paper, we employ transfer stacking as a means of studying the effects\nof cyclones whereby we evaluate if cyclones in different geographic locations\ncan be helpful for improving generalization performs. Moreover, we use\nconventional neural networks for evaluating the effects of duration on cyclones\nin prediction performance. Therefore, we develop an effective strategy that\nevaluates the relationships between different types of cyclones through\ntransfer learning and conventional learning methods via neural networks.\n", "versions": [{"version": "v1", "created": "Tue, 22 Aug 2017 08:49:36 GMT"}], "update_date": "2017-08-23", "authors_parsed": [["Deo", "Ratneel Vikash", ""], ["Chandra", "Rohitash", ""], ["Sharma", "Anuraganand", ""]]}, {"id": "1708.06633", "submitter": "Johannes Schmidt-Hieber", "authors": "Johannes Schmidt-Hieber", "title": "Nonparametric regression using deep neural networks with ReLU activation\n  function", "comments": "article, rejoinder and supplementary material", "journal-ref": "Article: Annals of Statistics, Volume 48, Number 4, 1875-1897,\n  2020, Rejoinder: Annals of Statistics, Volume 48, Number 4, 1916-1921, 2020", "doi": "10.1214/19-AOS1875", "report-no": null, "categories": "math.ST cs.LG stat.ML stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Consider the multivariate nonparametric regression model. It is shown that\nestimators based on sparsely connected deep neural networks with ReLU\nactivation function and properly chosen network architecture achieve the\nminimax rates of convergence (up to $\\log n$-factors) under a general\ncomposition assumption on the regression function. The framework includes many\nwell-studied structural constraints such as (generalized) additive models.\nWhile there is a lot of flexibility in the network architecture, the tuning\nparameter is the sparsity of the network. Specifically, we consider large\nnetworks with number of potential network parameters exceeding the sample size.\nThe analysis gives some insights into why multilayer feedforward neural\nnetworks perform well in practice. Interestingly, for ReLU activation function\nthe depth (number of layers) of the neural network architectures plays an\nimportant role and our theory suggests that for nonparametric regression,\nscaling the network depth with the sample size is natural. It is also shown\nthat under the composition assumption wavelet estimators can only achieve\nsuboptimal rates.\n", "versions": [{"version": "v1", "created": "Tue, 22 Aug 2017 14:25:55 GMT"}, {"version": "v2", "created": "Sat, 23 Sep 2017 08:08:07 GMT"}, {"version": "v3", "created": "Sun, 13 May 2018 14:57:26 GMT"}, {"version": "v4", "created": "Sat, 16 Mar 2019 01:45:13 GMT"}, {"version": "v5", "created": "Sun, 13 Sep 2020 10:18:51 GMT"}], "update_date": "2020-09-15", "authors_parsed": [["Schmidt-Hieber", "Johannes", ""]]}, {"id": "1708.06656", "submitter": "Zheyan Shen", "authors": "Zheyan Shen, Peng Cui, Kun Kuang, Bo Li, Peixuan Chen", "title": "Causally Regularized Learning with Agnostic Data Selection Bias", "comments": "Oral paper of 2018 ACM Multimedia Conference (MM'18)", "journal-ref": null, "doi": "10.1145/3240508.3240577", "report-no": null, "categories": "cs.CV cs.MM stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Most of previous machine learning algorithms are proposed based on the i.i.d.\nhypothesis. However, this ideal assumption is often violated in real\napplications, where selection bias may arise between training and testing\nprocess. Moreover, in many scenarios, the testing data is not even available\nduring the training process, which makes the traditional methods like transfer\nlearning infeasible due to their need on prior of test distribution. Therefore,\nhow to address the agnostic selection bias for robust model learning is of\nparamount importance for both academic research and real applications. In this\npaper, under the assumption that causal relationships among variables are\nrobust across domains, we incorporate causal technique into predictive modeling\nand propose a novel Causally Regularized Logistic Regression (CRLR) algorithm\nby jointly optimize global confounder balancing and weighted logistic\nregression. Global confounder balancing helps to identify causal features,\nwhose causal effect on outcome are stable across domains, then performing\nlogistic regression on those causal features constructs a robust predictive\nmodel against the agnostic bias. To validate the effectiveness of our CRLR\nalgorithm, we conduct comprehensive experiments on both synthetic and real\nworld datasets. Experimental results clearly demonstrate that our CRLR\nalgorithm outperforms the state-of-the-art methods, and the interpretability of\nour method can be fully depicted by the feature visualization.\n", "versions": [{"version": "v1", "created": "Tue, 22 Aug 2017 14:49:07 GMT"}, {"version": "v2", "created": "Sun, 19 Aug 2018 16:33:36 GMT"}], "update_date": "2018-08-24", "authors_parsed": [["Shen", "Zheyan", ""], ["Cui", "Peng", ""], ["Kuang", "Kun", ""], ["Li", "Bo", ""], ["Chen", "Peixuan", ""]]}, {"id": "1708.06678", "submitter": "Stratis Ioannidis", "authors": "Stratis Ioannidis and Andrea Montanari", "title": "Learning Combinations of Sigmoids Through Gradient Estimation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We develop a new approach to learn the parameters of regression models with\nhidden variables. In a nutshell, we estimate the gradient of the regression\nfunction at a set of random points, and cluster the estimated gradients. The\ncenters of the clusters are used as estimates for the parameters of hidden\nunits. We justify this approach by studying a toy model, whereby the regression\nfunction is a linear combination of sigmoids. We prove that indeed the\nestimated gradients concentrate around the parameter vectors of the hidden\nunits, and provide non-asymptotic bounds on the number of required samples. To\nthe best of our knowledge, no comparable guarantees have been proven for linear\ncombinations of sigmoids.\n", "versions": [{"version": "v1", "created": "Tue, 22 Aug 2017 15:39:18 GMT"}, {"version": "v2", "created": "Wed, 17 Jan 2018 15:54:27 GMT"}], "update_date": "2018-01-18", "authors_parsed": [["Ioannidis", "Stratis", ""], ["Montanari", "Andrea", ""]]}, {"id": "1708.06714", "submitter": "Sathya N. Ravi", "authors": "Sathya N. Ravi, Maxwell D. Collins, Vikas Singh", "title": "A Deterministic Nonsmooth Frank Wolfe Algorithm with Coreset Guarantees", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.OC cs.NA stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a new Frank-Wolfe (FW) type algorithm that is applicable to\nminimization problems with a nonsmooth convex objective. We provide convergence\nbounds and show that the scheme yields so-called coreset results for various\nMachine Learning problems including 1-median, Balanced Development, Sparse PCA,\nGraph Cuts, and the $\\ell_1$-norm-regularized Support Vector Machine (SVM)\namong others. This means that the algorithm provides approximate solutions to\nthese problems in time complexity bounds that are not dependent on the size of\nthe input problem. Our framework, motivated by a growing body of work on\nsublinear algorithms for various data analysis problems, is entirely\ndeterministic and makes no use of smoothing or proximal operators. Apart from\nthese theoretical results, we show experimentally that the algorithm is very\npractical and in some cases also offers significant computational advantages on\nlarge problem instances. We provide an open source implementation that can be\nadapted for other problems that fit the overall structure.\n", "versions": [{"version": "v1", "created": "Tue, 22 Aug 2017 16:43:33 GMT"}], "update_date": "2017-08-23", "authors_parsed": [["Ravi", "Sathya N.", ""], ["Collins", "Maxwell D.", ""], ["Singh", "Vikas", ""]]}, {"id": "1708.06724", "submitter": "Chao Shang", "authors": "Chao Shang, Aaron Palmer, Jiangwen Sun, Ko-Shin Chen, Jin Lu, Jinbo Bi", "title": "VIGAN: Missing View Imputation with Generative Adversarial Networks", "comments": "10 pages, 8 figures, conference", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In an era when big data are becoming the norm, there is less concern with the\nquantity but more with the quality and completeness of the data. In many\ndisciplines, data are collected from heterogeneous sources, resulting in\nmulti-view or multi-modal datasets. The missing data problem has been\nchallenging to address in multi-view data analysis. Especially, when certain\nsamples miss an entire view of data, it creates the missing view problem.\nClassic multiple imputations or matrix completion methods are hardly effective\nhere when no information can be based on in the specific view to impute data\nfor such samples. The commonly-used simple method of removing samples with a\nmissing view can dramatically reduce sample size, thus diminishing the\nstatistical power of a subsequent analysis. In this paper, we propose a novel\napproach for view imputation via generative adversarial networks (GANs), which\nwe name by VIGAN. This approach first treats each view as a separate domain and\nidentifies domain-to-domain mappings via a GAN using randomly-sampled data from\neach view, and then employs a multi-modal denoising autoencoder (DAE) to\nreconstruct the missing view from the GAN outputs based on paired data across\nthe views. Then, by optimizing the GAN and DAE jointly, our model enables the\nknowledge integration for domain mappings and view correspondences to\neffectively recover the missing view. Empirical results on benchmark datasets\nvalidate the VIGAN approach by comparing against the state of the art. The\nevaluation of VIGAN in a genetic study of substance use disorders further\nproves the effectiveness and usability of this approach in life science.\n", "versions": [{"version": "v1", "created": "Tue, 22 Aug 2017 17:05:38 GMT"}, {"version": "v2", "created": "Wed, 23 Aug 2017 20:14:32 GMT"}, {"version": "v3", "created": "Mon, 4 Sep 2017 21:18:52 GMT"}, {"version": "v4", "created": "Wed, 11 Oct 2017 16:21:13 GMT"}, {"version": "v5", "created": "Wed, 1 Nov 2017 15:43:36 GMT"}], "update_date": "2017-11-02", "authors_parsed": [["Shang", "Chao", ""], ["Palmer", "Aaron", ""], ["Sun", "Jiangwen", ""], ["Chen", "Ko-Shin", ""], ["Lu", "Jin", ""], ["Bi", "Jinbo", ""]]}, {"id": "1708.06742", "submitter": "Dmitriy Serdyuk", "authors": "Dmitriy Serdyuk, Nan Rosemary Ke, Alessandro Sordoni, Adam Trischler,\n  Chris Pal, Yoshua Bengio", "title": "Twin Networks: Matching the Future for Sequence Generation", "comments": "12 pages, 3 figures, published at ICLR 2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a simple technique for encouraging generative RNNs to plan ahead.\nWe train a \"backward\" recurrent network to generate a given sequence in reverse\norder, and we encourage states of the forward model to predict cotemporal\nstates of the backward model. The backward network is used only during\ntraining, and plays no role during sampling or inference. We hypothesize that\nour approach eases modeling of long-term dependencies by implicitly forcing the\nforward states to hold information about the longer-term future (as contained\nin the backward states). We show empirically that our approach achieves 9%\nrelative improvement for a speech recognition task, and achieves significant\nimprovement on a COCO caption generation task.\n", "versions": [{"version": "v1", "created": "Tue, 22 Aug 2017 17:44:56 GMT"}, {"version": "v2", "created": "Thu, 11 Jan 2018 22:09:36 GMT"}, {"version": "v3", "created": "Fri, 23 Feb 2018 19:54:36 GMT"}], "update_date": "2018-02-27", "authors_parsed": [["Serdyuk", "Dmitriy", ""], ["Ke", "Nan Rosemary", ""], ["Sordoni", "Alessandro", ""], ["Trischler", "Adam", ""], ["Pal", "Chris", ""], ["Bengio", "Yoshua", ""]]}, {"id": "1708.06819", "submitter": "Nathan Hilliard", "authors": "Nathan Hilliard, Nathan O. Hodas, Courtney D. Corley", "title": "Dynamic Input Structure and Network Assembly for Few-Shot Learning", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The ability to learn from a small number of examples has been a difficult\nproblem in machine learning since its inception. While methods have succeeded\nwith large amounts of training data, research has been underway in how to\naccomplish similar performance with fewer examples, known as one-shot or more\ngenerally few-shot learning. This technique has been shown to have promising\nperformance, but in practice requires fixed-size inputs making it impractical\nfor production systems where class sizes can vary. This impedes training and\nthe final utility of few-shot learning systems. This paper describes an\napproach to constructing and training a network that can handle arbitrary\nexample sizes dynamically as the system is used.\n", "versions": [{"version": "v1", "created": "Tue, 22 Aug 2017 21:00:50 GMT"}], "update_date": "2017-08-24", "authors_parsed": [["Hilliard", "Nathan", ""], ["Hodas", "Nathan O.", ""], ["Corley", "Courtney D.", ""]]}, {"id": "1708.06899", "submitter": "Johanna \\\"Arje", "authors": "Johanna \\\"Arje, Jenni Raitoharju, Alexandros Iosifidis, Ville\n  Tirronen, Kristian Meissner, Moncef Gabbouj, Serkan Kiranyaz, Salme\n  K\\\"arkk\\\"ainen", "title": "Human experts vs. machines in taxa recognition", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG q-bio.QM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The step of expert taxa recognition currently slows down the response time of\nmany bioassessments. Shifting to quicker and cheaper state-of-the-art machine\nlearning approaches is still met with expert scepticism towards the ability and\nlogic of machines. In our study, we investigate both the differences in\naccuracy and in the identification logic of taxonomic experts and machines. We\npropose a systematic approach utilizing deep Convolutional Neural Nets with the\ntransfer learning paradigm and extensively evaluate it over a multi-pose\ntaxonomic dataset with hierarchical labels specifically created for this\ncomparison. We also study the prediction accuracy on different ranks of\ntaxonomic hierarchy in detail. Our results revealed that human experts using\nactual specimens yield the lowest classification error ($\\overline{CE}=6.1\\%$).\nHowever, a much faster, automated approach using deep Convolutional Neural Nets\ncomes close to human accuracy ($\\overline{CE}=11.4\\%$). Contrary to previous\nfindings in the literature, we find that for machines following a typical flat\nclassification approach commonly used in machine learning performs better than\nforcing machines to adopt a hierarchical, local per parent node approach used\nby human taxonomic experts. Finally, we publicly share our unique dataset to\nserve as a public benchmark dataset in this field.\n", "versions": [{"version": "v1", "created": "Wed, 23 Aug 2017 06:52:33 GMT"}, {"version": "v2", "created": "Thu, 21 Dec 2017 10:22:16 GMT"}, {"version": "v3", "created": "Fri, 11 Jan 2019 11:42:52 GMT"}, {"version": "v4", "created": "Fri, 17 May 2019 08:26:17 GMT"}], "update_date": "2019-05-20", "authors_parsed": [["\u00c4rje", "Johanna", ""], ["Raitoharju", "Jenni", ""], ["Iosifidis", "Alexandros", ""], ["Tirronen", "Ville", ""], ["Meissner", "Kristian", ""], ["Gabbouj", "Moncef", ""], ["Kiranyaz", "Serkan", ""], ["K\u00e4rkk\u00e4inen", "Salme", ""]]}, {"id": "1708.06935", "submitter": "Laura Azzimonti", "authors": "L. Azzimonti, G. Corani, M. Zaffalon", "title": "Hierarchical Multinomial-Dirichlet model for the estimation of\n  conditional probability tables", "comments": null, "journal-ref": null, "doi": "10.1016/j.csda.2019.02.004", "report-no": null, "categories": "stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a novel approach for estimating conditional probability tables,\nbased on a joint, rather than independent, estimate of the conditional\ndistributions belonging to the same table. We derive exact analytical\nexpressions for the estimators and we analyse their properties both\nanalytically and via simulation. We then apply this method to the estimation of\nparameters in a Bayesian network. Given the structure of the network, the\nproposed approach better estimates the joint distribution and significantly\nimproves the classification performance with respect to traditional approaches.\n", "versions": [{"version": "v1", "created": "Wed, 23 Aug 2017 09:45:55 GMT"}], "update_date": "2020-08-05", "authors_parsed": [["Azzimonti", "L.", ""], ["Corani", "G.", ""], ["Zaffalon", "M.", ""]]}, {"id": "1708.06939", "submitter": "Battista Biggio", "authors": "Marco Melis, Ambra Demontis, Battista Biggio, Gavin Brown, Giorgio\n  Fumera and Fabio Roli", "title": "Is Deep Learning Safe for Robot Vision? Adversarial Examples against the\n  iCub Humanoid", "comments": "Accepted for publication at the ICCV 2017 Workshop on Vision in\n  Practice on Autonomous Robots (ViPAR)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.RO stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Deep neural networks have been widely adopted in recent years, exhibiting\nimpressive performances in several application domains. It has however been\nshown that they can be fooled by adversarial examples, i.e., images altered by\na barely-perceivable adversarial noise, carefully crafted to mislead\nclassification. In this work, we aim to evaluate the extent to which\nrobot-vision systems embodying deep-learning algorithms are vulnerable to\nadversarial examples, and propose a computationally efficient countermeasure to\nmitigate this threat, based on rejecting classification of anomalous inputs. We\nthen provide a clearer understanding of the safety properties of deep networks\nthrough an intuitive empirical analysis, showing that the mapping learned by\nsuch networks essentially violates the smoothness assumption of learning\nalgorithms. We finally discuss the main limitations of this work, including the\ncreation of real-world adversarial examples, and sketch promising research\ndirections.\n", "versions": [{"version": "v1", "created": "Wed, 23 Aug 2017 10:01:35 GMT"}], "update_date": "2017-08-24", "authors_parsed": [["Melis", "Marco", ""], ["Demontis", "Ambra", ""], ["Biggio", "Battista", ""], ["Brown", "Gavin", ""], ["Fumera", "Giorgio", ""], ["Roli", "Fabio", ""]]}, {"id": "1708.07012", "submitter": "Paolo Inglese", "authors": "Paolo Inglese, James L. Alexander, Anna Mroz, Zoltan Takats, Robert\n  Glen", "title": "Variational autoencoders for tissue heterogeneity exploration from\n  (almost) no preprocessed mass spectrometry imaging data", "comments": "mass spectrometry imaging, variational autoencoder, desorption\n  electrospray ionization, desi", "journal-ref": null, "doi": null, "report-no": null, "categories": "q-bio.QM cs.LG stat.ML", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  The paper presents the application of Variational Autoencoders (VAE) for data\ndimensionality reduction and explorative analysis of mass spectrometry imaging\ndata (MSI). The results confirm that VAEs are capable of detecting the patterns\nassociated with the different tissue sub-types with performance than standard\napproaches.\n", "versions": [{"version": "v1", "created": "Wed, 23 Aug 2017 14:12:53 GMT"}, {"version": "v2", "created": "Thu, 24 Aug 2017 13:59:31 GMT"}], "update_date": "2017-08-25", "authors_parsed": [["Inglese", "Paolo", ""], ["Alexander", "James L.", ""], ["Mroz", "Anna", ""], ["Takats", "Zoltan", ""], ["Glen", "Robert", ""]]}, {"id": "1708.07025", "submitter": "Philip Graff", "authors": "Cetin Savkli, J. Ryan Carr, Philip Graff, Lauren Kennell", "title": "Bayesian Learning of Clique Tree Structure", "comments": "7 pages, 11 figures; see\n  http://worldcomp-proceedings.com/proc/p2016/DMIN16_Contents.html", "journal-ref": "Proceedings of the International Conference on Data Mining (DMIN).\n  The Steering Committee of The World Congress in Computer Science, Computer\n  Engineering and Applied Computing (WorldComp). p 201, 2016", "doi": null, "report-no": null, "categories": "cs.LG math.PR stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The problem of categorical data analysis in high dimensions is considered. A\ndiscussion of the fundamental difficulties of probability modeling is provided,\nand a solution to the derivation of high dimensional probability distributions\nbased on Bayesian learning of clique tree decomposition is presented. The main\ncontributions of this paper are an automated determination of the optimal\nclique tree structure for probability modeling, the resulting derived\nprobability distribution, and a corresponding unified approach to clustering\nand anomaly detection based on the probability distribution.\n", "versions": [{"version": "v1", "created": "Wed, 23 Aug 2017 14:40:19 GMT"}], "update_date": "2017-08-24", "authors_parsed": [["Savkli", "Cetin", ""], ["Carr", "J. Ryan", ""], ["Graff", "Philip", ""], ["Kennell", "Lauren", ""]]}, {"id": "1708.07042", "submitter": "Wojciech Kot{\\l}owski", "authors": "Wojciech Kot{\\l}owski", "title": "Scale-invariant unconstrained online learning", "comments": "To appear in Proc. of the 28th International Conference on\n  Algorithmic Learning Theory (ALT) 2017", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider a variant of online convex optimization in which both the\ninstances (input vectors) and the comparator (weight vector) are unconstrained.\nWe exploit a natural scale invariance symmetry in our unconstrained setting:\nthe predictions of the optimal comparator are invariant under any linear\ntransformation of the instances. Our goal is to design online algorithms which\nalso enjoy this property, i.e. are scale-invariant. We start with the case of\ncoordinate-wise invariance, in which the individual coordinates (features) can\nbe arbitrarily rescaled. We give an algorithm, which achieves essentially\noptimal regret bound in this setup, expressed by means of a coordinate-wise\nscale-invariant norm of the comparator. We then study general invariance with\nrespect to arbitrary linear transformations. We first give a negative result,\nshowing that no algorithm can achieve a meaningful bound in terms of\nscale-invariant norm of the comparator in the worst case. Next, we compliment\nthis result with a positive one, providing an algorithm which \"almost\" achieves\nthe desired bound, incurring only a logarithmic overhead in terms of the norm\nof the instances.\n", "versions": [{"version": "v1", "created": "Wed, 23 Aug 2017 15:15:23 GMT"}], "update_date": "2017-08-24", "authors_parsed": [["Kot\u0142owski", "Wojciech", ""]]}, {"id": "1708.07120", "submitter": "Leslie Smith", "authors": "Leslie N. Smith and Nicholay Topin", "title": "Super-Convergence: Very Fast Training of Neural Networks Using Large\n  Learning Rates", "comments": "This paper was significantly revised to show super-convergence as a\n  general fast training methodology", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV cs.NE stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we describe a phenomenon, which we named \"super-convergence\",\nwhere neural networks can be trained an order of magnitude faster than with\nstandard training methods. The existence of super-convergence is relevant to\nunderstanding why deep networks generalize well. One of the key elements of\nsuper-convergence is training with one learning rate cycle and a large maximum\nlearning rate. A primary insight that allows super-convergence training is that\nlarge learning rates regularize the training, hence requiring a reduction of\nall other forms of regularization in order to preserve an optimal\nregularization balance. We also derive a simplification of the Hessian Free\noptimization method to compute an estimate of the optimal learning rate.\nExperiments demonstrate super-convergence for Cifar-10/100, MNIST and Imagenet\ndatasets, and resnet, wide-resnet, densenet, and inception architectures. In\naddition, we show that super-convergence provides a greater boost in\nperformance relative to standard training when the amount of labeled training\ndata is limited. The architectures and code to replicate the figures in this\npaper are available at github.com/lnsmith54/super-convergence. See\nhttp://www.fast.ai/2018/04/30/dawnbench-fastai/ for an application of\nsuper-convergence to win the DAWNBench challenge (see\nhttps://dawn.cs.stanford.edu/benchmark/).\n", "versions": [{"version": "v1", "created": "Wed, 23 Aug 2017 17:51:57 GMT"}, {"version": "v2", "created": "Wed, 13 Dec 2017 18:24:34 GMT"}, {"version": "v3", "created": "Thu, 17 May 2018 17:40:34 GMT"}], "update_date": "2018-05-18", "authors_parsed": [["Smith", "Leslie N.", ""], ["Topin", "Nicholay", ""]]}, {"id": "1708.07147", "submitter": "Ashley Prater", "authors": "Ashley Prater", "title": "Classification via Tensor Decompositions of Echo State Networks", "comments": "8 pages, 6 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.NE stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This work introduces a tensor-based method to perform supervised\nclassification on spatiotemporal data processed in an echo state network.\nTypically when performing supervised classification tasks on data processed in\nan echo state network, the entire collection of hidden layer node states from\nthe training dataset is shaped into a matrix, allowing one to use standard\nlinear algebra techniques to train the output layer. However, the collection of\nhidden layer states is multidimensional in nature, and representing it as a\nmatrix may lead to undesirable numerical conditions or loss of spatial and\ntemporal correlations in the data.\n  This work proposes a tensor-based supervised classification method on echo\nstate network data that preserves and exploits the multidimensional nature of\nthe hidden layer states. The method, which is based on orthogonal Tucker\ndecompositions of tensors, is compared with the standard linear output weight\napproach in several numerical experiments on both synthetic and natural data.\nThe results show that the tensor-based approach tends to outperform the\nstandard approach in terms of classification accuracy.\n", "versions": [{"version": "v1", "created": "Wed, 23 Aug 2017 18:51:08 GMT"}], "update_date": "2017-08-25", "authors_parsed": [["Prater", "Ashley", ""]]}, {"id": "1708.07164", "submitter": "Peng Xu", "authors": "Peng Xu, Fred Roosta, Michael W. Mahoney", "title": "Newton-Type Methods for Non-Convex Optimization Under Inexact Hessian\n  Information", "comments": "32 pages", "journal-ref": "Mathematical Programming 2019", "doi": null, "report-no": null, "categories": "math.OC cs.CC cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider variants of trust-region and cubic regularization methods for\nnon-convex optimization, in which the Hessian matrix is approximated. Under\nmild conditions on the inexact Hessian, and using approximate solution of the\ncorresponding sub-problems, we provide iteration complexity to achieve $\n\\epsilon $-approximate second-order optimality which have shown to be tight.\nOur Hessian approximation conditions constitute a major relaxation over the\nexisting ones in the literature. Consequently, we are able to show that such\nmild conditions allow for the construction of the approximate Hessian through\nvarious random sampling methods. In this light, we consider the canonical\nproblem of finite-sum minimization, provide appropriate uniform and non-uniform\nsub-sampling strategies to construct such Hessian approximations, and obtain\noptimal iteration complexity for the corresponding sub-sampled trust-region and\ncubic regularization methods.\n", "versions": [{"version": "v1", "created": "Wed, 23 Aug 2017 19:40:55 GMT"}, {"version": "v2", "created": "Tue, 29 Aug 2017 01:57:45 GMT"}, {"version": "v3", "created": "Thu, 15 Feb 2018 23:57:07 GMT"}, {"version": "v4", "created": "Tue, 14 May 2019 09:13:47 GMT"}], "update_date": "2019-05-15", "authors_parsed": [["Xu", "Peng", ""], ["Roosta", "Fred", ""], ["Mahoney", "Michael W.", ""]]}, {"id": "1708.07178", "submitter": "Giorgos Borboudakis", "authors": "Ioannis Tsamardinos and Giorgos Borboudakis and Pavlos Katsogridakis\n  and Polyvios Pratikakis and Vassilis Christophides", "title": "Massively-Parallel Feature Selection for Big Data", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present the Parallel, Forward-Backward with Pruning (PFBP) algorithm for\nfeature selection (FS) in Big Data settings (high dimensionality and/or sample\nsize). To tackle the challenges of Big Data FS PFBP partitions the data matrix\nboth in terms of rows (samples, training examples) as well as columns\n(features). By employing the concepts of $p$-values of conditional independence\ntests and meta-analysis techniques PFBP manages to rely only on computations\nlocal to a partition while minimizing communication costs. Then, it employs\npowerful and safe (asymptotically sound) heuristics to make early, approximate\ndecisions, such as Early Dropping of features from consideration in subsequent\niterations, Early Stopping of consideration of features within the same\niteration, or Early Return of the winner in each iteration. PFBP provides\nasymptotic guarantees of optimality for data distributions faithfully\nrepresentable by a causal network (Bayesian network or maximal ancestral\ngraph). Our empirical analysis confirms a super-linear speedup of the algorithm\nwith increasing sample size, linear scalability with respect to the number of\nfeatures and processing cores, while dominating other competitive algorithms in\nits class.\n", "versions": [{"version": "v1", "created": "Wed, 23 Aug 2017 20:23:36 GMT"}], "update_date": "2017-08-25", "authors_parsed": [["Tsamardinos", "Ioannis", ""], ["Borboudakis", "Giorgos", ""], ["Katsogridakis", "Pavlos", ""], ["Pratikakis", "Polyvios", ""], ["Christophides", "Vassilis", ""]]}, {"id": "1708.07193", "submitter": "Nikola Markovic", "authors": "Nikola Markovi\\'c, Przemys{\\l}aw Seku{\\l}a, Zachary Vander Laan,\n  Gennady Andrienko, Natalia Andrienko", "title": "Applications of Trajectory Data from the Perspective of a Road\n  Transportation Agency: Literature Review and Maryland Case Study", "comments": "Revised after peer review", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.CY", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Transportation agencies have an opportunity to leverage\nincreasingly-available trajectory datasets to improve their analyses and\ndecision-making processes. However, this data is typically purchased from\nvendors, which means agencies must understand its potential benefits beforehand\nin order to properly assess its value relative to the cost of acquisition.\nWhile the literature concerned with trajectory data is rich, it is naturally\nfragmented and focused on technical contributions in niche areas, which makes\nit difficult for government agencies to assess its value across different\ntransportation domains. To overcome this issue, the current paper explores\ntrajectory data from the perspective of a road transportation agency interested\nin acquiring trajectories to enhance its analyses. The paper provides a\nliterature review illustrating applications of trajectory data in six areas of\nroad transportation systems analysis: demand estimation, modeling human\nbehavior, designing public transit, traffic performance measurement and\nprediction, environment and safety. In addition, it visually explores 20\nmillion GPS traces in Maryland, illustrating existing and suggesting new\napplications of trajectory data.\n", "versions": [{"version": "v1", "created": "Wed, 23 Aug 2017 21:30:05 GMT"}, {"version": "v2", "created": "Wed, 30 May 2018 19:19:18 GMT"}], "update_date": "2018-06-01", "authors_parsed": [["Markovi\u0107", "Nikola", ""], ["Seku\u0142a", "Przemys\u0142aw", ""], ["Laan", "Zachary Vander", ""], ["Andrienko", "Gennady", ""], ["Andrienko", "Natalia", ""]]}, {"id": "1708.07242", "submitter": "Philip Graff", "authors": "Cetin Savkli, Jeffrey Lin, Philip Graff, Matthew Kinsey", "title": "GALILEO: A Generalized Low-Entropy Mixture Model", "comments": "7 pages, 8 figures, 3 tables", "journal-ref": "Proceedings of the International Conference on Data Mining (DMIN\n  17). The Steering Committee of The World Congress in Computer Science,\n  Computer Engineering and Applied Computing (WorldComp). 2017", "doi": null, "report-no": null, "categories": "stat.ML cs.DS cs.LG math.PR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a new method of generating mixture models for data with\ncategorical attributes. The keys to this approach are an entropy-based density\nmetric in categorical space and annealing of high-entropy/low-density\ncomponents from an initial state with many components. Pruning of low-density\ncomponents using the entropy-based density allows GALILEO to consistently find\nhigh-quality clusters and the same optimal number of clusters. GALILEO has\nshown promising results on a range of test datasets commonly used for\ncategorical clustering benchmarks. We demonstrate that the scaling of GALILEO\nis linear in the number of records in the dataset, making this method suitable\nfor very large categorical datasets.\n", "versions": [{"version": "v1", "created": "Thu, 24 Aug 2017 01:27:34 GMT"}], "update_date": "2017-08-25", "authors_parsed": [["Savkli", "Cetin", ""], ["Lin", "Jeffrey", ""], ["Graff", "Philip", ""], ["Kinsey", "Matthew", ""]]}, {"id": "1708.07244", "submitter": "Senjian An Dr.", "authors": "Senjian An, Mohammed Bennamoun and Farid Boussaid", "title": "On the Compressive Power of Deep Rectifier Networks for High Resolution\n  Representation of Class Boundaries", "comments": "18 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper provides a theoretical justification of the superior\nclassification performance of deep rectifier networks over shallow rectifier\nnetworks from the geometrical perspective of piecewise linear (PWL) classifier\nboundaries. We show that, for a given threshold on the approximation error, the\nrequired number of boundary facets to approximate a general smooth boundary\ngrows exponentially with the dimension of the data, and thus the number of\nboundary facets, referred to as boundary resolution, of a PWL classifier is an\nimportant quality measure that can be used to estimate a lower bound on the\nclassification errors. However, learning naively an exponentially large number\nof boundary facets requires the determination of an exponentially large number\nof parameters and also requires an exponentially large number of training\npatterns. To overcome this issue of \"curse of dimensionality\", compressive\nrepresentations of high resolution classifier boundaries are required. To show\nthe superior compressive power of deep rectifier networks over shallow\nrectifier networks, we prove that the maximum boundary resolution of a single\nhidden layer rectifier network classifier grows exponentially with the number\nof units when this number is smaller than the dimension of the patterns. When\nthe number of units is larger than the dimension of the patterns, the growth\nrate is reduced to a polynomial order. Consequently, the capacity of generating\na high resolution boundary will increase if the same large number of units are\narranged in multiple layers instead of a single hidden layer. Taking high\ndimensional spherical boundaries as examples, we show how deep rectifier\nnetworks can utilize geometric symmetries to approximate a boundary with the\nsame accuracy but with a significantly fewer number of parameters than single\nhidden layer nets.\n", "versions": [{"version": "v1", "created": "Thu, 24 Aug 2017 01:37:36 GMT"}], "update_date": "2017-08-25", "authors_parsed": [["An", "Senjian", ""], ["Bennamoun", "Mohammed", ""], ["Boussaid", "Farid", ""]]}, {"id": "1708.07259", "submitter": "Will Wei Sun", "authors": "Will Wei Sun and Lexin Li", "title": "Dynamic Tensor Clustering", "comments": "Accepted at Journal of the American Statistical Association", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Dynamic tensor data are becoming prevalent in numerous applications. Existing\ntensor clustering methods either fail to account for the dynamic nature of the\ndata, or are inapplicable to a general-order tensor. Also there is often a gap\nbetween statistical guarantee and computational efficiency for existing tensor\nclustering solutions. In this article, we aim to bridge this gap by proposing a\nnew dynamic tensor clustering method, which takes into account both sparsity\nand fusion structures, and enjoys strong statistical guarantees as well as high\ncomputational efficiency. Our proposal is based upon a new structured tensor\nfactorization that encourages both sparsity and smoothness in parameters along\nthe specified tensor modes. Computationally, we develop a highly efficient\noptimization algorithm that benefits from substantial dimension reduction. In\ntheory, we first establish a non-asymptotic error bound for the estimator from\nthe structured tensor factorization. Built upon this error bound, we then\nderive the rate of convergence of the estimated cluster centers, and show that\nthe estimated clusters recover the true cluster structures with a high\nprobability. Moreover, our proposed method can be naturally extended to\nco-clustering of multiple modes of the tensor data. The efficacy of our\napproach is illustrated via simulations and a brain dynamic functional\nconnectivity analysis from an Autism spectrum disorder study.\n", "versions": [{"version": "v1", "created": "Thu, 24 Aug 2017 02:50:15 GMT"}, {"version": "v2", "created": "Fri, 14 Sep 2018 15:41:54 GMT"}], "update_date": "2018-09-17", "authors_parsed": [["Sun", "Will Wei", ""], ["Li", "Lexin", ""]]}, {"id": "1708.07308", "submitter": "Ce Zhang", "authors": "Tian Li, Jie Zhong, Ji Liu, Wentao Wu, Ce Zhang", "title": "Ease.ml: Towards Multi-tenant Resource Sharing for Machine Learning\n  Workloads", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present ease.ml, a declarative machine learning service platform we built\nto support more than ten research groups outside the computer science\ndepartments at ETH Zurich for their machine learning needs. With ease.ml, a\nuser defines the high-level schema of a machine learning application and\nsubmits the task via a Web interface. The system automatically deals with the\nrest, such as model selection and data movement. In this paper, we describe the\nease.ml architecture and focus on a novel technical problem introduced by\nease.ml regarding resource allocation. We ask, as a \"service provider\" that\nmanages a shared cluster of machines among all our users running machine\nlearning workloads, what is the resource allocation strategy that maximizes the\nglobal satisfaction of all our users?\n  Resource allocation is a critical yet subtle issue in this multi-tenant\nscenario, as we have to balance between efficiency and fairness. We first\nformalize the problem that we call multi-tenant model selection, aiming for\nminimizing the total regret of all users running automatic model selection\ntasks. We then develop a novel algorithm that combines multi-armed bandits with\nBayesian optimization and prove a regret bound under the multi-tenant setting.\nFinally, we report our evaluation of ease.ml on synthetic data and on one\nservice we are providing to our users, namely, image classification with deep\nneural networks. Our experimental evaluation results show that our proposed\nsolution can be up to 9.8x faster in achieving the same global quality for all\nusers as the two popular heuristics used by our users before ease.ml.\n", "versions": [{"version": "v1", "created": "Thu, 24 Aug 2017 08:21:28 GMT"}], "update_date": "2017-08-25", "authors_parsed": [["Li", "Tian", ""], ["Zhong", "Jie", ""], ["Liu", "Ji", ""], ["Wu", "Wentao", ""], ["Zhang", "Ce", ""]]}, {"id": "1708.07367", "submitter": "Daniel Hsu", "authors": "Daniel Hsu, Aryeh Kontorovich, David A. Levin, Yuval Peres, Csaba\n  Szepesv\\'ari", "title": "Mixing time estimation in reversible Markov chains from a single sample\n  path", "comments": "34 pages, merges results of arXiv:1506.02903 and arXiv:1612.05330", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST cs.LG math.PR stat.ML stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The spectral gap $\\gamma$ of a finite, ergodic, and reversible Markov chain\nis an important parameter measuring the asymptotic rate of convergence. In\napplications, the transition matrix $P$ may be unknown, yet one sample of the\nchain up to a fixed time $n$ may be observed. We consider here the problem of\nestimating $\\gamma$ from this data. Let $\\pi$ be the stationary distribution of\n$P$, and $\\pi_\\star = \\min_x \\pi(x)$. We show that if $n =\n\\tilde{O}\\bigl(\\frac{1}{\\gamma \\pi_\\star}\\bigr)$, then $\\gamma$ can be\nestimated to within multiplicative constants with high probability. When $\\pi$\nis uniform on $d$ states, this matches (up to logarithmic correction) a lower\nbound of $\\tilde{\\Omega}\\bigl(\\frac{d}{\\gamma}\\bigr)$ steps required for\nprecise estimation of $\\gamma$. Moreover, we provide the first procedure for\ncomputing a fully data-dependent interval, from a single finite-length\ntrajectory of the chain, that traps the mixing time $t_{\\text{mix}}$ of the\nchain at a prescribed confidence level. The interval does not require the\nknowledge of any parameters of the chain. This stands in contrast to previous\napproaches, which either only provide point estimates, or require a reset\nmechanism, or additional prior knowledge. The interval is constructed around\nthe relaxation time $t_{\\text{relax}} = 1/\\gamma$, which is strongly related to\nthe mixing time, and the width of the interval converges to zero roughly at a\n$1/\\sqrt{n}$ rate, where $n$ is the length of the sample path.\n", "versions": [{"version": "v1", "created": "Thu, 24 Aug 2017 12:05:11 GMT"}], "update_date": "2017-08-25", "authors_parsed": [["Hsu", "Daniel", ""], ["Kontorovich", "Aryeh", ""], ["Levin", "David A.", ""], ["Peres", "Yuval", ""], ["Szepesv\u00e1ri", "Csaba", ""]]}, {"id": "1708.07450", "submitter": "Kaihui Liu", "authors": "Zhou Zhou, Kaihui Liu, and Jun Fang", "title": "Bayesian Compressive Sensing Using Normal Product Priors", "comments": null, "journal-ref": "IEEE SIGNAL PROCESSING LETTERS, VOL. 22, NO. 5, MAY 2015", "doi": null, "report-no": null, "categories": "stat.ML cs.IT cs.LG math.IT", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we introduce a new sparsity-promoting prior, namely, the\n\"normal product\" prior, and develop an efficient algorithm for sparse signal\nrecovery under the Bayesian framework. The normal product distribution is the\ndistribution of a product of two normally distributed variables with zero means\nand possibly different variances. Like other sparsity-encouraging distributions\nsuch as the Student's $t$-distribution, the normal product distribution has a\nsharp peak at origin, which makes it a suitable prior to encourage sparse\nsolutions. A two-stage normal product-based hierarchical model is proposed. We\nresort to the variational Bayesian (VB) method to perform the inference.\nSimulations are conducted to illustrate the effectiveness of our proposed\nalgorithm as compared with other state-of-the-art compressed sensing\nalgorithms.\n", "versions": [{"version": "v1", "created": "Thu, 24 Aug 2017 15:15:22 GMT"}], "update_date": "2017-08-25", "authors_parsed": [["Zhou", "Zhou", ""], ["Liu", "Kaihui", ""], ["Fang", "Jun", ""]]}, {"id": "1708.07469", "submitter": "Konstantinos Spiliopoulos", "authors": "Justin Sirignano and Konstantinos Spiliopoulos", "title": "DGM: A deep learning algorithm for solving partial differential\n  equations", "comments": "Deep learning, machine learning, partial differential equations", "journal-ref": null, "doi": "10.1016/j.jcp.2018.08.029", "report-no": null, "categories": "q-fin.MF math.NA q-fin.CP stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  High-dimensional PDEs have been a longstanding computational challenge. We\npropose to solve high-dimensional PDEs by approximating the solution with a\ndeep neural network which is trained to satisfy the differential operator,\ninitial condition, and boundary conditions. Our algorithm is meshfree, which is\nkey since meshes become infeasible in higher dimensions. Instead of forming a\nmesh, the neural network is trained on batches of randomly sampled time and\nspace points. The algorithm is tested on a class of high-dimensional free\nboundary PDEs, which we are able to accurately solve in up to $200$ dimensions.\nThe algorithm is also tested on a high-dimensional Hamilton-Jacobi-Bellman PDE\nand Burgers' equation. The deep learning algorithm approximates the general\nsolution to the Burgers' equation for a continuum of different boundary\nconditions and physical conditions (which can be viewed as a high-dimensional\nspace). We call the algorithm a \"Deep Galerkin Method (DGM)\" since it is\nsimilar in spirit to Galerkin methods, with the solution approximated by a\nneural network instead of a linear combination of basis functions. In addition,\nwe prove a theorem regarding the approximation power of neural networks for a\nclass of quasilinear parabolic PDEs.\n", "versions": [{"version": "v1", "created": "Thu, 24 Aug 2017 15:50:24 GMT"}, {"version": "v2", "created": "Tue, 5 Dec 2017 13:56:45 GMT"}, {"version": "v3", "created": "Sat, 16 Dec 2017 19:19:32 GMT"}, {"version": "v4", "created": "Fri, 27 Jul 2018 18:16:30 GMT"}, {"version": "v5", "created": "Wed, 5 Sep 2018 19:39:17 GMT"}], "update_date": "2018-10-17", "authors_parsed": [["Sirignano", "Justin", ""], ["Spiliopoulos", "Konstantinos", ""]]}, {"id": "1708.07480", "submitter": "Spencer Frank", "authors": "John Semerdjian and Spencer Frank", "title": "An Ensemble Classifier for Predicting the Onset of Type II Diabetes", "comments": "8 pages, 5 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Prediction of disease onset from patient survey and lifestyle data is quickly\nbecoming an important tool for diagnosing a disease before it progresses. In\nthis study, data from the National Health and Nutrition Examination Survey\n(NHANES) questionnaire is used to predict the onset of type II diabetes. An\nensemble model using the output of five classification algorithms was developed\nto predict the onset on diabetes based on 16 features. The ensemble model had\nan AUC of 0.834 indicating high performance.\n", "versions": [{"version": "v1", "created": "Thu, 24 Aug 2017 16:20:48 GMT"}], "update_date": "2017-08-25", "authors_parsed": [["Semerdjian", "John", ""], ["Frank", "Spencer", ""]]}, {"id": "1708.07481", "submitter": "Andrew Knyazev", "authors": "David Zhuzhunashvili and Andrew Knyazev", "title": "Preconditioned Spectral Clustering for Stochastic Block Partition\n  Streaming Graph Challenge", "comments": "6 pages. To appear in Proceedings of the 2017 IEEE High Performance\n  Extreme Computing Conference. Student Innovation Award Streaming Graph\n  Challenge: Stochastic Block Partition, see\n  http://graphchallenge.mit.edu/champions", "journal-ref": "2017 IEEE High Performance Extreme Computing Conference (HPEC),\n  Waltham, MA, USA, 2017, pp. 1-6", "doi": "10.1109/HPEC.2017.8091045", "report-no": null, "categories": "cs.MS cs.DC cs.DS stat.CO stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Locally Optimal Block Preconditioned Conjugate Gradient (LOBPCG) is\ndemonstrated to efficiently solve eigenvalue problems for graph Laplacians that\nappear in spectral clustering. For static graph partitioning, 10-20 iterations\nof LOBPCG without preconditioning result in ~10x error reduction, enough to\nachieve 100% correctness for all Challenge datasets with known truth\npartitions, e.g., for graphs with 5K/.1M (50K/1M) Vertices/Edges in 2 (7)\nseconds, compared to over 5,000 (30,000) seconds needed by the baseline Python\ncode. Our Python code 100% correctly determines 98 (160) clusters from the\nChallenge static graphs with 0.5M (2M) vertices in 270 (1,700) seconds using\n10GB (50GB) of memory. Our single-precision MATLAB code calculates the same\nclusters at half time and memory. For streaming graph partitioning, LOBPCG is\ninitiated with approximate eigenvectors of the graph Laplacian already computed\nfor the previous graph, in many cases reducing 2-3 times the number of required\nLOBPCG iterations, compared to the static case. Our spectral clustering is\ngeneric, i.e. assuming nothing specific of the block model or streaming, used\nto generate the graphs for the Challenge, in contrast to the base code.\nNevertheless, in 10-stage streaming comparison with the base code for the 5K\ngraph, the quality of our clusters is similar or better starting at stage 4 (7)\nfor emerging edging (snowballing) streaming, while the computations are over\n100-1000 faster.\n", "versions": [{"version": "v1", "created": "Mon, 21 Aug 2017 14:09:05 GMT"}], "update_date": "2017-11-09", "authors_parsed": [["Zhuzhunashvili", "David", ""], ["Knyazev", "Andrew", ""]]}, {"id": "1708.07485", "submitter": "Angshuman Roy", "authors": "Angshuman Roy, Alok Goswami, C. A. Murthy", "title": "Multivariate Dependency Measure based on Copula and Gaussian Kernel", "comments": "This work is postponed", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.ML stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a new multivariate dependency measure. It is obtained by\nconsidering a Gaussian kernel based distance between the copula transform of\nthe given d-dimensional distribution and the uniform copula and then\nappropriately normalizing it. The resulting measure is shown to satisfy a\nnumber of desirable properties. A nonparametric estimate is proposed for this\ndependency measure and its properties (finite sample as well as asymptotic) are\nderived. Some comparative studies of the proposed dependency measure estimate\nwith some widely used dependency measure estimates on artificial datasets are\nincluded. A non-parametric test of independence between two or more random\nvariables based on this measure is proposed. A comparison of the proposed test\nwith some existing nonparametric multivariate test for independence is\npresented.\n", "versions": [{"version": "v1", "created": "Thu, 24 Aug 2017 16:43:01 GMT"}, {"version": "v2", "created": "Wed, 10 Jan 2018 07:59:58 GMT"}, {"version": "v3", "created": "Sun, 10 Nov 2019 15:11:56 GMT"}], "update_date": "2019-11-12", "authors_parsed": [["Roy", "Angshuman", ""], ["Goswami", "Alok", ""], ["Murthy", "C. A.", ""]]}, {"id": "1708.07581", "submitter": "Francois Petitjean Ph.D.", "authors": "Francois Petitjean, Wray Buntine, Geoffrey I. Webb and Nayyar Zaidi", "title": "Accurate parameter estimation for Bayesian Network Classifiers using\n  Hierarchical Dirichlet Processes", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper introduces a novel parameter estimation method for the probability\ntables of Bayesian network classifiers (BNCs), using hierarchical Dirichlet\nprocesses (HDPs). The main result of this paper is to show that improved\nparameter estimation allows BNCs to outperform leading learning methods such as\nRandom Forest for both 0-1 loss and RMSE, albeit just on categorical datasets.\n  As data assets become larger, entering the hyped world of \"big\", efficient\naccurate classification requires three main elements: (1) classifiers with\nlow-bias that can capture the fine-detail of large datasets (2) out-of-core\nlearners that can learn from data without having to hold it all in main memory\nand (3) models that can classify new data very efficiently.\n  The latest Bayesian network classifiers (BNCs) satisfy these requirements.\nTheir bias can be controlled easily by increasing the number of parents of the\nnodes in the graph. Their structure can be learned out of core with a limited\nnumber of passes over the data. However, as the bias is made lower to\naccurately model classification tasks, so is the accuracy of their parameters'\nestimates, as each parameter is estimated from ever decreasing quantities of\ndata. In this paper, we introduce the use of Hierarchical Dirichlet Processes\nfor accurate BNC parameter estimation.\n  We conduct an extensive set of experiments on 68 standard datasets and\ndemonstrate that our resulting classifiers perform very competitively with\nRandom Forest in terms of prediction, while keeping the out-of-core capability\nand superior classification time.\n", "versions": [{"version": "v1", "created": "Fri, 25 Aug 2017 00:20:49 GMT"}, {"version": "v2", "created": "Tue, 19 Dec 2017 22:30:15 GMT"}, {"version": "v3", "created": "Tue, 8 May 2018 12:18:12 GMT"}], "update_date": "2018-05-09", "authors_parsed": [["Petitjean", "Francois", ""], ["Buntine", "Wray", ""], ["Webb", "Geoffrey I.", ""], ["Zaidi", "Nayyar", ""]]}, {"id": "1708.07644", "submitter": "Jean-Luc Meunier", "authors": "Jean-Luc Meunier", "title": "Joint Structured Learning and Predictions under Logical Constraints in\n  Conditional Random Fields", "comments": "CAp 2017 (Conf\\'erence sur l'Apprentissage automatique)", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper is concerned with structured machine learning, in a supervised\nmachine learning context. It discusses how to make joint structured learning on\ninterdependent objects of different nature, as well as how to enforce logical\ncon-straints when predicting labels. We explain how this need arose in a\nDocument Understanding task. We then discuss a general extension to Conditional\nRandom Field (CRF) for this purpose and present the contributed open source\nimplementation on top of the open source PyStruct library. We evaluate its\nperformance on a publicly available dataset.\n", "versions": [{"version": "v1", "created": "Fri, 25 Aug 2017 08:14:22 GMT"}], "update_date": "2017-08-28", "authors_parsed": [["Meunier", "Jean-Luc", ""]]}, {"id": "1708.07689", "submitter": "Sebastian Lapuschkin", "authors": "Sebastian Lapuschkin, Alexander Binder, Klaus-Robert M\\\"uller,\n  Wojciech Samek", "title": "Understanding and Comparing Deep Neural Networks for Age and Gender\n  Classification", "comments": "8 pages, 5 figures, 5 tables. Presented at ICCV 2017 Workshop: 7th\n  IEEE International Workshop on Analysis and Modeling of Faces and Gestures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.AI cs.CV cs.IR cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recently, deep neural networks have demonstrated excellent performances in\nrecognizing the age and gender on human face images. However, these models were\napplied in a black-box manner with no information provided about which facial\nfeatures are actually used for prediction and how these features depend on\nimage preprocessing, model initialization and architecture choice. We present a\nstudy investigating these different effects.\n  In detail, our work compares four popular neural network architectures,\nstudies the effect of pretraining, evaluates the robustness of the considered\nalignment preprocessings via cross-method test set swapping and intuitively\nvisualizes the model's prediction strategies in given preprocessing conditions\nusing the recent Layer-wise Relevance Propagation (LRP) algorithm. Our\nevaluations on the challenging Adience benchmark show that suitable parameter\ninitialization leads to a holistic perception of the input, compensating\nartefactual data representations. With a combination of simple preprocessing\nsteps, we reach state of the art performance in gender recognition.\n", "versions": [{"version": "v1", "created": "Fri, 25 Aug 2017 11:08:38 GMT"}], "update_date": "2017-08-28", "authors_parsed": [["Lapuschkin", "Sebastian", ""], ["Binder", "Alexander", ""], ["M\u00fcller", "Klaus-Robert", ""], ["Samek", "Wojciech", ""]]}, {"id": "1708.07747", "submitter": "Han Xiao", "authors": "Han Xiao, Kashif Rasul, Roland Vollgraf", "title": "Fashion-MNIST: a Novel Image Dataset for Benchmarking Machine Learning\n  Algorithms", "comments": "Dataset is freely available at\n  https://github.com/zalandoresearch/fashion-mnist Benchmark is available at\n  http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present Fashion-MNIST, a new dataset comprising of 28x28 grayscale images\nof 70,000 fashion products from 10 categories, with 7,000 images per category.\nThe training set has 60,000 images and the test set has 10,000 images.\nFashion-MNIST is intended to serve as a direct drop-in replacement for the\noriginal MNIST dataset for benchmarking machine learning algorithms, as it\nshares the same image size, data format and the structure of training and\ntesting splits. The dataset is freely available at\nhttps://github.com/zalandoresearch/fashion-mnist\n", "versions": [{"version": "v1", "created": "Fri, 25 Aug 2017 14:01:29 GMT"}, {"version": "v2", "created": "Fri, 15 Sep 2017 21:29:49 GMT"}], "update_date": "2017-09-19", "authors_parsed": [["Xiao", "Han", ""], ["Rasul", "Kashif", ""], ["Vollgraf", "Roland", ""]]}, {"id": "1708.07787", "submitter": "Lawrence Murray", "authors": "Lawrence M. Murray, Daniel Lund\\'en, Jan Kudlicka, David Broman,\n  Thomas B. Sch\\\"on", "title": "Delayed Sampling and Automatic Rao-Blackwellization of Probabilistic\n  Programs", "comments": "13 pages, 4 figures", "journal-ref": "Proceedings of the 21st International Conference on Artificial\n  Intelligence and Statistics (AISTATS) 2018", "doi": null, "report-no": null, "categories": "stat.ML stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We introduce a dynamic mechanism for the solution of analytically-tractable\nsubstructure in probabilistic programs, using conjugate priors and affine\ntransformations to reduce variance in Monte Carlo estimators. For inference\nwith Sequential Monte Carlo, this automatically yields improvements such as\nlocally-optimal proposals and Rao-Blackwellization. The mechanism maintains a\ndirected graph alongside the running program that evolves dynamically as\noperations are triggered upon it. Nodes of the graph represent random\nvariables, edges the analytically-tractable relationships between them. Random\nvariables remain in the graph for as long as possible, to be sampled only when\nthey are used by the program in a way that cannot be resolved analytically. In\nthe meantime, they are conditioned on as many observations as possible. We\ndemonstrate the mechanism with a few pedagogical examples, as well as a\nlinear-nonlinear state-space model with simulated data, and an epidemiological\nmodel with real data of a dengue outbreak in Micronesia. In all cases one or\nmore variables are automatically marginalized out to significantly reduce\nvariance in estimates of the marginal likelihood, in the final case\nfacilitating a random-weight or pseudo-marginal-type importance sampler for\nparameter estimation. We have implemented the approach in Anglican and a new\nprobabilistic programming language called Birch.\n", "versions": [{"version": "v1", "created": "Fri, 25 Aug 2017 15:48:20 GMT"}, {"version": "v2", "created": "Wed, 21 Mar 2018 15:30:49 GMT"}], "update_date": "2018-03-22", "authors_parsed": [["Murray", "Lawrence M.", ""], ["Lund\u00e9n", "Daniel", ""], ["Kudlicka", "Jan", ""], ["Broman", "David", ""], ["Sch\u00f6n", "Thomas B.", ""]]}, {"id": "1708.07807", "submitter": "Ting Wang", "authors": "Xinyang Zhang, Yujie Ji and Ting Wang", "title": "Modular Learning Component Attacks: Today's Reality, Tomorrow's\n  Challenge", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CR cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Many of today's machine learning (ML) systems are not built from scratch, but\nare compositions of an array of {\\em modular learning components} (MLCs). The\nincreasing use of MLCs significantly simplifies the ML system development\ncycles. However, as most MLCs are contributed and maintained by third parties,\ntheir lack of standardization and regulation entails profound security\nimplications.\n  In this paper, for the first time, we demonstrate that potentially harmful\nMLCs pose immense threats to the security of ML systems. We present a broad\nclass of {\\em logic-bomb} attacks in which maliciously crafted MLCs trigger\nhost systems to malfunction in a predictable manner. By empirically studying\ntwo state-of-the-art ML systems in the healthcare domain, we explore the\nfeasibility of such attacks. For example, we show that, without prior knowledge\nabout the host ML system, by modifying only 3.3{\\textperthousand} of the MLC's\nparameters, each with distortion below $10^{-3}$, the adversary is able to\nforce the misdiagnosis of target victims' skin cancers with 100\\% success rate.\nWe provide analytical justification for the success of such attacks, which\npoints to the fundamental characteristics of today's ML models: high\ndimensionality, non-linearity, and non-convexity. The issue thus seems\nfundamental to many ML systems. We further discuss potential countermeasures to\nmitigate MLC-based attacks and their potential technical challenges.\n", "versions": [{"version": "v1", "created": "Fri, 25 Aug 2017 16:48:54 GMT"}], "update_date": "2017-08-28", "authors_parsed": [["Zhang", "Xinyang", ""], ["Ji", "Yujie", ""], ["Wang", "Ting", ""]]}, {"id": "1708.07826", "submitter": "Raul Rojas Prof.", "authors": "Raul Rojas", "title": "Logistic Regression as Soft Perceptron Learning", "comments": "3 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We comment on the fact that gradient ascent for logistic regression has a\nconnection with the perceptron learning algorithm. Logistic learning is the\n\"soft\" variant of perceptron learning.\n", "versions": [{"version": "v1", "created": "Thu, 24 Aug 2017 20:19:20 GMT"}], "update_date": "2017-08-29", "authors_parsed": [["Rojas", "Raul", ""]]}, {"id": "1708.07827", "submitter": "Peng Xu", "authors": "Peng Xu, Farbod Roosta-Khorasani, Michael W. Mahoney", "title": "Second-Order Optimization for Non-Convex Machine Learning: An Empirical\n  Study", "comments": "21 pages, 11 figures. Restructure the paper and add experiments", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.OC cs.LG math.NA stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  While first-order optimization methods such as stochastic gradient descent\n(SGD) are popular in machine learning (ML), they come with well-known\ndeficiencies, including relatively-slow convergence, sensitivity to the\nsettings of hyper-parameters such as learning rate, stagnation at high training\nerrors, and difficulty in escaping flat regions and saddle points. These issues\nare particularly acute in highly non-convex settings such as those arising in\nneural networks. Motivated by this, there has been recent interest in\nsecond-order methods that aim to alleviate these shortcomings by capturing\ncurvature information. In this paper, we report detailed empirical evaluations\nof a class of Newton-type methods, namely sub-sampled variants of trust region\n(TR) and adaptive regularization with cubics (ARC) algorithms, for non-convex\nML problems. In doing so, we demonstrate that these methods not only can be\ncomputationally competitive with hand-tuned SGD with momentum, obtaining\ncomparable or better generalization performance, but also they are highly\nrobust to hyper-parameter settings. Further, in contrast to SGD with momentum,\nwe show that the manner in which these Newton-type methods employ curvature\ninformation allows them to seamlessly escape flat regions and saddle points.\n", "versions": [{"version": "v1", "created": "Fri, 25 Aug 2017 01:33:15 GMT"}, {"version": "v2", "created": "Fri, 16 Feb 2018 00:57:22 GMT"}], "update_date": "2018-02-19", "authors_parsed": [["Xu", "Peng", ""], ["Roosta-Khorasani", "Farbod", ""], ["Mahoney", "Michael W.", ""]]}, {"id": "1708.07888", "submitter": "Wei Chen", "authors": "Wei Chen, Mark Fuge", "title": "Active Expansion Sampling for Learning Feasible Domains in an Unbounded\n  Input Space", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Many engineering problems require identifying feasible domains under implicit\nconstraints. One example is finding acceptable car body styling designs based\non constraints like aesthetics and functionality. Current active-learning based\nmethods learn feasible domains for bounded input spaces. However, we usually\nlack prior knowledge about how to set those input variable bounds. Bounds that\nare too small will fail to cover all feasible domains; while bounds that are\ntoo large will waste query budget. To avoid this problem, we introduce Active\nExpansion Sampling (AES), a method that identifies (possibly disconnected)\nfeasible domains over an unbounded input space. AES progressively expands our\nknowledge of the input space, and uses successive exploitation and exploration\nstages to switch between learning the decision boundary and searching for new\nfeasible domains. We show that AES has a misclassification loss guarantee\nwithin the explored region, independent of the number of iterations or labeled\nsamples. Thus it can be used for real-time prediction of samples' feasibility\nwithin the explored region. We evaluate AES on three test examples and compare\nAES with two adaptive sampling methods -- the Neighborhood-Voronoi algorithm\nand the straddle heuristic -- that operate over fixed input variable bounds.\n", "versions": [{"version": "v1", "created": "Fri, 25 Aug 2017 21:12:40 GMT"}, {"version": "v2", "created": "Wed, 22 Nov 2017 22:19:23 GMT"}, {"version": "v3", "created": "Sat, 20 Jan 2018 19:29:56 GMT"}], "update_date": "2018-01-23", "authors_parsed": [["Chen", "Wei", ""], ["Fuge", "Mark", ""]]}, {"id": "1708.07906", "submitter": "Shanghua Teng", "authors": "Shang-Hua Teng", "title": "Network Essence: PageRank Completion and Centrality-Conforming Markov\n  Chains", "comments": "In \"A Journey Through Discrete Mathematics, A Tribute to Ji\\v{r}\\'i\n  Matou\\v{s}ek\", Editors Martin Loebl, Jaroslav Ne\\v{s}et\\v{r}il and Robin\n  Thomas, Springer International Publishing, 2017", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.SI cs.CG cs.DS math.CO stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Ji\\v{r}\\'i Matou\\v{s}ek (1963-2015) had many breakthrough contributions in\nmathematics and algorithm design. His milestone results are not only profound\nbut also elegant. By going beyond the original objects --- such as Euclidean\nspaces or linear programs --- Jirka found the essence of the challenging\nmathematical/algorithmic problems as well as beautiful solutions that were\nnatural to him, but were surprising discoveries to the field.\n  In this short exploration article, I will first share with readers my initial\nencounter with Jirka and discuss one of his fundamental geometric results from\nthe early 1990s. In the age of social and information networks, I will then\nturn the discussion from geometric structures to network structures, attempting\nto take a humble step towards the holy grail of network science, that is to\nunderstand the network essence that underlies the observed\nsparse-and-multifaceted network data. I will discuss a simple result which\nsummarizes some basic algebraic properties of personalized PageRank matrices.\nUnlike the traditional transitive closure of binary relations, the personalized\nPageRank matrices take \"accumulated Markovian closure\" of network data. Some of\nthese algebraic properties are known in various contexts. But I hope featuring\nthem together in a broader context will help to illustrate the desirable\nproperties of this Markovian completion of networks, and motivate systematic\ndevelopments of a network theory for understanding vast and ubiquitous\nmultifaceted network data.\n", "versions": [{"version": "v1", "created": "Fri, 25 Aug 2017 22:35:23 GMT"}], "update_date": "2017-08-29", "authors_parsed": [["Teng", "Shang-Hua", ""]]}, {"id": "1708.07918", "submitter": "Mo Yu", "authors": "Mo Yu, Xiaoxiao Guo, Jinfeng Yi, Shiyu Chang, Saloni Potdar, Gerald\n  Tesauro, Haoyu Wang, Bowen Zhou", "title": "Robust Task Clustering for Deep Many-Task Learning", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI cs.CL stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We investigate task clustering for deep-learning based multi-task and\nfew-shot learning in a many-task setting. We propose a new method to measure\ntask similarities with cross-task transfer performance matrix for the deep\nlearning scenario. Although this matrix provides us critical information\nregarding similarity between tasks, its asymmetric property and unreliable\nperformance scores can affect conventional clustering methods adversely.\nAdditionally, the uncertain task-pairs, i.e., the ones with extremely\nasymmetric transfer scores, may collectively mislead clustering algorithms to\noutput an inaccurate task-partition. To overcome these limitations, we propose\na novel task-clustering algorithm by using the matrix completion technique. The\nproposed algorithm constructs a partially-observed similarity matrix based on\nthe certainty of cluster membership of the task-pairs. We then use a matrix\ncompletion algorithm to complete the similarity matrix. Our theoretical\nanalysis shows that under mild constraints, the proposed algorithm will\nperfectly recover the underlying \"true\" similarity matrix with a high\nprobability. Our results show that the new task clustering method can discover\ntask clusters for training flexible and superior neural network models in a\nmulti-task learning setup for sentiment classification and dialog intent\nclassification tasks. Our task clustering approach also extends metric-based\nfew-shot learning methods to adapt multiple metrics, which demonstrates\nempirical advantages when the tasks are diverse.\n", "versions": [{"version": "v1", "created": "Sat, 26 Aug 2017 02:29:50 GMT"}, {"version": "v2", "created": "Fri, 18 May 2018 00:53:48 GMT"}], "update_date": "2018-05-21", "authors_parsed": [["Yu", "Mo", ""], ["Guo", "Xiaoxiao", ""], ["Yi", "Jinfeng", ""], ["Chang", "Shiyu", ""], ["Potdar", "Saloni", ""], ["Tesauro", "Gerald", ""], ["Wang", "Haoyu", ""], ["Zhou", "Bowen", ""]]}, {"id": "1708.07946", "submitter": "Kui Zhao", "authors": "Kui Zhao, Can Wang", "title": "Sales Forecast in E-commerce using Convolutional Neural Network", "comments": "8 pages, 7 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Sales forecast is an essential task in E-commerce and has a crucial impact on\nmaking informed business decisions. It can help us to manage the workforce,\ncash flow and resources such as optimizing the supply chain of manufacturers\netc. Sales forecast is a challenging problem in that sales is affected by many\nfactors including promotion activities, price changes, and user preferences\netc. Traditional sales forecast techniques mainly rely on historical sales data\nto predict future sales and their accuracies are limited. Some more recent\nlearning-based methods capture more information in the model to improve the\nforecast accuracy. However, these methods require case-by-case manual feature\nengineering for specific commercial scenarios, which is usually a difficult,\ntime-consuming task and requires expert knowledge. To overcome the limitations\nof existing methods, we propose a novel approach in this paper to learn\neffective features automatically from the structured data using the\nConvolutional Neural Network (CNN). When fed with raw log data, our approach\ncan automatically extract effective features from that and then forecast sales\nusing those extracted features. We test our method on a large real-world\ndataset from CaiNiao.com and the experimental results validate the\neffectiveness of our method.\n", "versions": [{"version": "v1", "created": "Sat, 26 Aug 2017 07:47:15 GMT"}], "update_date": "2017-09-02", "authors_parsed": [["Zhao", "Kui", ""], ["Wang", "Can", ""]]}, {"id": "1708.07967", "submitter": "Shuchin Aeron", "authors": "Brian Rappaport, Anuththari Gamage, Shuchin Aeron", "title": "Faster Clustering via Non-Backtracking Random Walks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG cs.SI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper presents VEC-NBT, a variation on the unsupervised graph clustering\ntechnique VEC, which improves upon the performance of the original algorithm\nsignificantly for sparse graphs. VEC employs a novel application of the\nstate-of-the-art word2vec model to embed a graph in Euclidean space via random\nwalks on the nodes of the graph. In VEC-NBT, we modify the original algorithm\nto use a non-backtracking random walk instead of the normal backtracking random\nwalk used in VEC. We introduce a modification to a non-backtracking random\nwalk, which we call a begrudgingly-backtracking random walk, and show\nempirically that using this model of random walks for VEC-NBT requires shorter\nwalks on the graph to obtain results with comparable or greater accuracy than\nVEC, especially for sparser graphs.\n", "versions": [{"version": "v1", "created": "Sat, 26 Aug 2017 13:40:22 GMT"}], "update_date": "2017-08-29", "authors_parsed": [["Rappaport", "Brian", ""], ["Gamage", "Anuththari", ""], ["Aeron", "Shuchin", ""]]}, {"id": "1708.07975", "submitter": "Vincent Bindschaedler", "authors": "Vincent Bindschaedler, Reza Shokri, Carl A. Gunter", "title": "Plausible Deniability for Privacy-Preserving Data Synthesis", "comments": "In PVLDB 2017", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CR cs.DB cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Releasing full data records is one of the most challenging problems in data\nprivacy. On the one hand, many of the popular techniques such as data\nde-identification are problematic because of their dependence on the background\nknowledge of adversaries. On the other hand, rigorous methods such as the\nexponential mechanism for differential privacy are often computationally\nimpractical to use for releasing high dimensional data or cannot preserve high\nutility of original data due to their extensive data perturbation.\n  This paper presents a criterion called plausible deniability that provides a\nformal privacy guarantee, notably for releasing sensitive datasets: an output\nrecord can be released only if a certain amount of input records are\nindistinguishable, up to a privacy parameter. This notion does not depend on\nthe background knowledge of an adversary. Also, it can efficiently be checked\nby privacy tests. We present mechanisms to generate synthetic datasets with\nsimilar statistical properties to the input data and the same format. We study\nthis technique both theoretically and experimentally. A key theoretical result\nshows that, with proper randomization, the plausible deniability mechanism\ngenerates differentially private synthetic data. We demonstrate the efficiency\nof this generative technique on a large dataset; it is shown to preserve the\nutility of original data with respect to various statistical analysis and\nmachine learning measures.\n", "versions": [{"version": "v1", "created": "Sat, 26 Aug 2017 14:13:28 GMT"}], "update_date": "2017-08-29", "authors_parsed": [["Bindschaedler", "Vincent", ""], ["Shokri", "Reza", ""], ["Gunter", "Carl A.", ""]]}, {"id": "1708.08012", "submitter": "Robin Tibor Schirrmeister", "authors": "Robin Tibor Schirrmeister, Lukas Gemein, Katharina Eggensperger, Frank\n  Hutter, Tonio Ball", "title": "Deep learning with convolutional neural networks for decoding and\n  visualization of EEG pathology", "comments": "Published at IEEE SPMB 2017 https://www.ieeespmb.org/2017/", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.NE stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We apply convolutional neural networks (ConvNets) to the task of\ndistinguishing pathological from normal EEG recordings in the Temple University\nHospital EEG Abnormal Corpus. We use two basic, shallow and deep ConvNet\narchitectures recently shown to decode task-related information from EEG at\nleast as well as established algorithms designed for this purpose. In decoding\nEEG pathology, both ConvNets reached substantially better accuracies (about 6%\nbetter, ~85% vs. ~79%) than the only published result for this dataset, and\nwere still better when using only 1 minute of each recording for training and\nonly six seconds of each recording for testing. We used automated methods to\noptimize architectural hyperparameters and found intriguingly different ConvNet\narchitectures, e.g., with max pooling as the only nonlinearity. Visualizations\nof the ConvNet decoding behavior showed that they used spectral power changes\nin the delta (0-4 Hz) and theta (4-8 Hz) frequency range, possibly alongside\nother features, consistent with expectations derived from spectral analysis of\nthe EEG data and from the textual medical reports. Analysis of the textual\nmedical reports also highlighted the potential for accuracy increases by\nintegrating contextual information, such as the age of subjects. In summary,\nthe ConvNets and visualization techniques used in this study constitute a next\nstep towards clinically useful automated EEG diagnosis and establish a new\nbaseline for future work on this topic.\n", "versions": [{"version": "v1", "created": "Sat, 26 Aug 2017 19:14:47 GMT"}, {"version": "v2", "created": "Thu, 7 Dec 2017 14:56:40 GMT"}, {"version": "v3", "created": "Thu, 11 Jan 2018 20:11:04 GMT"}], "update_date": "2018-01-15", "authors_parsed": [["Schirrmeister", "Robin Tibor", ""], ["Gemein", "Lukas", ""], ["Eggensperger", "Katharina", ""], ["Hutter", "Frank", ""], ["Ball", "Tonio", ""]]}, {"id": "1708.08022", "submitter": "Ilya Mironov", "authors": "Mart\\'in Abadi, \\'Ulfar Erlingsson, Ian Goodfellow, H. Brendan\n  McMahan, Ilya Mironov, Nicolas Papernot, Kunal Talwar, Li Zhang", "title": "On the Protection of Private Information in Machine Learning Systems:\n  Two Recent Approaches", "comments": null, "journal-ref": "IEEE 30th Computer Security Foundations Symposium (CSF), pages\n  1--6, 2017", "doi": "10.1109/CSF.2017.10", "report-no": null, "categories": "stat.ML cs.CR cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The recent, remarkable growth of machine learning has led to intense interest\nin the privacy of the data on which machine learning relies, and to new\ntechniques for preserving privacy. However, older ideas about privacy may well\nremain valid and useful. This note reviews two recent works on privacy in the\nlight of the wisdom of some of the early literature, in particular the\nprinciples distilled by Saltzer and Schroeder in the 1970s.\n", "versions": [{"version": "v1", "created": "Sat, 26 Aug 2017 23:23:39 GMT"}], "update_date": "2018-12-12", "authors_parsed": [["Abadi", "Mart\u00edn", ""], ["Erlingsson", "\u00dalfar", ""], ["Goodfellow", "Ian", ""], ["McMahan", "H. Brendan", ""], ["Mironov", "Ilya", ""], ["Papernot", "Nicolas", ""], ["Talwar", "Kunal", ""], ["Zhang", "Li", ""]]}, {"id": "1708.08042", "submitter": "Songqing Yue", "authors": "Songqing Yue", "title": "Imbalanced Malware Images Classification: a CNN based Approach", "comments": "5 pages, 6 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Deep convolutional neural networks (CNNs) can be applied to malware binary\ndetection through images classification. The performance, however, is degraded\ndue to the imbalance of malware families (classes). To mitigate this issue, we\npropose a simple yet effective weighted softmax loss which can be employed as\nthe final layer of deep CNNs. The original softmax loss is weighted, and the\nweight value can be determined according to class size. A scaling parameter is\nalso included in computing the weight. Proper selection of this parameter has\nbeen studied and an empirical option is given. The weighted loss aims at\nalleviating the impact of data imbalance in an end-to-end learning fashion. To\nvalidate the efficacy, we deploy the proposed weighted loss in a pre-trained\ndeep CNN model and fine-tune it to achieve promising results on malware images\nclassification. Extensive experiments also indicate that the new loss function\ncan fit other typical CNNs with an improved classification performance.\n", "versions": [{"version": "v1", "created": "Sun, 27 Aug 2017 02:27:59 GMT"}], "update_date": "2017-08-29", "authors_parsed": [["Yue", "Songqing", ""]]}, {"id": "1708.08053", "submitter": "Pelumi Oluwasanya", "authors": "Pelumi Oluwasanya", "title": "Anomaly Detection in Wireless Sensor Networks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Wireless sensor networks usually comprise a large number of sensors\nmonitoring changes in variables. These changes in variables represent changes\nin physical quantities. The changes can occur for various reasons; these\nreasons are highlighted in this work. Outliers are unusual measurements.\nOutliers are important; they are information-bearing occurrences. This work\nseeks to identify them based on an approach presented in [1]. A critical review\nof most previous works in this area has been presented in [2], and few more are\nconsidered here just to set the stage. The main work can be described as this;\ngiven a set of measurements from sensors that represent a normal situation, [1]\nproceeds by first estimating the probability density function (pdf) of the set\nusing a data-split approach, then estimate the entropy of the set using the\narithmetic mean as an approximation for the expectation. The increase in\nentropy that occurs when strange data is recorded is used to detect unusual\nmeasurements in the test set depending on the desired confidence interval or\nfalse alarm rate. The results presented in [1] have been confirmed for\ndifferent test signals such as the Gaussian, Beta, in one dimension and beta in\ntwo dimensions, and a beta and uniform mixture distribution in two dimensions.\nFinally, the method was confirmed on real data and the results are presented.\nThe major drawbacks of [1] were identified, and a method that seeks to mitigate\nthis using the Bhattacharyya distance is presented. This method detects more\nsubtle anomalies, especially the type that would pass as normal in [1].\nFinally, recommendations for future research are presented: the subject of\ninterpretability, especially for subtle measurements, being the most elusive as\nof today.\n", "versions": [{"version": "v1", "created": "Sun, 27 Aug 2017 05:39:17 GMT"}], "update_date": "2017-08-29", "authors_parsed": [["Oluwasanya", "Pelumi", ""]]}, {"id": "1708.08155", "submitter": "Waheed Bajwa", "authors": "Zhixiong Yang and Waheed U. Bajwa", "title": "ByRDiE: Byzantine-resilient distributed coordinate descent for\n  decentralized learning", "comments": "Preprint of a paper accepted into IEEE Transactions on Signal and\n  Information Processing Over Networks; 16 pages, 5 figures, and 1 table", "journal-ref": "IEEE Trans. Signal Inform. Proc. over Netw., vol. 5, no. 4, pp.\n  611-627, Dec. 2019", "doi": "10.1109/TSIPN.2019.2928176", "report-no": null, "categories": "cs.LG cs.DC math.OC stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Distributed machine learning algorithms enable learning of models from\ndatasets that are distributed over a network without gathering the data at a\ncentralized location. While efficient distributed algorithms have been\ndeveloped under the assumption of faultless networks, failures that can render\nthese algorithms nonfunctional occur frequently in the real world. This paper\nfocuses on the problem of Byzantine failures, which are the hardest to\nsafeguard against in distributed algorithms. While Byzantine fault tolerance\nhas a rich history, existing work does not translate into efficient and\npractical algorithms for high-dimensional learning in fully distributed (also\nknown as decentralized) settings. In this paper, an algorithm termed\nByzantine-resilient distributed coordinate descent (ByRDiE) is developed and\nanalyzed that enables distributed learning in the presence of Byzantine\nfailures. Theoretical analysis (convex settings) and numerical experiments\n(convex and nonconvex settings) highlight its usefulness for high-dimensional\ndistributed learning in the presence of Byzantine failures.\n", "versions": [{"version": "v1", "created": "Mon, 28 Aug 2017 00:22:18 GMT"}, {"version": "v2", "created": "Tue, 25 Sep 2018 20:29:11 GMT"}, {"version": "v3", "created": "Thu, 21 Mar 2019 15:08:58 GMT"}, {"version": "v4", "created": "Fri, 5 Jul 2019 17:00:42 GMT"}], "update_date": "2020-07-07", "authors_parsed": [["Yang", "Zhixiong", ""], ["Bajwa", "Waheed U.", ""]]}, {"id": "1708.08157", "submitter": "Zoltan Szabo", "authors": "Zoltan Szabo and Bharath K. Sriperumbudur", "title": "Characteristic and Universal Tensor Product Kernels", "comments": "final version appeared in JMLR", "journal-ref": "Journal of Machine Learning Research 18(233):1-29, 2018", "doi": null, "report-no": null, "categories": "stat.ML cs.IT math.IT stat.ME", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Maximum mean discrepancy (MMD), also called energy distance or N-distance in\nstatistics and Hilbert-Schmidt independence criterion (HSIC), specifically\ndistance covariance in statistics, are among the most popular and successful\napproaches to quantify the difference and independence of random variables,\nrespectively. Thanks to their kernel-based foundations, MMD and HSIC are\napplicable on a wide variety of domains. Despite their tremendous success,\nquite little is known about when HSIC characterizes independence and when MMD\nwith tensor product kernel can discriminate probability distributions. In this\npaper, we answer these questions by studying various notions of characteristic\nproperty of the tensor product kernel.\n", "versions": [{"version": "v1", "created": "Mon, 28 Aug 2017 00:37:38 GMT"}, {"version": "v2", "created": "Sun, 13 May 2018 11:33:18 GMT"}, {"version": "v3", "created": "Tue, 22 May 2018 19:46:33 GMT"}, {"version": "v4", "created": "Thu, 2 Aug 2018 12:33:18 GMT"}], "update_date": "2018-08-03", "authors_parsed": [["Szabo", "Zoltan", ""], ["Sriperumbudur", "Bharath K.", ""]]}, {"id": "1708.08227", "submitter": "Mostapha Benhenda", "authors": "Mostapha Benhenda (LAGA)", "title": "ChemGAN challenge for drug discovery: can AI reproduce natural chemical\n  diversity?", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.AI cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Generating molecules with desired chemical properties is important for drug\ndiscovery. The use of generative neural networks is promising for this task.\nHowever, from visual inspection, it often appears that generated samples lack\ndiversity. In this paper, we quantify this internal chemical diversity, and we\nraise the following challenge: can a nontrivial AI model reproduce natural\nchemical diversity for desired molecules? To illustrate this question, we\nconsider two generative models: a Reinforcement Learning model and the recently\nintroduced ORGAN. Both fail at this challenge. We hope this challenge will\nstimulate research in this direction.\n", "versions": [{"version": "v1", "created": "Mon, 28 Aug 2017 08:02:55 GMT"}, {"version": "v2", "created": "Wed, 30 Aug 2017 09:03:57 GMT"}, {"version": "v3", "created": "Thu, 31 Aug 2017 14:14:29 GMT"}], "update_date": "2017-09-01", "authors_parsed": [["Benhenda", "Mostapha", "", "LAGA"]]}, {"id": "1708.08231", "submitter": "Pittipol Kantavat", "authors": "Pittipol Kantavat, Boonserm Kijsirikul, Patoomsiri Songsiri, Ken-ichi\n  Fukui, Masayuki Numao", "title": "Efficient Decision Trees for Multi-class Support Vector Machines Using\n  Entropy and Generalization Error Estimation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose new methods for Support Vector Machines (SVMs) using tree\narchitecture for multi-class classi- fication. In each node of the tree, we\nselect an appropriate binary classifier using entropy and generalization error\nestimation, then group the examples into positive and negative classes based on\nthe selected classi- fier and train a new classifier for use in the\nclassification phase. The proposed methods can work in time complexity between\nO(log2N) to O(N) where N is the number of classes. We compared the performance\nof our proposed methods to the traditional techniques on the UCI machine\nlearning repository using 10-fold cross-validation. The experimental results\nshow that our proposed methods are very useful for the problems that need fast\nclassification time or problems with a large number of classes as the proposed\nmethods run much faster than the traditional techniques but still provide\ncomparable accuracy.\n", "versions": [{"version": "v1", "created": "Mon, 28 Aug 2017 08:16:28 GMT"}], "update_date": "2017-08-29", "authors_parsed": [["Kantavat", "Pittipol", ""], ["Kijsirikul", "Boonserm", ""], ["Songsiri", "Patoomsiri", ""], ["Fukui", "Ken-ichi", ""], ["Numao", "Masayuki", ""]]}, {"id": "1708.08282", "submitter": "Peng-Bo Zhang", "authors": "Peng-Bo Zhang and Zhi-Xin Yang", "title": "A New Learning Paradigm for Random Vector Functional-Link Network: RVFL+", "comments": "We have updated the previous work", "journal-ref": "Neural Networks 122(2019) 94-105", "doi": "10.1016/j.neunet.2019.09.039", "report-no": null, "categories": "stat.ML cs.LG cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In school, a teacher plays an important role in various classroom teaching\npatterns. Likewise to this human learning activity, the learning using\nprivileged information (LUPI) paradigm provides additional information\ngenerated by the teacher to 'teach' learning models during the training stage.\nTherefore, this novel learning paradigm is a typical Teacher-Student\nInteraction mechanism. This paper is the first to present a random vector\nfunctional link network based on the LUPI paradigm, called RVFL+. Rather than\nsimply combining two existing approaches, the newly-derived RVFL+ fills the gap\nbetween classical randomized neural networks and the newfashioned LUPI\nparadigm, which offers an alternative way to train RVFL networks. Moreover, the\nproposed RVFL+ can perform in conjunction with the kernel trick for highly\ncomplicated nonlinear feature learning, which is termed KRVFL+. Furthermore,\nthe statistical property of the proposed RVFL+ is investigated, and we present\na sharp and high-quality generalization error bound based on the Rademacher\ncomplexity. Competitive experimental results on 14 real-world datasets\nillustrate the great effectiveness and efficiency of the novel RVFL+ and\nKRVFL+, which can achieve better generalization performance than\nstate-of-the-art methods.\n", "versions": [{"version": "v1", "created": "Mon, 28 Aug 2017 11:55:00 GMT"}, {"version": "v2", "created": "Mon, 18 Sep 2017 10:55:16 GMT"}, {"version": "v3", "created": "Sat, 9 Mar 2019 14:31:36 GMT"}, {"version": "v4", "created": "Sun, 17 Mar 2019 03:40:19 GMT"}], "update_date": "2019-11-01", "authors_parsed": [["Zhang", "Peng-Bo", ""], ["Yang", "Zhi-Xin", ""]]}, {"id": "1708.08296", "submitter": "Wojciech Samek", "authors": "Wojciech Samek, Thomas Wiegand, Klaus-Robert M\\\"uller", "title": "Explainable Artificial Intelligence: Understanding, Visualizing and\n  Interpreting Deep Learning Models", "comments": "8 pages, 2 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.CY cs.NE stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  With the availability of large databases and recent improvements in deep\nlearning methodology, the performance of AI systems is reaching or even\nexceeding the human level on an increasing number of complex tasks. Impressive\nexamples of this development can be found in domains such as image\nclassification, sentiment analysis, speech understanding or strategic game\nplaying. However, because of their nested non-linear structure, these highly\nsuccessful machine learning and artificial intelligence models are usually\napplied in a black box manner, i.e., no information is provided about what\nexactly makes them arrive at their predictions. Since this lack of transparency\ncan be a major drawback, e.g., in medical applications, the development of\nmethods for visualizing, explaining and interpreting deep learning models has\nrecently attracted increasing attention. This paper summarizes recent\ndevelopments in this field and makes a plea for more interpretability in\nartificial intelligence. Furthermore, it presents two approaches to explaining\npredictions of deep learning models, one method which computes the sensitivity\nof the prediction with respect to changes in the input and one approach which\nmeaningfully decomposes the decision in terms of the input variables. These\nmethods are evaluated on three classification tasks.\n", "versions": [{"version": "v1", "created": "Mon, 28 Aug 2017 12:53:49 GMT"}], "update_date": "2017-08-29", "authors_parsed": [["Samek", "Wojciech", ""], ["Wiegand", "Thomas", ""], ["M\u00fcller", "Klaus-Robert", ""]]}, {"id": "1708.08310", "submitter": "Maria-Irina Nicolae", "authors": "Vincent P.A. Lonij, Ambrish Rawat, Maria-Irina Nicolae", "title": "Open-World Visual Recognition Using Knowledge Graphs", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In a real-world setting, visual recognition systems can be brought to make\npredictions for images belonging to previously unknown class labels. In order\nto make semantically meaningful predictions for such inputs, we propose a\ntwo-step approach that utilizes information from knowledge graphs. First, a\nknowledge-graph representation is learned to embed a large set of entities into\na semantic space. Second, an image representation is learned to embed images\ninto the same space. Under this setup, we are able to predict structured\nproperties in the form of relationship triples for any open-world image. This\nis true even when a set of labels has been omitted from the training protocols\nof both the knowledge graph and image embeddings. Furthermore, we append this\nlearning framework with appropriate smoothness constraints and show how prior\nknowledge can be incorporated into the model. Both these improvements combined\nincrease performance for visual recognition by a factor of six compared to our\nbaseline. Finally, we propose a new, extended dataset which we use for\nexperiments.\n", "versions": [{"version": "v1", "created": "Mon, 28 Aug 2017 13:45:07 GMT"}], "update_date": "2017-08-29", "authors_parsed": [["Lonij", "Vincent P. A.", ""], ["Rawat", "Ambrish", ""], ["Nicolae", "Maria-Irina", ""]]}, {"id": "1708.08311", "submitter": "Duc Nguyen", "authors": "Duc Minh Nguyen and Evaggelia Tsiligianni and Nikos Deligiannis", "title": "Deep Learning Sparse Ternary Projections for Compressed Sensing of\n  Images", "comments": "To appear in GlobalSIP 2017", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Compressed sensing (CS) is a sampling theory that allows reconstruction of\nsparse (or compressible) signals from an incomplete number of measurements,\nusing of a sensing mechanism implemented by an appropriate projection matrix.\nThe CS theory is based on random Gaussian projection matrices, which satisfy\nrecovery guarantees with high probability; however, sparse ternary {0, -1, +1}\nprojections are more suitable for hardware implementation. In this paper, we\npresent a deep learning approach to obtain very sparse ternary projections for\ncompressed sensing. Our deep learning architecture jointly learns a pair of a\nprojection matrix and a reconstruction operator in an end-to-end fashion. The\nexperimental results on real images demonstrate the effectiveness of the\nproposed approach compared to state-of-the-art methods, with significant\nadvantage in terms of complexity.\n", "versions": [{"version": "v1", "created": "Mon, 28 Aug 2017 13:51:09 GMT"}], "update_date": "2017-08-29", "authors_parsed": [["Nguyen", "Duc Minh", ""], ["Tsiligianni", "Evaggelia", ""], ["Deligiannis", "Nikos", ""]]}, {"id": "1708.08333", "submitter": "Jong Chul Ye", "authors": "Yoseob Han and Jong Chul Ye", "title": "Framing U-Net via Deep Convolutional Framelets: Application to\n  Sparse-view CT", "comments": "This will appear in IEEE Transaction on Medical Imaging, a special\n  issue of Machine Learning for Image Reconstruction", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  X-ray computed tomography (CT) using sparse projection views is a recent\napproach to reduce the radiation dose. However, due to the insufficient\nprojection views, an analytic reconstruction approach using the filtered back\nprojection (FBP) produces severe streaking artifacts. Recently, deep learning\napproaches using large receptive field neural networks such as U-Net have\ndemonstrated impressive performance for sparse- view CT reconstruction.\nHowever, theoretical justification is still lacking. Inspired by the recent\ntheory of deep convolutional framelets, the main goal of this paper is,\ntherefore, to reveal the limitation of U-Net and propose new multi-resolution\ndeep learning schemes. In particular, we show that the alternative U- Net\nvariants such as dual frame and the tight frame U-Nets satisfy the so-called\nframe condition which make them better for effective recovery of high frequency\nedges in sparse view- CT. Using extensive experiments with real patient data\nset, we demonstrate that the new network architectures provide better\nreconstruction performance.\n", "versions": [{"version": "v1", "created": "Mon, 28 Aug 2017 14:31:50 GMT"}, {"version": "v2", "created": "Tue, 6 Feb 2018 10:04:46 GMT"}, {"version": "v3", "created": "Wed, 28 Mar 2018 07:20:28 GMT"}], "update_date": "2018-03-29", "authors_parsed": [["Han", "Yoseob", ""], ["Ye", "Jong Chul", ""]]}, {"id": "1708.08487", "submitter": "Antonia Creswell", "authors": "Antonia Creswell, Kai Arulkumaran, Anil A. Bharath", "title": "On denoising autoencoders trained to minimise binary cross-entropy", "comments": "Submitted to Pattern Recognition Letters", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Denoising autoencoders (DAEs) are powerful deep learning models used for\nfeature extraction, data generation and network pre-training. DAEs consist of\nan encoder and decoder which may be trained simultaneously to minimise a loss\n(function) between an input and the reconstruction of a corrupted version of\nthe input. There are two common loss functions used for training autoencoders,\nthese include the mean-squared error (MSE) and the binary cross-entropy (BCE).\nWhen training autoencoders on image data a natural choice of loss function is\nBCE, since pixel values may be normalised to take values in [0,1] and the\ndecoder model may be designed to generate samples that take values in (0,1). We\nshow theoretically that DAEs trained to minimise BCE may be used to take\ngradient steps in the data space towards regions of high probability under the\ndata-generating distribution. Previously this had only been shown for DAEs\ntrained using MSE. As a consequence of the theory, iterative application of a\ntrained DAE moves a data sample from regions of low probability to regions of\nhigher probability under the data-generating distribution. Firstly, we validate\nthe theory by showing that novel data samples, consistent with the training\ndata, may be synthesised when the initial data samples are random noise.\nSecondly, we motivate the theory by showing that initial data samples\nsynthesised via other methods may be improved via iterative application of a\ntrained DAE to those initial samples.\n", "versions": [{"version": "v1", "created": "Mon, 28 Aug 2017 19:07:33 GMT"}, {"version": "v2", "created": "Mon, 9 Oct 2017 08:40:39 GMT"}], "update_date": "2017-10-10", "authors_parsed": [["Creswell", "Antonia", ""], ["Arulkumaran", "Kai", ""], ["Bharath", "Anil A.", ""]]}, {"id": "1708.08551", "submitter": "Mohammad Amin Nabian", "authors": "Mohammad Amin Nabian, Hadi Meidani", "title": "Deep Learning for Accelerated Reliability Analysis of Infrastructure\n  Networks", "comments": null, "journal-ref": "Nabian, M. A. and Meidani, H. (2018), Deep Learning for\n  Accelerated Seismic Reliability Analysis of Transportation Networks. Computer\n  Aided Civil and Infrastructure Engineering, 33: 443-458", "doi": "10.1111/mice.12359", "report-no": null, "categories": "cs.CE cs.AI stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Natural disasters can have catastrophic impacts on the functionality of\ninfrastructure systems and cause severe physical and socio-economic losses.\nGiven budget constraints, it is crucial to optimize decisions regarding\nmitigation, preparedness, response, and recovery practices for these systems.\nThis requires accurate and efficient means to evaluate the infrastructure\nsystem reliability. While numerous research efforts have addressed and\nquantified the impact of natural disasters on infrastructure systems, typically\nusing the Monte Carlo approach, they still suffer from high computational cost\nand, thus, are of limited applicability to large systems. This paper presents a\ndeep learning framework for accelerating infrastructure system reliability\nanalysis. In particular, two distinct deep neural network surrogates are\nconstructed and studied: (1) A classifier surrogate which speeds up the\nconnectivity determination of networks, and (2) An end-to-end surrogate that\nreplaces a number of components such as roadway status realization,\nconnectivity determination, and connectivity averaging. The proposed approach\nis applied to a simulation-based study of the two-terminal connectivity of a\nCalifornia transportation network subject to extreme probabilistic earthquake\nevents. Numerical results highlight the effectiveness of the proposed approach\nin accelerating the transportation system two-terminal reliability analysis\nwith extremely high prediction accuracy.\n", "versions": [{"version": "v1", "created": "Mon, 28 Aug 2017 22:41:11 GMT"}], "update_date": "2018-06-11", "authors_parsed": [["Nabian", "Mohammad Amin", ""], ["Meidani", "Hadi", ""]]}, {"id": "1708.08552", "submitter": "Xuanqing Liu", "authors": "Xuanqing Liu, Cho-Jui Hsieh, Jason D. Lee and Yuekai Sun", "title": "An inexact subsampled proximal Newton-type method for large-scale\n  machine learning", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.NA stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a fast proximal Newton-type algorithm for minimizing regularized\nfinite sums that returns an $\\epsilon$-suboptimal point in\n$\\tilde{\\mathcal{O}}(d(n + \\sqrt{\\kappa d})\\log(\\frac{1}{\\epsilon}))$ FLOPS,\nwhere $n$ is number of samples, $d$ is feature dimension, and $\\kappa$ is the\ncondition number. As long as $n > d$, the proposed method is more efficient\nthan state-of-the-art accelerated stochastic first-order methods for non-smooth\nregularizers which requires $\\tilde{\\mathcal{O}}(d(n + \\sqrt{\\kappa\nn})\\log(\\frac{1}{\\epsilon}))$ FLOPS. The key idea is to form the subsampled\nNewton subproblem in a way that preserves the finite sum structure of the\nobjective, thereby allowing us to leverage recent developments in stochastic\nfirst-order methods to solve the subproblem. Experimental results verify that\nthe proposed algorithm outperforms previous algorithms for $\\ell_1$-regularized\nlogistic regression on real datasets.\n", "versions": [{"version": "v1", "created": "Mon, 28 Aug 2017 22:47:48 GMT"}], "update_date": "2017-08-30", "authors_parsed": [["Liu", "Xuanqing", ""], ["Hsieh", "Cho-Jui", ""], ["Lee", "Jason D.", ""], ["Sun", "Yuekai", ""]]}, {"id": "1708.08587", "submitter": "Shashank Singh", "authors": "Shashank Singh, Barnab\\'as P\\'oczos, and Jian Ma", "title": "On the Reconstruction Risk of Convolutional Sparse Dictionary Learning", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST cs.IT cs.LG math.IT stat.ML stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Sparse dictionary learning (SDL) has become a popular method for adaptively\nidentifying parsimonious representations of a dataset, a fundamental problem in\nmachine learning and signal processing. While most work on SDL assumes a\ntraining dataset of independent and identically distributed samples, a variant\nknown as convolutional sparse dictionary learning (CSDL) relaxes this\nassumption, allowing more general sequential data sources, such as time series\nor other dependent data. Although recent work has explored the statistical\nproperties of classical SDL, the statistical properties of CSDL remain\nunstudied. This paper begins to study this by identifying the minimax\nconvergence rate of CSDL in terms of reconstruction risk, by both upper\nbounding the risk of an established CSDL estimator and proving a matching\ninformation-theoretic lower bound. Our results indicate that consistency in\nreconstruction risk is possible precisely in the `ultra-sparse' setting, in\nwhich the sparsity (i.e., the number of feature occurrences) is in $o(N)$ in\nterms of the length N of the training sequence. Notably, our results make very\nweak assumptions, allowing arbitrary dictionaries and dependent measurement\nnoise. Finally, we verify our theoretical results with numerical experiments on\nsynthetic data.\n", "versions": [{"version": "v1", "created": "Tue, 29 Aug 2017 04:15:39 GMT"}, {"version": "v2", "created": "Sat, 24 Feb 2018 23:49:15 GMT"}], "update_date": "2018-02-27", "authors_parsed": [["Singh", "Shashank", ""], ["P\u00f3czos", "Barnab\u00e1s", ""], ["Ma", "Jian", ""]]}, {"id": "1708.08591", "submitter": "Tanmoy Chakraborty", "authors": "Tanmoy Chakraborty", "title": "EC3: Combining Clustering and Classification for Ensemble Learning", "comments": "14 pages, 7 figures, 11 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Classification and clustering algorithms have been proved to be successful\nindividually in different contexts. Both of them have their own advantages and\nlimitations. For instance, although classification algorithms are more powerful\nthan clustering methods in predicting class labels of objects, they do not\nperform well when there is a lack of sufficient manually labeled reliable data.\nOn the other hand, although clustering algorithms do not produce label\ninformation for objects, they provide supplementary constraints (e.g., if two\nobjects are clustered together, it is more likely that the same label is\nassigned to both of them) that one can leverage for label prediction of a set\nof unknown objects. Therefore, systematic utilization of both these types of\nalgorithms together can lead to better prediction performance. In this paper,\nWe propose a novel algorithm, called EC3 that merges classification and\nclustering together in order to support both binary and multi-class\nclassification. EC3 is based on a principled combination of multiple\nclassification and multiple clustering methods using an optimization function.\nWe theoretically show the convexity and optimality of the problem and solve it\nby block coordinate descent method. We additionally propose iEC3, a variant of\nEC3 that handles imbalanced training data. We perform an extensive experimental\nanalysis by comparing EC3 and iEC3 with 14 baseline methods (7 well-known\nstandalone classifiers, 5 ensemble classifiers, and 2 existing methods that\nmerge classification and clustering) on 13 standard benchmark datasets. We show\nthat our methods outperform other baselines for every single dataset, achieving\nat most 10% higher AUC. Moreover our methods are faster (1.21 times faster than\nthe best baseline), more resilient to noise and class imbalance than the best\nbaseline method.\n", "versions": [{"version": "v1", "created": "Tue, 29 Aug 2017 05:12:10 GMT"}], "update_date": "2017-08-30", "authors_parsed": [["Chakraborty", "Tanmoy", ""]]}, {"id": "1708.08694", "submitter": "Zeyuan Allen-Zhu", "authors": "Zeyuan Allen-Zhu", "title": "Natasha 2: Faster Non-Convex Optimization Than SGD", "comments": "V2 and V3 polished writing; V4 was a deep revision and simplified\n  proofs", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.OC cs.DS cs.LG cs.NE stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We design a stochastic algorithm to train any smooth neural network to\n$\\varepsilon$-approximate local minima, using $O(\\varepsilon^{-3.25})$\nbackpropagations. The best result was essentially $O(\\varepsilon^{-4})$ by SGD.\n  More broadly, it finds $\\varepsilon$-approximate local minima of any smooth\nnonconvex function in rate $O(\\varepsilon^{-3.25})$, with only oracle access to\nstochastic gradients.\n", "versions": [{"version": "v1", "created": "Tue, 29 Aug 2017 10:56:28 GMT"}, {"version": "v2", "created": "Thu, 9 Nov 2017 11:23:34 GMT"}, {"version": "v3", "created": "Fri, 23 Feb 2018 09:40:29 GMT"}, {"version": "v4", "created": "Mon, 11 Jun 2018 10:25:50 GMT"}], "update_date": "2018-06-12", "authors_parsed": [["Allen-Zhu", "Zeyuan", ""]]}, {"id": "1708.08705", "submitter": "Jeremias Sulam", "authors": "Jeremias Sulam, Vardan Papyan, Yaniv Romano, Michael Elad", "title": "Multi-Layer Convolutional Sparse Modeling: Pursuit and Dictionary\n  Learning", "comments": null, "journal-ref": "IEEE Transactions on Signal Processing, vol. 66, no. 15, pp.\n  4090-4104, Aug.1, 1 2018", "doi": "10.1109/TSP.2018.2846226", "report-no": null, "categories": "cs.CV cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The recently proposed Multi-Layer Convolutional Sparse Coding (ML-CSC) model,\nconsisting of a cascade of convolutional sparse layers, provides a new\ninterpretation of Convolutional Neural Networks (CNNs). Under this framework,\nthe computation of the forward pass in a CNN is equivalent to a pursuit\nalgorithm aiming to estimate the nested sparse representation vectors -- or\nfeature maps -- from a given input signal. Despite having served as a pivotal\nconnection between CNNs and sparse modeling, a deeper understanding of the\nML-CSC is still lacking: there are no pursuit algorithms that can serve this\nmodel exactly, nor are there conditions to guarantee a non-empty model. While\none can easily obtain signals that approximately satisfy the ML-CSC\nconstraints, it remains unclear how to simply sample from the model and, more\nimportantly, how one can train the convolutional filters from real data.\n  In this work, we propose a sound pursuit algorithm for the ML-CSC model by\nadopting a projection approach. We provide new and improved bounds on the\nstability of the solution of such pursuit and we analyze different practical\nalternatives to implement this in practice. We show that the training of the\nfilters is essential to allow for non-trivial signals in the model, and we\nderive an online algorithm to learn the dictionaries from real data,\neffectively resulting in cascaded sparse convolutional layers. Last, but not\nleast, we demonstrate the applicability of the ML-CSC model for several\napplications in an unsupervised setting, providing competitive results. Our\nwork represents a bridge between matrix factorization, sparse dictionary\nlearning and sparse auto-encoders, and we analyze these connections in detail.\n", "versions": [{"version": "v1", "created": "Tue, 29 Aug 2017 11:43:40 GMT"}, {"version": "v2", "created": "Sat, 30 Jun 2018 19:46:15 GMT"}], "update_date": "2018-07-03", "authors_parsed": [["Sulam", "Jeremias", ""], ["Papyan", "Vardan", ""], ["Romano", "Yaniv", ""], ["Elad", "Michael", ""]]}, {"id": "1708.08722", "submitter": "Jose M. Pe\\~na", "authors": "Jose M. Pe\\~na", "title": "Unifying DAGs and UGs", "comments": "v2: A factorization property has been added in Section 5. v3: Minor\n  errors corrected. v4: Additional example added. v5: Learning algorithm added.\n  v6: Section 6 added. v7: Appendix B added", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We introduce a new class of graphical models that generalizes\nLauritzen-Wermuth-Frydenberg chain graphs by relaxing the semi-directed\nacyclity constraint so that only directed cycles are forbidden. Moreover, up to\ntwo edges are allowed between any pair of nodes. Specifically, we present\nlocal, pairwise and global Markov properties for the new graphical models and\nprove their equivalence. We also present an equivalent factorization property.\nFinally, we present a causal interpretation of the new models.\n", "versions": [{"version": "v1", "created": "Tue, 29 Aug 2017 12:17:59 GMT"}, {"version": "v2", "created": "Tue, 12 Sep 2017 17:46:13 GMT"}, {"version": "v3", "created": "Tue, 17 Oct 2017 11:43:48 GMT"}, {"version": "v4", "created": "Thu, 14 Dec 2017 15:40:53 GMT"}, {"version": "v5", "created": "Wed, 17 Jan 2018 13:42:29 GMT"}, {"version": "v6", "created": "Tue, 23 Jan 2018 20:57:19 GMT"}, {"version": "v7", "created": "Mon, 19 Feb 2018 09:47:52 GMT"}, {"version": "v8", "created": "Thu, 1 Mar 2018 17:02:21 GMT"}], "update_date": "2018-03-02", "authors_parsed": [["Pe\u00f1a", "Jose M.", ""]]}, {"id": "1708.08732", "submitter": "Maria Brbic", "authors": "Maria Brbic and Ivica Kopriva", "title": "Multi-view Low-rank Sparse Subspace Clustering", "comments": null, "journal-ref": null, "doi": "10.1016/j.patcog.2017.08.024", "report-no": null, "categories": "cs.CV cs.LG math.OC stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Most existing approaches address multi-view subspace clustering problem by\nconstructing the affinity matrix on each view separately and afterwards propose\nhow to extend spectral clustering algorithm to handle multi-view data. This\npaper presents an approach to multi-view subspace clustering that learns a\njoint subspace representation by constructing affinity matrix shared among all\nviews. Relying on the importance of both low-rank and sparsity constraints in\nthe construction of the affinity matrix, we introduce the objective that\nbalances between the agreement across different views, while at the same time\nencourages sparsity and low-rankness of the solution. Related low-rank and\nsparsity constrained optimization problem is for each view solved using the\nalternating direction method of multipliers. Furthermore, we extend our\napproach to cluster data drawn from nonlinear subspaces by solving the\ncorresponding problem in a reproducing kernel Hilbert space. The proposed\nalgorithm outperforms state-of-the-art multi-view subspace clustering\nalgorithms on one synthetic and four real-world datasets.\n", "versions": [{"version": "v1", "created": "Tue, 29 Aug 2017 13:07:56 GMT"}], "update_date": "2017-08-30", "authors_parsed": [["Brbic", "Maria", ""], ["Kopriva", "Ivica", ""]]}, {"id": "1708.08819", "submitter": "Thomas Unterthiner", "authors": "Thomas Unterthiner, Bernhard Nessler, Calvin Seward, G\\\"unter\n  Klambauer, Martin Heusel, Hubert Ramsauer, Sepp Hochreiter", "title": "Coulomb GANs: Provably Optimal Nash Equilibria via Potential Fields", "comments": "Published as a conference paper at ICLR (International Conference on\n  Learning Representations) 2018. Implementation available at\n  https://github.com/bioinf-jku/coulomb_gan", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.GT stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Generative adversarial networks (GANs) evolved into one of the most\nsuccessful unsupervised techniques for generating realistic images. Even though\nit has recently been shown that GAN training converges, GAN models often end up\nin local Nash equilibria that are associated with mode collapse or otherwise\nfail to model the target distribution. We introduce Coulomb GANs, which pose\nthe GAN learning problem as a potential field of charged particles, where\ngenerated samples are attracted to training set samples but repel each other.\nThe discriminator learns a potential field while the generator decreases the\nenergy by moving its samples along the vector (force) field determined by the\ngradient of the potential field. Through decreasing the energy, the GAN model\nlearns to generate samples according to the whole target distribution and does\nnot only cover some of its modes. We prove that Coulomb GANs possess only one\nNash equilibrium which is optimal in the sense that the model distribution\nequals the target distribution. We show the efficacy of Coulomb GANs on a\nvariety of image datasets. On LSUN and celebA, Coulomb GANs set a new state of\nthe art and produce a previously unseen variety of different samples.\n", "versions": [{"version": "v1", "created": "Tue, 29 Aug 2017 15:22:03 GMT"}, {"version": "v2", "created": "Mon, 30 Oct 2017 09:43:53 GMT"}, {"version": "v3", "created": "Tue, 30 Jan 2018 11:54:35 GMT"}], "update_date": "2018-01-31", "authors_parsed": [["Unterthiner", "Thomas", ""], ["Nessler", "Bernhard", ""], ["Seward", "Calvin", ""], ["Klambauer", "G\u00fcnter", ""], ["Heusel", "Martin", ""], ["Ramsauer", "Hubert", ""], ["Hochreiter", "Sepp", ""]]}, {"id": "1708.08826", "submitter": "Mojtaba Kadkhodaie Elyaderani", "authors": "Mojtaba Kadkhodaie Elyaderani and Swayambhoo Jain and Jeffrey Druce\n  and Stefano Gonella and Jarvis Haupt", "title": "Improved Support Recovery Guarantees for the Group Lasso With\n  Applications to Structural Health Monitoring", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IT math.IT stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper considers the problem of estimating an unknown high dimensional\nsignal from noisy linear measurements, {when} the signal is assumed to possess\na \\emph{group-sparse} structure in a {known,} fixed dictionary. We consider\nsignals generated according to a natural probabilistic model, and establish new\nconditions under which the set of indices of the non-zero groups of the signal\n(called the group-level support) may be accurately estimated via the group\nLasso. Our results strengthen existing coherence-based analyses that exhibit\nthe well-known \"square root\" bottleneck, allowing for the number of recoverable\nnonzero groups to be nearly as large as the total number of groups. We also\nestablish a sufficient recovery condition relating the number of nonzero groups\nand the signal to noise ratio (quantified in terms of the ratio of the squared\nEuclidean norms of nonzero groups and the variance of the random additive\n{measurement} noise), and validate this trend empirically. Finally, we examine\nthe implications of our results in the context of a structural health\nmonitoring application, where the group Lasso approach facilitates demixing of\na propagating acoustic wavefield, acquired on the material surface by a\nscanning laser Doppler vibrometer, into antithetical components, one of which\nindicates the locations of internal material defects.\n", "versions": [{"version": "v1", "created": "Tue, 29 Aug 2017 15:34:22 GMT"}, {"version": "v2", "created": "Sat, 19 May 2018 20:33:48 GMT"}], "update_date": "2018-05-22", "authors_parsed": [["Elyaderani", "Mojtaba Kadkhodaie", ""], ["Jain", "Swayambhoo", ""], ["Druce", "Jeffrey", ""], ["Gonella", "Stefano", ""], ["Haupt", "Jarvis", ""]]}, {"id": "1708.08863", "submitter": "Ziv Aharoni", "authors": "Ziv Aharoni, Gal Rattner, Haim Permuter", "title": "Gradual Learning of Recurrent Neural Networks", "comments": "8 pages, 2 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.IT cs.LG math.IT", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recurrent Neural Networks (RNNs) achieve state-of-the-art results in many\nsequence-to-sequence modeling tasks. However, RNNs are difficult to train and\ntend to suffer from overfitting. Motivated by the Data Processing Inequality\n(DPI), we formulate the multi-layered network as a Markov chain, introducing a\ntraining method that comprises training the network gradually and using\nlayer-wise gradient clipping. We found that applying our methods, combined with\npreviously introduced regularization and optimization methods, resulted in\nimprovements in state-of-the-art architectures operating in language modeling\ntasks.\n", "versions": [{"version": "v1", "created": "Tue, 29 Aug 2017 16:18:44 GMT"}, {"version": "v2", "created": "Mon, 21 May 2018 10:22:10 GMT"}], "update_date": "2018-05-24", "authors_parsed": [["Aharoni", "Ziv", ""], ["Rattner", "Gal", ""], ["Permuter", "Haim", ""]]}, {"id": "1708.08917", "submitter": "Caiwen Ding Kevin Ding", "authors": "Caiwen Ding, Siyu Liao, Yanzhi Wang, Zhe Li, Ning Liu, Youwei Zhuo,\n  Chao Wang, Xuehai Qian, Yu Bai, Geng Yuan, Xiaolong Ma, Yipeng Zhang, Jian\n  Tang, Qinru Qiu, Xue Lin, Bo Yuan", "title": "CirCNN: Accelerating and Compressing Deep Neural Networks Using\n  Block-CirculantWeight Matrices", "comments": "14 pages, 15 Figures, conference", "journal-ref": null, "doi": "10.1145/3123939.3124552", "report-no": null, "categories": "cs.CV cs.AI cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Large-scale deep neural networks (DNNs) are both compute and memory\nintensive. As the size of DNNs continues to grow, it is critical to improve the\nenergy efficiency and performance while maintaining accuracy. For DNNs, the\nmodel size is an important factor affecting performance, scalability and energy\nefficiency. Weight pruning achieves good compression ratios but suffers from\nthree drawbacks: 1) the irregular network structure after pruning; 2) the\nincreased training complexity; and 3) the lack of rigorous guarantee of\ncompression ratio and inference accuracy. To overcome these limitations, this\npaper proposes CirCNN, a principled approach to represent weights and process\nneural networks using block-circulant matrices. CirCNN utilizes the Fast\nFourier Transform (FFT)-based fast multiplication, simultaneously reducing the\ncomputational complexity (both in inference and training) from O(n2) to\nO(nlogn) and the storage complexity from O(n2) to O(n), with negligible\naccuracy loss. Compared to other approaches, CirCNN is distinct due to its\nmathematical rigor: it can converge to the same effectiveness as DNNs without\ncompression. The CirCNN architecture, a universal DNN inference engine that can\nbe implemented on various hardware/software platforms with configurable network\narchitecture. To demonstrate the performance and energy efficiency, we test\nCirCNN in FPGA, ASIC and embedded processors. Our results show that CirCNN\narchitecture achieves very high energy efficiency and performance with a small\nhardware footprint. Based on the FPGA implementation and ASIC synthesis\nresults, CirCNN achieves 6-102X energy efficiency improvements compared with\nthe best state-of-the-art results.\n", "versions": [{"version": "v1", "created": "Tue, 29 Aug 2017 04:18:57 GMT"}], "update_date": "2017-09-11", "authors_parsed": [["Ding", "Caiwen", ""], ["Liao", "Siyu", ""], ["Wang", "Yanzhi", ""], ["Li", "Zhe", ""], ["Liu", "Ning", ""], ["Zhuo", "Youwei", ""], ["Wang", "Chao", ""], ["Qian", "Xuehai", ""], ["Bai", "Yu", ""], ["Yuan", "Geng", ""], ["Ma", "Xiaolong", ""], ["Zhang", "Yipeng", ""], ["Tang", "Jian", ""], ["Qiu", "Qinru", ""], ["Lin", "Xue", ""], ["Yuan", "Bo", ""]]}, {"id": "1708.08994", "submitter": "Matteo Ruffini MR", "authors": "Matteo Ruffini, Ricard Gavald\\`a, Esther Lim\\'on", "title": "Clustering Patients with Tensor Decomposition", "comments": "Presented at 2017 Machine Learning for Healthcare Conference (MLHC\n  2017). Boston, MA", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper we present a method for the unsupervised clustering of\nhigh-dimensional binary data, with a special focus on electronic healthcare\nrecords. We present a robust and efficient heuristic to face this problem using\ntensor decomposition. We present the reasons why this approach is preferable\nfor tasks such as clustering patient records, to more commonly used\ndistance-based methods. We run the algorithm on two datasets of healthcare\nrecords, obtaining clinically meaningful results.\n", "versions": [{"version": "v1", "created": "Tue, 29 Aug 2017 19:53:08 GMT"}], "update_date": "2017-08-31", "authors_parsed": [["Ruffini", "Matteo", ""], ["Gavald\u00e0", "Ricard", ""], ["Lim\u00f3n", "Esther", ""]]}, {"id": "1708.09021", "submitter": "Martin Sundin", "authors": "Martin Sundin, Arun Venkitaraman, Magnus Jansson, Saikat Chatterjee", "title": "A Connectedness Constraint for Learning Sparse Graphs", "comments": "5 pages, presented at the European Signal Processing Conference\n  (EUSIPCO) 2017", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML stat.AP", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Graphs are naturally sparse objects that are used to study many problems\ninvolving networks, for example, distributed learning and graph signal\nprocessing. In some cases, the graph is not given, but must be learned from the\nproblem and available data. Often it is desirable to learn sparse graphs.\nHowever, making a graph highly sparse can split the graph into several\ndisconnected components, leading to several separate networks. The main\ndifficulty is that connectedness is often treated as a combinatorial property,\nmaking it hard to enforce in e.g. convex optimization problems. In this\narticle, we show how connectedness of undirected graphs can be formulated as an\nanalytical property and can be enforced as a convex constraint. We especially\nshow how the constraint relates to the distributed consensus problem and graph\nLaplacian learning. Using simulated and real data, we perform experiments to\nlearn sparse and connected graphs from data.\n", "versions": [{"version": "v1", "created": "Tue, 29 Aug 2017 20:45:24 GMT"}], "update_date": "2017-08-31", "authors_parsed": [["Sundin", "Martin", ""], ["Venkitaraman", "Arun", ""], ["Jansson", "Magnus", ""], ["Chatterjee", "Saikat", ""]]}, {"id": "1708.09022", "submitter": "Margarita Osadchy", "authors": "Jinchao Liu, Margarita Osadchy, Lorna Ashton, Michael Foster,\n  Christopher J. Solomon, Stuart J. Gibson", "title": "Deep Convolutional Neural Networks for Raman Spectrum Recognition: A\n  Unified Solution", "comments": null, "journal-ref": null, "doi": "10.1039/C7AN01371J", "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Machine learning methods have found many applications in Raman spectroscopy,\nespecially for the identification of chemical species. However, almost all of\nthese methods require non-trivial preprocessing such as baseline correction\nand/or PCA as an essential step. Here we describe our unified solution for the\nidentification of chemical species in which a convolutional neural network is\ntrained to automatically identify substances according to their Raman spectrum\nwithout the need of ad-hoc preprocessing steps. We evaluated our approach using\nthe RRUFF spectral database, comprising mineral sample data. Superior\nclassification performance is demonstrated compared with other frequently used\nmachine learning algorithms including the popular support vector machine.\n", "versions": [{"version": "v1", "created": "Fri, 18 Aug 2017 14:06:10 GMT"}], "update_date": "2018-02-07", "authors_parsed": [["Liu", "Jinchao", ""], ["Osadchy", "Margarita", ""], ["Ashton", "Lorna", ""], ["Foster", "Michael", ""], ["Solomon", "Christopher J.", ""], ["Gibson", "Stuart J.", ""]]}, {"id": "1708.09072", "submitter": "Toby Lightheart", "authors": "Toby Lightheart, Steven Grainger, Tien-Fu Lu", "title": "Continual One-Shot Learning of Hidden Spike-Patterns with Neural Network\n  Simulation Expansion and STDP Convergence Predictions", "comments": "41 pages, 16 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "q-bio.NC cs.CV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper presents a constructive algorithm that achieves successful\none-shot learning of hidden spike-patterns in a competitive detection task. It\nhas previously been shown (Masquelier et al., 2008) that spike-timing-dependent\nplasticity (STDP) and lateral inhibition can result in neurons competitively\ntuned to repeating spike-patterns concealed in high rates of overall\npresynaptic activity. One-shot construction of neurons with synapse weights\ncalculated as estimates of converged STDP outcomes results in immediate\nselective detection of hidden spike-patterns. The capability of continual\nlearning is demonstrated through the successful one-shot detection of new sets\nof spike-patterns introduced after long intervals in the simulation time.\nSimulation expansion (Lightheart et al., 2013) has been proposed as an approach\nto the development of constructive algorithms that are compatible with\nsimulations of biological neural networks. A simulation of a biological neural\nnetwork may have orders of magnitude fewer neurons and connections than the\nrelated biological neural systems; therefore, simulated neural networks can be\nassumed to be a subset of a larger neural system. The constructive algorithm is\ndeveloped using simulation expansion concepts to perform an operation\nequivalent to the exchange of neurons between the simulation and the larger\nhypothetical neural system. The dynamic selection of neurons to simulate within\na larger neural system (hypothetical or stored in memory) may be a starting\npoint for a wide range of developments and applications in machine learning and\nthe simulation of biology.\n", "versions": [{"version": "v1", "created": "Wed, 30 Aug 2017 01:07:18 GMT"}], "update_date": "2017-08-31", "authors_parsed": [["Lightheart", "Toby", ""], ["Grainger", "Steven", ""], ["Lu", "Tien-Fu", ""]]}, {"id": "1708.09252", "submitter": "Hongteng Xu", "authors": "Hongteng Xu and Hongyuan Zha", "title": "THAP: A Matlab Toolkit for Learning with Hawkes Processes", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  As a powerful tool of asynchronous event sequence analysis, point processes\nhave been studied for a long time and achieved numerous successes in different\nfields. Among various point process models, Hawkes process and its variants\nattract many researchers in statistics and computer science these years because\nthey capture the self- and mutually-triggering patterns between different\nevents in complicated sequences explicitly and quantitatively and are broadly\napplicable to many practical problems. In this paper, we describe an\nopen-source toolkit implementing many learning algorithms and analysis tools\nfor Hawkes process model and its variants. Our toolkit systematically\nsummarizes recent state-of-the-art algorithms as well as most classic\nalgorithms of Hawkes processes, which is beneficial for both academical\neducation and research. Source code can be downloaded from\nhttps://github.com/HongtengXu/Hawkes-Process-Toolkit.\n", "versions": [{"version": "v1", "created": "Mon, 28 Aug 2017 21:07:20 GMT"}], "update_date": "2017-08-31", "authors_parsed": [["Xu", "Hongteng", ""], ["Zha", "Hongyuan", ""]]}, {"id": "1708.09259", "submitter": "Amarjot Singh", "authors": "Amarjot Singh and Nick Kingsbury", "title": "Efficient Convolutional Network Learning using Parametric Log based\n  Dual-Tree Wavelet ScatterNet", "comments": "To Appear in the IEEE International Conference on Computer Vision\n  Workshops (ICCVW) 2017", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a DTCWT ScatterNet Convolutional Neural Network (DTSCNN) formed by\nreplacing the first few layers of a CNN network with a parametric log based\nDTCWT ScatterNet. The ScatterNet extracts edge based invariant representations\nthat are used by the later layers of the CNN to learn high-level features. This\nimproves the training of the network as the later layers can learn more complex\npatterns from the start of learning because the edge representations are\nalready present. The efficient learning of the DTSCNN network is demonstrated\non CIFAR-10 and Caltech-101 datasets. The generic nature of the ScatterNet\nfront-end is shown by an equivalent performance to pre-trained CNN front-ends.\nA comparison with the state-of-the-art on CIFAR-10 and Caltech-101 datasets is\nalso presented.\n", "versions": [{"version": "v1", "created": "Wed, 30 Aug 2017 13:33:19 GMT"}], "update_date": "2017-08-31", "authors_parsed": [["Singh", "Amarjot", ""], ["Kingsbury", "Nick", ""]]}, {"id": "1708.09344", "submitter": "Lovenoor Aulck", "authors": "Lovenoor Aulck, Rohan Aras, Lysia Li, Coulter L'Heureux, Peter Lu,\n  Jevin West", "title": "Stem-ming the Tide: Predicting STEM attrition using student transcript\n  data", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.OT cs.CY physics.ed-ph stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Science, technology, engineering, and math (STEM) fields play growing roles\nin national and international economies by driving innovation and generating\nhigh salary jobs. Yet, the US is lagging behind other highly industrialized\nnations in terms of STEM education and training. Furthermore, many economic\nforecasts predict a rising shortage of domestic STEM-trained professions in the\nUS for years to come. One potential solution to this deficit is to decrease the\nrates at which students leave STEM-related fields in higher education, as\ncurrently over half of all students intending to graduate with a STEM degree\neventually attrite. However, little quantitative research at scale has looked\nat causes of STEM attrition, let alone the use of machine learning to examine\nhow well this phenomenon can be predicted. In this paper, we detail our efforts\nto model and predict dropout from STEM fields using one of the largest known\ndatasets used for research on students at a traditional campus setting. Our\nresults suggest that attrition from STEM fields can be accurately predicted\nwith data that is routinely collected at universities using only information on\nstudents' first academic year. We also propose a method to model student STEM\nintentions for each academic term to better understand the timing of STEM\nattrition events. We believe these results show great promise in using machine\nlearning to improve STEM retention in traditional and non-traditional campus\nsettings.\n", "versions": [{"version": "v1", "created": "Mon, 28 Aug 2017 18:29:52 GMT"}], "update_date": "2017-08-31", "authors_parsed": [["Aulck", "Lovenoor", ""], ["Aras", "Rohan", ""], ["Li", "Lysia", ""], ["L'Heureux", "Coulter", ""], ["Lu", "Peter", ""], ["West", "Jevin", ""]]}, {"id": "1708.09427", "submitter": "Li Shen", "authors": "Li Shen, Laurie R. Margolies, Joseph H. Rothstein, Eugene Fluder,\n  Russell B. McBride, Weiva Sieh", "title": "Deep Learning to Improve Breast Cancer Early Detection on Screening\n  Mammography", "comments": "Major modification with an additional figure and new results", "journal-ref": "Scientific Reports, volume 9, Article number: 12495 (2019)", "doi": "10.1038/s41598-019-48995-4", "report-no": null, "categories": "cs.CV cs.AI stat.ML", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  The rapid development of deep learning, a family of machine learning\ntechniques, has spurred much interest in its application to medical imaging\nproblems. Here, we develop a deep learning algorithm that can accurately detect\nbreast cancer on screening mammograms using an \"end-to-end\" training approach\nthat efficiently leverages training datasets with either complete clinical\nannotation or only the cancer status (label) of the whole image. In this\napproach, lesion annotations are required only in the initial training stage,\nand subsequent stages require only image-level labels, eliminating the reliance\non rarely available lesion annotations. Our all convolutional network method\nfor classifying screening mammograms attained excellent performance in\ncomparison with previous methods. On an independent test set of digitized film\nmammograms from Digital Database for Screening Mammography (DDSM), the best\nsingle model achieved a per-image AUC of 0.88, and four-model averaging\nimproved the AUC to 0.91 (sensitivity: 86.1%, specificity: 80.1%). On a\nvalidation set of full-field digital mammography (FFDM) images from the\nINbreast database, the best single model achieved a per-image AUC of 0.95, and\nfour-model averaging improved the AUC to 0.98 (sensitivity: 86.7%, specificity:\n96.1%). We also demonstrate that a whole image classifier trained using our\nend-to-end approach on the DDSM digitized film mammograms can be transferred to\nINbreast FFDM images using only a subset of the INbreast data for fine-tuning\nand without further reliance on the availability of lesion annotations. These\nfindings show that automatic deep learning methods can be readily trained to\nattain high accuracy on heterogeneous mammography platforms, and hold\ntremendous promise for improving clinical tools to reduce false positive and\nfalse negative screening mammography results.\n", "versions": [{"version": "v1", "created": "Wed, 30 Aug 2017 18:46:16 GMT"}, {"version": "v2", "created": "Fri, 6 Oct 2017 14:40:06 GMT"}, {"version": "v3", "created": "Thu, 19 Oct 2017 02:03:40 GMT"}, {"version": "v4", "created": "Sat, 22 Sep 2018 16:10:10 GMT"}, {"version": "v5", "created": "Mon, 31 Dec 2018 23:23:07 GMT"}], "update_date": "2019-10-08", "authors_parsed": [["Shen", "Li", ""], ["Margolies", "Laurie R.", ""], ["Rothstein", "Joseph H.", ""], ["Fluder", "Eugene", ""], ["McBride", "Russell B.", ""], ["Sieh", "Weiva", ""]]}, {"id": "1708.09441", "submitter": "Shubhomoy Das", "authors": "Shubhomoy Das, Weng-Keen Wong, Alan Fern, Thomas G. Dietterich, Md\n  Amran Siddiqui", "title": "Incorporating Feedback into Tree-based Anomaly Detection", "comments": "8 Pages, KDD 2017 Workshop on Interactive Data Exploration and\n  Analytics (IDEA'17), August 14th, 2017, Halifax, Nova Scotia, Canada", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Anomaly detectors are often used to produce a ranked list of statistical\nanomalies, which are examined by human analysts in order to extract the actual\nanomalies of interest. Unfortunately, in realworld applications, this process\ncan be exceedingly difficult for the analyst since a large fraction of\nhigh-ranking anomalies are false positives and not interesting from the\napplication perspective. In this paper, we aim to make the analyst's job easier\nby allowing for analyst feedback during the investigation process. Ideally, the\nfeedback influences the ranking of the anomaly detector in a way that reduces\nthe number of false positives that must be examined before discovering the\nanomalies of interest. In particular, we introduce a novel technique for\nincorporating simple binary feedback into tree-based anomaly detectors. We\nfocus on the Isolation Forest algorithm as a representative tree-based anomaly\ndetector, and show that we can significantly improve its performance by\nincorporating feedback, when compared with the baseline algorithm that does not\nincorporate feedback. Our technique is simple and scales well as the size of\nthe data increases, which makes it suitable for interactive discovery of\nanomalies in large datasets.\n", "versions": [{"version": "v1", "created": "Wed, 30 Aug 2017 19:36:21 GMT"}], "update_date": "2017-09-01", "authors_parsed": [["Das", "Shubhomoy", ""], ["Wong", "Weng-Keen", ""], ["Fern", "Alan", ""], ["Dietterich", "Thomas G.", ""], ["Siddiqui", "Md Amran", ""]]}, {"id": "1708.09477", "submitter": "Daniel Mckenzie", "authors": "Ming-Jun Lai and Daniel Mckenzie", "title": "A Compressive Sensing Approach to Community Detection with Applications", "comments": "39 pages, 10 figures Version 2, disabled 'showkeys' package. Note\n  that there is an error in the proof of Lemma 5.1. A correct version of this\n  lemma, as well as a greatly improved version of the central algorithm of this\n  paper, is available at: arXiv:1808.05780", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IT cs.LG math.IT stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The community detection problem for graphs asks one to partition the n\nvertices V of a graph G into k communities, or clusters, such that there are\nmany intracluster edges and few intercluster edges. Of course this is\nequivalent to finding a permutation matrix P such that, if A denotes the\nadjacency matrix of G, then PAP^T is approximately block diagonal. As there are\nk^n possible partitions of n vertices into k subsets, directly determining the\noptimal clustering is clearly infeasible. Instead one seeks to solve a more\ntractable approximation to the clustering problem. In this paper we reformulate\nthe community detection problem via sparse solution of a linear system\nassociated with the Laplacian of a graph G and then develop a two-stage\napproach based on a thresholding technique and a compressive sensing algorithm\nto find a sparse solution which corresponds to the community containing a\nvertex of interest in G. Crucially, our approach results in an algorithm which\nis able to find a single cluster of size n_0 in O(nlog(n)n_0) operations and\nall k clusters in fewer than O(n^2ln(n)) operations. This is a marked\nimprovement over the classic spectral clustering algorithm, which is unable to\nfind a single cluster at a time and takes approximately O(n^3) operations to\nfind all k clusters. Moreover, we are able to provide robust guarantees of\nsuccess for the case where G is drawn at random from the Stochastic Block\nModel, a popular model for graphs with clusters. Extensive numerical results\nare also provided, showing the efficacy of our algorithm on both synthetic and\nreal-world data sets.\n", "versions": [{"version": "v1", "created": "Wed, 30 Aug 2017 21:19:30 GMT"}, {"version": "v2", "created": "Fri, 22 Sep 2017 14:22:28 GMT"}, {"version": "v3", "created": "Mon, 20 Aug 2018 13:50:41 GMT"}], "update_date": "2018-08-21", "authors_parsed": [["Lai", "Ming-Jun", ""], ["Mckenzie", "Daniel", ""]]}, {"id": "1708.09479", "submitter": "Salar Fattahi", "authors": "Salar Fattahi, Somayeh Sojoudi", "title": "Graphical Lasso and Thresholding: Equivalence and Closed-form Solutions", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Graphical Lasso (GL) is a popular method for learning the structure of an\nundirected graphical model, which is based on an $l_1$ regularization\ntechnique. The objective of this paper is to compare the computationally-heavy\nGL technique with a numerically-cheap heuristic method that is based on simply\nthresholding the sample covariance matrix. To this end, two notions of\nsign-consistent and inverse-consistent matrices are developed, and then it is\nshown that the thresholding and GL methods are equivalent if: (i) the\nthresholded sample covariance matrix is both sign-consistent and\ninverse-consistent, and (ii) the gap between the largest thresholded and the\nsmallest un-thresholded entries of the sample covariance matrix is not too\nsmall. By building upon this result, it is proved that the GL method---as a\nconic optimization problem---has an explicit closed-form solution if the\nthresholded sample covariance matrix has an acyclic structure. This result is\nthen generalized to arbitrary sparse support graphs, where a formula is found\nto obtain an approximate solution of GL. Furthermore, it is shown that the\napproximation error of the derived explicit formula decreases exponentially\nfast with respect to the length of the minimum-length cycle of the sparsity\ngraph. The developed results are demonstrated on synthetic data, functional MRI\ndata, traffic flows for transportation networks, and massive randomly generated\ndata sets. We show that the proposed method can obtain an accurate\napproximation of the GL for instances with the sizes as large as $80,000\\times\n80,000$ (more than 3.2 billion variables) in less than 30 minutes on a standard\nlaptop computer running MATLAB, while other state-of-the-art methods do not\nconverge within 4 hours.\n", "versions": [{"version": "v1", "created": "Wed, 30 Aug 2017 21:25:21 GMT"}, {"version": "v2", "created": "Sat, 6 Oct 2018 01:01:44 GMT"}, {"version": "v3", "created": "Fri, 28 Jun 2019 22:02:18 GMT"}], "update_date": "2019-07-02", "authors_parsed": [["Fattahi", "Salar", ""], ["Sojoudi", "Somayeh", ""]]}, {"id": "1708.09516", "submitter": "Vikramjit Mitra", "authors": "Vikramjit Mitra and Horacio Franco", "title": "Leveraging Deep Neural Network Activation Entropy to cope with Unseen\n  Data in Speech Recognition", "comments": "7 pages, Index Terms: automatic speech recognition, robust speech\n  recognition, unsupervised adaptation, neural network activations, confidence\n  measures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CL stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Unseen data conditions can inflict serious performance degradation on systems\nrelying on supervised machine learning algorithms. Because data can often be\nunseen, and because traditional machine learning algorithms are trained in a\nsupervised manner, unsupervised adaptation techniques must be used to adapt the\nmodel to the unseen data conditions. However, unsupervised adaptation is often\nchallenging, as one must generate some hypothesis given a model and then use\nthat hypothesis to bootstrap the model to the unseen data conditions.\nUnfortunately, reliability of such hypotheses is often poor, given the mismatch\nbetween the training and testing datasets. In such cases, a model hypothesis\nconfidence measure enables performing data selection for the model adaptation.\nUnderlying this approach is the fact that for unseen data conditions, data\nvariability is introduced to the model, which the model propagates to its\noutput decision, impacting decision reliability. In a fully connected network,\nthis data variability is propagated as distortions from one layer to the next.\nThis work aims to estimate the propagation of such distortion in the form of\nnetwork activation entropy, which is measured over a short- time running window\non the activation from each neuron of a given hidden layer, and these\nmeasurements are then used to compute summary entropy. This work demonstrates\nthat such an entropy measure can help to select data for unsupervised model\nadaptation, resulting in performance gains in speech recognition tasks. Results\nfrom standard benchmark speech recognition tasks show that the proposed\napproach can alleviate the performance degradation experienced under unseen\ndata conditions by iteratively adapting the model to the unseen datas acoustic\ncondition.\n", "versions": [{"version": "v1", "created": "Thu, 31 Aug 2017 01:00:19 GMT"}], "update_date": "2017-09-01", "authors_parsed": [["Mitra", "Vikramjit", ""], ["Franco", "Horacio", ""]]}, {"id": "1708.09708", "submitter": "Harald Oberhauser", "authors": "Terry Lyons, Harald Oberhauser", "title": "Sketching the order of events", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.DS math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We introduce features for massive data streams. These stream features can be\nthought of as \"ordered moments\" and generalize stream sketches from \"moments of\norder one\" to \"ordered moments of arbitrary order\". In analogy to classic\nmoments, they have theoretical guarantees such as universality that are\nimportant for learning algorithms.\n", "versions": [{"version": "v1", "created": "Thu, 31 Aug 2017 13:51:03 GMT"}], "update_date": "2017-09-01", "authors_parsed": [["Lyons", "Terry", ""], ["Oberhauser", "Harald", ""]]}, {"id": "1708.09794", "submitter": "Nihar Shah", "authors": "Nihar B. Shah, Behzad Tabibian, Krikamol Muandet, Isabelle Guyon,\n  Ulrike von Luxburg", "title": "Design and Analysis of the NIPS 2016 Review Process", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DL cs.LG cs.SI stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Neural Information Processing Systems (NIPS) is a top-tier annual conference\nin machine learning. The 2016 edition of the conference comprised more than\n2,400 paper submissions, 3,000 reviewers, and 8,000 attendees. This represents\na growth of nearly 40% in terms of submissions, 96% in terms of reviewers, and\nover 100% in terms of attendees as compared to the previous year. The massive\nscale as well as rapid growth of the conference calls for a thorough quality\nassessment of the peer-review process and novel means of improvement. In this\npaper, we analyze several aspects of the data collected during the review\nprocess, including an experiment investigating the efficacy of collecting\nordinal rankings from reviewers. Our goal is to check the soundness of the\nreview process, and provide insights that may be useful in the design of the\nreview process of subsequent conferences.\n", "versions": [{"version": "v1", "created": "Thu, 31 Aug 2017 16:09:33 GMT"}, {"version": "v2", "created": "Mon, 23 Apr 2018 18:22:09 GMT"}], "update_date": "2018-04-25", "authors_parsed": [["Shah", "Nihar B.", ""], ["Tabibian", "Behzad", ""], ["Muandet", "Krikamol", ""], ["Guyon", "Isabelle", ""], ["von Luxburg", "Ulrike", ""]]}, {"id": "1708.09811", "submitter": "Jaouad Mourtada", "authors": "Jaouad Mourtada and Odalric-Ambrym Maillard", "title": "Efficient tracking of a growing number of experts", "comments": "To appear in Proceedings of the 28th International Conference on\n  Algorithmic Learning Theory (ALT 2017)", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider a variation on the problem of prediction with expert advice,\nwhere new forecasters that were unknown until then may appear at each round. As\noften in prediction with expert advice, designing an algorithm that achieves\nnear-optimal regret guarantees is straightforward, using aggregation of\nexperts. However, when the comparison class is sufficiently rich, for instance\nwhen the best expert and the set of experts itself changes over time, such\nstrategies naively require to maintain a prohibitive number of weights\n(typically exponential with the time horizon). By contrast, designing\nstrategies that both achieve a near-optimal regret and maintain a reasonable\nnumber of weights is highly non-trivial. We consider three increasingly\nchallenging objectives (simple regret, shifting regret and sparse shifting\nregret) that extend existing notions defined for a fixed expert ensemble; in\neach case, we design strategies that achieve tight regret bounds, adaptive to\nthe parameters of the comparison class, while being computationally\ninexpensive. Moreover, our algorithms are anytime, agnostic to the number of\nincoming experts and completely parameter-free. Such remarkable results are\nmade possible thanks to two simple but highly effective recipes: first the\n\"abstention trick\" that comes from the specialist framework and enables to\nhandle the least challenging notions of regret, but is limited when addressing\nmore sophisticated objectives. Second, the \"muting trick\" that we introduce to\ngive more flexibility. We show how to combine these two tricks in order to\nhandle the most challenging class of comparison strategies.\n", "versions": [{"version": "v1", "created": "Thu, 31 Aug 2017 16:45:14 GMT"}], "update_date": "2017-09-01", "authors_parsed": [["Mourtada", "Jaouad", ""], ["Maillard", "Odalric-Ambrym", ""]]}]