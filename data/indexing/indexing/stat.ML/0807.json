[{"id": "0807.1005", "submitter": "Peter Grunwald", "authors": "Tim van Erven, Peter Grunwald and Steven de Rooij", "title": "Catching Up Faster by Switching Sooner: A Prequential Solution to the\n  AIC-BIC Dilemma", "comments": "A preliminary version of a part of this paper appeared at the NIPS\n  2007 conference", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST cs.IT cs.LG math.IT stat.ME stat.ML stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Bayesian model averaging, model selection and its approximations such as BIC\nare generally statistically consistent, but sometimes achieve slower rates og\nconvergence than other methods such as AIC and leave-one-out cross-validation.\nOn the other hand, these other methods can br inconsistent. We identify the\n\"catch-up phenomenon\" as a novel explanation for the slow convergence of\nBayesian methods. Based on this analysis we define the switch distribution, a\nmodification of the Bayesian marginal distribution. We show that, under broad\nconditions,model selection and prediction based on the switch distribution is\nboth consistent and achieves optimal convergence rates, thereby resolving the\nAIC-BIC dilemma. The method is practical; we give an efficient implementation.\nThe switch distribution has a data compression interpretation, and can thus be\nviewed as a \"prequential\" or MDL method; yet it is different from the MDL\nmethods that are usually considered in the literature. We compare the switch\ndistribution to Bayes factor model selection and leave-one-out\ncross-validation.\n", "versions": [{"version": "v1", "created": "Mon, 7 Jul 2008 12:57:23 GMT"}], "update_date": "2008-09-17", "authors_parsed": [["van Erven", "Tim", ""], ["Grunwald", "Peter", ""], ["de Rooij", "Steven", ""]]}, {"id": "0807.2569", "submitter": "Jeffrey Solka", "authors": "Jeffrey Solka", "title": "Text Data Mining: Theory and Methods", "comments": "Published in at http://dx.doi.org/10.1214/07-SS016 the Statistics\n  Surveys (http://www.i-journals.org/ss/) by the Institute of Mathematical\n  Statistics (http://www.imstat.org)", "journal-ref": "Statistics Surveys 2008, Vol. 2, 94-112", "doi": "10.1214/07-SS016", "report-no": "IMS-SS-SS_2007_16", "categories": "stat.ML cs.IR stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper provides the reader with a very brief introduction to some of the\ntheory and methods of text data mining. The intent of this article is to\nintroduce the reader to some of the current methodologies that are employed\nwithin this discipline area while at the same time making the reader aware of\nsome of the interesting challenges that remain to be solved within the area.\nFinally, the articles serves as a very rudimentary tutorial on some of\ntechniques while also providing the reader with a list of references for\nadditional study.\n", "versions": [{"version": "v1", "created": "Wed, 16 Jul 2008 13:39:32 GMT"}], "update_date": "2008-07-17", "authors_parsed": [["Solka", "Jeffrey", ""]]}, {"id": "0807.3423", "submitter": "De Vito Ernesto", "authors": "C. De Mol, E. De Vito, L. Rosasco", "title": "Elastic-Net Regularization in Learning Theory", "comments": "32 pages, 3 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Within the framework of statistical learning theory we analyze in detail the\nso-called elastic-net regularization scheme proposed by Zou and Hastie for the\nselection of groups of correlated variables. To investigate on the statistical\nproperties of this scheme and in particular on its consistency properties, we\nset up a suitable mathematical framework. Our setting is random-design\nregression where we allow the response variable to be vector-valued and we\nconsider prediction functions which are linear combination of elements ({\\em\nfeatures}) in an infinite-dimensional dictionary. Under the assumption that the\nregression function admits a sparse representation on the dictionary, we prove\nthat there exists a particular ``{\\em elastic-net representation}'' of the\nregression function such that, if the number of data increases, the elastic-net\nestimator is consistent not only for prediction but also for variable/feature\nselection. Our results include finite-sample bounds and an adaptive scheme to\nselect the regularization parameter. Moreover, using convex analysis tools, we\nderive an iterative thresholding algorithm for computing the elastic-net\nsolution which is different from the optimization procedure originally proposed\nby Zou and Hastie\n", "versions": [{"version": "v1", "created": "Tue, 22 Jul 2008 09:07:26 GMT"}], "update_date": "2008-07-23", "authors_parsed": [["De Mol", "C.", ""], ["De Vito", "E.", ""], ["Rosasco", "L.", ""]]}, {"id": "0807.3470", "submitter": "Jarkko Saloj\\\"arvi", "authors": "Jarkko Saloj\\\"arvi, Kai Puolam\\\"aki, Eerika Savia, Samuel Kaski", "title": "Inference with Discriminative Posterior", "comments": "28 pages, 3 figures, submitted to a journal", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study Bayesian discriminative inference given a model family $p(c,\\x,\n\\theta)$ that is assumed to contain all our prior information but still known\nto be incorrect. This falls in between \"standard\" Bayesian generative modeling\nand Bayesian regression, where the margin $p(\\x,\\theta)$ is known to be\nuninformative about $p(c|\\x,\\theta)$. We give an axiomatic proof that\ndiscriminative posterior is consistent for conditional inference; using the\ndiscriminative posterior is standard practice in classical Bayesian regression,\nbut we show that it is theoretically justified for model families of joint\ndensities as well. A practical benefit compared to Bayesian regression is that\nthe standard methods of handling missing values in generative modeling can be\nextended into discriminative inference, which is useful if the amount of data\nis small. Compared to standard generative modeling, discriminative posterior\nresults in better conditional inference if the model family is incorrect. If\nthe model family contains also the true model, the discriminative posterior\ngives the same result as standard Bayesian generative modeling. Practical\ncomputation is done with Markov chain Monte Carlo.\n", "versions": [{"version": "v1", "created": "Tue, 22 Jul 2008 13:20:14 GMT"}, {"version": "v2", "created": "Tue, 18 Nov 2008 11:17:47 GMT"}], "update_date": "2008-11-18", "authors_parsed": [["Saloj\u00e4rvi", "Jarkko", ""], ["Puolam\u00e4ki", "Kai", ""], ["Savia", "Eerika", ""], ["Kaski", "Samuel", ""]]}, {"id": "0807.3719", "submitter": "Tao Shi", "authors": "Tao Shi, Mikhail Belkin, Bin Yu", "title": "Data spectroscopy: Eigenspaces of convolution operators and clustering", "comments": "Published in at http://dx.doi.org/10.1214/09-AOS700 the Annals of\n  Statistics (http://www.imstat.org/aos/) by the Institute of Mathematical\n  Statistics (http://www.imstat.org)", "journal-ref": "Annals of Statistics 2009, Vol. 37, No. 6B, 3960-3984", "doi": "10.1214/09-AOS700", "report-no": "IMS-AOS-AOS700", "categories": "stat.ML stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper focuses on obtaining clustering information about a distribution\nfrom its i.i.d. samples. We develop theoretical results to understand and use\nclustering information contained in the eigenvectors of data adjacency matrices\nbased on a radial kernel function with a sufficiently fast tail decay. In\nparticular, we provide population analyses to gain insights into which\neigenvectors should be used and when the clustering information for the\ndistribution can be recovered from the sample. We learn that a fixed number of\ntop eigenvectors might at the same time contain redundant clustering\ninformation and miss relevant clustering information. We use this insight to\ndesign the data spectroscopic clustering (DaSpec) algorithm that utilizes\nproperly selected eigenvectors to determine the number of clusters\nautomatically and to group the data accordingly. Our findings extend the\nintuitions underlying existing spectral techniques such as spectral clustering\nand Kernel Principal Components Analysis, and provide new understanding into\ntheir usability and modes of failure. Simulation studies and experiments on\nreal-world data are conducted to show the potential of our algorithm. In\nparticular, DaSpec is found to handle unbalanced groups and recover clusters of\ndifferent shapes better than the competing methods.\n", "versions": [{"version": "v1", "created": "Wed, 23 Jul 2008 17:33:29 GMT"}, {"version": "v2", "created": "Fri, 20 Nov 2009 09:42:20 GMT"}], "update_date": "2009-11-20", "authors_parsed": [["Shi", "Tao", ""], ["Belkin", "Mikhail", ""], ["Yu", "Bin", ""]]}]