[{"id": "0706.0534", "submitter": "Shuheng Zhou", "authors": "Shuheng Zhou, John Lafferty, Larry Wasserman", "title": "Compressed Regression", "comments": "59 pages, 5 figure, Submitted for review", "journal-ref": "IEEE Transactions on Information Theory, Volume 55, No.2, pp\n  846--866, 2009", "doi": null, "report-no": null, "categories": "stat.ML cs.IT math.IT", "license": null, "abstract": "  Recent research has studied the role of sparsity in high dimensional\nregression and signal reconstruction, establishing theoretical limits for\nrecovering sparse models from sparse data. This line of work shows that\n$\\ell_1$-regularized least squares regression can accurately estimate a sparse\nlinear model from $n$ noisy examples in $p$ dimensions, even if $p$ is much\nlarger than $n$. In this paper we study a variant of this problem where the\noriginal $n$ input variables are compressed by a random linear transformation\nto $m \\ll n$ examples in $p$ dimensions, and establish conditions under which a\nsparse linear model can be successfully recovered from the compressed data. A\nprimary motivation for this compression procedure is to anonymize the data and\npreserve privacy by revealing little information about the original data. We\ncharacterize the number of random projections that are required for\n$\\ell_1$-regularized compressed regression to identify the nonzero coefficients\nin the true model with probability approaching one, a property called\n``sparsistence.'' In addition, we show that $\\ell_1$-regularized compressed\nregression asymptotically predicts as well as an oracle linear model, a\nproperty called ``persistence.'' Finally, we characterize the privacy\nproperties of the compression procedure in information-theoretic terms,\nestablishing upper bounds on the mutual information between the compressed and\nuncompressed data that decay to zero.\n", "versions": [{"version": "v1", "created": "Mon, 4 Jun 2007 20:42:54 GMT"}, {"version": "v2", "created": "Fri, 11 Jan 2008 06:33:58 GMT"}], "update_date": "2012-01-11", "authors_parsed": [["Zhou", "Shuheng", ""], ["Lafferty", "John", ""], ["Wasserman", "Larry", ""]]}, {"id": "0706.2040", "submitter": "Edoardo Airoldi", "authors": "Edoardo M Airoldi", "title": "Getting started in probabilistic graphical models", "comments": "12 pages, 1 figure", "journal-ref": "Airoldi EM (2007) Getting started in probabilistic graphical\n  models. PLoS Comput Biol 3(12): e252", "doi": "10.1371/journal.pcbi.0030252", "report-no": null, "categories": "q-bio.QM cs.LG physics.soc-ph stat.ME stat.ML", "license": null, "abstract": "  Probabilistic graphical models (PGMs) have become a popular tool for\ncomputational analysis of biological data in a variety of domains. But, what\nexactly are they and how do they work? How can we use PGMs to discover patterns\nthat are biologically relevant? And to what extent can PGMs help us formulate\nnew hypotheses that are testable at the bench? This note sketches out some\nanswers and illustrates the main ideas behind the statistical approach to\nbiological pattern discovery.\n", "versions": [{"version": "v1", "created": "Thu, 14 Jun 2007 14:52:06 GMT"}, {"version": "v2", "created": "Sat, 10 Nov 2007 19:25:59 GMT"}], "update_date": "2010-02-22", "authors_parsed": [["Airoldi", "Edoardo M", ""]]}, {"id": "0706.3188", "submitter": "Vladimir Vovk", "authors": "Glenn Shafer and Vladimir Vovk", "title": "A tutorial on conformal prediction", "comments": "58 pages, 9 figures", "journal-ref": "Journal of Machine Learning Research 9 (2008) 371-421.\n  http://www.jmlr.org/papers/v9/shafer08a.html", "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": null, "abstract": "  Conformal prediction uses past experience to determine precise levels of\nconfidence in new predictions. Given an error probability $\\epsilon$, together\nwith a method that makes a prediction $\\hat{y}$ of a label $y$, it produces a\nset of labels, typically containing $\\hat{y}$, that also contains $y$ with\nprobability $1-\\epsilon$. Conformal prediction can be applied to any method for\nproducing $\\hat{y}$: a nearest-neighbor method, a support-vector machine, ridge\nregression, etc.\n  Conformal prediction is designed for an on-line setting in which labels are\npredicted successively, each one being revealed before the next is predicted.\nThe most novel and valuable feature of conformal prediction is that if the\nsuccessive examples are sampled independently from the same distribution, then\nthe successive predictions will be right $1-\\epsilon$ of the time, even though\nthey are based on an accumulating dataset rather than on independent datasets.\n  In addition to the model under which successive examples are sampled\nindependently, other on-line compression models can also use conformal\nprediction. The widely used Gaussian linear model is one of these.\n  This tutorial presents a self-contained account of the theory of conformal\nprediction and works through several numerical examples. A more comprehensive\ntreatment of the topic is provided in \"Algorithmic Learning in a Random World\",\nby Vladimir Vovk, Alex Gammerman, and Glenn Shafer (Springer, 2005).\n", "versions": [{"version": "v1", "created": "Thu, 21 Jun 2007 16:40:06 GMT"}], "update_date": "2019-04-23", "authors_parsed": [["Shafer", "Glenn", ""], ["Vovk", "Vladimir", ""]]}, {"id": "0706.3434", "submitter": "Shuheng Zhou", "authors": "Avrim Blum, Amin Coja-Oghlan, Alan Frieze, Shuheng Zhou", "title": "Separating populations with wide data: A spectral analysis", "comments": "Published in at http://dx.doi.org/10.1214/08-EJS289 the Electronic\n  Journal of Statistics (http://www.i-journals.org/ejs/) by the Institute of\n  Mathematical Statistics (http://www.imstat.org)", "journal-ref": "Electronic Journal of Statistics 2009, Vol. 3, 76-113", "doi": "10.1214/08-EJS289", "report-no": "IMS-EJS-EJS_2008_289", "categories": "stat.ML stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we consider the problem of partitioning a small data sample\ndrawn from a mixture of $k$ product distributions. We are interested in the\ncase that individual features are of low average quality $\\gamma$, and we want\nto use as few of them as possible to correctly partition the sample. We analyze\na spectral technique that is able to approximately optimize the total data\nsize--the product of number of data points $n$ and the number of features\n$K$--needed to correctly perform this partitioning as a function of $1/\\gamma$\nfor $K>n$. Our goal is motivated by an application in clustering individuals\naccording to their population of origin using markers, when the divergence\nbetween any two of the populations is small.\n", "versions": [{"version": "v1", "created": "Mon, 25 Jun 2007 08:03:25 GMT"}, {"version": "v2", "created": "Thu, 29 Jan 2009 11:31:54 GMT"}], "update_date": "2017-11-17", "authors_parsed": [["Blum", "Avrim", ""], ["Coja-Oghlan", "Amin", ""], ["Frieze", "Alan", ""], ["Zhou", "Shuheng", ""]]}, {"id": "0706.3499", "submitter": "Bharath Sriperumbudur", "authors": "Bharath K. Sriperumbudur and Gert R. G. Lanckriet", "title": "Metric Embedding for Nearest Neighbor Classification", "comments": "9 pages, 1 table", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML", "license": null, "abstract": "  The distance metric plays an important role in nearest neighbor (NN)\nclassification. Usually the Euclidean distance metric is assumed or a\nMahalanobis distance metric is optimized to improve the NN performance. In this\npaper, we study the problem of embedding arbitrary metric spaces into a\nEuclidean space with the goal to improve the accuracy of the NN classifier. We\npropose a solution by appealing to the framework of regularization in a\nreproducing kernel Hilbert space and prove a representer-like theorem for NN\nclassification. The embedding function is then determined by solving a\nsemidefinite program which has an interesting connection to the soft-margin\nlinear binary support vector machine classifier. Although the main focus of\nthis paper is to present a general, theoretical framework for metric embedding\nin a NN setting, we demonstrate the performance of the proposed method on some\nbenchmark datasets and show that it performs better than the Mahalanobis metric\nlearning algorithm in terms of leave-one-out and generalization errors.\n", "versions": [{"version": "v1", "created": "Sun, 24 Jun 2007 06:50:24 GMT"}], "update_date": "2007-06-26", "authors_parsed": [["Sriperumbudur", "Bharath K.", ""], ["Lanckriet", "Gert R. G.", ""]]}]