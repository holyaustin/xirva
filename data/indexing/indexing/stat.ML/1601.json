[{"id": "1601.00024", "submitter": "Ashish Sabharwal", "authors": "Ashish Sabharwal, Horst Samulowitz, Gerald Tesauro", "title": "Selecting Near-Optimal Learners via Incremental Data Allocation", "comments": "AAAI-2016: The Thirtieth AAAI Conference on Artificial Intelligence", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study a novel machine learning (ML) problem setting of sequentially\nallocating small subsets of training data amongst a large set of classifiers.\nThe goal is to select a classifier that will give near-optimal accuracy when\ntrained on all data, while also minimizing the cost of misallocated samples.\nThis is motivated by large modern datasets and ML toolkits with many\ncombinations of learning algorithms and hyper-parameters. Inspired by the\nprinciple of \"optimism under uncertainty,\" we propose an innovative strategy,\nData Allocation using Upper Bounds (DAUB), which robustly achieves these\nobjectives across a variety of real-world datasets.\n  We further develop substantial theoretical support for DAUB in an idealized\nsetting where the expected accuracy of a classifier trained on $n$ samples can\nbe known exactly. Under these conditions we establish a rigorous sub-linear\nbound on the regret of the approach (in terms of misallocated data), as well as\na rigorous bound on suboptimality of the selected classifier. Our accuracy\nestimates using real-world datasets only entail mild violations of the\ntheoretical scenario, suggesting that the practical behavior of DAUB is likely\nto approach the idealized behavior.\n", "versions": [{"version": "v1", "created": "Thu, 31 Dec 2015 22:19:09 GMT"}], "update_date": "2016-01-05", "authors_parsed": [["Sabharwal", "Ashish", ""], ["Samulowitz", "Horst", ""], ["Tesauro", "Gerald", ""]]}, {"id": "1601.00034", "submitter": "Siamak Ravanbakhsh", "authors": "Siamak Ravanbakhsh, Barnabas Poczos, Jeff Schneider, Dale Schuurmans,\n  Russell Greiner", "title": "Stochastic Neural Networks with Monotonic Activation Functions", "comments": "AISTATS 2016", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a Laplace approximation that creates a stochastic unit from any\nsmooth monotonic activation function, using only Gaussian noise. This paper\ninvestigates the application of this stochastic approximation in training a\nfamily of Restricted Boltzmann Machines (RBM) that are closely linked to\nBregman divergences. This family, that we call exponential family RBM\n(Exp-RBM), is a subset of the exponential family Harmoniums that expresses\nfamily members through a choice of smooth monotonic non-linearity for each\nneuron. Using contrastive divergence along with our Gaussian approximation, we\nshow that Exp-RBM can learn useful representations using novel stochastic\nunits.\n", "versions": [{"version": "v1", "created": "Fri, 1 Jan 2016 00:47:29 GMT"}, {"version": "v2", "created": "Thu, 14 Apr 2016 15:38:53 GMT"}, {"version": "v3", "created": "Thu, 21 Jul 2016 15:52:02 GMT"}, {"version": "v4", "created": "Fri, 22 Jul 2016 17:38:18 GMT"}], "update_date": "2016-07-25", "authors_parsed": [["Ravanbakhsh", "Siamak", ""], ["Poczos", "Barnabas", ""], ["Schneider", "Jeff", ""], ["Schuurmans", "Dale", ""], ["Greiner", "Russell", ""]]}, {"id": "1601.00062", "submitter": "Hao-Jun Shi", "authors": "Jerry Luo, Kayla Shapiro, Hao-Jun Michael Shi, Qi Yang, and Kan Zhu", "title": "Practical Algorithms for Learning Near-Isometric Linear Embeddings", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG math.OC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose two practical non-convex approaches for learning near-isometric,\nlinear embeddings of finite sets of data points. Given a set of training points\n$\\mathcal{X}$, we consider the secant set $S(\\mathcal{X})$ that consists of all\npairwise difference vectors of $\\mathcal{X}$, normalized to lie on the unit\nsphere. The problem can be formulated as finding a symmetric and positive\nsemi-definite matrix $\\boldsymbol{\\Psi}$ that preserves the norms of all the\nvectors in $S(\\mathcal{X})$ up to a distortion parameter $\\delta$. Motivated by\nnon-negative matrix factorization, we reformulate our problem into a Frobenius\nnorm minimization problem, which is solved by the Alternating Direction Method\nof Multipliers (ADMM) and develop an algorithm, FroMax. Another method solves\nfor a projection matrix $\\boldsymbol{\\Psi}$ by minimizing the restricted\nisometry property (RIP) directly over the set of symmetric, postive\nsemi-definite matrices. Applying ADMM and a Moreau decomposition on a proximal\nmapping, we develop another algorithm, NILE-Pro, for dimensionality reduction.\nFroMax is shown to converge faster for smaller $\\delta$ while NILE-Pro\nconverges faster for larger $\\delta$. Both non-convex approaches are then\nempirically demonstrated to be more computationally efficient than prior convex\napproaches for a number of applications in machine learning and signal\nprocessing.\n", "versions": [{"version": "v1", "created": "Fri, 1 Jan 2016 09:06:11 GMT"}, {"version": "v2", "created": "Fri, 22 Apr 2016 21:47:56 GMT"}], "update_date": "2016-04-26", "authors_parsed": [["Luo", "Jerry", ""], ["Shapiro", "Kayla", ""], ["Shi", "Hao-Jun Michael", ""], ["Yang", "Qi", ""], ["Zhu", "Kan", ""]]}, {"id": "1601.00142", "submitter": "Ali Shojaie", "authors": "Takumi Saegusa and Ali Shojaie", "title": "Joint Estimation of Precision Matrices in Heterogeneous Populations", "comments": "55 pages, 5 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We introduce a general framework for estimation of inverse covariance, or\nprecision, matrices from heterogeneous populations. The proposed framework uses\na Laplacian shrinkage penalty to encourage similarity among estimates from\ndisparate, but related, subpopulations, while allowing for differences among\nmatrices. We propose an efficient alternating direction method of multipliers\n(ADMM) algorithm for parameter estimation, as well as its extension for faster\ncomputation in high dimensions by thresholding the empirical covariance matrix\nto identify the joint block diagonal structure in the estimated precision\nmatrices. We establish both variable selection and norm consistency of the\nproposed estimator for distributions with exponential or polynomial tails.\nFurther, to extend the applicability of the method to the settings with unknown\npopulations structure, we propose a Laplacian penalty based on hierarchical\nclustering, and discuss conditions under which this data-driven choice results\nin consistent estimation of precision matrices in heterogenous populations.\nExtensive numerical studies and applications to gene expression data from\nsubtypes of cancer with distinct clinical outcomes indicate the potential\nadvantages of the proposed method over existing approaches.\n", "versions": [{"version": "v1", "created": "Sat, 2 Jan 2016 06:11:06 GMT"}], "update_date": "2016-01-05", "authors_parsed": [["Saegusa", "Takumi", ""], ["Shojaie", "Ali", ""]]}, {"id": "1601.00236", "submitter": "Chetan Tonde", "authors": "Praneeth Vepakomma and Chetan Tonde and Ahmed Elgammal", "title": "Supervised Dimensionality Reduction via Distance Correlation\n  Maximization", "comments": "23 pages, 6 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In our work, we propose a novel formulation for supervised dimensionality\nreduction based on a nonlinear dependency criterion called Statistical Distance\nCorrelation, Szekely et. al. (2007). We propose an objective which is free of\ndistributional assumptions on regression variables and regression model\nassumptions. Our proposed formulation is based on learning a low-dimensional\nfeature representation $\\mathbf{z}$, which maximizes the squared sum of\nDistance Correlations between low dimensional features $\\mathbf{z}$ and\nresponse $y$, and also between features $\\mathbf{z}$ and covariates\n$\\mathbf{x}$. We propose a novel algorithm to optimize our proposed objective\nusing the Generalized Minimization Maximizaiton method of \\Parizi et. al.\n(2015). We show superior empirical results on multiple datasets proving the\neffectiveness of our proposed approach over several relevant state-of-the-art\nsupervised dimensionality reduction methods.\n", "versions": [{"version": "v1", "created": "Sun, 3 Jan 2016 00:14:23 GMT"}], "update_date": "2016-01-05", "authors_parsed": [["Vepakomma", "Praneeth", ""], ["Tonde", "Chetan", ""], ["Elgammal", "Ahmed", ""]]}, {"id": "1601.00238", "submitter": "Dacheng Tao", "authors": "Tongliang Liu, Dacheng Tao, and Dong Xu", "title": "Dimensionality-Dependent Generalization Bounds for $k$-Dimensional\n  Coding Schemes", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The $k$-dimensional coding schemes refer to a collection of methods that\nattempt to represent data using a set of representative $k$-dimensional\nvectors, and include non-negative matrix factorization, dictionary learning,\nsparse coding, $k$-means clustering and vector quantization as special cases.\nPrevious generalization bounds for the reconstruction error of the\n$k$-dimensional coding schemes are mainly dimensionality independent. A major\nadvantage of these bounds is that they can be used to analyze the\ngeneralization error when data is mapped into an infinite- or high-dimensional\nfeature space. However, many applications use finite-dimensional data features.\nCan we obtain dimensionality-dependent generalization bounds for\n$k$-dimensional coding schemes that are tighter than dimensionality-independent\nbounds when data is in a finite-dimensional feature space? The answer is\npositive. In this paper, we address this problem and derive a\ndimensionality-dependent generalization bound for $k$-dimensional coding\nschemes by bounding the covering number of the loss function class induced by\nthe reconstruction error. The bound is of order\n$\\mathcal{O}\\left(\\left(mk\\ln(mkn)/n\\right)^{\\lambda_n}\\right)$, where $m$ is\nthe dimension of features, $k$ is the number of the columns in the linear\nimplementation of coding schemes, $n$ is the size of sample, $\\lambda_n>0.5$\nwhen $n$ is finite and $\\lambda_n=0.5$ when $n$ is infinite. We show that our\nbound can be tighter than previous results, because it avoids inducing the\nworst-case upper bound on $k$ of the loss function and converges faster. The\nproposed generalization bound is also applied to some specific coding schemes\nto demonstrate that the dimensionality-dependent bound is an indispensable\ncomplement to these dimensionality-independent generalization bounds.\n", "versions": [{"version": "v1", "created": "Sun, 3 Jan 2016 01:17:04 GMT"}, {"version": "v2", "created": "Sat, 23 Apr 2016 08:13:29 GMT"}], "update_date": "2016-04-26", "authors_parsed": [["Liu", "Tongliang", ""], ["Tao", "Dacheng", ""], ["Xu", "Dong", ""]]}, {"id": "1601.00350", "submitter": "Mehdi Korki", "authors": "Hadi Zayyani, Mehdi Korki, Farrokh Marvasti", "title": "Sparse Diffusion Steepest-Descent for One Bit Compressed Sensing in\n  Wireless Sensor Networks", "comments": "4 pages, 3 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.IT cs.LG math.IT", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This letter proposes a sparse diffusion steepest-descent algorithm for one\nbit compressed sensing in wireless sensor networks. The approach exploits the\ndiffusion strategy from distributed learning in the one bit compressed sensing\nframework. To estimate a common sparse vector cooperatively from only the sign\nof measurements, steepest-descent is used to minimize the suitable global and\nlocal convex cost functions. A diffusion strategy is suggested for distributive\nlearning of the sparse vector. Simulation results show the effectiveness of the\nproposed distributed algorithm compared to the state-of-the-art non\ndistributive algorithms in the one bit compressed sensing framework.\n", "versions": [{"version": "v1", "created": "Sun, 3 Jan 2016 23:03:09 GMT"}], "update_date": "2016-01-05", "authors_parsed": [["Zayyani", "Hadi", ""], ["Korki", "Mehdi", ""], ["Marvasti", "Farrokh", ""]]}, {"id": "1601.00393", "submitter": "Hao Zhang", "authors": "Jincheng Mei, Hao Zhang, Bao-Liang Lu", "title": "On the Reducibility of Submodular Functions", "comments": "To appear in AISTATS 2016", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The scalability of submodular optimization methods is critical for their\nusability in practice. In this paper, we study the reducibility of submodular\nfunctions, a property that enables us to reduce the solution space of\nsubmodular optimization problems without performance loss. We introduce the\nconcept of reducibility using marginal gains. Then we show that by adding\nperturbation, we can endow irreducible functions with reducibility, based on\nwhich we propose the perturbation-reduction optimization framework. Our\ntheoretical analysis proves that given the perturbation scales, the\nreducibility gain could be computed, and the performance loss has additive\nupper bounds. We further conduct empirical studies and the results demonstrate\nthat our proposed framework significantly accelerates existing optimization\nmethods for irreducible submodular functions with a cost of only small\nperformance losses.\n", "versions": [{"version": "v1", "created": "Mon, 4 Jan 2016 07:16:35 GMT"}], "update_date": "2016-01-05", "authors_parsed": [["Mei", "Jincheng", ""], ["Zhang", "Hao", ""], ["Lu", "Bao-Liang", ""]]}, {"id": "1601.00449", "submitter": "Massimiliano Pontil", "authors": "Andrew M. McDonald, Massimiliano Pontil, Dimitris Stamos", "title": "Fitting Spectral Decay with the $k$-Support Norm", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The spectral $k$-support norm enjoys good estimation properties in low rank\nmatrix learning problems, empirically outperforming the trace norm. Its unit\nball is the convex hull of rank $k$ matrices with unit Frobenius norm. In this\npaper we generalize the norm to the spectral $(k,p)$-support norm, whose\nadditional parameter $p$ can be used to tailor the norm to the decay of the\nspectrum of the underlying model. We characterize the unit ball and we\nexplicitly compute the norm. We further provide a conditional gradient method\nto solve regularization problems with the norm, and we derive an efficient\nalgorithm to compute the Euclidean projection on the unit ball in the case\n$p=\\infty$. In numerical experiments, we show that allowing $p$ to vary\nsignificantly improves performance over the spectral $k$-support norm on\nvarious matrix completion benchmarks, and better captures the spectral decay of\nthe underlying model.\n", "versions": [{"version": "v1", "created": "Mon, 4 Jan 2016 10:48:29 GMT"}], "update_date": "2016-01-05", "authors_parsed": [["McDonald", "Andrew M.", ""], ["Pontil", "Massimiliano", ""], ["Stamos", "Dimitris", ""]]}, {"id": "1601.00496", "submitter": "S{\\o}ren F{\\o}ns Vind Nielsen", "authors": "S{\\o}ren F. V. Nielsen and Kristoffer H. Madsen and Rasmus R{\\o}ge and\n  Mikkel N. Schmidt and Morten M{\\o}rup", "title": "Nonparametric Modeling of Dynamic Functional Connectivity in fMRI Data", "comments": "8 pages, 1 figure. Presented at the Machine Learning and\n  Interpretation in Neuroimaging Workshop (MLINI-2015), 2015 (arXiv:1605.04435)", "journal-ref": null, "doi": null, "report-no": "MLINI/2015/08", "categories": "stat.AP q-bio.NC stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Dynamic functional connectivity (FC) has in recent years become a topic of\ninterest in the neuroimaging community. Several models and methods exist for\nboth functional magnetic resonance imaging (fMRI) and electroencephalography\n(EEG), and the results point towards the conclusion that FC exhibits dynamic\nchanges. The existing approaches modeling dynamic connectivity have primarily\nbeen based on time-windowing the data and k-means clustering. We propose a\nnon-parametric generative model for dynamic FC in fMRI that does not rely on\nspecifying window lengths and number of dynamic states. Rooted in Bayesian\nstatistical modeling we use the predictive likelihood to investigate if the\nmodel can discriminate between a motor task and rest both within and across\nsubjects. We further investigate what drives dynamic states using the model on\nthe entire data collated across subjects and task/rest. We find that the number\nof states extracted are driven by subject variability and preprocessing\ndifferences while the individual states are almost purely defined by either\ntask or rest. This questions how we in general interpret dynamic FC and points\nto the need for more research on what drives dynamic FC.\n", "versions": [{"version": "v1", "created": "Mon, 4 Jan 2016 13:24:45 GMT"}, {"version": "v2", "created": "Wed, 8 Jun 2016 07:42:53 GMT"}], "update_date": "2016-06-09", "authors_parsed": [["Nielsen", "S\u00f8ren F. V.", ""], ["Madsen", "Kristoffer H.", ""], ["R\u00f8ge", "Rasmus", ""], ["Schmidt", "Mikkel N.", ""], ["M\u00f8rup", "Morten", ""]]}, {"id": "1601.00504", "submitter": "Alexandra Carpentier", "authors": "Alexandra Carpentier, Teresa Schlueter", "title": "Learning relationships between data obtained independently", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The aim of this paper is to provide a new method for learning the\nrelationships between data that have been obtained independently. Unlike\nexisting methods like matching, the proposed technique does not require any\ncontextual information, provided that the dependency between the variables of\ninterest is monotone. It can therefore be easily combined with matching in\norder to exploit the advantages of both methods. This technique can be\ndescribed as a mix between quantile matching, and deconvolution. We provide for\nit a theoretical and an empirical validation.\n", "versions": [{"version": "v1", "created": "Mon, 4 Jan 2016 13:52:49 GMT"}], "update_date": "2016-01-05", "authors_parsed": [["Carpentier", "Alexandra", ""], ["Schlueter", "Teresa", ""]]}, {"id": "1601.00595", "submitter": "George Papageorgiou", "authors": "George Papageorgiou, Pantelis Bouboulis and Sergios Theodoridis", "title": "Robust Non-linear Regression: A Greedy Approach Employing Kernels with\n  Application to Image Denoising", "comments": null, "journal-ref": null, "doi": "10.1109/TSP.2017.2708029", "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider the task of robust non-linear regression in the presence of both\ninlier noise and outliers. Assuming that the unknown non-linear function\nbelongs to a Reproducing Kernel Hilbert Space (RKHS), our goal is to estimate\nthe set of the associated unknown parameters. Due to the presence of outliers,\ncommon techniques such as the Kernel Ridge Regression (KRR) or the Support\nVector Regression (SVR) turn out to be inadequate. Instead, we employ sparse\nmodeling arguments to explicitly model and estimate the outliers, adopting a\ngreedy approach. The proposed robust scheme, i.e., Kernel Greedy Algorithm for\nRobust Denoising (KGARD), is inspired by the classical Orthogonal Matching\nPursuit (OMP) algorithm. Specifically, the proposed method alternates between a\nKRR task and an OMP-like selection step. Theoretical results concerning the\nidentification of the outliers are provided. Moreover, KGARD is compared\nagainst other cutting edge methods, where its performance is evaluated via a\nset of experiments with various types of noise. Finally, the proposed robust\nestimation framework is applied to the task of image denoising, and its\nenhanced performance in the presence of outliers is demonstrated.\n", "versions": [{"version": "v1", "created": "Mon, 4 Jan 2016 18:11:45 GMT"}, {"version": "v2", "created": "Wed, 3 Aug 2016 06:29:42 GMT"}], "update_date": "2017-08-02", "authors_parsed": [["Papageorgiou", "George", ""], ["Bouboulis", "Pantelis", ""], ["Theodoridis", "Sergios", ""]]}, {"id": "1601.00670", "submitter": "Alp Kucukelbir", "authors": "David M. Blei, Alp Kucukelbir, Jon D. McAuliffe", "title": "Variational Inference: A Review for Statisticians", "comments": null, "journal-ref": "Journal of the American Statistical Association, Vol. 112 , Iss.\n  518, 2017", "doi": "10.1080/01621459.2017.1285773", "report-no": null, "categories": "stat.CO cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  One of the core problems of modern statistics is to approximate\ndifficult-to-compute probability densities. This problem is especially\nimportant in Bayesian statistics, which frames all inference about unknown\nquantities as a calculation involving the posterior density. In this paper, we\nreview variational inference (VI), a method from machine learning that\napproximates probability densities through optimization. VI has been used in\nmany applications and tends to be faster than classical methods, such as Markov\nchain Monte Carlo sampling. The idea behind VI is to first posit a family of\ndensities and then to find the member of that family which is close to the\ntarget. Closeness is measured by Kullback-Leibler divergence. We review the\nideas behind mean-field variational inference, discuss the special case of VI\napplied to exponential family models, present a full example with a Bayesian\nmixture of Gaussians, and derive a variant that uses stochastic optimization to\nscale up to massive data. We discuss modern research in VI and highlight\nimportant open problems. VI is powerful, but it is not yet well understood. Our\nhope in writing this paper is to catalyze statistical research on this class of\nalgorithms.\n", "versions": [{"version": "v1", "created": "Mon, 4 Jan 2016 21:28:04 GMT"}, {"version": "v2", "created": "Fri, 4 Mar 2016 20:33:40 GMT"}, {"version": "v3", "created": "Mon, 8 Aug 2016 15:57:26 GMT"}, {"version": "v4", "created": "Wed, 2 Nov 2016 17:58:48 GMT"}, {"version": "v5", "created": "Wed, 14 Jun 2017 13:44:33 GMT"}, {"version": "v6", "created": "Wed, 15 Nov 2017 20:13:02 GMT"}, {"version": "v7", "created": "Mon, 4 Dec 2017 23:07:00 GMT"}, {"version": "v8", "created": "Mon, 26 Mar 2018 01:40:27 GMT"}, {"version": "v9", "created": "Wed, 9 May 2018 20:52:28 GMT"}], "update_date": "2018-05-11", "authors_parsed": [["Blei", "David M.", ""], ["Kucukelbir", "Alp", ""], ["McAuliffe", "Jon D.", ""]]}, {"id": "1601.00863", "submitter": "Ming Yan", "authors": "Zhimin Peng, Tianyu Wu, Yangyang Xu, Ming Yan, Wotao Yin", "title": "Coordinate Friendly Structures, Algorithms and Applications", "comments": null, "journal-ref": "Annals of Mathematical Sciences and Applications, 1 (2016), 57-119", "doi": "10.4310/AMSA.2016.v1.n1.a2", "report-no": "UCLA CAM Report 16-13", "categories": "math.OC cs.CE cs.DC math.NA stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper focuses on coordinate update methods, which are useful for solving\nproblems involving large or high-dimensional datasets. They decompose a problem\ninto simple subproblems, where each updates one, or a small block of, variables\nwhile fixing others. These methods can deal with linear and nonlinear mappings,\nsmooth and nonsmooth functions, as well as convex and nonconvex problems. In\naddition, they are easy to parallelize.\n  The great performance of coordinate update methods depends on solving simple\nsub-problems. To derive simple subproblems for several new classes of\napplications, this paper systematically studies coordinate-friendly operators\nthat perform low-cost coordinate updates.\n  Based on the discovered coordinate friendly operators, as well as operator\nsplitting techniques, we obtain new coordinate update algorithms for a variety\nof problems in machine learning, image processing, as well as sub-areas of\noptimization. Several problems are treated with coordinate update for the first\ntime in history. The obtained algorithms are scalable to large instances\nthrough parallel and even asynchronous computing. We present numerical examples\nto illustrate how effective these algorithms are.\n", "versions": [{"version": "v1", "created": "Tue, 5 Jan 2016 15:33:05 GMT"}, {"version": "v2", "created": "Mon, 7 Mar 2016 23:05:07 GMT"}, {"version": "v3", "created": "Sun, 14 Aug 2016 14:29:53 GMT"}], "update_date": "2016-08-16", "authors_parsed": [["Peng", "Zhimin", ""], ["Wu", "Tianyu", ""], ["Xu", "Yangyang", ""], ["Yan", "Ming", ""], ["Yin", "Wotao", ""]]}, {"id": "1601.00909", "submitter": "Mihai Alexandru Petrovici", "authors": "Mihai A. Petrovici, Ilja Bytschok, Johannes Bill, Johannes Schemmel\n  and Karlheinz Meier", "title": "The high-conductance state enables neural sampling in networks of LIF\n  neurons", "comments": "3 pages, 1 figure", "journal-ref": null, "doi": "10.1186/1471-2202-16-S1-O2", "report-no": null, "categories": "q-bio.NC cond-mat.dis-nn cs.NE physics.bio-ph stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The apparent stochasticity of in-vivo neural circuits has long been\nhypothesized to represent a signature of ongoing stochastic inference in the\nbrain. More recently, a theoretical framework for neural sampling has been\nproposed, which explains how sample-based inference can be performed by\nnetworks of spiking neurons. One particular requirement of this approach is\nthat the neural response function closely follows a logistic curve.\n  Analytical approaches to calculating neural response functions have been the\nsubject of many theoretical studies. In order to make the problem tractable,\nparticular assumptions regarding the neural or synaptic parameters are usually\nmade. However, biologically significant activity regimes exist which are not\ncovered by these approaches: Under strong synaptic bombardment, as is often the\ncase in cortex, the neuron is shifted into a high-conductance state (HCS)\ncharacterized by a small membrane time constant. In this regime, synaptic time\nconstants and refractory periods dominate membrane dynamics.\n  The core idea of our approach is to separately consider two different \"modes\"\nof spiking dynamics: burst spiking and transient quiescence, in which the\nneuron does not spike for longer periods. We treat the former by propagating\nthe PDF of the effective membrane potential from spike to spike within a burst,\nwhile using a diffusion approximation for the latter. We find that our\nprediction of the neural response function closely matches simulation data.\nMoreover, in the HCS scenario, we show that the neural response function\nbecomes symmetric and can be well approximated by a logistic function, thereby\nproviding the correct dynamics in order to perform neural sampling. We hereby\nprovide not only a normative framework for Bayesian inference in cortex, but\nalso powerful applications of low-power, accelerated neuromorphic systems to\nrelevant machine learning tasks.\n", "versions": [{"version": "v1", "created": "Tue, 5 Jan 2016 17:15:37 GMT"}], "update_date": "2017-03-14", "authors_parsed": [["Petrovici", "Mihai A.", ""], ["Bytschok", "Ilja", ""], ["Bill", "Johannes", ""], ["Schemmel", "Johannes", ""], ["Meier", "Karlheinz", ""]]}, {"id": "1601.00955", "submitter": "Feng Nan", "authors": "Feng Nan, Joseph Wang, Venkatesh Saligrama", "title": "Optimally Pruning Decision Tree Ensembles With Feature Cost", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider the problem of learning decision rules for prediction with\nfeature budget constraint. In particular, we are interested in pruning an\nensemble of decision trees to reduce expected feature cost while maintaining\nhigh prediction accuracy for any test example. We propose a novel 0-1 integer\nprogram formulation for ensemble pruning. Our pruning formulation is general -\nit takes any ensemble of decision trees as input. By explicitly accounting for\nfeature-sharing across trees together with accuracy/cost trade-off, our method\nis able to significantly reduce feature cost by pruning subtrees that introduce\nmore loss in terms of feature cost than benefit in terms of prediction accuracy\ngain. Theoretically, we prove that a linear programming relaxation produces the\nexact solution of the original integer program. This allows us to use efficient\nconvex optimization tools to obtain an optimally pruned ensemble for any given\nbudget. Empirically, we see that our pruning algorithm significantly improves\nthe performance of the state of the art ensemble method BudgetRF.\n", "versions": [{"version": "v1", "created": "Tue, 5 Jan 2016 20:38:35 GMT"}], "update_date": "2016-01-06", "authors_parsed": [["Nan", "Feng", ""], ["Wang", "Joseph", ""], ["Saligrama", "Venkatesh", ""]]}, {"id": "1601.01073", "submitter": "Orhan Firat", "authors": "Orhan Firat, Kyunghyun Cho and Yoshua Bengio", "title": "Multi-Way, Multilingual Neural Machine Translation with a Shared\n  Attention Mechanism", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose multi-way, multilingual neural machine translation. The proposed\napproach enables a single neural translation model to translate between\nmultiple languages, with a number of parameters that grows only linearly with\nthe number of languages. This is made possible by having a single attention\nmechanism that is shared across all language pairs. We train the proposed\nmulti-way, multilingual model on ten language pairs from WMT'15 simultaneously\nand observe clear performance improvements over models trained on only one\nlanguage pair. In particular, we observe that the proposed model significantly\nimproves the translation quality of low-resource language pairs.\n", "versions": [{"version": "v1", "created": "Wed, 6 Jan 2016 04:00:50 GMT"}], "update_date": "2016-01-07", "authors_parsed": [["Firat", "Orhan", ""], ["Cho", "Kyunghyun", ""], ["Bengio", "Yoshua", ""]]}, {"id": "1601.01142", "submitter": "Yang Gao", "authors": "Yang Gao, Jianfei Chen, Jun Zhu", "title": "Streaming Gibbs Sampling for LDA Model", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Streaming variational Bayes (SVB) is successful in learning LDA models in an\nonline manner. However previous attempts toward developing online Monte-Carlo\nmethods for LDA have little success, often by having much worse perplexity than\ntheir batch counterparts. We present a streaming Gibbs sampling (SGS) method,\nan online extension of the collapsed Gibbs sampling (CGS). Our empirical study\nshows that SGS can reach similar perplexity as CGS, much better than SVB. Our\ndistributed version of SGS, DSGS, is much more scalable than SVB mainly because\nthe updates' communication complexity is small.\n", "versions": [{"version": "v1", "created": "Wed, 6 Jan 2016 11:15:45 GMT"}], "update_date": "2016-01-07", "authors_parsed": [["Gao", "Yang", ""], ["Chen", "Jianfei", ""], ["Zhu", "Jun", ""]]}, {"id": "1601.01190", "submitter": "Emilie Kaufmann", "authors": "Emilie Kaufmann (SEQUEL, CNRS, CRIStAL)", "title": "On Bayesian index policies for sequential resource allocation", "comments": "Annals of Statistics, Institute of Mathematical Statistics, A\n  Para\\^itre", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper is about index policies for minimizing (frequentist) regret in a\nstochastic multi-armed bandit model, inspired by a Bayesian view on the\nproblem. Our main contribution is to prove that the Bayes-UCB algorithm, which\nrelies on quantiles of posterior distributions, is asymptotically optimal when\nthe reward distributions belong to a one-dimensional exponential family, for a\nlarge class of prior distributions. We also show that the Bayesian literature\ngives new insight on what kind of exploration rates could be used in\nfrequentist, UCB-type algorithms. Indeed, approximations of the Bayesian\noptimal solution or the Finite Horizon Gittins indices provide a justification\nfor the kl-UCB+ and kl-UCB-H+ algorithms, whose asymptotic optimality is also\nestablished.\n", "versions": [{"version": "v1", "created": "Wed, 6 Jan 2016 14:24:59 GMT"}, {"version": "v2", "created": "Tue, 13 Sep 2016 13:48:07 GMT"}, {"version": "v3", "created": "Mon, 6 Nov 2017 09:26:58 GMT"}], "update_date": "2017-11-07", "authors_parsed": [["Kaufmann", "Emilie", "", "SEQUEL, CNRS, CRIStAL"]]}, {"id": "1601.01345", "submitter": "Benjamin Guedj", "authors": "Pierre Alquier and Benjamin Guedj", "title": "An Oracle Inequality for Quasi-Bayesian Non-Negative Matrix\n  Factorization", "comments": "This is the corrected version of the published paper P. Alquier, B.\n  Guedj, An Oracle Inequality for Quasi-Bayesian Non-negative Matrix\n  Factorization, Mathematical Methods of Statistics, 2017, vol. 26, no. 1, pp.\n  55-67. Since then Arnak Dalalyan (ENSAE) found a mistake in the proofs. We\n  fixed the mistake at the price of a slightly different logarithmic term in\n  the bound", "journal-ref": "Mathematical Methods of Statistics (MMS), 26(1): 55-67, 2017", "doi": "10.3103/S1066530717010045", "report-no": null, "categories": "stat.ML math.ST stat.TH", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  The aim of this paper is to provide some theoretical understanding of\nquasi-Bayesian aggregation methods non-negative matrix factorization. We derive\nan oracle inequality for an aggregated estimator. This result holds for a very\ngeneral class of prior distributions and shows how the prior affects the rate\nof convergence.\n", "versions": [{"version": "v1", "created": "Wed, 6 Jan 2016 22:28:50 GMT"}, {"version": "v2", "created": "Tue, 7 Jun 2016 17:15:36 GMT"}, {"version": "v3", "created": "Wed, 24 Aug 2016 16:00:22 GMT"}, {"version": "v4", "created": "Tue, 26 Jun 2018 07:51:34 GMT"}], "update_date": "2018-06-27", "authors_parsed": [["Alquier", "Pierre", ""], ["Guedj", "Benjamin", ""]]}, {"id": "1601.01411", "submitter": "Chetan Tonde", "authors": "Chetan Tonde and Ahmed Elgammal", "title": "Learning Kernels for Structured Prediction using Polynomial Kernel\n  Transformations", "comments": null, "journal-ref": null, "doi": null, "report-no": "21 pages, 10 figures", "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Learning the kernel functions used in kernel methods has been a vastly\nexplored area in machine learning. It is now widely accepted that to obtain\n'good' performance, learning a kernel function is the key challenge. In this\nwork we focus on learning kernel representations for structured regression. We\npropose use of polynomials expansion of kernels, referred to as Schoenberg\ntransforms and Gegenbaur transforms, which arise from the seminal result of\nSchoenberg (1938). These kernels can be thought of as polynomial combination of\ninput features in a high dimensional reproducing kernel Hilbert space (RKHS).\nWe learn kernels over input and output for structured data, such that,\ndependency between kernel features is maximized. We use Hilbert-Schmidt\nIndependence Criterion (HSIC) to measure this. We also give an efficient,\nmatrix decomposition-based algorithm to learn these kernel transformations, and\ndemonstrate state-of-the-art results on several real-world datasets.\n", "versions": [{"version": "v1", "created": "Thu, 7 Jan 2016 06:37:48 GMT"}], "update_date": "2016-01-08", "authors_parsed": [["Tonde", "Chetan", ""], ["Elgammal", "Ahmed", ""]]}, {"id": "1601.01507", "submitter": "Antti Airola", "authors": "Antti Airola, Tapio Pahikkala", "title": "Fast Kronecker product kernel methods via generalized vec trick", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Kronecker product kernel provides the standard approach in the kernel methods\nliterature for learning from graph data, where edges are labeled and both start\nand end vertices have their own feature representations. The methods allow\ngeneralization to such new edges, whose start and end vertices do not appear in\nthe training data, a setting known as zero-shot or zero-data learning. Such a\nsetting occurs in numerous applications, including drug-target interaction\nprediction, collaborative filtering and information retrieval. Efficient\ntraining algorithms based on the so-called vec trick, that makes use of the\nspecial structure of the Kronecker product, are known for the case where the\ntraining data is a complete bipartite graph. In this work we generalize these\nresults to non-complete training graphs. This allows us to derive a general\nframework for training Kronecker product kernel methods, as specific examples\nwe implement Kronecker ridge regression and support vector machine algorithms.\nExperimental results demonstrate that the proposed approach leads to accurate\nmodels, while allowing order of magnitude improvements in training and\nprediction time.\n", "versions": [{"version": "v1", "created": "Thu, 7 Jan 2016 12:25:53 GMT"}, {"version": "v2", "created": "Thu, 22 Sep 2016 10:18:12 GMT"}, {"version": "v3", "created": "Wed, 19 Apr 2017 07:31:24 GMT"}], "update_date": "2017-04-20", "authors_parsed": [["Airola", "Antti", ""], ["Pahikkala", "Tapio", ""]]}, {"id": "1601.01544", "submitter": "Alessio Benavoli", "authors": "Alessio Benavoli and Marco Zaffalon", "title": "State Space representation of non-stationary Gaussian Processes", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The state space (SS) representation of Gaussian processes (GP) has recently\ngained a lot of interest. The main reason is that it allows to compute GPs\nbased inferences in O(n), where $n$ is the number of observations. This\nimplementation makes GPs suitable for Big Data. For this reason, it is\nimportant to provide a SS representation of the most important kernels used in\nmachine learning. The aim of this paper is to show how to exploit the transient\nbehaviour of SS models to map non-stationary kernels to SS models.\n", "versions": [{"version": "v1", "created": "Thu, 7 Jan 2016 14:25:07 GMT"}], "update_date": "2016-01-08", "authors_parsed": [["Benavoli", "Alessio", ""], ["Zaffalon", "Marco", ""]]}, {"id": "1601.01653", "submitter": "Avi Ma'ayan", "authors": "Avi Ma'ayan and Neil R. Clark", "title": "Large Collection of Diverse Gene Set Search Queries Recapitulate Known\n  Protein-Protein Interactions and Gene-Gene Functional Associations", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "q-bio.MN cs.AI cs.SI q-bio.GN stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Popular online enrichment analysis tools from the field of molecular systems\nbiology provide users with the ability to submit their experimental results as\ngene sets for individual analysis. Such queries are kept private, and have\nnever before been considered as a resource for integrative analysis. By\nharnessing gene set query submissions from thousands of users, we aim to\ndiscover biological knowledge beyond the scope of an individual study. In this\nwork, we investigated a large collection of gene sets submitted to the tool\nEnrichr by thousands of users. Based on co-occurrence, we constructed a global\ngene-gene association network. We interpret this inferred network as providing\na summary of the structure present in this crowdsourced gene set library, and\nshow that this network recapitulates known protein-protein interactions and\nfunctional associations between genes. This finding implies that this network\nalso offers predictive value. Furthermore, we visualize this gene-gene\nassociation network using a new edge-pruning algorithm that retains both the\nlocal and global structures of large-scale networks. Our ability to make\npredictions for currently unknown gene associations, that may not be captured\nby individual researchers and data sources, is a demonstration of the potential\nof harnessing collective knowledge from users of popular tools in the field of\nmolecular systems biology.\n", "versions": [{"version": "v1", "created": "Thu, 7 Jan 2016 20:06:18 GMT"}], "update_date": "2016-01-08", "authors_parsed": [["Ma'ayan", "Avi", ""], ["Clark", "Neil R.", ""]]}, {"id": "1601.01892", "submitter": "Kirell Benzi", "authors": "Kirell Benzi, Vassilis Kalofolias, Xavier Bresson, Pierre\n  Vandergheynst", "title": "Song Recommendation with Non-Negative Matrix Factorization and Graph\n  Total Variation", "comments": "Code available at: https://github.com/kikohs/recog", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.IR cs.LG physics.data-an", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This work formulates a novel song recommender system as a matrix completion\nproblem that benefits from collaborative filtering through Non-negative Matrix\nFactorization (NMF) and content-based filtering via total variation (TV) on\ngraphs. The graphs encode both playlist proximity information and song\nsimilarity, using a rich combination of audio, meta-data and social features.\nAs we demonstrate, our hybrid recommendation system is very versatile and\nincorporates several well-known methods while outperforming them. Particularly,\nwe show on real-world data that our model overcomes w.r.t. two evaluation\nmetrics the recommendation of models solely based on low-rank information,\ngraph-based information or a combination of both.\n", "versions": [{"version": "v1", "created": "Fri, 8 Jan 2016 14:59:34 GMT"}, {"version": "v2", "created": "Wed, 13 Jan 2016 17:24:33 GMT"}], "update_date": "2016-01-14", "authors_parsed": [["Benzi", "Kirell", ""], ["Kalofolias", "Vassilis", ""], ["Bresson", "Xavier", ""], ["Vandergheynst", "Pierre", ""]]}, {"id": "1601.01944", "submitter": "Martha White", "authors": "Shantanu Jain, Martha White, Michael W. Trosset, Predrag Radivojac", "title": "Nonparametric semi-supervised learning of class proportions", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The problem of developing binary classifiers from positive and unlabeled data\nis often encountered in machine learning. A common requirement in this setting\nis to approximate posterior probabilities of positive and negative classes for\na previously unseen data point. This problem can be decomposed into two steps:\n(i) the development of accurate predictors that discriminate between positive\nand unlabeled data, and (ii) the accurate estimation of the prior probabilities\nof positive and negative examples. In this work we primarily focus on the\nlatter subproblem. We study nonparametric class prior estimation and formulate\nthis problem as an estimation of mixing proportions in two-component mixture\nmodels, given a sample from one of the components and another sample from the\nmixture itself. We show that estimation of mixing proportions is generally\nill-defined and propose a canonical form to obtain identifiability while\nmaintaining the flexibility to model any distribution. We use insights from\nthis theory to elucidate the optimization surface of the class priors and\npropose an algorithm for estimating them. To address the problems of\nhigh-dimensional density estimation, we provide practical transformations to\nlow-dimensional spaces that preserve class priors. Finally, we demonstrate the\nefficacy of our method on univariate and multivariate data.\n", "versions": [{"version": "v1", "created": "Fri, 8 Jan 2016 16:56:55 GMT"}], "update_date": "2016-01-11", "authors_parsed": [["Jain", "Shantanu", ""], ["White", "Martha", ""], ["Trosset", "Michael W.", ""], ["Radivojac", "Predrag", ""]]}, {"id": "1601.01966", "submitter": "Zenon Gniazdowski", "authors": "Zenon Gniazdowski, Michal Grabowski", "title": "Numerical Coding of Nominal Data", "comments": "9 pages", "journal-ref": "Zeszyty Naukowe WWSI, No 12, Vol. 9, 2015, pp. 53-61, ISSN\n  1896-396X", "doi": "10.26348/znwwsi.12.53", "report-no": null, "categories": "stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, a novel approach for coding nominal data is proposed. For the\ngiven nominal data, a rank in a form of complex number is assigned. The\nproposed method does not lose any information about the attribute and brings\nother properties previously unknown. The approach based on these knew\nproperties can been used for classification. The analyzed example shows that\nclassification with the use of coded nominal data or both numerical as well as\ncoded nominal data is more effective than the classification, which uses only\nnumerical data.\n", "versions": [{"version": "v1", "created": "Fri, 8 Jan 2016 18:24:52 GMT"}], "update_date": "2017-11-13", "authors_parsed": [["Gniazdowski", "Zenon", ""], ["Grabowski", "Michal", ""]]}, {"id": "1601.01972", "submitter": "David Schnoerr", "authors": "David Schnoerr, Ramon Grima and Guido Sanguinetti", "title": "Cox process representation and inference for stochastic\n  reaction-diffusion processes", "comments": "18 pages, 5 figures", "journal-ref": "Nature Communications, 7 (11729) (2016)", "doi": "10.1038/ncomms11729", "report-no": null, "categories": "cond-mat.stat-mech math.ST physics.data-an q-bio.QM stat.ML stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Complex behaviour in many systems arises from the stochastic interactions of\nspatially distributed particles or agents. Stochastic reaction-diffusion\nprocesses are widely used to model such behaviour in disciplines ranging from\nbiology to the social sciences, yet they are notoriously difficult to simulate\nand calibrate to observational data. Here we use ideas from statistical physics\nand machine learning to provide a solution to the inverse problem of learning a\nstochastic reaction-diffusion process from data. Our solution relies on a\nnon-trivial connection between stochastic reaction-diffusion processes and\nspatio-temporal Cox processes, a well-studied class of models from\ncomputational statistics. This connection leads to an efficient and flexible\nalgorithm for parameter inference and model selection. Our approach shows\nexcellent accuracy on numeric and real data examples from systems biology and\nepidemiology. Our work provides both insights into spatio-temporal stochastic\nsystems, and a practical solution to a long-standing problem in computational\nmodelling.\n", "versions": [{"version": "v1", "created": "Fri, 8 Jan 2016 18:44:03 GMT"}, {"version": "v2", "created": "Mon, 22 Aug 2016 15:41:23 GMT"}], "update_date": "2016-08-23", "authors_parsed": [["Schnoerr", "David", ""], ["Grima", "Ramon", ""], ["Sanguinetti", "Guido", ""]]}, {"id": "1601.02068", "submitter": "Yining Wang", "authors": "Yining Wang and Adams Wei Yu and Aarti Singh", "title": "On Computationally Tractable Selection of Experiments in\n  Measurement-Constrained Regression Models", "comments": "41 pages. Accepted for publication in Journal of Machine Learning\n  Research", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We derive computationally tractable methods to select a small subset of\nexperiment settings from a large pool of given design points. The primary focus\nis on linear regression models, while the technique extends to generalized\nlinear models and Delta's method (estimating functions of linear regression\nmodels) as well. The algorithms are based on a continuous relaxation of an\notherwise intractable combinatorial optimization problem, with sampling or\ngreedy procedures as post-processing steps. Formal approximation guarantees are\nestablished for both algorithms, and numerical results on both synthetic and\nreal-world data confirm the effectiveness of the proposed methods.\n", "versions": [{"version": "v1", "created": "Sat, 9 Jan 2016 03:05:31 GMT"}, {"version": "v2", "created": "Wed, 27 Jan 2016 21:05:35 GMT"}, {"version": "v3", "created": "Tue, 5 Jul 2016 21:29:33 GMT"}, {"version": "v4", "created": "Fri, 2 Dec 2016 20:07:28 GMT"}, {"version": "v5", "created": "Fri, 24 Mar 2017 00:57:10 GMT"}, {"version": "v6", "created": "Wed, 20 Dec 2017 06:52:49 GMT"}], "update_date": "2017-12-21", "authors_parsed": [["Wang", "Yining", ""], ["Yu", "Adams Wei", ""], ["Singh", "Aarti", ""]]}, {"id": "1601.02213", "submitter": "Michael Berthold", "authors": "Michael R. Berthold and Frank H\\\"oppner", "title": "On Clustering Time Series Using Euclidean Distance and Pearson\n  Correlation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  For time series comparisons, it has often been observed that z-score\nnormalized Euclidean distances far outperform the unnormalized variant. In this\npaper we show that a z-score normalized, squared Euclidean Distance is, in\nfact, equal to a distance based on Pearson Correlation. This has profound\nimpact on many distance-based classification or clustering methods. In addition\nto this theoretically sound result we also show that the often used k-Means\nalgorithm formally needs a mod ification to keep the interpretation as Pearson\ncorrelation strictly valid. Experimental results demonstrate that in many cases\nthe standard k-Means algorithm generally produces the same results.\n", "versions": [{"version": "v1", "created": "Sun, 10 Jan 2016 13:17:46 GMT"}], "update_date": "2016-01-12", "authors_parsed": [["Berthold", "Michael R.", ""], ["H\u00f6ppner", "Frank", ""]]}, {"id": "1601.02257", "submitter": "Robert Finn", "authors": "Robert Finn and Brian Kulis", "title": "A Sufficient Statistics Construction of Bayesian Nonparametric\n  Exponential Family Conjugate Models", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Conjugate pairs of distributions over infinite dimensional spaces are\nprominent in statistical learning theory, particularly due to the widespread\nadoption of Bayesian nonparametric methodologies for a host of models and\napplications. Much of the existing literature in the learning community focuses\non processes possessing some form of computationally tractable conjugacy as is\nthe case for the beta and gamma processes (and, via normalization, the\nDirichlet process). For these processes, proofs of conjugacy and requisite\nderivation of explicit computational formulae for posterior density parameters\nare idiosyncratic to the stochastic process in question. As such, Bayesian\nNonparametric models are currently available for a limited number of conjugate\npairs, e.g. the Dirichlet-multinomial and beta-Bernoulli process pairs. In each\nof these above cases the likelihood process belongs to the class of discrete\nexponential family distributions. The exclusion of continuous likelihood\ndistributions from the known cases of Bayesian Nonparametric Conjugate models\nstands as a disparity in the researcher's toolbox.\n  In this paper we first address the problem of obtaining a general\nconstruction of prior distributions over infinite dimensional spaces possessing\ndistributional properties amenable to conjugacy. Second, we bridge the divide\nbetween the discrete and continuous likelihoods by illustrating a canonical\nconstruction for stochastic processes whose Levy measure densities are from\npositive exponential families, and then demonstrate that these processes in\nfact form the prior, likelihood, and posterior in a conjugate family. Our\ncanonical construction subsumes known computational formulae for posterior\ndensity parameters in the cases where the likelihood is from a discrete\ndistribution belonging to an exponential family.\n", "versions": [{"version": "v1", "created": "Sun, 10 Jan 2016 19:23:27 GMT"}], "update_date": "2016-01-12", "authors_parsed": [["Finn", "Robert", ""], ["Kulis", "Brian", ""]]}, {"id": "1601.02300", "submitter": "Marian-Andrei Rizoiu", "authors": "Young-Min Kim, Julien Velcin, St\\'ephane Bonnevay, Marian-Andrei\n  Rizoiu", "title": "Temporal Multinomial Mixture for Instance-Oriented Evolutionary\n  Clustering", "comments": null, "journal-ref": null, "doi": "10.1007/978-3-319-16354-3_66", "report-no": null, "categories": "cs.IR cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Evolutionary clustering aims at capturing the temporal evolution of clusters.\nThis issue is particularly important in the context of social media data that\nare naturally temporally driven. In this paper, we propose a new probabilistic\nmodel-based evolutionary clustering technique. The Temporal Multinomial Mixture\n(TMM) is an extension of classical mixture model that optimizes feature\nco-occurrences in the trade-off with temporal smoothness. Our model is\nevaluated for two recent case studies on opinion aggregation over time. We\ncompare four different probabilistic clustering models and we show the\nsuperiority of our proposal in the task of instance-oriented clustering.\n", "versions": [{"version": "v1", "created": "Mon, 11 Jan 2016 02:06:36 GMT"}], "update_date": "2016-01-12", "authors_parsed": [["Kim", "Young-Min", ""], ["Velcin", "Julien", ""], ["Bonnevay", "St\u00e9phane", ""], ["Rizoiu", "Marian-Andrei", ""]]}, {"id": "1601.02513", "submitter": "Vassilis Kalofolias", "authors": "Vassilis Kalofolias", "title": "How to learn a graph from smooth signals", "comments": "8 pages + supplementary material. Accepted in AISTATS 2016, Cadiz,\n  Spain", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG physics.data-an", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a framework that learns the graph structure underlying a set of\nsmooth signals. Given $X\\in\\mathbb{R}^{m\\times n}$ whose rows reside on the\nvertices of an unknown graph, we learn the edge weights\n$w\\in\\mathbb{R}_+^{m(m-1)/2}$ under the smoothness assumption that\n$\\text{tr}{X^\\top LX}$ is small. We show that the problem is a weighted\n$\\ell$-1 minimization that leads to naturally sparse solutions. We point out\nhow known graph learning or construction techniques fall within our framework\nand propose a new model that performs better than the state of the art in many\nsettings. We present efficient, scalable primal-dual based algorithms for both\nour model and the previous state of the art, and evaluate their performance on\nartificial and real data.\n", "versions": [{"version": "v1", "created": "Mon, 11 Jan 2016 16:23:30 GMT"}], "update_date": "2016-01-12", "authors_parsed": [["Kalofolias", "Vassilis", ""]]}, {"id": "1601.02522", "submitter": "Nathanael Perraudin N. P.", "authors": "Nathana\\\"el Perraudin, Pierre Vandergheynst", "title": "Stationary signal processing on graphs", "comments": null, "journal-ref": null, "doi": "10.1109/TSP.2017.2690388", "report-no": null, "categories": "cs.DS stat.AP stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Graphs are a central tool in machine learning and information processing as\nthey allow to conveniently capture the structure of complex datasets. In this\ncontext, it is of high importance to develop flexible models of signals defined\nover graphs or networks. In this paper, we generalize the traditional concept\nof wide sense stationarity to signals defined over the vertices of arbitrary\nweighted undirected graphs. We show that stationarity is expressed through the\ngraph localization operator reminiscent of translation. We prove that\nstationary graph signals are characterized by a well-defined Power Spectral\nDensity that can be efficiently estimated even for large graphs. We leverage\nthis new concept to derive Wiener-type estimation procedures of noisy and\npartially observed signals and illustrate the performance of this new model for\ndenoising and regression.\n", "versions": [{"version": "v1", "created": "Mon, 11 Jan 2016 16:58:45 GMT"}, {"version": "v2", "created": "Tue, 12 Jan 2016 16:42:30 GMT"}, {"version": "v3", "created": "Thu, 21 Apr 2016 16:34:34 GMT"}, {"version": "v4", "created": "Fri, 8 Jul 2016 21:25:26 GMT"}, {"version": "v5", "created": "Fri, 21 Apr 2017 18:30:15 GMT"}], "update_date": "2017-05-24", "authors_parsed": [["Perraudin", "Nathana\u00ebl", ""], ["Vandergheynst", "Pierre", ""]]}, {"id": "1601.02712", "submitter": "Damian Straszak", "authors": "Damian Straszak and Nisheeth K. Vishnoi", "title": "IRLS and Slime Mold: Equivalence and Convergence", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.ET cs.IT math.IT math.NA math.OC stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper we present a connection between two dynamical systems arising\nin entirely different contexts: one in signal processing and the other in\nbiology. The first is the famous Iteratively Reweighted Least Squares (IRLS)\nalgorithm used in compressed sensing and sparse recovery while the second is\nthe dynamics of a slime mold (Physarum polycephalum). Both of these dynamics\nare geared towards finding a minimum l1-norm solution in an affine subspace.\nDespite its simplicity the convergence of the IRLS method has been shown only\nfor a certain regularization of it and remains an important open problem. Our\nfirst result shows that the two dynamics are projections of the same dynamical\nsystem in higher dimensions. As a consequence, and building on the recent work\non Physarum dynamics, we are able to prove convergence and obtain complexity\nbounds for a damped version of the IRLS algorithm.\n", "versions": [{"version": "v1", "created": "Tue, 12 Jan 2016 02:24:18 GMT"}], "update_date": "2016-01-13", "authors_parsed": [["Straszak", "Damian", ""], ["Vishnoi", "Nisheeth K.", ""]]}, {"id": "1601.02733", "submitter": "Ehsan Hosseini-Asl", "authors": "Ehsan Hosseini-Asl, Jacek M. Zurada, Olfa Nasraoui", "title": "Deep Learning of Part-based Representation of Data Using Sparse\n  Autoencoders with Nonnegativity Constraints", "comments": "Accepted for publication in IEEE Transactions of Neural Networks and\n  Learning Systems", "journal-ref": null, "doi": "10.1109/TNNLS.2015.2479223", "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We demonstrate a new deep learning autoencoder network, trained by a\nnonnegativity constraint algorithm (NCAE), that learns features which show\npart-based representation of data. The learning algorithm is based on\nconstraining negative weights. The performance of the algorithm is assessed\nbased on decomposing data into parts and its prediction performance is tested\non three standard image data sets and one text dataset. The results indicate\nthat the nonnegativity constraint forces the autoencoder to learn features that\namount to a part-based representation of data, while improving sparsity and\nreconstruction quality in comparison with the traditional sparse autoencoder\nand Nonnegative Matrix Factorization. It is also shown that this newly acquired\nrepresentation improves the prediction performance of a deep neural network.\n", "versions": [{"version": "v1", "created": "Tue, 12 Jan 2016 05:33:03 GMT"}], "update_date": "2016-01-13", "authors_parsed": [["Hosseini-Asl", "Ehsan", ""], ["Zurada", "Jacek M.", ""], ["Nasraoui", "Olfa", ""]]}, {"id": "1601.02748", "submitter": "Gregory Giecold", "authors": "Gregory Giecold, Eugenio Marco, Lorenzo Trippa and Guo-Cheng Yuan", "title": "Robust Lineage Reconstruction from High-Dimensional Single-Cell Data", "comments": "22 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "q-bio.QM stat.AP stat.CO stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Single-cell gene expression data provide invaluable resources for systematic\ncharacterization of cellular hierarchy in multi-cellular organisms. However,\ncell lineage reconstruction is still often associated with significant\nuncertainty due to technological constraints. Such uncertainties have not been\ntaken into account in current methods. We present ECLAIR, a novel computational\nmethod for the statistical inference of cell lineage relationships from\nsingle-cell gene expression data. ECLAIR uses an ensemble approach to improve\nthe robustness of lineage predictions, and provides a quantitative estimate of\nthe uncertainty of lineage branchings. We show that the application of ECLAIR\nto published datasets successfully reconstructs known lineage relationships and\nsignificantly improves the robustness of predictions. In conclusion, ECLAIR is\na powerful bioinformatics tool for single-cell data analysis. It can be used\nfor robust lineage reconstruction with quantitative estimate of prediction\naccuracy.\n", "versions": [{"version": "v1", "created": "Tue, 12 Jan 2016 07:01:55 GMT"}], "update_date": "2016-01-13", "authors_parsed": [["Giecold", "Gregory", ""], ["Marco", "Eugenio", ""], ["Trippa", "Lorenzo", ""], ["Yuan", "Guo-Cheng", ""]]}, {"id": "1601.02789", "submitter": "Krzysztof Wo{\\l}k", "authors": "Krzysztof Wo{\\l}k, Danijel Kor\\v{z}inek", "title": "Comparison and Adaptation of Automatic Evaluation Metrics for Quality\n  Assessment of Re-Speaking", "comments": "Comparison and Adaptation of Automatic Evaluation Metrics for Quality\n  Assessment of Re-Speaking. arXiv admin note: text overlap with\n  arXiv:1509.09088", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL stat.AP stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Re-speaking is a mechanism for obtaining high quality subtitles for use in\nlive broadcast and other public events. Because it relies on humans performing\nthe actual re-speaking, the task of estimating the quality of the results is\nnon-trivial. Most organisations rely on humans to perform the actual quality\nassessment, but purely automatic methods have been developed for other similar\nproblems, like Machine Translation. This paper will try to compare several of\nthese methods: BLEU, EBLEU, NIST, METEOR, METEOR-PL, TER and RIBES. These will\nthen be matched to the human-derived NER metric, commonly used in re-speaking.\n", "versions": [{"version": "v1", "created": "Tue, 12 Jan 2016 10:06:52 GMT"}], "update_date": "2016-01-13", "authors_parsed": [["Wo\u0142k", "Krzysztof", ""], ["Kor\u017einek", "Danijel", ""]]}, {"id": "1601.03073", "submitter": "Gautam Reddy", "authors": "Gautam Reddy, Antonio Celani and Massimo Vergassola", "title": "Infomax strategies for an optimal balance between exploration and\n  exploitation", "comments": "16 pages, 5 figures", "journal-ref": null, "doi": "10.1007/s10955-016-1521-0", "report-no": null, "categories": "cs.LG cs.IT math.IT physics.data-an q-bio.PE stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Proper balance between exploitation and exploration is what makes good\ndecisions, which achieve high rewards like payoff or evolutionary fitness. The\nInfomax principle postulates that maximization of information directs the\nfunction of diverse systems, from living systems to artificial neural networks.\nWhile specific applications are successful, the validity of information as a\nproxy for reward remains unclear. Here, we consider the multi-armed bandit\ndecision problem, which features arms (slot-machines) of unknown probabilities\nof success and a player trying to maximize cumulative payoff by choosing the\nsequence of arms to play. We show that an Infomax strategy (Info-p) which\noptimally gathers information on the highest mean reward among the arms\nsaturates known optimal bounds and compares favorably to existing policies. The\nhighest mean reward considered by Info-p is not the quantity actually needed\nfor the choice of the arm to play, yet it allows for optimal tradeoffs between\nexploration and exploitation.\n", "versions": [{"version": "v1", "created": "Tue, 12 Jan 2016 21:50:03 GMT"}], "update_date": "2016-05-25", "authors_parsed": [["Reddy", "Gautam", ""], ["Celani", "Antonio", ""], ["Vergassola", "Massimo", ""]]}, {"id": "1601.03117", "submitter": "Guangyong Chen", "authors": "Fengyuan Zhu, Guangyong Chen, Jianye Hao, Pheng-Ann Heng", "title": "Blind Image Denoising via Dependent Dirichlet Process Tree", "comments": "25 pages, 11 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Most existing image denoising approaches assumed the noise to be homogeneous\nwhite Gaussian distributed with known intensity. However, in real noisy images,\nthe noise models are usually unknown beforehand and can be much more complex.\nThis paper addresses this problem and proposes a novel blind image denoising\nalgorithm to recover the clean image from noisy one with the unknown noise\nmodel. To model the empirical noise of an image, our method introduces the\nmixture of Gaussian distribution, which is flexible enough to approximate\ndifferent continuous distributions. The problem of blind image denoising is\nreformulated as a learning problem. The procedure is to first build a two-layer\nstructural model for noisy patches and consider the clean ones as latent\nvariable. To control the complexity of the noisy patch model, this work\nproposes a novel Bayesian nonparametric prior called \"Dependent Dirichlet\nProcess Tree\" to build the model. Then, this study derives a variational\ninference algorithm to estimate model parameters and recover clean patches. We\napply our method on synthesis and real noisy images with different noise\nmodels. Comparing with previous approaches, ours achieves better performance.\nThe experimental results indicate the efficiency of the proposed algorithm to\ncope with practical image denoising tasks.\n", "versions": [{"version": "v1", "created": "Wed, 13 Jan 2016 02:44:36 GMT"}], "update_date": "2016-01-14", "authors_parsed": [["Zhu", "Fengyuan", ""], ["Chen", "Guangyong", ""], ["Hao", "Jianye", ""], ["Heng", "Pheng-Ann", ""]]}, {"id": "1601.03124", "submitter": "Guangyong Chen", "authors": "Guangyong Chen, Fengyuan Zhu, Pheng Ann Heng", "title": "Online Prediction of Dyadic Data with Heterogeneous Matrix Factorization", "comments": "26 pages, 10 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Dyadic Data Prediction (DDP) is an important problem in many research areas.\nThis paper develops a novel fully Bayesian nonparametric framework which\nintegrates two popular and complementary approaches, discrete mixed membership\nmodeling and continuous latent factor modeling into a unified Heterogeneous\nMatrix Factorization~(HeMF) model, which can predict the unobserved dyadics\naccurately. The HeMF can determine the number of communities automatically and\nexploit the latent linear structure for each bicluster efficiently. We propose\na Variational Bayesian method to estimate the parameters and missing data. We\nfurther develop a novel online learning approach for Variational inference and\nuse it for the online learning of HeMF, which can efficiently cope with the\nimportant large-scale DDP problem. We evaluate the performance of our method on\nthe EachMoive, MovieLens and Netflix Prize collaborative filtering datasets.\nThe experiment shows that, our model outperforms state-of-the-art methods on\nall benchmarks. Compared with Stochastic Gradient Method (SGD), our online\nlearning approach achieves significant improvement on the estimation accuracy\nand robustness.\n", "versions": [{"version": "v1", "created": "Wed, 13 Jan 2016 04:20:09 GMT"}], "update_date": "2016-01-15", "authors_parsed": [["Chen", "Guangyong", ""], ["Zhu", "Fengyuan", ""], ["Heng", "Pheng Ann", ""]]}, {"id": "1601.03764", "submitter": "Yingyu Liang", "authors": "Sanjeev Arora, Yuanzhi Li, Yingyu Liang, Tengyu Ma, Andrej Risteski", "title": "Linear Algebraic Structure of Word Senses, with Applications to Polysemy", "comments": "Appear in the Transactions of the Association for Computational\n  Linguistics 2018, link:\n  https://transacl.org/ojs/index.php/tacl/article/view/1346", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Word embeddings are ubiquitous in NLP and information retrieval, but it is\nunclear what they represent when the word is polysemous. Here it is shown that\nmultiple word senses reside in linear superposition within the word embedding\nand simple sparse coding can recover vectors that approximately capture the\nsenses. The success of our approach, which applies to several embedding\nmethods, is mathematically explained using a variant of the random walk on\ndiscourses model (Arora et al., 2016). A novel aspect of our technique is that\neach extracted word sense is accompanied by one of about 2000 \"discourse atoms\"\nthat gives a succinct description of which other words co-occur with that word\nsense. Discourse atoms can be of independent interest, and make the method\npotentially more useful. Empirical tests are used to verify and support the\ntheory.\n", "versions": [{"version": "v1", "created": "Thu, 14 Jan 2016 22:02:18 GMT"}, {"version": "v2", "created": "Thu, 21 Jul 2016 15:22:43 GMT"}, {"version": "v3", "created": "Sun, 15 Jul 2018 08:08:39 GMT"}, {"version": "v4", "created": "Thu, 19 Jul 2018 00:06:35 GMT"}, {"version": "v5", "created": "Fri, 20 Jul 2018 15:26:24 GMT"}, {"version": "v6", "created": "Fri, 7 Dec 2018 17:30:03 GMT"}], "update_date": "2018-12-10", "authors_parsed": [["Arora", "Sanjeev", ""], ["Li", "Yuanzhi", ""], ["Liang", "Yingyu", ""], ["Ma", "Tengyu", ""], ["Risteski", "Andrej", ""]]}, {"id": "1601.03822", "submitter": "Hossein Keshavarz", "authors": "Hossein Keshavarz, Clayton Scott, XuanLong Nguyen", "title": "On the consistency of inversion-free parameter estimation for Gaussian\n  random fields", "comments": "41 pages, 2 Figures", "journal-ref": "Journal of Multivariate Analysis (2016), pp. 245-266", "doi": "10.1016/j.jmva.2016.06.003", "report-no": null, "categories": "math.ST cs.LG stat.ML stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Gaussian random fields are a powerful tool for modeling environmental\nprocesses. For high dimensional samples, classical approaches for estimating\nthe covariance parameters require highly challenging and massive computations,\nsuch as the evaluation of the Cholesky factorization or solving linear systems.\nRecently, Anitescu, Chen and Stein \\cite{M.Anitescu} proposed a fast and\nscalable algorithm which does not need such burdensome computations. The main\nfocus of this article is to study the asymptotic behavior of the algorithm of\nAnitescu et al. (ACS) for regular and irregular grids in the increasing domain\nsetting. Consistency, minimax optimality and asymptotic normality of this\nalgorithm are proved under mild differentiability conditions on the covariance\nfunction. Despite the fact that ACS's method entails a non-concave\nmaximization, our results hold for any stationary point of the objective\nfunction. A numerical study is presented to evaluate the efficiency of this\nalgorithm for large data sets.\n", "versions": [{"version": "v1", "created": "Fri, 15 Jan 2016 05:47:29 GMT"}, {"version": "v2", "created": "Tue, 21 Jun 2016 04:26:08 GMT"}], "update_date": "2016-10-05", "authors_parsed": [["Keshavarz", "Hossein", ""], ["Scott", "Clayton", ""], ["Nguyen", "XuanLong", ""]]}, {"id": "1601.03945", "submitter": "Alberto N. Escalante-B.", "authors": "Alberto N. Escalante-B. and Laurenz Wiskott", "title": "Improved graph-based SFA: Information preservation complements the\n  slowness principle", "comments": "40 pages, 9 figures, 9 tables, submitted to Pattern Recognition", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Slow feature analysis (SFA) is an unsupervised-learning algorithm that\nextracts slowly varying features from a multi-dimensional time series. A\nsupervised extension to SFA for classification and regression is graph-based\nSFA (GSFA). GSFA is based on the preservation of similarities, which are\nspecified by a graph structure derived from the labels. It has been shown that\nhierarchical GSFA (HGSFA) allows learning from images and other\nhigh-dimensional data. The feature space spanned by HGSFA is complex due to the\ncomposition of the nonlinearities of the nodes in the network. However, we show\nthat the network discards useful information prematurely before it reaches\nhigher nodes, resulting in suboptimal global slowness and an under-exploited\nfeature space.\n  To counteract these problems, we propose an extension called hierarchical\ninformation-preserving GSFA (HiGSFA), where information preservation\ncomplements the slowness-maximization goal. We build a 10-layer HiGSFA network\nto estimate human age from facial photographs of the MORPH-II database,\nachieving a mean absolute error of 3.50 years, improving the state-of-the-art\nperformance. HiGSFA and HGSFA support multiple-labels and offer a rich feature\nspace, feed-forward training, and linear complexity in the number of samples\nand dimensions. Furthermore, HiGSFA outperforms HGSFA in terms of feature\nslowness, estimation accuracy and input reconstruction, giving rise to a\npromising hierarchical supervised-learning approach.\n", "versions": [{"version": "v1", "created": "Fri, 15 Jan 2016 15:00:20 GMT"}], "update_date": "2016-01-18", "authors_parsed": [["Escalante-B.", "Alberto N.", ""], ["Wiskott", "Laurenz", ""]]}, {"id": "1601.03958", "submitter": "Benjamin Chamberlain", "authors": "Benjamin Paul Chamberlain, Josh Levy-Kramer, Clive Humby, Marc Peter\n  Deisenroth", "title": "Real-Time Community Detection in Large Social Networks on a Laptop", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.SI stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  For a broad range of research, governmental and commercial applications it is\nimportant to understand the allegiances, communities and structure of key\nplayers in society. One promising direction towards extracting this information\nis to exploit the rich relational data in digital social networks (the social\ngraph). As social media data sets are very large, most approaches make use of\ndistributed computing systems for this purpose. Distributing graph processing\nrequires solving many difficult engineering problems, which has lead some\nresearchers to look at single-machine solutions that are faster and easier to\nmaintain. In this article, we present a single-machine real-time system for\nlarge-scale graph processing that allows analysts to interactively explore\ngraph structures. The key idea is that the aggregate actions of large numbers\nof users can be compressed into a data structure that encapsulates user\nsimilarities while being robust to noise and queryable in real-time. We achieve\nsingle machine real-time performance by compressing the neighbourhood of each\nvertex using minhash signatures and facilitate rapid queries through Locality\nSensitive Hashing. These techniques reduce query times from hours using\nindustrial desktop machines operating on the full graph to milliseconds on\nstandard laptops. Our method allows exploration of strongly associated regions\n(i.e. communities) of large graphs in real-time on a laptop. It has been\ndeployed in software that is actively used by social network analysts and\noffers another channel for media owners to monetise their data, helping them to\ncontinue to provide free services that are valued by billions of people\nglobally.\n", "versions": [{"version": "v1", "created": "Fri, 15 Jan 2016 15:16:00 GMT"}, {"version": "v2", "created": "Sun, 4 Sep 2016 19:03:06 GMT"}], "update_date": "2016-09-06", "authors_parsed": [["Chamberlain", "Benjamin Paul", ""], ["Levy-Kramer", "Josh", ""], ["Humby", "Clive", ""], ["Deisenroth", "Marc Peter", ""]]}, {"id": "1601.04033", "submitter": "Augustus Odena", "authors": "Augustus Odena", "title": "Faster Asynchronous SGD", "comments": "10 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Asynchronous distributed stochastic gradient descent methods have trouble\nconverging because of stale gradients. A gradient update sent to a parameter\nserver by a client is stale if the parameters used to calculate that gradient\nhave since been updated on the server. Approaches have been proposed to\ncircumvent this problem that quantify staleness in terms of the number of\nelapsed updates. In this work, we propose a novel method that quantifies\nstaleness in terms of moving averages of gradient statistics. We show that this\nmethod outperforms previous methods with respect to convergence speed and\nscalability to many clients. We also discuss how an extension to this method\ncan be used to dramatically reduce bandwidth costs in a distributed training\ncontext. In particular, our method allows reduction of total bandwidth usage by\na factor of 5 with little impact on cost convergence. We also describe (and\nlink to) a software library that we have used to simulate these algorithms\ndeterministically on a single machine.\n", "versions": [{"version": "v1", "created": "Fri, 15 Jan 2016 19:03:47 GMT"}], "update_date": "2016-01-18", "authors_parsed": [["Odena", "Augustus", ""]]}, {"id": "1601.04126", "submitter": "Kush Varshney", "authors": "Kush R. Varshney", "title": "Engineering Safety in Machine Learning", "comments": "2016 Information Theory and Applications Workshop, La Jolla,\n  California", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.AI cs.CY cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Machine learning algorithms are increasingly influencing our decisions and\ninteracting with us in all parts of our daily lives. Therefore, just like for\npower plants, highways, and myriad other engineered sociotechnical systems, we\nmust consider the safety of systems involving machine learning. In this paper,\nwe first discuss the definition of safety in terms of risk, epistemic\nuncertainty, and the harm incurred by unwanted outcomes. Then we examine\ndimensions, such as the choice of cost function and the appropriateness of\nminimizing the empirical average training cost, along which certain real-world\napplications may not be completely amenable to the foundational principle of\nmodern statistical machine learning: empirical risk minimization. In\nparticular, we note an emerging dichotomy of applications: ones in which safety\nis important and risk minimization is not the complete story (we name these\nType A applications), and ones in which safety is not so critical and risk\nminimization is sufficient (we name these Type B applications). Finally, we\ndiscuss how four different strategies for achieving safety in engineering\n(inherently safe design, safety reserves, safe fail, and procedural safeguards)\ncan be mapped to the machine learning context through interpretability and\ncausality of predictive models, objectives beyond expected prediction accuracy,\nhuman involvement for labeling difficult or rare examples, and user experience\ndesign of software.\n", "versions": [{"version": "v1", "created": "Sat, 16 Jan 2016 05:46:57 GMT"}], "update_date": "2016-01-19", "authors_parsed": [["Varshney", "Kush R.", ""]]}, {"id": "1601.04251", "submitter": "Giulia Prando", "authors": "Diego Romeres, Giulia Prando, Gianluigi Pillonetto, Alessandro Chiuso", "title": "On-line Bayesian System Identification", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.SY cs.LG stat.AP stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider an on-line system identification setting, in which new data\nbecome available at given time steps. In order to meet real-time estimation\nrequirements, we propose a tailored Bayesian system identification procedure,\nin which the hyper-parameters are still updated through Marginal Likelihood\nmaximization, but after only one iteration of a suitable iterative optimization\nalgorithm. Both gradient methods and the EM algorithm are considered for the\nMarginal Likelihood optimization. We compare this \"1-step\" procedure with the\nstandard one, in which the optimization method is run until convergence to a\nlocal minimum. The experiments we perform confirm the effectiveness of the\napproach we propose.\n", "versions": [{"version": "v1", "created": "Sun, 17 Jan 2016 05:20:19 GMT"}], "update_date": "2016-01-19", "authors_parsed": [["Romeres", "Diego", ""], ["Prando", "Giulia", ""], ["Pillonetto", "Gianluigi", ""], ["Chiuso", "Alessandro", ""]]}, {"id": "1601.04366", "submitter": "Martin Stra\\v{z}ar Martin Stra\\v{z}ar", "authors": "Martin Stra\\v{z}ar, Toma\\v{z} Curk", "title": "Learning the kernel matrix via predictive low-rank approximations", "comments": "10 pages, 4 figures", "journal-ref": null, "doi": "10.1016/j.neucom.2019.02.030", "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Efficient and accurate low-rank approximations of multiple data sources are\nessential in the era of big data. The scaling of kernel-based learning\nalgorithms to large datasets is limited by the O(n^2) computation and storage\ncomplexity of the full kernel matrix, which is required by most of the recent\nkernel learning algorithms.\n  We present the Mklaren algorithm to approximate multiple kernel matrices\nlearn a regression model, which is entirely based on geometrical concepts. The\nalgorithm does not require access to full kernel matrices yet it accounts for\nthe correlations between all kernels. It uses Incomplete Cholesky\ndecomposition, where pivot selection is based on least-angle regression in the\ncombined, low-dimensional feature space. The algorithm has linear complexity in\nthe number of data points and kernels. When explicit feature space induced by\nthe kernel can be constructed, a mapping from the dual to the primal Ridge\nregression weights is used for model interpretation.\n  The Mklaren algorithm was tested on eight standard regression datasets. It\noutperforms contemporary kernel matrix approximation approaches when learning\nwith multiple kernels. It identifies relevant kernels, achieving highest\nexplained variance than other multiple kernel learning methods for the same\nnumber of iterations. Test accuracy, equivalent to the one using full kernel\nmatrices, was achieved with at significantly lower approximation ranks. A\ndifference in run times of two orders of magnitude was observed when either the\nnumber of samples or kernels exceeds 3000.\n", "versions": [{"version": "v1", "created": "Sun, 17 Jan 2016 23:31:37 GMT"}, {"version": "v2", "created": "Mon, 9 May 2016 16:05:42 GMT"}], "update_date": "2020-12-10", "authors_parsed": [["Stra\u017ear", "Martin", ""], ["Curk", "Toma\u017e", ""]]}, {"id": "1601.04451", "submitter": "Robert Duin", "authors": "Robert P.W. Duin, Elzbieta Pekalska", "title": "Zero-error dissimilarity based classifiers", "comments": "5 pages. Paper originally written in 2003. Although it may proof an\n  obvious fact, it is significant for understanding the essential conditions it\n  is based on", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider general non-Euclidean distance measures between real world\nobjects that need to be classified. It is assumed that objects are represented\nby distances to other objects only. Conditions for zero-error dissimilarity\nbased classifiers are derived. Additional conditions are given under which the\nzero-error decision boundary is a continues function of the distances to a\nfinite set of training samples. These conditions affect the objects as well as\nthe distance measure used. It is argued that they can be met in practice.\n", "versions": [{"version": "v1", "created": "Mon, 18 Jan 2016 10:12:15 GMT"}], "update_date": "2016-01-19", "authors_parsed": [["Duin", "Robert P. W.", ""], ["Pekalska", "Elzbieta", ""]]}, {"id": "1601.04530", "submitter": "Robert Duin", "authors": "Robert P.W. Duin, Elzbieta Pekalska", "title": "Domain based classification", "comments": "8 pages, unpublished paper written in 2005, discussing a significant,\n  still almost not studied problem. missing reference links corrected", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The majority of traditional classification ru les minimizing the expected\nprobability of error (0-1 loss) are inappropriate if the class probability\ndistributions are ill-defined or impossible to estimate. We argue that in such\ncases class domains should be used instead of class distributions or densities\nto construct a reliable decision function. Proposals are presented for some\nevaluation criteria and classifier learning schemes, illustrated by an example.\n", "versions": [{"version": "v1", "created": "Mon, 18 Jan 2016 14:31:12 GMT"}, {"version": "v2", "created": "Sat, 11 Aug 2018 16:30:35 GMT"}], "update_date": "2018-08-14", "authors_parsed": [["Duin", "Robert P. W.", ""], ["Pekalska", "Elzbieta", ""]]}, {"id": "1601.04549", "submitter": "Raffaello Camoriano", "authors": "Raffaello Camoriano, Silvio Traversaro, Lorenzo Rosasco, Giorgio Metta\n  and Francesco Nori", "title": "Incremental Semiparametric Inverse Dynamics Learning", "comments": null, "journal-ref": null, "doi": "10.1109/ICRA.2016.7487177", "report-no": null, "categories": "stat.ML cs.LG cs.RO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper presents a novel approach for incremental semiparametric inverse\ndynamics learning. In particular, we consider the mixture of two approaches:\nParametric modeling based on rigid body dynamics equations and nonparametric\nmodeling based on incremental kernel methods, with no prior information on the\nmechanical properties of the system. This yields to an incremental\nsemiparametric approach, leveraging the advantages of both the parametric and\nnonparametric models. We validate the proposed technique learning the dynamics\nof one arm of the iCub humanoid robot.\n", "versions": [{"version": "v1", "created": "Mon, 18 Jan 2016 14:54:37 GMT"}], "update_date": "2021-03-25", "authors_parsed": [["Camoriano", "Raffaello", ""], ["Traversaro", "Silvio", ""], ["Rosasco", "Lorenzo", ""], ["Metta", "Giorgio", ""], ["Nori", "Francesco", ""]]}, {"id": "1601.04586", "submitter": "Binhuan Wang", "authors": "Binhuan Wang, Yilong Zhang, Will Wei Sun, Yixin Fang", "title": "Sparse Convex Clustering", "comments": null, "journal-ref": null, "doi": "10.1080/10618600.2017.1377081", "report-no": null, "categories": "stat.ME cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Convex clustering, a convex relaxation of k-means clustering and hierarchical\nclustering, has drawn recent attentions since it nicely addresses the\ninstability issue of traditional nonconvex clustering methods. Although its\ncomputational and statistical properties have been recently studied, the\nperformance of convex clustering has not yet been investigated in the\nhigh-dimensional clustering scenario, where the data contains a large number of\nfeatures and many of them carry no information about the clustering structure.\nIn this paper, we demonstrate that the performance of convex clustering could\nbe distorted when the uninformative features are included in the clustering. To\novercome it, we introduce a new clustering method, referred to as Sparse Convex\nClustering, to simultaneously cluster observations and conduct feature\nselection. The key idea is to formulate convex clustering in a form of\nregularization, with an adaptive group-lasso penalty term on cluster centers.\nIn order to optimally balance the tradeoff between the cluster fitting and\nsparsity, a tuning criterion based on clustering stability is developed. In\ntheory, we provide an unbiased estimator for the degrees of freedom of the\nproposed sparse convex clustering method. Finally, the effectiveness of the\nsparse convex clustering is examined through a variety of numerical experiments\nand a real data application.\n", "versions": [{"version": "v1", "created": "Mon, 18 Jan 2016 16:03:35 GMT"}, {"version": "v2", "created": "Thu, 21 Jan 2016 03:26:26 GMT"}, {"version": "v3", "created": "Tue, 18 Oct 2016 14:38:43 GMT"}, {"version": "v4", "created": "Fri, 10 Feb 2017 16:51:07 GMT"}], "update_date": "2019-01-01", "authors_parsed": [["Wang", "Binhuan", ""], ["Zhang", "Yilong", ""], ["Sun", "Will Wei", ""], ["Fang", "Yixin", ""]]}, {"id": "1601.04621", "submitter": "Benjamin Chamberlain", "authors": "Benjamin Paul Chamberlain, Clive Humby, Marc Peter Deisenroth", "title": "Probabilistic Inference of Twitter Users' Age based on What They Follow", "comments": "9 pages, 9 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.SI stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Twitter provides an open and rich source of data for studying human behaviour\nat scale and is widely used in social and network sciences. However, a major\ncriticism of Twitter data is that demographic information is largely absent.\nEnhancing Twitter data with user ages would advance our ability to study social\nnetwork structures, information flows and the spread of contagions. Approaches\ntoward age detection of Twitter users typically focus on specific properties of\ntweets, e.g., linguistic features, which are language dependent. In this paper,\nwe devise a language-independent methodology for determining the age of Twitter\nusers from data that is native to the Twitter ecosystem. The key idea is to use\na Bayesian framework to generalise ground-truth age information from a few\nTwitter users to the entire network based on what/whom they follow. Our\napproach scales to inferring the age of 700 million Twitter accounts with high\naccuracy.\n", "versions": [{"version": "v1", "created": "Mon, 18 Jan 2016 17:40:56 GMT"}, {"version": "v2", "created": "Fri, 24 Feb 2017 15:02:37 GMT"}], "update_date": "2017-02-27", "authors_parsed": [["Chamberlain", "Benjamin Paul", ""], ["Humby", "Clive", ""], ["Deisenroth", "Marc Peter", ""]]}, {"id": "1601.04650", "submitter": "Madhu Advani", "authors": "Madhu Advani and Surya Ganguli", "title": "Statistical Mechanics of High-Dimensional Inference", "comments": "See http://ganguli-gang.stanford.edu/pdf/HighDimInf.Supp.pdf for\n  supplementary material", "journal-ref": "Phys. Rev. X 6, 031034 (2016)", "doi": "10.1103/PhysRevX.6.031034", "report-no": null, "categories": "stat.ML cond-mat.dis-nn cond-mat.stat-mech math.ST q-bio.QM stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  To model modern large-scale datasets, we need efficient algorithms to infer a\nset of $P$ unknown model parameters from $N$ noisy measurements. What are\nfundamental limits on the accuracy of parameter inference, given finite\nsignal-to-noise ratios, limited measurements, prior information, and\ncomputational tractability requirements? How can we combine prior information\nwith measurements to achieve these limits? Classical statistics gives incisive\nanswers to these questions as the measurement density $\\alpha =\n\\frac{N}{P}\\rightarrow \\infty$. However, these classical results are not\nrelevant to modern high-dimensional inference problems, which instead occur at\nfinite $\\alpha$. We formulate and analyze high-dimensional inference as a\nproblem in the statistical physics of quenched disorder. Our analysis uncovers\nfundamental limits on the accuracy of inference in high dimensions, and reveals\nthat widely cherished inference algorithms like maximum likelihood (ML) and\nmaximum-a posteriori (MAP) inference cannot achieve these limits. We further\nfind optimal, computationally tractable algorithms that can achieve these\nlimits. Intriguingly, in high dimensions, these optimal algorithms become\ncomputationally simpler than MAP and ML, while still outperforming them. For\nexample, such optimal algorithms can lead to as much as a 20% reduction in the\namount of data to achieve the same performance relative to MAP. Moreover, our\nanalysis reveals simple relations between optimal high dimensional inference\nand low dimensional scalar Bayesian inference, insights into the nature of\ngeneralization and predictive power in high dimensions, information theoretic\nlimits on compressed sensing, phase transitions in quadratic inference, and\nconnections to central mathematical objects in convex optimization theory and\nrandom matrix theory.\n", "versions": [{"version": "v1", "created": "Mon, 18 Jan 2016 18:38:35 GMT"}, {"version": "v2", "created": "Mon, 22 Feb 2016 03:10:56 GMT"}], "update_date": "2016-09-07", "authors_parsed": [["Advani", "Madhu", ""], ["Ganguli", "Surya", ""]]}, {"id": "1601.04674", "submitter": "Peter Schulam", "authors": "Peter Schulam and Suchi Saria", "title": "A Framework for Individualizing Predictions of Disease Trajectories by\n  Exploiting Multi-Resolution Structure", "comments": "Appeared in Neural Information Processing Systems (NIPS) 2015", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  For many complex diseases, there is a wide variety of ways in which an\nindividual can manifest the disease. The challenge of personalized medicine is\nto develop tools that can accurately predict the trajectory of an individual's\ndisease, which can in turn enable clinicians to optimize treatments. We\nrepresent an individual's disease trajectory as a continuous-valued\ncontinuous-time function describing the severity of the disease over time. We\npropose a hierarchical latent variable model that individualizes predictions of\ndisease trajectories. This model shares statistical strength across\nobservations at different resolutions--the population, subpopulation and the\nindividual level. We describe an algorithm for learning population and\nsubpopulation parameters offline, and an online procedure for dynamically\nlearning individual-specific parameters. Finally, we validate our model on the\ntask of predicting the course of interstitial lung disease, a leading cause of\ndeath among patients with the autoimmune disease scleroderma. We compare our\napproach against state-of-the-art and demonstrate significant improvements in\npredictive accuracy.\n", "versions": [{"version": "v1", "created": "Mon, 18 Jan 2016 20:01:16 GMT"}, {"version": "v2", "created": "Thu, 21 Jan 2016 19:26:30 GMT"}], "update_date": "2016-01-22", "authors_parsed": [["Schulam", "Peter", ""], ["Saria", "Suchi", ""]]}, {"id": "1601.04737", "submitter": "Farbod Roosta-Khorasani", "authors": "Farbod Roosta-Khorasani and Michael W. Mahoney", "title": "Sub-Sampled Newton Methods I: Globally Convergent Algorithms", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.OC cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Large scale optimization problems are ubiquitous in machine learning and data\nanalysis and there is a plethora of algorithms for solving such problems. Many\nof these algorithms employ sub-sampling, as a way to either speed up the\ncomputations and/or to implicitly implement a form of statistical\nregularization. In this paper, we consider second-order iterative optimization\nalgorithms and we provide bounds on the convergence of the variants of Newton's\nmethod that incorporate uniform sub-sampling as a means to estimate the\ngradient and/or Hessian. Our bounds are non-asymptotic and quantitative. Our\nalgorithms are global and are guaranteed to converge from any initial iterate.\n  Using random matrix concentration inequalities, one can sub-sample the\nHessian to preserve the curvature information. Our first algorithm incorporates\nHessian sub-sampling while using the full gradient. We also give additional\nconvergence results for when the sub-sampled Hessian is regularized by\nmodifying its spectrum or ridge-type regularization. Next, in addition to\nHessian sub-sampling, we also consider sub-sampling the gradient as a way to\nfurther reduce the computational complexity per iteration. We use approximate\nmatrix multiplication results from randomized numerical linear algebra to\nobtain the proper sampling strategy. In all these algorithms, computing the\nupdate boils down to solving a large scale linear system, which can be\ncomputationally expensive. As a remedy, for all of our algorithms, we also give\nglobal convergence results for the case of inexact updates where such linear\nsystem is solved only approximately.\n  This paper has a more advanced companion paper, [42], in which we demonstrate\nthat, by doing a finer-grained analysis, we can get problem-independent bounds\nfor local convergence of these algorithms and explore trade-offs to improve\nupon the basic results of the present paper.\n", "versions": [{"version": "v1", "created": "Mon, 18 Jan 2016 21:59:21 GMT"}, {"version": "v2", "created": "Wed, 27 Jan 2016 01:16:32 GMT"}, {"version": "v3", "created": "Fri, 26 Feb 2016 04:04:24 GMT"}], "update_date": "2016-02-29", "authors_parsed": [["Roosta-Khorasani", "Farbod", ""], ["Mahoney", "Michael W.", ""]]}, {"id": "1601.04738", "submitter": "Farbod Roosta-Khorasani", "authors": "Farbod Roosta-Khorasani and Michael W. Mahoney", "title": "Sub-Sampled Newton Methods II: Local Convergence Rates", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.OC cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Many data-fitting applications require the solution of an optimization\nproblem involving a sum of large number of functions of high dimensional\nparameter. Here, we consider the problem of minimizing a sum of $n$ functions\nover a convex constraint set $\\mathcal{X} \\subseteq \\mathbb{R}^{p}$ where both\n$n$ and $p$ are large. In such problems, sub-sampling as a way to reduce $n$\ncan offer great amount of computational efficiency.\n  Within the context of second order methods, we first give quantitative local\nconvergence results for variants of Newton's method where the Hessian is\nuniformly sub-sampled. Using random matrix concentration inequalities, one can\nsub-sample in a way that the curvature information is preserved. Using such\nsub-sampling strategy, we establish locally Q-linear and Q-superlinear\nconvergence rates. We also give additional convergence results for when the\nsub-sampled Hessian is regularized by modifying its spectrum or Levenberg-type\nregularization.\n  Finally, in addition to Hessian sub-sampling, we consider sub-sampling the\ngradient as way to further reduce the computational complexity per iteration.\nWe use approximate matrix multiplication results from randomized numerical\nlinear algebra (RandNLA) to obtain the proper sampling strategy and we\nestablish locally R-linear convergence rates. In such a setting, we also show\nthat a very aggressive sample size increase results in a R-superlinearly\nconvergent algorithm.\n  While the sample size depends on the condition number of the problem, our\nconvergence rates are problem-independent, i.e., they do not depend on the\nquantities related to the problem. Hence, our analysis here can be used to\ncomplement the results of our basic framework from the companion paper, [38],\nby exploring algorithmic trade-offs that are important in practice.\n", "versions": [{"version": "v1", "created": "Mon, 18 Jan 2016 22:04:32 GMT"}, {"version": "v2", "created": "Wed, 27 Jan 2016 01:19:22 GMT"}, {"version": "v3", "created": "Fri, 26 Feb 2016 04:06:39 GMT"}], "update_date": "2016-02-29", "authors_parsed": [["Roosta-Khorasani", "Farbod", ""], ["Mahoney", "Michael W.", ""]]}, {"id": "1601.04800", "submitter": "Zhao Kang", "authors": "Zhao Kang, Chong Peng, Qiang Cheng", "title": "Top-N Recommender System via Matrix Completion", "comments": "AAAI 2016", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR cs.AI cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Top-N recommender systems have been investigated widely both in industry and\nacademia. However, the recommendation quality is far from satisfactory. In this\npaper, we propose a simple yet promising algorithm. We fill the user-item\nmatrix based on a low-rank assumption and simultaneously keep the original\ninformation. To do that, a nonconvex rank relaxation rather than the nuclear\nnorm is adopted to provide a better rank approximation and an efficient\noptimization strategy is designed. A comprehensive set of experiments on real\ndatasets demonstrates that our method pushes the accuracy of Top-N\nrecommendation to a new level.\n", "versions": [{"version": "v1", "created": "Tue, 19 Jan 2016 04:48:42 GMT"}], "update_date": "2016-01-20", "authors_parsed": [["Kang", "Zhao", ""], ["Peng", "Chong", ""], ["Cheng", "Qiang", ""]]}, {"id": "1601.04920", "submitter": "St\\'ephane Mallat", "authors": "St\\'ephane Mallat", "title": "Understanding Deep Convolutional Networks", "comments": "17 pages, 4 Figures", "journal-ref": null, "doi": "10.1098/rsta.2015.0203", "report-no": null, "categories": "stat.ML cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Deep convolutional networks provide state of the art classifications and\nregressions results over many high-dimensional problems. We review their\narchitecture, which scatters data with a cascade of linear filter weights and\nnon-linearities. A mathematical framework is introduced to analyze their\nproperties. Computations of invariants involve multiscale contractions, the\nlinearization of hierarchical symmetries, and sparse separations. Applications\nare discussed.\n", "versions": [{"version": "v1", "created": "Tue, 19 Jan 2016 13:40:47 GMT"}], "update_date": "2016-04-27", "authors_parsed": [["Mallat", "St\u00e9phane", ""]]}, {"id": "1601.05011", "submitter": "Tristan van Leeuwen", "authors": "Tristan van Leeuwen and Aleksandr Aravkin", "title": "Non-smooth Variable Projection", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.OC stat.CO stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Variable projection solves structured optimization problems by completely\nminimizing over a subset of the variables while iterating over the remaining\nvariables. Over the last 30 years, the technique has been widely used, with\nempirical and theoretical results demonstrating both greater efficacy and\ngreater stability compared to competing approaches. Classic examples have\nexploited closed-form projections and smoothness of the objective function. We\nextend the approach to problems that include non-smooth terms, and where the\nprojection subproblems can only be solved inexactly by iterative methods. We\npropose an inexact adaptive algonrithm for solving such problems and analyze\nits computational complexity. Finally, we show how the theory can be used to\ndesign methods for selected problems occurring frequently in machine-learning\nand inverse problems.\n", "versions": [{"version": "v1", "created": "Tue, 19 Jan 2016 17:38:29 GMT"}, {"version": "v2", "created": "Thu, 24 Aug 2017 20:40:06 GMT"}, {"version": "v3", "created": "Wed, 18 Jul 2018 14:43:25 GMT"}, {"version": "v4", "created": "Sun, 2 Dec 2018 12:40:01 GMT"}, {"version": "v5", "created": "Tue, 30 Jun 2020 11:41:26 GMT"}, {"version": "v6", "created": "Fri, 20 Nov 2020 14:54:07 GMT"}], "update_date": "2020-11-23", "authors_parsed": [["van Leeuwen", "Tristan", ""], ["Aravkin", "Aleksandr", ""]]}, {"id": "1601.05285", "submitter": "Tianwei Yu", "authors": "Tianwei Yu", "title": "Nonlinear variable selection with continuous outcome: a nonparametric\n  incremental forward stagewise approach", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a method of variable selection for the sparse generalized additive\nmodel. The method doesn't assume any specific functional form, and can select\nfrom a large number of candidates. It takes the form of incremental forward\nstagewise regression. Given no functional form is assumed, we devised an\napproach termed roughening to adjust the residuals in the iterations. In\nsimulations, we show the new method is competitive against popular machine\nlearning approaches. We also demonstrate its performance using some real\ndatasets. The method is available as a part of the nlnet package on CRAN\nhttps://cran.r-project.org/package=nlnet.\n", "versions": [{"version": "v1", "created": "Wed, 20 Jan 2016 14:44:54 GMT"}, {"version": "v2", "created": "Fri, 25 Mar 2016 18:31:31 GMT"}, {"version": "v3", "created": "Mon, 7 Nov 2016 14:27:14 GMT"}, {"version": "v4", "created": "Sat, 26 May 2018 07:00:47 GMT"}], "update_date": "2018-05-29", "authors_parsed": [["Yu", "Tianwei", ""]]}, {"id": "1601.05495", "submitter": "Sewoong Oh", "authors": "Ashish Khetan and Sewoong Oh", "title": "Data-driven Rank Breaking for Efficient Rank Aggregation", "comments": "46 pages, 12 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Rank aggregation systems collect ordinal preferences from individuals to\nproduce a global ranking that represents the social preference. Rank-breaking\nis a common practice to reduce the computational complexity of learning the\nglobal ranking. The individual preferences are broken into pairwise comparisons\nand applied to efficient algorithms tailored for independent paired\ncomparisons. However, due to the ignored dependencies in the data, naive\nrank-breaking approaches can result in inconsistent estimates. The key idea to\nproduce accurate and consistent estimates is to treat the pairwise comparisons\nunequally, depending on the topology of the collected data. In this paper, we\nprovide the optimal rank-breaking estimator, which not only achieves\nconsistency but also achieves the best error bound. This allows us to\ncharacterize the fundamental tradeoff between accuracy and complexity. Further,\nthe analysis identifies how the accuracy depends on the spectral gap of a\ncorresponding comparison graph.\n", "versions": [{"version": "v1", "created": "Thu, 21 Jan 2016 02:39:39 GMT"}, {"version": "v2", "created": "Fri, 7 Oct 2016 14:28:42 GMT"}], "update_date": "2016-10-10", "authors_parsed": [["Khetan", "Ashish", ""], ["Oh", "Sewoong", ""]]}, {"id": "1601.05675", "submitter": "Daniele Calandriello", "authors": "Daniele Calandriello, Alessandro Lazaric, Michal Valko and Ioannis\n  Koutis", "title": "Incremental Spectral Sparsification for Large-Scale Graph-Based\n  Semi-Supervised Learning", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  While the harmonic function solution performs well in many semi-supervised\nlearning (SSL) tasks, it is known to scale poorly with the number of samples.\nRecent successful and scalable methods, such as the eigenfunction method focus\non efficiently approximating the whole spectrum of the graph Laplacian\nconstructed from the data. This is in contrast to various subsampling and\nquantization methods proposed in the past, which may fail in preserving the\ngraph spectra. However, the impact of the approximation of the spectrum on the\nfinal generalization error is either unknown, or requires strong assumptions on\nthe data. In this paper, we introduce Sparse-HFS, an efficient\nedge-sparsification algorithm for SSL. By constructing an edge-sparse and\nspectrally similar graph, we are able to leverage the approximation guarantees\nof spectral sparsification methods to bound the generalization error of\nSparse-HFS. As a result, we obtain a theoretically-grounded approximation\nscheme for graph-based SSL that also empirically matches the performance of\nknown large-scale methods.\n", "versions": [{"version": "v1", "created": "Thu, 21 Jan 2016 15:31:35 GMT"}], "update_date": "2016-01-22", "authors_parsed": [["Calandriello", "Daniele", ""], ["Lazaric", "Alessandro", ""], ["Valko", "Michal", ""], ["Koutis", "Ioannis", ""]]}, {"id": "1601.05775", "submitter": "Twan van Laarhoven", "authors": "Twan van Laarhoven, Elena Marchiori", "title": "Local Network Community Detection with Continuous Optimization of\n  Conductance and Weighted Kernel K-Means", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.SI cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Local network community detection is the task of finding a single community\nof nodes concentrated around few given seed nodes in a localized way.\nConductance is a popular objective function used in many algorithms for local\ncommunity detection. This paper studies a continuous relaxation of conductance.\nWe show that continuous optimization of this objective still leads to discrete\ncommunities. We investigate the relation of conductance with weighted kernel\nk-means for a single community, which leads to the introduction of a new\nobjective function, $\\sigma$-conductance. Conductance is obtained by setting\n$\\sigma$ to $0$. Two algorithms, EMc and PGDc, are proposed to locally optimize\n$\\sigma$-conductance and automatically tune the parameter $\\sigma$. They are\nbased on expectation maximization and projected gradient descent, respectively.\nWe prove locality and give performance guarantees for EMc and PGDc for a class\nof dense and well separated communities centered around the seeds. Experiments\nare conducted on networks with ground-truth communities, comparing to\nstate-of-the-art graph diffusion algorithms for conductance optimization. On\nlarge graphs, results indicate that EMc and PGDc stay localized and produce\ncommunities most similar to the ground, while graph diffusion algorithms\ngenerate large communities of lower quality.\n", "versions": [{"version": "v1", "created": "Thu, 21 Jan 2016 20:36:59 GMT"}, {"version": "v2", "created": "Wed, 17 Aug 2016 18:03:01 GMT"}], "update_date": "2016-08-18", "authors_parsed": [["van Laarhoven", "Twan", ""], ["Marchiori", "Elena", ""]]}, {"id": "1601.05834", "submitter": "Hoi-To Wai", "authors": "Hoi-To Wai and Anna Scaglione and Amir Leshem", "title": "Active Sensing of Social Networks", "comments": "Final version appeared in IEEE Transactions on Signal and Information\n  Processing over Networks ( Volume: 2, Issue: 3, Sept. 2016 )", "journal-ref": null, "doi": "10.1109/TSIPN.2016.2555785", "report-no": null, "categories": "cs.SI stat.ML", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  This paper develops an active sensing method to estimate the relative weight\n(or trust) agents place on their neighbors' information in a social network.\nThe model used for the regression is based on the steady state equation in the\nlinear DeGroot model under the influence of stubborn agents, i.e., agents whose\nopinions are not influenced by their neighbors. This method can be viewed as a\n\\emph{social RADAR}, where the stubborn agents excite the system and the latter\ncan be estimated through the reverberation observed from the analysis of the\nagents' opinions. The social network sensing problem can be interpreted as a\nblind compressed sensing problem with a sparse measurement matrix. We prove\nthat the network structure will be revealed when a sufficient number of\nstubborn agents independently influence a number of ordinary (non-stubborn)\nagents. We investigate the scenario with a deterministic or randomized DeGroot\nmodel and propose a consistent estimator of the steady states for the latter\nscenario. Simulation results on synthetic and real world networks support our\nfindings.\n", "versions": [{"version": "v1", "created": "Thu, 21 Jan 2016 22:41:31 GMT"}, {"version": "v2", "created": "Wed, 5 Oct 2016 06:14:18 GMT"}], "update_date": "2016-10-06", "authors_parsed": [["Wai", "Hoi-To", ""], ["Scaglione", "Anna", ""], ["Leshem", "Amir", ""]]}, {"id": "1601.05936", "submitter": "Pranay Dighe", "authors": "Pranay Dighe, Gil Luyet, Afsaneh Asaei and Herve Bourlard", "title": "Exploiting Low-dimensional Structures to Enhance DNN Based Acoustic\n  Modeling in Speech Recognition", "comments": null, "journal-ref": null, "doi": "10.1109/ICASSP.2016.7472767", "report-no": null, "categories": "cs.CL cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose to model the acoustic space of deep neural network (DNN)\nclass-conditional posterior probabilities as a union of low-dimensional\nsubspaces. To that end, the training posteriors are used for dictionary\nlearning and sparse coding. Sparse representation of the test posteriors using\nthis dictionary enables projection to the space of training data. Relying on\nthe fact that the intrinsic dimensions of the posterior subspaces are indeed\nvery small and the matrix of all posteriors belonging to a class has a very low\nrank, we demonstrate how low-dimensional structures enable further enhancement\nof the posteriors and rectify the spurious errors due to mismatch conditions.\nThe enhanced acoustic modeling method leads to improvements in continuous\nspeech recognition task using hybrid DNN-HMM (hidden Markov model) framework in\nboth clean and noisy conditions, where upto 15.4% relative reduction in word\nerror rate (WER) is achieved.\n", "versions": [{"version": "v1", "created": "Fri, 22 Jan 2016 10:02:47 GMT"}], "update_date": "2017-09-07", "authors_parsed": [["Dighe", "Pranay", ""], ["Luyet", "Gil", ""], ["Asaei", "Afsaneh", ""], ["Bourlard", "Herve", ""]]}, {"id": "1601.06035", "submitter": "Cyril Stark", "authors": "Cyril Stark", "title": "Recommender systems inspired by the structure of quantum theory", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.IT math.IT math.OC quant-ph stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Physicists use quantum models to describe the behavior of physical systems.\nQuantum models owe their success to their interpretability, to their relation\nto probabilistic models (quantization of classical models) and to their high\npredictive power. Beyond physics, these properties are valuable in general data\nscience. This motivates the use of quantum models to analyze general\nnonphysical datasets. Here we provide both empirical and theoretical insights\ninto the application of quantum models in data science. In the theoretical part\nof this paper, we firstly show that quantum models can be exponentially more\nefficient than probabilistic models because there exist datasets that admit\nlow-dimensional quantum models and only exponentially high-dimensional\nprobabilistic models. Secondly, we explain in what sense quantum models realize\na useful relaxation of compressed probabilistic models. Thirdly, we show that\nsparse datasets admit low-dimensional quantum models and finally, we introduce\na method to compute hierarchical orderings of properties of users (e.g.,\npersonality traits) and items (e.g., genres of movies). In the empirical part\nof the paper, we evaluate quantum models in item recommendation and observe\nthat the predictive power of quantum-inspired recommender systems can compete\nwith state-of-the-art recommender systems like SVD++ and PureSVD. Furthermore,\nwe make use of the interpretability of quantum models by computing hierarchical\norderings of properties of users and items. This work establishes a connection\nbetween data science (item recommendation), information theory (communication\ncomplexity), mathematical programming (positive semidefinite factorizations)\nand physics (quantum models).\n", "versions": [{"version": "v1", "created": "Fri, 22 Jan 2016 15:09:18 GMT"}], "update_date": "2016-01-25", "authors_parsed": [["Stark", "Cyril", ""]]}, {"id": "1601.06105", "submitter": "Venkatesh Saligrama", "authors": "Jonathan Root, Venkatesh Saligrama, Jing Qian", "title": "Learning Minimum Volume Sets and Anomaly Detectors from KNN Graphs", "comments": "arXiv admin note: substantial text overlap with arXiv:1502.01783,\n  arXiv:1405.0530", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a non-parametric anomaly detection algorithm for high dimensional\ndata. We first rank scores derived from nearest neighbor graphs on $n$-point\nnominal training data. We then train limited complexity models to imitate these\nscores based on the max-margin learning-to-rank framework. A test-point is\ndeclared as an anomaly at $\\alpha$-false alarm level if the predicted score is\nin the $\\alpha$-percentile. The resulting anomaly detector is shown to be\nasymptotically optimal in that for any false alarm rate $\\alpha$, its decision\nregion converges to the $\\alpha$-percentile minimum volume level set of the\nunknown underlying density. In addition, we test both the statistical\nperformance and computational efficiency of our algorithm on a number of\nsynthetic and real-data experiments. Our results demonstrate the superiority of\nour algorithm over existing $K$-NN based anomaly detection algorithms, with\nsignificant computational savings.\n", "versions": [{"version": "v1", "created": "Fri, 22 Jan 2016 19:10:31 GMT"}], "update_date": "2016-01-25", "authors_parsed": [["Root", "Jonathan", ""], ["Saligrama", "Venkatesh", ""], ["Qian", "Jing", ""]]}, {"id": "1601.06116", "submitter": "James Mnatzaganian", "authors": "James Mnatzaganian, Ernest Fokou\\'e, and Dhireesha Kudithipudi", "title": "A Mathematical Formalization of Hierarchical Temporal Memory's Spatial\n  Pooler", "comments": "This work was submitted for publication and is currently under\n  review. For associated code, see https://github.com/tehtechguy/mHTM", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG q-bio.NC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Hierarchical temporal memory (HTM) is an emerging machine learning algorithm,\nwith the potential to provide a means to perform predictions on spatiotemporal\ndata. The algorithm, inspired by the neocortex, currently does not have a\ncomprehensive mathematical framework. This work brings together all aspects of\nthe spatial pooler (SP), a critical learning component in HTM, under a single\nunifying framework. The primary learning mechanism is explored, where a maximum\nlikelihood estimator for determining the degree of permanence update is\nproposed. The boosting mechanisms are studied and found to be only relevant\nduring the initial few iterations of the network. Observations are made\nrelating HTM to well-known algorithms such as competitive learning and\nattribute bagging. Methods are provided for using the SP for classification as\nwell as dimensionality reduction. Empirical evidence verifies that given the\nproper parameterizations, the SP may be used for feature learning.\n", "versions": [{"version": "v1", "created": "Fri, 22 Jan 2016 19:26:16 GMT"}, {"version": "v2", "created": "Thu, 31 Mar 2016 02:26:49 GMT"}, {"version": "v3", "created": "Thu, 8 Sep 2016 20:15:01 GMT"}], "update_date": "2016-09-12", "authors_parsed": [["Mnatzaganian", "James", ""], ["Fokou\u00e9", "Ernest", ""], ["Kudithipudi", "Dhireesha", ""]]}, {"id": "1601.06201", "submitter": "Prashant Khanduri", "authors": "Prashant Khanduri, Bhavya Kailkhura, Jayaraman J. Thiagarajan, Pramod\n  K. Varshney", "title": "Universal Collaboration Strategies for Signal Detection: A Sparse\n  Learning Approach", "comments": null, "journal-ref": null, "doi": "10.1109/LSP.2016.2601911", "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper considers the problem of high dimensional signal detection in a\nlarge distributed network whose nodes can collaborate with their one-hop\nneighboring nodes (spatial collaboration). We assume that only a small subset\nof nodes communicate with the Fusion Center (FC). We design optimal\ncollaboration strategies which are universal for a class of deterministic\nsignals. By establishing the equivalence between the collaboration strategy\ndesign problem and sparse PCA, we solve the problem efficiently and evaluate\nthe impact of collaboration on detection performance.\n", "versions": [{"version": "v1", "created": "Fri, 22 Jan 2016 23:15:42 GMT"}, {"version": "v2", "created": "Tue, 28 Jun 2016 21:35:46 GMT"}], "update_date": "2016-09-21", "authors_parsed": [["Khanduri", "Prashant", ""], ["Kailkhura", "Bhavya", ""], ["Thiagarajan", "Jayaraman J.", ""], ["Varshney", "Pramod K.", ""]]}, {"id": "1601.06207", "submitter": "Alican Nalci", "authors": "Alican Nalci, Igor Fedorov, Maher Al-Shoukairi, Thomas T. Liu, and\n  Bhaskar D. Rao", "title": "Rectified Gaussian Scale Mixtures and the Sparse Non-Negative Least\n  Squares Problem", "comments": "Under Review by IEEE Transactions on Signal Processing", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we develop a Bayesian evidence maximization framework to solve\nthe sparse non-negative least squares (S-NNLS) problem. We introduce a family\nof probability densities referred to as the Rectified Gaussian Scale Mixture\n(R- GSM) to model the sparsity enforcing prior distribution for the solution.\nThe R-GSM prior encompasses a variety of heavy-tailed densities such as the\nrectified Laplacian and rectified Student- t distributions with a proper choice\nof the mixing density. We utilize the hierarchical representation induced by\nthe R-GSM prior and develop an evidence maximization framework based on the\nExpectation-Maximization (EM) algorithm. Using the EM based method, we estimate\nthe hyper-parameters and obtain a point estimate for the solution. We refer to\nthe proposed method as rectified sparse Bayesian learning (R-SBL). We provide\nfour R- SBL variants that offer a range of options for computational complexity\nand the quality of the E-step computation. These methods include the Markov\nchain Monte Carlo EM, linear minimum mean-square-error estimation, approximate\nmessage passing and a diagonal approximation. Using numerical experiments, we\nshow that the proposed R-SBL method outperforms existing S-NNLS solvers in\nterms of both signal and support recovery performance, and is also very robust\nagainst the structure of the design matrix.\n", "versions": [{"version": "v1", "created": "Fri, 22 Jan 2016 23:47:36 GMT"}, {"version": "v2", "created": "Tue, 8 Mar 2016 10:14:35 GMT"}, {"version": "v3", "created": "Wed, 9 Mar 2016 02:38:23 GMT"}, {"version": "v4", "created": "Sun, 19 Mar 2017 00:03:43 GMT"}, {"version": "v5", "created": "Fri, 16 Feb 2018 21:27:34 GMT"}, {"version": "v6", "created": "Tue, 27 Mar 2018 18:36:04 GMT"}], "update_date": "2018-03-29", "authors_parsed": [["Nalci", "Alican", ""], ["Fedorov", "Igor", ""], ["Al-Shoukairi", "Maher", ""], ["Liu", "Thomas T.", ""], ["Rao", "Bhaskar D.", ""]]}, {"id": "1601.06259", "submitter": "Aaditya Ramdas", "authors": "Aaditya Ramdas, David Isenberg, Aarti Singh, Larry Wasserman", "title": "Minimax Lower Bounds for Linear Independence Testing", "comments": "9 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.IT cs.LG math.IT math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Linear independence testing is a fundamental information-theoretic and\nstatistical problem that can be posed as follows: given $n$ points\n$\\{(X_i,Y_i)\\}^n_{i=1}$ from a $p+q$ dimensional multivariate distribution\nwhere $X_i \\in \\mathbb{R}^p$ and $Y_i \\in\\mathbb{R}^q$, determine whether $a^T\nX$ and $b^T Y$ are uncorrelated for every $a \\in \\mathbb{R}^p, b\\in\n\\mathbb{R}^q$ or not. We give minimax lower bound for this problem (when $p+q,n\n\\to \\infty$, $(p+q)/n \\leq \\kappa < \\infty$, without sparsity assumptions). In\nsummary, our results imply that $n$ must be at least as large as $\\sqrt\n{pq}/\\|\\Sigma_{XY}\\|_F^2$ for any procedure (test) to have non-trivial power,\nwhere $\\Sigma_{XY}$ is the cross-covariance matrix of $X,Y$. We also provide\nsome evidence that the lower bound is tight, by connections to two-sample\ntesting and regression in specific settings.\n", "versions": [{"version": "v1", "created": "Sat, 23 Jan 2016 10:20:58 GMT"}], "update_date": "2016-01-26", "authors_parsed": [["Ramdas", "Aaditya", ""], ["Isenberg", "David", ""], ["Singh", "Aarti", ""], ["Wasserman", "Larry", ""]]}, {"id": "1601.06403", "submitter": "Ali Moharrer", "authors": "Ali Moharrer, Shuangqing Wei, George T. Amariucai, Jing Deng", "title": "Synthesis of Gaussian Trees with Correlation Sign Ambiguity: An\n  Information Theoretic Approach", "comments": "14 pages, 9 figures, part of this work is submitted to Allerton 2016\n  conference, UIUC, IL, USA", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IT cs.CV math.IT stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In latent Gaussian trees the pairwise correlation signs between the variables\nare intrinsically unrecoverable. Such information is vital since it completely\ndetermines the direction in which two variables are associated. In this work,\nwe resort to information theoretical approaches to achieve two fundamental\ngoals: First, we quantify the amount of information loss due to unrecoverable\nsign information. Second, we show the importance of such information in\ndetermining the maximum achievable rate region, in which the observed output\nvector can be synthesized, given its probability density function. In\nparticular, we model the graphical model as a communication channel and propose\na new layered encoding framework to synthesize observed data using upper layer\nGaussian inputs and independent Bernoulli correlation sign inputs from each\nlayer. We find the achievable rate region for the rate tuples of multi-layer\nlatent Gaussian messages to synthesize the desired observables.\n", "versions": [{"version": "v1", "created": "Sun, 24 Jan 2016 15:59:44 GMT"}, {"version": "v2", "created": "Thu, 14 Apr 2016 03:42:18 GMT"}, {"version": "v3", "created": "Sat, 23 Apr 2016 21:59:49 GMT"}, {"version": "v4", "created": "Mon, 4 Jul 2016 16:24:03 GMT"}, {"version": "v5", "created": "Thu, 7 Jul 2016 22:04:07 GMT"}], "update_date": "2016-07-11", "authors_parsed": [["Moharrer", "Ali", ""], ["Wei", "Shuangqing", ""], ["Amariucai", "George T.", ""], ["Deng", "Jing", ""]]}, {"id": "1601.06630", "submitter": "Mauricio Sadinle", "authors": "Mauricio Sadinle", "title": "Bayesian Estimation of Bipartite Matchings for Record Linkage", "comments": "This is a preprint of an article accepted for publication in the\n  Journal of the American Statistical Association. The final version contains\n  more materials and is organized differently", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.AP stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The bipartite record linkage task consists of merging two disparate datafiles\ncontaining information on two overlapping sets of entities. This is non-trivial\nin the absence of unique identifiers and it is important for a wide variety of\napplications given that it needs to be solved whenever we have to combine\ninformation from different sources. Most statistical techniques currently used\nfor record linkage are derived from a seminal paper by Fellegi and Sunter\n(1969). These techniques usually assume independence in the matching statuses\nof record pairs to derive estimation procedures and optimal point estimators.\nWe argue that this independence assumption is unreasonable and instead target a\nbipartite matching between the two datafiles as our parameter of interest.\nBayesian implementations allow us to quantify uncertainty on the matching\ndecisions and derive a variety of point estimators using different loss\nfunctions. We propose partial Bayes estimates that allow uncertain parts of the\nbipartite matching to be left unresolved. We evaluate our approach to record\nlinkage using a variety of challenging scenarios and show that it outperforms\nthe traditional methodology. We illustrate the advantages of our methods\nmerging two datafiles on casualties from the civil war of El Salvador.\n", "versions": [{"version": "v1", "created": "Mon, 25 Jan 2016 14:58:41 GMT"}], "update_date": "2016-01-26", "authors_parsed": [["Sadinle", "Mauricio", ""]]}, {"id": "1601.06650", "submitter": "Ilija Bogunovic Ilija Bogunovic", "authors": "Ilija Bogunovic, Jonathan Scarlett, Volkan Cevher", "title": "Time-Varying Gaussian Process Bandit Optimization", "comments": "To appear in AISTATS 2016", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider the sequential Bayesian optimization problem with bandit\nfeedback, adopting a formulation that allows for the reward function to vary\nwith time. We model the reward function using a Gaussian process whose\nevolution obeys a simple Markov model. We introduce two natural extensions of\nthe classical Gaussian process upper confidence bound (GP-UCB) algorithm. The\nfirst, R-GP-UCB, resets GP-UCB at regular intervals. The second, TV-GP-UCB,\ninstead forgets about old data in a smooth fashion. Our main contribution\ncomprises of novel regret bounds for these algorithms, providing an explicit\ncharacterization of the trade-off between the time horizon and the rate at\nwhich the function varies. We illustrate the performance of the algorithms on\nboth synthetic and real data, and we find the gradual forgetting of TV-GP-UCB\nto perform favorably compared to the sharp resetting of R-GP-UCB. Moreover,\nboth algorithms significantly outperform classical GP-UCB, since it treats\nstale and fresh data equally.\n", "versions": [{"version": "v1", "created": "Mon, 25 Jan 2016 16:02:50 GMT"}], "update_date": "2016-01-26", "authors_parsed": [["Bogunovic", "Ilija", ""], ["Scarlett", "Jonathan", ""], ["Cevher", "Volkan", ""]]}, {"id": "1601.06651", "submitter": "Jonas Hallgren", "authors": "Jonas Hallgren, Timo Koski", "title": "Testing for Causality in Continuous Time Bayesian Network Models of\n  High-Frequency Data", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML q-fin.TR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Continuous time Bayesian networks are investigated with a special focus on\ntheir ability to express causality. A framework is presented for doing\ninference in these networks. The central contributions are a representation of\nthe intensity matrices for the networks and the introduction of a causality\nmeasure. A new model for high-frequency financial data is presented. It is\ncalibrated to market data and by the new causality measure it performs better\nthan older models.\n", "versions": [{"version": "v1", "created": "Mon, 25 Jan 2016 16:07:54 GMT"}], "update_date": "2016-01-26", "authors_parsed": [["Hallgren", "Jonas", ""], ["Koski", "Timo", ""]]}, {"id": "1601.06680", "submitter": "Jos\\'e A. R. Fonollosa", "authors": "Jos\\'e A. R. Fonollosa", "title": "Conditional distribution variability measures for causality detection", "comments": "NIPS 2013 workshop on causality", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper we derive variability measures for the conditional probability\ndistributions of a pair of random variables, and we study its application in\nthe inference of causal-effect relationships. We also study the combination of\nthe proposed measures with standard statistical measures in the the framework\nof the ChaLearn cause-effect pair challenge. The developed model obtains an AUC\nscore of 0.82 on the final test database and ranked second in the challenge.\n", "versions": [{"version": "v1", "created": "Mon, 25 Jan 2016 17:14:31 GMT"}], "update_date": "2016-01-26", "authors_parsed": [["Fonollosa", "Jos\u00e9 A. R.", ""]]}, {"id": "1601.06750", "submitter": "Divya Padmanabhan", "authors": "Divya Padmanabhan, Satyanath Bhat, Dinesh Garg, Shirish Shevade, Y.\n  Narahari", "title": "A Robust UCB Scheme for Active Learning in Regression from Strategic\n  Crowds", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study the problem of training an accurate linear regression model by\nprocuring labels from multiple noisy crowd annotators, under a budget\nconstraint. We propose a Bayesian model for linear regression in crowdsourcing\nand use variational inference for parameter estimation. To minimize the number\nof labels crowdsourced from the annotators, we adopt an active learning\napproach. In this specific context, we prove the equivalence of well-studied\ncriteria of active learning like entropy minimization and expected error\nreduction. Interestingly, we observe that we can decouple the problems of\nidentifying an optimal unlabeled instance and identifying an annotator to label\nit. We observe a useful connection between the multi-armed bandit framework and\nthe annotator selection in active learning. Due to the nature of the\ndistribution of the rewards on the arms, we use the Robust Upper Confidence\nBound (UCB) scheme with truncated empirical mean estimator to solve the\nannotator selection problem. This yields provable guarantees on the regret. We\nfurther apply our model to the scenario where annotators are strategic and\ndesign suitable incentives to induce them to put in their best efforts.\n", "versions": [{"version": "v1", "created": "Mon, 25 Jan 2016 20:14:22 GMT"}, {"version": "v2", "created": "Fri, 29 Jan 2016 09:20:39 GMT"}], "update_date": "2016-02-01", "authors_parsed": [["Padmanabhan", "Divya", ""], ["Bhat", "Satyanath", ""], ["Garg", "Dinesh", ""], ["Shevade", "Shirish", ""], ["Narahari", "Y.", ""]]}, {"id": "1601.06911", "submitter": "Irene Epifanio", "authors": "Irene Epifanio", "title": "Functional archetype and archetypoid analysis", "comments": null, "journal-ref": "Computational Statistics & Data Analysis, Volume 104, December\n  2016, Pages 24-34", "doi": "10.1016/j.csda.2016.06.007", "report-no": null, "categories": "stat.ME stat.AP stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Archetype and archetypoid analysis can be extended to functional data. Each\nfunction is represented as a mixture of actual observations (functional\narchetypoids) or functional archetypes, which are a mixture of observations in\nthe data set. Well-known Canadian temperature data are used to illustrate the\nanalysis developed. Computational methods are proposed for performing these\nanalyses, based on the coefficients of a basis. Unlike a previous attempt to\ncompute functional archetypes, which was only valid for an orthogonal basis,\nthe proposed methodology can be used for any basis. It is computationally less\ndemanding than the simple approach of discretizing the functions. Multivariate\nfunctional archetype and archetypoid analysis are also introduced and applied\nin an interesting problem about the study of human development around the world\nover the last 50 years. These tools can contribute to the understanding of a\nfunctional data set, as in the multivariate case.\n", "versions": [{"version": "v1", "created": "Tue, 26 Jan 2016 07:34:41 GMT"}, {"version": "v2", "created": "Sat, 25 Jun 2016 06:15:21 GMT"}], "update_date": "2016-09-02", "authors_parsed": [["Epifanio", "Irene", ""]]}, {"id": "1601.07233", "submitter": "Andrew Schaumberg", "authors": "Andrew Schaumberg, Angela Yu, Tatsuhiro Koshi, Xiaochan Zong,\n  Santoshkalyan Rayadhurgam", "title": "Predicting Drug Interactions and Mutagenicity with Ensemble Classifiers\n  on Subgraphs of Molecules", "comments": "7 pages, 5 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  In this study, we intend to solve a mutual information problem in interacting\nmolecules of any type, such as proteins, nucleic acids, and small molecules.\nUsing machine learning techniques, we accurately predict pairwise interactions,\nwhich can be of medical and biological importance. Graphs are are useful in\nthis problem for their generality to all types of molecules, due to the\ninherent association of atoms through atomic bonds. Subgraphs can represent\ndifferent molecular domains. These domains can be biologically significant as\nmost molecules only have portions that are of functional significance and can\ninteract with other domains. Thus, we use subgraphs as features in different\nmachine learning algorithms to predict if two drugs interact and predict\npotential single molecule effects.\n", "versions": [{"version": "v1", "created": "Wed, 27 Jan 2016 00:31:02 GMT"}], "update_date": "2016-01-28", "authors_parsed": [["Schaumberg", "Andrew", ""], ["Yu", "Angela", ""], ["Koshi", "Tatsuhiro", ""], ["Zong", "Xiaochan", ""], ["Rayadhurgam", "Santoshkalyan", ""]]}, {"id": "1601.07243", "submitter": "Jean Honorio", "authors": "Jean Honorio", "title": "On the Sample Complexity of Learning Graphical Games", "comments": null, "journal-ref": "IEEE Allerton Conference on Communication, Control and Computing,\n  2017", "doi": null, "report-no": null, "categories": "cs.GT cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We analyze the sample complexity of learning graphical games from purely\nbehavioral data. We assume that we can only observe the players' joint actions\nand not their payoffs. We analyze the sufficient and necessary number of\nsamples for the correct recovery of the set of pure-strategy Nash equilibria\n(PSNE) of the true game. Our analysis focuses on directed graphs with $n$ nodes\nand at most $k$ parents per node. Sparse graphs correspond to ${k \\in O(1)}$\nwith respect to $n$, while dense graphs correspond to ${k \\in O(n)}$. By using\nVC dimension arguments, we show that if the number of samples is greater than\n${O(k n \\log^2{n})}$ for sparse graphs or ${O(n^2 \\log{n})}$ for dense graphs,\nthen maximum likelihood estimation correctly recovers the PSNE with high\nprobability. By using information-theoretic arguments, we show that if the\nnumber of samples is less than ${\\Omega(k n \\log^2{n})}$ for sparse graphs or\n${\\Omega(n^2 \\log{n})}$ for dense graphs, then any conceivable method fails to\nrecover the PSNE with arbitrary probability.\n", "versions": [{"version": "v1", "created": "Wed, 27 Jan 2016 01:49:50 GMT"}, {"version": "v2", "created": "Tue, 24 May 2016 17:52:18 GMT"}, {"version": "v3", "created": "Mon, 20 Feb 2017 20:45:46 GMT"}], "update_date": "2018-11-16", "authors_parsed": [["Honorio", "Jean", ""]]}, {"id": "1601.07252", "submitter": "Anshul Gupta", "authors": "Anshul Gupta, Ricardo Gutierrez-Osuna, Matthew Christy, Richard\n  Furuta, Laura Mandell", "title": "Font Identification in Historical Documents Using Active Learning", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI cs.DL stat.AP stat.ML", "license": "http://creativecommons.org/licenses/by-sa/4.0/", "abstract": "  Identifying the type of font (e.g., Roman, Blackletter) used in historical\ndocuments can help optical character recognition (OCR) systems produce more\naccurate text transcriptions. Towards this end, we present an active-learning\nstrategy that can significantly reduce the number of labeled samples needed to\ntrain a font classifier. Our approach extracts image-based features that\nexploit geometric differences between fonts at the word level, and combines\nthem into a bag-of-word representation for each page in a document. We evaluate\nsix sampling strategies based on uncertainty, dissimilarity and diversity\ncriteria, and test them on a database containing over 3,000 historical\ndocuments with Blackletter, Roman and Mixed fonts. Our results show that a\ncombination of uncertainty and diversity achieves the highest predictive\naccuracy (89% of test cases correctly classified) while requiring only a small\nfraction of the data (17%) to be labeled. We discuss the implications of this\nresult for mass digitization projects of historical documents.\n", "versions": [{"version": "v1", "created": "Wed, 27 Jan 2016 03:24:05 GMT"}], "update_date": "2016-01-28", "authors_parsed": [["Gupta", "Anshul", ""], ["Gutierrez-Osuna", "Ricardo", ""], ["Christy", "Matthew", ""], ["Furuta", "Richard", ""], ["Mandell", "Laura", ""]]}, {"id": "1601.07460", "submitter": "Asish Ghoshal", "authors": "Asish Ghoshal and Jean Honorio", "title": "Information-theoretic limits of Bayesian network structure learning", "comments": "Accepted to AISTATS 2017, Florida", "journal-ref": "International Conference on Artificial Intelligence and Statistics\n  (AISTATS), 2017", "doi": null, "report-no": null, "categories": "cs.LG cs.IT math.IT stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we study the information-theoretic limits of learning the\nstructure of Bayesian networks (BNs), on discrete as well as continuous random\nvariables, from a finite number of samples. We show that the minimum number of\nsamples required by any procedure to recover the correct structure grows as\n$\\Omega(m)$ and $\\Omega(k \\log m + (k^2/m))$ for non-sparse and sparse BNs\nrespectively, where $m$ is the number of variables and $k$ is the maximum\nnumber of parents per node. We provide a simple recipe, based on an extension\nof the Fano's inequality, to obtain information-theoretic limits of structure\nrecovery for any exponential family BN. We instantiate our result for specific\nconditional distributions in the exponential family to characterize the\nfundamental limits of learning various commonly used BNs, such as conditional\nprobability table based networks, gaussian BNs, noisy-OR networks, and logistic\nregression networks. En route to obtaining our main results, we obtain tight\nbounds on the number of sparse and non-sparse essential-DAGs. Finally, as a\nbyproduct, we recover the information-theoretic limits of sparse variable\nselection for logistic regression.\n", "versions": [{"version": "v1", "created": "Wed, 27 Jan 2016 17:41:05 GMT"}, {"version": "v2", "created": "Thu, 26 May 2016 04:27:52 GMT"}, {"version": "v3", "created": "Fri, 18 Nov 2016 01:11:16 GMT"}, {"version": "v4", "created": "Fri, 3 Mar 2017 05:57:15 GMT"}], "update_date": "2019-05-28", "authors_parsed": [["Ghoshal", "Asish", ""], ["Honorio", "Jean", ""]]}, {"id": "1601.07482", "submitter": "Cory Merkel", "authors": "Cory Merkel and Dhireesha Kudithipudi", "title": "Unsupervised Learning in Neuromemristive Systems", "comments": "To appear in the proceedings of the National Aerospace & Electronics\n  Conference & Ohio Innovation Summit (NAECON-OIS'15)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.ET cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Neuromemristive systems (NMSs) currently represent the most promising\nplatform to achieve energy efficient neuro-inspired computation. However, since\nthe research field is less than a decade old, there are still countless\nalgorithms and design paradigms to be explored within these systems. One\nparticular domain that remains to be fully investigated within NMSs is\nunsupervised learning. In this work, we explore the design of an NMS for\nunsupervised clustering, which is a critical element of several machine\nlearning algorithms. Using a simple memristor crossbar architecture and\nlearning rule, we are able to achieve performance which is on par with MATLAB's\nk-means clustering.\n", "versions": [{"version": "v1", "created": "Wed, 27 Jan 2016 18:19:32 GMT"}], "update_date": "2016-01-29", "authors_parsed": [["Merkel", "Cory", ""], ["Kudithipudi", "Dhireesha", ""]]}, {"id": "1601.07621", "submitter": "Evan Racah Mr.", "authors": "Evan Racah, Seyoon Ko, Peter Sadowski, Wahid Bhimji, Craig Tull,\n  Sang-Yun Oh, Pierre Baldi, Prabhat", "title": "Revealing Fundamental Physics from the Daya Bay Neutrino Experiment\n  using Deep Neural Networks", "comments": null, "journal-ref": null, "doi": "10.1109/ICMLA.2016.0160", "report-no": null, "categories": "stat.ML cs.LG physics.data-an", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Experiments in particle physics produce enormous quantities of data that must\nbe analyzed and interpreted by teams of physicists. This analysis is often\nexploratory, where scientists are unable to enumerate the possible types of\nsignal prior to performing the experiment. Thus, tools for summarizing,\nclustering, visualizing and classifying high-dimensional data are essential. In\nthis work, we show that meaningful physical content can be revealed by\ntransforming the raw data into a learned high-level representation using deep\nneural networks, with measurements taken at the Daya Bay Neutrino Experiment as\na case study. We further show how convolutional deep neural networks can\nprovide an effective classification filter with greater than 97% accuracy\nacross different classes of physics events, significantly better than other\nmachine learning approaches.\n", "versions": [{"version": "v1", "created": "Thu, 28 Jan 2016 01:53:13 GMT"}, {"version": "v2", "created": "Fri, 12 Aug 2016 01:42:12 GMT"}, {"version": "v3", "created": "Tue, 6 Dec 2016 21:50:26 GMT"}], "update_date": "2017-05-10", "authors_parsed": [["Racah", "Evan", ""], ["Ko", "Seyoon", ""], ["Sadowski", "Peter", ""], ["Bhimji", "Wahid", ""], ["Tull", "Craig", ""], ["Oh", "Sang-Yun", ""], ["Baldi", "Pierre", ""], ["Prabhat", "", ""]]}, {"id": "1601.07665", "submitter": "Hiroaki  Sasaki", "authors": "Hiroaki Sasaki, Gang Niu and Masashi Sugiyama", "title": "Non-Gaussian Component Analysis with Log-Density Gradient Estimation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Non-Gaussian component analysis (NGCA) is aimed at identifying a linear\nsubspace such that the projected data follows a non-Gaussian distribution. In\nthis paper, we propose a novel NGCA algorithm based on log-density gradient\nestimation. Unlike existing methods, the proposed NGCA algorithm identifies the\nlinear subspace by using the eigenvalue decomposition without any iterative\nprocedures, and thus is computationally reasonable. Furthermore, through\ntheoretical analysis, we prove that the identified subspace converges to the\ntrue subspace at the optimal parametric rate. Finally, the practical\nperformance of the proposed algorithm is demonstrated on both artificial and\nbenchmark datasets.\n", "versions": [{"version": "v1", "created": "Thu, 28 Jan 2016 06:49:34 GMT"}], "update_date": "2016-01-29", "authors_parsed": [["Sasaki", "Hiroaki", ""], ["Niu", "Gang", ""], ["Sugiyama", "Masashi", ""]]}, {"id": "1601.07714", "submitter": "Thomas Ketseoglou", "authors": "Brian Mohtashemi, Thomas Ketseoglou", "title": "Log-Normal Matrix Completion for Large Scale Link Prediction", "comments": "6 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.SI cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The ubiquitous proliferation of online social networks has led to the\nwidescale emergence of relational graphs expressing unique patterns in link\nformation and descriptive user node features. Matrix Factorization and\nCompletion have become popular methods for Link Prediction due to the low rank\nnature of mutual node friendship information, and the availability of parallel\ncomputer architectures for rapid matrix processing. Current Link Prediction\nliterature has demonstrated vast performance improvement through the\nutilization of sparsity in addition to the low rank matrix assumption. However,\nthe majority of research has introduced sparsity through the limited L1 or\nFrobenius norms, instead of considering the more detailed distributions which\nled to the graph formation and relationship evolution. In particular, social\nnetworks have been found to express either Pareto, or more recently discovered,\nLog Normal distributions. Employing the convexity-inducing Lovasz Extension, we\ndemonstrate how incorporating specific degree distribution information can lead\nto large scale improvements in Matrix Completion based Link prediction. We\nintroduce Log-Normal Matrix Completion (LNMC), and solve the complex\noptimization problem by employing Alternating Direction Method of Multipliers.\nUsing data from three popular social networks, our experiments yield up to 5%\nAUC increase over top-performing non-structured sparsity based methods.\n", "versions": [{"version": "v1", "created": "Thu, 28 Jan 2016 10:30:19 GMT"}], "update_date": "2016-01-29", "authors_parsed": [["Mohtashemi", "Brian", ""], ["Ketseoglou", "Thomas", ""]]}, {"id": "1601.07843", "submitter": "Nabin Mishra", "authors": "Nabin K. Mishra and M. Emre Celebi", "title": "An Overview of Melanoma Detection in Dermoscopy Images Using Image\n  Processing and Machine Learning", "comments": "15 pages, 3 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The incidence of malignant melanoma continues to increase worldwide. This\ncancer can strike at any age; it is one of the leading causes of loss of life\nin young persons. Since this cancer is visible on the skin, it is potentially\ndetectable at a very early stage when it is curable. New developments have\nconverged to make fully automatic early melanoma detection a real possibility.\nFirst, the advent of dermoscopy has enabled a dramatic boost in clinical\ndiagnostic ability to the point that melanoma can be detected in the clinic at\nthe very earliest stages. The global adoption of this technology has allowed\naccumulation of large collections of dermoscopy images of melanomas and benign\nlesions validated by histopathology. The development of advanced technologies\nin the areas of image processing and machine learning have given us the ability\nto allow distinction of malignant melanoma from the many benign mimics that\nrequire no biopsy. These new technologies should allow not only earlier\ndetection of melanoma, but also reduction of the large number of needless and\ncostly biopsy procedures. Although some of the new systems reported for these\ntechnologies have shown promise in preliminary trials, widespread\nimplementation must await further technical progress in accuracy and\nreproducibility. In this paper, we provide an overview of computerized\ndetection of melanoma in dermoscopy images. First, we discuss the various\naspects of lesion segmentation. Then, we provide a brief overview of clinical\nfeature segmentation. Finally, we discuss the classification stage where\nmachine learning algorithms are applied to the attributes generated from the\nsegmented features to predict the existence of melanoma.\n", "versions": [{"version": "v1", "created": "Thu, 28 Jan 2016 17:33:48 GMT"}], "update_date": "2016-01-29", "authors_parsed": [["Mishra", "Nabin K.", ""], ["Celebi", "M. Emre", ""]]}, {"id": "1601.07932", "submitter": "Keehwan Park", "authors": "Keehwan Park and Jean Honorio", "title": "Information-Theoretic Lower Bounds for Recovery of Diffusion Network\n  Structures", "comments": "ISIT'16", "journal-ref": "International Symposium on Information Theory (ISIT) 2016", "doi": null, "report-no": null, "categories": "cs.LG cs.IT math.IT stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study the information-theoretic lower bound of the sample complexity of\nthe correct recovery of diffusion network structures. We introduce a\ndiscrete-time diffusion model based on the Independent Cascade model for which\nwe obtain a lower bound of order $\\Omega(k \\log p)$, for directed graphs of $p$\nnodes, and at most $k$ parents per node. Next, we introduce a continuous-time\ndiffusion model, for which a similar lower bound of order $\\Omega(k \\log p)$ is\nobtained. Our results show that the algorithm of Pouget-Abadie et al. is\nstatistically optimal for the discrete-time regime. Our work also opens the\nquestion of whether it is possible to devise an optimal algorithm for the\ncontinuous-time regime.\n", "versions": [{"version": "v1", "created": "Thu, 28 Jan 2016 22:12:06 GMT"}, {"version": "v2", "created": "Mon, 23 May 2016 23:29:19 GMT"}], "update_date": "2019-05-28", "authors_parsed": [["Park", "Keehwan", ""], ["Honorio", "Jean", ""]]}, {"id": "1601.07947", "submitter": "Dimitrios Berberidis", "authors": "Fatemeh Sheikholeslami, Dimitris Berberidis, Georgios B.Giannakis", "title": "Large-scale Kernel-based Feature Extraction via Budgeted Nonlinear\n  Subspace Tracking", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Kernel-based methods enjoy powerful generalization capabilities in handling a\nvariety of learning tasks. When such methods are provided with sufficient\ntraining data, broadly-applicable classes of nonlinear functions can be\napproximated with desired accuracy. Nevertheless, inherent to the nonparametric\nnature of kernel-based estimators are computational and memory requirements\nthat become prohibitive with large-scale datasets. In response to this\nformidable challenge, the present work puts forward a low-rank, kernel-based,\nfeature extraction approach that is particularly tailored for online operation,\nwhere data streams need not be stored in memory. A novel generative model is\nintroduced to approximate high-dimensional (possibly infinite) features via a\nlow-rank nonlinear subspace, the learning of which leads to a direct kernel\nfunction approximation. Offline and online solvers are developed for the\nsubspace learning task, along with affordable versions, in which the number of\nstored data vectors is confined to a predefined budget. Analytical results\nprovide performance bounds on how well the kernel matrix as well as\nkernel-based classification and regression tasks can be approximated by\nleveraging budgeted online subspace learning and feature extraction schemes.\nTests on synthetic and real datasets demonstrate and benchmark the efficiency\nof the proposed method when linear classification and regression is applied to\nthe extracted features.\n", "versions": [{"version": "v1", "created": "Thu, 28 Jan 2016 23:38:44 GMT"}, {"version": "v2", "created": "Tue, 26 Dec 2017 21:45:26 GMT"}], "update_date": "2017-12-29", "authors_parsed": [["Sheikholeslami", "Fatemeh", ""], ["Berberidis", "Dimitris", ""], ["Giannakis", "Georgios B.", ""]]}, {"id": "1601.08057", "submitter": "Samuel Livingstone", "authors": "Samuel Livingstone, Michael Betancourt, Simon Byrne and Mark Girolami", "title": "On the Geometric Ergodicity of Hamiltonian Monte Carlo", "comments": "29 pages + supplement (included in arXival as Appendix), 1 figure", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.CO stat.ME stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We establish general conditions under which Markov chains produced by the\nHamiltonian Monte Carlo method will and will not be geometrically ergodic. We\nconsider implementations with both position-independent and position-dependent\nintegration times. In the former case we find that the conditions for geometric\nergodicity are essentially a gradient of the log-density which asymptotically\npoints towards the centre of the space and grows no faster than linearly. In an\nidealised scenario in which the integration time is allowed to change in\ndifferent regions of the space, we show that geometric ergodicity can be\nrecovered for a much broader class of tail behaviours, leading to some\nguidelines for the choice of this free parameter in practice.\n", "versions": [{"version": "v1", "created": "Fri, 29 Jan 2016 11:13:46 GMT"}, {"version": "v2", "created": "Mon, 19 Sep 2016 13:13:20 GMT"}, {"version": "v3", "created": "Fri, 13 Apr 2018 11:08:32 GMT"}, {"version": "v4", "created": "Fri, 16 Nov 2018 11:32:00 GMT"}], "update_date": "2018-11-19", "authors_parsed": [["Livingstone", "Samuel", ""], ["Betancourt", "Michael", ""], ["Byrne", "Simon", ""], ["Girolami", "Mark", ""]]}, {"id": "1601.08068", "submitter": "Hildo Bijl", "authors": "Hildo Bijl, Thomas B. Sch\\\"on, Jan-Willem van Wingerden, Michel\n  Verhaegen", "title": "System Identification through Online Sparse Gaussian Process Regression\n  with Input Noise", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG cs.SY", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  There has been a growing interest in using non-parametric regression methods\nlike Gaussian Process (GP) regression for system identification. GP regression\ndoes traditionally have three important downsides: (1) it is computationally\nintensive, (2) it cannot efficiently implement newly obtained measurements\nonline, and (3) it cannot deal with stochastic (noisy) input points. In this\npaper we present an algorithm tackling all these three issues simultaneously.\nThe resulting Sparse Online Noisy Input GP (SONIG) regression algorithm can\nincorporate new noisy measurements in constant runtime. A comparison has shown\nthat it is more accurate than similar existing regression algorithms. When\napplied to non-linear black-box system modeling, its performance is competitive\nwith existing non-linear ARX models.\n", "versions": [{"version": "v1", "created": "Fri, 29 Jan 2016 11:55:26 GMT"}, {"version": "v2", "created": "Tue, 16 May 2017 18:09:28 GMT"}, {"version": "v3", "created": "Tue, 15 Aug 2017 22:08:11 GMT"}], "update_date": "2017-08-17", "authors_parsed": [["Bijl", "Hildo", ""], ["Sch\u00f6n", "Thomas B.", ""], ["van Wingerden", "Jan-Willem", ""], ["Verhaegen", "Michel", ""]]}, {"id": "1601.08165", "submitter": "Emanuele Olivetti", "authors": "Thien Bao Nguyen, Emanuele Olivetti, Paolo Avesani", "title": "Mapping Tractography Across Subjects", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.CV q-bio.NC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Diffusion magnetic resonance imaging (dMRI) and tractography provide means to\nstudy the anatomical structures within the white matter of the brain. When\nstudying tractography data across subjects, it is usually necessary to align,\ni.e. to register, tractographies together. This registration step is most often\nperformed by applying the transformation resulting from the registration of\nother volumetric images (T1, FA). In contrast with registration methods that\n\"transform\" tractographies, in this work, we try to find which streamline in\none tractography correspond to which streamline in the other tractography,\nwithout any transformation. In other words, we try to find a \"mapping\" between\nthe tractographies. We propose a graph-based solution for the tractography\nmapping problem and we explain similarities and differences with the related\nwell-known graph matching problem. Specifically, we define a loss function\nbased on the pairwise streamline distance and reformulate the mapping problem\nas combinatorial optimization of that loss function. We show preliminary\npromising results where we compare the proposed method, implemented with\nsimulated annealing, against a standard registration techniques in a task of\nsegmentation of the corticospinal tract.\n", "versions": [{"version": "v1", "created": "Fri, 29 Jan 2016 15:50:20 GMT"}], "update_date": "2016-02-01", "authors_parsed": [["Nguyen", "Thien Bao", ""], ["Olivetti", "Emanuele", ""], ["Avesani", "Paolo", ""]]}, {"id": "1601.08169", "submitter": "Franz J. Kir\\'aly", "authors": "Franz J Kir\\'aly, Harald Oberhauser", "title": "Kernels for sequentially ordered data", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.DM cs.LG math.ST stat.ME stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a novel framework for kernel learning with sequential data of any\nkind, such as time series, sequences of graphs, or strings. Our approach is\nbased on signature features which can be seen as an ordered variant of sample\n(cross-)moments; it allows to obtain a \"sequentialized\" version of any static\nkernel. The sequential kernels are efficiently computable for discrete\nsequences and are shown to approximate a continuous moment form in a sampling\nsense.\n  A number of known kernels for sequences arise as \"sequentializations\" of\nsuitable static kernels: string kernels may be obtained as a special case, and\nalignment kernels are closely related up to a modification that resolves their\nopen non-definiteness issue. Our experiments indicate that our signature-based\nsequential kernel framework may be a promising approach to learning with\nsequential data, such as time series, that allows to avoid extensive manual\npre-processing.\n", "versions": [{"version": "v1", "created": "Fri, 29 Jan 2016 16:06:36 GMT"}], "update_date": "2016-02-01", "authors_parsed": [["Kir\u00e1ly", "Franz J", ""], ["Oberhauser", "Harald", ""]]}]