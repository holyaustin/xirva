[{"id": "0808.0711", "submitter": "Guillaume Obozinski", "authors": "Guillaume Obozinski, Martin J. Wainwright, Michael I. Jordan", "title": "Support union recovery in high-dimensional multivariate regression", "comments": "Published in at http://dx.doi.org/10.1214/09-AOS776 the Annals of\n  Statistics (http://www.imstat.org/aos/) by the Institute of Mathematical\n  Statistics (http://www.imstat.org)", "journal-ref": "Annals of Statistics 2011, Vol. 39, No. 1, 1-47", "doi": "10.1214/09-AOS776", "report-no": "IMS-AOS-AOS776", "categories": "stat.ML math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In multivariate regression, a $K$-dimensional response vector is regressed\nupon a common set of $p$ covariates, with a matrix $B^*\\in\\mathbb{R}^{p\\times\nK}$ of regression coefficients. We study the behavior of the multivariate group\nLasso, in which block regularization based on the $\\ell_1/\\ell_2$ norm is used\nfor support union recovery, or recovery of the set of $s$ rows for which $B^*$\nis nonzero. Under high-dimensional scaling, we show that the multivariate group\nLasso exhibits a threshold for the recovery of the exact row pattern with high\nprobability over the random design and noise that is specified by the sample\ncomplexity parameter $\\theta(n,p,s):=n/[2\\psi(B^*)\\log(p-s)]$. Here $n$ is the\nsample size, and $\\psi(B^*)$ is a sparsity-overlap function measuring a\ncombination of the sparsities and overlaps of the $K$-regression coefficient\nvectors that constitute the model. We prove that the multivariate group Lasso\nsucceeds for problem sequences $(n,p,s)$ such that $\\theta(n,p,s)$ exceeds a\ncritical level $\\theta_u$, and fails for sequences such that $\\theta(n,p,s)$\nlies below a critical level $\\theta_{\\ell}$. For the special case of the\nstandard Gaussian ensemble, we show that $\\theta_{\\ell}=\\theta_u$ so that the\ncharacterization is sharp. The sparsity-overlap function $\\psi(B^*)$ reveals\nthat, if the design is uncorrelated on the active rows, $\\ell_1/\\ell_2$\nregularization for multivariate regression never harms performance relative to\nan ordinary Lasso approach and can yield substantial improvements in sample\ncomplexity (up to a factor of $K$) when the coefficient vectors are suitably\northogonal. For more general designs, it is possible for the ordinary Lasso to\noutperform the multivariate group Lasso. We complement our analysis with\nsimulations that demonstrate the sharpness of our theoretical results, even for\nrelatively small problems.\n", "versions": [{"version": "v1", "created": "Tue, 5 Aug 2008 20:17:40 GMT"}, {"version": "v2", "created": "Mon, 7 Mar 2011 14:37:03 GMT"}], "update_date": "2011-03-08", "authors_parsed": [["Obozinski", "Guillaume", ""], ["Wainwright", "Martin J.", ""], ["Jordan", "Michael I.", ""]]}, {"id": "0808.0780", "submitter": "Yair Goldberg", "authors": "Yair Goldberg and Ya'acov Ritov", "title": "LLE with low-dimensional neighborhood representation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The local linear embedding algorithm (LLE) is a non-linear dimension-reducing\ntechnique, widely used due to its computational simplicity and intuitive\napproach. LLE first linearly reconstructs each input point from its nearest\nneighbors and then preserves these neighborhood relations in the\nlow-dimensional embedding. We show that the reconstruction weights computed by\nLLE capture the high-dimensional structure of the neighborhoods, and not the\nlow-dimensional manifold structure. Consequently, the weight vectors are highly\nsensitive to noise. Moreover, this causes LLE to converge to a linear\nprojection of the input, as opposed to its non-linear embedding goal. To\novercome both of these problems, we propose to compute the weight vectors using\na low-dimensional neighborhood representation. We prove theoretically that this\nstraightforward and computationally simple modification of LLE reduces LLE's\nsensitivity to noise. This modification also removes the need for\nregularization when the number of neighbors is larger than the dimension of the\ninput. We present numerical examples demonstrating both the perturbation and\nlinear projection problems, and the improved outputs using the low-dimensional\nneighborhood representation.\n", "versions": [{"version": "v1", "created": "Wed, 6 Aug 2008 06:25:52 GMT"}], "update_date": "2008-08-07", "authors_parsed": [["Goldberg", "Yair", ""], ["Ritov", "Ya'acov", ""]]}, {"id": "0808.1149", "submitter": "Jason Morton", "authors": "Jason Morton", "title": "Relations among conditional probabilities", "comments": "20 pages,11 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.PR math.AG math.ST stat.ML stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We describe a Groebner basis of relations among conditional probabilities in\na discrete probability space, with any set of conditioned-upon events. They may\nbe specialized to the partially-observed random variable case, the purely\nconditional case, and other special cases. We also investigate the connection\nto generalized permutohedra and describe a conditional probability simplex.\n", "versions": [{"version": "v1", "created": "Fri, 8 Aug 2008 17:56:45 GMT"}], "update_date": "2008-08-11", "authors_parsed": [["Morton", "Jason", ""]]}, {"id": "0808.2241", "submitter": "Facundo Memoli", "authors": "Gunnar Carlsson and Facundo Memoli", "title": "Persistent Clustering and a Theorem of J. Kleinberg", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We construct a framework for studying clustering algorithms, which includes\ntwo key ideas: persistence and functoriality. The first encodes the idea that\nthe output of a clustering scheme should carry a multiresolution structure, the\nsecond the idea that one should be able to compare the results of clustering\nalgorithms as one varies the data set, for example by adding points or by\napplying functions to it. We show that within this framework, one can prove a\ntheorem analogous to one of J. Kleinberg, in which one obtains an existence and\nuniqueness theorem instead of a non-existence result. We explore further\nproperties of this unique scheme, stability and convergence are established.\n", "versions": [{"version": "v1", "created": "Sat, 16 Aug 2008 08:16:11 GMT"}], "update_date": "2008-08-19", "authors_parsed": [["Carlsson", "Gunnar", ""], ["Memoli", "Facundo", ""]]}, {"id": "0808.2337", "submitter": "Ami Wiesel", "authors": "Ami Wiesel and Alfred O. Hero III", "title": "Decomposable Principal Component Analysis", "comments": null, "journal-ref": null, "doi": "10.1109/TSP.2009.2025806", "report-no": null, "categories": "stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider principal component analysis (PCA) in decomposable Gaussian\ngraphical models. We exploit the prior information in these models in order to\ndistribute its computation. For this purpose, we reformulate the problem in the\nsparse inverse covariance (concentration) domain and solve the global\neigenvalue problem using a sequence of local eigenvalue problems in each of the\ncliques of the decomposable graph. We demonstrate the application of our\nmethodology in the context of decentralized anomaly detection in the Abilene\nbackbone network. Based on the topology of the network, we propose an\napproximate statistical graphical model and distribute the computation of PCA.\n", "versions": [{"version": "v1", "created": "Mon, 18 Aug 2008 19:07:06 GMT"}], "update_date": "2015-05-13", "authors_parsed": [["Wiesel", "Ami", ""], ["Hero", "Alfred O.", "III"]]}, {"id": "0808.3416", "submitter": "Phaedon-Stelios Koutsourelakis", "authors": "Phaedon-Stelios Koutsourelakis", "title": "Uncertainty quantification in complex systems using approximate solvers", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.CO math.NA physics.comp-ph stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper proposes a novel uncertainty quantification framework for\ncomputationally demanding systems characterized by a large vector of\nnon-Gaussian uncertainties. It combines state-of-the-art techniques in advanced\nMonte Carlo sampling with Bayesian formulations. The key departure from\nexisting works is the use of inexpensive, approximate computational models in a\nrigorous manner. Such models can readily be derived by coarsening the\ndiscretization size in the solution of the governing PDEs, increasing the time\nstep when integration of ODEs is performed, using fewer iterations if a\nnon-linear solver is employed or making use of lower order models. It is shown\nthat even in cases where the inexact models provide very poor approximations of\nthe exact response, statistics of the latter can be quantified accurately with\nsignificant reductions in the computational effort. Multiple approximate models\ncan be used and rigorous confidence bounds of the estimates produced are\nprovided at all stages.\n", "versions": [{"version": "v1", "created": "Mon, 25 Aug 2008 23:10:09 GMT"}], "update_date": "2018-03-05", "authors_parsed": [["Koutsourelakis", "Phaedon-Stelios", ""]]}]