[{"id": "1702.00001", "submitter": "Emilie Kaufmann", "authors": "Emilie Kaufmann (SEQUEL, CRIStAL, CNRS), Aur\\'elien Garivier (IMT)", "title": "Learning the distribution with largest mean: two bandit frameworks", "comments": null, "journal-ref": "ESAIM: Proceedings and Surveys, EDP Sciences, A Para{\\^i}tre,\n  2017, pp.1 - 10", "doi": null, "report-no": null, "categories": "cs.LG math.ST stat.ML stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Over the past few years, the multi-armed bandit model has become increasingly\npopular in the machine learning community, partly because of applications\nincluding online content optimization. This paper reviews two different\nsequential learning tasks that have been considered in the bandit literature ;\nthey can be formulated as (sequentially) learning which distribution has the\nhighest mean among a set of distributions, with some constraints on the\nlearning process. For both of them (regret minimization and best arm\nidentification) we present recent, asymptotically optimal algorithms. We\ncompare the behaviors of the sampling rule of each algorithm as well as the\ncomplexity terms associated to each problem.\n", "versions": [{"version": "v1", "created": "Tue, 31 Jan 2017 07:45:32 GMT"}, {"version": "v2", "created": "Fri, 24 Mar 2017 07:30:56 GMT"}, {"version": "v3", "created": "Tue, 7 Nov 2017 07:06:06 GMT"}], "update_date": "2017-11-08", "authors_parsed": [["Kaufmann", "Emilie", "", "SEQUEL, CRIStAL, CNRS"], ["Garivier", "Aur\u00e9lien", "", "IMT"]]}, {"id": "1702.00027", "submitter": "C Van", "authors": "A.G.Ramm, C. Van", "title": "Representation of big data by dimension reduction", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IT cs.LG math.IT stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Suppose the data consist of a set $S$ of points $x_j, 1 \\leq j \\leq J$,\ndistributed in a bounded domain $D \\subset R^N$, where $N$ and $J$ are large\nnumbers. In this paper an algorithm is proposed for checking whether there\nexists a manifold $\\mathbb{M}$ of low dimension near which many of the points\nof $S$ lie and finding such $\\mathbb{M}$ if it exists. There are many dimension\nreduction algorithms, both linear and non-linear. Our algorithm is simple to\nimplement and has some advantages compared with the known algorithms. If there\nis a manifold of low dimension near which most of the data points lie, the\nproposed algorithm will find it. Some numerical results are presented\nillustrating the algorithm and analyzing its performance compared to the\nclassical PCA (principal component analysis) and Isomap.\n", "versions": [{"version": "v1", "created": "Tue, 31 Jan 2017 19:25:44 GMT"}], "update_date": "2017-02-02", "authors_parsed": [["Ramm", "A. G.", ""], ["Van", "C.", ""]]}, {"id": "1702.00156", "submitter": "Anjan Dutta", "authors": "Anjan Dutta and Hichem Sahbi", "title": "Stochastic Graphlet Embedding", "comments": "Accepted in IEEE TNNLS (14 pages, 7 figures, 10 tables)", "journal-ref": "IEEE TNNLS, pages 1-14, 2018", "doi": "10.1109/TNNLS.2018.2884700", "report-no": null, "categories": "cs.CV cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Graph-based methods are known to be successful in many machine learning and\npattern classification tasks. These methods consider semi-structured data as\ngraphs where nodes correspond to primitives (parts, interest points, segments,\netc.) and edges characterize the relationships between these primitives.\nHowever, these non-vectorial graph data cannot be straightforwardly plugged\ninto off-the-shelf machine learning algorithms without a preliminary step of --\nexplicit/implicit -- graph vectorization and embedding. This embedding process\nshould be resilient to intra-class graph variations while being highly\ndiscriminant. In this paper, we propose a novel high-order stochastic graphlet\nembedding (SGE) that maps graphs into vector spaces. Our main contribution\nincludes a new stochastic search procedure that efficiently parses a given\ngraph and extracts/samples unlimitedly high-order graphlets. We consider these\ngraphlets, with increasing orders, to model local primitives as well as their\nincreasingly complex interactions. In order to build our graph representation,\nwe measure the distribution of these graphlets into a given graph, using\nparticular hash functions that efficiently assign sampled graphlets into\nisomorphic sets with a very low probability of collision. When combined with\nmaximum margin classifiers, these graphlet-based representations have positive\nimpact on the performance of pattern comparison and recognition as corroborated\nthrough extensive experiments using standard benchmark databases.\n", "versions": [{"version": "v1", "created": "Wed, 1 Feb 2017 08:16:03 GMT"}, {"version": "v2", "created": "Fri, 17 Feb 2017 11:01:06 GMT"}, {"version": "v3", "created": "Wed, 5 Dec 2018 01:59:34 GMT"}], "update_date": "2019-01-01", "authors_parsed": [["Dutta", "Anjan", ""], ["Sahbi", "Hichem", ""]]}, {"id": "1702.00177", "submitter": "Mathias Seuret", "authors": "Mathias Seuret, Michele Alberti, Rolf Ingold, Marcus Liwicki", "title": "PCA-Initialized Deep Neural Networks Applied To Document Image Analysis", "comments": null, "journal-ref": "ICDAR 2017", "doi": "10.1109/ICDAR.2017.148", "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we present a novel approach for initializing deep neural\nnetworks, i.e., by turning PCA into neural layers. Usually, the initialization\nof the weights of a deep neural network is done in one of the three following\nways: 1) with random values, 2) layer-wise, usually as Deep Belief Network or\nas auto-encoder, and 3) re-use of layers from another network (transfer\nlearning). Therefore, typically, many training epochs are needed before\nmeaningful weights are learned, or a rather similar dataset is required for\nseeding a fine-tuning of transfer learning. In this paper, we describe how to\nturn a PCA into an auto-encoder, by generating an encoder layer of the PCA\nparameters and furthermore adding a decoding layer. We analyze the\ninitialization technique on real documents. First, we show that a PCA-based\ninitialization is quick and leads to a very stable initialization. Furthermore,\nfor the task of layout analysis we investigate the effectiveness of PCA-based\ninitialization and show that it outperforms state-of-the-art random weight\ninitialization methods.\n", "versions": [{"version": "v1", "created": "Wed, 1 Feb 2017 09:41:52 GMT"}], "update_date": "2018-04-25", "authors_parsed": [["Seuret", "Mathias", ""], ["Alberti", "Michele", ""], ["Ingold", "Rolf", ""], ["Liwicki", "Marcus", ""]]}, {"id": "1702.00317", "submitter": "Vivak Patel", "authors": "Vivak Patel", "title": "On SGD's Failure in Practice: Characterizing and Overcoming Stalling", "comments": "17 pages, 4 figures, 4 Tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG math.OC stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Stochastic Gradient Descent (SGD) is widely used in machine learning problems\nto efficiently perform empirical risk minimization, yet, in practice, SGD is\nknown to stall before reaching the actual minimizer of the empirical risk. SGD\nstalling has often been attributed to its sensitivity to the conditioning of\nthe problem; however, as we demonstrate, SGD will stall even when applied to a\nsimple linear regression problem with unity condition number for standard\nlearning rates. Thus, in this work, we numerically demonstrate and\nmathematically argue that stalling is a crippling and generic limitation of SGD\nand its variants in practice. Once we have established the problem of stalling,\nwe generalize an existing framework for hedging against its effects, which (1)\ndeters SGD and its variants from stalling, (2) still provides convergence\nguarantees, and (3) makes SGD and its variants more practical methods for\nminimization.\n", "versions": [{"version": "v1", "created": "Wed, 1 Feb 2017 15:33:01 GMT"}, {"version": "v2", "created": "Tue, 7 Feb 2017 22:13:25 GMT"}], "update_date": "2017-02-09", "authors_parsed": [["Patel", "Vivak", ""]]}, {"id": "1702.00403", "submitter": "Kevin Schawinski", "authors": "Kevin Schawinski, Ce Zhang, Hantian Zhang, Lucas Fowler and Gokula\n  Krishnan Santhanam", "title": "Generative Adversarial Networks recover features in astrophysical images\n  of galaxies beyond the deconvolution limit", "comments": "Accepted for publication in MNRAS, for the full code and a virtual\n  machine set up to run it, see http://space.ml/proj/GalaxyGAN.html", "journal-ref": null, "doi": "10.1093/mnrasl/slx008", "report-no": null, "categories": "astro-ph.IM astro-ph.GA cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Observations of astrophysical objects such as galaxies are limited by various\nsources of random and systematic noise from the sky background, the optical\nsystem of the telescope and the detector used to record the data. Conventional\ndeconvolution techniques are limited in their ability to recover features in\nimaging data by the Shannon-Nyquist sampling theorem. Here we train a\ngenerative adversarial network (GAN) on a sample of $4,550$ images of nearby\ngalaxies at $0.01<z<0.02$ from the Sloan Digital Sky Survey and conduct\n$10\\times$ cross validation to evaluate the results. We present a method using\na GAN trained on galaxy images that can recover features from artificially\ndegraded images with worse seeing and higher noise than the original with a\nperformance which far exceeds simple deconvolution. The ability to better\nrecover detailed features such as galaxy morphology from low-signal-to-noise\nand low angular resolution imaging data significantly increases our ability to\nstudy existing data sets of astrophysical objects as well as future\nobservations with observatories such as the Large Synoptic Sky Telescope (LSST)\nand the Hubble and James Webb space telescopes.\n", "versions": [{"version": "v1", "created": "Wed, 1 Feb 2017 19:00:02 GMT"}], "update_date": "2017-04-19", "authors_parsed": [["Schawinski", "Kevin", ""], ["Zhang", "Ce", ""], ["Zhang", "Hantian", ""], ["Fowler", "Lucas", ""], ["Santhanam", "Gokula Krishnan", ""]]}, {"id": "1702.00414", "submitter": "Francesco Rubbo", "authors": "Lucio Mwinmaarong Dery, Benjamin Nachman, Francesco Rubbo, Ariel\n  Schwartzman", "title": "Weakly Supervised Classification in High Energy Physics", "comments": "8 pages, 4 figures", "journal-ref": "JHEP 05 (2017) 145", "doi": "10.1007/JHEP05(2017)145", "report-no": null, "categories": "hep-ph physics.data-an stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  As machine learning algorithms become increasingly sophisticated to exploit\nsubtle features of the data, they often become more dependent on simulations.\nThis paper presents a new approach called weakly supervised classification in\nwhich class proportions are the only input into the machine learning algorithm.\nUsing one of the most challenging binary classification tasks in high energy\nphysics - quark versus gluon tagging - we show that weakly supervised\nclassification can match the performance of fully supervised algorithms.\nFurthermore, by design, the new algorithm is insensitive to any mis-modeling of\ndiscriminating features in the data by the simulation. Weakly supervised\nclassification is a general procedure that can be applied to a wide variety of\nlearning problems to boost performance and robustness when detailed simulations\nare not reliable or not available.\n", "versions": [{"version": "v1", "created": "Wed, 1 Feb 2017 19:02:11 GMT"}, {"version": "v2", "created": "Fri, 3 Feb 2017 18:12:28 GMT"}, {"version": "v3", "created": "Mon, 27 Feb 2017 13:15:52 GMT"}, {"version": "v4", "created": "Mon, 3 Jul 2017 02:08:08 GMT"}], "update_date": "2017-07-04", "authors_parsed": [["Dery", "Lucio Mwinmaarong", ""], ["Nachman", "Benjamin", ""], ["Rubbo", "Francesco", ""], ["Schwartzman", "Ariel", ""]]}, {"id": "1702.00482", "submitter": "Gabor Lugosi", "authors": "G\\'abor Lugosi and Shahar Mendelson", "title": "Sub-Gaussian estimators of the mean of a random vector", "comments": "12 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.ML stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study the problem of estimating the mean of a random vector $X$ given a\nsample of $N$ independent, identically distributed points. We introduce a new\nestimator that achieves a purely sub-Gaussian performance under the only\ncondition that the second moment of $X$ exists. The estimator is based on a\nnovel concept of a multivariate median.\n", "versions": [{"version": "v1", "created": "Wed, 1 Feb 2017 22:33:36 GMT"}], "update_date": "2017-02-03", "authors_parsed": [["Lugosi", "G\u00e1bor", ""], ["Mendelson", "Shahar", ""]]}, {"id": "1702.00518", "submitter": "Shantanu Jain", "authors": "Shantanu Jain, Martha White, Predrag Radivojac", "title": "Recovering True Classifier Performance in Positive-Unlabeled Learning", "comments": "Full paper with supplement", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A common approach in positive-unlabeled learning is to train a classification\nmodel between labeled and unlabeled data. This strategy is in fact known to\ngive an optimal classifier under mild conditions; however, it results in biased\nempirical estimates of the classifier performance. In this work, we show that\nthe typically used performance measures such as the receiver operating\ncharacteristic curve, or the precision-recall curve obtained on such data can\nbe corrected with the knowledge of class priors; i.e., the proportions of the\npositive and negative examples in the unlabeled data. We extend the results to\na noisy setting where some of the examples labeled positive are in fact\nnegative and show that the correction also requires the knowledge of the\nproportion of noisy examples in the labeled positives. Using state-of-the-art\nalgorithms to estimate the positive class prior and the proportion of noise, we\nexperimentally evaluate two correction approaches and demonstrate their\nefficacy on real-life data.\n", "versions": [{"version": "v1", "created": "Thu, 2 Feb 2017 01:22:18 GMT"}], "update_date": "2017-02-03", "authors_parsed": [["Jain", "Shantanu", ""], ["White", "Martha", ""], ["Radivojac", "Predrag", ""]]}, {"id": "1702.00564", "submitter": "Shravan Vasishth", "authors": "Shravan Vasishth, Nicolas Chopin, Robin Ryder, Bruno Nicenboim", "title": "Modelling dependency completion in sentence comprehension as a Bayesian\n  hierarchical mixture process: A case study involving Chinese relative clauses", "comments": "6 pages, 2 figures. To appear in the Proceedings of the Cognitive\n  Science Conference 2017, London, UK", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP cs.CL stat.ME stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a case-study demonstrating the usefulness of Bayesian hierarchical\nmixture modelling for investigating cognitive processes. In sentence\ncomprehension, it is widely assumed that the distance between linguistic\nco-dependents affects the latency of dependency resolution: the longer the\ndistance, the longer the retrieval time (the distance-based account). An\nalternative theory, direct-access, assumes that retrieval times are a mixture\nof two distributions: one distribution represents successful retrievals (these\nare independent of dependency distance) and the other represents an initial\nfailure to retrieve the correct dependent, followed by a reanalysis that leads\nto successful retrieval. We implement both models as Bayesian hierarchical\nmodels and show that the direct-access model explains Chinese relative clause\nreading time data better than the distance account.\n", "versions": [{"version": "v1", "created": "Thu, 2 Feb 2017 07:48:58 GMT"}, {"version": "v2", "created": "Fri, 5 May 2017 05:44:34 GMT"}], "update_date": "2017-05-08", "authors_parsed": [["Vasishth", "Shravan", ""], ["Chopin", "Nicolas", ""], ["Ryder", "Robin", ""], ["Nicenboim", "Bruno", ""]]}, {"id": "1702.00748", "submitter": "Kyle S. Cranmer", "authors": "Gilles Louppe, Kyunghyun Cho, Cyril Becot, Kyle Cranmer", "title": "QCD-Aware Recursive Neural Networks for Jet Physics", "comments": "16 pages, 5 figures, 3 appendices, corresponding code at\n  https://github.com/glouppe/recnn", "journal-ref": null, "doi": "10.1007/JHEP01(2019)057", "report-no": null, "categories": "hep-ph physics.data-an stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recent progress in applying machine learning for jet physics has been built\nupon an analogy between calorimeters and images. In this work, we present a\nnovel class of recursive neural networks built instead upon an analogy between\nQCD and natural languages. In the analogy, four-momenta are like words and the\nclustering history of sequential recombination jet algorithms is like the\nparsing of a sentence. Our approach works directly with the four-momenta of a\nvariable-length set of particles, and the jet-based tree structure varies on an\nevent-by-event basis. Our experiments highlight the flexibility of our method\nfor building task-specific jet embeddings and show that recursive architectures\nare significantly more accurate and data efficient than previous image-based\nnetworks. We extend the analogy from individual jets (sentences) to full events\n(paragraphs), and show for the first time an event-level classifier operating\non all the stable particles produced in an LHC event.\n", "versions": [{"version": "v1", "created": "Thu, 2 Feb 2017 16:58:37 GMT"}, {"version": "v2", "created": "Fri, 13 Jul 2018 20:48:10 GMT"}], "update_date": "2020-02-25", "authors_parsed": [["Louppe", "Gilles", ""], ["Cho", "Kyunghyun", ""], ["Becot", "Cyril", ""], ["Cranmer", "Kyle", ""]]}, {"id": "1702.00763", "submitter": "Zeyuan Allen-Zhu", "authors": "Zeyuan Allen-Zhu", "title": "Natasha: Faster Non-Convex Stochastic Optimization Via Strongly\n  Non-Convex Parameter", "comments": "V2-V5 corrected typos, polished writing, and added citations. (We\n  mis-stated the complexity of the prior work repeatSVRG in V1-V4, and have\n  fixed this mistake in V5.)", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.OC cs.DS cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Given a nonconvex function that is an average of $n$ smooth functions, we\ndesign stochastic first-order methods to find its approximate stationary\npoints. The convergence of our new methods depends on the smallest (negative)\neigenvalue $-\\sigma$ of the Hessian, a parameter that describes how nonconvex\nthe function is.\n  Our methods outperform known results for a range of parameter $\\sigma$, and\ncan be used to find approximate local minima. Our result implies an interesting\ndichotomy: there exists a threshold $\\sigma_0$ so that the currently fastest\nmethods for $\\sigma>\\sigma_0$ and for $\\sigma<\\sigma_0$ have different\nbehaviors: the former scales with $n^{2/3}$ and the latter scales with\n$n^{3/4}$.\n", "versions": [{"version": "v1", "created": "Thu, 2 Feb 2017 17:45:09 GMT"}, {"version": "v2", "created": "Mon, 27 Feb 2017 03:50:25 GMT"}, {"version": "v3", "created": "Tue, 5 Sep 2017 09:37:06 GMT"}, {"version": "v4", "created": "Sat, 16 Jun 2018 10:05:32 GMT"}, {"version": "v5", "created": "Thu, 27 Sep 2018 09:55:54 GMT"}], "update_date": "2018-09-28", "authors_parsed": [["Allen-Zhu", "Zeyuan", ""]]}, {"id": "1702.00852", "submitter": "Andrew Knyazev", "authors": "Andrew Knyazev, Akshay Gadde, Hassan Mansour, Dong Tian", "title": "Guided Signal Reconstruction Theory", "comments": "20 pages, 11 figures", "journal-ref": null, "doi": null, "report-no": "MERL TR2017-020", "categories": "cs.IT math.FA math.IT stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  An axiomatic approach to signal reconstruction is formulated, involving a\nsample consistent set and a guiding set, describing desired reconstructions.\nNew frame-less reconstruction methods are proposed, based on a novel concept of\na reconstruction set, defined as a shortest pathway between the sample\nconsistent set and the guiding set. Existence and uniqueness of the\nreconstruction set are investigated in a Hilbert space, where the guiding set\nis a closed subspace and the sample consistent set is a closed plane, formed by\na sampling subspace. Connections to earlier known consistent, generalized, and\nregularized reconstructions are clarified. New stability and reconstruction\nerror bounds are derived, using the largest nontrivial angle between the\nsampling and guiding subspaces. Conjugate gradient iterative reconstruction\nalgorithms are proposed and illustrated numerically for image magnification.\n", "versions": [{"version": "v1", "created": "Thu, 2 Feb 2017 22:28:11 GMT"}], "update_date": "2017-02-14", "authors_parsed": [["Knyazev", "Andrew", ""], ["Gadde", "Akshay", ""], ["Mansour", "Hassan", ""], ["Tian", "Dong", ""]]}, {"id": "1702.01000", "submitter": "Damian Kozbur", "authors": "Damian Kozbur", "title": "Sharp Convergence Rates for Forward Regression in High-Dimensional\n  Sparse Linear Models", "comments": "arXiv admin note: text overlap with arXiv:1512.02666", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Forward regression is a statistical model selection and estimation procedure\nwhich inductively selects covariates that add predictive power into a working\nstatistical regression model. Once a model is selected, unknown regression\nparameters are estimated by least squares. This paper analyzes forward\nregression in high-dimensional sparse linear models. Probabilistic bounds for\nprediction error norm and number of selected covariates are proved. The\nanalysis in this paper gives sharp rates and does not require beta-min or\nirrepresentability conditions.\n", "versions": [{"version": "v1", "created": "Fri, 3 Feb 2017 13:33:32 GMT"}, {"version": "v2", "created": "Fri, 19 May 2017 11:19:04 GMT"}, {"version": "v3", "created": "Wed, 11 Apr 2018 11:05:51 GMT"}], "update_date": "2018-04-12", "authors_parsed": [["Kozbur", "Damian", ""]]}, {"id": "1702.01125", "submitter": "Adedotun Akintayo", "authors": "Zhanhong Jiang, Chao Liu, Adedotun Akintayo, Gregor Henze, Soumik\n  Sarkar", "title": "Energy Prediction using Spatiotemporal Pattern Networks", "comments": "31 Pages, 24 Figures Preprint Submitted to Journal of Applied Energy", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper presents a novel data-driven technique based on the spatiotemporal\npattern network (STPN) for energy/power prediction for complex dynamical\nsystems. Built on symbolic dynamic filtering, the STPN framework is used to\ncapture not only the individual system characteristics but also the pair-wise\ncausal dependencies among different sub-systems. For quantifying the causal\ndependency, a mutual information based metric is presented. An energy\nprediction approach is subsequently proposed based on the STPN framework. For\nvalidating the proposed scheme, two case studies are presented, one involving\nwind turbine power prediction (supply side energy) using the Western Wind\nIntegration data set generated by the National Renewable Energy Laboratory\n(NREL) for identifying the spatiotemporal characteristics, and the other,\nresidential electric energy disaggregation (demand side energy) using the\nBuilding America 2010 data set from NREL for exploring the temporal features.\nIn the energy disaggregation context, convex programming techniques beyond the\nSTPN framework are developed and applied to achieve improved disaggregation\nperformance.\n", "versions": [{"version": "v1", "created": "Fri, 3 Feb 2017 19:13:38 GMT"}], "update_date": "2017-02-07", "authors_parsed": [["Jiang", "Zhanhong", ""], ["Liu", "Chao", ""], ["Akintayo", "Adedotun", ""], ["Henze", "Gregor", ""], ["Sarkar", "Soumik", ""]]}, {"id": "1702.01145", "submitter": "Kirthevasan Kandasamy", "authors": "Kirthevasan Kandasamy, Jeff Schneider, Barnab\\'as P\\'oczos", "title": "Query Efficient Posterior Estimation in Scientific Experiments via\n  Bayesian Active Learning", "comments": "Published in the Artificial Intelligence Journal (AIJ), Feb 2017 and\n  International Joint Conference on Artificial Intelligence (IJCAI) 2015", "journal-ref": null, "doi": "10.1016/j.artint.2016.11.002", "report-no": null, "categories": "stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A common problem in disciplines of applied Statistics research such as\nAstrostatistics is of estimating the posterior distribution of relevant\nparameters. Typically, the likelihoods for such models are computed via\nexpensive experiments such as cosmological simulations of the universe. An\nurgent challenge in these research domains is to develop methods that can\nestimate the posterior with few likelihood evaluations.\n  In this paper, we study active posterior estimation in a Bayesian setting\nwhen the likelihood is expensive to evaluate. Existing techniques for posterior\nestimation are based on generating samples representative of the posterior.\nSuch methods do not consider efficiency in terms of likelihood evaluations. In\norder to be query efficient we treat posterior estimation in an active\nregression framework. We propose two myopic query strategies to choose where to\nevaluate the likelihood and implement them using Gaussian processes. Via\nexperiments on a series of synthetic and real examples we demonstrate that our\napproach is significantly more query efficient than existing techniques and\nother heuristics for posterior estimation.\n", "versions": [{"version": "v1", "created": "Fri, 3 Feb 2017 20:10:24 GMT"}], "update_date": "2017-02-07", "authors_parsed": [["Kandasamy", "Kirthevasan", ""], ["Schneider", "Jeff", ""], ["P\u00f3czos", "Barnab\u00e1s", ""]]}, {"id": "1702.01166", "submitter": "HaiYing Wang", "authors": "HaiYing Wang, Rong Zhu, Ping Ma", "title": "Optimal Subsampling for Large Sample Logistic Regression", "comments": null, "journal-ref": null, "doi": "10.1080/01621459.2017.1292914", "report-no": null, "categories": "stat.CO stat.ME stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  For massive data, the family of subsampling algorithms is popular to downsize\nthe data volume and reduce computational burden. Existing studies focus on\napproximating the ordinary least squares estimate in linear regression, where\nstatistical leverage scores are often used to define subsampling probabilities.\nIn this paper, we propose fast subsampling algorithms to efficiently\napproximate the maximum likelihood estimate in logistic regression. We first\nestablish consistency and asymptotic normality of the estimator from a general\nsubsampling algorithm, and then derive optimal subsampling probabilities that\nminimize the asymptotic mean squared error of the resultant estimator. An\nalternative minimization criterion is also proposed to further reduce the\ncomputational cost. The optimal subsampling probabilities depend on the full\ndata estimate, so we develop a two-step algorithm to approximate the optimal\nsubsampling procedure. This algorithm is computationally efficient and has a\nsignificant reduction in computing time compared to the full data approach.\nConsistency and asymptotic normality of the estimator from a two-step algorithm\nare also established. Synthetic and real data sets are used to evaluate the\npractical performance of the proposed method.\n", "versions": [{"version": "v1", "created": "Fri, 3 Feb 2017 21:23:46 GMT"}, {"version": "v2", "created": "Wed, 7 Mar 2018 17:01:40 GMT"}], "update_date": "2019-06-27", "authors_parsed": [["Wang", "HaiYing", ""], ["Zhu", "Rong", ""], ["Ma", "Ping", ""]]}, {"id": "1702.01209", "submitter": "Niall Twomey", "authors": "Tom Diethe and Niall Twomey and Meelis Kull and Peter Flach and Ian\n  Craddock", "title": "Probabilistic Sensor Fusion for Ambient Assisted Living", "comments": "Journal article. 19 pages; 7 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.HC", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  There is a widely-accepted need to revise current forms of health-care\nprovision, with particular interest in sensing systems in the home. Given a\nmultiple-modality sensor platform with heterogeneous network connectivity, as\nis under development in the Sensor Platform for HEalthcare in Residential\nEnvironment (SPHERE) Interdisciplinary Research Collaboration (IRC), we face\nspecific challenges relating to the fusion of the heterogeneous sensor\nmodalities.\n  We introduce Bayesian models for sensor fusion, which aims to address the\nchallenges of fusion of heterogeneous sensor modalities. Using this approach we\nare able to identify the modalities that have most utility for each particular\nactivity, and simultaneously identify which features within that activity are\nmost relevant for a given activity.\n  We further show how the two separate tasks of location prediction and\nactivity recognition can be fused into a single model, which allows for\nsimultaneous learning an prediction for both tasks.\n  We analyse the performance of this model on data collected in the SPHERE\nhouse, and show its utility. We also compare against some benchmark models\nwhich do not have the full structure,and show how the proposed model compares\nfavourably to these methods\n", "versions": [{"version": "v1", "created": "Sat, 4 Feb 2017 00:05:29 GMT"}], "update_date": "2017-02-07", "authors_parsed": [["Diethe", "Tom", ""], ["Twomey", "Niall", ""], ["Kull", "Meelis", ""], ["Flach", "Peter", ""], ["Craddock", "Ian", ""]]}, {"id": "1702.01226", "submitter": "Mengchen Liu", "authors": "Shixia Liu, Xiting Wang, Mengchen Liu, Jun Zhu", "title": "Towards Better Analysis of Machine Learning Models: A Visual Analytics\n  Perspective", "comments": "This article will be published in Visual Infomatics", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Interactive model analysis, the process of understanding, diagnosing, and\nrefining a machine learning model with the help of interactive visualization,\nis very important for users to efficiently solve real-world artificial\nintelligence and data mining problems. Dramatic advances in big data analytics\nhas led to a wide variety of interactive model analysis tasks. In this paper,\nwe present a comprehensive analysis and interpretation of this rapidly\ndeveloping area. Specifically, we classify the relevant work into three\ncategories: understanding, diagnosis, and refinement. Each category is\nexemplified by recent influential work. Possible future research opportunities\nare also explored and discussed.\n", "versions": [{"version": "v1", "created": "Sat, 4 Feb 2017 02:23:55 GMT"}], "update_date": "2017-02-07", "authors_parsed": [["Liu", "Shixia", ""], ["Wang", "Xiting", ""], ["Liu", "Mengchen", ""], ["Zhu", "Jun", ""]]}, {"id": "1702.01229", "submitter": "Minnan Luo", "authors": "Minnan Luo and Xiaojun Chang and Zhihui Li and Liqiang Nie and\n  Alexander G. Hauptmann and Qinghua Zheng", "title": "Simple to Complex Cross-modal Learning to Rank", "comments": "14 pages; Accepted by Computer Vision and Image Understanding", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The heterogeneity-gap between different modalities brings a significant\nchallenge to multimedia information retrieval. Some studies formalize the\ncross-modal retrieval tasks as a ranking problem and learn a shared multi-modal\nembedding space to measure the cross-modality similarity. However, previous\nmethods often establish the shared embedding space based on linear mapping\nfunctions which might not be sophisticated enough to reveal more complicated\ninter-modal correspondences. Additionally, current studies assume that the\nrankings are of equal importance, and thus all rankings are used\nsimultaneously, or a small number of rankings are selected randomly to train\nthe embedding space at each iteration. Such strategies, however, always suffer\nfrom outliers as well as reduced generalization capability due to their lack of\ninsightful understanding of procedure of human cognition. In this paper, we\ninvolve the self-paced learning theory with diversity into the cross-modal\nlearning to rank and learn an optimal multi-modal embedding space based on\nnon-linear mapping functions. This strategy enhances the model's robustness to\noutliers and achieves better generalization via training the model gradually\nfrom easy rankings by diverse queries to more complex ones. An efficient\nalternative algorithm is exploited to solve the proposed challenging problem\nwith fast convergence in practice. Extensive experimental results on several\nbenchmark datasets indicate that the proposed method achieves significant\nimprovements over the state-of-the-arts in this literature.\n", "versions": [{"version": "v1", "created": "Sat, 4 Feb 2017 03:06:01 GMT"}, {"version": "v2", "created": "Fri, 7 Jul 2017 18:31:28 GMT"}], "update_date": "2017-07-11", "authors_parsed": [["Luo", "Minnan", ""], ["Chang", "Xiaojun", ""], ["Li", "Zhihui", ""], ["Nie", "Liqiang", ""], ["Hauptmann", "Alexander G.", ""], ["Zheng", "Qinghua", ""]]}, {"id": "1702.01268", "submitter": "Giorgio Valentini", "authors": "Jessica Gliozzo", "title": "Network-based methods for outcome prediction in the \"sample space\"", "comments": "MSc Thesis, Advisor: G. Valentini, Co-Advisors: A. Paccanaro and M.\n  Re, 92 pages, 36 figures, 10 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this thesis we present the novel semi-supervised network-based algorithm\nP-Net, which is able to rank and classify patients with respect to a specific\nphenotype or clinical outcome under study. The peculiar and innovative\ncharacteristic of this method is that it builds a network of samples/patients,\nwhere the nodes represent the samples and the edges are functional or genetic\nrelationships between individuals (e.g. similarity of expression profiles), to\npredict the phenotype under study. In other words, it constructs the network in\nthe \"sample space\" and not in the \"biomarker space\" (where nodes represent\nbiomolecules (e.g. genes, proteins) and edges represent functional or genetic\nrelationships between nodes), as usual in state-of-the-art methods. To assess\nthe performances of P-Net, we apply it on three different publicly available\ndatasets from patients afflicted with a specific type of tumor: pancreatic\ncancer, melanoma and ovarian cancer dataset, by using the data and following\nthe experimental set-up proposed in two recently published papers [Barter et\nal., 2014, Winter et al., 2012]. We show that network-based methods in the\n\"sample space\" can achieve results competitive with classical supervised\ninductive systems. Moreover, the graph representation of the samples can be\neasily visualized through networks and can be used to gain visual clues about\nthe relationships between samples, taking into account the phenotype associated\nor predicted for each sample. To our knowledge this is one of the first works\nthat proposes graph-based algorithms working in the \"sample space\" of the\nbiomolecular profiles of the patients to predict their phenotype or outcome,\nthus contributing to a novel research line in the framework of the Network\nMedicine.\n", "versions": [{"version": "v1", "created": "Sat, 4 Feb 2017 11:18:53 GMT"}], "update_date": "2017-02-07", "authors_parsed": [["Gliozzo", "Jessica", ""]]}, {"id": "1702.01313", "submitter": "Bas van Stein", "authors": "Bas van Stein, Hao Wang, Wojtek Kowalczyk, Michael Emmerich, Thomas\n  B\\\"ack", "title": "Cluster-based Kriging Approximation Algorithms for Complexity Reduction", "comments": "Submitted to IEEE Computational Intelligence Magazine for review", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Kriging or Gaussian Process Regression is applied in many fields as a\nnon-linear regression model as well as a surrogate model in the field of\nevolutionary computation. However, the computational and space complexity of\nKriging, that is cubic and quadratic in the number of data points respectively,\nbecomes a major bottleneck with more and more data available nowadays. In this\npaper, we propose a general methodology for the complexity reduction, called\ncluster Kriging, where the whole data set is partitioned into smaller clusters\nand multiple Kriging models are built on top of them. In addition, four Kriging\napproximation algorithms are proposed as candidate algorithms within the new\nframework. Each of these algorithms can be applied to much larger data sets\nwhile maintaining the advantages and power of Kriging. The proposed algorithms\nare explained in detail and compared empirically against a broad set of\nexisting state-of-the-art Kriging approximation methods on a well-defined\ntesting framework. According to the empirical study, the proposed algorithms\nconsistently outperform the existing algorithms. Moreover, some practical\nsuggestions are provided for using the proposed algorithms.\n", "versions": [{"version": "v1", "created": "Sat, 4 Feb 2017 17:54:59 GMT"}], "update_date": "2017-02-07", "authors_parsed": [["van Stein", "Bas", ""], ["Wang", "Hao", ""], ["Kowalczyk", "Wojtek", ""], ["Emmerich", "Michael", ""], ["B\u00e4ck", "Thomas", ""]]}, {"id": "1702.01373", "submitter": "Jun Song", "authors": "Chenchao Zhao and Jun S. Song", "title": "Exact heat kernel on a hypersphere and its applications in kernel SVM", "comments": null, "journal-ref": null, "doi": "10.3389/fams.2018.00001", "report-no": null, "categories": "stat.ML q-bio.QM stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Many contemporary statistical learning methods assume a Euclidean feature\nspace. This paper presents a method for defining similarity based on\nhyperspherical geometry and shows that it often improves the performance of\nsupport vector machine compared to other competing similarity measures.\nSpecifically, the idea of using heat diffusion on a hypersphere to measure\nsimilarity has been previously proposed, demonstrating promising results based\non a heuristic heat kernel obtained from the zeroth order parametrix expansion;\nhowever, how well this heuristic kernel agrees with the exact hyperspherical\nheat kernel remains unknown. This paper presents a higher order parametrix\nexpansion of the heat kernel on a unit hypersphere and discusses several\nproblems associated with this expansion method. We then compare the heuristic\nkernel with an exact form of the heat kernel expressed in terms of a uniformly\nand absolutely convergent series in high-dimensional angular momentum\neigenmodes. Being a natural measure of similarity between sample points\ndwelling on a hypersphere, the exact kernel often shows superior performance in\nkernel SVM classifications applied to text mining, tumor somatic mutation\nimputation, and stock market analysis.\n", "versions": [{"version": "v1", "created": "Sun, 5 Feb 2017 04:55:14 GMT"}, {"version": "v2", "created": "Mon, 20 Nov 2017 04:39:59 GMT"}], "update_date": "2018-08-07", "authors_parsed": [["Zhao", "Chenchao", ""], ["Song", "Jun S.", ""]]}, {"id": "1702.01414", "submitter": "Thanchanok Teeraratkul", "authors": "Thanchanok Teeraratkul, Daniel O'Neill, Sanjay Lall", "title": "Shape-Based Approach to Household Load Curve Clustering and Prediction", "comments": "14 pages, submitted to a transaction", "journal-ref": null, "doi": "10.1109/TSG.2017.2683461", "report-no": null, "categories": "stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Consumer Demand Response (DR) is an important research and industry problem,\nwhich seeks to categorize, predict and modify consumer's energy consumption.\nUnfortunately, traditional clustering methods have resulted in many hundreds of\nclusters, with a given consumer often associated with several clusters, making\nit difficult to classify consumers into stable representative groups and to\npredict individual energy consumption patterns. In this paper, we present a\nshape-based approach that better classifies and predicts consumer energy\nconsumption behavior at the household level. The method is based on Dynamic\nTime Warping. DTW seeks an optimal alignment between energy consumption\npatterns reflecting the effect of hidden patterns of regular consumer behavior.\nUsing real consumer 24-hour load curves from Opower Corporation, our method\nresults in a 50% reduction in the number of representative groups and an\nimprovement in prediction accuracy measured under DTW distance. We extend the\napproach to estimate which electrical devices will be used and in which hours.\n", "versions": [{"version": "v1", "created": "Sun, 5 Feb 2017 15:36:13 GMT"}], "update_date": "2017-08-23", "authors_parsed": [["Teeraratkul", "Thanchanok", ""], ["O'Neill", "Daniel", ""], ["Lall", "Sanjay", ""]]}, {"id": "1702.01417", "submitter": "Jiaqi Mu", "authors": "Jiaqi Mu, Suma Bhat, Pramod Viswanath", "title": "All-but-the-Top: Simple and Effective Postprocessing for Word\n  Representations", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Real-valued word representations have transformed NLP applications; popular\nexamples are word2vec and GloVe, recognized for their ability to capture\nlinguistic regularities. In this paper, we demonstrate a {\\em very simple}, and\nyet counter-intuitive, postprocessing technique -- eliminate the common mean\nvector and a few top dominating directions from the word vectors -- that\nrenders off-the-shelf representations {\\em even stronger}. The postprocessing\nis empirically validated on a variety of lexical-level intrinsic tasks (word\nsimilarity, concept categorization, word analogy) and sentence-level tasks\n(semantic textural similarity and { text classification}) on multiple datasets\nand with a variety of representation methods and hyperparameter choices in\nmultiple languages; in each case, the processed representations are\nconsistently better than the original ones.\n", "versions": [{"version": "v1", "created": "Sun, 5 Feb 2017 15:43:07 GMT"}, {"version": "v2", "created": "Mon, 19 Mar 2018 20:28:27 GMT"}], "update_date": "2018-03-21", "authors_parsed": [["Mu", "Jiaqi", ""], ["Bhat", "Suma", ""], ["Viswanath", "Pramod", ""]]}, {"id": "1702.01618", "submitter": "Andreas Svensson", "authors": "Andreas Svensson, Thomas B. Sch\\\"on and Fredrik Lindsten", "title": "Learning of state-space models with highly informative observations: a\n  tempered Sequential Monte Carlo solution", "comments": null, "journal-ref": "Mechanical Systems and Signal Processing, Volume 104 (May 2018),\n  Pages 915-928", "doi": "10.1016/j.ymssp.2017.09.016", "report-no": null, "categories": "stat.CO stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Probabilistic (or Bayesian) modeling and learning offers interesting\npossibilities for systematic representation of uncertainty using probability\ntheory. However, probabilistic learning often leads to computationally\nchallenging problems. Some problems of this type that were previously\nintractable can now be solved on standard personal computers thanks to recent\nadvances in Monte Carlo methods. In particular, for learning of unknown\nparameters in nonlinear state-space models, methods based on the particle\nfilter (a Monte Carlo method) have proven very useful. A notoriously\nchallenging problem, however, still occurs when the observations in the\nstate-space model are highly informative, i.e. when there is very little or no\nmeasurement noise present, relative to the amount of process noise. The\nparticle filter will then struggle in estimating one of the basic components\nfor probabilistic learning, namely the likelihood $p($data$|$parameters$)$. To\nthis end we suggest an algorithm which initially assumes that there is\nsubstantial amount of artificial measurement noise present. The variance of\nthis noise is sequentially decreased in an adaptive fashion such that we, in\nthe end, recover the original problem or possibly a very close approximation of\nit. The main component in our algorithm is a sequential Monte Carlo (SMC)\nsampler, which gives our proposed method a clear resemblance to the SMC^2\nmethod. Another natural link is also made to the ideas underlying the\napproximate Bayesian computation (ABC). We illustrate it with numerical\nexamples, and in particular show promising results for a challenging\nWiener-Hammerstein benchmark problem.\n", "versions": [{"version": "v1", "created": "Mon, 6 Feb 2017 14:01:20 GMT"}, {"version": "v2", "created": "Wed, 13 Dec 2017 10:24:40 GMT"}], "update_date": "2018-03-14", "authors_parsed": [["Svensson", "Andreas", ""], ["Sch\u00f6n", "Thomas B.", ""], ["Lindsten", "Fredrik", ""]]}, {"id": "1702.01717", "submitter": "Zeeshan Malik Khawar", "authors": "Zeeshan Khawar Malik, Mo Kobrosli and Peter Maas", "title": "Search Intelligence: Deep Learning For Dominant Category Prediction", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Deep Neural Networks, and specifically fully-connected convolutional neural\nnetworks are achieving remarkable results across a wide variety of domains.\nThey have been trained to achieve state-of-the-art performance when applied to\nproblems such as speech recognition, image classification, natural language\nprocessing and bioinformatics. Most of these deep learning models when applied\nto classification employ the softmax activation function for prediction and aim\nto minimize cross-entropy loss. In this paper, we have proposed a supervised\nmodel for dominant category prediction to improve search recall across all eBay\nclassifieds platforms. The dominant category label for each query in the last\n90 days is first calculated by summing the total number of collaborative clicks\namong all categories. The category having the highest number of collaborative\nclicks for the given query will be considered its dominant category. Second,\neach query is transformed to a numeric vector by mapping each unique word in\nthe query document to a unique integer value; all padded to equal length based\non the maximum document length within the pre-defined vocabulary size. A\nfully-connected deep convolutional neural network (CNN) is then applied for\nclassification. The proposed model achieves very high classification accuracy\ncompared to other state-of-the-art machine learning techniques.\n", "versions": [{"version": "v1", "created": "Mon, 6 Feb 2017 17:27:12 GMT"}], "update_date": "2017-02-07", "authors_parsed": [["Malik", "Zeeshan Khawar", ""], ["Kobrosli", "Mo", ""], ["Maas", "Peter", ""]]}, {"id": "1702.01780", "submitter": "Randal Olson", "authors": "Andrew Sohn and Randal S. Olson and Jason H. Moore", "title": "Toward the automated analysis of complex diseases in genome-wide\n  association studies using genetic programming", "comments": "9 pages, 4 figures, submitted to GECCO 2017 conference and currently\n  under review", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NE cs.LG q-bio.QM stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Machine learning has been gaining traction in recent years to meet the demand\nfor tools that can efficiently analyze and make sense of the ever-growing\ndatabases of biomedical data in health care systems around the world. However,\neffectively using machine learning methods requires considerable domain\nexpertise, which can be a barrier of entry for bioinformaticians new to\ncomputational data science methods. Therefore, off-the-shelf tools that make\nmachine learning more accessible can prove invaluable for bioinformaticians. To\nthis end, we have developed an open source pipeline optimization tool\n(TPOT-MDR) that uses genetic programming to automatically design machine\nlearning pipelines for bioinformatics studies. In TPOT-MDR, we implement\nMultifactor Dimensionality Reduction (MDR) as a feature construction method for\nmodeling higher-order feature interactions, and combine it with a new expert\nknowledge-guided feature selector for large biomedical data sets. We\ndemonstrate TPOT-MDR's capabilities using a combination of simulated and real\nworld data sets from human genetics and find that TPOT-MDR significantly\noutperforms modern machine learning methods such as logistic regression and\neXtreme Gradient Boosting (XGBoost). We further analyze the best pipeline\ndiscovered by TPOT-MDR for a real world problem and highlight TPOT-MDR's\nability to produce a high-accuracy solution that is also easily interpretable.\n", "versions": [{"version": "v1", "created": "Mon, 6 Feb 2017 20:10:10 GMT"}], "update_date": "2017-02-08", "authors_parsed": [["Sohn", "Andrew", ""], ["Olson", "Randal S.", ""], ["Moore", "Jason H.", ""]]}, {"id": "1702.01811", "submitter": "Adedotun Akintayo", "authors": "Adedotun Akintayo and Soumik Sarkar", "title": "Hierarchical Symbolic Dynamic Filtering of Streaming Non-stationary Time\n  Series Data", "comments": "26 pages, 11 figures preprint submitted to Journal of Signal\n  Processing", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper proposes a hierarchical feature extractor for non-stationary\nstreaming time series based on the concept of switching observable Markov chain\nmodels. The slow time-scale non-stationary behaviors are considered to be a\nmixture of quasi-stationary fast time-scale segments that are exhibited by\ncomplex dynamical systems. The idea is to model each unique stationary\ncharacteristic without a priori knowledge (e.g., number of possible unique\ncharacteristics) at a lower logical level, and capture the transitions from one\nlow-level model to another at a higher level. In this context, the concepts in\nthe recently developed Symbolic Dynamic Filtering (SDF) is extended, to build\nan online algorithm suited for handling quasi-stationary data at a lower level\nand a non-stationary behavior at a higher level without a priori knowledge. A\nkey observation made in this study is that the rate of change of data\nlikelihood seems to be a better indicator of change in data characteristics\ncompared to the traditional methods that mostly consider data likelihood for\nchange detection. The algorithm minimizes model complexity and captures data\nlikelihood. Efficacy demonstration and comparative evaluation of the proposed\nalgorithm are performed using time series data simulated from systems that\nexhibit nonlinear dynamics. We discuss results that show that the proposed\nhierarchical SDF algorithm can identify underlying features with significantly\nhigh degree of accuracy, even under very noisy conditions. Algorithm is\ndemonstrated to perform better than the baseline Hierarchical Dirichlet\nProcess-Hidden Markov Models (HDP-HMM). The low computational complexity of\nalgorithm makes it suitable for on-board, real time operations.\n", "versions": [{"version": "v1", "created": "Mon, 6 Feb 2017 22:34:44 GMT"}], "update_date": "2017-02-08", "authors_parsed": [["Akintayo", "Adedotun", ""], ["Sarkar", "Soumik", ""]]}, {"id": "1702.01816", "submitter": "David Ledbetter", "authors": "David Ledbetter, Long Ho, Kevin V Lemley", "title": "Prediction of Kidney Function from Biopsy Images Using Convolutional\n  Neural Networks", "comments": "11 pages, 7 figures, 1 page of Appendix", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML q-bio.QM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A Convolutional Neural Network was used to predict kidney function in\npatients with chronic kidney disease from high-resolution digital pathology\nscans of their kidney biopsies. Kidney biopsies were taken from participants of\nthe NEPTUNE study, a longitudinal cohort study whose goal is to set up\ninfrastructure for observing the evolution of 3 forms of idiopathic nephrotic\nsyndrome, including developing predictors for progression of kidney disease.\nThe knowledge of future kidney function is desirable as it can identify\nhigh-risk patients and influence treatment decisions, reducing the likelihood\nof irreversible kidney decline.\n", "versions": [{"version": "v1", "created": "Mon, 6 Feb 2017 22:52:43 GMT"}], "update_date": "2017-02-08", "authors_parsed": [["Ledbetter", "David", ""], ["Ho", "Long", ""], ["Lemley", "Kevin V", ""]]}, {"id": "1702.01824", "submitter": "Franziska Horn", "authors": "Franziska Horn and Klaus-Robert M\\\"uller", "title": "Predicting Pairwise Relations with Neural Similarity Encoders", "comments": "find code here: https://github.com/cod3licious/simec", "journal-ref": "Bulletin of the Polish Academy of Sciences: Technical Sciences,\n  66(6):821-830, 2018", "doi": "10.24425/bpas.2018.125929", "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Matrix factorization is at the heart of many machine learning algorithms, for\nexample, dimensionality reduction (e.g. kernel PCA) or recommender systems\nrelying on collaborative filtering. Understanding a singular value\ndecomposition (SVD) of a matrix as a neural network optimization problem\nenables us to decompose large matrices efficiently while dealing naturally with\nmissing values in the given matrix. But most importantly, it allows us to learn\nthe connection between data points' feature vectors and the matrix containing\ninformation about their pairwise relations. In this paper we introduce a novel\nneural network architecture termed Similarity Encoder (SimEc), which is\ndesigned to simultaneously factorize a given target matrix while also learning\nthe mapping to project the data points' feature vectors into a similarity\npreserving embedding space. This makes it possible to, for example, easily\ncompute out-of-sample solutions for new data points. Additionally, we\ndemonstrate that SimEc can preserve non-metric similarities and even predict\nmultiple pairwise relations between data points at once.\n", "versions": [{"version": "v1", "created": "Mon, 6 Feb 2017 23:46:40 GMT"}, {"version": "v2", "created": "Wed, 9 Jan 2019 21:56:45 GMT"}], "update_date": "2019-01-11", "authors_parsed": [["Horn", "Franziska", ""], ["M\u00fcller", "Klaus-Robert", ""]]}, {"id": "1702.01825", "submitter": "Niru Maheswaranathan", "authors": "Lane T. McIntosh, Niru Maheswaranathan, Aran Nayebi, Surya Ganguli,\n  Stephen A. Baccus", "title": "Deep Learning Models of the Retinal Response to Natural Scenes", "comments": "L.T.M. and N.M. contributed equally to this work. Presented at NIPS\n  2016", "journal-ref": "Advances in Neural Information Processing Systems 29 (2016)\n  1361-1369", "doi": null, "report-no": null, "categories": "q-bio.NC stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A central challenge in neuroscience is to understand neural computations and\ncircuit mechanisms that underlie the encoding of ethologically relevant,\nnatural stimuli. In multilayered neural circuits, nonlinear processes such as\nsynaptic transmission and spiking dynamics present a significant obstacle to\nthe creation of accurate computational models of responses to natural stimuli.\nHere we demonstrate that deep convolutional neural networks (CNNs) capture\nretinal responses to natural scenes nearly to within the variability of a\ncell's response, and are markedly more accurate than linear-nonlinear (LN)\nmodels and Generalized Linear Models (GLMs). Moreover, we find two additional\nsurprising properties of CNNs: they are less susceptible to overfitting than\ntheir LN counterparts when trained on small amounts of data, and generalize\nbetter when tested on stimuli drawn from a different distribution (e.g. between\nnatural scenes and white noise). Examination of trained CNNs reveals several\nproperties. First, a richer set of feature maps is necessary for predicting the\nresponses to natural scenes compared to white noise. Second, temporally precise\nresponses to slowly varying inputs originate from feedforward inhibition,\nsimilar to known retinal mechanisms. Third, the injection of latent noise\nsources in intermediate layers enables our model to capture the sub-Poisson\nspiking variability observed in retinal ganglion cells. Fourth, augmenting our\nCNNs with recurrent lateral connections enables them to capture contrast\nadaptation as an emergent property of accurately describing retinal responses\nto natural scenes. These methods can be readily generalized to other sensory\nmodalities and stimulus ensembles. Overall, this work demonstrates that CNNs\nnot only accurately capture sensory circuit responses to natural scenes, but\nalso yield information about the circuit's internal structure and function.\n", "versions": [{"version": "v1", "created": "Mon, 6 Feb 2017 23:48:19 GMT"}], "update_date": "2017-02-09", "authors_parsed": [["McIntosh", "Lane T.", ""], ["Maheswaranathan", "Niru", ""], ["Nayebi", "Aran", ""], ["Ganguli", "Surya", ""], ["Baccus", "Stephen A.", ""]]}, {"id": "1702.01831", "submitter": "James P. Crutchfield", "authors": "Ryan G. James, John R. Mahoney, and James P. Crutchfield", "title": "Trimming the Independent Fat: Sufficient Statistics, Mutual Information,\n  and Predictability from Effective Channel States", "comments": "6 pages, 4 figures; http://csc.ucdavis.edu/~cmg/compmech/pubs/mcs.htm", "journal-ref": "Phys. Rev. E 95, 060102 (2017)", "doi": "10.1103/PhysRevE.95.060102", "report-no": null, "categories": "cond-mat.stat-mech cs.IT math.IT nlin.CD stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  One of the most fundamental questions one can ask about a pair of random\nvariables X and Y is the value of their mutual information. Unfortunately, this\ntask is often stymied by the extremely large dimension of the variables. We\nmight hope to replace each variable by a lower-dimensional representation that\npreserves the relationship with the other variable. The theoretically ideal\nimplementation is the use of minimal sufficient statistics, where it is\nwell-known that either X or Y can be replaced by their minimal sufficient\nstatistic about the other while preserving the mutual information. While\nintuitively reasonable, it is not obvious or straightforward that both\nvariables can be replaced simultaneously. We demonstrate that this is in fact\npossible: the information X's minimal sufficient statistic preserves about Y is\nexactly the information that Y's minimal sufficient statistic preserves about\nX. As an important corollary, we consider the case where one variable is a\nstochastic process' past and the other its future and the present is viewed as\na memoryful channel. In this case, the mutual information is the channel\ntransmission rate between the channel's effective states. That is, the\npast-future mutual information (the excess entropy) is the amount of\ninformation about the future that can be predicted using the past. Translating\nour result about minimal sufficient statistics, this is equivalent to the\nmutual information between the forward- and reverse-time causal states of\ncomputational mechanics. We close by discussing multivariate extensions to this\nuse of minimal sufficient statistics.\n", "versions": [{"version": "v1", "created": "Tue, 7 Feb 2017 00:52:02 GMT"}], "update_date": "2017-06-21", "authors_parsed": [["James", "Ryan G.", ""], ["Mahoney", "John R.", ""], ["Crutchfield", "James P.", ""]]}, {"id": "1702.01847", "submitter": "Mostafa Rahmani", "authors": "Mostafa Rahmani, George Atia", "title": "Low Rank Matrix Recovery with Simultaneous Presence of Outliers and\n  Sparse Corruption", "comments": null, "journal-ref": null, "doi": "10.1109/JSTSP.2018.2876604", "report-no": null, "categories": "stat.ML cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study a data model in which the data matrix D can be expressed as D = L +\nS + C, where L is a low rank matrix, S an element-wise sparse matrix and C a\nmatrix whose non-zero columns are outlying data points. To date, robust PCA\nalgorithms have solely considered models with either S or C, but not both. As\nsuch, existing algorithms cannot account for simultaneous element-wise and\ncolumn-wise corruptions. In this paper, a new robust PCA algorithm that is\nrobust to simultaneous types of corruption is proposed. Our approach hinges on\nthe sparse approximation of a sparsely corrupted column so that the sparse\nexpansion of a column with respect to the other data points is used to\ndistinguish a sparsely corrupted inlier column from an outlying data point. We\nalso develop a randomized design which provides a scalable implementation of\nthe proposed approach. The core idea of sparse approximation is analyzed\nanalytically where we show that the underlying ell_1-norm minimization can\nobtain the representation of an inlier in presence of sparse corruptions.\n", "versions": [{"version": "v1", "created": "Tue, 7 Feb 2017 02:08:51 GMT"}], "update_date": "2019-01-30", "authors_parsed": [["Rahmani", "Mostafa", ""], ["Atia", "George", ""]]}, {"id": "1702.01935", "submitter": "Shuisheng Zhou", "authors": "Li Chen and Shuisheng Zhou", "title": "Sparse Algorithm for Robust LSSVM in Primal Space", "comments": "22 pages, 4 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  As enjoying the closed form solution, least squares support vector machine\n(LSSVM) has been widely used for classification and regression problems having\nthe comparable performance with other types of SVMs. However, LSSVM has two\ndrawbacks: sensitive to outliers and lacking sparseness. Robust LSSVM (R-LSSVM)\novercomes the first partly via nonconvex truncated loss function, but the\ncurrent algorithms for R-LSSVM with the dense solution are faced with the\nsecond drawback and are inefficient for training large-scale problems. In this\npaper, we interpret the robustness of R-LSSVM from a re-weighted viewpoint and\ngive a primal R-LSSVM by the representer theorem. The new model may have sparse\nsolution if the corresponding kernel matrix has low rank. Then approximating\nthe kernel matrix by a low-rank matrix and smoothing the loss function by\nentropy penalty function, we propose a convergent sparse R-LSSVM (SR-LSSVM)\nalgorithm to achieve the sparse solution of primal R-LSSVM, which overcomes two\ndrawbacks of LSSVM simultaneously. The proposed algorithm has lower complexity\nthan the existing algorithms and is very efficient for training large-scale\nproblems. Many experimental results illustrate that SR-LSSVM can achieve better\nor comparable performance with less training time than related algorithms,\nespecially for training large scale problems.\n", "versions": [{"version": "v1", "created": "Tue, 7 Feb 2017 09:24:26 GMT"}], "update_date": "2017-02-08", "authors_parsed": [["Chen", "Li", ""], ["Zhou", "Shuisheng", ""]]}, {"id": "1702.01975", "submitter": "Vladimir Dzyuba", "authors": "Vladimir Dzyuba, Matthijs van Leeuwen", "title": "Learning what matters - Sampling interesting patterns", "comments": "PAKDD 2017, extended version", "journal-ref": "Advances in Knowledge Discovery and Data Mining. PAKDD 2017.\n  Lecture Notes in Computer Science, vol.10234, 2017, pp.534-546", "doi": "10.1007/978-3-319-57454-7_42", "report-no": null, "categories": "stat.ML cs.AI cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In the field of exploratory data mining, local structure in data can be\ndescribed by patterns and discovered by mining algorithms. Although many\nsolutions have been proposed to address the redundancy problems in pattern\nmining, most of them either provide succinct pattern sets or take the interests\nof the user into account-but not both. Consequently, the analyst has to invest\nsubstantial effort in identifying those patterns that are relevant to her\nspecific interests and goals. To address this problem, we propose a novel\napproach that combines pattern sampling with interactive data mining. In\nparticular, we introduce the LetSIP algorithm, which builds upon recent\nadvances in 1) weighted sampling in SAT and 2) learning to rank in interactive\npattern mining. Specifically, it exploits user feedback to directly learn the\nparameters of the sampling distribution that represents the user's interests.\nWe compare the performance of the proposed algorithm to the state-of-the-art in\ninteractive pattern mining by emulating the interests of a user. The resulting\nsystem allows efficient and interleaved learning and sampling, thus\nuser-specific anytime data exploration. Finally, LetSIP demonstrates favourable\ntrade-offs concerning both quality-diversity and exploitation-exploration when\ncompared to existing methods.\n", "versions": [{"version": "v1", "created": "Tue, 7 Feb 2017 12:01:08 GMT"}, {"version": "v2", "created": "Fri, 10 Feb 2017 16:22:18 GMT"}], "update_date": "2017-04-28", "authors_parsed": [["Dzyuba", "Vladimir", ""], ["van Leeuwen", "Matthijs", ""]]}, {"id": "1702.01992", "submitter": "John Arevalo", "authors": "John Arevalo, Thamar Solorio, Manuel Montes-y-G\\'omez, Fabio A.\n  Gonz\\'alez", "title": "Gated Multimodal Units for Information Fusion", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper presents a novel model for multimodal learning based on gated\nneural networks. The Gated Multimodal Unit (GMU) model is intended to be used\nas an internal unit in a neural network architecture whose purpose is to find\nan intermediate representation based on a combination of data from different\nmodalities. The GMU learns to decide how modalities influence the activation of\nthe unit using multiplicative gates. It was evaluated on a multilabel scenario\nfor genre classification of movies using the plot and the poster. The GMU\nimproved the macro f-score performance of single-modality approaches and\noutperformed other fusion strategies, including mixture of experts models.\nAlong with this work, the MM-IMDb dataset is released which, to the best of our\nknowledge, is the largest publicly available multimodal dataset for genre\nprediction on movies.\n", "versions": [{"version": "v1", "created": "Tue, 7 Feb 2017 13:05:19 GMT"}], "update_date": "2017-02-08", "authors_parsed": [["Arevalo", "John", ""], ["Solorio", "Thamar", ""], ["Montes-y-G\u00f3mez", "Manuel", ""], ["Gonz\u00e1lez", "Fabio A.", ""]]}, {"id": "1702.01997", "submitter": "Dennis Forster", "authors": "Dennis Forster and J\\\"org L\\\"ucke", "title": "Truncated Variational EM for Semi-Supervised Neural Simpletrons", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Inference and learning for probabilistic generative networks is often very\nchallenging and typically prevents scalability to as large networks as used for\ndeep discriminative approaches. To obtain efficiently trainable, large-scale\nand well performing generative networks for semi-supervised learning, we here\ncombine two recent developments: a neural network reformulation of hierarchical\nPoisson mixtures (Neural Simpletrons), and a novel truncated variational EM\napproach (TV-EM). TV-EM provides theoretical guarantees for learning in\ngenerative networks, and its application to Neural Simpletrons results in\nparticularly compact, yet approximately optimal, modifications of learning\nequations. If applied to standard benchmarks, we empirically find, that\nlearning converges in fewer EM iterations, that the complexity per EM iteration\nis reduced, and that final likelihood values are higher on average. For the\ntask of classification on data sets with few labels, learning improvements\nresult in consistently lower error rates if compared to applications without\ntruncation. Experiments on the MNIST data set herein allow for comparison to\nstandard and state-of-the-art models in the semi-supervised setting. Further\nexperiments on the NIST SD19 data set show the scalability of the approach when\na manifold of additional unlabeled data is available.\n", "versions": [{"version": "v1", "created": "Tue, 7 Feb 2017 13:16:05 GMT"}], "update_date": "2017-02-08", "authors_parsed": [["Forster", "Dennis", ""], ["L\u00fccke", "J\u00f6rg", ""]]}, {"id": "1702.02025", "submitter": "Hau-tieng Wu", "authors": "Ruilin Li and Martin G. Frasch and Hau-tieng Wu", "title": "Efficient fetal-maternal ECG signal separation from two channel maternal\n  abdominal ECG via diffusion-based channel selection", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "physics.med-ph physics.data-an stat.AP stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  There is a need for affordable, widely deployable maternal-fetal ECG monitors\nto improve maternal and fetal health during pregnancy and delivery. Based on\nthe diffusion-based channel selection, here we present the mathematical\nformalism and clinical validation of an algorithm capable of accurate\nseparation of maternal and fetal ECG from a two channel signal acquired over\nmaternal abdomen.\n", "versions": [{"version": "v1", "created": "Tue, 7 Feb 2017 14:06:58 GMT"}], "update_date": "2017-02-08", "authors_parsed": [["Li", "Ruilin", ""], ["Frasch", "Martin G.", ""], ["Wu", "Hau-tieng", ""]]}, {"id": "1702.02103", "submitter": "Matthew Veres", "authors": "Matthew Veres, Medhat Moussa, Graham W. Taylor", "title": "An Integrated Simulator and Dataset that Combines Grasping and Vision\n  for Deep Learning", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.RO stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Deep learning is an established framework for learning hierarchical data\nrepresentations. While compute power is in abundance, one of the main\nchallenges in applying this framework to robotic grasping has been obtaining\nthe amount of data needed to learn these representations, and structuring the\ndata to the task at hand. Among contemporary approaches in the literature, we\nhighlight key properties that have encouraged the use of deep learning\ntechniques, and in this paper, detail our experience in developing a simulator\nfor collecting cylindrical precision grasps of a multi-fingered dexterous\nrobotic hand.\n", "versions": [{"version": "v1", "created": "Tue, 7 Feb 2017 17:16:15 GMT"}, {"version": "v2", "created": "Mon, 17 Apr 2017 22:22:46 GMT"}], "update_date": "2017-04-19", "authors_parsed": [["Veres", "Matthew", ""], ["Moussa", "Medhat", ""], ["Taylor", "Graham W.", ""]]}, {"id": "1702.02165", "submitter": "Joaquin Ortega", "authors": "Diego Rivera-Garc\\'ia and Luis Angel Garc\\'ia-Escudero and Agust\\'in\n  Mayo-Iscar and Joaqu\\'in Ortega", "title": "Robust Clustering for Time Series Using Spectral Densities and\n  Functional Data Analysis", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this work a robust clustering algorithm for stationary time series is\nproposed. The algorithm is based on the use of estimated spectral densities,\nwhich are considered as functional data, as the basic characteristic of\nstationary time series for clustering purposes. A robust algorithm for\nfunctional data is then applied to the set of spectral densities. Trimming\ntechniques and restrictions on the scatter within groups reduce the effect of\nnoise in the data and help to prevent the identification of spurious clusters.\nThe procedure is tested in a simulation study, and is also applied to a real\ndata set.\n", "versions": [{"version": "v1", "created": "Tue, 7 Feb 2017 19:08:41 GMT"}], "update_date": "2017-02-09", "authors_parsed": [["Rivera-Garc\u00eda", "Diego", ""], ["Garc\u00eda-Escudero", "Luis Angel", ""], ["Mayo-Iscar", "Agust\u00edn", ""], ["Ortega", "Joaqu\u00edn", ""]]}, {"id": "1702.02181", "submitter": "Moshe Looks", "authors": "Moshe Looks, Marcello Herreshoff, DeLesley Hutchins, Peter Norvig", "title": "Deep Learning with Dynamic Computation Graphs", "comments": "Published as a conference paper at ICLR 2017", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NE cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Neural networks that compute over graph structures are a natural fit for\nproblems in a variety of domains, including natural language (parse trees) and\ncheminformatics (molecular graphs). However, since the computation graph has a\ndifferent shape and size for every input, such networks do not directly support\nbatched training or inference. They are also difficult to implement in popular\ndeep learning libraries, which are based on static data-flow graphs. We\nintroduce a technique called dynamic batching, which not only batches together\noperations between different input graphs of dissimilar shape, but also between\ndifferent nodes within a single input graph. The technique allows us to create\nstatic graphs, using popular libraries, that emulate dynamic computation graphs\nof arbitrary shape and size. We further present a high-level library of\ncompositional blocks that simplifies the creation of dynamic graph models.\nUsing the library, we demonstrate concise and batch-wise parallel\nimplementations for a variety of models from the literature.\n", "versions": [{"version": "v1", "created": "Tue, 7 Feb 2017 19:59:43 GMT"}, {"version": "v2", "created": "Wed, 22 Feb 2017 04:43:02 GMT"}], "update_date": "2017-02-23", "authors_parsed": [["Looks", "Moshe", ""], ["Herreshoff", "Marcello", ""], ["Hutchins", "DeLesley", ""], ["Norvig", "Peter", ""]]}, {"id": "1702.02215", "submitter": "Eleni Rozaki", "authors": "Cormac Dullaghan and Eleni Rozaki", "title": "Integration of Machine Learning Techniques to Evaluate Dynamic Customer\n  Segmentation Analysis for Mobile Customers", "comments": "12 pages", "journal-ref": "International Journal of Data Mining & Knowledge Management\n  Process (IJDKP) Vol.7, No.1, January 2017", "doi": "10.5121/ijdkp.2017.7102", "report-no": null, "categories": "cs.CY cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The telecommunications industry is highly competitive, which means that the\nmobile providers need a business intelligence model that can be used to achieve\nan optimal level of churners, as well as a minimal level of cost in marketing\nactivities. Machine learning applications can be used to provide guidance on\nmarketing strategies. Furthermore, data mining techniques can be used in the\nprocess of customer segmentation. The purpose of this paper is to provide a\ndetailed analysis of the C.5 algorithm, within naive Bayesian modelling for the\ntask of segmenting telecommunication customers behavioural profiling according\nto their billing and socio-demographic aspects. Results have been\nexperimentally implemented.\n", "versions": [{"version": "v1", "created": "Tue, 31 Jan 2017 23:26:06 GMT"}], "update_date": "2017-02-09", "authors_parsed": [["Dullaghan", "Cormac", ""], ["Rozaki", "Eleni", ""]]}, {"id": "1702.02258", "submitter": "Ehsan Jahangiri", "authors": "Ehsan Jahangiri, Alan L. Yuille", "title": "Generating Multiple Diverse Hypotheses for Human 3D Pose Consistent with\n  2D Joint Detections", "comments": "accepted to ICCV 2017 (PeopleCap)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI cs.MM stat.ML", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  We propose a method to generate multiple diverse and valid human pose\nhypotheses in 3D all consistent with the 2D detection of joints in a monocular\nRGB image. We use a novel generative model uniform (unbiased) in the space of\nanatomically plausible 3D poses. Our model is compositional (produces a pose by\ncombining parts) and since it is restricted only by anatomical constraints it\ncan generalize to every plausible human 3D pose. Removing the model bias\nintrinsically helps to generate more diverse 3D pose hypotheses. We argue that\ngenerating multiple pose hypotheses is more reasonable than generating only a\nsingle 3D pose based on the 2D joint detection given the depth ambiguity and\nthe uncertainty due to occlusion and imperfect 2D joint detection. We hope that\nthe idea of generating multiple consistent pose hypotheses can give rise to a\nnew line of future work that has not received much attention in the literature.\nWe used the Human3.6M dataset for empirical evaluation.\n", "versions": [{"version": "v1", "created": "Wed, 8 Feb 2017 02:54:25 GMT"}, {"version": "v2", "created": "Sun, 20 Aug 2017 19:39:19 GMT"}], "update_date": "2017-08-22", "authors_parsed": [["Jahangiri", "Ehsan", ""], ["Yuille", "Alan L.", ""]]}, {"id": "1702.02262", "submitter": "Quang N. Tran", "authors": "Quang N. Tran, Ba-Ngu Vo, Dinh Phung and Ba-Tuong Vo", "title": "Clustering For Point Pattern Data", "comments": "Preprint: 23rd Int. Conf. Pattern Recognition (ICPR). Cancun, Mexico,\n  December 2016", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Clustering is one of the most common unsupervised learning tasks in machine\nlearning and data mining. Clustering algorithms have been used in a plethora of\napplications across several scientific fields. However, there has been limited\nresearch in the clustering of point patterns - sets or multi-sets of unordered\nelements - that are found in numerous applications and data sources. In this\npaper, we propose two approaches for clustering point patterns. The first is a\nnon-parametric method based on novel distances for sets. The second is a\nmodel-based approach, formulated via random finite set theory, and solved by\nthe Expectation-Maximization algorithm. Numerical experiments show that the\nproposed methods perform well on both simulated and real data.\n", "versions": [{"version": "v1", "created": "Wed, 8 Feb 2017 03:21:22 GMT"}], "update_date": "2017-02-09", "authors_parsed": [["Tran", "Quang N.", ""], ["Vo", "Ba-Ngu", ""], ["Phung", "Dinh", ""], ["Vo", "Ba-Tuong", ""]]}, {"id": "1702.02267", "submitter": "Quan Li", "authors": "David Gamarnik, Quan Li and Hongyi Zhang", "title": "Matrix Completion from $O(n)$ Samples in Linear Time", "comments": "45 pages, 1 figure. Short version accepted for presentation at\n  Conference on Learning Theory (COLT) 2017", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.DS cs.LG math.OC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider the problem of reconstructing a rank-$k$ $n \\times n$ matrix $M$\nfrom a sampling of its entries. Under a certain incoherence assumption on $M$\nand for the case when both the rank and the condition number of $M$ are\nbounded, it was shown in \\cite{CandesRecht2009, CandesTao2010, keshavan2010,\nRecht2011, Jain2012, Hardt2014} that $M$ can be recovered exactly or\napproximately (depending on some trade-off between accuracy and computational\ncomplexity) using $O(n \\, \\text{poly}(\\log n))$ samples in super-linear time\n$O(n^{a} \\, \\text{poly}(\\log n))$ for some constant $a \\geq 1$.\n  In this paper, we propose a new matrix completion algorithm using a novel\nsampling scheme based on a union of independent sparse random regular bipartite\ngraphs. We show that under the same conditions w.h.p. our algorithm recovers an\n$\\epsilon$-approximation of $M$ in terms of the Frobenius norm using $O(n\n\\log^2(1/\\epsilon))$ samples and in linear time $O(n \\log^2(1/\\epsilon))$. This\nprovides the best known bounds both on the sample complexity and computational\ncomplexity for reconstructing (approximately) an unknown low-rank matrix.\n  The novelty of our algorithm is two new steps of thresholding singular values\nand rescaling singular vectors in the application of the \"vanilla\" alternating\nminimization algorithm. The structure of sparse random regular graphs is used\nheavily for controlling the impact of these regularization steps.\n", "versions": [{"version": "v1", "created": "Wed, 8 Feb 2017 03:52:40 GMT"}, {"version": "v2", "created": "Sat, 3 Jun 2017 21:59:54 GMT"}, {"version": "v3", "created": "Tue, 6 Jun 2017 04:14:45 GMT"}, {"version": "v4", "created": "Tue, 22 Aug 2017 04:05:36 GMT"}], "update_date": "2017-08-23", "authors_parsed": [["Gamarnik", "David", ""], ["Li", "Quan", ""], ["Zhang", "Hongyi", ""]]}, {"id": "1702.02284", "submitter": "Sandy Huang", "authors": "Sandy Huang, Nicolas Papernot, Ian Goodfellow, Yan Duan, Pieter Abbeel", "title": "Adversarial Attacks on Neural Network Policies", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CR stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Machine learning classifiers are known to be vulnerable to inputs maliciously\nconstructed by adversaries to force misclassification. Such adversarial\nexamples have been extensively studied in the context of computer vision\napplications. In this work, we show adversarial attacks are also effective when\ntargeting neural network policies in reinforcement learning. Specifically, we\nshow existing adversarial example crafting techniques can be used to\nsignificantly degrade test-time performance of trained policies. Our threat\nmodel considers adversaries capable of introducing small perturbations to the\nraw input of the policy. We characterize the degree of vulnerability across\ntasks and training algorithms, for a subclass of adversarial-example attacks in\nwhite-box and black-box settings. Regardless of the learned task or training\nalgorithm, we observe a significant drop in performance, even with small\nadversarial perturbations that do not interfere with human perception. Videos\nare available at http://rll.berkeley.edu/adversarial.\n", "versions": [{"version": "v1", "created": "Wed, 8 Feb 2017 04:33:55 GMT"}], "update_date": "2017-02-09", "authors_parsed": [["Huang", "Sandy", ""], ["Papernot", "Nicolas", ""], ["Goodfellow", "Ian", ""], ["Duan", "Yan", ""], ["Abbeel", "Pieter", ""]]}, {"id": "1702.02519", "submitter": "Adrian Benton", "authors": "Adrian Benton, Huda Khayrallah, Biman Gujral, Dee Ann Reisinger, Sheng\n  Zhang, Raman Arora", "title": "Deep Generalized Canonical Correlation Analysis", "comments": "14 pages, 6 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present Deep Generalized Canonical Correlation Analysis (DGCCA) -- a\nmethod for learning nonlinear transformations of arbitrarily many views of\ndata, such that the resulting transformations are maximally informative of each\nother. While methods for nonlinear two-view representation learning (Deep CCA,\n(Andrew et al., 2013)) and linear many-view representation learning\n(Generalized CCA (Horst, 1961)) exist, DGCCA is the first CCA-style multiview\nrepresentation learning technique that combines the flexibility of nonlinear\n(deep) representation learning with the statistical power of incorporating\ninformation from many independent sources, or views. We present the DGCCA\nformulation as well as an efficient stochastic optimization algorithm for\nsolving it. We learn DGCCA representations on two distinct datasets for three\ndownstream tasks: phonetic transcription from acoustic and articulatory\nmeasurements, and recommending hashtags and friends on a dataset of Twitter\nusers. We find that DGCCA representations soundly beat existing methods at\nphonetic transcription and hashtag recommendation, and in general perform no\nworse than standard linear many-view techniques.\n", "versions": [{"version": "v1", "created": "Wed, 8 Feb 2017 16:57:48 GMT"}, {"version": "v2", "created": "Thu, 15 Jun 2017 00:06:08 GMT"}], "update_date": "2017-06-16", "authors_parsed": [["Benton", "Adrian", ""], ["Khayrallah", "Huda", ""], ["Gujral", "Biman", ""], ["Reisinger", "Dee Ann", ""], ["Zhang", "Sheng", ""], ["Arora", "Raman", ""]]}, {"id": "1702.02526", "submitter": "Michael Kampffmeyer", "authors": "Michael Kampffmeyer, Sigurd L{\\o}kse, Filippo Maria Bianchi, Robert\n  Jenssen and Lorenzo Livi", "title": "Deep Kernelized Autoencoders", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper we introduce the deep kernelized autoencoder, a neural network\nmodel that allows an explicit approximation of (i) the mapping from an input\nspace to an arbitrary, user-specified kernel space and (ii) the back-projection\nfrom such a kernel space to input space. The proposed method is based on\ntraditional autoencoders and is trained through a new unsupervised loss\nfunction. During training, we optimize both the reconstruction accuracy of\ninput samples and the alignment between a kernel matrix given as prior and the\ninner products of the hidden representations computed by the autoencoder.\nKernel alignment provides control over the hidden representation learned by the\nautoencoder. Experiments have been performed to evaluate both reconstruction\nand kernel alignment performance. Additionally, we applied our method to\nemulate kPCA on a denoising task obtaining promising results.\n", "versions": [{"version": "v1", "created": "Wed, 8 Feb 2017 17:11:34 GMT"}], "update_date": "2017-02-09", "authors_parsed": [["Kampffmeyer", "Michael", ""], ["L\u00f8kse", "Sigurd", ""], ["Bianchi", "Filippo Maria", ""], ["Jenssen", "Robert", ""], ["Livi", "Lorenzo", ""]]}, {"id": "1702.02530", "submitter": "Lukas Machlica", "authors": "Lukas Machlica and Karel Bartos and Michal Sofka", "title": "Learning detectors of malicious web requests for intrusion detection in\n  network traffic", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper proposes a generic classification system designed to detect\nsecurity threats based on the behavior of malware samples. The system relies on\nstatistical features computed from proxy log fields to train detectors using a\ndatabase of malware samples. The behavior detectors serve as basic reusable\nbuilding blocks of the multi-level detection architecture. The detectors\nidentify malicious communication exploiting encrypted URL strings and domains\ngenerated by a Domain Generation Algorithm (DGA) which are frequently used in\nCommand and Control (C&C), phishing, and click fraud. Surprisingly, very\nprecise detectors can be built given only a limited amount of information\nextracted from a single proxy log. This way, the computational requirements of\nthe detectors are kept low which allows for deployment on a wide range of\nsecurity devices and without depending on traffic context such as DNS logs,\nWhois records, webpage content, etc. Results on several weeks of live traffic\nfrom 100+ companies having 350k+ hosts show correct detection with a precision\nexceeding 95% of malicious flows, 95% of malicious URLs and 90% of infected\nhosts. In addition, a comparison with a signature and rule-based solution shows\nthat our system is able to detect significant amount of new threats.\n", "versions": [{"version": "v1", "created": "Wed, 8 Feb 2017 17:21:05 GMT"}], "update_date": "2017-02-09", "authors_parsed": [["Machlica", "Lukas", ""], ["Bartos", "Karel", ""], ["Sofka", "Michal", ""]]}, {"id": "1702.02540", "submitter": "William Murdoch", "authors": "W. James Murdoch and Arthur Szlam", "title": "Automatic Rule Extraction from Long Short Term Memory Networks", "comments": "ICLR 2017 accepted paper", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI cs.NE stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Although deep learning models have proven effective at solving problems in\nnatural language processing, the mechanism by which they come to their\nconclusions is often unclear. As a result, these models are generally treated\nas black boxes, yielding no insight of the underlying learned patterns. In this\npaper we consider Long Short Term Memory networks (LSTMs) and demonstrate a new\napproach for tracking the importance of a given input to the LSTM for a given\noutput. By identifying consistently important patterns of words, we are able to\ndistill state of the art LSTMs on sentiment analysis and question answering\ninto a set of representative phrases. This representation is then\nquantitatively validated by using the extracted phrases to construct a simple,\nrule-based classifier which approximates the output of the LSTM.\n", "versions": [{"version": "v1", "created": "Wed, 8 Feb 2017 17:46:37 GMT"}, {"version": "v2", "created": "Fri, 24 Feb 2017 22:20:25 GMT"}], "update_date": "2017-02-28", "authors_parsed": [["Murdoch", "W. James", ""], ["Szlam", "Arthur", ""]]}, {"id": "1702.02555", "submitter": "Matthew Parker", "authors": "Matt Parker, Colin Parker", "title": "A Modified Construction for a Support Vector Classifier to Accommodate\n  Class Imbalances", "comments": "6 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Given a training set with binary classification, the Support Vector Machine\nidentifies the hyperplane maximizing the margin between the two classes of\ntraining data. This general formulation is useful in that it can be applied\nwithout regard to variance differences between the classes. Ignoring these\ndifferences is not optimal, however, as the general SVM will give the class\nwith lower variance an unjustifiably wide berth. This increases the chance of\nmisclassification of the other class and results in an overall loss of\npredictive performance. An alternate construction is proposed in which the\nmargins of the separating hyperplane are different for each class, each\nproportional to the standard deviation of its class along the direction\nperpendicular to the hyperplane. The construction agrees with the SVM in the\ncase of equal class variances. This paper will then examine the impact to the\ndual representation of the modified constraint equations.\n", "versions": [{"version": "v1", "created": "Wed, 8 Feb 2017 18:34:15 GMT"}, {"version": "v2", "created": "Fri, 10 Feb 2017 05:26:13 GMT"}], "update_date": "2017-02-13", "authors_parsed": [["Parker", "Matt", ""], ["Parker", "Colin", ""]]}, {"id": "1702.02604", "submitter": "Mohammad Taha Bahadori", "authors": "Mohammad Taha Bahadori, Krzysztof Chalupka, Edward Choi, Robert Chen,\n  Walter F. Stewart, Jimeng Sun", "title": "Causal Regularization", "comments": "Adding theoretical analysis, revising the text", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI cs.NE stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In application domains such as healthcare, we want accurate predictive models\nthat are also causally interpretable. In pursuit of such models, we propose a\ncausal regularizer to steer predictive models towards causally-interpretable\nsolutions and theoretically study its properties. In a large-scale analysis of\nElectronic Health Records (EHR), our causally-regularized model outperforms its\nL1-regularized counterpart in causal accuracy and is competitive in predictive\nperformance. We perform non-linear causality analysis by causally regularizing\na special neural network architecture. We also show that the proposed causal\nregularizer can be used together with neural representation learning algorithms\nto yield up to 20% improvement over multilayer perceptron in detecting\nmultivariate causation, a situation common in healthcare, where many causal\nfactors should occur simultaneously to have an effect on the target variable.\n", "versions": [{"version": "v1", "created": "Wed, 8 Feb 2017 20:23:59 GMT"}, {"version": "v2", "created": "Thu, 23 Feb 2017 18:52:58 GMT"}], "update_date": "2017-02-24", "authors_parsed": [["Bahadori", "Mohammad Taha", ""], ["Chalupka", "Krzysztof", ""], ["Choi", "Edward", ""], ["Chen", "Robert", ""], ["Stewart", "Walter F.", ""], ["Sun", "Jimeng", ""]]}, {"id": "1702.02661", "submitter": "U. N. Niranjan", "authors": "U.N. Niranjan, Arun Rajkumar", "title": "Inductive Pairwise Ranking: Going Beyond the n log(n) Barrier", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.IT math.IT stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study the problem of ranking a set of items from nonactively chosen\npairwise preferences where each item has feature information with it. We\npropose and characterize a very broad class of preference matrices giving rise\nto the Feature Low Rank (FLR) model, which subsumes several models ranging from\nthe classic Bradley-Terry-Luce (BTL) (Bradley and Terry 1952) and Thurstone\n(Thurstone 1927) models to the recently proposed blade-chest (Chen and Joachims\n2016) and generic low-rank preference (Rajkumar and Agarwal 2016) models. We\nuse the technique of matrix completion in the presence of side information to\ndevelop the Inductive Pairwise Ranking (IPR) algorithm that provably learns a\ngood ranking under the FLR model, in a sample-efficient manner. In practice,\nthrough systematic synthetic simulations, we confirm our theoretical findings\nregarding improvements in the sample complexity due to the use of feature\ninformation. Moreover, on popular real-world preference learning datasets, with\nas less as 10% sampling of the pairwise comparisons, our method recovers a good\nranking.\n", "versions": [{"version": "v1", "created": "Thu, 9 Feb 2017 00:17:39 GMT"}], "update_date": "2017-02-10", "authors_parsed": [["Niranjan", "U. N.", ""], ["Rajkumar", "Arun", ""]]}, {"id": "1702.02670", "submitter": "Stefan Steinerberger", "authors": "Uri Shaham, Stefan Steinerberger", "title": "Stochastic Neighbor Embedding separates well-separated clusters", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Stochastic Neighbor Embedding and its variants are widely used dimensionality\nreduction techniques -- despite their popularity, no theoretical results are\nknown. We prove that the optimal SNE embedding of well-separated clusters from\nhigh dimensions to any Euclidean space R^d manages to successfully separate the\nclusters in a quantitative way. The result also applies to a larger family of\nmethods including a variant of t-SNE.\n", "versions": [{"version": "v1", "created": "Thu, 9 Feb 2017 01:30:53 GMT"}, {"version": "v2", "created": "Wed, 22 Feb 2017 19:10:35 GMT"}], "update_date": "2017-02-24", "authors_parsed": [["Shaham", "Uri", ""], ["Steinerberger", "Stefan", ""]]}, {"id": "1702.02686", "submitter": "Yining Wang", "authors": "Yining Wang, Jialei Wang, Sivaraman Balakrishnan, Aarti Singh", "title": "Rate Optimal Estimation and Confidence Intervals for High-dimensional\n  Regression with Missing Covariates", "comments": "41 pages, 1 figure, 3 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Although a majority of the theoretical literature in high-dimensional\nstatistics has focused on settings which involve fully-observed data, settings\nwith missing values and corruptions are common in practice. We consider the\nproblems of estimation and of constructing component-wise confidence intervals\nin a sparse high-dimensional linear regression model when some covariates of\nthe design matrix are missing completely at random. We analyze a variant of the\nDantzig selector [9] for estimating the regression model and we use a\nde-biasing argument to construct component-wise confidence intervals. Our first\nmain result is to establish upper bounds on the estimation error as a function\nof the model parameters (the sparsity level s, the expected fraction of\nobserved covariates $\\rho_*$, and a measure of the signal strength\n$\\|\\beta^*\\|_2$). We find that even in an idealized setting where the\ncovariates are assumed to be missing completely at random, somewhat\nsurprisingly and in contrast to the fully-observed setting, there is a\ndichotomy in the dependence on model parameters and much faster rates are\nobtained if the covariance matrix of the random design is known. To study this\nissue further, our second main contribution is to provide lower bounds on the\nestimation error showing that this discrepancy in rates is unavoidable in a\nminimax sense. We then consider the problem of high-dimensional inference in\nthe presence of missing data. We construct and analyze confidence intervals\nusing a de-biased estimator. In the presence of missing data, inference is\ncomplicated by the fact that the de-biasing matrix is correlated with the pilot\nestimator and this necessitates the design of a new estimator and a novel\nanalysis. We also complement our mathematical study with extensive simulations\non synthetic and semi-synthetic data that show the accuracy of our asymptotic\npredictions for finite sample sizes.\n", "versions": [{"version": "v1", "created": "Thu, 9 Feb 2017 03:10:13 GMT"}, {"version": "v2", "created": "Fri, 3 Nov 2017 00:04:20 GMT"}], "update_date": "2017-11-06", "authors_parsed": [["Wang", "Yining", ""], ["Wang", "Jialei", ""], ["Balakrishnan", "Sivaraman", ""], ["Singh", "Aarti", ""]]}, {"id": "1702.02715", "submitter": "Yanjun  Qi Dr.", "authors": "Beilun Wang, Ji Gao, Yanjun Qi", "title": "A Fast and Scalable Joint Estimator for Learning Multiple Related Sparse\n  Gaussian Graphical Models", "comments": "8 pages, accepted by AISTAT 2017", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG cs.PF", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Estimating multiple sparse Gaussian Graphical Models (sGGMs) jointly for many\nrelated tasks (large $K$) under a high-dimensional (large $p$) situation is an\nimportant task. Most previous studies for the joint estimation of multiple\nsGGMs rely on penalized log-likelihood estimators that involve expensive and\ndifficult non-smooth optimizations. We propose a novel approach, FASJEM for\n\\underline{fa}st and \\underline{s}calable \\underline{j}oint\nstructure-\\underline{e}stimation of \\underline{m}ultiple sGGMs at a large\nscale. As the first study of joint sGGM using the Elementary Estimator\nframework, our work has three major contributions: (1) We solve FASJEM through\nan entry-wise manner which is parallelizable. (2) We choose a proximal\nalgorithm to optimize FASJEM. This improves the computational efficiency from\n$O(Kp^3)$ to $O(Kp^2)$ and reduces the memory requirement from $O(Kp^2)$ to\n$O(K)$. (3) We theoretically prove that FASJEM achieves a consistent estimation\nwith a convergence rate of $O(\\log(Kp)/n_{tot})$. On several synthetic and four\nreal-world datasets, FASJEM shows significant improvements over baselines on\naccuracy, computational complexity, and memory costs.\n", "versions": [{"version": "v1", "created": "Thu, 9 Feb 2017 06:09:48 GMT"}, {"version": "v2", "created": "Thu, 2 Mar 2017 06:49:21 GMT"}, {"version": "v3", "created": "Tue, 20 Mar 2018 10:44:16 GMT"}], "update_date": "2018-03-21", "authors_parsed": [["Wang", "Beilun", ""], ["Gao", "Ji", ""], ["Qi", "Yanjun", ""]]}, {"id": "1702.02741", "submitter": "Bukweon Kim", "authors": "Jaeseong Jang, Yejin Park, Bukweon Kim, Sung Min Lee, Ja-Young Kwon,\n  and Jin Keun Seo", "title": "Automatic Estimation of Fetal Abdominal Circumference from Ultrasound\n  Images", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Ultrasound diagnosis is routinely used in obstetrics and gynecology for fetal\nbiometry, and owing to its time-consuming process, there has been a great\ndemand for automatic estimation. However, the automated analysis of ultrasound\nimages is complicated because they are patient-specific, operator-dependent,\nand machine-specific. Among various types of fetal biometry, the accurate\nestimation of abdominal circumference (AC) is especially difficult to perform\nautomatically because the abdomen has low contrast against surroundings,\nnon-uniform contrast, and irregular shape compared to other parameters.We\npropose a method for the automatic estimation of the fetal AC from 2D\nultrasound data through a specially designed convolutional neural network\n(CNN), which takes account of doctors' decision process, anatomical structure,\nand the characteristics of the ultrasound image. The proposed method uses CNN\nto classify ultrasound images (stomach bubble, amniotic fluid, and umbilical\nvein) and Hough transformation for measuring AC. We test the proposed method\nusing clinical ultrasound data acquired from 56 pregnant women. Experimental\nresults show that, with relatively small training samples, the proposed CNN\nprovides sufficient classification results for AC estimation through the Hough\ntransformation. The proposed method automatically estimates AC from ultrasound\nimages. The method is quantitatively evaluated, and shows stable performance in\nmost cases and even for ultrasound images deteriorated by shadowing artifacts.\nAs a result of experiments for our acceptance check, the accuracies are 0.809\nand 0.771 with the expert 1 and expert 2, respectively, while the accuracy\nbetween the two experts is 0.905. However, for cases of oversized fetus, when\nthe amniotic fluid is not observed or the abdominal area is distorted, it could\nnot correctly estimate AC.\n", "versions": [{"version": "v1", "created": "Thu, 9 Feb 2017 08:18:32 GMT"}, {"version": "v2", "created": "Thu, 2 Nov 2017 05:59:49 GMT"}], "update_date": "2017-11-03", "authors_parsed": [["Jang", "Jaeseong", ""], ["Park", "Yejin", ""], ["Kim", "Bukweon", ""], ["Lee", "Sung Min", ""], ["Kwon", "Ja-Young", ""], ["Seo", "Jin Keun", ""]]}, {"id": "1702.02828", "submitter": "Jason Klusowski M", "authors": "Jason M. Klusowski and Andrew R. Barron", "title": "Minimax Lower Bounds for Ridge Combinations Including Neural Nets", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Estimation of functions of $ d $ variables is considered using ridge\ncombinations of the form $ \\textstyle\\sum_{k=1}^m c_{1,k}\n\\phi(\\textstyle\\sum_{j=1}^d c_{0,j,k}x_j-b_k) $ where the activation function $\n\\phi $ is a function with bounded value and derivative. These include\nsingle-hidden layer neural networks, polynomials, and sinusoidal models. From a\nsample of size $ n $ of possibly noisy values at random sites $ X \\in B =\n[-1,1]^d $, the minimax mean square error is examined for functions in the\nclosure of the $ \\ell_1 $ hull of ridge functions with activation $ \\phi $. It\nis shown to be of order $ d/n $ to a fractional power (when $ d $ is of smaller\norder than $ n $), and to be of order $ (\\log d)/n $ to a fractional power\n(when $ d $ is of larger order than $ n $). Dependence on constraints $ v_0 $\nand $ v_1 $ on the $ \\ell_1 $ norms of inner parameter $ c_0 $ and outer\nparameter $ c_1 $, respectively, is also examined. Also, lower and upper bounds\non the fractional power are given. The heart of the analysis is development of\ninformation-theoretic packing numbers for these classes of functions.\n", "versions": [{"version": "v1", "created": "Thu, 9 Feb 2017 13:34:21 GMT"}], "update_date": "2017-02-10", "authors_parsed": [["Klusowski", "Jason M.", ""], ["Barron", "Andrew R.", ""]]}, {"id": "1702.02849", "submitter": "Adish Singla", "authors": "Christoph Hirnschall, Adish Singla, Sebastian Tschiatschek, Andreas\n  Krause", "title": "Coordinated Online Learning With Applications to Learning User\n  Preferences", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study an online multi-task learning setting, in which instances of related\ntasks arrive sequentially, and are handled by task-specific online learners. We\nconsider an algorithmic framework to model the relationship of these tasks via\na set of convex constraints. To exploit this relationship, we design a novel\nalgorithm -- COOL -- for coordinating the individual online learners: Our key\nidea is to coordinate their parameters via weighted projections onto a convex\nset. By adjusting the rate and accuracy of the projection, the COOL algorithm\nallows for a trade-off between the benefit of coordination and the required\ncomputation/communication. We derive regret bounds for our approach and analyze\nhow they are influenced by these trade-off factors. We apply our results on the\napplication of learning users' preferences on the Airbnb marketplace with the\ngoal of incentivizing users to explore under-reviewed apartments.\n", "versions": [{"version": "v1", "created": "Thu, 9 Feb 2017 14:44:49 GMT"}], "update_date": "2017-02-10", "authors_parsed": [["Hirnschall", "Christoph", ""], ["Singla", "Adish", ""], ["Tschiatschek", "Sebastian", ""], ["Krause", "Andreas", ""]]}, {"id": "1702.02896", "submitter": "Stefan Wager", "authors": "Susan Athey and Stefan Wager", "title": "Policy Learning with Observational Data", "comments": "Forthcoming in Econometrica. Original title: Efficient Policy\n  Learning", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST cs.LG econ.EM stat.ML stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In many areas, practitioners seek to use observational data to learn a\ntreatment assignment policy that satisfies application-specific constraints,\nsuch as budget, fairness, simplicity, or other functional form constraints. For\nexample, policies may be restricted to take the form of decision trees based on\na limited set of easily observable individual characteristics. We propose a new\napproach to this problem motivated by the theory of semiparametrically\nefficient estimation. Our method can be used to optimize either binary\ntreatments or infinitesimal nudges to continuous treatments, and can leverage\nobservational data where causal effects are identified using a variety of\nstrategies, including selection on observables and instrumental variables.\nGiven a doubly robust estimator of the causal effect of assigning everyone to\ntreatment, we develop an algorithm for choosing whom to treat, and establish\nstrong guarantees for the asymptotic utilitarian regret of the resulting\npolicy.\n", "versions": [{"version": "v1", "created": "Thu, 9 Feb 2017 17:03:01 GMT"}, {"version": "v2", "created": "Sun, 1 Oct 2017 22:20:23 GMT"}, {"version": "v3", "created": "Tue, 9 Oct 2018 11:04:03 GMT"}, {"version": "v4", "created": "Tue, 4 Dec 2018 23:51:38 GMT"}, {"version": "v5", "created": "Mon, 16 Sep 2019 22:46:50 GMT"}, {"version": "v6", "created": "Fri, 4 Sep 2020 21:45:04 GMT"}], "update_date": "2020-09-08", "authors_parsed": [["Athey", "Susan", ""], ["Wager", "Stefan", ""]]}, {"id": "1702.02982", "submitter": "Danica J. Sutherland", "authors": "Danica J. Sutherland", "title": "Fixing an error in Caponnetto and de Vito (2007)", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The seminal paper of Caponnetto and de Vito (2007) provides minimax-optimal\nrates for kernel ridge regression in a very general setting. Its proof,\nhowever, contains an error in its bound on the effective dimensionality. In\nthis note, we explain the mistake, provide a correct bound, and show that the\nmain theorem remains true.\n", "versions": [{"version": "v1", "created": "Thu, 9 Feb 2017 21:01:52 GMT"}, {"version": "v2", "created": "Thu, 14 Jan 2021 06:10:33 GMT"}], "update_date": "2021-01-15", "authors_parsed": [["Sutherland", "Danica J.", ""]]}, {"id": "1702.03054", "submitter": "Hiroshi Shinaoka", "authors": "Hiroshi Shinaoka, Junya Otsuki, Masayuki Ohzeki, Kazuyoshi Yoshimi", "title": "Compressing Green's function using intermediate representation between\n  imaginary-time and real-frequency domains", "comments": "9 pages, 7 figures", "journal-ref": "Phys. Rev. B 96, 035147 (2017)", "doi": "10.1103/PhysRevB.96.035147", "report-no": null, "categories": "cond-mat.str-el cond-mat.stat-mech stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  New model-independent compact representations of imaginary-time data are\npresented in terms of the intermediate representation (IR) of analytical\ncontinuation. This is motivated by a recent numerical finding by the authors\n[J. Otsuki et al., arXiv:1702.03056]. We demonstrate the efficiency of the IR\nthrough continuous-time quantum Monte Carlo calculations of an Anderson\nimpurity model. We find that the IR yields a significantly compact form of\nvarious types of correlation functions. The present framework will provide\ngeneral ways to boost the power of cutting-edge diagrammatic/quantum Monte\nCarlo treatments of many-body systems.\n", "versions": [{"version": "v1", "created": "Fri, 10 Feb 2017 03:15:41 GMT"}, {"version": "v2", "created": "Fri, 14 Apr 2017 00:52:14 GMT"}, {"version": "v3", "created": "Thu, 20 Jul 2017 09:25:58 GMT"}], "update_date": "2017-07-26", "authors_parsed": [["Shinaoka", "Hiroshi", ""], ["Otsuki", "Junya", ""], ["Ohzeki", "Masayuki", ""], ["Yoshimi", "Kazuyoshi", ""]]}, {"id": "1702.03056", "submitter": "Junya Otsuki", "authors": "Junya Otsuki, Masayuki Ohzeki, Hiroshi Shinaoka, Kazuyoshi Yoshimi", "title": "Sparse modeling approach to analytical continuation of imaginary-time\n  quantum Monte Carlo data", "comments": "7 pages, 5 figures", "journal-ref": "Phys. Rev. E 95, 061302 (2017)", "doi": "10.1103/PhysRevE.95.061302", "report-no": null, "categories": "cond-mat.str-el cond-mat.stat-mech stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A new approach of solving the ill-conditioned inverse problem for analytical\ncontinuation is proposed. The root of the problem lies in the fact that even\ntiny noise of imaginary-time input data has a serious impact on the inferred\nreal-frequency spectra. By means of a modern regularization technique, we\neliminate redundant degrees of freedom that essentially carry the noise,\nleaving only relevant information unaffected by the noise. The resultant\nspectrum is represented with minimal bases and thus a stable analytical\ncontinuation is achieved. This framework further provides a tool for analyzing\nto what extent the Monte Carlo data need to be accurate to resolve details of\nan expected spectral function.\n", "versions": [{"version": "v1", "created": "Fri, 10 Feb 2017 03:19:43 GMT"}, {"version": "v2", "created": "Wed, 10 May 2017 00:59:25 GMT"}], "update_date": "2017-06-28", "authors_parsed": [["Otsuki", "Junya", ""], ["Ohzeki", "Masayuki", ""], ["Shinaoka", "Hiroshi", ""], ["Yoshimi", "Kazuyoshi", ""]]}, {"id": "1702.03070", "submitter": "Praneeth Narayanamurthy", "authors": "Namrata Vaswani and Praneeth Narayanamurthy", "title": "PCA in Data-Dependent Noise (Correlated-PCA): Nearly Optimal Finite\n  Sample Guarantees", "comments": "made significant changes to paper. The completed new version is\n  available at https://arxiv.org/abs/1709.06255", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IT math.IT stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study Principal Component Analysis (PCA) in a setting where a part of the\ncorrupting noise is data-dependent and, as a result, the noise and the true\ndata are correlated. Under a bounded-ness assumption on the true data and the\nnoise, and a simple assumption on data-noise correlation, we obtain a nearly\noptimal sample complexity bound for the most commonly used PCA solution,\nsingular value decomposition (SVD). This bound is a significant improvement\nover the bound obtained by Vaswani and Guo in recent work (NIPS 2016) where\nthis \"correlated-PCA\" problem was first studied; and it holds under a\nsignificantly weaker data-noise correlation assumption than the one used for\nthis earlier result.\n", "versions": [{"version": "v1", "created": "Fri, 10 Feb 2017 05:35:36 GMT"}, {"version": "v2", "created": "Wed, 24 May 2017 20:09:48 GMT"}, {"version": "v3", "created": "Tue, 31 Oct 2017 15:34:50 GMT"}], "update_date": "2017-11-01", "authors_parsed": [["Vaswani", "Namrata", ""], ["Narayanamurthy", "Praneeth", ""]]}, {"id": "1702.03121", "submitter": "Ashutosh Modi", "authors": "Ashutosh Modi, Ivan Titov, Vera Demberg, Asad Sayeed and Manfred\n  Pinkal", "title": "Modeling Semantic Expectation: Using Script Knowledge for Referent\n  Prediction", "comments": "14 pages, published at TACL, 2017, Volume-5, Pg 31-44, 2017", "journal-ref": "Transactions of ACL, Volume-5, Pg 31-44 (2017)", "doi": null, "report-no": null, "categories": "cs.CL cs.AI stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recent research in psycholinguistics has provided increasing evidence that\nhumans predict upcoming content. Prediction also affects perception and might\nbe a key to robustness in human language processing. In this paper, we\ninvestigate the factors that affect human prediction by building a\ncomputational model that can predict upcoming discourse referents based on\nlinguistic knowledge alone vs. linguistic knowledge jointly with common-sense\nknowledge in the form of scripts. We find that script knowledge significantly\nimproves model estimates of human predictions. In a second study, we test the\nhighly controversial hypothesis that predictability influences referring\nexpression type but do not find evidence for such an effect.\n", "versions": [{"version": "v1", "created": "Fri, 10 Feb 2017 10:31:57 GMT"}], "update_date": "2017-02-13", "authors_parsed": [["Modi", "Ashutosh", ""], ["Titov", "Ivan", ""], ["Demberg", "Vera", ""], ["Sayeed", "Asad", ""], ["Pinkal", "Manfred", ""]]}, {"id": "1702.03244", "submitter": "Martin Spindler", "authors": "Ye Luo and Martin Spindler", "title": "$L_2$Boosting for Economic Applications", "comments": "Submitted to American Economic Review, Papers and Proceedings 2017.\n  arXiv admin note: text overlap with arXiv:1602.08927", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML econ.EM stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In the recent years more and more high-dimensional data sets, where the\nnumber of parameters $p$ is high compared to the number of observations $n$ or\neven larger, are available for applied researchers. Boosting algorithms\nrepresent one of the major advances in machine learning and statistics in\nrecent years and are suitable for the analysis of such data sets. While Lasso\nhas been applied very successfully for high-dimensional data sets in Economics,\nboosting has been underutilized in this field, although it has been proven very\npowerful in fields like Biostatistics and Pattern Recognition. We attribute\nthis to missing theoretical results for boosting. The goal of this paper is to\nfill this gap and show that boosting is a competitive method for inference of a\ntreatment effect or instrumental variable (IV) estimation in a high-dimensional\nsetting. First, we present the $L_2$Boosting with componentwise least squares\nalgorithm and variants which are tailored for regression problems which are the\nworkhorse for most Econometric problems. Then we show how $L_2$Boosting can be\nused for estimation of treatment effects and IV estimation. We highlight the\nmethods and illustrate them with simulations and empirical examples. For\nfurther results and technical details we refer to Luo and Spindler (2016, 2017)\nand to the online supplement of the paper.\n", "versions": [{"version": "v1", "created": "Fri, 10 Feb 2017 16:35:06 GMT"}], "update_date": "2017-09-28", "authors_parsed": [["Luo", "Ye", ""], ["Spindler", "Martin", ""]]}, {"id": "1702.03260", "submitter": "Eric Tramel", "authors": "Eric W. Tramel and Marylou Gabri\\'e and Andre Manoel and Francesco\n  Caltagirone and Florent Krzakala", "title": "A Deterministic and Generalized Framework for Unsupervised Learning with\n  Restricted Boltzmann Machines", "comments": null, "journal-ref": "Phys. Rev. X 8, 041006 (2018)", "doi": "10.1103/PhysRevX.8.041006", "report-no": null, "categories": "cs.LG cond-mat.dis-nn cs.NE stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Restricted Boltzmann machines (RBMs) are energy-based neural-networks which\nare commonly used as the building blocks for deep architectures neural\narchitectures. In this work, we derive a deterministic framework for the\ntraining, evaluation, and use of RBMs based upon the Thouless-Anderson-Palmer\n(TAP) mean-field approximation of widely-connected systems with weak\ninteractions coming from spin-glass theory. While the TAP approach has been\nextensively studied for fully-visible binary spin systems, our construction is\ngeneralized to latent-variable models, as well as to arbitrarily distributed\nreal-valued spin systems with bounded support. In our numerical experiments, we\ndemonstrate the effective deterministic training of our proposed models and are\nable to show interesting features of unsupervised learning which could not be\ndirectly observed with sampling. Additionally, we demonstrate how to utilize\nour TAP-based framework for leveraging trained RBMs as joint priors in\ndenoising problems.\n", "versions": [{"version": "v1", "created": "Fri, 10 Feb 2017 17:29:51 GMT"}, {"version": "v2", "created": "Mon, 2 Oct 2017 13:55:46 GMT"}, {"version": "v3", "created": "Tue, 9 Oct 2018 09:07:13 GMT"}], "update_date": "2018-10-17", "authors_parsed": [["Tramel", "Eric W.", ""], ["Gabri\u00e9", "Marylou", ""], ["Manoel", "Andre", ""], ["Caltagirone", "Francesco", ""], ["Krzakala", "Florent", ""]]}, {"id": "1702.03307", "submitter": "Ershad Banijamali Mr.", "authors": "Ershad Banijamali, Ali Ghodsi, Pascal Poupart", "title": "Generative Mixture of Networks", "comments": "9 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A generative model based on training deep architectures is proposed. The\nmodel consists of K networks that are trained together to learn the underlying\ndistribution of a given data set. The process starts with dividing the input\ndata into K clusters and feeding each of them into a separate network. After\nfew iterations of training networks separately, we use an EM-like algorithm to\ntrain the networks together and update the clusters of the data. We call this\nmodel Mixture of Networks. The provided model is a platform that can be used\nfor any deep structure and be trained by any conventional objective function\nfor distribution modeling. As the components of the model are neural networks,\nit has high capability in characterizing complicated data distributions as well\nas clustering data. We apply the algorithm on MNIST hand-written digits and\nYale face datasets. We also demonstrate the clustering ability of the model\nusing some real-world and toy examples.\n", "versions": [{"version": "v1", "created": "Fri, 10 Feb 2017 19:21:02 GMT"}], "update_date": "2017-02-14", "authors_parsed": [["Banijamali", "Ershad", ""], ["Ghodsi", "Ali", ""], ["Poupart", "Pascal", ""]]}, {"id": "1702.03334", "submitter": "Kirthevasan Kandasamy", "authors": "Kirthevasan Kandasamy, Yoram Bachrach, Ryota Tomioka, Daniel Tarlow,\n  David Carter", "title": "Batch Policy Gradient Methods for Improving Neural Conversation Models", "comments": "International Conference on Learning Representations (ICLR) 2017", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study reinforcement learning of chatbots with recurrent neural network\narchitectures when the rewards are noisy and expensive to obtain. For instance,\na chatbot used in automated customer service support can be scored by quality\nassurance agents, but this process can be expensive, time consuming and noisy.\nPrevious reinforcement learning work for natural language processing uses\non-policy updates and/or is designed for on-line learning settings. We\ndemonstrate empirically that such strategies are not appropriate for this\nsetting and develop an off-policy batch policy gradient method (BPG). We\ndemonstrate the efficacy of our method via a series of synthetic experiments\nand an Amazon Mechanical Turk experiment on a restaurant recommendations\ndataset.\n", "versions": [{"version": "v1", "created": "Fri, 10 Feb 2017 21:58:40 GMT"}], "update_date": "2017-02-14", "authors_parsed": [["Kandasamy", "Kirthevasan", ""], ["Bachrach", "Yoram", ""], ["Tomioka", "Ryota", ""], ["Tarlow", "Daniel", ""], ["Carter", "David", ""]]}, {"id": "1702.03446", "submitter": "Yaniv Romano", "authors": "Dmitry Batenkov, Yaniv Romano, Michael Elad", "title": "On the Global-Local Dichotomy in Sparsity Modeling", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IT math.IT stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The traditional sparse modeling approach, when applied to inverse problems\nwith large data such as images, essentially assumes a sparse model for small\noverlapping data patches. While producing state-of-the-art results, this\nmethodology is suboptimal, as it does not attempt to model the entire global\nsignal in any meaningful way - a nontrivial task by itself. In this paper we\npropose a way to bridge this theoretical gap by constructing a global model\nfrom the bottom up. Given local sparsity assumptions in a dictionary, we show\nthat the global signal representation must satisfy a constrained\nunderdetermined system of linear equations, which can be solved efficiently by\nmodern optimization methods such as Alternating Direction Method of Multipliers\n(ADMM). We investigate conditions for unique and stable recovery, and provide\nnumerical evidence corroborating the theory.\n", "versions": [{"version": "v1", "created": "Sat, 11 Feb 2017 18:05:38 GMT"}], "update_date": "2017-02-14", "authors_parsed": [["Batenkov", "Dmitry", ""], ["Romano", "Yaniv", ""], ["Elad", "Michael", ""]]}, {"id": "1702.03464", "submitter": "Nicolas Garcia Trillos", "authors": "Nicolas Garcia Trillos", "title": "Gromov-Hausdorff limit of Wasserstein spaces on point clouds", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.MG math.AP math.PR math.ST stat.ML stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider a point cloud $X_n := \\{ x_1, \\dots, x_n \\}$ uniformly\ndistributed on the flat torus $\\mathbb{T}^d : = \\mathbb{R}^d / \\mathbb{Z}^d $,\nand construct a geometric graph on the cloud by connecting points that are\nwithin distance $\\varepsilon$ of each other. We let $\\mathcal{P}(X_n)$ be the\nspace of probability measures on $X_n$ and endow it with a discrete Wasserstein\ndistance $W_n$ as introduced independently by Chow et al, Maas, and Mielke for\ngeneral finite Markov chains. We show that as long as $\\varepsilon=\n\\varepsilon_n$ decays towards zero slower than an explicit rate depending on\nthe level of uniformity of $X_n$, then the space $(\\mathcal{P}(X_n), W_n)$\nconverges in the Gromov-Hausdorff sense towards the space of probability\nmeasures on $\\mathbb{T}^d$ endowed with the Wasserstein distance. The analysis\npresented in this paper is a first step in the study of stability of evolution\nequations defined over random point clouds as the number of points grows to\ninfinity.\n", "versions": [{"version": "v1", "created": "Sat, 11 Feb 2017 22:29:06 GMT"}, {"version": "v2", "created": "Thu, 13 Apr 2017 13:04:51 GMT"}, {"version": "v3", "created": "Fri, 24 May 2019 09:03:07 GMT"}], "update_date": "2019-05-27", "authors_parsed": [["Trillos", "Nicolas Garcia", ""]]}, {"id": "1702.03522", "submitter": "Muni Sreenivas Pydi", "authors": "Muni Sreenivas Pydi and Ambedkar Dukkipati", "title": "On Consistency of Compressive Spectral Clustering", "comments": "Accepted for publication at the 2018 IEEE International Symposium on\n  Information Theory (ISIT), Vail, Colorado, USA", "journal-ref": "ISIT 2018", "doi": null, "report-no": null, "categories": "stat.ML cs.IT cs.LG math.IT", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Spectral clustering is one of the most popular methods for community\ndetection in graphs. A key step in spectral clustering algorithms is the eigen\ndecomposition of the $n{\\times}n$ graph Laplacian matrix to extract its $k$\nleading eigenvectors, where $k$ is the desired number of clusters among $n$\nobjects. This is prohibitively complex to implement for very large datasets.\nHowever, it has recently been shown that it is possible to bypass the eigen\ndecomposition by computing an approximate spectral embedding through graph\nfiltering of random signals. In this paper, we analyze the working of spectral\nclustering performed via graph filtering on the stochastic block model.\nSpecifically, we characterize the effects of sparsity, dimensionality and\nfilter approximation error on the consistency of the algorithm in recovering\nplanted clusters.\n", "versions": [{"version": "v1", "created": "Sun, 12 Feb 2017 13:15:03 GMT"}, {"version": "v2", "created": "Sun, 5 Mar 2017 12:38:04 GMT"}, {"version": "v3", "created": "Tue, 29 May 2018 22:00:06 GMT"}], "update_date": "2018-09-10", "authors_parsed": [["Pydi", "Muni Sreenivas", ""], ["Dukkipati", "Ambedkar", ""]]}, {"id": "1702.03537", "submitter": "Ahmed Hefny", "authors": "Ahmed Hefny, Carlton Downey, Geoffrey J. Gordon", "title": "An Efficient, Expressive and Local Minima-free Method for Learning\n  Controlled Dynamical Systems", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a framework for modeling and estimating the state of controlled\ndynamical systems, where an agent can affect the system through actions and\nreceives partial observations. Based on this framework, we propose the\nPredictive State Representation with Random Fourier Features (RFFPSR). A key\nproperty in RFF-PSRs is that the state estimate is represented by a conditional\ndistribution of future observations given future actions. RFF-PSRs combine this\nrepresentation with moment-matching, kernel embedding and local optimization to\nachieve a method that enjoys several favorable qualities: It can represent\ncontrolled environments which can be affected by actions; it has an efficient\nand theoretically justified learning algorithm; it uses a non-parametric\nrepresentation that has expressive power to represent continuous non-linear\ndynamics. We provide a detailed formulation, a theoretical analysis and an\nexperimental evaluation that demonstrates the effectiveness of our method.\n", "versions": [{"version": "v1", "created": "Sun, 12 Feb 2017 16:13:29 GMT"}, {"version": "v2", "created": "Wed, 28 Feb 2018 19:58:03 GMT"}], "update_date": "2018-03-02", "authors_parsed": [["Hefny", "Ahmed", ""], ["Downey", "Carlton", ""], ["Gordon", "Geoffrey J.", ""]]}, {"id": "1702.03605", "submitter": "Mingda Qiao", "authors": "Lijie Chen, Jian Li, Mingda Qiao", "title": "Nearly Instance Optimal Sample Complexity Bounds for Top-k Arm Selection", "comments": "Accepted by AISTATS 2017", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.DS stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In the Best-$k$-Arm problem, we are given $n$ stochastic bandit arms, each\nassociated with an unknown reward distribution. We are required to identify the\n$k$ arms with the largest means by taking as few samples as possible. In this\npaper, we make progress towards a complete characterization of the\ninstance-wise sample complexity bounds for the Best-$k$-Arm problem. On the\nlower bound side, we obtain a novel complexity term to measure the sample\ncomplexity that every Best-$k$-Arm instance requires. This is derived by an\ninteresting and nontrivial reduction from the Best-$1$-Arm problem. We also\nprovide an elimination-based algorithm that matches the instance-wise lower\nbound within doubly-logarithmic factors. The sample complexity of our algorithm\nstrictly dominates the state-of-the-art for Best-$k$-Arm (module constant\nfactors).\n", "versions": [{"version": "v1", "created": "Mon, 13 Feb 2017 01:31:46 GMT"}], "update_date": "2017-02-15", "authors_parsed": [["Chen", "Lijie", ""], ["Li", "Jian", ""], ["Qiao", "Mingda", ""]]}, {"id": "1702.03614", "submitter": "Jie Chen", "authors": "Jie Chen, C\\'edric Richard, Ali H. Sayed", "title": "Multitask diffusion adaptation over networks with common latent\n  representations", "comments": "30 pages, 8 figures, IEEE Journal of Selected Topics in Signal\n  Processing 2017", "journal-ref": null, "doi": "10.1109/JSTSP.2017.2671789", "report-no": null, "categories": "cs.MA stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Online learning with streaming data in a distributed and collaborative manner\ncan be useful in a wide range of applications. This topic has been receiving\nconsiderable attention in recent years with emphasis on both single-task and\nmultitask scenarios. In single-task adaptation, agents cooperate to track an\nobjective of common interest, while in multitask adaptation agents track\nmultiple objectives simultaneously. Regularization is one useful technique to\npromote and exploit similarity among tasks in the latter scenario. This work\nexamines an alternative way to model relations among tasks by assuming that\nthey all share a common latent feature representation. As a result, a new\nmultitask learning formulation is presented and algorithms are developed for\nits solution in a distributed online manner. We present a unified framework to\nanalyze the mean-square-error performance of the adaptive strategies, and\nconduct simulations to illustrate the theoretical findings and potential\napplications.\n", "versions": [{"version": "v1", "created": "Mon, 13 Feb 2017 02:50:55 GMT"}], "update_date": "2017-04-26", "authors_parsed": [["Chen", "Jie", ""], ["Richard", "C\u00e9dric", ""], ["Sayed", "Ali H.", ""]]}, {"id": "1702.03713", "submitter": "Adam Gaier", "authors": "Adam Gaier, Alexander Asteroth, and Jean-Baptiste Mouret", "title": "Data-Efficient Exploration, Optimization, and Modeling of Diverse\n  Designs through Surrogate-Assisted Illumination", "comments": "Genetic and Evolutionary Computation Conference 2017", "journal-ref": null, "doi": "10.1145/3071178.3071282", "report-no": null, "categories": "cs.NE stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The MAP-Elites algorithm produces a set of high-performing solutions that\nvary according to features defined by the user. This technique has the\npotential to be a powerful tool for design space exploration, but is limited by\nthe need for numerous evaluations. The Surrogate-Assisted Illumination\nalgorithm (SAIL), introduced here, integrates approximative models and\nintelligent sampling of the objective function to minimize the number of\nevaluations required by MAP-Elites.\n  The ability of SAIL to efficiently produce both accurate models and diverse\nhigh performing solutions is illustrated on a 2D airfoil design problem. The\nsearch space is divided into bins, each holding a design with a different\ncombination of features. In each bin SAIL produces a better performing solution\nthan MAP-Elites, and requires several orders of magnitude fewer evaluations.\nThe CMA-ES algorithm was used to produce an optimal design in each bin: with\nthe same number of evaluations required by CMA-ES to find a near-optimal\nsolution in a single bin, SAIL finds solutions of similar quality in every bin.\n", "versions": [{"version": "v1", "created": "Mon, 13 Feb 2017 10:48:56 GMT"}, {"version": "v2", "created": "Mon, 31 Jul 2017 09:44:28 GMT"}], "update_date": "2017-08-01", "authors_parsed": [["Gaier", "Adam", ""], ["Asteroth", "Alexander", ""], ["Mouret", "Jean-Baptiste", ""]]}, {"id": "1702.03849", "submitter": "Maxim Raginsky", "authors": "Maxim Raginsky, Alexander Rakhlin, Matus Telgarsky", "title": "Non-convex learning via Stochastic Gradient Langevin Dynamics: a\n  nonasymptotic analysis", "comments": "29 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG math.OC math.PR stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Stochastic Gradient Langevin Dynamics (SGLD) is a popular variant of\nStochastic Gradient Descent, where properly scaled isotropic Gaussian noise is\nadded to an unbiased estimate of the gradient at each iteration. This modest\nchange allows SGLD to escape local minima and suffices to guarantee asymptotic\nconvergence to global minimizers for sufficiently regular non-convex objectives\n(Gelfand and Mitter, 1991). The present work provides a nonasymptotic analysis\nin the context of non-convex learning problems, giving finite-time guarantees\nfor SGLD to find approximate minimizers of both empirical and population risks.\nAs in the asymptotic setting, our analysis relates the discrete-time SGLD\nMarkov chain to a continuous-time diffusion process. A new tool that drives the\nresults is the use of weighted transportation cost inequalities to quantify the\nrate of convergence of SGLD to a stationary distribution in the Euclidean\n$2$-Wasserstein distance.\n", "versions": [{"version": "v1", "created": "Mon, 13 Feb 2017 16:11:38 GMT"}, {"version": "v2", "created": "Tue, 14 Feb 2017 23:51:43 GMT"}, {"version": "v3", "created": "Sun, 4 Jun 2017 04:26:02 GMT"}], "update_date": "2017-06-06", "authors_parsed": [["Raginsky", "Maxim", ""], ["Rakhlin", "Alexander", ""], ["Telgarsky", "Matus", ""]]}, {"id": "1702.03877", "submitter": "Eric Strobl", "authors": "Eric V. Strobl, Kun Zhang, Shyam Visweswaran", "title": "Approximate Kernel-based Conditional Independence Tests for Fast\n  Non-Parametric Causal Discovery", "comments": "R package: github.com/ericstrobl/RCIT", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Constraint-based causal discovery (CCD) algorithms require fast and accurate\nconditional independence (CI) testing. The Kernel Conditional Independence Test\n(KCIT) is currently one of the most popular CI tests in the non-parametric\nsetting, but many investigators cannot use KCIT with large datasets because the\ntest scales cubicly with sample size. We therefore devise two relaxations\ncalled the Randomized Conditional Independence Test (RCIT) and the Randomized\nconditional Correlation Test (RCoT) which both approximate KCIT by utilizing\nrandom Fourier features. In practice, both of the proposed tests scale linearly\nwith sample size and return accurate p-values much faster than KCIT in the\nlarge sample size context. CCD algorithms run with RCIT or RCoT also return\ngraphs at least as accurate as the same algorithms run with KCIT but with large\nreductions in run time.\n", "versions": [{"version": "v1", "created": "Mon, 13 Feb 2017 17:07:29 GMT"}, {"version": "v2", "created": "Thu, 13 Apr 2017 02:40:31 GMT"}], "update_date": "2017-04-14", "authors_parsed": [["Strobl", "Eric V.", ""], ["Zhang", "Kun", ""], ["Visweswaran", "Shyam", ""]]}, {"id": "1702.03994", "submitter": "Patrick Miller", "authors": "Patrick J. Miller, Daniel B. McArtor, Gitta H. Lubke", "title": "metboost: Exploratory regression analysis with hierarchically clustered\n  data", "comments": "10K words, 9 figures, 4 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  As data collections become larger, exploratory regression analysis becomes\nmore important but more challenging. When observations are hierarchically\nclustered the problem is even more challenging because model selection with\nmixed effect models can produce misleading results when nonlinear effects are\nnot included into the model (Bauer and Cai, 2009). A machine learning method\ncalled boosted decision trees (Friedman, 2001) is a good approach for\nexploratory regression analysis in real data sets because it can detect\npredictors with nonlinear and interaction effects while also accounting for\nmissing data. We propose an extension to boosted decision decision trees called\nmetboost for hierarchically clustered data. It works by constraining the\nstructure of each tree to be the same across groups, but allowing the terminal\nnode means to differ. This allows predictors and split points to lead to\ndifferent predictions within each group, and approximates nonlinear group\nspecific effects. Importantly, metboost remains computationally feasible for\nthousands of observations and hundreds of predictors that may contain missing\nvalues. We apply the method to predict math performance for 15,240 students\nfrom 751 schools in data collected in the Educational Longitudinal Study 2002\n(Ingels et al., 2007), allowing 76 predictors to have unique effects for each\nschool. When comparing results to boosted decision trees, metboost has 15%\nimproved prediction performance. Results of a large simulation study show that\nmetboost has up to 70% improved variable selection performance and up to 30%\nimproved prediction performance compared to boosted decision trees when group\nsizes are small\n", "versions": [{"version": "v1", "created": "Mon, 13 Feb 2017 21:47:56 GMT"}], "update_date": "2017-02-15", "authors_parsed": [["Miller", "Patrick J.", ""], ["McArtor", "Daniel B.", ""], ["Lubke", "Gitta H.", ""]]}, {"id": "1702.04008", "submitter": "Karen Ullrich", "authors": "Karen Ullrich, Edward Meeds, Max Welling", "title": "Soft Weight-Sharing for Neural Network Compression", "comments": "ICLR2017", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The success of deep learning in numerous application domains created the de-\nsire to run and train them on mobile devices. This however, conflicts with\ntheir computationally, memory and energy intense nature, leading to a growing\ninterest in compression. Recent work by Han et al. (2015a) propose a pipeline\nthat involves retraining, pruning and quantization of neural network weights,\nobtaining state-of-the-art compression rates. In this paper, we show that\ncompetitive compression rates can be achieved by using a version of soft\nweight-sharing (Nowlan & Hinton, 1992). Our method achieves both quantization\nand pruning in one simple (re-)training procedure. This point of view also\nexposes the relation between compression and the minimum description length\n(MDL) principle.\n", "versions": [{"version": "v1", "created": "Mon, 13 Feb 2017 22:54:18 GMT"}, {"version": "v2", "created": "Tue, 9 May 2017 14:05:43 GMT"}], "update_date": "2017-05-10", "authors_parsed": [["Ullrich", "Karen", ""], ["Meeds", "Edward", ""], ["Welling", "Max", ""]]}, {"id": "1702.04013", "submitter": "Piotr Szyma\\'nski", "authors": "Piotr Szyma\\'nski and Tomasz Kajdanowicz", "title": "Is a Data-Driven Approach still Better than Random Choice with Naive\n  Bayes classifiers?", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study the performance of data-driven, a priori and random approaches to\nlabel space partitioning for multi-label classification with a Gaussian Naive\nBayes classifier. Experiments were performed on 12 benchmark data sets and\nevaluated on 5 established measures of classification quality: micro and macro\naveraged F1 score, Subset Accuracy and Hamming loss. Data-driven methods are\nsignificantly better than an average run of the random baseline. In case of F1\nscores and Subset Accuracy - data driven approaches were more likely to perform\nbetter than random approaches than otherwise in the worst case. There always\nexists a method that performs better than a priori methods in the worst case.\nThe advantage of data-driven methods against a priori methods with a weak\nclassifier is lesser than when tree classifiers are used.\n", "versions": [{"version": "v1", "created": "Mon, 13 Feb 2017 23:04:31 GMT"}], "update_date": "2017-02-15", "authors_parsed": [["Szyma\u0144ski", "Piotr", ""], ["Kajdanowicz", "Tomasz", ""]]}, {"id": "1702.04018", "submitter": "Thomas Vandal", "authors": "Thomas Vandal, Evan Kodra, Auroop R Ganguly", "title": "Intercomparison of Machine Learning Methods for Statistical Downscaling:\n  The Case of Daily and Extreme Precipitation", "comments": "20 pages, 6 figures, 3 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Statistical downscaling of global climate models (GCMs) allows researchers to\nstudy local climate change effects decades into the future. A wide range of\nstatistical models have been applied to downscaling GCMs but recent advances in\nmachine learning have not been explored. In this paper, we compare four\nfundamental statistical methods, Bias Correction Spatial Disaggregation (BCSD),\nOrdinary Least Squares, Elastic-Net, and Support Vector Machine, with three\nmore advanced machine learning methods, Multi-task Sparse Structure Learning\n(MSSL), BCSD coupled with MSSL, and Convolutional Neural Networks to downscale\ndaily precipitation in the Northeast United States. Metrics to evaluate of each\nmethod's ability to capture daily anomalies, large scale climate shifts, and\nextremes are analyzed. We find that linear methods, led by BCSD, consistently\noutperform non-linear approaches. The direct application of state-of-the-art\nmachine learning methods to statistical downscaling does not provide\nimprovements over simpler, longstanding approaches.\n", "versions": [{"version": "v1", "created": "Mon, 13 Feb 2017 23:20:22 GMT"}], "update_date": "2017-02-15", "authors_parsed": [["Vandal", "Thomas", ""], ["Kodra", "Evan", ""], ["Ganguly", "Auroop R", ""]]}, {"id": "1702.04077", "submitter": "Rachelle Rivero", "authors": "Tsuyoshi Kato and Rachelle Rivero", "title": "Mutual Kernel Matrix Completion", "comments": "10 pages, 4 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.NA stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  With the huge influx of various data nowadays, extracting knowledge from them\nhas become an interesting but tedious task among data scientists, particularly\nwhen the data come in heterogeneous form and have missing information. Many\ndata completion techniques had been introduced, especially in the advent of\nkernel methods. However, among the many data completion techniques available in\nthe literature, studies about mutually completing several incomplete kernel\nmatrices have not been given much attention yet. In this paper, we present a\nnew method, called Mutual Kernel Matrix Completion (MKMC) algorithm, that\ntackles this problem of mutually inferring the missing entries of multiple\nkernel matrices by combining the notions of data fusion and kernel matrix\ncompletion, applied on biological data sets to be used for classification task.\nWe first introduced an objective function that will be minimized by exploiting\nthe EM algorithm, which in turn results to an estimate of the missing entries\nof the kernel matrices involved. The completed kernel matrices are then\ncombined to produce a model matrix that can be used to further improve the\nobtained estimates. An interesting result of our study is that the E-step and\nthe M-step are given in closed form, which makes our algorithm efficient in\nterms of time and memory. After completion, the (completed) kernel matrices are\nthen used to train an SVM classifier to test how well the relationships among\nthe entries are preserved. Our empirical results show that the proposed\nalgorithm bested the traditional completion techniques in preserving the\nrelationships among the data points, and in accurately recovering the missing\nkernel matrix entries. By far, MKMC offers a promising solution to the problem\nof mutual estimation of a number of relevant incomplete kernel matrices.\n", "versions": [{"version": "v1", "created": "Tue, 14 Feb 2017 04:30:03 GMT"}, {"version": "v2", "created": "Fri, 17 Feb 2017 05:16:10 GMT"}, {"version": "v3", "created": "Wed, 10 May 2017 09:59:32 GMT"}], "update_date": "2017-05-11", "authors_parsed": [["Kato", "Tsuyoshi", ""], ["Rivero", "Rachelle", ""]]}, {"id": "1702.04121", "submitter": "Carlton Macdonald Downey", "authors": "Carlton Downey, Ahmed Hefny, Geoffrey Gordon", "title": "Practical Learning of Predictive State Representations", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Over the past decade there has been considerable interest in spectral\nalgorithms for learning Predictive State Representations (PSRs). Spectral\nalgorithms have appealing theoretical guarantees; however, the resulting models\ndo not always perform well on inference tasks in practice. One reason for this\nbehavior is the mismatch between the intended task (accurate filtering or\nprediction) and the loss function being optimized by the algorithm (estimation\nerror in model parameters).\n  A natural idea is to improve performance by refining PSRs using an algorithm\nsuch as EM. Unfortunately it is not obvious how to apply apply an EM style\nalgorithm in the context of PSRs as the Log Likelihood is not well defined for\nall PSRs. We show that it is possible to overcome this problem using ideas from\nPredictive State Inference Machines.\n  We combine spectral algorithms for PSRs as a consistent and efficient\ninitialization with PSIM-style updates to refine the resulting model\nparameters. By combining these two ideas we develop Inference Gradients, a\nsimple, fast, and robust method for practical learning of PSRs. Inference\nGradients performs gradient descent in the PSR parameter space to optimize an\ninference-based loss function like PSIM. Because Inference Gradients uses a\nspectral initialization we get the same consistency benefits as PSRs. We show\nthat Inference Gradients outperforms both PSRs and PSIMs on real and synthetic\ndata sets.\n", "versions": [{"version": "v1", "created": "Tue, 14 Feb 2017 09:06:07 GMT"}], "update_date": "2017-02-15", "authors_parsed": [["Downey", "Carlton", ""], ["Hefny", "Ahmed", ""], ["Gordon", "Geoffrey", ""]]}, {"id": "1702.04126", "submitter": "Ian Osband", "authors": "Ian Osband and Benjamin Van Roy", "title": "Gaussian-Dirichlet Posterior Dominance in Sequential Learning", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG math.PR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider the problem of sequential learning from categorical observations\nbounded in [0,1]. We establish an ordering between the Dirichlet posterior over\ncategorical outcomes and a Gaussian posterior under observations with N(0,1)\nnoise. We establish that, conditioned upon identical data with at least two\nobservations, the posterior mean of the categorical distribution will always\nsecond-order stochastically dominate the posterior mean of the Gaussian\ndistribution. These results provide a useful tool for the analysis of\nsequential learning under categorical outcomes.\n", "versions": [{"version": "v1", "created": "Tue, 14 Feb 2017 09:23:18 GMT"}, {"version": "v2", "created": "Thu, 1 Feb 2018 19:11:56 GMT"}, {"version": "v3", "created": "Fri, 9 Feb 2018 09:06:59 GMT"}], "update_date": "2018-02-12", "authors_parsed": [["Osband", "Ian", ""], ["Van Roy", "Benjamin", ""]]}, {"id": "1702.04265", "submitter": "Cory Merkel", "authors": "Cory Merkel", "title": "Design of a Time Delay Reservoir Using Stochastic Logic: A Feasibility\n  Study", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.ET", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper presents a stochastic logic time delay reservoir design. The\nreservoir is analyzed using a number of metrics, such as kernel quality,\ngeneralization rank, performance on simple benchmarks, and is also compared to\na deterministic design. A novel re-seeding method is introduced to reduce the\nadverse effects of stochastic noise, which may also be implemented in other\nstochastic logic reservoir computing designs, such as echo state networks.\nBenchmark results indicate that the proposed design performs well on\nnoise-tolerant classification problems, but more work needs to be done to\nimprove the stochastic logic time delay reservoir's robustness for regression\nproblems.\n", "versions": [{"version": "v1", "created": "Mon, 13 Feb 2017 15:45:56 GMT"}], "update_date": "2017-02-15", "authors_parsed": [["Merkel", "Cory", ""]]}, {"id": "1702.04267", "submitter": "Jan Hendrik Metzen", "authors": "Jan Hendrik Metzen, Tim Genewein, Volker Fischer, Bastian Bischoff", "title": "On Detecting Adversarial Perturbations", "comments": "Final version for ICLR2017 (see\n  https://openreview.net/forum?id=SJzCSf9xg&noteId=SJzCSf9xg)", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.AI cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Machine learning and deep learning in particular has advanced tremendously on\nperceptual tasks in recent years. However, it remains vulnerable against\nadversarial perturbations of the input that have been crafted specifically to\nfool the system while being quasi-imperceptible to a human. In this work, we\npropose to augment deep neural networks with a small \"detector\" subnetwork\nwhich is trained on the binary classification task of distinguishing genuine\ndata from data containing adversarial perturbations. Our method is orthogonal\nto prior work on addressing adversarial perturbations, which has mostly focused\non making the classification network itself more robust. We show empirically\nthat adversarial perturbations can be detected surprisingly well even though\nthey are quasi-imperceptible to humans. Moreover, while the detectors have been\ntrained to detect only a specific adversary, they generalize to similar and\nweaker adversaries. In addition, we propose an adversarial attack that fools\nboth the classifier and the detector and a novel training procedure for the\ndetector that counteracts this attack.\n", "versions": [{"version": "v1", "created": "Tue, 14 Feb 2017 15:44:26 GMT"}, {"version": "v2", "created": "Tue, 21 Feb 2017 06:53:38 GMT"}], "update_date": "2017-02-22", "authors_parsed": [["Metzen", "Jan Hendrik", ""], ["Genewein", "Tim", ""], ["Fischer", "Volker", ""], ["Bischoff", "Bastian", ""]]}, {"id": "1702.04289", "submitter": "Sebastian Krause", "authors": "Martin Theissen, Sebastian M. Krause and Thomas Guhr", "title": "Regularities and Irregularities in Order Flow Data", "comments": "10 pages, 9 figures", "journal-ref": null, "doi": "10.1140/epjb/e2017-80087-6", "report-no": null, "categories": "q-fin.TR stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We identify and analyze statistical regularities and irregularities in the\nrecent order flow of different NASDAQ stocks, focusing on the positions where\norders are placed in the orderbook. This includes limit orders being placed\noutside of the spread, inside the spread and (effective) market orders. We find\nthat limit order placement inside the spread is strongly determined by the\ndynamics of the spread size. Most orders, however, arrive outside of the\nspread. While for some stocks order placement on or next to the quotes is\ndominating, deeper price levels are more important for other stocks. As market\norders are usually adjusted to the quote volume, the impact of market orders\ndepends on the orderbook structure, which we find to be quite diverse among the\nanalyzed stocks as a result of the way limit order placement takes place.\n", "versions": [{"version": "v1", "created": "Tue, 14 Feb 2017 16:59:20 GMT"}], "update_date": "2017-11-22", "authors_parsed": [["Theissen", "Martin", ""], ["Krause", "Sebastian M.", ""], ["Guhr", "Thomas", ""]]}, {"id": "1702.04407", "submitter": "Boris Hejblum", "authors": "Boris P. Hejblum, Chariff Alkhassim, Raphael Gottardo, Fran\\c{c}ois\n  Caron, and Rodolphe Thi\\'ebaut", "title": "Sequential Dirichlet Process Mixtures of Multivariate Skew\n  t-distributions for Model-based Clustering of Flow Cytometry Data", "comments": "39 pages, 11 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Flow cytometry is a high-throughput technology used to quantify multiple\nsurface and intracellular markers at the level of a single cell. This enables\nto identify cell sub-types, and to determine their relative proportions.\nImprovements of this technology allow to describe millions of individual cells\nfrom a blood sample using multiple markers. This results in high-dimensional\ndatasets, whose manual analysis is highly time-consuming and poorly\nreproducible. While several methods have been developed to perform automatic\nrecognition of cell populations, most of them treat and analyze each sample\nindependently. However, in practice, individual samples are rarely independent\n(e.g. longitudinal studies). Here, we propose to use a Bayesian nonparametric\napproach with Dirichlet process mixture (DPM) of multivariate skew\n$t$-distributions to perform model based clustering of flow-cytometry data. DPM\nmodels directly estimate the number of cell populations from the data, avoiding\nmodel selection issues, and skew $t$-distributions provides robustness to\noutliers and non-elliptical shape of cell populations. To accommodate repeated\nmeasurements, we propose a sequential strategy relying on a parametric\napproximation of the posterior. We illustrate the good performance of our\nmethod on simulated data, on an experimental benchmark dataset, and on new\nlongitudinal data from the DALIA-1 trial which evaluates a therapeutic vaccine\nagainst HIV. On the benchmark dataset, the sequential strategy outperforms all\nother methods evaluated, and similarly, leads to improved performance on the\nDALIA-1 data. We have made the method available for the community in the R\npackage NPflow.\n", "versions": [{"version": "v1", "created": "Tue, 14 Feb 2017 22:32:01 GMT"}, {"version": "v2", "created": "Fri, 17 Feb 2017 08:25:21 GMT"}, {"version": "v3", "created": "Sun, 9 Jul 2017 09:38:22 GMT"}, {"version": "v4", "created": "Mon, 11 Sep 2017 09:22:45 GMT"}], "update_date": "2017-09-12", "authors_parsed": [["Hejblum", "Boris P.", ""], ["Alkhassim", "Chariff", ""], ["Gottardo", "Raphael", ""], ["Caron", "Fran\u00e7ois", ""], ["Thi\u00e9baut", "Rodolphe", ""]]}, {"id": "1702.04415", "submitter": "Feng Mao", "authors": "Feng Mao, Edgar Blanco, Mingang Fu, Rohit Jain, Anurag Gupta,\n  Sebastien Mancel, Rong Yuan, Stephen Guo, Sai Kumar, Yayang Tian", "title": "Small Boxes Big Data: A Deep Learning Approach to Optimize Variable\n  Sized Bin Packing", "comments": "The Third IEEE International Conference on Big Data Computing Service\n  and Applications, 2017", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Bin Packing problems have been widely studied because of their broad\napplications in different domains. Known as a set of NP-hard problems, they\nhave different vari- ations and many heuristics have been proposed for\nobtaining approximate solutions. Specifically, for the 1D variable sized bin\npacking problem, the two key sets of optimization heuristics are the bin\nassignment and the bin allocation. Usually the performance of a single static\noptimization heuristic can not beat that of a dynamic one which is tailored for\neach bin packing instance. Building such an adaptive system requires modeling\nthe relationship between bin features and packing perform profiles. The primary\ndrawbacks of traditional AI machine learnings for this task are the natural\nlimitations of feature engineering, such as the curse of dimensionality and\nfeature selection quality. We introduce a deep learning approach to overcome\nthe drawbacks by applying a large training data set, auto feature selection and\nfast, accurate labeling. We show in this paper how to build such a system by\nboth theoretical formulation and engineering practices. Our prediction system\nachieves up to 89% training accuracy and 72% validation accuracy to select the\nbest heuristic that can generate a better quality bin packing solution.\n", "versions": [{"version": "v1", "created": "Tue, 14 Feb 2017 22:59:32 GMT"}], "update_date": "2017-02-16", "authors_parsed": [["Mao", "Feng", ""], ["Blanco", "Edgar", ""], ["Fu", "Mingang", ""], ["Jain", "Rohit", ""], ["Gupta", "Anurag", ""], ["Mancel", "Sebastien", ""], ["Yuan", "Rong", ""], ["Guo", "Stephen", ""], ["Kumar", "Sai", ""], ["Tian", "Yayang", ""]]}, {"id": "1702.04459", "submitter": "Dianhui Wang", "authors": "Dianhui Wang, Ming Li", "title": "Robust Stochastic Configuration Networks with Kernel Density Estimation", "comments": "14 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NE cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Neural networks have been widely used as predictive models to fit data\ndistribution, and they could be implemented through learning a collection of\nsamples. In many applications, however, the given dataset may contain noisy\nsamples or outliers which may result in a poor learner model in terms of\ngeneralization. This paper contributes to a development of robust stochastic\nconfiguration networks (RSCNs) for resolving uncertain data regression\nproblems. RSCNs are built on original stochastic configuration networks with\nweighted least squares method for evaluating the output weights, and the input\nweights and biases are incrementally and randomly generated by satisfying with\na set of inequality constrains. The kernel density estimation (KDE) method is\nemployed to set the penalty weights for each training samples, so that some\nnegative impacts, caused by noisy data or outliers, on the resulting learner\nmodel can be reduced. The alternating optimization technique is applied for\nupdating a RSCN model with improved penalty weights computed from the kernel\ndensity estimation function. Performance evaluation is carried out by a\nfunction approximation, four benchmark datasets and a case study on engineering\napplication. Comparisons to other robust randomised neural modelling\ntechniques, including the probabilistic robust learning algorithm for neural\nnetworks with random weights and improved RVFL networks, indicate that the\nproposed RSCNs with KDE perform favourably and demonstrate good potential for\nreal-world applications.\n", "versions": [{"version": "v1", "created": "Wed, 15 Feb 2017 03:54:29 GMT"}, {"version": "v2", "created": "Mon, 29 May 2017 15:29:47 GMT"}], "update_date": "2017-05-30", "authors_parsed": [["Wang", "Dianhui", ""], ["Li", "Ming", ""]]}, {"id": "1702.04561", "submitter": "Janek Thomas", "authors": "Janek Thomas and Tobias Hepp and Andreas Mayr and Bernd Bischl", "title": "Probing for sparse and fast variable selection with model-based boosting", "comments": "14 pages, 2 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a new variable selection method based on model-based gradient\nboosting and randomly permuted variables. Model-based boosting is a tool to fit\na statistical model while performing variable selection at the same time. A\ndrawback of the fitting lies in the need of multiple model fits on slightly\naltered data (e.g. cross-validation or bootstrap) to find the optimal number of\nboosting iterations and prevent overfitting. In our proposed approach, we\naugment the data set with randomly permuted versions of the true variables, so\ncalled shadow variables, and stop the step-wise fitting as soon as such a\nvariable would be added to the model. This allows variable selection in a\nsingle fit of the model without requiring further parameter tuning. We show\nthat our probing approach can compete with state-of-the-art selection methods\nlike stability selection in a high-dimensional classification benchmark and\napply it on gene expression data for the estimation of riboflavin production of\nBacillus subtilis.\n", "versions": [{"version": "v1", "created": "Wed, 15 Feb 2017 11:52:14 GMT"}], "update_date": "2017-02-16", "authors_parsed": [["Thomas", "Janek", ""], ["Hepp", "Tobias", ""], ["Mayr", "Andreas", ""], ["Bischl", "Bernd", ""]]}, {"id": "1702.04649", "submitter": "Greg Wayne", "authors": "Mevlana Gemici, Chia-Chun Hung, Adam Santoro, Greg Wayne, Shakir\n  Mohamed, Danilo J. Rezende, David Amos, Timothy Lillicrap", "title": "Generative Temporal Models with Memory", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.NE stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider the general problem of modeling temporal data with long-range\ndependencies, wherein new observations are fully or partially predictable based\non temporally-distant, past observations. A sufficiently powerful temporal\nmodel should separate predictable elements of the sequence from unpredictable\nelements, express uncertainty about those unpredictable elements, and rapidly\nidentify novel elements that may help to predict the future. To create such\nmodels, we introduce Generative Temporal Models augmented with external memory\nsystems. They are developed within the variational inference framework, which\nprovides both a practical training methodology and methods to gain insight into\nthe models' operation. We show, on a range of problems with sparse, long-term\ntemporal dependencies, that these models store information from early in a\nsequence, and reuse this stored information efficiently. This allows them to\nperform substantially better than existing models based on well-known recurrent\nneural networks, like LSTMs.\n", "versions": [{"version": "v1", "created": "Wed, 15 Feb 2017 15:19:02 GMT"}, {"version": "v2", "created": "Tue, 21 Feb 2017 10:14:52 GMT"}], "update_date": "2017-02-22", "authors_parsed": [["Gemici", "Mevlana", ""], ["Hung", "Chia-Chun", ""], ["Santoro", "Adam", ""], ["Wayne", "Greg", ""], ["Mohamed", "Shakir", ""], ["Rezende", "Danilo J.", ""], ["Amos", "David", ""], ["Lillicrap", "Timothy", ""]]}, {"id": "1702.04656", "submitter": "Chao Gao", "authors": "Chao Gao", "title": "Robust Regression via Mutivariate Regression Depth", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.ML stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper studies robust regression in the settings of Huber's\n$\\epsilon$-contamination models. We consider estimators that are maximizers of\nmultivariate regression depth functions. These estimators are shown to achieve\nminimax rates in the settings of $\\epsilon$-contamination models for various\nregression problems including nonparametric regression, sparse linear\nregression, reduced rank regression, etc. We also discuss a general notion of\ndepth function for linear operators that has potential applications in robust\nfunctional linear regression.\n", "versions": [{"version": "v1", "created": "Wed, 15 Feb 2017 15:48:30 GMT"}], "update_date": "2017-02-16", "authors_parsed": [["Gao", "Chao", ""]]}, {"id": "1702.04684", "submitter": "Hyukjun Gweon", "authors": "Hyukjun Gweon and Matthias Schonlau and Stefan Steiner", "title": "Nearest Labelset Using Double Distances for Multi-label Classification", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Multi-label classification is a type of supervised learning where an instance\nmay belong to multiple labels simultaneously. Predicting each label\nindependently has been criticized for not exploiting any correlation between\nlabels. In this paper we propose a novel approach, Nearest Labelset using\nDouble Distances (NLDD), that predicts the labelset observed in the training\ndata that minimizes a weighted sum of the distances in both the feature space\nand the label space to the new instance. The weights specify the relative\ntradeoff between the two distances. The weights are estimated from a binomial\nregression of the number of misclassified labels as a function of the two\ndistances. Model parameters are estimated by maximum likelihood. NLDD only\nconsiders labelsets observed in the training data, thus implicitly taking into\naccount label dependencies. Experiments on benchmark multi-label data sets show\nthat the proposed method on average outperforms other well-known approaches in\nterms of Hamming loss, 0/1 loss, and multi-label accuracy and ranks second\nafter ECC on the F-measure.\n", "versions": [{"version": "v1", "created": "Wed, 15 Feb 2017 17:01:08 GMT"}], "update_date": "2017-02-16", "authors_parsed": [["Gweon", "Hyukjun", ""], ["Schonlau", "Matthias", ""], ["Steiner", "Stefan", ""]]}, {"id": "1702.04690", "submitter": "Jongbin Jung", "authors": "Jongbin Jung, Connor Concannon, Ravi Shroff, Sharad Goel, Daniel G.\n  Goldstein", "title": "Simple rules for complex decisions", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  From doctors diagnosing patients to judges setting bail, experts often base\ntheir decisions on experience and intuition rather than on statistical models.\nWhile understandable, relying on intuition over models has often been found to\nresult in inferior outcomes. Here we present a new method,\nselect-regress-and-round, for constructing simple rules that perform well for\ncomplex decisions. These rules take the form of a weighted checklist, can be\napplied mentally, and nonetheless rival the performance of modern machine\nlearning algorithms. Our method for creating these rules is itself simple, and\ncan be carried out by practitioners with basic statistics knowledge. We\ndemonstrate this technique with a detailed case study of judicial decisions to\nrelease or detain defendants while they await trial. In this application, as in\nmany policy settings, the effects of proposed decision rules cannot be directly\nobserved from historical data: if a rule recommends releasing a defendant that\nthe judge in reality detained, we do not observe what would have happened under\nthe proposed action. We address this key counterfactual estimation problem by\ndrawing on tools from causal inference. We find that simple rules significantly\noutperform judges and are on par with decisions derived from random forests\ntrained on all available features. Generalizing to 22 varied decision-making\ndomains, we find this basic result replicates. We conclude with an analytical\nframework that helps explain why these simple decision rules perform as well as\nthey do.\n", "versions": [{"version": "v1", "created": "Wed, 15 Feb 2017 17:33:37 GMT"}, {"version": "v2", "created": "Thu, 16 Feb 2017 21:06:31 GMT"}, {"version": "v3", "created": "Sun, 2 Apr 2017 22:37:29 GMT"}], "update_date": "2017-04-04", "authors_parsed": [["Jung", "Jongbin", ""], ["Concannon", "Connor", ""], ["Shroff", "Ravi", ""], ["Goel", "Sharad", ""], ["Goldstein", "Daniel G.", ""]]}, {"id": "1702.04775", "submitter": "Matthew Wheeler", "authors": "Matthew W. Wheeler", "title": "Bayesian Additive Adaptive Basis Tensor Product Models for Modeling High\n  Dimensional Surfaces: An application to high-throughput toxicity testing", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Many modern data sets are sampled with error from complex high-dimensional\nsurfaces. Methods such as tensor product splines or Gaussian processes are\neffective/well suited for characterizing a surface in two or three dimensions\nbut may suffer from difficulties when representing higher dimensional surfaces.\nMotivated by high throughput toxicity testing where observed dose-response\ncurves are cross sections of a surface defined by a chemical's structural\nproperties, a model is developed to characterize this surface to predict\nuntested chemicals' dose-responses. This manuscript proposes a novel approach\nthat models the multidimensional surface as a sum of learned basis functions\nformed as the tensor product of lower dimensional functions, which are\nthemselves representable by a basis expansion learned from the data. The model\nis described, a Gibbs sampling algorithm proposed, and is investigated in a\nsimulation study as well as data taken from the US EPA's ToxCast high\nthroughput toxicity testing platform.\n", "versions": [{"version": "v1", "created": "Wed, 15 Feb 2017 21:11:36 GMT"}, {"version": "v2", "created": "Thu, 15 Jun 2017 14:26:39 GMT"}], "update_date": "2017-06-16", "authors_parsed": [["Wheeler", "Matthew W.", ""]]}, {"id": "1702.04782", "submitter": "Zachary Lipton", "authors": "Zachary C. Lipton, Subarna Tripathi", "title": "Precise Recovery of Latent Vectors from Generative Adversarial Networks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.NE stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Generative adversarial networks (GANs) transform latent vectors into visually\nplausible images. It is generally thought that the original GAN formulation\ngives no out-of-the-box method to reverse the mapping, projecting images back\ninto latent space. We introduce a simple, gradient-based technique called\nstochastic clipping. In experiments, for images generated by the GAN, we\nprecisely recover their latent vector pre-images 100% of the time. Additional\nexperiments demonstrate that this method is robust to noise. Finally, we show\nthat even for unseen images, our method appears to recover unique encodings.\n", "versions": [{"version": "v1", "created": "Wed, 15 Feb 2017 21:26:21 GMT"}, {"version": "v2", "created": "Fri, 17 Feb 2017 01:56:36 GMT"}], "update_date": "2017-02-20", "authors_parsed": [["Lipton", "Zachary C.", ""], ["Tripathi", "Subarna", ""]]}, {"id": "1702.04832", "submitter": "Marc Goessling", "authors": "Marc Goessling, Yali Amit", "title": "Dynamic Partition Models", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a new approach for learning compact and intuitive distributed\nrepresentations with binary encoding. Rather than summing up expert votes as in\nproducts of experts, we employ for each variable the opinion of the most\nreliable expert. Data points are hence explained through a partitioning of the\nvariables into expert supports. The partitions are dynamically adapted based on\nwhich experts are active. During the learning phase we adopt a smoothed version\nof this model that uses separate mixtures for each data dimension. In our\nexperiments we achieve accurate reconstructions of high-dimensional data points\nwith at most a dozen experts.\n", "versions": [{"version": "v1", "created": "Thu, 16 Feb 2017 01:07:17 GMT"}], "update_date": "2017-02-17", "authors_parsed": [["Goessling", "Marc", ""], ["Amit", "Yali", ""]]}, {"id": "1702.04837", "submitter": "Shusen Wang", "authors": "Shusen Wang and Alex Gittens and Michael W. Mahoney", "title": "Sketched Ridge Regression: Optimization Perspective, Statistical\n  Perspective, and Model Averaging", "comments": "To appear in Journal of Machine Learning Research, 2018. A short\n  version has appeared in International Conference on Machine Learning (ICML),\n  2017", "journal-ref": "Journal of Machine Learning Research, 19, pp1-50, 2018", "doi": null, "report-no": null, "categories": "stat.ML cs.LG cs.NA", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We address the statistical and optimization impacts of the classical sketch\nand Hessian sketch used to approximately solve the Matrix Ridge Regression\n(MRR) problem. Prior research has quantified the effects of classical sketch on\nthe strictly simpler least squares regression (LSR) problem. We establish that\nclassical sketch has a similar effect upon the optimization properties of MRR\nas it does on those of LSR: namely, it recovers nearly optimal solutions. By\ncontrast, Hessian sketch does not have this guarantee, instead, the\napproximation error is governed by a subtle interplay between the \"mass\" in the\nresponses and the optimal objective value.\n  For both types of approximation, the regularization in the sketched MRR\nproblem results in significantly different statistical properties from those of\nthe sketched LSR problem. In particular, there is a bias-variance trade-off in\nsketched MRR that is not present in sketched LSR. We provide upper and lower\nbounds on the bias and variance of sketched MRR, these bounds show that\nclassical sketch significantly increases the variance, while Hessian sketch\nsignificantly increases the bias. Empirically, sketched MRR solutions can have\nrisks that are higher by an order-of-magnitude than those of the optimal MRR\nsolutions.\n  We establish theoretically and empirically that model averaging greatly\ndecreases the gap between the risks of the true and sketched solutions to the\nMRR problem. Thus, in parallel or distributed settings, sketching combined with\nmodel averaging is a powerful technique that quickly obtains near-optimal\nsolutions to the MRR problem while greatly mitigating the increased statistical\nrisk incurred by sketching.\n", "versions": [{"version": "v1", "created": "Thu, 16 Feb 2017 02:01:26 GMT"}, {"version": "v2", "created": "Sat, 25 Feb 2017 02:59:41 GMT"}, {"version": "v3", "created": "Sat, 10 Jun 2017 17:52:18 GMT"}, {"version": "v4", "created": "Sat, 5 May 2018 20:58:25 GMT"}], "update_date": "2018-05-08", "authors_parsed": [["Wang", "Shusen", ""], ["Gittens", "Alex", ""], ["Mahoney", "Michael W.", ""]]}, {"id": "1702.04956", "submitter": "Aaron Gerow", "authors": "Aaron Gerow, Mingyang Zhou, Stan Matwin, Feng Shi", "title": "Reflexive Regular Equivalence for Bipartite Data", "comments": "A condensed version of this paper will appear in Proceedings of the\n  30th Canadian Conference on Artificial Intelligence, Edmonton, Alberta,\n  Canada", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Bipartite data is common in data engineering and brings unique challenges,\nparticularly when it comes to clustering tasks that impose on strong structural\nassumptions. This work presents an unsupervised method for assessing similarity\nin bipartite data. Similar to some co-clustering methods, the method is based\non regular equivalence in graphs. The algorithm uses spectral properties of a\nbipartite adjacency matrix to estimate similarity in both dimensions. The\nmethod is reflexive in that similarity in one dimension is used to inform\nsimilarity in the other. Reflexive regular equivalence can also use the\nstructure of transitivities -- in a network sense -- the contribution of which\nis controlled by the algorithm's only free-parameter, $\\alpha$. The method is\ncompletely unsupervised and can be used to validate assumptions of\nco-similarity, which are required but often untested, in co-clustering\nanalyses. Three variants of the method with different normalizations are tested\non synthetic data. The method is found to be robust to noise and well-suited to\nasymmetric co-similar structure, making it particularly informative for cluster\nanalysis and recommendation in bipartite data of unknown structure. In\nexperiments, the convergence and speed of the algorithm are found to be stable\nfor different levels of noise. Real-world data from a network of malaria genes\nare analyzed, where the similarity produced by the reflexive method is shown to\nout-perform other measures' ability to correctly classify genes.\n", "versions": [{"version": "v1", "created": "Thu, 16 Feb 2017 13:29:30 GMT"}], "update_date": "2017-02-17", "authors_parsed": [["Gerow", "Aaron", ""], ["Zhou", "Mingyang", ""], ["Matwin", "Stan", ""], ["Shi", "Feng", ""]]}, {"id": "1702.05008", "submitter": "Malte Nalenz", "authors": "Malte Nalenz and Mattias Villani", "title": "Tree Ensembles with Rule Structured Horseshoe Regularization", "comments": "24 pages. R package", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a new Bayesian model for flexible nonlinear regression and\nclassification using tree ensembles. The model is based on the RuleFit approach\nin Friedman and Popescu (2008) where rules from decision trees and linear terms\nare used in a L1-regularized regression. We modify RuleFit by replacing the\nL1-regularization by a horseshoe prior, which is well known to give aggressive\nshrinkage of noise predictor while leaving the important signal essentially\nuntouched. This is especially important when a large number of rules are used\nas predictors as many of them only contribute noise. Our horseshoe prior has an\nadditional hierarchical layer that applies more shrinkage a priori to rules\nwith a large number of splits, and to rules that are only satisfied by a few\nobservations. The aggressive noise shrinkage of our prior also makes it\npossible to complement the rules from boosting in Friedman and Popescu (2008)\nwith an additional set of trees from random forest, which brings a desirable\ndiversity to the ensemble. We sample from the posterior distribution using a\nvery efficient and easily implemented Gibbs sampler. The new model is shown to\noutperform state-of-the-art methods like RuleFit, BART and random forest on 16\ndatasets. The model and its interpretation is demonstrated on the well known\nBoston housing data, and on gene expression data for cancer classification. The\nposterior sampling, prediction and graphical tools for interpreting the model\nresults are implemented in a publicly available R package.\n", "versions": [{"version": "v1", "created": "Thu, 16 Feb 2017 15:16:27 GMT"}, {"version": "v2", "created": "Thu, 15 Feb 2018 14:47:21 GMT"}], "update_date": "2018-02-16", "authors_parsed": [["Nalenz", "Malte", ""], ["Villani", "Mattias", ""]]}, {"id": "1702.05037", "submitter": "Veeranjaneyulu Sadhanala", "authors": "Veeranjaneyulu Sadhanala and Ryan J. Tibshirani", "title": "Additive Models with Trend Filtering", "comments": "63 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study additive models built with trend filtering, i.e., additive models\nwhose components are each regularized by the (discrete) total variation of\ntheir $k$th (discrete) derivative, for a chosen integer $k \\geq 0$. This\nresults in $k$th degree piecewise polynomial components, (e.g., $k=0$ gives\npiecewise constant components, $k=1$ gives piecewise linear, $k=2$ gives\npiecewise quadratic, etc.). Analogous to its advantages in the univariate case,\nadditive trend filtering has favorable theoretical and computational\nproperties, thanks in large part to the localized nature of the (discrete)\ntotal variation regularizer that it uses. On the theory side, we derive fast\nerror rates for additive trend filtering estimates, and show these rates are\nminimax optimal when the underlying function is additive and has component\nfunctions whose derivatives are of bounded variation. We also show that these\nrates are unattainable by additive smoothing splines (and by additive models\nbuilt from linear smoothers, in general). On the computational side, as per the\nstandard in additive models, backfitting is an appealing method for\noptimization, but it is particularly appealing for additive trend filtering\nbecause we can leverage a few highly efficient univariate trend filtering\nsolvers. Going one step further, we describe a new backfitting algorithm whose\niterations can be run in parallel, which (as far as we know) is the first of\nits kind. Lastly, we present experiments to examine the empirical performance\nof additive trend filtering.\n", "versions": [{"version": "v1", "created": "Thu, 16 Feb 2017 16:19:56 GMT"}, {"version": "v2", "created": "Fri, 24 Mar 2017 05:06:18 GMT"}, {"version": "v3", "created": "Sat, 28 Apr 2018 09:10:41 GMT"}, {"version": "v4", "created": "Thu, 22 Nov 2018 03:16:44 GMT"}], "update_date": "2018-11-26", "authors_parsed": [["Sadhanala", "Veeranjaneyulu", ""], ["Tibshirani", "Ryan J.", ""]]}, {"id": "1702.05056", "submitter": "Yunbo Ouyang", "authors": "Yunbo Ouyang, Feng Liang", "title": "An Empirical Bayes Approach for High Dimensional Classification", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose an empirical Bayes estimator based on Dirichlet process mixture\nmodel for estimating the sparse normalized mean difference, which could be\ndirectly applied to the high dimensional linear classification. In theory, we\nbuild a bridge to connect the estimation error of the mean difference and the\nmisclassification error, also provide sufficient conditions of sub-optimal\nclassifiers and optimal classifiers. In implementation, a variational Bayes\nalgorithm is developed to compute the posterior efficiently and could be\nparallelized to deal with the ultra-high dimensional case.\n", "versions": [{"version": "v1", "created": "Thu, 16 Feb 2017 17:11:01 GMT"}], "update_date": "2017-02-17", "authors_parsed": [["Ouyang", "Yunbo", ""], ["Liang", "Feng", ""]]}, {"id": "1702.05063", "submitter": "Adrien Saumard", "authors": "Adrien Saumard", "title": "A concentration inequality for the excess risk in least-squares\n  regression with random design and heteroscedastic noise", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.ML stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We prove a new and general concentration inequality for the excess risk in\nleast-squares regression with random design and heteroscedastic noise. No\nspecific structure is required on the model, except the existence of a suitable\nfunction that controls the local suprema of the empirical process. So far, only\nthe case of linear contrast estimation was tackled in the literature with this\nlevel of generality on the model. We solve here the case of a quadratic\ncontrast, by separating the behavior of a linearized empirical process and the\nempirical process driven by the squares of functions of models.\n", "versions": [{"version": "v1", "created": "Thu, 16 Feb 2017 17:35:06 GMT"}, {"version": "v2", "created": "Fri, 9 Mar 2018 14:25:00 GMT"}], "update_date": "2018-03-12", "authors_parsed": [["Saumard", "Adrien", ""]]}, {"id": "1702.05137", "submitter": "Jie Yang", "authors": "Jie Yang, Sergey Shebalov, Diego Klabjan", "title": "Semi-supervised Learning for Discrete Choice Models", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We introduce a semi-supervised discrete choice model to calibrate discrete\nchoice models when relatively few requests have both choice sets and stated\npreferences but the majority only have the choice sets. Two classic\nsemi-supervised learning algorithms, the expectation maximization algorithm and\nthe cluster-and-label algorithm, have been adapted to our choice modeling\nproblem setting. We also develop two new algorithms based on the\ncluster-and-label algorithm. The new algorithms use the Bayesian Information\nCriterion to evaluate a clustering setting to automatically adjust the number\nof clusters. Two computational studies including a hotel booking case and a\nlarge-scale airline itinerary shopping case are presented to evaluate the\nprediction accuracy and computational effort of the proposed algorithms.\nAlgorithmic recommendations are rendered under various scenarios.\n", "versions": [{"version": "v1", "created": "Thu, 16 Feb 2017 19:59:40 GMT"}], "update_date": "2017-02-20", "authors_parsed": [["Yang", "Jie", ""], ["Shebalov", "Sergey", ""], ["Klabjan", "Diego", ""]]}, {"id": "1702.05148", "submitter": "Elizabeth Hou", "authors": "Elizabeth Hou, Kumar Sricharan, and Alfred O. Hero", "title": "Latent Laplacian Maximum Entropy Discrimination for Detection of\n  High-Utility Anomalies", "comments": null, "journal-ref": "IEEE Transactions on Information Forensics and Security ( Volume:\n  13, Issue: 6, June 2018 )", "doi": null, "report-no": null, "categories": "stat.ML cs.CR cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Data-driven anomaly detection methods suffer from the drawback of detecting\nall instances that are statistically rare, irrespective of whether the detected\ninstances have real-world significance or not. In this paper, we are interested\nin the problem of specifically detecting anomalous instances that are known to\nhave high real-world utility, while ignoring the low-utility statistically\nanomalous instances. To this end, we propose a novel method called Latent\nLaplacian Maximum Entropy Discrimination (LatLapMED) as a potential solution.\nThis method uses the EM algorithm to simultaneously incorporate the Geometric\nEntropy Minimization principle for identifying statistical anomalies, and the\nMaximum Entropy Discrimination principle to incorporate utility labels, in\norder to detect high-utility anomalies. We apply our method in both simulated\nand real datasets to demonstrate that it has superior performance over existing\nalternatives that independently pre-process with unsupervised anomaly detection\nalgorithms before classifying.\n", "versions": [{"version": "v1", "created": "Thu, 16 Feb 2017 20:37:47 GMT"}, {"version": "v2", "created": "Thu, 10 Aug 2017 03:56:11 GMT"}, {"version": "v3", "created": "Thu, 14 Dec 2017 04:24:46 GMT"}], "update_date": "2018-03-08", "authors_parsed": [["Hou", "Elizabeth", ""], ["Sricharan", "Kumar", ""], ["Hero", "Alfred O.", ""]]}, {"id": "1702.05181", "submitter": "Akshay Soni", "authors": "Akshay Soni and Yashar Mehdad", "title": "RIPML: A Restricted Isometry Property based Approach to Multilabel\n  Learning", "comments": "6 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The multilabel learning problem with large number of labels, features, and\ndata-points has generated a tremendous interest recently. A recurring theme of\nthese problems is that only a few labels are active in any given datapoint as\ncompared to the total number of labels. However, only a small number of\nexisting work take direct advantage of this inherent extreme sparsity in the\nlabel space. By the virtue of Restricted Isometry Property (RIP), satisfied by\nmany random ensembles, we propose a novel procedure for multilabel learning\nknown as RIPML. During the training phase, in RIPML, labels are projected onto\na random low-dimensional subspace followed by solving a least-square problem in\nthis subspace. Inference is done by a k-nearest neighbor (kNN) based approach.\nWe demonstrate the effectiveness of RIPML by conducting extensive simulations\nand comparing results with the state-of-the-art linear dimensionality reduction\nbased approaches.\n", "versions": [{"version": "v1", "created": "Thu, 16 Feb 2017 23:08:50 GMT"}], "update_date": "2017-02-20", "authors_parsed": [["Soni", "Akshay", ""], ["Mehdad", "Yashar", ""]]}, {"id": "1702.05184", "submitter": "Nikos Kargas", "authors": "Nikos Kargas, Nicholas D. Sidiropoulos", "title": "Completing a joint PMF from projections: a low-rank coupled tensor\n  factorization approach", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.DM stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  There has recently been considerable interest in completing a low-rank matrix\nor tensor given only a small fraction (or few linear combinations) of its\nentries. Related approaches have found considerable success in the area of\nrecommender systems, under machine learning. From a statistical estimation\npoint of view, the gold standard is to have access to the joint probability\ndistribution of all pertinent random variables, from which any desired optimal\nestimator can be readily derived. In practice high-dimensional joint\ndistributions are very hard to estimate, and only estimates of low-dimensional\nprojections may be available. We show that it is possible to identify\nhigher-order joint PMFs from lower-order marginalized PMFs using coupled\nlow-rank tensor factorization. Our approach features guaranteed identifiability\nwhen the full joint PMF is of low-enough rank, and effective approximation\notherwise. We provide an algorithmic approach to compute the sought factors,\nand illustrate the merits of our approach using rating prediction as an\nexample.\n", "versions": [{"version": "v1", "created": "Thu, 16 Feb 2017 23:28:35 GMT"}], "update_date": "2017-02-20", "authors_parsed": [["Kargas", "Nikos", ""], ["Sidiropoulos", "Nicholas D.", ""]]}, {"id": "1702.05186", "submitter": "Max Simchowitz", "authors": "Max Simchowitz and Kevin Jamieson and Benjamin Recht", "title": "The Simulator: Understanding Adaptive Sampling in the\n  Moderate-Confidence Regime", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a novel technique for analyzing adaptive sampling called the {\\em\nSimulator}. Our approach differs from the existing methods by considering not\nhow much information could be gathered by any fixed sampling strategy, but how\ndifficult it is to distinguish a good sampling strategy from a bad one given\nthe limited amount of data collected up to any given time. This change of\nperspective allows us to match the strength of both Fano and change-of-measure\ntechniques, without succumbing to the limitations of either method. For\nconcreteness, we apply our techniques to a structured multi-arm bandit problem\nin the fixed-confidence pure exploration setting, where we show that the\nconstraints on the means imply a substantial gap between the\nmoderate-confidence sample complexity, and the asymptotic sample complexity as\n$\\delta \\to 0$ found in the literature. We also prove the first instance-based\nlower bounds for the top-k problem which incorporate the appropriate\nlog-factors. Moreover, our lower bounds zero-in on the number of times each\n\\emph{individual} arm needs to be pulled, uncovering new phenomena which are\ndrowned out in the aggregate sample complexity. Our new analysis inspires a\nsimple and near-optimal algorithm for the best-arm and top-k identification,\nthe first {\\em practical} algorithm of its kind for the latter problem which\nremoves extraneous log factors, and outperforms the state-of-the-art in\nexperiments.\n", "versions": [{"version": "v1", "created": "Thu, 16 Feb 2017 23:42:02 GMT"}], "update_date": "2017-02-20", "authors_parsed": [["Simchowitz", "Max", ""], ["Jamieson", "Kevin", ""], ["Recht", "Benjamin", ""]]}, {"id": "1702.05192", "submitter": "Mohammad-Parsa Hosseini", "authors": "Mohammad-Parsa Hosseini, Hamid Soltanian-Zadeh, Kost Elisevich, and\n  Dario Pompili", "title": "Cloud-based Deep Learning of Big EEG Data for Epileptic Seizure\n  Prediction", "comments": "IEEE Global Conference on Signal and Information Processing\n  (GlobalSIP), Greater Washington, DC, Dec 7-9, 2016", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Developing a Brain-Computer Interface~(BCI) for seizure prediction can help\nepileptic patients have a better quality of life. However, there are many\ndifficulties and challenges in developing such a system as a real-life support\nfor patients. Because of the nonstationary nature of EEG signals, normal and\nseizure patterns vary across different patients. Thus, finding a group of\nmanually extracted features for the prediction task is not practical. Moreover,\nwhen using implanted electrodes for brain recording massive amounts of data are\nproduced. This big data calls for the need for safe storage and high\ncomputational resources for real-time processing. To address these challenges,\na cloud-based BCI system for the analysis of this big EEG data is presented.\nFirst, a dimensionality-reduction technique is developed to increase\nclassification accuracy as well as to decrease the communication bandwidth and\ncomputation time. Second, following a deep-learning approach, a stacked\nautoencoder is trained in two steps for unsupervised feature extraction and\nclassification. Third, a cloud-computing solution is proposed for real-time\nanalysis of big EEG data. The results on a benchmark clinical dataset\nillustrate the superiority of the proposed patient-specific BCI as an\nalternative method and its expected usefulness in real-life support of epilepsy\npatients.\n", "versions": [{"version": "v1", "created": "Fri, 17 Feb 2017 00:00:38 GMT"}], "update_date": "2017-02-20", "authors_parsed": [["Hosseini", "Mohammad-Parsa", ""], ["Soltanian-Zadeh", "Hamid", ""], ["Elisevich", "Kost", ""], ["Pompili", "Dario", ""]]}, {"id": "1702.05222", "submitter": "Morteza Noshad Iranzad", "authors": "Morteza Noshad, Kevin R. Moon, Salimeh Yasaei Sekeh, Alfred O. Hero\n  III", "title": "Direct Estimation of Information Divergence Using Nearest Neighbor\n  Ratios", "comments": "2017 IEEE International Symposium on Information Theory (ISIT)", "journal-ref": "In Information Theory (ISIT), 2017 IEEE International Symposium on\n  (pp. 903-907). IEEE", "doi": "10.1109/ISIT.2017.8006659", "report-no": null, "categories": "cs.IT cs.AI math.IT stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a direct estimation method for R\\'{e}nyi and f-divergence measures\nbased on a new graph theoretical interpretation. Suppose that we are given two\nsample sets $X$ and $Y$, respectively with $N$ and $M$ samples, where\n$\\eta:=M/N$ is a constant value. Considering the $k$-nearest neighbor ($k$-NN)\ngraph of $Y$ in the joint data set $(X,Y)$, we show that the average powered\nratio of the number of $X$ points to the number of $Y$ points among all $k$-NN\npoints is proportional to R\\'{e}nyi divergence of $X$ and $Y$ densities. A\nsimilar method can also be used to estimate f-divergence measures. We derive\nbias and variance rates, and show that for the class of $\\gamma$-H\\\"{o}lder\nsmooth functions, the estimator achieves the MSE rate of\n$O(N^{-2\\gamma/(\\gamma+d)})$. Furthermore, by using a weighted ensemble\nestimation technique, for density functions with continuous and bounded\nderivatives of up to the order $d$, and some extra conditions at the support\nset boundary, we derive an ensemble estimator that achieves the parametric MSE\nrate of $O(1/N)$. Our estimators are more computationally tractable than other\ncompeting estimators, which makes them appealing in many practical\napplications.\n", "versions": [{"version": "v1", "created": "Fri, 17 Feb 2017 04:46:24 GMT"}, {"version": "v2", "created": "Mon, 20 Nov 2017 23:37:20 GMT"}], "update_date": "2017-11-22", "authors_parsed": [["Noshad", "Morteza", ""], ["Moon", "Kevin R.", ""], ["Sekeh", "Salimeh Yasaei", ""], ["Hero", "Alfred O.", "III"]]}, {"id": "1702.05243", "submitter": "Luca Ambrogioni", "authors": "Luca Ambrogioni, Umut G\\\"u\\c{c}l\\\"u, Eric Maris and Marcel van Gerven", "title": "Estimating Nonlinear Dynamics with the ConvNet Smoother", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Estimating the state of a dynamical system from a series of noise-corrupted\nobservations is fundamental in many areas of science and engineering. The most\nwell-known method, the Kalman smoother (and the related Kalman filter), relies\non assumptions of linearity and Gaussianity that are rarely met in practice. In\nthis paper, we introduced a new dynamical smoothing method that exploits the\nremarkable capabilities of convolutional neural networks to approximate complex\nnon-linear functions. The main idea is to generate a training set composed of\nboth latent states and observations from an ensemble of simulators and to train\nthe deep network to recover the former from the latter. Importantly, this\nmethod only requires the availability of the simulators and can therefore be\napplied in situations in which either the latent dynamical model or the\nobservation model cannot be easily expressed in closed form. In our simulation\nstudies, we show that the resulting ConvNet smoother has almost optimal\nperformance in the Gaussian case even when the parameters are unknown.\nFurthermore, the method can be successfully applied to extremely non-linear and\nnon-Gaussian systems. Finally, we empirically validate our approach via the\nanalysis of measured brain signals.\n", "versions": [{"version": "v1", "created": "Fri, 17 Feb 2017 07:37:46 GMT"}, {"version": "v2", "created": "Thu, 30 Mar 2017 09:09:04 GMT"}, {"version": "v3", "created": "Fri, 21 Apr 2017 15:33:09 GMT"}], "update_date": "2017-04-24", "authors_parsed": [["Ambrogioni", "Luca", ""], ["G\u00fc\u00e7l\u00fc", "Umut", ""], ["Maris", "Eric", ""], ["van Gerven", "Marcel", ""]]}, {"id": "1702.05289", "submitter": "Lionel Mathelin", "authors": "Lionel Mathelin, K\\'evin Kasper and Hisham Abou-Kandil", "title": "Observable dictionary learning for high-dimensional statistical\n  inference", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper introduces a method for efficiently inferring a high-dimensional\ndistributed quantity from a few observations. The quantity of interest (QoI) is\napproximated in a basis (dictionary) learned from a training set. The\ncoefficients associated with the approximation of the QoI in the basis are\ndetermined by minimizing the misfit with the observations. To obtain a\nprobabilistic estimate of the quantity of interest, a Bayesian approach is\nemployed. The QoI is treated as a random field endowed with a hierarchical\nprior distribution so that closed-form expressions can be obtained for the\nposterior distribution. The main contribution of the present work lies in the\nderivation of \\emph{a representation basis consistent with the observation\nchain} used to infer the associated coefficients. The resulting dictionary is\nthen tailored to be both observable by the sensors and accurate in\napproximating the posterior mean. An algorithm for deriving such an observable\ndictionary is presented. The method is illustrated with the estimation of the\nvelocity field of an open cavity flow from a handful of wall-mounted point\nsensors. Comparison with standard estimation approaches relying on Principal\nComponent Analysis and K-SVD dictionaries is provided and illustrates the\nsuperior performance of the present approach.\n", "versions": [{"version": "v1", "created": "Fri, 17 Feb 2017 10:25:24 GMT"}, {"version": "v2", "created": "Sat, 25 Mar 2017 13:39:17 GMT"}], "update_date": "2017-03-28", "authors_parsed": [["Mathelin", "Lionel", ""], ["Kasper", "K\u00e9vin", ""], ["Abou-Kandil", "Hisham", ""]]}, {"id": "1702.05327", "submitter": "Sohail Bahmani", "authors": "Sohail Bahmani and Justin Romberg", "title": "Solving Equations of Random Convex Functions via Anchored Regression", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.IT math.IT math.OC math.PR stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider the question of estimating a solution to a system of equations\nthat involve convex nonlinearities, a problem that is common in machine\nlearning and signal processing. Because of these nonlinearities, conventional\nestimators based on empirical risk minimization generally involve solving a\nnon-convex optimization program. We propose anchored regression, a new approach\nbased on convex programming that amounts to maximizing a linear functional\n(perhaps augmented by a regularizer) over a convex set. The proposed convex\nprogram is formulated in the natural space of the problem, and avoids the\nintroduction of auxiliary variables, making it computationally favorable.\nWorking in the native space also provides great flexibility as structural\npriors (e.g., sparsity) can be seamlessly incorporated.\n  For our analysis, we model the equations as being drawn from a fixed set\naccording to a probability law. Our main results provide guarantees on the\naccuracy of the estimator in terms of the number of equations we are solving,\nthe amount of noise present, a measure of statistical complexity of the random\nequations, and the geometry of the regularizer at the true solution. We also\nprovide recipes for constructing the anchor vector (that determines the linear\nfunctional to maximize) directly from the observed data.\n", "versions": [{"version": "v1", "created": "Fri, 17 Feb 2017 13:05:04 GMT"}, {"version": "v2", "created": "Sat, 30 Sep 2017 00:12:02 GMT"}, {"version": "v3", "created": "Mon, 13 Aug 2018 15:46:23 GMT"}], "update_date": "2018-08-14", "authors_parsed": [["Bahmani", "Sohail", ""], ["Romberg", "Justin", ""]]}, {"id": "1702.05376", "submitter": "Dmitry Ignatov", "authors": "Dmitry I. Ignatov and Bruce W. Watson", "title": "Towards a Unified Taxonomy of Biclustering Methods", "comments": "http://ceur-ws.org/Vol-1552/", "journal-ref": "Russian and South African Workshop on Knowledge Discovery\n  Techniques Based on Formal Concept Analysis (RuZA 2015), November 30 -\n  December 5, 2015, Stellenbosch, South Africa, In CEUR Workshop Proceedings,\n  Vol. 1552, p. 23-39", "doi": null, "report-no": null, "categories": "cs.AI cs.DM stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Being an unsupervised machine learning and data mining technique,\nbiclustering and its multimodal extensions are becoming popular tools for\nanalysing object-attribute data in different domains. Apart from conventional\nclustering techniques, biclustering is searching for homogeneous groups of\nobjects while keeping their common description, e.g., in binary setting, their\nshared attributes. In bioinformatics, biclustering is used to find genes, which\nare active in a subset of situations, thus being candidates for biomarkers.\nHowever, the authors of those biclustering techniques that are popular in gene\nexpression analysis, may overlook the existing methods. For instance, BiMax\nalgorithm is aimed at finding biclusters, which are well-known for decades as\nformal concepts. Moreover, even if bioinformatics classify the biclustering\nmethods according to reasonable domain-driven criteria, their classification\ntaxonomies may be different from survey to survey and not full as well. So, in\nthis paper we propose to use concept lattices as a tool for taxonomy building\n(in the biclustering domain) and attribute exploration as means for\ncross-domain taxonomy completion.\n", "versions": [{"version": "v1", "created": "Fri, 17 Feb 2017 15:12:31 GMT"}], "update_date": "2017-02-20", "authors_parsed": [["Ignatov", "Dmitry I.", ""], ["Watson", "Bruce W.", ""]]}, {"id": "1702.05386", "submitter": "Zachary Lipton", "authors": "Nathan Ng, Rodney A Gabriel, Julian McAuley, Charles Elkan, Zachary C\n  Lipton", "title": "Predicting Surgery Duration with Neural Heteroscedastic Regression", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Scheduling surgeries is a challenging task due to the fundamental uncertainty\nof the clinical environment, as well as the risks and costs associated with\nunder- and over-booking. We investigate neural regression algorithms to\nestimate the parameters of surgery case durations, focusing on the issue of\nheteroscedasticity. We seek to simultaneously estimate the duration of each\nsurgery, as well as a surgery-specific notion of our uncertainty about its\nduration. Estimating this uncertainty can lead to more nuanced and effective\nscheduling strategies, as we are able to schedule surgeries more efficiently\nwhile allowing an informed and case-specific margin of error. Using surgery\nrecords %from the UC San Diego Health System, from a large United States health\nsystem we demonstrate potential improvements on the order of 20% (in terms of\nminutes overbooked) compared to current scheduling techniques. Moreover, we\ndemonstrate that surgery durations are indeed heteroscedastic. We show that\nmodels that estimate case-specific uncertainty better fit the data (log\nlikelihood). Additionally, we show that the heteroscedastic predictions can\nmore optimally trade off between over and under-booking minutes, especially\nwhen idle minutes and scheduling collisions confer disparate costs.\n", "versions": [{"version": "v1", "created": "Fri, 17 Feb 2017 15:28:28 GMT"}, {"version": "v2", "created": "Thu, 13 Apr 2017 22:26:47 GMT"}, {"version": "v3", "created": "Thu, 13 Jul 2017 03:48:14 GMT"}], "update_date": "2017-07-14", "authors_parsed": [["Ng", "Nathan", ""], ["Gabriel", "Rodney A", ""], ["McAuley", "Julian", ""], ["Elkan", "Charles", ""], ["Lipton", "Zachary C", ""]]}, {"id": "1702.05390", "submitter": "Andreas Ruttor", "authors": "Philipp Batz, Andreas Ruttor, Manfred Opper", "title": "Approximate Bayes learning of stochastic differential equations", "comments": "18 pages, 22 figures", "journal-ref": "Phys. Rev. E 98, 022109 (2018)", "doi": "10.1103/PhysRevE.98.022109", "report-no": null, "categories": "physics.data-an stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We introduce a nonparametric approach for estimating drift and diffusion\nfunctions in systems of stochastic differential equations from observations of\nthe state vector. Gaussian processes are used as flexible models for these\nfunctions and estimates are calculated directly from dense data sets using\nGaussian process regression. We also develop an approximate expectation\nmaximization algorithm to deal with the unobserved, latent dynamics between\nsparse observations. The posterior over states is approximated by a piecewise\nlinearized process of the Ornstein-Uhlenbeck type and the maximum a posteriori\nestimation of the drift is facilitated by a sparse Gaussian process\napproximation.\n", "versions": [{"version": "v1", "created": "Fri, 17 Feb 2017 15:35:08 GMT"}], "update_date": "2018-08-15", "authors_parsed": [["Batz", "Philipp", ""], ["Ruttor", "Andreas", ""], ["Opper", "Manfred", ""]]}, {"id": "1702.05423", "submitter": "Yangyang Xu", "authors": "Yangyang Xu, Shuzhong Zhang", "title": "Accelerated Primal-Dual Proximal Block Coordinate Updating Methods for\n  Constrained Convex Optimization", "comments": "Accepted to Computational Optimization and Applications", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.OC cs.NA stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Block Coordinate Update (BCU) methods enjoy low per-update computational\ncomplexity because every time only one or a few block variables would need to\nbe updated among possibly a large number of blocks. They are also easily\nparallelized and thus have been particularly popular for solving problems\ninvolving large-scale dataset and/or variables. In this paper, we propose a\nprimal-dual BCU method for solving linearly constrained convex program in\nmulti-block variables. The method is an accelerated version of a primal-dual\nalgorithm proposed by the authors, which applies randomization in selecting\nblock variables to update and establishes an $O(1/t)$ convergence rate under\nweak convexity assumption. We show that the rate can be accelerated to\n$O(1/t^2)$ if the objective is strongly convex. In addition, if one block\nvariable is independent of the others in the objective, we then show that the\nalgorithm can be modified to achieve a linear rate of convergence. The\nnumerical experiments show that the accelerated method performs stably with a\nsingle set of parameters while the original method needs to tune the parameters\nfor different datasets in order to achieve a comparable level of performance.\n", "versions": [{"version": "v1", "created": "Fri, 17 Feb 2017 16:29:00 GMT"}, {"version": "v2", "created": "Wed, 15 Nov 2017 17:36:26 GMT"}, {"version": "v3", "created": "Mon, 20 Nov 2017 19:55:18 GMT"}], "update_date": "2017-11-22", "authors_parsed": [["Xu", "Yangyang", ""], ["Zhang", "Shuzhong", ""]]}, {"id": "1702.05443", "submitter": "Andreas Loukas", "authors": "Andreas Loukas", "title": "How close are the eigenvectors and eigenvalues of the sample and actual\n  covariance matrices?", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  How many samples are sufficient to guarantee that the eigenvectors and\neigenvalues of the sample covariance matrix are close to those of the actual\ncovariance matrix? For a wide family of distributions, including distributions\nwith finite second moment and distributions supported in a centered Euclidean\nball, we prove that the inner product between eigenvectors of the sample and\nactual covariance matrices decreases proportionally to the respective\neigenvalue distance. Our findings imply non-asymptotic concentration bounds for\neigenvectors, eigenspaces, and eigenvalues. They also provide conditions for\ndistinguishing principal components based on a constant number of samples.\n", "versions": [{"version": "v1", "created": "Fri, 17 Feb 2017 17:15:23 GMT"}], "update_date": "2017-02-20", "authors_parsed": [["Loukas", "Andreas", ""]]}, {"id": "1702.05462", "submitter": "Fabrizio Leisen", "authors": "Laurentiu Hinoveanu, Fabrizio Leisen and Cristiano Villa", "title": "Objective Bayesian Analysis for Change Point Problems", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME math.ST stat.AP stat.CO stat.ML stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper we present a loss-based approach to change point analysis. In\nparticular, we look at the problem from two perspectives. The first focuses on\nthe definition of a prior when the number of change points is known a priori.\nThe second contribution aims to estimate the number of change points by using a\nloss-based approach recently introduced in the literature. The latter considers\nchange point estimation as a model selection exercise. We show the performance\nof the proposed approach on simulated data and real data sets.\n", "versions": [{"version": "v1", "created": "Fri, 17 Feb 2017 18:06:27 GMT"}, {"version": "v2", "created": "Sun, 7 Jan 2018 13:58:48 GMT"}], "update_date": "2018-01-09", "authors_parsed": [["Hinoveanu", "Laurentiu", ""], ["Leisen", "Fabrizio", ""], ["Villa", "Cristiano", ""]]}, {"id": "1702.05471", "submitter": "Soheil Feizi", "authors": "Soheil Feizi and David Tse", "title": "Maximally Correlated Principal Component Analysis", "comments": "35 pages, 5 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.IT cs.LG math.IT", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In the era of big data, reducing data dimensionality is critical in many\nareas of science. Widely used Principal Component Analysis (PCA) addresses this\nproblem by computing a low dimensional data embedding that maximally explain\nvariance of the data. However, PCA has two major weaknesses. Firstly, it only\nconsiders linear correlations among variables (features), and secondly it is\nnot suitable for categorical data. We resolve these issues by proposing\nMaximally Correlated Principal Component Analysis (MCPCA). MCPCA computes\ntransformations of variables whose covariance matrix has the largest Ky Fan\nnorm. Variable transformations are unknown, can be nonlinear and are computed\nin an optimization. MCPCA can also be viewed as a multivariate extension of\nMaximal Correlation. For jointly Gaussian variables we show that the covariance\nmatrix corresponding to the identity (or the negative of the identity)\ntransformations majorizes covariance matrices of non-identity functions. Using\nthis result we characterize global MCPCA optimizers for nonlinear functions of\njointly Gaussian variables for every rank constraint. For categorical variables\nwe characterize global MCPCA optimizers for the rank one constraint based on\nthe leading eigenvector of a matrix computed using pairwise joint\ndistributions. For a general rank constraint we propose a block coordinate\ndescend algorithm and show its convergence to stationary points of the MCPCA\noptimization. We compare MCPCA with PCA and other state-of-the-art\ndimensionality reduction methods including Isomap, LLE, multilayer autoencoders\n(neural networks), kernel PCA, probabilistic PCA and diffusion maps on several\nsynthetic and real datasets. We show that MCPCA consistently provides improved\nperformance compared to other methods.\n", "versions": [{"version": "v1", "created": "Fri, 17 Feb 2017 18:43:58 GMT"}, {"version": "v2", "created": "Tue, 21 Feb 2017 20:38:13 GMT"}], "update_date": "2017-02-24", "authors_parsed": [["Feizi", "Soheil", ""], ["Tse", "David", ""]]}, {"id": "1702.05536", "submitter": "Zifan Li", "authors": "Zifan Li, Ambuj Tewari", "title": "Beyond the Hazard Rate: More Perturbation Algorithms for Adversarial\n  Multi-armed Bandits", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.GT stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recent work on follow the perturbed leader (FTPL) algorithms for the\nadversarial multi-armed bandit problem has highlighted the role of the hazard\nrate of the distribution generating the perturbations. Assuming that the hazard\nrate is bounded, it is possible to provide regret analyses for a variety of\nFTPL algorithms for the multi-armed bandit problem. This paper pushes the\ninquiry into regret bounds for FTPL algorithms beyond the bounded hazard rate\ncondition. There are good reasons to do so: natural distributions such as the\nuniform and Gaussian violate the condition. We give regret bounds for both\nbounded support and unbounded support distributions without assuming the hazard\nrate condition. We also disprove a conjecture that the Gaussian distribution\ncannot lead to a low-regret algorithm. In fact, it turns out that it leads to\nnear optimal regret, up to logarithmic factors. A key ingredient in our\napproach is the introduction of a new notion called the generalized hazard\nrate.\n", "versions": [{"version": "v1", "created": "Fri, 17 Feb 2017 22:39:37 GMT"}, {"version": "v2", "created": "Fri, 5 Jan 2018 19:31:42 GMT"}], "update_date": "2018-01-09", "authors_parsed": [["Li", "Zifan", ""], ["Tewari", "Ambuj", ""]]}, {"id": "1702.05538", "submitter": "Terrance DeVries", "authors": "Terrance DeVries, Graham W. Taylor", "title": "Dataset Augmentation in Feature Space", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Dataset augmentation, the practice of applying a wide array of\ndomain-specific transformations to synthetically expand a training set, is a\nstandard tool in supervised learning. While effective in tasks such as visual\nrecognition, the set of transformations must be carefully designed,\nimplemented, and tested for every new domain, limiting its re-use and\ngenerality. In this paper, we adopt a simpler, domain-agnostic approach to\ndataset augmentation. We start with existing data points and apply simple\ntransformations such as adding noise, interpolating, or extrapolating between\nthem. Our main insight is to perform the transformation not in input space, but\nin a learned feature space. A re-kindling of interest in unsupervised\nrepresentation learning makes this technique timely and more effective. It is a\nsimple proposal, but to-date one that has not been tested empirically. Working\nin the space of context vectors generated by sequence-to-sequence models, we\ndemonstrate a technique that is effective for both static and sequential data.\n", "versions": [{"version": "v1", "created": "Fri, 17 Feb 2017 23:13:15 GMT"}], "update_date": "2017-02-21", "authors_parsed": [["DeVries", "Terrance", ""], ["Taylor", "Graham W.", ""]]}, {"id": "1702.05574", "submitter": "Yury Polyanskiy", "authors": "Yury Polyanskiy, Ananda Theertha Suresh and Yihong Wu", "title": "Sample complexity of population recovery", "comments": "Earlier versions (incl. the one in proceedings) had a mistake in\n  Prop. 9 that propagated to Theorem 1 (lower bound) and Lemma 12. This version\n  (v3) fixes those", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST cs.IT math.IT stat.ML stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The problem of population recovery refers to estimating a distribution based\non incomplete or corrupted samples. Consider a random poll of sample size $n$\nconducted on a population of individuals, where each pollee is asked to answer\n$d$ binary questions. We consider one of the two polling impediments: (a) in\nlossy population recovery, a pollee may skip each question with probability\n$\\epsilon$, (b) in noisy population recovery, a pollee may lie on each question\nwith probability $\\epsilon$. Given $n$ lossy or noisy samples, the goal is to\nestimate the probabilities of all $2^d$ binary vectors simultaneously within\naccuracy $\\delta$ with high probability.\n  This paper settles the sample complexity of population recovery. For lossy\nmodel, the optimal sample complexity is\n$\\tilde\\Theta(\\delta^{-2\\max\\{\\frac{\\epsilon}{1-\\epsilon},1\\}})$, improving the\nstate of the art by Moitra and Saks in several ways: a lower bound is\nestablished, the upper bound is improved and the result depends at most on the\nlogarithm of the dimension. Surprisingly, the sample complexity undergoes a\nphase transition from parametric to nonparametric rate when $\\epsilon$ exceeds\n$1/2$. For noisy population recovery, the sharp sample complexity turns out to\nbe more sensitive to dimension and scales as $\\exp(\\Theta(d^{1/3}\n\\log^{2/3}(1/\\delta)))$ except for the trivial cases of $\\epsilon=0,1/2$ or\n$1$.\n  For both models, our estimators simply compute the empirical mean of a\ncertain function, which is found by pre-solving a linear program (LP).\nCuriously, the dual LP can be understood as Le Cam's method for lower-bounding\nthe minimax risk, thus establishing the statistical optimality of the proposed\nestimators. The value of the LP is determined by complex-analytic methods.\n", "versions": [{"version": "v1", "created": "Sat, 18 Feb 2017 06:11:25 GMT"}, {"version": "v2", "created": "Mon, 5 Jun 2017 01:56:29 GMT"}, {"version": "v3", "created": "Wed, 29 Apr 2020 09:23:13 GMT"}], "update_date": "2020-04-30", "authors_parsed": [["Polyanskiy", "Yury", ""], ["Suresh", "Ananda Theertha", ""], ["Wu", "Yihong", ""]]}, {"id": "1702.05575", "submitter": "Yuchen Zhang", "authors": "Yuchen Zhang, Percy Liang, Moses Charikar", "title": "A Hitting Time Analysis of Stochastic Gradient Langevin Dynamics", "comments": "Correct two mistakes in the proofs of Lemma 3 and Lemma 5", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG math.OC stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study the Stochastic Gradient Langevin Dynamics (SGLD) algorithm for\nnon-convex optimization. The algorithm performs stochastic gradient descent,\nwhere in each step it injects appropriately scaled Gaussian noise to the\nupdate. We analyze the algorithm's hitting time to an arbitrary subset of the\nparameter space. Two results follow from our general theory: First, we prove\nthat for empirical risk minimization, if the empirical risk is point-wise close\nto the (smooth) population risk, then the algorithm achieves an approximate\nlocal minimum of the population risk in polynomial time, escaping suboptimal\nlocal minima that only exist in the empirical risk. Second, we show that SGLD\nimproves on one of the best known learnability results for learning linear\nclassifiers under the zero-one loss.\n", "versions": [{"version": "v1", "created": "Sat, 18 Feb 2017 06:33:55 GMT"}, {"version": "v2", "created": "Tue, 6 Jun 2017 18:59:05 GMT"}, {"version": "v3", "created": "Mon, 9 Apr 2018 07:14:34 GMT"}], "update_date": "2018-04-10", "authors_parsed": [["Zhang", "Yuchen", ""], ["Liang", "Percy", ""], ["Charikar", "Moses", ""]]}, {"id": "1702.05581", "submitter": "Chicheng Zhang", "authors": "Songbai Yan and Chicheng Zhang", "title": "Revisiting Perceptron: Efficient and Label-Optimal Learning of\n  Halfspaces", "comments": "NIPS 2017", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  It has been a long-standing problem to efficiently learn a halfspace using as\nfew labels as possible in the presence of noise. In this work, we propose an\nefficient Perceptron-based algorithm for actively learning homogeneous\nhalfspaces under the uniform distribution over the unit sphere. Under the\nbounded noise condition~\\cite{MN06}, where each label is flipped with\nprobability at most $\\eta < \\frac 1 2$, our algorithm achieves a near-optimal\nlabel complexity of\n$\\tilde{O}\\left(\\frac{d}{(1-2\\eta)^2}\\ln\\frac{1}{\\epsilon}\\right)$ in time\n$\\tilde{O}\\left(\\frac{d^2}{\\epsilon(1-2\\eta)^3}\\right)$. Under the adversarial\nnoise condition~\\cite{ABL14, KLS09, KKMS08}, where at most a $\\tilde\n\\Omega(\\epsilon)$ fraction of labels can be flipped, our algorithm achieves a\nnear-optimal label complexity of $\\tilde{O}\\left(d\\ln\\frac{1}{\\epsilon}\\right)$\nin time $\\tilde{O}\\left(\\frac{d^2}{\\epsilon}\\right)$. Furthermore, we show that\nour active learning algorithm can be converted to an efficient passive learning\nalgorithm that has near-optimal sample complexities with respect to $\\epsilon$\nand $d$.\n", "versions": [{"version": "v1", "created": "Sat, 18 Feb 2017 07:26:08 GMT"}, {"version": "v2", "created": "Mon, 6 Nov 2017 15:01:39 GMT"}], "update_date": "2017-11-07", "authors_parsed": [["Yan", "Songbai", ""], ["Zhang", "Chicheng", ""]]}, {"id": "1702.05594", "submitter": "Hiroyuki Sato", "authors": "Hiroyuki Sato, Hiroyuki Kasai, Bamdev Mishra", "title": "Riemannian stochastic variance reduced gradient algorithm with\n  retraction and vector transport", "comments": "Published in SIAM Journal on Optimization. Extended and revised\n  version of arXiv:1605.07367", "journal-ref": "SIAM Journal on Optimization 29 (2019) 1444-1472", "doi": "10.1137/17M1116787", "report-no": null, "categories": "cs.LG math.OC stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In recent years, stochastic variance reduction algorithms have attracted\nconsiderable attention for minimizing the average of a large but finite number\nof loss functions. This paper proposes a novel Riemannian extension of the\nEuclidean stochastic variance reduced gradient (R-SVRG) algorithm to a manifold\nsearch space. The key challenges of averaging, adding, and subtracting multiple\ngradients are addressed with retraction and vector transport. For the proposed\nalgorithm, we present a global convergence analysis with a decaying step size\nas well as a local convergence rate analysis with a fixed step size under some\nnatural assumptions. In addition, the proposed algorithm is applied to the\ncomputation problem of the Riemannian centroid on the symmetric positive\ndefinite (SPD) manifold as well as the principal component analysis and\nlow-rank matrix completion problems on the Grassmann manifold. The results show\nthat the proposed algorithm outperforms the standard Riemannian stochastic\ngradient descent algorithm in each case.\n", "versions": [{"version": "v1", "created": "Sat, 18 Feb 2017 10:39:23 GMT"}, {"version": "v2", "created": "Mon, 10 Apr 2017 17:27:05 GMT"}, {"version": "v3", "created": "Fri, 31 May 2019 10:03:25 GMT"}], "update_date": "2019-06-03", "authors_parsed": [["Sato", "Hiroyuki", ""], ["Kasai", "Hiroyuki", ""], ["Mishra", "Bamdev", ""]]}, {"id": "1702.05677", "submitter": "Lunjia Hu", "authors": "Lunjia Hu, Ruihan Wu, Tianhong Li, Liwei Wang", "title": "Quadratic Upper Bound for Recursive Teaching Dimension of Finite VC\n  Classes", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this work we study the quantitative relation between the recursive\nteaching dimension (RTD) and the VC dimension (VCD) of concept classes of\nfinite sizes. The RTD of a concept class $\\mathcal C \\subseteq \\{0, 1\\}^n$,\nintroduced by Zilles et al. (2011), is a combinatorial complexity measure\ncharacterized by the worst-case number of examples necessary to identify a\nconcept in $\\mathcal C$ according to the recursive teaching model.\n  For any finite concept class $\\mathcal C \\subseteq \\{0,1\\}^n$ with\n$\\mathrm{VCD}(\\mathcal C)=d$, Simon & Zilles (2015) posed an open problem\n$\\mathrm{RTD}(\\mathcal C) = O(d)$, i.e., is RTD linearly upper bounded by VCD?\nPreviously, the best known result is an exponential upper bound\n$\\mathrm{RTD}(\\mathcal C) = O(d \\cdot 2^d)$, due to Chen et al. (2016). In this\npaper, we show a quadratic upper bound: $\\mathrm{RTD}(\\mathcal C) = O(d^2)$,\nmuch closer to an answer to the open problem. We also discuss the challenges in\nfully solving the problem.\n", "versions": [{"version": "v1", "created": "Sat, 18 Feb 2017 23:46:10 GMT"}], "update_date": "2017-02-21", "authors_parsed": [["Hu", "Lunjia", ""], ["Wu", "Ruihan", ""], ["Li", "Tianhong", ""], ["Wang", "Liwei", ""]]}, {"id": "1702.05683", "submitter": "Chao Qu", "authors": "Chao Qu, Yan Li, Huan Xu", "title": "SAGA and Restricted Strong Convexity", "comments": "arXiv admin note: text overlap with arXiv:1701.07808", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  SAGA is a fast incremental gradient method on the finite sum problem and its\neffectiveness has been tested on a vast of applications. In this paper, we\nanalyze SAGA on a class of non-strongly convex and non-convex statistical\nproblem such as Lasso, group Lasso, Logistic regression with $\\ell_1$\nregularization, linear regression with SCAD regularization and Correct Lasso.\nWe prove that SAGA enjoys the linear convergence rate up to the statistical\nestimation accuracy, under the assumption of restricted strong convexity (RSC).\nIt significantly extends the applicability of SAGA in convex and non-convex\noptimization.\n", "versions": [{"version": "v1", "created": "Sun, 19 Feb 2017 00:39:06 GMT"}, {"version": "v2", "created": "Mon, 27 Feb 2017 18:30:41 GMT"}], "update_date": "2017-02-28", "authors_parsed": [["Qu", "Chao", ""], ["Li", "Yan", ""], ["Xu", "Huan", ""]]}, {"id": "1702.05698", "submitter": "Wei Xiao", "authors": "Wei Xiao, Xiaolin Huang, Jorge Silva, Saba Emrani and Arin Chaudhuri", "title": "Online Robust Principal Component Analysis with Change Point Detection", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV stat.AP stat.CO stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Robust PCA methods are typically batch algorithms which requires loading all\nobservations into memory before processing. This makes them inefficient to\nprocess big data. In this paper, we develop an efficient online robust\nprincipal component methods, namely online moving window robust principal\ncomponent analysis (OMWRPCA). Unlike existing algorithms, OMWRPCA can\nsuccessfully track not only slowly changing subspace but also abruptly changed\nsubspace. By embedding hypothesis testing into the algorithm, OMWRPCA can\ndetect change points of the underlying subspaces. Extensive simulation studies\ndemonstrate the superior performance of OMWRPCA compared with other\nstate-of-art approaches. We also apply the algorithm for real-time background\nsubtraction of surveillance video.\n", "versions": [{"version": "v1", "created": "Sun, 19 Feb 2017 04:08:18 GMT"}, {"version": "v2", "created": "Mon, 20 Mar 2017 19:49:02 GMT"}], "update_date": "2017-03-22", "authors_parsed": [["Xiao", "Wei", ""], ["Huang", "Xiaolin", ""], ["Silva", "Jorge", ""], ["Emrani", "Saba", ""], ["Chaudhuri", "Arin", ""]]}, {"id": "1702.05777", "submitter": "Daniel Soudry", "authors": "Daniel Soudry, Elad Hoffer", "title": "Exponentially vanishing sub-optimal local minima in multilayer neural\n  networks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Background: Statistical mechanics results (Dauphin et al. (2014); Choromanska\net al. (2015)) suggest that local minima with high error are exponentially rare\nin high dimensions. However, to prove low error guarantees for Multilayer\nNeural Networks (MNNs), previous works so far required either a heavily\nmodified MNN model or training method, strong assumptions on the labels (e.g.,\n\"near\" linear separability), or an unrealistic hidden layer with\n$\\Omega\\left(N\\right)$ units.\n  Results: We examine a MNN with one hidden layer of piecewise linear units, a\nsingle output, and a quadratic loss. We prove that, with high probability in\nthe limit of $N\\rightarrow\\infty$ datapoints, the volume of differentiable\nregions of the empiric loss containing sub-optimal differentiable local minima\nis exponentially vanishing in comparison with the same volume of global minima,\ngiven standard normal input of dimension\n$d_{0}=\\tilde{\\Omega}\\left(\\sqrt{N}\\right)$, and a more realistic number of\n$d_{1}=\\tilde{\\Omega}\\left(N/d_{0}\\right)$ hidden units. We demonstrate our\nresults numerically: for example, $0\\%$ binary classification training error on\nCIFAR with only $N/d_{0}\\approx 16$ hidden neurons.\n", "versions": [{"version": "v1", "created": "Sun, 19 Feb 2017 18:12:51 GMT"}, {"version": "v2", "created": "Tue, 21 Feb 2017 08:50:32 GMT"}, {"version": "v3", "created": "Mon, 27 Feb 2017 15:40:15 GMT"}, {"version": "v4", "created": "Wed, 24 May 2017 13:53:45 GMT"}, {"version": "v5", "created": "Sat, 28 Oct 2017 08:23:22 GMT"}], "update_date": "2017-10-31", "authors_parsed": [["Soudry", "Daniel", ""], ["Hoffer", "Elad", ""]]}, {"id": "1702.05815", "submitter": "Johann Paratte", "authors": "Johan Paratte, Nathana\\\"el Perraudin, Pierre Vandergheynst", "title": "Compressive Embedding and Visualization using Graphs", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Visualizing high-dimensional data has been a focus in data analysis\ncommunities for decades, which has led to the design of many algorithms, some\nof which are now considered references (such as t-SNE for example). In our era\nof overwhelming data volumes, the scalability of such methods have become more\nand more important. In this work, we present a method which allows to apply any\nvisualization or embedding algorithm on very large datasets by considering only\na fraction of the data as input and then extending the information to all data\npoints using a graph encoding its global similarity. We show that in most\ncases, using only $\\mathcal{O}(\\log(N))$ samples is sufficient to diffuse the\ninformation to all $N$ data points. In addition, we propose quantitative\nmethods to measure the quality of embeddings and demonstrate the validity of\nour technique on both synthetic and real-world datasets.\n", "versions": [{"version": "v1", "created": "Sun, 19 Feb 2017 22:59:12 GMT"}], "update_date": "2017-02-21", "authors_parsed": [["Paratte", "Johan", ""], ["Perraudin", "Nathana\u00ebl", ""], ["Vandergheynst", "Pierre", ""]]}, {"id": "1702.05870", "submitter": "Luo Chunjie", "authors": "Chunjie Luo, Jianfeng Zhan, Lei Wang, Qiang Yang", "title": "Cosine Normalization: Using Cosine Similarity Instead of Dot Product in\n  Neural Networks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Traditionally, multi-layer neural networks use dot product between the output\nvector of previous layer and the incoming weight vector as the input to\nactivation function. The result of dot product is unbounded, thus increases the\nrisk of large variance. Large variance of neuron makes the model sensitive to\nthe change of input distribution, thus results in poor generalization, and\naggravates the internal covariate shift which slows down the training. To bound\ndot product and decrease the variance, we propose to use cosine similarity or\ncentered cosine similarity (Pearson Correlation Coefficient) instead of dot\nproduct in neural networks, which we call cosine normalization. We compare\ncosine normalization with batch, weight and layer normalization in\nfully-connected neural networks as well as convolutional networks on the data\nsets of MNIST, 20NEWS GROUP, CIFAR-10/100 and SVHN. Experiments show that\ncosine normalization achieves better performance than other normalization\ntechniques.\n", "versions": [{"version": "v1", "created": "Mon, 20 Feb 2017 06:17:02 GMT"}, {"version": "v2", "created": "Wed, 22 Feb 2017 06:33:34 GMT"}, {"version": "v3", "created": "Mon, 12 Jun 2017 06:05:03 GMT"}, {"version": "v4", "created": "Tue, 13 Jun 2017 07:22:58 GMT"}, {"version": "v5", "created": "Mon, 23 Oct 2017 03:31:59 GMT"}], "update_date": "2017-10-24", "authors_parsed": [["Luo", "Chunjie", ""], ["Zhan", "Jianfeng", ""], ["Wang", "Lei", ""], ["Yang", "Qiang", ""]]}, {"id": "1702.05882", "submitter": "Daniele Tantari", "authors": "Adriano Barra, Giuseppe Genovese, Peter Sollich, Daniele Tantari", "title": "Phase Diagram of Restricted Boltzmann Machines and Generalised Hopfield\n  Networks with Arbitrary Priors", "comments": "18 pages, 9 figures; typos added", "journal-ref": "Phys. Rev. E 97, 022310 (2018)", "doi": "10.1103/PhysRevE.97.022310", "report-no": null, "categories": "cond-mat.dis-nn cs.LG physics.data-an stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Restricted Boltzmann Machines are described by the Gibbs measure of a\nbipartite spin glass, which in turn corresponds to the one of a generalised\nHopfield network. This equivalence allows us to characterise the state of these\nsystems in terms of retrieval capabilities, both at low and high load. We study\nthe paramagnetic-spin glass and the spin glass-retrieval phase transitions, as\nthe pattern (i.e. weight) distribution and spin (i.e. unit) priors vary\nsmoothly from Gaussian real variables to Boolean discrete variables. Our\nanalysis shows that the presence of a retrieval phase is robust and not\npeculiar to the standard Hopfield model with Boolean patterns. The retrieval\nregion is larger when the pattern entries and retrieval units get more peaked\nand, conversely, when the hidden units acquire a broader prior and therefore\nhave a stronger response to high fields. Moreover, at low load retrieval always\nexists below some critical temperature, for every pattern distribution ranging\nfrom the Boolean to the Gaussian case.\n", "versions": [{"version": "v1", "created": "Mon, 20 Feb 2017 07:29:37 GMT"}, {"version": "v2", "created": "Sat, 29 Jul 2017 08:58:42 GMT"}], "update_date": "2018-02-28", "authors_parsed": [["Barra", "Adriano", ""], ["Genovese", "Giuseppe", ""], ["Sollich", "Peter", ""], ["Tantari", "Daniele", ""]]}, {"id": "1702.05960", "submitter": "Jun Fan", "authors": "Yunlong Feng, Jun Fan, and Johan A.K. Suykens", "title": "A Statistical Learning Approach to Modal Regression", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML math.ST stat.ME stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper studies the nonparametric modal regression problem systematically\nfrom a statistical learning view. Originally motivated by pursuing a\ntheoretical understanding of the maximum correntropy criterion based regression\n(MCCR), our study reveals that MCCR with a tending-to-zero scale parameter is\nessentially modal regression. We show that nonparametric modal regression\nproblem can be approached via the classical empirical risk minimization. Some\nefforts are then made to develop a framework for analyzing and implementing\nmodal regression. For instance, the modal regression function is described, the\nmodal regression risk is defined explicitly and its \\textit{Bayes} rule is\ncharacterized; for the sake of computational tractability, the surrogate modal\nregression risk, which is termed as the generalization risk in our study, is\nintroduced. On the theoretical side, the excess modal regression risk, the\nexcess generalization risk, the function estimation error, and the relations\namong the above three quantities are studied rigorously. It turns out that\nunder mild conditions, function estimation consistency and convergence may be\npursued in modal regression as in vanilla regression protocols, such as mean\nregression, median regression, and quantile regression. However, it outperforms\nthese regression models in terms of robustness as shown in our study from a\nre-descending M-estimation view. This coincides with and in return explains the\nmerits of MCCR on robustness. On the practical side, the implementation issues\nof modal regression including the computational algorithm and the tuning\nparameters selection are discussed. Numerical assessments on modal regression\nare also conducted to verify our findings empirically.\n", "versions": [{"version": "v1", "created": "Mon, 20 Feb 2017 13:31:39 GMT"}, {"version": "v2", "created": "Wed, 8 Mar 2017 18:22:26 GMT"}, {"version": "v3", "created": "Thu, 16 Jan 2020 06:39:00 GMT"}, {"version": "v4", "created": "Fri, 17 Jan 2020 05:29:25 GMT"}], "update_date": "2020-01-20", "authors_parsed": [["Feng", "Yunlong", ""], ["Fan", "Jun", ""], ["Suykens", "Johan A. K.", ""]]}, {"id": "1702.06103", "submitter": "Yevgeny Seldin", "authors": "Yevgeny Seldin and G\\'abor Lugosi", "title": "An Improved Parametrization and Analysis of the EXP3++ Algorithm for\n  Stochastic and Adversarial Bandits", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a new strategy for gap estimation in randomized algorithms for\nmultiarmed bandits and combine it with the EXP3++ algorithm of Seldin and\nSlivkins (2014). In the stochastic regime the strategy reduces dependence of\nregret on a time horizon from $(\\ln t)^3$ to $(\\ln t)^2$ and eliminates an\nadditive factor of order $\\Delta e^{1/\\Delta^2}$, where $\\Delta$ is the minimal\ngap of a problem instance. In the adversarial regime regret guarantee remains\nunchanged.\n", "versions": [{"version": "v1", "created": "Mon, 20 Feb 2017 18:43:05 GMT"}, {"version": "v2", "created": "Tue, 9 May 2017 14:29:38 GMT"}], "update_date": "2017-05-10", "authors_parsed": [["Seldin", "Yevgeny", ""], ["Lugosi", "G\u00e1bor", ""]]}, {"id": "1702.06166", "submitter": "Tammo Rukat", "authors": "Tammo Rukat and Chris C. Holmes and Michalis K. Titsias and\n  Christopher Yau", "title": "Bayesian Boolean Matrix Factorisation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG cs.NA q-bio.GN q-bio.QM stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Boolean matrix factorisation aims to decompose a binary data matrix into an\napproximate Boolean product of two low rank, binary matrices: one containing\nmeaningful patterns, the other quantifying how the observations can be\nexpressed as a combination of these patterns. We introduce the OrMachine, a\nprobabilistic generative model for Boolean matrix factorisation and derive a\nMetropolised Gibbs sampler that facilitates efficient parallel posterior\ninference. On real world and simulated data, our method outperforms all\ncurrently existing approaches for Boolean matrix factorisation and completion.\nThis is the first method to provide full posterior inference for Boolean Matrix\nfactorisation which is relevant in applications, e.g. for controlling false\npositive rates in collaborative filtering and, crucially, improves the\ninterpretability of the inferred patterns. The proposed algorithm scales to\nlarge datasets as we demonstrate by analysing single cell gene expression data\nin 1.3 million mouse brain cells across 11 thousand genes on commodity\nhardware.\n", "versions": [{"version": "v1", "created": "Mon, 20 Feb 2017 20:31:39 GMT"}, {"version": "v2", "created": "Sat, 25 Feb 2017 14:17:44 GMT"}], "update_date": "2017-02-28", "authors_parsed": [["Rukat", "Tammo", ""], ["Holmes", "Chris C.", ""], ["Titsias", "Michalis K.", ""], ["Yau", "Christopher", ""]]}, {"id": "1702.06175", "submitter": "Mahdi Soltanolkotabi", "authors": "Mahdi Soltanolkotabi", "title": "Structured signal recovery from quadratic measurements: Breaking sample\n  complexity barriers via nonconvex optimization", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.IT math.FA math.IT math.OC stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper concerns the problem of recovering an unknown but structured\nsignal $x \\in R^n$ from $m$ quadratic measurements of the form\n$y_r=|<a_r,x>|^2$ for $r=1,2,...,m$. We focus on the under-determined setting\nwhere the number of measurements is significantly smaller than the dimension of\nthe signal ($m<<n$). We formulate the recovery problem as a nonconvex\noptimization problem where prior structural information about the signal is\nenforced through constrains on the optimization variables. We prove that\nprojected gradient descent, when initialized in a neighborhood of the desired\nsignal, converges to the unknown signal at a linear rate. These results hold\nfor any constraint set (convex or nonconvex) providing convergence guarantees\nto the global optimum even when the objective function and constraint set is\nnonconvex. Furthermore, these results hold with a number of measurements that\nis only a constant factor away from the minimal number of measurements required\nto uniquely identify the unknown signal. Our results provide the first provably\ntractable algorithm for this data-poor regime, breaking local sample complexity\nbarriers that have emerged in recent literature. In a companion paper we\ndemonstrate favorable properties for the optimization problem that may enable\nsimilar results to continue to hold more globally (over the entire ambient\nspace). Collectively these two papers utilize and develop powerful tools for\nuniform convergence of empirical processes that may have broader implications\nfor rigorous understanding of constrained nonconvex optimization heuristics.\nThe mathematical results in this paper also pave the way for a new generation\nof data-driven phase-less imaging systems that can utilize prior information to\nsignificantly reduce acquisition time and enhance image reconstruction,\nenabling nano-scale imaging at unprecedented speeds and resolutions.\n", "versions": [{"version": "v1", "created": "Mon, 20 Feb 2017 21:04:36 GMT"}], "update_date": "2017-02-22", "authors_parsed": [["Soltanolkotabi", "Mahdi", ""]]}, {"id": "1702.06209", "submitter": "Mladen Kolar", "authors": "Jelena Bradic and Mladen Kolar", "title": "Uniform Inference for High-dimensional Quantile Regression: Linear\n  Functionals and Regression Rank Scores", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Hypothesis tests in models whose dimension far exceeds the sample size can be\nformulated much like the classical studentized tests only after the initial\nbias of estimation is removed successfully. The theory of debiased estimators\ncan be developed in the context of quantile regression models for a fixed\nquantile value. However, it is frequently desirable to formulate tests based on\nthe quantile regression process, as this leads to more robust tests and more\nstable confidence sets. Additionally, inference in quantile regression requires\nestimation of the so called sparsity function, which depends on the unknown\ndensity of the error. In this paper we consider a debiasing approach for the\nuniform testing problem. We develop high-dimensional regression rank scores and\nshow how to use them to estimate the sparsity function, as well as how to adapt\nthem for inference involving the quantile regression process. Furthermore, we\ndevelop a Kolmogorov-Smirnov test in a location-shift high-dimensional models\nand confidence sets that are uniformly valid for many quantile values. The main\ntechnical result are the development of a Bahadur representation of the\ndebiasing estimator that is uniform over a range of quantiles and uniform\nconvergence of the quantile process to the Brownian bridge process, which are\nof independent interest. Simulation studies illustrate finite sample properties\nof our procedure.\n", "versions": [{"version": "v1", "created": "Mon, 20 Feb 2017 23:38:40 GMT"}], "update_date": "2017-02-22", "authors_parsed": [["Bradic", "Jelena", ""], ["Kolar", "Mladen", ""]]}, {"id": "1702.06216", "submitter": "Michael Bloodgood", "authors": "Alan Mishler, Kevin Wonus, Wendy Chambers and Michael Bloodgood", "title": "Filtering Tweets for Social Unrest", "comments": "7 pages, 8 figures, 3 tables; published in Proceedings of the 2017\n  IEEE 11th International Conference on Semantic Computing (ICSC), San Diego,\n  CA, USA, pages 17-23, January 2017", "journal-ref": "In Proceedings of the 2017 IEEE 11th International Conference on\n  Semantic Computing (ICSC), pages 17-23, San Diego, CA, USA, January 2017.\n  IEEE", "doi": "10.1109/ICSC.2017.75", "report-no": null, "categories": "cs.CL cs.IR cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Since the events of the Arab Spring, there has been increased interest in\nusing social media to anticipate social unrest. While efforts have been made\ntoward automated unrest prediction, we focus on filtering the vast volume of\ntweets to identify tweets relevant to unrest, which can be provided to\ndownstream users for further analysis. We train a supervised classifier that is\nable to label Arabic language tweets as relevant to unrest with high\nreliability. We examine the relationship between training data size and\nperformance and investigate ways to optimize the model building process while\nminimizing cost. We also explore how confidence thresholds can be set to\nachieve desired levels of performance.\n", "versions": [{"version": "v1", "created": "Mon, 20 Feb 2017 23:48:39 GMT"}, {"version": "v2", "created": "Sat, 1 Apr 2017 22:37:35 GMT"}], "update_date": "2017-04-04", "authors_parsed": [["Mishler", "Alan", ""], ["Wonus", "Kevin", ""], ["Chambers", "Wendy", ""], ["Bloodgood", "Michael", ""]]}, {"id": "1702.06219", "submitter": "Shahin Shahrampour", "authors": "Shahin Shahrampour, Ali Jadbabaie", "title": "An Online Optimization Approach for Multi-Agent Tracking of Dynamic\n  Parameters in the Presence of Adversarial Noise", "comments": "8 pages, To appear in American Control Conference 2017", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.OC cs.MA stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper addresses tracking of a moving target in a multi-agent network.\nThe target follows a linear dynamics corrupted by an adversarial noise, i.e.,\nthe noise is not generated from a statistical distribution. The location of the\ntarget at each time induces a global time-varying loss function, and the global\nloss is a sum of local losses, each of which is associated to one agent. Agents\nnoisy observations could be nonlinear. We formulate this problem as a\ndistributed online optimization where agents communicate with each other to\ntrack the minimizer of the global loss. We then propose a decentralized version\nof the Mirror Descent algorithm and provide the non-asymptotic analysis of the\nproblem. Using the notion of dynamic regret, we measure the performance of our\nalgorithm versus its offline counterpart in the centralized setting. We prove\nthat the bound on dynamic regret scales inversely in the network spectral gap,\nand it represents the adversarial noise causing deviation with respect to the\nlinear dynamics. Our result subsumes a number of results in the distributed\noptimization literature. Finally, in a numerical experiment, we verify that our\nalgorithm can be simply implemented for multi-agent tracking with nonlinear\nobservations.\n", "versions": [{"version": "v1", "created": "Tue, 21 Feb 2017 00:18:14 GMT"}], "update_date": "2017-02-22", "authors_parsed": [["Shahrampour", "Shahin", ""], ["Jadbabaie", "Ali", ""]]}, {"id": "1702.06234", "submitter": "Seyoon Ko", "authors": "Seyoon Ko, Donghyeon Yu, Joong-Ho Won", "title": "Easily parallelizable and distributable class of algorithms for\n  structured sparsity, with optimal acceleration", "comments": "57 pages (30 pages excluding appendix), 4 figures (2 excluding\n  appendix)", "journal-ref": "Journal of Computational and Graphical Statistics 28.4 (2019):\n  pp.821-833", "doi": "10.1080/10618600.2019.1592757", "report-no": null, "categories": "stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Many statistical learning problems can be posed as minimization of a sum of\ntwo convex functions, one typically a composition of non-smooth and linear\nfunctions. Examples include regression under structured sparsity assumptions.\nPopular algorithms for solving such problems, e.g., ADMM, often involve\nnon-trivial optimization subproblems or smoothing approximation. We consider\ntwo classes of primal-dual algorithms that do not incur these difficulties, and\nunify them from a perspective of monotone operator theory. From this\nunification we propose a continuum of preconditioned forward-backward operator\nsplitting algorithms amenable to parallel and distributed computing. For the\nentire region of convergence of the whole continuum of algorithms, we establish\nits rates of convergence. For some known instances of this continuum, our\nanalysis closes the gap in theory. We further exploit the unification to\npropose a continuum of accelerated algorithms. We show that the whole continuum\nattains the theoretically optimal rate of convergence. The scalability of the\nproposed algorithms, as well as their convergence behavior, is demonstrated up\nto 1.2 million variables with a distributed implementation.\n", "versions": [{"version": "v1", "created": "Tue, 21 Feb 2017 01:25:51 GMT"}, {"version": "v2", "created": "Fri, 27 Oct 2017 05:54:12 GMT"}, {"version": "v3", "created": "Tue, 19 Jun 2018 08:18:45 GMT"}], "update_date": "2021-07-19", "authors_parsed": [["Ko", "Seyoon", ""], ["Yu", "Donghyeon", ""], ["Won", "Joong-Ho", ""]]}, {"id": "1702.06237", "submitter": "David Steurer", "authors": "Aaron Potechin, David Steurer", "title": "Exact tensor completion with sum-of-squares", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CC cs.DS cs.IT math.IT stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We obtain the first polynomial-time algorithm for exact tensor completion\nthat improves over the bound implied by reduction to matrix completion. The\nalgorithm recovers an unknown 3-tensor with $r$ incoherent, orthogonal\ncomponents in $\\mathbb R^n$ from $r\\cdot \\tilde O(n^{1.5})$ randomly observed\nentries of the tensor. This bound improves over the previous best one of\n$r\\cdot \\tilde O(n^{2})$ by reduction to exact matrix completion. Our bound\nalso matches the best known results for the easier problem of approximate\ntensor completion (Barak & Moitra, 2015).\n  Our algorithm and analysis extends seminal results for exact matrix\ncompletion (Candes & Recht, 2009) to the tensor setting via the sum-of-squares\nmethod. The main technical challenge is to show that a small number of randomly\nchosen monomials are enough to construct a degree-3 polynomial with precisely\nplanted orthogonal global optima over the sphere and that this fact can be\ncertified within the sum-of-squares proof system.\n", "versions": [{"version": "v1", "created": "Tue, 21 Feb 2017 02:14:31 GMT"}, {"version": "v2", "created": "Thu, 23 Feb 2017 02:48:22 GMT"}, {"version": "v3", "created": "Fri, 23 Jun 2017 18:24:50 GMT"}], "update_date": "2017-06-27", "authors_parsed": [["Potechin", "Aaron", ""], ["Steurer", "David", ""]]}, {"id": "1702.06240", "submitter": "Vira Semenova", "authors": "Vira Semenova, Victor Chernozhukov", "title": "Debiased Machine Learning of Conditional Average Treatment Effects and\n  Other Causal Functions", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper provides estimation and inference methods for the best linear\npredictor (approximation) of a structural function, such as conditional average\nstructural and treatment effects, and structural derivatives, based on modern\nmachine learning (ML) tools. We represent this structural function as a\nconditional expectation of an unbiased signal that depends on a nuisance\nparameter, which we estimate by modern machine learning techniques. We first\nadjust the signal to make it insensitive (Neyman-orthogonal) with respect to\nthe first-stage regularization bias. We then project the signal onto a set of\nbasis functions, growing with sample size, which gives us the best linear\npredictor of the structural function. We derive a complete set of results for\nestimation and simultaneous inference on all parameters of the best linear\npredictor, conducting inference by Gaussian bootstrap. When the structural\nfunction is smooth and the basis is sufficiently rich, our estimation and\ninference result automatically targets this function. When basis functions are\ngroup indicators, the best linear predictor reduces to group average\ntreatment/structural effect, and our inference automatically targets these\nparameters. We demonstrate our method by estimating uniform confidence bands\nfor the average price elasticity of gasoline demand conditional on income.\n", "versions": [{"version": "v1", "created": "Tue, 21 Feb 2017 02:23:41 GMT"}, {"version": "v2", "created": "Tue, 19 Jun 2018 15:38:42 GMT"}, {"version": "v3", "created": "Mon, 3 Feb 2020 19:46:22 GMT"}, {"version": "v4", "created": "Sun, 3 May 2020 03:07:49 GMT"}, {"version": "v5", "created": "Thu, 13 Aug 2020 16:25:06 GMT"}, {"version": "v6", "created": "Fri, 14 Aug 2020 14:11:05 GMT"}], "update_date": "2020-08-17", "authors_parsed": [["Semenova", "Vira", ""], ["Chernozhukov", "Victor", ""]]}, {"id": "1702.06278", "submitter": "Shahar Mendelson", "authors": "Shahar Mendelson", "title": "Column normalization of a random measurement matrix", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this note we answer a question of G. Lecu\\'{e}, by showing that column\nnormalization of a random matrix with iid entries need not lead to good sparse\nrecovery properties, even if the generating random variable has a reasonable\nmoment growth. Specifically, for every $2 \\leq p \\leq c_1\\log d$ we construct a\nrandom vector $X \\in R^d$ with iid, mean-zero, variance $1$ coordinates, that\nsatisfies $\\sup_{t \\in S^{d-1}} \\|<X,t>\\|_{L_q} \\leq c_2\\sqrt{q}$ for every\n$2\\leq q \\leq p$.\n  We show that if $m \\leq c_3\\sqrt{p}d^{1/p}$ and $\\tilde{\\Gamma}:R^d \\to R^m$\nis the column-normalized matrix generated by $m$ independent copies of $X$,\nthen with probability at least $1-2\\exp(-c_4m)$, $\\tilde{\\Gamma}$ does not\nsatisfy the exact reconstruction property of order $2$.\n", "versions": [{"version": "v1", "created": "Tue, 21 Feb 2017 06:42:28 GMT"}], "update_date": "2017-02-22", "authors_parsed": [["Mendelson", "Shahar", ""]]}, {"id": "1702.06280", "submitter": "Kathrin Grosse", "authors": "Kathrin Grosse, Praveen Manoharan, Nicolas Papernot, Michael Backes,\n  Patrick McDaniel", "title": "On the (Statistical) Detection of Adversarial Examples", "comments": "13 pages, 4 figures, 5 tables. New version: improved writing,\n  incorporating external feedback", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CR cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Machine Learning (ML) models are applied in a variety of tasks such as\nnetwork intrusion detection or Malware classification. Yet, these models are\nvulnerable to a class of malicious inputs known as adversarial examples. These\nare slightly perturbed inputs that are classified incorrectly by the ML model.\nThe mitigation of these adversarial inputs remains an open problem. As a step\ntowards understanding adversarial examples, we show that they are not drawn\nfrom the same distribution than the original data, and can thus be detected\nusing statistical tests. Using thus knowledge, we introduce a complimentary\napproach to identify specific inputs that are adversarial. Specifically, we\naugment our ML model with an additional output, in which the model is trained\nto classify all adversarial inputs. We evaluate our approach on multiple\nadversarial example crafting methods (including the fast gradient sign and\nsaliency map methods) with several datasets. The statistical test flags sample\nsets containing adversarial inputs confidently at sample sizes between 10 and\n100 data points. Furthermore, our augmented model either detects adversarial\nexamples as outliers with high accuracy (> 80%) or increases the adversary's\ncost - the perturbation added - by more than 150%. In this way, we show that\nstatistical properties of adversarial examples are essential to their\ndetection.\n", "versions": [{"version": "v1", "created": "Tue, 21 Feb 2017 07:03:11 GMT"}, {"version": "v2", "created": "Tue, 17 Oct 2017 11:12:44 GMT"}], "update_date": "2017-10-18", "authors_parsed": [["Grosse", "Kathrin", ""], ["Manoharan", "Praveen", ""], ["Papernot", "Nicolas", ""], ["Backes", "Michael", ""], ["McDaniel", "Patrick", ""]]}, {"id": "1702.06295", "submitter": "Armen Aghajanyan", "authors": "Armen Aghajanyan", "title": "Convolution Aware Initialization", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Initialization of parameters in deep neural networks has been shown to have a\nbig impact on the performance of the networks (Mishkin & Matas, 2015). The\ninitialization scheme devised by He et al, allowed convolution activations to\ncarry a constrained mean which allowed deep networks to be trained effectively\n(He et al., 2015a). Orthogonal initializations and more generally orthogonal\nmatrices in standard recurrent networks have been proved to eradicate the\nvanishing and exploding gradient problem (Pascanu et al., 2012). Majority of\ncurrent initialization schemes do not take fully into account the intrinsic\nstructure of the convolution operator. Using the duality of the Fourier\ntransform and the convolution operator, Convolution Aware Initialization builds\northogonal filters in the Fourier space, and using the inverse Fourier\ntransform represents them in the standard space. With Convolution Aware\nInitialization we noticed not only higher accuracy and lower loss, but faster\nconvergence. We achieve new state of the art on the CIFAR10 dataset, and\nachieve close to state of the art on various other tasks.\n", "versions": [{"version": "v1", "created": "Tue, 21 Feb 2017 09:01:46 GMT"}, {"version": "v2", "created": "Thu, 23 Feb 2017 06:00:34 GMT"}, {"version": "v3", "created": "Mon, 27 Feb 2017 17:38:58 GMT"}], "update_date": "2017-02-28", "authors_parsed": [["Aghajanyan", "Armen", ""]]}, {"id": "1702.06341", "submitter": "Gergely Neu", "authors": "Gergely Neu and Vicen\\c{c} G\\'omez", "title": "Fast rates for online learning in Linearly Solvable Markov Decision\n  Processes", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG math.OC stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study the problem of online learning in a class of Markov decision\nprocesses known as linearly solvable MDPs. In the stationary version of this\nproblem, a learner interacts with its environment by directly controlling the\nstate transitions, attempting to balance a fixed state-dependent cost and a\ncertain smooth cost penalizing extreme control inputs. In the current paper, we\nconsider an online setting where the state costs may change arbitrarily between\nconsecutive rounds, and the learner only observes the costs at the end of each\nrespective round. We are interested in constructing algorithms for the learner\nthat guarantee small regret against the best stationary control policy chosen\nin full knowledge of the cost sequence. Our main result is showing that the\nsmoothness of the control cost enables the simple algorithm of following the\nleader to achieve a regret of order $\\log^2 T$ after $T$ rounds, vastly\nimproving on the best known regret bound of order $T^{3/4}$ for this setting.\n", "versions": [{"version": "v1", "created": "Tue, 21 Feb 2017 11:56:33 GMT"}, {"version": "v2", "created": "Tue, 6 Jun 2017 11:01:30 GMT"}], "update_date": "2017-06-07", "authors_parsed": [["Neu", "Gergely", ""], ["G\u00f3mez", "Vicen\u00e7", ""]]}, {"id": "1702.06354", "submitter": "Makoto Yamada", "authors": "Makoto Yamada, Song Liu, Samuel Kaski", "title": "Interpreting Outliers: Localized Logistic Regression for Density Ratio\n  Estimation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose an inlier-based outlier detection method capable of both\nidentifying the outliers and explaining why they are outliers, by identifying\nthe outlier-specific features. Specifically, we employ an inlier-based outlier\ndetection criterion, which uses the ratio of inlier and test probability\ndensities as a measure of plausibility of being an outlier. For estimating the\ndensity ratio function, we propose a localized logistic regression algorithm.\nThanks to the locality of the model, variable selection can be\noutlier-specific, and will help interpret why points are outliers in a\nhigh-dimensional space. Through synthetic experiments, we show that the\nproposed algorithm can successfully detect the important features for outliers.\nMoreover, we show that the proposed algorithm tends to outperform existing\nalgorithms in benchmark datasets.\n", "versions": [{"version": "v1", "created": "Tue, 21 Feb 2017 12:37:35 GMT"}], "update_date": "2017-02-22", "authors_parsed": [["Yamada", "Makoto", ""], ["Liu", "Song", ""], ["Kaski", "Samuel", ""]]}, {"id": "1702.06385", "submitter": "Alexander Marx", "authors": "Alexander Marx and Jilles Vreeken", "title": "Causal Inference on Multivariate and Mixed-Type Data", "comments": "9 pages, submitted to sdm", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Given data over the joint distribution of two random variables $X$ and $Y$,\nwe consider the problem of inferring the most likely causal direction between\n$X$ and $Y$. In particular, we consider the general case where both $X$ and $Y$\nmay be univariate or multivariate, and of the same or mixed data types. We take\nan information theoretic approach, based on Kolmogorov complexity, from which\nit follows that first describing the data over cause and then that of effect\ngiven cause is shorter than the reverse direction.\n  The ideal score is not computable, but can be approximated through the\nMinimum Description Length (MDL) principle. Based on MDL, we propose two\nscores, one for when both $X$ and $Y$ are of the same single data type, and one\nfor when they are mixed-type. We model dependencies between $X$ and $Y$ using\nclassification and regression trees. As inferring the optimal model is NP-hard,\nwe propose Crack, a fast greedy algorithm to determine the most likely causal\ndirection directly from the data.\n  Empirical evaluation on a wide range of data shows that Crack reliably, and\nwith high accuracy, infers the correct causal direction on both univariate and\nmultivariate cause-effect pairs over both single and mixed-type data.\n", "versions": [{"version": "v1", "created": "Tue, 21 Feb 2017 13:59:23 GMT"}, {"version": "v2", "created": "Mon, 16 Oct 2017 12:18:45 GMT"}], "update_date": "2017-10-17", "authors_parsed": [["Marx", "Alexander", ""], ["Vreeken", "Jilles", ""]]}, {"id": "1702.06429", "submitter": "Nicolas Flammarion", "authors": "Nicolas Flammarion (LIENS, SIERRA), Francis Bach (LIENS, SIERRA)", "title": "Stochastic Composite Least-Squares Regression with convergence rate\n  O(1/n)", "comments": null, "journal-ref": null, "doi": null, "report-no": "hal-01472867", "categories": "math.OC stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider the minimization of composite objective functions composed of the\nexpectation of quadratic functions and an arbitrary convex function. We study\nthe stochastic dual averaging algorithm with a constant step-size, showing that\nit leads to a convergence rate of O(1/n) without strong convexity assumptions.\nThis thus extends earlier results on least-squares regression with the\nEuclidean geometry to (a) all convex regularizers and constraints, and (b) all\ngeome-tries represented by a Bregman divergence. This is achieved by a new\nproof technique that relates stochastic and deterministic recursions.\n", "versions": [{"version": "v1", "created": "Tue, 21 Feb 2017 15:05:36 GMT"}], "update_date": "2017-02-22", "authors_parsed": [["Flammarion", "Nicolas", "", "LIENS, SIERRA"], ["Bach", "Francis", "", "LIENS, SIERRA"]]}, {"id": "1702.06435", "submitter": "Yue Lu", "authors": "Yue M. Lu and Gen Li", "title": "Phase Transitions of Spectral Initialization for High-Dimensional\n  Nonconvex Estimation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IT math.IT stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study a spectral initialization method that serves a key role in recent\nwork on estimating signals in nonconvex settings. Previous analysis of this\nmethod focuses on the phase retrieval problem and provides only performance\nbounds. In this paper, we consider arbitrary generalized linear sensing models\nand present a precise asymptotic characterization of the performance of the\nmethod in the high-dimensional limit. Our analysis also reveals a phase\ntransition phenomenon that depends on the ratio between the number of samples\nand the signal dimension. When the ratio is below a minimum threshold, the\nestimates given by the spectral method are no better than random guesses drawn\nfrom a uniform distribution on the hypersphere, thus carrying no information;\nabove a maximum threshold, the estimates become increasingly aligned with the\ntarget signal. The computational complexity of the method, as measured by the\nspectral gap, is also markedly different in the two phases. Worked examples and\nnumerical results are provided to illustrate and verify the analytical\npredictions. In particular, simulations show that our asymptotic formulas\nprovide accurate predictions for the actual performance of the spectral method\neven at moderate signal dimensions.\n", "versions": [{"version": "v1", "created": "Tue, 21 Feb 2017 15:19:29 GMT"}, {"version": "v2", "created": "Tue, 11 Apr 2017 16:32:01 GMT"}, {"version": "v3", "created": "Sun, 21 Jul 2019 18:08:48 GMT"}], "update_date": "2019-07-23", "authors_parsed": [["Lu", "Yue M.", ""], ["Li", "Gen", ""]]}, {"id": "1702.06457", "submitter": "Martin Jaggi", "authors": "Francesco Locatello, Rajiv Khanna, Michael Tschannen, Martin Jaggi", "title": "A Unified Optimization View on Generalized Matching Pursuit and\n  Frank-Wolfe", "comments": "appearing at AISTATS 2017", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Two of the most fundamental prototypes of greedy optimization are the\nmatching pursuit and Frank-Wolfe algorithms. In this paper, we take a unified\nview on both classes of methods, leading to the first explicit convergence\nrates of matching pursuit methods in an optimization sense, for general sets of\natoms. We derive sublinear ($1/t$) convergence for both classes on general\nsmooth objectives, and linear convergence on strongly convex objectives, as\nwell as a clear correspondence of algorithm variants. Our presented algorithms\nand rates are affine invariant, and do not need any incoherence or sparsity\nassumptions.\n", "versions": [{"version": "v1", "created": "Tue, 21 Feb 2017 16:04:10 GMT"}, {"version": "v2", "created": "Tue, 7 Mar 2017 18:49:43 GMT"}], "update_date": "2017-03-08", "authors_parsed": [["Locatello", "Francesco", ""], ["Khanna", "Rajiv", ""], ["Tschannen", "Michael", ""], ["Jaggi", "Martin", ""]]}, {"id": "1702.06516", "submitter": "Alan Wisler", "authors": "Alan Wisler, Visar Berisha, Andreas Spanias, Alfred O. Hero", "title": "Direct estimation of density functionals using a polynomial basis", "comments": "Under review for IEEE Transactions on Signal Processing", "journal-ref": null, "doi": "10.1109/TSP.2017.2775587", "report-no": null, "categories": "cs.IT math.IT stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A number of fundamental quantities in statistical signal processing and\ninformation theory can be expressed as integral functions of two probability\ndensity functions. Such quantities are called density functionals as they map\ndensity functions onto the real line. For example, information divergence\nfunctions measure the dissimilarity between two probability density functions\nand are useful in a number of applications. Typically, estimating these\nquantities requires complete knowledge of the underlying distribution followed\nby multi-dimensional integration. Existing methods make parametric assumptions\nabout the data distribution or use non-parametric density estimation followed\nby high-dimensional integration. In this paper, we propose a new alternative.\nWe introduce the concept of \"data-driven basis functions\" - functions of\ndistributions whose value we can estimate given only samples from the\nunderlying distributions without requiring distribution fitting or direct\nintegration. We derive a new data-driven complete basis that is similar to the\ndeterministic Bernstein polynomial basis and develop two methods for performing\nbasis expansions of functionals of two distributions. We also show that the new\nbasis set allows us to approximate functions of distributions as closely as\ndesired. Finally, we evaluate the methodology by developing data driven\nestimators for the Kullback-Leibler divergences and the Hellinger distance and\nby constructing empirical estimates of tight bounds on the Bayes error rate.\n", "versions": [{"version": "v1", "created": "Tue, 21 Feb 2017 18:39:54 GMT"}, {"version": "v2", "created": "Mon, 20 Nov 2017 21:05:38 GMT"}], "update_date": "2018-02-14", "authors_parsed": [["Wisler", "Alan", ""], ["Berisha", "Visar", ""], ["Spanias", "Andreas", ""], ["Hero", "Alfred O.", ""]]}, {"id": "1702.06525", "submitter": "Quanquan Gu", "authors": "Xiao Zhang and Lingxiao Wang and Quanquan Gu", "title": "A Unified Framework for Low-Rank plus Sparse Matrix Recovery", "comments": "49 pages, 3 figures, 3 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a unified framework to solve general low-rank plus sparse matrix\nrecovery problems based on matrix factorization, which covers a broad family of\nobjective functions satisfying the restricted strong convexity and smoothness\nconditions. Based on projected gradient descent and the double thresholding\noperator, our proposed generic algorithm is guaranteed to converge to the\nunknown low-rank and sparse matrices at a locally linear rate, while matching\nthe best-known robustness guarantee (i.e., tolerance for sparsity). At the core\nof our theory is a novel structural Lipschitz gradient condition for low-rank\nplus sparse matrices, which is essential for proving the linear convergence\nrate of our algorithm, and we believe is of independent interest to prove fast\nrates for general superposition-structured models. We illustrate the\napplication of our framework through two concrete examples: robust matrix\nsensing and robust PCA. Experiments on both synthetic and real datasets\ncorroborate our theory.\n", "versions": [{"version": "v1", "created": "Tue, 21 Feb 2017 18:56:42 GMT"}, {"version": "v2", "created": "Thu, 13 Apr 2017 16:53:38 GMT"}, {"version": "v3", "created": "Tue, 20 Feb 2018 16:55:15 GMT"}], "update_date": "2018-02-21", "authors_parsed": [["Zhang", "Xiao", ""], ["Wang", "Lingxiao", ""], ["Gu", "Quanquan", ""]]}, {"id": "1702.06533", "submitter": "Weiran Wang", "authors": "Chao Gao, Dan Garber, Nathan Srebro, Jialei Wang, Weiran Wang", "title": "Stochastic Canonical Correlation Analysis", "comments": "Accepted by JMLR", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study the sample complexity of canonical correlation analysis (CCA), \\ie,\nthe number of samples needed to estimate the population canonical correlation\nand directions up to arbitrarily small error. With mild assumptions on the data\ndistribution, we show that in order to achieve $\\epsilon$-suboptimality in a\nproperly defined measure of alignment between the estimated canonical\ndirections and the population solution, we can solve the empirical objective\nexactly with $N(\\epsilon, \\Delta, \\gamma)$ samples, where $\\Delta$ is the\nsingular value gap of the whitened cross-covariance matrix and $1/\\gamma$ is an\nupper bound of the condition number of auto-covariance matrices. Moreover, we\ncan achieve the same learning accuracy by drawing the same level of samples and\nsolving the empirical objective approximately with a stochastic optimization\nalgorithm; this algorithm is based on the shift-and-invert power iterations and\nonly needs to process the dataset for $\\mathcal{O}\\left(\\log \\frac{1}{\\epsilon}\n\\right)$ passes. Finally, we show that, given an estimate of the canonical\ncorrelation, the streaming version of the shift-and-invert power iterations\nachieves the same learning accuracy with the same level of sample complexity,\nby processing the data only once.\n", "versions": [{"version": "v1", "created": "Tue, 21 Feb 2017 04:23:41 GMT"}, {"version": "v2", "created": "Mon, 21 Oct 2019 16:21:26 GMT"}], "update_date": "2019-10-22", "authors_parsed": [["Gao", "Chao", ""], ["Garber", "Dan", ""], ["Srebro", "Nathan", ""], ["Wang", "Jialei", ""], ["Wang", "Weiran", ""]]}, {"id": "1702.06602", "submitter": "Hongyu Guo", "authors": "Martin Renqiang Min, Hongyu Guo, Dongjin Song", "title": "Exemplar-Centered Supervised Shallow Parametric Data Embedding", "comments": "accepted to IJCAI2017", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Metric learning methods for dimensionality reduction in combination with\nk-Nearest Neighbors (kNN) have been extensively deployed in many\nclassification, data embedding, and information retrieval applications.\nHowever, most of these approaches involve pairwise training data comparisons,\nand thus have quadratic computational complexity with respect to the size of\ntraining set, preventing them from scaling to fairly big datasets. Moreover,\nduring testing, comparing test data against all the training data points is\nalso expensive in terms of both computational cost and resources required.\nFurthermore, previous metrics are either too constrained or too expressive to\nbe well learned. To effectively solve these issues, we present an\nexemplar-centered supervised shallow parametric data embedding model, using a\nMaximally Collapsing Metric Learning (MCML) objective. Our strategy learns a\nshallow high-order parametric embedding function and compares training/test\ndata only with learned or precomputed exemplars, resulting in a cost function\nwith linear computational complexity for both training and testing. We also\nempirically demonstrate, using several benchmark datasets, that for\nclassification in two-dimensional embedding space, our approach not only gains\nspeedup of kNN by hundreds of times, but also outperforms state-of-the-art\nsupervised embedding approaches.\n", "versions": [{"version": "v1", "created": "Tue, 21 Feb 2017 22:05:13 GMT"}, {"version": "v2", "created": "Wed, 5 Jul 2017 19:19:11 GMT"}], "update_date": "2017-07-07", "authors_parsed": [["Min", "Martin Renqiang", ""], ["Guo", "Hongyu", ""], ["Song", "Dongjin", ""]]}, {"id": "1702.06661", "submitter": "Meisam Hejazinia", "authors": "Meisam Hejazi Nia, Brian T. Ratchford, Norris Bruce", "title": "Social Learning and Diffusion of Pervasive Goods: An Empirical Study of\n  an African App Store", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.SI stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this study, the authors develop a structural model that combines a macro\ndiffusion model with a micro choice model to control for the effect of social\ninfluence on the mobile app choices of customers over app stores. Social\ninfluence refers to the density of adopters within the proximity of other\ncustomers. Using a large data set from an African app store and Bayesian\nestimation methods, the authors quantify the effect of social influence and\ninvestigate the impact of ignoring this process in estimating customer choices.\nThe findings show that customer choices in the app store are explained better\nby offline than online density of adopters and that ignoring social influence\nin estimations results in biased estimates. Furthermore, the findings show that\nthe mobile app adoption process is similar to adoption of music CDs, among all\nother classic economy goods. A counterfactual analysis shows that the app store\ncan increase its revenue by 13.6% through a viral marketing policy (e.g., a\nsharing with friends and family button).\n", "versions": [{"version": "v1", "created": "Wed, 22 Feb 2017 03:10:41 GMT"}], "update_date": "2017-02-23", "authors_parsed": [["Nia", "Meisam Hejazi", ""], ["Ratchford", "Brian T.", ""], ["Bruce", "Norris", ""]]}, {"id": "1702.06676", "submitter": "Nicholas Guttenberg", "authors": "Nicholas Guttenberg, Yen Yu, Ryota Kanai", "title": "Counterfactual Control for Free from Generative Models", "comments": "6 pages, 5 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We introduce a method by which a generative model learning the joint\ndistribution between actions and future states can be used to automatically\ninfer a control scheme for any desired reward function, which may be altered on\nthe fly without retraining the model. In this method, the problem of action\nselection is reduced to one of gradient descent on the latent space of the\ngenerative model, with the model itself providing the means of evaluating\noutcomes and finding the gradient, much like how the reward network in Deep\nQ-Networks (DQN) provides gradient information for the action generator. Unlike\nDQN or Actor-Critic, which are conditional models for a specific reward, using\na generative model of the full joint distribution permits the reward to be\nchanged on the fly. In addition, the generated futures can be inspected to gain\ninsight in to what the network 'thinks' will happen, and to what went wrong\nwhen the outcomes deviate from prediction.\n", "versions": [{"version": "v1", "created": "Wed, 22 Feb 2017 04:50:47 GMT"}, {"version": "v2", "created": "Thu, 9 Mar 2017 06:35:45 GMT"}], "update_date": "2017-03-10", "authors_parsed": [["Guttenberg", "Nicholas", ""], ["Yu", "Yen", ""], ["Kanai", "Ryota", ""]]}, {"id": "1702.06760", "submitter": "Yanjun  Qi Dr.", "authors": "Jack Lanchantin, Ritambhara Singh, Yanjun Qi", "title": "Memory Matching Networks for Genomic Sequence Classification", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG q-bio.GN stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  When analyzing the genome, researchers have discovered that proteins bind to\nDNA based on certain patterns of the DNA sequence known as \"motifs\". However,\nit is difficult to manually construct motifs due to their complexity. Recently,\nexternally learned memory models have proven to be effective methods for\nreasoning over inputs and supporting sets. In this work, we present memory\nmatching networks (MMN) for classifying DNA sequences as protein binding sites.\nOur model learns a memory bank of encoded motifs, which are dynamic memory\nmodules, and then matches a new test sequence to each of the motifs to classify\nthe sequence as a binding or nonbinding site.\n", "versions": [{"version": "v1", "created": "Wed, 22 Feb 2017 11:37:49 GMT"}], "update_date": "2017-02-23", "authors_parsed": [["Lanchantin", "Jack", ""], ["Singh", "Ritambhara", ""], ["Qi", "Yanjun", ""]]}, {"id": "1702.06818", "submitter": "Poorya Mianjy", "authors": "Raman Arora and Teodor V. Marinov and Poorya Mianjy and Nathan Srebro", "title": "Stochastic Approximation for Canonical Correlation Analysis", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose novel first-order stochastic approximation algorithms for\ncanonical correlation analysis (CCA). Algorithms presented are instances of\ninexact matrix stochastic gradient (MSG) and inexact matrix exponentiated\ngradient (MEG), and achieve $\\epsilon$-suboptimality in the population\nobjective in $\\operatorname{poly}(\\frac{1}{\\epsilon})$ iterations. We also\nconsider practical variants of the proposed algorithms and compare them with\nother methods for CCA both theoretically and empirically.\n", "versions": [{"version": "v1", "created": "Wed, 22 Feb 2017 14:45:02 GMT"}, {"version": "v2", "created": "Mon, 26 Feb 2018 16:52:53 GMT"}], "update_date": "2018-02-27", "authors_parsed": [["Arora", "Raman", ""], ["Marinov", "Teodor V.", ""], ["Mianjy", "Poorya", ""], ["Srebro", "Nathan", ""]]}, {"id": "1702.06819", "submitter": "Mohammad Raihanul Islam", "authors": "Mohammad Raihanul Islam and B. Aditya Prakash and Naren Ramakrishnan", "title": "Distributed Representations of Signed Networks", "comments": "Published in PAKDD 2018", "journal-ref": null, "doi": "10.1007/978-3-319-93037-4_13", "report-no": null, "categories": "stat.ML cs.LG cs.SI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recent successes in word embedding and document embedding have motivated\nresearchers to explore similar representations for networks and to use such\nrepresentations for tasks such as edge prediction, node label prediction, and\ncommunity detection. Such network embedding methods are largely focused on\nfinding distributed representations for unsigned networks and are unable to\ndiscover embeddings that respect polarities inherent in edges. We propose\nSIGNet, a fast scalable embedding method suitable for signed networks. Our\nproposed objective function aims to carefully model the social structure\nimplicit in signed networks by reinforcing the principles of social balance\ntheory. Our method builds upon the traditional word2vec family of embedding\napproaches and adds a new targeted node sampling strategy to maintain\nstructural balance in higher-order neighborhoods. We demonstrate the\nsuperiority of SIGNet over state-of-the-art methods proposed for both signed\nand unsigned networks on several real world datasets from different domains. In\nparticular, SIGNet offers an approach to generate a richer vocabulary of\nfeatures of signed networks to support representation and reasoning.\n", "versions": [{"version": "v1", "created": "Wed, 22 Feb 2017 14:51:48 GMT"}, {"version": "v2", "created": "Sat, 27 May 2017 02:51:37 GMT"}, {"version": "v3", "created": "Sat, 3 Jun 2017 15:31:59 GMT"}, {"version": "v4", "created": "Thu, 22 Mar 2018 17:23:06 GMT"}, {"version": "v5", "created": "Mon, 8 Apr 2019 01:30:49 GMT"}], "update_date": "2019-04-09", "authors_parsed": [["Islam", "Mohammad Raihanul", ""], ["Prakash", "B. Aditya", ""], ["Ramakrishnan", "Naren", ""]]}, {"id": "1702.06832", "submitter": "Jernej Kos", "authors": "Jernej Kos, Ian Fischer, Dawn Song", "title": "Adversarial examples for generative models", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We explore methods of producing adversarial examples on deep generative\nmodels such as the variational autoencoder (VAE) and the VAE-GAN. Deep learning\narchitectures are known to be vulnerable to adversarial examples, but previous\nwork has focused on the application of adversarial examples to classification\ntasks. Deep generative models have recently become popular due to their ability\nto model input data distributions and generate realistic examples from those\ndistributions. We present three classes of attacks on the VAE and VAE-GAN\narchitectures and demonstrate them against networks trained on MNIST, SVHN and\nCelebA. Our first attack leverages classification-based adversaries by\nattaching a classifier to the trained encoder of the target generative model,\nwhich can then be used to indirectly manipulate the latent representation. Our\nsecond attack directly uses the VAE loss function to generate a target\nreconstruction image from the adversarial example. Our third attack moves\nbeyond relying on classification or the standard loss for the gradient and\ndirectly optimizes against differences in source and target latent\nrepresentations. We also motivate why an attacker might be interested in\ndeploying such techniques against a target generative network.\n", "versions": [{"version": "v1", "created": "Wed, 22 Feb 2017 15:11:25 GMT"}], "update_date": "2017-02-23", "authors_parsed": [["Kos", "Jernej", ""], ["Fischer", "Ian", ""], ["Song", "Dawn", ""]]}, {"id": "1702.06838", "submitter": "Alp Yurtsever", "authors": "Alp Yurtsever and Madeleine Udell and Joel A. Tropp and Volkan Cevher", "title": "Sketchy Decisions: Convex Low-Rank Matrix Optimization with Optimal\n  Storage", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.OC stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper concerns a fundamental class of convex matrix optimization\nproblems. It presents the first algorithm that uses optimal storage and\nprovably computes a low-rank approximation of a solution. In particular, when\nall solutions have low rank, the algorithm converges to a solution. This\nalgorithm, SketchyCGM, modifies a standard convex optimization scheme, the\nconditional gradient method, to store only a small randomized sketch of the\nmatrix variable. After the optimization terminates, the algorithm extracts a\nlow-rank approximation of the solution from the sketch. In contrast to\nnonconvex heuristics, the guarantees for SketchyCGM do not rely on statistical\nmodels for the problem data. Numerical work demonstrates the benefits of\nSketchyCGM over heuristics.\n", "versions": [{"version": "v1", "created": "Wed, 22 Feb 2017 15:23:10 GMT"}], "update_date": "2017-02-23", "authors_parsed": [["Yurtsever", "Alp", ""], ["Udell", "Madeleine", ""], ["Tropp", "Joel A.", ""], ["Cevher", "Volkan", ""]]}, {"id": "1702.06861", "submitter": "Simon Du", "authors": "Simon S. Du and Yining Wang and Aarti Singh", "title": "On the Power of Truncated SVD for General High-rank Matrix Estimation\n  Problems", "comments": "Accepted by NIPS 2017. Add gap-dependent bounds", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG math.NA", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We show that given an estimate $\\widehat{A}$ that is close to a general\nhigh-rank positive semi-definite (PSD) matrix $A$ in spectral norm (i.e.,\n$\\|\\widehat{A}-A\\|_2 \\leq \\delta$), the simple truncated SVD of $\\widehat{A}$\nproduces a multiplicative approximation of $A$ in Frobenius norm. This\nobservation leads to many interesting results on general high-rank matrix\nestimation problems, which we briefly summarize below ($A$ is an $n\\times n$\nhigh-rank PSD matrix and $A_k$ is the best rank-$k$ approximation of $A$):\n  (1) High-rank matrix completion: By observing\n$\\Omega(\\frac{n\\max\\{\\epsilon^{-4},k^2\\}\\mu_0^2\\|A\\|_F^2\\log\nn}{\\sigma_{k+1}(A)^2})$ elements of $A$ where $\\sigma_{k+1}\\left(A\\right)$ is\nthe $\\left(k+1\\right)$-th singular value of $A$ and $\\mu_0$ is the incoherence,\nthe truncated SVD on a zero-filled matrix satisfies $\\|\\widehat{A}_k-A\\|_F \\leq\n(1+O(\\epsilon))\\|A-A_k\\|_F$ with high probability.\n  (2)High-rank matrix de-noising: Let $\\widehat{A}=A+E$ where $E$ is a Gaussian\nrandom noise matrix with zero mean and $\\nu^2/n$ variance on each entry. Then\nthe truncated SVD of $\\widehat{A}$ satisfies $\\|\\widehat{A}_k-A\\|_F \\leq\n(1+O(\\sqrt{\\nu/\\sigma_{k+1}(A)}))\\|A-A_k\\|_F + O(\\sqrt{k}\\nu)$.\n  (3) Low-rank Estimation of high-dimensional covariance: Given $N$\ni.i.d.~samples $X_1,\\cdots,X_N\\sim\\mathcal N_n(0,A)$, can we estimate $A$ with\na relative-error Frobenius norm bound? We show that if $N =\n\\Omega\\left(n\\max\\{\\epsilon^{-4},k^2\\}\\gamma_k(A)^2\\log N\\right)$ for\n$\\gamma_k(A)=\\sigma_1(A)/\\sigma_{k+1}(A)$, then $\\|\\widehat{A}_k-A\\|_F \\leq\n(1+O(\\epsilon))\\|A-A_k\\|_F$ with high probability, where\n$\\widehat{A}=\\frac{1}{N}\\sum_{i=1}^N{X_iX_i^\\top}$ is the sample covariance.\n", "versions": [{"version": "v1", "created": "Wed, 22 Feb 2017 15:43:15 GMT"}, {"version": "v2", "created": "Sun, 5 Nov 2017 16:15:43 GMT"}], "update_date": "2017-11-07", "authors_parsed": [["Du", "Simon S.", ""], ["Wang", "Yining", ""], ["Singh", "Aarti", ""]]}, {"id": "1702.06879", "submitter": "Th\\'eo Trouillon", "authors": "Th\\'eo Trouillon, Christopher R. Dance, Johannes Welbl, Sebastian\n  Riedel, \\'Eric Gaussier, Guillaume Bouchard", "title": "Knowledge Graph Completion via Complex Tensor Factorization", "comments": "38 pages, accepted in JMLR. This is an extended version of the\n  article \"Complex embeddings for simple link prediction\" (ICML 2016)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.LG math.SP stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In statistical relational learning, knowledge graph completion deals with\nautomatically understanding the structure of large knowledge graphs---labeled\ndirected graphs---and predicting missing relationships---labeled edges.\nState-of-the-art embedding models propose different trade-offs between modeling\nexpressiveness, and time and space complexity. We reconcile both expressiveness\nand complexity through the use of complex-valued embeddings and explore the\nlink between such complex-valued embeddings and unitary diagonalization. We\ncorroborate our approach theoretically and show that all real square\nmatrices---thus all possible relation/adjacency matrices---are the real part of\nsome unitarily diagonalizable matrix. This results opens the door to a lot of\nother applications of square matrices factorization. Our approach based on\ncomplex embeddings is arguably simple, as it only involves a Hermitian dot\nproduct, the complex counterpart of the standard dot product between real\nvectors, whereas other methods resort to more and more complicated composition\nfunctions to increase their expressiveness. The proposed complex embeddings are\nscalable to large data sets as it remains linear in both space and time, while\nconsistently outperforming alternative approaches on standard link prediction\nbenchmarks.\n", "versions": [{"version": "v1", "created": "Wed, 22 Feb 2017 16:28:11 GMT"}, {"version": "v2", "created": "Sun, 26 Nov 2017 20:39:34 GMT"}], "update_date": "2017-11-28", "authors_parsed": [["Trouillon", "Th\u00e9o", ""], ["Dance", "Christopher R.", ""], ["Welbl", "Johannes", ""], ["Riedel", "Sebastian", ""], ["Gaussier", "\u00c9ric", ""], ["Bouchard", "Guillaume", ""]]}, {"id": "1702.06890", "submitter": "Hongyang Li", "authors": "Yu Liu and Hongyang Li and Xiaogang Wang", "title": "Learning Deep Features via Congenerous Cosine Loss for Person\n  Recognition", "comments": "Post-rebuttal update. Add some comparison results; correct some\n  technical part; rewrite some sections to make it more readable; code link\n  available", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG stat.ML", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Person recognition aims at recognizing the same identity across time and\nspace with complicated scenes and similar appearance. In this paper, we propose\na novel method to address this task by training a network to obtain robust and\nrepresentative features. The intuition is that we directly compare and optimize\nthe cosine distance between two features - enlarging inter-class distinction as\nwell as alleviating inner-class variance. We propose a congenerous cosine loss\nby minimizing the cosine distance between samples and their cluster centroid in\na cooperative way. Such a design reduces the complexity and could be\nimplemented via softmax with normalized inputs. Our method also differs from\nprevious work in person recognition that we do not conduct a second training on\nthe test subset. The identity of a person is determined by measuring the\nsimilarity from several body regions in the reference set. Experimental results\nshow that the proposed approach achieves better classification accuracy against\nprevious state-of-the-arts.\n", "versions": [{"version": "v1", "created": "Wed, 22 Feb 2017 16:45:48 GMT"}, {"version": "v2", "created": "Fri, 31 Mar 2017 17:27:50 GMT"}], "update_date": "2017-04-03", "authors_parsed": [["Liu", "Yu", ""], ["Li", "Hongyang", ""], ["Wang", "Xiaogang", ""]]}, {"id": "1702.06899", "submitter": "Philipp Thomann", "authors": "Ingo Steinwart and Philipp Thomann", "title": "liquidSVM: A Fast and Versatile SVM package", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  liquidSVM is a package written in C++ that provides SVM-type solvers for\nvarious classification and regression tasks. Because of a fully integrated\nhyper-parameter selection, very carefully implemented solvers, multi-threading\nand GPU support, and several built-in data decomposition strategies it provides\nunprecedented speed for small training sizes as well as for data sets of tens\nof millions of samples. Besides the C++ API and a command line interface,\nbindings to R, MATLAB, Java, Python, and Spark are available. We present a\nbrief description of the package and report experimental comparisons to other\nSVM packages.\n", "versions": [{"version": "v1", "created": "Wed, 22 Feb 2017 17:17:26 GMT"}], "update_date": "2017-02-23", "authors_parsed": [["Steinwart", "Ingo", ""], ["Thomann", "Philipp", ""]]}, {"id": "1702.06917", "submitter": "Quentin Berthet", "authors": "Quentin Berthet, Vianney Perchet", "title": "Fast Rates for Bandit Optimization with Upper-Confidence Frank-Wolfe", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG math.OC stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider the problem of bandit optimization, inspired by stochastic\noptimization and online learning problems with bandit feedback. In this\nproblem, the objective is to minimize a global loss function of all the\nactions, not necessarily a cumulative loss. This framework allows us to study a\nvery general class of problems, with applications in statistics, machine\nlearning, and other fields. To solve this problem, we analyze the\nUpper-Confidence Frank-Wolfe algorithm, inspired by techniques for bandits and\nconvex optimization. We give theoretical guarantees for the performance of this\nalgorithm over various classes of functions, and discuss the optimality of\nthese results.\n", "versions": [{"version": "v1", "created": "Wed, 22 Feb 2017 17:55:09 GMT"}, {"version": "v2", "created": "Wed, 6 Sep 2017 16:09:56 GMT"}], "update_date": "2017-09-07", "authors_parsed": [["Berthet", "Quentin", ""], ["Perchet", "Vianney", ""]]}, {"id": "1702.06921", "submitter": "Bijaya Adhikari", "authors": "Bijaya Adhikari, Yao Zhang, Naren Ramakrishnan, and B. Aditya Prakash", "title": "Distributed Representation of Subgraphs", "comments": "9 pages, 7 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.SI cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Network embeddings have become very popular in learning effective feature\nrepresentations of networks. Motivated by the recent successes of embeddings in\nnatural language processing, researchers have tried to find network embeddings\nin order to exploit machine learning algorithms for mining tasks like node\nclassification and edge prediction. However, most of the work focuses on\nfinding distributed representations of nodes, which are inherently ill-suited\nto tasks such as community detection which are intuitively dependent on\nsubgraphs.\n  Here, we propose sub2vec, an unsupervised scalable algorithm to learn feature\nrepresentations of arbitrary subgraphs. We provide means to characterize\nsimilarties between subgraphs and provide theoretical analysis of sub2vec and\ndemonstrate that it preserves the so-called local proximity. We also highlight\nthe usability of sub2vec by leveraging it for network mining tasks, like\ncommunity detection. We show that sub2vec gets significant gains over\nstate-of-the-art methods and node-embedding methods. In particular, sub2vec\noffers an approach to generate a richer vocabulary of features of subgraphs to\nsupport representation and reasoning.\n", "versions": [{"version": "v1", "created": "Wed, 22 Feb 2017 18:06:13 GMT"}], "update_date": "2017-02-23", "authors_parsed": [["Adhikari", "Bijaya", ""], ["Zhang", "Yao", ""], ["Ramakrishnan", "Naren", ""], ["Prakash", "B. Aditya", ""]]}, {"id": "1702.06943", "submitter": "Fengan Li", "authors": "Fengan Li, Lingjiao Chen, Yijing Zeng, Arun Kumar, Jeffrey F.\n  Naughton, Jignesh M. Patel, Xi Wu", "title": "Tuple-oriented Compression for Large-scale Mini-batch Stochastic\n  Gradient Descent", "comments": "Accepted to Sigmod 2019", "journal-ref": null, "doi": "10.1145/3299869.3300070", "report-no": null, "categories": "cs.LG cs.DB stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Data compression is a popular technique for improving the efficiency of data\nprocessing workloads such as SQL queries and more recently, machine learning\n(ML) with classical batch gradient methods. But the efficacy of such ideas for\nmini-batch stochastic gradient descent (MGD), arguably the workhorse algorithm\nof modern ML, is an open question. MGD's unique data access pattern renders\nprior art, including those designed for batch gradient methods, less effective.\nWe fill this crucial research gap by proposing a new lossless compression\nscheme we call tuple-oriented compression (TOC) that is inspired by an unlikely\nsource, the string/text compression scheme Lempel-Ziv-Welch, but tailored to\nMGD in a way that preserves tuple boundaries within mini-batches. We then\npresent a suite of novel compressed matrix operation execution techniques\ntailored to the TOC compression scheme that operate directly over the\ncompressed data representation and avoid decompression overheads. An extensive\nempirical evaluation with real-world datasets shows that TOC consistently\nachieves substantial compression ratios by up to 51x and reduces runtimes for\nMGD workloads by up to 10.2x in popular ML systems.\n", "versions": [{"version": "v1", "created": "Wed, 22 Feb 2017 18:58:25 GMT"}, {"version": "v2", "created": "Wed, 1 Mar 2017 05:43:41 GMT"}, {"version": "v3", "created": "Sun, 20 Jan 2019 05:13:18 GMT"}], "update_date": "2019-01-23", "authors_parsed": [["Li", "Fengan", ""], ["Chen", "Lingjiao", ""], ["Zeng", "Yijing", ""], ["Kumar", "Arun", ""], ["Naughton", "Jeffrey F.", ""], ["Patel", "Jignesh M.", ""], ["Wu", "Xi", ""]]}, {"id": "1702.06972", "submitter": "Azadeh Khaleghi", "authors": "Steffen Grunewalder and Azadeh Khaleghi", "title": "Approximations of the Restless Bandit Problem", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST cs.LG math.PR stat.ML stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The multi-armed restless bandit problem is studied in the case where the\npay-off distributions are stationary $\\varphi$-mixing. This version of the\nproblem provides a more realistic model for most real-world applications, but\ncannot be optimally solved in practice, since it is known to be PSPACE-hard.\nThe objective of this paper is to characterize a sub-class of the problem where\n{\\em good} approximate solutions can be found using tractable approaches.\nSpecifically, it is shown that under some conditions on the $\\varphi$-mixing\ncoefficients, a modified version of UCB can prove effective. The main challenge\nis that, unlike in the i.i.d. setting, the distributions of the sampled\npay-offs may not have the same characteristics as those of the original bandit\narms. In particular, the $\\varphi$-mixing property does not necessarily carry\nover. This is overcome by carefully controlling the effect of a sampling policy\non the pay-off distributions. Some of the proof techniques developed in this\npaper can be more generally used in the context of online sampling under\ndependence. Proposed algorithms are accompanied with corresponding regret\nanalysis.\n", "versions": [{"version": "v1", "created": "Wed, 22 Feb 2017 19:22:55 GMT"}, {"version": "v2", "created": "Thu, 5 Jul 2018 17:17:14 GMT"}, {"version": "v3", "created": "Fri, 28 Dec 2018 14:21:04 GMT"}], "update_date": "2018-12-31", "authors_parsed": [["Grunewalder", "Steffen", ""], ["Khaleghi", "Azadeh", ""]]}, {"id": "1702.06976", "submitter": "Joseph Anderson", "authors": "Joseph Anderson and Navin Goyal and Anupama Nandi and Luis Rademacher", "title": "Heavy-Tailed Analogues of the Covariance Matrix for ICA", "comments": "16 Pages, 9 Figures, AAAI 2017", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Independent Component Analysis (ICA) is the problem of learning a square\nmatrix $A$, given samples of $X=AS$, where $S$ is a random vector with\nindependent coordinates. Most existing algorithms are provably efficient only\nwhen each $S_i$ has finite and moderately valued fourth moment. However, there\nare practical applications where this assumption need not be true, such as\nspeech and finance. Algorithms have been proposed for heavy-tailed ICA, but\nthey are not practical, using random walks and the full power of the ellipsoid\nalgorithm multiple times. The main contributions of this paper are:\n  (1) A practical algorithm for heavy-tailed ICA that we call HTICA. We provide\ntheoretical guarantees and show that it outperforms other algorithms in some\nheavy-tailed regimes, both on real and synthetic data. Like the current\nstate-of-the-art, the new algorithm is based on the centroid body (a first\nmoment analogue of the covariance matrix). Unlike the state-of-the-art, our\nalgorithm is practically efficient. To achieve this, we use explicit analytic\nrepresentations of the centroid body, which bypasses the use of the ellipsoid\nmethod and random walks.\n  (2) We study how heavy tails affect different ICA algorithms, including\nHTICA. Somewhat surprisingly, we show that some algorithms that use the\ncovariance matrix or higher moments can successfully solve a range of ICA\ninstances with infinite second moment. We study this theoretically and\nexperimentally, with both synthetic and real-world heavy-tailed data.\n", "versions": [{"version": "v1", "created": "Wed, 22 Feb 2017 19:27:38 GMT"}], "update_date": "2017-02-24", "authors_parsed": [["Anderson", "Joseph", ""], ["Goyal", "Navin", ""], ["Nandi", "Anupama", ""], ["Rademacher", "Luis", ""]]}, {"id": "1702.06980", "submitter": "Dong Xia", "authors": "Dong Xia and Ming Yuan", "title": "On Polynomial Time Methods for Exact Low Rank Tensor Completion", "comments": "56 pages, 4 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.IT cs.LG math.IT", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we investigate the sample size requirement for exact recovery\nof a high order tensor of low rank from a subset of its entries. We show that a\ngradient descent algorithm with initial value obtained from a spectral method\ncan, in particular, reconstruct a ${d\\times d\\times d}$ tensor of multilinear\nranks $(r,r,r)$ with high probability from as few as\n$O(r^{7/2}d^{3/2}\\log^{7/2}d+r^7d\\log^6d)$ entries. In the case when the ranks\n$r=O(1)$, our sample size requirement matches those for nuclear norm\nminimization (Yuan and Zhang, 2016a), or alternating least squares assuming\northogonal decomposability (Jain and Oh, 2014). Unlike these earlier\napproaches, however, our method is efficient to compute, easy to implement, and\ndoes not impose extra structures on the tensor. Numerical results are presented\nto further demonstrate the merits of the proposed approach.\n", "versions": [{"version": "v1", "created": "Wed, 22 Feb 2017 19:36:25 GMT"}], "update_date": "2017-02-27", "authors_parsed": [["Xia", "Dong", ""], ["Yuan", "Ming", ""]]}, {"id": "1702.07013", "submitter": "Hongteng Xu", "authors": "Hongteng Xu, Dixin Luo, Hongyuan Zha", "title": "Learning Hawkes Processes from Short Doubly-Censored Event Sequences", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG math.PR stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Many real-world applications require robust algorithms to learn point\nprocesses based on a type of incomplete data --- the so-called short\ndoubly-censored (SDC) event sequences. We study this critical problem of\nquantitative asynchronous event sequence analysis under the framework of Hawkes\nprocesses by leveraging the idea of data synthesis. Given SDC event sequences\nobserved in a variety of time intervals, we propose a sampling-stitching data\nsynthesis method --- sampling predecessors and successors for each SDC event\nsequence from potential candidates and stitching them together to synthesize\nlong training sequences. The rationality and the feasibility of our method are\ndiscussed in terms of arguments based on likelihood. Experiments on both\nsynthetic and real-world data demonstrate that the proposed data synthesis\nmethod improves learning results indeed for both time-invariant and\ntime-varying Hawkes processes.\n", "versions": [{"version": "v1", "created": "Wed, 22 Feb 2017 21:41:05 GMT"}, {"version": "v2", "created": "Wed, 7 Jun 2017 20:36:45 GMT"}], "update_date": "2017-06-09", "authors_parsed": [["Xu", "Hongteng", ""], ["Luo", "Dixin", ""], ["Zha", "Hongyuan", ""]]}, {"id": "1702.07021", "submitter": "Trang Pham", "authors": "Trang Pham, Truyen Tran, Svetha Venkatesh", "title": "One Size Fits Many: Column Bundle for Multi-X Learning", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Much recent machine learning research has been directed towards leveraging\nshared statistics among labels, instances and data views, commonly referred to\nas multi-label, multi-instance and multi-view learning. The underlying premises\nare that there exist correlations among input parts and among output targets,\nand the predictive performance would increase when the correlations are\nincorporated. In this paper, we propose Column Bundle (CLB), a novel deep\nneural network for capturing the shared statistics in data. CLB is generic that\nthe same architecture can be applied for various types of shared statistics by\nchanging only input and output handling. CLB is capable of scaling to thousands\nof input parts and output labels by avoiding explicit modeling of pairwise\nrelations. We evaluate CLB on different types of data: (a) multi-label, (b)\nmulti-view, (c) multi-view/multi-label and (d) multi-instance. CLB demonstrates\na comparable and competitive performance in all datasets against\nstate-of-the-art methods designed specifically for each type.\n", "versions": [{"version": "v1", "created": "Wed, 22 Feb 2017 21:54:12 GMT"}, {"version": "v2", "created": "Tue, 14 Mar 2017 00:44:14 GMT"}], "update_date": "2017-03-16", "authors_parsed": [["Pham", "Trang", ""], ["Tran", "Truyen", ""], ["Venkatesh", "Svetha", ""]]}, {"id": "1702.07066", "submitter": "Pierre Lafaye De Micheaux", "authors": "Pierre Lafaye de Micheaux and Benoit Liquet and Matthew Sutton", "title": "A Unified Parallel Algorithm for Regularized Group PLS Scalable to Big\n  Data", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Partial Least Squares (PLS) methods have been heavily exploited to analyse\nthe association between two blocs of data. These powerful approaches can be\napplied to data sets where the number of variables is greater than the number\nof observations and in presence of high collinearity between variables.\nDifferent sparse versions of PLS have been developed to integrate multiple data\nsets while simultaneously selecting the contributing variables. Sparse\nmodelling is a key factor in obtaining better estimators and identifying\nassociations between multiple data sets. The cornerstone of the sparsity\nversion of PLS methods is the link between the SVD of a matrix (constructed\nfrom deflated versions of the original matrices of data) and least squares\nminimisation in linear regression. We present here an accurate description of\nthe most popular PLS methods, alongside their mathematical proofs. A unified\nalgorithm is proposed to perform all four types of PLS including their\nregularised versions. Various approaches to decrease the computation time are\noffered, and we show how the whole procedure can be scalable to big data sets.\n", "versions": [{"version": "v1", "created": "Thu, 23 Feb 2017 02:08:51 GMT"}], "update_date": "2017-02-24", "authors_parsed": [["de Micheaux", "Pierre Lafaye", ""], ["Liquet", "Benoit", ""], ["Sutton", "Matthew", ""]]}, {"id": "1702.07083", "submitter": "Jianfei Chen", "authors": "Jianfei Chen, Jun Zhu, Jie Lu, Shixia Liu", "title": "Scalable Inference for Nested Chinese Restaurant Process Topic Models", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.DC cs.IR cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Nested Chinese Restaurant Process (nCRP) topic models are powerful\nnonparametric Bayesian methods to extract a topic hierarchy from a given text\ncorpus, where the hierarchical structure is automatically determined by the\ndata. Hierarchical Latent Dirichlet Allocation (hLDA) is a popular instance of\nnCRP topic models. However, hLDA has only been evaluated at small scale,\nbecause the existing collapsed Gibbs sampling and instantiated weight\nvariational inference algorithms either are not scalable or sacrifice inference\nquality with mean-field assumptions. Moreover, an efficient distributed\nimplementation of the data structures, such as dynamically growing count\nmatrices and trees, is challenging.\n  In this paper, we propose a novel partially collapsed Gibbs sampling (PCGS)\nalgorithm, which combines the advantages of collapsed and instantiated weight\nalgorithms to achieve good scalability as well as high model quality. An\ninitialization strategy is presented to further improve the model quality.\nFinally, we propose an efficient distributed implementation of PCGS through\nvectorization, pre-processing, and a careful design of the concurrent data\nstructures and communication strategy.\n  Empirical studies show that our algorithm is 111 times more efficient than\nthe previous open-source implementation for hLDA, with comparable or even\nbetter model quality. Our distributed implementation can extract 1,722 topics\nfrom a 131-million-document corpus with 28 billion tokens, which is 4-5 orders\nof magnitude larger than the previous largest corpus, with 50 machines in 7\nhours.\n", "versions": [{"version": "v1", "created": "Thu, 23 Feb 2017 03:34:07 GMT"}], "update_date": "2017-02-24", "authors_parsed": [["Chen", "Jianfei", ""], ["Zhu", "Jun", ""], ["Lu", "Jie", ""], ["Liu", "Shixia", ""]]}, {"id": "1702.07121", "submitter": "Assaf Hallak", "authors": "Assaf Hallak and Shie Mannor", "title": "Consistent On-Line Off-Policy Evaluation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The problem of on-line off-policy evaluation (OPE) has been actively studied\nin the last decade due to its importance both as a stand-alone problem and as a\nmodule in a policy improvement scheme. However, most Temporal Difference (TD)\nbased solutions ignore the discrepancy between the stationary distribution of\nthe behavior and target policies and its effect on the convergence limit when\nfunction approximation is applied. In this paper we propose the Consistent\nOff-Policy Temporal Difference (COP-TD($\\lambda$, $\\beta$)) algorithm that\naddresses this issue and reduces this bias at some computational expense. We\nshow that COP-TD($\\lambda$, $\\beta$) can be designed to converge to the same\nvalue that would have been obtained by using on-policy TD($\\lambda$) with the\ntarget policy. Subsequently, the proposed scheme leads to a related and\npromising heuristic we call log-COP-TD($\\lambda$, $\\beta$). Both algorithms\nhave favorable empirical results to the current state of the art on-line OPE\nalgorithms. Finally, our formulation sheds some new light on the recently\nproposed Emphatic TD learning.\n", "versions": [{"version": "v1", "created": "Thu, 23 Feb 2017 07:44:43 GMT"}], "update_date": "2017-02-24", "authors_parsed": [["Hallak", "Assaf", ""], ["Mannor", "Shie", ""]]}, {"id": "1702.07125", "submitter": "Assaf Hallak", "authors": "Assaf Hallak, Yishay Mansour and Elad Yom-Tov", "title": "Automatic Representation for Lifetime Value Recommender Systems", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Many modern commercial sites employ recommender systems to propose relevant\ncontent to users. While most systems are focused on maximizing the immediate\ngain (clicks, purchases or ratings), a better notion of success would be the\nlifetime value (LTV) of the user-system interaction. The LTV approach considers\nthe future implications of the item recommendation, and seeks to maximize the\ncumulative gain over time. The Reinforcement Learning (RL) framework is the\nstandard formulation for optimizing cumulative successes over time. However, RL\nis rarely used in practice due to its associated representation, optimization\nand validation techniques which can be complex. In this paper we propose a new\narchitecture for combining RL with recommendation systems which obviates the\nneed for hand-tuned features, thus automating the state-space representation\nconstruction process. We analyze the practical difficulties in this formulation\nand test our solutions on batch off-line real-world recommendation data.\n", "versions": [{"version": "v1", "created": "Thu, 23 Feb 2017 07:59:22 GMT"}], "update_date": "2017-02-24", "authors_parsed": [["Hallak", "Assaf", ""], ["Mansour", "Yishay", ""], ["Yom-Tov", "Elad", ""]]}, {"id": "1702.07186", "submitter": "Derek Greene", "authors": "Mark Belford and Brian Mac Namee and Derek Greene", "title": "Stability of Topic Modeling via Matrix Factorization", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR cs.CL cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Topic models can provide us with an insight into the underlying latent\nstructure of a large corpus of documents. A range of methods have been proposed\nin the literature, including probabilistic topic models and techniques based on\nmatrix factorization. However, in both cases, standard implementations rely on\nstochastic elements in their initialization phase, which can potentially lead\nto different results being generated on the same corpus when using the same\nparameter values. This corresponds to the concept of \"instability\" which has\npreviously been studied in the context of $k$-means clustering. In many\napplications of topic modeling, this problem of instability is not considered\nand topic models are treated as being definitive, even though the results may\nchange considerably if the initialization process is altered. In this paper we\ndemonstrate the inherent instability of popular topic modeling approaches,\nusing a number of new measures to assess stability. To address this issue in\nthe context of matrix factorization for topic modeling, we propose the use of\nensemble learning strategies. Based on experiments performed on annotated text\ncorpora, we show that a K-Fold ensemble strategy, combining both ensembles and\nstructured initialization, can significantly reduce instability, while\nsimultaneously yielding more accurate topic models.\n", "versions": [{"version": "v1", "created": "Thu, 23 Feb 2017 12:00:10 GMT"}, {"version": "v2", "created": "Sat, 9 Sep 2017 17:06:18 GMT"}], "update_date": "2017-09-12", "authors_parsed": [["Belford", "Mark", ""], ["Mac Namee", "Brian", ""], ["Greene", "Derek", ""]]}, {"id": "1702.07190", "submitter": "Sigurd L{\\o}kse", "authors": "Sigurd L{\\o}kse, Filippo Maria Bianchi, Arnt-B{\\o}rre Salberg, Robert\n  Jenssen", "title": "Spectral Clustering using PCKID - A Probabilistic Cluster Kernel for\n  Incomplete Data", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we propose PCKID, a novel, robust, kernel function for\nspectral clustering, specifically designed to handle incomplete data. By\ncombining posterior distributions of Gaussian Mixture Models for incomplete\ndata on different scales, we are able to learn a kernel for incomplete data\nthat does not depend on any critical hyperparameters, unlike the commonly used\nRBF kernel. To evaluate our method, we perform experiments on two real\ndatasets. PCKID outperforms the baseline methods for all fractions of missing\nvalues and in some cases outperforms the baseline methods with up to 25\npercentage points.\n", "versions": [{"version": "v1", "created": "Thu, 23 Feb 2017 12:19:31 GMT"}], "update_date": "2017-02-24", "authors_parsed": [["L\u00f8kse", "Sigurd", ""], ["Bianchi", "Filippo Maria", ""], ["Salberg", "Arnt-B\u00f8rre", ""], ["Jenssen", "Robert", ""]]}, {"id": "1702.07211", "submitter": "Pierre Menard", "authors": "Pierre M\\'enard (1), Aur\\'elien Garivier (1) ((1) IMT)", "title": "A minimax and asymptotically optimal algorithm for stochastic bandits", "comments": null, "journal-ref": "Algorithmic Learning Theory, Springer, 2017, 2017 Algorithmic\n  Learning Theory Conference 76", "doi": null, "report-no": null, "categories": "stat.ML cs.LG math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose the kl-UCB ++ algorithm for regret minimization in stochastic\nbandit models with exponential families of distributions. We prove that it is\nsimultaneously asymptotically optimal (in the sense of Lai and Robbins' lower\nbound) and minimax optimal. This is the first algorithm proved to enjoy these\ntwo properties at the same time. This work thus merges two different lines of\nresearch with simple and clear proofs.\n", "versions": [{"version": "v1", "created": "Thu, 23 Feb 2017 13:49:57 GMT"}, {"version": "v2", "created": "Wed, 20 Sep 2017 14:26:03 GMT"}], "update_date": "2017-09-21", "authors_parsed": [["M\u00e9nard", "Pierre", "", "IMT"], ["Garivier", "Aur\u00e9lien", "", "IMT"]]}, {"id": "1702.07254", "submitter": "Simon Fischer", "authors": "Simon Fischer and Ingo Steinwart", "title": "Sobolev Norm Learning Rates for Regularized Least-Squares Algorithm", "comments": "accepted manuscript in J. Mach. Learn. Res", "journal-ref": "J. Mach. Learn. Res. 21 (2020) 1-38", "doi": null, "report-no": null, "categories": "stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Learning rates for least-squares regression are typically expressed in terms\nof $L_2$-norms. In this paper we extend these rates to norms stronger than the\n$L_2$-norm without requiring the regression function to be contained in the\nhypothesis space. In the special case of Sobolev reproducing kernel Hilbert\nspaces used as hypotheses spaces, these stronger norms coincide with fractional\nSobolev norms between the used Sobolev space and $L_2$. As a consequence, not\nonly the target function but also some of its derivatives can be estimated\nwithout changing the algorithm. From a technical point of view, we combine the\nwell-known integral operator techniques with an embedding property, which so\nfar has only been used in combination with empirical process arguments. This\ncombination results in new finite sample bounds with respect to the stronger\nnorms. From these finite sample bounds our rates easily follow. Finally, we\nprove the asymptotic optimality of our results in many cases.\n", "versions": [{"version": "v1", "created": "Thu, 23 Feb 2017 15:10:15 GMT"}, {"version": "v2", "created": "Wed, 22 May 2019 13:50:53 GMT"}, {"version": "v3", "created": "Thu, 8 Oct 2020 13:35:22 GMT"}], "update_date": "2020-10-27", "authors_parsed": [["Fischer", "Simon", ""], ["Steinwart", "Ingo", ""]]}, {"id": "1702.07274", "submitter": "Nir Levine", "authors": "Nir Levine, Koby Crammer, Shie Mannor", "title": "Rotting Bandits", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The Multi-Armed Bandits (MAB) framework highlights the tension between\nacquiring new knowledge (Exploration) and leveraging available knowledge\n(Exploitation). In the classical MAB problem, a decision maker must choose an\narm at each time step, upon which she receives a reward. The decision maker's\nobjective is to maximize her cumulative expected reward over the time horizon.\nThe MAB problem has been studied extensively, specifically under the assumption\nof the arms' rewards distributions being stationary, or quasi-stationary, over\ntime. We consider a variant of the MAB framework, which we termed Rotting\nBandits, where each arm's expected reward decays as a function of the number of\ntimes it has been pulled. We are motivated by many real-world scenarios such as\nonline advertising, content recommendation, crowdsourcing, and more. We present\nalgorithms, accompanied by simulations, and derive theoretical guarantees.\n", "versions": [{"version": "v1", "created": "Thu, 23 Feb 2017 16:03:30 GMT"}, {"version": "v2", "created": "Fri, 24 Feb 2017 11:09:19 GMT"}, {"version": "v3", "created": "Mon, 12 Jun 2017 13:48:30 GMT"}, {"version": "v4", "created": "Thu, 2 Nov 2017 15:32:09 GMT"}], "update_date": "2017-11-03", "authors_parsed": [["Levine", "Nir", ""], ["Crammer", "Koby", ""], ["Mannor", "Shie", ""]]}, {"id": "1702.07305", "submitter": "Young Hun Jung", "authors": "Young Hun Jung, Jack Goetz, Ambuj Tewari", "title": "Online Multiclass Boosting", "comments": "28 pages, 2 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recent work has extended the theoretical analysis of boosting algorithms to\nmulticlass problems and to online settings. However, the multiclass extension\nis in the batch setting and the online extensions only consider binary\nclassification. We fill this gap in the literature by defining, and justifying,\na weak learning condition for online multiclass boosting. This condition leads\nto an optimal boosting algorithm that requires the minimal number of weak\nlearners to achieve a certain accuracy. Additionally, we propose an adaptive\nalgorithm which is near optimal and enjoys an excellent performance on real\ndata due to its adaptive property.\n", "versions": [{"version": "v1", "created": "Thu, 23 Feb 2017 17:46:22 GMT"}, {"version": "v2", "created": "Fri, 26 May 2017 01:41:47 GMT"}, {"version": "v3", "created": "Sun, 25 Feb 2018 01:09:26 GMT"}], "update_date": "2018-02-27", "authors_parsed": [["Jung", "Young Hun", ""], ["Goetz", "Jack", ""], ["Tewari", "Ambuj", ""]]}, {"id": "1702.07306", "submitter": "David Lopez-Paz", "authors": "Mateo Rojas-Carulla, Marco Baroni, David Lopez-Paz", "title": "Causal Discovery Using Proxy Variables", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Discovering causal relations is fundamental to reasoning and intelligence. In\nparticular, observational causal discovery algorithms estimate the cause-effect\nrelation between two random entities $X$ and $Y$, given $n$ samples from\n$P(X,Y)$.\n  In this paper, we develop a framework to estimate the cause-effect relation\nbetween two static entities $x$ and $y$: for instance, an art masterpiece $x$\nand its fraudulent copy $y$. To this end, we introduce the notion of proxy\nvariables, which allow the construction of a pair of random entities $(A,B)$\nfrom the pair of static entities $(x,y)$. Then, estimating the cause-effect\nrelation between $A$ and $B$ using an observational causal discovery algorithm\nleads to an estimation of the cause-effect relation between $x$ and $y$. For\nexample, our framework detects the causal relation between unprocessed\nphotographs and their modifications, and orders in time a set of shuffled\nframes from a video.\n  As our main case study, we introduce a human-elicited dataset of 10,000 pairs\nof casually-linked pairs of words from natural language. Our methods discover\n75% of these causal relations. Finally, we discuss the role of proxy variables\nin machine learning, as a general tool to incorporate static knowledge into\nprediction tasks.\n", "versions": [{"version": "v1", "created": "Thu, 23 Feb 2017 17:46:39 GMT"}], "update_date": "2017-02-24", "authors_parsed": [["Rojas-Carulla", "Mateo", ""], ["Baroni", "Marco", ""], ["Lopez-Paz", "David", ""]]}, {"id": "1702.07319", "submitter": "John Pearson", "authors": "Shariq Iqbal and John Pearson", "title": "A Goal-Based Movement Model for Continuous Multi-Agent Tasks", "comments": "New title; substantial simplifications of model", "journal-ref": null, "doi": null, "report-no": null, "categories": "q-bio.NC cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Despite increasing attention paid to the need for fast, scalable methods to\nanalyze next-generation neuroscience data, comparatively little attention has\nbeen paid to the development of similar methods for behavioral analysis. Just\nas the volume and complexity of brain data have grown, behavioral paradigms in\nsystems neuroscience have likewise become more naturalistic and less\nconstrained, necessitating an increase in the flexibility and scalability of\nthe models used to study them. In particular, key assumptions made in the\nanalysis of typical decision paradigms --- optimality; analytic tractability;\ndiscrete, low-dimensional action spaces --- may be untenable in richer tasks.\nHere, using the case of a two-player, real-time, continuous strategic game as\nan example, we show how the use of modern machine learning methods allows us to\nrelax each of these assumptions. Following an inverse reinforcement learning\napproach, we are able to succinctly characterize the joint distribution over\nplayers' actions via a generative model that allows us to simulate realistic\ngame play. We compare simulated play from a number of generative time series\nmodels and show that ours successfully resists mode collapse while generating\ntrajectories with the rich variability of real behavior. Together, these\nmethods offer a rich class of models for the analysis of continuous action\ntasks at the single-trial level.\n", "versions": [{"version": "v1", "created": "Thu, 23 Feb 2017 18:09:14 GMT"}, {"version": "v2", "created": "Tue, 31 Oct 2017 20:14:11 GMT"}], "update_date": "2017-11-02", "authors_parsed": [["Iqbal", "Shariq", ""], ["Pearson", "John", ""]]}, {"id": "1702.07339", "submitter": "Emmanouil Zampetakis", "authors": "Constantinos Daskalakis, Christos Tzamos, Manolis Zampetakis", "title": "A Converse to Banach's Fixed Point Theorem and its CLS Completeness", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CC cs.LG math.GN stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Banach's fixed point theorem for contraction maps has been widely used to\nanalyze the convergence of iterative methods in non-convex problems. It is a\ncommon experience, however, that iterative maps fail to be globally contracting\nunder the natural metric in their domain, making the applicability of Banach's\ntheorem limited. We explore how generally we can apply Banach's fixed point\ntheorem to establish the convergence of iterative methods when pairing it with\ncarefully designed metrics.\n  Our first result is a strong converse of Banach's theorem, showing that it is\na universal analysis tool for establishing global convergence of iterative\nmethods to unique fixed points, and for bounding their convergence rate. In\nother words, we show that, whenever an iterative map globally converges to a\nunique fixed point, there exists a metric under which the iterative map is\ncontracting and which can be used to bound the number of iterations until\nconvergence. We illustrate our approach in the widely used power method,\nproviding a new way of bounding its convergence rate through contraction\narguments.\n  We next consider the computational complexity of Banach's fixed point\ntheorem. Making the proof of our converse theorem constructive, we show that\ncomputing a fixed point whose existence is guaranteed by Banach's fixed point\ntheorem is CLS-complete. We thus provide the first natural complete problem for\nthe class CLS, which was defined in [Daskalakis, Papadimitriou 2011] to capture\nthe complexity of problems such as P-matrix LCP, computing KKT-points, and\nfinding mixed Nash equilibria in congestion and network coordination games.\n", "versions": [{"version": "v1", "created": "Thu, 23 Feb 2017 18:52:31 GMT"}, {"version": "v2", "created": "Wed, 5 Apr 2017 20:25:27 GMT"}, {"version": "v3", "created": "Tue, 13 Feb 2018 23:33:13 GMT"}], "update_date": "2018-02-15", "authors_parsed": [["Daskalakis", "Constantinos", ""], ["Tzamos", "Christos", ""], ["Zampetakis", "Manolis", ""]]}, {"id": "1702.07360", "submitter": "Randall Balestriero", "authors": "Randall Balestriero", "title": "Neural Decision Trees", "comments": "11 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  In this paper we propose a synergistic melting of neural networks and\ndecision trees (DT) we call neural decision trees (NDT). NDT is an architecture\na la decision tree where each splitting node is an independent multilayer\nperceptron allowing oblique decision functions or arbritrary nonlinear decision\nfunction if more than one layer is used. This way, each MLP can be seen as a\nnode of the tree. We then show that with the weight sharing asumption among\nthose units, we end up with a Hashing Neural Network (HNN) which is a\nmultilayer perceptron with sigmoid activation function for the last layer as\nopposed to the standard softmax. The output units then jointly represent the\nprobability to be in a particular region. The proposed framework allows for\nglobal optimization as opposed to greedy in DT and differentiability w.r.t. all\nparameters and the input, allowing easy integration in any learnable pipeline,\nfor example after CNNs for computer vision tasks. We also demonstrate the\nmodeling power of HNN allowing to learn union of disjoint regions for final\nclustering or classification making it more general and powerful than standard\nsoftmax MLP requiring linear separability thus reducing the need on the inner\nlayer to perform complex data transformations. We finally show experiments for\nsupervised, semi-suppervised and unsupervised tasks and compare results with\nstandard DTs and MLPs.\n", "versions": [{"version": "v1", "created": "Thu, 23 Feb 2017 19:02:32 GMT"}, {"version": "v2", "created": "Mon, 6 Mar 2017 15:39:47 GMT"}], "update_date": "2017-03-07", "authors_parsed": [["Balestriero", "Randall", ""]]}, {"id": "1702.07367", "submitter": "Matthias Chung", "authors": "Julianne Chung, Matthias Chung, J. Tanner Slagel, and Luis Tenorio", "title": "Stochastic Newton and Quasi-Newton Methods for Large Linear\n  Least-squares Problems", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.NA cs.NA stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We describe stochastic Newton and stochastic quasi-Newton approaches to\nefficiently solve large linear least-squares problems where the very large data\nsets present a significant computational burden (e.g., the size may exceed\ncomputer memory or data are collected in real-time). In our proposed framework,\nstochasticity is introduced in two different frameworks as a means to overcome\nthese computational limitations, and probability distributions that can exploit\nstructure and/or sparsity are considered. Theoretical results on consistency of\nthe approximations for both the stochastic Newton and the stochastic\nquasi-Newton methods are provided. The results show, in particular, that\nstochastic Newton iterates, in contrast to stochastic quasi-Newton iterates,\nmay not converge to the desired least-squares solution. Numerical examples,\nincluding an example from extreme learning machines, demonstrate the potential\napplications of these methods.\n", "versions": [{"version": "v1", "created": "Thu, 23 Feb 2017 19:09:19 GMT"}], "update_date": "2017-02-27", "authors_parsed": [["Chung", "Julianne", ""], ["Chung", "Matthias", ""], ["Slagel", "J. Tanner", ""], ["Tenorio", "Luis", ""]]}, {"id": "1702.07398", "submitter": "Wesley Tansey", "authors": "Wesley Tansey, Karl Pichotta, James G. Scott", "title": "Deep Nonparametric Estimation of Discrete Conditional Distributions via\n  Smoothed Dyadic Partitioning", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present an approach to deep estimation of discrete conditional probability\ndistributions. Such models have several applications, including generative\nmodeling of audio, image, and video data. Our approach combines two main\ntechniques: dyadic partitioning and graph-based smoothing of the discrete\nspace. By recursively decomposing each dimension into a series of binary splits\nand smoothing over the resulting distribution using graph-based trend\nfiltering, we impose a strict structure to the model and achieve much higher\nsample efficiency. We demonstrate the advantages of our model through a series\nof benchmarks on both synthetic and real-world datasets, in some cases reducing\nthe error by nearly half in comparison to other popular methods in the\nliterature. All of our models are implemented in Tensorflow and publicly\navailable at https://github.com/tansey/sdp .\n", "versions": [{"version": "v1", "created": "Thu, 23 Feb 2017 21:29:13 GMT"}, {"version": "v2", "created": "Tue, 28 Feb 2017 14:30:30 GMT"}], "update_date": "2017-03-01", "authors_parsed": [["Tansey", "Wesley", ""], ["Pichotta", "Karl", ""], ["Scott", "James G.", ""]]}, {"id": "1702.07400", "submitter": "Anindya Bhadra", "authors": "Anindya Bhadra, Jyotishka Datta, Nicholas G. Polson, Brandon Willard", "title": "Horseshoe Regularization for Feature Subset Selection", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Feature subset selection arises in many high-dimensional applications of\nstatistics, such as compressed sensing and genomics. The $\\ell_0$ penalty is\nideal for this task, the caveat being it requires the NP-hard combinatorial\nevaluation of all models. A recent area of considerable interest is to develop\nefficient algorithms to fit models with a non-convex $\\ell_\\gamma$ penalty for\n$\\gamma\\in (0,1)$, which results in sparser models than the convex $\\ell_1$ or\nlasso penalty, but is harder to fit. We propose an alternative, termed the\nhorseshoe regularization penalty for feature subset selection, and demonstrate\nits theoretical and computational advantages. The distinguishing feature from\nexisting non-convex optimization approaches is a full probabilistic\nrepresentation of the penalty as the negative of the logarithm of a suitable\nprior, which in turn enables efficient expectation-maximization and local\nlinear approximation algorithms for optimization and MCMC for uncertainty\nquantification. In synthetic and real data, the resulting algorithms provide\nbetter statistical performance, and the computation requires a fraction of time\nof state-of-the-art non-convex solvers.\n", "versions": [{"version": "v1", "created": "Thu, 23 Feb 2017 21:37:06 GMT"}, {"version": "v2", "created": "Thu, 22 Jun 2017 18:35:34 GMT"}], "update_date": "2017-06-26", "authors_parsed": [["Bhadra", "Anindya", ""], ["Datta", "Jyotishka", ""], ["Polson", "Nicholas G.", ""], ["Willard", "Brandon", ""]]}, {"id": "1702.07405", "submitter": "Wesley Tansey", "authors": "Wesley Tansey, James G. Scott", "title": "GapTV: Accurate and Interpretable Low-Dimensional Regression and\n  Classification", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider the problem of estimating a regression function in the common\nsituation where the number of features is small, where interpretability of the\nmodel is a high priority, and where simple linear or additive models fail to\nprovide adequate performance. To address this problem, we present GapTV, an\napproach that is conceptually related both to CART and to the more recent CRISP\nalgorithm, a state-of-the-art alternative method for interpretable nonlinear\nregression. GapTV divides the feature space into blocks of constant value and\nfits the value of all blocks jointly via a convex optimization routine. Our\nmethod is fully data-adaptive, in that it incorporates highly robust routines\nfor tuning all hyperparameters automatically. We compare our approach against\nCART and CRISP and demonstrate that GapTV finds a much better trade-off between\naccuracy and interpretability.\n", "versions": [{"version": "v1", "created": "Thu, 23 Feb 2017 21:58:27 GMT"}], "update_date": "2017-02-27", "authors_parsed": [["Tansey", "Wesley", ""], ["Scott", "James G.", ""]]}, {"id": "1702.07462", "submitter": "Kun He Prof.", "authors": "Kun He, Yingru Li, Sucheta Soundarajan, John E. Hopcroft", "title": "Hidden Community Detection in Social Networks", "comments": "10 pages, 6 figures, 4 tables, submitted to KDD 2017", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.SI physics.soc-ph stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We introduce a new paradigm that is important for community detection in the\nrealm of network analysis. Networks contain a set of strong, dominant\ncommunities, which interfere with the detection of weak, natural community\nstructure. When most of the members of the weak communities also belong to\nstronger communities, they are extremely hard to be uncovered. We call the weak\ncommunities the hidden community structure.\n  We present a novel approach called HICODE (HIdden COmmunity DEtection) that\nidentifies the hidden community structure as well as the dominant community\nstructure. By weakening the strength of the dominant structure, one can uncover\nthe hidden structure beneath. Likewise, by reducing the strength of the hidden\nstructure, one can more accurately identify the dominant structure. In this\nway, HICODE tackles both tasks simultaneously.\n  Extensive experiments on real-world networks demonstrate that HICODE\noutperforms several state-of-the-art community detection methods in uncovering\nboth the dominant and the hidden structure. In the Facebook university social\nnetworks, we find multiple non-redundant sets of communities that are strongly\nassociated with residential hall, year of registration or career position of\nthe faculties or students, while the state-of-the-art algorithms mainly locate\nthe dominant ground truth category. In the Due to the difficulty of labeling\nall ground truth communities in real-world datasets, HICODE provides a\npromising approach to pinpoint the existing latent communities and uncover\ncommunities for which there is no ground truth. Finding this unknown structure\nis an extremely important community detection problem.\n", "versions": [{"version": "v1", "created": "Fri, 24 Feb 2017 04:52:30 GMT"}], "update_date": "2017-02-27", "authors_parsed": [["He", "Kun", ""], ["Li", "Yingru", ""], ["Soundarajan", "Sucheta", ""], ["Hopcroft", "John E.", ""]]}, {"id": "1702.07463", "submitter": "Po-Sen Huang", "authors": "Chong Wang and Yining Wang and Po-Sen Huang and Abdelrahman Mohamed\n  and Dengyong Zhou and Li Deng", "title": "Sequence Modeling via Segmentations", "comments": "recurrent neural networks, dynamic programming, structured prediction", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Segmental structure is a common pattern in many types of sequences such as\nphrases in human languages. In this paper, we present a probabilistic model for\nsequences via their segmentations. The probability of a segmented sequence is\ncalculated as the product of the probabilities of all its segments, where each\nsegment is modeled using existing tools such as recurrent neural networks.\nSince the segmentation of a sequence is usually unknown in advance, we sum over\nall valid segmentations to obtain the final probability for the sequence. An\nefficient dynamic programming algorithm is developed for forward and backward\ncomputations without resorting to any approximation. We demonstrate our\napproach on text segmentation and speech recognition tasks. In addition to\nquantitative results, we also show that our approach can discover meaningful\nsegments in their respective application contexts.\n", "versions": [{"version": "v1", "created": "Fri, 24 Feb 2017 04:55:44 GMT"}, {"version": "v2", "created": "Mon, 27 Feb 2017 01:16:23 GMT"}, {"version": "v3", "created": "Sun, 19 Mar 2017 17:17:06 GMT"}, {"version": "v4", "created": "Wed, 7 Jun 2017 04:25:34 GMT"}, {"version": "v5", "created": "Mon, 19 Jun 2017 17:45:31 GMT"}, {"version": "v6", "created": "Fri, 20 Apr 2018 04:32:08 GMT"}, {"version": "v7", "created": "Wed, 18 Jul 2018 21:52:02 GMT"}], "update_date": "2018-07-20", "authors_parsed": [["Wang", "Chong", ""], ["Wang", "Yining", ""], ["Huang", "Po-Sen", ""], ["Mohamed", "Abdelrahman", ""], ["Zhou", "Dengyong", ""], ["Deng", "Li", ""]]}, {"id": "1702.07464", "submitter": "Briland Hitaj", "authors": "Briland Hitaj, Giuseppe Ateniese, Fernando Perez-Cruz", "title": "Deep Models Under the GAN: Information Leakage from Collaborative Deep\n  Learning", "comments": "ACM CCS'17, 16 pages, 18 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CR cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Deep Learning has recently become hugely popular in machine learning,\nproviding significant improvements in classification accuracy in the presence\nof highly-structured and large databases.\n  Researchers have also considered privacy implications of deep learning.\nModels are typically trained in a centralized manner with all the data being\nprocessed by the same training algorithm. If the data is a collection of users'\nprivate data, including habits, personal pictures, geographical positions,\ninterests, and more, the centralized server will have access to sensitive\ninformation that could potentially be mishandled. To tackle this problem,\ncollaborative deep learning models have recently been proposed where parties\nlocally train their deep learning structures and only share a subset of the\nparameters in the attempt to keep their respective training sets private.\nParameters can also be obfuscated via differential privacy (DP) to make\ninformation extraction even more challenging, as proposed by Shokri and\nShmatikov at CCS'15.\n  Unfortunately, we show that any privacy-preserving collaborative deep\nlearning is susceptible to a powerful attack that we devise in this paper. In\nparticular, we show that a distributed, federated, or decentralized deep\nlearning approach is fundamentally broken and does not protect the training\nsets of honest participants. The attack we developed exploits the real-time\nnature of the learning process that allows the adversary to train a Generative\nAdversarial Network (GAN) that generates prototypical samples of the targeted\ntraining set that was meant to be private (the samples generated by the GAN are\nintended to come from the same distribution as the training data).\nInterestingly, we show that record-level DP applied to the shared parameters of\nthe model, as suggested in previous work, is ineffective (i.e., record-level DP\nis not designed to address our attack).\n", "versions": [{"version": "v1", "created": "Fri, 24 Feb 2017 05:00:37 GMT"}, {"version": "v2", "created": "Tue, 23 May 2017 20:21:27 GMT"}, {"version": "v3", "created": "Thu, 14 Sep 2017 16:38:23 GMT"}], "update_date": "2017-09-15", "authors_parsed": [["Hitaj", "Briland", ""], ["Ateniese", "Giuseppe", ""], ["Perez-Cruz", "Fernando", ""]]}, {"id": "1702.07492", "submitter": "Ahmed Qureshi", "authors": "Ahmed Hussain Qureshi, Yutaka Nakamura, Yuichiro Yoshikawa and Hiroshi\n  Ishiguro", "title": "Robot gains Social Intelligence through Multimodal Deep Reinforcement\n  Learning", "comments": "The paper is published in IEEE-RAS International Conference on\n  Humanoid Robots (Humanoids) 2016", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.RO cs.AI cs.CV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  For robots to coexist with humans in a social world like ours, it is crucial\nthat they possess human-like social interaction skills. Programming a robot to\npossess such skills is a challenging task. In this paper, we propose a\nMultimodal Deep Q-Network (MDQN) to enable a robot to learn human-like\ninteraction skills through a trial and error method. This paper aims to develop\na robot that gathers data during its interaction with a human and learns human\ninteraction behaviour from the high-dimensional sensory information using\nend-to-end reinforcement learning. This paper demonstrates that the robot was\nable to learn basic interaction skills successfully, after 14 days of\ninteracting with people.\n", "versions": [{"version": "v1", "created": "Fri, 24 Feb 2017 08:30:43 GMT"}], "update_date": "2017-02-27", "authors_parsed": [["Qureshi", "Ahmed Hussain", ""], ["Nakamura", "Yutaka", ""], ["Yoshikawa", "Yuichiro", ""], ["Ishiguro", "Hiroshi", ""]]}, {"id": "1702.07549", "submitter": "Cl\\'elia De Mulatier Ph.D.", "authors": "Alberto Beretta, Claudia Battistin, Cl\\'elia de Mulatier, Iacopo\n  Mastromatteo, Matteo Marsili", "title": "The Stochastic complexity of spin models: Are pairwise models really\n  simple?", "comments": null, "journal-ref": null, "doi": "10.3390/e20100739", "report-no": null, "categories": "cond-mat.dis-nn stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Models can be simple for different reasons: because they yield a simple and\ncomputationally efficient interpretation of a generic dataset (e.g. in terms of\npairwise dependences) - as in statistical learning - or because they capture\nthe essential ingredients of a specific phenomenon - as e.g. in physics -\nleading to non-trivial falsifiable predictions. In information theory and\nBayesian inference, the simplicity of a model is precisely quantified in the\nstochastic complexity, which measures the number of bits needed to encode its\nparameters. In order to understand how simple models look like, we study the\nstochastic complexity of spin models with interactions of arbitrary order. We\nhighlight the existence of invariances with respect to bijections within the\nspace of operators, which allow us to partition the space of all models into\nequivalence classes, in which models share the same complexity. We thus found\nthat the complexity (or simplicity) of a model is not determined by the order\nof the interactions, but rather by their mutual arrangements. Models where\nstatistical dependencies are localized on non-overlapping groups of few\nvariables (and that afford predictions on independencies that are easy to\nfalsify) are simple. On the contrary, fully connected pairwise models, which\nare often used in statistical learning, appear to be highly complex, because of\ntheir extended set of interactions.\n", "versions": [{"version": "v1", "created": "Fri, 24 Feb 2017 11:55:13 GMT"}, {"version": "v2", "created": "Wed, 12 Apr 2017 07:58:42 GMT"}, {"version": "v3", "created": "Wed, 11 Apr 2018 18:11:53 GMT"}], "update_date": "2018-10-17", "authors_parsed": [["Beretta", "Alberto", ""], ["Battistin", "Claudia", ""], ["de Mulatier", "Cl\u00e9lia", ""], ["Mastromatteo", "Iacopo", ""], ["Marsili", "Matteo", ""]]}, {"id": "1702.07552", "submitter": "Muhammad Farooq", "authors": "Muhammad Farooq and Ingo Steinwart", "title": "Learning Rates for Kernel-Based Expectile Regression", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Conditional expectiles are becoming an increasingly important tool in finance\nas well as in other areas of applications. We analyse a support vector machine\ntype approach for estimating conditional expectiles and establish learning\nrates that are minimax optimal modulo a logarithmic factor if Gaussian RBF\nkernels are used and the desired expectile is smooth in a Besov sense. As a\nspecial case, our learning rates improve the best known rates for kernel-based\nleast squares regression in this scenario. Key ingredients of our statistical\nanalysis are a general calibration inequality for the asymmetric least squares\nloss, a corresponding variance bound as well as an improved entropy number\nbound for Gaussian RBF kernels.\n", "versions": [{"version": "v1", "created": "Fri, 24 Feb 2017 12:06:47 GMT"}, {"version": "v2", "created": "Mon, 27 Feb 2017 11:09:08 GMT"}], "update_date": "2017-02-28", "authors_parsed": [["Farooq", "Muhammad", ""], ["Steinwart", "Ingo", ""]]}, {"id": "1702.07608", "submitter": "Hongchao Song", "authors": "Hongchao Song, Yunpeng Li, Mark Coates, Aidong Men", "title": "Microwave breast cancer detection using Empirical Mode Decomposition\n  features", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Microwave-based breast cancer detection has been proposed as a complementary\napproach to compensate for some drawbacks of existing breast cancer detection\ntechniques. Among the existing microwave breast cancer detection methods,\nmachine learning-type algorithms have recently become more popular. These focus\non detecting the existence of breast tumours rather than performing imaging to\nidentify the exact tumour position. A key step of the machine learning\napproaches is feature extraction. One of the most widely used feature\nextraction method is principle component analysis (PCA). However, it can be\nsensitive to signal misalignment. This paper presents an empirical mode\ndecomposition (EMD)-based feature extraction method, which is more robust to\nthe misalignment. Experimental results involving clinical data sets combined\nwith numerically simulated tumour responses show that combined features from\nEMD and PCA improve the detection performance with an ensemble selection-based\nclassifier.\n", "versions": [{"version": "v1", "created": "Fri, 24 Feb 2017 14:42:06 GMT"}], "update_date": "2017-02-27", "authors_parsed": [["Song", "Hongchao", ""], ["Li", "Yunpeng", ""], ["Coates", "Mark", ""], ["Men", "Aidong", ""]]}, {"id": "1702.07630", "submitter": "Yannick Deville", "authors": "Charlotte Revel, Yannick Deville, V\\'eronique Achard, Xavier Briottet", "title": "Inertia-Constrained Pixel-by-Pixel Nonnegative Matrix Factorisation: a\n  Hyperspectral Unmixing Method Dealing with Intra-class Variability", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME cs.CV physics.data-an stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Blind source separation is a common processing tool to analyse the\nconstitution of pixels of hyperspectral images. Such methods usually suppose\nthat pure pixel spectra (endmembers) are the same in all the image for each\nclass of materials. In the framework of remote sensing, such an assumption is\nno more valid in the presence of intra-class variabilities due to illumination\nconditions, weathering, slight variations of the pure materials, etc... In this\npaper, we first describe the results of investigations highlighting intra-class\nvariability measured in real images. Considering these results, a new\nformulation of the linear mixing model is presented leading to two new methods.\nUnconstrained Pixel-by-pixel NMF (UP-NMF) is a new blind source separation\nmethod based on the assumption of a linear mixing model, which can deal with\nintra-class variability. To overcome UP-NMF limitations an extended method is\nproposed, named Inertia-constrained Pixel-by-pixel NMF (IP-NMF). For each\nsensed spectrum, these extended versions of NMF extract a corresponding set of\nsource spectra. A constraint is set to limit the spreading of each source's\nestimates in IP-NMF. The methods are tested on a semi-synthetic data set built\nwith spectra extracted from a real hyperspectral image and then numerically\nmixed. We thus demonstrate the interest of our methods for realistic source\nvariabilities. Finally, IP-NMF is tested on a real data set and it is shown to\nyield better performance than state of the art methods.\n", "versions": [{"version": "v1", "created": "Fri, 24 Feb 2017 15:43:10 GMT"}], "update_date": "2017-02-27", "authors_parsed": [["Revel", "Charlotte", ""], ["Deville", "Yannick", ""], ["Achard", "V\u00e9ronique", ""], ["Briottet", "Xavier", ""]]}, {"id": "1702.07652", "submitter": "Mahdi Imani", "authors": "Mahdi Imani and Ulisses Braga-Neto", "title": "Control of Gene Regulatory Networks with Noisy Measurements and\n  Uncertain Inputs", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "q-bio.MN cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper is concerned with the problem of stochastic control of gene\nregulatory networks (GRNs) observed indirectly through noisy measurements and\nwith uncertainty in the intervention inputs. The partial observability of the\ngene states and uncertainty in the intervention process are accounted for by\nmodeling GRNs using the partially-observed Boolean dynamical system (POBDS)\nsignal model with noisy gene expression measurements. Obtaining the optimal\ninfinite-horizon control strategy for this problem is not attainable in\ngeneral, and we apply reinforcement learning and Gaussian process techniques to\nfind a near-optimal solution. The POBDS is first transformed to a\ndirectly-observed Markov Decision Process in a continuous belief space, and the\nGaussian process is used for modeling the cost function over the belief and\nintervention spaces. Reinforcement learning then is used to learn the cost\nfunction from the available gene expression data. In addition, we employ\nsparsification, which enables the control of large partially-observed GRNs. The\nperformance of the resulting algorithm is studied through a comprehensive set\nof numerical experiments using synthetic gene expression data generated from a\nmelanoma gene regulatory network.\n", "versions": [{"version": "v1", "created": "Fri, 24 Feb 2017 16:32:57 GMT"}], "update_date": "2017-02-27", "authors_parsed": [["Imani", "Mahdi", ""], ["Braga-Neto", "Ulisses", ""]]}, {"id": "1702.07679", "submitter": "Alfredo Nava-Tudela", "authors": "Alfredo Nava-Tudela", "title": "A recommender system to restore images with impulse noise", "comments": "22 pages, 34 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We build a collaborative filtering recommender system to restore images with\nimpulse noise for which the noisy pixels have been previously identified. We\ndefine this recommender system in terms of a new color image representation\nusing three matrices that depend on the noise-free pixels of the image to\nrestore, and two parameters: $k$, the number of features; and $\\lambda$, the\nregularization factor. We perform experiments on a well known image database to\ntest our algorithm and we provide image quality statistics for the results\nobtained. We discuss the roles of bias and variance in the performance of our\nalgorithm as determined by the values of $k$ and $\\lambda$, and provide\nguidance on how to choose the values of these parameters. Finally, we discuss\nthe possibility of using our collaborative filtering recommender system to\nperform image inpainting and super-resolution.\n", "versions": [{"version": "v1", "created": "Fri, 24 Feb 2017 17:36:46 GMT"}], "update_date": "2017-02-27", "authors_parsed": [["Nava-Tudela", "Alfredo", ""]]}, {"id": "1702.07680", "submitter": "Cem Sahin", "authors": "Cem Safak Sahin, Rajmonda S. Caceres, Brandon Oselio, William M.\n  Campbell", "title": "Consistent Alignment of Word Embedding Models", "comments": "4 pages, 2 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.IR stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Word embedding models offer continuous vector representations that can\ncapture rich contextual semantics based on their word co-occurrence patterns.\nWhile these word vectors can provide very effective features used in many NLP\ntasks such as clustering similar words and inferring learning relationships,\nmany challenges and open research questions remain. In this paper, we propose a\nsolution that aligns variations of the same model (or different models) in a\njoint low-dimensional latent space leveraging carefully generated synthetic\ndata points. This generative process is inspired by the observation that a\nvariety of linguistic relationships is captured by simple linear operations in\nembedded space. We demonstrate that our approach can lead to substantial\nimprovements in recovering embeddings of local neighborhoods.\n", "versions": [{"version": "v1", "created": "Fri, 24 Feb 2017 17:40:28 GMT"}], "update_date": "2017-02-27", "authors_parsed": [["Sahin", "Cem Safak", ""], ["Caceres", "Rajmonda S.", ""], ["Oselio", "Brandon", ""], ["Campbell", "William M.", ""]]}, {"id": "1702.07694", "submitter": "Stephen Pallone", "authors": "Stephen N. Pallone, Peter I. Frazier, and Shane G. Henderson", "title": "Bayes-Optimal Entropy Pursuit for Active Choice-Based Preference\n  Learning", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.IT cs.LG math.IT", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We analyze the problem of learning a single user's preferences in an active\nlearning setting, sequentially and adaptively querying the user over a finite\ntime horizon. Learning is conducted via choice-based queries, where the user\nselects her preferred option among a small subset of offered alternatives.\nThese queries have been shown to be a robust and efficient way to learn an\nindividual's preferences. We take a parametric approach and model the user's\npreferences through a linear classifier, using a Bayesian prior to encode our\ncurrent knowledge of this classifier. The rate at which we learn depends on the\nalternatives offered at every time epoch. Under certain noise assumptions, we\nshow that the Bayes-optimal policy for maximally reducing entropy of the\nposterior distribution of this linear classifier is a greedy policy, and that\nthis policy achieves a linear lower bound when alternatives can be constructed\nfrom the continuum. Further, we analyze a different metric called\nmisclassification error, proving that the performance of the optimal policy\nthat minimizes misclassification error is bounded below by a linear function of\ndifferential entropy. Lastly, we numerically compare the greedy entropy\nreduction policy with a knowledge gradient policy under a number of scenarios,\nexamining their performance under both differential entropy and\nmisclassification error.\n", "versions": [{"version": "v1", "created": "Fri, 24 Feb 2017 18:28:18 GMT"}], "update_date": "2017-02-27", "authors_parsed": [["Pallone", "Stephen N.", ""], ["Frazier", "Peter I.", ""], ["Henderson", "Shane G.", ""]]}, {"id": "1702.07709", "submitter": "Simon Du", "authors": "Simon S. Du, Sivaraman Balakrishnan, Aarti Singh", "title": "Computationally Efficient Robust Estimation of Sparse Functionals", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.DS cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Many conventional statistical procedures are extremely sensitive to seemingly\nminor deviations from modeling assumptions. This problem is exacerbated in\nmodern high-dimensional settings, where the problem dimension can grow with and\npossibly exceed the sample size. We consider the problem of robust estimation\nof sparse functionals, and provide a computationally and statistically\nefficient algorithm in the high-dimensional setting. Our theory identifies a\nunified set of deterministic conditions under which our algorithm guarantees\naccurate recovery. By further establishing that these deterministic conditions\nhold with high-probability for a wide range of statistical models, our theory\napplies to many problems of considerable interest including sparse mean and\ncovariance estimation; sparse linear regression; and sparse generalized linear\nmodels.\n", "versions": [{"version": "v1", "created": "Fri, 24 Feb 2017 18:59:08 GMT"}], "update_date": "2017-02-27", "authors_parsed": [["Du", "Simon S.", ""], ["Balakrishnan", "Sivaraman", ""], ["Singh", "Aarti", ""]]}, {"id": "1702.07780", "submitter": "Augustus Odena", "authors": "Augustus Odena, Dieterich Lawson, Christopher Olah", "title": "Changing Model Behavior at Test-Time Using Reinforcement Learning", "comments": "Submitted to ICLR 2017 Workshop Track", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Machine learning models are often used at test-time subject to constraints\nand trade-offs not present at training-time. For example, a computer vision\nmodel operating on an embedded device may need to perform real-time inference,\nor a translation model operating on a cell phone may wish to bound its average\ncompute time in order to be power-efficient. In this work we describe a\nmixture-of-experts model and show how to change its test-time resource-usage on\na per-input basis using reinforcement learning. We test our method on a small\nMNIST-based example.\n", "versions": [{"version": "v1", "created": "Fri, 24 Feb 2017 22:00:41 GMT"}], "update_date": "2017-02-28", "authors_parsed": [["Odena", "Augustus", ""], ["Lawson", "Dieterich", ""], ["Olah", "Christopher", ""]]}, {"id": "1702.07790", "submitter": "Mark Harmon", "authors": "Mark Harmon, Diego Klabjan", "title": "Activation Ensembles for Deep Neural Networks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Many activation functions have been proposed in the past, but selecting an\nadequate one requires trial and error. We propose a new methodology of\ndesigning activation functions within a neural network at each layer. We call\nthis technique an \"activation ensemble\" because it allows the use of multiple\nactivation functions at each layer. This is done by introducing additional\nvariables, $\\alpha$, at each activation layer of a network to allow for\nmultiple activation functions to be active at each neuron. By design,\nactivations with larger $\\alpha$ values at a neuron is equivalent to having the\nlargest magnitude. Hence, those higher magnitude activations are \"chosen\" by\nthe network. We implement the activation ensembles on a variety of datasets\nusing an array of Feed Forward and Convolutional Neural Networks. By using the\nactivation ensemble, we achieve superior results compared to traditional\ntechniques. In addition, because of the flexibility of this methodology, we\nmore deeply explore activation functions and the features that they capture.\n", "versions": [{"version": "v1", "created": "Fri, 24 Feb 2017 22:30:29 GMT"}], "update_date": "2017-02-28", "authors_parsed": [["Harmon", "Mark", ""], ["Klabjan", "Diego", ""]]}, {"id": "1702.07798", "submitter": "Swayambhoo Jain", "authors": "Swayambhoo Jain, Akshay Soni, Nikolay Laptev, and Yashar Mehdad", "title": "Rank-to-engage: New Listwise Approaches to Maximize Engagement", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  For many internet businesses, presenting a given list of items in an order\nthat maximizes a certain metric of interest (e.g., click-through-rate, average\nengagement time etc.) is crucial. We approach the aforementioned task from a\nlearning-to-rank perspective which reveals a new problem setup. In traditional\nlearning-to-rank literature, it is implicitly assumed that during the training\ndata generation one has access to the \\emph{best or desired} order for the\ngiven list of items. In this work, we consider a problem setup where we do not\nobserve the desired ranking. We present two novel solutions: the first solution\nis an extension of already existing listwise learning-to-rank\ntechnique--Listwise maximum likelihood estimation (ListMLE)--while the second\none is a generic machine learning based framework that tackles the problem in\nits entire generality. We discuss several challenges associated with this\ngeneric framework, and propose a simple \\emph{item-payoff} and\n\\emph{positional-gain} model that addresses these challenges. We provide\ntraining algorithms, inference procedures, and demonstrate the effectiveness of\nthe two approaches over traditional ListMLE on synthetic as well as on\nreal-life setting of ranking news articles for increased dwell time.\n", "versions": [{"version": "v1", "created": "Fri, 24 Feb 2017 23:20:03 GMT"}], "update_date": "2017-02-28", "authors_parsed": [["Jain", "Swayambhoo", ""], ["Soni", "Akshay", ""], ["Laptev", "Nikolay", ""], ["Mehdad", "Yashar", ""]]}, {"id": "1702.07800", "submitter": "Haohan Wang", "authors": "Haohan Wang and Bhiksha Raj", "title": "On the Origin of Deep Learning", "comments": "70 pages, 200 references", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.NE stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper is a review of the evolutionary history of deep learning models.\nIt covers from the genesis of neural networks when associationism modeling of\nthe brain is studied, to the models that dominate the last decade of research\nin deep learning like convolutional neural networks, deep belief networks, and\nrecurrent neural networks. In addition to a review of these models, this paper\nprimarily focuses on the precedents of the models above, examining how the\ninitial ideas are assembled to construct the early models and how these\npreliminary models are developed into their current forms. Many of these\nevolutionary paths last more than half a century and have a diversity of\ndirections. For example, CNN is built on prior knowledge of biological vision\nsystem; DBN is evolved from a trade-off of modeling power and computation\ncomplexity of graphical models and many nowadays models are neural counterparts\nof ancient linear models. This paper reviews these evolutionary paths and\noffers a concise thought flow of how these models are developed, and aims to\nprovide a thorough background for deep learning. More importantly, along with\nthe path, this paper summarizes the gist behind these milestones and proposes\nmany directions to guide the future research of deep learning.\n", "versions": [{"version": "v1", "created": "Fri, 24 Feb 2017 23:30:08 GMT"}, {"version": "v2", "created": "Wed, 1 Mar 2017 08:30:41 GMT"}, {"version": "v3", "created": "Thu, 2 Mar 2017 01:41:09 GMT"}, {"version": "v4", "created": "Fri, 3 Mar 2017 03:03:32 GMT"}], "update_date": "2017-03-06", "authors_parsed": [["Wang", "Haohan", ""], ["Raj", "Bhiksha", ""]]}, {"id": "1702.07803", "submitter": "Shashank Singh", "authors": "Shashank Singh, Barnab\\'as P{\\o}czos", "title": "Nonparanormal Information Estimation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST cs.IT math.IT stat.ML stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study the problem of using i.i.d. samples from an unknown multivariate\nprobability distribution $p$ to estimate the mutual information of $p$. This\nproblem has recently received attention in two settings: (1) where $p$ is\nassumed to be Gaussian and (2) where $p$ is assumed only to lie in a large\nnonparametric smoothness class. Estimators proposed for the Gaussian case\nconverge in high dimensions when the Gaussian assumption holds, but are\nbrittle, failing dramatically when $p$ is not Gaussian. Estimators proposed for\nthe nonparametric case fail to converge with realistic sample sizes except in\nvery low dimensions. As a result, there is a lack of robust mutual information\nestimators for many realistic data. To address this, we propose estimators for\nmutual information when $p$ is assumed to be a nonparanormal (a.k.a., Gaussian\ncopula) model, a semiparametric compromise between Gaussian and nonparametric\nextremes. Using theoretical bounds and experiments, we show these estimators\nstrike a practical balance between robustness and scaling with dimensionality.\n", "versions": [{"version": "v1", "created": "Fri, 24 Feb 2017 23:43:06 GMT"}], "update_date": "2017-02-28", "authors_parsed": [["Singh", "Shashank", ""], ["P\u00f8czos", "Barnab\u00e1s", ""]]}, {"id": "1702.07811", "submitter": "Tolga Bolukbasi", "authors": "Tolga Bolukbasi, Joseph Wang, Ofer Dekel, Venkatesh Saligrama", "title": "Adaptive Neural Networks for Efficient Inference", "comments": null, "journal-ref": "Proceedings of the 34th International Conference on Machine\n  Learning, PMLR 70:527-536, 2017", "doi": null, "report-no": null, "categories": "cs.LG cs.CV cs.NE stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present an approach to adaptively utilize deep neural networks in order to\nreduce the evaluation time on new examples without loss of accuracy. Rather\nthan attempting to redesign or approximate existing networks, we propose two\nschemes that adaptively utilize networks. We first pose an adaptive network\nevaluation scheme, where we learn a system to adaptively choose the components\nof a deep network to be evaluated for each example. By allowing examples\ncorrectly classified using early layers of the system to exit, we avoid the\ncomputational time associated with full evaluation of the network. We extend\nthis to learn a network selection system that adaptively selects the network to\nbe evaluated for each example. We show that computational time can be\ndramatically reduced by exploiting the fact that many examples can be correctly\nclassified using relatively efficient networks and that complex,\ncomputationally costly networks are only necessary for a small fraction of\nexamples. We pose a global objective for learning an adaptive early exit or\nnetwork selection policy and solve it by reducing the policy learning problem\nto a layer-by-layer weighted binary classification problem. Empirically, these\napproaches yield dramatic reductions in computational cost, with up to a 2.8x\nspeedup on state-of-the-art networks from the ImageNet image recognition\nchallenge with minimal (<1%) loss of top5 accuracy.\n", "versions": [{"version": "v1", "created": "Sat, 25 Feb 2017 00:22:51 GMT"}, {"version": "v2", "created": "Mon, 18 Sep 2017 18:14:49 GMT"}], "update_date": "2017-09-20", "authors_parsed": [["Bolukbasi", "Tolga", ""], ["Wang", "Joseph", ""], ["Dekel", "Ofer", ""], ["Saligrama", "Venkatesh", ""]]}, {"id": "1702.07834", "submitter": "Weiran Wang", "authors": "Jialei Wang, Weiran Wang, Dan Garber, Nathan Srebro", "title": "Efficient coordinate-wise leading eigenvector computation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NA cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We develop and analyze efficient \"coordinate-wise\" methods for finding the\nleading eigenvector, where each step involves only a vector-vector product. We\nestablish global convergence with overall runtime guarantees that are at least\nas good as Lanczos's method and dominate it for slowly decaying spectrum. Our\nmethods are based on combining a shift-and-invert approach with coordinate-wise\nalgorithms for linear regression.\n", "versions": [{"version": "v1", "created": "Sat, 25 Feb 2017 05:11:25 GMT"}], "update_date": "2017-02-28", "authors_parsed": [["Wang", "Jialei", ""], ["Wang", "Weiran", ""], ["Garber", "Dan", ""], ["Srebro", "Nathan", ""]]}, {"id": "1702.07884", "submitter": "Mehran Safayani", "authors": "Mehran Safayani, Seyed Hashem Ahmadi, Homayun Afrabandpey and\n  Abdolreza Mirzaei", "title": "An EM Based Probabilistic Two-Dimensional CCA with Application to Face\n  Recognition", "comments": null, "journal-ref": null, "doi": "10.1007/s10489-017-1012-2", "report-no": null, "categories": "cs.CV cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recently, two-dimensional canonical correlation analysis (2DCCA) has been\nsuccessfully applied for image feature extraction. The method instead of\nconcatenating the columns of the images to the one-dimensional vectors,\ndirectly works with two-dimensional image matrices. Although 2DCCA works well\nin different recognition tasks, it lacks a probabilistic interpretation. In\nthis paper, we present a probabilistic framework for 2DCCA called probabilistic\n2DCCA (P2DCCA) and an iterative EM based algorithm for optimizing the\nparameters. Experimental results on synthetic and real data demonstrate\nsuperior performance in loading factor estimation for P2DCCA compared to 2DCCA.\nFor real data, three subsets of AR face database and also the UMIST face\ndatabase confirm the robustness of the proposed algorithm in face recognition\ntasks with different illumination conditions, facial expressions, poses and\nocclusions.\n", "versions": [{"version": "v1", "created": "Sat, 25 Feb 2017 12:50:35 GMT"}], "update_date": "2017-08-07", "authors_parsed": [["Safayani", "Mehran", ""], ["Ahmadi", "Seyed Hashem", ""], ["Afrabandpey", "Homayun", ""], ["Mirzaei", "Abdolreza", ""]]}, {"id": "1702.07933", "submitter": "Zilong Tan", "authors": "Zilong Tan and Sayan Mukherjee", "title": "Efficient Learning of Mixed Membership Models", "comments": "23 pages, Proceedings of the 34th International Conference on Machine\n  Learning (ICML), Sydney, Australia, 2017", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present an efficient algorithm for learning mixed membership models when\nthe number of variables $p$ is much larger than the number of hidden components\n$k$. This algorithm reduces the computational complexity of state-of-the-art\ntensor methods, which require decomposing an $O\\left(p^3\\right)$ tensor, to\nfactorizing $O\\left(p/k\\right)$ sub-tensors each of size $O\\left(k^3\\right)$.\nIn addition, we address the issue of negative entries in the empirical method\nof moments based estimators. We provide sufficient conditions under which our\napproach has provable guarantees. Our approach obtains competitive empirical\nresults on both simulated and real data.\n", "versions": [{"version": "v1", "created": "Sat, 25 Feb 2017 18:00:57 GMT"}, {"version": "v2", "created": "Sun, 16 Apr 2017 19:53:50 GMT"}, {"version": "v3", "created": "Sun, 2 Jul 2017 21:03:37 GMT"}], "update_date": "2017-07-04", "authors_parsed": [["Tan", "Zilong", ""], ["Mukherjee", "Sayan", ""]]}, {"id": "1702.07944", "submitter": "Simon Du", "authors": "Simon S. Du, Jianshu Chen, Lihong Li, Lin Xiao, Dengyong Zhou", "title": "Stochastic Variance Reduction Methods for Policy Evaluation", "comments": "Accepted by ICML 2017", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI cs.SY math.OC stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Policy evaluation is a crucial step in many reinforcement-learning\nprocedures, which estimates a value function that predicts states' long-term\nvalue under a given policy. In this paper, we focus on policy evaluation with\nlinear function approximation over a fixed dataset. We first transform the\nempirical policy evaluation problem into a (quadratic) convex-concave saddle\npoint problem, and then present a primal-dual batch gradient method, as well as\ntwo stochastic variance reduction methods for solving the problem. These\nalgorithms scale linearly in both sample size and feature dimension. Moreover,\nthey achieve linear convergence even when the saddle-point problem has only\nstrong concavity in the dual variables but no strong convexity in the primal\nvariables. Numerical experiments on benchmark problems demonstrate the\neffectiveness of our methods.\n", "versions": [{"version": "v1", "created": "Sat, 25 Feb 2017 20:15:55 GMT"}, {"version": "v2", "created": "Fri, 9 Jun 2017 06:02:47 GMT"}], "update_date": "2017-06-12", "authors_parsed": [["Du", "Simon S.", ""], ["Chen", "Jianshu", ""], ["Li", "Lihong", ""], ["Xiao", "Lin", ""], ["Zhou", "Dengyong", ""]]}, {"id": "1702.07956", "submitter": "Jia-Jie Zhu", "authors": "Jia-Jie Zhu, Jos\\'e Bento", "title": "Generative Adversarial Active Learning", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a new active learning by query synthesis approach using Generative\nAdversarial Networks (GAN). Different from regular active learning, the\nresulting algorithm adaptively synthesizes training instances for querying to\nincrease learning speed. We generate queries according to the uncertainty\nprinciple, but our idea can work with other active learning principles. We\nreport results from various numerical experiments to demonstrate the\neffectiveness the proposed approach. In some settings, the proposed algorithm\noutperforms traditional pool-based approaches. To the best our knowledge, this\nis the first active learning work using GAN.\n", "versions": [{"version": "v1", "created": "Sat, 25 Feb 2017 22:45:20 GMT"}, {"version": "v2", "created": "Tue, 28 Feb 2017 21:04:00 GMT"}, {"version": "v3", "created": "Sat, 20 May 2017 06:25:51 GMT"}, {"version": "v4", "created": "Sun, 28 May 2017 05:40:22 GMT"}, {"version": "v5", "created": "Wed, 15 Nov 2017 21:50:12 GMT"}], "update_date": "2017-11-17", "authors_parsed": [["Zhu", "Jia-Jie", ""], ["Bento", "Jos\u00e9", ""]]}, {"id": "1702.07958", "submitter": "Chicheng Zhang", "authors": "Alina Beygelzimer, Francesco Orabona, Chicheng Zhang", "title": "Efficient Online Bandit Multiclass Learning with $\\tilde{O}(\\sqrt{T})$\n  Regret", "comments": "22 pages, 2 figures; ICML 2017; this version includes additional\n  discussions of Newtron, and a variant of SOBA that directly uses an online\n  exp-concave optimization oracle", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present an efficient second-order algorithm with\n$\\tilde{O}(\\frac{1}{\\eta}\\sqrt{T})$ regret for the bandit online multiclass\nproblem. The regret bound holds simultaneously with respect to a family of loss\nfunctions parameterized by $\\eta$, for a range of $\\eta$ restricted by the norm\nof the competitor. The family of loss functions ranges from hinge loss\n($\\eta=0$) to squared hinge loss ($\\eta=1$). This provides a solution to the\nopen problem of (J. Abernethy and A. Rakhlin. An efficient bandit algorithm for\n$\\sqrt{T}$-regret in online multiclass prediction? In COLT, 2009). We test our\nalgorithm experimentally, showing that it also performs favorably against\nearlier algorithms.\n", "versions": [{"version": "v1", "created": "Sat, 25 Feb 2017 23:15:55 GMT"}, {"version": "v2", "created": "Tue, 13 Jun 2017 06:06:03 GMT"}, {"version": "v3", "created": "Wed, 17 Jan 2018 19:22:21 GMT"}], "update_date": "2018-01-19", "authors_parsed": [["Beygelzimer", "Alina", ""], ["Orabona", "Francesco", ""], ["Zhang", "Chicheng", ""]]}, {"id": "1702.07959", "submitter": "Abraham Smith", "authors": "Abraham Smith and Paul Bendich and John Harer and Alex Pieloch and Jay\n  Hineman", "title": "Supervised Learning of Labeled Pointcloud Differences via Cover-Tree\n  Entropy Reduction", "comments": "Distribution Statement A - Approved for public release, distribution\n  is unlimited. Version 2: added link to code, and some minor improvements.\n  Version 3: updated authors and thanks", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We introduce a new algorithm, called CDER, for supervised machine learning\nthat merges the multi-scale geometric properties of Cover Trees with the\ninformation-theoretic properties of entropy. CDER applies to a training set of\nlabeled pointclouds embedded in a common Euclidean space. If typical\npointclouds corresponding to distinct labels tend to differ at any scale in any\nsub-region, CDER can identify these differences in (typically) linear time,\ncreating a set of distributional coordinates which act as a feature extraction\nmechanism for supervised learning. We describe theoretical properties and\nimplementation details of CDER, and illustrate its benefits on several\nsynthetic examples.\n", "versions": [{"version": "v1", "created": "Sun, 26 Feb 2017 00:17:42 GMT"}, {"version": "v2", "created": "Wed, 28 Jun 2017 14:15:31 GMT"}, {"version": "v3", "created": "Fri, 19 Jan 2018 19:30:37 GMT"}], "update_date": "2018-01-23", "authors_parsed": [["Smith", "Abraham", ""], ["Bendich", "Paul", ""], ["Harer", "John", ""], ["Pieloch", "Alex", ""], ["Hineman", "Jay", ""]]}, {"id": "1702.07966", "submitter": "Alon Brutzkus", "authors": "Alon Brutzkus, Amir Globerson", "title": "Globally Optimal Gradient Descent for a ConvNet with Gaussian Inputs", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG math.OC stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Deep learning models are often successfully trained using gradient descent,\ndespite the worst case hardness of the underlying non-convex optimization\nproblem. The key question is then under what conditions can one prove that\noptimization will succeed. Here we provide a strong result of this kind. We\nconsider a neural net with one hidden layer and a convolutional structure with\nno overlap and a ReLU activation function. For this architecture we show that\nlearning is NP-complete in the general case, but that when the input\ndistribution is Gaussian, gradient descent converges to the global optimum in\npolynomial time. To the best of our knowledge, this is the first global\noptimality guarantee of gradient descent on a convolutional neural network with\nReLU activations.\n", "versions": [{"version": "v1", "created": "Sun, 26 Feb 2017 01:12:20 GMT"}], "update_date": "2017-02-28", "authors_parsed": [["Brutzkus", "Alon", ""], ["Globerson", "Amir", ""]]}, {"id": "1702.07976", "submitter": "Mert Al", "authors": "Mert Al, Shibiao Wan, Sun-Yuan Kung", "title": "Ratio Utility and Cost Analysis for Privacy Preserving Subspace\n  Projection", "comments": "Submitted to ICASSP 2017", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  With a rapidly increasing number of devices connected to the internet, big\ndata has been applied to various domains of human life. Nevertheless, it has\nalso opened new venues for breaching users' privacy. Hence it is highly\nrequired to develop techniques that enable data owners to privatize their data\nwhile keeping it useful for intended applications. Existing methods, however,\ndo not offer enough flexibility for controlling the utility-privacy trade-off\nand may incur unfavorable results when privacy requirements are high. To tackle\nthese drawbacks, we propose a compressive-privacy based method, namely RUCA\n(Ratio Utility and Cost Analysis), which can not only maximize performance for\na privacy-insensitive classification task but also minimize the ability of any\nclassifier to infer private information from the data. Experimental results on\nCensus and Human Activity Recognition data sets demonstrate that RUCA\nsignificantly outperforms existing privacy preserving data projection\ntechniques for a wide range of privacy pricings.\n", "versions": [{"version": "v1", "created": "Sun, 26 Feb 2017 02:14:05 GMT"}], "update_date": "2017-02-28", "authors_parsed": [["Al", "Mert", ""], ["Wan", "Shibiao", ""], ["Kung", "Sun-Yuan", ""]]}, {"id": "1702.08000", "submitter": "Rahul  Singh", "authors": "Rahul Singh and Taposh Banerjee", "title": "Kiefer Wolfowitz Algorithm is Asymptotically Optimal for a Class of\n  Non-Stationary Bandit Problems", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider the problem of designing an allocation rule or an \"online\nlearning algorithm\" for a class of bandit problems in which the set of control\nactions available at each time $s$ is a convex, compact subset of\n$\\mathbb{R}^d$. Upon choosing an action $x$ at time $s$, the algorithm obtains\na noisy value of the unknown and time-varying function $f_s$ evaluated at $x$.\nThe \"regret\" of an algorithm is the gap between its expected reward, and the\nreward earned by a strategy which has the knowledge of the function $f_s$ at\neach time $s$ and hence chooses the action $x_s$ that maximizes $f_s$.\n  For this non-stationary bandit problem set-up, we consider two variants of\nthe Kiefer Wolfowitz (KW) algorithm i) KW with fixed step-size $\\beta$, and ii)\nKW with sliding window of length $L$. We show that if the number of times that\nthe function $f_s$ varies during time $T$ is $o(T)$, and if the learning rates\nof the proposed algorithms are chosen \"optimally\", then the regret of the\nproposed algorithms is $o(T)$, and hence the algorithms are asymptotically\nefficient.\n", "versions": [{"version": "v1", "created": "Sun, 26 Feb 2017 08:24:11 GMT"}, {"version": "v2", "created": "Wed, 8 Mar 2017 08:38:08 GMT"}], "update_date": "2017-03-09", "authors_parsed": [["Singh", "Rahul", ""], ["Banerjee", "Taposh", ""]]}, {"id": "1702.08019", "submitter": "Kazuyoshi Yata", "authors": "Yugo Nakayama, Kazuyoshi Yata, Makoto Aoshima", "title": "Support vector machine and its bias correction in high-dimension,\n  low-sample-size settings", "comments": "23 pages, 3 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we consider asymptotic properties of the support vector\nmachine (SVM) in high-dimension, low-sample-size (HDLSS) settings. We show that\nthe hard-margin linear SVM holds a consistency property in which\nmisclassification rates tend to zero as the dimension goes to infinity under\ncertain severe conditions. We show that the SVM is very biased in HDLSS\nsettings and its performance is affected by the bias directly. In order to\novercome such difficulties, we propose a bias-corrected SVM (BC-SVM). We show\nthat the BC-SVM gives preferable performances in HDLSS settings. We also\ndiscuss the SVMs in multiclass HDLSS settings. Finally, we check the\nperformance of the classifiers in actual data analyses.\n", "versions": [{"version": "v1", "created": "Sun, 26 Feb 2017 10:38:39 GMT"}], "update_date": "2017-02-28", "authors_parsed": [["Nakayama", "Yugo", ""], ["Yata", "Kazuyoshi", ""], ["Aoshima", "Makoto", ""]]}, {"id": "1702.08134", "submitter": "Zhehui Chen", "authors": "Zhehui Chen, Lin F. Yang, Chris J. Li, Tuo Zhao", "title": "Dropping Convexity for More Efficient and Scalable Online Multiview\n  Learning", "comments": "A preliminary version appears in ICML 2017", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG math.OC stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Multiview representation learning is very popular for latent factor analysis.\nIt naturally arises in many data analysis, machine learning, and information\nretrieval applications to model dependent structures among multiple data\nsources. For computational convenience, existing approaches usually formulate\nthe multiview representation learning as convex optimization problems, where\nglobal optima can be obtained by certain algorithms in polynomial time.\nHowever, many pieces of evidence have corroborated that heuristic nonconvex\napproaches also have good empirical computational performance and convergence\nto the global optima, although there is a lack of theoretical justification.\nSuch a gap between theory and practice motivates us to study a nonconvex\nformulation for multiview representation learning, which can be efficiently\nsolved by a simple stochastic gradient descent (SGD) algorithm. We first\nillustrate the geometry of the nonconvex formulation; Then, we establish\nasymptotic global rates of convergence to the global optima by diffusion\napproximations. Numerical experiments are provided to support our theory.\n", "versions": [{"version": "v1", "created": "Mon, 27 Feb 2017 03:51:46 GMT"}, {"version": "v10", "created": "Mon, 16 Sep 2019 01:00:53 GMT"}, {"version": "v2", "created": "Thu, 2 Mar 2017 18:50:40 GMT"}, {"version": "v3", "created": "Fri, 24 Mar 2017 00:21:18 GMT"}, {"version": "v4", "created": "Mon, 5 Jun 2017 22:20:23 GMT"}, {"version": "v5", "created": "Sat, 22 Jul 2017 17:58:40 GMT"}, {"version": "v6", "created": "Sat, 14 Oct 2017 19:19:19 GMT"}, {"version": "v7", "created": "Wed, 23 May 2018 13:47:47 GMT"}, {"version": "v8", "created": "Thu, 31 May 2018 18:41:42 GMT"}, {"version": "v9", "created": "Sun, 6 Jan 2019 15:46:41 GMT"}], "update_date": "2019-09-17", "authors_parsed": [["Chen", "Zhehui", ""], ["Yang", "Lin F.", ""], ["Li", "Chris J.", ""], ["Zhao", "Tuo", ""]]}, {"id": "1702.08142", "submitter": "Mahito Sugiyama", "authors": "Mahito Sugiyama and Hiroyuki Nakahara and Koji Tsuda", "title": "Tensor Balancing on Statistical Manifold", "comments": "19 pages, 5 figures, accepted to the 34th International Conference on\n  Machine Learning (ICML 2017)", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME cs.IT cs.NA math.IT stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We solve tensor balancing, rescaling an Nth order nonnegative tensor by\nmultiplying N tensors of order N - 1 so that every fiber sums to one. This\ngeneralizes a fundamental process of matrix balancing used to compare matrices\nin a wide range of applications from biology to economics. We present an\nefficient balancing algorithm with quadratic convergence using Newton's method\nand show in numerical experiments that the proposed algorithm is several orders\nof magnitude faster than existing ones. To theoretically prove the correctness\nof the algorithm, we model tensors as probability distributions in a\nstatistical manifold and realize tensor balancing as projection onto a\nsubmanifold. The key to our algorithm is that the gradient of the manifold,\nused as a Jacobian matrix in Newton's method, can be analytically obtained\nusing the Moebius inversion formula, the essential of combinatorial\nmathematics. Our model is not limited to tensor balancing, but has a wide\napplicability as it includes various statistical and machine learning models\nsuch as weighted DAGs and Boltzmann machines.\n", "versions": [{"version": "v1", "created": "Mon, 27 Feb 2017 04:30:06 GMT"}, {"version": "v2", "created": "Tue, 13 Jun 2017 15:43:01 GMT"}, {"version": "v3", "created": "Mon, 29 Oct 2018 15:18:28 GMT"}], "update_date": "2018-10-30", "authors_parsed": [["Sugiyama", "Mahito", ""], ["Nakahara", "Hiroyuki", ""], ["Tsuda", "Koji", ""]]}, {"id": "1702.08159", "submitter": "J. D. Curt\\'o", "authors": "J. D. Curt\\'o and I. C. Zarza and Feng Yang and Alex Smola and\n  Fernando de la Torre and Chong Wah Ngo and Luc van Gool", "title": "McKernel: A Library for Approximate Kernel Expansions in Log-linear Time", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  McKernel introduces a framework to use kernel approximates in the mini-batch\nsetting with Stochastic Gradient Descent (SGD) as an alternative to Deep\nLearning. Based on Random Kitchen Sinks [Rahimi and Recht 2007], we provide a\nC++ library for Large-scale Machine Learning. It contains a CPU optimized\nimplementation of the algorithm in [Le et al. 2013], that allows the\ncomputation of approximated kernel expansions in log-linear time. The algorithm\nrequires to compute the product of matrices Walsh Hadamard. A cache friendly\nFast Walsh Hadamard that achieves compelling speed and outperforms current\nstate-of-the-art methods has been developed. McKernel establishes the\nfoundation of a new architecture of learning that allows to obtain large-scale\nnon-linear classification combining lightning kernel expansions and a linear\nclassifier. It travails in the mini-batch setting working analogously to Neural\nNetworks. We show the validity of our method through extensive experiments on\nMNIST and FASHION MNIST [Xiao et al. 2017].\n", "versions": [{"version": "v1", "created": "Mon, 27 Feb 2017 06:30:47 GMT"}, {"version": "v10", "created": "Sun, 6 Jan 2019 22:23:07 GMT"}, {"version": "v11", "created": "Thu, 24 Jan 2019 16:57:42 GMT"}, {"version": "v12", "created": "Wed, 20 Feb 2019 16:43:18 GMT"}, {"version": "v13", "created": "Sun, 24 Mar 2019 17:06:08 GMT"}, {"version": "v14", "created": "Tue, 31 Dec 2019 18:54:43 GMT"}, {"version": "v15", "created": "Fri, 17 Apr 2020 16:47:47 GMT"}, {"version": "v2", "created": "Tue, 27 Mar 2018 17:23:07 GMT"}, {"version": "v3", "created": "Thu, 19 Apr 2018 13:00:28 GMT"}, {"version": "v4", "created": "Tue, 24 Apr 2018 11:20:51 GMT"}, {"version": "v5", "created": "Thu, 10 May 2018 12:20:03 GMT"}, {"version": "v6", "created": "Sun, 20 May 2018 12:03:42 GMT"}, {"version": "v7", "created": "Wed, 30 May 2018 17:07:46 GMT"}, {"version": "v8", "created": "Thu, 31 May 2018 06:26:35 GMT"}, {"version": "v9", "created": "Sun, 10 Jun 2018 11:04:31 GMT"}], "update_date": "2020-04-20", "authors_parsed": [["Curt\u00f3", "J. D.", ""], ["Zarza", "I. C.", ""], ["Yang", "Feng", ""], ["Smola", "Alex", ""], ["de la Torre", "Fernando", ""], ["Ngo", "Chong Wah", ""], ["van Gool", "Luc", ""]]}, {"id": "1702.08185", "submitter": "Andreas Mayr", "authors": "Andreas Mayr, Benjamin Hofner, Elisabeth Waldmann, Tobias Hepp, Olaf\n  Gefeller, Matthias Schmid", "title": "An update on statistical boosting in biomedicine", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP stat.CO stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Statistical boosting algorithms have triggered a lot of research during the\nlast decade. They combine a powerful machine-learning approach with classical\nstatistical modelling, offering various practical advantages like automated\nvariable selection and implicit regularization of effect estimates. They are\nextremely flexible, as the underlying base-learners (regression functions\ndefining the type of effect for the explanatory variables) can be combined with\nany kind of loss function (target function to be optimized, defining the type\nof regression setting). In this review article, we highlight the most recent\nmethodological developments on statistical boosting regarding variable\nselection, functional regression and advanced time-to-event modelling.\nAdditionally, we provide a short overview on relevant applications of\nstatistical boosting in biomedicine.\n", "versions": [{"version": "v1", "created": "Mon, 27 Feb 2017 08:33:26 GMT"}], "update_date": "2017-02-28", "authors_parsed": [["Mayr", "Andreas", ""], ["Hofner", "Benjamin", ""], ["Waldmann", "Elisabeth", ""], ["Hepp", "Tobias", ""], ["Gefeller", "Olaf", ""], ["Schmid", "Matthias", ""]]}, {"id": "1702.08211", "submitter": "Sebastien Gerchinovitz", "authors": "Nicol\\`o Cesa-Bianchi, Pierre Gaillard (SIERRA), Claudio Gentile,\n  S\\'ebastien Gerchinovitz (IMT)", "title": "Algorithmic Chaining and the Role of Partial Feedback in Online\n  Nonparametric Learning", "comments": "This document is the full version of an extended abstract accepted\n  for presentation at COLT 2017", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We investigate contextual online learning with nonparametric (Lipschitz)\ncomparison classes under different assumptions on losses and feedback\ninformation. For full information feedback and Lipschitz losses, we design the\nfirst explicit algorithm achieving the minimax regret rate (up to log factors).\nIn a partial feedback model motivated by second-price auctions, we obtain\nalgorithms for Lipschitz and semi-Lipschitz losses with regret bounds improving\non the known bounds for standard bandit feedback. Our analysis combines novel\nresults for contextual second-price auctions with a novel algorithmic approach\nbased on chaining. When the context space is Euclidean, our chaining approach\nis efficient and delivers an even better regret bound.\n", "versions": [{"version": "v1", "created": "Mon, 27 Feb 2017 10:01:36 GMT"}, {"version": "v2", "created": "Fri, 30 Jun 2017 08:19:49 GMT"}], "update_date": "2017-07-03", "authors_parsed": [["Cesa-Bianchi", "Nicol\u00f2", "", "SIERRA"], ["Gaillard", "Pierre", "", "SIERRA"], ["Gentile", "Claudio", "", "IMT"], ["Gerchinovitz", "S\u00e9bastien", "", "IMT"]]}, {"id": "1702.08235", "submitter": "Ferenc Husz\\'ar", "authors": "Ferenc Husz\\'ar", "title": "Variational Inference using Implicit Distributions", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Generative adversarial networks (GANs) have given us a great tool to fit\nimplicit generative models to data. Implicit distributions are ones we can\nsample from easily, and take derivatives of samples with respect to model\nparameters. These models are highly expressive and we argue they can prove just\nas useful for variational inference (VI) as they are for generative modelling.\nSeveral papers have proposed GAN-like algorithms for inference, however,\nconnections to the theory of VI are not always well understood. This paper\nprovides a unifying review of existing algorithms establishing connections\nbetween variational autoencoders, adversarially learned inference, operator VI,\nGAN-based image reconstruction, and more. Secondly, the paper provides a\nframework for building new algorithms: depending on the way the variational\nbound is expressed we introduce prior-contrastive and joint-contrastive\nmethods, and show practical inference algorithms based on either density ratio\nestimation or denoising.\n", "versions": [{"version": "v1", "created": "Mon, 27 Feb 2017 11:16:54 GMT"}], "update_date": "2017-02-28", "authors_parsed": [["Husz\u00e1r", "Ferenc", ""]]}, {"id": "1702.08239", "submitter": "Juho Lee", "authors": "Juho Lee, Creighton Heaukulani, Zoubin Ghahramani, Lancelot F. James,\n  Seungjin Choi", "title": "Bayesian inference on random simple graphs with power law degree\n  distributions", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a model for random simple graphs with a degree distribution that\nobeys a power law (i.e., is heavy-tailed). To attain this behavior, the edge\nprobabilities in the graph are constructed from Bertoin-Fujita-Roynette-Yor\n(BFRY) random variables, which have been recently utilized in Bayesian\nstatistics for the construction of power law models in several applications.\nOur construction readily extends to capture the structure of latent factors,\nsimilarly to stochastic blockmodels, while maintaining its power law degree\ndistribution. The BFRY random variables are well approximated by gamma random\nvariables in a variational Bayesian inference routine, which we apply to\nseveral network datasets for which power law degree distributions are a natural\nassumption. By learning the parameters of the BFRY distribution via\nprobabilistic inference, we are able to automatically select the appropriate\npower law behavior from the data. In order to further scale our inference\nprocedure, we adopt stochastic gradient ascent routines where the gradients are\ncomputed on minibatches (i.e., subsets) of the edges in the graph.\n", "versions": [{"version": "v1", "created": "Mon, 27 Feb 2017 11:26:33 GMT"}, {"version": "v2", "created": "Sun, 18 Jun 2017 10:10:43 GMT"}], "update_date": "2017-06-20", "authors_parsed": [["Lee", "Juho", ""], ["Heaukulani", "Creighton", ""], ["Ghahramani", "Zoubin", ""], ["James", "Lancelot F.", ""], ["Choi", "Seungjin", ""]]}, {"id": "1702.08248", "submitter": "Olivier Bachem", "authors": "Olivier Bachem, Mario Lucic, Andreas Krause", "title": "Scalable k-Means Clustering via Lightweight Coresets", "comments": "To appear in the 24th ACM SIGKDD International Conference on\n  Knowledge Discovery & Data Mining (KDD)", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.DC cs.DS cs.LG stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Coresets are compact representations of data sets such that models trained on\na coreset are provably competitive with models trained on the full data set. As\nsuch, they have been successfully used to scale up clustering models to massive\ndata sets. While existing approaches generally only allow for multiplicative\napproximation errors, we propose a novel notion of lightweight coresets that\nallows for both multiplicative and additive errors. We provide a single\nalgorithm to construct lightweight coresets for k-means clustering as well as\nsoft and hard Bregman clustering. The algorithm is substantially faster than\nexisting constructions, embarrassingly parallel, and the resulting coresets are\nsmaller. We further show that the proposed approach naturally generalizes to\nstatistical k-means clustering and that, compared to existing results, it can\nbe used to compute smaller summaries for empirical risk minimization. In\nextensive experiments, we demonstrate that the proposed algorithm outperforms\nexisting data summarization strategies in practice.\n", "versions": [{"version": "v1", "created": "Mon, 27 Feb 2017 12:03:01 GMT"}, {"version": "v2", "created": "Wed, 6 Jun 2018 21:49:52 GMT"}], "update_date": "2018-06-08", "authors_parsed": [["Bachem", "Olivier", ""], ["Lucic", "Mario", ""], ["Krause", "Andreas", ""]]}, {"id": "1702.08249", "submitter": "Olivier Bachem", "authors": "Olivier Bachem, Mario Lucic, S. Hamed Hassani, Andreas Krause", "title": "Uniform Deviation Bounds for Unbounded Loss Functions like k-Means", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Uniform deviation bounds limit the difference between a model's expected loss\nand its loss on an empirical sample uniformly for all models in a learning\nproblem. As such, they are a critical component to empirical risk minimization.\nIn this paper, we provide a novel framework to obtain uniform deviation bounds\nfor loss functions which are *unbounded*. In our main application, this allows\nus to obtain bounds for $k$-Means clustering under weak assumptions on the\nunderlying distribution. If the fourth moment is bounded, we prove a rate of\n$\\mathcal{O}\\left(m^{-\\frac12}\\right)$ compared to the previously known\n$\\mathcal{O}\\left(m^{-\\frac14}\\right)$ rate. Furthermore, we show that the rate\nalso depends on the kurtosis - the normalized fourth moment which measures the\n\"tailedness\" of a distribution. We further provide improved rates under\nprogressively stronger assumptions, namely, bounded higher moments,\nsubgaussianity and bounded support.\n", "versions": [{"version": "v1", "created": "Mon, 27 Feb 2017 12:03:41 GMT"}], "update_date": "2017-02-28", "authors_parsed": [["Bachem", "Olivier", ""], ["Lucic", "Mario", ""], ["Hassani", "S. Hamed", ""], ["Krause", "Andreas", ""]]}, {"id": "1702.08259", "submitter": "Hiroshi Inoue", "authors": "Hiroshi Inoue", "title": "Adaptive Ensemble Prediction for Deep Neural Networks based on\n  Confidence Level", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Ensembling multiple predictions is a widely used technique for improving the\naccuracy of various machine learning tasks. One obvious drawback of ensembling\nis its higher execution cost during inference. In this paper, we first describe\nour insights on the relationship between the probability of prediction and the\neffect of ensembling with current deep neural networks; ensembling does not\nhelp mispredictions for inputs predicted with a high probability even when\nthere is a non-negligible number of mispredicted inputs. This finding motivated\nus to develop a way to adaptively control the ensembling. If the prediction for\nan input reaches a high enough probability, i.e., the output from the softmax\nfunction, on the basis of the confidence level, we stop ensembling for this\ninput to avoid wasting computation power. We evaluated the adaptive ensembling\nby using various datasets and showed that it reduces the computation cost\nsignificantly while achieving accuracy similar to that of static ensembling\nusing a pre-defined number of local predictions. We also show that our\nstatistically rigorous confidence-level-based early-exit condition reduces the\nburden of task-dependent threshold tuning better compared with naive early exit\nbased on a pre-defined threshold in addition to yielding a better accuracy with\nthe same cost.\n", "versions": [{"version": "v1", "created": "Mon, 27 Feb 2017 12:54:54 GMT"}, {"version": "v2", "created": "Sat, 28 Jul 2018 13:35:42 GMT"}, {"version": "v3", "created": "Fri, 8 Mar 2019 11:46:51 GMT"}], "update_date": "2019-03-11", "authors_parsed": [["Inoue", "Hiroshi", ""]]}, {"id": "1702.08320", "submitter": "Sinong Geng", "authors": "Sinong Geng, Zhaobin Kuang and David Page", "title": "An Efficient Pseudo-likelihood Method for Sparse Binary Pairwise Markov\n  Network Estimation", "comments": "7 pages, 11 figures, 2 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The pseudo-likelihood method is one of the most popular algorithms for\nlearning sparse binary pairwise Markov networks. In this paper, we formulate\nthe $L_1$ regularized pseudo-likelihood problem as a sparse multiple logistic\nregression problem. In this way, many insights and optimization procedures for\nsparse logistic regression can be applied to the learning of discrete Markov\nnetworks. Specifically, we use the coordinate descent algorithm for generalized\nlinear models with convex penalties, combined with strong screening rules, to\nsolve the pseudo-likelihood problem with $L_1$ regularization. Therefore a\nsubstantial speedup without losing any accuracy can be achieved. Furthermore,\nthis method is more stable than the node-wise logistic regression approach on\nunbalanced high-dimensional data when penalized by small regularization\nparameters. Thorough numerical experiments on simulated data and real world\ndata demonstrate the advantages of the proposed method.\n", "versions": [{"version": "v1", "created": "Mon, 27 Feb 2017 15:17:04 GMT"}, {"version": "v2", "created": "Fri, 7 Apr 2017 01:33:49 GMT"}], "update_date": "2017-04-10", "authors_parsed": [["Geng", "Sinong", ""], ["Kuang", "Zhaobin", ""], ["Page", "David", ""]]}, {"id": "1702.08343", "submitter": "Yingzhen Li", "authors": "Yingzhen Li, Richard E. Turner, Qiang Liu", "title": "Approximate Inference with Amortised MCMC", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a novel approximate inference algorithm that approximates a target\ndistribution by amortising the dynamics of a user-selected MCMC sampler. The\nidea is to initialise MCMC using samples from an approximation network, apply\nthe MCMC operator to improve these samples, and finally use the samples to\nupdate the approximation network thereby improving its quality. This provides a\nnew generic framework for approximate inference, allowing us to deploy highly\ncomplex, or implicitly defined approximation families with intractable\ndensities, including approximations produced by warping a source of randomness\nthrough a deep neural network. Experiments consider image modelling with deep\ngenerative models as a challenging test for the method. Deep models trained\nusing amortised MCMC are shown to generate realistic looking samples as well as\nproducing diverse imputations for images with regions of missing pixels.\n", "versions": [{"version": "v1", "created": "Mon, 27 Feb 2017 16:01:46 GMT"}, {"version": "v2", "created": "Mon, 22 May 2017 10:50:32 GMT"}], "update_date": "2017-05-23", "authors_parsed": [["Li", "Yingzhen", ""], ["Turner", "Richard E.", ""], ["Liu", "Qiang", ""]]}, {"id": "1702.08359", "submitter": "Robert Bamler", "authors": "Robert Bamler and Stephan Mandt", "title": "Dynamic Word Embeddings", "comments": "In the proceedings of the International Conference on Machine\n  Learning (ICML 2017); 8 pages + references and supplement", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a probabilistic language model for time-stamped text data which\ntracks the semantic evolution of individual words over time. The model\nrepresents words and contexts by latent trajectories in an embedding space. At\neach moment in time, the embedding vectors are inferred from a probabilistic\nversion of word2vec [Mikolov et al., 2013]. These embedding vectors are\nconnected in time through a latent diffusion process. We describe two scalable\nvariational inference algorithms--skip-gram smoothing and skip-gram\nfiltering--that allow us to train the model jointly over all times; thus\nlearning on all data while simultaneously allowing word and context vectors to\ndrift. Experimental results on three different corpora demonstrate that our\ndynamic model infers word embedding trajectories that are more interpretable\nand lead to higher predictive likelihoods than competing methods that are based\non static models trained separately on time slices.\n", "versions": [{"version": "v1", "created": "Mon, 27 Feb 2017 16:31:48 GMT"}, {"version": "v2", "created": "Mon, 17 Jul 2017 23:45:06 GMT"}], "update_date": "2017-07-19", "authors_parsed": [["Bamler", "Robert", ""], ["Mandt", "Stephan", ""]]}, {"id": "1702.08389", "submitter": "Siamak Ravanbakhsh", "authors": "Siamak Ravanbakhsh, Jeff Schneider, Barnabas Poczos", "title": "Equivariance Through Parameter-Sharing", "comments": "icml'17", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose to study equivariance in deep neural networks through parameter\nsymmetries. In particular, given a group $\\mathcal{G}$ that acts discretely on\nthe input and output of a standard neural network layer $\\phi_{W}: \\Re^{M} \\to\n\\Re^{N}$, we show that $\\phi_{W}$ is equivariant with respect to\n$\\mathcal{G}$-action iff $\\mathcal{G}$ explains the symmetries of the network\nparameters $W$. Inspired by this observation, we then propose two\nparameter-sharing schemes to induce the desirable symmetry on $W$. Our\nprocedures for tying the parameters achieve $\\mathcal{G}$-equivariance and,\nunder some conditions on the action of $\\mathcal{G}$, they guarantee\nsensitivity to all other permutation groups outside $\\mathcal{G}$.\n", "versions": [{"version": "v1", "created": "Mon, 27 Feb 2017 17:22:29 GMT"}, {"version": "v2", "created": "Tue, 13 Jun 2017 19:37:28 GMT"}], "update_date": "2017-06-15", "authors_parsed": [["Ravanbakhsh", "Siamak", ""], ["Schneider", "Jeff", ""], ["Poczos", "Barnabas", ""]]}, {"id": "1702.08396", "submitter": "Shengjia Zhao", "authors": "Shengjia Zhao, Jiaming Song, Stefano Ermon", "title": "Learning Hierarchical Features from Generative Models", "comments": "ICML'2017", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Deep neural networks have been shown to be very successful at learning\nfeature hierarchies in supervised learning tasks. Generative models, on the\nother hand, have benefited less from hierarchical models with multiple layers\nof latent variables. In this paper, we prove that hierarchical latent variable\nmodels do not take advantage of the hierarchical structure when trained with\nexisting variational methods, and provide some limitations on the kind of\nfeatures existing models can learn. Finally we propose an alternative\narchitecture that do not suffer from these limitations. Our model is able to\nlearn highly interpretable and disentangled hierarchical features on several\nnatural image datasets with no task specific regularization or prior knowledge.\n", "versions": [{"version": "v1", "created": "Mon, 27 Feb 2017 17:43:34 GMT"}, {"version": "v2", "created": "Fri, 9 Jun 2017 17:19:15 GMT"}], "update_date": "2017-06-12", "authors_parsed": [["Zhao", "Shengjia", ""], ["Song", "Jiaming", ""], ["Ermon", "Stefano", ""]]}, {"id": "1702.08398", "submitter": "Tom Sercu", "authors": "Youssef Mroueh, Tom Sercu, Vaibhava Goel", "title": "McGan: Mean and Covariance Feature Matching GAN", "comments": "15 pages; published at ICML 2017", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We introduce new families of Integral Probability Metrics (IPM) for training\nGenerative Adversarial Networks (GAN). Our IPMs are based on matching\nstatistics of distributions embedded in a finite dimensional feature space.\nMean and covariance feature matching IPMs allow for stable training of GANs,\nwhich we will call McGan. McGan minimizes a meaningful loss between\ndistributions.\n", "versions": [{"version": "v1", "created": "Mon, 27 Feb 2017 17:46:30 GMT"}, {"version": "v2", "created": "Thu, 8 Jun 2017 23:45:25 GMT"}], "update_date": "2017-06-12", "authors_parsed": [["Mroueh", "Youssef", ""], ["Sercu", "Tom", ""], ["Goel", "Vaibhava", ""]]}, {"id": "1702.08402", "submitter": "Sami Remes", "authors": "Sami Remes, Markus Heinonen, Samuel Kaski", "title": "A Mutually-Dependent Hadamard Kernel for Modelling Latent Variable\n  Couplings", "comments": "17 pages, 6 figures; accepted to ACML 2017", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We introduce a novel kernel that models input-dependent couplings across\nmultiple latent processes. The pairwise joint kernel measures covariance along\ninputs and across different latent signals in a mutually-dependent fashion. A\nlatent correlation Gaussian process (LCGP) model combines these non-stationary\nlatent components into multiple outputs by an input-dependent mixing matrix.\nProbit classification and support for multiple observation sets are derived by\nVariational Bayesian inference. Results on several datasets indicate that the\nLCGP model can recover the correlations between latent signals while\nsimultaneously achieving state-of-the-art performance. We highlight the latent\ncovariances with an EEG classification dataset where latent brain processes and\ntheir couplings simultaneously emerge from the model.\n", "versions": [{"version": "v1", "created": "Mon, 27 Feb 2017 17:48:46 GMT"}, {"version": "v2", "created": "Tue, 3 Oct 2017 09:53:58 GMT"}], "update_date": "2017-10-04", "authors_parsed": [["Remes", "Sami", ""], ["Heinonen", "Markus", ""], ["Kaski", "Samuel", ""]]}, {"id": "1702.08420", "submitter": "Michael Minyi Zhang", "authors": "Michael Minyi Zhang, Sinead A. Williamson", "title": "Embarrassingly Parallel Inference for Gaussian Processes", "comments": null, "journal-ref": "Journal of Machine Learning Research 20, no. 169 (2019): 1-26", "doi": null, "report-no": null, "categories": "stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Training Gaussian process-based models typically involves an $ O(N^3)$\ncomputational bottleneck due to inverting the covariance matrix. Popular\nmethods for overcoming this matrix inversion problem cannot adequately model\nall types of latent functions, and are often not parallelizable. However,\njudicious choice of model structure can ameliorate this problem. A\nmixture-of-experts model that uses a mixture of $K$ Gaussian processes offers\nmodeling flexibility and opportunities for scalable inference. Our\nembarrassingly parallel algorithm combines low-dimensional matrix inversions\nwith importance sampling to yield a flexible, scalable mixture-of-experts model\nthat offers comparable performance to Gaussian process regression at a much\nlower computational cost.\n", "versions": [{"version": "v1", "created": "Mon, 27 Feb 2017 18:26:45 GMT"}, {"version": "v2", "created": "Fri, 23 Jun 2017 03:10:48 GMT"}, {"version": "v3", "created": "Sat, 10 Feb 2018 19:22:49 GMT"}, {"version": "v4", "created": "Wed, 14 Feb 2018 02:51:55 GMT"}, {"version": "v5", "created": "Wed, 13 Jun 2018 20:29:56 GMT"}, {"version": "v6", "created": "Thu, 11 Apr 2019 23:55:10 GMT"}, {"version": "v7", "created": "Mon, 15 Apr 2019 02:49:51 GMT"}, {"version": "v8", "created": "Sun, 17 Nov 2019 21:57:08 GMT"}, {"version": "v9", "created": "Tue, 3 Mar 2020 09:31:40 GMT"}], "update_date": "2020-03-04", "authors_parsed": [["Zhang", "Michael Minyi", ""], ["Williamson", "Sinead A.", ""]]}, {"id": "1702.08431", "submitter": "R Devon Hjelm", "authors": "R Devon Hjelm and Athul Paul Jacob and Tong Che and Adam Trischler and\n  Kyunghyun Cho and Yoshua Bengio", "title": "Boundary-Seeking Generative Adversarial Networks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Generative adversarial networks (GANs) are a learning framework that rely on\ntraining a discriminator to estimate a measure of difference between a target\nand generated distributions. GANs, as normally formulated, rely on the\ngenerated samples being completely differentiable w.r.t. the generative\nparameters, and thus do not work for discrete data. We introduce a method for\ntraining GANs with discrete data that uses the estimated difference measure\nfrom the discriminator to compute importance weights for generated samples,\nthus providing a policy gradient for training the generator. The importance\nweights have a strong connection to the decision boundary of the discriminator,\nand we call our method boundary-seeking GANs (BGANs). We demonstrate the\neffectiveness of the proposed algorithm with discrete image and character-based\nnatural language generation. In addition, the boundary-seeking objective\nextends to continuous data, which can be used to improve stability of training,\nand we demonstrate this on Celeba, Large-scale Scene Understanding (LSUN)\nbedrooms, and Imagenet without conditioning.\n", "versions": [{"version": "v1", "created": "Mon, 27 Feb 2017 18:51:41 GMT"}, {"version": "v2", "created": "Mon, 22 May 2017 21:16:58 GMT"}, {"version": "v3", "created": "Wed, 31 Jan 2018 19:28:42 GMT"}, {"version": "v4", "created": "Wed, 21 Feb 2018 20:52:11 GMT"}], "update_date": "2018-02-23", "authors_parsed": [["Hjelm", "R Devon", ""], ["Jacob", "Athul Paul", ""], ["Che", "Tong", ""], ["Trischler", "Adam", ""], ["Cho", "Kyunghyun", ""], ["Bengio", "Yoshua", ""]]}, {"id": "1702.08435", "submitter": "Jing Zhang", "authors": "Jing Zhang and Ioannis Ch. Paschalidis", "title": "Statistical Anomaly Detection via Composite Hypothesis Testing for\n  Markov Models", "comments": "Preprint submitted to the IEEE Transactions on Signal Processing", "journal-ref": null, "doi": "10.1109/TSP.2017.2771722", "report-no": null, "categories": "cs.SY math.OC stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Under Markovian assumptions, we leverage a Central Limit Theorem (CLT) for\nthe empirical measure in the test statistic of the composite hypothesis\nHoeffding test so as to establish weak convergence results for the test\nstatistic, and, thereby, derive a new estimator for the threshold needed by the\ntest. We first show the advantages of our estimator over an existing estimator\nby conducting extensive numerical experiments. We find that our estimator\ncontrols better for false alarms while maintaining satisfactory detection\nprobabilities. We then apply the Hoeffding test with our threshold estimator to\ndetecting anomalies in two distinct applications domains: one in communication\nnetworks and the other in transportation networks. The former application seeks\nto enhance cyber security and the latter aims at building smarter\ntransportation systems in cities.\n", "versions": [{"version": "v1", "created": "Mon, 27 Feb 2017 18:58:44 GMT"}, {"version": "v2", "created": "Sat, 11 Mar 2017 18:24:15 GMT"}, {"version": "v3", "created": "Fri, 18 Aug 2017 22:56:27 GMT"}], "update_date": "2018-02-14", "authors_parsed": [["Zhang", "Jing", ""], ["Paschalidis", "Ioannis Ch.", ""]]}, {"id": "1702.08484", "submitter": "Aditya Grover", "authors": "Aditya Grover, Stefano Ermon", "title": "Boosted Generative Models", "comments": "AAAI 2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a novel approach for using unsupervised boosting to create an\nensemble of generative models, where models are trained in sequence to correct\nearlier mistakes. Our meta-algorithmic framework can leverage any existing base\nlearner that permits likelihood evaluation, including recent deep expressive\nmodels. Further, our approach allows the ensemble to include discriminative\nmodels trained to distinguish real data from model-generated data. We show\ntheoretical conditions under which incorporating a new model in the ensemble\nwill improve the fit and empirically demonstrate the effectiveness of our\nblack-box boosting algorithms on density estimation, classification, and sample\ngeneration on benchmark datasets for a wide range of generative models.\n", "versions": [{"version": "v1", "created": "Mon, 27 Feb 2017 19:28:40 GMT"}, {"version": "v2", "created": "Fri, 22 Dec 2017 10:13:51 GMT"}], "update_date": "2017-12-25", "authors_parsed": [["Grover", "Aditya", ""], ["Ermon", "Stefano", ""]]}, {"id": "1702.08489", "submitter": "Amit Daniely", "authors": "Amit Daniely", "title": "Depth Separation for Neural Networks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CC stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Let $f:\\mathbb{S}^{d-1}\\times \\mathbb{S}^{d-1}\\to\\mathbb{S}$ be a function of\nthe form $f(\\mathbf{x},\\mathbf{x}') = g(\\langle\\mathbf{x},\\mathbf{x}'\\rangle)$\nfor $g:[-1,1]\\to \\mathbb{R}$. We give a simple proof that shows that poly-size\ndepth two neural networks with (exponentially) bounded weights cannot\napproximate $f$ whenever $g$ cannot be approximated by a low degree polynomial.\nMoreover, for many $g$'s, such as $g(x)=\\sin(\\pi d^3x)$, the number of neurons\nmust be $2^{\\Omega\\left(d\\log(d)\\right)}$. Furthermore, the result holds\nw.r.t.\\ the uniform distribution on $\\mathbb{S}^{d-1}\\times \\mathbb{S}^{d-1}$.\nAs many functions of the above form can be well approximated by poly-size depth\nthree networks with poly-bounded weights, this establishes a separation between\ndepth two and depth three networks w.r.t.\\ the uniform distribution on\n$\\mathbb{S}^{d-1}\\times \\mathbb{S}^{d-1}$.\n", "versions": [{"version": "v1", "created": "Mon, 27 Feb 2017 19:46:15 GMT"}], "update_date": "2017-03-01", "authors_parsed": [["Daniely", "Amit", ""]]}, {"id": "1702.08503", "submitter": "Amit Daniely", "authors": "Amit Daniely", "title": "SGD Learns the Conjugate Kernel Class of the Network", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.DS stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We show that the standard stochastic gradient decent (SGD) algorithm is\nguaranteed to learn, in polynomial time, a function that is competitive with\nthe best function in the conjugate kernel space of the network, as defined in\nDaniely, Frostig and Singer. The result holds for log-depth networks from a\nrich family of architectures. To the best of our knowledge, it is the first\npolynomial-time guarantee for the standard neural network learning algorithm\nfor networks of depth more that two.\n  As corollaries, it follows that for neural networks of any depth between $2$\nand $\\log(n)$, SGD is guaranteed to learn, in polynomial time, constant degree\npolynomials with polynomially bounded coefficients. Likewise, it follows that\nSGD on large enough networks can learn any continuous function (not in\npolynomial time), complementing classical expressivity results.\n", "versions": [{"version": "v1", "created": "Mon, 27 Feb 2017 20:05:43 GMT"}, {"version": "v2", "created": "Fri, 19 May 2017 18:32:42 GMT"}], "update_date": "2017-05-23", "authors_parsed": [["Daniely", "Amit", ""]]}, {"id": "1702.08530", "submitter": "Richard Nock", "authors": "Amir Dezfouli, Edwin V. Bonilla, Richard Nock", "title": "Semi-parametric Network Structure Discovery Models", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a network structure discovery model for continuous observations\nthat generalizes linear causal models by incorporating a Gaussian process (GP)\nprior on a network-independent component, and random sparsity and weight\nmatrices as the network-dependent parameters. This approach provides flexible\nmodeling of network-independent trends in the observations as well as\nuncertainty quantification around the discovered network structure. We\nestablish a connection between our model and multi-task GPs and develop an\nefficient stochastic variational inference algorithm for it. Furthermore, we\nformally show that our approach is numerically stable and in fact numerically\neasy to carry out almost everywhere on the support of the random variables\ninvolved. Finally, we evaluate our model on three applications, showing that it\noutperforms previous approaches. We provide a qualitative and quantitative\nanalysis of the structures discovered for domains such as the study of the full\ngenome regulation of the yeast Saccharomyces cerevisiae.\n", "versions": [{"version": "v1", "created": "Mon, 27 Feb 2017 21:04:05 GMT"}], "update_date": "2017-03-01", "authors_parsed": [["Dezfouli", "Amir", ""], ["Bonilla", "Edwin V.", ""], ["Nock", "Richard", ""]]}, {"id": "1702.08536", "submitter": "Emma Pierson", "authors": "Emma Pierson, Sam Corbett-Davies, Sharad Goel", "title": "Fast Threshold Tests for Detecting Discrimination", "comments": "Accepted at AISTATS 2018; slightly shorter camera-ready version", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Threshold tests have recently been proposed as a useful method for detecting\nbias in lending, hiring, and policing decisions. For example, in the case of\ncredit extensions, these tests aim to estimate the bar for granting loans to\nwhite and minority applicants, with a higher inferred threshold for minorities\nindicative of discrimination. This technique, however, requires fitting a\ncomplex Bayesian latent variable model for which inference is often\ncomputationally challenging. Here we develop a method for fitting threshold\ntests that is two orders of magnitude faster than the existing approach,\nreducing computation from hours to minutes. To achieve these performance gains,\nwe introduce and analyze a flexible family of probability distributions on the\ninterval [0, 1] -- which we call discriminant distributions -- that is\ncomputationally efficient to work with. We demonstrate our technique by\nanalyzing 2.7 million police stops of pedestrians in New York City.\n", "versions": [{"version": "v1", "created": "Mon, 27 Feb 2017 21:18:19 GMT"}, {"version": "v2", "created": "Tue, 13 Feb 2018 19:00:29 GMT"}, {"version": "v3", "created": "Sat, 10 Mar 2018 20:17:57 GMT"}], "update_date": "2018-03-13", "authors_parsed": [["Pierson", "Emma", ""], ["Corbett-Davies", "Sam", ""], ["Goel", "Sharad", ""]]}, {"id": "1702.08540", "submitter": "Yazhou Yang", "authors": "Yazhou Yang and Marco Loog", "title": "Active Learning Using Uncertainty Information", "comments": "6 pages, 1 figure, International Conference on Pattern Recognition\n  (ICPR) 2016, Cancun, Mexico", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Many active learning methods belong to the retraining-based approaches, which\nselect one unlabeled instance, add it to the training set with its possible\nlabels, retrain the classification model, and evaluate the criteria that we\nbase our selection on. However, since the true label of the selected instance\nis unknown, these methods resort to calculating the average-case or worse-case\nperformance with respect to the unknown label. In this paper, we propose a\ndifferent method to solve this problem. In particular, our method aims to make\nuse of the uncertainty information to enhance the performance of\nretraining-based models. We apply our method to two state-of-the-art algorithms\nand carry out extensive experiments on a wide variety of real-world datasets.\nThe results clearly demonstrate the effectiveness of the proposed method and\nindicate it can reduce human labeling efforts in many real-life applications.\n", "versions": [{"version": "v1", "created": "Mon, 27 Feb 2017 21:33:47 GMT"}], "update_date": "2017-03-01", "authors_parsed": [["Yang", "Yazhou", ""], ["Loog", "Marco", ""]]}, {"id": "1702.08553", "submitter": "Christopher Tosh", "authors": "Christopher Tosh, Sanjoy Dasgupta", "title": "Diameter-Based Active Learning", "comments": "16 pages, 2 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  To date, the tightest upper and lower-bounds for the active learning of\ngeneral concept classes have been in terms of a parameter of the learning\nproblem called the splitting index. We provide, for the first time, an\nefficient algorithm that is able to realize this upper bound, and we\nempirically demonstrate its good performance.\n", "versions": [{"version": "v1", "created": "Mon, 27 Feb 2017 21:59:24 GMT"}, {"version": "v2", "created": "Fri, 9 Jun 2017 00:39:57 GMT"}], "update_date": "2017-06-12", "authors_parsed": [["Tosh", "Christopher", ""], ["Dasgupta", "Sanjoy", ""]]}, {"id": "1702.08557", "submitter": "Dmitry Ignatov", "authors": "Dmitry I. Ignatov and Alexander Semenov and Daria Komissarova and\n  Dmitry V. Gnatyshak", "title": "Multimodal Clustering for Community Detection", "comments": null, "journal-ref": "Lecture Notes in Social Networks. Formal Concept Analysis of\n  Social Networks. Eds.: Kuznetsov, Missaoui, Obiedkov, Springer, 2017", "doi": null, "report-no": null, "categories": "cs.SI cs.DM stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Multimodal clustering is an unsupervised technique for mining interesting\npatterns in $n$-adic binary relations or $n$-mode networks. Among different\ntypes of such generalized patterns one can find biclusters and formal concepts\n(maximal bicliques) for 2-mode case, triclusters and triconcepts for 3-mode\ncase, closed $n$-sets for $n$-mode case, etc. Object-attribute biclustering\n(OA-biclustering) for mining large binary datatables (formal contexts or 2-mode\nnetworks) arose by the end of the last decade due to intractability of\ncomputation problems related to formal concepts; this type of patterns was\nproposed as a meaningful and scalable approximation of formal concepts. In this\npaper, our aim is to present recent advance in OA-biclustering and its\nextensions to mining multi-mode communities in SNA setting. We also discuss\nconnection between clustering coefficients known in SNA community for 1-mode\nand 2-mode networks and OA-bicluster density, the main quality measure of an\nOA-bicluster. Our experiments with 2-, 3-, and 4-mode large real-world networks\nshow that this type of patterns is suitable for community detection in\nmulti-mode cases within reasonable time even though the number of corresponding\n$n$-cliques is still unknown due to computation difficulties. An interpretation\nof OA-biclusters for 1-mode networks is provided as well.\n", "versions": [{"version": "v1", "created": "Mon, 27 Feb 2017 22:11:54 GMT"}], "update_date": "2017-03-01", "authors_parsed": [["Ignatov", "Dmitry I.", ""], ["Semenov", "Alexander", ""], ["Komissarova", "Daria", ""], ["Gnatyshak", "Dmitry V.", ""]]}, {"id": "1702.08565", "submitter": "James P. Crutchfield", "authors": "Sarah E. Marzen and James P. Crutchfield", "title": "Nearly Maximally Predictive Features and Their Dimensions", "comments": "6 pages, 2 figures; Supplementary materials, 5 pages, 1 figure;\n  http://csc.ucdavis.edu/~cmg/compmech/pubs/nmpf.htm", "journal-ref": "Phys. Rev. E 95, 051301 (2017)", "doi": "10.1103/PhysRevE.95.051301", "report-no": null, "categories": "cond-mat.stat-mech cs.IT math.IT nlin.CD stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Scientific explanation often requires inferring maximally predictive features\nfrom a given data set. Unfortunately, the collection of minimal maximally\npredictive features for most stochastic processes is uncountably infinite. In\nsuch cases, one compromises and instead seeks nearly maximally predictive\nfeatures. Here, we derive upper-bounds on the rates at which the number and the\ncoding cost of nearly maximally predictive features scales with desired\npredictive power. The rates are determined by the fractal dimensions of a\nprocess' mixed-state distribution. These results, in turn, show how widely-used\nfinite-order Markov models can fail as predictors and that mixed-state\npredictive features offer a substantial improvement.\n", "versions": [{"version": "v1", "created": "Mon, 27 Feb 2017 22:29:11 GMT"}], "update_date": "2017-05-31", "authors_parsed": [["Marzen", "Sarah E.", ""], ["Crutchfield", "James P.", ""]]}, {"id": "1702.08567", "submitter": "AmirEmad Ghassami", "authors": "AmirEmad Ghassami, Saber Salehkaleybar, Negar Kiyavash", "title": "Optimal Experiment Design for Causal Discovery from Fixed Number of\n  Experiments", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study the problem of causal structure learning over a set of random\nvariables when the experimenter is allowed to perform at most $M$ experiments\nin a non-adaptive manner. We consider the optimal learning strategy in terms of\nminimizing the portions of the structure that remains unknown given the limited\nnumber of experiments in both Bayesian and minimax setting. We characterize the\ntheoretical optimal solution and propose an algorithm, which designs the\nexperiments efficiently in terms of time complexity. We show that for bounded\ndegree graphs, in the minimax case and in the Bayesian case with uniform\npriors, our proposed algorithm is a $\\rho$-approximation algorithm, where\n$\\rho$ is independent of the order of the underlying graph. Simulations on both\nsynthetic and real data show that the performance of our algorithm is very\nclose to the optimal solution.\n", "versions": [{"version": "v1", "created": "Mon, 27 Feb 2017 22:30:43 GMT"}], "update_date": "2017-03-01", "authors_parsed": [["Ghassami", "AmirEmad", ""], ["Salehkaleybar", "Saber", ""], ["Kiyavash", "Negar", ""]]}, {"id": "1702.08575", "submitter": "Saber Salehkaleybar", "authors": "Saber Salehkaleybar, Jalal Etesami, Negar Kiyavash, Kun Zhang", "title": "Learning Vector Autoregressive Models with Latent Processes", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study the problem of learning the support of transition matrix between\nrandom processes in a Vector Autoregressive (VAR) model from samples when a\nsubset of the processes are latent. It is well known that ignoring the effect\nof the latent processes may lead to very different estimates of the influences\namong observed processes, and we are concerned with identifying the influences\namong the observed processes, those between the latent ones, and those from the\nlatent to the observed ones. We show that the support of transition matrix\namong the observed processes and lengths of all latent paths between any two\nobserved processes can be identified successfully under some conditions on the\nVAR model. From the lengths of latent paths, we reconstruct the latent subgraph\n(representing the influences among the latent processes) with a minimum number\nof variables uniquely if its topology is a directed tree. Furthermore, we\npropose an algorithm that finds all possible minimal latent graphs under some\nconditions on the lengths of latent paths. Our results apply to both\nnon-Gaussian and Gaussian cases, and experimental results on various synthetic\nand real-world datasets validate our theoretical results.\n", "versions": [{"version": "v1", "created": "Mon, 27 Feb 2017 23:00:28 GMT"}, {"version": "v2", "created": "Wed, 31 May 2017 18:36:05 GMT"}, {"version": "v3", "created": "Fri, 10 Nov 2017 04:36:01 GMT"}], "update_date": "2017-11-13", "authors_parsed": [["Salehkaleybar", "Saber", ""], ["Etesami", "Jalal", ""], ["Kiyavash", "Negar", ""], ["Zhang", "Kun", ""]]}, {"id": "1702.08580", "submitter": "Haihao Lu", "authors": "Haihao Lu and Kenji Kawaguchi", "title": "Depth Creates No Bad Local Minima", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.NE math.OC stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In deep learning, \\textit{depth}, as well as \\textit{nonlinearity}, create\nnon-convex loss surfaces. Then, does depth alone create bad local minima? In\nthis paper, we prove that without nonlinearity, depth alone does not create bad\nlocal minima, although it induces non-convex loss surface. Using this insight,\nwe greatly simplify a recently proposed proof to show that all of the local\nminima of feedforward deep linear neural networks are global minima. Our\ntheoretical results generalize previous results with fewer assumptions, and\nthis analysis provides a method to show similar results beyond square loss in\ndeep linear models.\n", "versions": [{"version": "v1", "created": "Mon, 27 Feb 2017 23:27:36 GMT"}, {"version": "v2", "created": "Wed, 24 May 2017 03:42:41 GMT"}], "update_date": "2017-05-25", "authors_parsed": [["Lu", "Haihao", ""], ["Kawaguchi", "Kenji", ""]]}, {"id": "1702.08586", "submitter": "Lei Wang", "authors": "Lei Wang", "title": "Can Boltzmann Machines Discover Cluster Updates ?", "comments": "4 pages, 4 figures, and half page appendix", "journal-ref": "Phys. Rev. E 96, 051301 (2017)", "doi": "10.1103/PhysRevE.96.051301", "report-no": null, "categories": "physics.comp-ph cond-mat.stat-mech cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Boltzmann machines are physics informed generative models with wide\napplications in machine learning. They can learn the probability distribution\nfrom an input dataset and generate new samples accordingly. Applying them back\nto physics, the Boltzmann machines are ideal recommender systems to accelerate\nMonte Carlo simulation of physical systems due to their flexibility and\neffectiveness. More intriguingly, we show that the generative sampling of the\nBoltzmann Machines can even discover unknown cluster Monte Carlo algorithms.\nThe creative power comes from the latent representation of the Boltzmann\nmachines, which learn to mediate complex interactions and identify clusters of\nthe physical system. We demonstrate these findings with concrete examples of\nthe classical Ising model with and without four spin plaquette interactions.\nOur results endorse a fresh research paradigm where intelligent machines are\ndesigned to create or inspire human discovery of innovative algorithms.\n", "versions": [{"version": "v1", "created": "Tue, 28 Feb 2017 00:39:04 GMT"}], "update_date": "2017-11-20", "authors_parsed": [["Wang", "Lei", ""]]}, {"id": "1702.08591", "submitter": "David Balduzzi", "authors": "David Balduzzi, Marcus Frean, Lennox Leary, JP Lewis, Kurt Wan-Duo Ma,\n  Brian McWilliams", "title": "The Shattered Gradients Problem: If resnets are the answer, then what is\n  the question?", "comments": "ICML 2017, final version", "journal-ref": "PMLR volume 70 (2017)", "doi": null, "report-no": null, "categories": "cs.NE cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A long-standing obstacle to progress in deep learning is the problem of\nvanishing and exploding gradients. Although, the problem has largely been\novercome via carefully constructed initializations and batch normalization,\narchitectures incorporating skip-connections such as highway and resnets\nperform much better than standard feedforward architectures despite well-chosen\ninitialization and batch normalization. In this paper, we identify the\nshattered gradients problem. Specifically, we show that the correlation between\ngradients in standard feedforward networks decays exponentially with depth\nresulting in gradients that resemble white noise whereas, in contrast, the\ngradients in architectures with skip-connections are far more resistant to\nshattering, decaying sublinearly. Detailed empirical evidence is presented in\nsupport of the analysis, on both fully-connected networks and convnets.\nFinally, we present a new \"looks linear\" (LL) initialization that prevents\nshattering, with preliminary experiments showing the new initialization allows\nto train very deep networks without the addition of skip-connections.\n", "versions": [{"version": "v1", "created": "Tue, 28 Feb 2017 01:06:13 GMT"}, {"version": "v2", "created": "Wed, 6 Jun 2018 10:08:21 GMT"}], "update_date": "2018-06-07", "authors_parsed": [["Balduzzi", "David", ""], ["Frean", "Marcus", ""], ["Leary", "Lennox", ""], ["Lewis", "JP", ""], ["Ma", "Kurt Wan-Duo", ""], ["McWilliams", "Brian", ""]]}, {"id": "1702.08608", "submitter": "Been Kim", "authors": "Finale Doshi-Velez and Been Kim", "title": "Towards A Rigorous Science of Interpretable Machine Learning", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.AI cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  As machine learning systems become ubiquitous, there has been a surge of\ninterest in interpretable machine learning: systems that provide explanation\nfor their outputs. These explanations are often used to qualitatively assess\nother criteria such as safety or non-discrimination. However, despite the\ninterest in interpretability, there is very little consensus on what\ninterpretable machine learning is and how it should be measured. In this\nposition paper, we first define interpretability and describe when\ninterpretability is needed (and when it is not). Next, we suggest a taxonomy\nfor rigorous evaluation and expose open questions towards a more rigorous\nscience of interpretable machine learning.\n", "versions": [{"version": "v1", "created": "Tue, 28 Feb 2017 02:19:20 GMT"}, {"version": "v2", "created": "Thu, 2 Mar 2017 19:32:10 GMT"}], "update_date": "2017-03-06", "authors_parsed": [["Doshi-Velez", "Finale", ""], ["Kim", "Been", ""]]}, {"id": "1702.08626", "submitter": "Ahmed Qureshi", "authors": "Ahmed Hussain Qureshi, Yutaka Nakamura, Yuichiro Yoshikawa and Hiroshi\n  Ishiguro", "title": "Show, Attend and Interact: Perceivable Human-Robot Social Interaction\n  through Neural Attention Q-Network", "comments": "7 pages, 5 figures, accepted by IEEE-RAS ICRA'17", "journal-ref": null, "doi": "10.1109/ICRA.2017.7989193", "report-no": null, "categories": "cs.RO cs.AI cs.CV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  For a safe, natural and effective human-robot social interaction, it is\nessential to develop a system that allows a robot to demonstrate the\nperceivable responsive behaviors to complex human behaviors. We introduce the\nMultimodal Deep Attention Recurrent Q-Network using which the robot exhibits\nhuman-like social interaction skills after 14 days of interacting with people\nin an uncontrolled real world. Each and every day during the 14 days, the\nsystem gathered robot interaction experiences with people through a\nhit-and-trial method and then trained the MDARQN on these experiences using\nend-to-end reinforcement learning approach. The results of interaction based\nlearning indicate that the robot has learned to respond to complex human\nbehaviors in a perceivable and socially acceptable manner.\n", "versions": [{"version": "v1", "created": "Tue, 28 Feb 2017 03:16:40 GMT"}], "update_date": "2018-06-19", "authors_parsed": [["Qureshi", "Ahmed Hussain", ""], ["Nakamura", "Yutaka", ""], ["Yoshikawa", "Yuichiro", ""], ["Ishiguro", "Hiroshi", ""]]}, {"id": "1702.08635", "submitter": "Fei Tian", "authors": "Yang Fan and Fei Tian and Tao Qin and Jiang Bian and Tie-Yan Liu", "title": "Learning What Data to Learn", "comments": "A preliminary version will appear in ICLR 2017, workshop track.\n  https://openreview.net/forum?id=SyJNmVqgg&noteId=SyJNmVqgg", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Machine learning is essentially the sciences of playing with data. An\nadaptive data selection strategy, enabling to dynamically choose different data\nat various training stages, can reach a more effective model in a more\nefficient way. In this paper, we propose a deep reinforcement learning\nframework, which we call \\emph{\\textbf{N}eural \\textbf{D}ata \\textbf{F}ilter}\n(\\textbf{NDF}), to explore automatic and adaptive data selection in the\ntraining process. In particular, NDF takes advantage of a deep neural network\nto adaptively select and filter important data instances from a sequential\nstream of training data, such that the future accumulative reward (e.g., the\nconvergence speed) is maximized. In contrast to previous studies in data\nselection that is mainly based on heuristic strategies, NDF is quite generic\nand thus can be widely suitable for many machine learning tasks. Taking neural\nnetwork training with stochastic gradient descent (SGD) as an example,\ncomprehensive experiments with respect to various neural network modeling\n(e.g., multi-layer perceptron networks, convolutional neural networks and\nrecurrent neural networks) and several applications (e.g., image classification\nand text understanding) demonstrate that NDF powered SGD can achieve comparable\naccuracy with standard SGD process by using less data and fewer iterations.\n", "versions": [{"version": "v1", "created": "Tue, 28 Feb 2017 03:52:06 GMT"}], "update_date": "2017-03-01", "authors_parsed": [["Fan", "Yang", ""], ["Tian", "Fei", ""], ["Qin", "Tao", ""], ["Bian", "Jiang", ""], ["Liu", "Tie-Yan", ""]]}, {"id": "1702.08651", "submitter": "Quanquan Gu", "authors": "Pan Xu and Jian Ma and Quanquan Gu", "title": "Speeding Up Latent Variable Gaussian Graphical Model Estimation via\n  Nonconvex Optimizations", "comments": "29 pages, 5 figures, 3 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study the estimation of the latent variable Gaussian graphical model\n(LVGGM), where the precision matrix is the superposition of a sparse matrix and\na low-rank matrix. In order to speed up the estimation of the sparse plus\nlow-rank components, we propose a sparsity constrained maximum likelihood\nestimator based on matrix factorization, and an efficient alternating gradient\ndescent algorithm with hard thresholding to solve it. Our algorithm is orders\nof magnitude faster than the convex relaxation based methods for LVGGM. In\naddition, we prove that our algorithm is guaranteed to linearly converge to the\nunknown sparse and low-rank components up to the optimal statistical precision.\nExperiments on both synthetic and genomic data demonstrate the superiority of\nour algorithm over the state-of-the-art algorithms and corroborate our theory.\n", "versions": [{"version": "v1", "created": "Tue, 28 Feb 2017 05:38:40 GMT"}], "update_date": "2017-03-01", "authors_parsed": [["Xu", "Pan", ""], ["Ma", "Jian", ""], ["Gu", "Quanquan", ""]]}, {"id": "1702.08658", "submitter": "Shengjia Zhao", "authors": "Shengjia Zhao, Jiaming Song, Stefano Ermon", "title": "Towards Deeper Understanding of Variational Autoencoding Models", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a new family of optimization criteria for variational\nauto-encoding models, generalizing the standard evidence lower bound. We\nprovide conditions under which they recover the data distribution and learn\nlatent features, and formally show that common issues such as blurry samples\nand uninformative latent features arise when these conditions are not met.\nBased on these new insights, we propose a new sequential VAE model that can\ngenerate sharp samples on the LSUN image dataset based on pixel-wise\nreconstruction loss, and propose an optimization criterion that encourages\nunsupervised learning of informative latent features.\n", "versions": [{"version": "v1", "created": "Tue, 28 Feb 2017 06:04:23 GMT"}], "update_date": "2017-03-01", "authors_parsed": [["Zhao", "Shengjia", ""], ["Song", "Jiaming", ""], ["Ermon", "Stefano", ""]]}, {"id": "1702.08670", "submitter": "Vamsi Ithapu", "authors": "Vamsi K Ithapu, Sathya N Ravi, Vikas Singh", "title": "On architectural choices in deep learning: From network structure to\n  gradient convergence and parameter estimation", "comments": "87 Pages; 14 figures; Under review", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG math.OC stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study mechanisms to characterize how the asymptotic convergence of\nbackpropagation in deep architectures, in general, is related to the network\nstructure, and how it may be influenced by other design choices including\nactivation type, denoising and dropout rate. We seek to analyze whether network\narchitecture and input data statistics may guide the choices of learning\nparameters and vice versa. Given the broad applicability of deep architectures,\nthis issue is interesting both from theoretical and a practical standpoint.\nUsing properties of general nonconvex objectives (with first-order\ninformation), we first build the association between structural, distributional\nand learnability aspects of the network vis-\\`a-vis their interaction with\nparameter convergence rates. We identify a nice relationship between feature\ndenoising and dropout, and construct families of networks that achieve the same\nlevel of convergence. We then derive a workflow that provides systematic\nguidance regarding the choice of network sizes and learning parameters often\nmediated4 by input statistics. Our technical results are corroborated by an\nextensive set of evaluations, presented in this paper as well as independent\nempirical observations reported by other groups. We also perform experiments\nshowing the practical implications of our framework for choosing the best\nfully-connected design for a given problem.\n", "versions": [{"version": "v1", "created": "Tue, 28 Feb 2017 07:05:27 GMT"}], "update_date": "2017-03-02", "authors_parsed": [["Ithapu", "Vamsi K", ""], ["Ravi", "Sathya N", ""], ["Singh", "Vikas", ""]]}, {"id": "1702.08690", "submitter": "Weifeng Ge", "authors": "Weifeng Ge, Yizhou Yu", "title": "Borrowing Treasures from the Wealthy: Deep Transfer Learning through\n  Selective Joint Fine-tuning", "comments": "To appear in 2017 IEEE Conference on Computer Vision and Pattern\n  Recognition (CVPR 2017)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI cs.LG cs.NE stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Deep neural networks require a large amount of labeled training data during\nsupervised learning. However, collecting and labeling so much data might be\ninfeasible in many cases. In this paper, we introduce a source-target selective\njoint fine-tuning scheme for improving the performance of deep learning tasks\nwith insufficient training data. In this scheme, a target learning task with\ninsufficient training data is carried out simultaneously with another source\nlearning task with abundant training data. However, the source learning task\ndoes not use all existing training data. Our core idea is to identify and use a\nsubset of training images from the original source learning task whose\nlow-level characteristics are similar to those from the target learning task,\nand jointly fine-tune shared convolutional layers for both tasks. Specifically,\nwe compute descriptors from linear or nonlinear filter bank responses on\ntraining images from both tasks, and use such descriptors to search for a\ndesired subset of training samples for the source learning task.\n  Experiments demonstrate that our selective joint fine-tuning scheme achieves\nstate-of-the-art performance on multiple visual classification tasks with\ninsufficient training data for deep learning. Such tasks include Caltech 256,\nMIT Indoor 67, Oxford Flowers 102 and Stanford Dogs 120. In comparison to\nfine-tuning without a source domain, the proposed method can improve the\nclassification accuracy by 2% - 10% using a single model.\n", "versions": [{"version": "v1", "created": "Tue, 28 Feb 2017 08:40:44 GMT"}, {"version": "v2", "created": "Tue, 6 Jun 2017 11:51:03 GMT"}], "update_date": "2018-03-06", "authors_parsed": [["Ge", "Weifeng", ""], ["Yu", "Yizhou", ""]]}, {"id": "1702.08694", "submitter": "Mahito Sugiyama", "authors": "Mahito Sugiyama and Karsten Borgwardt", "title": "Finding Statistically Significant Interactions between Continuous\n  Features", "comments": "13 pages, 5 figures, 2 tables, accepted to the 28th International\n  Joint Conference on Artificial Intelligence (IJCAI 2019)", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The search for higher-order feature interactions that are statistically\nsignificantly associated with a class variable is of high relevance in fields\nsuch as Genetics or Healthcare, but the combinatorial explosion of the\ncandidate space makes this problem extremely challenging in terms of\ncomputational efficiency and proper correction for multiple testing. While\nrecent progress has been made regarding this challenge for binary features, we\nhere present the first solution for continuous features. We propose an\nalgorithm which overcomes the combinatorial explosion of the search space of\nhigher-order interactions by deriving a lower bound on the p-value for each\ninteraction, which enables us to massively prune interactions that can never\nreach significance and to thereby gain more statistical power. In our\nexperiments, our approach efficiently detects all significant interactions in a\nvariety of synthetic and real-world datasets.\n", "versions": [{"version": "v1", "created": "Tue, 28 Feb 2017 08:46:37 GMT"}, {"version": "v2", "created": "Mon, 22 May 2017 08:39:03 GMT"}, {"version": "v3", "created": "Fri, 10 May 2019 15:46:12 GMT"}], "update_date": "2019-05-13", "authors_parsed": [["Sugiyama", "Mahito", ""], ["Borgwardt", "Karsten", ""]]}, {"id": "1702.08701", "submitter": "Jinshan Zeng", "authors": "Shao-Bo Lin, Jinshan Zeng, Xiangyu Chang", "title": "Learning rates for classification with Gaussian kernels", "comments": "This paper has been accepted by Neural Computation", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG math.OC stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper aims at refined error analysis for binary classification using\nsupport vector machine (SVM) with Gaussian kernel and convex loss. Our first\nresult shows that for some loss functions such as the truncated quadratic loss\nand quadratic loss, SVM with Gaussian kernel can reach the almost optimal\nlearning rate, provided the regression function is smooth. Our second result\nshows that, for a large number of loss functions, under some Tsybakov noise\nassumption, if the regression function is infinitely smooth, then SVM with\nGaussian kernel can achieve the learning rate of order $m^{-1}$, where $m$ is\nthe number of samples.\n", "versions": [{"version": "v1", "created": "Tue, 28 Feb 2017 09:01:32 GMT"}, {"version": "v2", "created": "Wed, 16 Aug 2017 16:04:09 GMT"}, {"version": "v3", "created": "Thu, 5 Oct 2017 12:00:16 GMT"}], "update_date": "2017-10-06", "authors_parsed": [["Lin", "Shao-Bo", ""], ["Zeng", "Jinshan", ""], ["Chang", "Xiangyu", ""]]}, {"id": "1702.08704", "submitter": "Kevin Scaman", "authors": "Kevin Scaman (MSR - INRIA), Francis Bach (SIERRA), S\\'ebastien Bubeck,\n  Yin Tat Lee, Laurent Massouli\\'e (MSR - INRIA)", "title": "Optimal algorithms for smooth and strongly convex distributed\n  optimization in networks", "comments": "18 pages (v2: fixed mathematical expressions in the abstract)", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.OC stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we determine the optimal convergence rates for strongly convex\nand smooth distributed optimization in two settings: centralized and\ndecentralized communications over a network. For centralized (i.e.\nmaster/slave) algorithms, we show that distributing Nesterov's accelerated\ngradient descent is optimal and achieves a precision $\\varepsilon > 0$ in time\n$O(\\sqrt{\\kappa_g}(1+\\Delta\\tau)\\ln(1/\\varepsilon))$, where $\\kappa_g$ is the\ncondition number of the (global) function to optimize, $\\Delta$ is the diameter\nof the network, and $\\tau$ (resp. $1$) is the time needed to communicate values\nbetween two neighbors (resp. perform local computations). For decentralized\nalgorithms based on gossip, we provide the first optimal algorithm, called the\nmulti-step dual accelerated (MSDA) method, that achieves a precision\n$\\varepsilon > 0$ in time\n$O(\\sqrt{\\kappa_l}(1+\\frac{\\tau}{\\sqrt{\\gamma}})\\ln(1/\\varepsilon))$, where\n$\\kappa_l$ is the condition number of the local functions and $\\gamma$ is the\n(normalized) eigengap of the gossip matrix used for communication between\nnodes. We then verify the efficiency of MSDA against state-of-the-art methods\nfor two problems: least-squares regression and classification by logistic\nregression.\n", "versions": [{"version": "v1", "created": "Tue, 28 Feb 2017 09:09:04 GMT"}, {"version": "v2", "created": "Fri, 7 Apr 2017 08:41:54 GMT"}], "update_date": "2017-04-10", "authors_parsed": [["Scaman", "Kevin", "", "MSR - INRIA"], ["Bach", "Francis", "", "SIERRA"], ["Bubeck", "S\u00e9bastien", "", "MSR - INRIA"], ["Lee", "Yin Tat", "", "MSR - INRIA"], ["Massouli\u00e9", "Laurent", "", "MSR - INRIA"]]}, {"id": "1702.08712", "submitter": "Dacheng Tao", "authors": "Tongliang Liu and G\\'abor Lugosi and Gergely Neu and Dacheng Tao", "title": "Algorithmic stability and hypothesis complexity", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We introduce a notion of algorithmic stability of learning algorithms---that\nwe term \\emph{argument stability}---that captures stability of the hypothesis\noutput by the learning algorithm in the normed space of functions from which\nhypotheses are selected. The main result of the paper bounds the generalization\nerror of any learning algorithm in terms of its argument stability. The bounds\nare based on martingale inequalities in the Banach space to which the\nhypotheses belong. We apply the general bounds to bound the performance of some\nlearning algorithms based on empirical risk minimization and stochastic\ngradient descent.\n", "versions": [{"version": "v1", "created": "Tue, 28 Feb 2017 09:39:03 GMT"}, {"version": "v2", "created": "Thu, 3 Aug 2017 11:45:15 GMT"}], "update_date": "2017-08-04", "authors_parsed": [["Liu", "Tongliang", ""], ["Lugosi", "G\u00e1bor", ""], ["Neu", "Gergely", ""], ["Tao", "Dacheng", ""]]}, {"id": "1702.08720", "submitter": "Weihua Hu", "authors": "Weihua Hu, Takeru Miyato, Seiya Tokui, Eiichi Matsumoto, Masashi\n  Sugiyama", "title": "Learning Discrete Representations via Information Maximizing\n  Self-Augmented Training", "comments": "To appear at ICML 2017", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Learning discrete representations of data is a central machine learning task\nbecause of the compactness of the representations and ease of interpretation.\nThe task includes clustering and hash learning as special cases. Deep neural\nnetworks are promising to be used because they can model the non-linearity of\ndata and scale to large datasets. However, their model complexity is huge, and\ntherefore, we need to carefully regularize the networks in order to learn\nuseful representations that exhibit intended invariance for applications of\ninterest. To this end, we propose a method called Information Maximizing\nSelf-Augmented Training (IMSAT). In IMSAT, we use data augmentation to impose\nthe invariance on discrete representations. More specifically, we encourage the\npredicted representations of augmented data points to be close to those of the\noriginal data points in an end-to-end fashion. At the same time, we maximize\nthe information-theoretic dependency between data and their predicted discrete\nrepresentations. Extensive experiments on benchmark datasets show that IMSAT\nproduces state-of-the-art results for both clustering and unsupervised hash\nlearning.\n", "versions": [{"version": "v1", "created": "Tue, 28 Feb 2017 09:57:27 GMT"}, {"version": "v2", "created": "Wed, 1 Mar 2017 10:14:51 GMT"}, {"version": "v3", "created": "Wed, 14 Jun 2017 04:18:11 GMT"}], "update_date": "2017-06-15", "authors_parsed": [["Hu", "Weihua", ""], ["Miyato", "Takeru", ""], ["Tokui", "Seiya", ""], ["Matsumoto", "Eiichi", ""], ["Sugiyama", "Masashi", ""]]}, {"id": "1702.08811", "submitter": "Werner Zellinger", "authors": "Werner Zellinger, Thomas Grubinger, Edwin Lughofer, Thomas\n  Natschl\\\"ager, Susanne Saminger-Platz", "title": "Central Moment Discrepancy (CMD) for Domain-Invariant Representation\n  Learning", "comments": "Extended journal version published:\n  https://doi.org/10.1016/j.ins.2019.01.025", "journal-ref": "International Conference on Learning Representations (ICLR), 2017", "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The learning of domain-invariant representations in the context of domain\nadaptation with neural networks is considered. We propose a new regularization\nmethod that minimizes the discrepancy between domain-specific latent feature\nrepresentations directly in the hidden activation space. Although some standard\ndistribution matching approaches exist that can be interpreted as the matching\nof weighted sums of moments, e.g. Maximum Mean Discrepancy (MMD), an explicit\norder-wise matching of higher order moments has not been considered before. We\npropose to match the higher order central moments of probability distributions\nby means of order-wise moment differences. Our model does not require\ncomputationally expensive distance and kernel matrix computations. We utilize\nthe equivalent representation of probability distributions by moment sequences\nto define a new distance function, called Central Moment Discrepancy (CMD). We\nprove that CMD is a metric on the set of probability distributions on a compact\ninterval. We further prove that convergence of probability distributions on\ncompact intervals w.r.t. the new metric implies convergence in distribution of\nthe respective random variables. We test our approach on two different\nbenchmark data sets for object recognition (Office) and sentiment analysis of\nproduct reviews (Amazon reviews). CMD achieves a new state-of-the-art\nperformance on most domain adaptation tasks of Office and outperforms networks\ntrained with MMD, Variational Fair Autoencoders and Domain Adversarial Neural\nNetworks on Amazon reviews. In addition, a post-hoc parameter sensitivity\nanalysis shows that the new approach is stable w.r.t. parameter changes in a\ncertain interval. The source code of the experiments is publicly available.\n", "versions": [{"version": "v1", "created": "Tue, 28 Feb 2017 15:04:54 GMT"}, {"version": "v2", "created": "Tue, 4 Jul 2017 13:51:34 GMT"}, {"version": "v3", "created": "Thu, 2 May 2019 07:23:00 GMT"}], "update_date": "2019-05-03", "authors_parsed": [["Zellinger", "Werner", ""], ["Grubinger", "Thomas", ""], ["Lughofer", "Edwin", ""], ["Natschl\u00e4ger", "Thomas", ""], ["Saminger-Platz", "Susanne", ""]]}, {"id": "1702.08835", "submitter": "Zhi-Hua Zhou", "authors": "Zhi-Hua Zhou and Ji Feng", "title": "Deep Forest", "comments": null, "journal-ref": "National Science Review, 2019, 6(1): 74-86", "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Current deep learning models are mostly build upon neural networks, i.e.,\nmultiple layers of parameterized differentiable nonlinear modules that can be\ntrained by backpropagation. In this paper, we explore the possibility of\nbuilding deep models based on non-differentiable modules. We conjecture that\nthe mystery behind the success of deep neural networks owes much to three\ncharacteristics, i.e., layer-by-layer processing, in-model feature\ntransformation and sufficient model complexity. We propose the gcForest\napproach, which generates \\textit{deep forest} holding these characteristics.\nThis is a decision tree ensemble approach, with much less hyper-parameters than\ndeep neural networks, and its model complexity can be automatically determined\nin a data-dependent way. Experiments show that its performance is quite robust\nto hyper-parameter settings, such that in most cases, even across different\ndata from different domains, it is able to get excellent performance by using\nthe same default setting. This study opens the door of deep learning based on\nnon-differentiable modules, and exhibits the possibility of constructing deep\nmodels without using backpropagation.\n", "versions": [{"version": "v1", "created": "Tue, 28 Feb 2017 16:10:31 GMT"}, {"version": "v2", "created": "Wed, 31 May 2017 17:15:34 GMT"}, {"version": "v3", "created": "Mon, 14 May 2018 18:06:13 GMT"}, {"version": "v4", "created": "Mon, 6 Jul 2020 12:23:49 GMT"}], "update_date": "2020-07-07", "authors_parsed": [["Zhou", "Zhi-Hua", ""], ["Feng", "Ji", ""]]}, {"id": "1702.08840", "submitter": "Jungseul Ok", "authors": "Jungseul Ok, Sewoong Oh, Yunhun Jang, Jinwoo Shin, and Yung Yi", "title": "Iterative Bayesian Learning for Crowdsourced Regression", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Crowdsourcing platforms emerged as popular venues for purchasing human\nintelligence at low cost for large volume of tasks. As many low-paid workers\nare prone to give noisy answers, a common practice is to add redundancy by\nassigning multiple workers to each task and then simply average out these\nanswers. However, to fully harness the wisdom of the crowd, one needs to learn\nthe heterogeneous quality of each worker. We resolve this fundamental challenge\nin crowdsourced regression tasks, i.e., the answer takes continuous labels,\nwhere identifying good or bad workers becomes much more non-trivial compared to\na classification setting of discrete labels. In particular, we introduce a\nBayesian iterative scheme and show that it provably achieves the optimal mean\nsquared error. Our evaluations on synthetic and real-world datasets support our\ntheoretical results and show the superiority of the proposed scheme.\n", "versions": [{"version": "v1", "created": "Tue, 28 Feb 2017 16:15:13 GMT"}, {"version": "v2", "created": "Tue, 23 May 2017 11:27:14 GMT"}, {"version": "v3", "created": "Mon, 8 Oct 2018 17:02:49 GMT"}], "update_date": "2018-10-09", "authors_parsed": [["Ok", "Jungseul", ""], ["Oh", "Sewoong", ""], ["Jang", "Yunhun", ""], ["Shin", "Jinwoo", ""], ["Yi", "Yung", ""]]}, {"id": "1702.08848", "submitter": "Yang Kang", "authors": "Jose Blanchet and Yang Kang", "title": "Semi-supervised Learning based on Distributionally Robust Optimization", "comments": null, "journal-ref": "Proceedings of 5th Stochastic Modeling Techniques and Data\n  Analysis International Conference, 2018, Page 87-116", "doi": "10.1002/9781119721871.ch1", "report-no": null, "categories": "stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a novel method for semi-supervised learning (SSL) based on\ndata-driven distributionally robust optimization (DRO) using optimal transport\nmetrics. Our proposed method enhances generalization error by using the\nunlabeled data to restrict the support of the worst case distribution in our\nDRO formulation. We enable the implementation of our DRO formulation by\nproposing a stochastic gradient descent algorithm which allows to easily\nimplement the training procedure. We demonstrate that our Semi-supervised DRO\nmethod is able to improve the generalization error over natural supervised\nprocedures and state-of-the-art SSL estimators. Finally, we include a\ndiscussion on the large sample behavior of the optimal uncertainty region in\nthe DRO formulation. Our discussion exposes important aspects such as the role\nof dimension reduction in SSL.\n", "versions": [{"version": "v1", "created": "Tue, 28 Feb 2017 16:30:17 GMT"}, {"version": "v2", "created": "Fri, 19 May 2017 20:36:19 GMT"}, {"version": "v3", "created": "Fri, 4 Aug 2017 13:21:19 GMT"}, {"version": "v4", "created": "Thu, 21 Mar 2019 02:04:54 GMT"}], "update_date": "2020-04-21", "authors_parsed": [["Blanchet", "Jose", ""], ["Kang", "Yang", ""]]}, {"id": "1702.08882", "submitter": "Kenji Kawaguchi", "authors": "Kenji Kawaguchi, Bo Xie, Vikas Verma, Le Song", "title": "Deep Semi-Random Features for Nonlinear Function Approximation", "comments": "AAAI 2018 - Extended version", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.NE stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose semi-random features for nonlinear function approximation. The\nflexibility of semi-random feature lies between the fully adjustable units in\ndeep learning and the random features used in kernel methods. For one hidden\nlayer models with semi-random features, we prove with no unrealistic\nassumptions that the model classes contain an arbitrarily good function as the\nwidth increases (universality), and despite non-convexity, we can find such a\ngood function (optimization theory) that generalizes to unseen new data\n(generalization bound). For deep models, with no unrealistic assumptions, we\nprove universal approximation ability, a lower bound on approximation error, a\npartial optimization guarantee, and a generalization bound. Depending on the\nproblems, the generalization bound of deep semi-random features can be\nexponentially better than the known bounds of deep ReLU nets; our\ngeneralization error bound can be independent of the depth, the number of\ntrainable weights as well as the input dimensionality. In experiments, we show\nthat semi-random features can match the performance of neural networks by using\nslightly more units, and it outperforms random features by using significantly\nfewer units. Moreover, we introduce a new implicit ensemble method by using\nsemi-random features.\n", "versions": [{"version": "v1", "created": "Tue, 28 Feb 2017 17:47:34 GMT"}, {"version": "v2", "created": "Sun, 12 Mar 2017 22:51:06 GMT"}, {"version": "v3", "created": "Thu, 20 Apr 2017 22:31:17 GMT"}, {"version": "v4", "created": "Fri, 19 May 2017 02:39:31 GMT"}, {"version": "v5", "created": "Fri, 2 Jun 2017 04:05:15 GMT"}, {"version": "v6", "created": "Sat, 10 Jun 2017 17:11:27 GMT"}, {"version": "v7", "created": "Tue, 21 Nov 2017 03:44:50 GMT"}], "update_date": "2017-11-22", "authors_parsed": [["Kawaguchi", "Kenji", ""], ["Xie", "Bo", ""], ["Verma", "Vikas", ""], ["Song", "Le", ""]]}, {"id": "1702.08892", "submitter": "Ofir Nachum", "authors": "Ofir Nachum, Mohammad Norouzi, Kelvin Xu, Dale Schuurmans", "title": "Bridging the Gap Between Value and Policy Based Reinforcement Learning", "comments": "NIPS 2017", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We establish a new connection between value and policy based reinforcement\nlearning (RL) based on a relationship between softmax temporal value\nconsistency and policy optimality under entropy regularization. Specifically,\nwe show that softmax consistent action values correspond to optimal entropy\nregularized policy probabilities along any action sequence, regardless of\nprovenance. From this observation, we develop a new RL algorithm, Path\nConsistency Learning (PCL), that minimizes a notion of soft consistency error\nalong multi-step action sequences extracted from both on- and off-policy\ntraces. We examine the behavior of PCL in different scenarios and show that PCL\ncan be interpreted as generalizing both actor-critic and Q-learning algorithms.\nWe subsequently deepen the relationship by showing how a single model can be\nused to represent both a policy and the corresponding softmax state values,\neliminating the need for a separate critic. The experimental evaluation\ndemonstrates that PCL significantly outperforms strong actor-critic and\nQ-learning baselines across several benchmarks.\n", "versions": [{"version": "v1", "created": "Tue, 28 Feb 2017 18:06:15 GMT"}, {"version": "v2", "created": "Thu, 8 Jun 2017 19:31:32 GMT"}, {"version": "v3", "created": "Wed, 22 Nov 2017 23:11:20 GMT"}], "update_date": "2017-11-27", "authors_parsed": [["Nachum", "Ofir", ""], ["Norouzi", "Mohammad", ""], ["Xu", "Kelvin", ""], ["Schuurmans", "Dale", ""]]}, {"id": "1702.08896", "submitter": "Dustin Tran", "authors": "Dustin Tran, Rajesh Ranganath, David M. Blei", "title": "Hierarchical Implicit Models and Likelihood-Free Variational Inference", "comments": "Appears in Neural Information Processing Systems, 2017", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG stat.CO stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Implicit probabilistic models are a flexible class of models defined by a\nsimulation process for data. They form the basis for theories which encompass\nour understanding of the physical world. Despite this fundamental nature, the\nuse of implicit models remains limited due to challenges in specifying complex\nlatent structure in them, and in performing inferences in such models with\nlarge data sets. In this paper, we first introduce hierarchical implicit models\n(HIMs). HIMs combine the idea of implicit densities with hierarchical Bayesian\nmodeling, thereby defining models via simulators of data with rich hidden\nstructure. Next, we develop likelihood-free variational inference (LFVI), a\nscalable variational inference algorithm for HIMs. Key to LFVI is specifying a\nvariational family that is also implicit. This matches the model's flexibility\nand allows for accurate approximation of the posterior. We demonstrate diverse\napplications: a large-scale physical simulator for predator-prey populations in\necology; a Bayesian generative adversarial network for discrete data; and a\ndeep implicit model for text generation.\n", "versions": [{"version": "v1", "created": "Tue, 28 Feb 2017 18:33:32 GMT"}, {"version": "v2", "created": "Mon, 13 Mar 2017 17:28:04 GMT"}, {"version": "v3", "created": "Sun, 5 Nov 2017 01:52:45 GMT"}], "update_date": "2017-11-07", "authors_parsed": [["Tran", "Dustin", ""], ["Ranganath", "Rajesh", ""], ["Blei", "David M.", ""]]}, {"id": "1702.08898", "submitter": "Jan-Peter Calliess", "authors": "Jan-Peter Calliess", "title": "Lipschitz Optimisation for Lipschitz Interpolation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Techniques known as Nonlinear Set Membership prediction, Kinky Inference or\nLipschitz Interpolation are fast and numerically robust approaches to\nnonparametric machine learning that have been proposed to be utilised in the\ncontext of system identification and learning-based control. They utilise\npresupposed Lipschitz properties in order to compute inferences over unobserved\nfunction values. Unfortunately, most of these approaches rely on exact\nknowledge about the input space metric as well as about the Lipschitz constant.\nFurthermore, existing techniques to estimate the Lipschitz constants from the\ndata are not robust to noise or seem to be ad-hoc and typically are decoupled\nfrom the ultimate learning and prediction task. To overcome these limitations,\nwe propose an approach for optimising parameters of the presupposed metrics by\nminimising validation set prediction errors. To avoid poor performance due to\nlocal minima, we propose to utilise Lipschitz properties of the optimisation\nobjective to ensure global optimisation success. The resulting approach is a\nnew flexible method for nonparametric black-box learning. We provide\nexperimental evidence of the competitiveness of our approach on artificial as\nwell as on real data.\n", "versions": [{"version": "v1", "created": "Tue, 28 Feb 2017 18:36:16 GMT"}], "update_date": "2017-03-01", "authors_parsed": [["Calliess", "Jan-Peter", ""]]}]