[{"id": "0709.1516", "submitter": "Marcus Hutter", "authors": "Marcus Hutter", "title": "On Universal Prediction and Bayesian Confirmation", "comments": "24 pages", "journal-ref": "Theoretical Computer Science, 384 (2007) pages 33-48", "doi": null, "report-no": null, "categories": "math.ST cs.IT cs.LG math.IT stat.ML stat.TH", "license": null, "abstract": "  The Bayesian framework is a well-studied and successful framework for\ninductive reasoning, which includes hypothesis testing and confirmation,\nparameter estimation, sequence prediction, classification, and regression. But\nstandard statistical guidelines for choosing the model class and prior are not\nalways available or fail, in particular in complex situations. Solomonoff\ncompleted the Bayesian framework by providing a rigorous, unique, formal, and\nuniversal choice for the model class and the prior. We discuss in breadth how\nand in which sense universal (non-i.i.d.) sequence prediction solves various\n(philosophical) problems of traditional Bayesian sequence prediction. We show\nthat Solomonoff's model possesses many desirable properties: Strong total and\nweak instantaneous bounds, and in contrast to most classical continuous prior\ndensities has no zero p(oste)rior problem, i.e. can confirm universal\nhypotheses, is reparametrization and regrouping invariant, and avoids the\nold-evidence and updating problem. It even performs well (actually better) in\nnon-computable environments.\n", "versions": [{"version": "v1", "created": "Tue, 11 Sep 2007 01:39:20 GMT"}], "update_date": "2008-06-26", "authors_parsed": [["Hutter", "Marcus", ""]]}, {"id": "0709.2760", "submitter": "Chien-Yu Chen", "authors": "Yen-Jen Oyang, Darby Tien-Hao Chang, Yu-Yen Ou, Hao-Geng Hung,\n  Chih-Peng Wu and Chien-Yu Chen", "title": "Supervised Machine Learning with a Novel Kernel Density Estimator", "comments": "The new version includes an additional theorem, Theorem 3", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML", "license": null, "abstract": "  In recent years, kernel density estimation has been exploited by computer\nscientists to model machine learning problems. The kernel density estimation\nbased approaches are of interest due to the low time complexity of either O(n)\nor O(n*log(n)) for constructing a classifier, where n is the number of sampling\ninstances. Concerning design of kernel density estimators, one essential issue\nis how fast the pointwise mean square error (MSE) and/or the integrated mean\nsquare error (IMSE) diminish as the number of sampling instances increases. In\nthis article, it is shown that with the proposed kernel function it is feasible\nto make the pointwise MSE of the density estimator converge at O(n^-2/3)\nregardless of the dimension of the vector space, provided that the probability\ndensity function at the point of interest meets certain conditions.\n", "versions": [{"version": "v1", "created": "Tue, 18 Sep 2007 06:45:30 GMT"}, {"version": "v2", "created": "Fri, 21 Sep 2007 15:07:27 GMT"}, {"version": "v3", "created": "Tue, 16 Oct 2007 15:30:16 GMT"}], "update_date": "2007-10-16", "authors_parsed": [["Oyang", "Yen-Jen", ""], ["Chang", "Darby Tien-Hao", ""], ["Ou", "Yu-Yen", ""], ["Hung", "Hao-Geng", ""], ["Wu", "Chih-Peng", ""], ["Chen", "Chien-Yu", ""]]}, {"id": "0709.2936", "submitter": "Longhai Li", "authors": "Longhai Li", "title": "Bayesian Classification and Regression with High Dimensional Features", "comments": "PhD Thesis Submitted to University of Toronto, 129 Pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML", "license": null, "abstract": "  This thesis responds to the challenges of using a large number, such as\nthousands, of features in regression and classification problems.\n  There are two situations where such high dimensional features arise. One is\nwhen high dimensional measurements are available, for example, gene expression\ndata produced by microarray techniques. For computational or other reasons,\npeople may select only a small subset of features when modelling such data, by\nlooking at how relevant the features are to predicting the response, based on\nsome measure such as correlation with the response in the training data.\nAlthough it is used very commonly, this procedure will make the response appear\nmore predictable than it actually is. In Chapter 2, we propose a Bayesian\nmethod to avoid this selection bias, with application to naive Bayes models and\nmixture models.\n  High dimensional features also arise when we consider high-order\ninteractions. The number of parameters will increase exponentially with the\norder considered. In Chapter 3, we propose a method for compressing a group of\nparameters into a single one, by exploiting the fact that many predictor\nvariables derived from high-order interactions have the same values for all the\ntraining cases. The number of compressed parameters may have converged before\nconsidering the highest possible order. We apply this compression method to\nlogistic sequence prediction models and logistic classification models.\n  We use both simulated data and real data to test our methods in both\nchapters.\n", "versions": [{"version": "v1", "created": "Tue, 18 Sep 2007 23:56:17 GMT"}], "update_date": "2007-09-20", "authors_parsed": [["Li", "Longhai", ""]]}, {"id": "0709.2989", "submitter": "Andrea Lecchini-Visintini Dr", "authors": "A. Lecchini-Visintini, J. Lygeros, J. Maciejowski", "title": "Simulated Annealing: Rigorous finite-time guarantees for optimization on\n  continuous domains", "comments": "10 pages, 2 figures. Preprint. The final version will appear in:\n  Advances in Neural Information Processing Systems 20, Proceedings of NIPS\n  2007, MIT Press", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML", "license": null, "abstract": "  Simulated annealing is a popular method for approaching the solution of a\nglobal optimization problem. Existing results on its performance apply to\ndiscrete combinatorial optimization where the optimization variables can assume\nonly a finite set of possible values. We introduce a new general formulation of\nsimulated annealing which allows one to guarantee finite-time performance in\nthe optimization of functions of continuous variables. The results hold\nuniversally for any optimization problem on a bounded domain and establish a\nconnection between simulated annealing and up-to-date theory of convergence of\nMarkov chain Monte Carlo methods on continuous domains. This work is inspired\nby the concept of finite-time learning with known accuracy and confidence\ndeveloped in statistical learning theory.\n", "versions": [{"version": "v1", "created": "Wed, 19 Sep 2007 10:56:13 GMT"}], "update_date": "2007-09-20", "authors_parsed": [["Lecchini-Visintini", "A.", ""], ["Lygeros", "J.", ""], ["Maciejowski", "J.", ""]]}, {"id": "0709.3121", "submitter": "Francois Meyer", "authors": "Xilin Shen and Fran\\c{c}ois G. Meyer", "title": "Low Dimensional Embedding of fMRI datasets", "comments": "16 pages, 21 figures, submitted to Neuroimage, revised (Jan. 2008)", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML q-bio.NC stat.AP", "license": null, "abstract": "  We propose a novel method to embed a functional magnetic resonance imaging\n(fMRI) dataset in a low-dimensional space. The embedding optimally preserves\nthe local functional coupling between fMRI time series and provides a\nlow-dimensional coordinate system for detecting activated voxels. To compute\nthe embedding, we build a graph of functionally connected voxels. We use the\ncommute time, instead of the geodesic distance, to measure functional distances\non the graph. Because the commute time can be computed directly from the\neigenvectors of (a symmetric version) the graph probability transition matrix,\nwe use these eigenvectors to embed the dataset in low dimensions. After\nclustering the datasets in low dimensions, coherent structures emerge that can\nbe easily interpreted. We performed an extensive evaluation of our method\ncomparing it to linear and nonlinear techniques using synthetic datasets and in\nvivo datasets. We analyzed datasets from the EBC competition obtained with\nsubjects interacting in an urban virtual reality environment. Our exploratory\napproach is able to detect independently visual areas (V1/V2, V5/MT), auditory\nareas, and language areas. Our method can be used to analyze fMRI collected\nduring ``natural stimuli''.\n", "versions": [{"version": "v1", "created": "Wed, 19 Sep 2007 22:56:15 GMT"}, {"version": "v2", "created": "Wed, 16 Jan 2008 03:05:13 GMT"}], "update_date": "2008-01-16", "authors_parsed": [["Shen", "Xilin", ""], ["Meyer", "Fran\u00e7ois G.", ""]]}, {"id": "0709.3512", "submitter": "Jake Hofman", "authors": "Jake M. Hofman and Chris H. Wiggins", "title": "A Bayesian Approach to Network Modularity", "comments": "Phys. Rev. Lett. 100, 258701 (2008)", "journal-ref": "Phys. Rev. Lett. 100, 258701 (2008)", "doi": "10.1103/PhysRevLett.100.258701", "report-no": null, "categories": "physics.data-an cond-mat.stat-mech stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present an efficient, principled, and interpretable technique for\ninferring module assignments and for identifying the optimal number of modules\nin a given network. We show how several existing methods for finding modules\ncan be described as variant, special, or limiting cases of our work, and how\nthe method overcomes the resolution limit problem, accurately recovering the\ntrue number of modules. Our approach is based on Bayesian methods for model\nselection which have been used with success for almost a century, implemented\nusing a variational technique developed only in the past decade. We apply the\ntechnique to synthetic and real networks and outline how the method naturally\nallows selection among competing models.\n", "versions": [{"version": "v1", "created": "Fri, 21 Sep 2007 19:28:04 GMT"}, {"version": "v2", "created": "Thu, 27 Sep 2007 16:50:12 GMT"}, {"version": "v3", "created": "Mon, 23 Jun 2008 20:01:42 GMT"}], "update_date": "2008-06-23", "authors_parsed": [["Hofman", "Jake M.", ""], ["Wiggins", "Chris H.", ""]]}]