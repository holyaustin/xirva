[{"id": "1012.0363", "submitter": "Peter Orbanz", "authors": "Peter Orbanz", "title": "Conjugate Projective Limits", "comments": "49 pages; improved version: revised proof of theorem 3 (results\n  unchanged), discussion added, exposition revised", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.ML stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We characterize conjugate nonparametric Bayesian models as projective limits\nof conjugate, finite-dimensional Bayesian models. In particular, we identify a\nlarge class of nonparametric models representable as infinite-dimensional\nanalogues of exponential family distributions and their canonical conjugate\npriors. This class contains most models studied in the literature, including\nDirichlet processes and Gaussian process regression models. To derive these\nresults, we introduce a representation of infinite-dimensional Bayesian models\nby projective limits of regular conditional probabilities. We show under which\nconditions the nonparametric model itself, its sufficient statistics, and -- if\nthey exist -- conjugate updates of the posterior are projective limits of their\nrespective finite-dimensional counterparts. We illustrate our results both by\napplication to existing nonparametric models and by construction of a model on\ninfinite permutations.\n", "versions": [{"version": "v1", "created": "Thu, 2 Dec 2010 01:57:46 GMT"}, {"version": "v2", "created": "Fri, 7 Jan 2011 12:13:53 GMT"}], "update_date": "2011-01-10", "authors_parsed": [["Orbanz", "Peter", ""]]}, {"id": "1012.0366", "submitter": "Roman Belavkin", "authors": "Roman V. Belavkin", "title": "Optimal measures and Markov transition kernels", "comments": "Replaced with a final and accepted draft; Journal of Global\n  Optimization, Springer, Jan 1, 2012", "journal-ref": null, "doi": "10.1007/s10898-012-9851-1", "report-no": null, "categories": "math.OC cs.CC cs.IT math-ph math.FA math.IT math.MP stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study optimal solutions to an abstract optimization problem for measures,\nwhich is a generalization of classical variational problems in information\ntheory and statistical physics. In the classical problems, information and\nrelative entropy are defined using the Kullback-Leibler divergence, and for\nthis reason optimal measures belong to a one-parameter exponential family.\nMeasures within such a family have the property of mutual absolute continuity.\nHere we show that this property characterizes other families of optimal\npositive measures if a functional representing information has a strictly\nconvex dual. Mutual absolute continuity of optimal probability measures allows\nus to strictly separate deterministic and non-deterministic Markov transition\nkernels, which play an important role in theories of decisions, estimation,\ncontrol, communication and computation. We show that deterministic transitions\nare strictly sub-optimal, unless information resource with a strictly convex\ndual is unconstrained. For illustration, we construct an example where, unlike\nnon-deterministic, any deterministic kernel either has negatively infinite\nexpected utility (unbounded expected error) or communicates infinite\ninformation.\n", "versions": [{"version": "v1", "created": "Thu, 2 Dec 2010 02:08:15 GMT"}, {"version": "v2", "created": "Mon, 13 Dec 2010 22:53:01 GMT"}, {"version": "v3", "created": "Mon, 11 Jul 2011 15:15:14 GMT"}, {"version": "v4", "created": "Fri, 22 Jul 2011 16:42:50 GMT"}, {"version": "v5", "created": "Fri, 3 Feb 2012 03:27:13 GMT"}, {"version": "v6", "created": "Mon, 3 Sep 2012 18:01:59 GMT"}, {"version": "v7", "created": "Wed, 5 Sep 2012 14:56:52 GMT"}], "update_date": "2012-09-06", "authors_parsed": [["Belavkin", "Roman V.", ""]]}, {"id": "1012.0774", "submitter": "Matthias Hein", "authors": "Matthias Hein and Thomas B\\\"uhler", "title": "An Inverse Power Method for Nonlinear Eigenproblems with Applications in\n  1-Spectral Clustering and Sparse PCA", "comments": "Long version of paper accepted at NIPS 2010", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG math.OC stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Many problems in machine learning and statistics can be formulated as\n(generalized) eigenproblems. In terms of the associated optimization problem,\ncomputing linear eigenvectors amounts to finding critical points of a quadratic\nfunction subject to quadratic constraints. In this paper we show that a certain\nclass of constrained optimization problems with nonquadratic objective and\nconstraints can be understood as nonlinear eigenproblems. We derive a\ngeneralization of the inverse power method which is guaranteed to converge to a\nnonlinear eigenvector. We apply the inverse power method to 1-spectral\nclustering and sparse PCA which can naturally be formulated as nonlinear\neigenproblems. In both applications we achieve state-of-the-art results in\nterms of solution quality and runtime. Moving beyond the standard eigenproblem\nshould be useful also in many other applications and our inverse power method\ncan be easily adapted to new problems.\n", "versions": [{"version": "v1", "created": "Fri, 3 Dec 2010 15:58:47 GMT"}], "update_date": "2015-03-17", "authors_parsed": [["Hein", "Matthias", ""], ["B\u00fchler", "Thomas", ""]]}, {"id": "1012.0975", "submitter": "Xiaohui Xie", "authors": "Gui-Bo Ye, Jian-Feng Cai, Xiaohui Xie", "title": "Split Bregman Method for Sparse Inverse Covariance Estimation with\n  Matrix Iteration Acceleration", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider the problem of estimating the inverse covariance matrix by\nmaximizing the likelihood function with a penalty added to encourage the\nsparsity of the resulting matrix. We propose a new approach based on the split\nBregman method to solve the regularized maximum likelihood estimation problem.\nWe show that our method is significantly faster than the widely used graphical\nlasso method, which is based on blockwise coordinate descent, on both\nartificial and real-world data. More importantly, different from the graphical\nlasso, the split Bregman based method is much more general, and can be applied\nto a class of regularization terms other than the $\\ell_1$ norm\n", "versions": [{"version": "v1", "created": "Sun, 5 Dec 2010 07:27:42 GMT"}, {"version": "v2", "created": "Thu, 23 Dec 2010 23:02:28 GMT"}], "update_date": "2015-03-17", "authors_parsed": [["Ye", "Gui-Bo", ""], ["Cai", "Jian-Feng", ""], ["Xie", "Xiaohui", ""]]}, {"id": "1012.1416", "submitter": "Makoto Yamada", "authors": "Makoto Yamada and Masashi Sugiyama", "title": "Cross-Domain Object Matching with Model Selection", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The goal of cross-domain object matching (CDOM) is to find correspondence\nbetween two sets of objects in different domains in an unsupervised way. Photo\nalbum summarization is a typical application of CDOM, where photos are\nautomatically aligned into a designed frame expressed in the Cartesian\ncoordinate system. CDOM is usually formulated as finding a mapping from objects\nin one domain (photos) to objects in the other domain (frame) so that the\npairwise dependency is maximized. A state-of-the-art CDOM method employs a\nkernel-based dependency measure, but it has a drawback that the kernel\nparameter needs to be determined manually. In this paper, we propose\nalternative CDOM methods that can naturally address the model selection\nproblem. Through experiments on image matching, unpaired voice conversion, and\nphoto album summarization tasks, the effectiveness of the proposed methods is\ndemonstrated.\n", "versions": [{"version": "v1", "created": "Tue, 7 Dec 2010 07:19:59 GMT"}], "update_date": "2010-12-08", "authors_parsed": [["Yamada", "Makoto", ""], ["Sugiyama", "Masashi", ""]]}, {"id": "1012.1501", "submitter": "Francis Bach", "authors": "Francis Bach (LIENS, INRIA Paris - Rocquencourt)", "title": "Shaping Level Sets with Submodular Functions", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider a class of sparsity-inducing regularization terms based on\nsubmodular functions. While previous work has focused on non-decreasing\nfunctions, we explore symmetric submodular functions and their \\lova\nextensions. We show that the Lovasz extension may be seen as the convex\nenvelope of a function that depends on level sets (i.e., the set of indices\nwhose corresponding components of the underlying predictor are greater than a\ngiven constant): this leads to a class of convex structured regularization\nterms that impose prior knowledge on the level sets, and not only on the\nsupports of the underlying predictors. We provide a unified set of optimization\nalgorithms, such as proximal operators, and theoretical guarantees (allowed\nlevel sets and recovery conditions). By selecting specific submodular\nfunctions, we give a new interpretation to known norms, such as the total\nvariation; we also define new norms, in particular ones that are based on order\nstatistics with application to clustering and outlier detection, and on noisy\ncuts in graphs with application to change point detection in the presence of\noutliers.\n", "versions": [{"version": "v1", "created": "Tue, 7 Dec 2010 13:34:44 GMT"}, {"version": "v2", "created": "Fri, 10 Jun 2011 14:12:14 GMT"}], "update_date": "2011-06-13", "authors_parsed": [["Bach", "Francis", "", "LIENS, INRIA Paris - Rocquencourt"]]}, {"id": "1012.3407", "submitter": "Ilkka Huopaniemi", "authors": "Ilkka Huopaniemi, Tommi Suvitaival, Matej Ore\\v{s}i\\v{c}, Samuel Kaski", "title": "Translating biomarkers between multi-way time-series experiments", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Translating potential disease biomarkers between multi-species 'omics'\nexperiments is a new direction in biomedical research. The existing methods are\nlimited to simple experimental setups such as basic healthy-diseased\ncomparisons. Most of these methods also require an a priori matching of the\nvariables (e.g., genes or metabolites) between the species. However, many\nexperiments have a complicated multi-way experimental design often involving\nirregularly-sampled time-series measurements, and for instance metabolites do\nnot always have known matchings between organisms. We introduce a Bayesian\nmodelling framework for translating between multiple species the results from\n'omics' experiments having a complex multi-way, time-series experimental\ndesign. The underlying assumption is that the unknown matching can be inferred\nfrom the response of the variables to multiple covariates including time.\n", "versions": [{"version": "v1", "created": "Wed, 15 Dec 2010 18:01:25 GMT"}], "update_date": "2010-12-16", "authors_parsed": [["Huopaniemi", "Ilkka", ""], ["Suvitaival", "Tommi", ""], ["Ore\u0161i\u010d", "Matej", ""], ["Kaski", "Samuel", ""]]}, {"id": "1012.3476", "submitter": "Guillaume Desjardins", "authors": "Guillaume Desjardins, Aaron Courville, Yoshua Bengio", "title": "Adaptive Parallel Tempering for Stochastic Maximum Likelihood Learning\n  of RBMs", "comments": "Presented at the \"NIPS 2010 Workshop on Deep Learning and\n  Unsupervised Feature Learning\"", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Restricted Boltzmann Machines (RBM) have attracted a lot of attention of\nlate, as one the principle building blocks of deep networks. Training RBMs\nremains problematic however, because of the intractibility of their partition\nfunction. The maximum likelihood gradient requires a very robust sampler which\ncan accurately sample from the model despite the loss of ergodicity often\nincurred during learning. While using Parallel Tempering in the negative phase\nof Stochastic Maximum Likelihood (SML-PT) helps address the issue, it imposes a\ntrade-off between computational complexity and high ergodicity, and requires\ncareful hand-tuning of the temperatures. In this paper, we show that this\ntrade-off is unnecessary. The choice of optimal temperatures can be automated\nby minimizing average return time (a concept first proposed by [Katzgraber et\nal., 2006]) while chains can be spawned dynamically, as needed, thus minimizing\nthe computational overhead. We show on a synthetic dataset, that this results\nin better likelihood scores.\n", "versions": [{"version": "v1", "created": "Wed, 15 Dec 2010 21:23:09 GMT"}], "update_date": "2010-12-17", "authors_parsed": [["Desjardins", "Guillaume", ""], ["Courville", "Aaron", ""], ["Bengio", "Yoshua", ""]]}, {"id": "1012.3584", "submitter": "Matthias Seeger", "authors": "Matthias W. Seeger, Hannes Nickisch", "title": "Fast Convergent Algorithms for Expectation Propagation Approximate\n  Bayesian Inference", "comments": "16 pages, 3 figures, submitted for conference publication", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a novel algorithm to solve the expectation propagation relaxation\nof Bayesian inference for continuous-variable graphical models. In contrast to\nmost previous algorithms, our method is provably convergent. By marrying\nconvergent EP ideas from (Opper&Winther 05) with covariance decoupling\ntechniques (Wipf&Nagarajan 08, Nickisch&Seeger 09), it runs at least an order\nof magnitude faster than the most commonly used EP solver.\n", "versions": [{"version": "v1", "created": "Thu, 16 Dec 2010 12:47:34 GMT"}], "update_date": "2010-12-17", "authors_parsed": [["Seeger", "Matthias W.", ""], ["Nickisch", "Hannes", ""]]}, {"id": "1012.3795", "submitter": "Mladen Kolar", "authors": "Mladen Kolar and Eric P. Xing", "title": "Estimating Networks With Jumps", "comments": "Added references. Corrected typos", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study the problem of estimating a temporally varying coefficient and\nvarying structure (VCVS) graphical model underlying nonstationary time series\ndata, such as social states of interacting individuals or microarray expression\nprofiles of gene networks, as opposed to i.i.d. data from an invariant model\nwidely considered in current literature of structural estimation. In\nparticular, we consider the scenario in which the model evolves in a piece-wise\nconstant fashion. We propose a procedure that minimizes the so-called TESLA\nloss (i.e., temporally smoothed L1 regularized regression), which allows\njointly estimating the partition boundaries of the VCVS model and the\ncoefficient of the sparse precision matrix on each block of the partition. A\nhighly scalable proximal gradient method is proposed to solve the resultant\nconvex optimization problem; and the conditions for sparsistent estimation and\nthe convergence rate of both the partition boundaries and the network structure\nare established for the first time for such estimators.\n", "versions": [{"version": "v1", "created": "Fri, 17 Dec 2010 01:27:31 GMT"}, {"version": "v2", "created": "Mon, 20 Dec 2010 16:46:11 GMT"}], "update_date": "2010-12-21", "authors_parsed": [["Kolar", "Mladen", ""], ["Xing", "Eric P.", ""]]}, {"id": "1012.3880", "submitter": "Mladen Kolar", "authors": "Mladen Kolar and Eric P. Xing", "title": "Ultra-high Dimensional Multiple Output Learning With Simultaneous\n  Orthogonal Matching Pursuit: A Sure Screening Approach", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a novel application of the Simultaneous Orthogonal Matching\nPursuit (S-OMP) procedure for sparsistant variable selection in ultra-high\ndimensional multi-task regression problems. Screening of variables, as\nintroduced in \\cite{fan08sis}, is an efficient and highly scalable way to\nremove many irrelevant variables from the set of all variables, while retaining\nall the relevant variables. S-OMP can be applied to problems with hundreds of\nthousands of variables and once the number of variables is reduced to a\nmanageable size, a more computationally demanding procedure can be used to\nidentify the relevant variables for each of the regression outputs. To our\nknowledge, this is the first attempt to utilize relatedness of multiple outputs\nto perform fast screening of relevant variables. As our main theoretical\ncontribution, we prove that, asymptotically, S-OMP is guaranteed to reduce an\nultra-high number of variables to below the sample size without losing true\nrelevant variables. We also provide formal evidence that a modified Bayesian\ninformation criterion (BIC) can be used to efficiently determine the number of\niterations in S-OMP. We further provide empirical evidence on the benefit of\nvariable selection using multiple regression outputs jointly, as opposed to\nperforming variable selection for each output separately. The finite sample\nperformance of S-OMP is demonstrated on extensive simulation studies, and on a\ngenetic association mapping problem. $Keywords$ Adaptive Lasso; Greedy forward\nregression; Orthogonal matching pursuit; Multi-output regression; Multi-task\nlearning; Simultaneous orthogonal matching pursuit; Sure screening; Variable\nselection\n", "versions": [{"version": "v1", "created": "Fri, 17 Dec 2010 14:05:42 GMT"}], "update_date": "2010-12-20", "authors_parsed": [["Kolar", "Mladen", ""], ["Xing", "Eric P.", ""]]}, {"id": "1012.4116", "submitter": "Gilad Lerman Dr", "authors": "Gilad Lerman and Teng Zhang", "title": "lp-Recovery of the Most Significant Subspace among Multiple Subspaces\n  with Outliers", "comments": "This is a revised version of the part of 1002.1994 that deals with\n  single subspace recovery. V3: Improved estimates (in particular for Lemma 3.1\n  and for estimates relying on it), asymptotic dependence of probabilities and\n  constants on D and d and further clarifications; for simplicity it assumes\n  uniform distributions on spheres. V4: minor revision for the published\n  version", "journal-ref": "Constructive Approximation, December 2014, Volume 40, Issue 3, pp\n  329-385", "doi": "10.1007/s00365-014-9242-6", "report-no": null, "categories": "stat.ML cs.CV math.FA", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We assume data sampled from a mixture of d-dimensional linear subspaces with\nspherically symmetric distributions within each subspace and an additional\noutlier component with spherically symmetric distribution within the ambient\nspace (for simplicity we may assume that all distributions are uniform on their\ncorresponding unit spheres). We also assume mixture weights for the different\ncomponents. We say that one of the underlying subspaces of the model is most\nsignificant if its mixture weight is higher than the sum of the mixture weights\nof all other subspaces. We study the recovery of the most significant subspace\nby minimizing the lp-averaged distances of data points from d-dimensional\nsubspaces, where p>0. Unlike other lp minimization problems, this minimization\nis non-convex for all p>0 and thus requires different methods for its analysis.\nWe show that if 0<p<=1, then for any fraction of outliers the most significant\nsubspace can be recovered by lp minimization with overwhelming probability\n(which depends on the generating distribution and its parameters). We show that\nwhen adding small noise around the underlying subspaces the most significant\nsubspace can be nearly recovered by lp minimization for any 0<p<=1 with an\nerror proportional to the noise level. On the other hand, if p>1 and there is\nmore than one underlying subspace, then with overwhelming probability the most\nsignificant subspace cannot be recovered or nearly recovered. This last result\ndoes not require spherically symmetric outliers.\n", "versions": [{"version": "v1", "created": "Sat, 18 Dec 2010 20:11:29 GMT"}, {"version": "v2", "created": "Thu, 19 Apr 2012 19:26:55 GMT"}, {"version": "v3", "created": "Thu, 15 Aug 2013 10:45:42 GMT"}, {"version": "v4", "created": "Mon, 13 Jan 2014 14:05:36 GMT"}], "update_date": "2015-07-24", "authors_parsed": [["Lerman", "Gilad", ""], ["Zhang", "Teng", ""]]}, {"id": "1012.4188", "submitter": "Kumar Sricharan", "authors": "Kumar Sricharan, Raviv Raich, Alfred O. Hero III", "title": "Empirical estimation of entropy functionals with confidence", "comments": "Version 3 changes : Additional results on bias correction factors for\n  specifically estimating Shannon and Renyi entropy in Section 5", "journal-ref": null, "doi": null, "report-no": "CSPL Technical Report 398, Dept. of EECS, University of Michigan,\n  Ann Arbor", "categories": "math.ST stat.ML stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper introduces a class of k-nearest neighbor ($k$-NN) estimators\ncalled bipartite plug-in (BPI) estimators for estimating integrals of\nnon-linear functions of a probability density, such as Shannon entropy and\nR\\'enyi entropy. The density is assumed to be smooth, have bounded support, and\nbe uniformly bounded from below on this set. Unlike previous $k$-NN estimators\nof non-linear density functionals, the proposed estimator uses data-splitting\nand boundary correction to achieve lower mean square error. Specifically, we\nassume that $T$ i.i.d. samples ${X}_i \\in \\mathbb{R}^d$ from the density are\nsplit into two pieces of cardinality $M$ and $N$ respectively, with $M$ samples\nused for computing a k-nearest-neighbor density estimate and the remaining $N$\nsamples used for empirical estimation of the integral of the density\nfunctional. By studying the statistical properties of k-NN balls, explicit\nrates for the bias and variance of the BPI estimator are derived in terms of\nthe sample size, the dimension of the samples and the underlying probability\ndistribution. Based on these results, it is possible to specify optimal choice\nof tuning parameters $M/T$, $k$ for maximizing the rate of decrease of the mean\nsquare error (MSE). The resultant optimized BPI estimator converges faster and\nachieves lower mean squared error than previous $k$-NN entropy estimators. In\naddition, a central limit theorem is established for the BPI estimator that\nallows us to specify tight asymptotic confidence intervals.\n", "versions": [{"version": "v1", "created": "Sun, 19 Dec 2010 17:03:51 GMT"}, {"version": "v2", "created": "Sun, 3 Apr 2011 19:14:45 GMT"}, {"version": "v3", "created": "Sat, 25 Feb 2012 17:44:03 GMT"}], "update_date": "2012-02-28", "authors_parsed": [["Sricharan", "Kumar", ""], ["Raich", "Raviv", ""], ["Hero", "Alfred O.", "III"]]}, {"id": "1012.5066", "submitter": "Yilun Chen", "authors": "Yilun Chen, Yuantao Gu, Alfred O. Hero", "title": "Regularized Least-Mean-Square Algorithms", "comments": "9 pages, double column, submitted to IEEE Transactions on Signal\n  Processing", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider adaptive system identification problems with convex constraints\nand propose a family of regularized Least-Mean-Square (LMS) algorithms. We show\nthat with a properly selected regularization parameter the regularized LMS\nprovably dominates its conventional counterpart in terms of mean square\ndeviations. We establish simple and closed-form expressions for choosing this\nregularization parameter. For identifying an unknown sparse system we propose\nsparse and group-sparse LMS algorithms, which are special examples of the\nregularized LMS family. Simulation results demonstrate the advantages of the\nproposed filters in both convergence rate and steady-state error under sparsity\nassumptions on the true coefficient vector.\n", "versions": [{"version": "v1", "created": "Wed, 22 Dec 2010 18:34:08 GMT"}, {"version": "v2", "created": "Thu, 23 Dec 2010 16:50:42 GMT"}], "update_date": "2010-12-24", "authors_parsed": [["Chen", "Yilun", ""], ["Gu", "Yuantao", ""], ["Hero", "Alfred O.", ""]]}, {"id": "1012.5327", "submitter": "Paulo Urriza", "authors": "Paulo Urriza, Eric Rebeiz, Przemys{\\l}aw Pawe{\\l}czak, and Danijela\n  \\v{C}abri\\'c", "title": "Computationally Efficient Modulation Level Classification Based on\n  Probability Distribution Distance Functions", "comments": "3 pages, resubmitted to IEEE Communication Letters (modified based on\n  reviewer comments)", "journal-ref": null, "doi": "10.1109/LCOMM.2011.032811.110316", "report-no": null, "categories": "cs.IT cs.PF math.IT stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a novel modulation level classification (MLC) method based on\nprobability distribution distance functions. The proposed method uses modified\nKuiper and Kolmogorov-Smirnov distances to achieve low computational complexity\nand outperforms the state of the art methods based on cumulants and\ngoodness-of-fit tests. We derive the theoretical performance of the proposed\nMLC method and verify it via simulations. The best classification accuracy,\nunder AWGN with SNR mismatch and phase jitter, is achieved with the proposed\nMLC method using Kuiper distances.\n", "versions": [{"version": "v1", "created": "Fri, 24 Dec 2010 00:22:12 GMT"}, {"version": "v2", "created": "Mon, 27 Dec 2010 07:37:00 GMT"}, {"version": "v3", "created": "Sat, 19 Feb 2011 03:51:24 GMT"}], "update_date": "2011-09-05", "authors_parsed": [["Urriza", "Paulo", ""], ["Rebeiz", "Eric", ""], ["Pawe\u0142czak", "Przemys\u0142aw", ""], ["\u010cabri\u0107", "Danijela", ""]]}, {"id": "1012.5487", "submitter": "Yizhar Toren", "authors": "Yizhar Toren", "title": "Ordinal Risk-Group Classification", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Most classification methods provide either a prediction of class membership\nor an assessment of class membership probability. In the case of two-group\nclassification the predicted probability can be described as \"risk\" of\nbelonging to a \"special\" class . When the required output is a set of\nordinal-risk groups, a discretization of the continuous risk prediction is\nachieved by two common methods: by constructing a set of models that describe\nthe conditional risk function at specific points (quantile regression) or by\ndividing the output of an \"optimal\" classification model into adjacent\nintervals that correspond to the desired risk groups. By defining a new error\nmeasure for the distribution of risk onto intervals we are able to identify\nlower bounds on the accuracy of these methods, showing sub-optimality both in\ntheir distribution of risk and in the efficiency of their resulting partition\ninto intervals. By adding a new form of constraint to the existing maximum\nlikelihood optimization framework and by introducing a penalty function to\navoid degenerate solutions, we show how existing methods can be augmented to\nsolve the ordinal risk-group classification problem. We implement our method\nfor logistic regression (LR) and show a numeric example.\n", "versions": [{"version": "v1", "created": "Sat, 25 Dec 2010 18:10:52 GMT"}, {"version": "v2", "created": "Mon, 24 Jan 2011 21:40:48 GMT"}, {"version": "v3", "created": "Wed, 20 Jul 2011 09:19:45 GMT"}, {"version": "v4", "created": "Thu, 27 Oct 2011 09:07:17 GMT"}], "update_date": "2011-10-28", "authors_parsed": [["Toren", "Yizhar", ""]]}]