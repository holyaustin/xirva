[{"id": "1206.0111", "submitter": "Bjoern Andres", "authors": "Bjoern Andres, Thorsten Beier, Joerg H. Kappes", "title": "OpenGM: A C++ Library for Discrete Graphical Models", "comments": "5 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.MS stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  OpenGM is a C++ template library for defining discrete graphical models and\nperforming inference on these models, using a wide range of state-of-the-art\nalgorithms. No restrictions are imposed on the factor graph to allow for\nhigher-order factors and arbitrary neighborhood structures. Large models with\nrepetitive structure are handled efficiently because (i) functions that occur\nrepeatedly need to be stored only once, and (ii) distinct functions can be\nimplemented differently, using different encodings alongside each other in the\nsame model. Several parametric functions (e.g. metrics), sparse and dense value\ntables are provided and so is an interface for custom C++ code. Algorithms are\nseparated by design from the representation of graphical models and are easily\nexchangeable. OpenGM, its algorithms, HDF5 file format and command line tools\nare modular and extendible.\n", "versions": [{"version": "v1", "created": "Fri, 1 Jun 2012 07:36:54 GMT"}], "update_date": "2012-06-04", "authors_parsed": [["Andres", "Bjoern", ""], ["Beier", "Thorsten", ""], ["Kappes", "Joerg H.", ""]]}, {"id": "1206.0304", "submitter": "Samer Abdallah", "authors": "Samer A. Abdallah and Mark D. Plumbley", "title": "Predictive Information Rate in Discrete-time Gaussian Processes", "comments": "8 pages, 5 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We derive expressions for the predicitive information rate (PIR) for the\nclass of autoregressive Gaussian processes AR(N), both in terms of the\nprediction coefficients and in terms of the power spectral density. The latter\nresult suggests a duality between the PIR and the multi-information rate for\nprocesses with mutually inverse power spectra (i.e. with poles and zeros of the\ntransfer function exchanged). We investigate the behaviour of the PIR in\nrelation to the multi-information rate for some simple examples, which suggest,\nsomewhat counter-intuitively, that the PIR is maximised for very `smooth' AR\nprocesses whose power spectra have multiple poles at zero frequency. We also\nobtain results for moving average Gaussian processes which are consistent with\nthe duality conjectured earlier. One consequence of this is that the PIR is\nunbounded for MA(N) processes.\n", "versions": [{"version": "v1", "created": "Fri, 1 Jun 2012 20:36:32 GMT"}, {"version": "v2", "created": "Tue, 14 Aug 2012 11:49:30 GMT"}], "update_date": "2012-08-15", "authors_parsed": [["Abdallah", "Samer A.", ""], ["Plumbley", "Mark D.", ""]]}, {"id": "1206.0333", "submitter": "Jianhui Chen", "authors": "Jianhui Chen and Jieping Ye", "title": "Sparse Trace Norm Regularization", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study the problem of estimating multiple predictive functions from a\ndictionary of basis functions in the nonparametric regression setting. Our\nestimation scheme assumes that each predictive function can be estimated in the\nform of a linear combination of the basis functions. By assuming that the\ncoefficient matrix admits a sparse low-rank structure, we formulate the\nfunction estimation problem as a convex program regularized by the trace norm\nand the $\\ell_1$-norm simultaneously. We propose to solve the convex program\nusing the accelerated gradient (AG) method and the alternating direction method\nof multipliers (ADMM) respectively; we also develop efficient algorithms to\nsolve the key components in both AG and ADMM. In addition, we conduct\ntheoretical analysis on the proposed function estimation scheme: we derive a\nkey property of the optimal solution to the convex program; based on an\nassumption on the basis functions, we establish a performance bound of the\nproposed function estimation scheme (via the composite regularization).\nSimulation studies demonstrate the effectiveness and efficiency of the proposed\nalgorithms.\n", "versions": [{"version": "v1", "created": "Sat, 2 Jun 2012 00:48:27 GMT"}], "update_date": "2012-06-05", "authors_parsed": [["Chen", "Jianhui", ""], ["Ye", "Jieping", ""]]}, {"id": "1206.0387", "submitter": "Guido F.  Montufar", "authors": "Guido F. Montufar and Jason Morton", "title": "When Does a Mixture of Products Contain a Product of Mixtures?", "comments": "32 pages, 6 figures, 2 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML math.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We derive relations between theoretical properties of restricted Boltzmann\nmachines (RBMs), popular machine learning models which form the building blocks\nof deep learning models, and several natural notions from discrete mathematics\nand convex geometry. We give implications and equivalences relating\nRBM-representable probability distributions, perfectly reconstructible inputs,\nHamming modes, zonotopes and zonosets, point configurations in hyperplane\narrangements, linear threshold codes, and multi-covering numbers of hypercubes.\nAs a motivating application, we prove results on the relative representational\npower of mixtures of product distributions and products of mixtures of pairs of\nproduct distributions (RBMs) that formally justify widely held intuitions about\ndistributed representations. In particular, we show that a mixture of products\nrequiring an exponentially larger number of parameters is needed to represent\nthe probability distributions which can be obtained as products of mixtures.\n", "versions": [{"version": "v1", "created": "Sat, 2 Jun 2012 14:48:30 GMT"}, {"version": "v2", "created": "Tue, 6 Nov 2012 00:10:47 GMT"}, {"version": "v3", "created": "Fri, 15 Mar 2013 18:25:26 GMT"}, {"version": "v4", "created": "Fri, 8 Nov 2013 00:52:45 GMT"}, {"version": "v5", "created": "Thu, 18 Sep 2014 23:07:02 GMT"}], "update_date": "2014-09-22", "authors_parsed": [["Montufar", "Guido F.", ""], ["Morton", "Jason", ""]]}, {"id": "1206.0392", "submitter": "Vladimir Temlyakov", "authors": "V.N. Temlyakov", "title": "Greedy approximation in convex optimization", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML math.OC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study sparse approximate solutions to convex optimization problems. It is\nknown that in many engineering applications researchers are interested in an\napproximate solution of an optimization problem as a linear combination of\nelements from a given system of elements. There is an increasing interest in\nbuilding such sparse approximate solutions using different greedy-type\nalgorithms. The problem of approximation of a given element of a Banach space\nby linear combinations of elements from a given system (dictionary) is well\nstudied in nonlinear approximation theory. At a first glance the settings of\napproximation and optimization problems are very different. In the\napproximation problem an element is given and our task is to find a sparse\napproximation of it. In optimization theory an energy function is given and we\nshould find an approximate sparse solution to the minimization problem. It\nturns out that the same technique can be used for solving both problems. We\nshow how the technique developed in nonlinear approximation theory, in\nparticular, the greedy approximation technique can be adjusted for finding a\nsparse solution of an optimization problem.\n", "versions": [{"version": "v1", "created": "Sat, 2 Jun 2012 16:59:14 GMT"}], "update_date": "2012-06-05", "authors_parsed": [["Temlyakov", "V. N.", ""]]}, {"id": "1206.0393", "submitter": "Vladimir Temlyakov", "authors": "V.N. Temlyakov", "title": "Greedy expansions in convex optimization", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML math.OC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper is a follow up to the previous author's paper on convex\noptimization. In that paper we began the process of adjusting greedy-type\nalgorithms from nonlinear approximation for finding sparse solutions of convex\noptimization problems. We modified there three the most popular in nonlinear\napproximation in Banach spaces greedy algorithms -- Weak Chebyshev Greedy\nAlgorithm, Weak Greedy Algorithm with Free Relaxation and Weak Relaxed Greedy\nAlgorithm -- for solving convex optimization problems. We continue to study\nsparse approximate solutions to convex optimization problems. It is known that\nin many engineering applications researchers are interested in an approximate\nsolution of an optimization problem as a linear combination of elements from a\ngiven system of elements. There is an increasing interest in building such\nsparse approximate solutions using different greedy-type algorithms. In this\npaper we concentrate on greedy algorithms that provide expansions, which means\nthat the approximant at the $m$th iteration is equal to the sum of the\napproximant from the previous iteration ($(m-1)$th iteration) and one element\nfrom the dictionary with an appropriate coefficient. The problem of greedy\nexpansions of elements of a Banach space is well studied in nonlinear\napproximation theory. At a first glance the setting of a problem of expansion\nof a given element and the setting of the problem of expansion in an\noptimization problem are very different. However, it turns out that the same\ntechnique can be used for solving both problems. We show how the technique\ndeveloped in nonlinear approximation theory, in particular, the greedy\nexpansions technique can be adjusted for finding a sparse solution of an\noptimization problem given by an expansion with respect to a given dictionary.\n", "versions": [{"version": "v1", "created": "Sat, 2 Jun 2012 17:08:50 GMT"}], "update_date": "2012-06-05", "authors_parsed": [["Temlyakov", "V. N.", ""]]}, {"id": "1206.0500", "submitter": "Andrew Critch", "authors": "Andrew J. Critch", "title": "Binary hidden Markov models and varieties", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.AG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The technological applications of hidden Markov models have been extremely\ndiverse and successful, including natural language processing, gesture\nrecognition, gene sequencing, and Kalman filtering of physical measurements.\nHMMs are highly non-linear statistical models, and just as linear models are\namenable to linear algebraic techniques, non-linear models are amenable to\ncommutative algebra and algebraic geometry.\n  This paper closely examines HMMs in which all the hidden random variables are\nbinary. Its main contributions are (1) a birational parametrization for every\nsuch HMM, with an explicit inverse for recovering the hidden parameters in\nterms of observables, (2) a semialgebraic model membership test for every such\nHMM, and (3) minimal defining equations for the 4-node fully binary model,\ncomprising 21 quadrics and 29 cubics, which were computed using Grobner bases\nin the cumulant coordinates of Sturmfels and Zwiernik. The new model parameters\nin (1) are rationally identifiable in the sense of Sullivant, Garcia-Puente,\nand Spielvogel, and each model's Zariski closure is therefore a rational\nprojective variety of dimension 5. Grobner basis computations for the model and\nits graph are found to be considerably faster using these parameters. In the\ncase of two hidden states, item (2) supersedes a previous algorithm of\nSchonhuth which is only generically defined, and the defining equations (3)\nyield new invariants for HMMs of all lengths $\\geq 4$. Such invariants have\nbeen used successfully in model selection problems in phylogenetics, and one\ncan hope for similar applications in the case of HMMs.\n", "versions": [{"version": "v1", "created": "Sun, 3 Jun 2012 23:43:17 GMT"}, {"version": "v2", "created": "Tue, 10 Jul 2012 01:58:16 GMT"}, {"version": "v3", "created": "Mon, 3 Sep 2012 04:37:33 GMT"}], "update_date": "2012-09-04", "authors_parsed": [["Critch", "Andrew J.", ""]]}, {"id": "1206.0663", "submitter": "Yipeng Liu Dr.", "authors": "Yipeng Liu, Ivan Gligorijevic, Vladimir Matic, Maarten De Vos, Sabine\n  Van Huffel", "title": "Multi-Sparse Signal Recovery for Compressive Sensing", "comments": "4 pages, 7 figures; accepted by The 34th Annual International\n  Conference of the Engineering in Medicine and Biology Society (IEEE EMBC\n  2012)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IT cs.SY math.IT math.OC stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Signal recovery is one of the key techniques of Compressive sensing (CS). It\nreconstructs the original signal from the linear sub-Nyquist measurements.\nClassical methods exploit the sparsity in one domain to formulate the L0 norm\noptimization. Recent investigation shows that some signals are sparse in\nmultiple domains. To further improve the signal reconstruction performance, we\ncan exploit this multi-sparsity to generate a new convex programming model. The\nlatter is formulated with multiple sparsity constraints in multiple domains and\nthe linear measurement fitting constraint. It improves signal recovery\nperformance by additional a priori information. Since some EMG signals exhibit\nsparsity both in time and frequency domains, we take them as example in\nnumerical experiments. Results show that the newly proposed method achieves\nbetter performance for multi-sparse signals.\n", "versions": [{"version": "v1", "created": "Mon, 4 Jun 2012 16:22:34 GMT"}], "update_date": "2012-06-05", "authors_parsed": [["Liu", "Yipeng", ""], ["Gligorijevic", "Ivan", ""], ["Matic", "Vladimir", ""], ["De Vos", "Maarten", ""], ["Van Huffel", "Sabine", ""]]}, {"id": "1206.0771", "submitter": "Jesse Johnson", "authors": "Jesse Johnson", "title": "Topological graph clustering with thin position", "comments": "12 pages, 2 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.GT cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A clustering algorithm partitions a set of data points into smaller sets\n(clusters) such that each subset is more tightly packed than the whole. Many\napproaches to clustering translate the vector data into a graph with edges\nreflecting a distance or similarity metric on the points, then look for highly\nconnected subgraphs. We introduce such an algorithm based on ideas borrowed\nfrom the topological notion of thin position for knots and 3-dimensional\nmanifolds.\n", "versions": [{"version": "v1", "created": "Mon, 4 Jun 2012 21:22:26 GMT"}], "update_date": "2012-06-06", "authors_parsed": [["Johnson", "Jesse", ""]]}, {"id": "1206.0773", "submitter": "James Sharpnack", "authors": "James Sharpnack, Alessandro Rinaldo, Aarti Singh", "title": "Changepoint Detection over Graphs with the Spectral Scan Statistic", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST cs.IT math.IT stat.ML stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider the change-point detection problem of deciding, based on noisy\nmeasurements, whether an unknown signal over a given graph is constant or is\ninstead piecewise constant over two connected induced subgraphs of relatively\nlow cut size. We analyze the corresponding generalized likelihood ratio (GLR)\nstatistics and relate it to the problem of finding a sparsest cut in a graph.\nWe develop a tractable relaxation of the GLR statistic based on the\ncombinatorial Laplacian of the graph, which we call the spectral scan\nstatistic, and analyze its properties. We show how its performance as a testing\nprocedure depends directly on the spectrum of the graph, and use this result to\nexplicitly derive its asymptotic properties on few significant graph\ntopologies. Finally, we demonstrate both theoretically and by simulations that\nthe spectral scan statistic can outperform naive testing procedures based on\nedge thresholding and $\\chi^2$ testing.\n", "versions": [{"version": "v1", "created": "Mon, 4 Jun 2012 21:34:40 GMT"}], "update_date": "2012-06-06", "authors_parsed": [["Sharpnack", "James", ""], ["Rinaldo", "Alessandro", ""], ["Singh", "Aarti", ""]]}, {"id": "1206.0823", "submitter": "Yudong Chen", "authors": "Yudong Chen, Constantine Caramanis", "title": "Orthogonal Matching Pursuit with Noisy and Missing Data: Low and High\n  Dimensional Results", "comments": "Minor revision. Appeared at ICML 2013 under the title \"Noisy and\n  Missing Data Regression: Distribution-Oblivious Support Recovery\"", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST cs.IT math.IT stat.ML stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Many models for sparse regression typically assume that the covariates are\nknown completely, and without noise. Particularly in high-dimensional\napplications, this is often not the case. This paper develops efficient\nOMP-like algorithms to deal with precisely this setting. Our algorithms are as\nefficient as OMP, and improve on the best-known results for missing and noisy\ndata in regression, both in the high-dimensional setting where we seek to\nrecover a sparse vector from only a few measurements, and in the classical\nlow-dimensional setting where we recover an unstructured regressor. In the\nhigh-dimensional setting, our support-recovery algorithm requires no knowledge\nof even the statistics of the noise. Along the way, we also obtain improved\nperformance guarantees for OMP for the standard sparse regression problem with\nGaussian noise.\n", "versions": [{"version": "v1", "created": "Tue, 5 Jun 2012 05:51:33 GMT"}, {"version": "v2", "created": "Mon, 30 Mar 2015 18:02:48 GMT"}], "update_date": "2015-03-31", "authors_parsed": [["Chen", "Yudong", ""], ["Caramanis", "Constantine", ""]]}, {"id": "1206.0937", "submitter": "James Sharpnack", "authors": "James Sharpnack, Akshay Krishnamurthy, Aarti Singh", "title": "Detecting Activations over Graphs using Spanning Tree Wavelet Bases", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.IT math.IT math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider the detection of activations over graphs under Gaussian noise,\nwhere signals are piece-wise constant over the graph. Despite the wide\napplicability of such a detection algorithm, there has been little success in\nthe development of computationally feasible methods with proveable theoretical\nguarantees for general graph topologies. We cast this as a hypothesis testing\nproblem, and first provide a universal necessary condition for asymptotic\ndistinguishability of the null and alternative hypotheses. We then introduce\nthe spanning tree wavelet basis over graphs, a localized basis that reflects\nthe topology of the graph, and prove that for any spanning tree, this approach\ncan distinguish null from alternative in a low signal-to-noise regime. Lastly,\nwe improve on this result and show that using the uniform spanning tree in the\nbasis construction yields a randomized test with stronger theoretical\nguarantees that in many cases matches our necessary conditions. Specifically,\nwe obtain near-optimal performance in edge transitive graphs, $k$-nearest\nneighbor graphs, and $\\epsilon$-graphs.\n", "versions": [{"version": "v1", "created": "Tue, 5 Jun 2012 14:07:40 GMT"}, {"version": "v2", "created": "Wed, 11 Jul 2012 14:45:21 GMT"}, {"version": "v3", "created": "Thu, 12 Jul 2012 14:14:05 GMT"}], "update_date": "2012-07-13", "authors_parsed": [["Sharpnack", "James", ""], ["Krishnamurthy", "Akshay", ""], ["Singh", "Aarti", ""]]}, {"id": "1206.1088", "submitter": "Yutian Chen", "authors": "Yutian Chen, Max Welling", "title": "Bayesian Structure Learning for Markov Random Fields with a Spike and\n  Slab Prior", "comments": "Accepted in the Conference on Uncertainty in Artificial Intelligence\n  (UAI), 2012", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In recent years a number of methods have been developed for automatically\nlearning the (sparse) connectivity structure of Markov Random Fields. These\nmethods are mostly based on L1-regularized optimization which has a number of\ndisadvantages such as the inability to assess model uncertainty and expensive\ncross-validation to find the optimal regularization parameter. Moreover, the\nmodel's predictive performance may degrade dramatically with a suboptimal value\nof the regularization parameter (which is sometimes desirable to induce\nsparseness). We propose a fully Bayesian approach based on a \"spike and slab\"\nprior (similar to L0 regularization) that does not suffer from these\nshortcomings. We develop an approximate MCMC method combining Langevin dynamics\nand reversible jump MCMC to conduct inference in this model. Experiments show\nthat the proposed model learns a good combination of the structure and\nparameter values without the need for separate hyper-parameter tuning.\nMoreover, the model's predictive performance is much more robust than L1-based\nmethods with hyper-parameter settings that induce highly sparse model\nstructures.\n", "versions": [{"version": "v1", "created": "Tue, 5 Jun 2012 23:20:39 GMT"}, {"version": "v2", "created": "Sat, 23 Jun 2012 01:15:47 GMT"}], "update_date": "2012-06-26", "authors_parsed": [["Chen", "Yutian", ""], ["Welling", "Max", ""]]}, {"id": "1206.1106", "submitter": "Tom Schaul", "authors": "Tom Schaul, Sixin Zhang and Yann LeCun", "title": "No More Pesky Learning Rates", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The performance of stochastic gradient descent (SGD) depends critically on\nhow learning rates are tuned and decreased over time. We propose a method to\nautomatically adjust multiple learning rates so as to minimize the expected\nerror at any one time. The method relies on local gradient variations across\nsamples. In our approach, learning rates can increase as well as decrease,\nmaking it suitable for non-stationary problems. Using a number of convex and\nnon-convex learning tasks, we show that the resulting algorithm matches the\nperformance of SGD or other adaptive approaches with their best settings\nobtained through systematic search, and effectively removes the need for\nlearning rate tuning.\n", "versions": [{"version": "v1", "created": "Wed, 6 Jun 2012 02:06:57 GMT"}, {"version": "v2", "created": "Mon, 18 Feb 2013 16:09:50 GMT"}], "update_date": "2013-02-19", "authors_parsed": [["Schaul", "Tom", ""], ["Zhang", "Sixin", ""], ["LeCun", "Yann", ""]]}, {"id": "1206.1270", "submitter": "Benjamin Recht", "authors": "Victor Bittorf and Benjamin Recht and Christopher Re and Joel A. Tropp", "title": "Factoring nonnegative matrices with linear programs", "comments": "17 pages, 10 figures. Modified theorem statement for robust recovery\n  conditions. Revised proof techniques to make arguments more elementary.\n  Results on robustness when rows are duplicated have been superseded by\n  arxiv.org/1211.6687", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.OC cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper describes a new approach, based on linear programming, for\ncomputing nonnegative matrix factorizations (NMFs). The key idea is a\ndata-driven model for the factorization where the most salient features in the\ndata are used to express the remaining features. More precisely, given a data\nmatrix X, the algorithm identifies a matrix C such that X approximately equals\nCX and some linear constraints. The constraints are chosen to ensure that the\nmatrix C selects features; these features can then be used to find a low-rank\nNMF of X. A theoretical analysis demonstrates that this approach has guarantees\nsimilar to those of the recent NMF algorithm of Arora et al. (2012). In\ncontrast with this earlier work, the proposed method extends to more general\nnoise models and leads to efficient, scalable algorithms. Experiments with\nsynthetic and real datasets provide evidence that the new approach is also\nsuperior in practice. An optimized C++ implementation can factor a\nmultigigabyte matrix in a matter of minutes.\n", "versions": [{"version": "v1", "created": "Wed, 6 Jun 2012 16:42:27 GMT"}, {"version": "v2", "created": "Sat, 2 Feb 2013 23:40:56 GMT"}], "update_date": "2013-02-05", "authors_parsed": [["Bittorf", "Victor", ""], ["Recht", "Benjamin", ""], ["Re", "Christopher", ""], ["Tropp", "Joel A.", ""]]}, {"id": "1206.1275", "submitter": "Shiqian Ma", "authors": "Shiqian Ma and Lingzhou Xue and Hui Zou", "title": "Alternating Direction Methods for Latent Variable Gaussian Graphical\n  Model Selection", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.OC stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Chandrasekaran, Parrilo and Willsky (2010) proposed a convex optimization\nproblem to characterize graphical model selection in the presence of unobserved\nvariables. This convex optimization problem aims to estimate an inverse\ncovariance matrix that can be decomposed into a sparse matrix minus a low-rank\nmatrix from sample data. Solving this convex optimization problem is very\nchallenging, especially for large problems. In this paper, we propose two\nalternating direction methods for solving this problem. The first method is to\napply the classical alternating direction method of multipliers to solve the\nproblem as a consensus problem. The second method is a proximal gradient based\nalternating direction method of multipliers. Our methods exploit and take\nadvantage of the special structure of the problem and thus can solve large\nproblems very efficiently. Global convergence result is established for the\nproposed methods. Numerical results on both synthetic data and gene expression\ndata show that our methods usually solve problems with one million variables in\none to two minutes, and are usually five to thirty five times faster than a\nstate-of-the-art Newton-CG proximal point algorithm.\n", "versions": [{"version": "v1", "created": "Wed, 6 Jun 2012 16:51:40 GMT"}, {"version": "v2", "created": "Wed, 26 Sep 2012 05:15:34 GMT"}], "update_date": "2012-09-27", "authors_parsed": [["Ma", "Shiqian", ""], ["Xue", "Lingzhou", ""], ["Zou", "Hui", ""]]}, {"id": "1206.1386", "submitter": "Teng Zhang", "authors": "Teng Zhang", "title": "Robust subspace recovery by Tyler's M-estimator", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper considers the problem of robust subspace recovery: given a set of\n$N$ points in $\\mathbb{R}^D$, if many lie in a $d$-dimensional subspace, then\ncan we recover the underlying subspace? We show that Tyler's M-estimator can be\nused to recover the underlying subspace, if the percentage of the inliers is\nlarger than $d/D$ and the data points lie in general position. Empirically,\nTyler's M-estimator compares favorably with other convex subspace recovery\nalgorithms in both simulations and experiments on real data sets.\n", "versions": [{"version": "v1", "created": "Thu, 7 Jun 2012 02:07:48 GMT"}, {"version": "v2", "created": "Sun, 10 Jun 2012 15:28:54 GMT"}, {"version": "v3", "created": "Thu, 28 Apr 2016 16:10:32 GMT"}, {"version": "v4", "created": "Thu, 29 Apr 2021 12:07:11 GMT"}], "update_date": "2021-04-30", "authors_parsed": [["Zhang", "Teng", ""]]}, {"id": "1206.1402", "submitter": "Ali Jalali", "authors": "Ali Jalali and Sujay Sanghavi", "title": "A New Greedy Algorithm for Multiple Sparse Regression", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper proposes a new algorithm for multiple sparse regression in high\ndimensions, where the task is to estimate the support and values of several\n(typically related) sparse vectors from a few noisy linear measurements. Our\nalgorithm is a \"forward-backward\" greedy procedure that -- uniquely -- operates\non two distinct classes of objects. In particular, we organize our target\nsparse vectors as a matrix; our algorithm involves iterative addition and\nremoval of both (a) individual elements, and (b) entire rows (corresponding to\nshared features), of the matrix.\n  Analytically, we establish that our algorithm manages to recover the supports\n(exactly) and values (approximately) of the sparse vectors, under assumptions\nsimilar to existing approaches based on convex optimization. However, our\nalgorithm has a much smaller computational complexity. Perhaps most\ninterestingly, it is seen empirically to require visibly fewer samples. Ours\nrepresents the first attempt to extend greedy algorithms to the class of models\nthat can only/best be represented by a combination of component structural\nassumptions (sparse and group-sparse, in our case).\n", "versions": [{"version": "v1", "created": "Thu, 7 Jun 2012 05:14:22 GMT"}], "update_date": "2012-06-08", "authors_parsed": [["Jalali", "Ali", ""], ["Sanghavi", "Sujay", ""]]}, {"id": "1206.1529", "submitter": "Anastasios Kyrillidis", "authors": "Anastasios Kyrillidis, Stephen Becker, Volkan Cevher and, Christoph\n  Koch", "title": "Sparse projections onto the simplex", "comments": "9 Pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Most learning methods with rank or sparsity constraints use convex\nrelaxations, which lead to optimization with the nuclear norm or the\n$\\ell_1$-norm. However, several important learning applications cannot benefit\nfrom this approach as they feature these convex norms as constraints in\naddition to the non-convex rank and sparsity constraints. In this setting, we\nderive efficient sparse projections onto the simplex and its extension, and\nillustrate how to use them to solve high-dimensional learning problems in\nquantum tomography, sparse density estimation and portfolio selection with\nnon-convex constraints.\n", "versions": [{"version": "v1", "created": "Thu, 7 Jun 2012 15:33:12 GMT"}, {"version": "v2", "created": "Thu, 14 Jun 2012 07:21:01 GMT"}, {"version": "v3", "created": "Thu, 10 Jan 2013 16:33:23 GMT"}, {"version": "v4", "created": "Thu, 28 Mar 2013 15:01:33 GMT"}, {"version": "v5", "created": "Wed, 10 Apr 2013 08:39:10 GMT"}], "update_date": "2013-04-11", "authors_parsed": [["Kyrillidis", "Anastasios", ""], ["Becker", "Stephen", ""], ["and", "Volkan Cevher", ""], ["Koch", "Christoph", ""]]}, {"id": "1206.1557", "submitter": "Jay Gholap", "authors": "Jay Gholap, Anurag Ingole, Jayesh Gohil, Shailesh Gargade and Vahida\n  Attar", "title": "Soil Data Analysis Using Classification Techniques and Soil Attribute\n  Prediction", "comments": "4 pages, published in International Journal of Computer Science\n  Issues, Volume 9, Issue 3", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI stat.AP stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Agricultural research has been profited by technical advances such as\nautomation, data mining. Today, data mining is used in a vast areas and many\noff-the-shelf data mining system products and domain specific data mining\napplication soft wares are available, but data mining in agricultural soil\ndatasets is a relatively a young research field. The large amounts of data that\nare nowadays virtually harvested along with the crops have to be analyzed and\nshould be used to their full extent. This research aims at analysis of soil\ndataset using data mining techniques. It focuses on classification of soil\nusing various algorithms available. Another important purpose is to predict\nuntested attributes using regression technique, and implementation of automated\nsoil sample classification.\n", "versions": [{"version": "v1", "created": "Thu, 7 Jun 2012 17:28:20 GMT"}], "update_date": "2012-06-08", "authors_parsed": [["Gholap", "Jay", ""], ["Ingole", "Anurag", ""], ["Gohil", "Jayesh", ""], ["Gargade", "Shailesh", ""], ["Attar", "Vahida", ""]]}, {"id": "1206.1623", "submitter": "Jason Lee", "authors": "Jason D. Lee, Yuekai Sun, Michael A. Saunders", "title": "Proximal Newton-type methods for minimizing composite functions", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.DS cs.LG cs.NA math.OC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We generalize Newton-type methods for minimizing smooth functions to handle a\nsum of two convex functions: a smooth function and a nonsmooth function with a\nsimple proximal mapping. We show that the resulting proximal Newton-type\nmethods inherit the desirable convergence behavior of Newton-type methods for\nminimizing smooth functions, even when search directions are computed\ninexactly. Many popular methods tailored to problems arising in bioinformatics,\nsignal processing, and statistical learning are special cases of proximal\nNewton-type methods, and our analysis yields new convergence results for some\nof these methods.\n", "versions": [{"version": "v1", "created": "Thu, 7 Jun 2012 21:31:23 GMT"}, {"version": "v10", "created": "Thu, 16 May 2013 15:30:05 GMT"}, {"version": "v11", "created": "Mon, 3 Jun 2013 16:19:20 GMT"}, {"version": "v12", "created": "Wed, 25 Dec 2013 19:55:21 GMT"}, {"version": "v13", "created": "Mon, 17 Mar 2014 22:08:25 GMT"}, {"version": "v2", "created": "Wed, 10 Oct 2012 15:14:21 GMT"}, {"version": "v3", "created": "Sun, 14 Oct 2012 21:54:57 GMT"}, {"version": "v4", "created": "Sun, 11 Nov 2012 19:21:12 GMT"}, {"version": "v5", "created": "Mon, 15 Apr 2013 18:28:57 GMT"}, {"version": "v6", "created": "Tue, 16 Apr 2013 16:38:48 GMT"}, {"version": "v7", "created": "Wed, 17 Apr 2013 17:40:34 GMT"}, {"version": "v8", "created": "Mon, 22 Apr 2013 16:24:55 GMT"}, {"version": "v9", "created": "Mon, 29 Apr 2013 18:57:33 GMT"}], "update_date": "2014-03-19", "authors_parsed": [["Lee", "Jason D.", ""], ["Sun", "Yuekai", ""], ["Saunders", "Michael A.", ""]]}, {"id": "1206.1846", "submitter": "David Duvenaud", "authors": "Tomoharu Iwata, David Duvenaud, Zoubin Ghahramani", "title": "Warped Mixtures for Nonparametric Cluster Shapes", "comments": "10 pages, 6 figures, submitted for review", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML", "license": "http://creativecommons.org/licenses/by/3.0/", "abstract": "  A mixture of Gaussians fit to a single curved or heavy-tailed cluster will\nreport that the data contains many clusters. To produce more appropriate\nclusterings, we introduce a model which warps a latent mixture of Gaussians to\nproduce nonparametric cluster shapes. The possibly low-dimensional latent\nmixture model allows us to summarize the properties of the high-dimensional\nclusters (or density manifolds) describing the data. The number of manifolds,\nas well as the shape and dimension of each manifold is automatically inferred.\nWe derive a simple inference scheme for this model which analytically\nintegrates out both the mixture parameters and the warping function. We show\nthat our model is effective for density estimation, performs better than\ninfinite Gaussian mixture models at recovering the true number of clusters, and\nproduces interpretable summaries of high-dimensional datasets.\n", "versions": [{"version": "v1", "created": "Fri, 8 Jun 2012 19:45:49 GMT"}, {"version": "v2", "created": "Thu, 21 Mar 2013 20:50:18 GMT"}], "update_date": "2013-03-25", "authors_parsed": [["Iwata", "Tomoharu", ""], ["Duvenaud", "David", ""], ["Ghahramani", "Zoubin", ""]]}, {"id": "1206.1874", "submitter": "Bin Dai", "authors": "Bin Dai, Shilin Ding, Grace Wahba", "title": "Multivariate Bernoulli distribution", "comments": "Published in at http://dx.doi.org/10.3150/12-BEJSP10 the Bernoulli\n  (http://isi.cbs.nl/bernoulli/) by the International Statistical\n  Institute/Bernoulli Society (http://isi.cbs.nl/BS/bshome.htm)", "journal-ref": "Bernoulli 2013, Vol. 19, No. 4, 1465-1483", "doi": "10.3150/12-BEJSP10", "report-no": "IMS-BEJ-BEJSP10", "categories": "stat.AP math.ST stat.ML stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we consider the multivariate Bernoulli distribution as a model\nto estimate the structure of graphs with binary nodes. This distribution is\ndiscussed in the framework of the exponential family, and its statistical\nproperties regarding independence of the nodes are demonstrated. Importantly\nthe model can estimate not only the main effects and pairwise interactions\namong the nodes but also is capable of modeling higher order interactions,\nallowing for the existence of complex clique effects. We compare the\nmultivariate Bernoulli model with existing graphical inference models - the\nIsing model and the multivariate Gaussian model, where only the pairwise\ninteractions are considered. On the other hand, the multivariate Bernoulli\ndistribution has an interesting property in that independence and\nuncorrelatedness of the component random variables are equivalent. Both the\nmarginal and conditional distributions of a subset of variables in the\nmultivariate Bernoulli distribution still follow the multivariate Bernoulli\ndistribution. Furthermore, the multivariate Bernoulli logistic model is\ndeveloped under generalized linear model theory by utilizing the canonical link\nfunction in order to include covariate information on the nodes, edges and\ncliques. We also consider variable selection techniques such as LASSO in the\nlogistic model to impose sparsity structure on the graph. Finally, we discuss\nextending the smoothing spline ANOVA approach to the multivariate Bernoulli\nlogistic model to enable estimation of non-linear effects of the predictor\nvariables.\n", "versions": [{"version": "v1", "created": "Fri, 8 Jun 2012 20:49:42 GMT"}, {"version": "v2", "created": "Tue, 12 Nov 2013 11:03:31 GMT"}], "update_date": "2013-11-13", "authors_parsed": [["Dai", "Bin", ""], ["Ding", "Shilin", ""], ["Wahba", "Grace", ""]]}, {"id": "1206.1898", "submitter": "Pedro Alejandro Ortega", "authors": "Pedro A. Ortega, Jordi Grau-Moya, Tim Genewein, David Balduzzi, Daniel\n  A. Braun", "title": "A Nonparametric Conjugate Prior Distribution for the Maximizing Argument\n  of a Noisy Function", "comments": "9 pages, 5 figures", "journal-ref": "Neural Information Processing Systems (NIPS) 2012", "doi": null, "report-no": null, "categories": "stat.ML cs.AI math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a novel Bayesian approach to solve stochastic optimization\nproblems that involve finding extrema of noisy, nonlinear functions. Previous\nwork has focused on representing possible functions explicitly, which leads to\na two-step procedure of first, doing inference over the function space and\nsecond, finding the extrema of these functions. Here we skip the representation\nstep and directly model the distribution over extrema. To this end, we devise a\nnon-parametric conjugate prior based on a kernel regressor. The resulting\nposterior distribution directly captures the uncertainty over the maximum of\nthe unknown function. We illustrate the effectiveness of our model by\noptimizing a noisy, high-dimensional, non-convex objective function.\n", "versions": [{"version": "v1", "created": "Sat, 9 Jun 2012 01:57:02 GMT"}, {"version": "v2", "created": "Sat, 10 Nov 2012 18:09:17 GMT"}], "update_date": "2012-11-13", "authors_parsed": [["Ortega", "Pedro A.", ""], ["Grau-Moya", "Jordi", ""], ["Genewein", "Tim", ""], ["Balduzzi", "David", ""], ["Braun", "Daniel A.", ""]]}, {"id": "1206.2197", "submitter": "Yipeng Liu Dr.", "authors": "Rong Fan, Qun Wan, Yipeng Liu, Hui Chen and Xiao Zhang", "title": "Complex Orthogonal Matching Pursuit and Its Exact Recovery Conditions", "comments": "18 pages, 5 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IT math.IT math.NA stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we present new results on using orthogonal matching pursuit\n(OMP), to solve the sparse approximation problem over redundant dictionaries\nfor complex cases (i.e., complex measurement vector, complex dictionary and\ncomplex additive white Gaussian noise (CAWGN)). A sufficient condition that OMP\ncan recover the optimal representation of an exactly sparse signal in the\ncomplex cases is proposed both in noiseless and bound Gaussian noise settings.\nSimilar to exact recovery condition (ERC) results in real cases, we extend them\nto complex case and derivate the corresponding ERC in the paper. It leverages\nthis theory to show that OMP succeed for k-sparse signal from a class of\ncomplex dictionary. Besides, an application with geometrical theory of\ndiffraction (GTD) model is presented for complex cases. Finally, simulation\nexperiments illustrate the validity of the theoretical analysis.\n", "versions": [{"version": "v1", "created": "Mon, 11 Jun 2012 13:28:15 GMT"}], "update_date": "2012-06-12", "authors_parsed": [["Fan", "Rong", ""], ["Wan", "Qun", ""], ["Liu", "Yipeng", ""], ["Chen", "Hui", ""], ["Zhang", "Xiao", ""]]}, {"id": "1206.2248", "submitter": "Tammo Krueger", "authors": "Tammo Krueger, Danny Panknin, Mikio Braun", "title": "Fast Cross-Validation via Sequential Testing", "comments": null, "journal-ref": "Journal of Machine Learning Research, 16:1103-1155, 2015", "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  With the increasing size of today's data sets, finding the right parameter\nconfiguration in model selection via cross-validation can be an extremely\ntime-consuming task. In this paper we propose an improved cross-validation\nprocedure which uses nonparametric testing coupled with sequential analysis to\ndetermine the best parameter set on linearly increasing subsets of the data. By\neliminating underperforming candidates quickly and keeping promising candidates\nas long as possible, the method speeds up the computation while preserving the\ncapability of the full cross-validation. Theoretical considerations underline\nthe statistical power of our procedure. The experimental evaluation shows that\nour method reduces the computation time by a factor of up to 120 compared to a\nfull cross-validation with a negligible impact on the accuracy.\n", "versions": [{"version": "v1", "created": "Mon, 11 Jun 2012 15:14:59 GMT"}, {"version": "v2", "created": "Mon, 21 Jan 2013 10:40:40 GMT"}, {"version": "v3", "created": "Mon, 15 Apr 2013 13:50:10 GMT"}, {"version": "v4", "created": "Thu, 5 Sep 2013 18:27:04 GMT"}, {"version": "v5", "created": "Wed, 16 Jul 2014 12:21:09 GMT"}, {"version": "v6", "created": "Wed, 3 Feb 2016 21:13:20 GMT"}], "update_date": "2016-02-05", "authors_parsed": [["Krueger", "Tammo", ""], ["Panknin", "Danny", ""], ["Braun", "Mikio", ""]]}, {"id": "1206.2459", "submitter": "Tim van Erven", "authors": "Tim van Erven and Peter Harremo\\\"es", "title": "R\\'enyi Divergence and Kullback-Leibler Divergence", "comments": "To appear in IEEE Transactions on Information Theory", "journal-ref": null, "doi": "10.1109/TIT.2014.2320500", "report-no": null, "categories": "cs.IT math.IT math.ST stat.ML stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  R\\'enyi divergence is related to R\\'enyi entropy much like Kullback-Leibler\ndivergence is related to Shannon's entropy, and comes up in many settings. It\nwas introduced by R\\'enyi as a measure of information that satisfies almost the\nsame axioms as Kullback-Leibler divergence, and depends on a parameter that is\ncalled its order. In particular, the R\\'enyi divergence of order 1 equals the\nKullback-Leibler divergence.\n  We review and extend the most important properties of R\\'enyi divergence and\nKullback-Leibler divergence, including convexity, continuity, limits of\n$\\sigma$-algebras and the relation of the special order 0 to the Gaussian\ndichotomy and contiguity. We also show how to generalize the Pythagorean\ninequality to orders different from 1, and we extend the known equivalence\nbetween channel capacity and minimax redundancy to continuous channel inputs\n(for all orders) and present several other minimax results.\n", "versions": [{"version": "v1", "created": "Tue, 12 Jun 2012 08:34:47 GMT"}, {"version": "v2", "created": "Thu, 24 Apr 2014 09:15:51 GMT"}], "update_date": "2014-04-25", "authors_parsed": [["van Erven", "Tim", ""], ["Harremo\u00ebs", "Peter", ""]]}, {"id": "1206.2944", "submitter": "Jasper Snoek", "authors": "Jasper Snoek, Hugo Larochelle and Ryan P. Adams", "title": "Practical Bayesian Optimization of Machine Learning Algorithms", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Machine learning algorithms frequently require careful tuning of model\nhyperparameters, regularization terms, and optimization parameters.\nUnfortunately, this tuning is often a \"black art\" that requires expert\nexperience, unwritten rules of thumb, or sometimes brute-force search. Much\nmore appealing is the idea of developing automatic approaches which can\noptimize the performance of a given learning algorithm to the task at hand. In\nthis work, we consider the automatic tuning problem within the framework of\nBayesian optimization, in which a learning algorithm's generalization\nperformance is modeled as a sample from a Gaussian process (GP). The tractable\nposterior distribution induced by the GP leads to efficient use of the\ninformation gathered by previous experiments, enabling optimal choices about\nwhat parameters to try next. Here we show how the effects of the Gaussian\nprocess prior and the associated inference procedure can have a large impact on\nthe success or failure of Bayesian optimization. We show that thoughtful\nchoices can lead to results that exceed expert-level performance in tuning\nmachine learning algorithms. We also describe new algorithms that take into\naccount the variable cost (duration) of learning experiments and that can\nleverage the presence of multiple cores for parallel experimentation. We show\nthat these proposed algorithms improve on previous automatic procedures and can\nreach or surpass human expert-level optimization on a diverse set of\ncontemporary algorithms including latent Dirichlet allocation, structured SVMs\nand convolutional neural networks.\n", "versions": [{"version": "v1", "created": "Wed, 13 Jun 2012 21:23:15 GMT"}, {"version": "v2", "created": "Wed, 29 Aug 2012 06:36:23 GMT"}], "update_date": "2012-08-30", "authors_parsed": [["Snoek", "Jasper", ""], ["Larochelle", "Hugo", ""], ["Adams", "Ryan P.", ""]]}, {"id": "1206.3072", "submitter": "Matus Telgarsky", "authors": "Matus Telgarsky", "title": "Statistical Consistency of Finite-dimensional Unregularized Linear\n  Classification", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This manuscript studies statistical properties of linear classifiers obtained\nthrough minimization of an unregularized convex risk over a finite sample.\nAlthough the results are explicitly finite-dimensional, inputs may be passed\nthrough feature maps; in this way, in addition to treating the consistency of\nlogistic regression, this analysis also handles boosting over a finite weak\nlearning class with, for instance, the exponential, logistic, and hinge losses.\nIn this finite-dimensional setting, it is still possible to fit arbitrary\ndecision boundaries: scaling the complexity of the weak learning class with the\nsample size leads to the optimal classification risk almost surely.\n", "versions": [{"version": "v1", "created": "Thu, 14 Jun 2012 11:05:55 GMT"}], "update_date": "2012-06-15", "authors_parsed": [["Telgarsky", "Matus", ""]]}, {"id": "1206.3137", "submitter": "Daniel Hsu", "authors": "Daniel Hsu and Sham M. Kakade and Percy Liang", "title": "Identifiability and Unmixing of Latent Parse Trees", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper explores unsupervised learning of parsing models along two\ndirections. First, which models are identifiable from infinite data? We use a\ngeneral technique for numerically checking identifiability based on the rank of\na Jacobian matrix, and apply it to several standard constituency and dependency\nparsing models. Second, for identifiable models, how do we estimate the\nparameters efficiently? EM suffers from local optima, while recent work using\nspectral methods cannot be directly applied since the topology of the parse\ntree varies across sentences. We develop a strategy, unmixing, which deals with\nthis additional complexity for restricted classes of parsing models.\n", "versions": [{"version": "v1", "created": "Thu, 14 Jun 2012 15:21:24 GMT"}], "update_date": "2012-06-15", "authors_parsed": [["Hsu", "Daniel", ""], ["Kakade", "Sham M.", ""], ["Liang", "Percy", ""]]}, {"id": "1206.3231", "submitter": "Emma Brunskill", "authors": "Emma Brunskill, Bethany Leffler, Lihong Li, Michael L. Littman,\n  Nicholas Roy", "title": "CORL: A Continuous-state Offset-dynamics Reinforcement Learner", "comments": "Appears in Proceedings of the Twenty-Fourth Conference on Uncertainty\n  in Artificial Intelligence (UAI2008)", "journal-ref": null, "doi": null, "report-no": "UAI-P-2008-PG-53-61", "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Continuous state spaces and stochastic, switching dynamics characterize a\nnumber of rich, realworld domains, such as robot navigation across varying\nterrain. We describe a reinforcementlearning algorithm for learning in these\ndomains and prove for certain environments the algorithm is probably\napproximately correct with a sample complexity that scales polynomially with\nthe state-space dimension. Unfortunately, no optimal planning techniques exist\nin general for such problems; instead we use fitted value iteration to solve\nthe learned MDP, and include the error due to approximate planning in our\nbounds. Finally, we report an experiment using a robotic car driving over\nvarying terrain to demonstrate that these dynamics representations adequately\ncapture real-world dynamics and that our algorithm can be used to efficiently\nsolve such problems.\n", "versions": [{"version": "v1", "created": "Wed, 13 Jun 2012 12:32:13 GMT"}], "update_date": "2012-06-18", "authors_parsed": [["Brunskill", "Emma", ""], ["Leffler", "Bethany", ""], ["Li", "Lihong", ""], ["Littman", "Michael L.", ""], ["Roy", "Nicholas", ""]]}, {"id": "1206.3236", "submitter": "Vincent Auvray", "authors": "Vincent Auvray, Louis Wehenkel", "title": "Learning Inclusion-Optimal Chordal Graphs", "comments": "Appears in Proceedings of the Twenty-Fourth Conference on Uncertainty\n  in Artificial Intelligence (UAI2008)", "journal-ref": null, "doi": null, "report-no": "UAI-P-2008-PG-18-25", "categories": "cs.LG cs.DS stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Chordal graphs can be used to encode dependency models that are representable\nby both directed acyclic and undirected graphs. This paper discusses a very\nsimple and efficient algorithm to learn the chordal structure of a\nprobabilistic model from data. The algorithm is a greedy hill-climbing search\nalgorithm that uses the inclusion boundary neighborhood over chordal graphs. In\nthe limit of a large sample size and under appropriate hypotheses on the\nscoring criterion, we prove that the algorithm will find a structure that is\ninclusion-optimal when the dependency model of the data-generating distribution\ncan be represented exactly by an undirected graph. The algorithm is evaluated\non simulated datasets.\n", "versions": [{"version": "v1", "created": "Wed, 13 Jun 2012 14:17:24 GMT"}], "update_date": "2012-06-18", "authors_parsed": [["Auvray", "Vincent", ""], ["Wehenkel", "Louis", ""]]}, {"id": "1206.3237", "submitter": "David Barber", "authors": "David Barber", "title": "Clique Matrices for Statistical Graph Decomposition and Parameterising\n  Restricted Positive Definite Matrices", "comments": "Appears in Proceedings of the Twenty-Fourth Conference on Uncertainty\n  in Artificial Intelligence (UAI2008)", "journal-ref": null, "doi": null, "report-no": "UAI-P-2008-PG-26-33", "categories": "cs.DM cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We introduce Clique Matrices as an alternative representation of undirected\ngraphs, being a generalisation of the incidence matrix representation. Here we\nuse clique matrices to decompose a graph into a set of possibly overlapping\nclusters, de ned as well-connected subsets of vertices. The decomposition is\nbased on a statistical description which encourages clusters to be well\nconnected and few in number. Inference is carried out using a variational\napproximation. Clique matrices also play a natural role in parameterising\npositive de nite matrices under zero constraints on elements of the matrix. We\nshow that clique matrices can parameterise all positive de nite matrices\nrestricted according to a decomposable graph and form a structured Factor\nAnalysis approximation in the non-decomposable case.\n", "versions": [{"version": "v1", "created": "Wed, 13 Jun 2012 14:17:43 GMT"}], "update_date": "2012-06-18", "authors_parsed": [["Barber", "David", ""]]}, {"id": "1206.3238", "submitter": "Liefeng Bo", "authors": "Liefeng Bo, Cristian Sminchisescu", "title": "Greedy Block Coordinate Descent for Large Scale Gaussian Process\n  Regression", "comments": "Appears in Proceedings of the Twenty-Fourth Conference on Uncertainty\n  in Artificial Intelligence (UAI2008)", "journal-ref": null, "doi": null, "report-no": "UAI-P-2008-PG-43-52", "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a variable decomposition algorithm -greedy block coordinate\ndescent (GBCD)- in order to make dense Gaussian process regression practical\nfor large scale problems. GBCD breaks a large scale optimization into a series\nof small sub-problems. The challenge in variable decomposition algorithms is\nthe identification of a subproblem (the active set of variables) that yields\nthe largest improvement. We analyze the limitations of existing methods and\ncast the active set selection into a zero-norm constrained optimization problem\nthat we solve using greedy methods. By directly estimating the decrease in the\nobjective function, we obtain not only efficient approximate solutions for\nGBCD, but we are also able to demonstrate that the method is globally\nconvergent. Empirical comparisons against competing dense methods like\nConjugate Gradient or SMO show that GBCD is an order of magnitude faster.\nComparisons against sparse GP methods show that GBCD is both accurate and\ncapable of handling datasets of 100,000 samples or more.\n", "versions": [{"version": "v1", "created": "Wed, 13 Jun 2012 14:18:22 GMT"}], "update_date": "2012-06-18", "authors_parsed": [["Bo", "Liefeng", ""], ["Sminchisescu", "Cristian", ""]]}, {"id": "1206.3241", "submitter": "Arthur Choi", "authors": "Arthur Choi, Adnan Darwiche", "title": "Approximating the Partition Function by Deleting and then Correcting for\n  Model Edges", "comments": "Appears in Proceedings of the Twenty-Fourth Conference on Uncertainty\n  in Artificial Intelligence (UAI2008)", "journal-ref": null, "doi": null, "report-no": "UAI-P-2008-PG-79-87", "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose an approach for approximating the partition function which is\nbased on two steps: (1) computing the partition function of a simplified model\nwhich is obtained by deleting model edges, and (2) rectifying the result by\napplying an edge-by-edge correction. The approach leads to an intuitive\nframework in which one can trade-off the quality of an approximation with the\ncomplexity of computing it. It also includes the Bethe free energy\napproximation as a degenerate case. We develop the approach theoretically in\nthis paper and provide a number of empirical results that reveal its practical\nutility.\n", "versions": [{"version": "v1", "created": "Wed, 13 Jun 2012 15:04:13 GMT"}], "update_date": "2012-06-18", "authors_parsed": [["Choi", "Arthur", ""], ["Darwiche", "Adnan", ""]]}, {"id": "1206.3242", "submitter": "C. Christoudias", "authors": "C. Christoudias, Raquel Urtasun, Trevor Darrell", "title": "Multi-View Learning in the Presence of View Disagreement", "comments": "Appears in Proceedings of the Twenty-Fourth Conference on Uncertainty\n  in Artificial Intelligence (UAI2008)", "journal-ref": null, "doi": null, "report-no": "UAI-P-2008-PG-88-96", "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Traditional multi-view learning approaches suffer in the presence of view\ndisagreement,i.e., when samples in each view do not belong to the same class\ndue to view corruption, occlusion or other noise processes. In this paper we\npresent a multi-view learning approach that uses a conditional entropy\ncriterion to detect view disagreement. Once detected, samples with view\ndisagreement are filtered and standard multi-view learning methods can be\nsuccessfully applied to the remaining samples. Experimental evaluation on\nsynthetic and audio-visual databases demonstrates that the detection and\nfiltering of view disagreement considerably increases the performance of\ntraditional multi-view learning approaches.\n", "versions": [{"version": "v1", "created": "Wed, 13 Jun 2012 15:04:49 GMT"}], "update_date": "2012-06-18", "authors_parsed": [["Christoudias", "C.", ""], ["Urtasun", "Raquel", ""], ["Darrell", "Trevor", ""]]}, {"id": "1206.3243", "submitter": "Botond Cseke", "authors": "Botond Cseke, Tom Heskes", "title": "Bounds on the Bethe Free Energy for Gaussian Networks", "comments": "Appears in Proceedings of the Twenty-Fourth Conference on Uncertainty\n  in Artificial Intelligence (UAI2008)", "journal-ref": null, "doi": null, "report-no": "UAI-P-2008-PG-97-104", "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We address the problem of computing approximate marginals in Gaussian\nprobabilistic models by using mean field and fractional Bethe approximations.\nAs an extension of Welling and Teh (2001), we define the Gaussian fractional\nBethe free energy in terms of the moment parameters of the approximate\nmarginals and derive an upper and lower bound for it. We give necessary\nconditions for the Gaussian fractional Bethe free energies to be bounded from\nbelow. It turns out that the bounding condition is the same as the pairwise\nnormalizability condition derived by Malioutov et al. (2006) as a sufficient\ncondition for the convergence of the message passing algorithm. By giving a\ncounterexample, we disprove the conjecture in Welling and Teh (2001): even when\nthe Bethe free energy is not bounded from below, it can possess a local minimum\nto which the minimization algorithms can converge.\n", "versions": [{"version": "v1", "created": "Wed, 13 Jun 2012 15:05:35 GMT"}], "update_date": "2012-06-18", "authors_parsed": [["Cseke", "Botond", ""], ["Heskes", "Tom", ""]]}, {"id": "1206.3247", "submitter": "Justin Domke", "authors": "Justin Domke", "title": "Learning Convex Inference of Marginals", "comments": "Appears in Proceedings of the Twenty-Fourth Conference on Uncertainty\n  in Artificial Intelligence (UAI2008)", "journal-ref": null, "doi": null, "report-no": "UAI-P-2008-PG-137-144", "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Graphical models trained using maximum likelihood are a common tool for\nprobabilistic inference of marginal distributions. However, this approach\nsuffers difficulties when either the inference process or the model is\napproximate. In this paper, the inference process is first defined to be the\nminimization of a convex function, inspired by free energy approximations.\nLearning is then done directly in terms of the performance of the inference\nprocess at univariate marginal prediction. The main novelty is that this is a\ndirect minimization of emperical risk, where the risk measures the accuracy of\npredicted marginals.\n", "versions": [{"version": "v1", "created": "Wed, 13 Jun 2012 15:09:01 GMT"}], "update_date": "2012-06-18", "authors_parsed": [["Domke", "Justin", ""]]}, {"id": "1206.3249", "submitter": "John Duchi", "authors": "John Duchi, Stephen Gould, Daphne Koller", "title": "Projected Subgradient Methods for Learning Sparse Gaussians", "comments": "Appears in Proceedings of the Twenty-Fourth Conference on Uncertainty\n  in Artificial Intelligence (UAI2008)", "journal-ref": null, "doi": null, "report-no": "UAI-P-2008-PG-153-160", "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Gaussian Markov random fields (GMRFs) are useful in a broad range of\napplications. In this paper we tackle the problem of learning a sparse GMRF in\na high-dimensional space. Our approach uses the l1-norm as a regularization on\nthe inverse covariance matrix. We utilize a novel projected gradient method,\nwhich is faster than previous methods in practice and equal to the best\nperforming of these in asymptotic complexity. We also extend the l1-regularized\nobjective to the problem of sparsifying entire blocks within the inverse\ncovariance matrix. Our methods generalize fairly easily to this case, while\nother methods do not. We demonstrate that our extensions give better\ngeneralization performance on two real domains--biological network analysis and\na 2D-shape modeling image task.\n", "versions": [{"version": "v1", "created": "Wed, 13 Jun 2012 15:09:50 GMT"}], "update_date": "2012-06-18", "authors_parsed": [["Duchi", "John", ""], ["Gould", "Stephen", ""], ["Koller", "Daphne", ""]]}, {"id": "1206.3252", "submitter": "Gal Elidan", "authors": "Gal Elidan, Ben Packer, Geremy Heitz, Daphne Koller", "title": "Convex Point Estimation using Undirected Bayesian Transfer Hierarchies", "comments": "Appears in Proceedings of the Twenty-Fourth Conference on Uncertainty\n  in Artificial Intelligence (UAI2008)", "journal-ref": null, "doi": null, "report-no": "UAI-P-2008-PG-179-187", "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  When related learning tasks are naturally arranged in a hierarchy, an\nappealing approach for coping with scarcity of instances is that of transfer\nlearning using a hierarchical Bayes framework. As fully Bayesian computations\ncan be difficult and computationally demanding, it is often desirable to use\nposterior point estimates that facilitate (relatively) efficient prediction.\nHowever, the hierarchical Bayes framework does not always lend itself naturally\nto this maximum aposteriori goal. In this work we propose an undirected\nreformulation of hierarchical Bayes that relies on priors in the form of\nsimilarity measures. We introduce the notion of \"degree of transfer\" weights on\ncomponents of these similarity measures, and show how they can be automatically\nlearned within a joint probabilistic framework. Importantly, our reformulation\nresults in a convex objective for many learning problems, thus facilitating\noptimal posterior point estimation using standard optimization techniques. In\naddition, we no longer require proper priors, allowing for flexible and\nstraightforward specification of joint distributions over transfer hierarchies.\nWe show that our framework is effective for learning models that are part of\ntransfer hierarchies for two real-life tasks: object shape modeling using\nGaussian density estimation and document classification.\n", "versions": [{"version": "v1", "created": "Wed, 13 Jun 2012 15:11:36 GMT"}], "update_date": "2012-06-18", "authors_parsed": [["Elidan", "Gal", ""], ["Packer", "Ben", ""], ["Heitz", "Geremy", ""], ["Koller", "Daphne", ""]]}, {"id": "1206.3254", "submitter": "Amit Gruber", "authors": "Amit Gruber, Michal Rosen-Zvi, Yair Weiss", "title": "Latent Topic Models for Hypertext", "comments": "Appears in Proceedings of the Twenty-Fourth Conference on Uncertainty\n  in Artificial Intelligence (UAI2008)", "journal-ref": null, "doi": null, "report-no": "UAI-P-2008-PG-230-239", "categories": "cs.IR cs.CL cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Latent topic models have been successfully applied as an unsupervised topic\ndiscovery technique in large document collections. With the proliferation of\nhypertext document collection such as the Internet, there has also been great\ninterest in extending these approaches to hypertext [6, 9]. These approaches\ntypically model links in an analogous fashion to how they model words - the\ndocument-link co-occurrence matrix is modeled in the same way that the\ndocument-word co-occurrence matrix is modeled in standard topic models. In this\npaper we present a probabilistic generative model for hypertext document\ncollections that explicitly models the generation of links. Specifically, links\nfrom a word w to a document d depend directly on how frequent the topic of w is\nin d, in addition to the in-degree of d. We show how to perform EM learning on\nthis model efficiently. By not modeling links as analogous to words, we end up\nusing far fewer free parameters and obtain better link prediction results.\n", "versions": [{"version": "v1", "created": "Wed, 13 Jun 2012 15:30:14 GMT"}], "update_date": "2012-06-18", "authors_parsed": [["Gruber", "Amit", ""], ["Rosen-Zvi", "Michal", ""], ["Weiss", "Yair", ""]]}, {"id": "1206.3256", "submitter": "Kuzman Ganchev", "authors": "Kuzman Ganchev, Joao Graca, John Blitzer, Ben Taskar", "title": "Multi-View Learning over Structured and Non-Identical Outputs", "comments": "Appears in Proceedings of the Twenty-Fourth Conference on Uncertainty\n  in Artificial Intelligence (UAI2008)", "journal-ref": null, "doi": null, "report-no": "UAI-P-2008-PG-204-211", "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In many machine learning problems, labeled training data is limited but\nunlabeled data is ample. Some of these problems have instances that can be\nfactored into multiple views, each of which is nearly sufficent in determining\nthe correct labels. In this paper we present a new algorithm for probabilistic\nmulti-view learning which uses the idea of stochastic agreement between views\nas regularization. Our algorithm works on structured and unstructured problems\nand easily generalizes to partial agreement scenarios. For the full agreement\ncase, our algorithm minimizes the Bhattacharyya distance between the models of\neach view, and performs better than CoBoosting and two-view Perceptron on\nseveral flat and structured classification problems.\n", "versions": [{"version": "v1", "created": "Wed, 13 Jun 2012 15:31:21 GMT"}], "update_date": "2012-06-18", "authors_parsed": [["Ganchev", "Kuzman", ""], ["Graca", "Joao", ""], ["Blitzer", "John", ""], ["Taskar", "Ben", ""]]}, {"id": "1206.3257", "submitter": "Varun Ganapathi", "authors": "Varun Ganapathi, David Vickrey, John Duchi, Daphne Koller", "title": "Constrained Approximate Maximum Entropy Learning of Markov Random Fields", "comments": "Appears in Proceedings of the Twenty-Fourth Conference on Uncertainty\n  in Artificial Intelligence (UAI2008)", "journal-ref": null, "doi": null, "report-no": "UAI-P-2008-PG-196-203", "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Parameter estimation in Markov random fields (MRFs) is a difficult task, in\nwhich inference over the network is run in the inner loop of a gradient descent\nprocedure. Replacing exact inference with approximate methods such as loopy\nbelief propagation (LBP) can suffer from poor convergence. In this paper, we\nprovide a different approach for combining MRF learning and Bethe\napproximation. We consider the dual of maximum likelihood Markov network\nlearning - maximizing entropy with moment matching constraints - and then\napproximate both the objective and the constraints in the resulting\noptimization problem. Unlike previous work along these lines (Teh & Welling,\n2003), our formulation allows parameter sharing between features in a general\nlog-linear model, parameter regularization and conditional training. We show\nthat piecewise training (Sutton & McCallum, 2005) is a very restricted special\ncase of this formulation. We study two optimization strategies: one based on a\nsingle convex approximation and one that uses repeated convex approximations.\nWe show results on several real-world networks that demonstrate that these\nalgorithms can significantly outperform learning with loopy and piecewise. Our\nresults also provide a framework for analyzing the trade-offs of different\nrelaxations of the entropy objective and of the constraints.\n", "versions": [{"version": "v1", "created": "Wed, 13 Jun 2012 15:31:57 GMT"}], "update_date": "2012-06-18", "authors_parsed": [["Ganapathi", "Varun", ""], ["Vickrey", "David", ""], ["Duchi", "John", ""], ["Koller", "Daphne", ""]]}, {"id": "1206.3259", "submitter": "Jim Huang", "authors": "Jim Huang, Brendan J. Frey", "title": "Cumulative distribution networks and the derivative-sum-product\n  algorithm", "comments": "Appears in Proceedings of the Twenty-Fourth Conference on Uncertainty\n  in Artificial Intelligence (UAI2008)", "journal-ref": null, "doi": null, "report-no": "UAI-P-2008-PG-290-297", "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We introduce a new type of graphical model called a \"cumulative distribution\nnetwork\" (CDN), which expresses a joint cumulative distribution as a product of\nlocal functions. Each local function can be viewed as providing evidence about\npossible orderings, or rankings, of variables. Interestingly, we find that the\nconditional independence properties of CDNs are quite different from other\ngraphical models. We also describe a messagepassing algorithm that efficiently\ncomputes conditional cumulative distributions. Due to the unique independence\nproperties of the CDN, these messages do not in general have a one-to-one\ncorrespondence with messages exchanged in standard algorithms, such as belief\npropagation. We demonstrate the application of CDNs for structured ranking\nlearning using a previously-studied multi-player gaming dataset.\n", "versions": [{"version": "v1", "created": "Wed, 13 Jun 2012 15:33:06 GMT"}], "update_date": "2012-06-18", "authors_parsed": [["Huang", "Jim", ""], ["Frey", "Brendan J.", ""]]}, {"id": "1206.3260", "submitter": "Patrik O. Hoyer", "authors": "Patrik O. Hoyer, Aapo Hyvarinen, Richard Scheines, Peter L. Spirtes,\n  Joseph Ramsey, Gustavo Lacerda, Shohei Shimizu", "title": "Causal discovery of linear acyclic models with arbitrary distributions", "comments": "Appears in Proceedings of the Twenty-Fourth Conference on Uncertainty\n  in Artificial Intelligence (UAI2008)", "journal-ref": null, "doi": null, "report-no": "UAI-P-2008-PG-282-289", "categories": "stat.ML cs.AI cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  An important task in data analysis is the discovery of causal relationships\nbetween observed variables. For continuous-valued data, linear acyclic causal\nmodels are commonly used to model the data-generating process, and the\ninference of such models is a well-studied problem. However, existing methods\nhave significant limitations. Methods based on conditional independencies\n(Spirtes et al. 1993; Pearl 2000) cannot distinguish between\nindependence-equivalent models, whereas approaches purely based on Independent\nComponent Analysis (Shimizu et al. 2006) are inapplicable to data which is\npartially Gaussian. In this paper, we generalize and combine the two\napproaches, to yield a method able to learn the model structure in many cases\nfor which the previous methods provide answers that are either incorrect or are\nnot as informative as possible. We give exact graphical conditions for when two\ndistinct models represent the same family of distributions, and empirically\ndemonstrate the power of our method through thorough simulations.\n", "versions": [{"version": "v1", "created": "Wed, 13 Jun 2012 15:33:32 GMT"}], "update_date": "2012-06-18", "authors_parsed": [["Hoyer", "Patrik O.", ""], ["Hyvarinen", "Aapo", ""], ["Scheines", "Richard", ""], ["Spirtes", "Peter L.", ""], ["Ramsey", "Joseph", ""], ["Lacerda", "Gustavo", ""], ["Shimizu", "Shohei", ""]]}, {"id": "1206.3262", "submitter": "Tamir Hazan", "authors": "Tamir Hazan, Amnon Shashua", "title": "Convergent Message-Passing Algorithms for Inference over General Graphs\n  with Convex Free Energies", "comments": "Appears in Proceedings of the Twenty-Fourth Conference on Uncertainty\n  in Artificial Intelligence (UAI2008)", "journal-ref": null, "doi": null, "report-no": "UAI-P-2008-PG-264-273", "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Inference problems in graphical models can be represented as a constrained\noptimization of a free energy function. It is known that when the Bethe free\nenergy is used, the fixedpoints of the belief propagation (BP) algorithm\ncorrespond to the local minima of the free energy. However BP fails to converge\nin many cases of interest. Moreover, the Bethe free energy is non-convex for\ngraphical models with cycles thus introducing great difficulty in deriving\nefficient algorithms for finding local minima of the free energy for general\ngraphs. In this paper we introduce two efficient BP-like algorithms, one\nsequential and the other parallel, that are guaranteed to converge to the\nglobal minimum, for any graph, over the class of energies known as \"convex free\nenergies\". In addition, we propose an efficient heuristic for setting the\nparameters of the convex free energy based on the structure of the graph.\n", "versions": [{"version": "v1", "created": "Wed, 13 Jun 2012 15:34:21 GMT"}], "update_date": "2012-06-18", "authors_parsed": [["Hazan", "Tamir", ""], ["Shashua", "Amnon", ""]]}, {"id": "1206.3269", "submitter": "Tony S. Jebara", "authors": "Tony S. Jebara", "title": "Bayesian Out-Trees", "comments": "Appears in Proceedings of the Twenty-Fourth Conference on Uncertainty\n  in Artificial Intelligence (UAI2008)", "journal-ref": null, "doi": null, "report-no": "UAI-P-2008-PG-315-324", "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A Bayesian treatment of latent directed graph structure for non-iid data is\nprovided where each child datum is sampled with a directed conditional\ndependence on a single unknown parent datum. The latent graph structure is\nassumed to lie in the family of directed out-tree graphs which leads to\nefficient Bayesian inference. The latent likelihood of the data and its\ngradients are computable in closed form via Tutte's directed matrix tree\ntheorem using determinants and inverses of the out-Laplacian. This novel\nlikelihood subsumes iid likelihood, is exchangeable and yields efficient\nunsupervised and semi-supervised learning algorithms. In addition to handling\ntaxonomy and phylogenetic datasets the out-tree assumption performs\nsurprisingly well as a semi-parametric density estimator on standard iid\ndatasets. Experiments with unsupervised and semisupervised learning are shown\non various UCI and taxonomy datasets.\n", "versions": [{"version": "v1", "created": "Wed, 13 Jun 2012 15:37:30 GMT"}], "update_date": "2012-06-18", "authors_parsed": [["Jebara", "Tony S.", ""]]}, {"id": "1206.3270", "submitter": "Marina Meila", "authors": "Marina Meila, Le Bao", "title": "Estimation and Clustering with Infinite Rankings", "comments": "Appears in Proceedings of the Twenty-Fourth Conference on Uncertainty\n  in Artificial Intelligence (UAI2008)", "journal-ref": null, "doi": null, "report-no": "UAI-P-2008-PG-393-402", "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper presents a natural extension of stagewise ranking to the the case\nof infinitely many items. We introduce the infinite generalized Mallows model\n(IGM), describe its properties and give procedures to estimate it from data.\nFor estimation of multimodal distributions we introduce the\nExponential-Blurring-Mean-Shift nonparametric clustering algorithm. The\nexperiments highlight the properties of the new model and demonstrate that\ninfinite models can be simple, elegant and practical.\n", "versions": [{"version": "v1", "created": "Wed, 13 Jun 2012 15:38:07 GMT"}], "update_date": "2012-06-18", "authors_parsed": [["Meila", "Marina", ""], ["Bao", "Le", ""]]}, {"id": "1206.3274", "submitter": "Eric B. Laber", "authors": "Eric B. Laber, Susan A. Murphy", "title": "Small Sample Inference for Generalization Error in Classification Using\n  the CUD Bound", "comments": "Appears in Proceedings of the Twenty-Fourth Conference on Uncertainty\n  in Artificial Intelligence (UAI2008)", "journal-ref": null, "doi": null, "report-no": "UAI-P-2008-PG-357-365", "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Confidence measures for the generalization error are crucial when small\ntraining samples are used to construct classifiers. A common approach is to\nestimate the generalization error by resampling and then assume the resampled\nestimator follows a known distribution to form a confidence set [Kohavi 1995,\nMartin 1996,Yang 2006]. Alternatively, one might bootstrap the resampled\nestimator of the generalization error to form a confidence set. Unfortunately,\nthese methods do not reliably provide sets of the desired confidence. The poor\nperformance appears to be due to the lack of smoothness of the generalization\nerror as a function of the learned classifier. This results in a non-normal\ndistribution of the estimated generalization error. We construct a confidence\nset for the generalization error by use of a smooth upper bound on the\ndeviation between the resampled estimate and generalization error. The\nconfidence set is formed by bootstrapping this upper bound. In cases in which\nthe approximation class for the classifier can be represented as a parametric\nadditive model, we provide a computationally efficient algorithm. This method\nexhibits superior performance across a series of test and simulated data sets.\n", "versions": [{"version": "v1", "created": "Wed, 13 Jun 2012 15:39:51 GMT"}], "update_date": "2012-06-18", "authors_parsed": [["Laber", "Eric B.", ""], ["Murphy", "Susan A.", ""]]}, {"id": "1206.3279", "submitter": "Kurt T. Miller", "authors": "Kurt T. Miller, Thomas Griffiths, Michael I. Jordan", "title": "The Phylogenetic Indian Buffet Process: A Non-Exchangeable Nonparametric\n  Prior for Latent Features", "comments": "Appears in Proceedings of the Twenty-Fourth Conference on Uncertainty\n  in Artificial Intelligence (UAI2008)", "journal-ref": null, "doi": null, "report-no": "UAI-P-2008-PG-403-410", "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Nonparametric Bayesian models are often based on the assumption that the\nobjects being modeled are exchangeable. While appropriate in some applications\n(e.g., bag-of-words models for documents), exchangeability is sometimes assumed\nsimply for computational reasons; non-exchangeable models might be a better\nchoice for applications based on subject matter. Drawing on ideas from\ngraphical models and phylogenetics, we describe a non-exchangeable prior for a\nclass of nonparametric latent feature models that is nearly as efficient\ncomputationally as its exchangeable counterpart. Our model is applicable to the\ngeneral setting in which the dependencies between objects can be expressed\nusing a tree, where edge lengths indicate the strength of relationships. We\ndemonstrate an application to modeling probabilistic choice.\n", "versions": [{"version": "v1", "created": "Wed, 13 Jun 2012 15:42:35 GMT"}], "update_date": "2012-06-18", "authors_parsed": [["Miller", "Kurt T.", ""], ["Griffiths", "Thomas", ""], ["Jordan", "Michael I.", ""]]}, {"id": "1206.3287", "submitter": "Harald Steck", "authors": "Harald Steck", "title": "Learning the Bayesian Network Structure: Dirichlet Prior versus Data", "comments": "Appears in Proceedings of the Twenty-Fourth Conference on Uncertainty\n  in Artificial Intelligence (UAI2008)", "journal-ref": null, "doi": null, "report-no": "UAI-P-2008-PG-511-518", "categories": "cs.LG stat.ME stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In the Bayesian approach to structure learning of graphical models, the\nequivalent sample size (ESS) in the Dirichlet prior over the model parameters\nwas recently shown to have an important effect on the maximum-a-posteriori\nestimate of the Bayesian network structure. In our first contribution, we\ntheoretically analyze the case of large ESS-values, which complements previous\nwork: among other results, we find that the presence of an edge in a Bayesian\nnetwork is favoured over its absence even if both the Dirichlet prior and the\ndata imply independence, as long as the conditional empirical distribution is\nnotably different from uniform. In our second contribution, we focus on\nrealistic ESS-values, and provide an analytical approximation to the \"optimal\"\nESS-value in a predictive sense (its accuracy is also validated\nexperimentally): this approximation provides an understanding as to which\nproperties of the data have the main effect determining the \"optimal\"\nESS-value.\n", "versions": [{"version": "v1", "created": "Wed, 13 Jun 2012 15:45:39 GMT"}], "update_date": "2012-06-18", "authors_parsed": [["Steck", "Harald", ""]]}, {"id": "1206.3290", "submitter": "Jarno Vanhatalo", "authors": "Jarno Vanhatalo, Aki Vehtari", "title": "Modelling local and global phenomena with sparse Gaussian processes", "comments": "Appears in Proceedings of the Twenty-Fourth Conference on Uncertainty\n  in Artificial Intelligence (UAI2008)", "journal-ref": null, "doi": null, "report-no": "UAI-P-2008-PG-571-578", "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Much recent work has concerned sparse approximations to speed up the Gaussian\nprocess regression from the unfavorable O(n3) scaling in computational time to\nO(nm2). Thus far, work has concentrated on models with one covariance function.\nHowever, in many practical situations additive models with multiple covariance\nfunctions may perform better, since the data may contain both long and short\nlength-scale phenomena. The long length-scales can be captured with global\nsparse approximations, such as fully independent conditional (FIC), and the\nshort length-scales can be modeled naturally by covariance functions with\ncompact support (CS). CS covariance functions lead to naturally sparse\ncovariance matrices, which are computationally cheaper to handle than full\ncovariance matrices. In this paper, we propose a new sparse Gaussian process\nmodel with two additive components: FIC for the long length-scales and CS\ncovariance function for the short length-scales. We give theoretical and\nexperimental results and show that under certain conditions the proposed model\nhas the same computational complexity as FIC. We also compare the model\nperformance of the proposed model to additive models approximated by fully and\npartially independent conditional (PIC). We use real data sets and show that\nour model outperforms FIC and PIC approximations for data sets with two\nadditive phenomena.\n", "versions": [{"version": "v1", "created": "Wed, 13 Jun 2012 15:47:16 GMT"}], "update_date": "2012-06-18", "authors_parsed": [["Vanhatalo", "Jarno", ""], ["Vehtari", "Aki", ""]]}, {"id": "1206.3294", "submitter": "Daniel Tarlow", "authors": "Daniel Tarlow, Richard S. Zemel, Brendan J. Frey", "title": "Flexible Priors for Exemplar-based Clustering", "comments": "Appears in Proceedings of the Twenty-Fourth Conference on Uncertainty\n  in Artificial Intelligence (UAI2008)", "journal-ref": null, "doi": null, "report-no": "UAI-P-2008-PG-537-545", "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Exemplar-based clustering methods have been shown to produce state-of-the-art\nresults on a number of synthetic and real-world clustering problems. They are\nappealing because they offer computational benefits over latent-mean models and\ncan handle arbitrary pairwise similarity measures between data points. However,\nwhen trying to recover underlying structure in clustering problems, tailored\nsimilarity measures are often not enough; we also desire control over the\ndistribution of cluster sizes. Priors such as Dirichlet process priors allow\nthe number of clusters to be unspecified while expressing priors over data\npartitions. To our knowledge, they have not been applied to exemplar-based\nmodels. We show how to incorporate priors, including Dirichlet process priors,\ninto the recently introduced affinity propagation algorithm. We develop an\nefficient maxproduct belief propagation algorithm for our new model and\ndemonstrate experimentally how the expanded range of clustering priors allows\nus to better recover true clusterings in situations where we have some\ninformation about the generating process.\n", "versions": [{"version": "v1", "created": "Wed, 13 Jun 2012 15:52:35 GMT"}], "update_date": "2012-06-18", "authors_parsed": [["Tarlow", "Daniel", ""], ["Zemel", "Richard S.", ""], ["Frey", "Brendan J.", ""]]}, {"id": "1206.3297", "submitter": "Max Welling", "authors": "Max Welling, Yee Whye Teh, Hilbert Kappen", "title": "Hybrid Variational/Gibbs Collapsed Inference in Topic Models", "comments": "Appears in Proceedings of the Twenty-Fourth Conference on Uncertainty\n  in Artificial Intelligence (UAI2008)", "journal-ref": null, "doi": null, "report-no": "UAI-P-2008-PG-587-594", "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Variational Bayesian inference and (collapsed) Gibbs sampling are the two\nimportant classes of inference algorithms for Bayesian networks. Both have\ntheir advantages and disadvantages: collapsed Gibbs sampling is unbiased but is\nalso inefficient for large count values and requires averaging over many\nsamples to reduce variance. On the other hand, variational Bayesian inference\nis efficient and accurate for large count values but suffers from bias for\nsmall counts. We propose a hybrid algorithm that combines the best of both\nworlds: it samples very small counts and applies variational updates to large\ncounts. This hybridization is shown to significantly improve testset perplexity\nrelative to variational inference at no computational cost.\n", "versions": [{"version": "v1", "created": "Wed, 13 Jun 2012 15:56:12 GMT"}], "update_date": "2012-06-18", "authors_parsed": [["Welling", "Max", ""], ["Teh", "Yee Whye", ""], ["Kappen", "Hilbert", ""]]}, {"id": "1206.3298", "submitter": "Chong Wang", "authors": "Chong Wang, David Blei, David Heckerman", "title": "Continuous Time Dynamic Topic Models", "comments": "Appears in Proceedings of the Twenty-Fourth Conference on Uncertainty\n  in Artificial Intelligence (UAI2008)", "journal-ref": null, "doi": null, "report-no": "UAI-P-2008-PG-579-586", "categories": "cs.IR cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we develop the continuous time dynamic topic model (cDTM). The\ncDTM is a dynamic topic model that uses Brownian motion to model the latent\ntopics through a sequential collection of documents, where a \"topic\" is a\npattern of word use that we expect to evolve over the course of the collection.\nWe derive an efficient variational approximate inference algorithm that takes\nadvantage of the sparsity of observations in text, a property that lets us\neasily handle many time points. In contrast to the cDTM, the original\ndiscrete-time dynamic topic model (dDTM) requires that time be discretized.\nMoreover, the complexity of variational inference for the dDTM grows quickly as\ntime granularity increases, a drawback which limits fine-grained\ndiscretization. We demonstrate the cDTM on two news corpora, reporting both\npredictive perplexity and the novel task of time stamp prediction.\n", "versions": [{"version": "v1", "created": "Wed, 13 Jun 2012 15:56:33 GMT"}, {"version": "v2", "created": "Sat, 16 May 2015 22:57:04 GMT"}], "update_date": "2015-05-19", "authors_parsed": [["Wang", "Chong", ""], ["Blei", "David", ""], ["Heckerman", "David", ""]]}, {"id": "1206.3381", "submitter": "Tilmann Gneiting", "authors": "Tilmann Gneiting", "title": "On the Cover-Hart Inequality: What's a Sample of Size One Worth?", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST cs.IT math.IT stat.ML stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Bob predicts a future observation based on a sample of size one. Alice can\ndraw a sample of any size before issuing her prediction. How much better can\nshe do than Bob? Perhaps surprisingly, under a large class of loss functions,\nwhich we refer to as the Cover-Hart family, the best Alice can do is to halve\nBob's risk. In this sense, half the information in an infinite sample is\ncontained in a sample of size one. The Cover-Hart family is a convex cone that\nincludes metrics and negative definite functions, subject to slight regularity\nconditions. These results may help explain the small relative differences in\nempirical performance measures in applied classification and forecasting\nproblems, as well as the success of reasoning and learning by analogy in\ngeneral, and nearest neighbor techniques in particular.\n", "versions": [{"version": "v1", "created": "Fri, 15 Jun 2012 07:19:07 GMT"}], "update_date": "2012-06-18", "authors_parsed": [["Gneiting", "Tilmann", ""]]}, {"id": "1206.3493", "submitter": "Zhilin Zhang", "authors": "Zhilin Zhang, Tzyy-Ping Jung, Scott Makeig, Bhaskar D. Rao", "title": "Compressed Sensing of EEG for Wireless Telemonitoring with Low Energy\n  Consumption and Inexpensive Hardware", "comments": "Matlab codes can be downloaded at:\n  http://dsp.ucsd.edu/~zhilin/BSBL.html, or\n  http://sites.google.com/site/researchbyzhang/bsbl", "journal-ref": null, "doi": "10.1109/TBME.2012.2217959", "report-no": null, "categories": "stat.AP cs.IT math.IT stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Telemonitoring of electroencephalogram (EEG) through wireless body-area\nnetworks is an evolving direction in personalized medicine. Among various\nconstraints in designing such a system, three important constraints are energy\nconsumption, data compression, and device cost. Conventional data compression\nmethodologies, although effective in data compression, consumes significant\nenergy and cannot reduce device cost. Compressed sensing (CS), as an emerging\ndata compression methodology, is promising in catering to these constraints.\nHowever, EEG is non-sparse in the time domain and also non-sparse in\ntransformed domains (such as the wavelet domain). Therefore, it is extremely\ndifficult for current CS algorithms to recover EEG with the quality that\nsatisfies the requirements of clinical diagnosis and engineering applications.\nRecently, Block Sparse Bayesian Learning (BSBL) was proposed as a new method to\nthe CS problem. This study introduces the technique to the telemonitoring of\nEEG. Experimental results show that its recovery quality is better than\nstate-of-the-art CS algorithms, and sufficient for practical use. These results\nsuggest that BSBL is very promising for telemonitoring of EEG and other\nnon-sparse physiological signals.\n", "versions": [{"version": "v1", "created": "Wed, 13 Jun 2012 19:47:23 GMT"}, {"version": "v2", "created": "Wed, 5 Sep 2012 12:42:26 GMT"}, {"version": "v3", "created": "Sun, 2 Nov 2014 05:40:56 GMT"}], "update_date": "2014-11-04", "authors_parsed": [["Zhang", "Zhilin", ""], ["Jung", "Tzyy-Ping", ""], ["Makeig", "Scott", ""], ["Rao", "Bhaskar D.", ""]]}, {"id": "1206.3713", "submitter": "Jean Honorio", "authors": "Jean Honorio and Luis Ortiz", "title": "Learning the Structure and Parameters of Large-Population Graphical\n  Games from Behavioral Data", "comments": "Journal of Machine Learning Research. (accepted, pending\n  publication.) Last conference version: submitted March 30, 2012 to UAI 2012.\n  First conference version: entitled, Learning Influence Games, initially\n  submitted on June 1, 2010 to NIPS 2010", "journal-ref": "Journal of Machine Learning Research (JMLR), 16(Jul):1249--1274,\n  2015", "doi": null, "report-no": null, "categories": "cs.LG cs.GT stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider learning, from strictly behavioral data, the structure and\nparameters of linear influence games (LIGs), a class of parametric graphical\ngames introduced by Irfan and Ortiz (2014). LIGs facilitate causal strategic\ninference (CSI): Making inferences from causal interventions on stable behavior\nin strategic settings. Applications include the identification of the most\ninfluential individuals in large (social) networks. Such tasks can also support\npolicy-making analysis. Motivated by the computational work on LIGs, we cast\nthe learning problem as maximum-likelihood estimation (MLE) of a generative\nmodel defined by pure-strategy Nash equilibria (PSNE). Our simple formulation\nuncovers the fundamental interplay between goodness-of-fit and model\ncomplexity: good models capture equilibrium behavior within the data while\ncontrolling the true number of equilibria, including those unobserved. We\nprovide a generalization bound establishing the sample complexity for MLE in\nour framework. We propose several algorithms including convex loss minimization\n(CLM) and sigmoidal approximations. We prove that the number of exact PSNE in\nLIGs is small, with high probability; thus, CLM is sound. We illustrate our\napproach on synthetic data and real-world U.S. congressional voting records. We\nbriefly discuss our learning framework's generality and potential applicability\nto general graphical games.\n", "versions": [{"version": "v1", "created": "Sat, 16 Jun 2012 23:20:09 GMT"}, {"version": "v2", "created": "Fri, 25 Oct 2013 18:55:36 GMT"}, {"version": "v3", "created": "Fri, 24 Oct 2014 20:47:46 GMT"}, {"version": "v4", "created": "Mon, 4 May 2015 02:04:26 GMT"}], "update_date": "2016-02-22", "authors_parsed": [["Honorio", "Jean", ""], ["Ortiz", "Luis", ""]]}, {"id": "1206.3721", "submitter": "Kazuya Takabatake", "authors": "Kazuya Takabatake and Shotaro Akaho", "title": "Constraint-free Graphical Model with Fast Learning Algorithm", "comments": "9 pages, 11 figures, submitted to UAI2012", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we propose a simple, versatile model for learning the\nstructure and parameters of multivariate distributions from a data set.\nLearning a Markov network from a given data set is not a simple problem,\nbecause Markov networks rigorously represent Markov properties, and this rigor\nimposes complex constraints on the design of the networks. Our proposed model\nremoves these constraints, acquiring important aspects from the information\ngeometry. The proposed parameter- and structure-learning algorithms are simple\nto execute as they are based solely on local computation at each node.\nExperiments demonstrate that our algorithms work appropriately.\n", "versions": [{"version": "v1", "created": "Sun, 17 Jun 2012 04:40:09 GMT"}], "update_date": "2012-06-19", "authors_parsed": [["Takabatake", "Kazuya", ""], ["Akaho", "Shotaro", ""]]}, {"id": "1206.3881", "submitter": "Alessandro Rozza", "authors": "Claudio Ceruti and Simone Bassis and Alessandro Rozza and Gabriele\n  Lombardi and Elena Casiraghi and Paola Campadelli", "title": "DANCo: Dimensionality from Angle and Norm Concentration", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In the last decades the estimation of the intrinsic dimensionality of a\ndataset has gained considerable importance. Despite the great deal of research\nwork devoted to this task, most of the proposed solutions prove to be\nunreliable when the intrinsic dimensionality of the input dataset is high and\nthe manifold where the points lie is nonlinearly embedded in a higher\ndimensional space. In this paper we propose a novel robust intrinsic\ndimensionality estimator that exploits the twofold complementary information\nconveyed both by the normalized nearest neighbor distances and by the angles\ncomputed on couples of neighboring points, providing also closed-forms for the\nKullback-Leibler divergences of the respective distributions. Experiments\nperformed on both synthetic and real datasets highlight the robustness and the\neffectiveness of the proposed algorithm when compared to state of the art\nmethodologies.\n", "versions": [{"version": "v1", "created": "Mon, 18 Jun 2012 10:33:29 GMT"}], "update_date": "2012-06-19", "authors_parsed": [["Ceruti", "Claudio", ""], ["Bassis", "Simone", ""], ["Rozza", "Alessandro", ""], ["Lombardi", "Gabriele", ""], ["Casiraghi", "Elena", ""], ["Campadelli", "Paola", ""]]}, {"id": "1206.4074", "submitter": "Fuxin Li", "authors": "Fuxin Li, Guy Lebanon, Cristian Sminchisescu", "title": "A Linear Approximation to the chi^2 Kernel with Geometric Convergence", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a new analytical approximation to the $\\chi^2$ kernel that\nconverges geometrically. The analytical approximation is derived with\nelementary methods and adapts to the input distribution for optimal convergence\nrate. Experiments show the new approximation leads to improved performance in\nimage classification and semantic segmentation tasks using a random Fourier\nfeature approximation of the $\\exp-\\chi^2$ kernel. Besides, out-of-core\nprincipal component analysis (PCA) methods are introduced to reduce the\ndimensionality of the approximation and achieve better performance at the\nexpense of only an additional constant factor to the time complexity. Moreover,\nwhen PCA is performed jointly on the training and unlabeled testing data,\nfurther performance improvements can be obtained. Experiments conducted on the\nPASCAL VOC 2010 segmentation and the ImageNet ILSVRC 2010 datasets show\nstatistically significant improvements over alternative approximation methods.\n", "versions": [{"version": "v1", "created": "Mon, 18 Jun 2012 21:05:16 GMT"}, {"version": "v2", "created": "Thu, 18 Apr 2013 18:38:28 GMT"}, {"version": "v3", "created": "Wed, 12 Jun 2013 19:29:18 GMT"}], "update_date": "2015-03-20", "authors_parsed": [["Li", "Fuxin", ""], ["Lebanon", "Guy", ""], ["Sminchisescu", "Cristian", ""]]}, {"id": "1206.4116", "submitter": "Makoto Yamada", "authors": "Makoto Yamada, Leonid Sigal, Michalis Raptis, Masashi Sugiyama", "title": "Dependence Maximizing Temporal Alignment via Squared-Loss Mutual\n  Information", "comments": "11 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The goal of temporal alignment is to establish time correspondence between\ntwo sequences, which has many applications in a variety of areas such as speech\nprocessing, bioinformatics, computer vision, and computer graphics. In this\npaper, we propose a novel temporal alignment method called least-squares\ndynamic time warping (LSDTW). LSDTW finds an alignment that maximizes\nstatistical dependency between sequences, measured by a squared-loss variant of\nmutual information. The benefit of this novel information-theoretic formulation\nis that LSDTW can align sequences with different lengths, different\ndimensionality, high non-linearity, and non-Gaussianity in a computationally\nefficient manner. In addition, model parameters such as an initial alignment\nmatrix can be systematically optimized by cross-validation. We demonstrate the\nusefulness of LSDTW through experiments on synthetic and real-world Kinect\naction recognition datasets.\n", "versions": [{"version": "v1", "created": "Tue, 19 Jun 2012 03:35:52 GMT"}], "update_date": "2012-06-20", "authors_parsed": [["Yamada", "Makoto", ""], ["Sigal", "Leonid", ""], ["Raptis", "Michalis", ""], ["Sugiyama", "Masashi", ""]]}, {"id": "1206.4560", "submitter": "Alfredo Kalaitzis", "authors": "Alfredo Kalaitzis (University of Sheffield), Neil Lawrence (University\n  of Sheffield)", "title": "Residual Component Analysis: Generalising PCA for more flexible\n  inference in linear-Gaussian models", "comments": "ICML2012", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Probabilistic principal component analysis (PPCA) seeks a low dimensional\nrepresentation of a data set in the presence of independent spherical Gaussian\nnoise. The maximum likelihood solution for the model is an eigenvalue problem\non the sample covariance matrix. In this paper we consider the situation where\nthe data variance is already partially explained by other actors, for example\nsparse conditional dependencies between the covariates, or temporal\ncorrelations leaving some residual variance. We decompose the residual variance\ninto its components through a generalised eigenvalue problem, which we call\nresidual component analysis (RCA). We explore a range of new algorithms that\narise from the framework, including one that factorises the covariance of a\nGaussian density into a low-rank and a sparse-inverse component. We illustrate\nthe ideas on the recovery of a protein-signaling network, a gene expression\ntime-series data set and the recovery of the human skeleton from motion capture\n3-D cloud data.\n", "versions": [{"version": "v1", "created": "Mon, 18 Jun 2012 15:11:22 GMT"}], "update_date": "2012-06-22", "authors_parsed": [["Kalaitzis", "Alfredo", "", "University of Sheffield"], ["Lawrence", "Neil", "", "University\n  of Sheffield"]]}, {"id": "1206.4599", "submitter": "Akiko Takeda", "authors": "Akiko Takeda (Keio University), Hiroyuki Mitsugi (Keio University),\n  Takafumi Kanamori (Nagoya University)", "title": "A Unified Robust Classification Model", "comments": "ICML2012", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A wide variety of machine learning algorithms such as support vector machine\n(SVM), minimax probability machine (MPM), and Fisher discriminant analysis\n(FDA), exist for binary classification. The purpose of this paper is to provide\na unified classification model that includes the above models through a robust\noptimization approach. This unified model has several benefits. One is that the\nextensions and improvements intended for SVM become applicable to MPM and FDA,\nand vice versa. Another benefit is to provide theoretical results to above\nlearning methods at once by dealing with the unified model. We give a\nstatistical interpretation of the unified classification model and propose a\nnon-convex optimization algorithm that can be applied to non-convex variants of\nexisting learning methods.\n", "versions": [{"version": "v1", "created": "Mon, 18 Jun 2012 14:39:39 GMT"}], "update_date": "2012-06-22", "authors_parsed": [["Takeda", "Akiko", "", "Keio University"], ["Mitsugi", "Hiroyuki", "", "Keio University"], ["Kanamori", "Takafumi", "", "Nagoya University"]]}, {"id": "1206.4600", "submitter": "Murat Dundar", "authors": "Murat Dundar (IUPUI), Ferit Akova (IUPUI), Alan Qi (Purdue), Bartek\n  Rajwa (Purdue)", "title": "Bayesian Nonexhaustive Learning for Online Discovery and Modeling of\n  Emerging Classes", "comments": "ICML2012", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a framework for online inference in the presence of a\nnonexhaustively defined set of classes that incorporates supervised\nclassification with class discovery and modeling. A Dirichlet process prior\n(DPP) model defined over class distributions ensures that both known and\nunknown class distributions originate according to a common base distribution.\nIn an attempt to automatically discover potentially interesting class\nformations, the prior model is coupled with a suitably chosen data model, and\nsequential Monte Carlo sampling is used to perform online inference. Our\nresearch is driven by a biodetection application, where a new class of pathogen\nmay suddenly appear, and the rapid increase in the number of samples\noriginating from this class indicates the onset of an outbreak.\n", "versions": [{"version": "v1", "created": "Mon, 18 Jun 2012 14:40:38 GMT"}], "update_date": "2012-06-22", "authors_parsed": [["Dundar", "Murat", "", "IUPUI"], ["Akova", "Ferit", "", "IUPUI"], ["Qi", "Alan", "", "Purdue"], ["Rajwa", "Bartek", "", "Purdue"]]}, {"id": "1206.4601", "submitter": "Wenliang Zhong", "authors": "Wenliang Zhong (HKUST), James Kwok (HKUST)", "title": "Convex Multitask Learning with Flexible Task Clusters", "comments": "ICML2012", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Traditionally, multitask learning (MTL) assumes that all the tasks are\nrelated. This can lead to negative transfer when tasks are indeed incoherent.\nRecently, a number of approaches have been proposed that alleviate this problem\nby discovering the underlying task clusters or relationships. However, they are\nlimited to modeling these relationships at the task level, which may be\nrestrictive in some applications. In this paper, we propose a novel MTL\nformulation that captures task relationships at the feature-level. Depending on\nthe interactions among tasks and features, the proposed method construct\ndifferent task clusters for different features, without even the need of\npre-specifying the number of clusters. Computationally, the proposed\nformulation is strongly convex, and can be efficiently solved by accelerated\nproximal methods. Experiments are performed on a number of synthetic and\nreal-world data sets. Under various degrees of task relationships, the accuracy\nof the proposed method is consistently among the best. Moreover, the\nfeature-specific task clusters obtained agree with the known/plausible task\nstructures of the data.\n", "versions": [{"version": "v1", "created": "Mon, 18 Jun 2012 14:40:55 GMT"}], "update_date": "2012-06-22", "authors_parsed": [["Zhong", "Wenliang", "", "HKUST"], ["Kwok", "James", "", "HKUST"]]}, {"id": "1206.4602", "submitter": "Philipp Hennig", "authors": "Philipp Hennig (MPI Intelligent Systems), Martin Kiefel (MPI for\n  Intelligent Systems)", "title": "Quasi-Newton Methods: A New Direction", "comments": "ICML2012", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NA cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Four decades after their invention, quasi-Newton methods are still state of\nthe art in unconstrained numerical optimization. Although not usually\ninterpreted thus, these are learning algorithms that fit a local quadratic\napproximation to the objective function. We show that many, including the most\npopular, quasi-Newton methods can be interpreted as approximations of Bayesian\nlinear regression under varying prior assumptions. This new notion elucidates\nsome shortcomings of classical algorithms, and lights the way to a novel\nnonparametric quasi-Newton method, which is able to make more efficient use of\navailable information at computational cost similar to its predecessors.\n", "versions": [{"version": "v1", "created": "Mon, 18 Jun 2012 14:41:11 GMT"}], "update_date": "2012-06-22", "authors_parsed": [["Hennig", "Philipp", "", "MPI Intelligent Systems"], ["Kiefel", "Martin", "", "MPI for\n  Intelligent Systems"]]}, {"id": "1206.4606", "submitter": "Chao Liu", "authors": "Chao Liu (Tencent Inc.), Yi-Min Wang (Microsoft Research)", "title": "TrueLabel + Confusions: A Spectrum of Probabilistic Models in Analyzing\n  Multiple Ratings", "comments": "ICML2012", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper revisits the problem of analyzing multiple ratings given by\ndifferent judges. Different from previous work that focuses on distilling the\ntrue labels from noisy crowdsourcing ratings, we emphasize gaining diagnostic\ninsights into our in-house well-trained judges. We generalize the well-known\nDawidSkene model (Dawid & Skene, 1979) to a spectrum of probabilistic models\nunder the same \"TrueLabel + Confusion\" paradigm, and show that our proposed\nhierarchical Bayesian model, called HybridConfusion, consistently outperforms\nDawidSkene on both synthetic and real-world data sets.\n", "versions": [{"version": "v1", "created": "Mon, 18 Jun 2012 14:43:42 GMT"}], "update_date": "2012-06-25", "authors_parsed": [["Liu", "Chao", "", "Tencent Inc."], ["Wang", "Yi-Min", "", "Microsoft Research"]]}, {"id": "1206.4607", "submitter": "Fabio Massimo Zanzotto", "authors": "Fabio Massimo Zanzotto (University of Rome-Tor Vergata), Lorenzo\n  Dell'Arciprete (University of Rome-Tor Vergata)", "title": "Distributed Tree Kernels", "comments": "ICML2012", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we propose the distributed tree kernels (DTK) as a novel\nmethod to reduce time and space complexity of tree kernels. Using a linear\ncomplexity algorithm to compute vectors for trees, we embed feature spaces of\ntree fragments in low-dimensional spaces where the kernel computation is\ndirectly done with dot product. We show that DTKs are faster, correlate with\ntree kernels, and obtain a statistically similar performance in two natural\nlanguage processing tasks.\n", "versions": [{"version": "v1", "created": "Mon, 18 Jun 2012 14:44:09 GMT"}], "update_date": "2012-06-22", "authors_parsed": [["Zanzotto", "Fabio Massimo", "", "University of Rome-Tor Vergata"], ["Dell'Arciprete", "Lorenzo", "", "University of Rome-Tor Vergata"]]}, {"id": "1206.4608", "submitter": "Soeren Laue", "authors": "Soeren Laue (Friedrich-Schiller-University)", "title": "A Hybrid Algorithm for Convex Semidefinite Optimization", "comments": "ICML2012", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.DS cs.NA stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a hybrid algorithm for optimizing a convex, smooth function over\nthe cone of positive semidefinite matrices. Our algorithm converges to the\nglobal optimal solution and can be used to solve general large-scale\nsemidefinite programs and hence can be readily applied to a variety of machine\nlearning problems. We show experimental results on three machine learning\nproblems (matrix completion, metric learning, and sparse PCA) . Our approach\noutperforms state-of-the-art algorithms.\n", "versions": [{"version": "v1", "created": "Mon, 18 Jun 2012 14:44:28 GMT"}], "update_date": "2012-06-22", "authors_parsed": [["Laue", "Soeren", "", "Friedrich-Schiller-University"]]}, {"id": "1206.4609", "submitter": "Roland Memisevic", "authors": "Roland Memisevic (University of Frankfurt)", "title": "On multi-view feature learning", "comments": "ICML2012", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Sparse coding is a common approach to learning local features for object\nrecognition. Recently, there has been an increasing interest in learning\nfeatures from spatio-temporal, binocular, or other multi-observation data,\nwhere the goal is to encode the relationship between images rather than the\ncontent of a single image. We provide an analysis of multi-view feature\nlearning, which shows that hidden variables encode transformations by detecting\nrotation angles in the eigenspaces shared among multiple image warps. Our\nanalysis helps explain recent experimental results showing that\ntransformation-specific features emerge when training complex cell models on\nvideos. Our analysis also shows that transformation-invariant features can\nemerge as a by-product of learning representations of transformations.\n", "versions": [{"version": "v1", "created": "Mon, 18 Jun 2012 14:45:17 GMT"}], "update_date": "2012-06-22", "authors_parsed": [["Memisevic", "Roland", "", "University of Frankfurt"]]}, {"id": "1206.4610", "submitter": "Andreas Damianou", "authors": "Andreas Damianou (University of Sheffield), Carl Ek (KTH), Michalis\n  Titsias (University of Oxford), Neil Lawrence (University of Sheffield)", "title": "Manifold Relevance Determination", "comments": "ICML2012", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper we present a fully Bayesian latent variable model which\nexploits conditional nonlinear(in)-dependence structures to learn an efficient\nlatent representation. The latent space is factorized to represent shared and\nprivate information from multiple views of the data. In contrast to previous\napproaches, we introduce a relaxation to the discrete segmentation and allow\nfor a \"softly\" shared latent space. Further, Bayesian techniques allow us to\nautomatically estimate the dimensionality of the latent spaces. The model is\ncapable of capturing structure underlying extremely high dimensional spaces.\nThis is illustrated by modelling unprocessed images with tenths of thousands of\npixels. This also allows us to directly generate novel images from the trained\nmodel by sampling from the discovered latent spaces. We also demonstrate the\nmodel by prediction of human pose in an ambiguous setting. Our Bayesian\nframework allows us to perform disambiguation in a principled manner by\nincluding latent space priors which incorporate the dynamic nature of the data.\n", "versions": [{"version": "v1", "created": "Mon, 18 Jun 2012 14:45:37 GMT"}], "update_date": "2012-06-22", "authors_parsed": [["Damianou", "Andreas", "", "University of Sheffield"], ["Ek", "Carl", "", "KTH"], ["Titsias", "Michalis", "", "University of Oxford"], ["Lawrence", "Neil", "", "University of Sheffield"]]}, {"id": "1206.4611", "submitter": "Pratik Jawanpuria", "authors": "Pratik Jawanpuria (IIT Bombay), J. Saketha Nath (IIT Bombay)", "title": "A Convex Feature Learning Formulation for Latent Task Structure\n  Discovery", "comments": "ICML2012", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper considers the multi-task learning problem and in the setting where\nsome relevant features could be shared across few related tasks. Most of the\nexisting methods assume the extent to which the given tasks are related or\nshare a common feature space to be known apriori. In real-world applications\nhowever, it is desirable to automatically discover the groups of related tasks\nthat share a feature space. In this paper we aim at searching the exponentially\nlarge space of all possible groups of tasks that may share a feature space. The\nmain contribution is a convex formulation that employs a graph-based\nregularizer and simultaneously discovers few groups of related tasks, having\nclose-by task parameters, as well as the feature space shared within each\ngroup. The regularizer encodes an important structure among the groups of tasks\nleading to an efficient algorithm for solving it: if there is no feature space\nunder which a group of tasks has close-by task parameters, then there does not\nexist such a feature space for any of its supersets. An efficient active set\nalgorithm that exploits this simplification and performs a clever search in the\nexponentially large space is presented. The algorithm is guaranteed to solve\nthe proposed formulation (within some precision) in a time polynomial in the\nnumber of groups of related tasks discovered. Empirical results on benchmark\ndatasets show that the proposed formulation achieves good generalization and\noutperforms state-of-the-art multi-task learning algorithms in some cases.\n", "versions": [{"version": "v1", "created": "Mon, 18 Jun 2012 15:00:07 GMT"}], "update_date": "2012-06-22", "authors_parsed": [["Jawanpuria", "Pratik", "", "IIT Bombay"], ["Nath", "J. Saketha", "", "IIT Bombay"]]}, {"id": "1206.4613", "submitter": "Mauricio Araya", "authors": "Mauricio Araya (LORIA/INRIA), Olivier Buffet (LORIA/INRIA), Vincent\n  Thomas (LORIA/INRIA)", "title": "Near-Optimal BRL using Optimistic Local Transitions", "comments": "ICML2012", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Model-based Bayesian Reinforcement Learning (BRL) allows a found\nformalization of the problem of acting optimally while facing an unknown\nenvironment, i.e., avoiding the exploration-exploitation dilemma. However,\nalgorithms explicitly addressing BRL suffer from such a combinatorial explosion\nthat a large body of work relies on heuristic algorithms. This paper introduces\nBOLT, a simple and (almost) deterministic heuristic algorithm for BRL which is\noptimistic about the transition function. We analyze BOLT's sample complexity,\nand show that under certain parameters, the algorithm is near-optimal in the\nBayesian sense with high probability. Then, experimental results highlight the\nkey differences of this method compared to previous work.\n", "versions": [{"version": "v1", "created": "Mon, 18 Jun 2012 15:00:40 GMT"}], "update_date": "2012-06-22", "authors_parsed": [["Araya", "Mauricio", "", "LORIA/INRIA"], ["Buffet", "Olivier", "", "LORIA/INRIA"], ["Thomas", "Vincent", "", "LORIA/INRIA"]]}, {"id": "1206.4614", "submitter": "Gang Niu", "authors": "Gang Niu (Tokyo Institute of Technology), Bo Dai (Purdue University),\n  Makoto Yamada (Tokyo Institute of Technology), Masashi Sugiyama (Tokyo\n  Institute of Technology)", "title": "Information-theoretic Semi-supervised Metric Learning via Entropy\n  Regularization", "comments": "ICML2012", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a general information-theoretic approach called Seraph\n(SEmi-supervised metRic leArning Paradigm with Hyper-sparsity) for metric\nlearning that does not rely upon the manifold assumption. Given the probability\nparameterized by a Mahalanobis distance, we maximize the entropy of that\nprobability on labeled data and minimize it on unlabeled data following entropy\nregularization, which allows the supervised and unsupervised parts to be\nintegrated in a natural and meaningful way. Furthermore, Seraph is regularized\nby encouraging a low-rank projection induced from the metric. The optimization\nof Seraph is solved efficiently and stably by an EM-like scheme with the\nanalytical E-Step and convex M-Step. Experiments demonstrate that Seraph\ncompares favorably with many well-known global and local metric learning\nmethods.\n", "versions": [{"version": "v1", "created": "Mon, 18 Jun 2012 15:01:43 GMT"}], "update_date": "2012-06-22", "authors_parsed": [["Niu", "Gang", "", "Tokyo Institute of Technology"], ["Dai", "Bo", "", "Purdue University"], ["Yamada", "Makoto", "", "Tokyo Institute of Technology"], ["Sugiyama", "Masashi", "", "Tokyo\n  Institute of Technology"]]}, {"id": "1206.4616", "submitter": "Drausin Wulsin", "authors": "Drausin Wulsin (University of Pennsylvania), Shane Jensen (University\n  of Pennsylvania), Brian Litt (University of Pennsylvania)", "title": "A Hierarchical Dirichlet Process Model with Multiple Levels of\n  Clustering for Human EEG Seizure Modeling", "comments": "ICML2012", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Driven by the multi-level structure of human intracranial\nelectroencephalogram (iEEG) recordings of epileptic seizures, we introduce a\nnew variant of a hierarchical Dirichlet Process---the multi-level clustering\nhierarchical Dirichlet Process (MLC-HDP)---that simultaneously clusters\ndatasets on multiple levels. Our seizure dataset contains brain activity\nrecorded in typically more than a hundred individual channels for each seizure\nof each patient. The MLC-HDP model clusters over channels-types, seizure-types,\nand patient-types simultaneously. We describe this model and its implementation\nin detail. We also present the results of a simulation study comparing the\nMLC-HDP to a similar model, the Nested Dirichlet Process and finally\ndemonstrate the MLC-HDP's use in modeling seizures across multiple patients. We\nfind the MLC-HDP's clustering to be comparable to independent human physician\nclusterings. To our knowledge, the MLC-HDP model is the first in the epilepsy\nliterature capable of clustering seizures within and between patients.\n", "versions": [{"version": "v1", "created": "Mon, 18 Jun 2012 15:02:12 GMT"}], "update_date": "2012-06-22", "authors_parsed": [["Wulsin", "Drausin", "", "University of Pennsylvania"], ["Jensen", "Shane", "", "University\n  of Pennsylvania"], ["Litt", "Brian", "", "University of Pennsylvania"]]}, {"id": "1206.4617", "submitter": "Sergey Levine", "authors": "Sergey Levine (Stanford University), Vladlen Koltun (Stanford\n  University)", "title": "Continuous Inverse Optimal Control with Locally Optimal Examples", "comments": "ICML2012", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Inverse optimal control, also known as inverse reinforcement learning, is the\nproblem of recovering an unknown reward function in a Markov decision process\nfrom expert demonstrations of the optimal policy. We introduce a probabilistic\ninverse optimal control algorithm that scales gracefully with task\ndimensionality, and is suitable for large, continuous domains where even\ncomputing a full policy is impractical. By using a local approximation of the\nreward function, our method can also drop the assumption that the\ndemonstrations are globally optimal, requiring only local optimality. This\nallows it to learn from examples that are unsuitable for prior methods.\n", "versions": [{"version": "v1", "created": "Mon, 18 Jun 2012 15:02:28 GMT"}], "update_date": "2012-06-22", "authors_parsed": [["Levine", "Sergey", "", "Stanford University"], ["Koltun", "Vladlen", "", "Stanford\n  University"]]}, {"id": "1206.4618", "submitter": "Wei Liu", "authors": "Wei Liu (Columbia University), Jun Wang (IBM T. J. Watson Research\n  Center), Yadong Mu (Columbia University), Sanjiv Kumar (Google), Shih-Fu\n  Chang (Columbia University)", "title": "Compact Hyperplane Hashing with Bilinear Functions", "comments": "ICML2012", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Hyperplane hashing aims at rapidly searching nearest points to a hyperplane,\nand has shown practical impact in scaling up active learning with SVMs.\nUnfortunately, the existing randomized methods need long hash codes to achieve\nreasonable search accuracy and thus suffer from reduced search speed and large\nmemory overhead. To this end, this paper proposes a novel hyperplane hashing\ntechnique which yields compact hash codes. The key idea is the bilinear form of\nthe proposed hash functions, which leads to higher collision probability than\nthe existing hyperplane hash functions when using random projections. To\nfurther increase the performance, we propose a learning based framework in\nwhich the bilinear functions are directly learned from the data. This results\nin short yet discriminative codes, and also boosts the search performance over\nthe random projection based solutions. Large-scale active learning experiments\ncarried out on two datasets with up to one million samples demonstrate the\noverall superiority of the proposed approach.\n", "versions": [{"version": "v1", "created": "Mon, 18 Jun 2012 15:03:10 GMT"}], "update_date": "2012-06-22", "authors_parsed": [["Liu", "Wei", "", "Columbia University"], ["Wang", "Jun", "", "IBM T. J. Watson Research\n  Center"], ["Mu", "Yadong", "", "Columbia University"], ["Kumar", "Sanjiv", "", "Google"], ["Chang", "Shih-Fu", "", "Columbia University"]]}, {"id": "1206.4620", "submitter": "Sebastian Nowozin", "authors": "Sebastian Nowozin (Microsoft Research Cambridge)", "title": "Improved Information Gain Estimates for Decision Tree Induction", "comments": "ICML2012", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Ensembles of classification and regression trees remain popular machine\nlearning methods because they define flexible non-parametric models that\npredict well and are computationally efficient both during training and\ntesting. During induction of decision trees one aims to find predicates that\nare maximally informative about the prediction target. To select good\npredicates most approaches estimate an information-theoretic scoring function,\nthe information gain, both for classification and regression problems. We point\nout that the common estimation procedures are biased and show that by replacing\nthem with improved estimators of the discrete and the differential entropy we\ncan obtain better decision trees. In effect our modifications yield improved\npredictive performance and are simple to implement in any decision tree code.\n", "versions": [{"version": "v1", "created": "Mon, 18 Jun 2012 15:04:54 GMT"}], "update_date": "2012-06-22", "authors_parsed": [["Nowozin", "Sebastian", "", "Microsoft Research Cambridge"]]}, {"id": "1206.4622", "submitter": "Aaron Defazio", "authors": "Aaron Defazio (ANU), Tiberio Caetano (NICTA and Australian National\n  University)", "title": "A Graphical Model Formulation of Collaborative Filtering Neighbourhood\n  Methods with Fast Maximum Entropy Training", "comments": "ICML2012", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.IR stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Item neighbourhood methods for collaborative filtering learn a weighted graph\nover the set of items, where each item is connected to those it is most similar\nto. The prediction of a user's rating on an item is then given by that rating\nof neighbouring items, weighted by their similarity. This paper presents a new\nneighbourhood approach which we call item fields, whereby an undirected\ngraphical model is formed over the item graph. The resulting prediction rule is\na simple generalization of the classical approaches, which takes into account\nnon-local information in the graph, allowing its best results to be obtained\nwhen using drastically fewer edges than other neighbourhood approaches. A fast\napproximate maximum entropy training method based on the Bethe approximation is\npresented, which uses a simple gradient ascent procedure. When using\nprecomputed sufficient statistics on the Movielens datasets, our method is\nfaster than maximum likelihood approaches by two orders of magnitude.\n", "versions": [{"version": "v1", "created": "Mon, 18 Jun 2012 15:05:52 GMT"}], "update_date": "2012-06-22", "authors_parsed": [["Defazio", "Aaron", "", "ANU"], ["Caetano", "Tiberio", "", "NICTA and Australian National\n  University"]]}, {"id": "1206.4623", "submitter": "Yi Sun", "authors": "Yi Sun (IDSIA), Faustino Gomez (IDSIA), Juergen Schmidhuber (IDSIA)", "title": "On the Size of the Online Kernel Sparsification Dictionary", "comments": "ICML2012", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We analyze the size of the dictionary constructed from online kernel\nsparsification, using a novel formula that expresses the expected determinant\nof the kernel Gram matrix in terms of the eigenvalues of the covariance\noperator. Using this formula, we are able to connect the cardinality of the\ndictionary with the eigen-decay of the covariance operator. In particular, we\nshow that under certain technical conditions, the size of the dictionary will\nalways grow sub-linearly in the number of data points, and, as a consequence,\nthe kernel linear regressor constructed from the resulting dictionary is\nconsistent.\n", "versions": [{"version": "v1", "created": "Mon, 18 Jun 2012 15:06:34 GMT"}], "update_date": "2019-08-15", "authors_parsed": [["Sun", "Yi", "", "IDSIA"], ["Gomez", "Faustino", "", "IDSIA"], ["Schmidhuber", "Juergen", "", "IDSIA"]]}, {"id": "1206.4624", "submitter": "Dian Gong", "authors": "Dian Gong (Univ. of Southern California), Xuemei Zhao (Univ of\n  Southern California), Gerard Medioni (University of Southern California)", "title": "Robust Multiple Manifolds Structure Learning", "comments": "ICML2012", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a robust multiple manifolds structure learning (RMMSL) scheme to\nrobustly estimate data structures under the multiple low intrinsic dimensional\nmanifolds assumption. In the local learning stage, RMMSL efficiently estimates\nlocal tangent space by weighted low-rank matrix factorization. In the global\nlearning stage, we propose a robust manifold clustering method based on local\nstructure learning results. The proposed clustering method is designed to get\nthe flattest manifolds clusters by introducing a novel curved-level similarity\nfunction. Our approach is evaluated and compared to state-of-the-art methods on\nsynthetic data, handwritten digit images, human motion capture data and\nmotorbike videos. We demonstrate the effectiveness of the proposed approach,\nwhich yields higher clustering accuracy, and produces promising results for\nchallenging tasks of human motion segmentation and motion flow learning from\nvideos.\n", "versions": [{"version": "v1", "created": "Mon, 18 Jun 2012 15:06:49 GMT"}], "update_date": "2012-06-22", "authors_parsed": [["Gong", "Dian", "", "Univ. of Southern California"], ["Zhao", "Xuemei", "", "Univ of\n  Southern California"], ["Medioni", "Gerard", "", "University of Southern California"]]}, {"id": "1206.4627", "submitter": "Jean Honorio", "authors": "Jean Honorio (Stony Brook University)", "title": "Convergence Rates of Biased Stochastic Optimization for Learning Sparse\n  Ising Models", "comments": "ICML2012", "journal-ref": "International Conference on Machine Learning (ICML), 2012", "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study the convergence rate of stochastic optimization of exact (NP-hard)\nobjectives, for which only biased estimates of the gradient are available. We\nmotivate this problem in the context of learning the structure and parameters\nof Ising models. We first provide a convergence-rate analysis of deterministic\nerrors for forward-backward splitting (FBS). We then extend our analysis to\nbiased stochastic errors, by first characterizing a family of samplers and\nproviding a high probability bound that allows understanding not only FBS, but\nalso proximal gradient (PG) methods. We derive some interesting conclusions:\nFBS requires only a logarithmically increasing number of random samples in\norder to converge (although at a very low rate); the required number of random\nsamples is the same for the deterministic and the biased stochastic setting for\nFBS and basic PG; accelerated PG is not guaranteed to converge in the biased\nstochastic setting.\n", "versions": [{"version": "v1", "created": "Mon, 18 Jun 2012 15:07:39 GMT"}], "update_date": "2018-11-16", "authors_parsed": [["Honorio", "Jean", "", "Stony Brook University"]]}, {"id": "1206.4628", "submitter": "Jiashi Feng", "authors": "Jiashi Feng (NUS), Huan Xu (NUS), Shuicheng Yan (NUS)", "title": "Robust PCA in High-dimension: A Deterministic Approach", "comments": "ICML2012", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider principal component analysis for contaminated data-set in the\nhigh dimensional regime, where the dimensionality of each observation is\ncomparable or even more than the number of observations. We propose a\ndeterministic high-dimensional robust PCA algorithm which inherits all\ntheoretical properties of its randomized counterpart, i.e., it is tractable,\nrobust to contaminated points, easily kernelizable, asymptotic consistent and\nachieves maximal robustness -- a breakdown point of 50%. More importantly, the\nproposed method exhibits significantly better computational efficiency, which\nmakes it suitable for large-scale real applications.\n", "versions": [{"version": "v1", "created": "Mon, 18 Jun 2012 15:07:55 GMT"}], "update_date": "2012-06-22", "authors_parsed": [["Feng", "Jiashi", "", "NUS"], ["Xu", "Huan", "", "NUS"], ["Yan", "Shuicheng", "", "NUS"]]}, {"id": "1206.4631", "submitter": "Edoardo Airoldi", "authors": "Edoardo M Airoldi, Jonathan M Bischof", "title": "A Poisson convolution model for characterizing topical content with word\n  frequency and exclusivity", "comments": "Originally appeared in ICML2012", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CL cs.IR stat.ME stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  An ongoing challenge in the analysis of document collections is how to\nsummarize content in terms of a set of inferred themes that can be interpreted\nsubstantively in terms of topics. The current practice of parametrizing the\nthemes in terms of most frequent words limits interpretability by ignoring the\ndifferential use of words across topics. We argue that words that are both\ncommon and exclusive to a theme are more effective at characterizing topical\ncontent. We consider a setting where professional editors have annotated\ndocuments to a collection of topic categories, organized into a tree, in which\nleaf-nodes correspond to the most specific topics. Each document is annotated\nto multiple categories, at different levels of the tree. We introduce a\nhierarchical Poisson convolution model to analyze annotated documents in this\nsetting. The model leverages the structure among categories defined by\nprofessional editors to infer a clear semantic description for each topic in\nterms of words that are both frequent and exclusive. We carry out a large\nrandomized experiment on Amazon Turk to demonstrate that topic summaries based\non the FREX score are more interpretable than currently established frequency\nbased summaries, and that the proposed model produces more efficient estimates\nof exclusivity than with currently models. We also develop a parallelized\nHamiltonian Monte Carlo sampler that allows the inference to scale to millions\nof documents.\n", "versions": [{"version": "v1", "created": "Mon, 18 Jun 2012 15:11:38 GMT"}, {"version": "v2", "created": "Wed, 12 Dec 2012 17:32:26 GMT"}, {"version": "v3", "created": "Mon, 28 Jul 2014 03:02:39 GMT"}], "update_date": "2014-07-29", "authors_parsed": [["Airoldi", "Edoardo M", ""], ["Bischof", "Jonathan M", ""]]}, {"id": "1206.4632", "submitter": "Julia Vogt", "authors": "Julia Vogt (University of Basel), Volker Roth (University of Basel)", "title": "A Complete Analysis of the l_1,p Group-Lasso", "comments": "ICML2012", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG math.OC stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The Group-Lasso is a well-known tool for joint regularization in machine\nlearning methods. While the l_{1,2} and the l_{1,\\infty} version have been\nstudied in detail and efficient algorithms exist, there are still open\nquestions regarding other l_{1,p} variants. We characterize conditions for\nsolutions of the l_{1,p} Group-Lasso for all p-norms with 1 <= p <= \\infty, and\nwe present a unified active set algorithm. For all p-norms, a highly efficient\nprojected gradient algorithm is presented. This new algorithm enables us to\ncompare the prediction performance of many variants of the Group-Lasso in a\nmulti-task learning setting, where the aim is to solve many learning problems\nin parallel which are coupled via the Group-Lasso constraint. We conduct\nlarge-scale experiments on synthetic data and on two real-world data sets. In\naccordance with theoretical characterizations of the different norms we observe\nthat the weak-coupling norms with p between 1.5 and 2 consistently outperform\nthe strong-coupling norms with p >> 2.\n", "versions": [{"version": "v1", "created": "Mon, 18 Jun 2012 15:12:01 GMT"}], "update_date": "2012-06-22", "authors_parsed": [["Vogt", "Julia", "", "University of Basel"], ["Roth", "Volker", "", "University of Basel"]]}, {"id": "1206.4633", "submitter": "Steven C.H. Hoi", "authors": "Peilin Zhao (NTU), Jialei Wang (NTU), Pengcheng Wu (NTU), Rong Jin\n  (MSU), Steven C.H. Hoi (NTU)", "title": "Fast Bounded Online Gradient Descent Algorithms for Scalable\n  Kernel-Based Online Learning", "comments": "ICML2012", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Kernel-based online learning has often shown state-of-the-art performance for\nmany online learning tasks. It, however, suffers from a major shortcoming, that\nis, the unbounded number of support vectors, making it non-scalable and\nunsuitable for applications with large-scale datasets. In this work, we study\nthe problem of bounded kernel-based online learning that aims to constrain the\nnumber of support vectors by a predefined budget. Although several algorithms\nhave been proposed in literature, they are neither computationally efficient\ndue to their intensive budget maintenance strategy nor effective due to the use\nof simple Perceptron algorithm. To overcome these limitations, we propose a\nframework for bounded kernel-based online learning based on an online gradient\ndescent approach. We propose two efficient algorithms of bounded online\ngradient descent (BOGD) for scalable kernel-based online learning: (i) BOGD by\nmaintaining support vectors using uniform sampling, and (ii) BOGD++ by\nmaintaining support vectors using non-uniform sampling. We present theoretical\nanalysis of regret bound for both algorithms, and found promising empirical\nperformance in terms of both efficacy and efficiency by comparing them to\nseveral well-known algorithms for bounded kernel-based online learning on\nlarge-scale datasets.\n", "versions": [{"version": "v1", "created": "Mon, 18 Jun 2012 15:13:13 GMT"}], "update_date": "2012-06-22", "authors_parsed": [["Zhao", "Peilin", "", "NTU"], ["Wang", "Jialei", "", "NTU"], ["Wu", "Pengcheng", "", "NTU"], ["Jin", "Rong", "", "MSU"], ["Hoi", "Steven C. H.", "", "NTU"]]}, {"id": "1206.4634", "submitter": "Ning Xie", "authors": "Ning Xie (Tokyo Institute of Technology), Hirotaka Hachiya (Tokyo\n  Institute of Technology), Masashi Sugiyama (Tokyo Institute of Technology)", "title": "Artist Agent: A Reinforcement Learning Approach to Automatic Stroke\n  Generation in Oriental Ink Painting", "comments": "ICML2012", "journal-ref": null, "doi": "10.1587/transinf.E96.D.1134", "report-no": null, "categories": "cs.LG cs.GR stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Oriental ink painting, called Sumi-e, is one of the most appealing painting\nstyles that has attracted artists around the world. Major challenges in\ncomputer-based Sumi-e simulation are to abstract complex scene information and\ndraw smooth and natural brush strokes. To automatically find such strokes, we\npropose to model the brush as a reinforcement learning agent, and learn desired\nbrush-trajectories by maximizing the sum of rewards in the policy search\nframework. We also provide elaborate design of actions, states, and rewards\ntailored for a Sumi-e agent. The effectiveness of our proposed approach is\ndemonstrated through simulated Sumi-e experiments.\n", "versions": [{"version": "v1", "created": "Mon, 18 Jun 2012 15:14:24 GMT"}], "update_date": "2015-06-05", "authors_parsed": [["Xie", "Ning", "", "Tokyo Institute of Technology"], ["Hachiya", "Hirotaka", "", "Tokyo\n  Institute of Technology"], ["Sugiyama", "Masashi", "", "Tokyo Institute of Technology"]]}, {"id": "1206.4635", "submitter": "Yichuan Tang", "authors": "Yichuan Tang (University of Toronto), Ruslan Salakhutdinov (University\n  of Toronto), Geoffrey Hinton (University of Toronto)", "title": "Deep Mixtures of Factor Analysers", "comments": "ICML2012", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  An efficient way to learn deep density models that have many layers of latent\nvariables is to learn one layer at a time using a model that has only one layer\nof latent variables. After learning each layer, samples from the posterior\ndistributions for that layer are used as training data for learning the next\nlayer. This approach is commonly used with Restricted Boltzmann Machines, which\nare undirected graphical models with a single hidden layer, but it can also be\nused with Mixtures of Factor Analysers (MFAs) which are directed graphical\nmodels. In this paper, we present a greedy layer-wise learning algorithm for\nDeep Mixtures of Factor Analysers (DMFAs). Even though a DMFA can be converted\nto an equivalent shallow MFA by multiplying together the factor loading\nmatrices at different levels, learning and inference are much more efficient in\na DMFA and the sharing of each lower-level factor loading matrix by many\ndifferent higher level MFAs prevents overfitting. We demonstrate empirically\nthat DMFAs learn better density models than both MFAs and two types of\nRestricted Boltzmann Machine on a wide variety of datasets.\n", "versions": [{"version": "v1", "created": "Mon, 18 Jun 2012 15:14:57 GMT"}], "update_date": "2012-06-22", "authors_parsed": [["Tang", "Yichuan", "", "University of Toronto"], ["Salakhutdinov", "Ruslan", "", "University\n  of Toronto"], ["Hinton", "Geoffrey", "", "University of Toronto"]]}, {"id": "1206.4637", "submitter": "Paul Prasse", "authors": "Paul Prasse (University of Potsdam), Christoph Sawade (University of\n  Potsdam), Niels Landwehr (University of Potsdam), Tobias Scheffer (University\n  of Potsdam)", "title": "Learning to Identify Regular Expressions that Describe Email Campaigns", "comments": "ICML2012", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CL stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper addresses the problem of inferring a regular expression from a\ngiven set of strings that resembles, as closely as possible, the regular\nexpression that a human expert would have written to identify the language.\nThis is motivated by our goal of automating the task of postmasters of an email\nservice who use regular expressions to describe and blacklist email spam\ncampaigns. Training data contains batches of messages and corresponding regular\nexpressions that an expert postmaster feels confident to blacklist. We model\nthis task as a learning problem with structured output spaces and an\nappropriate loss function, derive a decoder and the resulting optimization\nproblem, and a report on a case study conducted with an email service.\n", "versions": [{"version": "v1", "created": "Mon, 18 Jun 2012 15:15:28 GMT"}], "update_date": "2012-06-22", "authors_parsed": [["Prasse", "Paul", "", "University of Potsdam"], ["Sawade", "Christoph", "", "University of\n  Potsdam"], ["Landwehr", "Niels", "", "University of Potsdam"], ["Scheffer", "Tobias", "", "University\n  of Potsdam"]]}, {"id": "1206.4638", "submitter": "Adams Wei Yu", "authors": "Adams Wei Yu (The University of Hong Kong), Hao Su (Stanford\n  University), Li Fei-Fei (Stanford University)", "title": "Efficient Euclidean Projections onto the Intersection of Norm Balls", "comments": "ICML2012", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Using sparse-inducing norms to learn robust models has received increasing\nattention from many fields for its attractive properties. Projection-based\nmethods have been widely applied to learning tasks constrained by such norms.\nAs a key building block of these methods, an efficient operator for Euclidean\nprojection onto the intersection of $\\ell_1$ and $\\ell_{1,q}$ norm balls\n$(q=2\\text{or}\\infty)$ is proposed in this paper. We prove that the projection\ncan be reduced to finding the root of an auxiliary function which is piecewise\nsmooth and monotonic. Hence, a bisection algorithm is sufficient to solve the\nproblem. We show that the time complexity of our solution is $O(n+g\\log g)$ for\n$q=2$ and $O(n\\log n)$ for $q=\\infty$, where $n$ is the dimensionality of the\nvector to be projected and $g$ is the number of disjoint groups; we confirm\nthis complexity by experimentation. Empirical study reveals that our method\nachieves significantly better performance than classical methods in terms of\nrunning time and memory usage. We further show that embedded with our efficient\nprojection operator, projection-based algorithms can solve regression problems\nwith composite norm constraints more efficiently than other methods and give\nsuperior accuracy.\n", "versions": [{"version": "v1", "created": "Mon, 18 Jun 2012 15:16:28 GMT"}], "update_date": "2012-06-22", "authors_parsed": [["Yu", "Adams Wei", "", "The University of Hong Kong"], ["Su", "Hao", "", "Stanford\n  University"], ["Fei-Fei", "Li", "", "Stanford University"]]}, {"id": "1206.4640", "submitter": "Huan Xu", "authors": "Yu-Xiang Wang (National University of Singapore), Huan Xu (National\n  University of Singapore)", "title": "Stability of matrix factorization for collaborative filtering", "comments": "ICML2012", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NA cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study the stability vis a vis adversarial noise of matrix factorization\nalgorithm for matrix completion. In particular, our results include: (I) we\nbound the gap between the solution matrix of the factorization method and the\nground truth in terms of root mean square error; (II) we treat the matrix\nfactorization as a subspace fitting problem and analyze the difference between\nthe solution subspace and the ground truth; (III) we analyze the prediction\nerror of individual users based on the subspace stability. We apply these\nresults to the problem of collaborative filtering under manipulator attack,\nwhich leads to useful insights and guidelines for collaborative filtering\nsystem design.\n", "versions": [{"version": "v1", "created": "Mon, 18 Jun 2012 15:18:05 GMT"}], "update_date": "2012-06-22", "authors_parsed": [["Wang", "Yu-Xiang", "", "National University of Singapore"], ["Xu", "Huan", "", "National\n  University of Singapore"]]}, {"id": "1206.4641", "submitter": "Tong Lin", "authors": "Tong Lin (Peking University), Hanlin Xue (Peking University), Ling\n  Wang (LTCI, Telecom ParisTech, Paris), Hongbin Zha (Peking University)", "title": "Total Variation and Euler's Elastica for Supervised Learning", "comments": "ICML2012", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In recent years, total variation (TV) and Euler's elastica (EE) have been\nsuccessfully applied to image processing tasks such as denoising and\ninpainting. This paper investigates how to extend TV and EE to the supervised\nlearning settings on high dimensional data. The supervised learning problem can\nbe formulated as an energy functional minimization under Tikhonov\nregularization scheme, where the energy is composed of a squared loss and a\ntotal variation smoothing (or Euler's elastica smoothing). Its solution via\nvariational principles leads to an Euler-Lagrange PDE. However, the PDE is\nalways high-dimensional and cannot be directly solved by common methods.\nInstead, radial basis functions are utilized to approximate the target\nfunction, reducing the problem to finding the linear coefficients of basis\nfunctions. We apply the proposed methods to supervised learning tasks\n(including binary classification, multi-class classification, and regression)\non benchmark data sets. Extensive experiments have demonstrated promising\nresults of the proposed methods.\n", "versions": [{"version": "v1", "created": "Mon, 18 Jun 2012 15:18:20 GMT"}], "update_date": "2012-06-22", "authors_parsed": [["Lin", "Tong", "", "Peking University"], ["Xue", "Hanlin", "", "Peking University"], ["Wang", "Ling", "", "LTCI, Telecom ParisTech, Paris"], ["Zha", "Hongbin", "", "Peking University"]]}, {"id": "1206.4642", "submitter": "Daisuke Kimura", "authors": "Daisuke Kimura (The University of Tokyo), Hisashi Kashima (The\n  University of Tokyo)", "title": "Fast Computation of Subpath Kernel for Trees", "comments": "ICML2012", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The kernel method is a potential approach to analyzing structured data such\nas sequences, trees, and graphs; however, unordered trees have not been\ninvestigated extensively. Kimura et al. (2011) proposed a kernel function for\nunordered trees on the basis of their subpaths, which are vertical\nsubstructures of trees responsible for hierarchical information in them. Their\nkernel exhibits practically good performance in terms of accuracy and speed;\nhowever, linear-time computation is not guaranteed theoretically, unlike the\ncase of the other unordered tree kernel proposed by Vishwanathan and Smola\n(2003). In this paper, we propose a theoretically guaranteed linear-time kernel\ncomputation algorithm that is practically fast, and we present an efficient\nprediction algorithm whose running time depends only on the size of the input\ntree. Experimental results show that the proposed algorithms are quite\nefficient in practice.\n", "versions": [{"version": "v1", "created": "Mon, 18 Jun 2012 15:18:51 GMT"}], "update_date": "2012-06-22", "authors_parsed": [["Kimura", "Daisuke", "", "The University of Tokyo"], ["Kashima", "Hisashi", "", "The\n  University of Tokyo"]]}, {"id": "1206.4644", "submitter": "Ruijiang Li", "authors": "Ruijiang Li (Fudan University), Bin Li (University of Technology,\n  Sydney), Ke Zhang (Fudan Univ.), Cheng Jin (Fudan University), Xiangyang Xue\n  (Fudan University)", "title": "Groupwise Constrained Reconstruction for Subspace Clustering", "comments": "ICML2012", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Reconstruction based subspace clustering methods compute a self\nreconstruction matrix over the samples and use it for spectral clustering to\nobtain the final clustering result. Their success largely relies on the\nassumption that the underlying subspaces are independent, which, however, does\nnot always hold in the applications with increasing number of subspaces. In\nthis paper, we propose a novel reconstruction based subspace clustering model\nwithout making the subspace independence assumption. In our model, certain\nproperties of the reconstruction matrix are explicitly characterized using the\nlatent cluster indicators, and the affinity matrix used for spectral clustering\ncan be directly built from the posterior of the latent cluster indicators\ninstead of the reconstruction matrix. Experimental results on both synthetic\nand real-world datasets show that the proposed model can outperform the\nstate-of-the-art methods.\n", "versions": [{"version": "v1", "created": "Mon, 18 Jun 2012 15:19:22 GMT"}], "update_date": "2012-06-22", "authors_parsed": [["Li", "Ruijiang", "", "Fudan University"], ["Li", "Bin", "", "University of Technology,\n  Sydney"], ["Zhang", "Ke", "", "Fudan Univ."], ["Jin", "Cheng", "", "Fudan University"], ["Xue", "Xiangyang", "", "Fudan University"]]}, {"id": "1206.4645", "submitter": "Lauren Hannah", "authors": "Lauren Hannah (Duke University), David Dunson (Duke University)", "title": "Ensemble Methods for Convex Regression with Applications to Geometric\n  Programming Based Circuit Design", "comments": "ICML2012", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.NA stat.ME stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Convex regression is a promising area for bridging statistical estimation and\ndeterministic convex optimization. New piecewise linear convex regression\nmethods are fast and scalable, but can have instability when used to\napproximate constraints or objective functions for optimization. Ensemble\nmethods, like bagging, smearing and random partitioning, can alleviate this\nproblem and maintain the theoretical properties of the underlying estimator. We\nempirically examine the performance of ensemble methods for prediction and\noptimization, and then apply them to device modeling and constraint\napproximation for geometric programming based circuit design.\n", "versions": [{"version": "v1", "created": "Mon, 18 Jun 2012 15:19:58 GMT"}], "update_date": "2012-06-22", "authors_parsed": [["Hannah", "Lauren", "", "Duke University"], ["Dunson", "David", "", "Duke University"]]}, {"id": "1206.4646", "submitter": "Miguel Carreira-Perpinan", "authors": "Max Vladymyrov (UC Merced), Miguel Carreira-Perpinan (UC Merced)", "title": "Partial-Hessian Strategies for Fast Learning of Nonlinear Embeddings", "comments": "ICML2012", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Stochastic neighbor embedding (SNE) and related nonlinear manifold learning\nalgorithms achieve high-quality low-dimensional representations of similarity\ndata, but are notoriously slow to train. We propose a generic formulation of\nembedding algorithms that includes SNE and other existing algorithms, and study\ntheir relation with spectral methods and graph Laplacians. This allows us to\ndefine several partial-Hessian optimization strategies, characterize their\nglobal and local convergence, and evaluate them empirically. We achieve up to\ntwo orders of magnitude speedup over existing training methods with a strategy\n(which we call the spectral direction) that adds nearly no overhead to the\ngradient and yet is simple, scalable and applicable to several existing and\nfuture embedding algorithms.\n", "versions": [{"version": "v1", "created": "Mon, 18 Jun 2012 15:20:14 GMT"}], "update_date": "2012-06-22", "authors_parsed": [["Vladymyrov", "Max", "", "UC Merced"], ["Carreira-Perpinan", "Miguel", "", "UC Merced"]]}, {"id": "1206.4649", "submitter": "Pablo Sprechmann", "authors": "Alex Bronstein (Tel Aviv University), Pablo Sprechmann (University of\n  Minnesota), Guillermo Sapiro (University of Minnesota)", "title": "Learning Efficient Structured Sparse Models", "comments": "ICML2012", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a comprehensive framework for structured sparse coding and\nmodeling extending the recent ideas of using learnable fast regressors to\napproximate exact sparse codes. For this purpose, we develop a novel\nblock-coordinate proximal splitting method for the iterative solution of\nhierarchical sparse coding problems, and show an efficient feed forward\narchitecture derived from its iteration. This architecture faithfully\napproximates the exact structured sparse codes with a fraction of the\ncomplexity of the standard optimization methods. We also show that by using\ndifferent training objective functions, learnable sparse encoders are no longer\nrestricted to be mere approximants of the exact sparse code for a pre-given\ndictionary, as in earlier formulations, but can be rather used as full-featured\nsparse encoders or even modelers. A simple implementation shows several orders\nof magnitude speedup compared to the state-of-the-art at minimal performance\ndegradation, making the proposed framework suitable for real time and\nlarge-scale applications.\n", "versions": [{"version": "v1", "created": "Mon, 18 Jun 2012 15:23:19 GMT"}], "update_date": "2012-06-22", "authors_parsed": [["Bronstein", "Alex", "", "Tel Aviv University"], ["Sprechmann", "Pablo", "", "University of\n  Minnesota"], ["Sapiro", "Guillermo", "", "University of Minnesota"]]}, {"id": "1206.4650", "submitter": "Yaoliang Yu", "authors": "Yaoliang Yu (University of Alberta), Csaba Szepesvari (University of\n  Alberta)", "title": "Analysis of Kernel Mean Matching under Covariate Shift", "comments": "ICML2012", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In real supervised learning scenarios, it is not uncommon that the training\nand test sample follow different probability distributions, thus rendering the\nnecessity to correct the sampling bias. Focusing on a particular covariate\nshift problem, we derive high probability confidence bounds for the kernel mean\nmatching (KMM) estimator, whose convergence rate turns out to depend on some\nregularity measure of the regression function and also on some capacity measure\nof the kernel. By comparing KMM with the natural plug-in estimator, we\nestablish the superiority of the former hence provide concrete\nevidence/understanding to the effectiveness of KMM under covariate shift.\n", "versions": [{"version": "v1", "created": "Mon, 18 Jun 2012 15:23:37 GMT"}], "update_date": "2012-06-22", "authors_parsed": [["Yu", "Yaoliang", "", "University of Alberta"], ["Szepesvari", "Csaba", "", "University of\n  Alberta"]]}, {"id": "1206.4651", "submitter": "Qinfeng Shi", "authors": "Qinfeng Shi (The University of Adelaide), Chunhua Shen (The University\n  of Adelaide), Rhys Hill (The University of Adelaide), Anton van den Hengel\n  (the University of Adelaide)", "title": "Is margin preserved after random projection?", "comments": "ICML2012", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Random projections have been applied in many machine learning algorithms.\nHowever, whether margin is preserved after random projection is non-trivial and\nnot well studied. In this paper we analyse margin distortion after random\nprojection, and give the conditions of margin preservation for binary\nclassification problems. We also extend our analysis to margin for multiclass\nproblems, and provide theoretical bounds on multiclass margin on the projected\ndata.\n", "versions": [{"version": "v1", "created": "Mon, 18 Jun 2012 15:24:01 GMT"}], "update_date": "2012-06-22", "authors_parsed": [["Shi", "Qinfeng", "", "The University of Adelaide"], ["Shen", "Chunhua", "", "The University\n  of Adelaide"], ["Hill", "Rhys", "", "The University of Adelaide"], ["Hengel", "Anton van den", "", "the University of Adelaide"]]}, {"id": "1206.4653", "submitter": "Maya Gupta", "authors": "Nathan Parrish (University of Washington), Maya Gupta (University of\n  Washington)", "title": "Dimensionality Reduction by Local Discriminative Gaussians", "comments": "ICML2012", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present local discriminative Gaussian (LDG) dimensionality reduction, a\nsupervised dimensionality reduction technique for classification. The LDG\nobjective function is an approximation to the leave-one-out training error of a\nlocal quadratic discriminant analysis classifier, and thus acts locally to each\ntraining point in order to find a mapping where similar data can be\ndiscriminated from dissimilar data. While other state-of-the-art linear\ndimensionality reduction methods require gradient descent or iterative solution\napproaches, LDG is solved with a single eigen-decomposition. Thus, it scales\nbetter for datasets with a large number of feature dimensions or training\nexamples. We also adapt LDG to the transfer learning setting, and show that it\nachieves good performance when the test data distribution differs from that of\nthe training data.\n", "versions": [{"version": "v1", "created": "Mon, 18 Jun 2012 15:24:49 GMT"}], "update_date": "2012-06-22", "authors_parsed": [["Parrish", "Nathan", "", "University of Washington"], ["Gupta", "Maya", "", "University of\n  Washington"]]}, {"id": "1206.4654", "submitter": "Russell Greiner", "authors": "Siamak Ravanbakhsh (University of Alberta), Chun-Nam Yu (University of\n  Alberta), Russell Greiner (University of Alberta)", "title": "A Generalized Loop Correction Method for Approximate Inference in\n  Graphical Models", "comments": "ICML2012", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Belief Propagation (BP) is one of the most popular methods for inference in\nprobabilistic graphical models. BP is guaranteed to return the correct answer\nfor tree structures, but can be incorrect or non-convergent for loopy graphical\nmodels. Recently, several new approximate inference algorithms based on cavity\ndistribution have been proposed. These methods can account for the effect of\nloops by incorporating the dependency between BP messages. Alternatively,\nregion-based approximations (that lead to methods such as Generalized Belief\nPropagation) improve upon BP by considering interactions within small clusters\nof variables, thus taking small loops within these clusters into account. This\npaper introduces an approach, Generalized Loop Correction (GLC), that benefits\nfrom both of these types of loop correction. We show how GLC relates to these\ntwo families of inference methods, then provide empirical evidence that GLC\nworks effectively in general, and can be significantly more accurate than both\ncorrection schemes.\n", "versions": [{"version": "v1", "created": "Mon, 18 Jun 2012 15:25:04 GMT"}], "update_date": "2012-06-22", "authors_parsed": [["Ravanbakhsh", "Siamak", "", "University of Alberta"], ["Yu", "Chun-Nam", "", "University of\n  Alberta"], ["Greiner", "Russell", "", "University of Alberta"]]}, {"id": "1206.4656", "submitter": "Kiri Wagstaff", "authors": "Kiri Wagstaff (Jet Propulsion Laboratory)", "title": "Machine Learning that Matters", "comments": "ICML2012", "journal-ref": "Proceedings of the Twenty-Ninth International Conference on\n  Machine Learning (ICML), p. 529-536", "doi": null, "report-no": null, "categories": "cs.LG cs.AI stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Much of current machine learning (ML) research has lost its connection to\nproblems of import to the larger world of science and society. From this\nperspective, there exist glaring limitations in the data sets we investigate,\nthe metrics we employ for evaluation, and the degree to which results are\ncommunicated back to their originating domains. What changes are needed to how\nwe conduct research to increase the impact that ML has? We present six Impact\nChallenges to explicitly focus the field?s energy and attention, and we discuss\nexisting obstacles that must be addressed. We aim to inspire ongoing discussion\nand focus on ML that matters.\n", "versions": [{"version": "v1", "created": "Mon, 18 Jun 2012 15:26:13 GMT"}], "update_date": "2018-02-19", "authors_parsed": [["Wagstaff", "Kiri", "", "Jet Propulsion Laboratory"]]}, {"id": "1206.4658", "submitter": "Dongwoo Kim", "authors": "Dongwoo Kim (KAIST), Suin Kim (KAIST), Alice Oh (KAIST)", "title": "Dirichlet Process with Mixed Random Measures: A Nonparametric Topic\n  Model for Labeled Data", "comments": "ICML2012", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We describe a nonparametric topic model for labeled data. The model uses a\nmixture of random measures (MRM) as a base distribution of the Dirichlet\nprocess (DP) of the HDP framework, so we call it the DP-MRM. To model labeled\ndata, we define a DP distributed random measure for each label, and the\nresulting model generates an unbounded number of topics for each label. We\napply DP-MRM on single-labeled and multi-labeled corpora of documents and\ncompare the performance on label prediction with MedLDA, LDA-SVM, and\nLabeled-LDA. We further enhance the model by incorporating ddCRP and modeling\nmulti-labeled images for image segmentation and object labeling, comparing the\nperformance with nCuts and rddCRP.\n", "versions": [{"version": "v1", "created": "Mon, 18 Jun 2012 15:27:40 GMT"}], "update_date": "2012-06-22", "authors_parsed": [["Kim", "Dongwoo", "", "KAIST"], ["Kim", "Suin", "", "KAIST"], ["Oh", "Alice", "", "KAIST"]]}, {"id": "1206.4659", "submitter": "Jun Zhu", "authors": "Jun Zhu (Tsinghua University)", "title": "Max-Margin Nonparametric Latent Feature Models for Link Prediction", "comments": "ICML2012", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a max-margin nonparametric latent feature model, which unites the\nideas of max-margin learning and Bayesian nonparametrics to discover\ndiscriminative latent features for link prediction and automatically infer the\nunknown latent social dimension. By minimizing a hinge-loss using the linear\nexpectation operator, we can perform posterior inference efficiently without\ndealing with a highly nonlinear link likelihood function; by using a\nfully-Bayesian formulation, we can avoid tuning regularization constants.\nExperimental results on real datasets appear to demonstrate the benefits\ninherited from max-margin learning and fully-Bayesian nonparametric inference.\n", "versions": [{"version": "v1", "created": "Mon, 18 Jun 2012 15:27:56 GMT"}], "update_date": "2012-06-22", "authors_parsed": [["Zhu", "Jun", "", "Tsinghua University"]]}, {"id": "1206.4661", "submitter": "Aditya Menon", "authors": "Aditya Menon (UC San Diego), Xiaoqian Jiang (UC San Diego), Shankar\n  Vembu (University of Toronto), Charles Elkan (UC San Diego), Lucila\n  Ohno-Machado (UC San Diego)", "title": "Predicting accurate probabilities with a ranking loss", "comments": "ICML2012", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In many real-world applications of machine learning classifiers, it is\nessential to predict the probability of an example belonging to a particular\nclass. This paper proposes a simple technique for predicting probabilities\nbased on optimizing a ranking loss, followed by isotonic regression. This\nsemi-parametric technique offers both good ranking and regression performance,\nand models a richer set of probability distributions than statistical\nworkhorses such as logistic regression. We provide experimental results that\nshow the effectiveness of this technique on real-world applications of\nprobability prediction.\n", "versions": [{"version": "v1", "created": "Mon, 18 Jun 2012 15:30:13 GMT"}], "update_date": "2012-06-22", "authors_parsed": [["Menon", "Aditya", "", "UC San Diego"], ["Jiang", "Xiaoqian", "", "UC San Diego"], ["Vembu", "Shankar", "", "University of Toronto"], ["Elkan", "Charles", "", "UC San Diego"], ["Ohno-Machado", "Lucila", "", "UC San Diego"]]}, {"id": "1206.4663", "submitter": "Mark Reid", "authors": "Mark Reid (The Australian National University and NICTA), Robert\n  Williamson (The Australian National University and NICTA), Peng Sun (Tsinghua\n  University)", "title": "The Convexity and Design of Composite Multiclass Losses", "comments": "ICML2012", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider composite loss functions for multiclass prediction comprising a\nproper (i.e., Fisher-consistent) loss over probability distributions and an\ninverse link function. We establish conditions for their (strong) convexity and\nexplore the implications. We also show how the separation of concerns afforded\nby using this composite representation allows for the design of families of\nlosses with the same Bayes risk.\n", "versions": [{"version": "v1", "created": "Mon, 18 Jun 2012 15:30:52 GMT"}], "update_date": "2012-06-22", "authors_parsed": [["Reid", "Mark", "", "The Australian National University and NICTA"], ["Williamson", "Robert", "", "The Australian National University and NICTA"], ["Sun", "Peng", "", "Tsinghua\n  University"]]}, {"id": "1206.4664", "submitter": "Avraham Ruderman", "authors": "Avraham Ruderman (Australian National University and NICTA), Mark Reid\n  (Australian National University and NICTA), Dario Garcia-Garcia (Australian\n  National University and NICTA), James Petterson (NICTA)", "title": "Tighter Variational Representations of f-Divergences via Restriction to\n  Probability Measures", "comments": "ICML2012", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We show that the variational representations for f-divergences currently used\nin the literature can be tightened. This has implications to a number of\nmethods recently proposed based on this representation. As an example\napplication we use our tighter representation to derive a general f-divergence\nestimator based on two i.i.d. samples and derive the dual program for this\nestimator that performs well empirically. We also point out a connection\nbetween our estimator and MMD.\n", "versions": [{"version": "v1", "created": "Mon, 18 Jun 2012 15:31:13 GMT"}], "update_date": "2012-06-22", "authors_parsed": [["Ruderman", "Avraham", "", "Australian National University and NICTA"], ["Reid", "Mark", "", "Australian National University and NICTA"], ["Garcia-Garcia", "Dario", "", "Australian\n  National University and NICTA"], ["Petterson", "James", "", "NICTA"]]}, {"id": "1206.4665", "submitter": "Samuel Gershman", "authors": "Samuel Gershman (Princeton University), Matt Hoffman (Princeton\n  University), David Blei (Princeton University)", "title": "Nonparametric variational inference", "comments": "ICML2012", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Variational methods are widely used for approximate posterior inference.\nHowever, their use is typically limited to families of distributions that enjoy\nparticular conjugacy properties. To circumvent this limitation, we propose a\nfamily of variational approximations inspired by nonparametric kernel density\nestimation. The locations of these kernels and their bandwidth are treated as\nvariational parameters and optimized to improve an approximate lower bound on\nthe marginal likelihood of the data. Using multiple kernels allows the\napproximation to capture multiple modes of the posterior, unlike most other\nvariational approximations. We demonstrate the efficacy of the nonparametric\napproximation with a hierarchical logistic regression model and a nonlinear\nmatrix factorization model. We obtain predictive performance as good as or\nbetter than more specialized variational methods and sample-based\napproximations. The method is easy to apply to more general graphical models\nfor which standard variational methods are difficult to derive.\n", "versions": [{"version": "v1", "created": "Mon, 18 Jun 2012 15:32:05 GMT"}], "update_date": "2012-06-22", "authors_parsed": [["Gershman", "Samuel", "", "Princeton University"], ["Hoffman", "Matt", "", "Princeton\n  University"], ["Blei", "David", "", "Princeton University"]]}, {"id": "1206.4668", "submitter": "Andrew McGregor", "authors": "Mark McCartin-Lim (University of Massachusetts), Andrew McGregor\n  (University of Massachusetts), Rui Wang (University of Massachusetts)", "title": "Approximate Principal Direction Trees", "comments": "ICML2012", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.DS stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We introduce a new spatial data structure for high dimensional data called\nthe \\emph{approximate principal direction tree} (APD tree) that adapts to the\nintrinsic dimension of the data. Our algorithm ensures vector-quantization\naccuracy similar to that of computationally-expensive PCA trees with similar\ntime-complexity to that of lower-accuracy RP trees.\n  APD trees use a small number of power-method iterations to find splitting\nplanes for recursively partitioning the data. As such they provide a natural\ntrade-off between the running-time and accuracy achieved by RP and PCA trees.\nOur theoretical results establish a) strong performance guarantees regardless\nof the convergence rate of the power-method and b) that $O(\\log d)$ iterations\nsuffice to establish the guarantee of PCA trees when the intrinsic dimension is\n$d$. We demonstrate this trade-off and the efficacy of our data structure on\nboth the CPU and GPU.\n", "versions": [{"version": "v1", "created": "Mon, 18 Jun 2012 15:33:25 GMT"}], "update_date": "2012-06-22", "authors_parsed": [["McCartin-Lim", "Mark", "", "University of Massachusetts"], ["McGregor", "Andrew", "", "University of Massachusetts"], ["Wang", "Rui", "", "University of Massachusetts"]]}, {"id": "1206.4669", "submitter": "Sivaraman Balakrishnan", "authors": "Sivaraman Balakrishnan (Carnegie Mellon University), Kriti Puniyani\n  (Carnegie Mellon University), John Lafferty (Carnegie Mellon University)", "title": "Sparse Additive Functional and Kernel CCA", "comments": "ICML2012", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Canonical Correlation Analysis (CCA) is a classical tool for finding\ncorrelations among the components of two random vectors. In recent years, CCA\nhas been widely applied to the analysis of genomic data, where it is common for\nresearchers to perform multiple assays on a single set of patient samples.\nRecent work has proposed sparse variants of CCA to address the high\ndimensionality of such data. However, classical and sparse CCA are based on\nlinear models, and are thus limited in their ability to find general\ncorrelations. In this paper, we present two approaches to high-dimensional\nnonparametric CCA, building on recent developments in high-dimensional\nnonparametric regression. We present estimation procedures for both approaches,\nand analyze their theoretical properties in the high-dimensional setting. We\ndemonstrate the effectiveness of these procedures in discovering nonlinear\ncorrelations via extensive simulations, as well as through experiments with\ngenomic data.\n", "versions": [{"version": "v1", "created": "Mon, 18 Jun 2012 15:34:07 GMT"}], "update_date": "2012-06-22", "authors_parsed": [["Balakrishnan", "Sivaraman", "", "Carnegie Mellon University"], ["Puniyani", "Kriti", "", "Carnegie Mellon University"], ["Lafferty", "John", "", "Carnegie Mellon University"]]}, {"id": "1206.4671", "submitter": "Changyou Chen", "authors": "Changyou Chen (ANU & NICTA), Nan Ding (Purdue University), Wray\n  Buntine (NICTA)", "title": "Dependent Hierarchical Normalized Random Measures for Dynamic Topic\n  Modeling", "comments": "ICML2012", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We develop dependent hierarchical normalized random measures and apply them\nto dynamic topic modeling. The dependency arises via superposition, subsampling\nand point transition on the underlying Poisson processes of these measures. The\nmeasures used include normalised generalised Gamma processes that demonstrate\npower law properties, unlike Dirichlet processes used previously in dynamic\ntopic modeling. Inference for the model includes adapting a recently developed\nslice sampler to directly manipulate the underlying Poisson process.\nExperiments performed on news, blogs, academic and Twitter collections\ndemonstrate the technique gives superior perplexity over a number of previous\nmodels.\n", "versions": [{"version": "v1", "created": "Mon, 18 Jun 2012 15:35:02 GMT"}], "update_date": "2012-06-22", "authors_parsed": [["Chen", "Changyou", "", "ANU & NICTA"], ["Ding", "Nan", "", "Purdue University"], ["Buntine", "Wray", "", "NICTA"]]}, {"id": "1206.4672", "submitter": "Akshay Krishnamurthy", "authors": "Akshay Krishnamurthy (Carnegie Mellon University), Sivaraman\n  Balakrishnan (Carnegie Mellon University), Min Xu (Carnegie Mellon\n  University), Aarti Singh (Carnegie Mellon University)", "title": "Efficient Active Algorithms for Hierarchical Clustering", "comments": "ICML2012", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Advances in sensing technologies and the growth of the internet have resulted\nin an explosion in the size of modern datasets, while storage and processing\npower continue to lag behind. This motivates the need for algorithms that are\nefficient, both in terms of the number of measurements needed and running time.\nTo combat the challenges associated with large datasets, we propose a general\nframework for active hierarchical clustering that repeatedly runs an\noff-the-shelf clustering algorithm on small subsets of the data and comes with\nguarantees on performance, measurement complexity and runtime complexity. We\ninstantiate this framework with a simple spectral clustering algorithm and\nprovide concrete results on its performance, showing that, under some\nassumptions, this algorithm recovers all clusters of size ?(log n) using O(n\nlog^2 n) similarities and runs in O(n log^3 n) time for a dataset of n objects.\nThrough extensive experimentation we also demonstrate that this framework is\npractically alluring.\n", "versions": [{"version": "v1", "created": "Mon, 18 Jun 2012 15:35:20 GMT"}], "update_date": "2012-06-22", "authors_parsed": [["Krishnamurthy", "Akshay", "", "Carnegie Mellon University"], ["Balakrishnan", "Sivaraman", "", "Carnegie Mellon University"], ["Xu", "Min", "", "Carnegie Mellon\n  University"], ["Singh", "Aarti", "", "Carnegie Mellon University"]]}, {"id": "1206.4673", "submitter": "Junming Yin", "authors": "Junming Yin (Carnegie Mellon University), Xi Chen (Carnegie Mellon\n  University), Eric Xing (Carnegie Mellon University)", "title": "Group Sparse Additive Models", "comments": "ICML2012", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider the problem of sparse variable selection in nonparametric\nadditive models, with the prior knowledge of the structure among the covariates\nto encourage those variables within a group to be selected jointly. Previous\nworks either study the group sparsity in the parametric setting (e.g., group\nlasso), or address the problem in the non-parametric setting without exploiting\nthe structural information (e.g., sparse additive models). In this paper, we\npresent a new method, called group sparse additive models (GroupSpAM), which\ncan handle group sparsity in additive models. We generalize the l1/l2 norm to\nHilbert spaces as the sparsity-inducing penalty in GroupSpAM. Moreover, we\nderive a novel thresholding condition for identifying the functional sparsity\nat the group level, and propose an efficient block coordinate descent algorithm\nfor constructing the estimate. We demonstrate by simulation that GroupSpAM\nsubstantially outperforms the competing methods in terms of support recovery\nand prediction accuracy in additive models, and also conduct a comparative\nexperiment on a real breast cancer dataset.\n", "versions": [{"version": "v1", "created": "Mon, 18 Jun 2012 15:35:38 GMT"}], "update_date": "2012-06-22", "authors_parsed": [["Yin", "Junming", "", "Carnegie Mellon University"], ["Chen", "Xi", "", "Carnegie Mellon\n  University"], ["Xing", "Eric", "", "Carnegie Mellon University"]]}, {"id": "1206.4674", "submitter": "Stratis Ioannidis", "authors": "Amin Karbasi (EPFL), Stratis Ioannidis (Technicolor), laurent\n  Massoulie (Technicolor)", "title": "Comparison-Based Learning with Rank Nets", "comments": "ICML2012", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.DS stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider the problem of search through comparisons, where a user is\npresented with two candidate objects and reveals which is closer to her\nintended target. We study adaptive strategies for finding the target, that\nrequire knowledge of rank relationships but not actual distances between\nobjects. We propose a new strategy based on rank nets, and show that for target\ndistributions with a bounded doubling constant, it finds the target in a number\nof comparisons close to the entropy of the target distribution and, hence, of\nthe optimum. We extend these results to the case of noisy oracles, and compare\nthis strategy to prior art over multiple datasets.\n", "versions": [{"version": "v1", "created": "Mon, 18 Jun 2012 15:36:16 GMT"}], "update_date": "2012-06-22", "authors_parsed": [["Karbasi", "Amin", "", "EPFL"], ["Ioannidis", "Stratis", "", "Technicolor"], ["Massoulie", "laurent", "", "Technicolor"]]}, {"id": "1206.4676", "submitter": "Zhirong Yang", "authors": "Zhirong Yang (Aalto University), Erkki Oja (Aalto University)", "title": "Clustering by Low-Rank Doubly Stochastic Matrix Decomposition", "comments": "ICML2012", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV cs.NA stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Clustering analysis by nonnegative low-rank approximations has achieved\nremarkable progress in the past decade. However, most approximation approaches\nin this direction are still restricted to matrix factorization. We propose a\nnew low-rank learning method to improve the clustering performance, which is\nbeyond matrix factorization. The approximation is based on a two-step bipartite\nrandom walk through virtual cluster nodes, where the approximation is formed by\nonly cluster assigning probabilities. Minimizing the approximation error\nmeasured by Kullback-Leibler divergence is equivalent to maximizing the\nlikelihood of a discriminative model, which endows our method with a solid\nprobabilistic interpretation. The optimization is implemented by a relaxed\nMajorization-Minimization algorithm that is advantageous in finding good local\nminima. Furthermore, we point out that the regularized algorithm with Dirichlet\nprior only serves as initialization. Experimental results show that the new\nmethod has strong performance in clustering purity for various datasets,\nespecially for large-scale manifold data.\n", "versions": [{"version": "v1", "created": "Mon, 18 Jun 2012 15:36:49 GMT"}], "update_date": "2012-06-22", "authors_parsed": [["Yang", "Zhirong", "", "Aalto University"], ["Oja", "Erkki", "", "Aalto University"]]}, {"id": "1206.4677", "submitter": "Marthinus Du Plessis", "authors": "Marthinus Du Plessis (Tokyo Institute of Technology), Masashi Sugiyama\n  (Tokyo Institute of Technology)", "title": "Semi-Supervised Learning of Class Balance under Class-Prior Change by\n  Distribution Matching", "comments": "ICML2012", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In real-world classification problems, the class balance in the training\ndataset does not necessarily reflect that of the test dataset, which can cause\nsignificant estimation bias. If the class ratio of the test dataset is known,\ninstance re-weighting or resampling allows systematical bias correction.\nHowever, learning the class ratio of the test dataset is challenging when no\nlabeled data is available from the test domain. In this paper, we propose to\nestimate the class ratio in the test dataset by matching probability\ndistributions of training and test input data. We demonstrate the utility of\nthe proposed approach through experiments.\n", "versions": [{"version": "v1", "created": "Mon, 18 Jun 2012 15:37:07 GMT"}], "update_date": "2012-06-22", "authors_parsed": [["Plessis", "Marthinus Du", "", "Tokyo Institute of Technology"], ["Sugiyama", "Masashi", "", "Tokyo Institute of Technology"]]}, {"id": "1206.4678", "submitter": "Tomer Koren", "authors": "Elad Hazan (Technion), Tomer Koren (Technion)", "title": "Linear Regression with Limited Observation", "comments": "ICML2012", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider the most common variants of linear regression, including Ridge,\nLasso and Support-vector regression, in a setting where the learner is allowed\nto observe only a fixed number of attributes of each example at training time.\nWe present simple and efficient algorithms for these problems: for Lasso and\nRidge regression they need the same total number of attributes (up to\nconstants) as do full-information algorithms, for reaching a certain accuracy.\nFor Support-vector regression, we require exponentially less attributes\ncompared to the state of the art. By that, we resolve an open problem recently\nposed by Cesa-Bianchi et al. (2010). Experiments show the theoretical bounds to\nbe justified by superior performance compared to the state of the art.\n", "versions": [{"version": "v1", "created": "Mon, 18 Jun 2012 15:37:23 GMT"}], "update_date": "2012-06-22", "authors_parsed": [["Hazan", "Elad", "", "Technion"], ["Koren", "Tomer", "", "Technion"]]}, {"id": "1206.4679", "submitter": "Ryohei Fujimaki", "authors": "Ryohei Fujimaki (NEC Laboratories America), Kohei Hayashi (Nara\n  Institute of Science and Technology)", "title": "Factorized Asymptotic Bayesian Hidden Markov Models", "comments": "ICML2012", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper addresses the issue of model selection for hidden Markov models\n(HMMs). We generalize factorized asymptotic Bayesian inference (FAB), which has\nbeen recently developed for model selection on independent hidden variables\n(i.e., mixture models), for time-dependent hidden variables. As with FAB in\nmixture models, FAB for HMMs is derived as an iterative lower bound\nmaximization algorithm of a factorized information criterion (FIC). It\ninherits, from FAB for mixture models, several desirable properties for\nlearning HMMs, such as asymptotic consistency of FIC with marginal\nlog-likelihood, a shrinkage effect for hidden state selection, monotonic\nincrease of the lower FIC bound through the iterative optimization. Further, it\ndoes not have a tunable hyper-parameter, and thus its model selection process\ncan be fully automated. Experimental results shows that FAB outperforms\nstates-of-the-art variational Bayesian HMM and non-parametric Bayesian HMM in\nterms of model selection accuracy and computational efficiency.\n", "versions": [{"version": "v1", "created": "Mon, 18 Jun 2012 15:37:59 GMT"}], "update_date": "2012-06-22", "authors_parsed": [["Fujimaki", "Ryohei", "", "NEC Laboratories America"], ["Hayashi", "Kohei", "", "Nara\n  Institute of Science and Technology"]]}, {"id": "1206.4681", "submitter": "Patrick Pletscher", "authors": "Patrick Pletscher (ETH Zurich), Sharon Wulff (ETH Zurich)", "title": "LPQP for MAP: Putting LP Solvers to Better Use", "comments": "ICML2012", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  MAP inference for general energy functions remains a challenging problem.\nWhile most efforts are channeled towards improving the linear programming (LP)\nbased relaxation, this work is motivated by the quadratic programming (QP)\nrelaxation. We propose a novel MAP relaxation that penalizes the\nKullback-Leibler divergence between the LP pairwise auxiliary variables, and QP\nequivalent terms given by the product of the unaries. We develop two efficient\nalgorithms based on variants of this relaxation. The algorithms minimize the\nnon-convex objective using belief propagation and dual decomposition as\nbuilding blocks. Experiments on synthetic and real-world data show that the\nsolutions returned by our algorithms substantially improve over the LP\nrelaxation.\n", "versions": [{"version": "v1", "created": "Mon, 18 Jun 2012 15:40:11 GMT"}], "update_date": "2012-06-22", "authors_parsed": [["Pletscher", "Patrick", "", "ETH Zurich"], ["Wulff", "Sharon", "", "ETH Zurich"]]}, {"id": "1206.4682", "submitter": "Barnabas Poczos", "authors": "Barnabas Poczos (Carnegie Mellon University), Zoubin Ghahramani\n  (University of Cambridge), Jeff Schneider (Carnegie Mellon University)", "title": "Copula-based Kernel Dependency Measures", "comments": "ICML2012", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG math.ST stat.ML stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The paper presents a new copula based method for measuring dependence between\nrandom variables. Our approach extends the Maximum Mean Discrepancy to the\ncopula of the joint distribution. We prove that this approach has several\nadvantageous properties. Similarly to Shannon mutual information, the proposed\ndependence measure is invariant to any strictly increasing transformation of\nthe marginal variables. This is important in many applications, for example in\nfeature selection. The estimator is consistent, robust to outliers, and uses\nrank statistics only. We derive upper bounds on the convergence rate and\npropose independence tests too. We illustrate the theoretical contributions\nthrough a series of experiments in feature selection and low-dimensional\nembedding of distributions.\n", "versions": [{"version": "v1", "created": "Mon, 18 Jun 2012 15:40:32 GMT"}], "update_date": "2019-08-15", "authors_parsed": [["Poczos", "Barnabas", "", "Carnegie Mellon University"], ["Ghahramani", "Zoubin", "", "University of Cambridge"], ["Schneider", "Jeff", "", "Carnegie Mellon University"]]}, {"id": "1206.4686", "submitter": "Edwin Bonilla", "authors": "Edwin Bonilla (NICTA), Antonio Robles-Kelly (NICTA)", "title": "Discriminative Probabilistic Prototype Learning", "comments": "ICML2012", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper we propose a simple yet powerful method for learning\nrepresentations in supervised learning scenarios where each original input\ndatapoint is described by a set of vectors and their associated outputs may be\ngiven by soft labels indicating, for example, class probabilities. We represent\nan input datapoint as a mixture of probabilities over the corresponding set of\nfeature vectors where each probability indicates how likely each vector is to\nbelong to an unknown prototype pattern. We propose a probabilistic model that\nparameterizes these prototype patterns in terms of hidden variables and\ntherefore it can be trained with conventional approaches based on likelihood\nmaximization. More importantly, both the model parameters and the prototype\npatterns can be learned from data in a discriminative way. We show that our\nmodel can be seen as a probabilistic generalization of learning vector\nquantization (LVQ). We apply our method to the problems of shape\nclassification, hyperspectral imaging classification and people's work class\ncategorization, showing the superior performance of our method compared to the\nstandard prototype-based classification approach and other competitive\nbenchmark methods.\n", "versions": [{"version": "v1", "created": "Mon, 18 Jun 2012 15:42:34 GMT"}], "update_date": "2012-06-22", "authors_parsed": [["Bonilla", "Edwin", "", "NICTA"], ["Robles-Kelly", "Antonio", "", "NICTA"]]}, {"id": "1206.5036", "submitter": "Lin Yuan", "authors": "Lin Yuan, Sergey Kirshner and Robert Givan", "title": "Estimating Densities with Non-Parametric Exponential Families", "comments": "22 pages, 5 figures", "journal-ref": null, "doi": null, "report-no": "TR12-02, Department of Statistics, Purdue University", "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a novel approach for density estimation with exponential families\nfor the case when the true density may not fall within the chosen family. Our\napproach augments the sufficient statistics with features designed to\naccumulate probability mass in the neighborhood of the observed points,\nresulting in a non-parametric model similar to kernel density estimators. We\nshow that under mild conditions, the resulting model uses only the sufficient\nstatistics if the density is within the chosen exponential family, and\nasymptotically, it approximates densities outside of the chosen exponential\nfamily. Using the proposed approach, we modify the exponential random graph\nmodel, commonly used for modeling small-size graph distributions, to address\nthe well-known issue of model degeneracy.\n", "versions": [{"version": "v1", "created": "Fri, 22 Jun 2012 00:12:05 GMT"}, {"version": "v2", "created": "Thu, 6 Sep 2012 13:27:18 GMT"}], "update_date": "2012-09-07", "authors_parsed": [["Yuan", "Lin", ""], ["Kirshner", "Sergey", ""], ["Givan", "Robert", ""]]}, {"id": "1206.5102", "submitter": "Stevenn Volant", "authors": "Stevenn Volant, Caroline B\\'erard, Marie-Laure Martin-Magniette and\n  St\\'ephane Robin", "title": "Hidden Markov Models with mixtures as emission distributions", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In unsupervised classification, Hidden Markov Models (HMM) are used to\naccount for a neighborhood structure between observations. The emission\ndistributions are often supposed to belong to some parametric family. In this\npaper, a semiparametric modeling where the emission distributions are a mixture\nof parametric distributions is proposed to get a higher flexibility. We show\nthat the classical EM algorithm can be adapted to infer the model parameters.\nFor the initialisation step, starting from a large number of components, a\nhierarchical method to combine them into the hidden states is proposed. Three\nlikelihood-based criteria to select the components to be combined are\ndiscussed. To estimate the number of hidden states, BIC-like criteria are\nderived. A simulation study is carried out both to determine the best\ncombination between the merging criteria and the model selection criteria and\nto evaluate the accuracy of classification. The proposed method is also\nillustrated using a biological dataset from the model plant Arabidopsis\nthaliana. A R package HMMmix is freely available on the CRAN.\n", "versions": [{"version": "v1", "created": "Fri, 22 Jun 2012 10:24:55 GMT"}], "update_date": "2012-06-25", "authors_parsed": [["Volant", "Stevenn", ""], ["B\u00e9rard", "Caroline", ""], ["Martin-Magniette", "Marie-Laure", ""], ["Robin", "St\u00e9phane", ""]]}, {"id": "1206.5162", "submitter": "James Hensman", "authors": "James Hensman, Magnus Rattray and Neil D. Lawrence", "title": "Fast Variational Inference in the Conjugate Exponential Family", "comments": "Accepted at NIPS 2012", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a general method for deriving collapsed variational inference\nalgo- rithms for probabilistic models in the conjugate exponential family. Our\nmethod unifies many existing approaches to collapsed variational inference. Our\ncollapsed variational inference leads to a new lower bound on the marginal\nlikelihood. We exploit the information geometry of the bound to derive much\nfaster optimization methods based on conjugate gradients for these models. Our\napproach is very general and is easily applied to any model where the mean\nfield update equations have been derived. Empirically we show significant\nspeed-ups for probabilistic models optimized using our bound.\n", "versions": [{"version": "v1", "created": "Fri, 22 Jun 2012 14:36:15 GMT"}, {"version": "v2", "created": "Tue, 4 Dec 2012 19:35:34 GMT"}], "update_date": "2012-12-05", "authors_parsed": [["Hensman", "James", ""], ["Rattray", "Magnus", ""], ["Lawrence", "Neil D.", ""]]}, {"id": "1206.5240", "submitter": "Gholam Reza Haffari", "authors": "Gholam Reza Haffari, Anoop Sarkar", "title": "Analysis of Semi-Supervised Learning with the Yarowsky Algorithm", "comments": "Appears in Proceedings of the Twenty-Third Conference on Uncertainty\n  in Artificial Intelligence (UAI2007)", "journal-ref": null, "doi": null, "report-no": "UAI-P-2007-PG-159-166", "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The Yarowsky algorithm is a rule-based semi-supervised learning algorithm\nthat has been successfully applied to some problems in computational\nlinguistics. The algorithm was not mathematically well understood until (Abney\n2004) which analyzed some specific variants of the algorithm, and also proposed\nsome new algorithms for bootstrapping. In this paper, we extend Abney's work\nand show that some of his proposed algorithms actually optimize (an upper-bound\non) an objective function based on a new definition of cross-entropy which is\nbased on a particular instantiation of the Bregman distance between probability\ndistributions. Moreover, we suggest some new algorithms for rule-based\nsemi-supervised learning and show connections with harmonic functions and\nminimum multi-way cuts in graph-based semi-supervised learning.\n", "versions": [{"version": "v1", "created": "Wed, 20 Jun 2012 14:52:04 GMT"}], "update_date": "2012-06-26", "authors_parsed": [["Haffari", "Gholam Reza", ""], ["Sarkar", "Anoop", ""]]}, {"id": "1206.5241", "submitter": "Roger Grosse", "authors": "Roger Grosse, Rajat Raina, Helen Kwong, Andrew Y. Ng", "title": "Shift-Invariance Sparse Coding for Audio Classification", "comments": "Appears in Proceedings of the Twenty-Third Conference on Uncertainty\n  in Artificial Intelligence (UAI2007)", "journal-ref": null, "doi": null, "report-no": "UAI-P-2007-PG-149-158", "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Sparse coding is an unsupervised learning algorithm that learns a succinct\nhigh-level representation of the inputs given only unlabeled data; it\nrepresents each input as a sparse linear combination of a set of basis\nfunctions. Originally applied to modeling the human visual cortex, sparse\ncoding has also been shown to be useful for self-taught learning, in which the\ngoal is to solve a supervised classification task given access to additional\nunlabeled data drawn from different classes than that in the supervised\nlearning problem. Shift-invariant sparse coding (SISC) is an extension of\nsparse coding which reconstructs a (usually time-series) input using all of the\nbasis functions in all possible shifts. In this paper, we present an efficient\nalgorithm for learning SISC bases. Our method is based on iteratively solving\ntwo large convex optimization problems: The first, which computes the linear\ncoefficients, is an L1-regularized linear least squares problem with\npotentially hundreds of thousands of variables. Existing methods typically use\na heuristic to select a small subset of the variables to optimize, but we\npresent a way to efficiently compute the exact solution. The second, which\nsolves for bases, is a constrained linear least squares problem. By optimizing\nover complex-valued variables in the Fourier domain, we reduce the coupling\nbetween the different variables, allowing the problem to be solved efficiently.\nWe show that SISC's learned high-level representations of speech and music\nprovide useful features for classification tasks within those domains. When\napplied to classification, under certain conditions the learned features\noutperform state of the art spectral and cepstral features.\n", "versions": [{"version": "v1", "created": "Wed, 20 Jun 2012 14:52:49 GMT"}], "update_date": "2012-06-26", "authors_parsed": [["Grosse", "Roger", ""], ["Raina", "Rajat", ""], ["Kwong", "Helen", ""], ["Ng", "Andrew Y.", ""]]}, {"id": "1206.5243", "submitter": "Amir Globerson", "authors": "Amir Globerson, Tommi S. Jaakkola", "title": "Convergent Propagation Algorithms via Oriented Trees", "comments": "Appears in Proceedings of the Twenty-Third Conference on Uncertainty\n  in Artificial Intelligence (UAI2007)", "journal-ref": null, "doi": null, "report-no": "UAI-P-2007-PG-133-140", "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Inference problems in graphical models are often approximated by casting them\nas constrained optimization problems. Message passing algorithms, such as\nbelief propagation, have previously been suggested as methods for solving these\noptimization problems. However, there are few convergence guarantees for such\nalgorithms, and the algorithms are therefore not guaranteed to solve the\ncorresponding optimization problem. Here we present an oriented tree\ndecomposition algorithm that is guaranteed to converge to the global optimum of\nthe Tree-Reweighted (TRW) variational problem. Our algorithm performs local\nupdates in the convex dual of the TRW problem - an unconstrained generalized\ngeometric program. Primal updates, also local, correspond to oriented\nreparametrization operations that leave the distribution intact.\n", "versions": [{"version": "v1", "created": "Wed, 20 Jun 2012 14:53:26 GMT"}], "update_date": "2012-06-26", "authors_parsed": [["Globerson", "Amir", ""], ["Jaakkola", "Tommi S.", ""]]}, {"id": "1206.5247", "submitter": "Daniel Eaton", "authors": "Daniel Eaton, Kevin Murphy", "title": "Bayesian structure learning using dynamic programming and MCMC", "comments": "Appears in Proceedings of the Twenty-Third Conference on Uncertainty\n  in Artificial Intelligence (UAI2007)", "journal-ref": null, "doi": null, "report-no": "UAI-P-2007-PG-101-108", "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  MCMC methods for sampling from the space of DAGs can mix poorly due to the\nlocal nature of the proposals that are commonly used. It has been shown that\nsampling from the space of node orders yields better results [FK03, EW06].\nRecently, Koivisto and Sood showed how one can analytically marginalize over\norders using dynamic programming (DP) [KS04, Koi06]. Their method computes the\nexact marginal posterior edge probabilities, thus avoiding the need for MCMC.\nUnfortunately, there are four drawbacks to the DP technique: it can only use\nmodular priors, it can only compute posteriors over modular features, it is\ndifficult to compute a predictive density, and it takes exponential time and\nspace. We show how to overcome the first three of these problems by using the\nDP algorithm as a proposal distribution for MCMC in DAG space. We show that\nthis hybrid technique converges to the posterior faster than other methods,\nresulting in more accurate structure learning and higher predictive likelihoods\non test data.\n", "versions": [{"version": "v1", "created": "Wed, 20 Jun 2012 14:54:43 GMT"}], "update_date": "2012-06-26", "authors_parsed": [["Eaton", "Daniel", ""], ["Murphy", "Kevin", ""]]}, {"id": "1206.5248", "submitter": "Joshua Dillon", "authors": "Joshua Dillon, Yi Mao, Guy Lebanon, Jian Zhang", "title": "Statistical Translation, Heat Kernels and Expected Distances", "comments": "Appears in Proceedings of the Twenty-Third Conference on Uncertainty\n  in Artificial Intelligence (UAI2007)", "journal-ref": null, "doi": null, "report-no": "UAI-P-2007-PG-93-100", "categories": "cs.LG cs.CV cs.IR stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  High dimensional structured data such as text and images is often poorly\nunderstood and misrepresented in statistical modeling. The standard histogram\nrepresentation suffers from high variance and performs poorly in general. We\nexplore novel connections between statistical translation, heat kernels on\nmanifolds and graphs, and expected distances. These connections provide a new\nframework for unsupervised metric learning for text documents. Experiments\nindicate that the resulting distances are generally superior to their more\nstandard counterparts.\n", "versions": [{"version": "v1", "created": "Wed, 20 Jun 2012 14:55:04 GMT"}], "update_date": "2012-06-26", "authors_parsed": [["Dillon", "Joshua", ""], ["Mao", "Yi", ""], ["Lebanon", "Guy", ""], ["Zhang", "Jian", ""]]}, {"id": "1206.5261", "submitter": "David S. Rosenberg", "authors": "David S. Rosenberg, Dan Klein, Ben Taskar", "title": "Mixture-of-Parents Maximum Entropy Markov Models", "comments": "Appears in Proceedings of the Twenty-Third Conference on Uncertainty\n  in Artificial Intelligence (UAI2007)", "journal-ref": null, "doi": null, "report-no": "UAI-P-2007-PG-318-325", "categories": "cs.LG cs.AI stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present the mixture-of-parents maximum entropy Markov model (MoP-MEMM), a\nclass of directed graphical models extending MEMMs. The MoP-MEMM allows\ntractable incorporation of long-range dependencies between nodes by restricting\nthe conditional distribution of each node to be a mixture of distributions\ngiven the parents. We show how to efficiently compute the exact marginal\nposterior node distributions, regardless of the range of the dependencies. This\nenables us to model non-sequential correlations present within text documents,\nas well as between interconnected documents, such as hyperlinked web pages. We\napply the MoP-MEMM to a named entity recognition task and a web page\nclassification task. In each, our model shows significant improvement over the\nbasic MEMM, and is competitive with other long-range sequence models that use\napproximate inference.\n", "versions": [{"version": "v1", "created": "Wed, 20 Jun 2012 15:00:46 GMT"}], "update_date": "2012-06-26", "authors_parsed": [["Rosenberg", "David S.", ""], ["Klein", "Dan", ""], ["Taskar", "Ben", ""]]}, {"id": "1206.5263", "submitter": "Jose M. Pena", "authors": "Jose M. Pena", "title": "Reading Dependencies from Polytree-Like Bayesian Networks", "comments": "Appears in Proceedings of the Twenty-Third Conference on Uncertainty\n  in Artificial Intelligence (UAI2007)", "journal-ref": null, "doi": null, "report-no": "UAI-P-2007-PG-303-309", "categories": "cs.AI cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a graphical criterion for reading dependencies from the minimal\ndirected independence map G of a graphoid p when G is a polytree and p\nsatisfies composition and weak transitivity. We prove that the criterion is\nsound and complete. We argue that assuming composition and weak transitivity is\nnot too restrictive.\n", "versions": [{"version": "v1", "created": "Wed, 20 Jun 2012 15:01:43 GMT"}], "update_date": "2012-06-26", "authors_parsed": [["Pena", "Jose M.", ""]]}, {"id": "1206.5264", "submitter": "Gergely Neu", "authors": "Gergely Neu, Csaba Szepesvari", "title": "Apprenticeship Learning using Inverse Reinforcement Learning and\n  Gradient Methods", "comments": "Appears in Proceedings of the Twenty-Third Conference on Uncertainty\n  in Artificial Intelligence (UAI2007)", "journal-ref": null, "doi": null, "report-no": "UAI-P-2007-PG-295-302", "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper we propose a novel gradient algorithm to learn a policy from an\nexpert's observed behavior assuming that the expert behaves optimally with\nrespect to some unknown reward function of a Markovian Decision Problem. The\nalgorithm's aim is to find a reward function such that the resulting optimal\npolicy matches well the expert's observed behavior. The main difficulty is that\nthe mapping from the parameters to policies is both nonsmooth and highly\nredundant. Resorting to subdifferentials solves the first difficulty, while the\nsecond one is over- come by computing natural gradients. We tested the proposed\nmethod in two artificial domains and found it to be more reliable and efficient\nthan some previous methods.\n", "versions": [{"version": "v1", "created": "Wed, 20 Jun 2012 15:02:01 GMT"}], "update_date": "2012-06-26", "authors_parsed": [["Neu", "Gergely", ""], ["Szepesvari", "Csaba", ""]]}, {"id": "1206.5265", "submitter": "Marina Meila", "authors": "Marina Meila, Kapil Phadnis, Arthur Patterson, Jeff A. Bilmes", "title": "Consensus ranking under the exponential model", "comments": "Appears in Proceedings of the Twenty-Third Conference on Uncertainty\n  in Artificial Intelligence (UAI2007)", "journal-ref": null, "doi": null, "report-no": "UAI-P-2007-PG-285-294", "categories": "cs.LG cs.AI stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We analyze the generalized Mallows model, a popular exponential model over\nrankings. Estimating the central (or consensus) ranking from data is NP-hard.\nWe obtain the following new results: (1) We show that search methods can\nestimate both the central ranking pi0 and the model parameters theta exactly.\nThe search is n! in the worst case, but is tractable when the true distribution\nis concentrated around its mode; (2) We show that the generalized Mallows model\nis jointly exponential in (pi0; theta), and introduce the conjugate prior for\nthis model class; (3) The sufficient statistics are the pairwise marginal\nprobabilities that item i is preferred to item j. Preliminary experiments\nconfirm the theoretical predictions and compare the new algorithm and existing\nheuristics.\n", "versions": [{"version": "v1", "created": "Wed, 20 Jun 2012 15:02:29 GMT"}], "update_date": "2012-06-26", "authors_parsed": [["Meila", "Marina", ""], ["Phadnis", "Kapil", ""], ["Patterson", "Arthur", ""], ["Bilmes", "Jeff A.", ""]]}, {"id": "1206.5267", "submitter": "Benjamin Marlin", "authors": "Benjamin Marlin, Richard S. Zemel, Sam Roweis, Malcolm Slaney", "title": "Collaborative Filtering and the Missing at Random Assumption", "comments": "Appears in Proceedings of the Twenty-Third Conference on Uncertainty\n  in Artificial Intelligence (UAI2007)", "journal-ref": null, "doi": null, "report-no": "UAI-P-2007-PG-267-275", "categories": "cs.LG cs.IR stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Rating prediction is an important application, and a popular research topic\nin collaborative filtering. However, both the validity of learning algorithms,\nand the validity of standard testing procedures rest on the assumption that\nmissing ratings are missing at random (MAR). In this paper we present the\nresults of a user study in which we collect a random sample of ratings from\ncurrent users of an online radio service. An analysis of the rating data\ncollected in the study shows that the sample of random ratings has markedly\ndifferent properties than ratings of user-selected songs. When asked to report\non their own rating behaviour, a large number of users indicate they believe\ntheir opinion of a song does affect whether they choose to rate that song, a\nviolation of the MAR condition. Finally, we present experimental results\nshowing that incorporating an explicit model of the missing data mechanism can\nlead to significant improvements in prediction performance on the random sample\nof ratings.\n", "versions": [{"version": "v1", "created": "Wed, 20 Jun 2012 15:03:41 GMT"}], "update_date": "2012-06-26", "authors_parsed": [["Marlin", "Benjamin", ""], ["Zemel", "Richard S.", ""], ["Roweis", "Sam", ""], ["Slaney", "Malcolm", ""]]}, {"id": "1206.5270", "submitter": "Wei Li", "authors": "Wei Li, David Blei, Andrew McCallum", "title": "Nonparametric Bayes Pachinko Allocation", "comments": "Appears in Proceedings of the Twenty-Third Conference on Uncertainty\n  in Artificial Intelligence (UAI2007)", "journal-ref": null, "doi": null, "report-no": "UAI-P-2007-PG-243-250", "categories": "cs.IR cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recent advances in topic models have explored complicated structured\ndistributions to represent topic correlation. For example, the pachinko\nallocation model (PAM) captures arbitrary, nested, and possibly sparse\ncorrelations between topics using a directed acyclic graph (DAG). While PAM\nprovides more flexibility and greater expressive power than previous models\nlike latent Dirichlet allocation (LDA), it is also more difficult to determine\nthe appropriate topic structure for a specific dataset. In this paper, we\npropose a nonparametric Bayesian prior for PAM based on a variant of the\nhierarchical Dirichlet process (HDP). Although the HDP can capture topic\ncorrelations defined by nested data structure, it does not automatically\ndiscover such correlations from unstructured data. By assuming an HDP-based\nprior for PAM, we are able to learn both the number of topics and how the\ntopics are correlated. We evaluate our model on synthetic and real-world text\ndatasets, and show that nonparametric PAM achieves performance matching the\nbest of PAM without manually tuning the number of topics.\n", "versions": [{"version": "v1", "created": "Wed, 20 Jun 2012 15:04:47 GMT"}], "update_date": "2012-06-26", "authors_parsed": [["Li", "Wei", ""], ["Blei", "David", ""], ["McCallum", "Andrew", ""]]}, {"id": "1206.5274", "submitter": "Ashish Kapoor", "authors": "Ashish Kapoor, Eric J. Horvitz", "title": "On Discarding, Caching, and Recalling Samples in Active Learning", "comments": "Appears in Proceedings of the Twenty-Third Conference on Uncertainty\n  in Artificial Intelligence (UAI2007)", "journal-ref": null, "doi": null, "report-no": "UAI-P-2007-PG-209-216", "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We address challenges of active learning under scarce informational resources\nin non-stationary environments. In real-world settings, data labeled and\nintegrated into a predictive model may become invalid over time. However, the\ndata can become informative again with switches in context and such changes may\nindicate unmodeled cyclic or other temporal dynamics. We explore principles for\ndiscarding, caching, and recalling labeled data points in active learning based\non computations of value of information. We review key concepts and study the\nvalue of the methods via investigations of predictive performance and costs of\nacquiring data for simulated and real-world data sets.\n", "versions": [{"version": "v1", "created": "Wed, 20 Jun 2012 15:06:08 GMT"}], "update_date": "2012-06-26", "authors_parsed": [["Kapoor", "Ashish", ""], ["Horvitz", "Eric J.", ""]]}, {"id": "1206.5277", "submitter": "Alexander T. Ihler", "authors": "Alexander T. Ihler", "title": "Accuracy Bounds for Belief Propagation", "comments": "Appears in Proceedings of the Twenty-Third Conference on Uncertainty\n  in Artificial Intelligence (UAI2007)", "journal-ref": null, "doi": null, "report-no": "UAI-P-2007-PG-183-190", "categories": "cs.AI cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The belief propagation (BP) algorithm is widely applied to perform\napproximate inference on arbitrary graphical models, in part due to its\nexcellent empirical properties and performance. However, little is known\ntheoretically about when this algorithm will perform well. Using recent\nanalysis of convergence and stability properties in BP and new results on\napproximations in binary systems, we derive a bound on the error in BP's\nestimates for pairwise Markov random fields over discrete valued random\nvariables. Our bound is relatively simple to compute, and compares favorably\nwith a previous method of bounding the accuracy of BP.\n", "versions": [{"version": "v1", "created": "Wed, 20 Jun 2012 15:07:42 GMT"}], "update_date": "2012-06-26", "authors_parsed": [["Ihler", "Alexander T.", ""]]}, {"id": "1206.5278", "submitter": "Michael P. Holmes", "authors": "Michael P. Holmes, Alexander G. Gray, Charles Lee Isbell", "title": "Fast Nonparametric Conditional Density Estimation", "comments": "Appears in Proceedings of the Twenty-Third Conference on Uncertainty\n  in Artificial Intelligence (UAI2007)", "journal-ref": null, "doi": null, "report-no": "UAI-P-2007-PG-175-182", "categories": "stat.ME cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Conditional density estimation generalizes regression by modeling a full\ndensity f(yjx) rather than only the expected value E(yjx). This is important\nfor many tasks, including handling multi-modality and generating prediction\nintervals. Though fundamental and widely applicable, nonparametric conditional\ndensity estimators have received relatively little attention from statisticians\nand little or none from the machine learning community. None of that work has\nbeen applied to greater than bivariate data, presumably due to the\ncomputational difficulty of data-driven bandwidth selection. We describe the\ndouble kernel conditional density estimator and derive fast dual-tree-based\nalgorithms for bandwidth selection using a maximum likelihood criterion. These\ntechniques give speedups of up to 3.8 million in our experiments, and enable\nthe first applications to previously intractable large multivariate datasets,\nincluding a redshift prediction problem from the Sloan Digital Sky Survey.\n", "versions": [{"version": "v1", "created": "Wed, 20 Jun 2012 15:08:36 GMT"}], "update_date": "2012-06-26", "authors_parsed": [["Holmes", "Michael P.", ""], ["Gray", "Alexander G.", ""], ["Isbell", "Charles Lee", ""]]}, {"id": "1206.5281", "submitter": "Brian D. Ziebart", "authors": "Brian D. Ziebart, Anind K. Dey, J Andrew Bagnell", "title": "Learning Selectively Conditioned Forest Structures with Applications to\n  DBNs and Classification", "comments": "Appears in Proceedings of the Twenty-Third Conference on Uncertainty\n  in Artificial Intelligence (UAI2007)", "journal-ref": null, "doi": null, "report-no": "UAI-P-2007-PG-458-465", "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Dealing with uncertainty in Bayesian Network structures using maximum a\nposteriori (MAP) estimation or Bayesian Model Averaging (BMA) is often\nintractable due to the superexponential number of possible directed, acyclic\ngraphs. When the prior is decomposable, two classes of graphs where efficient\nlearning can take place are tree structures, and fixed-orderings with limited\nin-degree. We show how MAP estimates and BMA for selectively conditioned\nforests (SCF), a combination of these two classes, can be computed efficiently\nfor ordered sets of variables. We apply SCFs to temporal data to learn Dynamic\nBayesian Networks having an intra-timestep forest and inter-timestep limited\nin-degree structure, improving model accuracy over DBNs without the combination\nof structures. We also apply SCFs to Bayes Net classification to learn\nselective forest augmented Naive Bayes classifiers. We argue that the built-in\nfeature selection of selective augmented Bayes classifiers makes them\npreferable to similar non-selective classifiers based on empirical evidence.\n", "versions": [{"version": "v1", "created": "Wed, 20 Jun 2012 15:12:35 GMT"}], "update_date": "2012-06-26", "authors_parsed": [["Ziebart", "Brian D.", ""], ["Dey", "Anind K.", ""], ["Bagnell", "J Andrew", ""]]}, {"id": "1206.5282", "submitter": "Jiji Zhang", "authors": "Jiji Zhang", "title": "A Characterization of Markov Equivalence Classes for Directed Acyclic\n  Graphs with Latent Variables", "comments": "Appears in Proceedings of the Twenty-Third Conference on Uncertainty\n  in Artificial Intelligence (UAI2007)", "journal-ref": null, "doi": null, "report-no": "UAI-P-2007-PG-450-457", "categories": "stat.ME cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Different directed acyclic graphs (DAGs) may be Markov equivalent in the\nsense that they entail the same conditional independence relations among the\nobserved variables. Meek (1995) characterizes Markov equivalence classes for\nDAGs (with no latent variables) by presenting a set of orientation rules that\ncan correctly identify all arrow orientations shared by all DAGs in a Markov\nequivalence class, given a member of that class. For DAG models with latent\nvariables, maximal ancestral graphs (MAGs) provide a neat representation that\nfacilitates model search. Earlier work (Ali et al. 2005) has identified a set\nof orientation rules sufficient to construct all arrowheads common to a Markov\nequivalence class of MAGs. In this paper, we provide extra rules sufficient to\nconstruct all common tails as well. We end up with a set of orientation rules\nsound and complete for identifying commonalities across a Markov equivalence\nclass of MAGs, which is particularly useful for causal inference.\n", "versions": [{"version": "v1", "created": "Wed, 20 Jun 2012 15:14:16 GMT"}], "update_date": "2012-06-26", "authors_parsed": [["Zhang", "Jiji", ""]]}, {"id": "1206.5283", "submitter": "Liu Yang", "authors": "Liu Yang, Rong Jin, Rahul Sukthankar", "title": "Bayesian Active Distance Metric Learning", "comments": "Appears in Proceedings of the Twenty-Third Conference on Uncertainty\n  in Artificial Intelligence (UAI2007)", "journal-ref": null, "doi": null, "report-no": "UAI-P-2007-PG-442-449", "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Distance metric learning is an important component for many tasks, such as\nstatistical classification and content-based image retrieval. Existing\napproaches for learning distance metrics from pairwise constraints typically\nsuffer from two major problems. First, most algorithms only offer point\nestimation of the distance metric and can therefore be unreliable when the\nnumber of training examples is small. Second, since these algorithms generally\nselect their training examples at random, they can be inefficient if labeling\neffort is limited. This paper presents a Bayesian framework for distance metric\nlearning that estimates a posterior distribution for the distance metric from\nlabeled pairwise constraints. We describe an efficient algorithm based on the\nvariational method for the proposed Bayesian approach. Furthermore, we apply\nthe proposed Bayesian framework to active distance metric learning by selecting\nthose unlabeled example pairs with the greatest uncertainty in relative\ndistance. Experiments in classification demonstrate that the proposed framework\nachieves higher classification accuracy and identifies more informative\ntraining examples than the non-Bayesian approach and state-of-the-art distance\nmetric learning algorithms.\n", "versions": [{"version": "v1", "created": "Wed, 20 Jun 2012 15:14:55 GMT"}], "update_date": "2012-06-26", "authors_parsed": [["Yang", "Liu", ""], ["Jin", "Rong", ""], ["Sukthankar", "Rahul", ""]]}, {"id": "1206.5286", "submitter": "Yair Weiss", "authors": "Yair Weiss, Chen Yanover, Talya Meltzer", "title": "MAP Estimation, Linear Programming and Belief Propagation with Convex\n  Free Energies", "comments": "Appears in Proceedings of the Twenty-Third Conference on Uncertainty\n  in Artificial Intelligence (UAI2007)", "journal-ref": null, "doi": null, "report-no": "UAI-P-2007-PG-416-425", "categories": "cs.AI cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Finding the most probable assignment (MAP) in a general graphical model is\nknown to be NP hard but good approximations have been attained with max-product\nbelief propagation (BP) and its variants. In particular, it is known that using\nBP on a single-cycle graph or tree reweighted BP on an arbitrary graph will\ngive the MAP solution if the beliefs have no ties. In this paper we extend the\nsetting under which BP can be used to provably extract the MAP. We define\nConvex BP as BP algorithms based on a convex free energy approximation and show\nthat this class includes ordinary BP with single-cycle, tree reweighted BP and\nmany other BP variants. We show that when there are no ties, fixed-points of\nconvex max-product BP will provably give the MAP solution. We also show that\nconvex sum-product BP at sufficiently small temperatures can be used to solve\nlinear programs that arise from relaxing the MAP problem. Finally, we derive a\nnovel condition that allows us to derive the MAP solution even if some of the\nconvex BP beliefs have ties. In experiments, we show that our theorems allow us\nto find the MAP in many real-world instances of graphical models where exact\ninference using junction-tree is impossible.\n", "versions": [{"version": "v1", "created": "Wed, 20 Jun 2012 15:16:08 GMT"}], "update_date": "2012-06-26", "authors_parsed": [["Weiss", "Yair", ""], ["Yanover", "Chen", ""], ["Meltzer", "Talya", ""]]}, {"id": "1206.5290", "submitter": "Umar Syed", "authors": "Umar Syed, Robert E. Schapire", "title": "Imitation Learning with a Value-Based Prior", "comments": "Appears in Proceedings of the Twenty-Third Conference on Uncertainty\n  in Artificial Intelligence (UAI2007)", "journal-ref": null, "doi": null, "report-no": "UAI-P-2007-PG-384-391", "categories": "cs.LG cs.AI stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The goal of imitation learning is for an apprentice to learn how to behave in\na stochastic environment by observing a mentor demonstrating the correct\nbehavior. Accurate prior knowledge about the correct behavior can reduce the\nneed for demonstrations from the mentor. We present a novel approach to\nencoding prior knowledge about the correct behavior, where we assume that this\nprior knowledge takes the form of a Markov Decision Process (MDP) that is used\nby the apprentice as a rough and imperfect model of the mentor's behavior.\nSpecifically, taking a Bayesian approach, we treat the value of a policy in\nthis modeling MDP as the log prior probability of the policy. In other words,\nwe assume a priori that the mentor's behavior is likely to be a high value\npolicy in the modeling MDP, though quite possibly different from the optimal\npolicy. We describe an efficient algorithm that, given a modeling MDP and a set\nof demonstrations by a mentor, provably converges to a stationary point of the\nlog posterior of the mentor's policy, where the posterior is computed with\nrespect to the \"value based\" prior. We also present empirical evidence that\nthis prior does in fact speed learning of the mentor's policy, and is an\nimprovement in our experiments over similar previous methods.\n", "versions": [{"version": "v1", "created": "Wed, 20 Jun 2012 15:18:02 GMT"}], "update_date": "2012-06-26", "authors_parsed": [["Syed", "Umar", ""], ["Schapire", "Robert E.", ""]]}, {"id": "1206.5291", "submitter": "Charles Sutton", "authors": "Charles Sutton, Andrew McCallum", "title": "Improved Dynamic Schedules for Belief Propagation", "comments": "Appears in Proceedings of the Twenty-Third Conference on Uncertainty\n  in Artificial Intelligence (UAI2007)", "journal-ref": null, "doi": null, "report-no": "UAI-P-2007-PG-376-383", "categories": "cs.LG cs.AI stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Belief propagation and its variants are popular methods for approximate\ninference, but their running time and even their convergence depend greatly on\nthe schedule used to send the messages. Recently, dynamic update schedules have\nbeen shown to converge much faster on hard networks than static schedules,\nnamely the residual BP schedule of Elidan et al. [2006]. But that RBP algorithm\nwastes message updates: many messages are computed solely to determine their\npriority, and are never actually performed. In this paper, we show that\nestimating the residual, rather than calculating it directly, leads to\nsignificant decreases in the number of messages required for convergence, and\nin the total running time. The residual is estimated using an upper bound based\non recent work on message errors in BP. On both synthetic and real-world\nnetworks, this dramatically decreases the running time of BP, in some cases by\na factor of five, without affecting the quality of the solution.\n", "versions": [{"version": "v1", "created": "Wed, 20 Jun 2012 15:18:24 GMT"}], "update_date": "2012-06-26", "authors_parsed": [["Sutton", "Charles", ""], ["McCallum", "Andrew", ""]]}, {"id": "1206.5293", "submitter": "Tomi Silander", "authors": "Tomi Silander, Petri Kontkanen, Petri Myllymaki", "title": "On Sensitivity of the MAP Bayesian Network Structure to the Equivalent\n  Sample Size Parameter", "comments": "Appears in Proceedings of the Twenty-Third Conference on Uncertainty\n  in Artificial Intelligence (UAI2007)", "journal-ref": null, "doi": null, "report-no": "UAI-P-2007-PG-360-367", "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  BDeu marginal likelihood score is a popular model selection criterion for\nselecting a Bayesian network structure based on sample data. This\nnon-informative scoring criterion assigns same score for network structures\nthat encode same independence statements. However, before applying the BDeu\nscore, one must determine a single parameter, the equivalent sample size alpha.\nUnfortunately no generally accepted rule for determining the alpha parameter\nhas been suggested. This is disturbing, since in this paper we show through a\nseries of concrete experiments that the solution of the network structure\noptimization problem is highly sensitive to the chosen alpha parameter value.\nBased on these results, we are able to give explanations for how and why this\nphenomenon happens, and discuss ideas for solving this problem.\n", "versions": [{"version": "v1", "created": "Wed, 20 Jun 2012 15:19:06 GMT"}], "update_date": "2012-06-26", "authors_parsed": [["Silander", "Tomi", ""], ["Kontkanen", "Petri", ""], ["Myllymaki", "Petri", ""]]}, {"id": "1206.5580", "submitter": "John Moeller", "authors": "John Moeller, Parasaran Raman, Avishek Saha, Suresh Venkatasubramanian", "title": "A Geometric Algorithm for Scalable Multiple Kernel Learning", "comments": "20 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a geometric formulation of the Multiple Kernel Learning (MKL)\nproblem. To do so, we reinterpret the problem of learning kernel weights as\nsearching for a kernel that maximizes the minimum (kernel) distance between two\nconvex polytopes. This interpretation combined with novel structural insights\nfrom our geometric formulation allows us to reduce the MKL problem to a simple\noptimization routine that yields provable convergence as well as quality\nguarantees. As a result our method scales efficiently to much larger data sets\nthan most prior methods can handle. Empirical evaluation on eleven datasets\nshows that we are significantly faster and even compare favorably with a\nuniform unweighted combination of kernels.\n", "versions": [{"version": "v1", "created": "Mon, 25 Jun 2012 05:57:29 GMT"}, {"version": "v2", "created": "Sat, 15 Mar 2014 04:33:18 GMT"}], "update_date": "2014-03-18", "authors_parsed": [["Moeller", "John", ""], ["Raman", "Parasaran", ""], ["Saha", "Avishek", ""], ["Venkatasubramanian", "Suresh", ""]]}, {"id": "1206.5754", "submitter": "Aki Vehtari", "authors": "Jarno Vanhatalo, Jaakko Riihim\\\"aki, Jouni Hartikainen, Pasi\n  Jyl\\\"anki, Ville Tolvanen and Aki Vehtari", "title": "Bayesian Modeling with Gaussian Processes using the GPstuff Toolbox", "comments": "- Updated according to GPstuff 4.6. Added, e.g., Pareto smoothed\n  importance sampling", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.AI cs.MS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Gaussian processes (GP) are powerful tools for probabilistic modeling\npurposes. They can be used to define prior distributions over latent functions\nin hierarchical Bayesian models. The prior over functions is defined implicitly\nby the mean and covariance function, which determine the smoothness and\nvariability of the function. The inference can then be conducted directly in\nthe function space by evaluating or approximating the posterior process.\nDespite their attractive theoretical properties GPs provide practical\nchallenges in their implementation. GPstuff is a versatile collection of\ncomputational tools for GP models compatible with Linux and Windows MATLAB and\nOctave. It includes, among others, various inference methods, sparse\napproximations and tools for model assessment. In this work, we review these\ntools and demonstrate the use of GPstuff in several models.\n", "versions": [{"version": "v1", "created": "Mon, 25 Jun 2012 18:19:45 GMT"}, {"version": "v2", "created": "Fri, 5 Oct 2012 12:44:54 GMT"}, {"version": "v3", "created": "Fri, 22 Mar 2013 08:01:15 GMT"}, {"version": "v4", "created": "Fri, 26 Apr 2013 06:39:27 GMT"}, {"version": "v5", "created": "Tue, 15 Apr 2014 08:53:01 GMT"}, {"version": "v6", "created": "Wed, 15 Jul 2015 13:04:29 GMT"}], "update_date": "2015-07-16", "authors_parsed": [["Vanhatalo", "Jarno", ""], ["Riihim\u00e4ki", "Jaakko", ""], ["Hartikainen", "Jouni", ""], ["Jyl\u00e4nki", "Pasi", ""], ["Tolvanen", "Ville", ""], ["Vehtari", "Aki", ""]]}, {"id": "1206.5766", "submitter": "Daniel Hsu", "authors": "Daniel Hsu, Sham M. Kakade", "title": "Learning mixtures of spherical Gaussians: moment methods and spectral\n  decompositions", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This work provides a computationally efficient and statistically consistent\nmoment-based estimator for mixtures of spherical Gaussians. Under the condition\nthat component means are in general position, a simple spectral decomposition\ntechnique yields consistent parameter estimates from low-order observable\nmoments, without additional minimum separation assumptions needed by previous\ncomputationally efficient estimation procedures. Thus computational and\ninformation-theoretic barriers to efficient estimation in mixture models are\nprecluded when the mixture components have means in general position and\nspherical covariances. Some connections are made to estimation problems related\nto independent component analysis.\n", "versions": [{"version": "v1", "created": "Mon, 25 Jun 2012 18:49:44 GMT"}, {"version": "v2", "created": "Tue, 3 Jul 2012 17:56:44 GMT"}, {"version": "v3", "created": "Mon, 17 Sep 2012 20:24:15 GMT"}, {"version": "v4", "created": "Sun, 28 Oct 2012 07:03:15 GMT"}], "update_date": "2012-10-30", "authors_parsed": [["Hsu", "Daniel", ""], ["Kakade", "Sham M.", ""]]}, {"id": "1206.6015", "submitter": "Sundararajan Sellamanickam", "authors": "Sundararajan Sellamanickam, Sathiya Keerthi Selvaraj", "title": "Transductive Classification Methods for Mixed Graphs", "comments": "8 Pages, 2 Tables, 2 Figures, KDD Workshop - MLG'11 San Diego, CA,\n  USA", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper we provide a principled approach to solve a transductive\nclassification problem involving a similar graph (edges tend to connect nodes\nwith same labels) and a dissimilar graph (edges tend to connect nodes with\nopposing labels). Most of the existing methods, e.g., Information\nRegularization (IR), Weighted vote Relational Neighbor classifier (WvRN) etc,\nassume that the given graph is only a similar graph. We extend the IR and WvRN\nmethods to deal with mixed graphs. We evaluate the proposed extensions on\nseveral benchmark datasets as well as two real world datasets and demonstrate\nthe usefulness of our ideas.\n", "versions": [{"version": "v1", "created": "Tue, 26 Jun 2012 14:56:33 GMT"}], "update_date": "2012-06-27", "authors_parsed": [["Sellamanickam", "Sundararajan", ""], ["Selvaraj", "Sathiya Keerthi", ""]]}, {"id": "1206.6030", "submitter": "Sundararajan Sellamanickam", "authors": "Sundararajan Sellamanickam, Shirish Shevade", "title": "An Additive Model View to Sparse Gaussian Process Classifier Design", "comments": "14 pages, 3 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider the problem of designing a sparse Gaussian process classifier\n(SGPC) that generalizes well. Viewing SGPC design as constructing an additive\nmodel like in boosting, we present an efficient and effective SGPC design\nmethod to perform a stage-wise optimization of a predictive loss function. We\nintroduce new methods for two key components viz., site parameter estimation\nand basis vector selection in any SGPC design. The proposed adaptive sampling\nbased basis vector selection method aids in achieving improved generalization\nperformance at a reduced computational cost. This method can also be used in\nconjunction with any other site parameter estimation methods. It has similar\ncomputational and storage complexities as the well-known information vector\nmachine and is suitable for large datasets. The hyperparameters can be\ndetermined by optimizing a predictive loss function. The experimental results\nshow better generalization performance of the proposed basis vector selection\nmethod on several benchmark datasets, particularly for relatively smaller basis\nvector set sizes or on difficult datasets.\n", "versions": [{"version": "v1", "created": "Tue, 26 Jun 2012 15:58:21 GMT"}], "update_date": "2012-06-27", "authors_parsed": [["Sellamanickam", "Sundararajan", ""], ["Shevade", "Shirish", ""]]}, {"id": "1206.6038", "submitter": "Sundararajan Sellamanickam", "authors": "Sundararajan Sellamanickam, Sathiya Keerthi Selvaraj", "title": "Predictive Approaches For Gaussian Process Classifier Model Selection", "comments": "21 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper we consider the problem of Gaussian process classifier (GPC)\nmodel selection with different Leave-One-Out (LOO) Cross Validation (CV) based\noptimization criteria and provide a practical algorithm using LOO predictive\ndistributions with such criteria to select hyperparameters. Apart from the\nstandard average negative logarithm of predictive probability (NLP), we also\nconsider smoothed versions of criteria such as F-measure and Weighted Error\nRate (WER), which are useful for handling imbalanced data. Unlike the\nregression case, LOO predictive distributions for the classifier case are\nintractable. We use approximate LOO predictive distributions arrived from\nExpectation Propagation (EP) approximation. We conduct experiments on several\nreal world benchmark datasets. When the NLP criterion is used for optimizing\nthe hyperparameters, the predictive approaches show better or comparable NLP\ngeneralization performance with existing GPC approaches. On the other hand,\nwhen the F-measure criterion is used, the F-measure generalization performance\nimproves significantly on several datasets. Overall, the EP-based predictive\nalgorithm comes out as an excellent choice for GP classifier model selection\nwith different optimization criteria.\n", "versions": [{"version": "v1", "created": "Tue, 26 Jun 2012 16:19:51 GMT"}], "update_date": "2012-06-27", "authors_parsed": [["Sellamanickam", "Sundararajan", ""], ["Selvaraj", "Sathiya Keerthi", ""]]}, {"id": "1206.6141", "submitter": "Yi-Hao Kao", "authors": "Yi-Hao Kao and Benjamin Van Roy", "title": "Directed Time Series Regression for Control", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.SY stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose directed time series regression, a new approach to estimating\nparameters of time-series models for use in certainty equivalent model\npredictive control. The approach combines merits of least squares regression\nand empirical optimization. Through a computational study involving a\nstochastic version of a well known inverted pendulum balancing problem, we\ndemonstrate that directed time series regression can generate significant\nimprovements in controller performance over either of the aforementioned\nalternatives.\n", "versions": [{"version": "v1", "created": "Tue, 26 Jun 2012 23:39:00 GMT"}], "update_date": "2012-07-02", "authors_parsed": [["Kao", "Yi-Hao", ""], ["Van Roy", "Benjamin", ""]]}, {"id": "1206.6361", "submitter": "Ehsan Khoshknauz", "authors": "Ehsan Khoshgnauz", "title": "Learning Markov Network Structure using Brownian Distance Covariance", "comments": "10 pages, 5 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we present a simple non-parametric method for learning the\nstructure of undirected graphs from data that drawn from an underlying unknown\ndistribution. We propose to use Brownian distance covariance to estimate the\nconditional independences between the random variables and encodes pairwise\nMarkov graph. This framework can be applied in high-dimensional setting, where\nthe number of parameters much be larger than the sample size.\n", "versions": [{"version": "v1", "created": "Wed, 27 Jun 2012 18:37:50 GMT"}], "update_date": "2012-06-28", "authors_parsed": [["Khoshgnauz", "Ehsan", ""]]}, {"id": "1206.6380", "submitter": "Sungjin Ahn", "authors": "Sungjin Ahn (UC Irvine), Anoop Korattikara (UC Irvine), Max Welling\n  (UC Irvine)", "title": "Bayesian Posterior Sampling via Stochastic Gradient Fisher Scoring", "comments": "Appears in Proceedings of the 29th International Conference on\n  Machine Learning (ICML 2012)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.CO stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper we address the following question: Can we approximately sample\nfrom a Bayesian posterior distribution if we are only allowed to touch a small\nmini-batch of data-items for every sample we generate?. An algorithm based on\nthe Langevin equation with stochastic gradients (SGLD) was previously proposed\nto solve this, but its mixing rate was slow. By leveraging the Bayesian Central\nLimit Theorem, we extend the SGLD algorithm so that at high mixing rates it\nwill sample from a normal approximation of the posterior, while for slow mixing\nrates it will mimic the behavior of SGLD with a pre-conditioner matrix. As a\nbonus, the proposed algorithm is reminiscent of Fisher scoring (with stochastic\ngradients) and as such an efficient optimizer during burn-in.\n", "versions": [{"version": "v1", "created": "Wed, 27 Jun 2012 19:59:59 GMT"}], "update_date": "2012-07-03", "authors_parsed": [["Ahn", "Sungjin", "", "UC Irvine"], ["Korattikara", "Anoop", "", "UC Irvine"], ["Welling", "Max", "", "UC Irvine"]]}, {"id": "1206.6381", "submitter": "Morteza Alamgir", "authors": "Morteza Alamgir (Max Planck Institute for Intelligent Systems), Ulrike\n  von Luxburg (Max Planck Institute for Intelligent Systems and University of\n  Hamburg)", "title": "Shortest path distance in random k-nearest neighbor graphs", "comments": "Appears in Proceedings of the 29th International Conference on\n  Machine Learning (ICML 2012)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Consider a weighted or unweighted k-nearest neighbor graph that has been\nbuilt on n data points drawn randomly according to some density p on R^d. We\nstudy the convergence of the shortest path distance in such graphs as the\nsample size tends to infinity. We prove that for unweighted kNN graphs, this\ndistance converges to an unpleasant distance function on the underlying space\nwhose properties are detrimental to machine learning. We also study the\nbehavior of the shortest path distance in weighted kNN graphs.\n", "versions": [{"version": "v1", "created": "Wed, 27 Jun 2012 19:59:59 GMT"}, {"version": "v2", "created": "Mon, 9 Jul 2012 08:36:42 GMT"}], "update_date": "2012-07-10", "authors_parsed": [["Alamgir", "Morteza", "", "Max Planck Institute for Intelligent Systems"], ["von Luxburg", "Ulrike", "", "Max Planck Institute for Intelligent Systems and University of\n  Hamburg"]]}, {"id": "1206.6382", "submitter": "Animashree Anandkumar", "authors": "Majid Janzamin (UC Irvine), Animashree Anandkumar (UC Irvine)", "title": "High-Dimensional Covariance Decomposition into Sparse Markov and\n  Independence Domains", "comments": "Appears in Proceedings of the 29th International Conference on\n  Machine Learning (ICML 2012)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we present a novel framework incorporating a combination of\nsparse models in different domains. We posit the observed data as generated\nfrom a linear combination of a sparse Gaussian Markov model (with a sparse\nprecision matrix) and a sparse Gaussian independence model (with a sparse\ncovariance matrix). We provide efficient methods for decomposition of the data\ninto two domains, \\viz Markov and independence domains. We characterize a set\nof sufficient conditions for identifiability and model consistency. Our\ndecomposition method is based on a simple modification of the popular\n$\\ell_1$-penalized maximum-likelihood estimator ($\\ell_1$-MLE). We establish\nthat our estimator is consistent in both the domains, i.e., it successfully\nrecovers the supports of both Markov and independence models, when the number\nof samples $n$ scales as $n = \\Omega(d^2 \\log p)$, where $p$ is the number of\nvariables and $d$ is the maximum node degree in the Markov model. Our\nconditions for recovery are comparable to those of $\\ell_1$-MLE for consistent\nestimation of a sparse Markov model, and thus, we guarantee successful\nhigh-dimensional estimation of a richer class of models under comparable\nconditions. Our experiments validate these results and also demonstrate that\nour models have better inference accuracy under simple algorithms such as loopy\nbelief propagation.\n", "versions": [{"version": "v1", "created": "Wed, 27 Jun 2012 19:59:59 GMT"}], "update_date": "2012-07-03", "authors_parsed": [["Janzamin", "Majid", "", "UC Irvine"], ["Anandkumar", "Animashree", "", "UC Irvine"]]}, {"id": "1206.6383", "submitter": "Nicholas Arnosti", "authors": "Andrea Danyluk (Williams College), Nicholas Arnosti (Stanford\n  University)", "title": "Feature Selection via Probabilistic Outputs", "comments": "Appears in Proceedings of the 29th International Conference on\n  Machine Learning (ICML 2012)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper investigates two feature-scoring criteria that make use of\nestimated class probabilities: one method proposed by \\citet{shen} and a\ncomplementary approach proposed below. We develop a theoretical framework to\nanalyze each criterion and show that both estimate the spread (across all\nvalues of a given feature) of the probability that an example belongs to the\npositive class. Based on our analysis, we predict when each scoring technique\nwill be advantageous over the other and give empirical results validating our\npredictions.\n", "versions": [{"version": "v1", "created": "Wed, 27 Jun 2012 19:59:59 GMT"}], "update_date": "2012-07-03", "authors_parsed": [["Danyluk", "Andrea", "", "Williams College"], ["Arnosti", "Nicholas", "", "Stanford\n  University"]]}, {"id": "1206.6384", "submitter": "Haim Avron", "authors": "Haim Avron (IBM T.J. Watson Research Center), Satyen Kale (IBM T.J.\n  Watson Research Center), Shiva Kasiviswanathan (IBM T.J. Watson Research\n  Center), Vikas Sindhwani (IBM T.J. Watson Research Center)", "title": "Efficient and Practical Stochastic Subgradient Descent for Nuclear Norm\n  Regularization", "comments": "Appears in Proceedings of the 29th International Conference on\n  Machine Learning (ICML 2012)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We describe novel subgradient methods for a broad class of matrix\noptimization problems involving nuclear norm regularization. Unlike existing\napproaches, our method executes very cheap iterations by combining low-rank\nstochastic subgradients with efficient incremental SVD updates, made possible\nby highly optimized and parallelizable dense linear algebra operations on small\nmatrices. Our practical algorithms always maintain a low-rank factorization of\niterates that can be conveniently held in memory and efficiently multiplied to\ngenerate predictions in matrix completion settings. Empirical comparisons\nconfirm that our approach is highly competitive with several recently proposed\nstate-of-the-art solvers for such problems.\n", "versions": [{"version": "v1", "created": "Wed, 27 Jun 2012 19:59:59 GMT"}], "update_date": "2012-07-03", "authors_parsed": [["Avron", "Haim", "", "IBM T.J. Watson Research Center"], ["Kale", "Satyen", "", "IBM T.J.\n  Watson Research Center"], ["Kasiviswanathan", "Shiva", "", "IBM T.J. Watson Research\n  Center"], ["Sindhwani", "Vikas", "", "IBM T.J. Watson Research Center"]]}, {"id": "1206.6385", "submitter": "Philip Bachman", "authors": "Doina Precup (McGill University), Philip Bachman (McGill University)", "title": "Improved Estimation in Time Varying Models", "comments": "Appears in Proceedings of the 29th International Conference on\n  Machine Learning (ICML 2012)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ME stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Locally adapted parameterizations of a model (such as locally weighted\nregression) are expressive but often suffer from high variance. We describe an\napproach for reducing the variance, based on the idea of estimating\nsimultaneously a transformed space for the model, as well as locally adapted\nparameterizations in this new space. We present a new problem formulation that\ncaptures this idea and illustrate it in the important context of time varying\nmodels. We develop an algorithm for learning a set of bases for approximating a\ntime varying sparse network; each learned basis constitutes an archetypal\nsparse network structure. We also provide an extension for learning task-driven\nbases. We present empirical results on synthetic data sets, as well as on a BCI\nEEG classification task.\n", "versions": [{"version": "v1", "created": "Wed, 27 Jun 2012 19:59:59 GMT"}], "update_date": "2012-07-03", "authors_parsed": [["Precup", "Doina", "", "McGill University"], ["Bachman", "Philip", "", "McGill University"]]}, {"id": "1206.6386", "submitter": "Yoram Bachrach", "authors": "Yoram Bachrach (Microsoft Research), Thore Graepel (Microsoft\n  Research), Tom Minka (Microsoft Research), John Guiver (Microsoft Research)", "title": "How To Grade a Test Without Knowing the Answers --- A Bayesian Graphical\n  Model for Adaptive Crowdsourcing and Aptitude Testing", "comments": "Appears in Proceedings of the 29th International Conference on\n  Machine Learning (ICML 2012)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a new probabilistic graphical model that jointly models the\ndifficulties of questions, the abilities of participants and the correct\nanswers to questions in aptitude testing and crowdsourcing settings. We devise\nan active learning/adaptive testing scheme based on a greedy minimization of\nexpected model entropy, which allows a more efficient resource allocation by\ndynamically choosing the next question to be asked based on the previous\nresponses. We present experimental results that confirm the ability of our\nmodel to infer the required parameters and demonstrate that the adaptive\ntesting scheme requires fewer questions to obtain the same accuracy as a static\ntest scenario.\n", "versions": [{"version": "v1", "created": "Wed, 27 Jun 2012 19:59:59 GMT"}], "update_date": "2012-07-03", "authors_parsed": [["Bachrach", "Yoram", "", "Microsoft Research"], ["Graepel", "Thore", "", "Microsoft\n  Research"], ["Minka", "Tom", "", "Microsoft Research"], ["Guiver", "John", "", "Microsoft Research"]]}, {"id": "1206.6387", "submitter": "Djalel Benbouzid", "authors": "Djalel Benbouzid (University of Paris-Sud / CNRS / IN2P3), Robert\n  Busa-Fekete (LAL, CNRS), Balazs Kegl (CNRS / University of Paris-Sud)", "title": "Fast classification using sparse decision DAGs", "comments": "Appears in Proceedings of the 29th International Conference on\n  Machine Learning (ICML 2012)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper we propose an algorithm that builds sparse decision DAGs\n(directed acyclic graphs) from a list of base classifiers provided by an\nexternal learning method such as AdaBoost. The basic idea is to cast the DAG\ndesign task as a Markov decision process. Each instance can decide to use or to\nskip each base classifier, based on the current state of the classifier being\nbuilt. The result is a sparse decision DAG where the base classifiers are\nselected in a data-dependent way. The method has a single hyperparameter with a\nclear semantics of controlling the accuracy/speed trade-off. The algorithm is\ncompetitive with state-of-the-art cascade detectors on three object-detection\nbenchmarks, and it clearly outperforms them when there is a small number of\nbase classifiers. Unlike cascades, it is also readily applicable for\nmulti-class classification. Using the multi-class setup, we show on a benchmark\nweb page ranking data set that we can significantly improve the decision speed\nwithout harming the performance of the ranker.\n", "versions": [{"version": "v1", "created": "Wed, 27 Jun 2012 19:59:59 GMT"}], "update_date": "2012-07-03", "authors_parsed": [["Benbouzid", "Djalel", "", "University of Paris-Sud / CNRS / IN2P3"], ["Busa-Fekete", "Robert", "", "LAL, CNRS"], ["Kegl", "Balazs", "", "CNRS / University of Paris-Sud"]]}, {"id": "1206.6388", "submitter": "Felix Biessmann", "authors": "Felix Biessmann (TU Berlin), Jens-Michalis Papaioannou (TU Berlin),\n  Mikio Braun (TU Berlin), Andreas Harth (Karlsruhe Institue of Technology)", "title": "Canonical Trends: Detecting Trend Setters in Web Data", "comments": "Appears in Proceedings of the 29th International Conference on\n  Machine Learning (ICML 2012)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.SI stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Much information available on the web is copied, reused or rephrased. The\nphenomenon that multiple web sources pick up certain information is often\ncalled trend. A central problem in the context of web data mining is to detect\nthose web sources that are first to publish information which will give rise to\na trend. We present a simple and efficient method for finding trends dominating\na pool of web sources and identifying those web sources that publish the\ninformation relevant to a trend before others. We validate our approach on real\ndata collected from influential technology news feeds.\n", "versions": [{"version": "v1", "created": "Wed, 27 Jun 2012 19:59:59 GMT"}], "update_date": "2012-07-03", "authors_parsed": [["Biessmann", "Felix", "", "TU Berlin"], ["Papaioannou", "Jens-Michalis", "", "TU Berlin"], ["Braun", "Mikio", "", "TU Berlin"], ["Harth", "Andreas", "", "Karlsruhe Institue of Technology"]]}, {"id": "1206.6389", "submitter": "Battista Biggio", "authors": "Battista Biggio (University of Cagliari), Blaine Nelson (University of\n  Tuebingen), Pavel Laskov (University of Tuebingen)", "title": "Poisoning Attacks against Support Vector Machines", "comments": "Appears in Proceedings of the 29th International Conference on\n  Machine Learning (ICML 2012)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CR stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We investigate a family of poisoning attacks against Support Vector Machines\n(SVM). Such attacks inject specially crafted training data that increases the\nSVM's test error. Central to the motivation for these attacks is the fact that\nmost learning algorithms assume that their training data comes from a natural\nor well-behaved distribution. However, this assumption does not generally hold\nin security-sensitive settings. As we demonstrate, an intelligent adversary\ncan, to some extent, predict the change of the SVM's decision function due to\nmalicious input and use this ability to construct malicious data. The proposed\nattack uses a gradient ascent strategy in which the gradient is computed based\non properties of the SVM's optimal solution. This method can be kernelized and\nenables the attack to be constructed in the input space even for non-linear\nkernels. We experimentally demonstrate that our gradient ascent procedure\nreliably identifies good local maxima of the non-convex validation error\nsurface, which significantly increases the classifier's test error.\n", "versions": [{"version": "v1", "created": "Wed, 27 Jun 2012 19:59:59 GMT"}, {"version": "v2", "created": "Fri, 20 Jul 2012 12:33:21 GMT"}, {"version": "v3", "created": "Mon, 25 Mar 2013 10:16:36 GMT"}], "update_date": "2013-03-26", "authors_parsed": [["Biggio", "Battista", "", "University of Cagliari"], ["Nelson", "Blaine", "", "University of\n  Tuebingen"], ["Laskov", "Pavel", "", "University of Tuebingen"]]}, {"id": "1206.6392", "submitter": "Nicolas Boulanger-Lewandowski", "authors": "Nicolas Boulanger-Lewandowski (Universite de Montreal), Yoshua Bengio\n  (Universite de Montreal), Pascal Vincent (Universite de Montreal)", "title": "Modeling Temporal Dependencies in High-Dimensional Sequences:\n  Application to Polyphonic Music Generation and Transcription", "comments": "Appears in Proceedings of the 29th International Conference on\n  Machine Learning (ICML 2012)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.SD stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We investigate the problem of modeling symbolic sequences of polyphonic music\nin a completely general piano-roll representation. We introduce a probabilistic\nmodel based on distribution estimators conditioned on a recurrent neural\nnetwork that is able to discover temporal dependencies in high-dimensional\nsequences. Our approach outperforms many traditional models of polyphonic music\non a variety of realistic datasets. We show how our musical language model can\nserve as a symbolic prior to improve the accuracy of polyphonic transcription.\n", "versions": [{"version": "v1", "created": "Wed, 27 Jun 2012 19:59:59 GMT"}], "update_date": "2012-07-03", "authors_parsed": [["Boulanger-Lewandowski", "Nicolas", "", "Universite de Montreal"], ["Bengio", "Yoshua", "", "Universite de Montreal"], ["Vincent", "Pascal", "", "Universite de Montreal"]]}, {"id": "1206.6393", "submitter": "Xavier Carreras", "authors": "Borja Balle (UPC), Ariadna Quattoni (UPC), Xavier Carreras (UPC)", "title": "Local Loss Optimization in Operator Models: A New Insight into Spectral\n  Learning", "comments": "Appears in Proceedings of the 29th International Conference on\n  Machine Learning (ICML 2012)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper re-visits the spectral method for learning latent variable models\ndefined in terms of observable operators. We give a new perspective on the\nmethod, showing that operators can be recovered by minimizing a loss defined on\na finite subset of the domain. A non-convex optimization similar to the\nspectral method is derived. We also propose a regularized convex relaxation of\nthis optimization. We show that in practice the availabilty of a continuous\nregularization parameter (in contrast with the discrete number of states in the\noriginal method) allows a better trade-off between accuracy and model\ncomplexity. We also prove that in general, a randomized strategy for choosing\nthe local loss will succeed with high probability.\n", "versions": [{"version": "v1", "created": "Wed, 27 Jun 2012 19:59:59 GMT"}], "update_date": "2012-07-03", "authors_parsed": [["Balle", "Borja", "", "UPC"], ["Quattoni", "Ariadna", "", "UPC"], ["Carreras", "Xavier", "", "UPC"]]}, {"id": "1206.6394", "submitter": "Deepayan Chakrabarti", "authors": "Purnamrita Sarkar (UC Berkeley), Deepayan Chakrabarti (Facebook),\n  Michael Jordan (UC Berkeley)", "title": "Nonparametric Link Prediction in Dynamic Networks", "comments": "Appears in Proceedings of the 29th International Conference on\n  Machine Learning (ICML 2012)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.SI stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a non-parametric link prediction algorithm for a sequence of graph\nsnapshots over time. The model predicts links based on the features of its\nendpoints, as well as those of the local neighborhood around the endpoints.\nThis allows for different types of neighborhoods in a graph, each with its own\ndynamics (e.g, growing or shrinking communities). We prove the consistency of\nour estimator, and give a fast implementation based on locality-sensitive\nhashing. Experiments with simulated as well as five real-world dynamic graphs\nshow that we outperform the state of the art, especially when sharp\nfluctuations or non-linearities are present.\n", "versions": [{"version": "v1", "created": "Wed, 27 Jun 2012 19:59:59 GMT"}], "update_date": "2012-07-03", "authors_parsed": [["Sarkar", "Purnamrita", "", "UC Berkeley"], ["Chakrabarti", "Deepayan", "", "Facebook"], ["Jordan", "Michael", "", "UC Berkeley"]]}, {"id": "1206.6395", "submitter": "Kamalika Chaudhuri", "authors": "Kamalika Chaudhuri (UCSD), Daniel Hsu (Microsoft Research)", "title": "Convergence Rates for Differentially Private Statistical Estimation", "comments": "Appears in Proceedings of the 29th International Conference on\n  Machine Learning (ICML 2012)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CR stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Differential privacy is a cryptographically-motivated definition of privacy\nwhich has gained significant attention over the past few years. Differentially\nprivate solutions enforce privacy by adding random noise to a function computed\nover the data, and the challenge in designing such algorithms is to control the\nadded noise in order to optimize the privacy-accuracy-sample size tradeoff.\n  This work studies differentially-private statistical estimation, and shows\nupper and lower bounds on the convergence rates of differentially private\napproximations to statistical estimators. Our results reveal a formal\nconnection between differential privacy and the notion of Gross Error\nSensitivity (GES) in robust statistics, by showing that the convergence rate of\nany differentially private approximation to an estimator that is accurate over\na large class of distributions has to grow with the GES of the estimator. We\nthen provide an upper bound on the convergence rate of a differentially private\napproximation to an estimator with bounded range and bounded GES. We show that\nthe bounded range condition is necessary if we wish to ensure a strict form of\ndifferential privacy.\n", "versions": [{"version": "v1", "created": "Wed, 27 Jun 2012 19:59:59 GMT"}], "update_date": "2012-07-03", "authors_parsed": [["Chaudhuri", "Kamalika", "", "UCSD"], ["Hsu", "Daniel", "", "Microsoft Research"]]}, {"id": "1206.6396", "submitter": "Bo Chen", "authors": "Bo Chen (Caltech), Rui Castro (Eindhoven University of Technology),\n  Andreas Krause (ETH Zurich)", "title": "Joint Optimization and Variable Selection of High-dimensional Gaussian\n  Processes", "comments": "Appears in Proceedings of the 29th International Conference on\n  Machine Learning (ICML 2012)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Maximizing high-dimensional, non-convex functions through noisy observations\nis a notoriously hard problem, but one that arises in many applications. In\nthis paper, we tackle this challenge by modeling the unknown function as a\nsample from a high-dimensional Gaussian process (GP) distribution. Assuming\nthat the unknown function only depends on few relevant variables, we show that\nit is possible to perform joint variable selection and GP optimization. We\nprovide strong performance guarantees for our algorithm, bounding the sample\ncomplexity of variable selection, and as well as providing cumulative regret\nbounds. We further provide empirical evidence on the effectiveness of our\nalgorithm on several benchmark optimization problems.\n", "versions": [{"version": "v1", "created": "Wed, 27 Jun 2012 19:59:59 GMT"}], "update_date": "2012-07-03", "authors_parsed": [["Chen", "Bo", "", "Caltech"], ["Castro", "Rui", "", "Eindhoven University of Technology"], ["Krause", "Andreas", "", "ETH Zurich"]]}, {"id": "1206.6397", "submitter": "Minhua Chen", "authors": "Minhua Chen (Duke University), William Carson (PA Consulting Group,\n  Cambridge Technology Centre), Miguel Rodrigues (University College London),\n  Robert Calderbank (Duke University), Lawrence Carin (Duke University)", "title": "Communications Inspired Linear Discriminant Analysis", "comments": "Appears in Proceedings of the 29th International Conference on\n  Machine Learning (ICML 2012)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study the problem of supervised linear dimensionality reduction, taking an\ninformation-theoretic viewpoint. The linear projection matrix is designed by\nmaximizing the mutual information between the projected signal and the class\nlabel (based on a Shannon entropy measure). By harnessing a recent theoretical\nresult on the gradient of mutual information, the above optimization problem\ncan be solved directly using gradient descent, without requiring simplification\nof the objective function. Theoretical analysis and empirical comparison are\nmade between the proposed method and two closely related methods (Linear\nDiscriminant Analysis and Information Discriminant Analysis), and comparisons\nare also made with a method in which Renyi entropy is used to define the mutual\ninformation (in this case the gradient may be computed simply, under a special\nparameter setting). Relative to these alternative approaches, the proposed\nmethod achieves promising results on real datasets.\n", "versions": [{"version": "v1", "created": "Wed, 27 Jun 2012 19:59:59 GMT"}], "update_date": "2012-07-03", "authors_parsed": [["Chen", "Minhua", "", "Duke University"], ["Carson", "William", "", "PA Consulting Group,\n  Cambridge Technology Centre"], ["Rodrigues", "Miguel", "", "University College London"], ["Calderbank", "Robert", "", "Duke University"], ["Carin", "Lawrence", "", "Duke University"]]}, {"id": "1206.6398", "submitter": "Bruno da Silva", "authors": "Bruno Da Silva (UMass Amherst), George Konidaris (MIT), Andrew Barto\n  (UMass Amherst)", "title": "Learning Parameterized Skills", "comments": "Appears in Proceedings of the 29th International Conference on\n  Machine Learning (ICML 2012)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We introduce a method for constructing skills capable of solving tasks drawn\nfrom a distribution of parameterized reinforcement learning problems. The\nmethod draws example tasks from a distribution of interest and uses the\ncorresponding learned policies to estimate the topology of the\nlower-dimensional piecewise-smooth manifold on which the skill policies lie.\nThis manifold models how policy parameters change as task parameters vary. The\nmethod identifies the number of charts that compose the manifold and then\napplies non-linear regression in each chart to construct a parameterized skill\nby predicting policy parameters from task parameters. We evaluate our method on\nan underactuated simulated robotic arm tasked with learning to accurately throw\ndarts at a parameterized target location.\n", "versions": [{"version": "v1", "created": "Wed, 27 Jun 2012 19:59:59 GMT"}, {"version": "v2", "created": "Mon, 3 Sep 2012 16:05:45 GMT"}], "update_date": "2015-03-20", "authors_parsed": [["Da Silva", "Bruno", "", "UMass Amherst"], ["Konidaris", "George", "", "MIT"], ["Barto", "Andrew", "", "UMass Amherst"]]}, {"id": "1206.6399", "submitter": "Jesse Davis", "authors": "Jesse Davis (KU Leuven), Vitor Santos Costa (University of Porto),\n  Peggy Peissig (Marshfield Clinic), Michael Caldwell (Marshfield Clinic),\n  Elizabeth Berg (University of Wisconsin - Madison), David Page (University of\n  Wisconsin - Madison)", "title": "Demand-Driven Clustering in Relational Domains for Predicting Adverse\n  Drug Events", "comments": "Appears in Proceedings of the 29th International Conference on\n  Machine Learning (ICML 2012)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Learning from electronic medical records (EMR) is challenging due to their\nrelational nature and the uncertain dependence between a patient's past and\nfuture health status. Statistical relational learning is a natural fit for\nanalyzing EMRs but is less adept at handling their inherent latent structure,\nsuch as connections between related medications or diseases. One way to capture\nthe latent structure is via a relational clustering of objects. We propose a\nnovel approach that, instead of pre-clustering the objects, performs a\ndemand-driven clustering during learning. We evaluate our algorithm on three\nreal-world tasks where the goal is to use EMRs to predict whether a patient\nwill have an adverse reaction to a medication. We find that our approach is\nmore accurate than performing no clustering, pre-clustering, and using\nexpert-constructed medical heterarchies.\n", "versions": [{"version": "v1", "created": "Wed, 27 Jun 2012 19:59:59 GMT"}], "update_date": "2012-07-03", "authors_parsed": [["Davis", "Jesse", "", "KU Leuven"], ["Costa", "Vitor Santos", "", "University of Porto"], ["Peissig", "Peggy", "", "Marshfield Clinic"], ["Caldwell", "Michael", "", "Marshfield Clinic"], ["Berg", "Elizabeth", "", "University of Wisconsin - Madison"], ["Page", "David", "", "University of\n  Wisconsin - Madison"]]}, {"id": "1206.6400", "submitter": "Ofer Dekel", "authors": "Raman Arora (TTIC), Ofer Dekel (Microsoft Research), Ambuj Tewari\n  (University of Texas)", "title": "Online Bandit Learning against an Adaptive Adversary: from Regret to\n  Policy Regret", "comments": "Appears in Proceedings of the 29th International Conference on\n  Machine Learning (ICML 2012)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Online learning algorithms are designed to learn even when their input is\ngenerated by an adversary. The widely-accepted formal definition of an online\nalgorithm's ability to learn is the game-theoretic notion of regret. We argue\nthat the standard definition of regret becomes inadequate if the adversary is\nallowed to adapt to the online algorithm's actions. We define the alternative\nnotion of policy regret, which attempts to provide a more meaningful way to\nmeasure an online algorithm's performance against adaptive adversaries.\nFocusing on the online bandit setting, we show that no bandit algorithm can\nguarantee a sublinear policy regret against an adaptive adversary with\nunbounded memory. On the other hand, if the adversary's memory is bounded, we\npresent a general technique that converts any bandit algorithm with a sublinear\nregret bound into an algorithm with a sublinear policy regret bound. We extend\nthis result to other variants of regret, such as switching regret, internal\nregret, and swap regret.\n", "versions": [{"version": "v1", "created": "Wed, 27 Jun 2012 19:59:59 GMT"}], "update_date": "2012-07-03", "authors_parsed": [["Arora", "Raman", "", "TTIC"], ["Dekel", "Ofer", "", "Microsoft Research"], ["Tewari", "Ambuj", "", "University of Texas"]]}, {"id": "1206.6401", "submitter": "Krzysztof DembczyDski", "authors": "Krzysztof Dembczynski (Poznan University of Technology), Wojciech\n  Kotlowski (Poznan University of Technology), Eyke Huellermeier (Marburg\n  University)", "title": "Consistent Multilabel Ranking through Univariate Losses", "comments": "Appears in Proceedings of the 29th International Conference on\n  Machine Learning (ICML 2012)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider the problem of rank loss minimization in the setting of\nmultilabel classification, which is usually tackled by means of convex\nsurrogate losses defined on pairs of labels. Very recently, this approach was\nput into question by a negative result showing that commonly used pairwise\nsurrogate losses, such as exponential and logistic losses, are inconsistent. In\nthis paper, we show a positive result which is arguably surprising in light of\nthe previous one: the simpler univariate variants of exponential and logistic\nsurrogates (i.e., defined on single labels) are consistent for rank loss\nminimization. Instead of directly proving convergence, we give a much stronger\nresult by deriving regret bounds and convergence rates. The proposed losses\nsuggest efficient and scalable algorithms, which are tested experimentally.\n", "versions": [{"version": "v1", "created": "Wed, 27 Jun 2012 19:59:59 GMT"}], "update_date": "2012-07-03", "authors_parsed": [["Dembczynski", "Krzysztof", "", "Poznan University of Technology"], ["Kotlowski", "Wojciech", "", "Poznan University of Technology"], ["Huellermeier", "Eyke", "", "Marburg\n  University"]]}, {"id": "1206.6402", "submitter": "Thomas Desautels", "authors": "Thomas Desautels (California Inst. of Technology), Andreas Krause (ETH\n  Zurich), Joel Burdick (California Inst. of Technology)", "title": "Parallelizing Exploration-Exploitation Tradeoffs with Gaussian Process\n  Bandit Optimization", "comments": "Appears in Proceedings of the 29th International Conference on\n  Machine Learning (ICML 2012)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Can one parallelize complex exploration exploitation tradeoffs? As an\nexample, consider the problem of optimal high-throughput experimental design,\nwhere we wish to sequentially design batches of experiments in order to\nsimultaneously learn a surrogate function mapping stimulus to response and\nidentify the maximum of the function. We formalize the task as a multi-armed\nbandit problem, where the unknown payoff function is sampled from a Gaussian\nprocess (GP), and instead of a single arm, in each round we pull a batch of\nseveral arms in parallel. We develop GP-BUCB, a principled algorithm for\nchoosing batches, based on the GP-UCB algorithm for sequential GP optimization.\nWe prove a surprising result; as compared to the sequential approach, the\ncumulative regret of the parallel algorithm only increases by a constant factor\nindependent of the batch size B. Our results provide rigorous theoretical\nsupport for exploiting parallelism in Bayesian global optimization. We\ndemonstrate the effectiveness of our approach on two real-world applications.\n", "versions": [{"version": "v1", "created": "Wed, 27 Jun 2012 19:59:59 GMT"}], "update_date": "2012-07-03", "authors_parsed": [["Desautels", "Thomas", "", "California Inst. of Technology"], ["Krause", "Andreas", "", "ETH\n  Zurich"], ["Burdick", "Joel", "", "California Inst. of Technology"]]}, {"id": "1206.6404", "submitter": "Dotan Di Castro", "authors": "Dotan Di Castro (Technion), Aviv Tamar (Technion), Shie Mannor\n  (Technion)", "title": "Policy Gradients with Variance Related Risk Criteria", "comments": "Appears in Proceedings of the 29th International Conference on\n  Machine Learning (ICML 2012)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CY math.OC stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Managing risk in dynamic decision problems is of cardinal importance in many\nfields such as finance and process control. The most common approach to\ndefining risk is through various variance related criteria such as the Sharpe\nRatio or the standard deviation adjusted reward. It is known that optimizing\nmany of the variance related risk criteria is NP-hard. In this paper we devise\na framework for local policy gradient style algorithms for reinforcement\nlearning for variance related criteria. Our starting point is a new formula for\nthe variance of the cost-to-go in episodic tasks. Using this formula we develop\npolicy gradient algorithms for criteria that involve both the expected cost and\nthe variance of the cost. We prove the convergence of these algorithms to local\nminima and demonstrate their applicability in a portfolio planning problem.\n", "versions": [{"version": "v1", "created": "Wed, 27 Jun 2012 19:59:59 GMT"}], "update_date": "2012-07-03", "authors_parsed": [["Di Castro", "Dotan", "", "Technion"], ["Tamar", "Aviv", "", "Technion"], ["Mannor", "Shie", "", "Technion"]]}, {"id": "1206.6405", "submitter": "Roy Fox", "authors": "Roy Fox (Hebrew University), Naftali Tishby (Hebrew University)", "title": "Bounded Planning in Passive POMDPs", "comments": "Appears in Proceedings of the 29th International Conference on\n  Machine Learning (ICML 2012)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In Passive POMDPs actions do not affect the world state, but still incur\ncosts. When the agent is bounded by information-processing constraints, it can\nonly keep an approximation of the belief. We present a variational principle\nfor the problem of maintaining the information which is most useful for\nminimizing the cost, and introduce an efficient and simple algorithm for\nfinding an optimum.\n", "versions": [{"version": "v1", "created": "Wed, 27 Jun 2012 19:59:59 GMT"}], "update_date": "2012-07-03", "authors_parsed": [["Fox", "Roy", "", "Hebrew University"], ["Tishby", "Naftali", "", "Hebrew University"]]}, {"id": "1206.6406", "submitter": "Roman Garnett", "authors": "Roman Garnett (Carnegie Mellon University), Yamuna Krishnamurthy\n  (Carnegie Mellon University), Xuehan Xiong (Carnegie Mellon University), Jeff\n  Schneider (Carnegie Mellon University), Richard Mann (Uppsala Universitet)", "title": "Bayesian Optimal Active Search and Surveying", "comments": "Appears in Proceedings of the 29th International Conference on\n  Machine Learning (ICML 2012)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider two active binary-classification problems with atypical\nobjectives. In the first, active search, our goal is to actively uncover as\nmany members of a given class as possible. In the second, active surveying, our\ngoal is to actively query points to ultimately predict the proportion of a\ngiven class. Numerous real-world problems can be framed in these terms, and in\neither case typical model-based concerns such as generalization error are only\nof secondary importance.\n  We approach these problems via Bayesian decision theory; after choosing\nnatural utility functions, we derive the optimal policies. We provide three\ncontributions. In addition to introducing the active surveying problem, we\nextend previous work on active search in two ways. First, we prove a novel\ntheoretical result, that less-myopic approximations to the optimal policy can\noutperform more-myopic approximations by any arbitrary degree. We then derive\nbounds that for certain models allow us to reduce (in practice dramatically)\nthe exponential search space required by a naive implementation of the optimal\npolicy, enabling further lookahead while still ensuring that optimal decisions\nare always made.\n", "versions": [{"version": "v1", "created": "Wed, 27 Jun 2012 19:59:59 GMT"}], "update_date": "2016-11-11", "authors_parsed": [["Garnett", "Roman", "", "Carnegie Mellon University"], ["Krishnamurthy", "Yamuna", "", "Carnegie Mellon University"], ["Xiong", "Xuehan", "", "Carnegie Mellon University"], ["Schneider", "Jeff", "", "Carnegie Mellon University"], ["Mann", "Richard", "", "Uppsala Universitet"]]}, {"id": "1206.6407", "submitter": "Ian Goodfellow", "authors": "Ian Goodfellow (Universite de Montreal), Aaron Courville (Universite\n  de Montreal), Yoshua Bengio (Universite de Montreal)", "title": "Large-Scale Feature Learning With Spike-and-Slab Sparse Coding", "comments": "Appears in Proceedings of the 29th International Conference on\n  Machine Learning (ICML 2012). arXiv admin note: substantial text overlap with\n  arXiv:1201.3382", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider the problem of object recognition with a large number of classes.\nIn order to overcome the low amount of labeled examples available in this\nsetting, we introduce a new feature learning and extraction procedure based on\na factor model we call spike-and-slab sparse coding (S3C). Prior work on S3C\nhas not prioritized the ability to exploit parallel architectures and scale S3C\nto the enormous problem sizes needed for object recognition. We present a novel\ninference procedure for appropriate for use with GPUs which allows us to\ndramatically increase both the training set size and the amount of latent\nfactors that S3C may be trained with. We demonstrate that this approach\nimproves upon the supervised learning capabilities of both sparse coding and\nthe spike-and-slab Restricted Boltzmann Machine (ssRBM) on the CIFAR-10\ndataset. We use the CIFAR-100 dataset to demonstrate that our method scales to\nlarge numbers of classes better than previous methods. Finally, we use our\nmethod to win the NIPS 2011 Workshop on Challenges In Learning Hierarchical\nModels? Transfer Learning Challenge.\n", "versions": [{"version": "v1", "created": "Wed, 27 Jun 2012 19:59:59 GMT"}], "update_date": "2012-07-03", "authors_parsed": [["Goodfellow", "Ian", "", "Universite de Montreal"], ["Courville", "Aaron", "", "Universite\n  de Montreal"], ["Bengio", "Yoshua", "", "Universite de Montreal"]]}, {"id": "1206.6409", "submitter": "Mahantesh Halappanavar", "authors": "Chad Scherrer (Pacific Northwest National Lab), Mahantesh Halappanavar\n  (Pacific Northwest National Lab), Ambuj Tewari (University of Texas), David\n  Haglin (Pacific Northwest National Lab)", "title": "Scaling Up Coordinate Descent Algorithms for Large $\\ell_1$\n  Regularization Problems", "comments": "Appears in Proceedings of the 29th International Conference on\n  Machine Learning (ICML 2012)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.DC stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a generic framework for parallel coordinate descent (CD)\nalgorithms that includes, as special cases, the original sequential algorithms\nCyclic CD and Stochastic CD, as well as the recent parallel Shotgun algorithm.\nWe introduce two novel parallel algorithms that are also special\ncases---Thread-Greedy CD and Coloring-Based CD---and give performance\nmeasurements for an OpenMP implementation of these.\n", "versions": [{"version": "v1", "created": "Wed, 27 Jun 2012 19:59:59 GMT"}], "update_date": "2012-07-04", "authors_parsed": [["Scherrer", "Chad", "", "Pacific Northwest National Lab"], ["Halappanavar", "Mahantesh", "", "Pacific Northwest National Lab"], ["Tewari", "Ambuj", "", "University of Texas"], ["Haglin", "David", "", "Pacific Northwest National Lab"]]}, {"id": "1206.6410", "submitter": "Tamir Hazan", "authors": "Tamir Hazan (TTIC), Tommi Jaakkola (MIT)", "title": "On the Partition Function and Random Maximum A-Posteriori Perturbations", "comments": "Appears in Proceedings of the 29th International Conference on\n  Machine Learning (ICML 2012)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper we relate the partition function to the max-statistics of\nrandom variables. In particular, we provide a novel framework for approximating\nand bounding the partition function using MAP inference on randomly perturbed\nmodels. As a result, we can use efficient MAP solvers such as graph-cuts to\nevaluate the corresponding partition function. We show that our method excels\nin the typical \"high signal - high coupling\" regime that results in ragged\nenergy landscapes difficult for alternative approaches.\n", "versions": [{"version": "v1", "created": "Wed, 27 Jun 2012 19:59:59 GMT"}], "update_date": "2012-07-03", "authors_parsed": [["Hazan", "Tamir", "", "TTIC"], ["Jaakkola", "Tommi", "", "MIT"]]}, {"id": "1206.6411", "submitter": "Junfeng He", "authors": "Junfeng He (Columbia University), Sanjiv Kumar (Google Research),\n  Shih-Fu Chang (Columbia University)", "title": "On the Difficulty of Nearest Neighbor Search", "comments": "Appears in Proceedings of the 29th International Conference on\n  Machine Learning (ICML 2012)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.DB cs.IR stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Fast approximate nearest neighbor (NN) search in large databases is becoming\npopular. Several powerful learning-based formulations have been proposed\nrecently. However, not much attention has been paid to a more fundamental\nquestion: how difficult is (approximate) nearest neighbor search in a given\ndata set? And which data properties affect the difficulty of nearest neighbor\nsearch and how? This paper introduces the first concrete measure called\nRelative Contrast that can be used to evaluate the influence of several crucial\ndata characteristics such as dimensionality, sparsity, and database size\nsimultaneously in arbitrary normed metric spaces. Moreover, we present a\ntheoretical analysis to prove how the difficulty measure (relative contrast)\ndetermines/affects the complexity of Local Sensitive Hashing, a popular\napproximate NN search method. Relative contrast also provides an explanation\nfor a family of heuristic hashing algorithms with good practical performance\nbased on PCA. Finally, we show that most of the previous works in measuring NN\nsearch meaningfulness/difficulty can be derived as special asymptotic cases for\ndense vectors of the proposed measure.\n", "versions": [{"version": "v1", "created": "Wed, 27 Jun 2012 19:59:59 GMT"}], "update_date": "2012-07-03", "authors_parsed": [["He", "Junfeng", "", "Columbia University"], ["Kumar", "Sanjiv", "", "Google Research"], ["Chang", "Shih-Fu", "", "Columbia University"]]}, {"id": "1206.6412", "submitter": "Ming Ji", "authors": "Ming Ji (UIUC), Tianbao Yang (Michigan State University), Binbin Lin\n  (Zhejiang University), Rong Jin (Michigan State University), Jiawei Han\n  (UIUC)", "title": "A Simple Algorithm for Semi-supervised Learning with Improved\n  Generalization Error Bound", "comments": "Appears in Proceedings of the 29th International Conference on\n  Machine Learning (ICML 2012)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this work, we develop a simple algorithm for semi-supervised regression.\nThe key idea is to use the top eigenfunctions of integral operator derived from\nboth labeled and unlabeled examples as the basis functions and learn the\nprediction function by a simple linear regression. We show that under\nappropriate assumptions about the integral operator, this approach is able to\nachieve an improved regression error bound better than existing bounds of\nsupervised learning. We also verify the effectiveness of the proposed algorithm\nby an empirical study.\n", "versions": [{"version": "v1", "created": "Wed, 27 Jun 2012 19:59:59 GMT"}], "update_date": "2012-07-03", "authors_parsed": [["Ji", "Ming", "", "UIUC"], ["Yang", "Tianbao", "", "Michigan State University"], ["Lin", "Binbin", "", "Zhejiang University"], ["Jin", "Rong", "", "Michigan State University"], ["Han", "Jiawei", "", "UIUC"]]}, {"id": "1206.6413", "submitter": "Armand Joulin", "authors": "Armand Joulin (INRIA - Ecole Normale Superieure), Francis Bach (INRIA\n  - Ecole Normale Superieure)", "title": "A Convex Relaxation for Weakly Supervised Classifiers", "comments": "Appears in Proceedings of the 29th International Conference on\n  Machine Learning (ICML 2012)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper introduces a general multi-class approach to weakly supervised\nclassification. Inferring the labels and learning the parameters of the model\nis usually done jointly through a block-coordinate descent algorithm such as\nexpectation-maximization (EM), which may lead to local minima. To avoid this\nproblem, we propose a cost function based on a convex relaxation of the\nsoft-max loss. We then propose an algorithm specifically designed to\nefficiently solve the corresponding semidefinite program (SDP). Empirically,\nour method compares favorably to standard ones on different datasets for\nmultiple instance learning and semi-supervised learning as well as on\nclustering tasks.\n", "versions": [{"version": "v1", "created": "Wed, 27 Jun 2012 19:59:59 GMT"}], "update_date": "2012-07-03", "authors_parsed": [["Joulin", "Armand", "", "INRIA - Ecole Normale Superieure"], ["Bach", "Francis", "", "INRIA\n  - Ecole Normale Superieure"]]}, {"id": "1206.6414", "submitter": "Dae Il Kim", "authors": "Dae Il Kim (Brown University), Michael Hughes (Brown University), Erik\n  Sudderth (Brown University)", "title": "The Nonparametric Metadata Dependent Relational Model", "comments": "Appears in Proceedings of the 29th International Conference on\n  Machine Learning (ICML 2012)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.SI stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We introduce the nonparametric metadata dependent relational (NMDR) model, a\nBayesian nonparametric stochastic block model for network data. The NMDR allows\nthe entities associated with each node to have mixed membership in an unbounded\ncollection of latent communities. Learned regression models allow these\nmemberships to depend on, and be predicted from, arbitrary node metadata. We\ndevelop efficient MCMC algorithms for learning NMDR models from partially\nobserved node relationships. Retrospective MCMC methods allow our sampler to\nwork directly with the infinite stick-breaking representation of the NMDR,\navoiding the need for finite truncations. Our results demonstrate recovery of\nuseful latent communities from real-world social and ecological networks, and\nthe usefulness of metadata in link prediction tasks.\n", "versions": [{"version": "v1", "created": "Wed, 27 Jun 2012 19:59:59 GMT"}], "update_date": "2012-07-03", "authors_parsed": [["Kim", "Dae Il", "", "Brown University"], ["Hughes", "Michael", "", "Brown University"], ["Sudderth", "Erik", "", "Brown University"]]}, {"id": "1206.6415", "submitter": "Ariel Kleiner", "authors": "Ariel Kleiner (UC Berkeley), Ameet Talwalkar (UC Berkeley), Purnamrita\n  Sarkar (UC Berkeley), Michael Jordan (UC Berkeley)", "title": "The Big Data Bootstrap", "comments": "Appears in Proceedings of the 29th International Conference on\n  Machine Learning (ICML 2012). arXiv admin note: text overlap with\n  arXiv:1112.5016", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The bootstrap provides a simple and powerful means of assessing the quality\nof estimators. However, in settings involving large datasets, the computation\nof bootstrap-based quantities can be prohibitively demanding. As an\nalternative, we present the Bag of Little Bootstraps (BLB), a new procedure\nwhich incorporates features of both the bootstrap and subsampling to obtain a\nrobust, computationally efficient means of assessing estimator quality. BLB is\nwell suited to modern parallel and distributed computing architectures and\nretains the generic applicability, statistical efficiency, and favorable\ntheoretical properties of the bootstrap. We provide the results of an extensive\nempirical and theoretical investigation of BLB's behavior, including a study of\nits statistical correctness, its large-scale implementation and performance,\nselection of hyperparameters, and performance on real data.\n", "versions": [{"version": "v1", "created": "Wed, 27 Jun 2012 19:59:59 GMT"}], "update_date": "2012-07-03", "authors_parsed": [["Kleiner", "Ariel", "", "UC Berkeley"], ["Talwalkar", "Ameet", "", "UC Berkeley"], ["Sarkar", "Purnamrita", "", "UC Berkeley"], ["Jordan", "Michael", "", "UC Berkeley"]]}, {"id": "1206.6416", "submitter": "David Knowles", "authors": "Konstantina Palla (University of Cambridge), David Knowles (University\n  of Cambridge), Zoubin Ghahramani (University of Cambridge)", "title": "An Infinite Latent Attribute Model for Network Data", "comments": "Appears in Proceedings of the 29th International Conference on\n  Machine Learning (ICML 2012)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Latent variable models for network data extract a summary of the relational\nstructure underlying an observed network. The simplest possible models\nsubdivide nodes of the network into clusters; the probability of a link between\nany two nodes then depends only on their cluster assignment. Currently\navailable models can be classified by whether clusters are disjoint or are\nallowed to overlap. These models can explain a \"flat\" clustering structure.\nHierarchical Bayesian models provide a natural approach to capture more complex\ndependencies. We propose a model in which objects are characterised by a latent\nfeature vector. Each feature is itself partitioned into disjoint groups\n(subclusters), corresponding to a second layer of hierarchy. In experimental\ncomparisons, the model achieves significantly improved predictive performance\non social and biological link prediction tasks. The results indicate that\nmodels with a single layer hierarchy over-simplify real networks.\n", "versions": [{"version": "v1", "created": "Wed, 27 Jun 2012 19:59:59 GMT"}], "update_date": "2012-07-03", "authors_parsed": [["Palla", "Konstantina", "", "University of Cambridge"], ["Knowles", "David", "", "University\n  of Cambridge"], ["Ghahramani", "Zoubin", "", "University of Cambridge"]]}, {"id": "1206.6417", "submitter": "Abhishek Kumar", "authors": "Abhishek Kumar (University of Maryland), Hal Daume III (University of\n  Maryland)", "title": "Learning Task Grouping and Overlap in Multi-task Learning", "comments": "Appears in Proceedings of the 29th International Conference on\n  Machine Learning (ICML 2012)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In the paradigm of multi-task learning, mul- tiple related prediction tasks\nare learned jointly, sharing information across the tasks. We propose a\nframework for multi-task learn- ing that enables one to selectively share the\ninformation across the tasks. We assume that each task parameter vector is a\nlinear combi- nation of a finite number of underlying basis tasks. The\ncoefficients of the linear combina- tion are sparse in nature and the overlap\nin the sparsity patterns of two tasks controls the amount of sharing across\nthese. Our model is based on on the assumption that task pa- rameters within a\ngroup lie in a low dimen- sional subspace but allows the tasks in differ- ent\ngroups to overlap with each other in one or more bases. Experimental results on\nfour datasets show that our approach outperforms competing methods.\n", "versions": [{"version": "v1", "created": "Wed, 27 Jun 2012 19:59:59 GMT"}], "update_date": "2012-07-03", "authors_parsed": [["Kumar", "Abhishek", "", "University of Maryland"], ["Daume", "Hal", "III", "University of\n  Maryland"]]}, {"id": "1206.6418", "submitter": "Honglak Lee", "authors": "Kihyuk Sohn (University of Michigan), Honglak Lee (University of\n  Michigan)", "title": "Learning Invariant Representations with Local Transformations", "comments": "Appears in Proceedings of the 29th International Conference on\n  Machine Learning (ICML 2012)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Learning invariant representations is an important problem in machine\nlearning and pattern recognition. In this paper, we present a novel framework\nof transformation-invariant feature learning by incorporating linear\ntransformations into the feature learning algorithms. For example, we present\nthe transformation-invariant restricted Boltzmann machine that compactly\nrepresents data by its weights and their transformations, which achieves\ninvariance of the feature representation via probabilistic max pooling. In\naddition, we show that our transformation-invariant feature learning framework\ncan also be extended to other unsupervised learning methods, such as\nautoencoders or sparse coding. We evaluate our method on several image\nclassification benchmark datasets, such as MNIST variations, CIFAR-10, and\nSTL-10, and show competitive or superior classification performance when\ncompared to the state-of-the-art. Furthermore, our method achieves\nstate-of-the-art performance on phone classification tasks with the TIMIT\ndataset, which demonstrates wide applicability of our proposed algorithms to\nother domains.\n", "versions": [{"version": "v1", "created": "Wed, 27 Jun 2012 19:59:59 GMT"}], "update_date": "2012-07-03", "authors_parsed": [["Sohn", "Kihyuk", "", "University of Michigan"], ["Lee", "Honglak", "", "University of\n  Michigan"]]}, {"id": "1206.6419", "submitter": "Xuejun Liao", "authors": "Shaobo Han (Duke University), Xuejun Liao (Duke University), Lawrence\n  Carin (Duke University)", "title": "Cross-Domain Multitask Learning with Latent Probit Models", "comments": "Appears in Proceedings of the 29th International Conference on\n  Machine Learning (ICML 2012)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Learning multiple tasks across heterogeneous domains is a challenging problem\nsince the feature space may not be the same for different tasks. We assume the\ndata in multiple tasks are generated from a latent common domain via sparse\ndomain transforms and propose a latent probit model (LPM) to jointly learn the\ndomain transforms, and the shared probit classifier in the common domain. To\nlearn meaningful task relatedness and avoid over-fitting in classification, we\nintroduce sparsity in the domain transforms matrices, as well as in the common\nclassifier. We derive theoretical bounds for the estimation error of the\nclassifier in terms of the sparsity of domain transforms. An\nexpectation-maximization algorithm is derived for learning the LPM. The\neffectiveness of the approach is demonstrated on several real datasets.\n", "versions": [{"version": "v1", "created": "Wed, 27 Jun 2012 19:59:59 GMT"}], "update_date": "2012-07-03", "authors_parsed": [["Han", "Shaobo", "", "Duke University"], ["Liao", "Xuejun", "", "Duke University"], ["Carin", "Lawrence", "", "Duke University"]]}, {"id": "1206.6420", "submitter": "Qiang Liu", "authors": "Qiang Liu (UC Irvine), Alexander Ihler (UC Irvine)", "title": "Distributed Parameter Estimation via Pseudo-likelihood", "comments": "Appears in Proceedings of the 29th International Conference on\n  Machine Learning (ICML 2012)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.DC stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Estimating statistical models within sensor networks requires distributed\nalgorithms, in which both data and computation are distributed across the nodes\nof the network. We propose a general approach for distributed learning based on\ncombining local estimators defined by pseudo-likelihood components,\nencompassing a number of combination methods, and provide both theoretical and\nexperimental analysis. We show that simple linear combination or max-voting\nmethods, when combined with second-order information, are statistically\ncompetitive with more advanced and costly joint optimization. Our algorithms\nhave many attractive properties including low communication and computational\ncost and \"any-time\" behavior.\n", "versions": [{"version": "v1", "created": "Wed, 27 Jun 2012 19:59:59 GMT"}], "update_date": "2012-07-03", "authors_parsed": [["Liu", "Qiang", "", "UC Irvine"], ["Ihler", "Alexander", "", "UC Irvine"]]}, {"id": "1206.6421", "submitter": "Xinghua Lou", "authors": "Xinghua Lou (University of Heidelberg), Fred Hamprecht (University of\n  Heidelberg)", "title": "Structured Learning from Partial Annotations", "comments": "Appears in Proceedings of the 29th International Conference on\n  Machine Learning (ICML 2012)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Structured learning is appropriate when predicting structured outputs such as\ntrees, graphs, or sequences. Most prior work requires the training set to\nconsist of complete trees, graphs or sequences. Specifying such detailed ground\ntruth can be tedious or infeasible for large outputs. Our main contribution is\na large margin formulation that makes structured learning from only partially\nannotated data possible. The resulting optimization problem is non-convex, yet\ncan be efficiently solve by concave-convex procedure (CCCP) with novel speedup\nstrategies. We apply our method to a challenging tracking-by-assignment problem\nof a variable number of divisible objects. On this benchmark, using only 25% of\na full annotation we achieve a performance comparable to a model learned with a\nfull annotation. Finally, we offer a unifying perspective of previous work\nusing the hinge, ramp, or max loss for structured learning, followed by an\nempirical comparison on their practical performance.\n", "versions": [{"version": "v1", "created": "Wed, 27 Jun 2012 19:59:59 GMT"}], "update_date": "2012-07-03", "authors_parsed": [["Lou", "Xinghua", "", "University of Heidelberg"], ["Hamprecht", "Fred", "", "University of\n  Heidelberg"]]}, {"id": "1206.6422", "submitter": "Chi-Jen Lu", "authors": "Shang-Tse Chen (Academia Sinica), Hsuan-Tien Lin (National Taiwan\n  University), Chi-Jen Lu (Academia Sinica)", "title": "An Online Boosting Algorithm with Theoretical Justifications", "comments": "Appears in Proceedings of the 29th International Conference on\n  Machine Learning (ICML 2012)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study the task of online boosting--combining online weak learners into an\nonline strong learner. While batch boosting has a sound theoretical foundation,\nonline boosting deserves more study from the theoretical perspective. In this\npaper, we carefully compare the differences between online and batch boosting,\nand propose a novel and reasonable assumption for the online weak learner.\nBased on the assumption, we design an online boosting algorithm with a strong\ntheoretical guarantee by adapting from the offline SmoothBoost algorithm that\nmatches the assumption closely. We further tackle the task of deciding the\nnumber of weak learners using established theoretical results for online convex\nprogramming and predicting with expert advice. Experiments on real-world data\nsets demonstrate that the proposed algorithm compares favorably with existing\nonline boosting algorithms.\n", "versions": [{"version": "v1", "created": "Wed, 27 Jun 2012 19:59:59 GMT"}], "update_date": "2012-07-03", "authors_parsed": [["Chen", "Shang-Tse", "", "Academia Sinica"], ["Lin", "Hsuan-Tien", "", "National Taiwan\n  University"], ["Lu", "Chi-Jen", "", "Academia Sinica"]]}, {"id": "1206.6424", "submitter": "Denis Maua", "authors": "Denis Maua (IDSIA), Cassio De Campos (IDSIA)", "title": "Anytime Marginal MAP Inference", "comments": "Appears in Proceedings of the 29th International Conference on\n  Machine Learning (ICML 2012)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper presents a new anytime algorithm for the marginal MAP problem in\ngraphical models. The algorithm is described in detail, its complexity and\nconvergence rate are studied, and relations to previous theoretical results for\nthe problem are discussed. It is shown that the algorithm runs in\npolynomial-time if the underlying graph of the model has bounded tree-width,\nand that it provides guarantees to the lower and upper bounds obtained within a\nfixed amount of computational resources. Experiments with both real and\nsynthetic generated models highlight its main characteristics and show that it\ncompares favorably against Park and Darwiche's systematic search, particularly\nin the case of problems with many MAP variables and moderate tree-width.\n", "versions": [{"version": "v1", "created": "Wed, 27 Jun 2012 19:59:59 GMT"}], "update_date": "2012-12-21", "authors_parsed": [["Maua", "Denis", "", "IDSIA"], ["De Campos", "Cassio", "", "IDSIA"]]}, {"id": "1206.6425", "submitter": "David Mimno", "authors": "David Mimno (Princeton University), Matt Hoffman (Columbia\n  University), David Blei (Princeton University)", "title": "Sparse Stochastic Inference for Latent Dirichlet allocation", "comments": "Appears in Proceedings of the 29th International Conference on\n  Machine Learning (ICML 2012)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a hybrid algorithm for Bayesian topic models that combines the\nefficiency of sparse Gibbs sampling with the scalability of online stochastic\ninference. We used our algorithm to analyze a corpus of 1.2 million books (33\nbillion words) with thousands of topics. Our approach reduces the bias of\nvariational inference and generalizes to many Bayesian hidden-variable models.\n", "versions": [{"version": "v1", "created": "Wed, 27 Jun 2012 19:59:59 GMT"}], "update_date": "2012-07-03", "authors_parsed": [["Mimno", "David", "", "Princeton University"], ["Hoffman", "Matt", "", "Columbia\n  University"], ["Blei", "David", "", "Princeton University"]]}, {"id": "1206.6427", "submitter": "Iftekhar Naim", "authors": "Iftekhar Naim (University of Rochester), Daniel Gildea (University of\n  Rochester)", "title": "Convergence of the EM Algorithm for Gaussian Mixtures with Unbalanced\n  Mixing Coefficients", "comments": "Appears in Proceedings of the 29th International Conference on\n  Machine Learning (ICML 2012)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The speed of convergence of the Expectation Maximization (EM) algorithm for\nGaussian mixture model fitting is known to be dependent on the amount of\noverlap among the mixture components. In this paper, we study the impact of\nmixing coefficients on the convergence of EM. We show that when the mixture\ncomponents exhibit some overlap, the convergence of EM becomes slower as the\ndynamic range among the mixing coefficients increases. We propose a\ndeterministic anti-annealing algorithm, that significantly improves the speed\nof convergence of EM for such mixtures with unbalanced mixing coefficients. The\nproposed algorithm is compared against other standard optimization techniques\nlike BFGS, Conjugate Gradient, and the traditional EM algorithm. Finally, we\npropose a similar deterministic anti-annealing based algorithm for the\nDirichlet process mixture model and demonstrate its advantages over the\nconventional variational Bayesian approach.\n", "versions": [{"version": "v1", "created": "Wed, 27 Jun 2012 19:59:59 GMT"}], "update_date": "2012-07-03", "authors_parsed": [["Naim", "Iftekhar", "", "University of Rochester"], ["Gildea", "Daniel", "", "University of\n  Rochester"]]}, {"id": "1206.6428", "submitter": "Alexandru Niculescu-Mizil", "authors": "Abhishek Kumar (University of Maryland), Alexandru Niculescu-Mizil\n  (NEC Laboratories America), Koray Kavukcuoglu (NEC Laboratories America), Hal\n  Daume III (University of Maryland)", "title": "A Binary Classification Framework for Two-Stage Multiple Kernel Learning", "comments": "Appears in Proceedings of the 29th International Conference on\n  Machine Learning (ICML 2012)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  With the advent of kernel methods, automating the task of specifying a\nsuitable kernel has become increasingly important. In this context, the\nMultiple Kernel Learning (MKL) problem of finding a combination of\npre-specified base kernels that is suitable for the task at hand has received\nsignificant attention from researchers. In this paper we show that Multiple\nKernel Learning can be framed as a standard binary classification problem with\nadditional constraints that ensure the positive definiteness of the learned\nkernel. Framing MKL in this way has the distinct advantage that it makes it\neasy to leverage the extensive research in binary classification to develop\nbetter performing and more scalable MKL algorithms that are conceptually\nsimpler, and, arguably, more accessible to practitioners. Experiments on nine\ndata sets from different domains show that, despite its simplicity, the\nproposed technique compares favorably with current leading MKL approaches.\n", "versions": [{"version": "v1", "created": "Wed, 27 Jun 2012 19:59:59 GMT"}], "update_date": "2012-07-03", "authors_parsed": [["Kumar", "Abhishek", "", "University of Maryland"], ["Niculescu-Mizil", "Alexandru", "", "NEC Laboratories America"], ["Kavukcuoglu", "Koray", "", "NEC Laboratories America"], ["Daume", "Hal", "III", "University of Maryland"]]}, {"id": "1206.6429", "submitter": "Deepti Pachauri", "authors": "Deepti Pachauri (University of Wisconsin Madison), Maxwell Collins\n  (University of Wisconsin Madison), Vikas SIngh (University of Wisconsin\n  Madison), Risi Kondor (University of Chicago)", "title": "Incorporating Domain Knowledge in Matching Problems via Harmonic\n  Analysis", "comments": "Appears in Proceedings of the 29th International Conference on\n  Machine Learning (ICML 2012)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Matching one set of objects to another is a ubiquitous task in machine\nlearning and computer vision that often reduces to some form of the quadratic\nassignment problem (QAP). The QAP is known to be notoriously hard, both in\ntheory and in practice. Here, we investigate if this difficulty can be\nmitigated when some additional piece of information is available: (a) that all\nQAP instances of interest come from the same application, and (b) the correct\nsolution for a set of such QAP instances is given. We propose a new approach to\naccelerate the solution of QAPs based on learning parameters for a modified\nobjective function from prior QAP instances. A key feature of our approach is\nthat it takes advantage of the algebraic structure of permutations, in\nconjunction with special methods for optimizing functions over the symmetric\ngroup Sn in Fourier space. Experiments show that in practical domains the new\nmethod can outperform existing approaches.\n", "versions": [{"version": "v1", "created": "Wed, 27 Jun 2012 19:59:59 GMT"}], "update_date": "2012-07-03", "authors_parsed": [["Pachauri", "Deepti", "", "University of Wisconsin Madison"], ["Collins", "Maxwell", "", "University of Wisconsin Madison"], ["SIngh", "Vikas", "", "University of Wisconsin\n  Madison"], ["Kondor", "Risi", "", "University of Chicago"]]}, {"id": "1206.6430", "submitter": "John Paisley", "authors": "John Paisley (UC Berkeley), David Blei (Princeton University), Michael\n  Jordan (UC Berkeley)", "title": "Variational Bayesian Inference with Stochastic Search", "comments": "Appears in Proceedings of the 29th International Conference on\n  Machine Learning (ICML 2012)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.CO stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Mean-field variational inference is a method for approximate Bayesian\nposterior inference. It approximates a full posterior distribution with a\nfactorized set of distributions by maximizing a lower bound on the marginal\nlikelihood. This requires the ability to integrate a sum of terms in the log\njoint likelihood using this factorized distribution. Often not all integrals\nare in closed form, which is typically handled by using a lower bound. We\npresent an alternative algorithm based on stochastic optimization that allows\nfor direct optimization of the variational lower bound. This method uses\ncontrol variates to reduce the variance of the stochastic search gradient, in\nwhich existing lower bounds can play an important role. We demonstrate the\napproach on two non-conjugate models: logistic regression and an approximation\nto the HDP.\n", "versions": [{"version": "v1", "created": "Wed, 27 Jun 2012 19:59:59 GMT"}], "update_date": "2012-07-03", "authors_parsed": [["Paisley", "John", "", "UC Berkeley"], ["Blei", "David", "", "Princeton University"], ["Jordan", "Michael", "", "UC Berkeley"]]}, {"id": "1206.6431", "submitter": "Robert Peharz", "authors": "Robert Peharz (Graz University of Technology), Franz Pernkopf (Graz\n  University of Technology)", "title": "Exact Maximum Margin Structure Learning of Bayesian Networks", "comments": "ICML", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recently, there has been much interest in finding globally optimal Bayesian\nnetwork structures. These techniques were developed for generative scores and\ncan not be directly extended to discriminative scores, as desired for\nclassification. In this paper, we propose an exact method for finding network\nstructures maximizing the probabilistic soft margin, a successfully applied\ndiscriminative score. Our method is based on branch-and-bound techniques within\na linear programming framework and maintains an any-time solution, together\nwith worst-case sub-optimality bounds. We apply a set of order constraints for\nenforcing the network structure to be acyclic, which allows a compact problem\nrepresentation and the use of general-purpose optimization techniques. In\nclassification experiments, our methods clearly outperform generatively trained\nnetwork structures and compete with support vector machines.\n", "versions": [{"version": "v1", "created": "Wed, 27 Jun 2012 19:59:59 GMT"}], "update_date": "2012-07-03", "authors_parsed": [["Peharz", "Robert", "", "Graz University of Technology"], ["Pernkopf", "Franz", "", "Graz\n  University of Technology"]]}, {"id": "1206.6432", "submitter": "Alain Rakotomamonjy", "authors": "Alain Rakotomamonjy (Universite de Rouen)", "title": "Sparse Support Vector Infinite Push", "comments": "Appears in Proceedings of the 29th International Conference on\n  Machine Learning (ICML 2012)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CE stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we address the problem of embedded feature selection for\nranking on top of the list problems. We pose this problem as a regularized\nempirical risk minimization with $p$-norm push loss function ($p=\\infty$) and\nsparsity inducing regularizers. We leverage the issues related to this\nchallenging optimization problem by considering an alternating direction method\nof multipliers algorithm which is built upon proximal operators of the loss\nfunction and the regularizer. Our main technical contribution is thus to\nprovide a numerical scheme for computing the infinite push loss function\nproximal operator. Experimental results on toy, DNA microarray and BCI problems\nshow how our novel algorithm compares favorably to competitors for ranking on\ntop while using fewer variables in the scoring function.\n", "versions": [{"version": "v1", "created": "Wed, 27 Jun 2012 19:59:59 GMT"}], "update_date": "2012-07-03", "authors_parsed": [["Rakotomamonjy", "Alain", "", "Universite de Rouen"]]}, {"id": "1206.6433", "submitter": "Melanie Rey", "authors": "Melanie Rey (University of Basel), Volker Roth (University of Basel)", "title": "Copula Mixture Model for Dependency-seeking Clustering", "comments": "Appears in Proceedings of the 29th International Conference on\n  Machine Learning (ICML 2012)", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We introduce a copula mixture model to perform dependency-seeking clustering\nwhen co-occurring samples from different data sources are available. The model\ntakes advantage of the great flexibility offered by the copulas framework to\nextend mixtures of Canonical Correlation Analysis to multivariate data with\narbitrary continuous marginal densities. We formulate our model as a\nnon-parametric Bayesian mixture, while providing efficient MCMC inference.\nExperiments on synthetic and real data demonstrate that the increased\nflexibility of the copula mixture significantly improves the clustering and the\ninterpretability of the results.\n", "versions": [{"version": "v1", "created": "Wed, 27 Jun 2012 19:59:59 GMT"}], "update_date": "2012-07-03", "authors_parsed": [["Rey", "Melanie", "", "University of Basel"], ["Roth", "Volker", "", "University of Basel"]]}, {"id": "1206.6434", "submitter": "Salah Rifai", "authors": "Salah Rifai (Universite de Montreal), Yoshua Bengio (Universite de\n  Montreal), Yann Dauphin (Universite de Montreal), Pascal Vincent (Universite\n  de Montreal)", "title": "A Generative Process for Sampling Contractive Auto-Encoders", "comments": "Appears in Proceedings of the 29th International Conference on\n  Machine Learning (ICML 2012)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The contractive auto-encoder learns a representation of the input data that\ncaptures the local manifold structure around each data point, through the\nleading singular vectors of the Jacobian of the transformation from input to\nrepresentation. The corresponding singular values specify how much local\nvariation is plausible in directions associated with the corresponding singular\nvectors, while remaining in a high-density region of the input space. This\npaper proposes a procedure for generating samples that are consistent with the\nlocal structure captured by a contractive auto-encoder. The associated\nstochastic process defines a distribution from which one can sample, and which\nexperimentally appears to converge quickly and mix well between modes, compared\nto Restricted Boltzmann Machines and Deep Belief Networks. The intuitions\nbehind this procedure can also be used to train the second layer of contraction\nthat pools lower-level features and learns to be invariant to the local\ndirections of variation discovered in the first layer. We show that this can\nhelp learn and represent invariances present in the data and improve\nclassification error.\n", "versions": [{"version": "v1", "created": "Wed, 27 Jun 2012 19:59:59 GMT"}], "update_date": "2012-07-03", "authors_parsed": [["Rifai", "Salah", "", "Universite de Montreal"], ["Bengio", "Yoshua", "", "Universite de\n  Montreal"], ["Dauphin", "Yann", "", "Universite de Montreal"], ["Vincent", "Pascal", "", "Universite\n  de Montreal"]]}, {"id": "1206.6435", "submitter": "Issei Sato", "authors": "Issei Sato (The University of Tokyo), Hiroshi Nakagawa (The University\n  of Tokyo)", "title": "Rethinking Collapsed Variational Bayes Inference for LDA", "comments": "Appears in Proceedings of the 29th International Conference on\n  Machine Learning (ICML 2012)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a novel interpretation of the collapsed variational Bayes\ninference with a zero-order Taylor expansion approximation, called CVB0\ninference, for latent Dirichlet allocation (LDA). We clarify the properties of\nthe CVB0 inference by using the alpha-divergence. We show that the CVB0\ninference is composed of two different divergence projections: alpha=1 and -1.\nThis interpretation will help shed light on CVB0 works.\n", "versions": [{"version": "v1", "created": "Wed, 27 Jun 2012 19:59:59 GMT"}], "update_date": "2012-07-03", "authors_parsed": [["Sato", "Issei", "", "The University of Tokyo"], ["Nakagawa", "Hiroshi", "", "The University\n  of Tokyo"]]}, {"id": "1206.6436", "submitter": "Alexander Schwing", "authors": "Alexander Schwing (ETH Zurich), Tamir Hazan (TTIC), Marc Pollefeys\n  (ETH Zurich), Raquel Urtasun (TTIC)", "title": "Efficient Structured Prediction with Latent Variables for General\n  Graphical Models", "comments": "Appears in Proceedings of the 29th International Conference on\n  Machine Learning (ICML 2012)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper we propose a unified framework for structured prediction with\nlatent variables which includes hidden conditional random fields and latent\nstructured support vector machines as special cases. We describe a local\nentropy approximation for this general formulation using duality, and derive an\nefficient message passing algorithm that is guaranteed to converge. We\ndemonstrate its effectiveness in the tasks of image segmentation as well as 3D\nindoor scene understanding from single images, showing that our approach is\nsuperior to latent structured support vector machines and hidden conditional\nrandom fields.\n", "versions": [{"version": "v1", "created": "Wed, 27 Jun 2012 19:59:59 GMT"}], "update_date": "2012-07-03", "authors_parsed": [["Schwing", "Alexander", "", "ETH Zurich"], ["Hazan", "Tamir", "", "TTIC"], ["Pollefeys", "Marc", "", "ETH Zurich"], ["Urtasun", "Raquel", "", "TTIC"]]}, {"id": "1206.6437", "submitter": "Matthias Seeger", "authors": "Young Jun Ko (Ecole Polytechnique Federale de Lausanne), Matthias\n  Seeger (Ecole Polytechnique Federale de Lausanne)", "title": "Large Scale Variational Bayesian Inference for Structured Scale Mixture\n  Models", "comments": "Appears in Proceedings of the 29th International Conference on\n  Machine Learning (ICML 2012)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Natural image statistics exhibit hierarchical dependencies across multiple\nscales. Representing such prior knowledge in non-factorial latent tree models\ncan boost performance of image denoising, inpainting, deconvolution or\nreconstruction substantially, beyond standard factorial \"sparse\" methodology.\nWe derive a large scale approximate Bayesian inference algorithm for linear\nmodels with non-factorial (latent tree-structured) scale mixture priors.\nExperimental results on a range of denoising and inpainting problems\ndemonstrate substantially improved performance compared to MAP estimation or to\ninference with factorial priors.\n", "versions": [{"version": "v1", "created": "Wed, 27 Jun 2012 19:59:59 GMT"}], "update_date": "2012-07-03", "authors_parsed": [["Ko", "Young Jun", "", "Ecole Polytechnique Federale de Lausanne"], ["Seeger", "Matthias", "", "Ecole Polytechnique Federale de Lausanne"]]}, {"id": "1206.6438", "submitter": "Fei Sha", "authors": "Yuan Shi (University of Southern California), Fei Sha (University of\n  Southern California)", "title": "Information-Theoretical Learning of Discriminative Clusters for\n  Unsupervised Domain Adaptation", "comments": "Appears in Proceedings of the 29th International Conference on\n  Machine Learning (ICML 2012)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study the problem of unsupervised domain adaptation, which aims to adapt\nclassifiers trained on a labeled source domain to an unlabeled target domain.\nMany existing approaches first learn domain-invariant features and then\nconstruct classifiers with them. We propose a novel approach that jointly learn\nthe both. Specifically, while the method identifies a feature space where data\nin the source and the target domains are similarly distributed, it also learns\nthe feature space discriminatively, optimizing an information-theoretic metric\nas an proxy to the expected misclassification error on the target domain. We\nshow how this optimization can be effectively carried out with simple\ngradient-based methods and how hyperparameters can be cross-validated without\ndemanding any labeled data from the target domain. Empirical studies on\nbenchmark tasks of object recognition and sentiment analysis validated our\nmodeling assumptions and demonstrated significant improvement of our method\nover competing ones in classification accuracies.\n", "versions": [{"version": "v1", "created": "Wed, 27 Jun 2012 19:59:59 GMT"}], "update_date": "2012-07-03", "authors_parsed": [["Shi", "Yuan", "", "University of Southern California"], ["Sha", "Fei", "", "University of\n  Southern California"]]}, {"id": "1206.6440", "submitter": "Or Sheffet", "authors": "Or Sheffet (Carnegie Mellon University), Nina Mishra (Microsoft\n  Research), Samuel Ieong (Microsoft Research)", "title": "Predicting Preference Flips in Commerce Search", "comments": "Appears in Proceedings of the 29th International Conference on\n  Machine Learning (ICML 2012)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Traditional approaches to ranking in web search follow the paradigm of\nrank-by-score: a learned function gives each query-URL combination an absolute\nscore and URLs are ranked according to this score. This paradigm ensures that\nif the score of one URL is better than another then one will always be ranked\nhigher than the other. Scoring contradicts prior work in behavioral economics\nthat showed that users' preferences between two items depend not only on the\nitems but also on the presented alternatives. Thus, for the same query, users'\npreference between items A and B depends on the presence/absence of item C. We\npropose a new model of ranking, the Random Shopper Model, that allows and\nexplains such behavior. In this model, each feature is viewed as a Markov chain\nover the items to be ranked, and the goal is to find a weighting of the\nfeatures that best reflects their importance. We show that our model can be\nlearned under the empirical risk minimization framework, and give an efficient\nlearning algorithm. Experiments on commerce search logs demonstrate that our\nalgorithm outperforms scoring-based approaches including regression and\nlistwise ranking.\n", "versions": [{"version": "v1", "created": "Wed, 27 Jun 2012 19:59:59 GMT"}], "update_date": "2012-07-03", "authors_parsed": [["Sheffet", "Or", "", "Carnegie Mellon University"], ["Mishra", "Nina", "", "Microsoft\n  Research"], ["Ieong", "Samuel", "", "Microsoft Research"]]}, {"id": "1206.6441", "submitter": "Athina Spiliopoulou", "authors": "Athina Spiliopoulou (University of Edinburgh), Amos Storkey\n  (University of Edinburgh)", "title": "A Topic Model for Melodic Sequences", "comments": "Appears in Proceedings of the 29th International Conference on\n  Machine Learning (ICML 2012)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.IR stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We examine the problem of learning a probabilistic model for melody directly\nfrom musical sequences belonging to the same genre. This is a challenging task\nas one needs to capture not only the rich temporal structure evident in music,\nbut also the complex statistical dependencies among different music components.\nTo address this problem we introduce the Variable-gram Topic Model, which\ncouples the latent topic formalism with a systematic model for contextual\ninformation. We evaluate the model on next-step prediction. Additionally, we\npresent a novel way of model evaluation, where we directly compare model\nsamples with data sequences using the Maximum Mean Discrepancy of string\nkernels, to assess how close is the model distribution to the data\ndistribution. We show that the model has the highest performance under both\nevaluation measures when compared to LDA, the Topic Bigram and related\nnon-topic models.\n", "versions": [{"version": "v1", "created": "Wed, 27 Jun 2012 19:59:59 GMT"}], "update_date": "2012-07-03", "authors_parsed": [["Spiliopoulou", "Athina", "", "University of Edinburgh"], ["Storkey", "Amos", "", "University of Edinburgh"]]}, {"id": "1206.6442", "submitter": "Karthik Sridharan", "authors": "Shai Ben-David (University of Waterloo), David Loker (University of\n  Waterloo), Nathan Srebro (TTIC), Karthik Sridharan (University of\n  Pennsylvania)", "title": "Minimizing The Misclassification Error Rate Using a Surrogate Convex\n  Loss", "comments": "Appears in Proceedings of the 29th International Conference on\n  Machine Learning (ICML 2012)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We carefully study how well minimizing convex surrogate loss functions,\ncorresponds to minimizing the misclassification error rate for the problem of\nbinary classification with linear predictors. In particular, we show that\namongst all convex surrogate losses, the hinge loss gives essentially the best\npossible bound, of all convex loss functions, for the misclassification error\nrate of the resulting linear predictor in terms of the best possible margin\nerror rate. We also provide lower bounds for specific convex surrogates that\nshow how different commonly used losses qualitatively differ from each other.\n", "versions": [{"version": "v1", "created": "Wed, 27 Jun 2012 19:59:59 GMT"}], "update_date": "2012-07-03", "authors_parsed": [["Ben-David", "Shai", "", "University of Waterloo"], ["Loker", "David", "", "University of\n  Waterloo"], ["Srebro", "Nathan", "", "TTIC"], ["Sridharan", "Karthik", "", "University of\n  Pennsylvania"]]}, {"id": "1206.6443", "submitter": "Amos Storkey", "authors": "Amos Storkey (University of Edinburgh), Jono Millin (University of\n  Edinburgh), Krzysztof Geras (University of Edinburgh)", "title": "Isoelastic Agents and Wealth Updates in Machine Learning Markets", "comments": "Appears in Proceedings of the 29th International Conference on\n  Machine Learning (ICML 2012)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.GT stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recently, prediction markets have shown considerable promise for developing\nflexible mechanisms for machine learning. In this paper, agents with isoelastic\nutilities are considered. It is shown that the costs associated with\nhomogeneous markets of agents with isoelastic utilities produce equilibrium\nprices corresponding to alpha-mixtures, with a particular form of mixing\ncomponent relating to each agent's wealth. We also demonstrate that wealth\naccumulation for logarithmic and other isoelastic agents (through payoffs on\nprediction of training targets) can implement both Bayesian model updates and\nmixture weight updates by imposing different market payoff structures. An\niterative algorithm is given for market equilibrium computation. We demonstrate\nthat inhomogeneous markets of agents with isoelastic utilities outperform state\nof the art aggregate classifiers such as random forests, as well as single\nclassifiers (neural networks, decision trees) on a number of machine learning\nbenchmarks, and show that isoelastic combination methods are generally better\nthan their logarithmic counterparts.\n", "versions": [{"version": "v1", "created": "Wed, 27 Jun 2012 19:59:59 GMT"}, {"version": "v2", "created": "Tue, 4 Sep 2012 17:50:18 GMT"}], "update_date": "2012-09-05", "authors_parsed": [["Storkey", "Amos", "", "University of Edinburgh"], ["Millin", "Jono", "", "University of\n  Edinburgh"], ["Geras", "Krzysztof", "", "University of Edinburgh"]]}, {"id": "1206.6444", "submitter": "Csaba Szepesvari", "authors": "Bernardo Avila Pires (University of Alberta), Csaba Szepesvari\n  (University of Alberta)", "title": "Statistical Linear Estimation with Penalized Estimators: an Application\n  to Reinforcement Learning", "comments": "Appears in Proceedings of the 29th International Conference on\n  Machine Learning (ICML 2012)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Motivated by value function estimation in reinforcement learning, we study\nstatistical linear inverse problems, i.e., problems where the coefficients of a\nlinear system to be solved are observed in noise. We consider penalized\nestimators, where performance is evaluated using a matrix-weighted two-norm of\nthe defect of the estimator measured with respect to the true, unknown\ncoefficients. Two objective functions are considered depending whether the\nerror of the defect measured with respect to the noisy coefficients is squared\nor unsquared. We propose simple, yet novel and theoretically well-founded\ndata-dependent choices for the regularization parameters for both cases that\navoid data-splitting. A distinguishing feature of our analysis is that we\nderive deterministic error bounds in terms of the error of the coefficients,\nthus allowing the complete separation of the analysis of the stochastic\nproperties of these errors. We show that our results lead to new insights and\nbounds for linear value function estimation in reinforcement learning.\n", "versions": [{"version": "v1", "created": "Wed, 27 Jun 2012 19:59:59 GMT"}], "update_date": "2012-07-03", "authors_parsed": [["Pires", "Bernardo Avila", "", "University of Alberta"], ["Szepesvari", "Csaba", "", "University of Alberta"]]}, {"id": "1206.6445", "submitter": "Yichuan Tang", "authors": "Yichuan Tang (University of Toronto), Ruslan Salakhutdinov (University\n  of Toronto), Geoffrey Hinton (University of Toronto)", "title": "Deep Lambertian Networks", "comments": "Appears in Proceedings of the 29th International Conference on\n  Machine Learning (ICML 2012)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Visual perception is a challenging problem in part due to illumination\nvariations. A possible solution is to first estimate an illumination invariant\nrepresentation before using it for recognition. The object albedo and surface\nnormals are examples of such representations. In this paper, we introduce a\nmultilayer generative model where the latent variables include the albedo,\nsurface normals, and the light source. Combining Deep Belief Nets with the\nLambertian reflectance assumption, our model can learn good priors over the\nalbedo from 2D images. Illumination variations can be explained by changing\nonly the lighting latent variable in our model. By transferring learned\nknowledge from similar objects, albedo and surface normals estimation from a\nsingle image is possible in our model. Experiments demonstrate that our model\nis able to generalize as well as improve over standard baselines in one-shot\nface recognition.\n", "versions": [{"version": "v1", "created": "Wed, 27 Jun 2012 19:59:59 GMT"}], "update_date": "2012-07-03", "authors_parsed": [["Tang", "Yichuan", "", "University of Toronto"], ["Salakhutdinov", "Ruslan", "", "University\n  of Toronto"], ["Hinton", "Geoffrey", "", "University of Toronto"]]}, {"id": "1206.6446", "submitter": "Matus Telgarsky", "authors": "Matus Telgarsky (UCSD), Sanjoy Dasgupta (UCSD)", "title": "Agglomerative Bregman Clustering", "comments": "Appears in Proceedings of the 29th International Conference on\n  Machine Learning (ICML 2012)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This manuscript develops the theory of agglomerative clustering with Bregman\ndivergences. Geometric smoothing techniques are developed to deal with\ndegenerate clusters. To allow for cluster models based on exponential families\nwith overcomplete representations, Bregman divergences are developed for\nnondifferentiable convex functions.\n", "versions": [{"version": "v1", "created": "Wed, 27 Jun 2012 19:59:59 GMT"}], "update_date": "2012-07-03", "authors_parsed": [["Telgarsky", "Matus", "", "UCSD"], ["Dasgupta", "Sanjoy", "", "UCSD"]]}, {"id": "1206.6447", "submitter": "Gael Varoquaux", "authors": "Gael Varoquaux (INRIA), Alexandre Gramfort (INRIA), Bertrand Thirion\n  (INRIA)", "title": "Small-sample Brain Mapping: Sparse Recovery on Spatially Correlated\n  Designs with Randomization and Clustering", "comments": "Appears in Proceedings of the 29th International Conference on\n  Machine Learning (ICML 2012)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV stat.AP stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Functional neuroimaging can measure the brain?s response to an external\nstimulus. It is used to perform brain mapping: identifying from these\nobservations the brain regions involved. This problem can be cast into a linear\nsupervised learning task where the neuroimaging data are used as predictors for\nthe stimulus. Brain mapping is then seen as a support recovery problem. On\nfunctional MRI (fMRI) data, this problem is particularly challenging as i) the\nnumber of samples is small due to limited acquisition time and ii) the\nvariables are strongly correlated. We propose to overcome these difficulties\nusing sparse regression models over new variables obtained by clustering of the\noriginal variables. The use of randomization techniques, e.g. bootstrap\nsamples, and clustering of the variables improves the recovery properties of\nsparse methods. We demonstrate the benefit of our approach on an extensive\nsimulation study as well as two fMRI datasets.\n", "versions": [{"version": "v1", "created": "Wed, 27 Jun 2012 19:59:59 GMT"}], "update_date": "2012-07-03", "authors_parsed": [["Varoquaux", "Gael", "", "INRIA"], ["Gramfort", "Alexandre", "", "INRIA"], ["Thirion", "Bertrand", "", "INRIA"]]}, {"id": "1206.6448", "submitter": "Huahua Wang", "authors": "Huahua Wang (University of Minnesota), Arindam Banerjee (University of\n  Minnesota)", "title": "Online Alternating Direction Method", "comments": "Appears in Proceedings of the 29th International Conference on\n  Machine Learning (ICML 2012)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Online optimization has emerged as powerful tool in large scale optimization.\nIn this paper, we introduce efficient online algorithms based on the\nalternating directions method (ADM). We introduce a new proof technique for ADM\nin the batch setting, which yields the O(1/T) convergence rate of ADM and forms\nthe basis of regret analysis in the online setting. We consider two scenarios\nin the online setting, based on whether the solution needs to lie in the\nfeasible set or not. In both settings, we establish regret bounds for both the\nobjective function as well as constraint violation for general and strongly\nconvex functions. Preliminary results are presented to illustrate the\nperformance of the proposed algorithms.\n", "versions": [{"version": "v1", "created": "Wed, 27 Jun 2012 19:59:59 GMT"}], "update_date": "2012-07-03", "authors_parsed": [["Wang", "Huahua", "", "University of Minnesota"], ["Banerjee", "Arindam", "", "University of\n  Minnesota"]]}, {"id": "1206.6449", "submitter": "Yi Wang", "authors": "Yi Wang (NUS), Kok Sung Won (NUS), David Hsu (NUS), Wee Sun Lee (NUS)", "title": "Monte Carlo Bayesian Reinforcement Learning", "comments": "Appears in Proceedings of the 29th International Conference on\n  Machine Learning (ICML 2012)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Bayesian reinforcement learning (BRL) encodes prior knowledge of the world in\na model and represents uncertainty in model parameters by maintaining a\nprobability distribution over them. This paper presents Monte Carlo BRL\n(MC-BRL), a simple and general approach to BRL. MC-BRL samples a priori a\nfinite set of hypotheses for the model parameter values and forms a discrete\npartially observable Markov decision process (POMDP) whose state space is a\ncross product of the state space for the reinforcement learning task and the\nsampled model parameter space. The POMDP does not require conjugate\ndistributions for belief representation, as earlier works do, and can be solved\nrelatively easily with point-based approximation algorithms. MC-BRL naturally\nhandles both fully and partially observable worlds. Theoretical and\nexperimental results show that the discrete POMDP approximates the underlying\nBRL task well with guaranteed performance.\n", "versions": [{"version": "v1", "created": "Wed, 27 Jun 2012 19:59:59 GMT"}], "update_date": "2012-07-03", "authors_parsed": [["Wang", "Yi", "", "NUS"], ["Won", "Kok Sung", "", "NUS"], ["Hsu", "David", "", "NUS"], ["Lee", "Wee Sun", "", "NUS"]]}, {"id": "1206.6450", "submitter": "Min Xu", "authors": "Min Xu (Carnegie Mellon University), John Lafferty (University of\n  Chicago)", "title": "Conditional Sparse Coding and Grouped Multivariate Regression", "comments": "Appears in Proceedings of the 29th International Conference on\n  Machine Learning (ICML 2012)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study the problem of multivariate regression where the data are naturally\ngrouped, and a regression matrix is to be estimated for each group. We propose\nan approach in which a dictionary of low rank parameter matrices is estimated\nacross groups, and a sparse linear combination of the dictionary elements is\nestimated to form a model within each group. We refer to the method as\nconditional sparse coding since it is a coding procedure for the response\nvectors Y conditioned on the covariate vectors X. This approach captures the\nshared information across the groups while adapting to the structure within\neach group. It exploits the same intuition behind sparse coding that has been\nsuccessfully developed in computer vision and computational neuroscience. We\npropose an algorithm for conditional sparse coding, analyze its theoretical\nproperties in terms of predictive accuracy, and present the results of\nsimulation and brain imaging experiments that compare the new technique to\nreduced rank regression.\n", "versions": [{"version": "v1", "created": "Wed, 27 Jun 2012 19:59:59 GMT"}], "update_date": "2012-07-03", "authors_parsed": [["Xu", "Min", "", "Carnegie Mellon University"], ["Lafferty", "John", "", "University of\n  Chicago"]]}, {"id": "1206.6451", "submitter": "Zhixiang Xu", "authors": "Zhixiang Xu (Washington University, St. Louis), Kilian Weinberger\n  (Washington University, St. Louis), Olivier Chapelle (Criteo)", "title": "The Greedy Miser: Learning under Test-time Budgets", "comments": "Appears in Proceedings of the 29th International Conference on\n  Machine Learning (ICML 2012)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  As machine learning algorithms enter applications in industrial settings,\nthere is increased interest in controlling their cpu-time during testing. The\ncpu-time consists of the running time of the algorithm and the extraction time\nof the features. The latter can vary drastically when the feature set is\ndiverse. In this paper, we propose an algorithm, the Greedy Miser, that\nincorporates the feature extraction cost during training to explicitly minimize\nthe cpu-time during testing. The algorithm is a straightforward extension of\nstage-wise regression and is equally suitable for regression or multi-class\nclassification. Compared to prior work, it is significantly more cost-effective\nand scales to larger data sets.\n", "versions": [{"version": "v1", "created": "Wed, 27 Jun 2012 19:59:59 GMT"}], "update_date": "2012-07-03", "authors_parsed": [["Xu", "Zhixiang", "", "Washington University, St. Louis"], ["Weinberger", "Kilian", "", "Washington University, St. Louis"], ["Chapelle", "Olivier", "", "Criteo"]]}, {"id": "1206.6452", "submitter": "Benjamin Yackley", "authors": "Benjamin Yackley (University of New Mexico), Terran Lane (University\n  of New Mexico)", "title": "Smoothness and Structure Learning by Proxy", "comments": "Appears in Proceedings of the 29th International Conference on\n  Machine Learning (ICML 2012)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG math.OC stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  As data sets grow in size, the ability of learning methods to find structure\nin them is increasingly hampered by the time needed to search the large spaces\nof possibilities and generate a score for each that takes all of the observed\ndata into account. For instance, Bayesian networks, the model chosen in this\npaper, have a super-exponentially large search space for a fixed number of\nvariables. One possible method to alleviate this problem is to use a proxy,\nsuch as a Gaussian Process regressor, in place of the true scoring function,\ntraining it on a selection of sampled networks. We prove here that the use of\nsuch a proxy is well-founded, as we can bound the smoothness of a commonly-used\nscoring function for Bayesian network structure learning. We show here that,\ncompared to an identical search strategy using the network?s exact scores, our\nproxy-based search is able to get equivalent or better scores on a number of\ndata sets in a fraction of the time.\n", "versions": [{"version": "v1", "created": "Wed, 27 Jun 2012 19:59:59 GMT"}], "update_date": "2012-07-03", "authors_parsed": [["Yackley", "Benjamin", "", "University of New Mexico"], ["Lane", "Terran", "", "University\n  of New Mexico"]]}, {"id": "1206.6453", "submitter": "Florian Yger", "authors": "Florian Yger (LITIS), Maxime Berar (LITIS), Gilles Gasso (INSA de\n  Rouen), Alain Rakotomamonjy (INSA de Rouen)", "title": "Adaptive Canonical Correlation Analysis Based On Matrix Manifolds", "comments": "Appears in Proceedings of the 29th International Conference on\n  Machine Learning (ICML 2012)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we formulate the Canonical Correlation Analysis (CCA) problem\non matrix manifolds. This framework provides a natural way for dealing with\nmatrix constraints and tools for building efficient algorithms even in an\nadaptive setting. Finally, an adaptive CCA algorithm is proposed and applied to\na change detection problem in EEG signals.\n", "versions": [{"version": "v1", "created": "Wed, 27 Jun 2012 19:59:59 GMT"}], "update_date": "2012-07-03", "authors_parsed": [["Yger", "Florian", "", "LITIS"], ["Berar", "Maxime", "", "LITIS"], ["Gasso", "Gilles", "", "INSA de\n  Rouen"], ["Rakotomamonjy", "Alain", "", "INSA de Rouen"]]}, {"id": "1206.6454", "submitter": "Yisong Yue", "authors": "Yisong Yue (Carnegie Mellon University), Sue Ann Hong (Carnegie Mellon\n  University), Carlos Guestrin (Carnegie Mellon University)", "title": "Hierarchical Exploration for Accelerating Contextual Bandits", "comments": "Appears in Proceedings of the 29th International Conference on\n  Machine Learning (ICML 2012)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Contextual bandit learning is an increasingly popular approach to optimizing\nrecommender systems via user feedback, but can be slow to converge in practice\ndue to the need for exploring a large feature space. In this paper, we propose\na coarse-to-fine hierarchical approach for encoding prior knowledge that\ndrastically reduces the amount of exploration required. Intuitively, user\npreferences can be reasonably embedded in a coarse low-dimensional feature\nspace that can be explored efficiently, requiring exploration in the\nhigh-dimensional space only as necessary. We introduce a bandit algorithm that\nexplores within this coarse-to-fine spectrum, and prove performance guarantees\nthat depend on how well the coarse space captures the user's preferences. We\ndemonstrate substantial improvement over conventional bandit algorithms through\nextensive simulation as well as a live user study in the setting of\npersonalized news recommendation.\n", "versions": [{"version": "v1", "created": "Wed, 27 Jun 2012 19:59:59 GMT"}], "update_date": "2012-07-03", "authors_parsed": [["Yue", "Yisong", "", "Carnegie Mellon University"], ["Hong", "Sue Ann", "", "Carnegie Mellon\n  University"], ["Guestrin", "Carlos", "", "Carnegie Mellon University"]]}, {"id": "1206.6455", "submitter": "Xinhua Zhang", "authors": "Yaoliang Yu (University of Alberta), James Neufeld (University of\n  Alberta), Ryan Kiros (University of Alberta), Xinhua Zhang (University of\n  Alberta), Dale Schuurmans (University of Alberta)", "title": "Regularizers versus Losses for Nonlinear Dimensionality Reduction: A\n  Factored View with New Convex Relaxations", "comments": "Appears in Proceedings of the 29th International Conference on\n  Machine Learning (ICML 2012)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We demonstrate that almost all non-parametric dimensionality reduction\nmethods can be expressed by a simple procedure: regularized loss minimization\nplus singular value truncation. By distinguishing the role of the loss and\nregularizer in such a process, we recover a factored perspective that reveals\nsome gaps in the current literature. Beyond identifying a useful new loss for\nmanifold unfolding, a key contribution is to derive new convex regularizers\nthat combine distance maximization with rank reduction. These regularizers can\nbe applied to any loss.\n", "versions": [{"version": "v1", "created": "Wed, 27 Jun 2012 19:59:59 GMT"}], "update_date": "2012-07-03", "authors_parsed": [["Yu", "Yaoliang", "", "University of Alberta"], ["Neufeld", "James", "", "University of\n  Alberta"], ["Kiros", "Ryan", "", "University of Alberta"], ["Zhang", "Xinhua", "", "University of\n  Alberta"], ["Schuurmans", "Dale", "", "University of Alberta"]]}, {"id": "1206.6457", "submitter": "Masrour Zoghi", "authors": "Nando de Freitas (University of British Columbia), Alex Smola (Yahoo!\n  Research), Masrour Zoghi (University of British Columbia)", "title": "Exponential Regret Bounds for Gaussian Process Bandits with\n  Deterministic Observations", "comments": "Appears in Proceedings of the 29th International Conference on\n  Machine Learning (ICML 2012). arXiv admin note: substantial text overlap with\n  arXiv:1203.2177", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper analyzes the problem of Gaussian process (GP) bandits with\ndeterministic observations. The analysis uses a branch and bound algorithm that\nis related to the UCB algorithm of (Srinivas et al, 2010). For GPs with\nGaussian observation noise, with variance strictly greater than zero, Srinivas\net al proved that the regret vanishes at the approximate rate of\n$O(1/\\sqrt{t})$, where t is the number of observations. To complement their\nresult, we attack the deterministic case and attain a much faster exponential\nconvergence rate. Under some regularity assumptions, we show that the regret\ndecreases asymptotically according to $O(e^{-\\frac{\\tau t}{(\\ln t)^{d/4}}})$\nwith high probability. Here, d is the dimension of the search space and tau is\na constant that depends on the behaviour of the objective function near its\nglobal maximum.\n", "versions": [{"version": "v1", "created": "Wed, 27 Jun 2012 19:59:59 GMT"}], "update_date": "2012-07-03", "authors_parsed": [["de Freitas", "Nando", "", "University of British Columbia"], ["Smola", "Alex", "", "Yahoo!\n  Research"], ["Zoghi", "Masrour", "", "University of British Columbia"]]}, {"id": "1206.6458", "submitter": "Javad Azimi", "authors": "Javad Azimi (Oregon State University), Alan Fern (Oregon State\n  University), Xiaoli Zhang-Fern (Oregon State University), Glencora Borradaile\n  (Oregon State University), Brent Heeringa (Williams College)", "title": "Batch Active Learning via Coordinated Matching", "comments": "Appears in Proceedings of the 29th International Conference on\n  Machine Learning (ICML 2012)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Most prior work on active learning of classifiers has focused on sequentially\nselecting one unlabeled example at a time to be labeled in order to reduce the\noverall labeling effort. In many scenarios, however, it is desirable to label\nan entire batch of examples at once, for example, when labels can be acquired\nin parallel. This motivates us to study batch active learning, which\niteratively selects batches of $k>1$ examples to be labeled. We propose a novel\nbatch active learning method that leverages the availability of high-quality\nand efficient sequential active-learning policies by attempting to approximate\ntheir behavior when applied for $k$ steps. Specifically, our algorithm first\nuses Monte-Carlo simulation to estimate the distribution of unlabeled examples\nselected by a sequential policy over $k$ step executions. The algorithm then\nattempts to select a set of $k$ examples that best matches this distribution,\nleading to a combinatorial optimization problem that we term \"bounded\ncoordinated matching\". While we show this problem is NP-hard in general, we\ngive an efficient greedy solution, which inherits approximation bounds from\nsupermodular minimization theory. Our experimental results on eight benchmark\ndatasets show that the proposed approach is highly effective\n", "versions": [{"version": "v1", "created": "Wed, 27 Jun 2012 19:59:59 GMT"}], "update_date": "2012-07-03", "authors_parsed": [["Azimi", "Javad", "", "Oregon State University"], ["Fern", "Alan", "", "Oregon State\n  University"], ["Zhang-Fern", "Xiaoli", "", "Oregon State University"], ["Borradaile", "Glencora", "", "Oregon State University"], ["Heeringa", "Brent", "", "Williams College"]]}, {"id": "1206.6460", "submitter": "Janardhan Rao Doppa", "authors": "Janardhan Rao Doppa (Oregon State University), Alan Fern (Oregon State\n  University), Prasad Tadepalli (Oregon State University)", "title": "Output Space Search for Structured Prediction", "comments": "Appears in Proceedings of the 29th International Conference on\n  Machine Learning (ICML 2012)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider a framework for structured prediction based on search in the\nspace of complete structured outputs. Given a structured input, an output is\nproduced by running a time-bounded search procedure guided by a learned cost\nfunction, and then returning the least cost output uncovered during the search.\nThis framework can be instantiated for a wide range of search spaces and search\nprocedures, and easily incorporates arbitrary structured-prediction loss\nfunctions. In this paper, we make two main technical contributions. First, we\ndefine the limited-discrepancy search space over structured outputs, which is\nable to leverage powerful classification learning algorithms to improve the\nsearch space quality. Second, we give a generic cost function learning\napproach, where the key idea is to learn a cost function that attempts to mimic\nthe behavior of conducting searches guided by the true loss function. Our\nexperiments on six benchmark domains demonstrate that using our framework with\nonly a small amount of search is sufficient for significantly improving on\nstate-of-the-art structured-prediction performance.\n", "versions": [{"version": "v1", "created": "Wed, 27 Jun 2012 19:59:59 GMT"}], "update_date": "2012-07-03", "authors_parsed": [["Doppa", "Janardhan Rao", "", "Oregon State University"], ["Fern", "Alan", "", "Oregon State\n  University"], ["Tadepalli", "Prasad", "", "Oregon State University"]]}, {"id": "1206.6461", "submitter": "Mohammad Gheshlaghi Azar", "authors": "Mohammad Gheshlaghi Azar (Radboud University), Remi Munos (INRIA\n  Lille), Bert Kappen (Radboud University)", "title": "On the Sample Complexity of Reinforcement Learning with a Generative\n  Model", "comments": "Appears in Proceedings of the 29th International Conference on\n  Machine Learning (ICML 2012)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider the problem of learning the optimal action-value function in the\ndiscounted-reward Markov decision processes (MDPs). We prove a new PAC bound on\nthe sample-complexity of model-based value iteration algorithm in the presence\nof the generative model, which indicates that for an MDP with N state-action\npairs and the discount factor \\gamma\\in[0,1) only\nO(N\\log(N/\\delta)/((1-\\gamma)^3\\epsilon^2)) samples are required to find an\n\\epsilon-optimal estimation of the action-value function with the probability\n1-\\delta. We also prove a matching lower bound of \\Theta\n(N\\log(N/\\delta)/((1-\\gamma)^3\\epsilon^2)) on the sample complexity of\nestimating the optimal action-value function by every RL algorithm. To the best\nof our knowledge, this is the first matching result on the sample complexity of\nestimating the optimal (action-) value function in which the upper bound\nmatches the lower bound of RL in terms of N, \\epsilon, \\delta and 1/(1-\\gamma).\nAlso, both our lower bound and our upper bound significantly improve on the\nstate-of-the-art in terms of 1/(1-\\gamma).\n", "versions": [{"version": "v1", "created": "Wed, 27 Jun 2012 19:59:59 GMT"}], "update_date": "2012-07-03", "authors_parsed": [["Azar", "Mohammad Gheshlaghi", "", "Radboud University"], ["Munos", "Remi", "", "INRIA\n  Lille"], ["Kappen", "Bert", "", "Radboud University"]]}, {"id": "1206.6462", "submitter": "Yun Jiang", "authors": "Yun Jiang (Cornell University), Marcus Lim (Cornell University),\n  Ashutosh Saxena (Cornell University)", "title": "Learning Object Arrangements in 3D Scenes using Human Context", "comments": "Appears in Proceedings of the 29th International Conference on\n  Machine Learning (ICML 2012)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV cs.RO stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider the problem of learning object arrangements in a 3D scene. The\nkey idea here is to learn how objects relate to human poses based on their\naffordances, ease of use and reachability. In contrast to modeling\nobject-object relationships, modeling human-object relationships scales\nlinearly in the number of objects. We design appropriate density functions\nbased on 3D spatial features to capture this. We learn the distribution of\nhuman poses in a scene using a variant of the Dirichlet process mixture model\nthat allows sharing of the density function parameters across the same object\ntypes. Then we can reason about arrangements of the objects in the room based\non these meaningful human poses. In our extensive experiments on 20 different\nrooms with a total of 47 objects, our algorithm predicted correct placements\nwith an average error of 1.6 meters from ground truth. In arranging five real\nscenes, it received a score of 4.3/5 compared to 3.7 for the best baseline\nmethod.\n", "versions": [{"version": "v1", "created": "Wed, 27 Jun 2012 19:59:59 GMT"}], "update_date": "2012-07-03", "authors_parsed": [["Jiang", "Yun", "", "Cornell University"], ["Lim", "Marcus", "", "Cornell University"], ["Saxena", "Ashutosh", "", "Cornell University"]]}, {"id": "1206.6463", "submitter": "Deguang Kong", "authors": "Deguang Kong (The University of Texas at Arlington), Chris H.Q. Ding\n  (The University of Texas at Arlington), Heng Huang (The University of Texas\n  at Arlington), Feiping Nie (The University of Texas at Arlington)", "title": "An Iterative Locally Linear Embedding Algorithm", "comments": "Appears in Proceedings of the 29th International Conference on\n  Machine Learning (ICML 2012)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Local Linear embedding (LLE) is a popular dimension reduction method. In this\npaper, we first show LLE with nonnegative constraint is equivalent to the\nwidely used Laplacian embedding. We further propose to iterate the two steps in\nLLE repeatedly to improve the results. Thirdly, we relax the kNN constraint of\nLLE and present a sparse similarity learning algorithm. The final Iterative LLE\ncombines these three improvements. Extensive experiment results show that\niterative LLE algorithm significantly improve both classification and\nclustering results.\n", "versions": [{"version": "v1", "created": "Wed, 27 Jun 2012 19:59:59 GMT"}], "update_date": "2012-07-03", "authors_parsed": [["Kong", "Deguang", "", "The University of Texas at Arlington"], ["Ding", "Chris H. Q.", "", "The University of Texas at Arlington"], ["Huang", "Heng", "", "The University of Texas\n  at Arlington"], ["Nie", "Feiping", "", "The University of Texas at Arlington"]]}, {"id": "1206.6464", "submitter": "James Martens", "authors": "James Martens (University of Toronto), Ilya Sutskever (University of\n  Toronto), Kevin Swersky (University of Toronto)", "title": "Estimating the Hessian by Back-propagating Curvature", "comments": "Appears in Proceedings of the 29th International Conference on\n  Machine Learning (ICML 2012)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this work we develop Curvature Propagation (CP), a general technique for\nefficiently computing unbiased approximations of the Hessian of any function\nthat is computed using a computational graph. At the cost of roughly two\ngradient evaluations, CP can give a rank-1 approximation of the whole Hessian,\nand can be repeatedly applied to give increasingly precise unbiased estimates\nof any or all of the entries of the Hessian. Of particular interest is the\ndiagonal of the Hessian, for which no general approach is known to exist that\nis both efficient and accurate. We show in experiments that CP turns out to\nwork well in practice, giving very accurate estimates of the Hessian of neural\nnetworks, for example, with a relatively small amount of work. We also apply CP\nto Score Matching, where a diagonal of a Hessian plays an integral role in the\nScore Matching objective, and where it is usually computed exactly using\ninefficient algorithms which do not scale to larger and more complex models.\n", "versions": [{"version": "v1", "created": "Wed, 27 Jun 2012 19:59:59 GMT"}, {"version": "v2", "created": "Tue, 4 Sep 2012 18:32:03 GMT"}], "update_date": "2012-09-05", "authors_parsed": [["Martens", "James", "", "University of Toronto"], ["Sutskever", "Ilya", "", "University of\n  Toronto"], ["Swersky", "Kevin", "", "University of Toronto"]]}, {"id": "1206.6465", "submitter": "Mehmet Gonen", "authors": "Mehmet Gonen (Aalto University)", "title": "Bayesian Efficient Multiple Kernel Learning", "comments": "Appears in Proceedings of the 29th International Conference on\n  Machine Learning (ICML 2012)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Multiple kernel learning algorithms are proposed to combine kernels in order\nto obtain a better similarity measure or to integrate feature representations\ncoming from different data sources. Most of the previous research on such\nmethods is focused on the computational efficiency issue. However, it is still\nnot feasible to combine many kernels using existing Bayesian approaches due to\ntheir high time complexity. We propose a fully conjugate Bayesian formulation\nand derive a deterministic variational approximation, which allows us to\ncombine hundreds or thousands of kernels very efficiently. We briefly explain\nhow the proposed method can be extended for multiclass learning and\nsemi-supervised learning. Experiments with large numbers of kernels on\nbenchmark data sets show that our inference method is quite fast, requiring\nless than a minute. On one bioinformatics and three image recognition data\nsets, our method outperforms previously reported results with better\ngeneralization performance.\n", "versions": [{"version": "v1", "created": "Wed, 27 Jun 2012 19:59:59 GMT"}], "update_date": "2012-07-03", "authors_parsed": [["Gonen", "Mehmet", "", "Aalto University"]]}, {"id": "1206.6467", "submitter": "Luke McDowell", "authors": "Luke McDowell (U.S. Naval Academy), David Aha (U.S. Naval Research\n  Laboratory)", "title": "Semi-Supervised Collective Classification via Hybrid Label\n  Regularization", "comments": "Appears in Proceedings of the 29th International Conference on\n  Machine Learning (ICML 2012)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Many classification problems involve data instances that are interlinked with\neach other, such as webpages connected by hyperlinks. Techniques for\n\"collective classification\" (CC) often increase accuracy for such data graphs,\nbut usually require a fully-labeled training graph. In contrast, we examine how\nto improve the semi-supervised learning of CC models when given only a\nsparsely-labeled graph, a common situation. We first describe how to use novel\ncombinations of classifiers to exploit the different characteristics of the\nrelational features vs. the non-relational features. We also extend the ideas\nof \"label regularization\" to such hybrid classifiers, enabling them to leverage\nthe unlabeled data to bias the learning process. We find that these techniques,\nwhich are efficient and easy to implement, significantly increase accuracy on\nthree real datasets. In addition, our results explain conflicting findings from\nprior related studies.\n", "versions": [{"version": "v1", "created": "Wed, 27 Jun 2012 19:59:59 GMT"}], "update_date": "2012-07-03", "authors_parsed": [["McDowell", "Luke", "", "U.S. Naval Academy"], ["Aha", "David", "", "U.S. Naval Research\n  Laboratory"]]}, {"id": "1206.6468", "submitter": "Gautham Mysore", "authors": "Gautham Mysore (Adobe Systems), Maneesh Sahani (University College\n  London)", "title": "Variational Inference in Non-negative Factorial Hidden Markov Models for\n  Efficient Audio Source Separation", "comments": "Appears in Proceedings of the 29th International Conference on\n  Machine Learning (ICML 2012)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.SD stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The past decade has seen substantial work on the use of non-negative matrix\nfactorization and its probabilistic counterparts for audio source separation.\nAlthough able to capture audio spectral structure well, these models neglect\nthe non-stationarity and temporal dynamics that are important properties of\naudio. The recently proposed non-negative factorial hidden Markov model\n(N-FHMM) introduces a temporal dimension and improves source separation\nperformance. However, the factorial nature of this model makes the complexity\nof inference exponential in the number of sound sources. Here, we present a\nBayesian variant of the N-FHMM suited to an efficient variational inference\nalgorithm, whose complexity is linear in the number of sound sources. Our\nalgorithm performs comparably to exact inference in the original N-FHMM but is\nsignificantly faster. In typical configurations of the N-FHMM, our method\nachieves around a 30x increase in speed.\n", "versions": [{"version": "v1", "created": "Wed, 27 Jun 2012 19:59:59 GMT"}], "update_date": "2012-07-03", "authors_parsed": [["Mysore", "Gautham", "", "Adobe Systems"], ["Sahani", "Maneesh", "", "University College\n  London"]]}, {"id": "1206.6469", "submitter": "Esther Salazar", "authors": "Esther Salazar (Duke University), Matthew Cain (Duke University),\n  Elise Darling (Duke University), Stephen Mitroff (Duke University), Lawrence\n  Carin (Duke University)", "title": "Inferring Latent Structure From Mixed Real and Categorical Relational\n  Data", "comments": "Appears in Proceedings of the 29th International Conference on\n  Machine Learning (ICML 2012)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider analysis of relational data (a matrix), in which the rows\ncorrespond to subjects (e.g., people) and the columns correspond to attributes.\nThe elements of the matrix may be a mix of real and categorical. Each subject\nand attribute is characterized by a latent binary feature vector, and an\ninferred matrix maps each row-column pair of binary feature vectors to an\nobserved matrix element. The latent binary features of the rows are modeled via\na multivariate Gaussian distribution with low-rank covariance matrix, and the\nGaussian random variables are mapped to latent binary features via a probit\nlink. The same type construction is applied jointly to the columns. The model\ninfers latent, low-dimensional binary features associated with each row and\neach column, as well correlation structure between all rows and between all\ncolumns.\n", "versions": [{"version": "v1", "created": "Wed, 27 Jun 2012 19:59:59 GMT"}], "update_date": "2012-07-03", "authors_parsed": [["Salazar", "Esther", "", "Duke University"], ["Cain", "Matthew", "", "Duke University"], ["Darling", "Elise", "", "Duke University"], ["Mitroff", "Stephen", "", "Duke University"], ["Carin", "Lawrence", "", "Duke University"]]}, {"id": "1206.6470", "submitter": "Franz Kiraly", "authors": "Franz Kiraly (TU Berlin), Ryota Tomioka (University of Tokyo)", "title": "A Combinatorial Algebraic Approach for the Identifiability of Low-Rank\n  Matrix Completion", "comments": "Appears in Proceedings of the 29th International Conference on\n  Machine Learning (ICML 2012)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.DM cs.NA stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we review the problem of matrix completion and expose its\nintimate relations with algebraic geometry, combinatorics and graph theory. We\npresent the first necessary and sufficient combinatorial conditions for\nmatrices of arbitrary rank to be identifiable from a set of matrix entries,\nyielding theoretical constraints and new algorithms for the problem of matrix\ncompletion. We conclude by algorithmically evaluating the tightness of the\ngiven conditions and algorithms for practically relevant matrix sizes, showing\nthat the algebraic-combinatoric approach can lead to improvements over\nstate-of-the-art matrix completion methods.\n", "versions": [{"version": "v1", "created": "Wed, 27 Jun 2012 19:59:59 GMT"}], "update_date": "2012-07-03", "authors_parsed": [["Kiraly", "Franz", "", "TU Berlin"], ["Tomioka", "Ryota", "", "University of Tokyo"]]}, {"id": "1206.6471", "submitter": "Bernhard Schoelkopf", "authors": "Bernhard Schoelkopf (Max Planck Institute for Intelligent Systems),\n  Dominik Janzing (Max Planck Institute for Intelligent Systems), Jonas Peters\n  (Max Planck Institute for Intelligent Systems), Eleni Sgouritsa (Max Planck\n  Institute for Intelligent Systems), Kun Zhang (Max Planck Institute for\n  Intelligent Systems), Joris Mooij (Radboud University)", "title": "On Causal and Anticausal Learning", "comments": "Appears in Proceedings of the 29th International Conference on\n  Machine Learning (ICML 2012). arXiv admin note: substantial text overlap with\n  arXiv:1112.2738", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider the problem of function estimation in the case where an\nunderlying causal model can be inferred. This has implications for popular\nscenarios such as covariate shift, concept drift, transfer learning and\nsemi-supervised learning. We argue that causal knowledge may facilitate some\napproaches for a given problem, and rule out others. In particular, we\nformulate a hypothesis for when semi-supervised learning can help, and\ncorroborate it with empirical results.\n", "versions": [{"version": "v1", "created": "Wed, 27 Jun 2012 19:59:59 GMT"}], "update_date": "2012-07-03", "authors_parsed": [["Schoelkopf", "Bernhard", "", "Max Planck Institute for Intelligent Systems"], ["Janzing", "Dominik", "", "Max Planck Institute for Intelligent Systems"], ["Peters", "Jonas", "", "Max Planck Institute for Intelligent Systems"], ["Sgouritsa", "Eleni", "", "Max Planck\n  Institute for Intelligent Systems"], ["Zhang", "Kun", "", "Max Planck Institute for\n  Intelligent Systems"], ["Mooij", "Joris", "", "Radboud University"]]}, {"id": "1206.6472", "submitter": "Luis Francisco Sanchez Merchante", "authors": "Luis Francisco Sanchez Merchante (UTC/CNRS), Yves Grandvalet\n  (UTC/CNRS), Gerrad Govaert (UTC/CNRS)", "title": "An Efficient Approach to Sparse Linear Discriminant Analysis", "comments": "Appears in Proceedings of the 29th International Conference on\n  Machine Learning (ICML 2012)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a novel approach to the formulation and the resolution of sparse\nLinear Discriminant Analysis (LDA). Our proposal, is based on penalized Optimal\nScoring. It has an exact equivalence with penalized LDA, contrary to the\nmulti-class approaches based on the regression of class indicator that have\nbeen proposed so far. Sparsity is obtained thanks to a group-Lasso penalty that\nselects the same features in all discriminant directions. Our experiments\ndemonstrate that this approach generates extremely parsimonious models without\ncompromising prediction performances. Besides prediction, the resulting sparse\ndiscriminant directions are also amenable to low-dimensional representations of\ndata. Our algorithm is highly efficient for medium to large number of\nvariables, and is thus particularly well suited to the analysis of gene\nexpression data.\n", "versions": [{"version": "v1", "created": "Wed, 27 Jun 2012 19:59:59 GMT"}], "update_date": "2012-07-03", "authors_parsed": [["Merchante", "Luis Francisco Sanchez", "", "UTC/CNRS"], ["Grandvalet", "Yves", "", "UTC/CNRS"], ["Govaert", "Gerrad", "", "UTC/CNRS"]]}, {"id": "1206.6474", "submitter": "Pierre-Andre Savalle", "authors": "Emile Richard (ENS Cachan), Pierre-Andre Savalle (Ecole Centrale de\n  Paris), Nicolas Vayatis (ENS Cachan)", "title": "Estimation of Simultaneously Sparse and Low Rank Matrices", "comments": "Appears in Proceedings of the 29th International Conference on\n  Machine Learning (ICML 2012)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.LG cs.NA stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The paper introduces a penalized matrix estimation procedure aiming at\nsolutions which are sparse and low-rank at the same time. Such structures arise\nin the context of social networks or protein interactions where underlying\ngraphs have adjacency matrices which are block-diagonal in the appropriate\nbasis. We introduce a convex mixed penalty which involves $\\ell_1$-norm and\ntrace norm simultaneously. We obtain an oracle inequality which indicates how\nthe two effects interact according to the nature of the target matrix. We bound\ngeneralization error in the link prediction problem. We also develop proximal\ndescent strategies to solve the optimization problem efficiently and evaluate\nperformance on synthetic and real data sets.\n", "versions": [{"version": "v1", "created": "Wed, 27 Jun 2012 19:59:59 GMT"}], "update_date": "2012-07-03", "authors_parsed": [["Richard", "Emile", "", "ENS Cachan"], ["Savalle", "Pierre-Andre", "", "Ecole Centrale de\n  Paris"], ["Vayatis", "Nicolas", "", "ENS Cachan"]]}, {"id": "1206.6475", "submitter": "Qiaoliang Xiang", "authors": "Qiaoliang Xiang (Nanyang Technological University), Qi Mao (Nanyang\n  Technological University), Kian Ming Chai (DSO National Laboratories), Hai\n  Leong Chieu (DSO National Laboratories), Ivor Tsang (Nanyang Technological\n  University), Zhendong Zhao (Macquarie University)", "title": "A Split-Merge Framework for Comparing Clusterings", "comments": "Appears in Proceedings of the 29th International Conference on\n  Machine Learning (ICML 2012)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Clustering evaluation measures are frequently used to evaluate the\nperformance of algorithms. However, most measures are not properly normalized\nand ignore some information in the inherent structure of clusterings. We model\nthe relation between two clusterings as a bipartite graph and propose a general\ncomponent-based decomposition formula based on the components of the graph.\nMost existing measures are examples of this formula. In order to satisfy\nconsistency in the component, we further propose a split-merge framework for\ncomparing clusterings of different data sets. Our framework gives measures that\nare conditionally normalized, and it can make use of data point information,\nsuch as feature vectors and pairwise distances. We use an entropy-based\ninstance of the framework and a coreference resolution data set to demonstrate\nempirically the utility of our framework over other measures.\n", "versions": [{"version": "v1", "created": "Wed, 27 Jun 2012 19:59:59 GMT"}, {"version": "v2", "created": "Tue, 4 Sep 2012 17:42:41 GMT"}], "update_date": "2012-09-05", "authors_parsed": [["Xiang", "Qiaoliang", "", "Nanyang Technological University"], ["Mao", "Qi", "", "Nanyang\n  Technological University"], ["Chai", "Kian Ming", "", "DSO National Laboratories"], ["Chieu", "Hai Leong", "", "DSO National Laboratories"], ["Tsang", "Ivor", "", "Nanyang Technological\n  University"], ["Zhao", "Zhendong", "", "Macquarie University"]]}, {"id": "1206.6476", "submitter": "Aurelien Bellet", "authors": "Aurelien Bellet (University of Saint-Etienne), Amaury Habrard\n  (University of Saint-Etienne), Marc Sebban (University of Saint-Etienne)", "title": "Similarity Learning for Provably Accurate Sparse Linear Classification", "comments": "Appears in Proceedings of the 29th International Conference on\n  Machine Learning (ICML 2012)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In recent years, the crucial importance of metrics in machine learning\nalgorithms has led to an increasing interest for optimizing distance and\nsimilarity functions. Most of the state of the art focus on learning\nMahalanobis distances (requiring to fulfill a constraint of positive\nsemi-definiteness) for use in a local k-NN algorithm. However, no theoretical\nlink is established between the learned metrics and their performance in\nclassification. In this paper, we make use of the formal framework of good\nsimilarities introduced by Balcan et al. to design an algorithm for learning a\nnon PSD linear similarity optimized in a nonlinear feature space, which is then\nused to build a global linear classifier. We show that our approach has uniform\nstability and derive a generalization bound on the classification error.\nExperiments performed on various datasets confirm the effectiveness of our\napproach compared to state-of-the-art methods and provide evidence that (i) it\nis fast, (ii) robust to overfitting and (iii) produces very sparse classifiers.\n", "versions": [{"version": "v1", "created": "Wed, 27 Jun 2012 19:59:59 GMT"}], "update_date": "2019-01-25", "authors_parsed": [["Bellet", "Aurelien", "", "University of Saint-Etienne"], ["Habrard", "Amaury", "", "University of Saint-Etienne"], ["Sebban", "Marc", "", "University of Saint-Etienne"]]}, {"id": "1206.6477", "submitter": "Yiteng Zhai", "authors": "Yiteng Zhai (Nanyang Technological University), Mingkui Tan (Nanyang\n  Technological University), Ivor Tsang (Nanyang Technological University), Yew\n  Soon Ong (Nanyang Technological University)", "title": "Discovering Support and Affiliated Features from Very High Dimensions", "comments": "Appears in Proceedings of the 29th International Conference on\n  Machine Learning (ICML 2012)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, a novel learning paradigm is presented to automatically\nidentify groups of informative and correlated features from very high\ndimensions. Specifically, we explicitly incorporate correlation measures as\nconstraints and then propose an efficient embedded feature selection method\nusing recently developed cutting plane strategy. The benefits of the proposed\nalgorithm are two-folds. First, it can identify the optimal discriminative and\nuncorrelated feature subset to the output labels, denoted here as Support\nFeatures, which brings about significant improvements in prediction performance\nover other state of the art feature selection methods considered in the paper.\nSecond, during the learning process, the underlying group structures of\ncorrelated features associated with each support feature, denoted as Affiliated\nFeatures, can also be discovered without any additional cost. These affiliated\nfeatures serve to improve the interpretations on the learning tasks. Extensive\nempirical studies on both synthetic and very high dimensional real-world\ndatasets verify the validity and efficiency of the proposed method.\n", "versions": [{"version": "v1", "created": "Wed, 27 Jun 2012 19:59:59 GMT"}], "update_date": "2012-07-03", "authors_parsed": [["Zhai", "Yiteng", "", "Nanyang Technological University"], ["Tan", "Mingkui", "", "Nanyang\n  Technological University"], ["Tsang", "Ivor", "", "Nanyang Technological University"], ["Ong", "Yew Soon", "", "Nanyang Technological University"]]}, {"id": "1206.6478", "submitter": "Yi Zhang", "authors": "Yi Zhang (Carnegie Mellon University), Jeff Schneider (Carnegie Mellon\n  University)", "title": "Maximum Margin Output Coding", "comments": "Appears in Proceedings of the 29th International Conference on\n  Machine Learning (ICML 2012)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper we study output coding for multi-label prediction. For a\nmulti-label output coding to be discriminative, it is important that codewords\nfor different label vectors are significantly different from each other. In the\nmeantime, unlike in traditional coding theory, codewords in output coding are\nto be predicted from the input, so it is also critical to have a predictable\nlabel encoding.\n  To find output codes that are both discriminative and predictable, we first\npropose a max-margin formulation that naturally captures these two properties.\nWe then convert it to a metric learning formulation, but with an exponentially\nlarge number of constraints as commonly encountered in structured prediction\nproblems. Without a label structure for tractable inference, we use\novergenerating (i.e., relaxation) techniques combined with the cutting plane\nmethod for optimization.\n  In our empirical study, the proposed output coding scheme outperforms a\nvariety of existing multi-label prediction methods for image, text and music\nclassification.\n", "versions": [{"version": "v1", "created": "Wed, 27 Jun 2012 19:59:59 GMT"}], "update_date": "2012-07-03", "authors_parsed": [["Zhang", "Yi", "", "Carnegie Mellon University"], ["Schneider", "Jeff", "", "Carnegie Mellon\n  University"]]}, {"id": "1206.6479", "submitter": "Krishnakumar Balasubramanian", "authors": "Krishnakumar Balasubramanian (Georgia Institute of Technology), Guy\n  Lebanon (Georgia Institute of Technology)", "title": "The Landmark Selection Method for Multiple Output Prediction", "comments": "Appears in Proceedings of the 29th International Conference on\n  Machine Learning (ICML 2012)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Conditional modeling x \\to y is a central problem in machine learning. A\nsubstantial research effort is devoted to such modeling when x is high\ndimensional. We consider, instead, the case of a high dimensional y, where x is\neither low dimensional or high dimensional. Our approach is based on selecting\na small subset y_L of the dimensions of y, and proceed by modeling (i) x \\to\ny_L and (ii) y_L \\to y. Composing these two models, we obtain a conditional\nmodel x \\to y that possesses convenient statistical properties. Multi-label\nclassification and multivariate regression experiments on several datasets show\nthat this model outperforms the one vs. all approach as well as several\nsophisticated multiple output prediction methods.\n", "versions": [{"version": "v1", "created": "Wed, 27 Jun 2012 19:59:59 GMT"}], "update_date": "2012-07-03", "authors_parsed": [["Balasubramanian", "Krishnakumar", "", "Georgia Institute of Technology"], ["Lebanon", "Guy", "", "Georgia Institute of Technology"]]}, {"id": "1206.6480", "submitter": "Matthieu Geist", "authors": "Matthieu Geist (Supelec), Bruno Scherrer (INRIA Nancy), Alessandro\n  Lazaric (INRIA Lille), Mohammad Ghavamzadeh (INRIA Lille)", "title": "A Dantzig Selector Approach to Temporal Difference Learning", "comments": "Appears in Proceedings of the 29th International Conference on\n  Machine Learning (ICML 2012)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  LSTD is a popular algorithm for value function approximation. Whenever the\nnumber of features is larger than the number of samples, it must be paired with\nsome form of regularization. In particular, L1-regularization methods tend to\nperform feature selection by promoting sparsity, and thus, are well-suited for\nhigh-dimensional problems. However, since LSTD is not a simple regression\nalgorithm, but it solves a fixed--point problem, its integration with\nL1-regularization is not straightforward and might come with some drawbacks\n(e.g., the P-matrix assumption for LASSO-TD). In this paper, we introduce a\nnovel algorithm obtained by integrating LSTD with the Dantzig Selector. We\ninvestigate the performance of the proposed algorithm and its relationship with\nthe existing regularized approaches, and show how it addresses some of their\ndrawbacks.\n", "versions": [{"version": "v1", "created": "Wed, 27 Jun 2012 19:59:59 GMT"}], "update_date": "2012-07-03", "authors_parsed": [["Geist", "Matthieu", "", "Supelec"], ["Scherrer", "Bruno", "", "INRIA Nancy"], ["Lazaric", "Alessandro", "", "INRIA Lille"], ["Ghavamzadeh", "Mohammad", "", "INRIA Lille"]]}, {"id": "1206.6482", "submitter": "Yuening Hu", "authors": "Ke Zhai (University of Maryland), Yuening Hu (University of Maryland),\n  Sinead Williamson (Carnegie Mellon University), Jordan Boyd-Graber\n  (University of Maryland)", "title": "Modeling Images using Transformed Indian Buffet Processes", "comments": "Appears in Proceedings of the 29th International Conference on\n  Machine Learning (ICML 2012)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Latent feature models are attractive for image modeling, since images\ngenerally contain multiple objects. However, many latent feature models ignore\nthat objects can appear at different locations or require pre-segmentation of\nimages. While the transformed Indian buffet process (tIBP) provides a method\nfor modeling transformation-invariant features in unsegmented binary images,\nits current form is inappropriate for real images because of its computational\ncost and modeling assumptions. We combine the tIBP with likelihoods appropriate\nfor real images and develop an efficient inference, using the cross-correlation\nbetween images and features, that is theoretically and empirically faster than\nexisting inference techniques. Our method discovers reasonable components and\nachieve effective image reconstruction in natural images.\n", "versions": [{"version": "v1", "created": "Wed, 27 Jun 2012 19:59:59 GMT"}], "update_date": "2012-07-03", "authors_parsed": [["Zhai", "Ke", "", "University of Maryland"], ["Hu", "Yuening", "", "University of Maryland"], ["Williamson", "Sinead", "", "Carnegie Mellon University"], ["Boyd-Graber", "Jordan", "", "University of Maryland"]]}, {"id": "1206.6483", "submitter": "Nils Kriege", "authors": "Nils Kriege (TU Dortmund), Petra Mutzel (TU Dortmund)", "title": "Subgraph Matching Kernels for Attributed Graphs", "comments": "Appears in Proceedings of the 29th International Conference on\n  Machine Learning (ICML 2012)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose graph kernels based on subgraph matchings, i.e.\nstructure-preserving bijections between subgraphs. While recently proposed\nkernels based on common subgraphs (Wale et al., 2008; Shervashidze et al.,\n2009) in general can not be applied to attributed graphs, our approach allows\nto rate mappings of subgraphs by a flexible scoring scheme comparing vertex and\nedge attributes by kernels. We show that subgraph matching kernels generalize\nseveral known kernels. To compute the kernel we propose a graph-theoretical\nalgorithm inspired by a classical relation between common subgraphs of two\ngraphs and cliques in their product graph observed by Levi (1973). Encouraging\nexperimental results on a classification task of real-world graphs are\npresented.\n", "versions": [{"version": "v1", "created": "Wed, 27 Jun 2012 19:59:59 GMT"}], "update_date": "2012-07-03", "authors_parsed": [["Kriege", "Nils", "", "TU Dortmund"], ["Mutzel", "Petra", "", "TU Dortmund"]]}, {"id": "1206.6484", "submitter": "Takaki Makino", "authors": "Takaki Makino (University of Tokyo), Johane Takeuchi (Honda Research\n  Institute Japan)", "title": "Apprenticeship Learning for Model Parameters of Partially Observable\n  Environments", "comments": "Appears in Proceedings of the 29th International Conference on\n  Machine Learning (ICML 2012)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider apprenticeship learning, i.e., having an agent learn a task by\nobserving an expert demonstrating the task in a partially observable\nenvironment when the model of the environment is uncertain. This setting is\nuseful in applications where the explicit modeling of the environment is\ndifficult, such as a dialogue system. We show that we can extract information\nabout the environment model by inferring action selection process behind the\ndemonstration, under the assumption that the expert is choosing optimal actions\nbased on knowledge of the true model of the target environment. Proposed\nalgorithms can achieve more accurate estimates of POMDP parameters and better\npolicies from a short demonstration, compared to methods that learns only from\nthe reaction from the environment.\n", "versions": [{"version": "v1", "created": "Wed, 27 Jun 2012 19:59:59 GMT"}], "update_date": "2012-07-03", "authors_parsed": [["Makino", "Takaki", "", "University of Tokyo"], ["Takeuchi", "Johane", "", "Honda Research\n  Institute Japan"]]}, {"id": "1206.6485", "submitter": "Christopher Painter-Wakefield", "authors": "Christopher Painter-Wakefield (Duke University), Ronald Parr (Duke\n  University)", "title": "Greedy Algorithms for Sparse Reinforcement Learning", "comments": "Appears in Proceedings of the 29th International Conference on\n  Machine Learning (ICML 2012)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Feature selection and regularization are becoming increasingly prominent\ntools in the efforts of the reinforcement learning (RL) community to expand the\nreach and applicability of RL. One approach to the problem of feature selection\nis to impose a sparsity-inducing form of regularization on the learning method.\nRecent work on $L_1$ regularization has adapted techniques from the supervised\nlearning literature for use with RL. Another approach that has received renewed\nattention in the supervised learning community is that of using a simple\nalgorithm that greedily adds new features. Such algorithms have many of the\ngood properties of the $L_1$ regularization methods, while also being extremely\nefficient and, in some cases, allowing theoretical guarantees on recovery of\nthe true form of a sparse target function from sampled data. This paper\nconsiders variants of orthogonal matching pursuit (OMP) applied to\nreinforcement learning. The resulting algorithms are analyzed and compared\nexperimentally with existing $L_1$ regularized approaches. We demonstrate that\nperhaps the most natural scenario in which one might hope to achieve sparse\nrecovery fails; however, one variant, OMP-BRM, provides promising theoretical\nguarantees under certain assumptions on the feature dictionary. Another\nvariant, OMP-TD, empirically outperforms prior methods both in approximation\naccuracy and efficiency on several benchmark problems.\n", "versions": [{"version": "v1", "created": "Wed, 27 Jun 2012 19:59:59 GMT"}], "update_date": "2012-07-03", "authors_parsed": [["Painter-Wakefield", "Christopher", "", "Duke University"], ["Parr", "Ronald", "", "Duke\n  University"]]}, {"id": "1206.6486", "submitter": "Piyush Rai", "authors": "Alexandre Passos (UMass Amherst), Piyush Rai (University of Utah),\n  Jacques Wainer (University of Campinas), Hal Daume III (University of\n  Maryland)", "title": "Flexible Modeling of Latent Task Structures in Multitask Learning", "comments": "Appears in Proceedings of the 29th International Conference on\n  Machine Learning (ICML 2012)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Multitask learning algorithms are typically designed assuming some fixed, a\npriori known latent structure shared by all the tasks. However, it is usually\nunclear what type of latent task structure is the most appropriate for a given\nmultitask learning problem. Ideally, the \"right\" latent task structure should\nbe learned in a data-driven manner. We present a flexible, nonparametric\nBayesian model that posits a mixture of factor analyzers structure on the\ntasks. The nonparametric aspect makes the model expressive enough to subsume\nmany existing models of latent task structures (e.g, mean-regularized tasks,\nclustered tasks, low-rank or linear/non-linear subspace assumption on tasks,\netc.). Moreover, it can also learn more general task structures, addressing the\nshortcomings of such models. We present a variational inference algorithm for\nour model. Experimental results on synthetic and real-world datasets, on both\nregression and classification problems, demonstrate the effectiveness of the\nproposed method.\n", "versions": [{"version": "v1", "created": "Wed, 27 Jun 2012 19:59:59 GMT"}], "update_date": "2012-07-03", "authors_parsed": [["Passos", "Alexandre", "", "UMass Amherst"], ["Rai", "Piyush", "", "University of Utah"], ["Wainer", "Jacques", "", "University of Campinas"], ["Daume", "Hal", "III", "University of\n  Maryland"]]}, {"id": "1206.6487", "submitter": "Csaba Szepesvari", "authors": "Gabor Bartok (University of Alberta), Navid Zolghadr (University of\n  Alberta), Csaba Szepesvari (University of Alberta)", "title": "An Adaptive Algorithm for Finite Stochastic Partial Monitoring", "comments": "Appears in Proceedings of the 29th International Conference on\n  Machine Learning (ICML 2012)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.GT stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a new anytime algorithm that achieves near-optimal regret for any\ninstance of finite stochastic partial monitoring. In particular, the new\nalgorithm achieves the minimax regret, within logarithmic factors, for both\n\"easy\" and \"hard\" problems. For easy problems, it additionally achieves\nlogarithmic individual regret. Most importantly, the algorithm is adaptive in\nthe sense that if the opponent strategy is in an \"easy region\" of the strategy\nspace then the regret grows as if the problem was easy. As an implication, we\nshow that under some reasonable additional assumptions, the algorithm enjoys an\nO(\\sqrt{T}) regret in Dynamic Pricing, proven to be hard by Bartok et al.\n(2011).\n", "versions": [{"version": "v1", "created": "Wed, 27 Jun 2012 19:59:59 GMT"}], "update_date": "2012-07-03", "authors_parsed": [["Bartok", "Gabor", "", "University of Alberta"], ["Zolghadr", "Navid", "", "University of\n  Alberta"], ["Szepesvari", "Csaba", "", "University of Alberta"]]}, {"id": "1206.6488", "submitter": "Han Liu", "authors": "Han Liu (Johns Hopkins University), Fang Han (Johns Hopkins\n  University), Ming Yuan (Georgia Institute of Technology), John Lafferty\n  (University of Chicago), Larry Wasserman (Carnegie Mellon University)", "title": "The Nonparanormal SKEPTIC", "comments": "Appears in Proceedings of the 29th International Conference on\n  Machine Learning (ICML 2012)", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a semiparametric approach, named nonparanormal skeptic, for\nestimating high dimensional undirected graphical models. In terms of modeling,\nwe consider the nonparanormal family proposed by Liu et al (2009). In terms of\nestimation, we exploit nonparametric rank-based correlation coefficient\nestimators including the Spearman's rho and Kendall's tau. In high dimensional\nsettings, we prove that the nonparanormal skeptic achieves the optimal\nparametric rate of convergence in both graph and parameter estimation. This\nresult suggests that the nonparanormal graphical models are a safe replacement\nof the Gaussian graphical models, even when the data are Gaussian.\n", "versions": [{"version": "v1", "created": "Wed, 27 Jun 2012 19:59:59 GMT"}], "update_date": "2012-07-03", "authors_parsed": [["Liu", "Han", "", "Johns Hopkins University"], ["Han", "Fang", "", "Johns Hopkins\n  University"], ["Yuan", "Ming", "", "Georgia Institute of Technology"], ["Lafferty", "John", "", "University of Chicago"], ["Wasserman", "Larry", "", "Carnegie Mellon University"]]}, {"id": "1206.6519", "submitter": "Noah Simon", "authors": "Noah Simon and Robert Tibshirani", "title": "A Permutation Approach to Testing Interactions in Many Dimensions", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML stat.CO stat.ME", "license": "http://creativecommons.org/licenses/by/3.0/", "abstract": "  To date, testing interactions in high dimensions has been a challenging task.\nExisting methods often have issues with sensitivity to modeling assumptions and\nheavily asymptotic nominal p-values. To help alleviate these issues, we propose\na permutation-based method for testing marginal interactions with a binary\nresponse. Our method searches for pairwise correlations which differ between\nclasses. In this manuscript, we compare our method on real and simulated data\nto the standard approach of running many pairwise logistic models. On simulated\ndata our method finds more significant interactions at a lower false discovery\nrate (especially in the presence of main effects). On real genomic data,\nalthough there is no gold standard, our method finds apparent signal and tells\na believable story, while logistic regression does not. We also give asymptotic\nconsistency results under not too restrictive assumptions.\n", "versions": [{"version": "v1", "created": "Wed, 27 Jun 2012 20:38:20 GMT"}], "update_date": "2012-06-29", "authors_parsed": [["Simon", "Noah", ""], ["Tibshirani", "Robert", ""]]}, {"id": "1206.6679", "submitter": "Tim Salimans", "authors": "Tim Salimans and David A. Knowles", "title": "Fixed-Form Variational Posterior Approximation through Stochastic Linear\n  Regression", "comments": null, "journal-ref": "Bayesian Analysis, Volume 8, Number 4 (2013), 837-882", "doi": "10.1214/13-BA858", "report-no": null, "categories": "stat.CO cs.CV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a general algorithm for approximating nonstandard Bayesian\nposterior distributions. The algorithm minimizes the Kullback-Leibler\ndivergence of an approximating distribution to the intractable posterior\ndistribution. Our method can be used to approximate any posterior distribution,\nprovided that it is given in closed form up to the proportionality constant.\nThe approximation can be any distribution in the exponential family or any\nmixture of such distributions, which means that it can be made arbitrarily\nprecise. Several examples illustrate the speed and accuracy of our\napproximation method in practice.\n", "versions": [{"version": "v1", "created": "Thu, 28 Jun 2012 13:25:04 GMT"}, {"version": "v2", "created": "Wed, 1 Aug 2012 11:38:52 GMT"}, {"version": "v3", "created": "Thu, 13 Jun 2013 06:22:58 GMT"}, {"version": "v4", "created": "Sat, 26 Oct 2013 15:09:54 GMT"}, {"version": "v5", "created": "Wed, 27 Nov 2013 13:19:48 GMT"}, {"version": "v6", "created": "Mon, 28 Jul 2014 11:16:19 GMT"}], "update_date": "2014-07-29", "authors_parsed": [["Salimans", "Tim", ""], ["Knowles", "David A.", ""]]}, {"id": "1206.6813", "submitter": "Sanjoy Dasgupta", "authors": "Sanjoy Dasgupta, Daniel Hsu, Nakul Verma", "title": "A concentration theorem for projections", "comments": "Appears in Proceedings of the Twenty-Second Conference on Uncertainty\n  in Artificial Intelligence (UAI2006)", "journal-ref": null, "doi": null, "report-no": "UAI-P-2006-PG-114-121", "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  X in R^D has mean zero and finite second moments. We show that there is a\nprecise sense in which almost all linear projections of X into R^d (for d < D)\nlook like a scale-mixture of spherical Gaussians -- specifically, a mixture of\ndistributions N(0, sigma^2 I_d) where the weight of the particular sigma\ncomponent is P (| X |^2 = sigma^2 D). The extent of this effect depends upon\nthe ratio of d to D, and upon a particular coefficient of eccentricity of X's\ndistribution. We explore this result in a variety of experiments.\n", "versions": [{"version": "v1", "created": "Wed, 27 Jun 2012 15:36:47 GMT"}], "update_date": "2012-07-02", "authors_parsed": [["Dasgupta", "Sanjoy", ""], ["Hsu", "Daniel", ""], ["Verma", "Nakul", ""]]}, {"id": "1206.6815", "submitter": "Koby Crammer", "authors": "Koby Crammer, Amir Globerson", "title": "Discriminative Learning via Semidefinite Probabilistic Models", "comments": "Appears in Proceedings of the Twenty-Second Conference on Uncertainty\n  in Artificial Intelligence (UAI2006)", "journal-ref": null, "doi": null, "report-no": "UAI-P-2006-PG-98-105", "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Discriminative linear models are a popular tool in machine learning. These\ncan be generally divided into two types: The first is linear classifiers, such\nas support vector machines, which are well studied and provide state-of-the-art\nresults. One shortcoming of these models is that their output (known as the\n'margin') is not calibrated, and cannot be translated naturally into a\ndistribution over the labels. Thus, it is difficult to incorporate such models\nas components of larger systems, unlike probabilistic based approaches. The\nsecond type of approach constructs class conditional distributions using a\nnonlinearity (e.g. log-linear models), but is occasionally worse in terms of\nclassification error. We propose a supervised learning method which combines\nthe best of both approaches. Specifically, our method provides a distribution\nover the labels, which is a linear function of the model parameters. As a\nconsequence, differences between probabilities are linear functions, a property\nwhich most probabilistic models (e.g. log-linear) do not have.\n  Our model assumes that classes correspond to linear subspaces (rather than to\nhalf spaces). Using a relaxed projection operator, we construct a measure which\nevaluates the degree to which a given vector 'belongs' to a subspace, resulting\nin a distribution over labels. Interestingly, this view is closely related to\nsimilar concepts in quantum detection theory. The resulting models can be\ntrained either to maximize the margin or to optimize average likelihood\nmeasures. The corresponding optimization problems are semidefinite programs\nwhich can be solved efficiently. We illustrate the performance of our algorithm\non real world datasets, and show that it outperforms 2nd order kernel methods.\n", "versions": [{"version": "v1", "created": "Wed, 27 Jun 2012 15:38:14 GMT"}], "update_date": "2012-07-02", "authors_parsed": [["Crammer", "Koby", ""], ["Globerson", "Amir", ""]]}, {"id": "1206.6824", "submitter": "Matthew Beal", "authors": "Matthew Beal, Praveen Krishnamurthy", "title": "Gene Expression Time Course Clustering with Countably Infinite Hidden\n  Markov Models", "comments": "Appears in Proceedings of the Twenty-Second Conference on Uncertainty\n  in Artificial Intelligence (UAI2006)", "journal-ref": null, "doi": null, "report-no": "UAI-P-2006-PG-23-30", "categories": "cs.LG cs.CE stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Most existing approaches to clustering gene expression time course data treat\nthe different time points as independent dimensions and are invariant to\npermutations, such as reversal, of the experimental time course. Approaches\nutilizing HMMs have been shown to be helpful in this regard, but are hampered\nby having to choose model architectures with appropriate complexities. Here we\npropose for a clustering application an HMM with a countably infinite state\nspace; inference in this model is possible by recasting it in the hierarchical\nDirichlet process (HDP) framework (Teh et al. 2006), and hence we call it the\nHDP-HMM. We show that the infinite model outperforms model selection methods\nover finite models, and traditional time-independent methods, as measured by a\nvariety of external and internal indices for clustering on two large publicly\navailable data sets. Moreover, we show that the infinite models utilize more\nhidden states and employ richer architectures (e.g. state-to-state transitions)\nwithout the damaging effects of overfitting.\n", "versions": [{"version": "v1", "created": "Wed, 27 Jun 2012 15:41:07 GMT"}], "update_date": "2012-07-02", "authors_parsed": [["Beal", "Matthew", ""], ["Krishnamurthy", "Praveen", ""]]}, {"id": "1206.6828", "submitter": "Mikko Koivisto", "authors": "Mikko Koivisto", "title": "Advances in exact Bayesian structure discovery in Bayesian networks", "comments": "Appears in Proceedings of the Twenty-Second Conference on Uncertainty\n  in Artificial Intelligence (UAI2006)", "journal-ref": null, "doi": null, "report-no": "UAI-P-2006-PG-241-248", "categories": "cs.LG cs.AI stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider a Bayesian method for learning the Bayesian network structure\nfrom complete data. Recently, Koivisto and Sood (2004) presented an algorithm\nthat for any single edge computes its marginal posterior probability in O(n\n2^n) time, where n is the number of attributes; the number of parents per\nattribute is bounded by a constant. In this paper we show that the posterior\nprobabilities for all the n (n - 1) potential edges can be computed in O(n 2^n)\ntotal time. This result is achieved by a forward-backward technique and fast\nMoebius transform algorithms, which are of independent interest. The resulting\nspeedup by a factor of about n^2 allows us to experimentally study the\nstatistical power of learning moderate-size networks. We report results from a\nsimulation study that covers data sets with 20 to 10,000 records over 5 to 25\ndiscrete attributes\n", "versions": [{"version": "v1", "created": "Wed, 27 Jun 2012 16:15:14 GMT"}], "update_date": "2012-07-02", "authors_parsed": [["Koivisto", "Mikko", ""]]}, {"id": "1206.6832", "submitter": "Yuhong Guo", "authors": "Yuhong Guo, Dale Schuurmans", "title": "Convex Structure Learning for Bayesian Networks: Polynomial Feature\n  Selection and Approximate Ordering", "comments": "Appears in Proceedings of the Twenty-Second Conference on Uncertainty\n  in Artificial Intelligence (UAI2006)", "journal-ref": null, "doi": null, "report-no": "UAI-P-2006-PG-208-216", "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a new approach to learning the structure and parameters of a\nBayesian network based on regularized estimation in an exponential family\nrepresentation. Here we show that, given a fixed variable order, the optimal\nstructure and parameters can be learned efficiently, even without restricting\nthe size of the parent sets. We then consider the problem of optimizing the\nvariable order for a given set of features. This is still a computationally\nhard problem, but we present a convex relaxation that yields an optimal 'soft'\nordering in polynomial time. One novel aspect of the approach is that we do not\nperform a discrete search over DAG structures, nor over variable orders, but\ninstead solve a continuous relaxation that can then be rounded to obtain a\nvalid network structure. We conduct an experimental comparison against standard\nstructure search procedures over standard objectives, which cope with local\nminima, and evaluate the advantages of using convex relaxations that reduce the\neffects of local minima.\n", "versions": [{"version": "v1", "created": "Wed, 27 Jun 2012 16:17:52 GMT"}], "update_date": "2012-07-02", "authors_parsed": [["Guo", "Yuhong", ""], ["Schuurmans", "Dale", ""]]}, {"id": "1206.6833", "submitter": "Inmar Givoni", "authors": "Inmar Givoni, Vincent Cheung, Brendan J. Frey", "title": "Matrix Tile Analysis", "comments": "Appears in Proceedings of the Twenty-Second Conference on Uncertainty\n  in Artificial Intelligence (UAI2006)", "journal-ref": null, "doi": null, "report-no": "UAI-P-2006-PG-200-207", "categories": "cs.LG cs.CE cs.NA stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Many tasks require finding groups of elements in a matrix of numbers, symbols\nor class likelihoods. One approach is to use efficient bi- or tri-linear\nfactorization techniques including PCA, ICA, sparse matrix factorization and\nplaid analysis. These techniques are not appropriate when addition and\nmultiplication of matrix elements are not sensibly defined. More directly,\nmethods like bi-clustering can be used to classify matrix elements, but these\nmethods make the overly-restrictive assumption that the class of each element\nis a function of a row class and a column class. We introduce a general\ncomputational problem, `matrix tile analysis' (MTA), which consists of\ndecomposing a matrix into a set of non-overlapping tiles, each of which is\ndefined by a subset of usually nonadjacent rows and columns. MTA does not\nrequire an algebra for combining tiles, but must search over discrete\ncombinations of tile assignments. Exact MTA is a computationally intractable\ninteger programming problem, but we describe an approximate iterative technique\nand a computationally efficient sum-product relaxation of the integer program.\nWe compare the effectiveness of these methods to PCA and plaid on hundreds of\nrandomly generated tasks. Using double-gene-knockout data, we show that MTA\nfinds groups of interacting yeast genes that have biologically-related\nfunctions.\n", "versions": [{"version": "v1", "created": "Wed, 27 Jun 2012 16:18:05 GMT"}], "update_date": "2012-07-02", "authors_parsed": [["Givoni", "Inmar", ""], ["Cheung", "Vincent", ""], ["Frey", "Brendan J.", ""]]}, {"id": "1206.6842", "submitter": "Thomas Degris", "authors": "Thomas Degris, Olivier Sigaud, Pierre-Henri Wuillemin", "title": "Chi-square Tests Driven Method for Learning the Structure of Factored\n  MDPs", "comments": "Appears in Proceedings of the Twenty-Second Conference on Uncertainty\n  in Artificial Intelligence (UAI2006)", "journal-ref": null, "doi": null, "report-no": "UAI-P-2006-PG-122-129", "categories": "cs.LG cs.AI stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  SDYNA is a general framework designed to address large stochastic\nreinforcement learning problems. Unlike previous model based methods in FMDPs,\nit incrementally learns the structure and the parameters of a RL problem using\nsupervised learning techniques. Then, it integrates decision-theoric planning\nalgorithms based on FMDPs to compute its policy. SPITI is an instanciation of\nSDYNA that exploits ITI, an incremental decision tree algorithm, to learn the\nreward function and the Dynamic Bayesian Networks with local structures\nrepresenting the transition function of the problem. These representations are\nused by an incremental version of the Structured Value Iteration algorithm. In\norder to learn the structure, SPITI uses Chi-Square tests to detect the\nindependence between two probability distributions. Thus, we study the relation\nbetween the threshold used in the Chi-Square test, the size of the model built\nand the relative error of the value function of the induced policy with respect\nto the optimal value. We show that, on stochastic problems, one can tune the\nthreshold so as to generate both a compact model and an efficient policy. Then,\nwe show that SPITI, while keeping its model compact, uses the generalization\nproperty of its learning method to perform better than a stochastic classical\ntabular algorithm in large RL problem with an unknown structure. We also\nintroduce a new measure based on Chi-Square to qualify the accuracy of the\nmodel learned by SPITI. We qualitatively show that the generalization property\nin SPITI within the FMDP framework may prevent an exponential growth of the\ntime required to learn the structure of large stochastic RL problems.\n", "versions": [{"version": "v1", "created": "Wed, 27 Jun 2012 16:20:30 GMT"}], "update_date": "2012-07-02", "authors_parsed": [["Degris", "Thomas", ""], ["Sigaud", "Olivier", ""], ["Wuillemin", "Pierre-Henri", ""]]}, {"id": "1206.6845", "submitter": "Ian Porteous", "authors": "Ian Porteous, Alexander T. Ihler, Padhraic Smyth, Max Welling", "title": "Gibbs Sampling for (Coupled) Infinite Mixture Models in the Stick\n  Breaking Representation", "comments": "Appears in Proceedings of the Twenty-Second Conference on Uncertainty\n  in Artificial Intelligence (UAI2006)", "journal-ref": null, "doi": null, "report-no": "UAI-P-2006-PG-385-392", "categories": "stat.ME cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Nonparametric Bayesian approaches to clustering, information retrieval,\nlanguage modeling and object recognition have recently shown great promise as a\nnew paradigm for unsupervised data analysis. Most contributions have focused on\nthe Dirichlet process mixture models or extensions thereof for which efficient\nGibbs samplers exist. In this paper we explore Gibbs samplers for infinite\ncomplexity mixture models in the stick breaking representation. The advantage\nof this representation is improved modeling flexibility. For instance, one can\ndesign the prior distribution over cluster sizes or couple multiple infinite\nmixture models (e.g. over time) at the level of their parameters (i.e. the\ndependent Dirichlet process model). However, Gibbs samplers for infinite\nmixture models (as recently introduced in the statistics literature) seem to\nmix poorly over cluster labels. Among others issues, this can have the adverse\neffect that labels for the same cluster in coupled mixture models are mixed up.\nWe introduce additional moves in these samplers to improve mixing over cluster\nlabels and to bring clusters into correspondence. An application to modeling of\nstorm trajectories is used to illustrate these ideas.\n", "versions": [{"version": "v1", "created": "Wed, 27 Jun 2012 16:21:35 GMT"}], "update_date": "2012-07-02", "authors_parsed": [["Porteous", "Ian", ""], ["Ihler", "Alexander T.", ""], ["Smyth", "Padhraic", ""], ["Welling", "Max", ""]]}, {"id": "1206.6846", "submitter": "Avi Pfeffer", "authors": "Avi Pfeffer", "title": "Approximate Separability for Weak Interaction in Dynamic Systems", "comments": "Appears in Proceedings of the Twenty-Second Conference on Uncertainty\n  in Artificial Intelligence (UAI2006)", "journal-ref": null, "doi": null, "report-no": "UAI-P-2006-PG-375-384", "categories": "cs.LG cs.AI stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  One approach to monitoring a dynamic system relies on decomposition of the\nsystem into weakly interacting subsystems. An earlier paper introduced a notion\nof weak interaction called separability, and showed that it leads to exact\npropagation of marginals for prediction. This paper addresses two questions\nleft open by the earlier paper: can we define a notion of approximate\nseparability that occurs naturally in practice, and do separability and\napproximate separability lead to accurate monitoring? The answer to both\nquestions is afirmative. The paper also analyzes the structure of approximately\nseparable decompositions, and provides some explanation as to why these models\nperform well.\n", "versions": [{"version": "v1", "created": "Wed, 27 Jun 2012 16:23:17 GMT"}], "update_date": "2012-07-02", "authors_parsed": [["Pfeffer", "Avi", ""]]}, {"id": "1206.6847", "submitter": "Jose M. Pena", "authors": "Jose M. Pena, Roland Nilsson, Johan Bj\\\"orkegren, Jesper Tegn\\'er", "title": "Identifying the Relevant Nodes Without Learning the Model", "comments": "Appears in Proceedings of the Twenty-Second Conference on Uncertainty\n  in Artificial Intelligence (UAI2006)", "journal-ref": null, "doi": null, "report-no": "UAI-P-2006-PG-367-374", "categories": "cs.LG cs.AI stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a method to identify all the nodes that are relevant to compute\nall the conditional probability distributions for a given set of nodes. Our\nmethod is simple, effcient, consistent, and does not require learning a\nBayesian network first. Therefore, our method can be applied to\nhigh-dimensional databases, e.g. gene expression databases.\n", "versions": [{"version": "v1", "created": "Wed, 27 Jun 2012 16:23:41 GMT"}], "update_date": "2012-07-02", "authors_parsed": [["Pena", "Jose M.", ""], ["Nilsson", "Roland", ""], ["Bj\u00f6rkegren", "Johan", ""], ["Tegn\u00e9r", "Jesper", ""]]}, {"id": "1206.6851", "submitter": "Bhaskara Marthi", "authors": "Bhaskara Marthi, Stuart Russell, David Andre", "title": "A compact, hierarchical Q-function decomposition", "comments": "Appears in Proceedings of the Twenty-Second Conference on Uncertainty\n  in Artificial Intelligence (UAI2006)", "journal-ref": null, "doi": null, "report-no": "UAI-P-2006-PG-332-340", "categories": "cs.LG cs.AI stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Previous work in hierarchical reinforcement learning has faced a dilemma:\neither ignore the values of different possible exit states from a subroutine,\nthereby risking suboptimal behavior, or represent those values explicitly\nthereby incurring a possibly large representation cost because exit values\nrefer to nonlocal aspects of the world (i.e., all subsequent rewards). This\npaper shows that, in many cases, one can avoid both of these problems. The\nsolution is based on recursively decomposing the exit value function in terms\nof Q-functions at higher levels of the hierarchy. This leads to an intuitively\nappealing runtime architecture in which a parent subroutine passes to its child\na value function on the exit states and the child reasons about how its choices\naffect the exit value. We also identify structural conditions on the value\nfunction and transition distributions that allow much more concise\nrepresentations of exit state distributions, leading to further state\nabstraction. In essence, the only variables whose exit values need be\nconsidered are those that the parent cares about and the child affects. We\ndemonstrate the utility of our algorithms on a series of increasingly complex\nenvironments.\n", "versions": [{"version": "v1", "created": "Wed, 27 Jun 2012 16:24:43 GMT"}], "update_date": "2012-07-02", "authors_parsed": [["Marthi", "Bhaskara", ""], ["Russell", "Stuart", ""], ["Andre", "David", ""]]}, {"id": "1206.6852", "submitter": "Vikash Mansinghka", "authors": "Vikash Mansinghka, Charles Kemp, Thomas Griffiths, Joshua Tenenbaum", "title": "Structured Priors for Structure Learning", "comments": "Appears in Proceedings of the Twenty-Second Conference on Uncertainty\n  in Artificial Intelligence (UAI2006)", "journal-ref": null, "doi": null, "report-no": "UAI-P-2006-PG-324-331", "categories": "cs.LG cs.AI stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Traditional approaches to Bayes net structure learning typically assume\nlittle regularity in graph structure other than sparseness. However, in many\ncases, we expect more systematicity: variables in real-world systems often\ngroup into classes that predict the kinds of probabilistic dependencies they\nparticipate in. Here we capture this form of prior knowledge in a hierarchical\nBayesian framework, and exploit it to enable structure learning and type\ndiscovery from small datasets. Specifically, we present a nonparametric\ngenerative model for directed acyclic graphs as a prior for Bayes net structure\nlearning. Our model assumes that variables come in one or more classes and that\nthe prior probability of an edge existing between two variables is a function\nonly of their classes. We derive an MCMC algorithm for simultaneous inference\nof the number of classes, the class assignments of variables, and the Bayes net\nstructure over variables. For several realistic, sparse datasets, we show that\nthe bias towards systematicity of connections provided by our model yields more\naccurate learned networks than a traditional, uniform prior approach, and that\nthe classes found by our model are appropriate.\n", "versions": [{"version": "v1", "created": "Wed, 27 Jun 2012 16:24:57 GMT"}], "update_date": "2012-07-02", "authors_parsed": [["Mansinghka", "Vikash", ""], ["Kemp", "Charles", ""], ["Griffiths", "Thomas", ""], ["Tenenbaum", "Joshua", ""]]}, {"id": "1206.6857", "submitter": "Dongryeol Lee", "authors": "Dongryeol Lee, Alexander G. Gray", "title": "Faster Gaussian Summation: Theory and Experiment", "comments": "Appears in Proceedings of the Twenty-Second Conference on Uncertainty\n  in Artificial Intelligence (UAI2006)", "journal-ref": null, "doi": null, "report-no": "UAI-P-2006-PG-281-288", "categories": "cs.LG cs.NA stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We provide faster algorithms for the problem of Gaussian summation, which\noccurs in many machine learning methods. We develop two new extensions - an\nO(Dp) Taylor expansion for the Gaussian kernel with rigorous error bounds and a\nnew error control scheme integrating any arbitrary approximation method -\nwithin the best discretealgorithmic framework using adaptive hierarchical data\nstructures. We rigorously evaluate these techniques empirically in the context\nof optimal bandwidth selection in kernel density estimation, revealing the\nstrengths and weaknesses of current state-of-the-art approaches for the first\ntime. Our results demonstrate that the new error control scheme yields improved\nperformance, whereas the series expansion approach is only effective in low\ndimensions (five or less).\n", "versions": [{"version": "v1", "created": "Wed, 27 Jun 2012 16:26:27 GMT"}], "update_date": "2012-07-02", "authors_parsed": [["Lee", "Dongryeol", ""], ["Gray", "Alexander G.", ""]]}, {"id": "1206.6860", "submitter": "John Langford", "authors": "John Langford, Roberto Oliveira, Bianca Zadrozny", "title": "Predicting Conditional Quantiles via Reduction to Classification", "comments": "Appears in Proceedings of the Twenty-Second Conference on Uncertainty\n  in Artificial Intelligence (UAI2006)", "journal-ref": null, "doi": null, "report-no": "UAI-P-2006-PG-257-264", "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We show how to reduce the process of predicting general order statistics (and\nthe median in particular) to solving classification. The accompanying\ntheoretical statement shows that the regret of the classifier bounds the regret\nof the quantile regression under a quantile loss. We also test this reduction\nempirically against existing quantile regression methods on large real-world\ndatasets and discover that it provides state-of-the-art performance.\n", "versions": [{"version": "v1", "created": "Wed, 27 Jun 2012 16:27:25 GMT"}], "update_date": "2012-07-02", "authors_parsed": [["Langford", "John", ""], ["Oliveira", "Roberto", ""], ["Zadrozny", "Bianca", ""]]}, {"id": "1206.6862", "submitter": "Or Zuk", "authors": "Or Zuk, Shiri Margel, Eytan Domany", "title": "On the Number of Samples Needed to Learn the Correct Structure of a\n  Bayesian Network", "comments": "Appears in Proceedings of the Twenty-Second Conference on Uncertainty\n  in Artificial Intelligence (UAI2006)", "journal-ref": null, "doi": null, "report-no": "UAI-P-2006-PG-560-567", "categories": "cs.LG cs.AI stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Bayesian Networks (BNs) are useful tools giving a natural and compact\nrepresentation of joint probability distributions. In many applications one\nneeds to learn a Bayesian Network (BN) from data. In this context, it is\nimportant to understand the number of samples needed in order to guarantee a\nsuccessful learning. Previous work have studied BNs sample complexity, yet it\nmainly focused on the requirement that the learned distribution will be close\nto the original distribution which generated the data. In this work, we study a\ndifferent aspect of the learning, namely the number of samples needed in order\nto learn the correct structure of the network. We give both asymptotic results,\nvalid in the large sample limit, and experimental results, demonstrating the\nlearning behavior for feasible sample sizes. We show that structure learning is\na more difficult task, compared to approximating the correct distribution, in\nthe sense that it requires a much larger number of samples, regardless of the\ncomputational power available for the learner.\n", "versions": [{"version": "v1", "created": "Wed, 27 Jun 2012 16:28:06 GMT"}], "update_date": "2012-07-02", "authors_parsed": [["Zuk", "Or", ""], ["Margel", "Shiri", ""], ["Domany", "Eytan", ""]]}, {"id": "1206.6863", "submitter": "Zhihua Zhang", "authors": "Zhihua Zhang, Michael I. Jordan", "title": "Bayesian Multicategory Support Vector Machines", "comments": "Appears in Proceedings of the Twenty-Second Conference on Uncertainty\n  in Artificial Intelligence (UAI2006)", "journal-ref": null, "doi": null, "report-no": "UAI-P-2006-PG-552-559", "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We show that the multi-class support vector machine (MSVM) proposed by Lee\net. al. (2004), can be viewed as a MAP estimation procedure under an\nappropriate probabilistic interpretation of the classifier. We also show that\nthis interpretation can be extended to a hierarchical Bayesian architecture and\nto a fully-Bayesian inference procedure for multi-class classification based on\ndata augmentation. We present empirical results that show that the advantages\nof the Bayesian formalism are obtained without a loss in classification\naccuracy.\n", "versions": [{"version": "v1", "created": "Wed, 27 Jun 2012 16:28:18 GMT"}], "update_date": "2012-07-02", "authors_parsed": [["Zhang", "Zhihua", ""], ["Jordan", "Michael I.", ""]]}, {"id": "1206.6865", "submitter": "Frank Wood", "authors": "Frank Wood, Thomas Griffiths, Zoubin Ghahramani", "title": "A Non-Parametric Bayesian Method for Inferring Hidden Causes", "comments": "Appears in Proceedings of the Twenty-Second Conference on Uncertainty\n  in Artificial Intelligence (UAI2006)", "journal-ref": null, "doi": null, "report-no": "UAI-P-2006-PG-536-543", "categories": "cs.LG cs.AI stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a non-parametric Bayesian approach to structure learning with\nhidden causes. Previous Bayesian treatments of this problem define a prior over\nthe number of hidden causes and use algorithms such as reversible jump Markov\nchain Monte Carlo to move between solutions. In contrast, we assume that the\nnumber of hidden causes is unbounded, but only a finite number influence\nobservable variables. This makes it possible to use a Gibbs sampler to\napproximate the distribution over causal structures. We evaluate the\nperformance of both approaches in discovering hidden causes in simulated data,\nand use our non-parametric approach to discover hidden causes in a real medical\ndataset.\n", "versions": [{"version": "v1", "created": "Wed, 27 Jun 2012 16:28:41 GMT"}], "update_date": "2012-07-02", "authors_parsed": [["Wood", "Frank", ""], ["Griffiths", "Thomas", ""], ["Ghahramani", "Zoubin", ""]]}, {"id": "1206.6868", "submitter": "Max Welling", "authors": "Max Welling, Sridevi Parise", "title": "Bayesian Random Fields: The Bethe-Laplace Approximation", "comments": "Appears in Proceedings of the Twenty-Second Conference on Uncertainty\n  in Artificial Intelligence (UAI2006)", "journal-ref": null, "doi": null, "report-no": "UAI-P-2006-PG-512-519", "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  While learning the maximum likelihood value of parameters of an undirected\ngraphical model is hard, modelling the posterior distribution over parameters\ngiven data is harder. Yet, undirected models are ubiquitous in computer vision\nand text modelling (e.g. conditional random fields). But where Bayesian\napproaches for directed models have been very successful, a proper Bayesian\ntreatment of undirected models in still in its infant stages. We propose a new\nmethod for approximating the posterior of the parameters given data based on\nthe Laplace approximation. This approximation requires the computation of the\ncovariance matrix over features which we compute using the linear response\napproximation based in turn on loopy belief propagation. We develop the theory\nfor conditional and 'unconditional' random fields with or without hidden\nvariables. In the conditional setting we introduce a new variant of bagging\nsuitable for structured domains. Here we run the loopy max-product algorithm on\na 'super-graph' composed of graphs for individual models sampled from the\nposterior and connected by constraints. Experiments on real world data validate\nthe proposed methods.\n", "versions": [{"version": "v1", "created": "Wed, 27 Jun 2012 16:29:18 GMT"}], "update_date": "2012-07-02", "authors_parsed": [["Welling", "Max", ""], ["Parise", "Sridevi", ""]]}, {"id": "1206.6870", "submitter": "Alexander L. Strehl", "authors": "Alexander L. Strehl, Lihong Li, Michael L. Littman", "title": "Incremental Model-based Learners With Formal Learning-Time Guarantees", "comments": "Appears in Proceedings of the Twenty-Second Conference on Uncertainty\n  in Artificial Intelligence (UAI2006)", "journal-ref": null, "doi": null, "report-no": "UAI-P-2006-PG-485-493", "categories": "cs.LG cs.AI stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Model-based learning algorithms have been shown to use experience efficiently\nwhen learning to solve Markov Decision Processes (MDPs) with finite state and\naction spaces. However, their high computational cost due to repeatedly solving\nan internal model inhibits their use in large-scale problems. We propose a\nmethod based on real-time dynamic programming (RTDP) to speed up two\nmodel-based algorithms, RMAX and MBIE (model-based interval estimation),\nresulting in computationally much faster algorithms with little loss compared\nto existing bounds. Specifically, our two new learning algorithms, RTDP-RMAX\nand RTDP-IE, have considerably smaller computational demands than RMAX and\nMBIE. We develop a general theoretical framework that allows us to prove that\nboth are efficient learners in a PAC (probably approximately correct) sense. We\nalso present an experimental evaluation of these new algorithms that helps\nquantify the tradeoff between computational and experience demands.\n", "versions": [{"version": "v1", "created": "Wed, 27 Jun 2012 16:29:41 GMT"}], "update_date": "2012-07-02", "authors_parsed": [["Strehl", "Alexander L.", ""], ["Li", "Lihong", ""], ["Littman", "Michael L.", ""]]}, {"id": "1206.6871", "submitter": "Harald Steck", "authors": "Harald Steck", "title": "Ranking by Dependence - A Fair Criteria", "comments": "Appears in Proceedings of the Twenty-Second Conference on Uncertainty\n  in Artificial Intelligence (UAI2006)", "journal-ref": null, "doi": null, "report-no": "UAI-P-2006-PG-477-484", "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Estimating the dependences between random variables, and ranking them\naccordingly, is a prevalent problem in machine learning. Pursuing frequentist\nand information-theoretic approaches, we first show that the p-value and the\nmutual information can fail even in simplistic situations. We then propose two\nconditions for regularizing an estimator of dependence, which leads to a simple\nyet effective new measure. We discuss its advantages and compare it to\nwell-established model-selection criteria. Apart from that, we derive a simple\nconstraint for regularizing parameter estimates in a graphical model. This\nresults in an analytical approximation for the optimal value of the equivalent\nsample size, which agrees very well with the more involved Bayesian approach in\nour experiments.\n", "versions": [{"version": "v1", "created": "Wed, 27 Jun 2012 16:29:52 GMT"}], "update_date": "2012-07-02", "authors_parsed": [["Steck", "Harald", ""]]}, {"id": "1206.6873", "submitter": "Edward Snelson", "authors": "Edward Snelson, Zoubin Ghahramani", "title": "Variable noise and dimensionality reduction for sparse Gaussian\n  processes", "comments": "Appears in Proceedings of the Twenty-Second Conference on Uncertainty\n  in Artificial Intelligence (UAI2006)", "journal-ref": null, "doi": null, "report-no": "UAI-P-2006-PG-461-468", "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The sparse pseudo-input Gaussian process (SPGP) is a new approximation method\nfor speeding up GP regression in the case of a large number of data points N.\nThe approximation is controlled by the gradient optimization of a small set of\nM `pseudo-inputs', thereby reducing complexity from N^3 to NM^2. One limitation\nof the SPGP is that this optimization space becomes impractically big for high\ndimensional data sets. This paper addresses this limitation by performing\nautomatic dimensionality reduction. A projection of the input space to a low\ndimensional space is learned in a supervised manner, alongside the\npseudo-inputs, which now live in this reduced space. The paper also\ninvestigates the suitability of the SPGP for modeling data with input-dependent\nnoise. A further extension of the model is made to make it even more powerful\nin this regard - we learn an uncertainty parameter for each pseudo-input. The\ncombination of sparsity, reduced dimension, and input-dependent noise makes it\npossible to apply GPs to much larger and more complex data sets than was\npreviously practical. We demonstrate the benefits of these methods on several\nsynthetic and real world problems.\n", "versions": [{"version": "v1", "created": "Wed, 27 Jun 2012 16:30:17 GMT"}], "update_date": "2012-07-02", "authors_parsed": [["Snelson", "Edward", ""], ["Ghahramani", "Zoubin", ""]]}, {"id": "1206.6927", "submitter": "Cheryl Brooks", "authors": "Cheryl J. Flynn, Patrick O. Perry", "title": "Profile Likelihood Biclustering", "comments": "40 pages, 11 figures; R package in development at\n  https://github.com/patperry/biclustpl", "journal-ref": "Electron. J. Statist., Volume 14, Number 1 (2020), 731-768", "doi": "10.1214/19-EJS1667", "report-no": null, "categories": "stat.ME math.ST stat.ML stat.TH", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Biclustering, the process of simultaneously clustering the rows and columns\nof a data matrix, is a popular and effective tool for finding structure in a\nhigh-dimensional dataset. Many biclustering procedures appear to work well in\npractice, but most do not have associated consistency guarantees. To address\nthis shortcoming, we propose a new biclustering procedure based on profile\nlikelihood. The procedure applies to a broad range of data modalities,\nincluding binary, count, and continuous observations. We prove that the\nprocedure recovers the true row and column classes when the dimensions of the\ndata matrix tend to infinity, even if the functional form of the data\ndistribution is misspecified. The procedure requires computing a combinatorial\nsearch, which can be expensive in practice. Rather than performing this search\ndirectly, we propose a new heuristic optimization procedure based on the\nKernighan-Lin heuristic, which has nice computational properties and performs\nwell in simulations. We demonstrate our procedure with applications to\ncongressional voting records, and microarray analysis.\n", "versions": [{"version": "v1", "created": "Fri, 29 Jun 2012 01:19:35 GMT"}, {"version": "v2", "created": "Fri, 4 Jan 2013 19:49:59 GMT"}, {"version": "v3", "created": "Fri, 8 Jan 2016 17:16:09 GMT"}, {"version": "v4", "created": "Tue, 2 Jun 2020 18:48:47 GMT"}], "update_date": "2020-06-04", "authors_parsed": [["Flynn", "Cheryl J.", ""], ["Perry", "Patrick O.", ""]]}, {"id": "1206.7051", "submitter": "David Blei", "authors": "Matt Hoffman, David M. Blei, Chong Wang, John Paisley", "title": "Stochastic Variational Inference", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.AI stat.CO stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We develop stochastic variational inference, a scalable algorithm for\napproximating posterior distributions. We develop this technique for a large\nclass of probabilistic models and we demonstrate it with two probabilistic\ntopic models, latent Dirichlet allocation and the hierarchical Dirichlet\nprocess topic model. Using stochastic variational inference, we analyze several\nlarge collections of documents: 300K articles from Nature, 1.8M articles from\nThe New York Times, and 3.8M articles from Wikipedia. Stochastic inference can\neasily handle data sets of this size and outperforms traditional variational\ninference, which can only handle a smaller subset. (We also show that the\nBayesian nonparametric topic model outperforms its parametric counterpart.)\nStochastic variational inference lets us apply complex Bayesian models to\nmassive data sets.\n", "versions": [{"version": "v1", "created": "Fri, 29 Jun 2012 15:23:11 GMT"}, {"version": "v2", "created": "Thu, 18 Apr 2013 15:40:02 GMT"}, {"version": "v3", "created": "Mon, 22 Apr 2013 20:23:40 GMT"}], "update_date": "2013-04-24", "authors_parsed": [["Hoffman", "Matt", ""], ["Blei", "David M.", ""], ["Wang", "Chong", ""], ["Paisley", "John", ""]]}, {"id": "1206.7112", "submitter": "Yi-Hao Kao", "authors": "Yi-Hao Kao and Benjamin Van Roy and Daniel Rubin and Jiajing Xu and\n  Jessica Faruque and Sandy Napel", "title": "A Hybrid Method for Distance Metric Learning", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.IR stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider the problem of learning a measure of distance among vectors in a\nfeature space and propose a hybrid method that simultaneously learns from\nsimilarity ratings assigned to pairs of vectors and class labels assigned to\nindividual vectors. Our method is based on a generative model in which class\nlabels can provide information that is not encoded in feature vectors but yet\nrelates to perceived similarity between objects. Experiments with synthetic\ndata as well as a real medical image retrieval problem demonstrate that\nleveraging class labels through use of our method improves retrieval\nperformance significantly.\n", "versions": [{"version": "v1", "created": "Fri, 29 Jun 2012 19:33:47 GMT"}], "update_date": "2012-07-02", "authors_parsed": [["Kao", "Yi-Hao", ""], ["Van Roy", "Benjamin", ""], ["Rubin", "Daniel", ""], ["Xu", "Jiajing", ""], ["Faruque", "Jessica", ""], ["Napel", "Sandy", ""]]}]