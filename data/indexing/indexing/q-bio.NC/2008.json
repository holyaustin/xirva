[{"id": "2008.00053", "submitter": "Jacob Adamczyk", "authors": "Jacob Adamczyk", "title": "Neural Network Degeneration and its Relationship to the Brain", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "q-bio.NC cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This report discusses the application of neural networks (NNs) as small\nsegments of the brain. The networks representing the biological connectome are\naltered both spatially and temporally. The degradation techniques applied here\nare \"weight degradation\", \"weight scrambling\", and variable activation\nfunction. These methods aim to shine light on the study of neurodegenerative\ndiseases such as Alzheimer's, Huntington's and Parkinson's disease as well as\nstrokes and brain tumors disrupting the flow of information in the brain's\nnetwork. Fundamental insights to memory loss and generalized learning\ndysfunction are gained by monitoring the network's error function during\nnetwork degradation. The biological significance of each facet is also\ndiscussed.\n", "versions": [{"version": "v1", "created": "Fri, 31 Jul 2020 19:42:23 GMT"}], "update_date": "2020-08-04", "authors_parsed": [["Adamczyk", "Jacob", ""]]}, {"id": "2008.00317", "submitter": "Shashwat Shukla", "authors": "Shashwat Shukla, Rohan Pathak, Vivek Saraswat and Udayan Ganguly", "title": "Adaptive Chemotaxis for improved Contour Tracking using Spiking Neural\n  Networks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "q-bio.NC cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper we present a Spiking Neural Network (SNN) for autonomous\nnavigation, inspired by the chemotaxis network of the worm Caenorhabditis\nelegans. In particular, we focus on the problem of contour tracking, wherein\nthe bot must reach and subsequently follow a desired concentration setpoint.\nPast schemes that used only klinokinesis can follow the contour efficiently but\ntake excessive time to reach the setpoint. We address this shortcoming by\nproposing a novel adaptive klinotaxis mechanism that builds upon a previously\nproposed gradient climbing circuit. We demonstrate how our klinotaxis circuit\ncan autonomously be configured to perform gradient ascent, gradient descent and\nsubsequently be disabled to seamlessly integrate with the aforementioned\nklinokinesis circuit. We also incorporate speed regulation (orthokinesis) to\nfurther improve contour tracking performance. Thus for the first time, we\npresent a model that successfully integrates klinokinesis, klinotaxis and\northokinesis. We demonstrate via contour tracking simulations that our proposed\nscheme achieves an 2.4x reduction in the time to reach the setpoint, along with\na simultaneous 8.7x reduction in average deviation from the setpoint.\n", "versions": [{"version": "v1", "created": "Sat, 1 Aug 2020 18:49:48 GMT"}], "update_date": "2020-08-04", "authors_parsed": [["Shukla", "Shashwat", ""], ["Pathak", "Rohan", ""], ["Saraswat", "Vivek", ""], ["Ganguly", "Udayan", ""]]}, {"id": "2008.00531", "submitter": "Luis F Seoane PhD", "authors": "Lu\\'is F Seoane", "title": "Fate of Duplicated Neural Structures", "comments": "Review with novel results. Position paper. 16 pages, 3 figures", "journal-ref": "Entropy 22, 928 (2020)", "doi": "10.3390/e22090928", "report-no": null, "categories": "q-bio.NC nlin.AO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Statistical mechanics determines the abundance of different arrangements of\nmatter depending on cost-benefit balances. Its formalism and phenomenology\npercolate throughout biological processes and set limits to effective\ncomputation. Under specific conditions, self-replicating and computationally\ncomplex patterns become favored, yielding life, cognition, and Darwinian\nevolution. Neurons and neural circuits sit at a crossroads between statistical\nmechanics, computation, and (through their role in cognition) natural\nselection. Can we establish a {\\em statistical physics} of neural circuits?\nSuch theory would tell what kinds of brains to expect under set energetic,\nevolutionary, and computational conditions. With this big picture in mind, we\nfocus on the fate of duplicated neural circuits. We look at examples from\ncentral nervous systems, with a stress on computational thresholds that might\nprompt this redundancy. We also study a naive cost-benefit balance for\nduplicated circuits implementing complex phenotypes. From this we derive {\\em\nphase diagrams} and (phase-like) transitions between single and duplicated\ncircuits, which constrain evolutionary paths to complex cognition. Back to the\nbig picture, similar phase diagrams and transitions might constrain I/O and\ninternal connectivity patterns of neural circuits at large. The formalism of\nstatistical mechanics seems a natural framework for thsi worthy line of\nresearch.\n", "versions": [{"version": "v1", "created": "Sun, 2 Aug 2020 17:43:57 GMT"}, {"version": "v2", "created": "Sat, 29 Aug 2020 14:12:28 GMT"}, {"version": "v3", "created": "Mon, 1 Feb 2021 19:06:11 GMT"}], "update_date": "2021-02-03", "authors_parsed": [["Seoane", "Lu\u00eds F", ""]]}, {"id": "2008.00629", "submitter": "Hyun Jin Kim", "authors": "Jimmy H. J. Kim and Ila Fiete and David J. Schwab", "title": "Superlinear Precision and Memory in Simple Population Codes", "comments": "5 pages, 4 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "q-bio.NC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The brain constructs population codes to represent stimuli through widely\ndistributed patterns of activity across neurons. An important figure of merit\nof population codes is how much information about the original stimulus can be\ndecoded from them. Fisher information is widely used to quantify coding\nprecision and specify optimal codes, because of its relationship to mean\nsquared error (MSE) under certain assumptions. When neural firing is sparse,\nhowever, optimizing Fisher information can result in codes that are highly\nsub-optimal in terms of MSE. We find that this discrepancy arises from the\nnon-local component of error not accounted for by the Fisher information. Using\nthis insight, we construct optimal population codes by directly minimizing the\nMSE. We study the scaling properties of MSE with coding parameters, focusing on\nthe tuning curve width. We find that the optimal tuning curve width for coding\nno longer scales as the inverse population size, and the quadratic scaling of\nprecision with system size predicted by Fisher information alone no longer\nholds. However, superlinearity is still preserved with only a logarithmic\nslowdown. We derive analogous results for networks storing the memory of a\nstimulus through continuous attractor dynamics, and show that similar scaling\nproperties optimize memory and representation.\n", "versions": [{"version": "v1", "created": "Mon, 3 Aug 2020 03:30:28 GMT"}], "update_date": "2020-08-04", "authors_parsed": [["Kim", "Jimmy H. J.", ""], ["Fiete", "Ila", ""], ["Schwab", "David J.", ""]]}, {"id": "2008.00754", "submitter": "Robert Pepperell", "authors": "Robert Pepperell", "title": "Vision as an Energy-Driven Process", "comments": "32 pages, 5 figures, under review", "journal-ref": null, "doi": null, "report-no": null, "categories": "q-bio.NC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  It is frequently assumed that the fundamental purpose of vision is to extract\ninformation from light and process it in the brain in order to gain knowledge\nabout environmental objects. Treating vision as an information-driven process\nhas been fruitful for many areas of vision science. But this approach has\ninherent limitations and may not be sufficient to explain at the most\nfundamental level why and how we see. Energy is a prime mover in the evolution\nand operation of all biological systems, including vision, and early\nresearchers in physiological optics and experimental psychology regarded vision\nas an energy-driven process. Yet to date there has been little attempt to\nanalyze the fundamentals of vision in energetic terms. This paper is a\nprovisional attempt to sketch an account of the early stages of vision as an\nenergy-driven process. On this account, vision is a form of biological work in\nwhich energy from the environment is absorbed and processed by neurobiological\nsystems in order to guide behaviour. Evidence from the evolution and\nneurobiology of vision is presented in support of this approach. I conclude\nthat treating vision as an energy-driven process can contribute to our\nunderstanding of its fundamental nature and operation.\n", "versions": [{"version": "v1", "created": "Mon, 3 Aug 2020 10:12:58 GMT"}], "update_date": "2020-08-04", "authors_parsed": [["Pepperell", "Robert", ""]]}, {"id": "2008.01032", "submitter": "Carina Curto", "authors": "Carina Curto, Christopher Langdon, Katherine Morrison", "title": "Combinatorial Geometry of Threshold-Linear Networks", "comments": "24 pages, 16 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.CO q-bio.NC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The architecture of a neural network constrains the potential dynamics that\ncan emerge. Some architectures may only allow for a single dynamic regime,\nwhile others display a great deal of flexibility with qualitatively different\ndynamics that can be reached by modulating connection strengths. In this work,\nwe develop novel mathematical techniques to study the dynamic constraints\nimposed by different network architectures in the context of competitive\nthreshold-linear networks (TLNs). Any given TLN is naturally characterized by a\nhyperplane arrangement in $\\mathbb{R}^n$, and the combinatorial properties of\nthis arrangement determine the pattern of fixed points of the dynamics. This\nobservation enables us to recast the question of network flexibility in the\nlanguage of oriented matroids, allowing us to employ tools and results from\nthis theory in order to characterize the different dynamic regimes a given\narchitecture can support. In particular, fixed points of a TLN correspond to\ncocircuits of an associated oriented matroid; and mutations of the matroid\ncorrespond to bifurcations in the collection of fixed points. As an\napplication, we provide a complete characterization of all possible sets of\nfixed points that can arise in networks through size $n=3$, together with\ndescriptions of how to modulate synaptic strengths of the network in order to\naccess the different dynamic regimes. These results provide a framework for\nstudying the possible computational roles of various motifs observed in real\nneural networks.\n", "versions": [{"version": "v1", "created": "Mon, 3 Aug 2020 17:17:35 GMT"}], "update_date": "2020-08-04", "authors_parsed": [["Curto", "Carina", ""], ["Langdon", "Christopher", ""], ["Morrison", "Katherine", ""]]}, {"id": "2008.01810", "submitter": "Neta Maimon", "authors": "Assaf Suberry, Neta B. Maimon and Zohar Eitan", "title": "Sad syntax? Tonal closure Affects Children's Perception of Emotional\n  Valence", "comments": "44 pages, 7 figures, 1 table, 1 appendix", "journal-ref": null, "doi": null, "report-no": null, "categories": "q-bio.NC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Western music is largely governed by tonality, a quasi syntactic system\nregulating musical continuity and closure. Converging measures have established\nthe psychological reality of tonality as a cognitive schema raising distinct\nexpectancy for both adults and children. However, while tonal expectations were\nassociated with emotion in adults, little is known about the tonality emotional\neffects in children. Here we examine whether children associate levels of tonal\nclosure with emotional valence, whether such associations are age dependent,\nand how they interact with other musical dimensions. 52 children, aged 7, 11,\nlistened to chord progressions implying closure followed by a probe tone.\nProbes could realize closure (tonic note), violate it mildly (unstable diatonic\nnote) or extremely (out of key note). Three timbres (piano, guitar, woodwinds)\nand three pitch heights were used for each closure level. Stimuli were\ndescribed to participants as exchanges between two children (chords, probe).\nParticipants chose one of two emojis, suggesting positive or negative emotions,\nas representing the 2nd child response. A significant effect of tonal closure\nwas found, with no interactions with age, instrument, or pitch height. Results\nsuggest that tonality, a non referential cognitive schema, affects children\nperception of emotion in music early, robustly and independently of basic\nmusical dimensions.\n", "versions": [{"version": "v1", "created": "Tue, 4 Aug 2020 20:13:53 GMT"}], "update_date": "2020-08-06", "authors_parsed": [["Suberry", "Assaf", ""], ["Maimon", "Neta B.", ""], ["Eitan", "Zohar", ""]]}, {"id": "2008.02595", "submitter": "Peter Harrison", "authors": "Peter M. C. Harrison, Raja Marjieh, Federico Adolfi, Pol van Rijn,\n  Manuel Anglada-Tort, Ofer Tchernichovski, Pauline Larrouy-Maestri, Nori\n  Jacoby", "title": "Gibbs Sampling with People", "comments": "Accepted for oral presentation at NeurIPS 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "q-bio.NC cs.AI cs.CV stat.AP", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  A core problem in cognitive science and machine learning is to understand how\nhumans derive semantic representations from perceptual objects, such as color\nfrom an apple, pleasantness from a musical chord, or seriousness from a face.\nMarkov Chain Monte Carlo with People (MCMCP) is a prominent method for studying\nsuch representations, in which participants are presented with binary choice\ntrials constructed such that the decisions follow a Markov Chain Monte Carlo\nacceptance rule. However, while MCMCP has strong asymptotic properties, its\nbinary choice paradigm generates relatively little information per trial, and\nits local proposal function makes it slow to explore the parameter space and\nfind the modes of the distribution. Here we therefore generalize MCMCP to a\ncontinuous-sampling paradigm, where in each iteration the participant uses a\nslider to continuously manipulate a single stimulus dimension to optimize a\ngiven criterion such as 'pleasantness'. We formulate both methods from a\nutility-theory perspective, and show that the new method can be interpreted as\n'Gibbs Sampling with People' (GSP). Further, we introduce an aggregation\nparameter to the transition step, and show that this parameter can be\nmanipulated to flexibly shift between Gibbs sampling and deterministic\noptimization. In an initial study, we show GSP clearly outperforming MCMCP; we\nthen show that GSP provides novel and interpretable results in three other\ndomains, namely musical chords, vocal emotions, and faces. We validate these\nresults through large-scale perceptual rating experiments. The final\nexperiments use GSP to navigate the latent space of a state-of-the-art image\nsynthesis network (StyleGAN), a promising approach for applying GSP to\nhigh-dimensional perceptual spaces. We conclude by discussing future cognitive\napplications and ethical implications.\n", "versions": [{"version": "v1", "created": "Thu, 6 Aug 2020 11:57:07 GMT"}, {"version": "v2", "created": "Mon, 2 Nov 2020 16:55:40 GMT"}], "update_date": "2020-11-03", "authors_parsed": [["Harrison", "Peter M. C.", ""], ["Marjieh", "Raja", ""], ["Adolfi", "Federico", ""], ["van Rijn", "Pol", ""], ["Anglada-Tort", "Manuel", ""], ["Tchernichovski", "Ofer", ""], ["Larrouy-Maestri", "Pauline", ""], ["Jacoby", "Nori", ""]]}, {"id": "2008.02788", "submitter": "Pedro Mediano", "authors": "Rodrigo Cofr\\'e, Rub\\'en Herzog, Pedro A.M. Mediano, Juan Piccinini,\n  Fernando E. Rosas, Yonatan Sanz Perl and Enzo Tagliazucchi", "title": "Whole-brain models to explore altered states of consciousness from the\n  bottom up", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "q-bio.NC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The scope of human consciousness includes states departing from what most of\nus experience as ordinary wakefulness. These altered states of consciousness\nconstitute a prime opportunity to study how global changes in brain activity\nrelate to different varieties of subjective experience. We consider the problem\nof explaining how global signatures of altered consciousness arise from the\ninterplay between large-scale connectivity and local dynamical rules that can\nbe traced to known properties of neural tissue. For this purpose, we advocate a\nresearch program aimed at bridging the gap between bottom-up generative models\nof whole-brain activity and the top-down signatures proposed by theories of\nconsciousness. Throughout this paper, we define altered states of\nconsciousness, discuss relevant signatures of consciousness observed in brain\nactivity, and introduce whole-brain models to explore the mechanisms of altered\nconsciousness from the bottom-up. We discuss the potential of our proposal in\nview of the current state of the art, give specific examples of how this\nresearch agenda might play out, and emphasise how a systematic investigation of\naltered states of consciousness via bottom-up modelling may help us better\nunderstand the biophysical, informational, and dynamical underpinnings of\nconsciousness.\n", "versions": [{"version": "v1", "created": "Thu, 6 Aug 2020 17:54:37 GMT"}], "update_date": "2020-08-07", "authors_parsed": [["Cofr\u00e9", "Rodrigo", ""], ["Herzog", "Rub\u00e9n", ""], ["Mediano", "Pedro A. M.", ""], ["Piccinini", "Juan", ""], ["Rosas", "Fernando E.", ""], ["Perl", "Yonatan Sanz", ""], ["Tagliazucchi", "Enzo", ""]]}, {"id": "2008.02961", "submitter": "Gia Ngo", "authors": "Gia H. Ngo, Meenakshi Khosla, Keith Jamison, Amy Kuceyeski, Mert R.\n  Sabuncu", "title": "From Connectomic to Task-evoked Fingerprints: Individualized Prediction\n  of Task Contrasts from Resting-state Functional Connectivity", "comments": "Accepted to MICCAI 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG q-bio.NC stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Resting-state functional MRI (rsfMRI) yields functional connectomes that can\nserve as cognitive fingerprints of individuals. Connectomic fingerprints have\nproven useful in many machine learning tasks, such as predicting\nsubject-specific behavioral traits or task-evoked activity. In this work, we\npropose a surface-based convolutional neural network (BrainSurfCNN) model to\npredict individual task contrasts from their resting-state fingerprints. We\nintroduce a reconstructive-contrastive loss that enforces subject-specificity\nof model outputs while minimizing predictive error. The proposed approach\nsignificantly improves the accuracy of predicted contrasts over a\nwell-established baseline. Furthermore, BrainSurfCNN's prediction also\nsurpasses test-retest benchmark in a subject identification task.\n", "versions": [{"version": "v1", "created": "Fri, 7 Aug 2020 02:44:16 GMT"}], "update_date": "2020-08-10", "authors_parsed": [["Ngo", "Gia H.", ""], ["Khosla", "Meenakshi", ""], ["Jamison", "Keith", ""], ["Kuceyeski", "Amy", ""], ["Sabuncu", "Mert R.", ""]]}, {"id": "2008.03198", "submitter": "Jannes Jegminat", "authors": "Jannes Jegminat, Jean-Pascal Pfister", "title": "Learning as filtering: implications for spike-based plasticity", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "q-bio.NC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Most normative models in computational neuroscience describe the task of\nlearning as the optimisation of a cost function with respect to a set of\nparameters. However, learning as optimisation fails to account for a time\nvarying environment during the learning process; and the resulting point\nestimate in parameter space does not account for uncertainty. Here, we frame\nlearning as filtering, i.e., a principled method for including time and\nparameter uncertainty. We derive the filtering-based learning rule for a\nspiking neuronal network - the Synaptic Filter - and show its computational and\nbiological relevance. For the computational relevance, we show that filtering\nin combination with Bayesian regression improves performance compared to a\ngradient learning rule with optimal learning rate in terms of weight\nestimation. Furthermore, the filtering-based rule outperforms gradient-based\nrules in the presence of model mismatch, indicating a better generalisation\nperformance. The dynamics of the mean of the Synaptic Filter is consistent with\nthe spike-timing dependent plasticity (STDP) while the dynamics of the variance\nmakes novel predictions regarding spike-timing dependent changes of EPSP\nvariability. Moreover, the Synaptic Filter explains experimentally observed\nnegative correlations between homo- and heterosynaptic plasticity.\n", "versions": [{"version": "v1", "created": "Fri, 7 Aug 2020 14:26:29 GMT"}], "update_date": "2020-08-10", "authors_parsed": [["Jegminat", "Jannes", ""], ["Pfister", "Jean-Pascal", ""]]}, {"id": "2008.03238", "submitter": "Maxwell J. D. Ramstead", "authors": "Maxwell J. D. Ramstead, Casper Hesp, Alec Tschantz, Ryan Smith, Axel\n  Constant, Karl Friston", "title": "Neural and phenotypic representation under the free-energy principle", "comments": "30 pages; 6 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "q-bio.NC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The aim of this paper is to leverage the free-energy principle and its\ncorollary process theory, active inference, to develop a generic, generalizable\nmodel of the representational capacities of living creatures; that is, a theory\nof phenotypic representation. Given their ubiquity, we are concerned with\ndistributed forms of representation (e.g., population codes), whereby patterns\nof ensemble activity in living tissue come to represent the causes of sensory\ninput or data. The active inference framework rests on the Markov blanket\nformalism, which allows us to partition systems of interest, such as biological\nsystems, into internal states, external states, and the blanket (active and\nsensory) states that render internal and external states conditionally\nindependent of each other. In this framework, the representational capacity of\nliving creatures emerges as a consequence of their Markovian structure and\nnonequilibrium dynamics, which together entail a dual-aspect information\ngeometry. This entails a modest representational capacity: internal states have\nan intrinsic information geometry that describes their trajectory over time in\nstate space, as well as an extrinsic information geometry that allows internal\nstates to encode (the parameters of) probabilistic beliefs about (fictive)\nexternal states. Building on this, we describe here how, in an automatic and\nemergent manner, information about stimuli can come to be encoded by groups of\nneurons bound by a Markov blanket; what is known as the neuronal packet\nhypothesis. As a concrete demonstration of this type of emergent\nrepresentation, we present numerical simulations showing that self-organizing\nensembles of active inference agents sharing the right kind of probabilistic\ngenerative model are able to encode recoverable information about a stimulus\narray.\n", "versions": [{"version": "v1", "created": "Fri, 7 Aug 2020 15:55:42 GMT"}, {"version": "v2", "created": "Tue, 1 Dec 2020 11:53:00 GMT"}], "update_date": "2020-12-02", "authors_parsed": [["Ramstead", "Maxwell J. D.", ""], ["Hesp", "Casper", ""], ["Tschantz", "Alec", ""], ["Smith", "Ryan", ""], ["Constant", "Axel", ""], ["Friston", "Karl", ""]]}, {"id": "2008.03377", "submitter": "Andrey L. Shilnikov", "authors": "Aaron Kelley, Andrey L. Shilnikov", "title": "2$\\theta$-burster for rhythm-generating circuits", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "q-bio.NC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose and demonstrate the use of a minimal 2$\\theta$ model for\nendogenous bursters coupled in 3-cell neural circuits. This 2$\\theta$ model\noffers the benefit of simplicity of designing larger neural networks along with\nan acute reduction on the computation cost.\n", "versions": [{"version": "v1", "created": "Wed, 29 Jul 2020 21:08:57 GMT"}], "update_date": "2020-08-11", "authors_parsed": [["Kelley", "Aaron", ""], ["Shilnikov", "Andrey L.", ""]]}, {"id": "2008.03519", "submitter": "Lucas Tian", "authors": "Lucas Y. Tian, Kevin Ellis, Marta Kryven, Joshua B. Tenenbaum", "title": "Learning abstract structure for drawing by efficient motor program\n  induction", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.LG q-bio.NC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Humans flexibly solve new problems that differ qualitatively from those they\nwere trained on. This ability to generalize is supported by learned concepts\nthat capture structure common across different problems. Here we develop a\nnaturalistic drawing task to study how humans rapidly acquire structured prior\nknowledge. The task requires drawing visual objects that share underlying\nstructure, based on a set of composable geometric rules. We show that people\nspontaneously learn abstract drawing procedures that support generalization,\nand propose a model of how learners can discover these reusable drawing\nprograms. Trained in the same setting as humans, and constrained to produce\nefficient motor actions, this model discovers new drawing routines that\ntransfer to test objects and resemble learned features of human sequences.\nThese results suggest that two principles guiding motor program induction in\nthe model - abstraction (general programs that ignore object-specific details)\nand compositionality (recombining previously learned programs) - are key for\nexplaining how humans learn structured internal representations that guide\nflexible reasoning and learning.\n", "versions": [{"version": "v1", "created": "Sat, 8 Aug 2020 13:31:14 GMT"}], "update_date": "2020-08-11", "authors_parsed": [["Tian", "Lucas Y.", ""], ["Ellis", "Kevin", ""], ["Kryven", "Marta", ""], ["Tenenbaum", "Joshua B.", ""]]}, {"id": "2008.03881", "submitter": "Ulises Pereira", "authors": "Xiao-Jing Wang, Ulises Pereira, Marcello G.P. Rosa, Henry Kennedy", "title": "Brain Connectomes Come of Age", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "q-bio.NC", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Databases of directed- and weighted- connectivity for mouse, macaque and\nmarmoset monkeys, have recently become available and begun to be used to build\ndynamical models. A hierarchical organization can be defined based on laminar\npatterns of cortical connections, possibly improved by thalamocortical\nprojections. A large-scale model of the macaque cortex endowed with a laminar\nstructure accounts for layer-dependent and frequency-modulated interplays\nbetween bottom-up and top-down processes. Signal propagation in a version of\nthe model with spiking neurons displays a threshold of stimulus amplitude for\nthe activity to gain access to the prefrontal cortex, reminiscent of the\nignition phenomenon associated with conscious perception. These two examples\nillustrate how connectomics may inform theory leading to discoveries.\nComputational modeling raises open questions for future empirical research, in\na back-and-forth collaboration of experimentalists and theorists.\n", "versions": [{"version": "v1", "created": "Mon, 10 Aug 2020 03:20:08 GMT"}], "update_date": "2020-08-11", "authors_parsed": [["Wang", "Xiao-Jing", ""], ["Pereira", "Ulises", ""], ["Rosa", "Marcello G. P.", ""], ["Kennedy", "Henry", ""]]}, {"id": "2008.03972", "submitter": "Audrey B\\\"urki", "authors": "Audrey B\\\"urki, F.-Xavier Alario, Shravan Vasishth", "title": "When words collide: Bayesian meta-analyses of distractor and target\n  properties in the picture-word interference paradigm", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "q-bio.NC stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In the picture-word interference paradigm, participants name pictures while\nignoring a written or spoken distractor word. Naming times to the pictures are\nslowed down by the presence of the distractor word. Various properties of the\ndistractor modulate this slow down, for example naming times are shorter with\nfrequent vs. infrequent distractors. Building on this line of research, the\npresent study investigates in more detail the impact of distractor and target\nword properties on picture naming times. We report the results of several\nBayesian meta-analyses, based on 35 datasets. The aim of the first analysis was\nto obtain an estimation of the size of the distractor frequency effect, and of\nits precision, in typical picture-word interference experiments where this\nvariable is not manipulated. The analysis shows that a one-unit increase in log\nfrequency results in response times to the pictures decreasing by about 4ms\n(95% Credible Interval: [-6, -2]). With the second and third analyses, we show\nthat after accounting for the effect of frequency, two variables known to\ninfluence processing times in visual word processing tasks also influence\npicture naming times: distractor length and orthographic neighborhood. Finally,\nwe found that distractor word frequency and target word frequency interact; the\neffect of distractor frequency decreases as the frequency of the target word\nincreases. We discuss the theoretical and methodological implications of these\nfindings, as well as the importance of obtaining high-precision estimates of\nexperimental effects.\n", "versions": [{"version": "v1", "created": "Mon, 10 Aug 2020 09:13:57 GMT"}], "update_date": "2020-08-11", "authors_parsed": [["B\u00fcrki", "Audrey", ""], ["Alario", "F. -Xavier", ""], ["Vasishth", "Shravan", ""]]}, {"id": "2008.04053", "submitter": "Ehtibar Dzhafarov", "authors": "V\\'ictor H. Cervantes and Ehtibar N. Dzhafarov", "title": "Contextuality Analysis of Impossible Figures", "comments": "Entropy 2020, 22(9), 981; format of the published paper differs from\n  this preprint", "journal-ref": "Entropy 2020, 22(9), 981", "doi": "10.3390/e22090981", "report-no": null, "categories": "q-bio.NC math.PR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper has two purposes. One is to demonstrate contextuality analysis of\nsystems of epistemic random variables. The other is to evaluate the performance\nof a new, hierarchical version of the measure of (non)contextuality introduced\nin earlier publications. As objects of analysis we use impossible figures of\nthe kind created by the Penroses and Escher. We make no assumptions as to how\nan impossible figure is perceived, taking it instead as a fixed physical object\nallowing one of several deterministic descriptions. Systems of epistemic random\nvariables are obtained by probabilistically mixing these deterministic systems.\nThis probabilistic mixture reflects our uncertainty or lack of knowledge rather\nthan random variability in the frequentist sense.\n", "versions": [{"version": "v1", "created": "Wed, 5 Aug 2020 21:01:49 GMT"}, {"version": "v2", "created": "Mon, 31 Aug 2020 15:04:26 GMT"}, {"version": "v3", "created": "Thu, 3 Sep 2020 11:33:36 GMT"}], "update_date": "2020-09-04", "authors_parsed": [["Cervantes", "V\u00edctor H.", ""], ["Dzhafarov", "Ehtibar N.", ""]]}, {"id": "2008.04208", "submitter": "Seyed Mohammad Mahdi Heidarpoor Yazdi", "authors": "Seyed Mohammad Mahdi Heidarpoor Yazdi, Abdolhossein Abbassian", "title": "Working Memory for Online Memory Binding Tasks: A Hybrid Model", "comments": "23 pages, 17 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NE cs.AI cs.LG q-bio.NC", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Working Memory is the brain module that holds and manipulates information\nonline. In this work, we design a hybrid model in which a simple feed-forward\nnetwork is coupled to a balanced random network via a read-write vector called\nthe interface vector. Three cases and their results are discussed similar to\nthe n-back task called, first-order memory binding task, generalized\nfirst-order memory task, and second-order memory binding task. The important\nresult is that our dual-component model of working memory shows good\nperformance with learning restricted to the feed-forward component only. Here\nwe take advantage of the random network property without learning. Finally, a\nmore complex memory binding task called, a cue-based memory binding task, is\nintroduced in which a cue is given as input representing a binding relation\nthat prompts the network to choose the useful chunk of memory. To our\nknowledge, this is the first time that random networks as a flexible memory is\nshown to play an important role in online binding tasks. We may interpret our\nresults as a candidate model of working memory in which the feed-forward\nnetwork learns to interact with the temporary storage random network as an\nattentional-controlling executive system.\n", "versions": [{"version": "v1", "created": "Wed, 5 Aug 2020 14:06:07 GMT"}, {"version": "v2", "created": "Sat, 12 Dec 2020 08:33:38 GMT"}], "update_date": "2020-12-15", "authors_parsed": [["Yazdi", "Seyed Mohammad Mahdi Heidarpoor", ""], ["Abbassian", "Abdolhossein", ""]]}, {"id": "2008.04435", "submitter": "Mia Morrell", "authors": "Mia C. Morrell, Audrey J. Sederberg, Ilya Nemenman", "title": "Latent dynamical variables produce signatures of spatiotemporal\n  criticality in large biological systems", "comments": null, "journal-ref": "Phys. Rev. Lett. 126, 118302 (2021)", "doi": "10.1103/PhysRevLett.126.118302", "report-no": null, "categories": "q-bio.NC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Understanding the activity of large populations of neurons is difficult due\nto the combinatorial complexity of possible cell-cell interactions. To reduce\nthe complexity, coarse-graining had been previously applied to experimental\nneural recordings, which showed over two decades of scaling in free energy,\nactivity variance, eigenvalue spectra, and correlation time, hinting that the\nmouse hippocampus operates in a critical regime. We model the experiment by\nsimulating conditionally independent binary neurons coupled to a small number\nof long-timescale stochastic fields and then replicating the coarse-graining\nprocedure and analysis. This reproduces the experimentally-observed scalings,\nsuggesting that they may arise from coupling the neural population activity to\nlatent dynamic stimuli. Further, parameter sweeps for our model suggest that\nemergence of scaling requires most of the cells in a population to couple to\nthe latent stimuli, predicting that even the celebrated place cells must also\nrespond to non-place stimuli.\n", "versions": [{"version": "v1", "created": "Mon, 10 Aug 2020 22:07:12 GMT"}], "update_date": "2021-03-24", "authors_parsed": [["Morrell", "Mia C.", ""], ["Sederberg", "Audrey J.", ""], ["Nemenman", "Ilya", ""]]}, {"id": "2008.04940", "submitter": "Corey Weistuch", "authors": "Corey Weistuch, Lilianne R. Mujica-Parodi, and Ken Dill", "title": "The refractory period matters: unifying mechanisms of macroscopic brain\n  waves", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "q-bio.NC", "license": "http://creativecommons.org/licenses/by-sa/4.0/", "abstract": "  The relationship between complex, brain oscillations and the dynamics of\nindividual neurons is poorly understood. Here we utilize Maximum Caliber, a\ndynamical inference principle, to build a minimal, yet general model of the\ncollective (mean-field) dynamics of large populations of neurons. In agreement\nwith previous experimental observations, we describe a simple, testable\nmechanism, involving only a single type of neuron, by which many of these\ncomplex oscillatory patterns may emerge. Our model predicts that the refractory\nperiod of neurons, which has been previously neglected, is essential for these\nbehaviors.\n", "versions": [{"version": "v1", "created": "Tue, 11 Aug 2020 18:10:43 GMT"}, {"version": "v2", "created": "Thu, 3 Sep 2020 16:31:43 GMT"}], "update_date": "2020-09-04", "authors_parsed": [["Weistuch", "Corey", ""], ["Mujica-Parodi", "Lilianne R.", ""], ["Dill", "Ken", ""]]}, {"id": "2008.04978", "submitter": "Victoria Webster-Wood", "authors": "Victoria A. Webster-Wood, Jeffrey P. Gill, Peter J. Thomas, Hillel J.\n  Chiel", "title": "Control for Multifunctionality: Bioinspired Control Based on Feeding in\n  Aplysia californica", "comments": "Revisions have been made to improve manuscript clarity and expand the\n  introduction and discussion. The results are unchanged", "journal-ref": "Biol Cybern (2020)", "doi": "10.1007/s00422-020-00851-9", "report-no": null, "categories": "q-bio.NC cs.NE", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Animals exhibit remarkable feats of behavioral flexibility and\nmultifunctional control that remain challenging for robotic systems. The neural\nand morphological basis of multifunctionality in animals can provide a source\nof bio-inspiration for robotic controllers. However, many existing approaches\nto modeling biological neural networks rely on computationally expensive models\nand tend to focus solely on the nervous system, often neglecting the\nbiomechanics of the periphery. As a consequence, while these models are\nexcellent tools for neuroscience, they fail to predict functional behavior in\nreal time, which is a critical capability for robotic control. To meet the need\nfor real-time multifunctional control, we have developed a hybrid Boolean model\nframework capable of modeling neural bursting activity and simple biomechanics\nat speeds faster than real time. Using this approach, we present a\nmultifunctional model of Aplysia californica feeding that qualitatively\nreproduces three key feeding behaviors (biting, swallowing, and rejection),\ndemonstrates behavioral switching in response to external sensory cues, and\nincorporates both known neural connectivity and a simple bioinspired mechanical\nmodel of the feeding apparatus. We demonstrate that the model can be used for\nformulating testable hypotheses and discuss the implications of this approach\nfor robotic control and neuroscience.\n", "versions": [{"version": "v1", "created": "Tue, 11 Aug 2020 19:26:50 GMT"}, {"version": "v2", "created": "Sat, 21 Nov 2020 14:25:23 GMT"}], "update_date": "2020-12-14", "authors_parsed": [["Webster-Wood", "Victoria A.", ""], ["Gill", "Jeffrey P.", ""], ["Thomas", "Peter J.", ""], ["Chiel", "Hillel J.", ""]]}, {"id": "2008.04987", "submitter": "Neta Maimon", "authors": "Neta B. Maimon, Lior Molcho, Nathan Intrator, and Dominique Lamy", "title": "Single-channel EEG features during n-back task correlate with working\n  memory load", "comments": "25 pages, 6 figures, 3 tables, 1 appendix", "journal-ref": null, "doi": null, "report-no": null, "categories": "q-bio.NC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Working Memory (WM) load is an important cognitive feature that is highly\ncorrelated with mental effort. Several neurological biomarkers such as theta\npower and mid-frontal activity show increased activity with increasing WM load.\nSuch correlations often break down in cognitively impaired individuals, making\nWM load biomarkers a valuable tool for the detection of cognitive impairment.\nHowever, most studies have used a multi-channel EEG or an fMRI, which are not\nmassively accessible. In the present study, we evaluate the ability of novel\nfeatures extracted from a single-channel EEG located on the forehead, to serve\nas markers of WM load. We employed the widely used n-back task to manipulate WM\nload. Fourteen participants performed the n-back task while their brain\nactivity was recorded with the Neurosteer inc Aurora EEG device. The results\nshowed that the activity of the newly introduced features increased with WM\nload, similar to the theta band, but exhibited higher sensitivity to finer WM\nload changes. These more sensitive biomarkers of WM load are a promising tool\nfor mass screening of mild cognitive impairment.\n", "versions": [{"version": "v1", "created": "Tue, 11 Aug 2020 19:54:37 GMT"}], "update_date": "2020-08-13", "authors_parsed": [["Maimon", "Neta B.", ""], ["Molcho", "Lior", ""], ["Intrator", "Nathan", ""], ["Lamy", "Dominique", ""]]}, {"id": "2008.05580", "submitter": "Richard Granger", "authors": "Richard Granger", "title": "Toward the quantification of cognition", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "q-bio.NC cs.AI", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  The machinery of the human brain -- analog, probabilistic, embodied -- can be\ncharacterized computationally, but what machinery confers what computational\npowers? Any such system can be abstractly cast in terms of two computational\ncomponents: a finite state machine carrying out computational steps, whether\nvia currents, chemistry, or mechanics; plus a set of allowable memory\noperations, typically formulated in terms of an information store that can be\nread from and written to, whether via synaptic change, state transition, or\nrecurrent activity. Probing these mechanisms for their information content, we\ncan capture the difference in computational power that various systems are\ncapable of. Most human cognitive abilities, from perception to action to\nmemory, are shared with other species; we seek to characterize those (few)\ncapabilities that are ubiquitously present among humans and absent from other\nspecies. Three realms of formidable constraints -- a) measurable human\ncognitive abilities, b) measurable allometric anatomic brain characteristics,\nand c) measurable features of specific automata and formal grammars --\nillustrate remarkably sharp restrictions on human abilities, unexpectedly\nconfining human cognition to a specific class of automata (\"nested stack\"),\nwhich are markedly below Turing machines.\n", "versions": [{"version": "v1", "created": "Wed, 12 Aug 2020 21:45:29 GMT"}], "update_date": "2020-08-14", "authors_parsed": [["Granger", "Richard", ""]]}, {"id": "2008.05591", "submitter": "Daniel L. Cox", "authors": "Yuduo Zhi and Daniel L. Cox", "title": "Neurodegenerative damage reduces firing coherence in a continuous\n  attractor model of grid cells", "comments": "24 pages, 9 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "q-bio.NC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Grid cells in the dorsolateral band of the medial entorhinal cortex(dMEC)\ndisplay strikingly regular periodic firing patterns on a lattice of positions\nin 2-D space. This helps animals to encode relative spatial location without\nreference to external cues. The dMEC is damaged in the early stages of\nAlzheimer's Disease, which affects navigation ability of a disease victim,\nreducing the synaptic density of neurons in the network. Within an established\n2-dimensional continuous attractor neural network model of grid cell activity,\nwe introduce damage parameterized by radius and by the strength of the synaptic\noutput for neurons in the damaged region. The proportionality of the grid field\nflow on the dMEX to the velocity of the model organism is maintained, but when\nwe examine the coherence of the grid cell firing field in the form of the\nFourier transform (Bragg peaks) of the grid lattice, we find that a wide range\nof damage radius and strength induces an incoherent structure with only a\nsingle central peak, adjacent to narrow bands of striped (two additional\npeaks), which abut an orthorhombic pattern (four additional peaks), that abuts\nthe undamaged hexagonal region (six additional peaks). Within the damaged\nregion, grid cells show no Bragg peaks, and outside the damaged region the\ncentral Bragg peak strength is largely unaffected. There is a re-entrant region\nof normal grid firing for very large damage area. We anticipate that the\nmodified grid cell behavior can be observed in non-invasive fMRI imaging of the\ndMEC.\n", "versions": [{"version": "v1", "created": "Wed, 12 Aug 2020 22:38:33 GMT"}, {"version": "v2", "created": "Wed, 14 Apr 2021 00:45:00 GMT"}], "update_date": "2021-04-15", "authors_parsed": [["Zhi", "Yuduo", ""], ["Cox", "Daniel L.", ""]]}, {"id": "2008.06093", "submitter": "Francesco Randi", "authors": "Francesco Randi and Andrew M. Leifer", "title": "Nonequilibrium Green's functions for functional connectivity in the\n  brain", "comments": "6 pages (incl. bibliography), 2 figures + supplement (6 pages)", "journal-ref": "Phys. Rev. Lett. 126, 118102 (2021)", "doi": "10.1103/PhysRevLett.126.118102", "report-no": null, "categories": "q-bio.NC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A theoretical framework describing the set of interactions between neurons in\nthe brain, or functional connectivity, should include dynamical functions\nrepresenting the propagation of signal from one neuron to another. Green's\nfunctions and response functions are natural candidates for this but, while\nthey are conceptually very useful, they are usually defined only for linear\ntime-translationally invariant systems. The brain, instead, behaves nonlinearly\nand in a time-dependent way. Here, we use nonequilibrium Green's functions to\ndescribe the time-dependent functional connectivity of a continuous-variable\nnetwork of neurons. We show how the connectivity is related to the measurable\nresponse functions, and provide two illustrative examples via numerical\ncalculations, inspired from $\\textit{C. elegans}$.\n", "versions": [{"version": "v1", "created": "Thu, 13 Aug 2020 20:02:45 GMT"}], "update_date": "2021-03-24", "authors_parsed": [["Randi", "Francesco", ""], ["Leifer", "Andrew M.", ""]]}, {"id": "2008.06151", "submitter": "Emanuel Azcona", "authors": "Emanuel A. Azcona, Pierre Besson, Yunan Wu, Arjun Punjabi, Adam\n  Martersteck, Amil Dravid, Todd B. Parrish, S. Kathleen Bandt, Aggelos K.\n  Katsaggelos", "title": "Interpretation of Brain Morphology in Association to Alzheimer's Disease\n  Dementia Classification Using Graph Convolutional Networks on Triangulated\n  Meshes", "comments": "Accepted for the Shape in Medical Imaging (ShapeMI) workshop at\n  MICCAI International Conference 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV cs.LG math.SP q-bio.NC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a mesh-based technique to aid in the classification of Alzheimer's\ndisease dementia (ADD) using mesh representations of the cortex and subcortical\nstructures. Deep learning methods for classification tasks that utilize\nstructural neuroimaging often require extensive learning parameters to\noptimize. Frequently, these approaches for automated medical diagnosis also\nlack visual interpretability for areas in the brain involved in making a\ndiagnosis. This work: (a) analyzes brain shape using surface information of the\ncortex and subcortical structures, (b) proposes a residual learning framework\nfor state-of-the-art graph convolutional networks which offer a significant\nreduction in learnable parameters, and (c) offers visual interpretability of\nthe network via class-specific gradient information that localizes important\nregions of interest in our inputs. With our proposed method leveraging the use\nof cortical and subcortical surface information, we outperform other machine\nlearning methods with a 96.35% testing accuracy for the ADD vs. healthy control\nproblem. We confirm the validity of our model by observing its performance in a\n25-trial Monte Carlo cross-validation. The generated visualization maps in our\nstudy show correspondences with current knowledge regarding the structural\nlocalization of pathological changes in the brain associated to dementia of the\nAlzheimer's type.\n", "versions": [{"version": "v1", "created": "Fri, 14 Aug 2020 01:10:39 GMT"}, {"version": "v2", "created": "Tue, 18 Aug 2020 23:59:56 GMT"}, {"version": "v3", "created": "Thu, 20 Aug 2020 06:58:20 GMT"}], "update_date": "2020-08-21", "authors_parsed": [["Azcona", "Emanuel A.", ""], ["Besson", "Pierre", ""], ["Wu", "Yunan", ""], ["Punjabi", "Arjun", ""], ["Martersteck", "Adam", ""], ["Dravid", "Amil", ""], ["Parrish", "Todd B.", ""], ["Bandt", "S. Kathleen", ""], ["Katsaggelos", "Aggelos K.", ""]]}, {"id": "2008.06276", "submitter": "Bart Nieuwenhuis PhD", "authors": "Tiger Cross (1), Rasika Navarange (1), Joon-Ho Son (1), William Burr\n  (1), Arjun Singh (1), Kelvin Zhang (1), Miruna Rusu (1), Konstantinos\n  Gkoutzis (1), Andrew Osborne (2) and Bart Nieuwenhuis (2 and 3) ((1)\n  Department of Computing, Imperial College London, (2) John van Geest Centre\n  for Brain Repair, Department of Clinical Neurosciences, University of\n  Cambridge, (3) Laboratory for Regeneration of Sensorimotor Systems,\n  Netherlands Institute for Neuroscience, Royal Netherlands Academy of Arts and\n  Sciences (KNAW))", "title": "Simple RGC: ImageJ plugins for counting retinal ganglion cells and\n  determining the transduction efficiency of viral vectors in retinal\n  wholemounts", "comments": "Authors: Tiger Cross, Rasika Navarange, Joon-Ho Son, William Burr,\n  Arjun Singh, Kelvin Zhang. Comment: These authors have contributed equally to\n  this work. Authors: Andrew Osborne and Bart Nieuwenhuis. Comment: These\n  authors share senior authorship and correspondence", "journal-ref": "Journal of Open Research Software, 9(1), 2021, p.15", "doi": "10.5334/jors.342", "report-no": null, "categories": "q-bio.NC", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Simple RGC consists of a collection of ImageJ plugins to assist researchers\ninvestigating retinal ganglion cell (RGC) injury models in addition to helping\nassess the effectiveness of treatments. The first plugin named RGC Counter\naccurately calculates the total number of RGCs from retinal wholemount images.\nThe second plugin named RGC Transduction measures the co-localisation between\ntwo channels making it possible to determine the transduction efficiencies of\nviral vectors and transgene expression levels. The third plugin named RGC Batch\nis a batch image processor to deliver fast analysis of large groups of\nmicroscope images. These ImageJ plugins make analysis of RGCs in retinal\nwholemounts easy, quick, consistent, and less prone to unconscious bias by the\ninvestigator. The plugins are freely available from the ImageJ update site\nhttps://sites.imagej.net/Sonjoonho/.\n", "versions": [{"version": "v1", "created": "Fri, 14 Aug 2020 10:07:06 GMT"}, {"version": "v2", "created": "Wed, 21 Apr 2021 14:20:44 GMT"}], "update_date": "2021-06-22", "authors_parsed": [["Cross", "Tiger", "", "2 and 3"], ["Navarange", "Rasika", "", "2 and 3"], ["Son", "Joon-Ho", "", "2 and 3"], ["Burr", "William", "", "2 and 3"], ["Singh", "Arjun", "", "2 and 3"], ["Zhang", "Kelvin", "", "2 and 3"], ["Rusu", "Miruna", "", "2 and 3"], ["Gkoutzis", "Konstantinos", "", "2 and 3"], ["Osborne", "Andrew", "", "2 and 3"], ["Nieuwenhuis", "Bart", "", "2 and 3"]]}, {"id": "2008.06937", "submitter": "Brian Gardner BG", "authors": "Brian Gardner, Andr\\'e Gr\\\"uning", "title": "Supervised Learning with First-to-Spike Decoding in Multilayer Spiking\n  Neural Networks", "comments": "41 pages, 13 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NE cs.LG q-bio.NC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Experimental studies support the notion of spike-based neuronal information\nprocessing in the brain, with neural circuits exhibiting a wide range of\ntemporally-based coding strategies to rapidly and efficiently represent sensory\nstimuli. Accordingly, it would be desirable to apply spike-based computation to\ntackling real-world challenges, and in particular transferring such theory to\nneuromorphic systems for low-power embedded applications. Motivated by this, we\npropose a new supervised learning method that can train multilayer spiking\nneural networks to solve classification problems based on a rapid,\nfirst-to-spike decoding strategy. The proposed learning rule supports multiple\nspikes fired by stochastic hidden neurons, and yet is stable by relying on\nfirst-spike responses generated by a deterministic output layer. In addition to\nthis, we also explore several distinct, spike-based encoding strategies in\norder to form compact representations of presented input data. We demonstrate\nthe classification performance of the learning rule as applied to several\nbenchmark datasets, including MNIST. The learning rule is capable of\ngeneralising from the data, and is successful even when used with constrained\nnetwork architectures containing few input and hidden layer neurons.\nFurthermore, we highlight a novel encoding strategy, termed `scanline\nencoding', that can transform image data into compact spatiotemporal patterns\nfor subsequent network processing. Designing constrained, but optimised,\nnetwork structures and performing input dimensionality reduction has strong\nimplications for neuromorphic applications.\n", "versions": [{"version": "v1", "created": "Sun, 16 Aug 2020 15:34:48 GMT"}], "update_date": "2020-08-18", "authors_parsed": [["Gardner", "Brian", ""], ["Gr\u00fcning", "Andr\u00e9", ""]]}, {"id": "2008.06939", "submitter": "Richard Granger", "authors": "Elijah Bowen, Antonio Rodriguez, Damian Sowinski, Richard Granger", "title": "Visual stream connectivity predicts assessments of image quality", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV q-bio.NC", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Some biological mechanisms of early vision are comparatively well understood,\nbut they have yet to be evaluated for their ability to accurately predict and\nexplain human judgments of image similarity. From well-studied simple\nconnectivity patterns in early vision, we derive a novel formalization of the\npsychophysics of similarity, showing the differential geometry that provides\naccurate and explanatory accounts of perceptual similarity judgments. These\npredictions then are further improved via simple regression on human behavioral\nreports, which in turn are used to construct more elaborate hypothesized neural\nconnectivity patterns. Both approaches outperform standard successful measures\nof perceived image fidelity from the literature, as well as providing\nexplanatory principles of similarity perception.\n", "versions": [{"version": "v1", "created": "Sun, 16 Aug 2020 15:38:17 GMT"}], "update_date": "2020-08-18", "authors_parsed": [["Bowen", "Elijah", ""], ["Rodriguez", "Antonio", ""], ["Sowinski", "Damian", ""], ["Granger", "Richard", ""]]}, {"id": "2008.06996", "submitter": "Dmitry Krotov", "authors": "Dmitry Krotov, John Hopfield", "title": "Large Associative Memory Problem in Neurobiology and Machine Learning", "comments": "Accepted for publication at ICLR 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "q-bio.NC cond-mat.dis-nn cs.CL cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Dense Associative Memories or modern Hopfield networks permit storage and\nreliable retrieval of an exponentially large (in the dimension of feature\nspace) number of memories. At the same time, their naive implementation is\nnon-biological, since it seemingly requires the existence of many-body synaptic\njunctions between the neurons. We show that these models are effective\ndescriptions of a more microscopic (written in terms of biological degrees of\nfreedom) theory that has additional (hidden) neurons and only requires two-body\ninteractions between them. For this reason our proposed microscopic theory is a\nvalid model of large associative memory with a degree of biological\nplausibility. The dynamics of our network and its reduced dimensional\nequivalent both minimize energy (Lyapunov) functions. When certain dynamical\nvariables (hidden neurons) are integrated out from our microscopic theory, one\ncan recover many of the models that were previously discussed in the\nliterature, e.g. the model presented in \"Hopfield Networks is All You Need\"\npaper. We also provide an alternative derivation of the energy function and the\nupdate rule proposed in the aforementioned paper and clarify the relationships\nbetween various models of this class.\n", "versions": [{"version": "v1", "created": "Sun, 16 Aug 2020 21:03:52 GMT"}, {"version": "v2", "created": "Tue, 2 Mar 2021 20:06:50 GMT"}, {"version": "v3", "created": "Tue, 27 Apr 2021 22:20:05 GMT"}], "update_date": "2021-04-29", "authors_parsed": [["Krotov", "Dmitry", ""], ["Hopfield", "John", ""]]}, {"id": "2008.07092", "submitter": "Mahima Chaudhary", "authors": "Mahima Chaudhary, Sumona Mukhopadhyay, Marin Litoiu, Lauren E Sergio,\n  Meaghan S Adams", "title": "Understanding Brain Dynamics for Color Perception using Wearable EEG\n  headband", "comments": "10 pages,10 figures, Conference- EVOKE CASCON 2020", "journal-ref": "Proceedings of 30th Annual International Conference on Computer\n  Science and Software Engineering 2020", "doi": null, "report-no": null, "categories": "cs.LG q-bio.NC stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The perception of color is an important cognitive feature of the human brain.\nThe variety of colors that impinge upon the human eye can trigger changes in\nbrain activity which can be captured using electroencephalography (EEG). In\nthis work, we have designed a multiclass classification model to detect the\nprimary colors from the features of raw EEG signals. In contrast to previous\nresearch, our method employs spectral power features, statistical features as\nwell as correlation features from the signal band power obtained from\ncontinuous Morlet wavelet transform instead of raw EEG, for the classification\ntask. We have applied dimensionality reduction techniques such as Forward\nFeature Selection and Stacked Autoencoders to reduce the dimension of data\neventually increasing the model's efficiency. Our proposed methodology using\nForward Selection and Random Forest Classifier gave the best overall accuracy\nof 80.6\\% for intra-subject classification. Our approach shows promise in\ndeveloping techniques for cognitive tasks using color cues such as controlling\nInternet of Thing (IoT) devices by looking at primary colors for individuals\nwith restricted motor abilities.\n", "versions": [{"version": "v1", "created": "Mon, 17 Aug 2020 05:25:16 GMT"}], "update_date": "2020-10-22", "authors_parsed": [["Chaudhary", "Mahima", ""], ["Mukhopadhyay", "Sumona", ""], ["Litoiu", "Marin", ""], ["Sergio", "Lauren E", ""], ["Adams", "Meaghan S", ""]]}, {"id": "2008.07408", "submitter": "Thomas Rood", "authors": "Thomas Rood and Marcel van Gerven and Pablo Lanillos", "title": "A deep active inference model of the rubber-hand illusion", "comments": "8 pages, 3 figures, Accepted in 1st International Workshop on Active\n  Inference, in Conjunction with European Conference of Machine Learning 2020.\n  The final authenticated publication is available online at\n  https://doi.org/10.1007/978-3-030-64919-7_10", "journal-ref": null, "doi": "10.1007/978-3-030-64919-7_10", "report-no": null, "categories": "cs.AI q-bio.NC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Understanding how perception and action deal with sensorimotor conflicts,\nsuch as the rubber-hand illusion (RHI), is essential to understand how the body\nadapts to uncertain situations. Recent results in humans have shown that the\nRHI not only produces a change in the perceived arm location, but also causes\ninvoluntary forces. Here, we describe a deep active inference agent in a\nvirtual environment, which we subjected to the RHI, that is able to account for\nthese results. We show that our model, which deals with visual high-dimensional\ninputs, produces similar perceptual and force patterns to those found in\nhumans.\n", "versions": [{"version": "v1", "created": "Mon, 17 Aug 2020 15:28:57 GMT"}, {"version": "v2", "created": "Tue, 22 Dec 2020 13:48:59 GMT"}], "update_date": "2020-12-23", "authors_parsed": [["Rood", "Thomas", ""], ["van Gerven", "Marcel", ""], ["Lanillos", "Pablo", ""]]}, {"id": "2008.07574", "submitter": "David Franklin", "authors": "Sae Franklin and David W. Franklin", "title": "Feedback Gains modulate with Motor Memory Uncertainty", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "q-bio.NC", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  A sudden change in dynamics produces large errors leading to increases in\nmuscle co-contraction and feedback gains during early adaptation. We previously\nproposed that internal model uncertainty drives these changes, whereby the\nsensorimotor system reacts to the change in dynamics by up regulating stiffness\nand feedback gains to reduce the effect of model errors. However, these\nfeedback gain increases have also been suggested to represent part of the\nadaptation mechanism. Here, we investigate this by examining changes in\nvisuomotor feedback gains during gradual or abrupt force field adaptation.\nParticipants grasped a robotic manipulandum and reached while a curl force\nfield was introduced gradually or abruptly. Abrupt introduction of dynamics\nelicited large initial increases in kinematic error, muscle co-contraction and\nvisuomotor feedback gains, while gradual introduction showed little initial\nchange in these measures despite evidence of adaptation. After adaptation had\nplateaued,there was a change in the co-contraction and visuomotor feedback\ngains relative to null field movements, but no differences (apart from the\nfinal muscle activation pattern) between the abrupt and gradual introduction of\ndynamics. This suggests that the initial increase in feedback gains is not part\nof the adaptation process, but instead an automatic reactive response to\ninternal model uncertainty. In contrast, the final level of feedback gains is a\npredictive tuning of the feedback gains to the external dynamics as part of the\ninternal model adaptation. Together, the reactive and predictive feedback gains\nexplain the wide variety of previous experimental results of feedback changes\nduring adaptation.\n", "versions": [{"version": "v1", "created": "Mon, 17 Aug 2020 18:52:38 GMT"}, {"version": "v2", "created": "Sat, 10 Apr 2021 17:25:22 GMT"}], "update_date": "2021-04-13", "authors_parsed": [["Franklin", "Sae", ""], ["Franklin", "David W.", ""]]}, {"id": "2008.07612", "submitter": "Tomislav Stankovski Ph.D.", "authors": "Tomislav Stankovski", "title": "Coupling Functions in Neuroscience", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "nlin.AO q-bio.NC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The interactions play one of the central roles in the brain mediating various\nprocesses and functions. They are particularly important for the brain as a\ncomplex system that has many different functions from the same structural\nconnectivity. When studying such neural interactions the coupling functions are\nvery suitable, as inherently they can reveal the underlaying functional\nmechanism. This chapter overviews some recent and widely used aspects of\ncoupling functions for studying neural interactions. Coupling functions are\ndiscussed in connection to two different levels of brain interactions - that of\nneuron interactions and brainwave cross-frequency interactions. Aspects\nrelevant to this from both, theory and methods, are presented. Although the\ndiscussion is based on neuroscience, there are strong implications from, and\nto, other fields as well.\n", "versions": [{"version": "v1", "created": "Mon, 17 Aug 2020 20:39:47 GMT"}], "update_date": "2020-08-19", "authors_parsed": [["Stankovski", "Tomislav", ""]]}, {"id": "2008.07981", "submitter": "Arjun Haridas Pallath", "authors": "Arjun Haridas Pallath, Martin Dyrba", "title": "Comparison of Convolutional neural network training parameters for\n  detecting Alzheimers disease and effect on visualization", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV q-bio.NC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Convolutional neural networks (CNN) have become a powerful tool for detecting\npatterns in image data. Recent papers report promising results in the domain of\ndisease detection using brain MRI data. Despite the high accuracy obtained from\nCNN models for MRI data so far, almost no papers provided information on the\nfeatures or image regions driving this accuracy as adequate methods were\nmissing or challenging to apply. Recently, the toolbox iNNvestigate has become\navailable, implementing various state of the art methods for deep learning\nvisualizations. Currently, there is a great demand for a comparison of\nvisualization algorithms to provide an overview of the practical usefulness and\ncapability of these algorithms.\n  Therefore, this thesis has two goals: 1. To systematically evaluate the\ninfluence of CNN hyper-parameters on model accuracy. 2. To compare various\nvisualization methods with respect to the quality (i.e. randomness/focus,\nsoundness).\n", "versions": [{"version": "v1", "created": "Tue, 18 Aug 2020 15:21:50 GMT"}], "update_date": "2020-08-19", "authors_parsed": [["Pallath", "Arjun Haridas", ""], ["Dyrba", "Martin", ""]]}, {"id": "2008.08073", "submitter": "Jerome Feldman", "authors": "Jerome A. Feldman (ICSI and UC Berkeley)", "title": "On the Evolution of Subjective Experience", "comments": "49 pages 5 figures. This 7/22/2021 version preserves all the content\n  of the previous version and adds additional discussion (in italics). It also\n  includes several new references to connect with current literature. A\n  companion arXiv article has also been updated", "journal-ref": null, "doi": null, "report-no": null, "categories": "q-bio.NC cs.NE q-bio.PE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Subjective Experience (SE) is part of the ancient mind-body problem, which\ncontinues to be one of deepest mysteries of science. Despite major advances in\nmany fields, there is still no plausible causal link between SE and its\nrealization in the body. The core issue is the incompatibility of objective\n(3rd person) public science with subjective (1st person) private experience.\nAny scientific approach to SE assumes that it arose from extended evolutionary\nprocesses and that examining evolutionary history should help us understand it.\nWhile the core mystery remains, converging evidence from theoretical,\nexperimental, and computational studies yields strong constraints on SE and\nsome suggestions for further research. All animals confront many of the same\nfitness challenges. They all need some kind of internal model to relate their\nlife goals and actionable sensed information to action. We understand the\nevolution of the bodily aspects of human perception and emotion, but not the\nSE. The first evolutionary evidence for SE appears in vertebrates and much of\nits neural substrate and simulation mechanism is preserved in mammals and\nhumans. People exhibit the same phenomena, but there are remaining mysteries of\neveryday experience that are demonstrably incompatible with current\nneuroscience. In spite of this limitation, there is considerable progress on\nunderstanding the role of SE in the success of prostheses.\n", "versions": [{"version": "v1", "created": "Tue, 18 Aug 2020 17:54:39 GMT"}, {"version": "v2", "created": "Fri, 23 Jul 2021 17:41:53 GMT"}], "update_date": "2021-07-26", "authors_parsed": [["Feldman", "Jerome A.", "", "ICSI and UC Berkeley"]]}, {"id": "2008.08226", "submitter": "Robert Strauss", "authors": "Robert Strauss", "title": "Augmenting Neural Differential Equations to Model Unknown Dynamical\n  Systems with Incomplete State Information", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "q-bio.NC cs.LG physics.comp-ph", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Neural Ordinary Differential Equations replace the right-hand side of a\nconventional ODE with a neural net, which by virtue of the universal\napproximation theorem, can be trained to the representation of any function.\nWhen we do not know the function itself, but have state trajectories (time\nevolution) of the ODE system we can still train the neural net to learn the\nrepresentation of the underlying but unknown ODE. However if the state of the\nsystem is incompletely known then the right-hand side of the ODE cannot be\ncalculated. The derivatives to propagate the system are unavailable. We show\nthat a specially augmented Neural ODE can learn the system when given\nincomplete state information. As a worked example we apply neural ODEs to the\nLotka-Voltera problem of 3 species, rabbits, wolves, and bears. We show that\neven when the data for the bear time series is removed the remaining time\nseries of the rabbits and wolves is sufficient to learn the dynamical system\ndespite the missing the incomplete state information. This is surprising since\na conventional ODE system cannot output the correct derivatives without the\nfull state as the input. We implement augmented neural ODEs and differential\nequation solvers in the julia programming language.\n", "versions": [{"version": "v1", "created": "Wed, 19 Aug 2020 02:21:13 GMT"}, {"version": "v2", "created": "Thu, 20 Aug 2020 00:11:16 GMT"}, {"version": "v3", "created": "Sat, 22 Aug 2020 02:59:26 GMT"}], "update_date": "2020-08-25", "authors_parsed": [["Strauss", "Robert", ""]]}, {"id": "2008.08653", "submitter": "Julia Steinberg", "authors": "Julia Steinberg, Madhu Advani, Haim Sompolinsky", "title": "A new role for circuit expansion for learning in neural networks", "comments": "13+10 pages, 13 figures", "journal-ref": "Phys. Rev. E 103, 022404 (2021)", "doi": "10.1103/PhysRevE.103.022404", "report-no": null, "categories": "cond-mat.dis-nn cond-mat.stat-mech cs.LG q-bio.NC stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Many sensory pathways in the brain rely on sparsely active populations of\nneurons downstream from the input stimuli. The biological reason for the\noccurrence of expanded structure in the brain is unclear, but may be because\nexpansion can increase the expressive power of a neural network. In this work,\nwe show that expanding a neural network can improve its generalization\nperformance even in cases in which the expanded structure is pruned after the\nlearning period. To study this setting we use a teacher-student framework where\na perceptron teacher network generates labels which are corrupted with small\namounts of noise. We then train a student network that is structurally matched\nto the teacher and can achieve optimal accuracy if given the teacher's synaptic\nweights. We find that sparse expansion of the input of a student perceptron\nnetwork both increases its capacity and improves the generalization performance\nof the network when learning a noisy rule from a teacher perceptron when these\nexpansions are pruned after learning. We find similar behavior when the\nexpanded units are stochastic and uncorrelated with the input and analyze this\nnetwork in the mean field limit. We show by solving the mean field equations\nthat the generalization error of the stochastic expanded student network\ncontinues to drop as the size of the network increases. The improvement in\ngeneralization performance occurs despite the increased complexity of the\nstudent network relative to the teacher it is trying to learn. We show that\nthis effect is closely related to the addition of slack variables in artificial\nneural networks and suggest possible implications for artificial and biological\nneural networks.\n", "versions": [{"version": "v1", "created": "Wed, 19 Aug 2020 20:00:44 GMT"}, {"version": "v2", "created": "Mon, 21 Dec 2020 21:34:27 GMT"}], "update_date": "2021-02-24", "authors_parsed": [["Steinberg", "Julia", ""], ["Advani", "Madhu", ""], ["Sompolinsky", "Haim", ""]]}, {"id": "2008.09287", "submitter": "Paulo Protachevicz", "authors": "F S Borges, P R Protachevicz, V Santos, M S Santos, E C Gabrick, K C\n  Iarosz, E L Lameu, M S Baptista, I L Caldas, A M Batista", "title": "Influence of inhibitory synapses on the criticality of excitable\n  neuronal networks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "q-bio.NC nlin.AO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this work, we study the dynamic range of a neuronal network of excitable\nneurons with excitatory and inhibitory synapses. We obtain an analytical\nexpression for the critical point as a function of the excitatory and\ninhibitory synaptic intensities. We also determine an analytical expression\nthat gives the critical point value in which the maximal dynamic range occurs.\nDepending on the mean connection degree and coupl\\-ing weights, the critical\npoints can exhibit ceasing or ceaseless dynamics. However, the dynamic range is\nequal in both cases. We observe that the external stimulus mask some effects of\nself-sustained activity (ceaseless dynamic) in the region where the dynamic\nrange is calculated. In these regions, the firing rate is the same for\nceaseless dynamics and ceasing activity. Furthermore, we verify that excitatory\nand inhibitory inputs are approximately equal for a network with a large number\nof connections, showing excitatory-inhibitory balance as reported\nexperimentally.\n", "versions": [{"version": "v1", "created": "Fri, 21 Aug 2020 03:21:16 GMT"}], "update_date": "2020-08-24", "authors_parsed": [["Borges", "F S", ""], ["Protachevicz", "P R", ""], ["Santos", "V", ""], ["Santos", "M S", ""], ["Gabrick", "E C", ""], ["Iarosz", "K C", ""], ["Lameu", "E L", ""], ["Baptista", "M S", ""], ["Caldas", "I L", ""], ["Batista", "A M", ""]]}, {"id": "2008.09535", "submitter": "Aaron Julian Gutknecht", "authors": "Aaron J. Gutknecht, Michael Wibral, Abdullah Makkeh", "title": "Bits and Pieces: Understanding Information Decomposition from Part-whole\n  Relationships and Formal Logic", "comments": "25 pages, 16 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.IT math.IT math.LO q-bio.NC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Partial information decomposition (PID) seeks to decompose the multivariate\nmutual information that a set of source variables contains about a target\nvariable into basic pieces, the so called \"atoms of information\". Each atom\ndescribes a distinct way in which the sources may contain information about the\ntarget. In this paper we show, first, that the entire theory of partial\ninformation decomposition can be derived from considerations of elementary\nparthood relationships between information contributions. This way of\napproaching the problem has the advantage of directly characterizing the atoms\nof information, instead of taking an indirect approach via the concept of\nredundancy. Secondly, we describe several intriguing links between PID and\nformal logic. In particular, we show how to define a measure of PID based on\nthe information provided by certain statements about source realizations.\nFurthermore, we show how the mathematical lattice structure underlying PID\ntheory can be translated into an isomorphic structure of logical statements\nwith a particularly simple ordering relation: logical implication. The\nconclusion to be drawn from these considerations is that there are three\nisomorphic \"worlds\" of partial information decomposition, i.e. three equivalent\nways to mathematically describe the decomposition of the information carried by\na set of sources about a target: the world of parthood relationships, the world\nof logical statements, and the world of antichains that was utilized by\nWilliams and Beer in their original exposition of PID theory. We additionally\nshow how the parthood perspective provides a systematic way to answer a type of\nquestion that has been much discussed in the PID field: whether a partial\ninformation decomposition can be uniquely determined based on concepts other\nthan redundant information.\n", "versions": [{"version": "v1", "created": "Fri, 21 Aug 2020 15:26:10 GMT"}], "update_date": "2020-08-24", "authors_parsed": [["Gutknecht", "Aaron J.", ""], ["Wibral", "Michael", ""], ["Makkeh", "Abdullah", ""]]}, {"id": "2008.09574", "submitter": "William Bialek", "authors": "William Bialek", "title": "What do we mean by the dimensionality of behavior?", "comments": "Based in part on a presentation at the Physics of Behavior Virtual\n  Workshop (30 April 2020). Videos of the lectures and discussion are available\n  at https://www.youtube.com/watch?v=xSwWAgp2VdU", "journal-ref": null, "doi": null, "report-no": null, "categories": "q-bio.NC cond-mat.stat-mech q-bio.QM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  There is growing effort in the \"physics of behavior\" that aims at complete\nquantitative characterization of animal movements under more complex,\nnaturalistic conditions. One reaction to the resulting explosion of data is the\nsearch for low dimensional structure. Here I try to define more clearly what we\nmean by the dimensionality of behavior, where observable behavior may consist\neither of continuous trajectories or sequences of discrete states. This\ndiscussion also serves to isolate situations in which the dimensionality of\nbehavior is effectively infinite. I conclude with some more general\nperspectives about the importance of quantitative phenomenology.\n", "versions": [{"version": "v1", "created": "Fri, 21 Aug 2020 16:34:11 GMT"}], "update_date": "2020-08-24", "authors_parsed": [["Bialek", "William", ""]]}, {"id": "2008.09927", "submitter": "Chang Sub Kim", "authors": "Chang Sub Kim", "title": "Bayesian mechanics of perceptual inference and motor control in the\n  brain", "comments": "28 pages, 7 figures", "journal-ref": null, "doi": "10.1007/s00422-021-00859-9", "report-no": null, "categories": "q-bio.NC", "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/", "abstract": "  The free energy principle (FEP) in the neurosciences stipulates that all\nviable agents induce and minimize informational free energy in the brain to fit\ntheir environmental niche. In this study, we continue our effort to make the\nFEP a more physically principled formalism by implementing free energy\nminimization based on the principle of least action. We build a Bayesian\nmechanics (BM) by casting the formulation reported in the earlier publication\n(Kim 2018) to considering active inference beyond passive perception. The BM is\na neural implementation of variational Bayes under the FEP in continuous time.\nThe resulting BM is provided as an effective Hamilton's equation of motion and\nsubject to the control signal arising from the brain's prediction errors at the\nproprioceptive level. To demonstrate the utility of our approach, we adopt a\nsimple agent-based model and present a concrete numerical illustration of the\nbrain performing recognition dynamics by integrating BM in neural phase space.\nFurthermore, we recapitulate the major theoretical architectures in the FEP by\ncomparing our approach with the common state-space formulations.\n", "versions": [{"version": "v1", "created": "Sat, 22 Aug 2020 23:20:37 GMT"}, {"version": "v2", "created": "Sat, 14 Nov 2020 08:15:48 GMT"}, {"version": "v3", "created": "Thu, 21 Jan 2021 23:52:45 GMT"}], "update_date": "2021-01-25", "authors_parsed": [["Kim", "Chang Sub", ""]]}, {"id": "2008.10276", "submitter": "Mohammad Mahdi Dehshibi Dr.", "authors": "Mohammad Mahdi Dehshibi and Andrew Adamatzky", "title": "Electrical activity of fungi: Spikes detection and complexity analysis", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "q-bio.NC cs.ET", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Oyster fungi \\emph{Pleurotus djamor} generate actin potential like spikes of\nelectrical potential. The trains of spikes might manifest propagation of\ngrowing mycelium in a substrate, transportation of nutrients and metabolites\nand communication processes in the mycelium network. The spiking activity of\nthe mycelium networks is highly variable compared to neural activity and\ntherefore can not be analysed by standard tools from neuroscience. We propose\noriginal techniques for detecting and classifying the spiking activity of\nfungi. Using these techniques, we analyse the information-theoretic complexity\nof the fungal electrical activity. The results can pave ways for future\nresearch on sensorial fusion and decision making of fungi.\n", "versions": [{"version": "v1", "created": "Mon, 24 Aug 2020 09:11:56 GMT"}], "update_date": "2020-08-25", "authors_parsed": [["Dehshibi", "Mohammad Mahdi", ""], ["Adamatzky", "Andrew", ""]]}, {"id": "2008.10760", "submitter": "Fernando Najman", "authors": "F. A. Najman, A. Galves, C.D. Vargas", "title": "Fingerprints of data compression in EEG sequences", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "q-bio.NC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  It has been classically conjectured that the brain compresses data by\nassigning probabilistic models to sequences of stimuli. An important issue\nassociated to this conjecture is what class of models is used by the brain to\nperform its compression task. We address this issue by introducing a new\nstatistical model selection procedure aiming to study the manner by which the\nbrain performs data compression. Our procedure uses context tree models to\nrepresent sequences of stimuli and a new projective method for clustering EEG\nsegments. The starting point is an experimental protocol in which EEG data is\nrecorded while a participant is exposed to auditory stimuli generated by a\nstochastic chain. A simulation study using sequences of stimuli generated by\ntwo different context tree models with EEG segments generated by two distinct\nalgorithms concludes this article.\n", "versions": [{"version": "v1", "created": "Tue, 25 Aug 2020 00:30:21 GMT"}], "update_date": "2020-08-26", "authors_parsed": [["Najman", "F. A.", ""], ["Galves", "A.", ""], ["Vargas", "C. D.", ""]]}, {"id": "2008.11167", "submitter": "Fernanda Matias", "authors": "Francisco-Leandro P. Carlos, Maciel-Monteiro Ubirakitan, Marcelo\n  Cairr\\~ao Ara\\'ujo Rodrigues, Mois\\'es Aguilar-Domingo, Eva\n  Herrera-Guti\\'errez, Jes\\'us G\\'omez-Amor, Mauro Copelli, Pedro V. Carelli,\n  Fernanda S. Matias", "title": "Anticipated synchronization in human EEG data: unidirectional causality\n  with negative phase-lag", "comments": null, "journal-ref": null, "doi": "10.1103/PhysRevE.102.032216", "report-no": null, "categories": "q-bio.NC physics.bio-ph", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Understanding the functional connectivity of the brain has become a major\ngoal of neuroscience. In many situatons, the relative phase difference,\ntogether with coherence patterns, have been employed to infer the direction of\nthe information flow. However, it has been recently shown in local field\npotential data from monkeys the existence of a synchronized regime in which\nunidirectionally coupled areas can present both positive and negative phase\ndifferences. During the counterintuitive regime, called anticipated\nsynchronization (AS), the phase difference does not reflect the causality. Here\nwe investigate coherence and causality at the alpha frequency band (10 Hz)\nbetween pairs of electroencephalogram (EEG) electrodes in humans during a\nGO/NO-GO task. We show that human EEG signals can exhibit anticipated\nsynchronization, which is characterized by a unidirectional influence from an\nelectrode A to an electrode B, but the electrode B leads the electrode A in\ntime. To the best of our knowledge, this is the first verification of AS in EEG\nsignals and in the human brain. The usual delayed synchronization (DS) regime\nis also present between many pairs. DS is characterized by a unidirectional\ninfluence from an electrode A to an electrode B and a positive phase difference\nbetween A and B which indicates that the electrode A leads the electrode B in\ntime. Moreover, we show that EEG signals exhibit diversity in phase relations:\nthe pairs of electrodes can present in-phase, anti-phase, or out-of-phase\nsynchronization with a similar distribution of positive and negative phase\ndifferences.\n", "versions": [{"version": "v1", "created": "Tue, 25 Aug 2020 16:59:54 GMT"}], "update_date": "2020-10-28", "authors_parsed": [["Carlos", "Francisco-Leandro P.", ""], ["Ubirakitan", "Maciel-Monteiro", ""], ["Rodrigues", "Marcelo Cairr\u00e3o Ara\u00fajo", ""], ["Aguilar-Domingo", "Mois\u00e9s", ""], ["Herrera-Guti\u00e9rrez", "Eva", ""], ["G\u00f3mez-Amor", "Jes\u00fas", ""], ["Copelli", "Mauro", ""], ["Carelli", "Pedro V.", ""], ["Matias", "Fernanda S.", ""]]}, {"id": "2008.11241", "submitter": "Jean-Julien Aucouturier", "authors": "Marco Liuni, Luc Ardaillon, Louise Bonal, Lou Seropian, Jean-Julien\n  Aucouturier", "title": "ANGUS: Real-time manipulation of vocal roughness for emotional speech\n  transformations", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.SD q-bio.NC", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Vocal arousal, the non-linear acoustic features taken on by human and animal\nvocalizations when highly aroused, has an important communicative function\nbecause it signals aversive states such as fear, pain or distress. In this\nwork, we present a computationally-efficient, real-time voice transformation\nalgorithm, ANGUS, which uses amplitude modulation and time-domain filtering to\nsimulate roughness, an important component of vocal arousal, in arbitrary voice\nrecordings. In a series of 4 studies, we show that ANGUS allows parametric\ncontrol over the spectral features of roughness like the presence of\nsub-harmonics and noise; that ANGUS increases the emotional negativity\nperceived by listeners, to a comparable level as a non-real-time\nanalysis/resynthesis algorithm from the state-of-the-art; that listeners cannot\ndistinguish transformed and non-transformed sounds above chance level; and that\nANGUS has a similar emotional effect on animal vocalizations and musical\ninstrument sounds than on human vocalizations. A real-time implementation of\nANGUS is made available as open-source software, for use in experimental\nemotion reseach and affective computing.\n", "versions": [{"version": "v1", "created": "Tue, 25 Aug 2020 19:06:03 GMT"}], "update_date": "2020-08-27", "authors_parsed": [["Liuni", "Marco", ""], ["Ardaillon", "Luc", ""], ["Bonal", "Louise", ""], ["Seropian", "Lou", ""], ["Aucouturier", "Jean-Julien", ""]]}, {"id": "2008.11491", "submitter": "Sam Blakeman", "authors": "Sam Blakeman, Denis Mareschal", "title": "Selective Particle Attention: Visual Feature-Based Attention in Deep\n  Reinforcement Learning", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "q-bio.NC cs.CV cs.LG cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The human brain uses selective attention to filter perceptual input so that\nonly the components that are useful for behaviour are processed using its\nlimited computational resources. We focus on one particular form of visual\nattention known as feature-based attention, which is concerned with identifying\nfeatures of the visual input that are important for the current task regardless\nof their spatial location. Visual feature-based attention has been proposed to\nimprove the efficiency of Reinforcement Learning (RL) by reducing the\ndimensionality of state representations and guiding learning towards relevant\nfeatures. Despite achieving human level performance in complex perceptual-motor\ntasks, Deep RL algorithms have been consistently criticised for their poor\nefficiency and lack of flexibility. Visual feature-based attention therefore\nrepresents one option for addressing these criticisms. Nevertheless, it is\nstill an open question how the brain is able to learn which features to attend\nto during RL. To help answer this question we propose a novel algorithm, termed\nSelective Particle Attention (SPA), which imbues a Deep RL agent with the\nability to perform selective feature-based attention. SPA learns which\ncombinations of features to attend to based on their bottom-up saliency and how\naccurately they predict future reward. We evaluate SPA on a multiple choice\ntask and a 2D video game that both involve raw pixel input and dynamic changes\nto the task structure. We show various benefits of SPA over approaches that\nnaively attend to either all or random subsets of features. Our results\ndemonstrate (1) how visual feature-based attention in Deep RL models can\nimprove their learning efficiency and ability to deal with sudden changes in\ntask structure and (2) that particle filters may represent a viable\ncomputational account of how visual feature-based attention occurs in the\nbrain.\n", "versions": [{"version": "v1", "created": "Wed, 26 Aug 2020 11:07:50 GMT"}], "update_date": "2020-08-31", "authors_parsed": [["Blakeman", "Sam", ""], ["Mareschal", "Denis", ""]]}, {"id": "2008.11674", "submitter": "Xiaowen Chen", "authors": "Xiaowen Chen and William Bialek", "title": "Searching for long time scales without fine tuning", "comments": "13 pages, 5 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "physics.bio-ph nlin.AO q-bio.NC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Most of animal and human behavior occurs on time scales much longer than the\nresponse times of individual neurons. In many cases it is plausible that these\nlong time scales emerge from the recurrent dynamics of electrical activity in\nnetworks of neurons. In linear models, time scales are set by the eigenvalues\nof a dynamical matrix whose elements measure the strengths of synaptic\nconnections between neurons. It is not clear to what extent these matrix\nelements need to be tuned in order to generate long time scales; in some cases,\none needs not just a single long time scale but a whole range. Starting from\nthe simplest case of random symmetric connections, we combine maximum entropy\nand random matrix theory methods to construct ensembles of networks, exploring\nthe constraints required for long time scales to become generic. We argue that\na single long time scale can emerge generically from realistic constraints, but\na full spectrum of slow modes requires more tuning. Langevin dynamics that will\ngenerate patterns of synaptic connections drawn from these ensembles involve a\ncombination of Hebbian learning and activity-dependent synaptic scaling.\n", "versions": [{"version": "v1", "created": "Wed, 26 Aug 2020 16:57:48 GMT"}], "update_date": "2020-08-27", "authors_parsed": [["Chen", "Xiaowen", ""], ["Bialek", "William", ""]]}, {"id": "2008.11891", "submitter": "Fernanda Matias", "authors": "Julio Nunes Machado and Fernanda Selingardi Matias", "title": "Phase-bistability between anticipated and delayed synchronization in\n  neuronal populations", "comments": null, "journal-ref": "Phys. Rev. E 102, 032412 (2020)", "doi": "10.1103/PhysRevE.102.032412", "report-no": null, "categories": "q-bio.NC physics.bio-ph", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Two dynamical systems unidirectionally coupled in a sender-receiver\nconfiguration can synchronize with a nonzero phase-lag. In particular, the\nsystem can exhibit anticipated synchronization (AS), which is characterized by\na negative phase-lag, if the receiver (R) also receives a delayed negative\nself-feedback. Recently, AS was shown to occur between cortical-like neuronal\npopulations in which the self-feedback is mediated by inhibitory synapses. In\nthis biologically plausible scenario, a transition from the usual delayed\nsynchronization (DS, with positive phase-lag) to AS can be mediated by the\ninhibitory conductances in the receiver population. Here we show that depending\non the relation between excitatory and inhibitory synaptic conductances the\nsystem can also exhibit phase-bistability between anticipated and delayed\nsynchronization. Furthermore, we show that the amount of noise at the receiver\nand the synaptic conductances can mediate the transition from stable\nphase-locking to a bistable regime and eventually to a phase-drift (PD). We\nsuggest that our spiking neuronal populations model could be potentially useful\nto study phase-bistability in cortical regions related to bistable perception.\n", "versions": [{"version": "v1", "created": "Thu, 27 Aug 2020 02:34:25 GMT"}], "update_date": "2020-09-30", "authors_parsed": [["Machado", "Julio Nunes", ""], ["Matias", "Fernanda Selingardi", ""]]}, {"id": "2008.12219", "submitter": "Sergei Nechaev", "authors": "O.V. Valba, A.S. Gorsky, S.K. Nechaev, and M.V. Tamm", "title": "Analysis of English free association network reveals mechanisms of\n  efficient solution of Remote Association Tests", "comments": "16 pages, 4 figures, 3 tables (article is substantially revised)", "journal-ref": null, "doi": "10.1371/journal.pone.0248986", "report-no": null, "categories": "cs.AI q-bio.NC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study correlations between the structure and properties of a free\nassociation network of the English language, and solutions of psycholinguistic\nRemote Association Tests (RATs). We show that average hardness of individual\nRATs is largely determined by relative positions of test words (stimuli and\nresponse) on the free association network. We argue that the solution of RATs\ncan be interpreted as a first passage search problem on a network whose\nvertices are words and links are associations between words. We propose\ndifferent heuristic search algorithms and demonstrate that in \"easily-solving\"\nRATs (those that are solved in 15 seconds by more than 64\\% subjects) the\nsolution is governed by \"strong\" network links (i.e. strong associations)\ndirectly connecting stimuli and response, and thus the efficient strategy\nconsist in activating such strong links. In turn, the most efficient mechanism\nof solving medium and hard RATs consists of preferentially following sequence\nof \"moderately weak\" associations.\n", "versions": [{"version": "v1", "created": "Thu, 27 Aug 2020 16:17:38 GMT"}, {"version": "v2", "created": "Thu, 22 Oct 2020 17:30:00 GMT"}], "update_date": "2021-06-09", "authors_parsed": [["Valba", "O. V.", ""], ["Gorsky", "A. S.", ""], ["Nechaev", "S. K.", ""], ["Tamm", "M. V.", ""]]}, {"id": "2008.12568", "submitter": "Pedro Mediano", "authors": "Fernando E. Rosas, Pedro A.M. Mediano, Martin Biehl, Shamil Chandaria,\n  Daniel Polani", "title": "Causal blankets: Theory and algorithmic framework", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "nlin.AO cs.AI q-bio.NC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We introduce a novel framework to identify perception-action loops (PALOs)\ndirectly from data based on the principles of computational mechanics. Our\napproach is based on the notion of causal blanket, which captures sensory and\nactive variables as dynamical sufficient statistics -- i.e. as the \"differences\nthat make a difference.\" Moreover, our theory provides a broadly applicable\nprocedure to construct PALOs that requires neither a steady-state nor Markovian\ndynamics. Using our theory, we show that every bipartite stochastic process has\na causal blanket, but the extent to which this leads to an effective PALO\nformulation varies depending on the integrated information of the bipartition.\n", "versions": [{"version": "v1", "created": "Fri, 28 Aug 2020 10:26:17 GMT"}, {"version": "v2", "created": "Tue, 29 Sep 2020 10:11:26 GMT"}], "update_date": "2020-09-30", "authors_parsed": [["Rosas", "Fernando E.", ""], ["Mediano", "Pedro A. M.", ""], ["Biehl", "Martin", ""], ["Chandaria", "Shamil", ""], ["Polani", "Daniel", ""]]}, {"id": "2008.13118", "submitter": "Islem Rekik", "authors": "Mert Lostar and Islem Rekik", "title": "Deep Hypergraph U-Net for Brain Graph Embedding and Classification", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "q-bio.NC cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  -Background. Network neuroscience examines the brain as a complex system\nrepresented by a network (or connectome), providing deeper insights into the\nbrain morphology and function, allowing the identification of atypical brain\nconnectivity alterations, which can be used as diagnostic markers of\nneurological disorders. -Existing Methods. Graph embedding methods which map\ndata samples (e.g., brain networks) into a low dimensional space have been\nwidely used to explore the relationship between samples for classification or\nprediction tasks. However, the majority of these works are based on modeling\nthe pair-wise relationships between samples, failing to capture their\nhigher-order relationships. -New Method. In this paper, inspired by the nascent\nfield of geometric deep learning, we propose Hypergraph U-Net (HUNet), a novel\ndata embedding framework leveraging the hypergraph structure to learn\nlow-dimensional embeddings of data samples while capturing their high-order\nrelationships. Specifically, we generalize the U-Net architecture, naturally\noperating on graphs, to hypergraphs by improving local feature aggregation and\npreserving the high-order relationships present in the data. -Results. We\ntested our method on small-scale and large-scale heterogeneous brain\nconnectomic datasets including morphological and functional brain networks of\nautistic and demented patients, respectively. -Conclusion. Our HUNet\noutperformed state-of-the-art geometric graph and hypergraph data embedding\ntechniques with a gain of 4-14% in classification accuracy, demonstrating both\nscalability and generalizability. HUNet code is available at\nhttps://github.com/basiralab/HUNet.\n", "versions": [{"version": "v1", "created": "Sun, 30 Aug 2020 08:15:18 GMT"}], "update_date": "2020-09-01", "authors_parsed": [["Lostar", "Mert", ""], ["Rekik", "Islem", ""]]}, {"id": "2008.13192", "submitter": "Katherine Johnston", "authors": "Katherine Johnston, Anne Shiu, and Clare Spinner", "title": "Neural Codes With Three Maximal Codewords: Convexity and Minimal\n  Embedding Dimension", "comments": "8 pages, 6 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.CO q-bio.NC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Neural codes, represented as collections of binary strings called codewords,\nare used to encode neural activity. A code is called convex if its codewords\nare represented as an arrangement of convex open sets in Euclidean space.\nPrevious work has focused on addressing the question: how can we tell when a\nneural code is convex? Giusti and Itskov identified a local obstruction and\nproved that convex neural codes have no local obstructions. The converse is\ntrue for codes on up to four neurons, but false in general. Nevertheless, we\nprove this converse holds for codes with up to three maximal codewords, and\nmoreover the minimal embedding dimension of such codes is at most two.\n", "versions": [{"version": "v1", "created": "Sun, 30 Aug 2020 15:16:54 GMT"}], "update_date": "2020-09-01", "authors_parsed": [["Johnston", "Katherine", ""], ["Shiu", "Anne", ""], ["Spinner", "Clare", ""]]}, {"id": "2008.13211", "submitter": "Edwin Dalmaijer", "authors": "Edwin S. Dalmaijer, Thomas Armstrong", "title": "The human behavioural immune system is a product of cultural evolution", "comments": "21 pages, 4 figures, 2 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "q-bio.PE q-bio.NC", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  To avoid disease, humans show far greater contamination sensitivity and\nhygienic behaviour compared to our closest living relatives, likely due to our\nincreased propensity to experience disgust. While contemporary theories argue\ndisgust is a genetic adaptation, there is surprisingly little evidence to\nsupport this claim. Here, we simulated 100 000 years of evolution in human\nhunter-gatherers to test a wide variety of theoretical models. Our results\nindicate that natural selection for monogenic or polygenic pathogen-avoidance\ntraits is plausible. However, the cultural inter-generational transmission of\nsuch traits operated more quickly in realistic scenarios, and continued to work\neven when artificially constrained. In the absence of reliable empirical data,\nour computational work supports the hypothesis that cultural evolution outpaced\nits biological counterpart to select health-improving behaviours that benefited\nsurvival. This study serves not only as evidence of cultural evolution of the\nbehavioural immune system, but is also an illustration of emerging theories\nthat paint cognitive mechanisms as socially transmitted rather than\nbiologically hardwired functions.\n", "versions": [{"version": "v1", "created": "Sun, 30 Aug 2020 16:26:42 GMT"}, {"version": "v2", "created": "Sun, 6 Sep 2020 09:44:30 GMT"}], "update_date": "2020-09-08", "authors_parsed": [["Dalmaijer", "Edwin S.", ""], ["Armstrong", "Thomas", ""]]}, {"id": "2008.13229", "submitter": "Ernest Greene", "authors": "Ernest Greene", "title": "An evolutionary perspective on the design of neuromorphic shape filters", "comments": null, "journal-ref": "IEEE Access, 2020, 8, 114228-114238", "doi": "10.1109/ACCESS.2020_3004412", "report-no": null, "categories": "q-bio.NC cs.AI cs.CV cs.NE eess.IV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  A substantial amount of time and energy has been invested to develop machine\nvision using connectionist (neural network) principles. Most of that work has\nbeen inspired by theories advanced by neuroscientists and behaviorists for how\ncortical systems store stimulus information. Those theories call for\ninformation flow through connections among several neuron populations, with the\ninitial connections being random (or at least non-functional). Then the\nstrength or location of connections are modified through training trials to\nachieve an effective output, such as the ability to identify an object. Those\ntheories ignored the fact that animals that have no cortex, e.g., fish, can\ndemonstrate visual skills that outpace the best neural network models. Neural\ncircuits that allow for immediate effective vision and quick learning have been\npreprogrammed by hundreds of millions of years of evolution and the visual\nskills are available shortly after hatching. Cortical systems may be providing\nadvanced image processing, but most likely are using design principles that had\nbeen proven effective in simpler systems. The present article provides a brief\noverview of retinal and cortical mechanisms for registering shape information,\nwith the hope that it might contribute to the design of shape-encoding circuits\nthat more closely match the mechanisms of biological vision.\n", "versions": [{"version": "v1", "created": "Sun, 30 Aug 2020 17:53:44 GMT"}], "update_date": "2020-09-01", "authors_parsed": [["Greene", "Ernest", ""]]}, {"id": "2008.13273", "submitter": "Vince Grolmusz", "authors": "Balint Varga and Vince Grolmusz", "title": "The braingraph.org Database with more than 1000 Robust Human Structural\n  Connectomes in Five Resolutions", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "q-bio.NC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The human brain is the most complex object of study we encounter today.\nMapping the neuronal-level connections between the more than 80 billion neurons\nin the brain is a hopeless task for science. By the recent advancement of\nmagnetic resonance imaging (MRI), we are able to map the macroscopic\nconnections between about 1000 brain areas. The MRI data acquisition and the\nsubsequent algorithmic workflow contain several complex steps, where errors can\noccur. In the present contribution, we describe and publish 1064 human\nconnectomes, computed from the public release of the Human Connectome Project.\nEach connectome is available in 5 resolutions, with 83, 129, 234, 463, and 1015\nanatomically labeled nodes. For error correction, we follow an averaging and\nextreme value deleting strategy for each edge and for each connectome. The\nresulting 5320 braingraphs can be downloaded from the\n\\url{https://braingraph.org} site. This dataset makes possible the access to\nthese graphs for scientists unfamiliar with neuroimaging- and\nconnectome-related tools: mathematicians, physicists, and engineers can use\ntheir expertize and ideas in the analysis of the connections of the human\nbrain. Brain scientists also have a robust and large, multi-resolution set for\nconnectomical studies.\n", "versions": [{"version": "v1", "created": "Sun, 30 Aug 2020 20:55:53 GMT"}], "update_date": "2020-09-01", "authors_parsed": [["Varga", "Balint", ""], ["Grolmusz", "Vince", ""]]}, {"id": "2008.13483", "submitter": "Matej Hoffmann", "authors": "Filipe Gama and Maksym Shcherban and Matthias Rolf and Matej Hoffmann", "title": "Active exploration for body model learning through self-touch on a\n  humanoid robot with artificial skin", "comments": "8 pages, 10 figures; Joint IEEE 10th International Conference on\n  Development and Learning and Epigenetic Robotics (ICDL-EpiRob)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.RO q-bio.NC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The mechanisms of infant development are far from understood. Learning about\none's own body is likely a foundation for subsequent development. Here we look\nspecifically at the problem of how spontaneous touches to the body in early\ninfancy may give rise to first body models and bootstrap further development\nsuch as reaching competence. Unlike visually elicited reaching, reaching to own\nbody requires connections of the tactile and motor space only, bypassing\nvision. Still, the problems of high dimensionality and redundancy of the motor\nsystem persist. In this work, we present an embodied computational model on a\nsimulated humanoid robot with artificial sensitive skin on large areas of its\nbody. The robot should autonomously develop the capacity to reach for every\ntactile sensor on its body. To do this efficiently, we employ the computational\nframework of intrinsic motivations and variants of goal babbling, as opposed to\nmotor babbling, that prove to make the exploration process faster and alleviate\nthe ill-posedness of learning inverse kinematics. Based on our results, we\ndiscuss the next steps in relation to infant studies: what information will be\nnecessary to further ground this computational model in behavioral data.\n", "versions": [{"version": "v1", "created": "Mon, 31 Aug 2020 10:54:39 GMT"}], "update_date": "2020-09-01", "authors_parsed": [["Gama", "Filipe", ""], ["Shcherban", "Maksym", ""], ["Rolf", "Matthias", ""], ["Hoffmann", "Matej", ""]]}, {"id": "2008.13625", "submitter": "Milena \\v{C}uki\\'c Radenkovi\\'c Dr", "authors": "Milena Cukic, Slavoljub Radenkovic, Miodrag Stokic, and Danka Savic", "title": "Transfer entropy applied on EEG in depression reveals aberrated dynamics", "comments": "23 pages, 1 figure", "journal-ref": null, "doi": null, "report-no": null, "categories": "q-bio.NC cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We applied transfer entropy analysis on samples of electroencephalogram\nrecorded from patients diagnosed with major depressive disorder and matched\nhealthy controls. This is the first graphical representation of aberrated\ndynamics in terms of connectivity and the direction of information between\nstandard centers in MDD.\n", "versions": [{"version": "v1", "created": "Mon, 31 Aug 2020 14:07:20 GMT"}], "update_date": "2020-09-01", "authors_parsed": [["Cukic", "Milena", ""], ["Radenkovic", "Slavoljub", ""], ["Stokic", "Miodrag", ""], ["Savic", "Danka", ""]]}]