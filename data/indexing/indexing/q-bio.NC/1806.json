[{"id": "1806.00080", "submitter": "Mason A. Porter", "authors": "Ryan Flanagan, Lucas Lacasa, Emma K. Towlson, Sang Hoon Lee, and Mason\n  A. Porter", "title": "Effect of antipsychotics on community structure in functional brain\n  networks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "physics.med-ph nlin.AO physics.soc-ph q-bio.NC stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Schizophrenia, a mental disorder that is characterized by abnormal social\nbehavior and failure to distinguish one's own thoughts and ideas from reality,\nhas been associated with structural abnormalities in the architecture of\nfunctional brain networks. Using various methods from network analysis, we\nexamine the effect of two classical therapeutic antipsychotics --- Aripiprazole\nand Sulpiride --- on the structure of functional brain networks of healthy\ncontrols and patients who have been diagnosed with schizophrenia. We compare\nthe community structures of functional brain networks of different individuals\nusing mesoscopic response functions, which measure how community structure\nchanges across different scales of a network. We are able to do a reasonably\ngood job of distinguishing patients from controls, and we are most successful\nat this task on people who have been treated with Aripiprazole. We demonstrate\nthat this increased separation between patients and controls is related only to\na change in the control group, as the functional brain networks of the patient\ngroup appear to be predominantly unaffected by this drug. This suggests that\nAripiprazole has a significant and measurable effect on community structure in\nhealthy individuals but not in individuals who are diagnosed with\nschizophrenia. In contrast, we find for individuals are given the drug\nSulpiride that it is more difficult to separate the networks of patients from\nthose of controls. Overall, we observe differences in the effects of the drugs\n(and a placebo) on community structure in patients and controls and also that\nthis effect differs across groups. We thereby demonstrate that different types\nof antipsychotic drugs selectively affect mesoscale structures of brain\nnetworks, providing support that mesoscale structures such as communities are\nmeaningful functional units in the brain.\n", "versions": [{"version": "v1", "created": "Thu, 31 May 2018 20:24:19 GMT"}, {"version": "v2", "created": "Mon, 4 Jun 2018 01:12:44 GMT"}], "update_date": "2018-06-06", "authors_parsed": [["Flanagan", "Ryan", ""], ["Lacasa", "Lucas", ""], ["Towlson", "Emma K.", ""], ["Lee", "Sang Hoon", ""], ["Porter", "Mason A.", ""]]}, {"id": "1806.00111", "submitter": "Jonathan Vacher", "authors": "Jonathan Vacher, Pascal Mamassian, Ruben Coen-Cagli", "title": "Probabilistic Model of Visual Segmentation", "comments": "13 pages, 5 figures, 1 table", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV q-bio.NC", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Visual segmentation is a key perceptual function that partitions visual space\nand allows for detection, recognition and discrimination of objects in complex\nenvironments. The processes underlying human segmentation of natural images are\nstill poorly understood. In part, this is because we lack segmentation models\nconsistent with experimental and theoretical knowledge in visual neuroscience.\nBiological sensory systems have been shown to approximate probabilistic\ninference to interpret their inputs. This requires a generative model that\ncaptures both the statistics of the sensory inputs and expectations about the\ncauses of those inputs. Following this hypothesis, we propose a probabilistic\ngenerative model of visual segmentation that combines knowledge about 1) the\nsensitivity of neurons in the visual cortex to statistical regularities in\nnatural images; and 2) the preference of humans to form contiguous partitions\nof visual space. We develop an efficient algorithm for training and inference\nbased on expectation-maximization and validate it on synthetic data.\nImportantly, with the appropriate choice of the prior, we derive an intuitive\nclosed--form update rule for assigning pixels to segments: at each iteration,\nthe pixel assignment probabilities to segments is the sum of the evidence (i.e.\nlocal pixel statistics) and prior (i.e. the assignments of neighboring pixels)\nweighted by their relative uncertainty. The model performs competitively on\nnatural images from the Berkeley Segmentation Dataset (BSD), and we illustrate\nhow the likelihood and prior components improve segmentation relative to\ntraditional mixture models. Furthermore, our model explains some variability\nacross human subjects as reflecting local uncertainty about the number of\nsegments. Our model thus provides a viable approach to probe human visual\nsegmentation.\n", "versions": [{"version": "v1", "created": "Thu, 31 May 2018 21:48:43 GMT"}, {"version": "v2", "created": "Mon, 4 Jun 2018 16:09:11 GMT"}, {"version": "v3", "created": "Wed, 1 May 2019 18:54:39 GMT"}], "update_date": "2019-05-03", "authors_parsed": [["Vacher", "Jonathan", ""], ["Mamassian", "Pascal", ""], ["Coen-Cagli", "Ruben", ""]]}, {"id": "1806.00128", "submitter": "Emma Towlson", "authors": "Emma K. Towlson, Petra E. V\\'ertes, Ulrich M\\\"uller, and Sebastian E.\n  Ahnert", "title": "Brain networks reveal the effects of antipsychotic drugs on\n  schizophrenia patients and controls", "comments": "18 pages, 2 figures, 3 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "q-bio.NC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The study of brain networks, including derived from functional neuroimaging\ndata, attracts broad interest and represents a rapidly growing\ninterdisciplinary field. Comparing networks of healthy volunteers with those of\npatients can potentially offer new, quantitative diagnostic methods, and a\nframework for better understanding brain and mind disorders. We explore resting\nstate fMRI data through network measures, and demonstrate that not only is\nthere a distinctive network architecture in the healthy brain that is disrupted\nin schizophrenia, but also that both networks respond to medication. We\nconstruct networks representing 15 healthy individuals and 12 schizophrenia\npatients (males and females), all of whom are administered three drug\ntreatments: (i) a placebo; and two antipsychotic medications (ii) aripiprazole\nand; (iii) sulpiride. We first reproduce the established finding that brain\nnetworks of schizophrenia patients exhibit increased efficiency and reduced\nclustering compared to controls. Our data then reveals that the antipsychotic\nmedications mitigate this effect, shifting the metrics towards those observed\nin healthy volunteers, with a marked difference in efficacy between the two\ndrugs. Additionally, we find that aripiprazole considerably alters the network\nstatistics of healthy controls. Using a test of cognitive ability, we establish\nthat aripiprazole also adversely affects their performance. This provides\nevidence that changes to macroscopic brain network architecture result in\nmeasurable behavioural differences. This is the first time different\nmedications have been assessed in this way. Our results lay the groundwork for\nan objective methodology with which to calculate and compare the efficacy of\ndifferent treatments of mind and brain disorders.\n", "versions": [{"version": "v1", "created": "Thu, 31 May 2018 23:05:04 GMT"}, {"version": "v2", "created": "Mon, 4 Jun 2018 00:58:10 GMT"}], "update_date": "2018-06-05", "authors_parsed": [["Towlson", "Emma K.", ""], ["V\u00e9rtes", "Petra E.", ""], ["M\u00fcller", "Ulrich", ""], ["Ahnert", "Sebastian E.", ""]]}, {"id": "1806.00546", "submitter": "Yuankai Huo", "authors": "Yuankai Huo, Zhoubing Xu, Katherine Aboud, Prasanna Parvathaneni,\n  Shunxing Bao, Camilo Bermudez, Susan M. Resnick, Laurie E. Cutting, Bennett\n  A. Landman", "title": "Spatially Localized Atlas Network Tiles Enables 3D Whole Brain\n  Segmentation from Limited Data", "comments": "To appear in MICCAI2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV q-bio.NC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Whole brain segmentation on a structural magnetic resonance imaging (MRI) is\nessential in non-invasive investigation for neuroanatomy. Historically,\nmulti-atlas segmentation (MAS) has been regarded as the de facto standard\nmethod for whole brain segmentation. Recently, deep neural network approaches\nhave been applied to whole brain segmentation by learning random patches or 2D\nslices. Yet, few previous efforts have been made on detailed whole brain\nsegmentation using 3D networks due to the following challenges: (1) fitting\nentire whole brain volume into 3D networks is restricted by the current GPU\nmemory, and (2) the large number of targeting labels (e.g., > 100 labels) with\nlimited number of training 3D volumes (e.g., < 50 scans). In this paper, we\npropose the spatially localized atlas network tiles (SLANT) method to\ndistribute multiple independent 3D fully convolutional networks to cover\noverlapped sub-spaces in a standard atlas space. This strategy simplifies the\nwhole brain learning task to localized sub-tasks, which was enabled by combing\ncanonical registration and label fusion techniques with deep learning. To\naddress the second challenge, auxiliary labels on 5111 initially unlabeled\nscans were created by MAS for pre-training. From empirical validation, the\nstate-of-the-art MAS method achieved mean Dice value of 0.76, 0.71, and 0.68,\nwhile the proposed method achieved 0.78, 0.73, and 0.71 on three validation\ncohorts. Moreover, the computational time reduced from > 30 hours using MAS to\n~15 minutes using the proposed method. The source code is available online\nhttps://github.com/MASILab/SLANTbrainSeg\n", "versions": [{"version": "v1", "created": "Fri, 1 Jun 2018 21:39:47 GMT"}, {"version": "v2", "created": "Tue, 5 Jun 2018 04:49:05 GMT"}], "update_date": "2018-06-06", "authors_parsed": [["Huo", "Yuankai", ""], ["Xu", "Zhoubing", ""], ["Aboud", "Katherine", ""], ["Parvathaneni", "Prasanna", ""], ["Bao", "Shunxing", ""], ["Bermudez", "Camilo", ""], ["Resnick", "Susan M.", ""], ["Cutting", "Laurie E.", ""], ["Landman", "Bennett A.", ""]]}, {"id": "1806.00975", "submitter": "Louis Kang", "authors": "Louis Kang, Vijay Balasubramanian", "title": "A geometric attractor mechanism for self-organization of entorhinal grid\n  modules", "comments": "Main text, Supplementary Information and Figures, Supplementary Video", "journal-ref": null, "doi": null, "report-no": null, "categories": "q-bio.NC cond-mat.dis-nn", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Grid cells in the medial entorhinal cortex (MEC) respond when an animal\noccupies a periodic lattice of \"grid fields\" in the environment. The grids are\norganized in modules with spatial periods, or scales, clustered around discrete\nvalues separated by ratios in the range 1.2--2.0. We propose a mechanism that\nproduces this modular structure through dynamical self-organization in the MEC.\nIn attractor network models of grid formation, the grid scale of a single\nmodule is set by the distance of recurrent inhibition between neurons. We show\nthat the MEC forms a hierarchy of discrete modules if a smooth increase in\ninhibition distance along its dorso-ventral axis is accompanied by excitatory\ninteractions along this axis. Moreover, constant scale ratios between\nsuccessive modules arise through geometric relationships between triangular\ngrids and have values that fall within the observed range. We discuss how\ninteractions required by our model might be tested experimentally.\n", "versions": [{"version": "v1", "created": "Mon, 4 Jun 2018 06:38:58 GMT"}, {"version": "v2", "created": "Mon, 11 Mar 2019 18:08:42 GMT"}], "update_date": "2019-03-13", "authors_parsed": [["Kang", "Louis", ""], ["Balasubramanian", "Vijay", ""]]}, {"id": "1806.01029", "submitter": "Alexander Duthie", "authors": "A. Bradley Duthie", "title": "Component response rate variation underlies the stability of highly\n  complex finite systems", "comments": "43 pages, 19 figures", "journal-ref": null, "doi": "10.1038/s41598-020-64401-w", "report-no": null, "categories": "q-bio.PE q-bio.MN q-bio.NC", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  The stability of a complex system generally decreases with increasing system\nsize and interconnectivity, a counterintuitive result of widespread importance\nacross the physical, life, and social sciences. Despite recent interest in the\nrelationship between system properties and stability, the effect of variation\nin response rate across system components remains unconsidered. Here I vary the\ncomponent response rates ($\\boldsymbol{\\gamma}$) of randomly generated complex\nsystems. I use numerical simulations to show that when component response rates\nvary, the potential for system stability increases. These results are robust to\ncommon network structures, including small-world and scale-free networks, and\ncascade food webs. Variation in $\\boldsymbol{\\gamma}$ is especially important\nfor stability in highly complex systems, in which the probability of stability\nwould otherwise be negligible. At such extremes of simulated system complexity,\nthe largest stable complex systems would be unstable if not for variation in\n$\\boldsymbol{\\gamma}$. My results therefore reveal a previously unconsidered\naspect of system stability that is likely to be pervasive across all realistic\ncomplex systems.\n", "versions": [{"version": "v1", "created": "Mon, 4 Jun 2018 09:47:26 GMT"}, {"version": "v2", "created": "Thu, 14 Jun 2018 12:51:12 GMT"}, {"version": "v3", "created": "Mon, 18 Jun 2018 16:35:45 GMT"}, {"version": "v4", "created": "Mon, 30 Jul 2018 11:23:44 GMT"}, {"version": "v5", "created": "Wed, 6 Mar 2019 23:31:34 GMT"}, {"version": "v6", "created": "Fri, 27 Mar 2020 17:59:16 GMT"}, {"version": "v7", "created": "Mon, 4 May 2020 15:13:37 GMT"}], "update_date": "2020-05-20", "authors_parsed": [["Duthie", "A. Bradley", ""]]}, {"id": "1806.01423", "submitter": "Hananel Hazan", "authors": "Hananel Hazan and Daniel J. Saunders and Hassaan Khan and Darpan T.\n  Sanghavi and Hava T. Siegelmann and Robert Kozma", "title": "BindsNET: A machine learning-oriented spiking neural networks library in\n  Python", "comments": null, "journal-ref": "Frontiers in Neuroinformatics. 12 December 2018", "doi": "10.3389/fninf.2018.00089", "report-no": null, "categories": "cs.NE q-bio.NC", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  The development of spiking neural network simulation software is a critical\ncomponent enabling the modeling of neural systems and the development of\nbiologically inspired algorithms. Existing software frameworks support a wide\nrange of neural functionality, software abstraction levels, and hardware\ndevices, yet are typically not suitable for rapid prototyping or application to\nproblems in the domain of machine learning. In this paper, we describe a new\nPython package for the simulation of spiking neural networks, specifically\ngeared towards machine learning and reinforcement learning. Our software,\ncalled BindsNET, enables rapid building and simulation of spiking networks and\nfeatures user-friendly, concise syntax. BindsNET is built on top of the PyTorch\ndeep neural networks library, enabling fast CPU and GPU computation for large\nspiking networks. The BindsNET framework can be adjusted to meet the needs of\nother existing computing and hardware environments, e.g., TensorFlow. We also\nprovide an interface into the OpenAI gym library, allowing for training and\nevaluation of spiking networks on reinforcement learning problems. We argue\nthat this package facilitates the use of spiking networks for large-scale\nmachine learning experimentation, and show some simple examples of how we\nenvision BindsNET can be used in practice. BindsNET code is available at\nhttps://github.com/Hananel-Hazan/bindsnet\n", "versions": [{"version": "v1", "created": "Mon, 4 Jun 2018 23:09:52 GMT"}, {"version": "v2", "created": "Mon, 10 Dec 2018 19:27:34 GMT"}], "update_date": "2019-03-27", "authors_parsed": [["Hazan", "Hananel", ""], ["Saunders", "Daniel J.", ""], ["Khan", "Hassaan", ""], ["Sanghavi", "Darpan T.", ""], ["Siegelmann", "Hava T.", ""], ["Kozma", "Robert", ""]]}, {"id": "1806.01597", "submitter": "Javier Buldu", "authors": "David Papo and Javier M. Buld\\'u", "title": "Brain synchronizability, a false friend", "comments": "4 pages, 1 figure", "journal-ref": null, "doi": null, "report-no": null, "categories": "q-bio.NC nlin.AO physics.bio-ph physics.soc-ph", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Synchronization plays a fundamental role in healthy cognitive and motor\nfunction. However, how synchronization depends on the interplay between local\ndynamics, coupling and topology and how prone to synchronization a network with\ngiven topological organization is are still poorly understood issues. To\ninvestigate the synchronizability of both anatomical and functional brain\nnetworks various studies resorted to the Master Stability Function (MSF)\nformalism, an elegant tool which allows analysing the stability of synchronous\nstates in a dynamical system consisting of many coupled oscillators. Here, we\nargue that brain dynamics does not fulfil the formal criteria under which\nsynchronizability is usually quantified and, perhaps more importantly, what\nthis measure itself quantifies refers to a global dynamical condition that\nnever holds in the brain (not even in the most pathological conditions), and\ntherefore no neurophysiological conclusions should be drawn based on it. We\ndiscuss the meaning of synchronizability and its applicability to neuroscience\nand propose alternative ways to quantify brain networks synchronization.\n", "versions": [{"version": "v1", "created": "Tue, 5 Jun 2018 10:22:05 GMT"}], "update_date": "2018-06-06", "authors_parsed": [["Papo", "David", ""], ["Buld\u00fa", "Javier M.", ""]]}, {"id": "1806.01661", "submitter": "Avi Loeb", "authors": "Abraham Loeb (Harvard)", "title": "Experimental Tests of Spirituality", "comments": "2 pages, accepted for publication in Scientific American", "journal-ref": null, "doi": null, "report-no": null, "categories": "physics.pop-ph astro-ph.IM cs.AI q-bio.NC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We currently harness technologies that could shed new light on old\nphilosophical questions, such as whether our mind entails anything beyond our\nbody or whether our moral values reflect universal truth.\n", "versions": [{"version": "v1", "created": "Mon, 4 Jun 2018 16:28:21 GMT"}], "update_date": "2018-06-07", "authors_parsed": [["Loeb", "Abraham", "", "Harvard"]]}, {"id": "1806.01778", "submitter": "Gianluca Calcagni", "authors": "Gianluca Calcagni, Ernesto Caballero-Garrido, Ricardo Pell\\'on", "title": "Behavior stability and individual differences in Pavlovian extended\n  conditioning", "comments": "29 pages, 8 figures, 7 tables; v2-v3: theoretical motivation\n  clarified, data of Harris et al. (2015) included in improved analysis,\n  conclusions strengthened, typos corrected, references added, technicalities\n  and data analysis moved into Supplementary Material (46 pages, 22 figures, 7\n  tables; available at\n  https://www.frontiersin.org/articles/10.3389/fpsyg.2020.00612/full#supplementary-material)", "journal-ref": "Frontiers in Psychology 11 (2020) 612", "doi": "10.3389/fpsyg.2020.00612", "report-no": null, "categories": "q-bio.NC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  How stable and general is behavior once maximum learning is reached? To\nanswer this question and understand post-acquisition behavior and its related\nindividual differences, we propose a psychological principle that naturally\nextends associative models of Pavlovian conditioning to a dynamical oscillatory\nmodel where subjects have a greater memory capacity than usually postulated,\nbut with greater forecast uncertainty. This results in a greater resistance to\nlearning in the first few sessions followed by an over-optimal response peak\nand a sequence of progressively damped response oscillations. We detected the\nfirst peak and trough of the new learning curve in our data, but their\ndispersion was too large to also check the presence of oscillations with\nsmaller amplitude. We ran an unusually long experiment with 32 rats over 3960\ntrials, where we excluded habituation and other well-known phenomena as sources\nof variability in the subjects' performance. Using the data of this and another\nPavlovian experiment by Harris et al. (2015), as an illustration of the\nprinciple we tested the theory against the basic associative single-cue\nRescorla-Wagner (RW) model. We found evidence that the RW model is the best\nnonlinear regression to data only for a minority of the subjects, while its\ndynamical extension can explain the almost totality of data with strong to very\nstrong evidence. Finally, an analysis of short-scale fluctuations of individual\nresponses showed that they are described by random white noise, in contrast\nwith the colored-noise findings in human performance.\n", "versions": [{"version": "v1", "created": "Mon, 28 May 2018 07:22:46 GMT"}, {"version": "v2", "created": "Sun, 26 Aug 2018 12:42:33 GMT"}, {"version": "v3", "created": "Wed, 22 Apr 2020 08:49:51 GMT"}], "update_date": "2020-04-23", "authors_parsed": [["Calcagni", "Gianluca", ""], ["Caballero-Garrido", "Ernesto", ""], ["Pell\u00f3n", "Ricardo", ""]]}, {"id": "1806.01823", "submitter": "Luis Sanchez Giraldo", "authors": "Luis Gonzalo Sanchez Giraldo and Odelia Schwartz", "title": "Integrating Flexible Normalization into Mid-Level Representations of\n  Deep Convolutional Neural Networks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "q-bio.NC cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Deep convolutional neural networks (CNNs) are becoming increasingly popular\nmodels to predict neural responses in visual cortex. However, contextual\neffects, which are prevalent in neural processing and in perception, are not\nexplicitly handled by current CNNs, including those used for neural prediction.\nIn primary visual cortex, neural responses are modulated by stimuli spatially\nsurrounding the classical receptive field in rich ways. These effects have been\nmodeled with divisive normalization approaches, including flexible models,\nwhere spatial normalization is recruited only to the degree responses from\ncenter and surround locations are deemed statistically dependent. We propose a\nflexible normalization model applied to mid-level representations of deep CNNs\nas a tractable way to study contextual normalization mechanisms in mid-level\ncortical areas. This approach captures non-trivial spatial dependencies among\nmid-level features in CNNs, such as those present in textures and other visual\nstimuli, that arise from tiling high order features, geometrically. We expect\nthat the proposed approach can make predictions about when spatial\nnormalization might be recruited in mid-level cortical areas. We also expect\nthis approach to be useful as part of the CNN toolkit, therefore going beyond\nmore restrictive fixed forms of normalization.\n", "versions": [{"version": "v1", "created": "Tue, 5 Jun 2018 17:26:07 GMT"}, {"version": "v2", "created": "Mon, 27 Aug 2018 14:10:35 GMT"}, {"version": "v3", "created": "Mon, 24 Dec 2018 05:29:25 GMT"}], "update_date": "2018-12-27", "authors_parsed": [["Giraldo", "Luis Gonzalo Sanchez", ""], ["Schwartz", "Odelia", ""]]}, {"id": "1806.01875", "submitter": "Kay Gregor Hartmann", "authors": "Kay Gregor Hartmann, Robin Tibor Schirrmeister, Tonio Ball", "title": "EEG-GAN: Generative adversarial networks for electroencephalograhic\n  (EEG) brain signals", "comments": "6 pages, 6 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.SP cs.LG q-bio.NC stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Generative adversarial networks (GANs) are recently highly successful in\ngenerative applications involving images and start being applied to time series\ndata. Here we describe EEG-GAN as a framework to generate\nelectroencephalographic (EEG) brain signals. We introduce a modification to the\nimproved training of Wasserstein GANs to stabilize training and investigate a\nrange of architectural choices critical for time series generation (most\nnotably up- and down-sampling). For evaluation we consider and compare\ndifferent metrics such as Inception score, Frechet inception distance and\nsliced Wasserstein distance, together showing that our EEG-GAN framework\ngenerated naturalistic EEG examples. It thus opens up a range of new generative\napplication scenarios in the neuroscientific and neurological context, such as\ndata augmentation in brain-computer interfacing tasks, EEG super-sampling, or\nrestoration of corrupted data segments. The possibility to generate signals of\na certain class and/or with specific properties may also open a new avenue for\nresearch into the underlying structure of brain signals.\n", "versions": [{"version": "v1", "created": "Tue, 5 Jun 2018 18:10:11 GMT"}], "update_date": "2018-06-07", "authors_parsed": [["Hartmann", "Kay Gregor", ""], ["Schirrmeister", "Robin Tibor", ""], ["Ball", "Tonio", ""]]}, {"id": "1806.01979", "submitter": "Andrew Song", "authors": "Andrew H. Song, Francisco Flores, Demba Ba", "title": "Spike Sorting by Convolutional Dictionary Learning", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME eess.SP q-bio.NC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Spike sorting refers to the problem of assigning action potentials observed\nin extra-cellular recordings of neural activity to the neuron(s) from which\nthey originate. We cast this problem as one of learning a convolutional\ndictionary from raw multi-electrode waveform data, subject to sparsity\nconstraints. In this context, sparsity refers to the number of neurons that are\nallowed to spike simultaneously. The convolutional dictionary setting, along\nwith its assumptions (e.g. refractoriness) that are motivated by the\nspike-sorting problem, let us give theoretical bounds on the sample complexity\nof spike sorting as a function of the number of underlying neurons, the rate of\noccurrence of simultaneous spiking, and the firing rate of the neurons. We\nderive memory/computation-efficient convolutional versions of OMP (cOMP) and\nKSVD (cKSVD), popular algorithms for sparse coding and dictionary learning\nrespectively. We demonstrate via simulations that an algorithm that alternates\nbetween cOMP and cKSVD can recover the underlying spike waveforms successfully,\nassuming few neurons spike simultaneously, and is stable in the presence of\nnoise. We also apply the algorithm to extra-cellular recordings from a tetrode\nin the rat Hippocampus.\n", "versions": [{"version": "v1", "created": "Wed, 6 Jun 2018 02:12:20 GMT"}], "update_date": "2018-06-07", "authors_parsed": [["Song", "Andrew H.", ""], ["Flores", "Francisco", ""], ["Ba", "Demba", ""]]}, {"id": "1806.02230", "submitter": "Xim Cerd\\'a-Company", "authors": "Xim Cerda-Company and Xavier Otazu", "title": "Color induction in equiluminant flashed stimuli", "comments": null, "journal-ref": null, "doi": "10.1364/JOSAA.36.000022", "report-no": null, "categories": "q-bio.NC", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Color induction is the influence of the surrounding color (inducer) on the\nperceived color of a central region. There are two different types of color\ninduction: color contrast (the color of the central region shifts away from\nthat of the inducer) and color assimilation (the color shifts towards the color\nof the inducer). Several studies on these effects used uniform and striped\nsurrounds, reporting color contrast and color assimilation, respectively. Other\nauthors (Kaneko and Murakami, J Vision, 2012) studied color induction using\nflashed uniform surrounds, reporting that the contrast was higher for shorter\nflash duration. Extending their work, we present new psychophysical results\nusing both flashed and static (i.e., non-flashed) equiluminant stimuli for both\nstriped and uniform surround. Similarly to them, for uniform surround stimuli\nwe observed color contrast, but we did not obtain the maximum contrast for the\nshortest (10 ms) flashed stimuli, but for 40 ms. We only observed this maximum\ncontrast for red, green and lime inducers, while for a purple inducer we\nobtained an asymptotic profile along flash duration. For striped stimuli, we\nobserved color assimilation only for the static (infinite flash duration)\nred-green surround inducers (red 1st inducer, green 2nd inducer). For the other\ninducers' configurations, we observed color contrast or no induction. Since\nother works showed that non-equiluminant striped static stimuli induce color\nassimilation, our results also suggest that luminance differences could be a\nkey factor to induce it.\n", "versions": [{"version": "v1", "created": "Mon, 4 Jun 2018 08:22:54 GMT"}], "update_date": "2018-12-26", "authors_parsed": [["Cerda-Company", "Xim", ""], ["Otazu", "Xavier", ""]]}, {"id": "1806.02300", "submitter": "Yuankai Huo", "authors": "Yuankai Huo, Katherine Swett, Susan M. Resnick, Laurie E. Cutting,\n  Bennett A. Landman", "title": "Data-driven Probabilistic Atlases Capture Whole-brain Individual\n  Variation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG q-bio.NC stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Probabilistic atlases provide essential spatial contextual information for\nimage interpretation, Bayesian modeling, and algorithmic processing. Such\natlases are typically constructed by grouping subjects with similar demographic\ninformation. Importantly, use of the same scanner minimizes inter-group\nvariability. However, generalizability and spatial specificity of such\napproaches is more limited than one might like. Inspired by Commowick\n\"Frankenstein's creature paradigm\" which builds a personal specific anatomical\natlas, we propose a data-driven framework to build a personal specific\nprobabilistic atlas under the large-scale data scheme. The data-driven\nframework clusters regions with similar features using a point distribution\nmodel to learn different anatomical phenotypes. Regional structural atlases and\ncorresponding regional probabilistic atlases are used as indices and targets in\nthe dictionary. By indexing the dictionary, the whole brain probabilistic\natlases adapt to each new subject quickly and can be used as spatial priors for\nvisualization and processing. The novelties of this approach are (1) it\nprovides a new perspective of generating personal specific whole brain\nprobabilistic atlases (132 regions) under data-driven scheme across sites. (2)\nThe framework employs the large amount of heterogeneous data (2349 images). (3)\nThe proposed framework achieves low computational cost since only one affine\nregistration and Pearson correlation operation are required for a new subject.\nOur method matches individual regions better with higher Dice similarity value\nwhen testing the probabilistic atlases. Importantly, the advantage the\nlarge-scale scheme is demonstrated by the better performance of using\nlarge-scale training data (1888 images) than smaller training set (720 images).\n", "versions": [{"version": "v1", "created": "Wed, 6 Jun 2018 16:53:55 GMT"}], "update_date": "2018-06-07", "authors_parsed": [["Huo", "Yuankai", ""], ["Swett", "Katherine", ""], ["Resnick", "Susan M.", ""], ["Cutting", "Laurie E.", ""], ["Landman", "Bennett A.", ""]]}, {"id": "1806.02649", "submitter": "Manuel Baltieri Mr", "authors": "Manuel Baltieri and Christopher L. Buckley", "title": "The modularity of action and perception revisited using control theory\n  and active inference", "comments": "Accepted at the International conference on Artificial Life, Tokyo,\n  2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "q-bio.NC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The assumption that action and perception can be investigated independently\nis entrenched in theories, models and experimental approaches across the brain\nand mind sciences. In cognitive science, this has been a central point of\ncontention between computationalist and 4Es (enactive, embodied, extended and\nembedded) theories of cognition, with the former embracing the \"classical\nsandwich\", modular, architecture of the mind and the latter actively denying\nthis separation can be made. In this work we suggest that the modular\nindependence of action and perception strongly resonates with the separation\nprinciple of control theory and furthermore that this principle provides formal\ncriteria within which to evaluate the implications of the modularity of action\nand perception. We will also see that real-time feedback with the environment,\noften considered necessary for the definition of 4Es ideas, is not however a\nsufficient condition to avoid the \"classical sandwich\". Finally, we argue that\nan emerging framework in the cognitive and brain sciences, active inference,\nextends ideas derived from control theory to the study of biological systems\nwhile disposing of the separation principle, describing non-modular models of\nbehaviour strongly aligned with 4Es theories of cognition.\n", "versions": [{"version": "v1", "created": "Thu, 7 Jun 2018 13:02:30 GMT"}], "update_date": "2018-06-08", "authors_parsed": [["Baltieri", "Manuel", ""], ["Buckley", "Christopher L.", ""]]}, {"id": "1806.02824", "submitter": "Samuel Chiquita", "authors": "Samuel Chiquita", "title": "A theoretical framework for retinal computations: insights from textbook\n  knowledge", "comments": null, "journal-ref": null, "doi": null, "report-no": "ICBASUP preprint 18-06", "categories": "q-bio.NC", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Neural circuits in the retina divide the incoming visual scene into more than\na dozen distinct representations that are sent on to central brain areas, such\nas the lateral geniculate nucleus and the superior colliculus. The retina can\nbe viewed as a parallel image processor made of a multitude of small\ncomputational devices. Neural circuits of the retina are constituted by various\ncell types that separate the incoming visual information in different channels.\nVisual information is processed by retinal neural circuits and several\ncomputations are performed extracting distinct features from the visual scene.\nThe aim of this article is to understand the computational basis involved in\nprocessing visual information which finally leads to several feature detectors.\nTherefore, the elements that form the basis of retinal computations will be\nexplored by explaining how oscillators can lead to a final output with\ncomputational meaning. Linear versus nonlinear systems will be presented and\nthe retina will be placed in the context of a nonlinear system. Finally,\nsimulations will be presented exploring the concept of the retina as a\nnonlinear system which can perform understandable computations converting a\nknown input into a predictable output.\n", "versions": [{"version": "v1", "created": "Thu, 7 Jun 2018 23:21:21 GMT"}], "update_date": "2018-06-11", "authors_parsed": [["Chiquita", "Samuel", ""]]}, {"id": "1806.02888", "submitter": "Md Nasir Uddin Laskar", "authors": "Md Nasir Uddin Laskar, Luis G Sanchez Giraldo, and Odelia Schwartz", "title": "Correspondence of Deep Neural Networks and the Brain for Visual Textures", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "q-bio.NC cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Deep convolutional neural networks (CNNs) trained on objects and scenes have\nshown intriguing ability to predict some response properties of visual cortical\nneurons. However, the factors and computations that give rise to such ability,\nand the role of intermediate processing stages in explaining changes that\ndevelop across areas of the cortical hierarchy, are poorly understood. We\nfocused on the sensitivity to textures as a paradigmatic example, since recent\nneurophysiology experiments provide rich data pointing to texture sensitivity\nin secondary but not primary visual cortex. We developed a quantitative\napproach for selecting a subset of the neural unit population from the CNN that\nbest describes the brain neural recordings. We found that the first two layers\nof the CNN showed qualitative and quantitative correspondence to the cortical\ndata across a number of metrics. This compatibility was reduced for the\narchitecture alone rather than the learned weights, for some other related\nhierarchical models, and only mildly in the absence of a nonlinear computation\nakin to local divisive normalization. Our results show that the CNN class of\nmodel is effective for capturing changes that develop across early areas of\ncortex, and has the potential to facilitate understanding of the computations\nthat give rise to hierarchical processing in the brain.\n", "versions": [{"version": "v1", "created": "Thu, 7 Jun 2018 20:20:07 GMT"}], "update_date": "2018-06-11", "authors_parsed": [["Laskar", "Md Nasir Uddin", ""], ["Giraldo", "Luis G Sanchez", ""], ["Schwartz", "Odelia", ""]]}, {"id": "1806.02893", "submitter": "William T Redman", "authors": "William T Redman", "title": "An O(n) method of calculating Kendall correlations of spike trains", "comments": "7 pages, 1 figure, 1 table", "journal-ref": "PLoS ONE (2019) 14(2): e0212190", "doi": "10.1371/journal.pone.0212190", "report-no": null, "categories": "q-bio.QM q-bio.NC", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  The ability to record from increasingly large numbers of neurons, and the\nincreasing attention being paid to large scale neural network simulations,\ndemands computationally fast algorithms to compute relevant statistical\nmeasures. We present an O(n) algorithm for calculating the Kendall correlation\nof spike trains, a correlation measure that is becoming especially recognized\nas an important tool in neuroscience. We show that our method is around 50\ntimes faster than the O (n ln n) method which is a current standard for quickly\ncomputing the Kendall correlation. In addition to providing a faster algorithm,\nwe emphasize the role that taking the specific nature of spike trains had on\nreducing the run time. We imagine that there are many other useful algorithms\nthat can be even more significantly sped up when taking this into\nconsideration. A MATLAB function executing the method described here has been\nmade freely available on-line.\n", "versions": [{"version": "v1", "created": "Thu, 7 Jun 2018 20:49:07 GMT"}, {"version": "v2", "created": "Thu, 23 Jan 2020 06:38:40 GMT"}, {"version": "v3", "created": "Tue, 25 Feb 2020 05:35:41 GMT"}], "update_date": "2020-02-26", "authors_parsed": [["Redman", "William T", ""]]}, {"id": "1806.03047", "submitter": "Andriy Temko Dr", "authors": "Sergi Gomez, Mark O'Sullivan, Emanuel Popovici, Sean Mathieson,\n  Geraldine Boylan, Andriy Temko", "title": "On sound-based interpretation of neonatal EEG", "comments": "ISSC 2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "q-bio.NC cs.SD eess.AS stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Significant training is required to visually interpret neonatal EEG signals.\nThis study explores alternative sound-based methods for EEG interpretation\nwhich are designed to allow for intuitive and quick differentiation between\nhealthy background activity and abnormal activity such as seizures. A novel\nmethod based on frequency and amplitude modulation (FM/AM) is presented. The\nalgorithm is tuned to facilitate the audio domain perception of rhythmic\nactivity which is specific to neonatal seizures. The method is compared with\nthe previously developed phase vocoder algorithm for different time compressing\nfactors. A survey is conducted amongst a cohort of non-EEG experts to\nquantitatively and qualitatively examine the performance of sound-based methods\nin comparison with the visual interpretation. It is shown that both\nsonification methods perform similarly well, with a smaller inter-observer\nvariability in comparison with visual. A post-survey analysis of results is\nperformed by examining the sensitivity of the ear to frequency evolution in\naudio.\n", "versions": [{"version": "v1", "created": "Fri, 8 Jun 2018 09:40:32 GMT"}], "update_date": "2018-06-11", "authors_parsed": [["Gomez", "Sergi", ""], ["O'Sullivan", "Mark", ""], ["Popovici", "Emanuel", ""], ["Mathieson", "Sean", ""], ["Boylan", "Geraldine", ""], ["Temko", "Andriy", ""]]}, {"id": "1806.03504", "submitter": "Yujiang Wang", "authors": "Yujiang Wang, Joe Necus, Luis Peraza Rodriguez, Peter Neal Taylor,\n  Bruno Mota", "title": "Universality in human cortical folding across lobes of individual brains", "comments": null, "journal-ref": null, "doi": "10.1038/s42003-019-0421-7", "report-no": null, "categories": "q-bio.NC physics.app-ph", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Background: We have previously demonstrated that cortical folding across\nmammalian species follows a universal scaling law that can be derived from a\nsimple theoretical model. The same scaling law has also been shown to hold\nacross brains of our own species, irrespective of age or sex. These results,\nhowever, only relate measures of complete cortical hemispheres. There are known\nsystematic variations in morphology between different brain regions, and\nregion-specific changes with age. It is therefore of interest to extend our\nanalyses to different cortical regions, and analyze the scaling law within an\nindividual brain.\n  Methods: To directly compare the morphology of sub-divisions of the cortical\nsurface in a size-independent manner, we base our method on a topological\ninvariant of closed surfaces. We reconstruct variables of a complete hemisphere\nfrom each lobe of the brain so that it has the same gyrification index, average\nthickness and average Gaussian curvature.\n  Results: We show that different lobes are morphologically diverse but obey\nthe same scaling law that was observed across human subjects and across\nmammalian species. This is also the case for subjects with Alzheimer's disease.\nThe age-dependent offset changes at similar rates for all lobes in healthy\nsubjects, but differs most dramatically in the temporal lobe in Alzheimer's\ndisease.\n  Significance: Our results further support the idea that while morphological\nparameters can vary locally across the cortical surface/across subjects of the\nsame species/across species, the processes that drive cortical gyrification are\nuniversal.\n", "versions": [{"version": "v1", "created": "Sat, 9 Jun 2018 16:36:04 GMT"}, {"version": "v2", "created": "Wed, 13 Jun 2018 08:52:50 GMT"}], "update_date": "2020-06-01", "authors_parsed": [["Wang", "Yujiang", ""], ["Necus", "Joe", ""], ["Rodriguez", "Luis Peraza", ""], ["Taylor", "Peter Neal", ""], ["Mota", "Bruno", ""]]}, {"id": "1806.03704", "submitter": "Andrew Doyle", "authors": "J. Andrew Doyle, Alan C. Evans", "title": "What Colour is Neural Noise?", "comments": "4-page paper submitted to 2018 Conference on Cognitive Computational\n  Neuroscience", "journal-ref": null, "doi": null, "report-no": null, "categories": "q-bio.NC", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Random noise plays a beneficial role in cognitive processing and produces\nmeasurable improvement in simulations and biological agents' task performance.\nStochastic facilitation, the phenomenon of additive noise improving signal\ntransmission in complex systems, has been shown to occur in a variety of neural\ncontexts. However, neuroscience analyses to date have not fully explored the\ncolours that neural noise could be. The literature shows a 1/f pink noise power\nspectrum distribution at many levels, but many less rigourous studies assume\nwhite noise, with little justification of why that assumption is made. In this\nwork, we briefly review the colours of noise and their useful applications in\nother fields. If we consider that noise is not so black and white, we could\nmore colourfully regularize artificial neural networks and re-investigate some\nsurprising results about how the brain benefits from noise.\n", "versions": [{"version": "v1", "created": "Sun, 10 Jun 2018 18:27:03 GMT"}], "update_date": "2018-06-12", "authors_parsed": [["Doyle", "J. Andrew", ""], ["Evans", "Alan C.", ""]]}, {"id": "1806.03872", "submitter": "Zachary Kilpatrick PhD", "authors": "Khanh P Nguyen, Kresimir Josic, and Zachary P Kilpatrick", "title": "Optimizing sequential decisions in the drift-diffusion model", "comments": "20 pages, 6 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "q-bio.NC math.PR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  To make decisions organisms often accumulate information across multiple\ntimescales. However, most experimental and modeling studies of decision-making\nfocus on sequences of independent trials. On the other hand, natural\nenvironments are characterized by long temporal correlations, and evidence used\nto make a present choice is often relevant to future decisions. To understand\ndecision-making under these conditions we analyze how a model ideal observer\naccumulates evidence to freely make choices across a sequence of correlated\ntrials. We use principles of probabilistic inference to show that an ideal\nobserver incorporates information obtained on one trial as an initial bias on\nthe next. This bias decreases the time, but not the accuracy of the next\ndecision. Furthermore, in finite sequences of trials the rate of reward is\nmaximized when the observer deliberates longer for early decisions, but\nresponds more quickly towards the end of the sequence. Our model also explains\nexperimentally observed patterns in decision times and choices, thus providing\na mathematically principled foundation for evidence-accumulation models of\nsequential decisions.\n", "versions": [{"version": "v1", "created": "Mon, 11 Jun 2018 09:24:54 GMT"}], "update_date": "2018-06-12", "authors_parsed": [["Nguyen", "Khanh P", ""], ["Josic", "Kresimir", ""], ["Kilpatrick", "Zachary P", ""]]}, {"id": "1806.03881", "submitter": "Michael Schmuker", "authors": "Michael Schmuker, R\\\"udiger Kupper, Ad Aertsen, Thomas Wachtler,\n  Marc-Oliver Gewaltig", "title": "Feed-forward and noise-tolerant detection of feature homogeneity in\n  spiking networks with a latency code", "comments": "Accepted for publication in Biological Cybernetics", "journal-ref": null, "doi": null, "report-no": null, "categories": "q-bio.NC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In studies of the visual system as well as in computer vision, the focus is\noften on contrast edges. However, the primate visual system contains a large\nnumber of cells that are insensitive to spatial contrast and, instead, respond\nto uniform homogeneous illumination of their visual field. The purpose of this\ninformation remains unclear. Here, we propose a mechanism that detects feature\nhomogeneity in visual areas, based on latency coding and spike time\ncoincidence, in a purely feed-forward and therefore rapid manner. We\ndemonstrate how homogeneity information can interact with information on\ncontrast edges to potentially support rapid image segmentation. Furthermore, we\nanalyze how neuronal crosstalk (noise) affects the mechanism's performance. We\nshow that the detrimental effects of crosstalk can be partly mitigated through\ndelayed feed-forward inhibition that shapes bi-phasic post-synaptic events. The\ndelay of the feed-forward inhibition allows effectively controlling the size of\nthe temporal integration window and, thereby, the coincidence threshold. The\nproposed model is based on single-spike latency codes in a purely feed-forward\narchitecture that supports low-latency processing, making it an attractive\nscheme of computation in spiking neuronal networks where rapid responses and\nlow spike counts are desired.\n", "versions": [{"version": "v1", "created": "Mon, 11 Jun 2018 09:40:46 GMT"}, {"version": "v2", "created": "Wed, 26 Aug 2020 19:14:17 GMT"}, {"version": "v3", "created": "Tue, 15 Dec 2020 10:38:17 GMT"}, {"version": "v4", "created": "Mon, 29 Mar 2021 12:41:38 GMT"}], "update_date": "2021-03-30", "authors_parsed": [["Schmuker", "Michael", ""], ["Kupper", "R\u00fcdiger", ""], ["Aertsen", "Ad", ""], ["Wachtler", "Thomas", ""], ["Gewaltig", "Marc-Oliver", ""]]}, {"id": "1806.03889", "submitter": "Lucas Gabriel Souza Fran\\c{c}a", "authors": "Lucas G. S. Fran\\c{c}a, Jos\\'e G. V. Miranda, Marco Leite, Niraj K.\n  Sharma, Matthew C. Walker, Louis Lemieux, Yujiang Wang", "title": "Fractal and Multifractal Properties of Electrographic Recordings of\n  Human Brain Activity: Toward Its Use as a Signal Feature for Machine Learning\n  in Clinical Applications", "comments": "Final version published at Frontiers in Physiology.\n  https://doi.org/10.3389/fphys.2018.01767", "journal-ref": "Fran\\c{c}a, LGS et al., (2018) Fractal and Multifractal Properties\n  of Electrographic Recordings of Human Brain Activity: Toward Its Use as a\n  Signal Feature for Machine Learning in Clinical Applications. Front. Physiol.\n  9:1767", "doi": "10.3389/fphys.2018.01767", "report-no": null, "categories": "q-bio.NC cond-mat.stat-mech cs.IT math.IT nlin.AO nlin.CD", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The brain is a system operating on multiple time scales, and characterisation\nof dynamics across time scales remains a challenge. One framework to study such\ndynamics is that of fractal geometry. However, currently there exists no\nestablished method for the study of brain dynamics using fractal geometry, due\nto the many challenges in the conceptual and technical understanding of the\nmethods. We aim to highlight some of the practical challenges of applying\nfractal geometry to brain dynamics and propose solutions to enable its wider\nuse in neuroscience. Using intracranially recorded EEG and simulated data, we\ncompared monofractal and multifractal methods with regards to their sensitivity\nto signal variance. We found that both correlate closely with signal variance,\nthus not offering new information about the signal. However, after applying an\nepoch-wise standardisation procedure to the signal, we found that multifractal\nmeasures could offer non-redundant information compared to signal variance,\npower and other established EEG signal measures. We also compared different\nmultifractal estimation methods and found that the Chhabra-Jensen algorithm\nperformed best. Finally, we investigated the impact of sampling frequency and\nepoch length on multifractal properties. Using epileptic seizures as an example\nevent in the EEG, we show that there may be an optimal time scale for detecting\ntemporal changes in multifractal properties around seizures. The practical\nissues we highlighted and our suggested solutions should help in developing a\nrobust method for the application of fractal geometry in EEG signals. Our\nanalyses and observations also aid the theoretical understanding of the\nmultifractal properties of the brain and might provide grounds for new\ndiscoveries in the study of brain signals. These could be crucial for\nunderstanding of neurological function and for the developments of new\ntreatments.\n", "versions": [{"version": "v1", "created": "Mon, 11 Jun 2018 09:55:33 GMT"}, {"version": "v2", "created": "Tue, 11 Dec 2018 17:12:09 GMT"}], "update_date": "2018-12-12", "authors_parsed": [["Fran\u00e7a", "Lucas G. S.", ""], ["Miranda", "Jos\u00e9 G. V.", ""], ["Leite", "Marco", ""], ["Sharma", "Niraj K.", ""], ["Walker", "Matthew C.", ""], ["Lemieux", "Louis", ""], ["Wang", "Yujiang", ""]]}, {"id": "1806.04037", "submitter": "Andriy Temko Dr", "authors": "Mark O'Sullivan, Sergi Gomez, Alison O'Shea, Eduard Salgado, Kevin\n  Huillca, Sean Mathieson, Geraldine Boylan, Emanuel Popovici, Andriy Temko", "title": "Neonatal EEG Interpretation and Decision Support Framework for Mobile\n  Platforms", "comments": "EMBC 2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "q-bio.NC cs.SE stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper proposes and implements an intuitive and pervasive solution for\nneonatal EEG monitoring assisted by sonification and deep learning AI that\nprovides information about neonatal brain health to all neonatal healthcare\nprofessionals, particularly those without EEG interpretation expertise. The\nsystem aims to increase the demographic of clinicians capable of diagnosing\nabnormalities in neonatal EEG. The proposed system uses a low-cost and\nlow-power EEG acquisition system. An Android app provides single-channel EEG\nvisualization, traffic-light indication of the presence of neonatal seizures\nprovided by a trained, deep convolutional neural network and an algorithm for\nEEG sonification, designed to facilitate the perception of changes in EEG\nmorphology specific to neonatal seizures. The multifaceted EEG interpretation\nframework is presented and the implemented mobile platform architecture is\nanalyzed with respect to its power consumption and accuracy.\n", "versions": [{"version": "v1", "created": "Fri, 8 Jun 2018 09:35:02 GMT"}], "update_date": "2018-06-12", "authors_parsed": [["O'Sullivan", "Mark", ""], ["Gomez", "Sergi", ""], ["O'Shea", "Alison", ""], ["Salgado", "Eduard", ""], ["Huillca", "Kevin", ""], ["Mathieson", "Sean", ""], ["Boylan", "Geraldine", ""], ["Popovici", "Emanuel", ""], ["Temko", "Andriy", ""]]}, {"id": "1806.04122", "submitter": "Marc Howard", "authors": "Marc W. Howard, Andre Luzardo, and Zoran Tiganj", "title": "Evidence accumulation in a Laplace domain decision space", "comments": "Revised for CBB", "journal-ref": null, "doi": null, "report-no": null, "categories": "q-bio.NC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Evidence accumulation models of simple decision-making have long assumed that\nthe brain estimates a scalar decision variable corresponding to the\nlog-likelihood ratio of the two alternatives. Typical neural implementations of\nthis algorithmic cognitive model assume that large numbers of neurons are each\nnoisy exemplars of the scalar decision variable. Here we propose a neural\nimplementation of the diffusion model in which many neurons construct and\nmaintain the Laplace transform of the distance to each of the decision bounds.\nAs in classic findings from brain regions including LIP, the firing rate of\nneurons coding for the Laplace transform of net accumulated evidence grows to a\nbound during random dot motion tasks. However, rather than noisy exemplars of a\nsingle mean value, this approach makes the novel prediction that firing rates\ngrow to the bound exponentially, across neurons there should be a distribution\nof different rates. A second set of neurons records an approximate inversion of\nthe Laplace transform, these neurons directly estimate net accumulated\nevidence. In analogy to time cells and place cells observed in the hippocampus\nand other brain regions, the neurons in this second set have receptive fields\nalong a \"decision axis.\" This finding is consistent with recent findings from\nrodent recordings. This theoretical approach places simple evidence\naccumulation models in the same mathematical language as recent proposals for\nrepresenting time and space in cognitive models for memory.\n", "versions": [{"version": "v1", "created": "Mon, 11 Jun 2018 17:43:56 GMT"}, {"version": "v2", "created": "Tue, 23 Oct 2018 03:11:12 GMT"}], "update_date": "2018-10-24", "authors_parsed": [["Howard", "Marc W.", ""], ["Luzardo", "Andre", ""], ["Tiganj", "Zoran", ""]]}, {"id": "1806.04258", "submitter": "Joaquin Torres", "authors": "Muhammet Uzuntarla, Joaquin J. Torres, Ali \\c{C}al{\\i}m, Ernest\n  Barreto", "title": "Synchronization-Induced Spike Termination in Networks of Bistable\n  Neurons", "comments": "26 pages, 8 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cond-mat.dis-nn physics.bio-ph q-bio.NC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We observe and study a self-organized phenomenon whereby the activity in a\nnetwork of spiking neurons spontaneously terminates. We consider different\ntypes of populations, consisting of bistable model neurons connected\nelectrically by gap junctions, or by either excitatory or inhibitory synapses,\nin a scale-free connection topology. We find that strongly synchronized\npopulation spiking events lead to complete cessation of activity in excitatory\nnetworks, but not in gap junction or inhibitory networks. We identify the\nunderlying mechanism responsible for this phenomenon by examining the\nparticular shape of the excitatory postsynaptic currents that arise in the\nneurons. We also examine the effects of the synaptic time constant, coupling\nstrength, and channel noise on the occurrence of the phenomenon.\n", "versions": [{"version": "v1", "created": "Mon, 11 Jun 2018 22:27:32 GMT"}, {"version": "v2", "created": "Wed, 21 Nov 2018 11:05:57 GMT"}], "update_date": "2018-11-22", "authors_parsed": [["Uzuntarla", "Muhammet", ""], ["Torres", "Joaquin J.", ""], ["\u00c7al\u0131m", "Ali", ""], ["Barreto", "Ernest", ""]]}, {"id": "1806.04703", "submitter": "Yuriy Gerasimov", "authors": "Max Talanov, Yuriy Gerasimov, Victor Erokhin", "title": "Electronic schematic for bio-plausible dopamine neuromodulation of eSTDP\n  and iSTDP", "comments": "7 pages, 6 figures", "journal-ref": null, "doi": null, "report-no": "Memristive-brain-2018-01", "categories": "q-bio.NC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this technical report we present novel results of the dopamine\nbio-plausible neuromodulation excitatory (eSTDP) and inhibitory (iSTDP)\nlearning. We present the principal schematic for the neuromodulation of D1 and\nD2 receptors of dopamine, wiring schematic for both cases as well as the\nsimulatory experiments results done in LTSpice.\n", "versions": [{"version": "v1", "created": "Mon, 28 May 2018 18:10:04 GMT"}], "update_date": "2018-06-14", "authors_parsed": [["Talanov", "Max", ""], ["Gerasimov", "Yuriy", ""], ["Erokhin", "Victor", ""]]}, {"id": "1806.04704", "submitter": "Gerard Rinkus", "authors": "Gerard Rinkus", "title": "Sparse distributed representation, hierarchy, critical periods,\n  metaplasticity: the keys to lifelong fixed-time learning and best-match\n  retrieval", "comments": "6 pages, 4 figs. Accepted for talk at Biological Distributed\n  Algorithms 2018. July 23, 2018. London", "journal-ref": null, "doi": null, "report-no": null, "categories": "q-bio.NC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Among the more important hallmarks of human intelligence, which any\nartificial general intelligence (AGI) should have, are the following. 1. It\nmust be capable of on-line learning, including with single/few trials. 2.\nMemories/knowledge must be permanent over lifelong durations, safe from\ncatastrophic forgetting. Some confabulation, i.e., semantically plausible\nretrieval errors, may gradually accumulate over time. 3. The time to both: a)\nlearn a new item, and b) retrieve the best-matching / most relevant item(s),\ni.e., do similarity-based retrieval, must remain constant throughout the\nlifetime. 4. The system should never become full: it must remain able to store\nnew information, i.e., make new permanent memories, throughout very long\nlifetimes. No artificial computational system has been shown to have all these\nproperties. Here, we describe a neuromorphic associative memory model, Sparsey,\nwhich does, in principle, possess them all. We cite prior results supporting\npossession of hallmarks 1 and 3 and sketch an argument, hinging on strongly\nrecursive, hierarchical, part-whole compositional structure of natural data,\nthat Sparsey also possesses hallmarks 2 and 4.\n", "versions": [{"version": "v1", "created": "Sat, 2 Jun 2018 17:16:10 GMT"}], "update_date": "2018-06-14", "authors_parsed": [["Rinkus", "Gerard", ""]]}, {"id": "1806.04710", "submitter": "Hao Wang", "authors": "Hao Wang, Jiahui Wang, Xin Yuan Thow, Chengkuo Lee", "title": "A quantized physical framework for understanding the working mechanism\n  of ion channels", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "q-bio.NC physics.bio-ph", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A quantized physical framework, called the five-anchor model, is developed\nfor a general understanding of the working mechanism of ion channels. According\nto the hypotheses of this model, the following two basic physical principles\nare assigned to each anchor: the polarity change induced by an electron\ntransition and the mutual repulsion and attraction induced by an electrostatic\nforce. Consequently, many unique phenomena, such as fast and slow inactivation,\nthe stochastic gating pattern and constant conductance of a single ion channel,\nthe difference between electrical and optical stimulation (optogenetics), nerve\nconduction block and the generation of an action potential, become intrinsic\nfeatures of this physical model. Moreover, this model also provides a\nfoundation for the probability equation used to calculate the results of\nelectrical stimulation in our previous C-P theory.\n", "versions": [{"version": "v1", "created": "Wed, 13 Jun 2018 08:58:45 GMT"}, {"version": "v2", "created": "Sat, 4 Aug 2018 09:17:38 GMT"}], "update_date": "2018-08-07", "authors_parsed": [["Wang", "Hao", ""], ["Wang", "Jiahui", ""], ["Thow", "Xin Yuan", ""], ["Lee", "Chengkuo", ""]]}, {"id": "1806.04793", "submitter": "Fabian Tschopp", "authors": "Fabian David Tschopp, Michael B. Reiser, Srinivas C. Turaga", "title": "A Connectome Based Hexagonal Lattice Convolutional Network Model of the\n  Drosophila Visual System", "comments": "Work in progress. Final paper with results from an updated model with\n  new connectome data will be coming soon", "journal-ref": null, "doi": null, "report-no": null, "categories": "q-bio.NC cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  What can we learn from a connectome? We constructed a simplified model of the\nfirst two stages of the fly visual system, the lamina and medulla. The\nresulting hexagonal lattice convolutional network was trained using\nbackpropagation through time to perform object tracking in natural scene\nvideos. Networks initialized with weights from connectome reconstructions\nautomatically discovered well-known orientation and direction selectivity\nproperties in T4 neurons and their inputs, while networks initialized at random\ndid not. Our work is the first demonstration, that knowledge of the connectome\ncan enable in silico predictions of the functional properties of individual\nneurons in a circuit, leading to an understanding of circuit function from\nstructure alone.\n", "versions": [{"version": "v1", "created": "Tue, 12 Jun 2018 22:57:14 GMT"}, {"version": "v2", "created": "Sun, 24 Jun 2018 10:36:40 GMT"}], "update_date": "2018-06-26", "authors_parsed": [["Tschopp", "Fabian David", ""], ["Reiser", "Michael B.", ""], ["Turaga", "Srinivas C.", ""]]}, {"id": "1806.05167", "submitter": "Ann Sizemore", "authors": "Ann E. Sizemore, Jennifer Phillips-Cremins, Robert Ghrist, Danielle S.\n  Bassett", "title": "The importance of the whole: topological data analysis for the network\n  neuroscientist", "comments": "17 pages, 6 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "q-bio.QM q-bio.NC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The application of network techniques to the analysis of neural data has\ngreatly improved our ability to quantify and describe these rich interacting\nsystems. Among many important contributions, networks have proven useful in\nidentifying sets of node pairs that are densely connected and that collectively\nsupport brain function. Yet the restriction to pairwise interactions prevents\nus from realizing intrinsic topological features such as cavities within the\ninterconnection structure that may be just as crucial for proper function. To\ndetect and quantify these topological features we must turn to methods from\nalgebraic topology that encode data as a simplicial complex built of sets of\ninteracting nodes called simplices. On this substrate, we can then use the\nrelations between simplices and higher-order connectivity to expose cavities\nwithin the complex, thereby summarizing its topological nature. Here we provide\nan introduction to persistent homology, a fundamental method from applied\ntopology that builds a global descriptor of system structure by chronicling the\nevolution of cavities as we move through a combinatorial object such as a\nweighted network. We detail the underlying mathematics and perform\ndemonstrative calculations on the mouse structural connectome, electrical and\nchemical synapses in \\textit{C. elegans}, and genomic interaction data. Finally\nwe suggest avenues for future work and highlight new advances in mathematics\nthat appear ready for use in revealing the architecture and function of neural\nsystems.\n", "versions": [{"version": "v1", "created": "Wed, 13 Jun 2018 17:55:27 GMT"}], "update_date": "2018-06-14", "authors_parsed": [["Sizemore", "Ann E.", ""], ["Phillips-Cremins", "Jennifer", ""], ["Ghrist", "Robert", ""], ["Bassett", "Danielle S.", ""]]}, {"id": "1806.05177", "submitter": "Subba Reddy Oota", "authors": "Subba Reddy Oota, Naresh Manwani, and Bapi Raju S", "title": "fMRI Semantic Category Decoding using Linguistic Encoding of Word\n  Embeddings", "comments": "12 pages, 7 Figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "q-bio.NC cs.CL cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  The dispute of how the human brain represents conceptual knowledge has been\nargued in many scientific fields. Brain imaging studies have shown that the\nspatial patterns of neural activation in the brain are correlated with thinking\nabout different semantic categories of words (for example, tools, animals, and\nbuildings) or when viewing the related pictures. In this paper, we present a\ncomputational model that learns to predict the neural activation captured in\nfunctional magnetic resonance imaging (fMRI) data of test words. Unlike the\nmodels with hand-crafted features that have been used in the literature, in\nthis paper we propose a novel approach wherein decoding models are built with\nfeatures extracted from popular linguistic encodings of Word2Vec, GloVe,\nMeta-Embeddings in conjunction with the empirical fMRI data associated with\nviewing several dozen concrete nouns. We compared these models with several\nother models that use word features extracted from FastText, Randomly-generated\nfeatures, Mitchell's 25 features [1]. The experimental results show that the\npredicted fMRI images using Meta-Embeddings meet the state-of-the-art\nperformance. Although models with features from GloVe and Word2Vec predict fMRI\nimages similar to the state-of-the-art model, model with features from\nMeta-Embeddings predicts significantly better. The proposed scheme that uses\npopular linguistic encoding offers a simple and easy approach for semantic\ndecoding from fMRI experiments.\n", "versions": [{"version": "v1", "created": "Wed, 13 Jun 2018 10:59:33 GMT"}], "update_date": "2018-06-15", "authors_parsed": [["Oota", "Subba Reddy", ""], ["Manwani", "Naresh", ""], ["S", "Bapi Raju", ""]]}, {"id": "1806.05279", "submitter": "Ernest Montbrio", "authors": "Ernest Montbri\\'o, Diego Paz\\'o", "title": "Kuramoto model for excitation-inhibition-based oscillations", "comments": null, "journal-ref": "Physical Review Letters 120, 244101 (2018)", "doi": "10.1103/PhysRevLett.120.244101", "report-no": null, "categories": "nlin.AO nlin.CD q-bio.NC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The Kuramoto model (KM) is a theoretical paradigm for investigating the\nemergence of rhythmic activity in large populations of oscillators. A\nremarkable example of rhythmogenesis is the feedback loop between excitatory\n(E) and inhibitory (I) cells in large neuronal networks. Yet, although the\nEI-feedback mechanism plays a central role in the generation of brain\noscillations, it remains unexplored whether the KM has enough biological\nrealism to describe it. Here we derive a two-population KM that fully accounts\nfor the onset of EI-based neuronal rhythms and that, as the original KM, is\nanalytically solvable to a large extent. Our results provide a powerful\ntheoretical tool for the analysis of large-scale neuronal oscillations.\n", "versions": [{"version": "v1", "created": "Wed, 13 Jun 2018 21:48:06 GMT"}], "update_date": "2018-06-15", "authors_parsed": [["Montbri\u00f3", "Ernest", ""], ["Paz\u00f3", "Diego", ""]]}, {"id": "1806.05882", "submitter": "Yang Yue", "authors": "Yang Yue, Liuyuan He, Gan He, Jian.K.Liu, Kai Du, Yonghong Tian,\n  Tiejun Huang", "title": "A simple blind-denoising filter inspired by electrically coupled\n  photoreceptors in the retina", "comments": "16 pages, 8 figures, 9 tables, Submitted to NIPS 2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV eess.IV q-bio.NC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Photoreceptors in the retina are coupled by electrical synapses called \"gap\njunctions\". It has long been established that gap junctions increase the\nsignal-to-noise ratio of photoreceptors. Inspired by electrically coupled\nphotoreceptors, we introduced a simple filter, the PR-filter, with only one\nvariable. On BSD68 dataset, PR-filter showed outstanding performance in SSIM\nduring blind denoising tasks. It also significantly improved the performance of\nstate-of-the-art convolutional neural network blind denosing on non-Gaussian\nnoise. The performance of keeping more details might be attributed to small\nreceptive field of the photoreceptors.\n", "versions": [{"version": "v1", "created": "Fri, 15 Jun 2018 10:08:59 GMT"}, {"version": "v2", "created": "Thu, 28 Jun 2018 05:53:00 GMT"}, {"version": "v3", "created": "Mon, 2 Jul 2018 03:01:55 GMT"}, {"version": "v4", "created": "Mon, 27 Aug 2018 10:04:13 GMT"}], "update_date": "2018-08-28", "authors_parsed": [["Yue", "Yang", ""], ["He", "Liuyuan", ""], ["He", "Gan", ""], ["Liu", "Jian. K.", ""], ["Du", "Kai", ""], ["Tian", "Yonghong", ""], ["Huang", "Tiejun", ""]]}, {"id": "1806.06412", "submitter": "Yuri A. Dabaghian", "authors": "Luca Perotti, Justin DeVito, Daniel Bessis, Yuri Dabaghian", "title": "Discrete structure of the brain rhythms", "comments": "17 pages, 9 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "q-bio.QM q-bio.NC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Neuronal activity in the brain generates synchronous oscillations of the\nLocal Field Potential (LFP). The traditional analyses of the LFPs are based on\ndecomposing the signal into simpler components, such as sinusoidal harmonics.\nHowever, a common drawback of such methods is that the decomposition primitives\nare usually presumed from the onset, which may bias our understanding of the\nsignal's structure. Here, we introduce an alternative approach that allows an\nimpartial, high resolution, hands-off decomposition of the brain waves into a\nsmall number of discrete, frequency-modulated oscillatory processes, which we\ncall oscillons. In particular, we demonstrate that mouse hippocampal LFP\ncontain a single oscillon that occupies the $\\theta$-frequency band and a\ncouple of $\\gamma$-oscillons that correspond, respectively, to slow and fast\n$\\gamma$-waves. Since the oscillons were identified empirically, they may\nrepresent the actual, physical structure of synchronous oscillations in\nneuronal ensembles, whereas Fourier-defined \"brain waves\" are nothing but\npoorly resolved oscillons.\n", "versions": [{"version": "v1", "created": "Sun, 17 Jun 2018 16:42:50 GMT"}], "update_date": "2018-06-19", "authors_parsed": [["Perotti", "Luca", ""], ["DeVito", "Justin", ""], ["Bessis", "Daniel", ""], ["Dabaghian", "Yuri", ""]]}, {"id": "1806.06545", "submitter": "Nicolas Rougier", "authors": "Anthony Strock (Mnemosyne), Nicolas Rougier (Mnemosyne), Xavier Hinaut\n  (Mnemosyne)", "title": "A Simple Reservoir Model of Working Memory with Real Values", "comments": null, "journal-ref": "International Joint Conference on Neural Networks (IJCNN), Jul\n  2018, Rio de Janeiro, Brazil", "doi": null, "report-no": null, "categories": "q-bio.NC cs.LG cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The prefrontal cortex is known to be involved in many high-level cognitive\nfunctions, in particular, working memory. Here, we study to what extent a group\nof randomly connected units (namely an Echo State Network, ESN) can store and\nmaintain (as output) an arbitrary real value from a streamed input, i.e. can\nact as a sustained working memory unit. Furthermore, we explore to what extent\nsuch an architecture can take advantage of the stored value in order to produce\nnon-linear computations. Comparison between different architectures (with and\nwithout feedback, with and without a working memory unit) shows that an\nexplicit memory improves the performances.\n", "versions": [{"version": "v1", "created": "Mon, 18 Jun 2018 08:22:45 GMT"}], "update_date": "2018-06-19", "authors_parsed": [["Strock", "Anthony", "", "Mnemosyne"], ["Rougier", "Nicolas", "", "Mnemosyne"], ["Hinaut", "Xavier", "", "Mnemosyne"]]}, {"id": "1806.06765", "submitter": "Jason Jo", "authors": "Jason Jo, Vikas Verma, Yoshua Bengio", "title": "Modularity Matters: Learning Invariant Relational Reasoning Tasks", "comments": "Modified abstract to fit arXiv character limit", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.NE q-bio.NC stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We focus on two supervised visual reasoning tasks whose labels encode a\nsemantic relational rule between two or more objects in an image: the MNIST\nParity task and the colorized Pentomino task. The objects in the images undergo\nrandom translation, scaling, rotation and coloring transformations. Thus these\ntasks involve invariant relational reasoning. We report uneven performance of\nvarious deep CNN models on these two tasks. For the MNIST Parity task, we\nreport that the VGG19 model soundly outperforms a family of ResNet models.\nMoreover, the family of ResNet models exhibits a general sensitivity to random\ninitialization for the MNIST Parity task. For the colorized Pentomino task, now\nboth the VGG19 and ResNet models exhibit sluggish optimization and very poor\ntest generalization, hovering around 30% test error. The CNN we tested all\nlearn hierarchies of fully distributed features and thus encode the distributed\nrepresentation prior. We are motivated by a hypothesis from cognitive\nneuroscience which posits that the human visual cortex is modularized, and this\nallows the visual cortex to learn higher order invariances. To this end, we\nconsider a modularized variant of the ResNet model, referred to as a Residual\nMixture Network (ResMixNet) which employs a mixture-of-experts architecture to\ninterleave distributed representations with more specialized, modular\nrepresentations. We show that very shallow ResMixNets are capable of learning\neach of the two tasks well, attaining less than 2% and 1% test error on the\nMNIST Parity and the colorized Pentomino tasks respectively. Most importantly,\nthe ResMixNet models are extremely parameter efficient: generalizing better\nthan various non-modular CNNs that have over 10x the number of parameters.\nThese experimental results support the hypothesis that modularity is a robust\nprior for learning invariant relational reasoning.\n", "versions": [{"version": "v1", "created": "Mon, 18 Jun 2018 15:19:04 GMT"}], "update_date": "2018-06-19", "authors_parsed": [["Jo", "Jason", ""], ["Verma", "Vikas", ""], ["Bengio", "Yoshua", ""]]}, {"id": "1806.06809", "submitter": "Pablo Lanillos", "authors": "Nina-Alisa Hinz, Pablo Lanillos, Hermann Mueller, Gordon Cheng", "title": "Drifting perceptual patterns suggest prediction errors fusion rather\n  than hypothesis selection: replicating the rubber-hand illusion on a robot", "comments": "Proceedings of the 2018 IEEE International Conference on Development\n  and Learning and Epigenetic Robotics", "journal-ref": "Joint IEEE 8th International Conference on Development and\n  Learning and Epigenetic Robotics (ICDL-EpiRob), Tokyo, Japan, 2018, pp.\n  125-132", "doi": "10.1109/DEVLRN.2018.8761005", "report-no": null, "categories": "q-bio.NC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Humans can experience fake body parts as theirs just by simple visuo-tactile\nsynchronous stimulation. This body-illusion is accompanied by a drift in the\nperception of the real limb towards the fake limb, suggesting an update of body\nestimation resulting from stimulation. This work compares body limb drifting\npatterns of human participants, in a rubber hand illusion experiment, with the\nend-effector estimation displacement of a multisensory robotic arm enabled with\npredictive processing perception. Results show similar drifting patterns in\nboth human and robot experiments, and they also suggest that the perceptual\ndrift is due to prediction error fusion, rather than hypothesis selection. We\npresent body inference through prediction error minimization as one single\nprocess that unites predictive coding and causal inference and that it is\nresponsible for the effects in perception when we are subjected to intermodal\nsensory perturbations.\n", "versions": [{"version": "v1", "created": "Mon, 18 Jun 2018 16:25:48 GMT"}, {"version": "v2", "created": "Thu, 5 Jul 2018 16:48:00 GMT"}], "update_date": "2019-10-24", "authors_parsed": [["Hinz", "Nina-Alisa", ""], ["Lanillos", "Pablo", ""], ["Mueller", "Hermann", ""], ["Cheng", "Gordon", ""]]}, {"id": "1806.06823", "submitter": "Michael Hersche", "authors": "Michael Hersche, Tino Rellstab, Pasquale Davide Schiavone, Lukas\n  Cavigelli, Luca Benini, Abbas Rahimi", "title": "Fast and Accurate Multiclass Inference for MI-BCIs Using Large\n  Multiscale Temporal and Spectral Features", "comments": "Published as a conference paper at the IEEE European Signal\n  Processing Conference (EUSIPCO), 2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.SP q-bio.NC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Accurate, fast, and reliable multiclass classification of\nelectroencephalography (EEG) signals is a challenging task towards the\ndevelopment of motor imagery brain-computer interface (MI-BCI) systems. We\npropose enhancements to different feature extractors, along with a support\nvector machine (SVM) classifier, to simultaneously improve classification\naccuracy and execution time during training and testing. We focus on the\nwell-known common spatial pattern (CSP) and Riemannian covariance methods, and\nsignificantly extend these two feature extractors to multiscale temporal and\nspectral cases. The multiscale CSP features achieve 73.70$\\pm$15.90% (mean$\\pm$\nstandard deviation across 9 subjects) classification accuracy that surpasses\nthe state-of-the-art method [1], 70.6$\\pm$14.70%, on the 4-class BCI\ncompetition IV-2a dataset. The Riemannian covariance features outperform the\nCSP by achieving 74.27$\\pm$15.5% accuracy and executing 9x faster in training\nand 4x faster in testing. Using more temporal windows for Riemannian features\nresults in 75.47$\\pm$12.8% accuracy with 1.6x faster testing than CSP.\n", "versions": [{"version": "v1", "created": "Mon, 18 Jun 2018 17:01:28 GMT"}, {"version": "v2", "created": "Mon, 25 Jun 2018 07:06:21 GMT"}, {"version": "v3", "created": "Wed, 12 Dec 2018 21:38:32 GMT"}], "update_date": "2018-12-14", "authors_parsed": [["Hersche", "Michael", ""], ["Rellstab", "Tino", ""], ["Schiavone", "Pasquale Davide", ""], ["Cavigelli", "Lukas", ""], ["Benini", "Luca", ""], ["Rahimi", "Abbas", ""]]}, {"id": "1806.07108", "submitter": "Qiqi Zhang", "authors": "Qiqi Zhang and Ying Liu", "title": "Improving brain computer interface performance by data augmentation with\n  conditional Deep Convolutional Generative Adversarial Networks", "comments": "4 pages, 5 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC cs.LG q-bio.NC stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  One of the big restrictions in brain computer interface field is the very\nlimited training samples, it is difficult to build a reliable and usable system\nwith such limited data. Inspired by generative adversarial networks, we propose\na conditional Deep Convolutional Generative Adversarial (cDCGAN) Networks\nmethod to generate more artificial EEG signal automatically for data\naugmentation to improve the performance of convolutional neural networks in\nbrain computer interface field and overcome the small training dataset\nproblems. We evaluate the proposed cDCGAN method on BCI competition dataset of\nmotor imagery. The results show that the generated artificial EEG data from\nGaussian noise can learn the features from raw EEG data and has no less than\nthe classification accuracy of raw EEG data in the testing dataset. Also by\nusing generated artificial data can effectively improve classification accuracy\nat the same model with limited training data.\n", "versions": [{"version": "v1", "created": "Tue, 19 Jun 2018 08:49:50 GMT"}, {"version": "v2", "created": "Thu, 27 Dec 2018 08:32:32 GMT"}], "update_date": "2018-12-31", "authors_parsed": [["Zhang", "Qiqi", ""], ["Liu", "Ying", ""]]}, {"id": "1806.07365", "submitter": "Trang-Anh Estelle Nghiem", "authors": "Trang-Anh Nghiem, Jean-Marc Lina, Matteo di Volo, Cristiano Capone,\n  Alan C. Evans, Alain Destexhe, and Jennifer S. Goldman", "title": "State equation from the spectral structure of human brain activity", "comments": "6 pages, 4 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "q-bio.NC cond-mat.dis-nn", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Neural electromagnetic (EM) signals recorded non-invasively from individual\nhuman subjects vary in complexity and magnitude. Nonetheless, variation in\nneural activity has been difficult to quantify and interpret, due to complex,\nbroad-band features in the frequency domain. Studying signals recorded with\nmagnetoencephalography (MEG) from healthy young adult subjects while in resting\nand active states, a systematic framework inspired by thermodynamics is applied\nto neural EM signals. Despite considerable inter-subject variation in terms of\nspectral entropy and energy across time epochs, data support the existence of a\nrobust and linear relationship defining an effective state equation, with\nhigher energy and lower entropy in the resting state compared to active,\nconsistently across subjects. Mechanisms underlying the emergence of\nrelationships between empirically measured effective state functions are\nfurther investigated using a model network of coupled oscillators, suggesting\nan interplay between noise and coupling strength can account for coherent\nvariation of empirically observed quantities. Taken together, the results show\nmacroscopic neural observables follow a robust, non-trivial conservation rule\nfor energy modulation and information generation.\n", "versions": [{"version": "v1", "created": "Tue, 19 Jun 2018 17:49:40 GMT"}, {"version": "v2", "created": "Tue, 3 Jul 2018 14:45:06 GMT"}], "update_date": "2018-07-04", "authors_parsed": [["Nghiem", "Trang-Anh", ""], ["Lina", "Jean-Marc", ""], ["di Volo", "Matteo", ""], ["Capone", "Cristiano", ""], ["Evans", "Alan C.", ""], ["Destexhe", "Alain", ""], ["Goldman", "Jennifer S.", ""]]}, {"id": "1806.07406", "submitter": "Georgios Detorakis", "authors": "Georgios Detorakis, Travis Bartley, Emre Neftci", "title": "Contrastive Hebbian Learning with Random Feedback Weights", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG q-bio.NC stat.ML", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Neural networks are commonly trained to make predictions through learning\nalgorithms. Contrastive Hebbian learning, which is a powerful rule inspired by\ngradient backpropagation, is based on Hebb's rule and the contrastive\ndivergence algorithm. It operates in two phases, the forward (or free) phase,\nwhere the data are fed to the network, and a backward (or clamped) phase, where\nthe target signals are clamped to the output layer of the network and the\nfeedback signals are transformed through the transpose synaptic weight\nmatrices. This implies symmetries at the synaptic level, for which there is no\nevidence in the brain. In this work, we propose a new variant of the algorithm,\ncalled random contrastive Hebbian learning, which does not rely on any synaptic\nweights symmetries. Instead, it uses random matrices to transform the feedback\nsignals during the clamped phase, and the neural dynamics are described by\nfirst order non-linear differential equations. The algorithm is experimentally\nverified by solving a Boolean logic task, classification tasks (handwritten\ndigits and letters), and an autoencoding task. This article also shows how the\nparameters affect learning, especially the random matrices. We use the\npseudospectra analysis to investigate further how random matrices impact the\nlearning process. Finally, we discuss the biological plausibility of the\nproposed algorithm, and how it can give rise to better computational models for\nlearning.\n", "versions": [{"version": "v1", "created": "Tue, 19 Jun 2018 18:02:34 GMT"}], "update_date": "2018-06-21", "authors_parsed": [["Detorakis", "Georgios", ""], ["Bartley", "Travis", ""], ["Neftci", "Emre", ""]]}, {"id": "1806.07741", "submitter": "Felix Alexander Heilmeyer", "authors": "Felix A. Heilmeyer, Robin T. Schirrmeister, Lukas D. J. Fiederer,\n  Martin V\\\"olker, Joos Behncke, Tonio Ball", "title": "A large-scale evaluation framework for EEG deep learning architectures", "comments": "7 pages, 3 figures, final version accepted for presentation at IEEE\n  SMC 2018 conference", "journal-ref": null, "doi": "10.1109/SMC.2018.00185", "report-no": null, "categories": "eess.SP cs.LG cs.NE q-bio.NC stat.ML", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  EEG is the most common signal source for noninvasive BCI applications. For\nsuch applications, the EEG signal needs to be decoded and translated into\nappropriate actions. A recently emerging EEG decoding approach is deep learning\nwith Convolutional or Recurrent Neural Networks (CNNs, RNNs) with many\ndifferent architectures already published. Here we present a novel framework\nfor the large-scale evaluation of different deep-learning architectures on\ndifferent EEG datasets. This framework comprises (i) a collection of EEG\ndatasets currently including 100 examples (recording sessions) from six\ndifferent classification problems, (ii) a collection of different EEG decoding\nalgorithms, and (iii) a wrapper linking the decoders to the data as well as\nhandling structured documentation of all settings and (hyper-) parameters and\nstatistics, designed to ensure transparency and reproducibility. As an\napplications example we used our framework by comparing three publicly\navailable CNN architectures: the Braindecode Deep4 ConvNet, Braindecode Shallow\nConvNet, and two versions of EEGNet. We also show how our framework can be used\nto study similarities and differences in the performance of different decoding\nmethods across tasks. We argue that the deep learning EEG framework as\ndescribed here could help to tap the full potential of deep learning for BCI\napplications.\n", "versions": [{"version": "v1", "created": "Mon, 18 Jun 2018 15:49:23 GMT"}, {"version": "v2", "created": "Wed, 25 Jul 2018 15:25:46 GMT"}], "update_date": "2019-01-25", "authors_parsed": [["Heilmeyer", "Felix A.", ""], ["Schirrmeister", "Robin T.", ""], ["Fiederer", "Lukas D. J.", ""], ["V\u00f6lker", "Martin", ""], ["Behncke", "Joos", ""], ["Ball", "Tonio", ""]]}, {"id": "1806.07879", "submitter": "Miguel Aguilera", "authors": "Miguel Aguilera, Ezequiel Di Paolo", "title": "Integrated information in the thermodynamic limit", "comments": "arXiv admin note: substantial text overlap with arXiv:1805.00393", "journal-ref": null, "doi": "10.1016/j.neunet.2019.03.001", "report-no": "Neural Networks, 2019, Volume 114, pp 136-146", "categories": "q-bio.NC cond-mat.dis-nn cond-mat.stat-mech nlin.AO physics.bio-ph", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  The capacity to integrate information is a prominent feature of biological\nand cognitive systems. Integrated Information Theory (IIT) provides a\nmathematical approach to quantify the level of integration in a system, yet its\ncomputational cost generally precludes its applications beyond relatively small\nmodels. In consequence, it is not yet well understood how integration scales up\nwith the size of a system or with different temporal scales of activity, nor\nhow a system maintains its integration as its interacts with its environment.\nHere, we show for the first time how measures of information integration scale\nwhen systems become very large. Using kinetic Ising models and mean-field\napproximations from statistical mechanics, we show that information integration\ndiverges in the thermodynamic limit at certain critical points. Moreover, by\ncomparing different divergent tendencies of blocks of a system at these\ncritical points, we delimit the boundary between an integrated unit and its\nenvironment. Finally, we present a model that adaptively maintains its\nintegration despite changes in its environment by generating a critical surface\nwhere its integrity is preserved. We argue that the exploration of integrated\ninformation for these limit cases helps in addressing a variety of poorly\nunderstood questions about the organization of biological, neural, and\ncognitive systems.\n", "versions": [{"version": "v1", "created": "Wed, 20 Jun 2018 08:30:37 GMT"}, {"version": "v2", "created": "Tue, 3 Jul 2018 11:11:27 GMT"}, {"version": "v3", "created": "Wed, 4 Jul 2018 07:42:45 GMT"}], "update_date": "2020-08-31", "authors_parsed": [["Aguilera", "Miguel", ""], ["Di Paolo", "Ezequiel", ""]]}, {"id": "1806.07903", "submitter": "Gino Del Ferraro", "authors": "Gino Del Ferraro, Andrea Moreno, Byungjoon Min, Flaviano Morone,\n  \\'Ursula P\\'erez-Ram\\'irez, Laura P\\'erez-Cervera, Lucas C. Parra, Andrei\n  Holodny, Santiago Canals, Hern\\'an A. Makse", "title": "Finding influential nodes for integration in brain networks using\n  optimal percolation theory", "comments": "20 pages, 6 figures, Supplementary Info", "journal-ref": "Nature Communications, 9, 2274, (2018)", "doi": "10.1038/s41467-018-04718-3", "report-no": null, "categories": "q-bio.NC cond-mat.dis-nn physics.bio-ph", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Global integration of information in the brain results from complex\ninteractions of segregated brain networks. Identifying the most influential\nneuronal populations that efficiently bind these networks is a fundamental\nproblem of systems neuroscience. Here we apply optimal percolation theory and\npharmacogenetic interventions in-vivo to predict and subsequently target nodes\nthat are essential for global integration of a memory network in rodents. The\ntheory predicts that integration in the memory network is mediated by a set of\nlow-degree nodes located in the nucleus accumbens. This result is confirmed\nwith pharmacogenetic inactivation of the nucleus accumbens, which eliminates\nthe formation of the memory network, while inactivations of other brain areas\nleave the network intact. Thus, optimal percolation theory predicts essential\nnodes in brain networks. This could be used to identify targets of\ninterventions to modulate brain function.\n", "versions": [{"version": "v1", "created": "Wed, 20 Jun 2018 18:01:10 GMT"}], "update_date": "2018-06-22", "authors_parsed": [["Del Ferraro", "Gino", ""], ["Moreno", "Andrea", ""], ["Min", "Byungjoon", ""], ["Morone", "Flaviano", ""], ["P\u00e9rez-Ram\u00edrez", "\u00darsula", ""], ["P\u00e9rez-Cervera", "Laura", ""], ["Parra", "Lucas C.", ""], ["Holodny", "Andrei", ""], ["Canals", "Santiago", ""], ["Makse", "Hern\u00e1n A.", ""]]}, {"id": "1806.07990", "submitter": "David G. Nagy", "authors": "David G. Nagy, Bal\\'azs T\\\"or\\\"ok, Gerg\\H{o} Orb\\'an", "title": "Semantic Compression of Episodic Memories", "comments": "CogSci2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "q-bio.NC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Storing knowledge of an agent's environment in the form of a probabilistic\ngenerative model has been established as a crucial ingredient in a multitude of\ncognitive tasks. Perception has been formalised as probabilistic inference over\nthe state of latent variables, whereas in decision making the model of the\nenvironment is used to predict likely consequences of actions. Such generative\nmodels have earlier been proposed to underlie semantic memory but it remained\nunclear if this model also underlies the efficient storage of experiences in\nepisodic memory. We formalise the compression of episodes in the normative\nframework of information theory and argue that semantic memory provides the\ndistortion function for compression of experiences. Recent advances and\ninsights from machine learning allow us to approximate semantic compression in\nnaturalistic domains and contrast the resulting deviations in compressed\nepisodes with memory errors observed in the experimental literature on human\nmemory.\n", "versions": [{"version": "v1", "created": "Wed, 20 Jun 2018 21:19:47 GMT"}], "update_date": "2018-06-22", "authors_parsed": [["Nagy", "David G.", ""], ["T\u00f6r\u00f6k", "Bal\u00e1zs", ""], ["Orb\u00e1n", "Gerg\u0151", ""]]}, {"id": "1806.08001", "submitter": "Hector Vasquez", "authors": "Hector G. Vasquez and Giovanni Zocchi", "title": "Analog control with two Artificial Axons", "comments": "8 pages, 11 figures", "journal-ref": null, "doi": "10.1088/1748-3190/aaf123", "report-no": null, "categories": "physics.bio-ph q-bio.NC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The artificial axon is a recently introduced synthetic assembly of supported\nlipid bilayers and voltage gated ion channels, displaying the basic\nelectrophysiology of nerve cells. Here we demonstrate the use of two artificial\naxons as control elements to achieve a simple task. Namely, we steer a remote\ncontrol car towards a light source, using the sensory input dependent firing\nrate of the axons as the control signal for turning left or right. We present\nthe result in the form of the analysis of a movie of the car approaching the\nlight source. In general terms, with this work we pursue a constructivist\napproach to exploring the nexus between machine language at the nerve cell\nlevel and behavior.\n", "versions": [{"version": "v1", "created": "Wed, 20 Jun 2018 21:50:54 GMT"}], "update_date": "2019-03-06", "authors_parsed": [["Vasquez", "Hector G.", ""], ["Zocchi", "Giovanni", ""]]}, {"id": "1806.08167", "submitter": "Thierry Mora", "authors": "Christophe Gardella, Olivier Marre, Thierry Mora", "title": "Modeling the correlated activity of neural populations: A review", "comments": null, "journal-ref": "Neural Computation 31(2) 233-269 (2019)", "doi": "10.1162/neco_a_01154", "report-no": null, "categories": "q-bio.NC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The principles of neural encoding and computations are inherently collective\nand usually involve large populations of interacting neurons with highly\ncorrelated activities. While theories of neural function have long recognized\nthe importance of collective effects in populations of neurons, only in the\npast two decades has it become possible to record from many cells\nsimulatenously using advanced experimental techniques with single-spike\nresolution, and to relate these correlations to function and behaviour. This\nreview focuses on the modeling and inference approaches that have been recently\ndeveloped to describe the correlated spiking activity of populations of\nneurons. We cover a variety of models describing correlations between pairs of\nneurons as well as between larger groups, synchronous or delayed in time, with\nor without the explicit influence of the stimulus, and including or not latent\nvariables. We discuss the advantages and drawbacks or each method, as well as\nthe computational challenges related to their application to recordings of ever\nlarger populations.\n", "versions": [{"version": "v1", "created": "Thu, 21 Jun 2018 11:00:13 GMT"}], "update_date": "2019-05-14", "authors_parsed": [["Gardella", "Christophe", ""], ["Marre", "Olivier", ""], ["Mora", "Thierry", ""]]}, {"id": "1806.08634", "submitter": "Juan Eugenio Iglesias", "authors": "Juan Eugenio Iglesias, Ricardo Insausti, Garikoitz Lerma-Usabiaga,\n  Martina Bocchetta, Koen Van Leemput, Douglas N Greve, Andre van der Kouwe,\n  Bruce Fischl, Cesar Caballero-Gaudes, Pedro M Paz-Alonso", "title": "A probabilistic atlas of the human thalamic nuclei combining ex vivo MRI\n  and histology", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "q-bio.NC cs.CV physics.med-ph", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The human thalamus is a brain structure that comprises numerous, highly\nspecific nuclei. Since these nuclei are known to have different functions and\nto be connected to different areas of the cerebral cortex, it is of great\ninterest for the neuroimaging community to study their volume, shape and\nconnectivity in vivo with MRI. In this study, we present a probabilistic atlas\nof the thalamic nuclei built using ex vivo brain MRI scans and histological\ndata, as well as the application of the atlas to in vivo MRI segmentation. The\natlas was built using manual delineation of 26 thalamic nuclei on the serial\nhistology of 12 whole thalami from six autopsy samples, combined with manual\nsegmentations of the whole thalamus and surrounding structures (caudate,\nputamen, hippocampus, etc.) made on in vivo brain MR data from 39 subjects. The\n3D structure of the histological data and corresponding manual segmentations\nwas recovered using the ex vivo MRI as reference frame, and stacks of blockface\nphotographs acquired during the sectioning as intermediate target. The atlas,\nwhich was encoded as an adaptive tetrahedral mesh, shows a good agreement with\nwith previous histological studies of the thalamus in terms of volumes of\nrepresentative nuclei. When applied to segmentation of in vivo scans using\nBayesian inference, the atlas shows excellent test-retest reliability,\nrobustness to changes in input MRI contrast, and ability to detect differential\nthalamic effects in subjects with Alzheimer's disease. The probabilistic atlas\nand companion segmentation tool are publicly available as part of the\nneuroimaging package FreeSurfer.\n", "versions": [{"version": "v1", "created": "Fri, 22 Jun 2018 12:42:37 GMT"}], "update_date": "2018-06-25", "authors_parsed": [["Iglesias", "Juan Eugenio", ""], ["Insausti", "Ricardo", ""], ["Lerma-Usabiaga", "Garikoitz", ""], ["Bocchetta", "Martina", ""], ["Van Leemput", "Koen", ""], ["Greve", "Douglas N", ""], ["van der Kouwe", "Andre", ""], ["Fischl", "Bruce", ""], ["Caballero-Gaudes", "Cesar", ""], ["Paz-Alonso", "Pedro M", ""]]}, {"id": "1806.09334", "submitter": "Hamed Heidari-Gorji", "authors": "Hamed Heidari Gorji (1 and 2), Sajjad Zabbah (2), Reza Ebrahimpour (1\n  and 2) ((1) Faculty of Computer Engineering, Shahid Rajaee Teacher Training\n  University, Tehran, Iran (2) School of Cognitive Sciences, Institute for\n  Research in Fundamental Sciences, Tehran, Iran)", "title": "A temporal neural network model for object recognition using a\n  biologically plausible decision making layer", "comments": "Version 2 contains more details about model. Comparisons with some\n  known deep neural networks have been included and are shown in figure 7. text\n  was corrected and edited", "journal-ref": null, "doi": null, "report-no": null, "categories": "q-bio.NC cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Brain can recognize different objects as ones that it has experienced before.\nThe recognition accuracy and its processing time depend on task properties such\nas viewing condition, level of noise and etc. Recognition accuracy can be well\nexplained by different models. However, less attention has been paid to the\nprocessing time and the ones that do, are not biologically plausible. By\nextracting features temporally as well as utilizing an accumulation to bound\ndecision making model, an object recognition model accounting for both\nrecognition time and accuracy is proposed. To temporally extract informative\nfeatures in support of possible classes of stimuli, a hierarchical spiking\nneural network, called spiking HMAX is modified. In the decision making part of\nthe model the extracted information accumulates over time using accumulator\nunits. The input category is determined as soon as any of the accumulators\nreaches a threshold, called decision bound. Results show that not only does the\nmodel follow human accuracy in a psychophysics task better than the classic\nspiking HMAX model, but also it predicts human response time in each choice.\nResults provide enough evidence that temporal representation of features are\ninformative since they can improve the accuracy of a biological plausible\ndecision maker over time. This is also in line with the well-known idea of\nspeed accuracy trade-off in decision making studies.\n", "versions": [{"version": "v1", "created": "Mon, 25 Jun 2018 09:04:28 GMT"}, {"version": "v2", "created": "Thu, 22 Nov 2018 18:24:50 GMT"}], "update_date": "2018-11-27", "authors_parsed": [["Gorji", "Hamed Heidari", "", "1 and 2"], ["Zabbah", "Sajjad", "", "1\n  and 2"], ["Ebrahimpour", "Reza", "", "1\n  and 2"]]}, {"id": "1806.09373", "submitter": "Pedro Mediano", "authors": "Pedro A.M. Mediano, Anil K. Seth and Adam B. Barrett", "title": "Measuring Integrated Information: Comparison of Candidate Measures in\n  Theory and Simulation", "comments": null, "journal-ref": null, "doi": "10.3390/e21010017", "report-no": null, "categories": "q-bio.NC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Integrated Information Theory (IIT) is a prominent theory of consciousness\nthat has at its centre measures that quantify the extent to which a system\ngenerates more information than the sum of its parts. While several candidate\nmeasures of integrated information (`$\\Phi$') now exist, little is known about\nhow they compare, especially in terms of their behaviour on non-trivial network\nmodels. In this article we provide clear and intuitive descriptions of six\ndistinct candidate measures. We then explore the properties of each of these\nmeasures in simulation on networks consisting of eight interacting nodes,\nanimated with Gaussian linear autoregressive dynamics. We find a striking\ndiversity in the behaviour of these measures -- no two measures show consistent\nagreement across all analyses. Further, only a subset of the measures appear to\ngenuinely reflect some form of dynamical complexity, in the sense of\nsimultaneous segregation and integration between system components. Our results\nhelp guide the operationalisation of IIT and advance the development of\nmeasures of integrated information that may have more general applicability.\n", "versions": [{"version": "v1", "created": "Mon, 25 Jun 2018 10:37:48 GMT"}], "update_date": "2019-01-30", "authors_parsed": [["Mediano", "Pedro A. M.", ""], ["Seth", "Anil K.", ""], ["Barrett", "Adam B.", ""]]}, {"id": "1806.09532", "submitter": "Joos Behncke", "authors": "Joos Behncke, Robin Tibor Schirrmeister, Martin V\\\"olker, Ji\\v{r}\\'i\n  Hammer, Petr Marusi\\v{c}, Andreas Schulze-Bonhage, Wolfram Burgard, Tonio\n  Ball", "title": "Cross-paradigm pretraining of convolutional networks improves\n  intracranial EEG decoding", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "q-bio.NC cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  When it comes to the classification of brain signals in real-life\napplications, the training and the prediction data are often described by\ndifferent distributions. Furthermore, diverse data sets, e.g., recorded from\nvarious subjects or tasks, can even exhibit distinct feature spaces. The fact\nthat data that have to be classified are often only available in small amounts\nreinforces the need for techniques to generalize learned information, as\nperformances of brain-computer interfaces (BCIs) are enhanced by increasing\nquantity of available data. In this paper, we apply transfer learning to a\nframework based on deep convolutional neural networks (deep ConvNets) to prove\nthe transferability of learned patterns in error-related brain signals across\ndifferent tasks. The experiments described in this paper demonstrate the\nusefulness of transfer learning, especially improving performances when only\nlittle data can be used to distinguish between erroneous and correct\nrealization of a task. This effect could be delimited from a transfer of merely\ngeneral brain signal characteristics, underlining the transfer of\nerror-specific information. Furthermore, we could extract similar patterns in\ntime-frequency analyses in identical channels, leading to selective high signal\ncorrelations between the two different paradigms. Classification on the\nintracranial data yields in median accuracies up to $(81.50 \\pm 9.49)\\,\\%$.\nDecoding on only $10\\%$ of the data without pre-training reaches performances\nof $(54.76 \\pm 3.56)\\,\\%$, compared to $(64.95 \\pm 0.79)\\,\\%$ with\npre-training.\n", "versions": [{"version": "v1", "created": "Wed, 20 Jun 2018 11:34:36 GMT"}, {"version": "v2", "created": "Fri, 20 Jul 2018 10:18:28 GMT"}], "update_date": "2018-07-23", "authors_parsed": [["Behncke", "Joos", ""], ["Schirrmeister", "Robin Tibor", ""], ["V\u00f6lker", "Martin", ""], ["Hammer", "Ji\u0159\u00ed", ""], ["Marusi\u010d", "Petr", ""], ["Schulze-Bonhage", "Andreas", ""], ["Burgard", "Wolfram", ""], ["Ball", "Tonio", ""]]}, {"id": "1806.09559", "submitter": "Joaquin Rapela", "authors": "Joaquin Rapela", "title": "Traveling waves appear and disappear in unison with produced speech", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "q-bio.NC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In Rapela (2016) we reported traveling waves (TWs) on electrocorticographic\n(ECoG) recordings from an epileptic subject over speech processing brain\nregions, while the subject rhythmically produced consonant-vowel syllables\n(CVSs). In Rapela (2017) we showed that TWs are precisely synchronized, in\ndynamical systems terms, to these productions. Here we show that TWs do not\noccur continuously, but tend to appear before the initiation of CVSs and tend\nto disappear before their termination. During moments of silence, between\nproductions of CVSs, TWs tend to reverse direction. To our knowledge, this is\nthe first study showing TWs related to the production of speech and, more\ngenerally, the first report of behavioral correlates of mesoscale TWs in\nbehaving humans.\n", "versions": [{"version": "v1", "created": "Mon, 25 Jun 2018 16:44:05 GMT"}], "update_date": "2018-06-26", "authors_parsed": [["Rapela", "Joaquin", ""]]}, {"id": "1806.09963", "submitter": "Elke Kirschbaum", "authors": "Elke Kirschbaum, Manuel Hau{\\ss}mann, Steffen Wolf, Hannah Sonntag,\n  Justus Schneider, Shehabeldin Elzoheiry, Oliver Kann, Daniel Durstewitz, Fred\n  A. Hamprecht", "title": "LeMoNADe: Learned Motif and Neuronal Assembly Detection in calcium\n  imaging videos", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "q-bio.NC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Neuronal assemblies, loosely defined as subsets of neurons with reoccurring\nspatio-temporally coordinated activation patterns, or \"motifs\", are thought to\nbe building blocks of neural representations and information processing. We\nhere propose LeMoNADe, a new exploratory data analysis method that facilitates\nhunting for motifs in calcium imaging videos, the dominant microscopic\nfunctional imaging modality in neurophysiology. Our nonparametric method\nextracts motifs directly from videos, bypassing the difficult intermediate step\nof spike extraction. Our technique augments variational autoencoders with a\ndiscrete stochastic node, and we show in detail how a differentiable\nreparametrization and relaxation can be used. An evaluation on simulated data,\nwith available ground truth, reveals excellent quantitative performance. In\nreal video data acquired from brain slices, with no ground truth available,\nLeMoNADe uncovers nontrivial candidate motifs that can help generate hypotheses\nfor more focused biological investigations.\n", "versions": [{"version": "v1", "created": "Tue, 26 Jun 2018 13:21:48 GMT"}, {"version": "v2", "created": "Tue, 2 Oct 2018 05:44:00 GMT"}, {"version": "v3", "created": "Fri, 22 Feb 2019 12:03:29 GMT"}], "update_date": "2019-02-25", "authors_parsed": [["Kirschbaum", "Elke", ""], ["Hau\u00dfmann", "Manuel", ""], ["Wolf", "Steffen", ""], ["Sonntag", "Hannah", ""], ["Schneider", "Justus", ""], ["Elzoheiry", "Shehabeldin", ""], ["Kann", "Oliver", ""], ["Durstewitz", "Daniel", ""], ["Hamprecht", "Fred A.", ""]]}, {"id": "1806.10161", "submitter": "Yarden Katz", "authors": "Yarden Katz, Michael Springer, Walter Fontana", "title": "Embodying probabilistic inference in biochemical circuits", "comments": "11 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "q-bio.MN q-bio.NC", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Probabilistic inference provides a language for describing how organisms may\nlearn from and adapt to their environment. The computations needed to implement\nprobabilistic inference often require specific representations, akin to having\nthe suitable data structures for implementing certain algorithms in computer\nprogramming. Yet it is unclear how such representations can be instantiated in\nthe stochastic, parallel-running biochemical machinery found in cells (such as\nsingle-celled organisms). Here, we show how representations for supporting\ninference in Markov models can be embodied in cellular circuits, by combining a\nconcentration-dependent scheme for encoding probabilities with a mechanism for\ndirectional counting. We show how the logic of protein production and\ndegradation constrains the computation we set out to implement. We argue that\nthis process by which an abstract computation is shaped by its biochemical\nrealization strikes a compromise between \"rationalistic\" information-processing\nperspectives and alternative approaches that emphasize embodiment.\n", "versions": [{"version": "v1", "created": "Tue, 26 Jun 2018 18:28:30 GMT"}], "update_date": "2018-06-28", "authors_parsed": [["Katz", "Yarden", ""], ["Springer", "Michael", ""], ["Fontana", "Walter", ""]]}, {"id": "1806.10181", "submitter": "Dmitry Krotov", "authors": "Dmitry Krotov, John Hopfield", "title": "Unsupervised Learning by Competing Hidden Units", "comments": null, "journal-ref": "Proceedings of the National Academy of Sciences of the USA, 116\n  (16) 7723-7731 (2019)", "doi": "10.1073/pnas.1820458116", "report-no": null, "categories": "cs.LG cs.CV cs.NE q-bio.NC stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  It is widely believed that the backpropagation algorithm is essential for\nlearning good feature detectors in early layers of artificial neural networks,\nso that these detectors are useful for the task performed by the higher layers\nof that neural network. At the same time, the traditional form of\nbackpropagation is biologically implausible. In the present paper we propose an\nunusual learning rule, which has a degree of biological plausibility, and which\nis motivated by Hebb's idea that change of the synapse strength should be local\n- i.e. should depend only on the activities of the pre and post synaptic\nneurons. We design a learning algorithm that utilizes global inhibition in the\nhidden layer, and is capable of learning early feature detectors in a\ncompletely unsupervised way. These learned lower layer feature detectors can be\nused to train higher layer weights in a usual supervised way so that the\nperformance of the full network is comparable to the performance of standard\nfeedforward networks trained end-to-end with a backpropagation algorithm.\n", "versions": [{"version": "v1", "created": "Tue, 26 Jun 2018 19:32:58 GMT"}, {"version": "v2", "created": "Thu, 29 Aug 2019 02:36:17 GMT"}], "update_date": "2019-08-30", "authors_parsed": [["Krotov", "Dmitry", ""], ["Hopfield", "John", ""]]}, {"id": "1806.10228", "submitter": "Naoki Masuda Dr.", "authors": "Naoki Masuda, Michiko Sakaki, Takahiro Ezaki, Takamitsu Watanabe", "title": "Clustering coefficients for correlation networks", "comments": "5 figures, 2 tables; abstract is made shorter than in the journal\n  version due to the length limit", "journal-ref": "Frontiers in Neuroinformatics, 12, 7 (2018)", "doi": "10.3389/fninf.2018.00007", "report-no": null, "categories": "physics.soc-ph q-bio.NC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The clustering coefficient quantifies the abundance of connected triangles in\na network and is a major descriptive statistics of networks. For example, it\nfinds an application in the assessment of small-worldness of brain networks,\nwhich is affected by attentional and cognitive conditions, age, psychiatric\ndisorders and so forth. However, it remains unclear how the clustering\ncoefficient should be measured in a correlation-based network, which is among\nmajor representations of brain networks. In the present article, we propose\nclustering coefficients tailored to correlation matrices. The key idea is to\nuse three-way partial correlation or partial mutual information to measure the\nstrength of the association between the two neighbouring nodes of a focal node\nrelative to the amount of pseudo-correlation expected from indirect paths\nbetween the nodes. Our method avoids the difficulties of previous applications\nof clustering coefficient (and other) measures in defining correlational\nnetworks, i.e., thresholding on the correlation value, discarding of negative\ncorrelation values, the pseudo-correlation problem and full partial correlation\nmatrices whose estimation is computationally difficult. For proof of concept,\nwe apply the proposed clustering coefficient measures to functional magnetic\nresonance imaging data obtained from healthy participants of various ages and\ncompare them with conventional clustering coefficients. We show that the\nclustering coefficients decline with the age. The proposed clustering\ncoefficients are more strongly correlated with age than the conventional ones\nare. We also show that the local variants of the proposed clustering\ncoefficients are useful in characterising individual nodes. In contrast, the\nconventional local clustering coefficients were strongly correlated with and\ntherefore may be confounded by the node's connectivity.\n", "versions": [{"version": "v1", "created": "Tue, 26 Jun 2018 22:05:48 GMT"}], "update_date": "2018-06-28", "authors_parsed": [["Masuda", "Naoki", ""], ["Sakaki", "Michiko", ""], ["Ezaki", "Takahiro", ""], ["Watanabe", "Takamitsu", ""]]}, {"id": "1806.10409", "submitter": "Gianluca Susi PhD", "authors": "Gianluca Susi, Luis Anton Toro, Leonides Canuet, Maria Eugenia Lopez,\n  Fernando Maestu, Claudio R. Mirasso, Ernesto Pereda", "title": "A neuro-inspired system for online learning and recognition of parallel\n  spike trains, based on spike latency and heterosynaptic STDP", "comments": "Submitted to Frontiers in Neuroscience", "journal-ref": null, "doi": null, "report-no": null, "categories": "q-bio.NC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Humans perform remarkably well in many cognitive tasks including pattern\nrecognition. However, the neuronal mechanisms underlying this process are not\nwell understood. Nevertheless, artificial neural networks, inspired in brain\ncircuits, have been designed and used to tackle spatio-temporal pattern\nrecognition tasks. In this paper we present a multineuronal spike pattern\ndetection structure able to autonomously implement online learning and\nrecognition of parallel spike sequences (i.e., sequences of pulses belonging to\ndifferent neurons/neural ensembles). The operating principle of this structure\nis based on two spiking/synaptic neurocomputational characteristics: spike\nlatency, that enables neurons to fire spikes with a certain delay and\nheterosynaptic plasticity, that allows the own regulation of synaptic weights.\nFrom the perspective of the information representation, the structure allows\nmapping a spatio-temporal stimulus into a multidimensional, temporal, feature\nspace. In this space, the parameter coordinate and the time at which a neuron\nfires represent one specific feature. In this sense, each feature can be\nconsidered to span a single temporal axis. We applied our proposed scheme to\nexperimental data obtained from a motor inhibitory cognitive task. The test\nexhibits good classification performance, indicating the adequateness of our\napproach. In addition to its effectiveness, its simplicity and low\ncomputational cost suggest a large scale implementation for real time\nrecognition applications in several areas, such as brain computer interface,\npersonal biometrics authentication or early detection of diseases.\n", "versions": [{"version": "v1", "created": "Wed, 27 Jun 2018 10:56:55 GMT"}], "update_date": "2018-06-28", "authors_parsed": [["Susi", "Gianluca", ""], ["Toro", "Luis Anton", ""], ["Canuet", "Leonides", ""], ["Lopez", "Maria Eugenia", ""], ["Maestu", "Fernando", ""], ["Mirasso", "Claudio R.", ""], ["Pereda", "Ernesto", ""]]}, {"id": "1806.10428", "submitter": "Jos\\'e Antonio Villacorta-Atienza", "authors": "Jose Antonio Villacorta-Atienza (1,2), Carlos Calvo-Tapia (2), Sergio\n  Diez-Hermano (1), Abel Sanchez-Jimenez (1,2), Sergey Lobov (3), Nadia Krilova\n  (3), Antonio Murciano (1), Gabriela Lopez-Tolsa (4), Ricardo Pellon (4),\n  Valeri Makarov (2) ((1) Biomathematics Unit (BEE Department), Faculty of\n  Biology, Complutense University of Madrid, Spain, (2) Institute of\n  Interdisciplinary Mathematics (IMI), Faculty of Mathematics, Complutense\n  University of Madrid, Spain, (3) Lobachevsky State University of Nizhny\n  Novgorod, Russia, (4) Department of Basic Psychology, Universidad de\n  Educaci\\'on a Distancia (UNED), Spain)", "title": "Static Internal Representation Of Dynamic Situations Reveals Time\n  Compaction In Human Cognition", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "q-bio.NC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The time-changing nature of our world demands processing of huge amounts of\ninformation in fast and reliable way to generate successful behaviors.\nTherefore, significant brain resources are devoted to process spatiotemporal\ninformation. Neural basis of spatial processing and their cognitive correlates\nare well established mostly for static environments. Nonetheless, in\ntime-changing situations the brain exploits specific processing mechanisms for\ntemporal information based on prediction and anticipation, as time compression\nduring visual perception and mental navigation. Alternative hypothesis of time\ncompaction integrates both views, postulating that dynamic situations are\ninternally represented as static spatial maps where temporal information is\nextracted by predicting and structuring the relevant interactions.\nNevertheless, empirical approaches tackling the biological soundness of time\ncompaction are still lacking. Here we show that performance in a discrimination\nlearning task involving dynamic situations can be either favored or hampered\nvia previous exposition to interfering static scenes. In this sense, men were\neffectively conditioned in contrast to a control group, in coherence with the\nhypothesis. Meanwhile, women performed on par with control men, regardless of\nthe previous conditioning. This suggests time compaction is a salient cognitive\nstrategy in men when dealing with dynamic situations, while women seem to rely\non a broader range of information processing strategies. Finally, we further\ncorroborated the time compaction mechanism involved in these experimental\nfindings through a mathematical model of the experimental process. Our results\npoint to some form of static internal representation mechanism at cognitive\nlevel involved in decision-making and strategy planning in dynamic situations\n[...]\n", "versions": [{"version": "v1", "created": "Wed, 27 Jun 2018 12:10:56 GMT"}], "update_date": "2018-06-28", "authors_parsed": [["Villacorta-Atienza", "Jose Antonio", ""], ["Calvo-Tapia", "Carlos", ""], ["Diez-Hermano", "Sergio", ""], ["Sanchez-Jimenez", "Abel", ""], ["Lobov", "Sergey", ""], ["Krilova", "Nadia", ""], ["Murciano", "Antonio", ""], ["Lopez-Tolsa", "Gabriela", ""], ["Pellon", "Ricardo", ""], ["Makarov", "Valeri", ""]]}, {"id": "1806.11039", "submitter": "Maximilian Voit", "authors": "Maximilian Voit and Hildegard Meyer-Ortmanns", "title": "A hierarchical heteroclinic network: Controlling the time evolution\n  along its paths", "comments": "15 pages, 9 figures; submitted to EPJ ST", "journal-ref": null, "doi": "10.1140/epjst/e2018-800040-x", "report-no": null, "categories": "nlin.AO math.DS q-bio.NC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider a heteroclinic network in the framework of winnerless competition\nof species. It consists of two levels of heteroclinic cycles. On the lower\nlevel, the heteroclinic cycle connects three saddles, each representing the\nsurvival of a single species; on the higher level, the cycle connects three\nsuch heteroclinic cycles, in which nine species are involved. We show how to\ntune the predation rates in order to generate the long time scales on the\nhigher level from the shorter time scales on the lower level. Moreover, when we\ntune a single bifurcation parameter, first the motion along the lower and next\nalong the higher-level heteroclinic cycles are replaced by a heteroclinic cycle\nbetween 3-species coexistence-fixed points and by a 9-species coexistence-fixed\npoint, respectively. We also observe a similar impact of additive noise. Beyond\nits usual role of preventing the slowing-down of heteroclinic trajectories at\nsmall noise level, its increasing strength can replace the lower-level\nheteroclinic cycle by 3-species coexistence fixed-points, connected by an\neffective limit cycle, and for even stronger noise the trajectories converge to\nthe 9-species coexistence-fixed point. The model has applications to systems in\nwhich slow oscillations modulate fast oscillations with sudden transitions\nbetween the temporary winners.\n", "versions": [{"version": "v1", "created": "Thu, 28 Jun 2018 15:40:09 GMT"}], "update_date": "2018-12-26", "authors_parsed": [["Voit", "Maximilian", ""], ["Meyer-Ortmanns", "Hildegard", ""]]}, {"id": "1806.11077", "submitter": "Ihor Lubashevsky", "authors": "Ihor Lubashevsky", "title": "Psychophysical laws as reflection of mental space properties", "comments": null, "journal-ref": null, "doi": "10.1016/j.plrev.2018.10.003", "report-no": null, "categories": "q-bio.NC eess.IV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The paper is devoted to the relationship between psychophysics and physics of\nmind. The basic trends in psychophysics development are briefly discussed with\nspecial attention focused on Teghtsoonian's hypotheses. These hypotheses pose\nthe concept of the universality of inner psychophysics and enable to speak\nabout psychological space as an individual object with its own properties.\nTurning to the two-component description of human behavior (I. Lubashevsky,\nPhysics of the Human Mind, Springer, 2017) the notion of mental space is\nformulated and human perception of external stimuli is treated as the emergence\nof the corresponding images in the mental space. On one hand, these images are\ncaused by external stimuli and their magnitude bears the information about the\nintensity of the corresponding stimuli. On the other hand, the individual\nstructure of such images as well as their subsistence after emergence is\ndetermined only by the properties of mental space on its own. Finally, the\nmental operations of image comparison and their scaling are defined in a way\nallowing for the bounded capacity of human cognition. As demonstrated, the\ndeveloped theory of stimulus perception is able to explain the basic\nregularities of psychophysics, e.g., (i) the regression and range effects\nleading to the overestimation of weak stimuli and the underestimation of strong\nstimuli, (ii) scalar variability (Weber's and Ekman' laws), and (\\textit{iii})\nthe sequential (memory) effects. As the final result, a solution to the\nFechner-Stevens dilemma is proposed. This solution posits that Fechner's\nlogarithmic law is not a consequences of Weber's law but stems from the\ninterplay of uncertainty in evaluating stimulus intensities and the multi-step\nscaling required to overcome the stimulus incommensurability.\n", "versions": [{"version": "v1", "created": "Wed, 27 Jun 2018 11:17:04 GMT"}], "update_date": "2020-01-29", "authors_parsed": [["Lubashevsky", "Ihor", ""]]}]