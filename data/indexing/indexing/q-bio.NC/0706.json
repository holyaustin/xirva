[{"id": "0706.0077", "submitter": "Bruno. Cessac", "authors": "B. Cessac", "title": "A discrete time neural network model with spiking neurons. Rigorous\n  results on the spontaneous dynamics", "comments": "56 pages, 1 Figure, to appear in Journal of Mathematical Biology", "journal-ref": "Journal of Mathematical Biology, Volume 56, Number 3, 311-345\n  (2008).", "doi": null, "report-no": null, "categories": "math.DS nlin.CD q-bio.NC", "license": null, "abstract": "  We derive rigorous results describing the asymptotic dynamics of a discrete\ntime model of spiking neurons introduced in \\cite{BMS}. Using symbolic dynamic\ntechniques we show how the dynamics of membrane potential has a one to one\ncorrespondence with sequences of spikes patterns (``raster plots''). Moreover,\nthough the dynamics is generically periodic, it has a weak form of initial\nconditions sensitivity due to the presence of a sharp threshold in the model\ndefinition. As a consequence, the model exhibits a dynamical regime\nindistinguishable from chaos in numerical experiments.\n", "versions": [{"version": "v1", "created": "Fri, 1 Jun 2007 06:49:55 GMT"}], "update_date": "2008-02-12", "authors_parsed": [["Cessac", "B.", ""]]}, {"id": "0706.0163", "submitter": "Alexander K. Vidybida", "authors": "Alexander K. Vidybida", "title": "Output Stream of Binding Neuron with Feedback", "comments": "Version #1: 4 pages, 5 figures, manuscript submitted to Biological\n  Cybernetics. Version #2 (this version): added 3 pages of new text with\n  additional analytical and numerical calculations, 2 more figures, 11 more\n  references, added Discussion section", "journal-ref": "Eur. Phys. J. B 65, 577-584 (2008); Eur. Phys. J. B 69, 313 (2009)", "doi": "10.1140/epjb/e2008-00360-1", "report-no": null, "categories": "q-bio.NC q-bio.OT", "license": null, "abstract": "  The binding neuron model is inspired by numerical simulation of\nHodgkin-Huxley-type point neuron, as well as by the leaky integrate-and-fire\nmodel. In the binding neuron, the trace of an input is remembered for a fixed\nperiod of time after which it disappears completely. This is in the contrast\nwith the above two models, where the postsynaptic potentials decay\nexponentially and can be forgotten only after triggering. The finiteness of\nmemory in the binding neuron allows one to construct fast recurrent networks\nfor computer modeling. Recently, the finiteness is utilized for exact\nmathematical description of the output stochastic process if the binding neuron\nis driven with the Poissonian input stream. In this paper, the simplest\nnetworking is considered for binding neuron. Namely, it is expected that every\noutput spike of single neuron is immediately fed into its input. For this\nconstruction, externally fed with Poissonian stream, the output stream is\ncharacterized in terms of interspike interval probability density distribution\nif the binding neuron has threshold 2. For higher thresholds, the distribution\nis calculated numerically. The distributions are compared with those found for\nbinding neuron without feedback, and for leaky integrator. Sample distributions\nfor leaky integrator with feedback are calculated numerically as well. It is\noncluded that even the simplest networking can radically alter spikng\nstatistics. Information condensation at the level of single neuron is\ndiscussed.\n", "versions": [{"version": "v1", "created": "Fri, 1 Jun 2007 14:20:19 GMT"}, {"version": "v2", "created": "Tue, 25 Sep 2007 15:00:26 GMT"}], "update_date": "2011-07-20", "authors_parsed": [["Vidybida", "Alexander K.", ""]]}, {"id": "0706.1306", "submitter": "Alain Destexhe", "authors": "Zuzanna Piwkowska, Martin Pospischil, Romain Brette, Julia Sliwa,\n  Michelle Rudolph-Lilith, Thierry Bal and Alain Destexhe", "title": "Characterizing synaptic conductance fluctuations in cortical neurons and\n  their influence on spike generation", "comments": "9 figures, Journal of Neuroscience Methods (in press, 2008)", "journal-ref": "Journal of Neuroscience Methods 169: 302-322, 2008.", "doi": null, "report-no": null, "categories": "q-bio.NC", "license": null, "abstract": "  Cortical neurons are subject to sustained and irregular synaptic activity\nwhich causes important fluctuations of the membrane potential (Vm). We review\nhere different methods to characterize this activity and its impact on spike\ngeneration. The simplified, fluctuating point-conductance model of synaptic\nactivity provides the starting point of a variety of methods for the analysis\nof intracellular Vm recordings. In this model, the synaptic excitatory and\ninhibitory conductances are described by Gaussian-distributed stochastic\nvariables, or colored conductance noise. The matching of experimentally\nrecorded Vm distributions to an invertible theoretical expression derived from\nthe model allows the extraction of parameters characterizing the synaptic\nconductance distributions. This analysis can be complemented by the matching of\nexperimental Vm power spectral densities (PSDs) to a theoretical template, even\nthough the unexpected scaling properties of experimental PSDs limit the\nprecision of this latter approach. Building on this stochastic characterization\nof synaptic activity, we also propose methods to qualitatively and\nquantitatively evaluate spike-triggered averages of synaptic time-courses\npreceding spikes. This analysis points to an essential role for synaptic\nconductance variance in determining spike times. The presented methods are\nevaluated using controlled conductance injection in cortical neurons in vitro\nwith the dynamic-clamp technique. We review their applications to the analysis\nof in vivo intracellular recordings in cat association cortex, which suggest a\npredominant role for inhibition in determining both sub- and supra-threshold\ndynamics of cortical neurons embedded in active networks.\n", "versions": [{"version": "v1", "created": "Sat, 9 Jun 2007 12:32:51 GMT"}, {"version": "v2", "created": "Thu, 15 Nov 2007 20:52:01 GMT"}, {"version": "v3", "created": "Thu, 15 Nov 2007 21:28:00 GMT"}], "update_date": "2009-04-29", "authors_parsed": [["Piwkowska", "Zuzanna", ""], ["Pospischil", "Martin", ""], ["Brette", "Romain", ""], ["Sliwa", "Julia", ""], ["Rudolph-Lilith", "Michelle", ""], ["Bal", "Thierry", ""], ["Destexhe", "Alain", ""]]}, {"id": "0706.1611", "submitter": "Vladislav Volman", "authors": "Vladislav Volman, Richard Gerkin, Pak-Ming Lau, Eshel Ben-Jacob, and\n  Guo-Qiang Bi", "title": "Calcium and synaptic dynamics underlying reverberatory activity in\n  neuronal networks", "comments": null, "journal-ref": "Physical Biology, vol.4, pp.91-103, 2007", "doi": "10.1088/1478-3975/4/2/003", "report-no": null, "categories": "q-bio.NC q-bio.PE", "license": null, "abstract": "  Persistent activity is postulated to drive neural network plasticity and\nlearning. To investigate its underlying cellular mechanisms, we developed a\nbiophysically tractable model that explains the emergence, sustenance, and\neventual termination of short-term persistent activity. Using the model, we\nreproduced the features of reverberating activity that were observed in small\n(50-100 cells) networks of cultured hippocampal neurons, such as the appearance\nof polysynaptic current clusters, the typical inter-cluster intervals, the\ntypical duration of reverberation, and the response to changes in\nextra-cellular ionic composition. The model relies on action\npotential-triggered residual presynaptic calcium, which we suggest plays an\nimportant role in sustaining reverberations. We show that reverberatory\nactivity is maintained by enhanced asynchronous transmitter release from\npre-synaptic terminals, which in itself depends on the dynamics of residual\npresynaptic calcium. Hence, asynchronous release, rather than being a \"synaptic\nnoise\", can play an important role in network dynamics. Additionally, we found\nthat a fast timescale synaptic depression is responsible for oscillatory\nnetwork activation during reverberations, whereas the onset of a slow timescale\ndepression leads to the termination of reverberation. The simplicity of our\nmodel enabled a number of predictions that were confirmed by additional\nanalyses of experimental manipulations.\n", "versions": [{"version": "v1", "created": "Tue, 12 Jun 2007 06:14:07 GMT"}], "update_date": "2009-11-13", "authors_parsed": [["Volman", "Vladislav", ""], ["Gerkin", "Richard", ""], ["Lau", "Pak-Ming", ""], ["Ben-Jacob", "Eshel", ""], ["Bi", "Guo-Qiang", ""]]}, {"id": "0706.2602", "submitter": "Hugues Berry", "authors": "Benoit Siri (INRIA Futurs), Mathias Quoy (ETIS), Bruno Delord (ANIM),\n  Bruno Cessac (INLN, INRIA Sophia Antipolis), Hugues Berry (INRIA Futurs)", "title": "Effects of Hebbian learning on the dynamics and structure of random\n  networks with inhibitory and excitatory neurons", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "q-bio.NC", "license": null, "abstract": "  The aim of the present paper is to study the effects of Hebbian learning in\nrandom recurrent neural networks with biological connectivity, i.e. sparse\nconnections and separate populations of excitatory and inhibitory neurons. We\nfurthermore consider that the neuron dynamics may occur at a (shorter) time\nscale than synaptic plasticity and consider the possibility of learning rules\nwith passive forgetting. We show that the application of such Hebbian learning\nleads to drastic changes in the network dynamics and structure. In particular,\nthe learning rule contracts the norm of the weight matrix and yields a rapid\ndecay of the dynamics complexity and entropy. In other words, the network is\nrewired by Hebbian learning into a new synaptic structure that emerges with\nlearning on the basis of the correlations that progressively build up between\nneurons. We also observe that, within this emerging structure, the strongest\nsynapses organize as a small-world network. The second effect of the decay of\nthe weight matrix spectral radius consists in a rapid contraction of the\nspectral radius of the Jacobian matrix. This drives the system through the\n``edge of chaos'' where sensitivity to the input pattern is maximal. Taken\ntogether, this scenario is remarkably predicted by theoretical arguments\nderived from dynamical systems and graph theory.\n", "versions": [{"version": "v1", "created": "Mon, 18 Jun 2007 13:42:16 GMT"}], "update_date": "2007-06-19", "authors_parsed": [["Siri", "Benoit", "", "INRIA Futurs"], ["Quoy", "Mathias", "", "ETIS"], ["Delord", "Bruno", "", "ANIM"], ["Cessac", "Bruno", "", "INLN, INRIA Sophia Antipolis"], ["Berry", "Hugues", "", "INRIA Futurs"]]}, {"id": "0706.2770", "submitter": "Yohan Payan", "authors": "Nicolas Vuillerme (TIMC - IMAG), Nicolas Pinsault (TIMC - IMAG),\n  Matthieu Boisgontier (TIMC - IMAG), Olivier Chenu (TIMC - IMAG), Jacques\n  Demongeot (TIMC - IMAG), Yohan Payan (TIMC - IMAG)", "title": "Inter-individual variability in sensory weighting of a plantar\n  pressure-based, tongue-placed tactile biofeedback for controlling posture", "comments": null, "journal-ref": "Neuroscience Letters 431 (2007) 173 - 177", "doi": "10.1016/j.neulet.2007.03.076", "report-no": null, "categories": "physics.med-ph q-bio.NC", "license": null, "abstract": "  The purpose of the present experiment was to investigate whether the sensory\nweighting of a plantar pressure-based, tongue-placed tactile biofeedback for\ncontrolling posture could be subject to inter-individual variability. To\nachieve this goal, 60 young healthy adults were asked to stand as immobile as\npossible with their eyes closed in two conditions of No-biofeedback and\nBiofeedback. Centre of foot pressure (CoP) displacements were recorded using a\nforce platform. Overall, results showed reduced CoP displacements in the\nBiofeedback relative to the No-biofeedback condition, evidencing the ability of\nthe central nervous system to efficiently integrate an artificial\nplantar-based, tongue-placed tactile biofeedback for controlling posture during\nquiet standing. Results further showed a significant positive correlation\nbetween the CoP displacements measured in the No-biofeedback condition and the\ndecrease in the CoP displacements induced by the use of the biofeedback. In\nother words, the degree of postural stabilization appeared to depend on each\nsubject's balance control capabilities, the biofeedback yielding a greater\nstabilizing effect in subjects exhibiting the largest CoP displacements when\nstanding in the No-biofeedback condition. On the whole, by evidencing a\nsignificant inter-individual variability in sensory weighting of an additional\ntactile information related to foot sole pressure distribution for controlling\nposture, the present findings underscore the need and the necessity to address\nthe issue of inter-individual variability in the field of neuroscience.\n", "versions": [{"version": "v1", "created": "Tue, 19 Jun 2007 11:56:22 GMT"}], "update_date": "2007-06-20", "authors_parsed": [["Vuillerme", "Nicolas", "", "TIMC - IMAG"], ["Pinsault", "Nicolas", "", "TIMC - IMAG"], ["Boisgontier", "Matthieu", "", "TIMC - IMAG"], ["Chenu", "Olivier", "", "TIMC - IMAG"], ["Demongeot", "Jacques", "", "TIMC - IMAG"], ["Payan", "Yohan", "", "TIMC - IMAG"]]}, {"id": "0706.2867", "submitter": "Kaushik Majumdar", "authors": "Kaushik Majumdar", "title": "Outline of a novel architecture for cortical computation", "comments": "21 pages, four figures", "journal-ref": null, "doi": "10.1007/s11571-007-9034-9", "report-no": null, "categories": "q-bio.NC q-bio.QM", "license": null, "abstract": "  In this paper a novel architecture for cortical computation has been\nproposed. This architecture is composed of computing paths consisting of\nneurons and synapses only. These paths have been decomposed into lateral,\nlongitudinal and vertical components. Cortical computation has then been\ndecomposed into lateral computation (LaC), longitudinal computation (LoC) and\nvertical computation (VeC). It has been shown that various loop structures in\nthe cortical circuit play important roles in cortical computation as well as in\nmemory storage and retrieval, keeping in conformity with the molecular basis of\nshort and long term memory. A new learning scheme for the brain has also been\nproposed and how it is implemented within the proposed architecture has been\nexplained. A number of mathematical results about the architecture have been\nproposed, many of which without proof.\n", "versions": [{"version": "v1", "created": "Tue, 19 Jun 2007 20:13:09 GMT"}, {"version": "v2", "created": "Wed, 12 Dec 2007 10:55:48 GMT"}], "update_date": "2007-12-12", "authors_parsed": [["Majumdar", "Kaushik", ""]]}, {"id": "0706.3177", "submitter": "Laurent Perrinet", "authors": "Laurent Perrinet (INT, INCM)", "title": "Role of homeostasis in learning sparse representations", "comments": null, "journal-ref": "Neural Computation, Massachusetts Institute of Technology Press\n  (MIT Press), 2010, 22 (7), pp.1812-36", "doi": "10.1162/neco.2010.05-08-795", "report-no": null, "categories": "q-bio.NC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Neurons in the input layer of primary visual cortex in primates develop\nedge-like receptive fields. One approach to understanding the emergence of this\nresponse is to state that neural activity has to efficiently represent sensory\ndata with respect to the statistics of natural scenes. Furthermore, it is\nbelieved that such an efficient coding is achieved using a competition across\nneurons so as to generate a sparse representation, that is, where a relatively\nsmall number of neurons are simultaneously active. Indeed, different models of\nsparse coding, coupled with Hebbian learning and homeostasis, have been\nproposed that successfully match the observed emergent response. However, the\nspecific role of homeostasis in learning such sparse representations is still\nlargely unknown. By quantitatively assessing the efficiency of the neural\nrepresentation during learning, we derive a cooperative homeostasis mechanism\nthat optimally tunes the competition between neurons within the sparse coding\nalgorithm. We apply this homeostasis while learning small patches taken from\nnatural images and compare its efficiency with state-of-the-art algorithms.\nResults show that while different sparse coding algorithms give similar coding\nresults, the homeostasis provides an optimal balance for the representation of\nnatural images within the population of neurons. Competition in sparse coding\nis optimized when it is fair. By contributing to optimizing statistical\ncompetition across neurons, homeostasis is crucial in providing a more\nefficient solution to the emergence of independent components.\n", "versions": [{"version": "v1", "created": "Thu, 21 Jun 2007 15:32:54 GMT"}, {"version": "v2", "created": "Wed, 5 Sep 2007 12:44:09 GMT"}, {"version": "v3", "created": "Wed, 6 Feb 2008 08:10:52 GMT"}, {"version": "v4", "created": "Wed, 19 Mar 2008 08:00:43 GMT"}, {"version": "v5", "created": "Fri, 19 Sep 2008 19:26:23 GMT"}, {"version": "v6", "created": "Fri, 25 Jun 2010 13:33:29 GMT"}, {"version": "v7", "created": "Thu, 8 Dec 2016 12:52:51 GMT"}], "update_date": "2016-12-09", "authors_parsed": [["Perrinet", "Laurent", "", "INT, INCM"]]}]