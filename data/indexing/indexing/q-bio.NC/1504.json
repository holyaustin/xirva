[{"id": "1504.00156", "submitter": "Patricia Wollstadt", "authors": "Patricia Wollstadt and Ulrich Meyer and Michael Wibral", "title": "A Graph Algorithmic Approach to Separate Direct from Indirect Neural\n  Interactions", "comments": "24 pages, 8 figures, published in PLOS One", "journal-ref": "PLoS ONE 10(10): e0140530 (2015)", "doi": "10.1371/journal.pone.0140530", "report-no": null, "categories": "cs.IT math.IT q-bio.NC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Network graphs have become a popular tool to represent complex systems\ncomposed of many interacting subunits; especially in neuroscience, network\ngraphs are increasingly used to represent and analyze functional interactions\nbetween neural sources. Interactions are often reconstructed using pairwise\nbivariate analyses, overlooking their multivariate nature: it is neglected that\ninvestigating the effect of one source on a target necessitates to take all\nother sources as potential nuisance variables into account; also combinations\nof sources may act jointly on a given target. Bivariate analyses produce\nnetworks that may contain spurious interactions, which reduce the\ninterpretability of the network and its graph metrics. A truly multivariate\nreconstruction, however, is computationally intractable due to combinatorial\nexplosion in the number of potential interactions. Thus, we have to resort to\napproximative methods to handle the intractability of multivariate interaction\nreconstruction, and thereby enable the use of networks in neuroscience. Here,\nwe suggest such an approximative approach in the form of an algorithm that\nextends fast bivariate interaction reconstruction by identifying potentially\nspurious interactions post-hoc: the algorithm flags potentially spurious edges,\nwhich may then be pruned from the network. This produces a statistically\nconservative network approximation that is guaranteed to contain non-spurious\ninteractions only. We describe the algorithm and present a reference\nimplementation to test its performance. We discuss the algorithm in relation to\nother approximative multivariate methods and highlight suitable application\nscenarios. Our approach is a tractable and data-efficient way of reconstructing\napproximative networks of multivariate interactions. It is preferable if\navailable data are limited or if fully multivariate approaches are\ncomputationally infeasible.\n", "versions": [{"version": "v1", "created": "Wed, 1 Apr 2015 09:23:55 GMT"}, {"version": "v2", "created": "Mon, 23 Nov 2015 10:56:19 GMT"}], "update_date": "2015-11-24", "authors_parsed": [["Wollstadt", "Patricia", ""], ["Meyer", "Ulrich", ""], ["Wibral", "Michael", ""]]}, {"id": "1504.01502", "submitter": "Tony Lindeberg", "authors": "Tony Lindeberg", "title": "Separable time-causal and time-recursive spatio-temporal receptive\n  fields", "comments": "12 pages, 2 figures, 2 tables. arXiv admin note: substantial text\n  overlap with arXiv:1404.2037", "journal-ref": "Proc SSVM 2015: Scale-Space and Variational Methods for Computer\n  Vision, Springer LNCS vol 9087, pages 90-102, 2015", "doi": "10.1007/978-3-319-18461-6_8", "report-no": null, "categories": "cs.CV q-bio.NC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present an improved model and theory for time-causal and time-recursive\nspatio-temporal receptive fields, obtained by a combination of Gaussian\nreceptive fields over the spatial domain and first-order integrators or\nequivalently truncated exponential filters coupled in cascade over the temporal\ndomain. Compared to previous spatio-temporal scale-space formulations in terms\nof non-enhancement of local extrema or scale invariance, these receptive fields\nare based on different scale-space axiomatics over time by ensuring\nnon-creation of new local extrema or zero-crossings with increasing temporal\nscale. Specifically, extensions are presented about parameterizing the\nintermediate temporal scale levels, analysing the resulting temporal dynamics\nand transferring the theory to a discrete implementation in terms of recursive\nfilters over time.\n", "versions": [{"version": "v1", "created": "Tue, 7 Apr 2015 07:29:54 GMT"}], "update_date": "2021-01-25", "authors_parsed": [["Lindeberg", "Tony", ""]]}, {"id": "1504.01792", "submitter": "Paul Smolen", "authors": "Paul Smolen", "title": "Modeling Maintenance of Long-Term Potentiation in Clustered Synapses,\n  Long-Term Memory Without Bistability", "comments": "17 pages, 5 figures", "journal-ref": "Neural Plasticity, volume 2015 (2015), article ID 185410", "doi": "10.1155/2015/185410", "report-no": null, "categories": "q-bio.NC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Memories are stored, at least partly, as patterns of strong synapses. Given\nmolecular turnover, how can synapses maintain strong for the years that\nmemories can persist? Some models postulate that biochemical bistability\nmaintains strong synapses. However, bistability should give a bimodal\ndistribution of synaptic strength or weight, whereas current data show unimodal\ndistributions for weights and for a correlated variable, dendritic spine\nvolume. Bistability of single synapses has also never been empirically\ndemonstrated. Thus it is important for models to simulate both unimodal\ndistributions and long-term memory persistence. Here a model is developed that\nconnects ongoing, competing processes of synaptic growth and weakening to\nstochastic processes of receptor insertion and removal in dendritic spines. The\nmodel simulates long-term (in excess of 1 yr) persistence of groups of strong\nsynapses. A unimodal weight distribution results. For stability of this\ndistribution it proved essential to incorporate resource competition between\nsynapses organized into small clusters. With competition, these clusters are\nstable for years. These simulations concur with recent data to support the\nclustered plasticity hypothesis, which suggests clusters, rather than single\nsynaptic contacts, may be a fundamental unit for storage of long-term memory.\nThe model makes empirical predictions, and may provide a framework to\ninvestigate mechanisms maintaining the balance between synaptic plasticity and\nstability of memory.\n", "versions": [{"version": "v1", "created": "Wed, 8 Apr 2015 00:37:47 GMT"}], "update_date": "2015-04-09", "authors_parsed": [["Smolen", "Paul", ""]]}, {"id": "1504.02265", "submitter": "Tiago Simas", "authors": "Tiago Simas, Mario Chavez, Pablo Rodriguez, and Albert Diaz-Guilera", "title": "An Algebraic Topological Method for Multimodal Brain Networks\n  Comparisons", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "q-bio.NC nlin.AO physics.soc-ph", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Understanding brain connectivity has become one of the most important issues\nin neuroscience. But connectivity data can reflect either the functional\nrelationships of the brain activities or the anatomical properties between\nbrain areas. Although one should expect a clear relationship between both\nrepresentations it is not straightforward. Here we present a formalism that\nallows for the comparison of structural (DTI) and functional (fMRI) networks by\nembedding both in a common metric space. In this metric space one can then find\nfor which regions the two networks are significantly different. Our methodology\ncan be used not only to compare multimodal networks but also to extract\nstatistically significant aggregated networks of a set of subjects. Actually,\nwe use this procedure to aggregate a set of functional (fMRI) networks from\ndifferent subjects in an aggregated network that is compared with the\nanatomical (DTI) connectivity. The comparison of the aggregated network reveals\nsome features that are not observed when the comparison is done with the\nclassical averaged network.\n", "versions": [{"version": "v1", "created": "Thu, 9 Apr 2015 11:32:44 GMT"}, {"version": "v2", "created": "Fri, 10 Apr 2015 17:04:25 GMT"}], "update_date": "2015-04-13", "authors_parsed": [["Simas", "Tiago", ""], ["Chavez", "Mario", ""], ["Rodriguez", "Pablo", ""], ["Diaz-Guilera", "Albert", ""]]}, {"id": "1504.02496", "submitter": "Miguel Nicolelis", "authors": "Sankaranarayani Rajangam, Po-He Tseng, Allen Yin, Mikhail A. Lebedev,\n  Miguel A. L. Nicolelis", "title": "Direct Cortical Control of Primate Whole-Body Navigation in a Mobile\n  Robotic Wheelchair", "comments": "15 pages, 4 main figure, 9 supplementary figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "q-bio.NC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We and others have previously developed brain-machine-interfaces (BMIs),\nwhich allowed ensembles of cortical neurons to control artificial limbs (1-4).\nHowever, it is unclear whether cortical ensembles could operate a BMI for\nwhole-body navigation. Here we show that rhesus monkeys can learn to navigate a\nrobotic wheelchair while seated on top of it, and using their cortical activity\nas the robot control signal. Two monkeys were chronically implanted with\nmultichannel electrode arrays which simultaneously sampled activity of roughly\n150 premotor and sensorimotor cortex neurons per monkey. This neuronal ensemble\nactivity was transformed by a linear decoder into the robotic wheelchair's\ntranslational and rotational velocities. During several weeks of training,\nmonkeys significantly improved their ability to navigate the wheelchair toward\nthe location of a food reward. The navigation was enacted by ensemble\nmodulations attuned to the whole-body displacements, and also to the distance\nto the food location. These results demonstrate that intracranial BMIs could\nrestore whole-body mobility to severely paralyzed patients in the future.\n", "versions": [{"version": "v1", "created": "Thu, 9 Apr 2015 20:57:48 GMT"}], "update_date": "2015-04-13", "authors_parsed": [["Rajangam", "Sankaranarayani", ""], ["Tseng", "Po-He", ""], ["Yin", "Allen", ""], ["Lebedev", "Mikhail A.", ""], ["Nicolelis", "Miguel A. L.", ""]]}, {"id": "1504.02648", "submitter": "Tony Lindeberg", "authors": "Tony Lindeberg", "title": "Time-causal and time-recursive spatio-temporal receptive fields", "comments": "39 pages, 12 figures, 5 tables in Journal of Mathematical Imaging and\n  Vision, published online Dec 2015", "journal-ref": "Journal of Mathematical Imaging and Vision, 55(1): 50-88, 2016", "doi": "10.1007/s10851-015-0613-9", "report-no": null, "categories": "cs.CV q-bio.NC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present an improved model and theory for time-causal and time-recursive\nspatio-temporal receptive fields, based on a combination of Gaussian receptive\nfields over the spatial domain and first-order integrators or equivalently\ntruncated exponential filters coupled in cascade over the temporal domain.\n  Compared to previous spatio-temporal scale-space formulations in terms of\nnon-enhancement of local extrema or scale invariance, these receptive fields\nare based on different scale-space axiomatics over time by ensuring\nnon-creation of new local extrema or zero-crossings with increasing temporal\nscale. Specifically, extensions are presented about (i) parameterizing the\nintermediate temporal scale levels, (ii) analysing the resulting temporal\ndynamics, (iii) transferring the theory to a discrete implementation, (iv)\ncomputing scale-normalized spatio-temporal derivative expressions for\nspatio-temporal feature detection and (v) computational modelling of receptive\nfields in the lateral geniculate nucleus (LGN) and the primary visual cortex\n(V1) in biological vision.\n  We show that by distributing the intermediate temporal scale levels according\nto a logarithmic distribution, we obtain much faster temporal response\nproperties (shorter temporal delays) compared to a uniform distribution.\nSpecifically, these kernels converge very rapidly to a limit kernel possessing\ntrue self-similar scale-invariant properties over temporal scales, thereby\nallowing for true scale invariance over variations in the temporal scale,\nalthough the underlying temporal scale-space representation is based on a\ndiscretized temporal scale parameter.\n  We show how scale-normalized temporal derivatives can be defined for these\ntime-causal scale-space kernels and how the composed theory can be used for\ncomputing basic types of scale-normalized spatio-temporal derivative\nexpressions in a computationally efficient manner.\n", "versions": [{"version": "v1", "created": "Fri, 10 Apr 2015 12:06:27 GMT"}, {"version": "v2", "created": "Mon, 7 Dec 2015 15:50:45 GMT"}], "update_date": "2016-03-23", "authors_parsed": [["Lindeberg", "Tony", ""]]}, {"id": "1504.02927", "submitter": "Michael Paulin", "authors": "Michael G. Paulin", "title": "The Origin of Inference: Ediacaran Ecology and the Evolution of Bayesian\n  Brains", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "q-bio.NC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The evolution of spiking neurons and nervous systems in the late Ediacaran\nperiod simultaneously with the evolution of carnivores around 550 million years\nago can be explained by the need for accurately timed decisions under an\nimminent threat of being eaten. A simple model shows that threshold triggering\ndevices, spiking neurons, are utility-maximizing decision-makers for the timing\nof escape reflexes given the sensory cues available to Ediacaran animals at the\nonset of carnivory. Decisions are suboptimal for very weak stimuli, providing\nselection pressure for secondary processing of primary spike train data. A\nsimple network can make approximately Bayes optimal decisions given stochastic\nspike trains. Decisions that are arbitrarily close to Bayes optimal can be\nobtained by enlarging this network. A subnetwork that computes the Bayesian\nposterior density of the critical state variable - distance between predator\nand prey - emerges as a core component of the decision-making mechanism. This\nis a neural analog of a Bayesian particle filter with cerebellar-like\narchitecture. The model explains fundamental properties of neurons and nervous\nsystems in modern animals and makes testable predictions.\n", "versions": [{"version": "v1", "created": "Sun, 12 Apr 2015 02:23:17 GMT"}], "update_date": "2015-04-14", "authors_parsed": [["Paulin", "Michael G.", ""]]}, {"id": "1504.02953", "submitter": "Nils Berglund", "authors": "Nils Berglund and Christian Kuehn", "title": "Regularity structures and renormalisation of FitzHugh-Nagumo SPDEs in\n  three space dimensions", "comments": "51 pages. The extension procedure in Section 4 has been substantially\n  modified. Minor changes in Sections 5 and 6 and in the main result", "journal-ref": "Electron. J. Probab. 21 (2016), no.18, 1-48", "doi": "10.1214/16-EJP4371", "report-no": "Erratum at arXiv:1805.02890", "categories": "math.PR math-ph math.MP q-bio.NC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We prove local existence of solutions for a class of suitably renormalised\ncoupled SPDE-ODE systems driven by space-time white noise, where the space\ndimension is equal to 2 or 3. This class includes in particular the\nFitzHugh-Nagumo system describing the evolution of action potentials of a large\npopulation of neurons, as well as models with multidimensional gating\nvariables. The proof relies on the theory of regularity structures recently\ndeveloped by M. Hairer, which is extended to include situations with semigroups\nthat are not regularising in space. We also provide explicit expressions for\nthe renormalisation constants, for a large class of cubic nonlinearities.\n", "versions": [{"version": "v1", "created": "Sun, 12 Apr 2015 10:18:45 GMT"}, {"version": "v2", "created": "Tue, 16 Jun 2015 21:13:07 GMT"}], "update_date": "2018-05-10", "authors_parsed": [["Berglund", "Nils", ""], ["Kuehn", "Christian", ""]]}, {"id": "1504.03063", "submitter": "Sang-Yoon  Kim", "authors": "Sang-Yoon Kim and Woochang Lim", "title": "Fast Sparsely Synchronized Brain Rhythms in A Scale-Free Neural Network", "comments": "54 pages, 13 figures. arXiv admin note: text overlap with\n  arXiv:1403.1034", "journal-ref": null, "doi": "10.1103/PhysRevE.92.022717", "report-no": null, "categories": "q-bio.NC physics.bio-ph", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider a directed Barab\\'{a}si-Albert scale-free network model with\nsymmetric preferential attachment with the same in- and out-degrees, and study\nemergence of sparsely synchronized rhythms for a fixed attachment degree in an\ninhibitory population of fast spiking Izhikevich interneurons. For a study on\nthe fast sparsely synchronized rhythms, we fix $J$ (synaptic inhibition\nstrength) at a sufficiently large value, and investigate the population states\nby increasing $D$ (noise intensity). For small $D$, full synchronization with\nthe same population-rhythm frequency $f_p$ and mean firing rate (MFR) $f_i$ of\nindividual neurons occurs, while for sufficiently large $D$ partial\nsynchronization with $f_p > {\\langle f_i \\rangle}$ ($\\langle f_i \\rangle$:\nensemble-averaged MFR) appears due to intermittent discharge of individual\nneurons; particularly, the case of $f_p > 4 {\\langle f_i \\rangle}$ is referred\nto as sparse synchronization. Only for the partial and sparse synchronization,\nMFRs and contributions of individual neuronal dynamics to population\nsynchronization change depending on their degrees, unlike the case of full\nsynchronization. Consequently, dynamics of individual neurons reveal the\ninhomogeneous network structure for the case of partial and sparse\nsynchronization, which is in contrast to the case of statistically homogeneous\nrandom graphs and small-world networks. Finally, we investigate the effect of\nnetwork architecture on sparse synchronization in the following three cases:\n(1) variation in the degree of symmetric attachment (2) asymmetric preferential\nattachment of new nodes with different in- and out-degrees (3) preferential\nattachment between pre-existing nodes (without addition of new nodes). In these\nthree cases, both relation between network topology and sparse synchronization\nand contributions of individual dynamics to the sparse synchronization are\ndiscussed.\n", "versions": [{"version": "v1", "created": "Mon, 13 Apr 2015 04:49:15 GMT"}], "update_date": "2015-09-02", "authors_parsed": [["Kim", "Sang-Yoon", ""], ["Lim", "Woochang", ""]]}, {"id": "1504.03132", "submitter": "Takashi Hayakawa Dr.", "authors": "Takashi Hayakawa and Toshio Aoyagi", "title": "Learning in Neural Networks Based on a Generalized Fluctuation Theorem", "comments": "13 pages, 6 figures", "journal-ref": "Phys. Rev. E 92, 052710 (2015)", "doi": "10.1103/PhysRevE.92.052710", "report-no": null, "categories": "cond-mat.dis-nn q-bio.NC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Information maximization has been investigated as a possible mechanism of\nlearning governing the self-organization that occurs within the neural systems\nof animals. Within the general context of models of neural systems\nbidirectionally interacting with environments, however, the role of information\nmaximization remains to be elucidated. For bidirectionally interacting physical\nsystems, universal laws describing the fluctuation they exhibit and the\ninformation they possess have recently been discovered. These laws are termed\nfluctuation theorems. In the present study, we formulate a theory of learning\nin neural networks bidirectionally interacting with environments based on the\nprinciple of information maximization. Our formulation begins with the\nintroduction of a generalized fluctuation theorem, employing an interpretation\nappropriate for the present application, which differs from the original\nthermodynamic interpretation. We analytically and numerically demonstrate that\nthe learning mechanism presented in our theory allows neural networks to\nefficiently explore their environments and optimally encode information about\nthem.\n", "versions": [{"version": "v1", "created": "Mon, 13 Apr 2015 11:17:37 GMT"}], "update_date": "2015-11-18", "authors_parsed": [["Hayakawa", "Takashi", ""], ["Aoyagi", "Toshio", ""]]}, {"id": "1504.03343", "submitter": "Apoorvagiri Lnu", "authors": "Apoorvagiri, M.S. Nagananda", "title": "Quantization of mental stress using various physiological markers", "comments": "16 pages,11 Figures, 2 Tables; can also be found at PeerJ PrePrints\n  3:e1250 2015, page 1-16", "journal-ref": null, "doi": "10.7287/peerj.preprints.777v3", "report-no": null, "categories": "q-bio.NC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The aim of this study is to quantize mental stress by integrating different\nphysiological markers like reaction time, photoplethysmograph (PPG), heart rate\nvariability (HRV) and subjective markers like questionnaire. The study included\n10 subjects of age between 22 and 26 years. Study materials included the\nresults of PSS questionnaire, simple reaction time, PPG data, and HRV data\nduring a stress inducing stroop test. The study suggests that mental stress can\nbe quantized when stress is induced acquisitively and more accurate\nquantification of stress can be achieved by integrating many physiological\nparameters.\n", "versions": [{"version": "v1", "created": "Thu, 5 Feb 2015 19:19:30 GMT"}, {"version": "v2", "created": "Fri, 1 May 2015 03:30:45 GMT"}], "update_date": "2015-05-04", "authors_parsed": [["Apoorvagiri", "", ""], ["Nagananda", "M. S.", ""]]}, {"id": "1504.03580", "submitter": "Robert Burger PhD", "authors": "John Robert Burger", "title": "An New Type Of Artificial Brain Using Controlled Neurons", "comments": "10 pages; 6 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.ET q-bio.NC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Plans for a new type of artificial brain are possible because of realistic\nneurons in logically structured arrays of controlled toggles, one toggle per\nneuron. Controlled toggles can be made to compute, in parallel, parameters of\ncritical importance for each of several complex images recalled from\nassociative long term memory. Controlled toggles are shown below to amount to a\nnew type of neural network that supports autonomous behavior and action.\n", "versions": [{"version": "v1", "created": "Tue, 14 Apr 2015 15:06:46 GMT"}, {"version": "v2", "created": "Fri, 17 Apr 2015 19:41:31 GMT"}], "update_date": "2015-04-20", "authors_parsed": [["Burger", "John Robert", ""]]}, {"id": "1504.03584", "submitter": "Daniele Marinazzo", "authors": "Sebastiano Stramaglia, Leonardo Angelini, Guorong Wu, Jesus M.\n  Cort\\'es, Luca Faes, Daniele Marinazzo", "title": "Synergetic and redundant information flow detected by unnormalized\n  Granger causality: application to resting state fMRI", "comments": "6 figures. arXiv admin note: text overlap with arXiv:1403.5156", "journal-ref": "Published in IEEE Transactions on Biomedical Engineering, 2016", "doi": "10.1109/TBME.2016.2559578", "report-no": null, "categories": "cs.IT math.IT physics.data-an q-bio.NC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Objectives: We develop a framework for the analysis of synergy and redundancy\nin the pattern of information flow between subsystems of a complex network.\nMethods: The presence of redundancy and/or synergy in multivariate time series\ndata renders difficult to estimate the neat flow of information from each\ndriver variable to a given target. We show that adopting an unnormalized\ndefinition of Granger causality one may put in evidence redundant multiplets of\nvariables influencing the target by maximizing the total Granger causality to a\ngiven target, over all the possible partitions of the set of driving variables.\nConsequently we introduce a pairwise index of synergy which is zero when two\nindependent sources additively influence the future state of the system,\ndifferently from previous definitions of synergy. Results: We report the\napplication of the proposed approach to resting state fMRI data from the Human\nConnectome Project, showing that redundant pairs of regions arise mainly due to\nspace contiguity and interhemispheric symmetry, whilst synergy occurs mainly\nbetween non-homologous pairs of regions in opposite hemispheres. Conclusions:\nRedundancy and synergy, in healthy resting brains, display characteristic\npatterns, revealed by the proposed approach. Significance: The pairwise synergy\nindex, here introduced, maps the informational character of the system at hand\ninto a weighted complex network: the same approach can be applied to other\ncomplex systems whose normal state corresponds to a balance between redundant\nand synergetic circuits.\n", "versions": [{"version": "v1", "created": "Tue, 14 Apr 2015 15:22:03 GMT"}, {"version": "v2", "created": "Fri, 8 Jan 2016 14:14:19 GMT"}, {"version": "v3", "created": "Mon, 16 May 2016 13:03:38 GMT"}], "update_date": "2016-05-17", "authors_parsed": [["Stramaglia", "Sebastiano", ""], ["Angelini", "Leonardo", ""], ["Wu", "Guorong", ""], ["Cort\u00e9s", "Jesus M.", ""], ["Faes", "Luca", ""], ["Marinazzo", "Daniele", ""]]}, {"id": "1504.03622", "submitter": "Alexander Huth", "authors": "Alexander G. Huth, Thomas L. Griffiths, Frederic E. Theunissen, Jack\n  L. Gallant", "title": "PrAGMATiC: a Probabilistic and Generative Model of Areas Tiling the\n  Cortex", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "q-bio.QM q-bio.NC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Much of the human cortex seems to be organized into topographic cortical\nmaps. Yet few quantitative methods exist for characterizing these maps. To\naddress this issue we developed a modeling framework that can reveal\ngroup-level cortical maps based on neuroimaging data. PrAGMATiC, a\nprobabilistic and generative model of areas tiling the cortex, is a\nhierarchical Bayesian generative model of cortical maps. This model assumes\nthat the cortical map in each individual subject is a sample from a single\nunderlying probability distribution. Learning the parameters of this\ndistribution reveals the properties of a cortical map that are common across a\ngroup of subjects while avoiding the potentially lossy step of co-registering\neach subject into a group anatomical space. In this report we give a\nmathematical description of PrAGMATiC, describe approximations that make it\npractical to use, show preliminary results from its application to a real\ndataset, and describe a number of possible future extensions.\n", "versions": [{"version": "v1", "created": "Tue, 14 Apr 2015 16:52:31 GMT"}], "update_date": "2015-04-15", "authors_parsed": [["Huth", "Alexander G.", ""], ["Griffiths", "Thomas L.", ""], ["Theunissen", "Frederic E.", ""], ["Gallant", "Jack L.", ""]]}, {"id": "1504.03746", "submitter": "Willem Wybo", "authors": "Willem A.M. Wybo, Daniele Boccalini, Benjamin Torben-Nielsen,\n  Marc-Oliver Gewaltig", "title": "A sparse reformulation of the Green's function formalism allows\n  efficient simulations of partial differential equations on tree graphs", "comments": "41 pages, 4 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "q-bio.NC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We prove that when a class of partial differential equations, generalized\nfrom the cable equation, is defined on tree graphs, and when the inputs are\nrestricted to a spatially discrete, well chosen set of points, the Green's\nfunction (GF) formalism can be rewritten to scale as $O(n)$ with the number $n$\nof input locations, contrary to the previously reported $O(n^2)$ scaling. We\nshow that the linear scaling can be combined with an expansion of the remaining\nkernels as sums of exponentials, to allow efficient simulations of equations\nfrom the aforementioned class. We furthermore validate this simulation paradigm\non models of nerve cells and explore its relation with more traditional finite\ndifference approaches. Situations in which a gain in computational performance\nis expected, are discussed.\n", "versions": [{"version": "v1", "created": "Tue, 14 Apr 2015 23:59:40 GMT"}, {"version": "v2", "created": "Wed, 16 Sep 2015 09:47:52 GMT"}], "update_date": "2015-09-17", "authors_parsed": [["Wybo", "Willem A. M.", ""], ["Boccalini", "Daniele", ""], ["Torben-Nielsen", "Benjamin", ""], ["Gewaltig", "Marc-Oliver", ""]]}, {"id": "1504.03855", "submitter": "Irene Costantini", "authors": "Irene Costantini, Jean-Pierre Ghobril, Antonino Paolo Di Giovanna,\n  Anna Letizia Allegra Mascaro, Ludovico Silvestri, Marie Caroline\n  M\\\"ullenbroich, Leonardo Onofri, Valerio Conti, Francesco Vanzi, Leonardo\n  Sacconi, Renzo Guerrini, Henry Markram, Giulio Iannello and Francesco Saverio\n  Pavone", "title": "A versatile clearing agent for multi-modal brain imaging", "comments": "in Scientific Reports 2015", "journal-ref": null, "doi": "10.1038/srep09808", "report-no": null, "categories": "q-bio.NC physics.med-ph", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Extensive mapping of neuronal connections in the central nervous system\nrequires high-throughput um-scale imaging of large volumes. In recent years,\ndifferent approaches have been developed to overcome the limitations due to\ntissue light scattering. These methods are generally developed to improve the\nperformance of a specific imaging modality, thus limiting comprehensive\nneuroanatomical exploration by multimodal optical techniques. Here, we\nintroduce a versatile brain clearing agent (2,2'-thiodiethanol; TDE) suitable\nfor various applications and imaging techniques. TDE is cost-efficient,\nwater-soluble and low-viscous and, more importantly, it preserves fluorescence,\nis compatible with immunostaining and does not cause deformations at\nsub-cellular level. We demonstrate the effectiveness of this method in\ndifferent applications: in fixed samples by imaging a whole mouse hippocampus\nwith serial two-photon tomography; in combination with CLARITY by\nreconstructing an entire mouse brain with light sheet microscopy and in\ntranslational research by imaging immunostained human dysplastic brain tissue.\n", "versions": [{"version": "v1", "created": "Wed, 15 Apr 2015 10:38:48 GMT"}, {"version": "v2", "created": "Thu, 16 Apr 2015 11:30:16 GMT"}], "update_date": "2015-04-17", "authors_parsed": [["Costantini", "Irene", ""], ["Ghobril", "Jean-Pierre", ""], ["Di Giovanna", "Antonino Paolo", ""], ["Mascaro", "Anna Letizia Allegra", ""], ["Silvestri", "Ludovico", ""], ["M\u00fcllenbroich", "Marie Caroline", ""], ["Onofri", "Leonardo", ""], ["Conti", "Valerio", ""], ["Vanzi", "Francesco", ""], ["Sacconi", "Leonardo", ""], ["Guerrini", "Renzo", ""], ["Markram", "Henry", ""], ["Iannello", "Giulio", ""], ["Pavone", "Francesco Saverio", ""]]}, {"id": "1504.03871", "submitter": "Saeed Reza Kheradpisheh", "authors": "Saeed Reza Kheradpisheh, Mohammad Ganjtabesh, Timoth\\'ee Masquelier", "title": "Bio-inspired Unsupervised Learning of Visual Features Leads to Robust\n  Invariant Object Recognition", "comments": null, "journal-ref": "Neurocomputing 205 (2016) 382-392", "doi": "10.1016/j.neucom.2016.04.029", "report-no": null, "categories": "cs.CV q-bio.NC", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Retinal image of surrounding objects varies tremendously due to the changes\nin position, size, pose, illumination condition, background context, occlusion,\nnoise, and nonrigid deformations. But despite these huge variations, our visual\nsystem is able to invariantly recognize any object in just a fraction of a\nsecond. To date, various computational models have been proposed to mimic the\nhierarchical processing of the ventral visual pathway, with limited success.\nHere, we show that the association of both biologically inspired network\narchitecture and learning rule significantly improves the models' performance\nwhen facing challenging invariant object recognition problems. Our model is an\nasynchronous feedforward spiking neural network. When the network is presented\nwith natural images, the neurons in the entry layers detect edges, and the most\nactivated ones fire first, while neurons in higher layers are equipped with\nspike timing-dependent plasticity. These neurons progressively become selective\nto intermediate complexity visual features appropriate for object\ncategorization. The model is evaluated on 3D-Object and ETH-80 datasets which\nare two benchmarks for invariant object recognition, and is shown to outperform\nstate-of-the-art models, including DeepConvNet and HMAX. This demonstrates its\nability to accurately recognize different instances of multiple object classes\neven under various appearance conditions (different views, scales, tilts, and\nbackgrounds). Several statistical analysis techniques are used to show that our\nmodel extracts class specific and highly informative features.\n", "versions": [{"version": "v1", "created": "Wed, 15 Apr 2015 11:47:21 GMT"}, {"version": "v2", "created": "Sun, 3 May 2015 12:40:59 GMT"}, {"version": "v3", "created": "Tue, 28 Jun 2016 10:54:22 GMT"}], "update_date": "2017-02-14", "authors_parsed": [["Kheradpisheh", "Saeed Reza", ""], ["Ganjtabesh", "Mohammad", ""], ["Masquelier", "Timoth\u00e9e", ""]]}, {"id": "1504.03954", "submitter": "Anna Cattani", "authors": "Anna Cattani and Gaute T. Einevoll and Stefano Panzeri", "title": "Phase-of-firing code", "comments": "In press, Encyclopedia of Computational Neuroscience, 2015", "journal-ref": null, "doi": null, "report-no": null, "categories": "q-bio.NC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Definition. The phase-of-firing code is a neural coding scheme whereby\nneurons encode information using the time at which they fire spikes within a\ncycle of the ongoing oscillatory pattern of network activity. This coding\nscheme may allow neurons to use their temporal pattern of spikes to encode\ninformation that is not encoded in their firing rate.\n", "versions": [{"version": "v1", "created": "Wed, 15 Apr 2015 15:49:56 GMT"}], "update_date": "2015-04-16", "authors_parsed": [["Cattani", "Anna", ""], ["Einevoll", "Gaute T.", ""], ["Panzeri", "Stefano", ""]]}, {"id": "1504.03983", "submitter": "Wilhelm Braun", "authors": "Wilhelm Braun, Paul C. Matthews, R\\\"udiger Thul", "title": "First passage times in integrate-and-fire neurons with stochastic\n  thresholds", "comments": "8 pages, 7 figures. Accepted for publication in Physical Review E", "journal-ref": "Phys. Rev. E 91, 052701 (2015)", "doi": "10.1103/PhysRevE.91.052701", "report-no": null, "categories": "q-bio.NC math.PR physics.bio-ph", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider a leaky integrate-and-fire neuron with deterministic subthreshold\ndynamics and a firing threshold that evolves as an Ornstein-Uhlenbeck process.\nThe formulation of this minimal model is motivated by the experimentally\nobserved widespread variation of neural firing thresholds. We show numerically\nthat the mean first passage time can depend non-monotonically on the noise\namplitude. For sufficiently large values of the correlation time of the\nstochastic threshold the mean first passage time is maximal for non-vanishing\nnoise. We provide an explanation for this effect by analytically transforming\nthe original model into a first passage time problem for Brownian motion. This\ntransformation also allows for a perturbative calculation of the first passage\ntime histograms. In turn this provides quantitative insights into the\nmechanisms that lead to the non-monotonic behaviour of the mean first passage\ntime. The perturbation expansion is in excellent agreement with direct\nnumerical simulations. The approach developed here can be applied to any\ndeterministic subthreshold dynamics and any Gauss-Markov processes for the\nfiring threshold. This opens up the possibility to incorporate biophysically\ndetailed components into the subthreshold dynamics, rendering our approach a\npowerful framework that sits between traditional integrate-and-fire models and\ncomplex mechanistic descriptions of neural dynamics.\n", "versions": [{"version": "v1", "created": "Wed, 15 Apr 2015 18:08:49 GMT"}, {"version": "v2", "created": "Thu, 16 Apr 2015 21:38:03 GMT"}], "update_date": "2015-05-12", "authors_parsed": [["Braun", "Wilhelm", ""], ["Matthews", "Paul C.", ""], ["Thul", "R\u00fcdiger", ""]]}, {"id": "1504.04756", "submitter": "James P. Crutchfield", "authors": "Sarah E. Marzen and Michael R. DeWeese and James P. Crutchfield", "title": "Time Resolution Dependence of Information Measures for Spiking Neurons:\n  Atoms, Scaling, and Universality", "comments": "20 pages, 6 figures;\n  http://csc.ucdavis.edu/~cmg/compmech/pubs/trdctim.htm", "journal-ref": null, "doi": null, "report-no": null, "categories": "q-bio.NC cond-mat.dis-nn cs.NE math.PR nlin.CD", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The mutual information between stimulus and spike-train response is commonly\nused to monitor neural coding efficiency, but neuronal computation broadly\nconceived requires more refined and targeted information measures of\ninput-output joint processes. A first step towards that larger goal is to\ndevelop information measures for individual output processes, including\ninformation generation (entropy rate), stored information (statistical\ncomplexity), predictable information (excess entropy), and active information\naccumulation (bound information rate). We calculate these for spike trains\ngenerated by a variety of noise-driven integrate-and-fire neurons as a function\nof time resolution and for alternating renewal processes. We show that their\ntime-resolution dependence reveals coarse-grained structural properties of\ninterspike interval statistics; e.g., $\\tau$-entropy rates that diverge less\nquickly than the firing rate indicate interspike interval correlations. We also\nfind evidence that the excess entropy and regularized statistical complexity of\ndifferent types of integrate-and-fire neurons are universal in the\ncontinuous-time limit in the sense that they do not depend on mechanism\ndetails. This suggests a surprising simplicity in the spike trains generated by\nthese model neurons. Interestingly, neurons with gamma-distributed ISIs and\nneurons whose spike trains are alternating renewal processes do not fall into\nthe same universality class. These results lead to two conclusions. First, the\ndependence of information measures on time resolution reveals mechanistic\ndetails about spike train generation. Second, information measures can be used\nas model selection tools for analyzing spike train processes.\n", "versions": [{"version": "v1", "created": "Sat, 18 Apr 2015 20:14:19 GMT"}], "update_date": "2015-04-21", "authors_parsed": [["Marzen", "Sarah E.", ""], ["DeWeese", "Michael R.", ""], ["Crutchfield", "James P.", ""]]}, {"id": "1504.05143", "submitter": "David Kappel", "authors": "David Kappel, Stefan Habenschuss, Robert Legenstein, Wolfgang Maass", "title": "Network Plasticity as Bayesian Inference", "comments": "33 pages, 5 figures, the supplement is available on the author's web\n  page http://www.igi.tugraz.at/kappel", "journal-ref": null, "doi": "10.1371/journal.pcbi.1004485", "report-no": null, "categories": "cs.NE q-bio.NC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  General results from statistical learning theory suggest to understand not\nonly brain computations, but also brain plasticity as probabilistic inference.\nBut a model for that has been missing. We propose that inherently stochastic\nfeatures of synaptic plasticity and spine motility enable cortical networks of\nneurons to carry out probabilistic inference by sampling from a posterior\ndistribution of network configurations. This model provides a viable\nalternative to existing models that propose convergence of parameters to\nmaximum likelihood values. It explains how priors on weight distributions and\nconnection probabilities can be merged optimally with learned experience, how\ncortical networks can generalize learned information so well to novel\nexperiences, and how they can compensate continuously for unforeseen\ndisturbances of the network. The resulting new theory of network plasticity\nexplains from a functional perspective a number of experimental data on\nstochastic aspects of synaptic plasticity that previously appeared to be quite\npuzzling.\n", "versions": [{"version": "v1", "created": "Mon, 20 Apr 2015 18:18:18 GMT"}], "update_date": "2016-02-17", "authors_parsed": [["Kappel", "David", ""], ["Habenschuss", "Stefan", ""], ["Legenstein", "Robert", ""], ["Maass", "Wolfgang", ""]]}, {"id": "1504.05261", "submitter": "Takahiro Wada", "authors": "Takahiro Wada, Norimasa Kamij and Shunichi Doi", "title": "A Mathematical Model of Motion Sickness in 6DOF Motion and Its\n  Application to Vehicle Passengers", "comments": "in International Digital Human Modeling Symposium, 2013", "journal-ref": null, "doi": null, "report-no": null, "categories": "q-bio.NC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A mathematical model of motion sickness incidence (MSI) is derived by\nintegrating neurophysiological knowledge of the vestibular system to predict\nthe severity of motion sickness of humans. Bos et al. proposed the successful\nmathematical model of motion sickness based on the neurophysiological mechanism\nbased on the subject vertical conflict (SVC) theory. We expand this model to\n6-DOF motion, including head rotation, by introducing the otolith-canal\ninteraction. Then the model is applied to an analysis of passengers' comfort.\nIt is known that the driver is less susceptible to motion sickness than are the\npassengers. In addition, it is known that the driver tilts his/her head toward\nthe curve direction when curve driving, whereas the passengers' head movement\nis likely to occur in the opposite direction. Thus, the effect of the head tilt\nstrategy on motion sickness was investigated by the proposed mathematical\nmodel. The head movements of drivers and passengers were measured in slalom\ndriving. Then, the MSI of the drivers and that of the passengers predicted by\nthe proposed model were compared. The results revealed that the head movement\ntoward the centripetal direction has a significant effect in reducing the MSI\nin the sense of SVC theory.\n", "versions": [{"version": "v1", "created": "Mon, 20 Apr 2015 23:42:20 GMT"}], "update_date": "2015-04-22", "authors_parsed": [["Wada", "Takahiro", ""], ["Kamij", "Norimasa", ""], ["Doi", "Shunichi", ""]]}, {"id": "1504.05440", "submitter": "Patrizio Tressoldi E", "authors": "Patrizio Tressoldi, Markus A. Maier, Vanessa L. B\\\"uechner and Andrei\n  Khrennikov", "title": "A Macroscopic Behavioral Violation of No Signaling In Time Inequalities", "comments": "9 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "q-bio.NC quant-ph", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper we applied the no-signaling in time (NSIT) formalism discussed\nby Kofler and Brukner to investigate temporal entanglement between binary human\nbehavioral unconscious choices at t1 with binary random outcomes at t2. NSIT\nconsists of a set of inequalities and represents mathematical conditions for\nmacro-realism which require only two measurements in time. The analyses of\nthree independent experiments show a strong violation of NSIT in two out of\nthree of them, supporting the hypothesis of a quantum-like temporal\nentanglement between human choices at t1 with binary random outcomes at t2\n", "versions": [{"version": "v1", "created": "Sun, 19 Apr 2015 13:22:42 GMT"}], "update_date": "2015-04-22", "authors_parsed": [["Tressoldi", "Patrizio", ""], ["Maier", "Markus A.", ""], ["B\u00fcechner", "Vanessa L.", ""], ["Khrennikov", "Andrei", ""]]}, {"id": "1504.06290", "submitter": "Zachary Kilpatrick PhD", "authors": "Zachary McCleney and Zachary P. Kilpatrick", "title": "Entrainment in up and down states of neural populations: non-smooth and\n  stochastic models", "comments": "23 pages, 7 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "q-bio.NC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study the impact of noise on a neural population rate model of up and down\nstates. Up and down states are typically observed in neuronal networks as a\nslow oscillation, where the population switches between high and low firing\nrates (Sanchez-Vivez and McCormick, 2000). A neural population model with spike\nrate adaptation is used to model such slow oscillations, and the timescale of\nadaptation determines the oscillation period. Furthermore, the period depends\nnon-monotonically on the background tonic input driving the population, having\nlong periods for very weak and very strong stimuli. Using both linearization\nand fast-slow timescale separation methods, we can compute the phase\nsensitivity function of the slow oscillation. We find that the phase response\nis most strongly impacted by perturbations to the adaptation variable. Phase\nsensitivity functions can then be utilized to quantify the impact of noise on\noscillating populations. Noise alters the period of oscillations by speeding up\nthe rate of transition between the up and down states. When common noise is\npresented to two distinct populations, their transitions will eventually become\nentrained to one another through stochastic synchrony.\n", "versions": [{"version": "v1", "created": "Thu, 23 Apr 2015 18:36:01 GMT"}], "update_date": "2015-04-24", "authors_parsed": [["McCleney", "Zachary", ""], ["Kilpatrick", "Zachary P.", ""]]}, {"id": "1504.06732", "submitter": "Paul Moore", "authors": "P. J. Moore", "title": "A predictive coding account of OCD", "comments": "arXiv admin note: substantial text overlap with arXiv:1503.00999", "journal-ref": null, "doi": null, "report-no": null, "categories": "q-bio.NC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper presents a predictive coding account of obsessive-compulsive\ndisorder (OCD). We extend the predictive coding model to include the concept of\na 'formal narrative', or temporal sequence of cognitive states inferred from\nsense data. We propose that human cognition uses a hierarchy of narratives to\npredict changes in the natural and social environment. Each layer in the\nhierarchy represents a distinct view of the world, but it also contributes to a\nglobal unitary perspective. We suggest that the global perspective remains\nintact in OCD but there is a dysfunction at a sub-linguistic level of\ncognition. The consequent failure of recognition is experienced as the external\nworld being 'not just right', and its automatic correction is felt as\ncompulsion. A wide variety of symptoms and some neuropsychological findings are\nthus explained by a single dysfunction. We conclude that the model provides a\ndeeper explanation for behavioural observations than current models, and that\nit has potential for further development for application to neuropsychological\ndata.\n", "versions": [{"version": "v1", "created": "Sat, 25 Apr 2015 14:40:07 GMT"}, {"version": "v2", "created": "Mon, 8 Jun 2015 14:15:16 GMT"}], "update_date": "2015-06-09", "authors_parsed": [["Moore", "P. J.", ""]]}, {"id": "1504.07089", "submitter": "Alessandro Farini", "authors": "Tito Arecchi, Alessandro Farini, Nicola Megna", "title": "A test of multiple correlation temporal window characteristic of\n  non-Markov processes", "comments": "arXiv admin note: substantial text overlap with arXiv:1204.4559", "journal-ref": null, "doi": null, "report-no": null, "categories": "q-bio.NC physics.data-an", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We introduce a sensitive test of memory effects in successive events. The\ntest consists of a combination K of binary correlations at successive times. K\ndecays monotonically from K = 1 for uncorrelated events as a Markov process;\nwhereas memory effects provide a temporal window with K > 1. For a monotonic\nmemory fading, K < 1 always. Here we report evidence of a K > 1 temporal window\nin cognitive tasks consisting of the visual identification of the front face of\nthe Necker cube after a previous presentation of the same. The K > 1 behaviour\nis maximal at an inter-measurement time {\\tau} around 2 sec with inter-subject\ndifferences. The K > 1 persists over a time window of 1 sec around {\\tau};\noutside this window the K < 1 behaviour is recovered. The universal occurrence\nof a K > 1 window in pairs of successive perceptions suggests that, at variance\nwith single visual stimuli eliciting a suitable response, a pair of stimuli\nshortly separated in time displays mutual correlations.\n", "versions": [{"version": "v1", "created": "Mon, 27 Apr 2015 13:52:52 GMT"}], "update_date": "2015-04-28", "authors_parsed": [["Arecchi", "Tito", ""], ["Farini", "Alessandro", ""], ["Megna", "Nicola", ""]]}, {"id": "1504.07422", "submitter": "Ehtibar Dzhafarov", "authors": "Ehtibar Dzhafarov, Ru Zhang, and Janne Kujala", "title": "Is there contextuality in behavioral and social systems?", "comments": "To be published in Phil. Trans. R. Soc. A, text with supplementary\n  files is not the journal's format", "journal-ref": "Phil. Trans. R. Soc. A 374: 20150099, 2015", "doi": "10.1098/rsta.2015.0099", "report-no": null, "categories": "q-bio.NC math.PR quant-ph", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Most behavioral and social experiments aimed at revealing contextuality are\nconfined to cyclic systems with binary outcomes. In quantum physics, this broad\nclass of systems includes as special cases\nKlyachko-Can-Binicioglu-Shumovsky-type, Einstein-Podolsky-Rosen-Bell-type, and\nSuppes-Zanotti-Leggett-Garg-type systems. The theory of contextuality known as\nContextuality-by-Default allows one to define and measure contextuality in all\nsuch system, even if there are context-dependent errors in measurements, or if\nsomething in the contexts directly interacts with the measurements. This makes\nthe theory especially suitable for behavioral and social systems, where direct\ninteractions of \"everything with everything\" are ubiquitous. For cyclic systems\nwith binary outcomes the theory provides necessary and sufficient conditions\nfor noncontextuality, and these conditions are known to be breached in certain\nquantum systems. We review several behavioral and social data sets (from polls\nof public opinion to visual illusions to conjoint choices to word combinations\nto psychophysical matching), and none of these data provides any evidence for\ncontextuality. Our working hypothesis is that this may be a broadly applicable\nrule: behavioral and social systems are noncontextual, i.e., all \"contextual\neffects\" in them result from the ubiquitous dependence of response\ndistributions on the elements of contexts other than the ones to which the\nresponse is presumably or normatively directed.\n", "versions": [{"version": "v1", "created": "Tue, 28 Apr 2015 10:59:51 GMT"}, {"version": "v2", "created": "Wed, 29 Apr 2015 23:34:43 GMT"}, {"version": "v3", "created": "Mon, 25 May 2015 19:55:31 GMT"}, {"version": "v4", "created": "Sat, 13 Jun 2015 23:15:08 GMT"}, {"version": "v5", "created": "Sun, 23 Aug 2015 09:01:41 GMT"}], "update_date": "2016-02-12", "authors_parsed": [["Dzhafarov", "Ehtibar", ""], ["Zhang", "Ru", ""], ["Kujala", "Janne", ""]]}, {"id": "1504.07523", "submitter": "Luca Salasnich", "authors": "Luca Salasnich", "title": "Power spectrum and diffusion of the Amari neural field", "comments": "8 pages, 2 figures, improved version with inclusion of\n  reaction-diffusion equation and dual neural field. To be published in the\n  open access journal Symmetry", "journal-ref": "Symmetry 11, 134 (2019)", "doi": "10.3390/sym11020134", "report-no": null, "categories": "q-bio.NC nlin.PS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study the power spectrum of a space-time dependent neural field which\ndescribes the average membrane potential of neurons in a single layer. This\nneural field is modelled by a dissipative integro-differential equation, the\nso-called Amari equation. By considering a small perturbation with respect to a\nstationary and uniform configuration of the neural field we derive a linearized\nequation which is solved for a generic external stimulus by using the Fourier\ntransform into wavevector-freqency domain, finding an analytical formula for\nthe power spectrum of the neural field. In addition, after proving that for\nlarge wavelengths the linearized Amari equation is equivalent to a diffusion\nequation which admits space-time dependent analytical solutions, we take into\naccount the nonlinearity of the Amari equation. We find that for large\nwavelengths a weak nonlinearity in the Amari equation gives rise to a\nreaction-diffusion equation which can be formally derived from a neural action\nfunctional by introducing a dual neural field. For some initial conditions, we\ndiscuss analytical solutions of this reaction-diffusion equation.\n", "versions": [{"version": "v1", "created": "Tue, 28 Apr 2015 15:11:40 GMT"}, {"version": "v2", "created": "Thu, 30 Jul 2015 10:21:56 GMT"}, {"version": "v3", "created": "Mon, 31 Aug 2015 12:46:11 GMT"}, {"version": "v4", "created": "Thu, 27 Jul 2017 15:28:41 GMT"}, {"version": "v5", "created": "Mon, 31 Jul 2017 10:21:25 GMT"}, {"version": "v6", "created": "Thu, 24 Jan 2019 17:21:06 GMT"}], "update_date": "2019-01-29", "authors_parsed": [["Salasnich", "Luca", ""]]}, {"id": "1504.08255", "submitter": "Takahiro Wada", "authors": "Takahiro Wada, Hiroyuki Konno, Satoru Fujisawa, Shunichi Doi", "title": "Can Passenger's Active Head Tilt Decrease The Severity of Carsickness? -\n  Effect of Head Tilt on Severity of Motion Sickness in a Lateral Acceleration\n  Environment", "comments": null, "journal-ref": "Human Factors, 54(2), pp.71-78, 2012", "doi": "10.1177/0018720812436584", "report-no": null, "categories": "q-bio.NC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Objective: We investigated the effect of the passenger head-tilt strategy on\nthe severity of carsickness in lateral acceleration situations in automobiles.\nBackground: It is well known that the driver is generally less susceptible to\ncarsickness than are the passengers. However, it is also known that the driver\ntilts his or her head toward the curve center when negotiating a curve, whereas\nthe passenger's head moves in the opposite direction. Therefore, we\nhypothesized that the head-tilt strategy has the effect of reducing the\nseverity of carsickness. Method: A passenger car was driven on a quasi-oval\ntrack with a pylon slalom while the participant sat in the navigator seat. The\nexperiment was terminated when either the participant felt the initial symptoms\nof motion sickness or the car finished 20 laps. In the natural head-tilt\ncondition, the participants were instructed to sit naturally, to relax, and not\nto oppose the lateral acceleration intentionally. In the active head-tilt\ncondition, the participants were asked to tilt their heads against the\ncentrifugal acceleration, thus imitating the driver's head tilt. Results: The\nnumber of laps achieved in the active condition was significantly greater than\nthat in the natural condition. In addition, the subjective ratings of motion\nsickness and symptoms in the active condition were significantly lower than\nthose in the natural condition. Conclusion: We suggest that an active head tilt\nagainst centrifugal acceleration reduces the severity of motion sickness.\nApplication: Potential applications of this study include development of a\nmethodology to reduce carsickness.\n", "versions": [{"version": "v1", "created": "Thu, 30 Apr 2015 14:51:51 GMT"}], "update_date": "2015-05-01", "authors_parsed": [["Wada", "Takahiro", ""], ["Konno", "Hiroyuki", ""], ["Fujisawa", "Satoru", ""], ["Doi", "Shunichi", ""]]}]