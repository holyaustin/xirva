[{"id": "1103.0182", "submitter": "Choongseok Park", "authors": "Choongseok Park, Robert M. Worth, Leonid L. Rubchinsky", "title": "Neural Dynamics in Parkinsonian Brain:The Boundary Between Synchronized\n  and Nonsynchronized Dynamics", "comments": null, "journal-ref": "Phys. Rev. E 83, 042901 (2011)", "doi": "10.1103/PhysRevE.83.042901", "report-no": null, "categories": "q-bio.NC nlin.AO physics.bio-ph", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Synchronous oscillatory dynamics is frequently observed in the human brain.\nWe analyze the fine temporal structure of phase-locking in a realistic network\nmodel and match it with the experimental data from parkinsonian patients. We\nshow that the experimentally observed intermittent synchrony can be generated\njust by moderately increased coupling strength in the basal ganglia circuits\ndue to the lack of dopamine. Comparison of the experimental and modeling data\nsuggest that brain activity in Parkinson's disease resides in the large\nboundary region between synchronized and nonsynchronized dynamics. Being on the\nedge of synchrony may allow for easy formation of transient neuronal\nassemblies.\n", "versions": [{"version": "v1", "created": "Tue, 1 Mar 2011 14:51:16 GMT"}], "update_date": "2011-04-18", "authors_parsed": [["Park", "Choongseok", ""], ["Worth", "Robert M.", ""], ["Rubchinsky", "Leonid L.", ""]]}, {"id": "1103.0451", "submitter": "Roland Koberle", "authors": "Ingrid M. Esteves, Nelson M. Fernandes, Roland K\\\"oberle", "title": "How to take turns: the fly's way to encode and decode rotational\n  information", "comments": "16 pages including 5 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "q-bio.NC q-bio.QM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Sensory systems take continuously varying stimuli as their input and encode\nfeatures relevant for the organism's survival into a sequence of action\npotentials - spike trains. The full dynamic range of complex dynamical inputs\nhas to be compressed into a set of discrete spike times and the question,\nfacing any sensory system, arises: which features of the stimulus are thereby\nencoded and how does the animal decode them to recover its external sensory\nworld?\n  Here we study this issue for the two motion-sensitive H1 neurons of the fly's\noptical system, which are sensitive to horizontal velocity stimuli, each neuron\nresponding to oppositely pointing preferred directions. They constitute an\nefficient detector for rotations of the fly's body about a vertical axis.\nSurprisingly the spike trains $\\rho_B(t)$ generated by an empoverished stimulus\n$S_B(t)$, containing just the instants when the of velocity $S(t)$ reverses its\ndirection, convey the same amount of global (Shannon) information as spike\ntrains $\\rho(t)$ generated by the complete stimulus $S(t)$. This amount of\ninformation is just enough to encode the instants of velocity reversal. Yet\nthis suffices to give the motor system just one, yet vital order: go left or\nright, turning the H1 neurons into efficient analog-to-digital converters.\nFurthermore also probability distributions computed from $\\rho(t)$ and\n$\\rho_B(t)$ are identical. Still there are regions in the spike trains\nfollowing velocity reversals, 80 msec long and containing about 3-6 msec long\nspike intervals, where detailed stimulus properties are encoded. We suggest a\ndecoding scheme - how to reconstruct the stimulus from the spike train, which\nis fast and works in real time.\n", "versions": [{"version": "v1", "created": "Wed, 2 Mar 2011 15:11:59 GMT"}], "update_date": "2011-03-03", "authors_parsed": [["Esteves", "Ingrid M.", ""], ["Fernandes", "Nelson M.", ""], ["K\u00f6berle", "Roland", ""]]}, {"id": "1103.0668", "submitter": "Wei Wei", "authors": "Wei Wei and Fred Wolf", "title": "Spike Onset Dynamics and Response Speed in Neuronal Populations", "comments": null, "journal-ref": "Phys. Rev. Lett. 106, 088102 (2011)", "doi": "10.1103/PhysRevLett.106.088102", "report-no": null, "categories": "q-bio.NC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recent studies of cortical neurons driven by fluctuating currents revealed\ncutoff frequencies for action potential encoding of several hundred Hz.\nTheoretical studies of biophysical neuron models have predicted a much lower\ncutoff frequency of the order of average firing rate or the inverse membrane\ntime constant. The biophysical origin of the observed high cutoff frequencies\nis thus not well understood. Here we introduce a neuron model with dynamical\naction potential generation, in which the linear response can be analytically\ncalculated for uncorrelated synaptic noise. We find that the cutoff frequencies\nincrease to very large values when the time scale of action potential\ninitiation becomes short.\n", "versions": [{"version": "v1", "created": "Thu, 3 Mar 2011 12:37:57 GMT"}], "update_date": "2015-05-27", "authors_parsed": [["Wei", "Wei", ""], ["Wolf", "Fred", ""]]}, {"id": "1103.1167", "submitter": "Alexander Bershadskii", "authors": "A. Bershadskii", "title": "Prime numbers and spontaneous neuron activity", "comments": "extended", "journal-ref": "Adv. Math. Phys., 519178, (2011)", "doi": "10.1155/2011/519178", "report-no": null, "categories": "q-bio.NC math.NT nlin.CD", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Logarithmic gaps have been used in order to find a periodic component of the\nsequence of prime numbers, hidden by a random noise (stochastic or chaotic). It\nis shown that multiplicative nature of the noise is the main reason for the\nsuccessful application of the logarithmic gaps transforming the multiplicative\nnoise into an additive one. A relation of this phenomenon to spontaneous neuron\nactivity and to chaotic brain computations has been discussed.\n", "versions": [{"version": "v1", "created": "Sun, 6 Mar 2011 21:56:07 GMT"}, {"version": "v2", "created": "Tue, 15 Mar 2011 12:02:21 GMT"}, {"version": "v3", "created": "Fri, 6 May 2011 15:16:57 GMT"}], "update_date": "2012-06-26", "authors_parsed": [["Bershadskii", "A.", ""]]}, {"id": "1103.1791", "submitter": "Chris Adami", "authors": "Jeffrey Edlund, Nicolas Chaumont, Arend Hintze, Christof Koch, Giulio\n  Tononi, and Christoph Adami", "title": "Integrated information increases with fitness in the evolution of\n  animats", "comments": "27 pages, 8 figures, one supplementary figure. Three supplementary\n  video files available on request. Version commensurate with published text in\n  PLoS Comput. Biol", "journal-ref": "PLoS Computational Biology 7 (2001) e1002236", "doi": "10.1371/journal.pcbi.1002236", "report-no": null, "categories": "q-bio.PE cs.AI nlin.AO q-bio.NC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  One of the hallmarks of biological organisms is their ability to integrate\ndisparate information sources to optimize their behavior in complex\nenvironments. How this capability can be quantified and related to the\nfunctional complexity of an organism remains a challenging problem, in\nparticular since organismal functional complexity is not well-defined. We\npresent here several candidate measures that quantify information and\nintegration, and study their dependence on fitness as an artificial agent\n(\"animat\") evolves over thousands of generations to solve a navigation task in\na simple, simulated environment. We compare the ability of these measures to\npredict high fitness with more conventional information-theoretic processing\nmeasures. As the animat adapts by increasing its \"fit\" to the world,\ninformation integration and processing increase commensurately along the\nevolutionary line of descent. We suggest that the correlation of fitness with\ninformation integration and with processing measures implies that high fitness\nrequires both information processing as well as integration, but that\ninformation integration may be a better measure when the task requires memory.\nA correlation of measures of information integration (but also information\nprocessing) and fitness strongly suggests that these measures reflect the\nfunctional complexity of the animat, and that such measures can be used to\nquantify functional complexity even in the absence of fitness data.\n", "versions": [{"version": "v1", "created": "Wed, 9 Mar 2011 14:33:21 GMT"}, {"version": "v2", "created": "Mon, 3 Oct 2011 16:59:02 GMT"}], "update_date": "2011-11-08", "authors_parsed": [["Edlund", "Jeffrey", ""], ["Chaumont", "Nicolas", ""], ["Hintze", "Arend", ""], ["Koch", "Christof", ""], ["Tononi", "Giulio", ""], ["Adami", "Christoph", ""]]}, {"id": "1103.2070", "submitter": "Dante Chialvo", "authors": "Enzo Tagliazucchi, Dante R. Chialvo", "title": "The collective brain is critical", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "q-bio.NC cond-mat.dis-nn physics.bio-ph", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The unique dynamical features of the critical state endow the brain with\nproperties which are fundamental for adaptive behavior. This proposal, put\nforward with Per Bak several years ago, is now supported by a wide body of\nempirical evidence at different scales demonstrating that the spatiotemporal\nbrain dynamics exhibits key signatures of critical dynamics previously\nrecognized in other complex systems. The rationale behind this program is\ndiscussed in these notes, followed by an account of the most recent results,\ntogether with a discussion of the physiological significance of these ideas.\n", "versions": [{"version": "v1", "created": "Thu, 10 Mar 2011 16:23:58 GMT"}], "update_date": "2011-03-15", "authors_parsed": [["Tagliazucchi", "Enzo", ""], ["Chialvo", "Dante R.", ""]]}, {"id": "1103.2366", "submitter": "Gerhard Werner MD", "authors": "Gerhard Werner", "title": "Consciousness Viewed in the Framework of Brain Phase Space Dynamics,\n  Criticality, and the Renormalization Group", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "q-bio.NC cond-mat.dis-nn", "license": "http://creativecommons.org/licenses/by/3.0/", "abstract": "  To set the stage for viewing Consciousness in terms of brain phase space\ndynamics and criticality, I will first review currently prominent theoretical\nconceptualizations and, where appropriate, identify ill-advised and flawed\nnotions in Theoretical Neuroscience that may impede viewing Consciousness as a\nphenomenon in Physics. I will furthermore introduce relevant facts that tend\nnot to receive adequate attention in much of the current Consciousness\ndiscourse. As a new approach to conceptualizing Consciousness, I propose\nconsidering it as a collective achievement of the brain' s complex neural\ndynamics that is amenable to study in the framework of state space dynamics and\ncriticality. In Physics, concepts of phase space transitions and the\nRenormalization Group are powerful tools for interpreting phenomena involving\nmany scales of length and time in complex systems. The significance of these\nconcepts lies in their accounting for the emergence of different levels of new\ncollective behaviors in complex systems, each level with its distinct ontology,\norganization and laws, as a new pattern of reality. The presumption of this\nproposal is that the subjectivity of Consciousness is the epistemic\ninterpretation of a level of reality that originates in phase transitions of\nthe brain-body-environment system.\n", "versions": [{"version": "v1", "created": "Mon, 7 Mar 2011 23:51:22 GMT"}], "update_date": "2011-03-14", "authors_parsed": [["Werner", "Gerhard", ""]]}, {"id": "1103.2373", "submitter": "Leonid Perlovsky", "authors": "Leonid Perlovsky and Daniel Levine", "title": "Drive for Creativity", "comments": "9 pages, 1 fig", "journal-ref": null, "doi": null, "report-no": null, "categories": "q-bio.NC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We advance a hypothesis that creativity has evolved with evolution of\ninternal representations, possibly from amniotes to primates, and further in\nhuman cultural evolution. Representations separated sensing from acting and\ngave \"internal room\" for creativity. To see (or perform any sensing), creatures\nwith internal representations had to modify these representations to fit sensor\nsignals. Therefore the knowledge instinct, KI, the drive to fit representations\nto the world, had to evolve along with internal representations. Until\nprimates, it remained simple, without language internal representations could\nnot evolve from perceptions to abstract representations, and abstract thoughts\nwere not possible. We consider creative vs. non-creative decision making, and\ncompare KI with Kahneman-Tversky's heuristic thinking. We identify higher,\nconscious levels of KI with the drive for creativity (DC) and discuss the roles\nof language and music, brain mechanisms involved, and experimental directions\nfor testing the advanced hypotheses.\n", "versions": [{"version": "v1", "created": "Fri, 11 Mar 2011 21:05:18 GMT"}], "update_date": "2011-03-15", "authors_parsed": [["Perlovsky", "Leonid", ""], ["Levine", "Daniel", ""]]}, {"id": "1103.2382", "submitter": "Francois Rivest", "authors": "Francois Rivest and Yoshua Bengio", "title": "Adaptive Drift-Diffusion Process to Learn Time Intervals", "comments": "9 pages, 4 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "q-bio.NC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Animals learn the timing between consecutive events very easily. Their\nprecision is usually proportional to the interval to time (Weber's law for\ntiming). Most current timing models either require a central clock and\nunbounded accumulator or whole pre-defined populations of delay lines, decaying\ntraces or oscillators to represent elapsing time. Current adaptive recurrent\nneural networks fail at learning to predict the timing of future events (the\n'when') in a realistic manner. In this paper, we present a new model of\ninterval timing, based on simple temporal integrators, derived from\ndrift-diffusion models. We develop a simple geometric rule to learn 'when'\ninstead of 'what'. We provide an analytical proof that the model can learn\ninter-event intervals in a number of trials independent of the interval size\nand that the temporal precision of the system is proportional to the timed\ninterval. This new model uses no clock, no gradient, no unbounded accumulators,\nno delay lines, and has internal noise allowing generations of individual\ntrials. Three interesting predictions are made.\n", "versions": [{"version": "v1", "created": "Fri, 11 Mar 2011 21:35:45 GMT"}], "update_date": "2011-03-15", "authors_parsed": [["Rivest", "Francois", ""], ["Bengio", "Yoshua", ""]]}, {"id": "1103.2605", "submitter": "Jesus M. Cortes", "authors": "J.M. Cortes and D. Marinazzo and P. Series and M.W. Oram and T.J.\n  Sejnowski and M.C.W. van Rossum", "title": "The effect of neural adaptation of population coding accuracy", "comments": "35 pages, 8 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "q-bio.NC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Most neurons in the primary visual cortex initially respond vigorously when a\npreferred stimulus is presented, but adapt as stimulation continues. The\nfunctional consequences of adaptation are unclear. Typically a reduction of\nfiring rate would reduce single neuron accuracy as less spikes are available\nfor decoding, but it has been suggested that on the population level,\nadaptation increases coding accuracy. This question requires careful analysis\nas adaptation not only changes the firing rates of neurons, but also the neural\nvariability and correlations between neurons, which affect coding accuracy as\nwell. We calculate the coding accuracy using a computational model that\nimplements two forms of adaptation: spike frequency adaptation and synaptic\nadaptation in the form of short-term synaptic plasticity. We find that the net\neffect of adaptation is subtle and heterogeneous. Depending on adaptation\nmechanism and test stimulus, adaptation can either increase or decrease coding\naccuracy. We discuss the neurophysiological and psychophysical implications of\nthe findings and relate it to published experimental data.\n", "versions": [{"version": "v1", "created": "Mon, 14 Mar 2011 09:02:42 GMT"}], "update_date": "2011-03-15", "authors_parsed": [["Cortes", "J. M.", ""], ["Marinazzo", "D.", ""], ["Series", "P.", ""], ["Oram", "M. W.", ""], ["Sejnowski", "T. J.", ""], ["van Rossum", "M. C. W.", ""]]}, {"id": "1103.2852", "submitter": "Roberto D. Pascual-Marqui", "authors": "Roberto D. Pascual-Marqui and Rolando J. Biscay-Lirio", "title": "Interaction patterns of brain activity across space, time and frequency.\n  Part I: methods", "comments": "Technical report 2011-March-15, The KEY Institute for Brain-Mind\n  Research Zurich, KMU Osaka", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME math.ST q-bio.NC stat.TH", "license": "http://creativecommons.org/licenses/by-nc-sa/3.0/", "abstract": "  We consider exploratory methods for the discovery of cortical functional\nconnectivity. Typically, data for the i-th subject (i=1...NS) is represented as\nan NVxNT matrix Xi, corresponding to brain activity sampled at NT moments in\ntime from NV cortical voxels. A widely used method of analysis first\nconcatenates all subjects along the temporal dimension, and then performs an\nindependent component analysis (ICA) for estimating the common cortical\npatterns of functional connectivity. There exist many other interesting\nvariations of this technique, as reviewed in [Calhoun et al. 2009 Neuroimage\n45: S163-172]. We present methods for the more general problem of discovering\nfunctional connectivity occurring at all possible time lags. For this purpose,\nbrain activity is viewed as a function of space and time, which allows the use\nof the relatively new techniques of functional data analysis [Ramsay &\nSilverman 2005: Functional data analysis. New York: Springer]. In essence, our\nmethod first vectorizes the data from each subject, which constitutes the\nnatural discrete representation of a function of several variables, followed by\nconcatenation of all subjects. The singular value decomposition (SVD), as well\nas the ICA of this new matrix of dimension [rows=(NT*NV); columns=NS] will\nreveal spatio-temporal patterns of connectivity. As a further example, in the\ncase of EEG neuroimaging, Xi of size NVxNW may represent spectral density for\nelectric neuronal activity at NW discrete frequencies from NV cortical voxels,\nfrom the i-th EEG epoch. In this case our functional data analysis approach\nwould reveal coupling of brain regions at possibly different frequencies.\n", "versions": [{"version": "v1", "created": "Tue, 15 Mar 2011 06:36:17 GMT"}, {"version": "v2", "created": "Wed, 16 Mar 2011 01:02:17 GMT"}], "update_date": "2011-03-17", "authors_parsed": [["Pascual-Marqui", "Roberto D.", ""], ["Biscay-Lirio", "Rolando J.", ""]]}, {"id": "1103.3634", "submitter": "Ido Kanter", "authors": "I. Kanter, E. Kopelowitz, R. Vardi, M. Zigzag, W. Kinzel, M. Abeles\n  and D. Cohen", "title": "Nonlocal mechanism for cluster synchronization in neural circuits", "comments": "8 pges, 6 figures", "journal-ref": "EPL 93, 66001 (2011)", "doi": "10.1209/0295-5075/93/66001", "report-no": null, "categories": "nlin.CD q-bio.NC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The interplay between the topology of cortical circuits and synchronized\nactivity modes in distinct cortical areas is a key enigma in neuroscience. We\npresent a new nonlocal mechanism governing the periodic activity mode: the\ngreatest common divisor (GCD) of network loops. For a stimulus to one node, the\nnetwork splits into GCD-clusters in which cluster neurons are in zero-lag\nsynchronization. For complex external stimuli, the number of clusters can be\nany common divisor. The synchronized mode and the transients to synchronization\npinpoint the type of external stimuli. The findings, supported by an\ninformation mixing argument and simulations of Hodgkin Huxley population\ndynamic networks with unidirectional connectivity and synaptic noise, call for\nreexamining sources of correlated activity in cortex and shorter information\nprocessing time scales.\n", "versions": [{"version": "v1", "created": "Fri, 18 Mar 2011 14:41:39 GMT"}], "update_date": "2011-03-21", "authors_parsed": [["Kanter", "I.", ""], ["Kopelowitz", "E.", ""], ["Vardi", "R.", ""], ["Zigzag", "M.", ""], ["Kinzel", "W.", ""], ["Abeles", "M.", ""], ["Cohen", "D.", ""]]}, {"id": "1103.3886", "submitter": "Ozgur Doruk R", "authors": "Resat Ozgur Doruk", "title": "Control of the repetitive firing in the squid giant axon using\n  electrical fields", "comments": "14 pages, 3 figures, 2 tables, submitted to journal of Computer\n  Methods and Programs in Biomedicine", "journal-ref": null, "doi": null, "report-no": null, "categories": "q-bio.NC physics.bio-ph", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this research, the aim is to develop a repetitive firing stopper mechanism\nusing electrical fields exerted on the fiber. The Hodgkin - Huxley nerve fiber\nmodel is used for modeling the membrane potential behavior. The repetitive\nfiring of the nerve fiber can be stopped using approaches based on the control\ntheory where the nonlinear Hodgkin - Huxley model is used to achieve this goal.\nThe effects of the electrical field are considered as an additive quantity over\nthe equilibrium potentials of the cell membrane channels. The study is a\nrepresentative of an experimental application.\n", "versions": [{"version": "v1", "created": "Sun, 20 Mar 2011 20:46:29 GMT"}], "update_date": "2015-03-19", "authors_parsed": [["Doruk", "Resat Ozgur", ""]]}, {"id": "1103.5112", "submitter": "Mikail Rubinov", "authors": "Mikail Rubinov and Olaf Sporns", "title": "Weight-conserving characterization of complex functional brain networks", "comments": "NeuroImage, in press", "journal-ref": "Neuroimage. 2011 Jun 15;56(4):2068-79. Epub 2011 Apr 1", "doi": "10.1016/j.neuroimage.2011.03.069", "report-no": null, "categories": "q-bio.NC cond-mat.dis-nn physics.data-an", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Complex functional brain networks are large networks of brain regions and\nfunctional brain connections. Statistical characterizations of these networks\naim to quantify global and local properties of brain activity with a small\nnumber of network measures. Important functional network measures include\nmeasures of modularity (measures of the goodness with which a network is\noptimally partitioned into functional subgroups) and measures of centrality\n(measures of the functional influence of individual brain regions).\nCharacterizations of functional networks are increasing in popularity, but are\nassociated with several important methodological problems. These problems\ninclude the inability to characterize densely connected and weighted functional\nnetworks, the neglect of degenerate topologically distinct high-modularity\npartitions of these networks, and the absence of a network null model for\ntesting hypotheses of association between observed nontrivial network\nproperties and simple weighted connectivity properties. In this study we\ndescribe a set of methods to overcome these problems. Specifically, we\ngeneralize measures of modularity and centrality to fully connected and\nweighted complex networks, describe the detection of degenerate high-modularity\npartitions of these networks, and introduce a weighted-connectivity null model\nof these networks. We illustrate our methods by demonstrating degenerate\nhigh-modularity partitions and strong correlations between two complementary\nmeasures of centrality in resting-state functional magnetic resonance imaging\n(MRI) networks from the 1000 Functional Connectomes Project, an open-access\nrepository of resting-state functional MRI datasets. Our methods may allow more\nsound and reliable characterizations and comparisons of functional brain\nnetworks across conditions and subjects.\n", "versions": [{"version": "v1", "created": "Sat, 26 Mar 2011 06:57:37 GMT"}], "update_date": "2011-07-19", "authors_parsed": [["Rubinov", "Mikail", ""], ["Sporns", "Olaf", ""]]}, {"id": "1103.5279", "submitter": "Chun-Chung Chen", "authors": "Chun-Chung Chen and David Jasnow", "title": "Event-driven simulations of a plastic, spiking neural network", "comments": "9 pages, 6 figures", "journal-ref": "Phys. Rev. E 84, 031908 (2011)", "doi": "10.1103/PhysRevE.84.031908", "report-no": null, "categories": "q-bio.NC cond-mat.dis-nn physics.bio-ph", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider a fully-connected network of leaky integrate-and-fire neurons\nwith spike-timing-dependent plasticity. The plasticity is controlled by a\nparameter representing the expected weight of a synapse between neurons that\nare firing randomly with the same mean frequency. For low values of the\nplasticity parameter, the activities of the system are dominated by noise,\nwhile large values of the plasticity parameter lead to self-sustaining activity\nin the network. We perform event-driven simulations on finite-size networks\nwith up to 128 neurons to find the stationary synaptic weight conformations for\ndifferent values of the plasticity parameter. In both the low and high activity\nregimes, the synaptic weights are narrowly distributed around the plasticity\nparameter value consistent with the predictions of mean-field theory. However,\nthe distribution broadens in the transition region between the two regimes,\nrepresenting emergent network structures. Using a pseudophysical approach for\nvisualization, we show that the emergent structures are of \"path\" or \"hub\"\ntype, observed at different values of the plasticity parameter in the\ntransition region.\n", "versions": [{"version": "v1", "created": "Mon, 28 Mar 2011 04:22:50 GMT"}], "update_date": "2011-09-23", "authors_parsed": [["Chen", "Chun-Chung", ""], ["Jasnow", "David", ""]]}, {"id": "1103.5934", "submitter": "Christian Meisel", "authors": "Christian Kuehn and Christian Meisel", "title": "On spatial and temporal multilevel dynamics and scaling effects in\n  epileptic seizures", "comments": "24 pages, 9 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "q-bio.NC math.DS nlin.CD nlin.PS physics.med-ph", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Epileptic seizures are one of the most well-known dysfunctions of the nervous\nsystem. During a seizure, a highly synchronized behavior of neural activity is\nobserved that can cause symptoms ranging from mild sensual malfunctions to the\ncomplete loss of body control. In this paper, we aim to contribute towards a\nbetter understanding of the dynamical systems phenomena that cause seizures.\nBased on data analysis and modelling, seizure dynamics can be identified to\npossess multiple spatial scales and on each spatial scale also multiple time\nscales. At each scale, we reach several novel insights. On the smallest spatial\nscale we consider single model neurons and investigate early-warning signs of\nspiking. This introduces the theory of critical transitions to excitable\nsystems. For clusters of neurons (or neuronal regions) we use patient data and\nfind oscillatory behavior and new scaling laws near the seizure onset. These\nscalings lead to substantiate the conjecture obtained from mean-field models\nthat a Hopf bifurcation could be involved near seizure onset. On the largest\nspatial scale we introduce a measure based on phase-locking intervals and\nwavelets into seizure modelling. It is used to resolve synchronization between\ndifferent regions in the brain and identifies time-shifted scaling laws at\ndifferent wavelet scales. We also compare our wavelet-based multiscale approach\nwith maximum linear cross-correlation and mean-phase coherence measures.\n", "versions": [{"version": "v1", "created": "Wed, 30 Mar 2011 14:12:25 GMT"}, {"version": "v2", "created": "Wed, 6 Apr 2011 09:26:48 GMT"}, {"version": "v3", "created": "Fri, 29 Jul 2011 15:21:45 GMT"}], "update_date": "2011-08-01", "authors_parsed": [["Kuehn", "Christian", ""], ["Meisel", "Christian", ""]]}, {"id": "1103.5952", "submitter": "Vahid Salari", "authors": "I. Bokkon, R.L.P. Vimal, C. Wang, J. Dai, V. Salari, F. Grass, I.\n  Antal", "title": "Visible light induced ocular delayed bioluminescence as a possible\n  origin of negative afterimage", "comments": "accepted to be published in J. Photochem. Photobiol. B", "journal-ref": "J Photochem Photobiol B , 103 , 192-199, (2011)", "doi": null, "report-no": null, "categories": "physics.bio-ph q-bio.NC quant-ph", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The delayed luminescence of biological tissues is an ultraweak reemission of\nabsorbed photons after exposure to external monochromatic or white light\nillumination. Recently, Wang, B\\'okkon, Dai and Antal (Brain Res. 2011)\npresented the first experimental proof of the existence of spontaneous\nultraweak biophoton emission and visible light induced delayed ultraweak photon\nemission from in vitro freshly isolated rat's whole eye, lens, vitreous humor\nand retina. Here, we suggest that the photobiophysical source of negative\nafterimage can also occur within the eye by delayed bioluminescent photons. In\nother words, when we stare at a colored (or white) image for few seconds,\nexternal photons can induce excited electronic states within different parts of\nthe eye that is followed by a delayed reemission of absorbed photons for\nseveral seconds. Finally, these reemitted photons can be absorbed by\nnonbleached photoreceptors that produce a negative afterimage. Although this\nsuggests the photobiophysical source of negative afterimages is related retinal\nmechanisms, cortical neurons have also essential contribution in the\ninterpretation and modulation of negative afterimages.\n", "versions": [{"version": "v1", "created": "Wed, 30 Mar 2011 14:56:50 GMT"}], "update_date": "2011-04-19", "authors_parsed": [["Bokkon", "I.", ""], ["Vimal", "R. L. P.", ""], ["Wang", "C.", ""], ["Dai", "J.", ""], ["Salari", "V.", ""], ["Grass", "F.", ""], ["Antal", "I.", ""]]}, {"id": "1103.6007", "submitter": "Marius Buliga", "authors": "Marius Buliga", "title": "Computing with space: a tangle formalism for chora and difference", "comments": "56 pages, added content and reorganized the paper, title changed,\n  many figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.MG cs.LO q-bio.NC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  What is space computing, simulation, or understanding? Converging from\nseveral sources, this seems to be something more primitive than what is usually\nmeant by computation, something that was along with us since antiquity (the\nword \"choros\", \"chora\", denotes \"space\" or \"place\" and is seemingly the most\nmysterious notion from Plato, described in Timaeus 48e - 53c) which has to do\nwith cybernetics and with the understanding of the front end visual system. It\nmay have some unexpected applications, also. Here, inspired by Bateson (see\nSupplementary Material), I explore from the mathematical side the point of view\nthat there is no difference between the map and the territory, but instead the\ntransformation of one into another can be understood by using a formalism of\ntangle diagrams.\n  This paper continues arXiv:1009.5028 \"What is a space? Computations in\nemergent algebras and the front end visual system\" and the arXiv:1007.2362\n\"Introduction to metric spaces with dilations\".\n", "versions": [{"version": "v1", "created": "Wed, 30 Mar 2011 18:06:30 GMT"}, {"version": "v2", "created": "Thu, 21 Apr 2011 12:35:32 GMT"}], "update_date": "2011-04-22", "authors_parsed": [["Buliga", "Marius", ""]]}]