[{"id": "1407.0644", "submitter": "Emin Orhan", "authors": "A. Emin Orhan, Robert A. Jacobs", "title": "Are Performance Limitations in Visual Short-Term Memory Tasks Due to\n  Capacity Limitations or Model Mismatch?", "comments": "42 pages, 23 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "q-bio.NC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Performance limitations in visual short-term memory (VSTM) tasks have\ntraditionally been explained in terms of resource or capacity limitations. It\nhas been claimed, for example, that VSTM possesses a limited amount of\ncognitive or neural \"resources\" that can be used to remember a visual display.\nIn this paper, we highlight the potential importance of a previously neglected\nfactor that might contribute significantly to performance limitations in VSTM\ntasks: namely, a mismatch between the prior expectations and/or the internal\nnoise properties of the visual system based primarily on its adaptation to the\nstatistics of the natural environment and the statistics of the visual stimuli\nused in most VSTM experiments. We call this 'model mismatch'. Surprisingly, we\nshow that model mismatch alone, without assuming a general resource or capacity\nlimitation, can, in principle, account for some of the main qualitative\ncharacteristics of performance limitations observed in VSTM tasks, including:\n(i) monotonic decline in memory precision with increasing set size; (ii)\nvariability in memory precision across items and trials; and (iii) different\nset-size dependencies for initial encoding rate and asymptotic precision when\nthe duration of image presentation is varied. We also investigate the\nconsequences of using experimental stimuli that more closely match the prior\nexpectations and/or internal noise properties of the visual system. The results\nreveal qualitatively very different patterns of behavior for such stimuli,\nsuggesting that researchers should be cautious about generalizing the results\nof experiments using ecologically unrealistic stimulus statistics to\necologically more realistic stimuli.\n", "versions": [{"version": "v1", "created": "Wed, 2 Jul 2014 16:49:33 GMT"}], "update_date": "2014-07-03", "authors_parsed": [["Orhan", "A. Emin", ""], ["Jacobs", "Robert A.", ""]]}, {"id": "1407.0733", "submitter": "Davide Barbieri", "authors": "Giacomo Cocci, Davide Barbieri, Giovanna Citti, Alessandro Sarti", "title": "Cortical spatio-temporal dimensionality reduction for visual grouping", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.NE q-bio.NC stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The visual systems of many mammals, including humans, is able to integrate\nthe geometric information of visual stimuli and to perform cognitive tasks\nalready at the first stages of the cortical processing. This is thought to be\nthe result of a combination of mechanisms, which include feature extraction at\nsingle cell level and geometric processing by means of cells connectivity. We\npresent a geometric model of such connectivities in the space of detected\nfeatures associated to spatio-temporal visual stimuli, and show how they can be\nused to obtain low-level object segmentation. The main idea is that of defining\na spectral clustering procedure with anisotropic affinities over datasets\nconsisting of embeddings of the visual stimuli into higher dimensional spaces.\nNeural plausibility of the proposed arguments will be discussed.\n", "versions": [{"version": "v1", "created": "Wed, 2 Jul 2014 22:07:06 GMT"}, {"version": "v2", "created": "Fri, 3 Oct 2014 16:46:41 GMT"}], "update_date": "2014-10-06", "authors_parsed": [["Cocci", "Giacomo", ""], ["Barbieri", "Davide", ""], ["Citti", "Giovanna", ""], ["Sarti", "Alessandro", ""]]}, {"id": "1407.0868", "submitter": "B\\'oris Marin", "authors": "B\\'oris Marin, Reynaldo Daniel Pinto, Robert C Elson, Eduardo Colli", "title": "Noise, transient dynamics, and the generation of realistic interspike\n  interval variation in square-wave burster neurons", "comments": "REVTeX4-1, 18 pages, 9 figures", "journal-ref": null, "doi": "10.1103/PhysRevE.90.042718", "report-no": null, "categories": "q-bio.NC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  First return maps of interspike intervals for biological neurons that\ngenerate repetitive bursts of impulses can display stereotyped structures\n(neuronal signatures). Such structures have been linked to the possibility of\nmulticoding and multifunctionality in neural networks that produce and control\nrhythmical motor patterns. In some cases, isolating the neurons from their\nsynaptic network revealsirregular, complex signatures that have been regarded\nas evidence of intrinsic, chaotic behavior.\n  We show that incorporation of dynamical noise into minimal neuron models of\nsquare-wave bursting (either conductance-based or abstract) produces signatures\nakin to those observed in biological examples, without the need for fine-tuning\nof parameters or ad hoc constructions for inducing chaotic activity. The form\nof the stochastic term is not strongly constrained, and can approximate several\npossible sources of noise, e.g. random channel gating or synaptic bombardment.\n  The cornerstone of this signature generation mechanism is the rich,\ntransient, but deterministic dynamics inherent in the square-wave\n(saddle-node/homoclinic) mode of neuronal bursting. We show that noise causes\nthe dynamics to populate a complex transient scaffolding or skeleton in state\nspace, even for models that (without added noise) generate only periodic\nactivity (whether in bursting or tonic spiking mode).\n", "versions": [{"version": "v1", "created": "Thu, 3 Jul 2014 11:26:11 GMT"}], "update_date": "2015-06-22", "authors_parsed": [["Marin", "B\u00f3ris", ""], ["Pinto", "Reynaldo Daniel", ""], ["Elson", "Robert C", ""], ["Colli", "Eduardo", ""]]}, {"id": "1407.0973", "submitter": "Laurence Aitchison", "authors": "Laurence Aitchison and M\\'at\\'e Lengyel", "title": "The Hamiltonian brain: efficient probabilistic inference with\n  excitatory-inhibitory neural circuit dynamics", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "q-bio.NC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Probabilistic inference offers a principled framework for understanding both\nbehaviour and cortical computation. However, two basic and ubiquitous\nproperties of cortical responses seem difficult to reconcile with probabilistic\ninference: neural activity displays prominent oscillations in response to\nconstant input, and large transient changes in response to stimulus onset. Here\nwe show that these dynamical behaviours may in fact be understood as hallmarks\nof the specific representation and algorithm that the cortex employs to perform\nprobabilistic inference. We demonstrate that a particular family of\nprobabilistic inference algorithms, Hamiltonian Monte Carlo (HMC), naturally\nmaps onto the dynamics of excitatory-inhibitory neural networks. Specifically,\nwe constructed a model of an excitatory-inhibitory circuit in primary visual\ncortex that performed HMC inference, and thus inherently gave rise to\noscillations and transients. These oscillations were not mere epiphenomena but\nserved an important functional role: speeding up inference by rapidly spanning\na large volume of state space. Inference thus became an order of magnitude more\nefficient than in a non-oscillatory variant of the model. In addition, the\nnetwork matched two specific properties of observed neural dynamics that would\notherwise be difficult to account for in the context of probabilistic\ninference. First, the frequency of oscillations as well as the magnitude of\ntransients increased with the contrast of the image stimulus. Second,\nexcitation and inhibition were balanced, and inhibition lagged excitation.\nThese results suggest a new functional role for the separation of cortical\npopulations into excitatory and inhibitory neurons, and for the neural\noscillations that emerge in such excitatory-inhibitory networks: enhancing the\nefficiency of cortical computations.\n", "versions": [{"version": "v1", "created": "Thu, 3 Jul 2014 16:13:13 GMT"}, {"version": "v2", "created": "Fri, 4 Jul 2014 06:57:48 GMT"}, {"version": "v3", "created": "Sat, 31 Dec 2016 15:44:36 GMT"}], "update_date": "2017-01-03", "authors_parsed": [["Aitchison", "Laurence", ""], ["Lengyel", "M\u00e1t\u00e9", ""]]}, {"id": "1407.1195", "submitter": "Irene Gannaz", "authors": "Ir\\`ene Gannaz (ICJ, GIPSA-lab)", "title": "Classification of EEG recordings in auditory brain activity via a\n  logistic functional linear regression model", "comments": null, "journal-ref": "International Workshop on Functional and Operatorial Statistics,\n  Italy (2014)", "doi": null, "report-no": null, "categories": "stat.AP math.ST q-bio.NC stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We want to analyse EEG recordings in order to investigate the phonemic\ncategorization at a very early stage of auditory processing. This problem can\nbe modelled by a supervised classification of functional data. Discrimination\nis explored via a logistic functional linear model, using a wavelet\nrepresentation of the data. Different procedures are investigated, based on\npenalized likelihood and principal component reduction or partial least squares\nreduction.\n", "versions": [{"version": "v1", "created": "Fri, 4 Jul 2014 11:33:46 GMT"}], "update_date": "2014-07-07", "authors_parsed": [["Gannaz", "Ir\u00e8ne", "", "ICJ, GIPSA-lab"]]}, {"id": "1407.1333", "submitter": "Lennaert van Veen", "authors": "Lennaert van Veen and Kevin Green", "title": "Periodic solutions to a mean-field model for electrocortical activity", "comments": "9 pages, 5 figures", "journal-ref": null, "doi": "10.1140/epjst/e2014-02311-y", "report-no": null, "categories": "q-bio.NC nlin.PS physics.bio-ph", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider a continuum model of electrical signals in the human cortex,\nwhich takes the form of a system of semilinear, hyperbolic partial differential\nequations for the inhibitory and excitatory membrane potentials and the\nsynaptic inputs. The coupling of these components is represented by sigmoidal\nand quadratic nonlinearities. We consider these equations on a square domain\nwith periodic boundary conditions, in the vicinity of the primary transition\nfrom a stable equilibrium to time-periodic motion through an equivariant Hopf\nbifurcation. We compute part of a family of standing wave solutions, emanating\nfrom this point.\n", "versions": [{"version": "v1", "created": "Fri, 4 Jul 2014 21:24:53 GMT"}], "update_date": "2015-06-22", "authors_parsed": [["van Veen", "Lennaert", ""], ["Green", "Kevin", ""]]}, {"id": "1407.1549", "submitter": "Somwrita Sarkar", "authors": "S. Sarkar, S. Chawla and H. Weng", "title": "Resilience of human brain functional coactivation networks under\n  thresholding", "comments": "8 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.SI physics.soc-ph q-bio.NC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recent studies have demonstrated the existence of community structure and\nrich club nodes, (i.e., highly interconnected, high degree hub nodes), in human\nbrain functional networks. The cognitive relevance of the detected modules and\nhubs has also been demonstrated, for both task based and default mode networks,\nsuggesting that the brain self-organizes into patterns of co-activated sets of\nregions for performing specific tasks or in resting state. In this paper, we\nreport studies on the resilience or robustness of this modular structure: under\nsystematic erosion of connectivity in the network under thresholding, how\nresilient is the modularity and hub structure? The results show that the\nnetwork shows show strong resilience properties, with the modularity and hub\nstructure maintaining itself over a large range of connection strengths. Then,\nat a certain critical threshold that falls very close to 0, the connectivity,\nthe modularity, and hub structure suddenly break down, showing a phase\ntransition like property. Additionally, the spatial and topological\norganization of erosion of connectivity at all levels was found to be\nhomogenous rather than heterogenous; i.e., no \"structural holes\" of any\nsignificant sizes were found, and no gradual increases in numbers of components\nwere detected. Any loss of connectivity is homogenously spread out across the\nnetwork. The results suggest that human task-based functional brain networks\nare very resilient, where the whole network structure fails only when\nconnectivity is almost fully removed from the network. The findings may help\nfurther the understanding of dynamics of and relationships between structural\nand functional brain networks.\n", "versions": [{"version": "v1", "created": "Sun, 6 Jul 2014 22:15:45 GMT"}], "update_date": "2014-07-08", "authors_parsed": [["Sarkar", "S.", ""], ["Chawla", "S.", ""], ["Weng", "H.", ""]]}, {"id": "1407.1642", "submitter": "Christoph von der Malsburg", "authors": "Christoph von der Malsburg", "title": "A Vision Architecture", "comments": "6 pages, 2 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "q-bio.NC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We are offering a particular interpretation (well within the range of\nexperimentally and theoretically accepted notions) of neural connectivity and\ndynamics and discuss it as the data-and-process architecture of the visual\nsystem. In this interpretation the permanent connectivity of cortex is an\noverlay of well-structured networks, nets, which are formed on the slow\ntime-scale of learning by self-interaction of the network under the influence\nof sensory input, and which are selectively activated on the fast perceptual\ntime-scale. Nets serve as an explicit, hierarchically structured representation\nof visual structure in the various sub-modalities, as constraint networks\nfavouring mutually consistent sets of latent variables and as projection\nmappings to deal with invariance.\n", "versions": [{"version": "v1", "created": "Mon, 7 Jul 2014 09:41:47 GMT"}], "update_date": "2014-07-08", "authors_parsed": [["von der Malsburg", "Christoph", ""]]}, {"id": "1407.2210", "submitter": "Simon DeDeo", "authors": "Simon DeDeo", "title": "Group Minds and the Case of Wikipedia", "comments": "21 pages, 6 figures; matches published version", "journal-ref": "Human Computation (2014) 1:1:5-29", "doi": "10.15346/hc.v1i1.2", "report-no": "SFI Working Paper #14-10-037", "categories": "q-bio.NC cs.GT cs.SI physics.soc-ph q-bio.PE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Group-level cognitive states are widely observed in human social systems, but\ntheir discussion is often ruled out a priori in quantitative approaches. In\nthis paper, we show how reference to the irreducible mental states and\npsychological dynamics of a group is necessary to make sense of large scale\nsocial phenomena. We introduce the problem of mental boundaries by reference to\na classic problem in the evolution of cooperation. We then provide an explicit\nquantitative example drawn from ongoing work on cooperation and conflict among\nWikipedia editors, showing how some, but not all, effects of individual\nexperience persist in the aggregate. We show the limitations of methodological\nindividualism, and the substantial benefits that come from being able to refer\nto collective intentions, and attributions of cognitive states of the form\n\"what the group believes\" and \"what the group values\".\n", "versions": [{"version": "v1", "created": "Tue, 8 Jul 2014 18:46:21 GMT"}, {"version": "v2", "created": "Mon, 13 Oct 2014 21:34:32 GMT"}], "update_date": "2014-10-15", "authors_parsed": [["DeDeo", "Simon", ""]]}, {"id": "1407.2227", "submitter": "Ninan Sajeeth Philip", "authors": "Arun Kumar A, Ninan Sajeeth Philip, Vincent J Samar, James A\n  Desjardins, Sidney J Segalowitz", "title": "A Wavelet Based Algorithm for the Identification of Oscillatory\n  Event-Related Potential Components", "comments": "Journal of neuroscience methods 06/2014", "journal-ref": null, "doi": "10.1016/j.jneumeth.2014.06.004", "report-no": null, "categories": "cs.OH q-bio.NC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Event Related Potentials (ERPs) are very feeble alterations in the ongoing\nElectroencephalogram (EEG) and their detection is a challenging problem. Based\non the unique time-based parameters derived from wavelet coefficients and the\nasymmetry property of wavelets a novel algorithm to separate ERP components in\nsingle-trial EEG data is described. Though illustrated as a specific\napplication to N170 ERP detection, the algorithm is a generalized approach that\ncan be easily adapted to isolate different kinds of ERP components. The\nalgorithm detected the N170 ERP component with a high level of accuracy. We\ndemonstrate that the asymmetry method is more accurate than the matching\nwavelet algorithm and t-CWT method by 48.67 and 8.03 percent respectively. This\npaper provides an off-line demonstration of the algorithm and considers issues\nrelated to the extension of the algorithm to real-time applications.\n", "versions": [{"version": "v1", "created": "Fri, 20 Jun 2014 15:31:48 GMT"}], "update_date": "2014-07-09", "authors_parsed": [["A", "Arun Kumar", ""], ["Philip", "Ninan Sajeeth", ""], ["Samar", "Vincent J", ""], ["Desjardins", "James A", ""], ["Segalowitz", "Sidney J", ""]]}, {"id": "1407.2297", "submitter": "Johnatan Aljadeff", "authors": "Johnatan Aljadeff, Merav Stern, Tatyana O. Sharpee", "title": "Transition to chaos in random networks with cell-type-specific\n  connectivity", "comments": null, "journal-ref": "Phys. Rev. Lett. 114, 088101 (2015)", "doi": "10.1103/PhysRevLett.114.088101", "report-no": null, "categories": "q-bio.NC cond-mat.dis-nn math.PR nlin.CD", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In neural circuits, statistical connectivity rules strongly depend on\nneuronal type. Here we study dynamics of neural networks with cell-type\nspecific connectivity by extending the dynamic mean field method, and find that\nthese networks exhibit a phase transition between silent and chaotic activity.\nBy analyzing the locus of this transition, we derive a new result in random\nmatrix theory: the spectral radius of a random connectivity matrix with\nblock-structured variances. We apply our results to show how a small group of\nhyper-excitable neurons within the network can significantly increase the\nnetwork's computational capacity.\n", "versions": [{"version": "v1", "created": "Tue, 8 Jul 2014 23:30:19 GMT"}], "update_date": "2015-02-24", "authors_parsed": [["Aljadeff", "Johnatan", ""], ["Stern", "Merav", ""], ["Sharpee", "Tatyana O.", ""]]}, {"id": "1407.2480", "submitter": "Alain Destexhe", "authors": "Claude Bedard and Alain Destexhe", "title": "Generalized cable formalism to calculate the magnetic field of single\n  neurons and neuronal populations", "comments": "Physical Review E (in press); 24 pages, 16 figures", "journal-ref": "Physical Review E 90: 042723 (2014)", "doi": "10.1103/PhysRevE.90.042723", "report-no": null, "categories": "q-bio.NC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Neurons generate magnetic fields which can be recorded with macroscopic\ntechniques such as magneto-encephalography. The theory that accounts for the\ngenesis of neuronal magnetic fields involves dendritic cable structures in\nhomogeneous resistive extracellular media. Here, we generalize this model by\nconsidering dendritic cables in extracellular media with arbitrarily complex\nelectric properties. This method is based on a multi-scale mean-field theory\nwhere the neuron is considered in interaction with a \"mean\" extracellular\nmedium (characterized by a specific impedance). We first show that, as\nexpected, the generalized cable equation and the standard cable generate\nmagnetic fields that mostly depend on the axial current in the cable, with a\nmoderate contribution of extracellular currents. Less expected, we also show\nthat the nature of the extracellular and intracellular media influence the\naxial current, and thus also influence neuronal magnetic fields. We illustrate\nthese properties by numerical simulations and suggest experiments to test these\nfindings.\n", "versions": [{"version": "v1", "created": "Wed, 9 Jul 2014 13:53:23 GMT"}, {"version": "v2", "created": "Fri, 10 Oct 2014 21:37:23 GMT"}], "update_date": "2014-11-04", "authors_parsed": [["Bedard", "Claude", ""], ["Destexhe", "Alain", ""]]}, {"id": "1407.2776", "submitter": "Seyed-Mahdi Khaligh-Razavi", "authors": "Seyed-Mahdi Khaligh-Razavi", "title": "What you need to know about the state-of-the-art computational models of\n  object-vision: A tour through the models", "comments": "36 pages, 22 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI cs.LG q-bio.NC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Models of object vision have been of great interest in computer vision and\nvisual neuroscience. During the last decades, several models have been\ndeveloped to extract visual features from images for object recognition tasks.\nSome of these were inspired by the hierarchical structure of primate visual\nsystem, and some others were engineered models. The models are varied in\nseveral aspects: models that are trained by supervision, models trained without\nsupervision, and models (e.g. feature extractors) that are fully hard-wired and\ndo not need training. Some of the models come with a deep hierarchical\nstructure consisting of several layers, and some others are shallow and come\nwith only one or two layers of processing. More recently, new models have been\ndeveloped that are not hand-tuned but trained using millions of images, through\nwhich they learn how to extract informative task-related features. Here I will\nsurvey all these different models and provide the reader with an intuitive, as\nwell as a more detailed, understanding of the underlying computations in each\nof the models.\n", "versions": [{"version": "v1", "created": "Thu, 10 Jul 2014 13:15:18 GMT"}], "update_date": "2014-07-11", "authors_parsed": [["Khaligh-Razavi", "Seyed-Mahdi", ""]]}, {"id": "1407.2916", "submitter": "Samuel Vaccaro", "authors": "Samuel R. Vaccaro", "title": "Voltage dependence of Hodgkin-Huxley rate functions for a multi-stage K\n  channel voltage sensor within a membrane", "comments": "31 pages, 14 figures", "journal-ref": null, "doi": "10.1103/PhysRevE.90.052713", "report-no": null, "categories": "physics.bio-ph q-bio.NC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The activation of a $K^+$ channel sensor in two sequential stages during a\nvoltage clamp may be described as the translocation of a Brownian particle in\nan energy landscape with two large barriers between states. A solution of the\nSmoluchowski equation for a square-well approximation to the potential function\nof the S4 voltage sensor satisfies a master equation, and has two frequencies\nthat may be determined from the forward and backward rate functions. When the\nhigher frequency terms have small amplitude, the solution reduces to the\nrelaxation of a rate equation, where the derived two-state rate functions are\ndependent on the relative magnitude of the forward rates ($\\alpha$ and\n$\\gamma$) and the backward rates ($\\beta$ and $\\delta$) for each stage. In\nparticular, the voltage dependence of the Hodgkin-Huxley rate functions for a\n$K^+$ channel may be derived by assuming that the rate functions of the first\nstage are large relative to those of the second stage - $\\alpha \\gg \\gamma $\nand $\\beta \\gg \\delta$. For a {\\em Shaker} IR $K^+$ channel, the first forward\nand backward transitions are rate limiting ($\\alpha < \\gamma $ and $\\delta \\ll\n\\beta$), and for an activation process with either two or three stages, the\nderived two-state rate functions also have a voltage dependence that is of a\nsimilar form to that determined for the squid axon. The potential variation\ngenerated by the interaction between a two-stage $K^+$ ion channel and a\nnoninactivating $Na^+$ ion channel is determined by the master equation for\n$K^+$ ion channel activation and the ionic current equation when the $Na^+$ ion\nchannel activation time is small, and if $\\beta \\ll \\delta $ and $\\alpha \\ll\n\\gamma $, the system may exhibit a small amplitude oscillation between spikes,\nor mixed-mode oscillation.\n", "versions": [{"version": "v1", "created": "Thu, 10 Jul 2014 19:40:30 GMT"}, {"version": "v2", "created": "Thu, 11 Sep 2014 19:42:28 GMT"}, {"version": "v3", "created": "Tue, 21 Oct 2014 16:20:12 GMT"}], "update_date": "2015-06-22", "authors_parsed": [["Vaccaro", "Samuel R.", ""]]}, {"id": "1407.2917", "submitter": "Bhavin Shastri", "authors": "Bhavin J. Shastri, Alexander N. Tait, Mitchell A. Nahmias, and Paul R.\n  Prucnal", "title": "Photonic spike processing: ultrafast laser neurons and an integrated\n  photonic network", "comments": "11 pages, 8 figures", "journal-ref": "IEEE Photonics Society Newsletter, vol. 28, no. 3, pp. 4--11, Jun.\n  2014", "doi": null, "report-no": null, "categories": "q-bio.NC cs.ET physics.optics", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The marriage of two vibrant fields---photonics and neuromorphic\nprocessing---is fundamentally enabled by the strong analogies within the\nunderlying physics between the dynamics of biological neurons and lasers, both\nof which can be understood within the framework of nonlinear dynamical systems\ntheory. Whereas neuromorphic engineering exploits the biophysics of neuronal\ncomputation algorithms to provide a wide range of computing and signal\nprocessing applications, photonics offer an alternative approach to\nneuromorphic systems by exploiting the high speed, high bandwidth, and low\ncrosstalk available to photonic interconnects which potentially grants the\ncapacity for complex, ultrafast categorization and decision-making. Here we\nhighlight some recent progress on this exciting field.\n", "versions": [{"version": "v1", "created": "Tue, 8 Jul 2014 22:16:03 GMT"}], "update_date": "2014-07-11", "authors_parsed": [["Shastri", "Bhavin J.", ""], ["Tait", "Alexander N.", ""], ["Nahmias", "Mitchell A.", ""], ["Prucnal", "Paul R.", ""]]}, {"id": "1407.3249", "submitter": "Brian Mulloney PhD", "authors": "Brian Mulloney, Carmen Smarandache-Wellmann, Cynthia Weller, Wendy M.\n  Hall, Ralph A. DiCaprio", "title": "Proprioceptive feedback modulates coordinating information in a system\n  of segmentally-distributed microcircuits", "comments": "33 pages, 8 figures, two tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "q-bio.NC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The system of modular neural circuits that controls crustacean swimmerets\ndrives a metachronal sequence of power-stroke and return-stroke movements that\npropels the animal forward efficiently. These neural modules are synchronized\nby an intersegmental coordinating circuit that imposes characteristic phase\ndifferences between these modules. Using a semi-intact preparation that left\none swimmeret attached to an otherwise isolated crayfish central nervous system\nof the crayfish, we investigated how the rhythmic activity of this system\nresponded to imposed movements. We recorded extracellularly from the Ps and Rs\nnerves that innervated the attached limb and from coordinating axons that\nencode efference copies of the periodic bursts in Ps and Rs axons.\nSimultaneously we recorded from homologous nerves in more anterior and\nposterior segments. Maintained retractions did not affect cycle period, but\npromptly weakened Ps bursts, strengthened Rs bursts, and caused corresponding\nchanges in the strength and timing of efference copies in the coordinating\naxons. These changes in strength and timing of these efference copies then\ncaused changes in the phase and duration, but not the strength, of Ps bursts in\nmodules controlling neighboring swimmerets. These changes were promptly\nreversed when the limb was released. Each swimmeret is innervated by two\nnonspiking stretch receptors, Nssrs, that depolarize when the limb is\nretracted. Voltage-clamp of an Nssr changed the durations and strengths of\nbursts in Ps and Rs axons innervating the same limb, and caused corresponding\nchanges in the efference copies of this motor output.\n", "versions": [{"version": "v1", "created": "Fri, 11 Jul 2014 18:47:18 GMT"}], "update_date": "2014-07-14", "authors_parsed": [["Mulloney", "Brian", ""], ["Smarandache-Wellmann", "Carmen", ""], ["Weller", "Cynthia", ""], ["Hall", "Wendy M.", ""], ["DiCaprio", "Ralph A.", ""]]}, {"id": "1407.3432", "submitter": "Randall O'Reilly", "authors": "Randall C. O'Reilly and Dean Wyatte and John Rohrlich", "title": "Learning Through Time in the Thalamocortical Loops", "comments": "37 pages, 11 figures. arXiv admin note: text overlap with\n  arXiv:1112.0778 by other authors", "journal-ref": null, "doi": null, "report-no": null, "categories": "q-bio.NC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a comprehensive, novel framework for understanding how the\nneocortex, including the thalamocortical loops through the deep layers, can\nsupport a temporal context representation in the service of predictive\nlearning. Many have argued that predictive learning provides a compelling,\npowerful source of learning signals to drive the development of human\nintelligence: if we constantly predict what will happen next, and learn based\non the discrepancies from our predictions (error-driven learning), then we can\nlearn to improve our predictions by developing internal representations that\ncapture the regularities of the environment (e.g., physical laws governing the\ntime-evolution of object motions). Our version of this idea builds upon\nexisting work with simple recurrent networks (SRN's), which have a\ndiscretely-updated temporal context representations that are a direct copy of\nthe prior internal state representation. We argue that this discretization of\ntemporal context updating has a number of important computational and\nfunctional advantages, and further show how the strong alpha-frequency (10hz,\n100ms cycle time) oscillations in the posterior neocortex could reflect this\ntemporal context updating. We examine a wide range of data from biology to\nbehavior through the lens of this LeabraTI model, and find that it provides a\nunified account of a number of otherwise disconnected findings, all of which\nconverge to support this new model of neocortical learning and processing. We\ndescribe an implemented model showing how predictive learning of tumbling\nobject trajectories can facilitate object recognition with cluttered\nbackgrounds.\n", "versions": [{"version": "v1", "created": "Sun, 13 Jul 2014 05:43:44 GMT"}], "update_date": "2014-07-15", "authors_parsed": [["O'Reilly", "Randall C.", ""], ["Wyatte", "Dean", ""], ["Rohrlich", "John", ""]]}, {"id": "1407.3501", "submitter": "Jean-Baptiste Mouret", "authors": "Antoine Cully, Jeff Clune, Danesh Tarapore, Jean-Baptiste Mouret", "title": "Robots that can adapt like animals", "comments": null, "journal-ref": null, "doi": "10.1038/nature14422", "report-no": null, "categories": "cs.RO cs.AI cs.LG cs.NE q-bio.NC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  As robots leave the controlled environments of factories to autonomously\nfunction in more complex, natural environments, they will have to respond to\nthe inevitable fact that they will become damaged. However, while animals can\nquickly adapt to a wide variety of injuries, current robots cannot \"think\noutside the box\" to find a compensatory behavior when damaged: they are limited\nto their pre-specified self-sensing abilities, can diagnose only anticipated\nfailure modes, and require a pre-programmed contingency plan for every type of\npotential damage, an impracticality for complex robots. Here we introduce an\nintelligent trial and error algorithm that allows robots to adapt to damage in\nless than two minutes, without requiring self-diagnosis or pre-specified\ncontingency plans. Before deployment, a robot exploits a novel algorithm to\ncreate a detailed map of the space of high-performing behaviors: This map\nrepresents the robot's intuitions about what behaviors it can perform and their\nvalue. If the robot is damaged, it uses these intuitions to guide a\ntrial-and-error learning algorithm that conducts intelligent experiments to\nrapidly discover a compensatory behavior that works in spite of the damage.\nExperiments reveal successful adaptations for a legged robot injured in five\ndifferent ways, including damaged, broken, and missing legs, and for a robotic\narm with joints broken in 14 different ways. This new technique will enable\nmore robust, effective, autonomous robots, and suggests principles that animals\nmay use to adapt to injury.\n", "versions": [{"version": "v1", "created": "Sun, 13 Jul 2014 19:06:08 GMT"}, {"version": "v2", "created": "Wed, 23 Jul 2014 16:08:20 GMT"}, {"version": "v3", "created": "Mon, 18 Aug 2014 14:07:53 GMT"}, {"version": "v4", "created": "Wed, 27 May 2015 22:43:04 GMT"}], "update_date": "2015-05-29", "authors_parsed": [["Cully", "Antoine", ""], ["Clune", "Jeff", ""], ["Tarapore", "Danesh", ""], ["Mouret", "Jean-Baptiste", ""]]}, {"id": "1407.3778", "submitter": "J. M. Schwarz", "authors": "O. V. Manyuhina, David Mayett, and J. M. Schwarz", "title": "Elastic instabilities in a layered cerebral cortex: A revised axonal\n  tension model for cortex folding", "comments": "9.3 pages, 6 figures, accepted to New Journal of Physics", "journal-ref": "New Journal of Physics 16, 123058 (2014)", "doi": "10.1088/1367-2630/16/12/123058", "report-no": null, "categories": "cond-mat.soft physics.bio-ph q-bio.NC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We model the elasticity of the cerebral cortex as a layered material with\nbending energy along the layers and elastic energy between them in both planar\nand polar geometries. The cortex is also subjected to axons pulling from the\nunderlying white matter. Above a critical threshold force, a \"flat\" cortex\nconfiguration becomes unstable and periodic unduluations emerge, i.e. a\nbuckling instability occurs. These undulations may indeed initiate folds in the\ncortex. We identify analytically the critical force and the critical wavelength\nof the undulations. Both quantities are physiologically relevant values. Our\nmodel is a revised version of the axonal tension model for cortex folding, with\nour version taking into account the layered structure of the cortex. Moreover,\nour model draws a connection with another competing model for cortex folding,\nnamely the differential growth-induced buckling model. For the polar geometry,\nwe study the relationship between brain size and the critical force and\nwavelength to understand why small mice brains exhibit no folds, while larger\nhuman brains do, for example. Finally, an estimate of the bending rigidity\nconstant for the cortex can be made based on the critical wavelength.\n", "versions": [{"version": "v1", "created": "Mon, 14 Jul 2014 19:46:41 GMT"}, {"version": "v2", "created": "Thu, 4 Dec 2014 19:52:13 GMT"}], "update_date": "2016-02-09", "authors_parsed": [["Manyuhina", "O. V.", ""], ["Mayett", "David", ""], ["Schwarz", "J. M.", ""]]}, {"id": "1407.3809", "submitter": "Axel Wismueller", "authors": "Axel Wism\\\"uller, Xixi Wang, Adora M. DSouza and Mahesh B. Nagarajan", "title": "A Framework for Exploring Non-Linear Functional Connectivity and\n  Causality in the Human Brain: Mutual Connectivity Analysis (MCA) of\n  Resting-State Functional MRI with Convergent Cross-Mapping and Non-Metric\n  Clustering", "comments": "Axel Wism\\\"uller and Mahesh B. Nagarajan contributed equally to the\n  preparation of this manuscript. Pre-publication draft: 18 pages, 6 figures, 1\n  table", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NE q-bio.NC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a computational framework for analysis and visualization of\nnon-linear functional connectivity in the human brain from resting state\nfunctional MRI (fMRI) data for purposes of recovering the underlying network\ncommunity structure and exploring causality between network components. Our\nproposed methodology of non-linear mutual connectivity analysis (MCA) involves\ntwo computational steps. First, the pair-wise cross-prediction performance\nbetween resting state fMRI pixel time series within the brain is evaluated. The\nunderlying network structure is subsequently recovered from the affinity matrix\nconstructed through MCA using non-metric network partitioning/clustering with\nthe so-called Louvain method. We demonstrate our methodology in the task of\nidentifying regions of the motor cortex associated with hand movement on\nresting state fMRI data acquired from eight slice locations in four subjects.\nFor comparison, we also localized regions of the motor cortex through a\ntask-based fMRI sequence involving a finger-tapping stimulus paradigm. Finally,\nwe integrate convergent cross mapping (CCM) into the first step of MCA for\ninvestigating causality between regions of the motor cortex. Results regarding\ncausation between regions of the motor cortex revealed a significant\ndirectional variability and were not readily interpretable in a consistent\nmanner across all subjects. However, our results on whole-slice fMRI analysis\ndemonstrate that MCA-based model-free recovery of regions associated with the\nprimary motor cortex and supplementary motor area are in close agreement with\nlocalization of similar regions achieved with a task-based fMRI acquisition.\nThus, we conclude that our computational framework MCA can extract and\nvisualize valuable information concerning the underlying network structure and\ncausation between different regions of the brain in resting state fMRI.\n", "versions": [{"version": "v1", "created": "Mon, 14 Jul 2014 20:32:38 GMT"}], "update_date": "2014-07-16", "authors_parsed": [["Wism\u00fcller", "Axel", ""], ["Wang", "Xixi", ""], ["DSouza", "Adora M.", ""], ["Nagarajan", "Mahesh B.", ""]]}, {"id": "1407.3990", "submitter": "Georgi Medvedev S.", "authors": "Georgi S. Medvedev and Xuezhi Tang", "title": "Stability of twisted states in the Kuramoto model on Cayley and random\n  graphs", "comments": "Journal of Nonlinear Science, 2015", "journal-ref": null, "doi": "10.1007/s00332-015-9252-y", "report-no": null, "categories": "nlin.PS math.DS q-bio.NC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The Kuramoto model (KM) of coupled phase oscillators on complete, Paley, and\nErdos-Renyi (ER) graphs is analyzed in this work. As quasirandom graphs, the\ncomplete, Paley, and ER graphs share many structural properties. For instance,\nthey exhibit the same asymptotics of the edge distributions, homomorphism\ndensities, graph spectra, and have constant graph limits. Nonetheless, we show\nthat the asymptotic behavior of solutions in the KM on these graphs can be\nqualitatively different. Specifically, we identify twisted states, steady state\nsolutions of the KM on complete and Paley graphs, which are stable for one\nfamily of graphs but not for the other. On the other hand, we show that the\nsolutions of the IVPs for the KM on complete and random graphs remain close on\nfinite time intervals, provided they start from close initial conditions and\nthe graphs are sufficiently large. Therefore, the results of this paper\nelucidate the relation between the network structure and dynamics in coupled\nnonlinear dynamical systems. Furthermore, we present new results on\nsynchronization and stability of twisted states for the KM on Cayley and random\ngraphs.\n", "versions": [{"version": "v1", "created": "Tue, 15 Jul 2014 13:34:32 GMT"}, {"version": "v2", "created": "Wed, 25 Feb 2015 15:31:43 GMT"}, {"version": "v3", "created": "Sun, 10 May 2015 18:41:17 GMT"}], "update_date": "2015-05-12", "authors_parsed": [["Medvedev", "Georgi S.", ""], ["Tang", "Xuezhi", ""]]}, {"id": "1407.4137", "submitter": "Eric Jonas", "authors": "Eric Jonas and Konrad Kording", "title": "Automatic discovery of cell types and microcircuitry from neural\n  connectomics", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "q-bio.NC stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Neural connectomics has begun producing massive amounts of data,\nnecessitating new analysis methods to discover the biological and computational\nstructure. It has long been assumed that discovering neuron types and their\nrelation to microcircuitry is crucial to understanding neural function. Here we\ndeveloped a nonparametric Bayesian technique that identifies neuron types and\nmicrocircuitry patterns in connectomics data. It combines the information\ntraditionally used by biologists, including connectivity, cell body location\nand the spatial distribution of synapses, in a principled and\nprobabilistically-coherent manner. We show that the approach recovers known\nneuron types in the retina and enables predictions of connectivity, better than\nsimpler algorithms. It also can reveal interesting structure in the nervous\nsystem of C. elegans, and automatically discovers the structure of a\nmicroprocessor. Our approach extracts structural meaning from connectomics,\nenabling new approaches of automatically deriving anatomical insights from\nthese emerging datasets.\n", "versions": [{"version": "v1", "created": "Tue, 15 Jul 2014 20:14:05 GMT"}], "update_date": "2014-07-17", "authors_parsed": [["Jonas", "Eric", ""], ["Kording", "Konrad", ""]]}, {"id": "1407.4240", "submitter": "Ulrike von Luxburg", "authors": "Volker H. Franz and Ulrike von Luxburg", "title": "Unconscious lie detection as an example of a widespread fallacy in the\n  Neurosciences", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP q-bio.NC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Neuroscientists frequently use a certain statistical reasoning to establish\nthe existence of distinct neuronal processes in the brain. We show that this\nreasoning is flawed and that the large corresponding literature needs\nreconsideration. We illustrate the fallacy with a recent study that received an\nenormous press coverage because it concluded that humans detect deceit better\nif they use unconscious processes instead of conscious deliberations. The study\nwas published under a new open-data policy that enabled us to reanalyze the\ndata with more appropriate methods. We found that unconscious performance was\nclose to chance - just as the conscious performance. This illustrates the flaws\nof this widely used statistical reasoning, the benefits of open-data practices,\nand the need for careful reconsideration of studies using the same rationale.\n", "versions": [{"version": "v1", "created": "Wed, 16 Jul 2014 09:13:26 GMT"}], "update_date": "2014-07-17", "authors_parsed": [["Franz", "Volker H.", ""], ["von Luxburg", "Ulrike", ""]]}, {"id": "1407.4380", "submitter": "Fabrizio De Vico Fallani", "authors": "Fabrizio De Vico Fallani, Martina Corazzol, Jenna R. Sternberg, Claire\n  Wyart, Mario Chavez", "title": "Hierarchy of neural organization in the embryonic spinal cord:\n  Granger-causality graph analysis of in vivo calcium imaging data", "comments": null, "journal-ref": null, "doi": "10.1109/TNSRE.2014.2341632", "report-no": null, "categories": "q-bio.NC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The recent development of genetically encoded calcium indicators enables\nmonitoring in vivo the activity of neuronal populations. Most analysis of these\ncalcium transients relies on linear regression analysis based on the sensory\nstimulus applied or the behavior observed. To estimate the basic properties of\nthe functional neural circuitry, we propose a network-based approach based on\ncalcium imaging recorded at single cell resolution. Differently from previous\nanalysis based on cross-correlation, we used Granger-causality estimates to\ninfer activity propagation between the activities of different neurons. The\nresulting functional networks were then modeled as directed graphs and\ncharacterized in terms of connectivity and node centralities. We applied our\napproach to calcium transients recorded at low frequency (4 Hz) in ventral\nneurons of the zebrafish spinal cord at the embryonic stage when spontaneous\ncoiling of the tail occurs. Our analysis on population calcium imaging data\nrevealed a strong ipsilateral connectivity and a characteristic hierarchical\norganization of the network hubs that supported established propagation of\nactivity from rostral to caudal spinal cord. Our method could be used for\ndetecting functional defects in neuronal circuitry during development and\npathological conditions.\n", "versions": [{"version": "v1", "created": "Wed, 16 Jul 2014 16:43:06 GMT"}], "update_date": "2014-09-10", "authors_parsed": [["Fallani", "Fabrizio De Vico", ""], ["Corazzol", "Martina", ""], ["Sternberg", "Jenna R.", ""], ["Wyart", "Claire", ""], ["Chavez", "Mario", ""]]}, {"id": "1407.5019", "submitter": "Adriano Barra Dr.", "authors": "Elena Agliari, Adriano Barra, Andrea Galluzzi, Francesco Guerra,\n  Daniele Tantari, Flavia Tavani", "title": "From Dyson to Hopfield: Processing on hierarchical networks", "comments": null, "journal-ref": "Physical Review Letters 114, 028103, (2015)", "doi": "10.1103/PhysRevLett.114.028103", "report-no": "Roma01.Math", "categories": "cond-mat.dis-nn physics.soc-ph q-bio.NC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider statistical-mechanical models for spin systems built on\nhierarchical structures, which provide a simple example of non-mean-field\nframework. We show that the coupling decay with spin distance can give rise to\npeculiar features and phase diagrams much richer that their mean-field\ncounterpart. In particular, we consider the Dyson model, mimicking\nferromagnetism in lattices, and we prove the existence of a number of\nmeta-stabilities, beyond the ordered state, which get stable in the\nthermodynamic limit. Such a feature is retained when the hierarchical structure\nis coupled with the Hebb rule for learning, hence mimicking the modular\narchitecture of neurons, and gives rise to an associative network able to\nperform both as a serial processor as well as a parallel processor, depending\ncrucially on the external stimuli and on the rate of interaction decay with\ndistance; however, those emergent multitasking features reduce the network\ncapacity with respect to the mean-field counterpart. The analysis is\naccomplished through statistical mechanics, graph theory, signal-to-noise\ntechnique and numerical simulations in full consistency. Our results shed light\non the biological complexity shown by real networks, and suggest future\ndirections for understanding more realistic models.\n", "versions": [{"version": "v1", "created": "Fri, 18 Jul 2014 14:55:27 GMT"}], "update_date": "2016-02-02", "authors_parsed": [["Agliari", "Elena", ""], ["Barra", "Adriano", ""], ["Galluzzi", "Andrea", ""], ["Guerra", "Francesco", ""], ["Tantari", "Daniele", ""], ["Tavani", "Flavia", ""]]}, {"id": "1407.5104", "submitter": "Pulkit Agrawal", "authors": "Pulkit Agrawal, Dustin Stansbury, Jitendra Malik, Jack L. Gallant", "title": "Pixels to Voxels: Modeling Visual Representation in the Human Brain", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "q-bio.NC cs.CV cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The human brain is adept at solving difficult high-level visual processing\nproblems such as image interpretation and object recognition in natural scenes.\nOver the past few years neuroscientists have made remarkable progress in\nunderstanding how the human brain represents categories of objects and actions\nin natural scenes. However, all current models of high-level human vision\noperate on hand annotated images in which the objects and actions have been\nassigned semantic tags by a human operator. No current models can account for\nhigh-level visual function directly in terms of low-level visual input (i.e.,\npixels). To overcome this fundamental limitation we sought to develop a new\nclass of models that can predict human brain activity directly from low-level\nvisual input (i.e., pixels). We explored two classes of models drawn from\ncomputer vision and machine learning. The first class of models was based on\nFisher Vectors (FV) and the second was based on Convolutional Neural Networks\n(ConvNets). We find that both classes of models accurately predict brain\nactivity in high-level visual areas, directly from pixels and without the need\nfor any semantic tags or hand annotation of images. This is the first time that\nsuch a mapping has been obtained. The fit models provide a new platform for\nexploring the functional principles of human vision, and they show that modern\nmethods of computer vision and machine learning provide important tools for\ncharacterizing brain function.\n", "versions": [{"version": "v1", "created": "Fri, 18 Jul 2014 20:10:06 GMT"}], "update_date": "2014-07-22", "authors_parsed": [["Agrawal", "Pulkit", ""], ["Stansbury", "Dustin", ""], ["Malik", "Jitendra", ""], ["Gallant", "Jack L.", ""]]}, {"id": "1407.5105", "submitter": "Ankit Khambhati", "authors": "Ankit Khambhati, Brian Litt, Danielle S. Bassett", "title": "Dynamic network drivers of seizure generation, propagation and\n  termination in human epilepsy", "comments": "7 pages, 5 figures; Supplementary Materials: 5 pages, 3 figures, 1\n  table", "journal-ref": null, "doi": "10.1371/journal.pcbi.1004608", "report-no": null, "categories": "q-bio.NC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Drug-resistant epilepsy is traditionally characterized by pathologic cortical\ntissue comprised of seizure-initiating `foci'. These `foci' are thought to be\nembedded within an epileptic network whose functional architecture dynamically\nreorganizes during seizures through synchronous and asynchronous\nneurophysiologic processes. Critical to understanding these dynamics is\nidentifying the synchronous connections that link foci to surrounding tissue\nand investigating how these connections facilitate seizure generation and\ntermination. We use intracranial recordings from neocortical epilepsy patients\nundergoing pre-surgical evaluation to analyze functional connectivity before\nand during seizures. We develop and apply a novel technique to track network\nreconfiguration in time and to parse these reconfiguration dynamics into\ndistinct seizure states, each characterized by unique patterns of network\nconnections that differ in their strength and topography. Our approach suggests\nthat seizures are generated when the synchronous relationships that isolate\nseizure `foci' from the surrounding epileptic network are broken down. As\nseizures progress, foci reappear as isolated subnetworks, marking a shift in\nnetwork state that may aid seizure termination. Collectively, our observations\nhave important theoretical implications for understanding the spatial\ninvolvement of distributed cortical structures in the dynamics of seizure\ngeneration, propagation and termination, and have practical significance in\ndetermining which circuits to modulate with implantable devices.\n", "versions": [{"version": "v1", "created": "Fri, 18 Jul 2014 20:12:16 GMT"}], "update_date": "2016-02-17", "authors_parsed": [["Khambhati", "Ankit", ""], ["Litt", "Brian", ""], ["Bassett", "Danielle S.", ""]]}, {"id": "1407.5328", "submitter": "Dean Wyatte", "authors": "Dean Wyatte", "title": "What happens next and when \"next\" happens: Mechanisms of spatial and\n  temporal prediction", "comments": "Doctoral thesis (May 2014)", "journal-ref": null, "doi": null, "report-no": null, "categories": "q-bio.NC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The physics of the environment provide a rich spatiotemporal structure for\nour experience. Objects move in predictable ways and their features and\nidentity remain stable across time and space. How does the brain leverage this\nstructure to make predictions about and learn from the environment? This thesis\ndescribes research centered around a mechanistic description of sensory\nprediction called LeabraTI (TI: Temporal Integration) that explains precisely\nhow predictive processing is accomplished in neocortical microcircuits. The\nfundamental prediction of LeabraTI is that predictions and sensations are\ninterleaved across the same neural tissue at an overall rate of 10 Hz,\ncorresponding to the widely studied alpha rhythm of posterior cortex.\nExperiments described herein tested this prediction by manipulating the\nspatiotemporal properties of three-dimensional object stimuli in a laboratory\nsetting. EEG results indicated that predictions were subserved by ~10 Hz\noscillations that reliably tracked the onset of stimuli and differentiated\nbetween spatially predictable and unpredictable object sequences. There was a\nbehavioral advantage for combined spatial and temporal predictability for\ndiscrimination of unlearned objects, but prolonged study of objects under this\ncombined predictability context impaired discriminability relative to other\nlearning contexts. This counterintuitive pattern of results was accounted for\nby a neural network model that learned three-dimensional viewpoint invariance\nwith LeabraTI's spatiotemporal prediction rule. Synaptic weight scaling from\nprolonged learning built viewpoint invariance, but led to confusion between\nambiguous views of objects, producing slightly lower performance on average.\nOverall, this work advances a biological architecture for sensory prediction\naccompanied by empirical evidence that supports learning of realistic time- and\nspace-varying inputs.\n", "versions": [{"version": "v1", "created": "Sun, 20 Jul 2014 19:11:51 GMT"}], "update_date": "2014-07-22", "authors_parsed": [["Wyatte", "Dean", ""]]}, {"id": "1407.5525", "submitter": "Cedric Ginestet", "authors": "Cedric E. Ginestet, Jun Li, Prakash Balachandran, Steven Rosenberg and\n  Eric D. Kolaczyk", "title": "Hypothesis Testing For Network Data in Functional Neuroimaging", "comments": "34 pages. 5 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP q-bio.NC stat.ME", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  In recent years, it has become common practice in neuroscience to use\nnetworks to summarize relational information in a set of measurements,\ntypically assumed to be reflective of either functional or structural\nrelationships between regions of interest in the brain. One of the most basic\ntasks of interest in the analysis of such data is the testing of hypotheses, in\nanswer to questions such as \"Is there a difference between the networks of\nthese two groups of subjects?\" In the classical setting, where the unit of\ninterest is a scalar or a vector, such questions are answered through the use\nof familiar two-sample testing strategies. Networks, however, are not Euclidean\nobjects, and hence classical methods do not directly apply. We address this\nchallenge by drawing on concepts and techniques from geometry, and\nhigh-dimensional statistical inference. Our work is based on a precise\ngeometric characterization of the space of graph Laplacian matrices and a\nnonparametric notion of averaging due to Fr\\'echet. We motivate and illustrate\nour resulting methodologies for testing in the context of networks derived from\nfunctional neuroimaging data on human subjects from the 1000 Functional\nConnectomes Project. In particular, we show that this global test is more\nstatistical powerful, than a mass-univariate approach. In addition, we have\nalso provided a method for visualizing the individual contribution of each edge\nto the overall test statistic.\n", "versions": [{"version": "v1", "created": "Mon, 21 Jul 2014 15:20:34 GMT"}, {"version": "v2", "created": "Mon, 28 Jul 2014 16:40:50 GMT"}, {"version": "v3", "created": "Fri, 2 Sep 2016 15:15:47 GMT"}, {"version": "v4", "created": "Thu, 2 Feb 2017 15:49:34 GMT"}, {"version": "v5", "created": "Fri, 3 Feb 2017 10:53:01 GMT"}, {"version": "v6", "created": "Fri, 17 Mar 2017 16:41:13 GMT"}], "update_date": "2017-03-20", "authors_parsed": [["Ginestet", "Cedric E.", ""], ["Li", "Jun", ""], ["Balachandran", "Prakash", ""], ["Rosenberg", "Steven", ""], ["Kolaczyk", "Eric D.", ""]]}, {"id": "1407.5590", "submitter": "Petko Bogdanov", "authors": "Petko Bogdanov, Nazli Dereli, Danielle S. Bassett, Scott T. Grafton,\n  Ambuj K. Singh", "title": "Learning about Learning: Human Brain Sub-Network Biomarkers in fMRI Data", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "q-bio.NC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  It has become increasingly popular to study the brain as a network due to the\nrealization that functionality cannot be explained exclusively by independent\nactivation of specialized regions. Instead, across a large spectrum of\nbehaviors, function arises due to the dynamic interactions between brain\nregions. The existing literature on functional brain networks focuses mainly on\na battery of network properties characterizing the \"resting state\" using for\nexample the modularity, clustering, or path length among regions. In contrast,\nwe seek to uncover subgraphs of functional connectivity that predict or drive\nindividual differences in sensorimotor learning across subjects. We employ a\nprincipled approach for the discovery of significant subgraphs of functional\nconnectivity, induced by brain activity (measured via fMRI imaging) while\nsubjects perform a motor learning task. Our aim is to uncover patterns of\nfunctional connectivity that discriminate between high and low rates of\nlearning among subjects. The discovery of such significant discriminative\nsubgraphs promises a better data-driven understanding of the dynamic brain\nprocesses associated with brain plasticity.\n", "versions": [{"version": "v1", "created": "Mon, 21 Jul 2014 18:23:58 GMT"}], "update_date": "2014-07-22", "authors_parsed": [["Bogdanov", "Petko", ""], ["Dereli", "Nazli", ""], ["Bassett", "Danielle S.", ""], ["Grafton", "Scott T.", ""], ["Singh", "Ambuj K.", ""]]}, {"id": "1407.5946", "submitter": "William Bialek", "authors": "Gasper Tkacik, Thierry Mora, Olivier Marre, Dario Amodei, Michael J.\n  Berry II, and William Bialek", "title": "Thermodynamics for a network of neurons: Signatures of criticality", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "q-bio.NC cond-mat.dis-nn cond-mat.stat-mech", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The activity of a neural network is defined by patterns of spiking and\nsilence from the individual neurons. Because spikes are (relatively) sparse,\npatterns of activity with increasing numbers of spikes are less probable, but\nwith more spikes the number of possible patterns increases. This tradeoff\nbetween probability and numerosity is mathematically equivalent to the\nrelationship between entropy and energy in statistical physics. We construct\nthis relationship for populations of up to N=160 neurons in a small patch of\nthe vertebrate retina, using a combination of direct and model-based analyses\nof experiments on the response of this network to naturalistic movies. We see\nsigns of a thermodynamic limit, where the entropy per neuron approaches a\nsmooth function of the energy per neuron as N increases. The form of this\nfunction corresponds to the distribution of activity being poised near an\nunusual kind of critical point. Networks with more or less correlation among\nneurons would not reach this critical state. We suggest further tests of\ncriticality, and give a brief discussion of its functional significance.\n", "versions": [{"version": "v1", "created": "Tue, 22 Jul 2014 17:16:12 GMT"}], "update_date": "2014-07-23", "authors_parsed": [["Tkacik", "Gasper", ""], ["Mora", "Thierry", ""], ["Marre", "Olivier", ""], ["Amodei", "Dario", ""], ["Berry", "Michael J.", "II"], ["Bialek", "William", ""]]}, {"id": "1407.6029", "submitter": "Ngoc Mai Tran", "authors": "Ila Fiete, David J. Schwab and Ngoc M. Tran", "title": "A binary Hopfield network with $1/\\log(n)$ information rate and\n  applications to grid cell decoding", "comments": "extended abstract, 4 pages, 2 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "q-bio.NC math.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A Hopfield network is an auto-associative, distributive model of neural\nmemory storage and retrieval. A form of error-correcting code, the Hopfield\nnetwork can learn a set of patterns as stable points of the network dynamic,\nand retrieve them from noisy inputs -- thus Hopfield networks are their own\ndecoders. Unlike in coding theory, where the information rate of a good code\n(in the Shannon sense) is finite but the cost of decoding does not play a role\nin the rate, the information rate of Hopfield networks trained with\nstate-of-the-art learning algorithms is of the order ${\\log(n)}/{n}$, a\nquantity that tends to zero asymptotically with $n$, the number of neurons in\nthe network. For specially constructed networks, the best information rate\ncurrently achieved is of order ${1}/{\\sqrt{n}}$. In this work, we design simple\nbinary Hopfield networks that have asymptotically vanishing error rates at an\ninformation rate of ${1}/{\\log(n)}$. These networks can be added as the\ndecoders of any neural code with noisy neurons. As an example, we apply our\nnetwork to a binary neural decoder of the grid cell code to attain information\nrate ${1}/{\\log(n)}$.\n", "versions": [{"version": "v1", "created": "Tue, 22 Jul 2014 20:32:46 GMT"}], "update_date": "2014-07-24", "authors_parsed": [["Fiete", "Ila", ""], ["Schwab", "David J.", ""], ["Tran", "Ngoc M.", ""]]}, {"id": "1407.6110", "submitter": "Jacek Dmochowski", "authors": "Jacek P. Dmochowski, Alex S. Greaves, Anthony M. Norcia", "title": "Maximally reliable spatial filtering of steady state visual evoked\n  potentials", "comments": "12 pages, 7 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "q-bio.QM q-bio.NC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Due to their high signal-to-noise ratio (SNR) and robustness to artifacts,\nsteady state visual evoked potentials (SSVEPs) are a popular technique for\nstudying neural processing in the human visual system. SSVEPs are\nconventionally analyzed at individual electrodes or linear combinations of\nelectrodes which maximize some variant of the SNR. Here we exploit the\nfundamental assumption of evoked responses -- reproducibility across trials --\nto develop a technique that extracts a small number of high SNR, maximally\nreliable SSVEP components. This novel spatial filtering method operates on an\narray of Fourier coefficients and projects the data into a low-dimensional\nspace in which the trial-to-trial spectral covariance is maximized. When\napplied to two sample data sets, the resulting technique recovers\nphysiologically plausible components (i.e., the recovered topographies match\nthe lead fields of the underlying sources) while drastically reducing the\ndimensionality of the data (i.e., more than 90% of the trial-to-trial\nreliability is captured in the first four components). Moreover, the proposed\ntechnique achieves a higher SNR than that of the single-best electrode or the\nPrincipal Components. We provide a freely-available MATLAB implementation of\nthe proposed technique, herein termed \"Reliable Components Analysis\".\n", "versions": [{"version": "v1", "created": "Wed, 23 Jul 2014 06:05:26 GMT"}], "update_date": "2014-07-24", "authors_parsed": [["Dmochowski", "Jacek P.", ""], ["Greaves", "Alex S.", ""], ["Norcia", "Anthony M.", ""]]}, {"id": "1407.6525", "submitter": "Christian Albers", "authors": "Christian Albers, Maren Westkott, Klaus Pawelzik", "title": "Learning of Precise Spike Times with Membrane Potential Dependent\n  Synaptic Plasticity", "comments": "24 pages, 6 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "q-bio.NC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Precise spatio-temporal patterns of neuronal action potentials underly e.g.\nsensory representations and control of muscle activities. However, it is not\nknown how the synaptic efficacies in the neuronal networks of the brain adapt\nsuch that they can reliably generate spikes at specific points in time.\nExisting activity-dependent plasticity rules like Spike-Timing-Dependent\nPlasticity are agnostic to the goal of learning spike times. On the other hand,\nthe existing formal and supervised learning algorithms perform a temporally\nprecise comparison of projected activity with the target, but there is no known\nbiologically plausible implementation of this comparison. Here, we propose a\nsimple and local unsupervised synaptic plasticity mechanism that is derived\nfrom the requirement of a balanced membrane potential. Since the relevant\nsignal for synaptic change is the postsynaptic voltage rather than spike times,\nwe call the plasticity rule Membrane Potential Dependent Plasticity (MPDP).\nCombining our plasticity mechanism with spike after-hyperpolarization causes a\nsensitivity of synaptic change to pre- and postsynaptic spike times which can\nreproduce Hebbian spike timing dependent plasticity for inhibitory synapses as\nwas found in experiments. In addition, the sensitivity of MPDP to the time\ncourse of the voltage when generating a spike allows MPDP to distinguish\nbetween weak (spurious) and strong (teacher) spikes, which therefore provides a\nneuronal basis for the comparison of actual and target activity. For\nspatio-temporal input spike patterns our conceptually simple plasticity rule\nachieves a surprisingly high storage capacity for spike associations. The\nsensitivity of the MPDP to the subthreshold membrane potential during training\nallows robust memory retrieval after learning even in the presence of activity\ncorrupted by noise.\n", "versions": [{"version": "v1", "created": "Thu, 24 Jul 2014 10:58:55 GMT"}, {"version": "v2", "created": "Mon, 23 Feb 2015 14:34:51 GMT"}], "update_date": "2015-02-24", "authors_parsed": [["Albers", "Christian", ""], ["Westkott", "Maren", ""], ["Pawelzik", "Klaus", ""]]}, {"id": "1407.7027", "submitter": "Peilei Liu", "authors": "Peilei Liu and Ting Wang", "title": "Motor Learning Mechanism on the Neuron Scale", "comments": "8 pages, 4 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "q-bio.NC cs.NE physics.bio-ph", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Based on existing data, we wish to put forward a biological model of motor\nsystem on the neuron scale. Then we indicate its implications in statistics and\nlearning. Specifically, neuron firing frequency and synaptic strength are\nprobability estimates in essence. And the lateral inhibition also has\nstatistical implications. From the standpoint of learning, dendritic\ncompetition through retrograde messengers is the foundation of conditional\nreflex and grandmother cell coding. And they are the kernel mechanisms of motor\nlearning and sensory motor integration respectively. Finally, we compare motor\nsystem with sensory system. In short, we would like to bridge the gap between\nmolecule evidences and computational models.\n", "versions": [{"version": "v1", "created": "Fri, 18 Jul 2014 14:18:29 GMT"}], "update_date": "2014-07-28", "authors_parsed": [["Liu", "Peilei", ""], ["Wang", "Ting", ""]]}, {"id": "1407.7135", "submitter": "Laurence Aitchison", "authors": "Laurence Aitchison, Nicola Corradi and Peter E. Latham", "title": "Zipf's law arises naturally in structured, high-dimensional data", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "q-bio.NC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Zipf's law, which states that the probability of an observation is inversely\nproportional to its rank, has been observed in many domains. While there are\nmodels that explain Zipf's law in each of them, those explanations are\ntypically domain specific. Recently, methods from statistical physics were used\nto show that a fairly broad class of models does provide a general explanation\nof Zipf's law. This explanation rests on the observation that real world data\nis often generated from underlying causes, known as latent variables. Those\nlatent variables mix together multiple models that do not obey Zipf's law,\ngiving a model that does. Here we extend that work both theoretically and\nempirically. Theoretically, we provide a far simpler and more intuitive\nexplanation of Zipf's law, which at the same time considerably extends the\nclass of models to which this explanation can apply. Furthermore, we also give\nmethods for verifying whether this explanation applies to a particular dataset.\nEmpirically, these advances allowed us extend this explanation to important\nclasses of data, including word frequencies (the first domain in which Zipf's\nlaw was discovered), data with variable sequence length, and multi-neuron\nspiking activity.\n", "versions": [{"version": "v1", "created": "Sat, 26 Jul 2014 14:47:23 GMT"}, {"version": "v2", "created": "Tue, 29 Jul 2014 05:08:47 GMT"}, {"version": "v3", "created": "Wed, 30 Jul 2014 06:13:09 GMT"}, {"version": "v4", "created": "Tue, 5 Jul 2016 14:42:36 GMT"}], "update_date": "2016-07-06", "authors_parsed": [["Aitchison", "Laurence", ""], ["Corradi", "Nicola", ""], ["Latham", "Peter E.", ""]]}, {"id": "1407.7392", "submitter": "Paolo Moretti", "authors": "Paula Villa Mart\\'in, Paolo Moretti, Miguel A. Mu\\~noz", "title": "Rounding of abrupt phase transitions in brain networks", "comments": "10 pages", "journal-ref": null, "doi": "10.1088/1742-5468/2015/01/P01003", "report-no": null, "categories": "q-bio.NC cond-mat.dis-nn nlin.AO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The observation of critical-like behavior in cortical networks represents a\nmajor step forward in elucidating how the brain manages information.\nUnderstanding the origin and functionality of critical-like dynamics, as well\nas their robustness, is a major challenge in contemporary neuroscience. Here,\nwe present an extensive numerical study of a family of simple dynamic models,\nwhich describe activity propagation in brain networks through the integration\nof different neighboring spiking potentials, mimicking basic neural\ninteractions. The requirement of signal integration may lead to discontinuous\nphase transitions in networks that are well described by the mean field\napproximation, thus preventing the emergence of critical points in such\nsystems. Here we show that criticality in the brain is instead robust, as a\nconsequence of the hierarchical organization of the higher layers of cortical\nnetworks, which signals a departure from the mean-field paradigm. We show that,\nin finite-dimensional hierarchical networks, discontinuous phase transitions\nexhibit a rounding phenomenon and turn continuous for values of the topological\ndimension $D\\le 2$, due to the presence of structural or topological disorder.\nOur results may prove significant in explaining the observation of traits of\ncritical behavior in large-scale measurements of brain activity.\n", "versions": [{"version": "v1", "created": "Mon, 28 Jul 2014 12:22:31 GMT"}], "update_date": "2015-06-22", "authors_parsed": [["Mart\u00edn", "Paula Villa", ""], ["Moretti", "Paolo", ""], ["Mu\u00f1oz", "Miguel A.", ""]]}, {"id": "1407.7600", "submitter": "Benjamin Machta", "authors": "Ahmed El Hady and Benjamin B. Machta", "title": "Mechanical Surface Waves Accompany Action Potential Propagation", "comments": "6 pages 3 figures + 2 page supplement", "journal-ref": null, "doi": "10.1038/ncomms7697", "report-no": null, "categories": "q-bio.NC cond-mat.soft physics.bio-ph", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Many studies have shown that a mechanical displacement of the axonal membrane\naccompanies the electrical pulse defining the Action Potential (AP). Despite a\nlarge and diverse body of experimental evidence, there is no theoretical\nconsensus either for the physical basis of this mechanical wave nor its\ninterdependence with the electrical signal. In this manuscript we present a\nmodel for these mechanical displacements as arising from the driving of surface\nwave modes in which potential energy is stored in elastic properties of the\nneuronal membrane and cytoskeleton while kinetic energy is carried by the\naxoplasmic fluid. In our model these surface waves are driven by the traveling\nwave of electrical depolarization that characterizes the AP, altering the\ncompressive electrostatic forces across the membrane as it passes. This driving\nleads to co-propagating mechanical displacements, which we term Action Waves\n(AWs). Our model for these AWs allows us to predict, in terms of elastic\nconstants, axon radius and axoplasmic density and viscosity, the shape of the\nAW that should accompany any traveling wave of voltage, including the AP\npredicted by the Hodgkin and Huxley (HH) equations. We show that our model\nmakes predictions that are in agreement with results in experimental systems\nincluding the garfish olfactory nerve and the squid giant axon. We expect our\nmodel to serve as a framework for understanding the physical origins and\npossible functional roles of these AWs in neurobiology.\n", "versions": [{"version": "v1", "created": "Mon, 28 Jul 2014 23:46:21 GMT"}, {"version": "v2", "created": "Sun, 5 Oct 2014 17:28:18 GMT"}], "update_date": "2015-06-22", "authors_parsed": [["Hady", "Ahmed El", ""], ["Machta", "Benjamin B.", ""]]}, {"id": "1407.7741", "submitter": "Alessio Gizzi Dr", "authors": "Alessandro Loppini, Antonio Capolupo, Christian Cherubini, Alessio\n  Gizzi, Marta Bertolaso, Simonetta Filippi, Giuseppe Vitiello", "title": "On the coherent behavior of pancreatic beta cell clusters", "comments": "19 pages, 7 figures, 1 table", "journal-ref": null, "doi": "10.1016/j.physleta.2014.09.041", "report-no": null, "categories": "cond-mat.soft physics.bio-ph q-bio.NC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Beta cells in pancreas represent an example of coupled biological oscillators\nwhich via communication pathways, are able to synchronize their electrical\nactivity, giving rise to pulsatile insulin release. In this work we numerically\nanalyze scale free self-similarity features of membrane voltage signal power\ndensity spectrum, through a stochastic dynamical model for beta cells in the\nislets of Langerhans fine tuned on mouse experimental data. Adopting the\nalgebraic approach of coherent state formalism, we show how coherent molecular\ndomains can arise from proper functional conditions leading to a parallelism\nwith \"phase transition\" phenomena of field theory.\n", "versions": [{"version": "v1", "created": "Tue, 29 Jul 2014 14:40:46 GMT"}], "update_date": "2015-06-22", "authors_parsed": [["Loppini", "Alessandro", ""], ["Capolupo", "Antonio", ""], ["Cherubini", "Christian", ""], ["Gizzi", "Alessio", ""], ["Bertolaso", "Marta", ""], ["Filippi", "Simonetta", ""], ["Vitiello", "Giuseppe", ""]]}, {"id": "1407.7999", "submitter": "Jens Christian Claussen", "authors": "Hong-Viet V. Ngo, Jens Christian Claussen, Jan Born, and Matthias\n  M\\\"olle", "title": "Induction of slow oscillations by rhythmic acoustic stimulation", "comments": null, "journal-ref": "J. Sleep Res. 22, 22-31 (2013)", "doi": "10.1111/j.1365-2869.2012.01039.x", "report-no": null, "categories": "q-bio.NC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Slow oscillations are electrical potential oscillations with a spectral peak\nfrequency of $\\sim$0.8 Hz, and hallmark the electroencephalogram during\nslow-wave sleep. Recent studies have indicated a causal contribution of slow\noscillations to the consolidation of memories during slow-wave sleep, raising\nthe question to what extent such oscillations can be induced by external\nstimulation. Here, we examined whether slow oscillations can be effectively\ninduced by rhythmic acoustic stimulation. Human subjects were examined in three\nconditions: (i) with tones presented at a rate of 0.8 Hz (`0.8-Hz\nstimulation'); (ii) with tones presented at a random sequence (`random\nstimulation'); and (iii) with no tones presented in a control condition\n(`sham'). Stimulation started during wakefulness before sleep and continued for\nthe first $\\sim$90 min of sleep. Compared with the other two conditions, 0.8-Hz\nstimulation significantly delayed sleep onset. However, once sleep was\nestablished, 0.8-Hz stimulation significantly increased and entrained\nendogenous slow oscillation activity. Sleep after the 90-min period of\nstimulation did not differ between the conditions. Our data show that rhythmic\nacoustic stimulation can be used to effectively enhance slow oscillation\nactivity. However, the effect depends on the brain state, requiring the\npresence of stable non-rapid eye movement sleep.\n", "versions": [{"version": "v1", "created": "Wed, 30 Jul 2014 11:08:20 GMT"}], "update_date": "2014-07-31", "authors_parsed": [["Ngo", "Hong-Viet V.", ""], ["Claussen", "Jens Christian", ""], ["Born", "Jan", ""], ["M\u00f6lle", "Matthias", ""]]}, {"id": "1407.8234", "submitter": "Jean Carlson", "authors": "Elizabeth N. Davison, Kimberly J. Schlesinger, Danielle S. Bassett,\n  Mary-Ellen Lynall, Michael B. Miller, Scott T. Grafton, Jean M. Carlson", "title": "Brain Network Adaptability Across Task States", "comments": "22 pages, 9 figures, 1 table", "journal-ref": null, "doi": "10.1371/journal.pcbi.1004029", "report-no": null, "categories": "q-bio.NC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Activity in the human brain moves between diverse functional states to meet\nthe demands of our dynamic environment, but fundamental principles guiding\nthese transitions remain poorly understood. Here, we capitalize on recent\nadvances in network science to analyze patterns of functional interactions\nbetween brain regions. We use dynamic network representations to probe the\nlandscape of brain reconfigurations that accompany task performance both within\nand between four cognitive states: a task-free resting state, an\nattention-demanding state, and two memory-demanding states. Using the formalism\nof hypergraphs, we identify the presence of groups of functional interactions\nthat fluctuate coherently in strength over time both within (task-specific) and\nacross (task-general) brain states. In contrast to prior emphases on the\ncomplexity of many dyadic (region-to-region) relationships, these results\ndemonstrate that brain adapt- ability can be described by common processes that\ndrive the dynamic integration of cognitive systems. Moreover, our results\nestablish the hypergraph as an effective measure for understanding functional\nbrain dynamics, which may also prove useful in examining cross-task, cross-age,\nand cross-cohort functional change.\n", "versions": [{"version": "v1", "created": "Wed, 30 Jul 2014 22:51:41 GMT"}], "update_date": "2015-06-22", "authors_parsed": [["Davison", "Elizabeth N.", ""], ["Schlesinger", "Kimberly J.", ""], ["Bassett", "Danielle S.", ""], ["Lynall", "Mary-Ellen", ""], ["Miller", "Michael B.", ""], ["Grafton", "Scott T.", ""], ["Carlson", "Jean M.", ""]]}]