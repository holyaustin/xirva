[{"id": "1608.00064", "submitter": "Gabriel Ocker", "authors": "Gabriel Koch Ocker and Brent Doiron", "title": "Training and spontaneous reinforcement of neuronal assemblies by spike\n  timing", "comments": "36 pages total, 6 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "q-bio.NC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The synaptic connectivity of cortex is plastic, with experience shaping the\nongoing interactions between neurons. Theoretical studies of spike\ntiming-dependent plasticity (STDP) have focused on either just pairs of neurons\nor large-scale simulations where analytic insight is lacking. A simple account\nfor how fast spike time correlations affect both micro- and macroscopic network\nstructure remains lacking. We develop a low-dimensional mean field theory\nshowing how STDP gives rise to strongly coupled assemblies of neurons with\nshared stimulus preferences, with the connectivity actively reinforced by spike\ntrain correlations during spontaneous dynamics. Furthermore, the stimulus\ncoding by cell assemblies is actively maintained by these internally generated\nspiking correlations, suggesting a new role for noise correlations in neural\ncoding. Assembly formation has often been associated with firing rate-based\nplasticity schemes; our theory provides an alternative and complementary\nframework, where temporal correlations and STDP form and actively maintain\nlearned structure in cortical networks.\n", "versions": [{"version": "v1", "created": "Sat, 30 Jul 2016 03:08:40 GMT"}], "update_date": "2016-08-02", "authors_parsed": [["Ocker", "Gabriel Koch", ""], ["Doiron", "Brent", ""]]}, {"id": "1608.00431", "submitter": "Danielle Bassett", "authors": "Laura Wiles, Shi Gu, Fabio Pasqualetti, Danielle S. Bassett, David F.\n  Meaney", "title": "Autaptic Connections Shift Network Excitability and Bursting", "comments": "31 pages, 6 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "q-bio.NC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Network architecture forms a critical constraint on neuronal function. Here\nwe examine the role of structural autapses, when a neuron synapses onto itself,\nin driving network-wide bursting behavior. Using a simple spiking model of\nneuronal activity, we study how autaptic connections affect activity patterns,\nand evaluate if neuronal degree or controllability are significant factors that\naffect changes in bursting from these autaptic connections. We observed that\nadding increasing numbers of autaptic connections to excitatory neurons\nincreased the number of spiking events in the network and the number of\nnetwork-wide bursts, particularly in the portion of the phase space in which\nexcitatory synapses were stronger contributors to bursting behavior than\ninhibitory synapses. In comparison, autaptic connections to excitatory neurons\nwith high average controllability led to higher burst frequencies than adding\nthe same number of self-looping connections to neurons with high modal\ncontrollability. The number of autaptic connections required to induce bursting\nbehavior could be lowered by selectively adding autapses to high degree\nexcitatory neurons. These results suggest a role of autaptic connections in\ncontrolling network-wide bursts in diverse cortical and subcortical regions of\nmammalian brain. Moreover, they open up new avenues for the study of dynamic\nneurophysiological correlates of structural controllability.\n", "versions": [{"version": "v1", "created": "Mon, 1 Aug 2016 13:58:50 GMT"}], "update_date": "2016-08-02", "authors_parsed": [["Wiles", "Laura", ""], ["Gu", "Shi", ""], ["Pasqualetti", "Fabio", ""], ["Bassett", "Danielle S.", ""], ["Meaney", "David F.", ""]]}, {"id": "1608.00936", "submitter": "Saad Nadeem", "authors": "Saad Nadeem and Arie Kaufman", "title": "Multimodal Brain Visualization", "comments": "SPIE Medical Imaging 2016, Proc. SPIE Medical Imaging: Biomedical\n  Applications in Molecular, Structural, and Functional Imaging, 2016", "journal-ref": "SPIE Medical Imaging, pp. 97881Y-97881Y. 2016", "doi": "10.1117/12.2217003", "report-no": null, "categories": "cs.GR q-bio.NC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Current connectivity diagrams of human brain image data are either overly\ncomplex or overly simplistic. In this work we introduce simple yet accurate\ninteractive visual representations of multiple brain image structures and the\nconnectivity among them. We map cortical surfaces extracted from human brain\nmagnetic resonance imaging (MRI) data onto 2D surfaces that preserve shape\n(angle), extent (area), and spatial (neighborhood) information for 2D (circular\ndisk) and 3D (spherical) mapping, split these surfaces into separate patches,\nand cluster functional and diffusion tractography MRI connections between pairs\nof these patches. The resulting visualizations are easier to compute on and\nmore visually intuitive to interact with than the original data, and facilitate\nsimultaneous exploration of multiple data sets, modalities, and statistical\nmaps.\n", "versions": [{"version": "v1", "created": "Tue, 2 Aug 2016 19:02:40 GMT"}, {"version": "v2", "created": "Sat, 6 Aug 2016 17:01:31 GMT"}, {"version": "v3", "created": "Tue, 9 Aug 2016 19:55:31 GMT"}, {"version": "v4", "created": "Thu, 1 Sep 2016 14:51:28 GMT"}], "update_date": "2016-09-02", "authors_parsed": [["Nadeem", "Saad", ""], ["Kaufman", "Arie", ""]]}, {"id": "1608.01161", "submitter": "Richard Betzel", "authors": "Richard F. Betzel, John D. Medaglia, Lia Papadopoulos, Graham Baum,\n  Ruben Gur, Raquel Gur, David Roalf, Theodore D. Satterthwaite, Danielle S.\n  Bassett", "title": "The modular organization of human anatomical brain networks: Accounting\n  for the cost of wiring", "comments": "22 pages, 7 figures, 9 supplemental figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "q-bio.NC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Brain networks are expected to be modular. However, existing techniques for\nestimating a network's modules make it difficult to assess the influence of\norganizational principles such as wiring cost reduction on the detected\nmodules. Here, we present a modification of an existing module detection\nalgorithm that allows us to focus on connections that are unexpected under a\ncost-reduction wiring rule and to identify modules from among these\nconnections. We apply this technique to anatomical brain networks and show that\nthe modules we detect differ from those detected using the standard technique.\nWe demonstrate that these novel modules are spatially distributed, exhibit\nunique functional fingerprints, and overlap considerably with rich clubs,\ngiving rise to an alternative and complementary interpretation of the\nfunctional roles of specific brain regions. Finally, we demonstrate that, using\nthe modified module detection approach, we can detect modules in a\ndevelopmental dataset that track normative patterns of maturation.\nCollectively, these findings support the hypothesis that brain networks are\ncomposed of modules and provide additional insight into the function of those\nmodules.\n", "versions": [{"version": "v1", "created": "Wed, 3 Aug 2016 12:07:40 GMT"}, {"version": "v2", "created": "Fri, 11 Nov 2016 23:26:09 GMT"}], "update_date": "2016-11-15", "authors_parsed": [["Betzel", "Richard F.", ""], ["Medaglia", "John D.", ""], ["Papadopoulos", "Lia", ""], ["Baum", "Graham", ""], ["Gur", "Ruben", ""], ["Gur", "Raquel", ""], ["Roalf", "David", ""], ["Satterthwaite", "Theodore D.", ""], ["Bassett", "Danielle S.", ""]]}, {"id": "1608.01179", "submitter": "David Zwicker", "authors": "David Zwicker", "title": "Normalized neural representations of natural odors", "comments": "10 pages, 5 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "q-bio.NC physics.bio-ph", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The olfactory system removes correlations in natural odors using a network of\ninhibitory neurons in the olfactory bulb. It has been proposed that this\nnetwork integrates the response from all olfactory receptors and inhibits them\nequally. However, how such global inhibition influences the neural\nrepresentations of odors is unclear. Here, we study a simple statistical model\nof this situation, which leads to concentration-invariant, sparse\nrepresentations of the odor composition. We show that the inhibition strength\ncan be tuned to obtain sparse representations that are still useful to\ndiscriminate odors that vary in relative concentration, size, and composition.\nThe model reveals two generic consequences of global inhibition: (i) odors with\nmany molecular species are more difficult to discriminate and (ii) receptor\narrays with heterogeneous sensitivities perform badly. Our work can thus help\nto understand how global inhibition shapes normalized odor representations for\nfurther processing in the brain.\n", "versions": [{"version": "v1", "created": "Wed, 3 Aug 2016 13:14:05 GMT"}], "update_date": "2016-08-04", "authors_parsed": [["Zwicker", "David", ""]]}, {"id": "1608.01191", "submitter": "Dominique Vuillaume", "authors": "Simon Desbief, Michele di Lauro, Stefano Casalini, David Guerin,\n  Silvia Tortorella, Marianna Barbalinardo, Adrica Kyndiah, Mauro Murgia,\n  Tobias Cramer, Fabio Biscarini and Dominique Vuillaume", "title": "Electrolyte-gated organic synapse transistor interfaced with neurons", "comments": "Full paper, figures and supporting information", "journal-ref": "Organic Electronics, 38, 21-28 (2016)", "doi": "10.1016/j.orgel.2016.07.028", "report-no": null, "categories": "cond-mat.mes-hall physics.bio-ph q-bio.NC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We demonstrate an electrolyte-gated hybrid nanoparticle/organic synapstor\n(synapse-transistor, termed EGOS) that exhibits short-term plasticity as\nbiological synapses. The response of EGOS makes it suitable to be interfaced\nwith neurons: short-term plasticity is observed at spike voltage as low as 50\nmV (in a par with the amplitude of action potential in neurons) and with a\ntypical response time in the range of tens milliseconds. Human neuroblastoma\nstem cells are adhered and differentiated into neurons on top of EGOS. We\nobserve that the presence of the cells does not alter short-term plasticity of\nthe device.\n", "versions": [{"version": "v1", "created": "Wed, 3 Aug 2016 13:42:16 GMT"}], "update_date": "2016-08-04", "authors_parsed": [["Desbief", "Simon", ""], ["di Lauro", "Michele", ""], ["Casalini", "Stefano", ""], ["Guerin", "David", ""], ["Tortorella", "Silvia", ""], ["Barbalinardo", "Marianna", ""], ["Kyndiah", "Adrica", ""], ["Murgia", "Mauro", ""], ["Cramer", "Tobias", ""], ["Biscarini", "Fabio", ""], ["Vuillaume", "Dominique", ""]]}, {"id": "1608.02027", "submitter": "Nikolaus Kriegeskorte", "authors": "Nikolaus Kriegeskorte, J\\\"orn Diedrichsen", "title": "Inferring brain-computational mechanisms with models of activity\n  measurements", "comments": "25 pages, 9 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "q-bio.NC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  High-resolution functional imaging is providing increasingly rich\nmeasurements of brain activity in animals and humans. A major challenge is to\nleverage such data to gain insight into the brain's computational mechanisms.\nThe first step is to define candidate brain-computational models (BCMs) that\ncan perform the behavioural task in question. We would then like to infer,\nwhich of the candidate BCMs best accounts for measured brain-activity data.\nHere we describe a method that complements each BCM by a measurement model\n(MM), which simulates the way the brain-activity measurements reflect neuronal\nactivity (e.g. local averaging in fMRI voxels or sparse sampling in array\nrecordings). The resulting generative model (BCM-MM) produces simulated\nmeasurements. In order to avoid having to fit the MM to predict each individual\nmeasurement channel of the brain-activity data, we compare the measured and\npredicted data at the level of summary statistics. We describe a novel\nparticular implementation of this approach, called probabilistic RSA (pRSA)\nwith measurement models, which uses representational dissimilarity matrices\n(RDMs) as the summary statistics. We validate this method by simulations of\nfMRI measurements (locally averaging voxels) based on a deep convolutional\nneural network for visual object recognition. Results indicate that the way the\nmeasurements sample the activity patterns strongly affects the apparent\nrepresentational dissimilarities. However, modelling of the measurement process\ncan account for these effects and different BCMs remain distinguishable even\nunder substantial noise. The pRSA method enables us to perform Bayesian\ninference on the set of BCMs and to recognise the data-generating model in each\ncase.\n", "versions": [{"version": "v1", "created": "Fri, 5 Aug 2016 21:38:37 GMT"}], "update_date": "2016-08-09", "authors_parsed": [["Kriegeskorte", "Nikolaus", ""], ["Diedrichsen", "J\u00f6rn", ""]]}, {"id": "1608.02229", "submitter": "Fernando Corbacho", "authors": "Fernando Corbacho", "title": "Towards the Self-constructive Brain: emergence of adaptive behavior", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NE cs.AI q-bio.NC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Adaptive behavior is mainly the result of adaptive brains. We go a step\nbeyond and claim that the brain does not only adapt to its surrounding reality\nbut rather, it builds itself up to constructs its own reality. That is, rather\nthan just trying to passively understand its environment, the brain is the\narchitect of its own reality in an active process where its internal models of\nthe external world frame how its new interactions with the environment are\nassimilated. These internal models represent relevant predictive patterns of\ninteraction all over the different brain structures: perceptual, sensorimotor,\nmotor, etc. The emergence of adaptive behavior arises from this\nself-constructive nature of the brain, based on the following principles of\norganization: self-experimental, self- growing, and self-repairing.\nSelf-experimental, since to ensure survival, the self-constructive brain (SCB)\nis an active machine capable of performing experiments of its own interactions\nwith the environment by mental simulation. Self-growing, since it dynamically\nand incrementally constructs internal structures in order to build a model of\nthe world as it gathers statistics from its interactions with the environment.\nSelf-repairing, since to survive the SCB must also be robust and capable of\nfinding ways to repair parts of previously working structures and hence\nre-construct a previous relevant pattern of activity.\n", "versions": [{"version": "v1", "created": "Sun, 7 Aug 2016 15:52:28 GMT"}], "update_date": "2016-08-09", "authors_parsed": [["Corbacho", "Fernando", ""]]}, {"id": "1608.02838", "submitter": "Bulcs\\'u S\\'andor", "authors": "Laura Martin, Bulcs\\'u S\\'andor and Claudius Gros", "title": "Closed-loop robots driven by short-term synaptic plasticity: Emergent\n  explorative vs. limit-cycle locomotion", "comments": "15 pages, 12 figures", "journal-ref": "Front. Neurorobot. 10:12 (2016)", "doi": "10.3389/fnbot.2016.00012", "report-no": null, "categories": "q-bio.NC cs.RO nlin.AO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We examine the hypothesis, that short-term synaptic plasticity (STSP) may\ngenerate self-organized motor patterns. We simulated sphere-shaped autonomous\nrobots, within the LPZRobots simulation package, containing three weights\nmoving along orthogonal internal rods. The position of a weight is controlled\nby a single neuron receiving excitatory input from the sensor, measuring its\nactual position, and inhibitory inputs from the other two neurons. The\ninhibitory connections are transiently plastic, following physiologically\ninspired STSP-rules. We find that a wide palette of motion patterns are\ngenerated through the interaction of STSP, robot, and environment (closed-loop\nconfiguration), including various forward meandering and circular motions,\ntogether with chaotic trajectories. The observed locomotion is robust with\nrespect to additional interactions with obstacles. In the chaotic phase the\nrobot is seemingly engaged in actively exploring its environment. We believe\nthat our results constitute a concept of proof that transient synaptic\nplasticity, as described by STSP, may potentially be important for the\ngeneration of motor commands and for the emergence of complex locomotion\npatterns, adapting seamlessly also to unexpected environmental feedback. We\nobserve spontaneous and collision induced mode switchings, finding in addition,\nthat locomotion may follow transiently limit cycles which are otherwise\nunstable. Regular locomotion corresponds to stable limit cycles in the\nsensorimotor loop, which may be characterized in turn by arbitrary angles of\npropagation. This degeneracy is, in our analysis, one of the drivings for the\nchaotic wandering observed for selected parameter settings, which is induced by\nthe smooth diffusion of the angle of propagation.\n", "versions": [{"version": "v1", "created": "Tue, 9 Aug 2016 15:29:31 GMT"}, {"version": "v2", "created": "Thu, 20 Oct 2016 09:02:18 GMT"}, {"version": "v3", "created": "Wed, 14 Mar 2018 12:55:33 GMT"}], "update_date": "2019-01-18", "authors_parsed": [["Martin", "Laura", ""], ["S\u00e1ndor", "Bulcs\u00fa", ""], ["Gros", "Claudius", ""]]}, {"id": "1608.03425", "submitter": "Haiguang Wen", "authors": "Haiguang Wen, Junxing Shi, Yizhen Zhang, Kun-Han Lu, Jiayue Cao,\n  Zhongming Liu", "title": "Neural Encoding and Decoding with Deep Learning for Dynamic Natural\n  Vision", "comments": "27 pages, 10 figures, 1 table", "journal-ref": "Cerebral Cortex. 2017 pp.1-25", "doi": "10.1093/cercor/bhx268", "report-no": null, "categories": "q-bio.NC q-bio.QM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Convolutional neural network (CNN) driven by image recognition has been shown\nto be able to explain cortical responses to static pictures at ventral-stream\nareas. Here, we further showed that such CNN could reliably predict and decode\nfunctional magnetic resonance imaging data from humans watching natural movies,\ndespite its lack of any mechanism to account for temporal dynamics or feedback\nprocessing. Using separate data, encoding and decoding models were developed\nand evaluated for describing the bi-directional relationships be-tween the CNN\nand the brain. Through the encoding models, the CNN-predicted areas covered not\nonly the ventral stream, but also the dorsal stream, albe-it to a lesser\ndegree; single-voxel response was visualized as the specific pixel pattern that\ndrove the response, revealing the distinct representation of individual\ncortical location; cortical activation was synthesized from natural images with\nhigh-throughput to map category representation, con-trast, and selectivity.\nThrough the decoding models, fMRI signals were directly decoded to estimate the\nfeature representations in both visual and semantic spaces, for direct visual\nreconstruction and seman-tic categorization, respectively. These results\ncor-roborate, generalize, and extend previous findings, and highlight the value\nof using deep learning, as an all-in-one model of the visual cortex, to\nunderstand and decode natural vision.\n", "versions": [{"version": "v1", "created": "Thu, 11 Aug 2016 11:51:21 GMT"}, {"version": "v2", "created": "Tue, 14 Nov 2017 17:35:51 GMT"}], "update_date": "2017-11-15", "authors_parsed": [["Wen", "Haiguang", ""], ["Shi", "Junxing", ""], ["Zhang", "Yizhen", ""], ["Lu", "Kun-Han", ""], ["Cao", "Jiayue", ""], ["Liu", "Zhongming", ""]]}, {"id": "1608.03461", "submitter": "William Marshall", "authors": "William Marshall and Larissa Albantakis and Giulio Tononi", "title": "Black-boxing and cause-effect power", "comments": "45 pages (32 main text, 13 supplementary), 14 figures (9 main text, 5\n  supplementary)", "journal-ref": "PLoS computational biology 14.4 (2018): e1006114", "doi": "10.1371/journal.pcbi.1006114", "report-no": null, "categories": "q-bio.NC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Reductionism assumes that causation in the physical world occurs at the micro\nlevel, excluding the emergence of macro-level causation. We challenge this\nreductionist assumption by employing a principled, well-defined measure of\nintrinsic cause-effect power - integrated information ({\\Phi}), and showing\nthat, according to this measure, it is possible for a macro level to \"beat\" the\nmicro level. Simple systems were evaluated for {\\Phi} across different spatial\nand temporal scales by systematically considering all possible black boxes.\nThese are macro elements that consist of one or more micro elements over one or\nmore micro updates. Cause-effect power was evaluated based on the inputs and\noutputs of the black boxes, ignoring the internal micro elements that support\ntheir input-output function. We show how black-box elements can have more\ncommon inputs and outputs than the corresponding micro elements, revealing the\nemergence of high-order mechanisms and joint constraints that are not apparent\nat the micro level. As a consequence, a macro, black-box system can have higher\n{\\Phi} than its micro constituents by having more mechanisms (higher\ncomposition) that are more interconnected (higher integration). We also show\nthat, for a given micro system, one can identify local maxima of {\\Phi} across\nseveral spatiotemporal scales. The framework is demonstrated on a simple\nbiological system, the Boolean network model of the fission-yeast cell-cycle,\nfor which we identify stable local maxima during the course of its simulated\nbiological function. These local maxima correspond to macro levels of\norganization at which emergent cause-effect properties of physical systems come\ninto focus, and provide a natural vantage point for scientific inquiries.\n", "versions": [{"version": "v1", "created": "Sun, 7 Aug 2016 22:07:17 GMT"}, {"version": "v2", "created": "Tue, 14 May 2019 02:05:45 GMT"}], "update_date": "2019-05-15", "authors_parsed": [["Marshall", "William", ""], ["Albantakis", "Larissa", ""], ["Tononi", "Giulio", ""]]}, {"id": "1608.03463", "submitter": "David Cox", "authors": "David Cox", "title": "Clique Topology Reveals Intrinsic Geometric Structure in Neural\n  Correlations: An Overview", "comments": "10 pages, 8 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "q-bio.NC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This publication serves as an overview of clique topology -- a novel matrix\nanalysis technique used to extract structural features from neural activity\ndata that contains hidden nonlinearities. We highlight work done by Gusti et\nal. which introduces clique topology and verifies its applicability to neural\nfeature extraction by showing that neural correlations in the rat hippocampus\nare determined by geometric structure of hippocampal circuits, rather than\nbeing a consequence of positional coding.\n", "versions": [{"version": "v1", "created": "Mon, 8 Aug 2016 00:00:41 GMT"}], "update_date": "2016-08-12", "authors_parsed": [["Cox", "David", ""]]}, {"id": "1608.03464", "submitter": "Alan Paris", "authors": "Alan Paris, George Atia, Azadeh Vosoughi, Stephen Berman", "title": "A New Statistical Model of Electroencephalogram Noise Spectra for\n  Real-time Brain-Computer Interfaces", "comments": "Revised submission to IEEE EMBS Trans. Biomed. Eng. 12 pages, 9\n  figures", "journal-ref": null, "doi": "10.1109/TBME.2016.2606595", "report-no": null, "categories": "q-bio.NC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  $Objective$: A characteristic of neurological signal processing is high\nlevels of noise from sub-cellular ion channels up to whole-brain processes. In\nthis paper, we propose a new model of electroencephalogram (EEG) background\nperiodograms, based on a family of functions which we call generalized van der\nZiel--McWhorter (GVZM) power spectral densities (PSDs). To the best of our\nknowledge, the GVZM PSD function is the only EEG noise model which has\nrelatively few parameters, matches recorded EEG PSD's with high accuracy from 0\nHz to over 30 Hz, and has approximately $1/f^\\theta$ behavior in the\nmid-frequencies without infinities. $Methods$: We validate this model using\nthree approaches. First, we show how GVZM PSDs can arise in population of ion\nchannels in maximum entropy equilibrium. Second, we present a class of mixed\nautoregressive models, which simulate brain background noise and whose\nperiodograms are asymptotic to the GVZM PSD. Third, we present two real-time\nestimation algorithms for steady-state visual evoked potential (SSVEP)\nfrequencies, and analyze their performance statistically. $Results$: In\npairwise comparisons, the GVZM-based algorithms showed statistically\nsignificant accuracy improvement over two well-known and widely-used SSVEP\nestimators. $Conclusion$: The GVZM noise model can be a useful and reliable\ntechnique for EEG signal processing. $Significance$: Understanding EEG noise is\nessential for EEG-based neurology and applications such as real-time\nbrain-computer interfaces (BCIs), which must make accurate control decisions\nfrom very short data epochs. The GVZM approach represents a successful new\nparadigm for understanding and managing this neurological noise.\n", "versions": [{"version": "v1", "created": "Sun, 24 Jul 2016 06:36:59 GMT"}], "update_date": "2016-11-17", "authors_parsed": [["Paris", "Alan", ""], ["Atia", "George", ""], ["Vosoughi", "Azadeh", ""], ["Berman", "Stephen", ""]]}, {"id": "1608.03465", "submitter": "Danilo Bzdok", "authors": "Danilo Bzdok and B. T. Thomas Yeo", "title": "The Future of Data Analysis in the Neurosciences", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "q-bio.NC stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Neuroscience is undergoing faster changes than ever before. Over 100 years\nour field qualitatively described and invasively manipulated single or few\norganisms to gain anatomical, physiological, and pharmacological insights. In\nthe last 10 years neuroscience spawned quantitative big-sample datasets on\nmicroanatomy, synaptic connections, optogenetic brain-behavior assays, and\nhigh-level cognition. While growing data availability and information\ngranularity have been amply discussed, we direct attention to a routinely\nneglected question: How will the unprecedented data richness shape data\nanalysis practices? Statistical reasoning is becoming more central to distill\nneurobiological knowledge from healthy and pathological brain recordings. We\nbelieve that large-scale data analysis will use more models that are\nnon-parametric, generative, mixing frequentist and Bayesian aspects, and\ngrounded in different statistical inferences.\n", "versions": [{"version": "v1", "created": "Fri, 5 Aug 2016 20:43:21 GMT"}], "update_date": "2016-08-12", "authors_parsed": [["Bzdok", "Danilo", ""], ["Yeo", "B. T. Thomas", ""]]}, {"id": "1608.03467", "submitter": "Laurens Michiels Van Kessenich", "authors": "L. Michiels van Kessenich, L. de Arcangelis and H. J. Herrmann", "title": "Synaptic plasticity and neuronal refractory time cause scaling behaviour\n  of neuronal avalanches", "comments": "9 pages, 4 figures, to be published in Scientific Reports", "journal-ref": null, "doi": null, "report-no": null, "categories": "q-bio.NC nlin.AO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Neuronal avalanches measured in vitro and in vivo in different cortical\nnetworks consistently exhibit power law behaviour for the size and duration\ndistributions with exponents typical for a mean field self-organized branching\nprocess. These exponents are also recovered in neuronal network simulations\nimplementing various neuronal dynamics on different network topologies. They\ncan therefore be considered a very robust feature of spontaneous neuronal\nactivity. Interestingly, this scaling behaviour is also observed on regular\nlattices in finite dimensions, which raises the question about the origin of\nthe mean field behaviour observed experimentally. In this study we provide an\nanswer to this open question by investigating the effect of activity dependent\nplasticity in combination with the neuronal refractory time in a neuronal\nnetwork. Results show that the refractory time hinders backward avalanches\nforcing a directed propagation. Hebbian plastic adaptation plays the role of\nsculpting these directed avalanche patterns into the topology of the network\nslowly changing it into a branched structure where loops are marginal.\n", "versions": [{"version": "v1", "created": "Wed, 10 Aug 2016 13:37:04 GMT"}], "update_date": "2016-08-12", "authors_parsed": [["van Kessenich", "L. Michiels", ""], ["de Arcangelis", "L.", ""], ["Herrmann", "H. J.", ""]]}, {"id": "1608.03477", "submitter": "Shigekazu Oda", "authors": "Shigekazu Oda, Yu Toyoshima and Mario de Bono", "title": "Modulation of sensory information processing by a neuroglobin in C.\n  elegans", "comments": "29 page, 5 figures, 11 supplementary figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "q-bio.NC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Sensory receptor neurons match their dynamic range to ecologically relevant\nstimulus intensities. How this tuning is achieved is poorly understood in most\nreceptors. We show that in the C. elegans URX O2 sensing neurons two putative\nmolecular O2 sensors, a neuroglobin and O2-binding soluble guanylate cyclases,\nwork antagonistically to sculpt a slowly sigmoidal O2 response curve tuned to\napproach saturation when O2 reaches 21%. glb-5 imposes this sigmoidal function\nby inhibiting O2-evoked Ca2+ responses in URX when O2 levels fall. Without\nGLB-5, the URX response curve approaches saturation at 15% O2. Behaviorally,\nGLB-5 signaling broadens the O2 preference of C. elegans while maintaining\nstrong avoidance of 21% O2. Our computational aerotaxis model suggests that the\nrelationship between GLB-5-modulated URX responses and reversal behavior is\nsufficient to broaden O2-preference. Thus, a neuroglobin can shift neural\ninformation coding leading to a change in perception and altered behavior.\n", "versions": [{"version": "v1", "created": "Thu, 11 Aug 2016 14:24:40 GMT"}, {"version": "v2", "created": "Wed, 14 Sep 2016 15:25:16 GMT"}], "update_date": "2016-09-15", "authors_parsed": [["Oda", "Shigekazu", ""], ["Toyoshima", "Yu", ""], ["de Bono", "Mario", ""]]}, {"id": "1608.03520", "submitter": "Ann Sizemore", "authors": "Ann Sizemore, Chad Giusti, Ari Kahn, Richard F. Betzel, and Danielle\n  S. Bassett", "title": "Cliques and Cavities in the Human Connectome", "comments": "19 pages, 6 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "q-bio.NC math.AT math.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Encoding brain regions and their connections as a network of nodes and edges\ncaptures many of the possible paths along which information can be transmitted\nas humans process and perform complex behaviors. Because cognitive processes\ninvolve large and distributed networks of brain areas, examinations of\nmulti-node routes within larger connection patterns can offer fundamental\ninsights into the complexities of brain function. Here, we investigate both\ndensely connected groups of nodes that could perform local computations as well\nas larger patterns of interactions that would allow for parallel processing.\nFinding such structures necessitates we move from considering pairwise\ninteractions to capturing higher order relations, concepts naturally expressed\nin the language of algebraic topology. These tools can be used to study\nmesoscale structures arising from the arrangement of densely connected\nsubstructures called cliques in otherwise sparsely connected brain networks. We\ndetect cliques (all-to-all connected sets of brain regions) in the average\nstructural connectomes of 8 healthy adults and discover the presence of more\nlarge cliques than expected in null networks constructed via wiring\nminimization, providing architecture through which brain network can perform\nrapid, local processing. We then locate topological cavities of different\ndimensions, around which information may flow in either diverging or converging\npatterns. These cavities exist consistently across subjects, differ from those\nobserved in null model networks, and link regions of early and late\nevolutionary origin in long loops, underscoring their unique role in\ncontrolling brain function. These results offer a first demonstration that\ntechniques from algebraic topology offer a novel perspective on structural\nconnectomics, highlighting loop-like paths as crucial features in the human\nbrain's structural architecture.\n", "versions": [{"version": "v1", "created": "Thu, 11 Aug 2016 16:17:57 GMT"}, {"version": "v2", "created": "Tue, 20 Dec 2016 14:04:30 GMT"}], "update_date": "2016-12-21", "authors_parsed": [["Sizemore", "Ann", ""], ["Giusti", "Chad", ""], ["Kahn", "Ari", ""], ["Betzel", "Richard F.", ""], ["Bassett", "Danielle S.", ""]]}, {"id": "1608.03616", "submitter": "Rastko Ciric", "authors": "Rastko Ciric, Daniel H. Wolf, Jonathan D. Power, David R. Roalf,\n  Graham Baum, Kosha Ruparel, Russell T. Shinohara, Mark A. Elliott, Simon B.\n  Eickhoff, Christos Davatzikos, Ruben C. Gur, Raquel E. Gur, Danielle S.\n  Bassett, Theodore D. Satterthwaite", "title": "Benchmarking confound regression strategies for the control of motion\n  artifact in studies of functional connectivity", "comments": "16 pages, 5 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "q-bio.NC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Since initial reports regarding the impact of motion artifact on measures of\nfunctional connectivity, there has been a proliferation of confound regression\nmethods to limit its impact. However, recent techniques have not been\nsystematically evaluated using consistent outcome measures. Here, we provide a\nsystematic evaluation of 12 commonly used confound regression methods in 193\nyoung adults. Specifically, we compare methods according to three benchmarks,\nincluding the residual relationship between motion and connectivity,\ndistance-dependent effects of motion on connectivity, and additional degrees of\nfreedom lost in confound regression. Our results delineate two clear trade-offs\namong methods. First, methods that include global signal regression minimize\nthe relationship between connectivity and motion, but unmask distance-dependent\nartifact. In contrast, censoring methods mitigate both motion artifact and\ndistance-dependence, but use additional degrees of freedom. Taken together,\nthese results emphasize the heterogeneous efficacy of proposed methods, and\nsuggest that different confound regression strategies may be appropriate in the\ncontext of specific scientific goals.\n", "versions": [{"version": "v1", "created": "Thu, 11 Aug 2016 20:48:18 GMT"}], "update_date": "2016-08-15", "authors_parsed": [["Ciric", "Rastko", ""], ["Wolf", "Daniel H.", ""], ["Power", "Jonathan D.", ""], ["Roalf", "David R.", ""], ["Baum", "Graham", ""], ["Ruparel", "Kosha", ""], ["Shinohara", "Russell T.", ""], ["Elliott", "Mark A.", ""], ["Eickhoff", "Simon B.", ""], ["Davatzikos", "Christos", ""], ["Gur", "Ruben C.", ""], ["Gur", "Raquel E.", ""], ["Bassett", "Danielle S.", ""], ["Satterthwaite", "Theodore D.", ""]]}, {"id": "1608.03619", "submitter": "Graham Baum", "authors": "Graham L. Baum, Rastko Ciric, David R. Roalf, Richard F. Betzel, Tyler\n  M. Moore, Russel T. Shinohara, Ari E. Kahn, Megan Quarmley, Philip A. Cook,\n  Mark A. Elliot, Kosha Ruparel, Raquel E. Gur, Ruben C. Gur, Danielle S.\n  Bassett, Theodore D. Satterthwaite", "title": "Modular Segregation of Structural Brain Networks Supports the\n  Development of Executive Function in Youth", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "q-bio.NC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The human brain is organized into large-scale functional modules that have\nbeen shown to evolve in childhood and adolescence. However, it remains unknown\nwhether structural brain networks are similarly refined during development,\npotentially allowing for improvements in executive function. In a sample of 882\nparticipants (ages 8-22) who underwent diffusion imaging as part of the\nPhiladelphia Neurodevelopmental Cohort, we demonstrate that structural network\nmodules become more segregated with age, with weaker connections between\nmodules and stronger connections within modules. Evolving modular topology\nfacilitated network integration, driven by age-related strengthening of hub\nedges that were present both within and between modules. Critically, both\nmodular segregation and network integration were associated with enhanced\nexecutive performance, and mediated the improvement of executive functioning\nwith age. Together, results delineate a process of structural network\nmaturation that supports executive function in youth.\n", "versions": [{"version": "v1", "created": "Thu, 11 Aug 2016 21:09:40 GMT"}], "update_date": "2016-08-15", "authors_parsed": [["Baum", "Graham L.", ""], ["Ciric", "Rastko", ""], ["Roalf", "David R.", ""], ["Betzel", "Richard F.", ""], ["Moore", "Tyler M.", ""], ["Shinohara", "Russel T.", ""], ["Kahn", "Ari E.", ""], ["Quarmley", "Megan", ""], ["Cook", "Philip A.", ""], ["Elliot", "Mark A.", ""], ["Ruparel", "Kosha", ""], ["Gur", "Raquel E.", ""], ["Gur", "Ruben C.", ""], ["Bassett", "Danielle S.", ""], ["Satterthwaite", "Theodore D.", ""]]}, {"id": "1608.03655", "submitter": "Maurizio De Pitt\\`a", "authors": "Maurizio De Pitt\\`a and Nicolas Brunel", "title": "Modulation of synaptic plasticity by glutamatergic gliotransmission: A\n  modeling study", "comments": "55 pages, 7 figures, 1 table", "journal-ref": "Neural Plasticity, volume 2016 (2016), Article ID 7607924", "doi": "10.1155/2016/7607924", "report-no": null, "categories": "q-bio.NC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Glutamatergic gliotransmission, that is the release of glutamate from\nperisynaptic astrocyte processes in an activity-dependent manner, has emerged\nas a potentially crucial signaling pathway for regulation of synaptic\nplasticity, yet its modes of expression and function in vivo remain unclear.\nHere, we focus on two experimentally well-identified gliotransmitter patwhays:\n(i)~modulations of synaptic release and (ii)~postynaptic slow inward currents\nmediated by glutamate released from astrocytes, and investigate their possible\nfunctional relevance on synaptic plasticity in a biophysical model of an\nastrocyte-regulated synapse. Our model predicts that both pathways could\nprofoundly affect both short- and long-term plasticity. In particular,\nactivity-dependent glutamate release from astrocytes, could dramatically change\nspike-timing--dependent plasticity, turning potentiation into depression (and\nvice versa) for the same protocol.\n", "versions": [{"version": "v1", "created": "Fri, 12 Aug 2016 01:46:26 GMT"}], "update_date": "2016-08-15", "authors_parsed": [["De Pitt\u00e0", "Maurizio", ""], ["Brunel", "Nicolas", ""]]}, {"id": "1608.03714", "submitter": "Haiping Huang", "authors": "Haiping Huang and Taro Toyoizumi", "title": "Unsupervised feature learning from finite data by message passing:\n  discontinuous versus continuous phase transition", "comments": "8 pages, 7 figures (5 pages, 4 figures in the main text and 3 pages\n  of appendix)", "journal-ref": "Phys. Rev. E 94, 062310 (2016)", "doi": "10.1103/PhysRevE.94.062310", "report-no": null, "categories": "cond-mat.dis-nn cond-mat.stat-mech cs.LG q-bio.NC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Unsupervised neural network learning extracts hidden features from unlabeled\ntraining data. This is used as a pretraining step for further supervised\nlearning in deep networks. Hence, understanding unsupervised learning is of\nfundamental importance. Here, we study the unsupervised learning from a finite\nnumber of data, based on the restricted Boltzmann machine learning. Our study\ninspires an efficient message passing algorithm to infer the hidden feature,\nand estimate the entropy of candidate features consistent with the data. Our\nanalysis reveals that the learning requires only a few data if the feature is\nsalient and extensively many if the feature is weak. Moreover, the entropy of\ncandidate features monotonically decreases with data size and becomes negative\n(i.e., entropy crisis) before the message passing becomes unstable, suggesting\na discontinuous phase transition. In terms of convergence time of the message\npassing algorithm, the unsupervised learning exhibits an easy-hard-easy\nphenomenon as the training data size increases. All these properties are\nreproduced in an approximate Hopfield model, with an exception that the entropy\ncrisis is absent, and only continuous phase transition is observed. This key\ndifference is also confirmed in a handwritten digits dataset. This study\ndeepens our understanding of unsupervised learning from a finite number of\ndata, and may provide insights into its role in training deep networks.\n", "versions": [{"version": "v1", "created": "Fri, 12 Aug 2016 08:35:22 GMT"}, {"version": "v2", "created": "Fri, 11 Nov 2016 01:49:13 GMT"}], "update_date": "2016-12-23", "authors_parsed": [["Huang", "Haiping", ""], ["Toyoizumi", "Taro", ""]]}, {"id": "1608.03908", "submitter": "Alessio Franci", "authors": "Fernando Casta\\~nos, Alessio Franci", "title": "Implementing robust neuromodulation in neuromorphic circuits", "comments": null, "journal-ref": "Neurocomputing, 233:3 - 13, April 2017", "doi": "10.1016/j.neucom.2016.08.099", "report-no": "2017", "categories": "math.OC q-bio.NC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We introduce a methodology to implement the physiological transition {between\ndistinct neuronal spiking modes} in electronic circuits composed of resistors,\ncapacitors and transistors. The result is a simple neuromorphic device\norganized by the same geometry {and exhibiting the same input--output\nproperties as} high-dimensional electrophysiological neuron models.\n{Preliminary} experimental results highlight the robustness of the approach in\nreal-world applications.\n", "versions": [{"version": "v1", "created": "Fri, 12 Aug 2016 20:44:26 GMT"}], "update_date": "2019-11-14", "authors_parsed": [["Casta\u00f1os", "Fernando", ""], ["Franci", "Alessio", ""]]}, {"id": "1608.04005", "submitter": "Nirag Kadakia", "authors": "Nirag Kadakia, Eve Armstrong, Daniel Breen, Uriel Morone, Arij Daou,\n  Daniel Margoliash, Henry DI Abarbanel", "title": "Nonlinear Statistical Data Assimilation for HVC$_{\\text{RA}}$ Neurons in\n  the Avian Song System", "comments": "19 pages, 14 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "q-bio.NC physics.bio-ph physics.data-an", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  With the goal of building a model of the HVC nucleus in the avian song\nsystem, we discuss in detail a model of HVC$_{\\text{RA}}$ projection neurons\ncomprised of a somatic compartment with fast Na$^+$ and K$^+$ currents and a\ndendritic compartment with slower Ca$^{2+}$ dynamics. We show this model\nqualitatively exhibits many observed electrophysiological behaviors. We then\nshow in numerical procedures how one can design and analyze feasible laboratory\nexperiments that allow the estimation of all of the many parameters and\nunmeasured dynamical variables, given observations of the somatic voltage\n$V_s(t)$ alone. A key to this procedure is to initially estimate the slow\ndynamics associated with Ca, blocking the fast Na and K variations, and then\nwith the Ca parameters fixed, estimate the fast Na and K dynamics. This\nseparation of time scales provides a numerically robust method for completing\nthe full neuron model, and the efficacy of the method is tested by prediction\nwhen observations are complete. The simulation provides a framework for the\nslice preparation experiments and illustrates the use of data assimilation\nmethods for the design of those experiments.\n", "versions": [{"version": "v1", "created": "Sat, 13 Aug 2016 16:45:07 GMT"}, {"version": "v2", "created": "Thu, 29 Sep 2016 17:35:22 GMT"}], "update_date": "2016-09-30", "authors_parsed": [["Kadakia", "Nirag", ""], ["Armstrong", "Eve", ""], ["Breen", "Daniel", ""], ["Morone", "Uriel", ""], ["Daou", "Arij", ""], ["Margoliash", "Daniel", ""], ["DI Abarbanel", "Henry", ""]]}, {"id": "1608.04098", "submitter": "Henry Tuckwell", "authors": "Daniel Gandolfo, Roger Rodriguez and Henry C. Tuckwell", "title": "Mean field analysis of large-scale interacting populations of stochastic\n  conductance-based spiking neurons using the Klimontovich method", "comments": "20 pages", "journal-ref": null, "doi": "10.1007/s10955-016-1702-x", "report-no": null, "categories": "q-bio.NC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We investigate the dynamics of large-scale interacting neural populations,\ncomposed of conductance based, spiking model neurons with modifiable synaptic\nconnection strengths, which are possibly also subjected to external noisy\ncurrents. The network dynamics is controlled by a set of neural population\nprobability distributions ($\\mathrm{PPD}$)which are constructed along the same\nlines as in the Klimontovich approach to the kinetic theory of plasmas. An\nexact non-closed, nonlinear, system of integro-partial differential equations\nis derived for the $\\mathrm{PPD}$s. As is customary, a closing procedure leads\nto a mean field limit. The equations we have obtained are of the same type as\nthose which have been recently derived using rigorous techniques of probability\ntheory. The numerical solutions of these so called McKean-Vlasov-Fokker-Planck\nequations, which are only valid in the limit of infinite size networks,\nactually shows that the statistical measures as obtained from $\\mathrm{PPD}$s\nare in good agreement with those obtained through direct integration of the\nstochastic dynamical system for large but finite size networks. Although\nnumerical solutions have been obtained in the case of Fitzhugh-Nagumo model\nneurons, the theory can be readily applied to systems of Hodgkin-Huxley type\nmodel neurons of arbitrary dimension.\n", "versions": [{"version": "v1", "created": "Sun, 14 Aug 2016 13:07:55 GMT"}, {"version": "v2", "created": "Mon, 23 Jan 2017 15:25:42 GMT"}], "update_date": "2017-02-01", "authors_parsed": [["Gandolfo", "Daniel", ""], ["Rodriguez", "Roger", ""], ["Tuckwell", "Henry C.", ""]]}, {"id": "1608.04433", "submitter": "Daniel Breen", "authors": "Daniel Breen, Sasha Shirman, Eve Armstrong, Nirag Kadakia, Henry\n  Abarbanel", "title": "HVC Interneuron Properties from Statistical Data Assimilation", "comments": "28 pages, 32 figures. Not yet submitted to any journal", "journal-ref": null, "doi": null, "report-no": null, "categories": "q-bio.NC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Data assimilation (DA) solves the inverse problem of inferring initial\nconditions given data and a model. Here we use biophysically motivated\nHodgkin-Huxley (HH) models of avian HVCI neurons, experimentally obtained\nrecordings of these neurons, and our data assimilation algorithm to infer the\nfull set of parameters and a minimal set of ionic currents precisely\nreproducing the observed waveform information. We find many distinct validated\nsets of parameters selected by our DA method and choice of model. We conclude\nexploring variations on the inverse problem applied to neurons producing\naccurate or inaccurate results; by manipulating data presented to the\nalgorithm, varying sample rate and waveform; and by manipulating the model by\nadding and subtracting ionic currents.\n", "versions": [{"version": "v1", "created": "Mon, 15 Aug 2016 23:02:29 GMT"}], "update_date": "2016-08-17", "authors_parsed": [["Breen", "Daniel", ""], ["Shirman", "Sasha", ""], ["Armstrong", "Eve", ""], ["Kadakia", "Nirag", ""], ["Abarbanel", "Henry", ""]]}, {"id": "1608.04540", "submitter": "Gabriele Scheler", "authors": "Gabriele Scheler, Jean-Marc Fellous", "title": "Dopamine modulation of prefrontal delay activity-reverberatory activity\n  and sharpness of tuning curves", "comments": "CNS Conference 2001; 2 figures", "journal-ref": "Neurocomputing 38-40:1549-1556; June 2001", "doi": "10.1016/S0925-2312(01)00559-8", "report-no": null, "categories": "q-bio.NC cs.NE q-bio.SC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recent electrophysiological experiments have shown that dopamine (D1)\nmodulation of pyramidal cells in prefrontal cortex reduces spike frequency\nadaptation and enhances NMDA transmission. Using four models, from\nmulticompartmental to integrate and fire, we examine the effects of these\nmodulations on sustained (delay) activity in a reverberatory network. We find\nthat D1 modulation may enable robust network bistability yielding selective\nreverberation among cells that code for a particular item or location. We\nfurther show that the tuning curve of such cells is sharpened, and that\nsignal-to-noise ratio is increased. We postulate that D1 modulation affects the\ntuning of \"memory fields\" and yield efficient distributed dynamic\nrepresentations.\n", "versions": [{"version": "v1", "created": "Tue, 16 Aug 2016 10:14:55 GMT"}], "update_date": "2017-12-12", "authors_parsed": [["Scheler", "Gabriele", ""], ["Fellous", "Jean-Marc", ""]]}, {"id": "1608.04652", "submitter": "Francesco Alderisio", "authors": "Francesco Alderisio, Maria Lombardi, Gianfranco Fiore, Mario di\n  Bernardo", "title": "Study of movement coordination in human ensembles via a novel\n  computer-based set-up", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC physics.data-an physics.soc-ph q-bio.NC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Movement coordination in human ensembles has been studied little in the\ncurrent literature. In the existing experimental works, situations where all\nsubjects are connected with each other through direct visual and auditory\ncoupling, and social interaction affects their coordination, have been\ninvestigated. Here, we study coordination in human ensembles via a novel\ncomputer-based set-up that enables individuals to coordinate each other's\nmotion from a distance so as to minimize the influence of social interaction.\nThe proposed platform makes it possible to implement different visual\ninteraction patterns among the players, so that participants take into\nconsideration the motion of a designated subset of the others. This allows the\nevaluation of the exclusive effects on coordination of the structure of\ninterconnections among the players and their own dynamics. Our set-up enables\nalso the deployment of virtual players to investigate dyadic interaction\nbetween a human and a virtual agent, as well as group synchronization in mixed\nteams of human and virtual agents. We use this novel set-up to study\ncoordination both in dyads and in groups over different structures of\ninterconnections, with and without virtual agents. We find that, in dual\ninteraction, virtual players manage to interact with participants in a\nhuman-like fashion, thus confirming findings in previous work. We also observe\nthat, in group interaction, the level of coordination among humans in the\nabsence of direct visual and auditory coupling depends on the structure of\ninterconnections among participants. This confirms, as recently suggested in\nthe literature, that different coordination levels are achieved over diverse\nvisual pairings in the presence and in the absence of social interaction. We\npresent preliminary experimental results on the effect on group coordination of\ndeploying virtual computer agents in the human ensemble.\n", "versions": [{"version": "v1", "created": "Fri, 29 Jul 2016 14:24:21 GMT"}], "update_date": "2016-08-17", "authors_parsed": [["Alderisio", "Francesco", ""], ["Lombardi", "Maria", ""], ["Fiore", "Gianfranco", ""], ["di Bernardo", "Mario", ""]]}, {"id": "1608.04680", "submitter": "Dan Schult", "authors": "Ken Segall, Matthew LeGro, Steven Kaplan, Oleksiy Svitelskiy, Shreeya\n  Khadka, Patrick Crotty, Daniel Schult", "title": "Synchronization dynamics on the picosecond timescale in coupled\n  Josephson junction neurons", "comments": "13 pages, 8 figures", "journal-ref": "Phys. Rev. E 95, 032220 (2017)", "doi": "10.1103/PhysRevE.95.032220", "report-no": null, "categories": "q-bio.NC cond-mat.dis-nn cs.ET physics.bio-ph", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Conventional digital computation is rapidly approaching physical limits for\nspeed and energy dissipation. Here we fabricate and test a simple neuromorphic\ncircuit that models neuronal somas, axons and synapses with superconducting\nJosephson junctions. The circuit models two mutually coupled excitatory\nneurons. In some regions of parameter space the neurons are desynchronized. In\nothers, the Josephson neurons synchronize in one of two states, in-phase or\nanti-phase. An experimental alteration of the delay and strength of the\nconnecting synapses can toggle the system back and forth in a phase-flip\nbifurcation. Firing synchronization states are calculated >70,000 times faster\nthan conventional digital approaches. With their speed and low energy\ndissipation (10-17 Joules/spike), this set of proof-of- concept experiments\nestablishes Josephson junction neurons as a viable approach for improvements in\nneuronal computation as well as applications in neuromorphic computing.\n", "versions": [{"version": "v1", "created": "Tue, 16 Aug 2016 17:33:44 GMT"}, {"version": "v2", "created": "Sat, 3 Sep 2016 22:01:54 GMT"}, {"version": "v3", "created": "Thu, 9 Feb 2017 01:00:35 GMT"}], "update_date": "2017-03-29", "authors_parsed": [["Segall", "Ken", ""], ["LeGro", "Matthew", ""], ["Kaplan", "Steven", ""], ["Svitelskiy", "Oleksiy", ""], ["Khadka", "Shreeya", ""], ["Crotty", "Patrick", ""], ["Schult", "Daniel", ""]]}, {"id": "1608.04935", "submitter": "Mahmoud Hassan", "authors": "Mahmoud Hassan, Isabelle Merlet, Ahmad Mheich, Aya Kabbara, Arnaud\n  Biraben, Anca Nica and Fabrice Wendling", "title": "Identification of interictal epileptic networks from dense-EEG", "comments": "30 pages, 5 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "q-bio.NC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Epilepsy is a network disease. The epileptic network usually involves\nspatially distributed brain regions. In this context, noninvasive M/EEG source\nconnectivity is an emerging technique to identify functional brain networks at\ncortical level from noninvasive recordings. In this paper, we analyze the\neffect of the two key factors involved in EEG source connectivity processing:\ni) the algorithm used in the solution of the EEG inverse problem and ii) the\nmethod used in the estimation of the functional connectivity. We evaluate four\ninverse solutions algorithms and four connectivity measures on data simulated\nfrom a combined biophysical/physiological model to generate realistic\ninterictal epileptic spikes reflected in scalp EEG. We use a new network-based\nsimilarity index (SI) to compare between the network identified by each of the\ninverse/connectivity combination and the original network generated in the\nmodel. The method will be also applied on real data recorded from one epileptic\npatient who underwent a full presurgical evaluation for drug-resistant focal\nepilepsy. In simulated data, results revealed that the selection of the\ninverse/connectivity combination has a significant impact on the identified\nnetworks. Results suggested that nonlinear methods for measuring the\nconnectivity are more efficient than the linear one. The wMNE inverse solution\nshowed higher performance than dSPM, cMEM and sLORETA. In real data, the\ncombination (wMNE/PLV) led to a very good matching between the interictal\nepileptic network identified from noninvasive EEG recordings and the network\nobtained from connectivity analysis of intracerebral EEG recordings. These\nresults suggest that source connectivity method, when appropriately configured,\nis able to extract highly relevant diagnostic information about networks\ninvolved in interictal epileptic spikes from non-invasive dense-EEG data.\n", "versions": [{"version": "v1", "created": "Wed, 17 Aug 2016 12:07:14 GMT"}], "update_date": "2016-08-18", "authors_parsed": [["Hassan", "Mahmoud", ""], ["Merlet", "Isabelle", ""], ["Mheich", "Ahmad", ""], ["Kabbara", "Aya", ""], ["Biraben", "Arnaud", ""], ["Nica", "Anca", ""], ["Wendling", "Fabrice", ""]]}, {"id": "1608.04972", "submitter": "Pablo A. Alvarado", "authors": "Pablo A. Alvarado, Mauricio A. \\'Alvarez, \\'Alvaro A. Orozco", "title": "A Three Spatial Dimension Wave Latent Force Model for Describing\n  Excitation Sources and Electric Potentials Produced by Deep Brain Stimulation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "q-bio.NC stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Deep brain stimulation (DBS) is a surgical treatment for Parkinson's Disease.\nStatic models based on quasi-static approximation are common approaches for DBS\nmodeling. While this simplification has been validated for bioelectric sources,\nits application to rapid stimulation pulses, which contain more high-frequency\npower, may not be appropriate, as DBS therapeutic results depend on stimulus\nparameters such as frequency and pulse width, which are related to time\nvariations of the electric field. We propose an alternative hybrid approach\nbased on probabilistic models and differential equations, by using Gaussian\nprocesses and wave equation. Our model avoids quasi-static approximation,\nmoreover, it is able to describe dynamic behavior of DBS. Therefore, the\nproposed model may be used to obtain a more realistic phenomenon description.\nThe proposed model can also solve inverse problems, i.e. to recover the\ncorresponding source of excitation, given electric potential distribution. The\nelectric potential produced by a time-varying source was predicted using\nproposed model. For static sources, the electric potential produced by\ndifferent electrode configurations were modeled. Four different sources of\nexcitation were recovered by solving the inverse problem. We compare our\noutcomes with the electric potential obtained by solving Poisson's equation\nusing the Finite Element Method (FEM). Our approach is able to take into\naccount time variations of the source and the produced field. Also, inverse\nproblem can be addressed using the proposed model. The electric potential\ncalculated with the proposed model is close to the potential obtained by\nsolving Poisson's equation using FEM.\n", "versions": [{"version": "v1", "created": "Wed, 17 Aug 2016 14:19:35 GMT"}], "update_date": "2016-08-18", "authors_parsed": [["Alvarado", "Pablo A.", ""], ["\u00c1lvarez", "Mauricio A.", ""], ["Orozco", "\u00c1lvaro A.", ""]]}, {"id": "1608.05501", "submitter": "Hugo Gabriel Eyherabide Dr", "authors": "Hugo Gabriel Eyherabide", "title": "Disambiguating the role of noise correlations when decoding neural\n  populations together", "comments": "To improve readability, this version has more material, more\n  explanations, more figures, less symbols, and more demonstrations than the\n  previous version", "journal-ref": null, "doi": null, "report-no": null, "categories": "q-bio.NC cs.IT math.IT q-bio.QM stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  One of the most controversial problems in neural decoding is quantifying the\ninformation loss caused by ignoring noise correlations during optimal brain\ncomputations. For more than a decade, the measure here called $ \\Delta I^{DL} $\nhas been believed exact. However, we have recently shown that it can exceed the\ninformation loss $ \\Delta I^{B} $ caused by optimal decoders constructed\nignoring noise correlations. Unfortunately, the different information notions\nunderlying $ \\Delta I^{DL} $ and $ \\Delta I^{B} $, and the putative rigorous\ninformation-theoretical derivation of $ \\Delta I^{DL} $, both render unclear\nwhether those findings indicate either flaws in $ \\Delta I^{DL} $ or major\ndepartures from traditional relations between information and decoding. Here we\nresolve this paradox and prove that, under certain conditions, observing $\n\\Delta I^{DL} {>}\\Delta I^{B} $ implies that $ \\Delta I^{DL} $ is flawed.\nMotivated by this analysis, we test both measures using neural populations that\ntransmit independent information. Our results show that $ \\Delta I^{DL} $ may\ndeem noise correlations more important when decoding the populations together\nthan when decoding them in parallel, whereas the opposite may occur for $\n\\Delta I^{B} $. We trace these phenomena back, for $ \\Delta I^{B} $, to the\nchoice of tie-breaking rules, and for $ \\Delta I^{DL} $, to unforeseen\nlimitations within its information-theoretical foundations. Our study\ncontributes with better estimates that potentially improve theoretical and\nexperimental inferences currently drawn from $ \\Delta I^{DL} $ without noticing\nthat it may constitute an upper bound. On the practical side, our results\npromote the design of optimal decoding algorithms and neuroprosthetics without\nrecording noise correlations, thereby saving experimental and computational\nresources.\n", "versions": [{"version": "v1", "created": "Fri, 19 Aug 2016 06:00:22 GMT"}, {"version": "v2", "created": "Fri, 13 Jan 2017 10:22:28 GMT"}], "update_date": "2017-01-16", "authors_parsed": [["Eyherabide", "Hugo Gabriel", ""]]}, {"id": "1608.05665", "submitter": "Danielle Bassett", "authors": "Danielle S. Bassett and Edward T. Bullmore", "title": "Small-World Brain Networks Revisited", "comments": "13 pages, 8 figures in The Neuroscientist, 2016", "journal-ref": null, "doi": null, "report-no": null, "categories": "q-bio.NC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  It is nearly 20 years since the concept of a small-world network was first\nquantitatively defined, by a combination of high clustering and short path\nlength; and about 10 years since this metric of complex network topology began\nto be widely applied to analysis of neuroimaging and other neuroscience data as\npart of the rapid growth of the new field of connectomics. Here we review\nbriefly the foundational concepts of graph theoretical estimation and\ngeneration of small-world networks. We take stock of some of the key\ndevelopments in the field in the past decade and we consider in some detail the\nimplications of recent studies using high-resolution tract-tracing methods to\nmap the anatomical networks of the macaque and the mouse. In doing so, we draw\nattention to the important methodological distinction between topological\nanalysis of binary or unweighted graphs, which have provided a popular but\nsimple approach to brain network analysis in the past, and the topology of\nweighted graphs, which retain more biologically relevant information and are\nmore appropriate to the increasingly sophisticated data on brain connectivity\nemerging from contemporary tract-tracing and other imaging studies. We conclude\nby highlighting some possible future trends in the further development of\nweighted small-worldness as part of a deeper and broader understanding of the\ntopology and the functional value of the strong and weak links between areas of\nmammalian cortex.\n", "versions": [{"version": "v1", "created": "Fri, 19 Aug 2016 16:34:24 GMT"}], "update_date": "2016-08-22", "authors_parsed": [["Bassett", "Danielle S.", ""], ["Bullmore", "Edward T.", ""]]}, {"id": "1608.05706", "submitter": "Joel Zylberberg", "authors": "Joel Zylberberg, Alexandre Pouget, Peter E. Latham, and Eric\n  Shea-Brown", "title": "Robust information propagation through noisy neural circuits", "comments": null, "journal-ref": "PLoS Computational Biology 13: e1005497 (2017)", "doi": "10.1371/journal.pcbi.1005497", "report-no": null, "categories": "q-bio.NC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Sensory neurons give highly variable responses to stimulation, which can\nlimit the amount of stimulus information available to downstream circuits. Much\nwork has investigated the factors that affect the amount of information encoded\nin these population responses, leading to insights about the role of\ncovariability among neurons, tuning curve shape, etc. However, the\ninformativeness of neural responses is not the only relevant feature of\npopulation codes; of potentially equal importance is how robustly that\ninformation propagates to downstream structures. For instance, to quantify the\nretina's performance, one must consider not only the informativeness of the\noptic nerve responses, but also the amount of information that survives the\nspike-generating nonlinearity and noise corruption in the next stage of\nprocessing, the lateral geniculate nucleus. Our study identifies the set of\ncovariance structures for the upstream cells that optimize the ability of\ninformation to propagate through noisy, nonlinear circuits. Within this optimal\nfamily are covariances with \"differential correlations\", which are known to\nreduce the information encoded in neural population activities. Thus,\ncovariance structures that maximize information in neural population codes, and\nthose that maximize the ability of this information to propagate, can be very\ndifferent.\n", "versions": [{"version": "v1", "created": "Fri, 19 Aug 2016 19:52:11 GMT"}, {"version": "v2", "created": "Sun, 26 Feb 2017 15:41:09 GMT"}], "update_date": "2017-04-20", "authors_parsed": [["Zylberberg", "Joel", ""], ["Pouget", "Alexandre", ""], ["Latham", "Peter E.", ""], ["Shea-Brown", "Eric", ""]]}, {"id": "1608.05751", "submitter": "Mark Ioffe", "authors": "Mark L. Ioffe and Michael J. Berry II", "title": "The Structured `Low Temperature' Phase of the Retinal Population Code", "comments": null, "journal-ref": null, "doi": "10.1371/journal.pcbi.1005792", "report-no": null, "categories": "q-bio.NC cond-mat.dis-nn cond-mat.stat-mech", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recent advances in experimental techniques have allowed the simultaneous\nrecording of populations of hundreds of neurons, allowing more comprehensive\ninvestigation into the nature of the collective structure of population neural\nactivity. While recent studies have reported a phase transition in the\nparameter space of maximum entropy models describing the neural probability\ndistributions, the interpretation of these findings has been debated. Here, we\nsuggest that this phase transition may be evidence of a `structured',\ncollective state in the neural population. We show that this phase transition\nis robust to changes in stimulus ensemble and adaptive state. We find that the\npattern of pairwise correlations between neurons has a strength that is well\nwithin the strongly correlated regime and does not require fine tuning,\nsuggesting that this state is generic for populations of 100+ neurons. We find\na clear correspondence between the emergence of a phase transition, and the\nemergence of attractor-like structure in the inferred energy landscape. A\ncollective state in the neural population, in which neural activity patterns\nnaturally form clusters, provides a consistent interpretation for our results.\n", "versions": [{"version": "v1", "created": "Fri, 19 Aug 2016 22:35:45 GMT"}], "update_date": "2018-02-07", "authors_parsed": [["Ioffe", "Mark L.", ""], ["Berry", "Michael J.", "II"]]}, {"id": "1608.05778", "submitter": "Pedro Maia", "authors": "Pedro D. Maia and J. Nathan Kutz", "title": "Reaction time impairments in decision-making networks as a diagnostic\n  marker for traumatic brain injuries and neurodegenerative diseases", "comments": "18 pages, 10 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "q-bio.NC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The presence of diffuse Focal Axonal Swellings (FAS) is a hallmark cellular\nfeature in many neurodegenerative diseases and traumatic brain injury. Among\nother things, the FAS have a significant impact on spike-train encodings that\npropagate through the affected neurons, leading to compromised signal\nprocessing on a neuronal network level. This work merges, for the first time,\nthree fields of study: (i) signal processing in excitatory-inhibitory (EI)\nnetworks of neurons via population codes, (ii) decision-making theory driven by\nthe production of evidence from stimulus, and (iii) compromised spike-train\npropagation through FAS. As such, we demonstrate a mathematical architecture\ncapable of characterizing compromised decision-making driven by cellular\nmechanisms. The computational model also leads to several novel predictions and\ndiagnostics for understanding injury level and cognitive deficits, including a\nkey finding that decision-making reaction times, rather than accuracy, are\nindicative of network level damage. The results have a number of translational\nimplications, including that the level of network damage can be characterized\nby the reaction times in simple cognitive and motor tests.\n", "versions": [{"version": "v1", "created": "Sat, 20 Aug 2016 04:23:27 GMT"}], "update_date": "2016-08-23", "authors_parsed": [["Maia", "Pedro D.", ""], ["Kutz", "J. Nathan", ""]]}, {"id": "1608.06132", "submitter": "J\\\"orn Fischer", "authors": "J. Fischer, P. Manoonpong, S. Lackner", "title": "Reconstructing Neural Parameters and Synapses of arbitrary\n  interconnected Neurons from their Simulated Spiking Activity", "comments": "6 pages, 7 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NE q-bio.NC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  To understand the behavior of a neural circuit it is a presupposition that we\nhave a model of the dynamical system describing this circuit. This model is\ndetermined by several parameters, including not only the synaptic weights, but\nalso the parameters of each neuron. Existing works mainly concentrate on either\nthe synaptic weights or the neural parameters. In this paper we present an\nalgorithm to reconstruct all parameters including the synaptic weights of a\nspiking neuron model. The model based on works of Eugene M. Izhikevich\n(Izhikevich 2007) consists of two differential equations and covers different\ntypes of cortical neurons. It combines the dynamical properties of\nHodgkin-Huxley-type dynamics with a high computational efficiency. The\npresented algorithm uses the recordings of the corresponding membrane\npotentials of the model for the reconstruction and consists of two main\ncomponents. The first component is a rank based Genetic Algorithm (GA) which is\nused to find the neural parameters of the model. The second one is a Least Mean\nSquares approach which computes the synaptic weights of all interconnected\nneurons by minimizing the squared error between the calculated and the measured\nmembrane potentials for each time step. In preparation for the reconstruction\nof the neural parameters and of the synaptic weights from real measured\nmembrane potentials, promising results based on simulated data generated with a\nrandomly parametrized Izhikevich model are presented. The reconstruction does\nnot only converge to a global minimum of neural parameters, but also\napproximates the synaptic weights with high precision.\n", "versions": [{"version": "v1", "created": "Mon, 22 Aug 2016 11:47:18 GMT"}], "update_date": "2016-08-23", "authors_parsed": [["Fischer", "J.", ""], ["Manoonpong", "P.", ""], ["Lackner", "S.", ""]]}, {"id": "1608.06277", "submitter": "Micah Richert", "authors": "Micah Richert, Dimitry Fisher, Filip Piekniewski, Eugene M.\n  Izhikevich, Todd L. Hylton", "title": "Fundamental principles of cortical computation: unsupervised learning\n  with prediction, compression and feedback", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV q-bio.NC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  There has been great progress in understanding of anatomical and functional\nmicrocircuitry of the primate cortex. However, the fundamental principles of\ncortical computation - the principles that allow the visual cortex to bind\nretinal spikes into representations of objects, scenes and scenarios - have so\nfar remained elusive. In an attempt to come closer to understanding the\nfundamental principles of cortical computation, here we present a functional,\nphenomenological model of the primate visual cortex. The core part of the model\ndescribes four hierarchical cortical areas with feedforward, lateral, and\nrecurrent connections. The three main principles implemented in the model are\ninformation compression, unsupervised learning by prediction, and use of\nlateral and top-down context. We show that the model reproduces key aspects of\nthe primate ventral stream of visual processing including Simple and Complex\ncells in V1, increasingly complicated feature encoding, and increased\nseparability of object representations in higher cortical areas. The model\nlearns representations of the visual environment that allow for accurate\nclassification and state-of-the-art visual tracking performance on novel\nobjects.\n", "versions": [{"version": "v1", "created": "Fri, 19 Aug 2016 21:05:35 GMT"}], "update_date": "2016-08-24", "authors_parsed": [["Richert", "Micah", ""], ["Fisher", "Dimitry", ""], ["Piekniewski", "Filip", ""], ["Izhikevich", "Eugene M.", ""], ["Hylton", "Todd L.", ""]]}, {"id": "1608.06315", "submitter": "David Sussillo", "authors": "David Sussillo, Rafal Jozefowicz, L. F. Abbott, Chethan Pandarinath", "title": "LFADS - Latent Factor Analysis via Dynamical Systems", "comments": "16 pages, 11 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG q-bio.NC stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Neuroscience is experiencing a data revolution in which many hundreds or\nthousands of neurons are recorded simultaneously. Currently, there is little\nconsensus on how such data should be analyzed. Here we introduce LFADS (Latent\nFactor Analysis via Dynamical Systems), a method to infer latent dynamics from\nsimultaneously recorded, single-trial, high-dimensional neural spiking data.\nLFADS is a sequential model based on a variational auto-encoder. By making a\ndynamical systems hypothesis regarding the generation of the observed data,\nLFADS reduces observed spiking to a set of low-dimensional temporal factors,\nper-trial initial conditions, and inferred inputs. We compare LFADS to existing\nmethods on synthetic data and show that it significantly out-performs them in\ninferring neural firing rates and latent dynamics.\n", "versions": [{"version": "v1", "created": "Mon, 22 Aug 2016 21:15:00 GMT"}], "update_date": "2016-08-24", "authors_parsed": [["Sussillo", "David", ""], ["Jozefowicz", "Rafal", ""], ["Abbott", "L. F.", ""], ["Pandarinath", "Chethan", ""]]}, {"id": "1608.06546", "submitter": "Yuan Zhao", "authors": "Yuan Zhao and Il Memming Park", "title": "Interpretable Nonlinear Dynamic Modeling of Neural Trajectories", "comments": "Accepted by 29th Conference on Neural Information Processing Systems\n  (NIPS 2016)", "journal-ref": null, "doi": null, "report-no": null, "categories": "q-bio.QM q-bio.NC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A central challenge in neuroscience is understanding how neural system\nimplements computation through its dynamics. We propose a nonlinear time series\nmodel aimed at characterizing interpretable dynamics from neural trajectories.\nOur model assumes low-dimensional continuous dynamics in a finite volume. It\nincorporates a prior assumption about globally contractional dynamics to avoid\noverly enthusiastic extrapolation outside of the support of observed\ntrajectories. We show that our model can recover qualitative features of the\nphase portrait such as attractors, slow points, and bifurcations, while also\nproducing reliable long-term future predictions in a variety of dynamical\nmodels and in real neural data.\n", "versions": [{"version": "v1", "created": "Tue, 23 Aug 2016 15:27:24 GMT"}, {"version": "v2", "created": "Thu, 27 Oct 2016 19:52:19 GMT"}], "update_date": "2016-10-28", "authors_parsed": [["Zhao", "Yuan", ""], ["Park", "Il Memming", ""]]}, {"id": "1608.06548", "submitter": "Joshua Vogelstein", "authors": "Joshua T. Vogelstein, Katrin Amunts, Andreas Andreou, Dora Angelaki,\n  Giorgio Ascoli, Cori Bargmann, Randal Burns, Corrado Cali, Frances Chance,\n  Miyoung Chun, George Church, Hollis Cline, Todd Coleman, Stephanie de La\n  Rochefoucauld, Winfried Denk, Ana Belen Elgoyhen, Ralph Etienne Cummings,\n  Alan Evans, Kenneth Harris, Michael Hausser, Sean Hill, Samuel Inverso, Chad\n  Jackson, Viren Jain, Rob Kass, Bobby Kasthuri, Gregory Kiar, Konrad Kording,\n  Sandhya Koushika, John Krakauer, Story Landis, Jeff Layton, Qingming Luo,\n  Adam Marblestone, David Markowitz, Justin McArthur, Brett Mensh, Michael\n  Milham, Partha Mitra, Pedja Neskovic, Miguel Nicolelis, Richard O'Brien, Aude\n  Oliva, Gergo Orban, Hanchuan Peng, Alyssa Picchini-Schaffer, Marina\n  Picciotto, Jean-Baptiste Poline, Mu-ming Poo, Alex Pouget, Sri Raghavachari,\n  Jane Roskams, Terry Sejnowski, Fritz Sommer, Nelson Spruston, Larry Swanson,\n  Arthur Toga, R. Jacob Vogelstein, Rafael Yuste, Anthony Zador, Richard\n  Huganir, Michael Miller", "title": "Grand Challenges for Global Brain Sciences", "comments": "6 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "q-bio.NC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The next grand challenges for society and science are in the brain sciences.\nA collection of 60+ scientists from around the world, together with 10+\nobservers from national, private, and foundations, spent two days together\ndiscussing the top challenges that we could solve as a global community in the\nnext decade. We eventually settled on three challenges, spanning anatomy,\nphysiology, and medicine. Addressing all three challenges requires novel\ncomputational infrastructure. The group proposed the advent of The\nInternational Brain Station (TIBS), to address these challenges, and launch\nbrain sciences to the next level of understanding.\n", "versions": [{"version": "v1", "created": "Tue, 23 Aug 2016 15:33:12 GMT"}, {"version": "v2", "created": "Tue, 30 Aug 2016 19:36:38 GMT"}, {"version": "v3", "created": "Thu, 27 Oct 2016 15:31:08 GMT"}], "update_date": "2016-10-28", "authors_parsed": [["Vogelstein", "Joshua T.", ""], ["Amunts", "Katrin", ""], ["Andreou", "Andreas", ""], ["Angelaki", "Dora", ""], ["Ascoli", "Giorgio", ""], ["Bargmann", "Cori", ""], ["Burns", "Randal", ""], ["Cali", "Corrado", ""], ["Chance", "Frances", ""], ["Chun", "Miyoung", ""], ["Church", "George", ""], ["Cline", "Hollis", ""], ["Coleman", "Todd", ""], ["de La Rochefoucauld", "Stephanie", ""], ["Denk", "Winfried", ""], ["Elgoyhen", "Ana Belen", ""], ["Cummings", "Ralph Etienne", ""], ["Evans", "Alan", ""], ["Harris", "Kenneth", ""], ["Hausser", "Michael", ""], ["Hill", "Sean", ""], ["Inverso", "Samuel", ""], ["Jackson", "Chad", ""], ["Jain", "Viren", ""], ["Kass", "Rob", ""], ["Kasthuri", "Bobby", ""], ["Kiar", "Gregory", ""], ["Kording", "Konrad", ""], ["Koushika", "Sandhya", ""], ["Krakauer", "John", ""], ["Landis", "Story", ""], ["Layton", "Jeff", ""], ["Luo", "Qingming", ""], ["Marblestone", "Adam", ""], ["Markowitz", "David", ""], ["McArthur", "Justin", ""], ["Mensh", "Brett", ""], ["Milham", "Michael", ""], ["Mitra", "Partha", ""], ["Neskovic", "Pedja", ""], ["Nicolelis", "Miguel", ""], ["O'Brien", "Richard", ""], ["Oliva", "Aude", ""], ["Orban", "Gergo", ""], ["Peng", "Hanchuan", ""], ["Picchini-Schaffer", "Alyssa", ""], ["Picciotto", "Marina", ""], ["Poline", "Jean-Baptiste", ""], ["Poo", "Mu-ming", ""], ["Pouget", "Alex", ""], ["Raghavachari", "Sri", ""], ["Roskams", "Jane", ""], ["Sejnowski", "Terry", ""], ["Sommer", "Fritz", ""], ["Spruston", "Nelson", ""], ["Swanson", "Larry", ""], ["Toga", "Arthur", ""], ["Vogelstein", "R. Jacob", ""], ["Yuste", "Rafael", ""], ["Zador", "Anthony", ""], ["Huganir", "Richard", ""], ["Miller", "Michael", ""]]}, {"id": "1608.06590", "submitter": "Joshua D. Salvi", "authors": "Joshua D. Salvi, D\\'aibhid \\'O Maoil\\'eidigh, A. J. Hudspeth", "title": "Identification of Bifurcations from Observations of Noisy Biological\n  Oscillators", "comments": "6 figures, 1 supporting material", "journal-ref": "Biophysical Journal 111(4):798-812 (2016)", "doi": "10.1016/j.bpj.2016.07.027", "report-no": null, "categories": "physics.bio-ph q-bio.CB q-bio.NC", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Hair bundles are biological oscillators that actively transduce mechanical\nstimuli into electrical signals in the auditory, vestibular, and lateral-line\nsystems of vertebrates. A bundle's function can be explained in part by its\noperation near a particular type of bifurcation, a qualitative change in\nbehavior. By operating near different varieties of bifurcation, the bundle\nresponds best to disparate classes of stimuli. We show how to determine the\nidentity of and proximity to distinct bifurcations despite the presence of\nsubstantial environmental noise.\n", "versions": [{"version": "v1", "created": "Tue, 23 Aug 2016 17:41:47 GMT"}], "update_date": "2016-08-29", "authors_parsed": [["Salvi", "Joshua D.", ""], ["Maoil\u00e9idigh", "D\u00e1ibhid \u00d3", ""], ["Hudspeth", "A. J.", ""]]}, {"id": "1608.06671", "submitter": "Joaquin Torres", "authors": "Ibon Recio and Joaqu\\'in J. Torres", "title": "Emergence of low noise \\emph{frustrated} states in E/I balanced neural\n  networks", "comments": "26 pages, 8 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cond-mat.dis-nn q-bio.NC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study emerging phenomena in binary neural networks where, with a\nprobability c synaptic intensities are chosen according with a Hebbian\nprescription, and with probability (1-c) there is an extra random contribution\nto synaptic weights. This new term, randomly taken from a Gaussian bimodal\ndistribution, balances the synaptic population in the network so that one has\n80-20 relation in E/I population ratio, mimicking the balance observed in\nmammals cortex. For some regions of the relevant parameters, our system depicts\nstandard memory (at low temperature) and non-memory attractors (at high\ntemperature). However, as c decreases and the level of the underlying noise\nalso decreases below a certain temperature T_t, a kind of memory-frustrated\nstate, which resembles spin-glass behavior, sharply emerges. Contrary to what\noccurs in Hopfield-like neural networks, the frustrated state appears here even\nin the limit of the loading parameter alpha-->0. Moreover, we observed that the\nfrustrated state in fact corresponds to two states of non-vanishing activity\nuncorrelated with stored memories, associated, respectively, to a high activity\nor Up state and to a low activity or Down state. Using a linear stability\nanalysis, we found regions in the space of relevant parameters for locally\nstable steady states and demonstrated that frustrated states coexist with\nmemory attractors below T_t. Then, multistability between memory and frustrated\nstates is present for relatively small c, and metastability of memory\nattractors can emerge as c decreases even more. We studied our system using\nstandard mean-field techniques and with Monte Carlo simulations, obtaining a\nperfect agreement between theory and simulations. Our study can be useful to\n...\n", "versions": [{"version": "v1", "created": "Tue, 23 Aug 2016 23:46:13 GMT"}], "update_date": "2016-08-25", "authors_parsed": [["Recio", "Ibon", ""], ["Torres", "Joaqu\u00edn J.", ""]]}, {"id": "1608.07035", "submitter": "Jens Wilting", "authors": "Jens Wilting, Viola Priesemann", "title": "Inferring collective dynamical states from widely unobserved systems", "comments": "7 pages + 12 pages supplementary information + 7 supplementary\n  figures. Title changed to match journal reference", "journal-ref": "Wilting, J. & Priesemann, V. Inferring collective dynamical states\n  from widely unobserved systems. Nature Communications 9, 2325 (2018)", "doi": "10.1038/s41467-018-04725-4", "report-no": null, "categories": "physics.data-an q-bio.NC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  When assessing spatially-extended complex systems, one can rarely sample the\nstates of all components. We show that this spatial subsampling typically leads\nto severe underestimation of the risk of instability in systems with\npropagating events. We derive a subsampling-invariant estimator, and\ndemonstrate that it correctly infers the infectiousness of various diseases\nunder subsampling, making it particularly useful in countries with unreliable\ncase reports. In neuroscience, recordings are strongly limited by subsampling.\nHere, the subsampling-invariant estimator allows to revisit two prominent\nhypotheses about the brain's collective spiking dynamics:\nasynchronous-irregular or critical. We identify consistently for rat, cat and\nmonkey a state that combines features of both and allows input to reverberate\nin the network for hundreds of milliseconds. Overall, owing to its ready\napplicability, the novel estimator paves the way to novel insight for the study\nof spatially-extended dynamical systems.\n", "versions": [{"version": "v1", "created": "Thu, 25 Aug 2016 07:26:13 GMT"}, {"version": "v2", "created": "Fri, 23 Mar 2018 09:59:11 GMT"}, {"version": "v3", "created": "Thu, 5 Jul 2018 13:46:26 GMT"}], "update_date": "2018-07-06", "authors_parsed": [["Wilting", "Jens", ""], ["Priesemann", "Viola", ""]]}, {"id": "1608.07244", "submitter": "Arash Khodadadi", "authors": "Arash Khodadadi, Pegah Fakhari, Jerome R Busemeyer", "title": "A Neuro-Fuzzy Model of Time-Varying Decision Boundaries", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "q-bio.NC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In a recent study, we reported the results of a new decision making paradigm\nin which the participants were asked to balance between their speed and\naccuracy to maximize the total reward they achieve during the experiment. The\nresults of computational modeling provided strong evidence suggesting that the\nparticipants used time-varying decision boundaries. Previous theoretical\nstudies of the optimal speed-accuracy trade-off suggested that the participants\nmay learn to use these time-varying boundaries to maximize their average reward\nrate. The results in our experiment, however, showed that the participants used\nsuch boundaries even at the beginning of the experiment and without any prior\nexperience in the task. In this paper, we hypothesize that these boundaries are\nthe results of using some heuristic rules to make decisions in the task. To\nformulate decision making by these heuristic rules as a computational\nframework, we use the fuzzy logic theory. Based on this theory, we propose a\nnew computational framework for decision making in evidence accumulation tasks.\nIn this framework, there is no explicit decision boundary. Instead, the\nsubject's desire to stop accumulating evidence and responding at each moment\nwithin a trial and for a given value of the accumulated evidence, is determined\nby a set of fuzzy \"IF-TEHN rules\". We then use the back-propagation method to\nderive an algorithm for fitting the fuzzy model to each participant's data. We\nthen investigate how the difference in the participants' performance in the\nexperiment is reflected in the difference in the parameters of the fitted model\n", "versions": [{"version": "v1", "created": "Thu, 25 Aug 2016 18:32:52 GMT"}], "update_date": "2016-08-26", "authors_parsed": [["Khodadadi", "Arash", ""], ["Fakhari", "Pegah", ""], ["Busemeyer", "Jerome R", ""]]}, {"id": "1608.07364", "submitter": "Javier Roulet", "authors": "Javier Roulet and Bernardo Gabriel Mindlin", "title": "Average activity of excitatory and inhibitory neural populations", "comments": "18 pages, 7 figures, accepted for publication in Chaos: An\n  Interdisciplinary Journal of Nonlinear Science", "journal-ref": null, "doi": "10.1063/1.4962326", "report-no": null, "categories": "q-bio.NC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We develop an extension of the Ott-Antonsen method that allows obtaining the\nmean activity (spiking rate) of a population of excitable units. By means of\nthe Ott-Antonsen method, equations for the dynamics of the order parameters of\ncoupled excitatory and inhibitory populations of excitable units are obtained,\nand their mean activities are computed. Two different excitable systems are\nstudied: Adler units and theta neurons. The resulting bifurcation diagrams are\ncompared to those obtained from studying the phenomenological Wilson-Cowan\nmodel in some regions of the parameter space. Compatible behaviors, as well as\nhigher dimensional chaotic solutions, are observed. We study numerical\nsimulations to further validate the equations.\n", "versions": [{"version": "v1", "created": "Fri, 26 Aug 2016 05:05:09 GMT"}], "update_date": "2016-09-21", "authors_parsed": [["Roulet", "Javier", ""], ["Mindlin", "Bernardo Gabriel", ""]]}, {"id": "1608.07499", "submitter": "Ankur Patel", "authors": "Ankur Patel, Grishma joshi, Rupali Ugile", "title": "Stem Cell Therapy for Alzheimer's Disease", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "q-bio.NC q-bio.CB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The loss of neuronal cells in the central nervous system may happen in\nnumerous neurodegenerative illnesses. Alzheimer's Disease (AD) is an intricate,\nirreversible, dynamic neurodegenerative sickness. It is the main source of\nage-related dementia, influencing roughly 5.3 million individuals in the United\nStates alone. Promotion is a typical feeble ailment in individuals more than 65\nyears, bringing on disability described by decrease in memory, failure to learn\nand do every day exercises, intellectual weakness and influences the personal\nsatisfaction of patients. Pathologic qualities of AD are an irregular\ndevelopment of specific proteins called Beta-amyloid \"plaques\" and Tau\n\"Tangles\" in the mind. Notwithstanding, current treatments against AD are just\nto calm manifestations and palliative yet are not the cure and a few promising\nmedications competitors have fizzled in late clinical trials. There is\nconsequently a critical need to enhance our comprehension for pathogenesis of\nthis sickness, making new and creative prescient models with powerful\ntreatments. As of late, stem cell treatment has been appeared to have a\npotential way to deal with different illnesses, including neurodegenerative\ndisorders. In light of the far reaching nature of AD pathology, stem cell\nsubstitution procedures have been seen as an extraordinarily difficult and\nimpossible treatment approach. Stem Cell may likewise offer an effective new\nway to deal with model and concentrate AD. Patient derived induced Pluripotent\nStem Cells (iPSCs), for instance, may propel our comprehension of disease\nmechanism. In this review we will examine the capability of stem cells to help\nin these testing tries.\n", "versions": [{"version": "v1", "created": "Fri, 26 Aug 2016 15:54:07 GMT"}], "update_date": "2016-08-29", "authors_parsed": [["Patel", "Ankur", ""], ["joshi", "Grishma", ""], ["Ugile", "Rupali", ""]]}, {"id": "1608.08040", "submitter": "Tiberiu Tesileanu", "authors": "Tiberiu Tesileanu, Bence \\\"Olveczky, Vijay Balasubramanian", "title": "Rules and mechanisms for efficient two-stage learning in neural circuits", "comments": "27 pages including appendices, 10 figures; to appear in eLife", "journal-ref": null, "doi": "10.1101/071910", "report-no": null, "categories": "q-bio.NC physics.bio-ph", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Trial-and-error learning requires evaluating variable actions and reinforcing\nsuccessful variants. In songbirds, vocal exploration is induced by LMAN, the\noutput of a basal ganglia-circuit that also contributes a corrective bias to\nthe vocal output. This bias is gradually consolidated in RA, a motor cortex\nanalogue downstream of LMAN. We develop a new model of such two-stage learning.\nUsing stochastic gradient descent, we derive how the activity in 'tutor'\ncircuits (e.g., LMAN) should match plasticity mechanisms in 'student' circuits\n(e.g., RA) to achieve efficient learning. We further describe a reinforcement\nlearning framework through which the tutor can build its teaching signal. We\nshow that mismatches between the tutor signal and the plasticity mechanism can\nimpair learning. Applied to birdsong, our results predict the temporal\nstructure of the corrective bias from LMAN given a plasticity rule in RA. Our\nframework can be applied predictively to other paired brain areas showing\ntwo-stage learning.\n", "versions": [{"version": "v1", "created": "Mon, 29 Aug 2016 13:11:58 GMT"}, {"version": "v2", "created": "Wed, 8 Mar 2017 18:14:34 GMT"}], "update_date": "2017-03-09", "authors_parsed": [["Tesileanu", "Tiberiu", ""], ["\u00d6lveczky", "Bence", ""], ["Balasubramanian", "Vijay", ""]]}, {"id": "1608.08267", "submitter": "Peter Diehl Peter U. Diehl", "authors": "Peter U. Diehl and Matthew Cook", "title": "Learning and Inferring Relations in Cortical Networks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NE q-bio.NC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A pressing scientific challenge is to understand how brains work. Of\nparticular interest is the neocortex,the part of the brain that is especially\nlarge in humans, capable of handling a wide variety of tasks including visual,\nauditory, language, motor, and abstract processing. These functionalities are\nprocessed in different self-organized regions of the neocortical sheet, and yet\nthe anatomical structure carrying out the processing is relatively uniform\nacross the sheet. We are at a loss to explain, simulate, or understand such a\nmulti-functional homogeneous sheet-like computational structure - we do not\nhave computational models which work in this way. Here we present an important\nstep towards developing such models: we show how uniform modules of excitatory\nand inhibitory neurons can be connected bidirectionally in a network that, when\nexposed to input in the form of population codes, learns the input encodings as\nwell as the relationships between the inputs. STDP learning rules lead the\nmodules to self-organize into a relational network, which is able to infer\nmissing inputs,restore noisy signals, decide between conflicting inputs, and\ncombine cues to improve estimates. These networks show that it is possible for\na homogeneous network of spiking units to self-organize so as to provide\nmeaningful processing of its inputs. If such networks can be scaled up, they\ncould provide an initial computational model relevant to the large scale\nanatomy of the neocortex.\n", "versions": [{"version": "v1", "created": "Mon, 29 Aug 2016 22:11:01 GMT"}], "update_date": "2016-09-03", "authors_parsed": [["Diehl", "Peter U.", ""], ["Cook", "Matthew", ""]]}, {"id": "1608.08387", "submitter": "Patricia Wollstadt", "authors": "Patricia Wollstadt, Kristin K. Sellers, Lucas Rudelt, Viola\n  Priesemann, Axel Hutt, Flavio Fr\\\"ohlich, Michael Wibral", "title": "Breakdown of local information processing may underlie isoflurane\n  anesthesia effects", "comments": "48 pages, 11 Figures", "journal-ref": null, "doi": "10.1371/journal.pcbi.1005511", "report-no": null, "categories": "q-bio.NC", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  The disruption of coupling between brain areas has been suggested as the\nmechanism underlying loss of consciousness in anesthesia. This hypothesis has\nbeen tested previously by measuring the information transfer between brain\nareas, and by taking reduced information transfer as a proxy for decoupling.\nYet, information transfer is a function of the amount of information available\nin the information source-such that transfer decreases even for unchanged\ncoupling when less source information is available. Therefore, we asked whether\nimpaired local information processing leads to a loss of information transfer.\nAn important prediction of this alternative hypothesis is that changes in\nlocally available information (signal entropy) should be at least as pronounced\nas changes in information transfer. We tested this prediction by recording\nlocal field potentials in two ferrets after administration of isoflurane in\nconcentrations of 0.0 %, 0.5 %, and 1.0 %.\n  We found strong decreases in the source entropy under isoflurane in area V1\nand the prefrontal cortex (PFC)-as predicted by our alternative hypothesis. The\ndecrease in source entropy was stronger in PFC. Information transfer between V1\nand PFC was reduced bidirectionally, but with a stronger decrease from PFC to\nV1. This links the stronger decrease in information transfer to the stronger\ndecrease in source entropy, suggesting reduced source entropy reduces\ninformation transfer. This conclusion fits the observation that the synaptic\ntargets of isoflurane are located in local cortical circuits rather than on the\nsynapses formed by interareal axonal projections. Thus, changes in information\ntransfer under isoflurane seem to be a consequence of changes in local\nprocessing more than of decoupling between brain areas. We suggest that source\nentropy changes must be considered whenever interpreting changes in information\ntransfer as decoupling.\n", "versions": [{"version": "v1", "created": "Tue, 30 Aug 2016 09:38:22 GMT"}, {"version": "v2", "created": "Tue, 10 Jan 2017 15:11:07 GMT"}, {"version": "v3", "created": "Thu, 30 Mar 2017 08:16:52 GMT"}], "update_date": "2017-07-05", "authors_parsed": [["Wollstadt", "Patricia", ""], ["Sellers", "Kristin K.", ""], ["Rudelt", "Lucas", ""], ["Priesemann", "Viola", ""], ["Hutt", "Axel", ""], ["Fr\u00f6hlich", "Flavio", ""], ["Wibral", "Michael", ""]]}, {"id": "1608.08450", "submitter": "Mohit Virmani", "authors": "Mohit Virmani, Nithin Nagaraj", "title": "A Compression-Complexity Measure of Integrated Information", "comments": "28 pages, 10 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IT math.IT q-bio.NC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Quantifying integrated information is a leading approach towards building a\nfundamental theory of consciousness. Integrated Information Theory (IIT) has\ngained attention in this regard due to its theoretically strong framework.\nHowever, it faces some limitations such as current state dependence,\ncomputationally expensive and inability to be applied to real brain data. On\nthe other hand, Perturbational Complexity Index (PCI) is a clinical measure for\ndistinguishing different levels of consciousness. Though PCI claims to capture\nthe functional differentiation and integration in brain networks (similar to\nIIT), its link to integrated information theories is rather weak. Inspired by\nthese two approaches, we propose a new measure - $\\Phi^C$ using a novel\ncompression-complexity perspective that serves as a bridge between the two, for\nthe first time. $\\Phi^C$ is founded on the principles of lossless data\ncompression based complexity measures which characterize the dynamical\ncomplexity of brain networks. $\\Phi^{C}$ exhibits following salient\ninnovations: (i) mathematically well bounded, (ii) negligible current state\ndependence unlike $\\Phi$, (iii) integrated information measured as\ncompression-complexity rather than as an infotheoretic quantity, and (iv)\nfaster to compute since number of atomic bipartitions scales linearly with the\nnumber of nodes of the network, thus avoiding combinatorial explosion. Our\ncomputer simulations show that $\\Phi^C$ has similar hierarchy to $<\\Phi>$ for\nseveral multiple-node networks and it demonstrates a rich interplay between\ndifferentiation, integration and entropy of the nodes of a network. $\\Phi^C$ is\na promising heuristic measure to characterize the quantity of integrated\ninformation (and hence a measure of quantity of consciousness) in larger\nnetworks like human brain and provides an opportunity to test the predictions\nof brain complexity on real neural data.\n", "versions": [{"version": "v1", "created": "Tue, 23 Aug 2016 16:33:04 GMT"}, {"version": "v2", "created": "Tue, 13 Dec 2016 22:25:59 GMT"}], "update_date": "2016-12-15", "authors_parsed": [["Virmani", "Mohit", ""], ["Nagaraj", "Nithin", ""]]}, {"id": "1608.08492", "submitter": "Per B{\\ae}kgaard", "authors": "Per B{\\ae}kgaard, Michael Kai Petersen, Jakob Eg Larsen", "title": "Separating Components of Attention and Surprise", "comments": "13 pages, 10 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC q-bio.NC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Cognitive processes involved in both allocation of attention during decision\nmaking as well as surprise when making mistakes trigger release of the\nneurotransmitter norepinephrine, which has been shown to be correlated with an\nincrease in pupil dilation, in turn reflecting raised levels of arousal.\nExtending earlier experiments based on the Attention Network Test (ANT),\nseparating the neural components of alertness and spatial re-orientation from\nthe attention involved in more demanding conflict resolution tasks, we\ndemonstrate that these signatures of attention are so robust that they may be\nretrieved even when applying low cost eye tracking in an everyday mobile\ncomputing context. Furthermore we find that the reaction of surprise elicited\nwhen committing mistakes in a decision task, which in the neuroimaging EEG\nliterature have been referred to as a negativity feedback error correction\nsignal, may likewise be retrieved solely based on an increase in pupil\ndilation.\n", "versions": [{"version": "v1", "created": "Tue, 30 Aug 2016 15:10:33 GMT"}], "update_date": "2016-09-03", "authors_parsed": [["B\u00e6kgaard", "Per", ""], ["Petersen", "Michael Kai", ""], ["Larsen", "Jakob Eg", ""]]}, {"id": "1608.08793", "submitter": "William Schafer", "authors": "Barry Bentley, Robyn Branicky, Christopher L. Barnes, Edward T.\n  Bullmore, Petra E. V\\'ertes and William R. Schafer", "title": "The multilayer connectome of Caenorhabditis elegans", "comments": null, "journal-ref": null, "doi": "10.1371/journal.pcbi.1005283", "report-no": null, "categories": "q-bio.NC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Connectomics has focused primarily on the mapping of synaptic links in the\nbrain; yet it is well established that extrasynaptic volume transmission,\nespecially via monoamines and neuropeptides, is also critical to brain\nfunction. Here we present a draft monoamine connectome, along with a partial\nneuropeptide connectome, for the nematode C. elegans, based on new and\npublished expression data for biosynthetic genes and receptors. Thus, the\nneuronal connectome can be represented as a multiplex network, with synaptic,\ngap junction, and neuromodulatory layers representing alternative modes of\ninterneuronal interaction and with distinct network structures. In particular,\nthe monoamine network exhibits novel topological properties, with a highly\ndisassortative star-like structure and a rich-club of interconnected\nbroadcasting hubs. Despite the low degree of overlap between layers, we find\nhighly significant modes of interaction, pinpointing network locations and\nmultilink motifs where aminergic and neuropeptide signalling modulate synaptic\nactivity. The multilayer connectome of C. elegans represents a clear exemplar\nof a biological multiplex network and provides a prototype for understanding\nhow extrasynaptic signalling can be integrated into the wired circuitry in\nlarger brains.\n", "versions": [{"version": "v1", "created": "Wed, 31 Aug 2016 10:02:09 GMT"}], "update_date": "2017-02-08", "authors_parsed": [["Bentley", "Barry", ""], ["Branicky", "Robyn", ""], ["Barnes", "Christopher L.", ""], ["Bullmore", "Edward T.", ""], ["V\u00e9rtes", "Petra E.", ""], ["Schafer", "William R.", ""]]}, {"id": "1608.08828", "submitter": "Richard Betzel", "authors": "Richard F. Betzel, Danielle S. Bassett", "title": "Multi-scale brain networks", "comments": "12 pages, 3 figures, review article", "journal-ref": null, "doi": null, "report-no": null, "categories": "q-bio.NC physics.soc-ph", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The network architecture of the human brain has become a feature of\nincreasing interest to the neuroscientific community, largely because of its\npotential to illuminate human cognition, its variation over development and\naging, and its alteration in disease or injury. Traditional tools and\napproaches to study this architecture have largely focused on single scales --\nof topology, time, and space. Expanding beyond this narrow view, we focus this\nreview on pertinent questions and novel methodological advances for the\nmulti-scale brain. We separate our exposition into content related to\nmulti-scale topological structure, multi-scale temporal structure, and\nmulti-scale spatial structure. In each case, we recount empirical evidence for\nsuch structures, survey network-based methodological approaches to reveal these\nstructures, and outline current frontiers and open questions. Although\npredominantly peppered with examples from human neuroimaging, we hope that this\naccount will offer an accessible guide to any neuroscientist aiming to measure,\ncharacterize, and understand the full richness of the brain's multiscale\nnetwork structure -- irrespective of species, imaging modality, or spatial\nresolution.\n", "versions": [{"version": "v1", "created": "Wed, 31 Aug 2016 12:43:05 GMT"}, {"version": "v2", "created": "Fri, 4 Nov 2016 15:33:27 GMT"}], "update_date": "2016-11-07", "authors_parsed": [["Betzel", "Richard F.", ""], ["Bassett", "Danielle S.", ""]]}]