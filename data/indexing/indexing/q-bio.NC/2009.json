[{"id": "2009.00029", "submitter": "Filippo Maria Castelli", "authors": "Filippo Maria Castelli, Matteo Roffilli, Giacomo Mazzamuto, Irene\n  Costantini, Ludovico Silvestri and Francesco Saverio Pavone", "title": "Semantic Segmentation of Neuronal Bodies in Fluorescence Microscopy\n  Using a 2D+3D CNN Training Strategy with Sparsely Annotated Data", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV q-bio.NC", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Semantic segmentation of neuronal structures in 3D high-resolution\nfluorescence microscopy imaging of the human brain cortex can take advantage of\nbidimensional CNNs, which yield good results in neuron localization but lead to\ninaccurate surface reconstruction. 3D CNNs, on the other hand, would require\nmanually annotated volumetric data on a large scale and hence considerable\nhuman effort. Semi-supervised alternative strategies which make use only of\nsparse annotations suffer from longer training times and achieved models tend\nto have increased capacity compared to 2D CNNs, needing more ground truth data\nto attain similar results. To overcome these issues we propose a two-phase\nstrategy for training native 3D CNN models on sparse 2D annotations where\nmissing labels are inferred by a 2D CNN model and combined with manual\nannotations in a weighted manner during loss calculation.\n", "versions": [{"version": "v1", "created": "Mon, 31 Aug 2020 18:01:02 GMT"}, {"version": "v2", "created": "Wed, 2 Sep 2020 00:37:53 GMT"}], "update_date": "2020-09-03", "authors_parsed": [["Castelli", "Filippo Maria", ""], ["Roffilli", "Matteo", ""], ["Mazzamuto", "Giacomo", ""], ["Costantini", "Irene", ""], ["Silvestri", "Ludovico", ""], ["Pavone", "Francesco Saverio", ""]]}, {"id": "2009.00564", "submitter": "Alexander Mathis", "authors": "Alexander Mathis and Steffen Schneider and Jessy Lauer and Mackenzie\n  W. Mathis", "title": "A Primer on Motion Capture with Deep Learning: Principles, Pitfalls and\n  Perspectives", "comments": "Review, 21 pages, 8 figures and 5 boxes", "journal-ref": "Neuron Volume 108, Issue 1, 14 October 2020, Pages 44-65", "doi": "10.1016/j.neuron.2020.09.017", "report-no": null, "categories": "cs.CV cs.LG q-bio.NC q-bio.QM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Extracting behavioral measurements non-invasively from video is stymied by\nthe fact that it is a hard computational problem. Recent advances in deep\nlearning have tremendously advanced predicting posture from videos directly,\nwhich quickly impacted neuroscience and biology more broadly. In this primer we\nreview the budding field of motion capture with deep learning. In particular,\nwe will discuss the principles of those novel algorithms, highlight their\npotential as well as pitfalls for experimentalists, and provide a glimpse into\nthe future.\n", "versions": [{"version": "v1", "created": "Tue, 1 Sep 2020 16:51:33 GMT"}, {"version": "v2", "created": "Wed, 2 Sep 2020 20:29:12 GMT"}], "update_date": "2020-11-13", "authors_parsed": [["Mathis", "Alexander", ""], ["Schneider", "Steffen", ""], ["Lauer", "Jessy", ""], ["Mathis", "Mackenzie W.", ""]]}, {"id": "2009.00756", "submitter": "Madhur Mangalam", "authors": "Madhur Mangalam and Damian G. Kelty-Stephen", "title": "Point estimates, Simpson's paradox and nonergodicity in biological\n  sciences", "comments": "22 pages, 3 figures", "journal-ref": "Neuroscience & Biobehavioral Reviews 125 (2021) 98-107", "doi": "10.1016/j.neubiorev.2021.02.017", "report-no": null, "categories": "q-bio.NC q-bio.QM", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Modern biomedical, behavioral and psychological inference about cause-effect\nrelationships respects an ergodic assumption, that is, that mean response of\nrepresentative samples allow predictions about individual members of those\nsamples. Recent empirical evidence in all of the same fields indicates\nsystematic violations of the ergodic assumption. Indeed, violation of\nergodicity in biomedical, behavioral and psychological causes is precisely the\ninspiration behind our research inquiry. Here, we review the long term costs to\nscientific progress in these domains and a practical way forward. Specifically,\nwe advocate the use of statistical measures that can themselves encode the\ndegree and type of non-ergodicity in measurements. Taking such steps will lead\nto a paradigm shift, allowing researchers to investigate the nonstationary,\nfar-from-equilibrium processes that characterize the creativity and emergence\nof biological and psychological behavior.\n", "versions": [{"version": "v1", "created": "Wed, 2 Sep 2020 00:20:11 GMT"}], "update_date": "2021-05-31", "authors_parsed": [["Mangalam", "Madhur", ""], ["Kelty-Stephen", "Damian G.", ""]]}, {"id": "2009.01083", "submitter": "Alvaro Pastor", "authors": "Alvaro Pastor", "title": "Memory systems of the brain", "comments": "36 pages, 4 figures, draft", "journal-ref": null, "doi": "10.31219/OSF.IO/W6KN9", "report-no": null, "categories": "q-bio.NC", "license": "http://creativecommons.org/licenses/by-sa/4.0/", "abstract": "  Humans have long been fascinated by how memories are formed, how they can be\ndamaged or lost, or still seem vibrant after many years. Thus the search for\nthe locus and organization of memory has had a long history, in which the\nnotion that is is composed of distinct systems developed during the second half\nof the 20th century. A fundamental dichotomy between conscious and unconscious\nmemory processes was first drawn based on evidences from the study of amnesiac\nsubjects and the systematic experimental work with animals. The use of\nbehavioral and neural measures together with imaging techniques have\nprogressively led researchers to agree in the existence of a variety of neural\narchitectures that support multiple memory systems. This article presents a\nhistorical lens with which to contextualize these idea on memory systems, and\nprovides a current account for the multiple memory systems model.\n", "versions": [{"version": "v1", "created": "Tue, 1 Sep 2020 11:59:17 GMT"}], "update_date": "2020-09-03", "authors_parsed": [["Pastor", "Alvaro", ""]]}, {"id": "2009.01269", "submitter": "Ilenna Jones", "authors": "Ilenna Simone Jones, Konrad Paul Kording", "title": "Can Single Neurons Solve MNIST? The Computational Power of Biological\n  Dendritic Trees", "comments": "21 pages, 4 main figures, 1 supplementary figure, 2 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "q-bio.NC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Physiological experiments have highlighted how the dendrites of biological\nneurons can nonlinearly process distributed synaptic inputs. This is in stark\ncontrast to units in artificial neural networks that are generally linear apart\nfrom an output nonlinearity. If dendritic trees can be nonlinear, biological\nneurons may have far more computational power than their artificial\ncounterparts. Here we use a simple model where the dendrite is implemented as a\nsequence of thresholded linear units. We find that such dendrites can readily\nsolve machine learning problems, such as MNIST or CIFAR-10, and that they\nbenefit from having the same input onto several branches of the dendritic tree.\nThis dendrite model is a special case of sparse network. This work suggests\nthat popular neuron models may severely underestimate the computational power\nenabled by the biological fact of nonlinear dendrites and multiple synapses per\npair of neurons. The next generation of artificial neural networks may\nsignificantly benefit from these biologically inspired dendritic architectures.\n", "versions": [{"version": "v1", "created": "Wed, 2 Sep 2020 18:07:39 GMT"}], "update_date": "2020-09-04", "authors_parsed": [["Jones", "Ilenna Simone", ""], ["Kording", "Konrad Paul", ""]]}, {"id": "2009.01445", "submitter": "Ernest Montbrio", "authors": "Ernest Montbri\\'o and Diego Paz\\'o", "title": "Exact mean-field theory explains the dual role of electrical synapses in\n  collective synchronization", "comments": null, "journal-ref": "Phys. Rev. Lett. 125, 248101 (2020)", "doi": "10.1103/PhysRevLett.125.248101", "report-no": null, "categories": "q-bio.NC nlin.AO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Electrical synapses play a major role in setting up neuronal synchronization,\nbut the precise mechanisms whereby these synapses contribute to synchrony are\nsubtle and remain elusive. To investigate these mechanisms mean-field theories\nfor quadratic integrate-and-fire neurons with electrical synapses have been\nrecently put forward. Still, the validity of these theories is controversial\nsince they assume that the neurons produce unrealistic, symmetric spikes,\nignoring the well-known impact of spike shape on synchronization. Here we show\nthat the assumption of symmetric spikes can be relaxed in such theories. The\nresulting mean-field equations reveal a dual role of electrical synapses:\nFirst, they equalize membrane potentials favoring the emergence of synchrony.\nSecond, electrical synapses act as \"virtual chemical synapses\", which can be\neither excitatory or inhibitory depending upon the spike shape. Our results\noffer a precise mathematical explanation of the intricate effect of electrical\nsynapses in collective synchronization. This reconciles previous theoretical\nand numerical works, and confirms the suitability of recent low-dimensional\nmean-field theories to investigate electrically coupled neuronal networks.\n", "versions": [{"version": "v1", "created": "Thu, 3 Sep 2020 04:27:19 GMT"}], "update_date": "2020-12-15", "authors_parsed": [["Montbri\u00f3", "Ernest", ""], ["Paz\u00f3", "Diego", ""]]}, {"id": "2009.01661", "submitter": "Jordan Smith", "authors": "Jordan Smith, Hadi Zadeh Haghighi, Dennis Salahub, and Christoph Simon", "title": "Radical pairs may play a role in xenon-induced general anesthesia", "comments": "17 pages, 10 figures", "journal-ref": "Sci Rep 11, 6287 (2021)", "doi": "10.1038/s41598-021-85673-w", "report-no": null, "categories": "physics.bio-ph q-bio.NC quant-ph", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Understanding the mechanisms underlying general anesthesia would be a key\nstep towards understanding consciousness. The process of xenon-induced general\nanesthesia has been shown to involve electron transfer, and the potency of\nxenon as a general anesthetic exhibits isotopic dependence. We propose that\nthese observations can be explained by a mechanism in which the xenon nuclear\nspin influences the recombination dynamics of a naturally occurring radical\npair of electrons. We develop a simple model inspired by the body of work on\nthe radical-pair mechanism in cryptochrome in the context of avian\nmagnetoreception, and we show that our model can reproduce the observed\nisotopic dependence of the general anesthetic potency of xenon in mice. Our\nresults are consistent with the idea that radical pairs of electrons with\nentangled spins could be important for consciousness.\n", "versions": [{"version": "v1", "created": "Tue, 1 Sep 2020 17:03:31 GMT"}, {"version": "v2", "created": "Thu, 18 Mar 2021 20:44:22 GMT"}], "update_date": "2021-03-22", "authors_parsed": [["Smith", "Jordan", ""], ["Haghighi", "Hadi Zadeh", ""], ["Salahub", "Dennis", ""], ["Simon", "Christoph", ""]]}, {"id": "2009.02038", "submitter": "Alessandro Sarracino", "authors": "A. Sarracino, O. Arviv, O. Shriki, and L. de Arcangelis", "title": "Predicting brain evoked response to external stimuli from temporal\n  correlations of spontaneous activity", "comments": "9 pages, 8 figures", "journal-ref": "Physical Review Research 2, 033355 (2020)", "doi": "10.1103/PhysRevResearch.2.033355", "report-no": null, "categories": "q-bio.NC cond-mat.dis-nn cond-mat.stat-mech", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The relation between spontaneous and stimulated global brain activity is a\nfundamental problem in the understanding of brain functions. This question is\ninvestigated both theoretically and experimentally within the context of\nnonequilibrium fluctuation-dissipation relations. We consider the stochastic\ncoarse-grained Wilson-Cowan model in the linear noise approximation and compare\nanalytical results to experimental data from magnetoencephalography (MEG) of\nhuman brain. The short time behavior of the autocorrelation function for\nspontaneous activity is characterized by a double-exponential decay, with two\ncharacteristic times, differing by two orders of magnitude. Conversely, the\nresponse function exhibits a single exponential decay in agreement with\nexperimental data for evoked activity under visual stimulation. Results suggest\nthat the brain response to weak external stimuli can be predicted from the\nobservation of spontaneous activity and pave the way to controlled experiments\non the brain response under different external perturbations.\n", "versions": [{"version": "v1", "created": "Fri, 4 Sep 2020 07:42:06 GMT"}], "update_date": "2020-09-07", "authors_parsed": [["Sarracino", "A.", ""], ["Arviv", "O.", ""], ["Shriki", "O.", ""], ["de Arcangelis", "L.", ""]]}, {"id": "2009.02081", "submitter": "Selma Souihel", "authors": "Selma Souihel (BIOVISION), Bruno Cessac (BIOVISION)", "title": "On the potential role of lateral connectivity in retinal anticipation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "q-bio.QM q-bio.NC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We analyse the potential effects of lateral connectivity (amacrine cells and\ngap junctions) on motion anticipation in the retina. Our main result is that\nlateral connectivity can-under conditions analysed in the paper-trigger a wave\nof activity enhancing the anticipation mechanism provided by local gain control\n[8, 17]. We illustrate these predictions by two examples studied in the\nexperimental literature: differential motion sensitive cells [1] and direction\nsensitive cells where direction sensitivity is inherited from asymmetry in gap\njunctions connectivity [73]. We finally present reconstructions of retinal\nresponses to 2D visual inputs to assess the ability of our model to anticipate\nmotion in the case of three different 2D stimuli.\n", "versions": [{"version": "v1", "created": "Fri, 4 Sep 2020 09:18:41 GMT"}], "update_date": "2020-09-07", "authors_parsed": [["Souihel", "Selma", "", "BIOVISION"], ["Cessac", "Bruno", "", "BIOVISION"]]}, {"id": "2009.02310", "submitter": "Jack Cook", "authors": "Jack A. Cook", "title": "A Differential Topological Model for Olfactory Learning and\n  Representation", "comments": "167 pages, 4 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "q-bio.NC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This thesis is designed to be a self-contained exposition of the\nneurobiological and mathematical aspects of sensory perception, memory, and\nlearning with a bias towards olfaction. The final chapters introduce a new\napproach to modeling focusing more on the geometry of the system as opposed to\nelement wise dynamics. Additionally, we construct an organism independent model\nfor olfactory processing: something which is currently missing from the\nliterature.\n", "versions": [{"version": "v1", "created": "Fri, 4 Sep 2020 17:20:14 GMT"}], "update_date": "2020-09-07", "authors_parsed": [["Cook", "Jack A.", ""]]}, {"id": "2009.02707", "submitter": "Bryan M. Li", "authors": "Bryan M. Li, Theoklitos Amvrosiadis, Nathalie Rochefort, Arno Onken", "title": "CalciumGAN: A Generative Adversarial Network Model for Synthesising\n  Realistic Calcium Imaging Data of Neuronal Populations", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "q-bio.NC cs.AI cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Calcium imaging has become a powerful and popular technique to monitor the\nactivity of large populations of neurons in vivo. However, for ethical\nconsiderations and despite recent technical developments, recordings are still\nconstrained to a limited number of trials and animals. This limits the amount\nof data available from individual experiments and hinders the development of\nanalysis techniques and models for more realistic size of neuronal populations.\nThe ability to artificially synthesize realistic neuronal calcium signals could\ngreatly alleviate this problem by scaling up the number of trials. Here we\npropose a Generative Adversarial Network (GAN) model to generate realistic\ncalcium signals as seen in neuronal somata with calcium imaging. To this end,\nwe adapt the WaveGAN architecture and train it with the Wasserstein distance.\nWe test the model on artificial data with known ground-truth and show that the\ndistribution of the generated signals closely resembles the underlying data\ndistribution. Then, we train the model on real calcium signals recorded from\nthe primary visual cortex of behaving mice and confirm that the deconvolved\nspike trains match the statistics of the recorded data. Together, these results\ndemonstrate that our model can successfully generate realistic calcium imaging\ndata, thereby providing the means to augment existing datasets of neuronal\nactivity for enhanced data exploration and modeling.\n", "versions": [{"version": "v1", "created": "Sun, 6 Sep 2020 10:58:11 GMT"}, {"version": "v2", "created": "Tue, 8 Sep 2020 03:58:43 GMT"}], "update_date": "2020-09-09", "authors_parsed": [["Li", "Bryan M.", ""], ["Amvrosiadis", "Theoklitos", ""], ["Rochefort", "Nathalie", ""], ["Onken", "Arno", ""]]}, {"id": "2009.02816", "submitter": "Jae Moon", "authors": "Jae Moon, Silvia Orlandi, Tom Chau", "title": "A comparison of oscillatory characteristics in covert speech and speech\n  perception", "comments": "22 pages, 10 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "q-bio.NC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Covert speech, the silent production of words in the mind, has been studied\nincreasingly to understand and decode thoughts. This task has often been\ncompared to speech perception as it brings about similar topographical\nactivation patterns in common brain areas. In studies of speech comprehension,\nneural oscillations are thought to play a key role in the sampling of speech at\nvarying temporal scales. However, very little is known about the role of\noscillations in covert speech. In this study, we aimed to determine to what\nextent each oscillatory frequency band is used to process words in covert\nspeech and speech perception tasks. Secondly, we asked whether the {\\theta} and\n{\\gamma} activity in the two tasks are related through phase-amplitude coupling\n(PAC). First, continuous wavelet transform was performed on epoched signals and\nsubsequently two-tailed t-tests between two classes were conducted to determine\nstatistical distinctions in frequency and time. While the perception task\ndynamically uses all frequencies with more prominent {\\theta} and {\\gamma}\nactivity, the covert task favoured higher frequencies with significantly higher\n{\\gamma} activity than perception. Moreover, the perception condition produced\nsignificant {\\theta}-{\\gamma} PAC suggesting a linkage of syllabic and\nphonological sampling. Although this was found to be suppressed in the covert\ncondition, we found significant pseudo-coupling between perception {\\theta} and\ncovert speech {\\gamma}. We report that covert speech processing is largely\nconducted by higher frequencies, and that the {\\gamma}- and {\\theta}-bands may\nfunction similarly and differently across tasks, respectively. This study is\nthe first to characterize covert speech in terms of neural oscillatory\nengagement. Future studies are directed to explore oscillatory characteristics\nand inter-task relationships with a more diverse vocabulary.\n", "versions": [{"version": "v1", "created": "Sun, 6 Sep 2020 21:00:47 GMT"}], "update_date": "2020-09-08", "authors_parsed": [["Moon", "Jae", ""], ["Orlandi", "Silvia", ""], ["Chau", "Tom", ""]]}, {"id": "2009.03187", "submitter": "Keith Dillon", "authors": "Keith Dillon", "title": "The Resolution Matrix for Visualizing Functional Network Connectivity", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "q-bio.NC cs.SI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The resolution matrix is a mathematical tool for analyzing inverse problems\nsuch as computational imaging systems. When treating network connectivity\nestimation as an inverse problem, the resolution matrix describes the degree to\nwhich network nodes and edges can be resolved. This is useful both for\nquantifying robustness of the network estimate, as well as identifying\ncorrelated activity. In this report we analyze the resolution matrix for\nfunctional MRI data from the Human Connectome project. We find that common\nmetrics of the resolution metric can be used to identify networked activity,\nthough with a new twist on the relationship between default mode network and\nthe frontoparietal attention network.\n", "versions": [{"version": "v1", "created": "Fri, 4 Sep 2020 13:10:19 GMT"}], "update_date": "2020-09-08", "authors_parsed": [["Dillon", "Keith", ""]]}, {"id": "2009.03238", "submitter": "Niharika Shimona D'Souza", "authors": "Niharika Shimona D'Souza, Mary Beth Nebel, Nicholas Wymbs, Stewart H.\n  Mostofsky, Archana Venkataraman", "title": "A Joint Network Optimization Framework to Predict Clinical Severity from\n  Resting State Functional MRI Data", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "q-bio.NC cs.LG eess.SP stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a novel optimization framework to predict clinical severity from\nresting state fMRI (rs-fMRI) data. Our model consists of two coupled terms. The\nfirst term decomposes the correlation matrices into a sparse set of\nrepresentative subnetworks that define a network manifold. These subnetworks\nare modeled as rank-one outer-products which correspond to the elemental\npatterns of co-activation across the brain; the subnetworks are combined via\npatient-specific non-negative coefficients. The second term is a linear\nregression model that uses the patient-specific coefficients to predict a\nmeasure of clinical severity. We validate our framework on two separate\ndatasets in a ten fold cross validation setting. The first is a cohort of\nfifty-eight patients diagnosed with Autism Spectrum Disorder (ASD). The second\ndataset consists of sixty three patients from a publicly available ASD\ndatabase. Our method outperforms standard semi-supervised frameworks, which\nemploy conventional graph theoretic and statistical representation learning\ntechniques to relate the rs-fMRI correlations to behavior. In contrast, our\njoint network optimization framework exploits the structure of the rs-fMRI\ncorrelation matrices to simultaneously capture group level effects and patient\nheterogeneity. Finally, we demonstrate that our proposed framework robustly\nidentifies clinically relevant networks characteristic of ASD.\n", "versions": [{"version": "v1", "created": "Thu, 27 Aug 2020 23:43:25 GMT"}], "update_date": "2020-09-08", "authors_parsed": [["D'Souza", "Niharika Shimona", ""], ["Nebel", "Mary Beth", ""], ["Wymbs", "Nicholas", ""], ["Mostofsky", "Stewart H.", ""], ["Venkataraman", "Archana", ""]]}, {"id": "2009.03272", "submitter": "Vivek Subramanian", "authors": "Vivek Subramanian, Joshua Khani", "title": "Graph Convolutional Networks Reveal Neural Connections Encoding\n  Prosthetic Sensation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "q-bio.NC cs.LG q-bio.QM stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Extracting stimulus features from neuronal ensembles is of great interest to\nthe development of neuroprosthetics that project sensory information directly\nto the brain via electrical stimulation. Machine learning strategies that\noptimize stimulation parameters as the subject learns to interpret the\nartificial input could improve device efficacy, increase prosthetic\nperformance, ensure stability of evoked sensations, and improve power\nconsumption by eliminating extraneous input. Recent advances extending deep\nlearning techniques to non-Euclidean graph data provide a novel approach to\ninterpreting neuronal spiking activity. For this study, we apply graph\nconvolutional networks (GCNs) to infer the underlying functional relationship\nbetween neurons that are involved in the processing of artificial sensory\ninformation. Data was collected from a freely behaving rat using a four\ninfrared (IR) sensor, ICMS-based neuroprosthesis to localize IR light sources.\nWe use GCNs to predict the stimulation frequency across four stimulating\nchannels in the prosthesis, which encode relative distance and directional\ninformation to an IR-emitting reward port. Our GCN model is able to achieve a\npeak performance of 73.5% on a modified ordinal regression performance metric\nin a multiclass classification problem consisting of 7 classes, where chance is\n14.3%. Additionally, the inferred adjacency matrix provides a adequate\nrepresentation of the underlying neural circuitry encoding the artificial\nsensation.\n", "versions": [{"version": "v1", "created": "Sun, 23 Aug 2020 01:43:46 GMT"}], "update_date": "2020-09-08", "authors_parsed": [["Subramanian", "Vivek", ""], ["Khani", "Joshua", ""]]}, {"id": "2009.03857", "submitter": "William Podlaski", "authors": "Michele Nardin, James W Phillips, William F Podlaski, Sander W Keemink", "title": "Nonlinear computations in spiking neural networks through multiplicative\n  synapses", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "q-bio.NC cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The brain efficiently performs nonlinear computations through its intricate\nnetworks of spiking neurons, but how this is done remains elusive. While\nrecurrent spiking networks implementing linear computations can be directly\nderived and easily understood (e.g., in the spike coding network framework),\nthe connectivity required for nonlinear computations can be harder to\ninterpret, as they require additional non-linearities (e.g., dendritic or\nsynaptic) weighted through supervised training. Here we extend the spike coding\nframework to implement any polynomial dynamical system. This results in\nnetworks requiring multiplicative synapses, which we term the multiplicative\nspike coding network (mSCN). We demonstrate how the required connectivity for\nseveral nonlinear dynamical systems can be directly implemented in mSCNs,\nwithout training. We also show how to carry out higher-order polynomials with\ncoupled networks that use only pair-wise multiplicative synapses, and provide\nexpected numbers of connections for each synapse type. Overall, our work\nprovides an alternative method for implementing nonlinear computations in\nspiking neural networks, while keeping all the attractive features of standard\nSCNs (such as robustness, irregular and sparse firing, and interpretable\nconnectivity). Finally, we discuss the biological plausibility of our approach,\nand how the high accuracy and robustness of the approach may be of interest for\nneuromorphic computing.\n", "versions": [{"version": "v1", "created": "Tue, 8 Sep 2020 16:47:27 GMT"}, {"version": "v2", "created": "Wed, 31 Mar 2021 16:36:28 GMT"}], "update_date": "2021-04-01", "authors_parsed": [["Nardin", "Michele", ""], ["Phillips", "James W", ""], ["Podlaski", "William F", ""], ["Keemink", "Sander W", ""]]}, {"id": "2009.04006", "submitter": "Kai Ueltzh\\\"offer", "authors": "Kai Ueltzh\\\"offer", "title": "On the thermodynamics of prediction under dissipative adaptation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "q-bio.NC cond-mat.stat-mech", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  On the one hand, the dissipated heat of a thermodynamic work extraction\nprocess upper bounds the non-predictive information, which the associated\nsystem encodes about its environment. Thus, emergent information processing\ncapabilities can be understood from the perspective of a pressure towards high\nthermodynamic efficiency. On the other hand, the second law of thermodynamics\nplays a crucial role in the emergence of complex, self-organising dissipative\nstructures. Such structures are thermodynamically favoured, because they can\ndissipate free energy reservoirs, which would not be accessible otherwise.\nThereby, they allow a closed system to move from one meta-stable state to\nanother meta-stable state of higher entropy. This paper will argue, that these\ntwo views are not contradictory, but that their combination allows to\nunderstand the transition from simple self-organising dissipative structures to\ncomplex information processing systems. If the efficiency required by a\ndissipative structure to harvest enough work from the channeled flow of free\nenergy to maintain its own structure is high, there is a drive for this system\nto be predictive. Still, the existence of this dissipative system is\nthermodynamically favoured, compared to a situation without any dissipative\nstructure. Due to the emergence of a hierarchy of dissipative systems, which by\nthemselves are non-equilibrium structures that can be dissipated, such a drive\ndevelops naturally, as one ascends in this hierarchy further and further away\nfrom the initial driving disequilibrium.\n", "versions": [{"version": "v1", "created": "Tue, 8 Sep 2020 21:55:16 GMT"}], "update_date": "2020-09-10", "authors_parsed": [["Ueltzh\u00f6ffer", "Kai", ""]]}, {"id": "2009.04027", "submitter": "Sophia David", "authors": "Sophia U. David (1), Sophie E. Loman (1), Christopher W. Lynn (1 and\n  2), Ann S. Blevins (1), Danielle S. Bassett (1-6) ((1) Department of\n  Bioengineering, School of Engineering & Applied Science, University of\n  Pennsylvania, Philadelphia, USA, (2) Department of Physics & Astronomy,\n  College of Arts & Sciences University of Pennsylvania, Philadelphia, USA, (3)\n  Department of Electrical & Systems Engineering, University of Pennsylvania,\n  Philadelphia, USA, (4) Department of Neurology, Perelman School of Medicine,\n  University of Pennsylvania, Philadelphia, USA, (5) Department of Psychiatry,\n  Perelman School of Medicine, University of Pennsylvania, Philadelphia, USA,\n  (6) Santa Fe Institute, Santa Fe, USA)", "title": "How We Learn About our Networked World", "comments": "11 pages, 3 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "q-bio.NC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  When presented with information of any type, from music to language to\nmathematics, the human mind subconsciously arranges it into a network. A\nnetwork puts pieces of information like musical notes, syllables or\nmathematical concepts into context by linking them together. These networks\nhelp our minds organize information and anticipate what is coming. Here we\npresent two questions about network building. 1) Can humans more easily learn\nsome types of networks than others? 2) Do humans find some links between ideas\nmore surprising than others? The answer to both questions is \"Yes,\" and we\nexplain why. The findings provide much-needed insight into the ways that humans\nlearn about the networked world around them. Moreover, the study paves the way\nfor future efforts seeking to optimize how information is presented to\naccelerate human learning.\n", "versions": [{"version": "v1", "created": "Tue, 8 Sep 2020 23:17:54 GMT"}], "update_date": "2020-09-10", "authors_parsed": [["David", "Sophia U.", "", "1 and\n  2"], ["Loman", "Sophie E.", "", "1 and\n  2"], ["Lynn", "Christopher W.", "", "1 and\n  2"], ["Blevins", "Ann S.", "", "1-6"], ["Bassett", "Danielle S.", "", "1-6"]]}, {"id": "2009.04216", "submitter": "Benjamin Ambrosio", "authors": "B. Ambrosio", "title": "Beyond the brain: towards a mathematical modeling of emotions", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "q-bio.NC math.OC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Emotions are a central key for understanding human beings and of fundamental\nimportance regarding their impact in human and animal behaviors. They have been\nfor a long time a subject of study for various scholars including in particular\nphilosophers and mystics. In modern science, the emotional phenomenon has\nattracted for a few decades an increasing number of studies, notably in the\nfields of Psychology, Psychiatry, Neuroscience and Biochemistry. However, since\nour perception of emotions is not, so far, directly detectable nor recordable\nby our measure instruments, Physics and Mathematics have not been so far used\nacademically to provide a precise description of the phenomenon of feeling an\nemotion. Relying upon the works of O. Elahi and on the hypothesis that the\nhuman soul and its psyche may manifest in ourselves (in both conscious and\nunconscious manner) in an analog way as electromagnetic waves, we propose here\na few mathematical descriptions consistent with the human personal experience,\nof the feeling and cognition of emotions. As far as we know, such a\nmathematical description has never been provided before. It allows a\nquantitative (intensity) and qualitative (nature of feelings/frequency) of the\nemotional phenomenon which provides a novel scientific approach of the nature\nof the mind, complementary to the on going research of physiological\nmanifestation of emotions. We anticipate such an approach and the associated\nmathematical modeling to become an important tool to describe emotions and\ntheir subsequent behavior. In complement of the modeling of oscillations and\nbrain dynamics, it provides a fruitful direction of research with potentially\nbroad and deep impacts in both applied mathematics, physics, cognitive and\nbehavioral sciences.\n", "versions": [{"version": "v1", "created": "Mon, 7 Sep 2020 21:56:08 GMT"}, {"version": "v2", "created": "Mon, 14 Sep 2020 02:25:58 GMT"}], "update_date": "2020-09-15", "authors_parsed": [["Ambrosio", "B.", ""]]}, {"id": "2009.04518", "submitter": "Kristine Heiney", "authors": "Kristine Heiney, Gunnar Tufte, Stefano Nichele", "title": "On Artificial Life and Emergent Computation in Physical Substrates", "comments": "Accepted conference paper. HPCS2020, Session 3: BICAS", "journal-ref": null, "doi": null, "report-no": null, "categories": "q-bio.NC cs.AI", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  In living systems, we often see the emergence of the ingredients necessary\nfor computation -- the capacity for information transmission, storage, and\nmodification -- begging the question of how we may exploit or imitate such\nbiological systems in unconventional computing applications. What can we gain\nfrom artificial life in the advancement of computing technology? Artificial\nlife provides us with powerful tools for understanding the dynamic behavior of\nbiological systems and capturing this behavior in manmade substrates. With this\napproach, we can move towards a new computing paradigm concerned with\nharnessing emergent computation in physical substrates not governed by the\nconstraints of Moore's law and ultimately realize massively parallel and\ndistributed computing technology. In this paper, we argue that the lens of\nartificial life offers valuable perspectives for the advancement of\nhigh-performance computing technology. We first present a brief foundational\nbackground on artificial life and some relevant tools that may be applicable to\nunconventional computing. Two specific substrates are then discussed in detail:\nbiological neurons and ensembles of nanomagnets. These substrates are the focus\nof the authors' ongoing work, and they are illustrative of the two sides of the\napproach outlined here -- the close study of living systems and the\nconstruction of artificial systems to produce life-like behaviors. We conclude\nwith a philosophical discussion on what we can learn from approaching\ncomputation with the curiosity inherent to the study of artificial life. The\nmain contribution of this paper is to present the great potential of using\nartificial life methodologies to uncover and harness the inherent computational\npower of physical substrates toward applications in unconventional\nhigh-performance computing.\n", "versions": [{"version": "v1", "created": "Wed, 9 Sep 2020 18:59:53 GMT"}], "update_date": "2020-09-11", "authors_parsed": [["Heiney", "Kristine", ""], ["Tufte", "Gunnar", ""], ["Nichele", "Stefano", ""]]}, {"id": "2009.04765", "submitter": "Damian Pascual", "authors": "Nicolas Affolter, Beni Egressy, Damian Pascual, Roger Wattenhofer", "title": "Brain2Word: Decoding Brain Activity for Language Generation", "comments": "Preprint. Work in progress", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.LG q-bio.NC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Brain decoding, understood as the process of mapping brain activities to the\nstimuli that generated them, has been an active research area in the last\nyears. In the case of language stimuli, recent studies have shown that it is\npossible to decode fMRI scans into an embedding of the word a subject is\nreading. However, such word embeddings are designed for natural language\nprocessing tasks rather than for brain decoding. Therefore, they limit our\nability to recover the precise stimulus. In this work, we propose to directly\nclassify an fMRI scan, mapping it to the corresponding word within a fixed\nvocabulary. Unlike existing work, we evaluate on scans from previously unseen\nsubjects. We argue that this is a more realistic setup and we present a model\nthat can decode fMRI data from unseen subjects. Our model achieves 5.22% Top-1\nand 13.59% Top-5 accuracy in this challenging task, significantly outperforming\nall the considered competitive baselines. Furthermore, we use the decoded words\nto guide language generation with the GPT-2 model. This way, we advance the\nquest for a system that translates brain activities into coherent text.\n", "versions": [{"version": "v1", "created": "Thu, 10 Sep 2020 10:47:36 GMT"}, {"version": "v2", "created": "Tue, 13 Oct 2020 08:05:08 GMT"}, {"version": "v3", "created": "Wed, 11 Nov 2020 08:07:08 GMT"}], "update_date": "2020-11-12", "authors_parsed": [["Affolter", "Nicolas", ""], ["Egressy", "Beni", ""], ["Pascual", "Damian", ""], ["Wattenhofer", "Roger", ""]]}, {"id": "2009.04854", "submitter": "Jingwen Zhang", "authors": "Jingwen Zhang, Defu Yang, Wei He, Guorong Wu, Minghan Chen", "title": "A Network-Guided Reaction-Diffusion Model of AT[N] Biomarkers in\n  Alzheimer's Disease", "comments": "11 pages, 8 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "q-bio.NC q-bio.MN", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Currently, many studies of Alzheimer's disease (AD) are investigating the\nneurobiological factors behind the acquisition of beta-amyloid (A), pathologic\ntau (T), and neurodegeneration ([N]) biomarkers from neuroimages. However, a\nsystem-level mechanism of how these neuropathological burdens promote\nneurodegeneration and why AD exhibits characteristic progression is largely\nelusive. In this study, we combined the power of systems biology and network\nneuroscience to understand the dynamic interaction and diffusion process of\nAT[N] biomarkers from an unprecedented amount of longitudinal Amyloid PET scan,\nMRI imaging, and DTI data. Specifically, we developed a network-guided\nbiochemical model to jointly (1) model the interaction of AT[N] biomarkers at\neach brain region and (2) characterize their propagation pattern across the\nfiber pathways in the structural brain network, where the brain resilience is\nalso considered as a moderator of cognitive decline. Our biochemical model\noffers a greater mathematical insight to understand the physiopathological\nmechanism of AD progression by studying the system dynamics and stability.\nThus, an in-depth system-level analysis allows us to gain a new understanding\nof how AT[N] biomarkers spread throughout the brain, capture the early sign of\ncognitive decline, and predict the AD progression from the preclinical stage.\n", "versions": [{"version": "v1", "created": "Thu, 10 Sep 2020 13:39:56 GMT"}, {"version": "v2", "created": "Sun, 20 Sep 2020 13:34:03 GMT"}, {"version": "v3", "created": "Fri, 2 Oct 2020 06:56:01 GMT"}], "update_date": "2020-10-05", "authors_parsed": [["Zhang", "Jingwen", ""], ["Yang", "Defu", ""], ["He", "Wei", ""], ["Wu", "Guorong", ""], ["Chen", "Minghan", ""]]}, {"id": "2009.05359", "submitter": "Beren Millidge Mr", "authors": "Beren Millidge, Alexander Tschantz, Anil K Seth, Christopher L Buckley", "title": "Activation Relaxation: A Local Dynamical Approximation to\n  Backpropagation in the Brain", "comments": "initial upload; revised version (updated abstract, related work)\n  28-09-20; 05/10/20: revised for ICLR submission; 10/10/20: minor revisions", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NE cs.AI cs.LG q-bio.NC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The backpropagation of error algorithm (backprop) has been instrumental in\nthe recent success of deep learning. However, a key question remains as to\nwhether backprop can be formulated in a manner suitable for implementation in\nneural circuitry. The primary challenge is to ensure that any candidate\nformulation uses only local information, rather than relying on global signals\nas in standard backprop. Recently several algorithms for approximating backprop\nusing only local signals have been proposed. However, these algorithms\ntypically impose other requirements which challenge biological plausibility:\nfor example, requiring complex and precise connectivity schemes, or multiple\nsequential backwards phases with information being stored across phases. Here,\nwe propose a novel algorithm, Activation Relaxation (AR), which is motivated by\nconstructing the backpropagation gradient as the equilibrium point of a\ndynamical system. Our algorithm converges rapidly and robustly to the correct\nbackpropagation gradients, requires only a single type of computational unit,\nutilises only a single parallel backwards relaxation phase, and can operate on\narbitrary computation graphs. We illustrate these properties by training deep\nneural networks on visual classification tasks, and describe simplifications to\nthe algorithm which remove further obstacles to neurobiological implementation\n(for example, the weight-transport problem, and the use of nonlinear\nderivatives), while preserving performance.\n", "versions": [{"version": "v1", "created": "Fri, 11 Sep 2020 11:56:34 GMT"}, {"version": "v2", "created": "Tue, 15 Sep 2020 10:37:05 GMT"}, {"version": "v3", "created": "Mon, 28 Sep 2020 21:19:22 GMT"}, {"version": "v4", "created": "Mon, 5 Oct 2020 18:21:01 GMT"}, {"version": "v5", "created": "Sat, 10 Oct 2020 14:16:15 GMT"}], "update_date": "2020-10-13", "authors_parsed": [["Millidge", "Beren", ""], ["Tschantz", "Alexander", ""], ["Seth", "Anil K", ""], ["Buckley", "Christopher L", ""]]}, {"id": "2009.06118", "submitter": "Gabriel Kreiman", "authors": "Joseph Olson and Gabriel Kreiman", "title": "Simple Learning Rules Generate Complex Canonical Circuits", "comments": "4 figures, 8 supplementary figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "q-bio.NC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Cortical circuits are characterized by exquisitely complex connectivity\npatterns that emerge during development from undifferentiated networks. The\ndevelopment of these circuits is governed by a combination of precise molecular\ncues that dictate neuronal identity and location along with activity dependent\nmechanisms that help establish, refine, and maintain neuronal connectivity.\nHere we ask whether simple plasticity mechanisms can lead to assembling a\ncortical microcircuit with canonical inter-laminar connectivity, starting from\na network with all-to-all connectivity. The target canonical microcircuit is\nbased on the pattern of connections between cortical layers typically found in\nmultiple cortical areas in rodents, cats and monkeys. We use a computational\nmodel as a proof-of-principle to demonstrate that classical and reverse\nspike-timing dependent plasticity rules lead to a formation of networks that\nresemble canonical microcircuits. The model converges to biologically\nreasonable solutions provided that there is a balance between potentiation and\ndepression and enhanced inputs to layer 4, only for a small combination of\nplasticity rules. The model makes specific testable predictions about the\nlearning computations operant across cortical layers and their dynamic\ndeployment during development.\n", "versions": [{"version": "v1", "created": "Sun, 13 Sep 2020 23:59:32 GMT"}], "update_date": "2020-09-15", "authors_parsed": [["Olson", "Joseph", ""], ["Kreiman", "Gabriel", ""]]}, {"id": "2009.06808", "submitter": "Timoleon Moraitis", "authors": "Timoleon Moraitis, Abu Sebastian, Evangelos Eleftheriou (IBM Research\n  - Zurich)", "title": "Optimality of short-term synaptic plasticity in modelling certain\n  dynamic environments", "comments": "Main paper: 12 pages, 4 figures. Supplementary Information: 13 pages,\n  4 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NE cs.CV cs.LG q-bio.NC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Biological neurons and their in-silico emulations for neuromorphic artificial\nintelligence (AI) use extraordinarily energy-efficient mechanisms, such as\nspike-based communication and local synaptic plasticity. It remains unclear\nwhether these neuronal mechanisms only offer efficiency or also underlie the\nsuperiority of biological intelligence. Here, we prove rigorously that, indeed,\nthe Bayes-optimal prediction and inference of randomly but continuously\ntransforming environments, a common natural setting, relies on short-term\nspike-timing-dependent plasticity, a hallmark of biological synapses. Further,\nthis dynamic Bayesian inference through plasticity enables circuits of the\ncerebral cortex in simulations to recognize previously unseen, highly distorted\ndynamic stimuli. Strikingly, this also introduces a biologically-modelled AI,\nthe first to overcome multiple limitations of deep learning and outperform\nartificial neural networks in a visual task. The cortical-like network is\nspiking and event-based, trained only with unsupervised and local plasticity,\non a small, narrow, and static training dataset, but achieves recognition of\nunseen, transformed, and dynamic data better than deep neural networks with\ncontinuous activations, trained with supervised backpropagation on the\ntransforming data. These results link short-term plasticity to high-level\ncortical function, suggest optimality of natural intelligence for natural\nenvironments, and repurpose neuromorphic AI from mere efficiency to\ncomputational supremacy altogether.\n", "versions": [{"version": "v1", "created": "Tue, 15 Sep 2020 01:04:28 GMT"}, {"version": "v2", "created": "Tue, 15 Jun 2021 22:14:34 GMT"}], "update_date": "2021-06-17", "authors_parsed": [["Moraitis", "Timoleon", "", "IBM Research\n  - Zurich"], ["Sebastian", "Abu", "", "IBM Research\n  - Zurich"], ["Eleftheriou", "Evangelos", "", "IBM Research\n  - Zurich"]]}, {"id": "2009.06899", "submitter": "Xuyun Wen", "authors": "Xuyun Wen, Liming Hsu, Weili Lin, Han Zhang, Dinggang Shen", "title": "Co-evolution of Functional Brain Network at Multiple Scales during Early\n  Infancy", "comments": "10 pages, 4 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "q-bio.NC cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The human brains are organized into hierarchically modular networks\nfacilitating efficient and stable information processing and supporting diverse\ncognitive processes during the course of development. While the remarkable\nreconfiguration of functional brain network has been firmly established in\nearly life, all these studies investigated the network development from a\n\"single-scale\" perspective, which ignore the richness engendered by its\nhierarchical nature. To fill this gap, this paper leveraged a longitudinal\ninfant resting-state functional magnetic resonance imaging dataset from birth\nto 2 years of age, and proposed an advanced methodological framework to\ndelineate the multi-scale reconfiguration of functional brain network during\nearly development. Our proposed framework is consist of two parts. The first\npart developed a novel two-step multi-scale module detection method that could\nuncover efficient and consistent modular structure for longitudinal dataset\nfrom multiple scales in a completely data-driven manner. The second part\ndesigned a systematic approach that employed the linear mixed-effect model to\nfour global and nodal module-related metrics to delineate scale-specific\nage-related changes of network organization. By applying our proposed\nmethodological framework on the collected longitudinal infant dataset, we\nprovided the first evidence that, in the first 2 years of life, the brain\nfunctional network is co-evolved at different scales, where each scale displays\nthe unique reconfiguration pattern in terms of modular organization.\n", "versions": [{"version": "v1", "created": "Tue, 15 Sep 2020 07:21:04 GMT"}], "update_date": "2020-09-16", "authors_parsed": [["Wen", "Xuyun", ""], ["Hsu", "Liming", ""], ["Lin", "Weili", ""], ["Zhang", "Han", ""], ["Shen", "Dinggang", ""]]}, {"id": "2009.07233", "submitter": "Kanika Bansal", "authors": "Nina Lauharatanahirun, Kanika Bansal, Steven M. Thurman, Jean M.\n  Vettel, Barry Giesbrecht, Scott Grafton, James C. Elliott, Erin Flynn-Evans,\n  Emily Falk, and Javier O. Garcia", "title": "Flexibility of brain regions during working memory curtails cognitive\n  consequences to lack of sleep", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "q-bio.NC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Previous research has shown a clear relationship between sleep and memory,\nexamining the impact of sleep deprivation on key cognitive processes over very\nshort durations or in special populations. Here, we show, in a longitudinal 16\nweek study, that naturalistic, unfettered sleep modulations in healthy adults\nhave significant impacts on the brain. Using a dynamic networks approach\ncombined with hierarchical statistical modelling, we show that the flexibility\nof particular brain regions that span a large network including regions in\noccipital, temporal, and frontal cortex increased when participants performed a\nworking memory task following low sleep episodes. Critically, performance\nitself did not change as a function of sleep, implying adaptability in brain\nnetworks to compensate for having a poor night's sleep by recruiting the\nnecessary resources to complete the task. We further explore whether this\ncompensatory effect is driven by a (i) increase in the recruitment of network\nresources over time and/or (ii) an expansion of the network itself. Our results\nadd to the literature linking sleep and memory, provide an analytical framework\nin which to investigate compensatory modulations in the brain, and highlight\nthe brain's resilience to day-to-day fluctuations of external pressures to\nperformance.\n", "versions": [{"version": "v1", "created": "Tue, 15 Sep 2020 17:15:01 GMT"}], "update_date": "2020-09-16", "authors_parsed": [["Lauharatanahirun", "Nina", ""], ["Bansal", "Kanika", ""], ["Thurman", "Steven M.", ""], ["Vettel", "Jean M.", ""], ["Giesbrecht", "Barry", ""], ["Grafton", "Scott", ""], ["Elliott", "James C.", ""], ["Flynn-Evans", "Erin", ""], ["Falk", "Emily", ""], ["Garcia", "Javier O.", ""]]}, {"id": "2009.07479", "submitter": "Anisleidy Gonz\\'alez-Mitjans", "authors": "A. Gonz\\'alez-Mitjans, D. Paz-Linares, A. Areces-Gonzalez, M. Li, Y.\n  Wang, ML. Bringas-Vega, and P.A Vald\\'es-Sosa", "title": "Neuroinformatic tool to study high dimensional dynamics with distributed\n  delays in Neural Mass Models", "comments": "12 pages, 6 figures, 2 tables and appendix", "journal-ref": null, "doi": null, "report-no": null, "categories": "q-bio.NC cs.CE cs.NE", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Neuroscience has shown great progress in recent years. Several of the\ntheoretical bases have arisen from the examination of dynamic systems, using\nNeural Mass Models (NMMs). Due to the largescale brain dynamics of NMMs and the\ndifficulty of studying nonlinear systems, the local linearization approach to\ndiscretize the state equation was used via an algebraic formulation, as it\nintervenes favorably in the speed and efficiency of numerical integration. To\nstudy the spacetime organization of the brain and generate more complex\ndynamics, three structural levels (cortical unit, population and system) were\ndefined and assumed, in which the new assumed representation for conduction\ndelays and new ways of connecting were defined. This is a new time-delay NMM,\nwhich can simulate several types of EEG activities since kinetics information\nwas considered at three levels of complexity. Results obtained in this analysis\nprovide additional theoretical foundations and indicate specific\ncharacteristics for understanding neurodynamic.\n", "versions": [{"version": "v1", "created": "Wed, 16 Sep 2020 05:55:17 GMT"}, {"version": "v2", "created": "Fri, 25 Sep 2020 02:17:54 GMT"}, {"version": "v3", "created": "Tue, 23 Feb 2021 07:02:09 GMT"}, {"version": "v4", "created": "Fri, 14 May 2021 05:54:11 GMT"}], "update_date": "2021-05-17", "authors_parsed": [["Gonz\u00e1lez-Mitjans", "A.", ""], ["Paz-Linares", "D.", ""], ["Areces-Gonzalez", "A.", ""], ["Li", "M.", ""], ["Wang", "Y.", ""], ["Bringas-Vega", "ML.", ""], ["Vald\u00e9s-Sosa", "P. A", ""]]}, {"id": "2009.08101", "submitter": "Robert Prentner", "authors": "Robert Prentner", "title": "Attracting Sets in Perceptual Networks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "q-bio.NC cs.NE", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  This document gives a specification for the model used in [1]. It presents a\nsimple way of optimizing mutual information between some input and the\nattractors of a (noisy) network, using a genetic algorithm. The nodes of this\nnetwork are modeled as simplified versions of the structures described in the\n\"interface theory of perception\" [2]. Accordingly, the system is referred to as\na \"perceptual network\".\n  The present paper is an edited version of technical parts of [1] and serves\nas accompanying text for the Python implementation PerceptualNetworks, freely\navailable under [3].\n  1. Prentner, R., and Fields, C.. Using AI methods to Evaluate a Minimal Model\nfor Perception. OpenPhilosophy 2019, 2, 503-524.\n  2. Hoffman, D. D., Prakash, C., and Singh, M.. The Interface Theory of\nPerception. Psychonomic Bulletin and Review 2015, 22, 1480-1506.\n  3. Prentner, R.. PerceptualNetworks.\nhttps://github.com/RobertPrentner/PerceptualNetworks. (accessed September 17\n2020)\n", "versions": [{"version": "v1", "created": "Thu, 17 Sep 2020 06:46:44 GMT"}], "update_date": "2020-09-18", "authors_parsed": [["Prentner", "Robert", ""]]}, {"id": "2009.08111", "submitter": "Lancelot Da Costa", "authors": "Lancelot Da Costa, Noor Sajid, Thomas Parr, Karl Friston, Ryan Smith", "title": "The relationship between dynamic programming and active inference: the\n  discrete, finite-horizon case", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI math.OC q-bio.NC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Active inference is a normative framework for generating behaviour based upon\nthe free energy principle, a theory of self-organisation. This framework has\nbeen successfully used to solve reinforcement learning and stochastic control\nproblems, yet, the formal relation between active inference and reward\nmaximisation has not been fully explicated. In this paper, we consider the\nrelation between active inference and dynamic programming under the Bellman\nequation, which underlies many approaches to reinforcement learning and\ncontrol. We show that, on partially observable Markov decision processes,\ndynamic programming is a limiting case of active inference. In active\ninference, agents select actions to minimise expected free energy. In the\nabsence of ambiguity about states, this reduces to matching expected states\nwith a target distribution encoding the agent's preferences. When target states\ncorrespond to rewarding states, this maximises expected reward, as in\nreinforcement learning. When states are ambiguous, active inference agents will\nchoose actions that simultaneously minimise ambiguity. This allows active\ninference agents to supplement their reward maximising (or exploitative)\nbehaviour with novelty-seeking (or exploratory) behaviour. This clarifies the\nconnection between active inference and reinforcement learning, and how both\nframeworks may benefit from each other.\n", "versions": [{"version": "v1", "created": "Thu, 17 Sep 2020 07:13:59 GMT"}, {"version": "v2", "created": "Sat, 19 Sep 2020 17:28:07 GMT"}, {"version": "v3", "created": "Tue, 22 Sep 2020 17:19:26 GMT"}], "update_date": "2020-09-23", "authors_parsed": [["Da Costa", "Lancelot", ""], ["Sajid", "Noor", ""], ["Parr", "Thomas", ""], ["Friston", "Karl", ""], ["Smith", "Ryan", ""]]}, {"id": "2009.08309", "submitter": "Charis Mesaritakis", "authors": "Menelaos Skontranis, George Sarantoglou, Stavros Deligiannidis, Adonis\n  Bogris, Charis Mesaritakis", "title": "Unsupervised Image Classification Through Time-Multiplexed Photonic\n  Multi-Layer Spiking Convolutional Neural Network", "comments": "4 pages, 3 figures", "journal-ref": null, "doi": "10.1109/ECOC48923.2020.9333320", "report-no": null, "categories": "q-bio.NC eess.IV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present results of a deep photonic spiking convolutional neural network,\nbased on two-section VCSELs, targeting image classification. Training is based\non unsupervised spike-timing dependent plasticity, whereas neuron\ntime-multiplexing and ultra-fast response are exploited towards a a reduction\nof the physical neuron count by 90%\n", "versions": [{"version": "v1", "created": "Wed, 16 Sep 2020 07:52:32 GMT"}], "update_date": "2021-03-30", "authors_parsed": [["Skontranis", "Menelaos", ""], ["Sarantoglou", "George", ""], ["Deligiannidis", "Stavros", ""], ["Bogris", "Adonis", ""], ["Mesaritakis", "Charis", ""]]}, {"id": "2009.08373", "submitter": "Melanie Sclar", "authors": "M. Sclar, G. Bujia, S. Vita, G. Solovey, J. E. Kamienkowski", "title": "Modeling human visual search: A combined Bayesian searcher and saliency\n  map approach for eye movement guidance in natural scenes", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.CV q-bio.NC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Finding objects is essential for almost any daily-life visual task. Saliency\nmodels have been useful to predict fixation locations in natural images, but\nare static, i.e., they provide no information about the time-sequence of\nfixations. Nowadays, one of the biggest challenges in the field is to go beyond\nsaliency maps to predict a sequence of fixations related to a visual task, such\nas searching for a given target. Bayesian observer models have been proposed\nfor this task, as they represent visual search as an active sampling process.\nNevertheless, they were mostly evaluated on artificial images, and how they\nadapt to natural images remains largely unexplored.\n  Here, we propose a unified Bayesian model for visual search guided by\nsaliency maps as prior information. We validated our model with a visual search\nexperiment in natural scenes recording eye movements. We show that, although\nstate-of-the-art saliency models perform well in predicting the first two\nfixations in a visual search task, their performance degrades to chance\nafterward. This suggests that saliency maps alone are good to model bottom-up\nfirst impressions, but are not enough to explain the scanpaths when top-down\ntask information is critical. Thus, we propose to use them as priors of\nBayesian searchers. This approach leads to a behavior very similar to humans\nfor the whole scanpath, both in the percentage of target found as a function of\nthe fixation rank and the scanpath similarity, reproducing the entire sequence\nof eye movements.\n", "versions": [{"version": "v1", "created": "Thu, 17 Sep 2020 15:38:23 GMT"}, {"version": "v2", "created": "Tue, 8 Dec 2020 04:02:44 GMT"}], "update_date": "2020-12-09", "authors_parsed": [["Sclar", "M.", ""], ["Bujia", "G.", ""], ["Vita", "S.", ""], ["Solovey", "G.", ""], ["Kamienkowski", "J. E.", ""]]}, {"id": "2009.08378", "submitter": "Timo C. Wunderlich", "authors": "Timo C. Wunderlich, Christian Pehle", "title": "Event-Based Backpropagation can compute Exact Gradients for Spiking\n  Neural Networks", "comments": null, "journal-ref": null, "doi": "10.1038/s41598-021-91786-z", "report-no": null, "categories": "q-bio.NC cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Spiking neural networks combine analog computation with event-based\ncommunication using discrete spikes. While the impressive advances of deep\nlearning are enabled by training non-spiking artificial neural networks using\nthe backpropagation algorithm, applying this algorithm to spiking networks was\npreviously hindered by the existence of discrete spike events and\ndiscontinuities. For the first time, this work derives the backpropagation\nalgorithm for a continuous-time spiking neural network and a general loss\nfunction by applying the adjoint method together with the proper partial\nderivative jumps, allowing for backpropagation through discrete spike events\nwithout approximations. This algorithm, EventProp, backpropagates errors at\nspike times in order to compute the exact gradient in an event-based,\ntemporally and spatially sparse fashion. We use gradients computed via\nEventProp to train networks on the Yin-Yang and MNIST datasets using either a\nspike time or voltage based loss function and report competitive performance.\nOur work supports the rigorous study of gradient-based learning algorithms in\nspiking neural networks and provides insights toward their implementation in\nnovel brain-inspired hardware.\n", "versions": [{"version": "v1", "created": "Thu, 17 Sep 2020 15:45:00 GMT"}, {"version": "v2", "created": "Mon, 21 Sep 2020 15:59:39 GMT"}, {"version": "v3", "created": "Mon, 31 May 2021 18:00:07 GMT"}], "update_date": "2021-06-22", "authors_parsed": [["Wunderlich", "Timo C.", ""], ["Pehle", "Christian", ""]]}, {"id": "2009.08385", "submitter": "Benedetta Franceschiello Dr.", "authors": "Katharina Glomb, Joana Cabral, Anna Cattani, Alberto Mazzoni, Ashish\n  Raj, Benedetta Franceschiello", "title": "Computational models in Electroencephalography", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "q-bio.NC cs.CE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Computational models lie at the intersection of basic neuroscience and\nhealthcare applications because they allow researchers to test hypotheses\n\\textit{in silico} and predict the outcome of experiments and interactions that\nare very hard to test in reality. Yet, what is meant by \"computational model\"\nis understood in many different ways by researchers in different fields of\nneuroscience and psychology, hindering communication and collaboration. In this\nreview, we point out the state of the art of computational modeling in\nElectroencephalography (EEG) and outline how these models can be used to\nintegrate findings from electrophysiology, network-level models, and behavior.\nOn the one hand, computational models serve to investigate the mechanisms that\ngenerate brain activity, for example measured with EEG, such as the transient\nemergence of oscillations at different frequency bands and/or with different\nspatial topographies. On the other hand, computational models serve to design\nexperiments and test hypotheses \\emph{in silico}. The final purpose of\ncomputational models of EEG is to obtain a comprehensive understanding of the\nmechanisms that underlie the EEG signal. This is crucial for an accurate\ninterpretation of EEG measurements that may ultimately serve in the development\nof novel clinical applications.\n", "versions": [{"version": "v1", "created": "Thu, 17 Sep 2020 15:59:35 GMT"}], "update_date": "2020-09-18", "authors_parsed": [["Glomb", "Katharina", ""], ["Cabral", "Joana", ""], ["Cattani", "Anna", ""], ["Mazzoni", "Alberto", ""], ["Raj", "Ashish", ""], ["Franceschiello", "Benedetta", ""]]}, {"id": "2009.08667", "submitter": "Jonas Stapmanns", "authors": "Jonas Stapmanns, Jan Hahne, Moritz Helias, Matthias Bolten, Markus\n  Diesmann and David Dahmen", "title": "Event-based update of synapses in voltage-based learning rules", "comments": "45 pages, 13 figures, 7 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "q-bio.NC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Due to the point-like nature of neuronal spiking, efficient neural network\nsimulators often employ event-based simulation schemes for synapses. Yet many\ntypes of synaptic plasticity rely on the membrane potential of the postsynaptic\ncell as a third factor in addition to pre- and postsynaptic spike times.\nSynapses therefore require continuous information to update their strength\nwhich a priori necessitates a continuous update in a time-driven manner. The\nlatter hinders scaling of simulations to realistic cortical network sizes and\nrelevant time scales for learning. Here, we derive two efficient algorithms for\narchiving postsynaptic membrane potentials, both compatible with modern\nsimulation engines based on event-based synapse updates. We theoretically\ncontrast the two algorithms with a time-driven synapse update scheme to analyze\nadvantages in terms of memory and computations. We further present a reference\nimplementation in the spiking neural network simulator NEST for two\nprototypical voltage-based plasticity rules: the Clopath rule and the\nUrbanczik-Senn rule. For both rules, the two event-based algorithms\nsignificantly outperform the time-driven scheme. Depending on the amount of\ndata to be stored for plasticity, which heavily differs between the rules, a\nstrong performance increase can be achieved by compressing or sampling of\ninformation on membrane potentials. Our results on computational efficiency\nrelated to archiving of information provide guidelines for the design of\nlearning rules in order to make them practically usable in large-scale\nnetworks.\n", "versions": [{"version": "v1", "created": "Fri, 18 Sep 2020 07:37:50 GMT"}, {"version": "v2", "created": "Wed, 10 Mar 2021 15:00:03 GMT"}], "update_date": "2021-03-11", "authors_parsed": [["Stapmanns", "Jonas", ""], ["Hahne", "Jan", ""], ["Helias", "Moritz", ""], ["Bolten", "Matthias", ""], ["Diesmann", "Markus", ""], ["Dahmen", "David", ""]]}, {"id": "2009.08793", "submitter": "Dhananjay Sonawane", "authors": "Dhananjay Sonawane, Krishna Prasad Miyapuram, Bharatesh RS, Derek J.\n  Lomas", "title": "GuessTheMusic: Song Identification from Electroencephalography response", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "q-bio.NC cs.SD", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The music signal comprises of different features like rhythm, timbre, melody,\nharmony. Its impact on the human brain has been an exciting research topic for\nthe past several decades. Electroencephalography (EEG) signal enables\nnon-invasive measurement of brain activity. Leveraging the recent advancements\nin deep learning, we proposed a novel approach for song identification using a\nConvolution Neural network given the electroencephalography (EEG) responses. We\nrecorded the EEG signals from a group of 20 participants while listening to a\nset of 12 song clips, each of approximately 2 minutes, that were presented in\nrandom order. The repeating nature of Music is captured by a data slicing\napproach considering brain signals of 1 second duration as representative of\neach song clip. More specifically, we predict the song corresponding to one\nsecond of EEG data when given as input rather than a complete two-minute\nresponse. We have also discussed pre-processing steps to handle large\ndimensions of a dataset and various CNN architectures. For all the experiments,\nwe have considered each participant's EEG response for each song in both train\nand test data. We have obtained 84.96\\% accuracy for the same. The performance\nobserved gives appropriate implication towards the notion that listening to a\nsong creates specific patterns in the brain, and these patterns vary from\nperson to person.\n", "versions": [{"version": "v1", "created": "Thu, 17 Sep 2020 11:45:37 GMT"}], "update_date": "2020-09-30", "authors_parsed": [["Sonawane", "Dhananjay", ""], ["Miyapuram", "Krishna Prasad", ""], ["RS", "Bharatesh", ""], ["Lomas", "Derek J.", ""]]}, {"id": "2009.08889", "submitter": "Alexander van Meegen", "authors": "Alexander van Meegen, Tobias K\\\"uhn, Moritz Helias", "title": "Large Deviation Approach to Random Recurrent Neuronal Networks:\n  Parameter Inference, Activity Prediction, and Fluctuation-Induced Transitions", "comments": "12 pages, 4 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cond-mat.dis-nn q-bio.NC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We here unify the field theoretical approach to neuronal networks with large\ndeviation theory. For a prototypical random recurrent network model with\ncontinuous-valued units, we show that the effective action is identical to the\nrate function and derive the latter using field theory. This rate function\ntakes the form of a Kullback-Leibler divergence which enables data-driven\ninference of model parameters, Bayesian prediction of time series, and\ncalculation of fluctuations beyond mean--field theory. Lastly, we expose a\nregime with fluctuation--induced transitions between mean--field solutions.\n", "versions": [{"version": "v1", "created": "Fri, 18 Sep 2020 15:28:28 GMT"}, {"version": "v2", "created": "Tue, 16 Mar 2021 11:59:11 GMT"}], "update_date": "2021-03-17", "authors_parsed": [["van Meegen", "Alexander", ""], ["K\u00fchn", "Tobias", ""], ["Helias", "Moritz", ""]]}, {"id": "2009.09081", "submitter": "Jingyue Zhao", "authors": "Jingyue Zhao, Nicoletta Risi, Marco Monforte, Chiara Bartolozzi,\n  Giacomo Indiveri, and Elisa Donati", "title": "Closed-loop spiking control on a neuromorphic processor implemented on\n  the iCub", "comments": null, "journal-ref": null, "doi": "10.1109/JETCAS.2020.3040390", "report-no": null, "categories": "cs.ET cs.NE cs.RO q-bio.NC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Despite neuromorphic engineering promises the deployment of low latency,\nadaptive and low power systems that can lead to the design of truly autonomous\nartificial agents, the development of a fully neuromorphic artificial agent is\nstill missing. While neuromorphic sensing and perception, as well as\ndecision-making systems, are now mature, the control and actuation part is\nlagging behind. In this paper, we present a closed-loop motor controller\nimplemented on mixed-signal analog-digital neuromorphic hardware using a\nspiking neural network. The network performs a proportional control action by\nencoding target, feedback, and error signals using a spiking relational\nnetwork. It continuously calculates the error through a connectivity pattern,\nwhich relates the three variables by means of feed-forward connections.\nRecurrent connections within each population are used to speed up the\nconvergence, decrease the effect of mismatch and improve selectivity. The\nneuromorphic motor controller is interfaced with the iCub robot simulator. We\ntested our spiking P controller in a single joint control task, specifically\nfor the robot head yaw. The spiking controller sends the target positions,\nreads the motor state from its encoder, and sends back the motor commands to\nthe joint. The performance of the spiking controller is tested in a step\nresponse experiment and in a target pursuit task. In this work, we optimize the\nnetwork structure to make it more robust to noisy inputs and device mismatch,\nwhich leads to better control performances.\n", "versions": [{"version": "v1", "created": "Tue, 1 Sep 2020 14:17:48 GMT"}], "update_date": "2021-03-05", "authors_parsed": [["Zhao", "Jingyue", ""], ["Risi", "Nicoletta", ""], ["Monforte", "Marco", ""], ["Bartolozzi", "Chiara", ""], ["Indiveri", "Giacomo", ""], ["Donati", "Elisa", ""]]}, {"id": "2009.09250", "submitter": "Akke Mats Houben", "authors": "Akke Mats Houben", "title": "Frequency selectivity of neural circuits with heterogeneous transmission\n  delays", "comments": "11 pages, 6 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "q-bio.NC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Neurons are connected to other neurons by axons and dendrites that conduct\nsignals with finite velocities, resulting in delays between the firing of a\nneuron and the arrival of the resultant impulse at other neurons. Since delays\ngreatly complicate the analytical treatment and interpretation of models, they\nare usually neglected or taken to be uniform, leading to a lack in the\ncomprehension of the effects of delays in neural systems. This paper shows that\nheterogeneous transmission delays make small groups of neurons respond\nselectively to inputs with differing frequency spectra. By studying a single\nintegrate-and-fire neuron receiving correlated time-shifted inputs, it is shown\nhow the frequency response is linked to both the strengths and delay times of\nthe afferent connections. The results show that incorporating delays alters the\nfunctioning of neural networks, and changes the effect that neural connections\nand synaptic strengths have.\n", "versions": [{"version": "v1", "created": "Sat, 19 Sep 2020 15:09:41 GMT"}], "update_date": "2020-09-22", "authors_parsed": [["Houben", "Akke Mats", ""]]}, {"id": "2009.10015", "submitter": "Pedro Mediano", "authors": "Pedro A.M. Mediano, Fernando E. Rosas, Adam B. Barrett, Daniel Bor", "title": "Decomposing spectral and phasic differences in non-linear features\n  between datasets", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME q-bio.NC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  When employing non-linear methods to characterise complex systems, it is\nimportant to determine to what extent they are capturing genuine non-linear\nphenomena that could not be assessed by simpler spectral methods. Specifically,\nwe are concerned with the problem of quantifying spectral and phasic effects on\nan observed difference in a non-linear feature between two systems (or two\nstates of the same system). Here we derive, from a sequence of null models, a\ndecomposition of the difference in an observable into spectral, phasic, and\nspectrum-phase interaction components. Our approach makes no assumptions about\nthe structure of the data and adds nuance to a wide range of time series\nanalyses.\n", "versions": [{"version": "v1", "created": "Mon, 21 Sep 2020 16:46:45 GMT"}], "update_date": "2020-09-22", "authors_parsed": [["Mediano", "Pedro A. M.", ""], ["Rosas", "Fernando E.", ""], ["Barrett", "Adam B.", ""], ["Bor", "Daniel", ""]]}, {"id": "2009.10308", "submitter": "Melia Bonomo", "authors": "Melia E. Bonomo, Christof Karmonik, Anthony K. Brandt, J. Todd Frazier", "title": "Modularity allows classification of human brain networks during music\n  and speech perception", "comments": "17 pages, 6 figures, 5 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "q-bio.NC physics.bio-ph", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We investigate the use of modularity as a quantifier of whole-brain\nfunctional networks. Brain networks are constructed from functional magnetic\nresonance imaging while subjects listened to auditory pieces that varied in\nemotivity and cultural familiarity. The results of our analysis reveal high and\nlow modularity groups based on the network configuration during a subject's\nfavorite song, and this classification can predict network reconfiguration\nduring the other auditory pieces. In particular, subjects in the low modularity\ngroup show significant brain network reconfiguration during both familiar and\nunfamiliar pieces. In contrast, the high modularity brain networks appear more\nrobust and only exhibit significant changes during the unfamiliar music and\nspeech. We also find differences in the stability of module composition for the\ntwo groups during each auditory piece. Our results suggest that the modularity\nof the whole-brain network plays a significant role in the way the network\nreconfigures during varying auditory processing demands, and it may therefore\ncontribute to individual differences in neuroplasticity capability during\ntherapeutic music engagement.\n", "versions": [{"version": "v1", "created": "Tue, 22 Sep 2020 04:21:05 GMT"}], "update_date": "2020-09-23", "authors_parsed": [["Bonomo", "Melia E.", ""], ["Karmonik", "Christof", ""], ["Brandt", "Anthony K.", ""], ["Frazier", "J. Todd", ""]]}, {"id": "2009.10615", "submitter": "Danko Georgiev", "authors": "Danko D. Georgiev, Stefan K. Kolev, Eliahu Cohen, James F. Glazebrook", "title": "Computational capacity of pyramidal neurons in the cerebral cortex", "comments": "18 pages, 4 figures", "journal-ref": "Brain Research 2020; 1748: 147069", "doi": "10.1016/j.brainres.2020.147069", "report-no": null, "categories": "q-bio.NC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The electric activities of cortical pyramidal neurons are supported by\nstructurally stable, morphologically complex axo-dendritic trees. Anatomical\ndifferences between axons and dendrites in regard to their length or caliber\nreflect the underlying functional specializations, for input or output of\nneural information, respectively. For a proper assessment of the computational\ncapacity of pyramidal neurons, we have analyzed an extensive dataset of\nthree-dimensional digital reconstructions from the NeuroMorpho.Org database,\nand quantified basic dendritic or axonal morphometric measures in different\nregions and layers of the mouse, rat or human cerebral cortex. Physical\nestimates of the total number and type of ions involved in neuronal electric\nspiking based on the obtained morphometric data, combined with energetics of\nneurotransmitter release and signaling fueled by glucose consumed by the active\nbrain, support highly efficient cerebral computation performed at the\nthermodynamically allowed Landauer limit for implementation of irreversible\nlogical operations. Individual proton tunneling events in voltage-sensing S4\nprotein $\\alpha$-helices of Na$^{+}$, K$^{+}$ or Ca$^{2+}$ ion channels are\nideally suited to serve as single Landauer elementary logical operations that\nare then amplified by selective ionic currents traversing the open channel\npores. This miniaturization of computational gating allows the execution of\nover 1.2 zetta logical operations per second in the human cerebral cortex\nwithout combusting the brain by the released heat.\n", "versions": [{"version": "v1", "created": "Thu, 3 Sep 2020 05:42:29 GMT"}], "update_date": "2020-09-23", "authors_parsed": [["Georgiev", "Danko D.", ""], ["Kolev", "Stefan K.", ""], ["Cohen", "Eliahu", ""], ["Glazebrook", "James F.", ""]]}, {"id": "2009.11245", "submitter": "Karla Burelo", "authors": "Mohammadali Sharifshazileh (1 and 2), Karla Burelo (1 and 2), Johannes\n  Sarnthein (2) and Giacomo Indiveri (1) ((1) Institute of Neuroinformatics,\n  University of Zurich and ETH Zurich, (2) Klinik f\\\"ur Neurochirurgie,\n  Universit\\\"atsSpital und Universit\\\"at Z\\\"urich)", "title": "An electronic neuromorphic system for real-time detection of High\n  Frequency Oscillations (HFOs) in intracranial EEG", "comments": "16 pages. A short video describing the rationale underlying the study\n  can be viewed on https://youtu.be/NuAA91fdmaM", "journal-ref": null, "doi": "10.1038/s41467-021-23342-2", "report-no": null, "categories": "eess.SP cs.AI cs.NE q-bio.NC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this work, we present a neuromorphic system that combines for the first\ntime a neural recording headstage with a signal-to-spike conversion circuit and\na multi-core spiking neural network (SNN) architecture on the same die for\nrecording, processing, and detecting High Frequency Oscillations (HFO), which\nare biomarkers for the epileptogenic zone. The device was fabricated using a\nstandard 0.18$\\mu$m CMOS technology node and has a total area of 99mm$^{2}$. We\ndemonstrate its application to HFO detection in the iEEG recorded from 9\npatients with temporal lobe epilepsy who subsequently underwent epilepsy\nsurgery. The total average power consumption of the chip during the detection\ntask was 614.3$\\mu$W. We show how the neuromorphic system can reliably detect\nHFOs: the system predicts postsurgical seizure outcome with state-of-the-art\naccuracy, specificity and sensitivity (78%, 100%, and 33% respectively). This\nis the first feasibility study towards identifying relevant features in\nintracranial human data in real-time, on-chip, using event-based processors and\nspiking neural networks. By providing \"neuromorphic intelligence\" to neural\nrecording circuits the approach proposed will pave the way for the development\nof systems that can detect HFO areas directly in the operation room and improve\nthe seizure outcome of epilepsy surgery.\n", "versions": [{"version": "v1", "created": "Wed, 23 Sep 2020 16:40:44 GMT"}, {"version": "v2", "created": "Thu, 22 Oct 2020 14:22:30 GMT"}], "update_date": "2021-06-09", "authors_parsed": [["Sharifshazileh", "Mohammadali", "", "1 and 2"], ["Burelo", "Karla", "", "1 and 2"], ["Sarnthein", "Johannes", ""], ["Indiveri", "Giacomo", ""]]}, {"id": "2009.11781", "submitter": "Stefan Bornholdt", "authors": "Stefan Landmann, Lorenz Baumgarten and Stefan Bornholdt", "title": "Self-organized criticality in neural networks from activity-based\n  rewiring", "comments": "9 pages, 4 figures", "journal-ref": "Phys. Rev. E 103, 032304 (2021)", "doi": "10.1103/PhysRevE.103.032304", "report-no": null, "categories": "cond-mat.dis-nn cond-mat.stat-mech nlin.AO q-bio.NC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Neural systems process information in a dynamical regime between silence and\nchaotic dynamics. This has lead to the criticality hypothesis which suggests\nthat neural systems reach such a state by self-organizing towards the critical\npoint of a dynamical phase transition. Here, we study a minimal neural network\nmodel that exhibits self-organized criticality in the presence of stochastic\nnoise using a rewiring rule which only utilizes local information. For network\nevolution, incoming links are added to a node or deleted, depending on the\nnode's average activity. Based on this rewiring-rule only, the network evolves\ntowards a critcal state, showing typical power-law distributed avalanche\nstatistics. The observed exponents are in accord with criticality as predicted\nby dynamical scaling theory, as well as with the observed exponents of neural\navalanches. The critical state of the model is reached autonomously without\nneed for parameter tuning, is independent of initial conditions, is robust\nunder stochastic noise, and independent of details of the implementation as\ndifferent variants of the model indicate. We argue that this supports the\nhypothesis that real neural systems may utilize similar mechanisms to\nself-organize towards criticality especially during early developmental stages.\n", "versions": [{"version": "v1", "created": "Thu, 24 Sep 2020 16:15:39 GMT"}], "update_date": "2021-03-10", "authors_parsed": [["Landmann", "Stefan", ""], ["Baumgarten", "Lorenz", ""], ["Bornholdt", "Stefan", ""]]}, {"id": "2009.11928", "submitter": "Nishad Singhi", "authors": "Nishad Singhi, Hritik Bansal", "title": "Resting state-fMRI approach towards understanding impairments in mTLE", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "q-bio.NC eess.IV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Mesial temporal lobe epilepsy (mTLE) is the most common form of epilepsy.\nWhile it is characterized by an epileptogenic focus in the mesial temporal\nlobe, it is increasingly understood as a network disorder. Hence, understanding\nthe nature of impairments on a network level is essential for its diagnosis and\ntreatment. In this work, we review recent works that apply resting-state\nfunctional MRI to provide key insights into the impairments to the functional\narchitecture in mTLE. We discuss changes on both regional and global scales.\nFinally, we describe how Machine Learning can be applied to rs-fMRI data to\nextract resting-state networks specific to mTLE and for automated diagnosis of\nthis disease.\n", "versions": [{"version": "v1", "created": "Thu, 24 Sep 2020 19:48:00 GMT"}], "update_date": "2020-09-28", "authors_parsed": [["Singhi", "Nishad", ""], ["Bansal", "Hritik", ""]]}, {"id": "2009.12023", "submitter": "Alessandro Sanzeni", "authors": "Alessandro Sanzeni and Mark H Histed and Nicolas Brunel", "title": "Emergence of irregular activity in networks of strongly coupled\n  conductance-based neurons", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "q-bio.NC physics.bio-ph", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Cortical neurons are characterized by irregular firing and a broad\ndistribution of rates. The balanced state model explains these observations\nwith a cancellation of mean excitatory and inhibitory currents, which makes\nfluctuations drive firing. In networks of neurons with current-based synapses,\nthe balanced state emerges dynamically if coupling is strong, i.e. if the mean\nnumber of synapses per neuron $K$ is large and synaptic efficacy is of order\n$1/\\sqrt{K}$. When synapses are conductance-based, current fluctuations are\nsuppressed when coupling is strong, questioning the applicability of the\nbalanced state idea to biological neural networks. We analyze networks of\nstrongly coupled conductance-based neurons and show that asynchronous irregular\nactivity and broad distributions of rates emerge if synapses are of order\n$1/\\log(K)$. In such networks, unlike in the standard balanced state model,\ncurrent fluctuations are small and firing is maintained by a drift-diffusion\nbalance. This balance emerges dynamically, without fine tuning, if inputs are\nsmaller than a critical value, which depends on synaptic time constants and\ncoupling strength, and is significantly more robust to connection\nheterogeneities than the classical balanced state model. Our analysis makes\nexperimentally testable predictions of how the network response properties\nshould evolve as input increases.\n", "versions": [{"version": "v1", "created": "Fri, 25 Sep 2020 04:05:22 GMT"}, {"version": "v2", "created": "Tue, 13 Oct 2020 22:02:23 GMT"}], "update_date": "2020-10-15", "authors_parsed": [["Sanzeni", "Alessandro", ""], ["Histed", "Mark H", ""], ["Brunel", "Nicolas", ""]]}, {"id": "2009.12660", "submitter": "Ying Wang", "authors": "Ying Wang, Floris Beuving, Jorik Nonnekes, Mike X Cohen, Xi Long,\n  Ronald M Aarts, Richard Van Wezel", "title": "Characterizing and Detecting Freezing of Gait using Multi-modal\n  Physiological Signals", "comments": "9 pages, 8 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.SP q-bio.NC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Freezing-of-gait a mysterious symptom of Parkinsons disease and defined as a\nsudden loss of ability to move forward. Common treatments of freezing episodes\nare currently of moderate efficacy and can likely be improved through a\nreliable freezing evaluation. Basic-science studies about the characterization\nof freezing episodes and a 24/7 evidence-support freezing detection system can\ncontribute to the reliability of the evaluation in daily life. In this study,\nwe analyzed multi-modal features from brain, eye, heart, motion, and gait\nactivity from 15 participants with idiopathic Parkinsons disease and 551\nfreezing episodes induced by turning in place. Statistical analysis was first\napplied on 248 of the 551 to determine which multi-modal features were\nassociated with freezing episodes. Features significantly associated with\nfreezing episodes were ranked and used for the freezing detection. We found\nthat eye-stabilization speed during turning and lower-body trembling measure\nsignificantly associated with freezing episodes and used for freezing\ndetection. Using a leave-one-subject-out cross-validation, we obtained a\nsensitivity of 97%+/-3%, a specificity of 96%+/-7%, a precision of 73%+/-21%, a\nMatthews correlation coefficient of 0.82+/-0.15, and an area under the\nPrecision-Recall curve of 0.94+/-0.05. According to the Precision-Recall\ncurves, the proposed freezing detection method using the multi-modal features\nperformed better than using single-modal features.\n", "versions": [{"version": "v1", "created": "Sat, 26 Sep 2020 18:11:08 GMT"}], "update_date": "2020-09-29", "authors_parsed": [["Wang", "Ying", ""], ["Beuving", "Floris", ""], ["Nonnekes", "Jorik", ""], ["Cohen", "Mike X", ""], ["Long", "Xi", ""], ["Aarts", "Ronald M", ""], ["Van Wezel", "Richard", ""]]}, {"id": "2009.12696", "submitter": "Paul Smolen", "authors": "Paul Smolen, Douglas A. Baxter, John H. Byrne", "title": "Comparing Theories for the Maintenance of Late LTP and Long-Term Memory:\n  Computational Analysis of the Roles of Kinase Feedback Pathways and Synaptic\n  Reactivation", "comments": "Submitted, under review", "journal-ref": null, "doi": null, "report-no": null, "categories": "q-bio.NC", "license": "http://creativecommons.org/licenses/by-sa/4.0/", "abstract": "  How can memories be maintained from days to a lifetime, given turnover of\nproteins that underlie expression of long-term synaptic potentiation (LTP)? One\nlikely solution relies on synaptic positive feedback loops, prominently\nincluding persistent activation of CaM kinase II (CaMKII) and self-activated\nsynthesis of protein kinase M zeta (PKM). Recent studies also suggest positive\nfeedback based on recurrent synaptic reactivation within neuron assemblies, or\nengrams, is necessary to maintain memories. The relative importance of these\nfeedback mechanisms is controversial. To explore the likelihood that each\nmechanism is necessary or sufficient, we simulated LTP maintenance with a\nsimplified model incorporating persistent kinase activation, synaptic tagging,\nand preferential reactivation of strong synapses, and analyzed implications of\nrecent data. We simulated three model variants, each maintaining LTP with one\nfeedback loop: self-activated PKM synthesis (variant I); self-activated CamKII\n(variant II); and recurrent reactivation of strengthened synapses (variant\nIII). Variant I requires and predicts that PKM must contribute to synaptic\ntagging. Variant II maintains LTP and suggests persistent CaMKII activation\ncould maintain PKM activity, a feedforward interaction not previously\nconsidered. However we note data challenging this feedback loop. In variant III\nsynaptic reactivation drives, and thus predicts, recurrent or persistent\nactivity elevations of CamKII and other necessary kinases, plausibly\ncontributing to empirically persistent elevation of PKM levels. Reactivation is\nthus predicted to sustain recurrent rounds of synaptic tagging and\nincorporation of plasticity-related proteins. We also suggest (model variant\nIV) that synaptic reactivation and autonomous kinase activation could\nsynergistically maintain LTP. We propose experiments that could discriminate\nthese maintenance mechanisms.\n", "versions": [{"version": "v1", "created": "Sat, 26 Sep 2020 21:57:34 GMT"}], "update_date": "2020-09-29", "authors_parsed": [["Smolen", "Paul", ""], ["Baxter", "Douglas A.", ""], ["Byrne", "John H.", ""]]}, {"id": "2009.12855", "submitter": "Ryan Blything", "authors": "Ryan Blything, Valerio Biscione, Ivan I. Vankov, Casimir J.H. Ludwig,\n  and Jeffrey S. Bowers", "title": "The human visual system and CNNs can both support robust online\n  translation tolerance following extreme displacements", "comments": "Main manuscript contains 5 figures plus 2 tables. SI contains 2\n  tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "q-bio.NC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Visual translation tolerance refers to our capacity to recognize objects over\na wide range of different retinal locations. Although translation is perhaps\nthe simplest spatial transform that the visual system needs to cope with, the\nextent to which the human visual system can identify objects at previously\nunseen locations is unclear, with some studies reporting near complete\ninvariance over 10{\\deg} and other reporting zero invariance at 4{\\deg} of\nvisual angle. Similarly, there is confusion regarding the extent of translation\ntolerance in computational models of vision, as well as the degree of match\nbetween human and model performance. Here we report a series of eye-tracking\nstudies (total N=70) demonstrating that novel objects trained at one retinal\nlocation can be recognized at high accuracy rates following translations up to\n18{\\deg}. We also show that standard deep convolutional networks (DCNNs)\nsupport our findings when pretrained to classify another set of stimuli across\na range of locations, or when a Global Average Pooling (GAP) layer is added to\nproduce larger receptive fields. Our findings provide a strong constraint for\ntheories of human vision and help explain inconsistent findings previously\nreported with CNNs.\n", "versions": [{"version": "v1", "created": "Sun, 27 Sep 2020 14:33:32 GMT"}, {"version": "v2", "created": "Tue, 8 Dec 2020 09:59:58 GMT"}], "update_date": "2020-12-09", "authors_parsed": [["Blything", "Ryan", ""], ["Biscione", "Valerio", ""], ["Vankov", "Ivan I.", ""], ["Ludwig", "Casimir J. H.", ""], ["Bowers", "Jeffrey S.", ""]]}, {"id": "2009.13402", "submitter": "Sana Yasin", "authors": "Sana Yasin, Syed Asad Hussain, Sinem Aslan, Imran Raza, Muhammad\n  Muzammel, Alice Othmani", "title": "EEG based Major Depressive disorder and Bipolar disorder detection using\n  Neural Networks: A review", "comments": "29 pages,2 figures and 18 Tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "q-bio.NC cs.LG eess.SP", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Mental disorders represent critical public health challenges as they are\nleading contributors to the global burden of disease and intensely influence\nsocial and financial welfare of individuals. The present comprehensive review\nconcentrate on the two mental disorders: Major depressive Disorder (MDD) and\nBipolar Disorder (BD) with noteworthy publications during the last ten years.\nThere is a big need nowadays for phenotypic characterization of psychiatric\ndisorders with biomarkers. Electroencephalography (EEG) signals could offer a\nrich signature for MDD and BD and then they could improve understanding of\npathophysiological mechanisms underling these mental disorders. In this review,\nwe focus on the literature works adopting neural networks fed by EEG signals.\nAmong those studies using EEG and neural networks, we have discussed a variety\nof EEG based protocols, biomarkers and public datasets for depression and\nbipolar disorder detection. We conclude with a discussion and valuable\nrecommendations that will help to improve the reliability of developed models\nand for more accurate and more deterministic computational intelligence based\nsystems in psychiatry. This review will prove to be a structured and valuable\ninitial point for the researchers working on depression and bipolar disorders\nrecognition by using EEG signals.\n", "versions": [{"version": "v1", "created": "Mon, 28 Sep 2020 15:19:54 GMT"}, {"version": "v2", "created": "Thu, 4 Feb 2021 18:58:54 GMT"}], "update_date": "2021-02-05", "authors_parsed": [["Yasin", "Sana", ""], ["Hussain", "Syed Asad", ""], ["Aslan", "Sinem", ""], ["Raza", "Imran", ""], ["Muzammel", "Muhammad", ""], ["Othmani", "Alice", ""]]}, {"id": "2009.13567", "submitter": "Nishant Sinha", "authors": "Nishant Sinha, Natalie Peternell, Gabrielle M. Schroeder, Jane de\n  Tisi, Sjoerd B. Vos, Gavin P. Winston, John S. Duncan, Yujiang Wang, Peter N.\n  Taylor", "title": "Focal to bilateral tonic-clonic seizures are associated with widespread\n  network abnormality in temporal lobe epilepsy", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "q-bio.NC", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Objective: To identify if whole-brain structural network alterations in\npatients with temporal lobe epilepsy (TLE) and focal to bilateral tonic-clonic\nseizures (FBTCS) differ from alterations in patients without FBTCS.\n  Methods: We dichotomized a cohort of 83 drug-resistant patients with TLE into\nthose with and without FBTCS and compared each group to 29 healthy controls.\nFor each subject, we used diffusion MRI to construct whole-brain structural\nnetworks. First, we measured the extent of alterations by performing\nFBTCS-negative (FBTCS-) versus control and FBTCS-positive (FBTCS+) versus\ncontrol comparisons, thereby delineating altered sub-networks of the\nwhole-brain structural network. Second, by standardising networks of each\npatient using control networks, we measured the subject-specific abnormality at\nevery brain region in the network, thereby quantifying the spatial localisation\nand the amount of abnormality in every patient.\n  Results: Both FBTCS+ and FBTCS- patient groups had altered sub-networks with\nreduced fractional anisotropy (FA) and increased mean diffusivity (MD) compared\nto controls. The altered subnetwork in FBTCS+ patients was more widespread than\nin FBTCS- patients (441 connections altered at t>3, p<0.001 in FBTCS+ compared\nto 21 connections altered at t>3, p=0.01 in FBTCS-). Significantly greater\nabnormalities-aggregated over the entire brain network as well as assessed at\nthe resolution of individual brain areas-were present in FBTCS+ patients\n(p<0.001, d=0.82). In contrast, the fewer abnormalities present in FBTCS-\npatients were mainly localised to the temporal and frontal areas.\n  Significance: The whole-brain structural network is altered to a greater and\nmore widespread extent in patients with TLE and FBTCS. We suggest that these\nabnormal networks may serve as an underlying structural basis or consequence of\nthe greater seizure spread observed in FBTCS.\n", "versions": [{"version": "v1", "created": "Mon, 28 Sep 2020 18:31:28 GMT"}], "update_date": "2020-09-30", "authors_parsed": [["Sinha", "Nishant", ""], ["Peternell", "Natalie", ""], ["Schroeder", "Gabrielle M.", ""], ["de Tisi", "Jane", ""], ["Vos", "Sjoerd B.", ""], ["Winston", "Gavin P.", ""], ["Duncan", "John S.", ""], ["Wang", "Yujiang", ""], ["Taylor", "Peter N.", ""]]}, {"id": "2009.14264", "submitter": "Neta Maimon", "authors": "Amitai Bickel, Alexander Gavrilov, Shimon Ivry, Neta Maimon, Lior\n  Molcho and Nathan Intrator", "title": "Reduced neural activity during volatile anesthesia compared to TIVA:\n  evidence from a novel EEG signal processing analysis through a randomized\n  controlled trial", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "q-bio.NC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Background: General anesthesia can be accomplished by inhalation-based\n(volatile) or total intravenous anesthesia (TIVA). While their effects on\npost-operative symptoms have been investigated, little is known about their\ninfluence on brain functionalities during the surgery itself. Objective: To\nassess differences in brain activity between volatile and TIVA anesthetics\nduring surgery. Participants: Seventeen patients who were electively admitted\nfor laparoscopic cholecystectomy in Galilee Medical Center, Nahariya, gave\nwritten consent to participate in the study, and were randomly divided to\nreceive either volatile anesthesia (9), or TIVA (8). An additional 17 healthy\nvolunteers were used as an awake control group. Outcome measures: A single\nbipolar EEG electrode was placed on the participants foreheads and collected\ntheir electroencephalographic data during the surgery. The signal was analyzed\nin two different methods. First, extracting real-time amplitudes of the\nclassical qEEG frequency bands and second applying novel harmonic signal\nprocessing that created three novel biomarkers. Results: All surgeries were\nuneventful, and all patients showed less than 60 bispectral index BIS score.\nBrain activity under volatile anesthesia showed significantly decreased delta\nand beta frequencies and significant decrease in activity of two biomarkers of\nthe novel analysis with a difference in decrease between volatile and TIVA\nanesthesia. These two novel biomarkers exhibited significantly higher\nactivation in the awake control group compared to the anesthetized patients.\nConclusions: Both EEG frequency bands and novel brain activity biomarkers\nprovide evidence that volatile anesthesia reduces brain activity to a much\nlower extent compared to TIVA anesthesia.\n", "versions": [{"version": "v1", "created": "Tue, 29 Sep 2020 19:11:14 GMT"}], "update_date": "2020-10-01", "authors_parsed": [["Bickel", "Amitai", ""], ["Gavrilov", "Alexander", ""], ["Ivry", "Shimon", ""], ["Maimon", "Neta", ""], ["Molcho", "Lior", ""], ["Intrator", "Nathan", ""]]}, {"id": "2009.14280", "submitter": "Yusheng Jiao", "authors": "Yusheng Jiao, Feng Ling, Sina Heydari, Nicolas Heess, Josh Merel and\n  Eva Kanso", "title": "Learning to swim in potential flow", "comments": null, "journal-ref": "Phys. Rev. Fluids 6, 050505 (2021)", "doi": "10.1103/PhysRevFluids.6.050505", "report-no": null, "categories": "q-bio.QM cs.LG physics.flu-dyn q-bio.NC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Fish swim by undulating their bodies. These propulsive motions require\ncoordinated shape changes of a body that interacts with its fluid environment,\nbut the specific shape coordination that leads to robust turning and swimming\nmotions remains unclear. To address the problem of underwater motion planning,\nwe propose a simple model of a three-link fish swimming in a potential flow\nenvironment and we use model-free reinforcement learning for shape control. We\narrive at optimal shape changes for two swimming tasks: swimming in a desired\ndirection and swimming towards a known target. This fish model belongs to a\nclass of problems in geometric mechanics, known as driftless dynamical systems,\nwhich allow us to analyze the swimming behavior in terms of geometric phases\nover the shape space of the fish. These geometric methods are less intuitive in\nthe presence of drift. Here, we use the shape space analysis as a tool for\nassessing, visualizing, and interpreting the control policies obtained via\nreinforcement learning in the absence of drift. We then examine the robustness\nof these policies to drift-related perturbations. Although the fish has no\ndirect control over the drift itself, it learns to take advantage of the\npresence of moderate drift to reach its target.\n", "versions": [{"version": "v1", "created": "Wed, 30 Sep 2020 06:31:27 GMT"}, {"version": "v2", "created": "Mon, 7 Dec 2020 23:59:01 GMT"}], "update_date": "2021-05-19", "authors_parsed": [["Jiao", "Yusheng", ""], ["Ling", "Feng", ""], ["Heydari", "Sina", ""], ["Heess", "Nicolas", ""], ["Merel", "Josh", ""], ["Kanso", "Eva", ""]]}, {"id": "2009.14283", "submitter": "Nathaniel Linden", "authors": "Nathaniel J Linden, Dennis R Tabuena, Nicholas A Steinmetz, William J\n  Moody, Steven L Brunton, Bingni W Brunton", "title": "Go with the FLOW: Visualizing spatiotemporal dynamics in optical\n  widefield calcium imaging", "comments": "25 pages, 8 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "q-bio.NC math.DS", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Widefield calcium imaging has recently emerged as a powerful experimental\ntechnique to record coordinated large-scale brain activity. These measurements\npresent a unique opportunity to characterize spatiotemporal coherent structures\nthat underlie neural activity across many regions of the brain. In this work,\nwe leverage analytic techniques from fluid dynamics to develop a visualization\nframework that highlights features of flow across the cortex, mapping wave\nfronts that may be correlated with behavioral events. First, we transform the\ntime series of widefield calcium images into time-varying vector fields using\noptic flow. Next, we extract concise diagrams summarizing the dynamics, which\nwe refer to as FLOW (flow lines in optical widefield imaging) portraits. These\nFLOW portraits provide an intuitive map of dynamic calcium activity, including\nregions of initiation and termination, as well as the direction and extent of\nactivity spread. To extract these structures, we use the finite-time Lyapunov\nexponent (FTLE) technique developed to analyze time-varying manifolds in\nunsteady fluids. Importantly, our approach captures coherent structures that\nare poorly represented by traditional modal decomposition techniques. We\ndemonstrate the application of FLOW portraits on three simple synthetic\ndatasets and two widefield calcium imaging datasets, including cortical waves\nin the developing mouse and spontaneous cortical activity in an adult mouse.\n", "versions": [{"version": "v1", "created": "Tue, 29 Sep 2020 19:47:29 GMT"}, {"version": "v2", "created": "Thu, 24 Jun 2021 20:13:40 GMT"}], "update_date": "2021-06-28", "authors_parsed": [["Linden", "Nathaniel J", ""], ["Tabuena", "Dennis R", ""], ["Steinmetz", "Nicholas A", ""], ["Moody", "William J", ""], ["Brunton", "Steven L", ""], ["Brunton", "Bingni W", ""]]}, {"id": "2009.14293", "submitter": "Sharon Chiang", "authors": "Sharon Chiang, Ankit N. Khambhati, Emily T. Wang, Marina Vannucci,\n  Edward F. Chang, Vikram R. Rao", "title": "Evidence of state-dependence in the effectiveness of responsive\n  neurostimulation for seizure modulation", "comments": null, "journal-ref": "Brain Stimulation (2021); 14(2):366-375", "doi": "10.1016/j.brs.2021.01.023", "report-no": null, "categories": "q-bio.NC", "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/", "abstract": "  An implanted device for brain-responsive neurostimulation (RNS System) is\napproved as an effective treatment to reduce seizures in adults with\nmedically-refractory focal epilepsy. Clinical trials of the RNS System\ndemonstrate population-level reduction in average seizure frequency, but\ntherapeutic response is highly variable. Recent evidence links seizures to\ncyclical fluctuations in underlying risk. We tested the hypothesis that\neffectiveness of responsive neurostimulation varies based on current state\nwithin cyclical risk fluctuations. We analyzed retrospective data from 25\nadults with medically-refractory focal epilepsy implanted with the RNS System.\nChronic electrocorticography was used to record electrographic seizures, and\nhidden Markov models decoded seizures into fluctuations in underlying risk.\nState-dependent associations of RNS System stimulation parameters with changes\nin risk were estimated. Higher charge density was associated with improved\noutcomes, both for remaining in a low seizure risk state and for transitioning\nfrom a high to a low seizure risk state. The effect of stimulation frequency\ndepended on initial seizure risk state: when starting in a low risk state,\nhigher stimulation frequencies were associated with remaining in a low risk\nstate, but when starting in a high risk state, lower stimulation frequencies\nwere associated with transition to a low risk state. Findings were consistent\nacross bipolar and monopolar stimulation configurations. The impact of RNS on\nseizure frequency exhibits state-dependence, such that stimulation parameters\nwhich are effective in one seizure risk state may not be effective in another.\nThese findings represent conceptual advances in understanding the therapeutic\nmechanism of RNS, and directly inform current practices of RNS tuning and the\ndevelopment of next-generation neurostimulation systems.\n", "versions": [{"version": "v1", "created": "Tue, 29 Sep 2020 20:20:16 GMT"}, {"version": "v2", "created": "Fri, 19 Feb 2021 04:44:52 GMT"}], "update_date": "2021-02-22", "authors_parsed": [["Chiang", "Sharon", ""], ["Khambhati", "Ankit N.", ""], ["Wang", "Emily T.", ""], ["Vannucci", "Marina", ""], ["Chang", "Edward F.", ""], ["Rao", "Vikram R.", ""]]}, {"id": "2009.14744", "submitter": "Pierre Gosselin", "authors": "Pierre Gosselin, A\\\"ileen Lotz, Marc Wambst", "title": "Statistical Field Theory and Networks of Spiking Neurons", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "q-bio.NC cond-mat.dis-nn hep-th math-ph math.MP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper models the dynamics of a large set of interacting neurons within\nthe framework of statistical field theory. We use a method initially developed\nin the context of statistical field theory [44] and later adapted to complex\nsystems in interaction [45][46]. Our model keeps track of individual\ninteracting neurons dynamics but also preserves some of the features and goals\nof neural field dynamics, such as indexing a large number of neurons by a space\nvariable. Thus, this paper bridges the scale of individual interacting neurons\nand the macro-scale modelling of neural field theory.\n", "versions": [{"version": "v1", "created": "Wed, 30 Sep 2020 15:29:13 GMT"}], "update_date": "2020-10-01", "authors_parsed": [["Gosselin", "Pierre", ""], ["Lotz", "A\u00efleen", ""], ["Wambst", "Marc", ""]]}]