[{"id": "1709.00050", "submitter": "Enzo Tagliazucchi", "authors": "Enzo Tagliazucchi", "title": "The signatures of conscious access and phenomenology are consistent with\n  large-scale brain communication at criticality", "comments": "25 pages, 7 figures", "journal-ref": "Consciousness and Cognition 55C (2017) pp. 136-147", "doi": "10.1016/j.concog.2017.08.008", "report-no": null, "categories": "q-bio.NC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Conscious awareness refers to the association of information processing in\nthe brain that is accompanied by subjective, reportable experiences. Current\nmodels of conscious access propose that sufficiently strong sensory stimuli\nignite a global network of regions allowing further processing. The immense\nnumber of possible experiences indicates that brain activity associated with\nconscious awareness must be highly differentiated. However, information must\nalso be integrated to account for the unitary nature of consciousness. We\npresent a conceptual computational model that identifies conscious access with\nself-sustained percolation in an anatomical network. We show that if activity\npropagates at the critical threshold, the amount of integrated information\n(Phi) is maximal after conscious access, as well as other related markers. We\nalso identify a posterior hotspot of regions with high levels of information\nsharing during conscious access. Finally, competitive activity spreading\nqualitatively describes the results of paradigms such as backward masking and\nbinocular rivalry.\n", "versions": [{"version": "v1", "created": "Thu, 31 Aug 2017 19:32:57 GMT"}], "update_date": "2017-09-04", "authors_parsed": [["Tagliazucchi", "Enzo", ""]]}, {"id": "1709.00133", "submitter": "Ann Sizemore", "authors": "Ann E. Sizemore, Elisabeth A. Karuza, Chad Giusti, Danielle S. Bassett", "title": "Knowledge gaps in the early growth of semantic networks", "comments": "17 pages, 6 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "q-bio.NC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Understanding the features of and mechanisms behind language learning can\nprovide insights into the general process of knowledge acquisition. Recent\nmethods from network science applied to language learning have advanced the\nfield, particularly by noting associations between densely connected words and\nacquisition. However, the importance of sparse areas of the network, or\nknowledge gaps, remains unexplored. Here we create a semantic feature network\nin which words correspond to nodes and in which connections correspond to\nsemantic similarity. We develop a new analytical approach built on principles\nof applied topology to query the prevalence of knowledge gaps, which we propose\nmanifest as cavities within the network. We detect topological cavities of\nmultiple dimensions in the growing semantic feature network of children ages 16\nto 30 months. The pattern of cavity appearance matches that of a constrained\nnull model, created by predefining the affinity of each node for connections.\nFurthermore, when word acquisition time is computed from children of mothers\nwith differing levels of education, we find that despite variation at the word\nlevel, the global organization as measured by persistent homology remains\ncomparable. We show that topological properties of a node correlate with\nfilling in cavities better than simple lexical properties such as the length\nand frequency of the corresponding word. Finally, we show that the large-scale\narchitecture of the semantic feature network is topologically accommodating to\nmany node orders. We discuss the importance of topology in language learning,\nand we speculate that the formation and filling of knowledge gaps may be a\nrobust feature of knowledge acquisition.\n", "versions": [{"version": "v1", "created": "Fri, 1 Sep 2017 02:30:24 GMT"}], "update_date": "2017-09-04", "authors_parsed": [["Sizemore", "Ann E.", ""], ["Karuza", "Elisabeth A.", ""], ["Giusti", "Chad", ""], ["Bassett", "Danielle S.", ""]]}, {"id": "1709.00339", "submitter": "Mirko Lukovic", "authors": "Tatiana A. Amor, Mirko Lukovic, Hans J. Herrmann, and Jose S. Andrade\n  Jr", "title": "How images determine our visual search strategy", "comments": null, "journal-ref": "J. R. Soc. Interface 14, 20170406, 2017", "doi": "10.1098/rsif.2017.0406", "report-no": null, "categories": "q-bio.NC cond-mat.other", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  When searching for a target within an image our brain can adopt different\nstrategies, but which one does it choose? This question can be answered by\ntracking the motion of the eye while it executes the task. Following many\nindividuals performing various search tasks we distinguish between two\ncompeting strategies. Motivated by these findings, we introduce a model that\ncaptures the interplay of the search strategies and allows us to create\nartificial eye-tracking trajectories, which could be compared to the\nexperimental ones. Identifying the model parameters allows us to quantify the\nstrategy employed in terms of ensemble averages, characterizing each\nexperimental cohort. In this way we can discern with high sensitivity the\nrelation between the visual landscape and the average strategy, disclosing how\nsmall variations in the image induce changes in the strategy.\n", "versions": [{"version": "v1", "created": "Thu, 31 Aug 2017 11:14:59 GMT"}], "update_date": "2017-09-04", "authors_parsed": [["Amor", "Tatiana A.", ""], ["Lukovic", "Mirko", ""], ["Herrmann", "Hans J.", ""], ["Andrade", "Jose S.", "Jr"]]}, {"id": "1709.00374", "submitter": "David Pascucci Ph.D.", "authors": "David Pascucci, Clayton Hickey, Jorge Jovicich and Massimo Turatto", "title": "Independent circuits in basal ganglia and cortex for the processing of\n  reward and precision feedback", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "q-bio.NC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In order to understand human decision making it is necessary to understand\nhow the brain uses feedback to guide goal-directed behavior. The ventral\nstriatum (VS) appears to be a key structure in this function, responding\nstrongly to explicit reward feedback. However, recent results have also shown\nstriatal activity following correct task performance even in the absence of\nfeedback. This raises the possibility that, in addition to processing external\nfeedback, the dopamine-centered reward circuit might regulate endogenous\nreinforcement signals, like those triggered by satisfaction in accurate task\nperformance. Here we use functional magnetic resonance imaging (fMRI) to test\nthis idea. Participants completed a simple task that garnered both reward\nfeedback and feedback about the precision of performance. Importantly, the\ndesign was such that we could manipulate information about the precision of\nperformance within different levels of reward magnitude. Using parametric\nmodulation and functional connectivity analysis we identified brain regions\nsensitive to each of these signals. Our results show a double dissociation:\nfrontal and posterior cingulate regions responded to explicit reward but were\ninsensitive to task precision, whereas the dorsal striatum - and putamen in\nparticular - was insensitive to reward but responded strongly to precision\nfeedback in reward-present trials. Both types of feedback activated the VS, and\nsensitivity in this structure to precision feedback was predicted by\npersonality traits related to approach behavior and reward responsiveness. Our\nfindings shed new light on the role of specific brain regions in integrating\ndifferent sources of feedback to guide goal-directed behavior.\n", "versions": [{"version": "v1", "created": "Fri, 1 Sep 2017 15:55:36 GMT"}], "update_date": "2017-09-04", "authors_parsed": [["Pascucci", "David", ""], ["Hickey", "Clayton", ""], ["Jovicich", "Jorge", ""], ["Turatto", "Massimo", ""]]}, {"id": "1709.00455", "submitter": "Kelly Iarosz", "authors": "Rafael R. Borges, Fernando S. Borges, Ewandson L. Lameu, Paulo R.\n  Protachevicz, Kelly C. Iarosz, Iber\\^e L. Caldas, Ricardo L. Viana, Elbert E.\n  N. Macau, Murilo S. Baptista, Celso Grebogi, Antonio M. Batista", "title": "Synaptic Plasticity and Spike Synchronisation in Neuronal Networks", "comments": null, "journal-ref": null, "doi": "10.1007/s13538-017-0529-5", "report-no": null, "categories": "q-bio.NC physics.bio-ph", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Brain plasticity, also known as neuroplasticity, is a fundamental mechanism\nof neuronal adaptation in response to changes in the environment or due to\nbrain injury. In this review, we show our results about the effects of synaptic\nplasticity on neuronal networks composed by Hodgkin-Huxley neurons. We show\nthat the final topology of the evolved network depends crucially on the ratio\nbetween the strengths of the inhibitory and excitatory synapses. Excitation of\nthe same order of inhibition revels an evolved network that presents the\nrich-club phenomenon, well known to exist in the brain. For initial networks\nwith considerably larger inhibitory strengths, we observe the emergence of a\ncomplex evolved topology, where neurons sparsely connected to other neurons,\nalso a typical topology of the brain. The presence of noise enhances the\nstrength of both types of synapses, but if the initial network has synapses of\nboth natures with similar strengths. Finally, we show how the synchronous\nbehaviour of the evolved network will reflect its evolved topology.\n", "versions": [{"version": "v1", "created": "Fri, 1 Sep 2017 19:41:25 GMT"}], "update_date": "2017-10-25", "authors_parsed": [["Borges", "Rafael R.", ""], ["Borges", "Fernando S.", ""], ["Lameu", "Ewandson L.", ""], ["Protachevicz", "Paulo R.", ""], ["Iarosz", "Kelly C.", ""], ["Caldas", "Iber\u00ea L.", ""], ["Viana", "Ricardo L.", ""], ["Macau", "Elbert E. N.", ""], ["Baptista", "Murilo S.", ""], ["Grebogi", "Celso", ""], ["Batista", "Antonio M.", ""]]}, {"id": "1709.00583", "submitter": "Chaofei Hong", "authors": "Chaofei Hong", "title": "Training Spiking Neural Networks for Cognitive Tasks: A Versatile\n  Framework Compatible to Various Temporal Codes", "comments": "This work has been submitted to the IEEE for possible publication.\n  Copyright may be transferred without notice, after which this version will be\n  superseded", "journal-ref": null, "doi": null, "report-no": null, "categories": "q-bio.NC cs.NE stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Conventional modeling approaches have found limitations in matching the\nincreasingly detailed neural network structures and dynamics recorded in\nexperiments to the diverse brain functionalities. On another approach, studies\nhave demonstrated to train spiking neural networks for simple functions using\nsupervised learning. Here, we introduce a modified SpikeProp learning\nalgorithm, which achieved better learning stability in different activity\nstates. In addition, we show biological realistic features such as lateral\nconnections and sparse activities can be included in the network. We\ndemonstrate the versatility of this framework by implementing three well-known\ntemporal codes for different types of cognitive tasks, which are MNIST digits\nrecognition, spatial coordinate transformation, and motor sequence generation.\nMoreover, we find several characteristic features have evolved alongside the\ntask training, such as selective activity, excitatory-inhibitory balance, and\nweak pair-wise correlation. The coincidence between the self-evolved and\nexperimentally observed features indicates their importance on the brain\nfunctionality. Our results suggest a unified setting in which diverse cognitive\ncomputations and mechanisms can be studied.\n", "versions": [{"version": "v1", "created": "Sat, 2 Sep 2017 13:59:39 GMT"}], "update_date": "2017-09-05", "authors_parsed": [["Hong", "Chaofei", ""]]}, {"id": "1709.01116", "submitter": "Dimitrios Adamos Dr", "authors": "Fotis Kalaganis (1), Dimitrios A. Adamos (2 and 3), Nikos Laskaris (1\n  and 3) ((1) AIIA Lab, Department of Informatics, Aristotle University of\n  Thessaloniki, (2) School of Music Studies, Aristotle University of\n  Thessaloniki, (3) Neuroinformatics GRoup, Aristotle University of\n  Thessaloniki)", "title": "Musical NeuroPicks: a consumer-grade BCI for on-demand music streaming\n  services", "comments": null, "journal-ref": "Neurocomputing 2017", "doi": null, "report-no": null, "categories": "q-bio.NC cs.CY cs.HC cs.MM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We investigated the possibility of using a machine-learning scheme in\nconjunction with commercial wearable EEG-devices for translating listener's\nsubjective experience of music into scores that can be used in popular\non-demand music streaming services. Our study resulted into two variants,\ndiffering in terms of performance and execution time, and hence, subserving\ndistinct applications in online streaming music platforms. The first method,\nNeuroPicks, is extremely accurate but slower. It is based on the\nwell-established neuroscientific concepts of brainwave frequency bands,\nactivation asymmetry index and cross frequency coupling (CFC). The second\nmethod, NeuroPicksVQ, offers prompt predictions of lower credibility and relies\non a custom-built version of vector quantization procedure that facilitates a\nnovel parameterization of the music-modulated brainwaves. Beyond the feature\nengineering step, both methods exploit the inherent efficiency of extreme\nlearning machines (ELMs) so as to translate, in a personalized fashion, the\nderived patterns into a listener's score. NeuroPicks method may find\napplications as an integral part of contemporary music recommendation systems,\nwhile NeuroPicksVQ can control the selection of music tracks. Encouraging\nexperimental results, from a pragmatic use of the systems, are presented.\n", "versions": [{"version": "v1", "created": "Mon, 4 Sep 2017 18:55:35 GMT"}], "update_date": "2017-09-06", "authors_parsed": [["Kalaganis", "Fotis", "", "2 and 3"], ["Adamos", "Dimitrios A.", "", "2 and 3"], ["Laskaris", "Nikos", "", "1\n  and 3"]]}, {"id": "1709.01437", "submitter": "Sina Tootoonian", "authors": "Sina Tootoonian and Peter Latham", "title": "Sparse connectivity for MAP inference in linear models using sister\n  mitral cells", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "q-bio.NC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Sensory processing is hard because the variables of interest are encoded in\nspike trains in a relatively complex way. A major goal in sensory processing is\nto understand how the brain extracts those variables. Here we revisit a common\nencoding model in which variables are encoded linearly. Although there are\ntypically more variables than neurons, this problem is still solvable because\nonly a small number of variables appear at any one time (sparse prior).\nHowever, previous solutions usually require all-to-all connectivity,\ninconsistent with the sparse connectivity seen in the brain. Here we propose a\nprincipled algorithm that provably reaches the MAP inference solution but using\nsparse connectivity. Our algorithm is inspired by the mouse olfactory bulb, but\nour approach is general enough to apply to other modalities; in addition, it\nshould be possible to extend it to nonlinear encoding models.\n", "versions": [{"version": "v1", "created": "Tue, 5 Sep 2017 15:02:02 GMT"}], "update_date": "2017-09-06", "authors_parsed": [["Tootoonian", "Sina", ""], ["Latham", "Peter", ""]]}, {"id": "1709.02008", "submitter": "Zachary Kilpatrick PhD", "authors": "Zachary P Kilpatrick and Daniel B Poll", "title": "Neural field model of memory-guided search", "comments": "17 pages, 10 figures", "journal-ref": "Phys. Rev. E 96, 062411 (2017)", "doi": "10.1103/PhysRevE.96.062411", "report-no": null, "categories": "q-bio.NC nlin.PS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Many organisms can remember locations they have previously visited during a\nsearch. Visual search experiments have shown exploration is guided away from\nthese locations, reducing the overlap of the search path before finding a\nhidden target. We develop and analyze a two-layer neural field model that\nencodes positional information during a search task. A position-encoding layer\nsustains a bump attractor corresponding to the searching agent's current\nlocation, and search is modeled by velocity input that propagates the bump. A\nmemory layer sustains persistent activity bounded by a wave front, whose edges\nexpand in response to excitatory input from the position layer. Search can then\nbe biased in response to remembered locations, influencing velocity inputs to\nthe position layer. Asymptotic techniques are used to reduce the dynamics of\nour model to a low-dimensional system of equations that track the bump position\nand front boundary. Performance is compared for different target-finding tasks.\n", "versions": [{"version": "v1", "created": "Wed, 6 Sep 2017 21:34:57 GMT"}], "update_date": "2017-12-27", "authors_parsed": [["Kilpatrick", "Zachary P", ""], ["Poll", "Daniel B", ""]]}, {"id": "1709.02323", "submitter": "Andreea Diaconescu Dr.", "authors": "Andreea O. Diaconescu, Vladimir Litvak, Christoph Mathys, Lars Kasper,\n  Karl J. Friston and Klaas E. Stephan", "title": "A computational hierarchy in human cortex", "comments": "34 pages, 5 figures, 3 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "q-bio.NC q-bio.QM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Hierarchies feature prominently in anatomical accounts of cortical\norganisation. An open question is which computational (algorithmic) processes\nare implemented by these hierarchies. One renowned hypothesis is that cortical\nhierarchies implement a model of the world's causal structure and serve to\ninfer environmental states from sensory inputs. This view, which casts\nperception as hierarchical Bayesian inference, has become a highly influential\nconcept in both basic and clinical neuroscience. So far, however, a direct\ncorrespondence between the predicted order of hierarchical Bayesian\ncomputations and the sequence of evoked neuronal activity has not been\ndemonstrated. Here, we present evidence for this correspondence from\nneuroimaging and electrophysiological data in healthy volunteers. Trial-wise\nsequences of hierarchical computations were inferred from participants'\nbehaviour during a social learning task that required multi-level inference\nabout intentions. We found that the temporal sequence of neuronal activity\nmatched the order of computations as predicted by the theory. These findings\nprovide strong evidence for the operation of hierarchical Bayesian inference in\nhuman cortex. Furthermore, our approach offers a novel strategy for the\ncombined computational-physiological phenotyping of patients with disorders of\nperception, such as schizophrenia or autism.\n", "versions": [{"version": "v1", "created": "Thu, 7 Sep 2017 16:02:04 GMT"}], "update_date": "2017-09-08", "authors_parsed": [["Diaconescu", "Andreea O.", ""], ["Litvak", "Vladimir", ""], ["Mathys", "Christoph", ""], ["Kasper", "Lars", ""], ["Friston", "Karl J.", ""], ["Stephan", "Klaas E.", ""]]}, {"id": "1709.02325", "submitter": "Michael Vaiana", "authors": "Michael Vaiana and Sarah Muldoon", "title": "Multilayer Brain Networks", "comments": null, "journal-ref": null, "doi": "10.1007/s00332-017-9436-8", "report-no": null, "categories": "q-bio.NC physics.soc-ph", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The field of neuroscience is facing an unprecedented expanse in the volume\nand diversity of available data. Traditionally, network models have provided\nkey insights into the structure and function of the brain. With the advent of\nbig data in neuroscience, both more sophisticated models capable of\ncharacterizing the increasing complexity of the data and novel methods of\nquantitative analysis are needed. Recently multilayer networks, a mathematical\nextension of traditional networks, have gained increasing popularity in\nneuroscience due to their ability to capture the full information of\nmulti-model, multi-scale, spatiotemporal data sets. Here, we review multilayer\nnetworks and their applications in neuroscience, showing how incorporating the\nmultilayer framework into network neuroscience analysis has uncovered\npreviously hidden features of brain networks. We specifically highlight the use\nof multilayer networks to model disease, structure-function relationships,\nnetwork evolution, and link multi-scale data. Finally, we close with a\ndiscussion of promising new directions of multilayer network neuroscience\nresearch and propose a modified definition of multilayer networks designed to\nunite and clarify the use of the multilayer formalism in describing real-world\nsystems.\n", "versions": [{"version": "v1", "created": "Thu, 7 Sep 2017 16:03:48 GMT"}], "update_date": "2018-02-14", "authors_parsed": [["Vaiana", "Michael", ""], ["Muldoon", "Sarah", ""]]}, {"id": "1709.02341", "submitter": "Kai Ueltzh\\\"offer", "authors": "Kai Ueltzh\\\"offer", "title": "Deep Active Inference", "comments": null, "journal-ref": null, "doi": "10.1007/s00422-018-0785-7", "report-no": null, "categories": "q-bio.NC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This work combines the free energy principle from cognitive neuroscience and\nthe ensuing active inference dynamics with recent advances in variational\ninference in deep generative models, and evolution strategies to introduce the\n\"deep active inference\" agent. This agent minimises a variational free energy\nbound on the average surprise of its sensations, which is motivated by a\nhomeostatic argument. It does so by optimising the parameters of a generative\nlatent variable model of its sensory inputs, together with a variational\ndensity approximating the posterior distribution over the latent variables,\ngiven its observations, and by acting on its environment to actively sample\ninput that is likely under this generative model. The internal dynamics of the\nagent are implemented using deep and recurrent neural networks, as used in\nmachine learning, making the deep active inference agent a scalable and very\nflexible class of active inference agent. Using the mountain car problem, we\nshow how goal directed behaviour can be implemented by defining appropriate\npriors on the latent states in the agent's model. Furthermore, we show that the\ndeep active inference agent can learn a generative model of the environment,\nwhich can be sampled from to understand the agent's beliefs about the\nenvironment and its interaction therewith.\n", "versions": [{"version": "v1", "created": "Thu, 7 Sep 2017 16:36:52 GMT"}, {"version": "v2", "created": "Sun, 17 Sep 2017 18:14:23 GMT"}, {"version": "v3", "created": "Tue, 18 Sep 2018 19:55:07 GMT"}, {"version": "v4", "created": "Wed, 10 Oct 2018 13:42:11 GMT"}, {"version": "v5", "created": "Thu, 11 Oct 2018 17:38:26 GMT"}], "update_date": "2018-10-24", "authors_parsed": [["Ueltzh\u00f6ffer", "Kai", ""]]}, {"id": "1709.02369", "submitter": "Marianna La Rocca Mrs", "authors": "Nicola Amoroso, Marianna La Rocca, Stefania Bruno, Tommaso Maggipinto,\n  Alfonso Monaco, Roberto Bellotti, Sabina Tangaro", "title": "Brain structural connectivity atrophy in Alzheimer's disease", "comments": "16 pages, 8 figures, 2 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "physics.med-ph q-bio.NC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Analysis and quantification of brain structural changes, using Magnetic\nresonance imaging (MRI), are increasingly used to define novel biomarkers of\nbrain pathologies, such as Alzheimer's disease (AD). Network-based models of\nthe brain have shown that both local and global topological properties can\nreveal patterns of disease propagation. On the other hand, intra-subject\ndescriptions cannot exploit the whole information context, accessible through\ninter-subject comparisons. To address this, we developed a novel approach,\nwhich models brain structural connectivity atrophy with a multiplex network and\nsummarizes it within a classification score. On an independent dataset\nmultiplex networks were able to correctly segregate, from normal controls (NC),\nAD patients and subjects with mild cognitive impairment that will convert to AD\n(cMCI) with an accuracy of, respectively, $0.86 \\pm 0.01$ and $0.84 \\pm 0.01$.\nThe model also shows that illness effects are maximally detected by parceling\nthe brain in equal volumes of $3000$ $mm^3$ (\"patches\"), without any $a$\n$priori$ segmentation based on anatomical features. A direct comparison to\nstandard voxel-based morphometry on the same dataset showed that the multiplex\nnetwork approach had higher sensitivity. This method is general and can have\ntwofold potential applications: providing a reliable tool for clinical trials\nand a disease signature of neurodegenerative pathologies.\n", "versions": [{"version": "v1", "created": "Thu, 7 Sep 2017 17:41:42 GMT"}, {"version": "v2", "created": "Sat, 9 Sep 2017 16:12:58 GMT"}], "update_date": "2017-09-12", "authors_parsed": [["Amoroso", "Nicola", ""], ["La Rocca", "Marianna", ""], ["Bruno", "Stefania", ""], ["Maggipinto", "Tommaso", ""], ["Monaco", "Alfonso", ""], ["Bellotti", "Roberto", ""], ["Tangaro", "Sabina", ""]]}, {"id": "1709.02443", "submitter": "Juan Manuel Romero", "authors": "Erick J. L\\'opez-S\\'anchez, Juan M. Romero and Huitzilin\n  Y\\'epez-Mart\\'inez", "title": "Fractional cable equation for general geometry, a model of axons with\n  swellings and anomalous diffusion", "comments": "18 pages, 8 figures. Accepted for publication in Physical Review E", "journal-ref": "Phys. Rev. E 96, 032411 (2017)", "doi": "10.1103/PhysRevE.96.032411", "report-no": null, "categories": "q-bio.NC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Different experimental studies have reported anomalous diffusion in brain\ntissues and notably this anomalous diffusion is expressed through fractional\nderivatives. Axons are important to understand neurodegenerative diseases such\nas multiple sclerosis, Alzheimer's disease and Parkinson's disease. Indeed,\nabnormal accumulation of proteins and organelles in axons is a hallmark feature\nof these diseases. The diffusion in the axons can become to anomalous as a\nresult from this abnormality. In this case the voltage propagation in axons is\naffected. Another hallmark feature of different neurodegenerative diseases is\ngiven by discrete swellings along the axon. In order to model the voltage\npropagation in axons with anomalous diffusion and swellings, in this paper we\npropose a fractional cable equation for general geometry. This generalized\nequation depends on fractional parameters and geometric quantities such as the\ncurvature and torsion of the cable. For a cable with a constant radius we show\nthat the voltage decreases when the fractional effect increases. In cables with\nswellings we find that when the fractional effect or the swelling radius\nincrease, the voltage decreases. A similar behavior is obtained when the number\nof swellings and the fractional effect increase. Moreover, we find that when\nthe radius swelling (or the number of swellings) and the fractional effect\nincrease at the same time, the voltage dramatically decreases.\n", "versions": [{"version": "v1", "created": "Thu, 7 Sep 2017 20:35:38 GMT"}], "update_date": "2017-09-27", "authors_parsed": [["L\u00f3pez-S\u00e1nchez", "Erick J.", ""], ["Romero", "Juan M.", ""], ["Y\u00e9pez-Mart\u00ednez", "Huitzilin", ""]]}, {"id": "1709.02470", "submitter": "J\\'er\\^ome Tubiana", "authors": "Simona Cocco, R\\'emi Monasson, Lorenzo Posani, Sophie Rosay,\n  J\\'er\\^ome Tubiana", "title": "Statistical Physics and Representations in Real and Artificial Neural\n  Networks", "comments": null, "journal-ref": null, "doi": "10.1016/j.physa.2017.11.153", "report-no": null, "categories": "physics.data-an q-bio.NC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This document presents the material of two lectures on statistical physics\nand neural representations, delivered by one of us (R.M.) at the Fundamental\nProblems in Statistical Physics XIV summer school in July 2017. In a first\npart, we consider the neural representations of space (maps) in the\nhippocampus. We introduce an extension of the Hopfield model, able to store\nmultiple spatial maps as continuous, finite-dimensional attractors. The phase\ndiagram and dynamical properties of the model are analyzed. We then show how\nspatial representations can be dynamically decoded using an effective Ising\nmodel capturing the correlation structure in the neural data, and compare\napplications to data obtained from hippocampal multi-electrode recordings and\nby (sub)sampling our attractor model. In a second part, we focus on the problem\nof learning data representations in machine learning, in particular with\nartificial neural networks. We start by introducing data representations\nthrough some illustrations. We then analyze two important algorithms, Principal\nComponent Analysis and Restricted Boltzmann Machines, with tools from\nstatistical physics.\n", "versions": [{"version": "v1", "created": "Thu, 7 Sep 2017 22:11:41 GMT"}], "update_date": "2018-06-13", "authors_parsed": [["Cocco", "Simona", ""], ["Monasson", "R\u00e9mi", ""], ["Posani", "Lorenzo", ""], ["Rosay", "Sophie", ""], ["Tubiana", "J\u00e9r\u00f4me", ""]]}, {"id": "1709.02684", "submitter": "Jonathan George", "authors": "Jonathan K. George, Cesare Soci, Volker J. Sorger", "title": "Identifying Mirror Symmetry Density with Delay in Spiking Neural\n  Networks", "comments": "8 pages, 8 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "q-bio.NC cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The ability to rapidly identify symmetry and anti-symmetry is an essential\nattribute of intelligence. Symmetry perception is a central process in human\nvision and may be key to human 3D visualization. While previous work in\nunderstanding neuron symmetry perception has concentrated on the neuron as an\nintegrator, here we show how the coincidence detecting property of the spiking\nneuron can be used to reveal symmetry density in spatial data. We develop a\nmethod for synchronizing symmetry-identifying spiking artificial neural\nnetworks to enable layering and feedback in the network. We show a method for\nbuilding a network capable of identifying symmetry density between sets of data\nand present a digital logic implementation demonstrating an 8x8\nleaky-integrate-and-fire symmetry detector in a field programmable gate array.\nOur results show that the efficiencies of spiking neural networks can be\nharnessed to rapidly identify symmetry in spatial data with applications in\nimage processing, 3D computer vision, and robotics.\n", "versions": [{"version": "v1", "created": "Fri, 25 Aug 2017 17:18:16 GMT"}], "update_date": "2017-09-11", "authors_parsed": [["George", "Jonathan K.", ""], ["Soci", "Cesare", ""], ["Sorger", "Volker J.", ""]]}, {"id": "1709.02719", "submitter": "Kelly Iarosz", "authors": "P. R. Protachevicz, F. S. Borges, K. C. Iarosz, I. L. Caldas, M. S.\n  Baptista, R. L. Viana, E. L. Lameu, E. E. N. Macau, A. M. Batista", "title": "How synapses can enhance sensibility of a neural network", "comments": null, "journal-ref": null, "doi": "10.1016/j.physa.2017.11.034", "report-no": null, "categories": "physics.bio-ph q-bio.NC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this work, we study the dynamic range in a neuronal network modelled by\ncellular automaton. We consider deterministic and non-deterministic rules to\nsimulate electrical and chemical synapses. Chemical synapses have an intrinsic\ntime-delay and are susceptible to parameter variations guided by learning\nHebbian rules of behaviour. Our results show that chemical synapses can\nabruptly enhance sensibility of the neural network, a manifestation that can\nbecome even more predominant if learning rules of evolution are applied to the\nchemical synapses.\n", "versions": [{"version": "v1", "created": "Fri, 8 Sep 2017 14:31:18 GMT"}], "update_date": "2018-01-17", "authors_parsed": [["Protachevicz", "P. R.", ""], ["Borges", "F. S.", ""], ["Iarosz", "K. C.", ""], ["Caldas", "I. L.", ""], ["Baptista", "M. S.", ""], ["Viana", "R. L.", ""], ["Lameu", "E. L.", ""], ["Macau", "E. E. N.", ""], ["Batista", "A. M.", ""]]}, {"id": "1709.02852", "submitter": "Steven Tompson", "authors": "Steven H. Tompson, Ari E. Kahn, Emily B. Falk, Jean M. Vettel,\n  Danielle S. Bassett", "title": "Individual Differences in Learning Social and Non-Social Network\n  Structures", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "q-bio.NC", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Learning about complex associations between pieces of information enables\nindividuals to quickly adjust their expectations and develop mental models.\nYet, the degree to which humans can learn higher-order information about\ncomplex associations is not well understood; nor is it known whether the\nlearning process differs for social and non-social information. Here, we employ\na paradigm in which the order of stimulus presentation forms temporal\nassociations between the stimuli, collectively constituting a complex network\nstructure. We examined individual differences in the ability to learn network\ntopology for which stimuli were social versus non-social. Although participants\nwere able to learn both social and non-social networks, their performance in\nsocial network learning was uncorrelated with their performance in non-social\nnetwork learning. Importantly, social traits, including social orientation and\nperspective-taking, uniquely predicted the learning of social networks but not\nthe learning of non-social networks. Taken together, our results suggest that\nthe process of learning higher-order structure in social networks is\nindependent from the process of learning higher-order structure in non-social\nnetworks. Our study design provides a promising approach to identify\nneurophysiological drivers of social network versus non-social network\nlearning, extending our knowledge about the impact of individual differences on\nthese learning processes. Implications for how people learn and adapt to new\nsocial contexts that require integration into a new social network are\ndiscussed.\n", "versions": [{"version": "v1", "created": "Fri, 8 Sep 2017 20:31:46 GMT"}], "update_date": "2017-09-12", "authors_parsed": [["Tompson", "Steven H.", ""], ["Kahn", "Ari E.", ""], ["Falk", "Emily B.", ""], ["Vettel", "Jean M.", ""], ["Bassett", "Danielle S.", ""]]}, {"id": "1709.03000", "submitter": "Ari Kahn", "authors": "Ari E. Kahn, Elisabeth A. Karuza, Jean M. Vettel, Danielle S. Bassett", "title": "Network constraints on learnability of probabilistic motor sequences", "comments": "29 pages, 4 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "q-bio.NC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Human learners are adept at grasping the complex relationships underlying\nincoming sequential input. In the present work, we formalize complex\nrelationships as graph structures derived from temporal associations in motor\nsequences. Next, we explore the extent to which learners are sensitive to key\nvariations in the topological properties inherent to those graph structures.\nParticipants performed a probabilistic motor sequence task in which the order\nof button presses was determined by the traversal of graphs with modular,\nlattice-like, or random organization. Graph nodes each represented a unique\nbutton press and edges represented a transition between button presses. Results\nindicate that learning, indexed here by participants' response times, was\nstrongly mediated by the graph's meso-scale organization, with modular graphs\nbeing associated with shorter response times than random and lattice graphs.\nMoreover, variations in a node's number of connections (degree) and a node's\nrole in mediating long-distance communication (betweenness centrality) impacted\ngraph learning, even after accounting for level of practice on that node. These\nresults demonstrate that the graph architecture underlying temporal sequences\nof stimuli fundamentally constrains learning, and moreover that tools from\nnetwork science provide a valuable framework for assessing how learners encode\ncomplex, temporally structured information.\n", "versions": [{"version": "v1", "created": "Sat, 9 Sep 2017 20:13:18 GMT"}, {"version": "v2", "created": "Wed, 25 Apr 2018 20:51:44 GMT"}, {"version": "v3", "created": "Tue, 30 Oct 2018 15:34:12 GMT"}], "update_date": "2018-10-31", "authors_parsed": [["Kahn", "Ari E.", ""], ["Karuza", "Elisabeth A.", ""], ["Vettel", "Jean M.", ""], ["Bassett", "Danielle S.", ""]]}, {"id": "1709.03113", "submitter": "Clemens Kornd\\\"orfer", "authors": "Clemens Kornd\\\"orfer, Ekkehard Ullner, Jordi Garc\\'ia-Ojalvo, Gordon\n  Pipa", "title": "Cortical Spike Synchrony as a Measure of Input Familiarity", "comments": "fix order in figure 1 to match drawing", "journal-ref": "Neural Computation 2017 29:9, 2491-2510", "doi": "10.1162/neco_a_00987", "report-no": null, "categories": "q-bio.NC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Spike synchrony, which occurs in various cortical areas in response to\nspecific perception, action and memory tasks, has sparked a long-standing\ndebate on the nature of temporal organization in cortex. One prominent view is\nthat this type of synchrony facilitates the binding or grouping of separate\nstimulus components. We argue instead for a more general function: A measure of\nthe prior probability of incoming stimuli, implemented by long-range,\nhorizontal, intra-cortical connections. We show that networks of this kind --\npulse-coupled excitatory spiking networks in a noisy environment -- can provide\na sufficient substrate for stimulus-dependent spike synchrony. This allows a\nquick (few spikes) estimate of the match between inputs and the input history\nas encoded in the network structure. Given the ubiquity of small, strongly\nexcitatory subnetworks in cortex, we thus propose that many experimental\nobservations of spike synchrony can be viewed as signs of input patterns that\nresemble long-term experience, i.e. patterns of high prior probability.\n", "versions": [{"version": "v1", "created": "Sun, 10 Sep 2017 15:21:19 GMT"}, {"version": "v2", "created": "Fri, 15 Dec 2017 13:25:54 GMT"}], "update_date": "2017-12-18", "authors_parsed": [["Kornd\u00f6rfer", "Clemens", ""], ["Ullner", "Ekkehard", ""], ["Garc\u00eda-Ojalvo", "Jordi", ""], ["Pipa", "Gordon", ""]]}, {"id": "1709.03114", "submitter": "Yu. I. Manin", "authors": "Dmitrii Yu. Manin, Yuri I. Manin", "title": "Cognitive networks: brains, internet, and civilizations", "comments": "16 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.SI physics.soc-ph q-bio.NC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this short essay, we discuss some basic features of cognitive activity at\nseveral different space-time scales: from neural networks in the brain to\ncivilizations. One motivation for such comparative study is its heuristic\nvalue. Attempts to better understand the functioning of \"wetware\" involved in\ncognitive activities of central nervous system by comparing it with a computing\ndevice have a long tradition. We suggest that comparison with Internet might be\nmore adequate. We briefly touch upon such subjects as encoding, compression,\nand Saussurean trichotomy langue/langage/parole in various environments.\n", "versions": [{"version": "v1", "created": "Sun, 10 Sep 2017 15:26:34 GMT"}], "update_date": "2017-09-12", "authors_parsed": [["Manin", "Dmitrii Yu.", ""], ["Manin", "Yuri I.", ""]]}, {"id": "1709.03978", "submitter": "Marinho Lopes", "authors": "Marinho A. Lopes, KyoungEun Lee, Alexander V. Goltsev", "title": "A neuronal network model of interictal and recurrent ictal activity", "comments": "9 pages, 7 figures", "journal-ref": "Phys. Rev. E 96, 062412 (2017)", "doi": "10.1103/PhysRevE.96.062412", "report-no": null, "categories": "q-bio.NC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a neuronal network model which undergoes a saddle-node bifurcation\non an invariant circle as the mechanism of the transition from the interictal\nto the ictal (seizure) state. In the vicinity of this transition, the model\ncaptures important dynamical features of both interictal and ictal states. We\nstudy the nature of interictal spikes and early warnings of the transition\npredicted by this model. We further demonstrate that recurrent seizures emerge\ndue to the interaction between two networks.\n", "versions": [{"version": "v1", "created": "Mon, 11 Sep 2017 22:39:35 GMT"}], "update_date": "2017-12-27", "authors_parsed": [["Lopes", "Marinho A.", ""], ["Lee", "KyoungEun", ""], ["Goltsev", "Alexander V.", ""]]}, {"id": "1709.04090", "submitter": "Yanjun  Qi Dr.", "authors": "Chandan Singh, Beilun Wang, Yanjun Qi", "title": "A Constrained, Weighted-L1 Minimization Approach for Joint Discovery of\n  Heterogeneous Neural Connectivity Graphs", "comments": "8 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "q-bio.NC cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Determining functional brain connectivity is crucial to understanding the\nbrain and neural differences underlying disorders such as autism. Recent\nstudies have used Gaussian graphical models to learn brain connectivity via\nstatistical dependencies across brain regions from neuroimaging. However,\nprevious studies often fail to properly incorporate priors tailored to\nneuroscience, such as preferring shorter connections. To remedy this problem,\nthe paper here introduces a novel, weighted-$\\ell_1$, multi-task graphical\nmodel (W-SIMULE). This model elegantly incorporates a flexible prior, along\nwith a parallelizable formulation. Additionally, W-SIMULE extends the\noften-used Gaussian assumption, leading to considerable performance increases.\nHere, applications to fMRI data show that W-SIMULE succeeds in determining\nfunctional connectivity in terms of (1) log-likelihood, (2) finding edges that\ndifferentiate groups, and (3) classifying different groups based on their\nconnectivity, achieving 58.6\\% accuracy on the ABIDE dataset. Having\nestablished W-SIMULE's effectiveness, it links four key areas to autism, all of\nwhich are consistent with the literature. Due to its elegant domain adaptivity,\nW-SIMULE can be readily applied to various data types to effectively estimate\nconnectivity.\n", "versions": [{"version": "v1", "created": "Wed, 13 Sep 2017 00:05:20 GMT"}, {"version": "v2", "created": "Thu, 21 Sep 2017 15:18:23 GMT"}], "update_date": "2017-09-22", "authors_parsed": [["Singh", "Chandan", ""], ["Wang", "Beilun", ""], ["Qi", "Yanjun", ""]]}, {"id": "1709.04300", "submitter": "Archana Ram", "authors": "Archana Ram, Andrew Lo", "title": "Is Smaller Better: A Proposal To Consider Bacteria For Biologically\n  Inspired Modeling", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "q-bio.NC cs.ET", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Bacteria are easily characterizable model organisms with an impressively\ncomplicated set of capabilities. Among their capabilities is quorum sensing, a\ndetailed cell-cell signaling system that may have a common origin with\neukaryotic cell-cell signaling. Not only are the two phenomena similar, but\nquorum sensing, as is the case with any bacterial phenomenon when compared to\neukaryotes, is also easier to study in depth than eukaryotic cell-cell\nsignaling. This ease of study is a contrast to the only partially understood\ncellular dynamics of neurons. Here we review the literature on the strikingly\nneuron-like qualities of bacterial colonies and biofilms, including ion-based\nand hormonal signaling, and action potential-like behavior. This allows them to\nfeasibly act as an analog for neurons that could produce more detailed and more\naccurate biologically-based computational models. Using bacteria as the basis\nfor biologically feasible computational models may allow models to better\nharness the tremendous ability of biological organisms to make decisions and\nprocess information. Additionally, principles gleaned from bacterial function\nhave the potential to influence computational efforts divorced from biology,\njust as neuronal function has in the abstract influenced countless machine\nlearning efforts.\n", "versions": [{"version": "v1", "created": "Tue, 12 Sep 2017 00:16:30 GMT"}], "update_date": "2017-09-14", "authors_parsed": [["Ram", "Archana", ""], ["Lo", "Andrew", ""]]}, {"id": "1709.04550", "submitter": "Jinhui Yu", "authors": "Jinhui Yu, Kailin Wu, Kang Zhang, Xianjun Sam Zheng", "title": "A Computational Model of Afterimages based on Simultaneous and\n  Successive Contrasts", "comments": "10 pages, 6 figues", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV q-bio.NC", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Negative afterimage appears in our vision when we shift our gaze from an over\nstimulated original image to a new area with a uniform color. The colors of\nnegative afterimages differ from the old stimulating colors in the original\nimage when the color in the new area is either neutral or chromatic. The\ninteraction between stimulating colors in the test and inducing field in the\noriginal image changes our color perception due to simultaneous contrast, and\nthe interaction between changed colors perceived in the previously-viewed field\nand the color in the currently-viewed field also affects our perception of\ncolors in negative afterimages due to successive contrast. Based on these\nobservations we propose a computational model to estimate colors of negative\nafterimages in more general cases where the original stimulating color in the\ntest field is chromatic, and the original stimulating color in the inducing\nfield and the new stimulating color can be either neutral or chromatic. We\nvalidate our model with human experiments.\n", "versions": [{"version": "v1", "created": "Wed, 13 Sep 2017 21:54:58 GMT"}], "update_date": "2017-09-15", "authors_parsed": [["Yu", "Jinhui", ""], ["Wu", "Kailin", ""], ["Zhang", "Kang", ""], ["Zheng", "Xianjun Sam", ""]]}, {"id": "1709.04654", "submitter": "Randall O'Reilly", "authors": "Randall C. O'Reilly, Dean R. Wyatte, and John Rohrlich", "title": "Deep Predictive Learning: A Comprehensive Model of Three Visual Streams", "comments": "64 pages, 24 figures, 291 references. Submitted for publication", "journal-ref": null, "doi": null, "report-no": null, "categories": "q-bio.NC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  How does the neocortex learn and develop the foundations of all our\nhigh-level cognitive abilities? We present a comprehensive framework spanning\nbiological, computational, and cognitive levels, with a clear theoretical\ncontinuity between levels, providing a coherent answer directly supported by\nextensive data at each level. Learning is based on making predictions about\nwhat the senses will report at 100 msec (alpha frequency) intervals, and\nadapting synaptic weights to improve prediction accuracy. The pulvinar nucleus\nof the thalamus serves as a projection screen upon which predictions are\ngenerated, through deep-layer 6 corticothalamic inputs from multiple brain\nareas and levels of abstraction. The sparse driving inputs from layer 5\nintrinsic bursting neurons provide the target signal, and the temporal\ndifference between it and the prediction reverberates throughout the cortex,\ndriving synaptic changes that approximate error backpropagation, using only\nlocal activation signals in equations derived directly from a detailed\nbiophysical model. In vision, predictive learning requires a\ncarefully-organized developmental progression and anatomical organization of\nthree pathways (What, Where, and What * Where), according to two central\nprinciples: top-down input from compact, high-level, abstract representations\nis essential for accurate prediction of low-level sensory inputs; and the\ncollective, low-level prediction error must be progressively and\nopportunistically partitioned to enable extraction of separable factors that\ndrive the learning of further high-level abstractions. Our model self-organized\nsystematic invariant object representations of 100 different objects from\nsimple movies, accounts for a wide range of data, and makes many testable\npredictions.\n", "versions": [{"version": "v1", "created": "Thu, 14 Sep 2017 08:02:37 GMT"}], "update_date": "2017-09-15", "authors_parsed": [["O'Reilly", "Randall C.", ""], ["Wyatte", "Dean R.", ""], ["Rohrlich", "John", ""]]}, {"id": "1709.04974", "submitter": "Vince Grolmusz", "authors": "Balazs Szalkai, Balint Varga and Vince Grolmusz", "title": "Comparing Advanced Graph-Theoretical Parameters of the Connectomes of\n  the Lobes of the Human Brain", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "q-bio.NC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Deep, classical graph-theoretical parameters, like the size of the minimum\nvertex cover, the chromatic number, or the eigengap of the adjacency matrix of\nthe graph were studied widely by mathematicians in the last century. Most\nresearchers today study much simpler parameters of braingraphs or connectomes\nwhich were defined in the last twenty years for enormous networks -- like the\ngraph of the World Wide Web -- with hundreds of millions of nodes. Since the\nconnectomes, describing the connections of the human brain, typically contain\nseveral hundred vertices today, one can compute and analyze the much deeper,\nharder-to-compute classical graph parameters for these, relatively small graphs\nof the brain. This deeper approach has proven to be very successful in the\ncomparison of the connectomes of the sexes in our earlier works: we have shown\nthat graph parameters, deeply characterizing the graph connectivity are\nsignificantly better in women's connectomes than in men's. In the present\ncontribution we compare numerous graph parameters in the three largest lobes\n--- frontal, parietal, temporal --- and in both hemispheres of the brain. We\napply the diffusion weighted imaging data of 423 subjects of the NIH-funded\nHuman Connectome Project, and present some findings, never described before,\nincluding that the right parietal lobe contains significantly more edges, has\nhigher average degree, density, larger minimum vertex cover and Hoffman bound\nthan the left parietal lobe. Similar advantages in the deep graph connectivity\nproperties are hold for the left frontal vs. the right frontal and for the\nright temporal vs. the left temporal lobes.\n", "versions": [{"version": "v1", "created": "Thu, 14 Sep 2017 20:59:32 GMT"}], "update_date": "2017-09-18", "authors_parsed": [["Szalkai", "Balazs", ""], ["Varga", "Balint", ""], ["Grolmusz", "Vince", ""]]}, {"id": "1709.05022", "submitter": "John Medaglia", "authors": "John D. Medaglia, Denise Y. Harvey, Nicole White, Danielle S. Bassett,\n  Roy H. Hamilton", "title": "Network Controllability in the IFG Relates to Controlled Language\n  Variability and Susceptibility to TMS", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "q-bio.NC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In language production, humans are confronted with considerable word\nselection demands. Often, we must select a word from among similar, acceptable,\nand competing alternative words in order to construct a sentence that conveys\nan intended meaning. In recent years, the left inferior frontal gyrus (LIFG)\nhas been identified as critical to this ability. Despite a recent emphasis on\nnetwork approaches to understanding language, how the LIFG interacts with the\nbrain's complex networks to facilitate controlled language performance remains\nunknown. Here, we take a novel approach to understand word selection as a\nnetwork control process in the brain. Using an anatomical brain network derived\nfrom high-resolution diffusion spectrum imaging (DSI), we computed network\ncontrollability underlying the site of transcranial magnetic stimulation in the\nLIFG between administrations of two word selection tasks. We find that a\nstatistic that quantifies the LIFG's theoretically predicted control of\ndifficult-to-reach states explains vulnerability to TMS in language tasks that\nvary in response (cognitive control) demands: open-response (word generation)\nvs. closed-response (number naming) tasks. Moreover, we find that a statistic\nthat quantifies the LIFG's theoretically predicted control of communication\nacross modules in the human connectome explains TMS-induced changes in\nopen-response language task performance only. These findings establish a link\nbetween network controllability, cognitive function, and TMS effects.\n", "versions": [{"version": "v1", "created": "Fri, 15 Sep 2017 00:59:08 GMT"}, {"version": "v2", "created": "Thu, 25 Jan 2018 20:04:46 GMT"}, {"version": "v3", "created": "Tue, 30 Jan 2018 20:53:30 GMT"}], "update_date": "2018-02-01", "authors_parsed": [["Medaglia", "John D.", ""], ["Harvey", "Denise Y.", ""], ["White", "Nicole", ""], ["Bassett", "Danielle S.", ""], ["Hamilton", "Roy H.", ""]]}, {"id": "1709.05601", "submitter": "Arend Hintze", "authors": "Arend Hintze, Jeffrey A. Edlund, Randal S. Olson, David B. Knoester,\n  Jory Schossau, Larissa Albantakis, Ali Tehrani-Saleh, Peter Kvam, Leigh\n  Sheneman, Heather Goldsby, Clifford Bohm, Christoph Adami", "title": "Markov Brains: A Technical Introduction", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI q-bio.NC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Markov Brains are a class of evolvable artificial neural networks (ANN). They\ndiffer from conventional ANNs in many aspects, but the key difference is that\ninstead of a layered architecture, with each node performing the same function,\nMarkov Brains are networks built from individual computational components.\nThese computational components interact with each other, receive inputs from\nsensors, and control motor outputs. The function of the computational\ncomponents, their connections to each other, as well as connections to sensors\nand motors are all subject to evolutionary optimization. Here we describe in\ndetail how a Markov Brain works, what techniques can be used to study them, and\nhow they can be evolved.\n", "versions": [{"version": "v1", "created": "Sun, 17 Sep 2017 03:12:06 GMT"}], "update_date": "2018-01-22", "authors_parsed": [["Hintze", "Arend", ""], ["Edlund", "Jeffrey A.", ""], ["Olson", "Randal S.", ""], ["Knoester", "David B.", ""], ["Schossau", "Jory", ""], ["Albantakis", "Larissa", ""], ["Tehrani-Saleh", "Ali", ""], ["Kvam", "Peter", ""], ["Sheneman", "Leigh", ""], ["Goldsby", "Heather", ""], ["Bohm", "Clifford", ""], ["Adami", "Christoph", ""]]}, {"id": "1709.05650", "submitter": "Jakob Jordan", "authors": "Jakob Jordan, Philipp Weidel, Abigail Morrison", "title": "Closing the loop between neural network simulators and the OpenAI Gym", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "q-bio.NC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Since the enormous breakthroughs in machine learning over the last decade,\nfunctional neural network models are of growing interest for many researchers\nin the field of computational neuroscience. One major branch of research is\nconcerned with biologically plausible implementations of reinforcement\nlearning, with a variety of different models developed over the recent years.\nHowever, most studies in this area are conducted with custom simulation scripts\nand manually implemented tasks. This makes it hard for other researchers to\nreproduce and build upon previous work and nearly impossible to compare the\nperformance of different learning architectures. In this work, we present a\nnovel approach to solve this problem, connecting benchmark tools from the field\nof machine learning and state-of-the-art neural network simulators from\ncomputational neuroscience. This toolchain enables researchers in both fields\nto make use of well-tested high-performance simulation software supporting\nbiologically plausible neuron, synapse and network models and allows them to\nevaluate and compare their approach on the basis of standardized environments\nof varying complexity. We demonstrate the functionality of the toolchain by\nimplementing a neuronal actor-critic architecture for reinforcement learning in\nthe NEST simulator and successfully training it on two different environments\nfrom the OpenAI Gym.\n", "versions": [{"version": "v1", "created": "Sun, 17 Sep 2017 12:25:33 GMT"}], "update_date": "2017-09-19", "authors_parsed": [["Jordan", "Jakob", ""], ["Weidel", "Philipp", ""], ["Morrison", "Abigail", ""]]}, {"id": "1709.05939", "submitter": "Nancy Xin Ru Wang", "authors": "Nancy Xin Ru Wang, Ali Farhadi, Rajesh Rao, Bingni Brunton", "title": "AJILE Movement Prediction: Multimodal Deep Learning for Natural Human\n  Neural Recordings and Video", "comments": null, "journal-ref": "Thirty-Second AAAI Conference On Artificial Intelligence (2018)", "doi": null, "report-no": null, "categories": "cs.CV q-bio.NC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Developing useful interfaces between brains and machines is a grand challenge\nof neuroengineering. An effective interface has the capacity to not only\ninterpret neural signals, but predict the intentions of the human to perform an\naction in the near future; prediction is made even more challenging outside\nwell-controlled laboratory experiments. This paper describes our approach to\ndetect and to predict natural human arm movements in the future, a key\nchallenge in brain computer interfacing that has never before been attempted.\nWe introduce the novel Annotated Joints in Long-term ECoG (AJILE) dataset;\nAJILE includes automatically annotated poses of 7 upper body joints for four\nhuman subjects over 670 total hours (more than 72 million frames), along with\nthe corresponding simultaneously acquired intracranial neural recordings. The\nsize and scope of AJILE greatly exceeds all previous datasets with movements\nand electrocorticography (ECoG), making it possible to take a deep learning\napproach to movement prediction. We propose a multimodal model that combines\ndeep convolutional neural networks (CNN) with long short-term memory (LSTM)\nblocks, leveraging both ECoG and video modalities. We demonstrate that our\nmodels are able to detect movements and predict future movements up to 800 msec\nbefore movement initiation. Further, our multimodal movement prediction models\nexhibit resilience to simulated ablation of input neural signals. We believe a\nmultimodal approach to natural neural decoding that takes context into account\nis critical in advancing bioelectronic technologies and human neuroscience.\n", "versions": [{"version": "v1", "created": "Wed, 13 Sep 2017 01:28:44 GMT"}, {"version": "v2", "created": "Thu, 1 Mar 2018 22:40:58 GMT"}], "update_date": "2018-04-04", "authors_parsed": [["Wang", "Nancy Xin Ru", ""], ["Farhadi", "Ali", ""], ["Rao", "Rajesh", ""], ["Brunton", "Bingni", ""]]}, {"id": "1709.06134", "submitter": "Yu Zheng", "authors": "Zhe Wang, Yu Zheng, David C. Zhu, Jian Ren and Tongtong Li", "title": "Discrete Dynamic Causal Modeling and Its Relationship with Directed\n  Information", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "q-bio.NC cs.IT math.IT stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper explores the discrete Dynamic Causal Modeling (DDCM) and its\nrelationship with Directed Information (DI). We prove the conditional\nequivalence between DDCM and DI in characterizing the causal relationship\nbetween two brain regions. The theoretical results are demonstrated using fMRI\ndata obtained under both resting state and stimulus based state. Our numerical\nanalysis is consistent with that reported in previous study.\n", "versions": [{"version": "v1", "created": "Mon, 18 Sep 2017 19:43:11 GMT"}], "update_date": "2017-09-20", "authors_parsed": [["Wang", "Zhe", ""], ["Zheng", "Yu", ""], ["Zhu", "David C.", ""], ["Ren", "Jian", ""], ["Li", "Tongtong", ""]]}, {"id": "1709.06144", "submitter": "Olivier Colliot", "authors": "Kuldeep Kumar, Pietro Gori, Benjamin Charlier, Stanley Durrleman,\n  Olivier Colliot, Christian Desrosiers", "title": "White Matter Fiber Segmentation Using Functional Varifolds", "comments": null, "journal-ref": "Graphs in Biomedical Image Analysis, Computational Anatomy and\n  Imaging Genetics, pp 92-100, Lecture Notes in Computer Science, volume 10551,\n  Springer, 2017", "doi": null, "report-no": null, "categories": "cs.CV q-bio.NC q-bio.QM", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  The extraction of fibers from dMRI data typically produces a large number of\nfibers, it is common to group fibers into bundles. To this end, many\nspecialized distance measures, such as MCP, have been used for fiber\nsimilarity. However, these distance based approaches require point-wise\ncorrespondence and focus only on the geometry of the fibers. Recent\npublications have highlighted that using microstructure measures along fibers\nimproves tractography analysis. Also, many neurodegenerative diseases impacting\nwhite matter require the study of microstructure measures as well as the white\nmatter geometry. Motivated by these, we propose to use a novel computational\nmodel for fibers, called functional varifolds, characterized by a metric that\nconsiders both the geometry and microstructure measure (e.g. GFA) along the\nfiber pathway. We use it to cluster fibers with a dictionary learning and\nsparse coding-based framework, and present a preliminary analysis using HCP\ndata.\n", "versions": [{"version": "v1", "created": "Mon, 18 Sep 2017 20:05:19 GMT"}], "update_date": "2017-09-20", "authors_parsed": [["Kumar", "Kuldeep", ""], ["Gori", "Pietro", ""], ["Charlier", "Benjamin", ""], ["Durrleman", "Stanley", ""], ["Colliot", "Olivier", ""], ["Desrosiers", "Christian", ""]]}, {"id": "1709.06151", "submitter": "Olivier Colliot", "authors": "Kuldeep Kumar, Laurent Chauvin, Mathew Toews, Olivier Colliot,\n  Christian Desrosiers", "title": "Multi-modal analysis of genetically-related subjects using SIFT\n  descriptors in brain MRI", "comments": null, "journal-ref": "Proc. Computational Diffusion MRI, MICCAI Workshop, Qu\\'ebec City,\n  Canada, September 2017", "doi": null, "report-no": null, "categories": "cs.CV q-bio.NC q-bio.QM", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  So far, fingerprinting studies have focused on identifying features from\nsingle-modality MRI data, which capture individual characteristics in terms of\nbrain structure, function, or white matter microstructure. However, due to the\nlack of a framework for comparing across multiple modalities, studies based on\nmulti-modal data remain elusive. This paper presents a multi-modal analysis of\ngenetically-related subjects to compare and contrast the information provided\nby various MRI modalities. The proposed framework represents MRI scans as bags\nof SIFT features, and uses these features in a nearest-neighbor graph to\nmeasure subject similarity. Experiments using the T1/T2-weighted MRI and\ndiffusion MRI data of 861 Human Connectome Project subjects demonstrate strong\nlinks between the proposed similarity measure and genetic proximity.\n", "versions": [{"version": "v1", "created": "Mon, 18 Sep 2017 20:12:32 GMT"}], "update_date": "2017-09-20", "authors_parsed": [["Kumar", "Kuldeep", ""], ["Chauvin", "Laurent", ""], ["Toews", "Mathew", ""], ["Colliot", "Olivier", ""], ["Desrosiers", "Christian", ""]]}, {"id": "1709.06563", "submitter": "William Holmes", "authors": "Jennifer S. Trueblood, William R. Holmes, Adam C. Seegmiller, Jonathan\n  Douds, Margaret Compton, Megan Woodruff, Wenrui Huang, Charles Stratton,\n  Quentin Eichbaum", "title": "The Impact of Speed and Bias on the Cognitive Processes of Experts and\n  Novices in Medical Image Decision-making", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "q-bio.NC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Training individuals to make accurate decisions from medical images is a\ncritical component of education in diagnostic pathology. We describe a joint\nexperimental and computational modeling approach to examine the similarities\nand differences in the cognitive processes of novice participants and\nexperienced participants (pathology residents and pathology faculty) in cancer\ncell image identification. For this study we collected a bank of hundreds of\ndigital images that were identified by cell type and classified by difficulty\nby a panel of expert hematopathologists. The key manipulations in our study\nincluded examining the speed-accuracy tradeoff as well as the impact of prior\nexpectations on decisions. In addition, our study examined individual\ndifferences in decision-making by comparing task performance to domain general\nvisual ability (as measured using the Novel Object Memory Test (NOMT) (Richler\net al., 2017). Using Signal Detection Theory (SDT) and the Diffusion Decision\nModel (DDM), we found many similarities between expert and novices in our task.\nWhile experts tended to have better discriminability, the two groups responded\nsimilarly to time pressure (i.e., reduced caution under speed instructions in\nthe DDM) and to the introduction of a probabilistic cue (i.e., increased\nresponse bias in the DDM). These results have important implications for\ntraining in this area as well as using novice participants in research on\nmedical image perception and decision-making.\n", "versions": [{"version": "v1", "created": "Tue, 19 Sep 2017 17:34:35 GMT"}, {"version": "v2", "created": "Fri, 18 May 2018 15:11:43 GMT"}], "update_date": "2018-05-21", "authors_parsed": [["Trueblood", "Jennifer S.", ""], ["Holmes", "William R.", ""], ["Seegmiller", "Adam C.", ""], ["Douds", "Jonathan", ""], ["Compton", "Margaret", ""], ["Woodruff", "Megan", ""], ["Huang", "Wenrui", ""], ["Stratton", "Charles", ""], ["Eichbaum", "Quentin", ""]]}, {"id": "1709.06824", "submitter": "Tomas Van Pottelbergh", "authors": "Tomas Van Pottelbergh, Guillaume Drion, Rodolphe Sepulchre", "title": "Robust modulation of integrate-and-fire models", "comments": "This is the authors' final version. The article has been accepted for\n  publication in Neural Computation", "journal-ref": "Neural Computation 30:4 (2018) 987-1011", "doi": "10.1162/neco_a_01065", "report-no": null, "categories": "q-bio.NC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  By controlling the state of neuronal populations, neuromodulators ultimately\naffect behaviour. A key neuromodulation mechanism is the alteration of neuronal\nexcitability via the modulation of ion channel expression. This type of\nneuromodulation is normally studied via conductance-based models, but those\nmodels are computationally challenging for large-scale network simulations\nneeded in population studies. This paper studies the modulation properties of\nthe Multi-Quadratic Integrate-and-Fire (MQIF) model, a generalisation of the\nclassical Quadratic Integrate-and-Fire (QIF) model. The model is shown to\ncombine the computational economy of integrate-and-fire modelling and the\nphysiological interpretability of conductance-based modelling. It is therefore\na good candidate for affordable computational studies of neuromodulation in\nlarge networks.\n", "versions": [{"version": "v1", "created": "Wed, 20 Sep 2017 11:57:15 GMT"}, {"version": "v2", "created": "Fri, 13 Oct 2017 15:38:07 GMT"}, {"version": "v3", "created": "Thu, 16 Nov 2017 13:26:40 GMT"}, {"version": "v4", "created": "Tue, 5 Dec 2017 15:47:30 GMT"}], "update_date": "2020-09-29", "authors_parsed": [["Van Pottelbergh", "Tomas", ""], ["Drion", "Guillaume", ""], ["Sepulchre", "Rodolphe", ""]]}, {"id": "1709.06950", "submitter": "Damian Berger", "authors": "Damian L. Berger, Lucilla de Arcangelis, and Hans J. Herrmann", "title": "Spatial features of synaptic adaptation affecting learning performance", "comments": null, "journal-ref": "Scientific Reports 7, 11016 (2017)", "doi": "10.1038/s41598-017-11424-5", "report-no": null, "categories": "q-bio.NC cond-mat.dis-nn cs.LG cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recent studies have proposed that the diffusion of messenger molecules, such\nas monoamines, can mediate the plastic adaptation of synapses in supervised\nlearning of neural networks. Based on these findings we developed a model for\nneural learning, where the signal for plastic adaptation is assumed to\npropagate through the extracellular space. We investigate the conditions\nallowing learning of Boolean rules in a neural network. Even fully excitatory\nnetworks show very good learning performances. Moreover, the investigation of\nthe plastic adaptation features optimizing the performance suggests that\nlearning is very sensitive to the extent of the plastic adaptation and the\nspatial range of synaptic connections.\n", "versions": [{"version": "v1", "created": "Wed, 20 Sep 2017 16:18:17 GMT"}], "update_date": "2017-09-21", "authors_parsed": [["Berger", "Damian L.", ""], ["de Arcangelis", "Lucilla", ""], ["Herrmann", "Hans J.", ""]]}, {"id": "1709.06958", "submitter": "Julien Chevallier", "authors": "Julien Chevallier (SVH)", "title": "Stimulus sensitivity of a spiking neural network model", "comments": null, "journal-ref": null, "doi": "10.1007/s10955-017-1948-y", "report-no": null, "categories": "math.PR q-bio.NC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Some recent papers relate the criticality of complex systems to their maximal\ncapacity of information processing. In the present paper, we consider high\ndimensional point processes, known as age-dependent Hawkes processes, which\nhave been used to model spiking neural networks. Using mean-field\napproximation, the response of the network to a stimulus is computed and we\nprovide a notion of stimulus sensitivity. It appears that the maximal\nsensitivity is achieved in the sub-critical regime, yet almost critical for a\nrange of biologically relevant parameters.\n", "versions": [{"version": "v1", "created": "Tue, 19 Sep 2017 13:38:43 GMT"}, {"version": "v2", "created": "Tue, 14 Nov 2017 08:00:11 GMT"}], "update_date": "2018-02-14", "authors_parsed": [["Chevallier", "Julien", "", "SVH"]]}, {"id": "1709.07211", "submitter": "Mahmoud Hassan", "authors": "Ahmad Mheich (LTSI), Mahmoud Hassan (LTSI), Mohamad Khalil, Vincent\n  Gripon (ELEC), Olivier Dufor, Fabrice Wendling (LTSI)", "title": "SimiNet: a Novel Method for Quantifying Brain Network Similarity", "comments": null, "journal-ref": "IEEE Transactions on Pattern Analysis and Machine Intelligence,\n  Institute of Electrical and Electronics Engineers, 2017, pp.1 - 1", "doi": "10.1109/TPAMI.2017.2750160", "report-no": null, "categories": "q-bio.NC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Quantifying the similarity between two networks is critical in many\napplications. A number of algorithms have been proposed to compute graph\nsimilarity, mainly based on the properties of nodes and edges. Interestingly,\nmost of these algorithms ignore the physical location of the nodes, which is a\nkey factor in the context of brain networks involving spatially defined\nfunctional areas. In this paper, we present a novel algorithm called \"SimiNet\"\nfor measuring similarity between two graphs whose nodes are defined a priori\nwithin a 3D coordinate system. SimiNet provides a quantified index (ranging\nfrom 0 to 1) that accounts for node, edge and spatiality features. Complex\ngraphs were simulated to evaluate the performance of SimiNet that is compared\nwith eight state-of-art methods. Results show that SimiNet is able to detect\nweak spatial variations in compared graphs in addition to computing similarity\nusing both nodes and edges. SimiNet was also applied to real brain networks\nobtained during a visual recognition task. The algorithm shows high performance\nto detect spatial variation of brain networks obtained during a naming task of\ntwo categories of visual stimuli: animals and tools. A perspective to this work\nis a better understanding of object categorization in the human brain.\n", "versions": [{"version": "v1", "created": "Thu, 21 Sep 2017 08:38:42 GMT"}], "update_date": "2017-09-22", "authors_parsed": [["Mheich", "Ahmad", "", "LTSI"], ["Hassan", "Mahmoud", "", "LTSI"], ["Khalil", "Mohamad", "", "ELEC"], ["Gripon", "Vincent", "", "ELEC"], ["Dufor", "Olivier", "", "LTSI"], ["Wendling", "Fabrice", "", "LTSI"]]}, {"id": "1709.07267", "submitter": "Olivier Colliot", "authors": "Jorge Samper-Gonz\\'alez, Ninon Burgos, Sabrina Fontanella, Hugo\n  Bertin, Marie-Odile Habert, Stanley Durrleman, Theodoros Evgeniou, Olivier\n  Colliot", "title": "Yet Another ADNI Machine Learning Paper? Paving The Way Towards\n  Fully-reproducible Research on Classification of Alzheimer's Disease", "comments": null, "journal-ref": "Proc. Machine Learning in Medical Imaging MLMI 2017, MICCAI\n  Worskhop, Lecture Notes in Computer Science, volume 10541, pp 53-60, Springer", "doi": "10.1007/978-3-319-67389-9_7", "report-no": null, "categories": "stat.ML cs.CV q-bio.NC q-bio.QM", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  In recent years, the number of papers on Alzheimer's disease classification\nhas increased dramatically, generating interesting methodological ideas on the\nuse machine learning and feature extraction methods. However, practical impact\nis much more limited and, eventually, one could not tell which of these\napproaches are the most efficient. While over 90\\% of these works make use of\nADNI an objective comparison between approaches is impossible due to variations\nin the subjects included, image pre-processing, performance metrics and\ncross-validation procedures. In this paper, we propose a framework for\nreproducible classification experiments using multimodal MRI and PET data from\nADNI. The core components are: 1) code to automatically convert the full ADNI\ndatabase into BIDS format; 2) a modular architecture based on Nipype in order\nto easily plug-in different classification and feature extraction tools; 3)\nfeature extraction pipelines for MRI and PET data; 4) baseline classification\napproaches for unimodal and multimodal features. This provides a flexible\nframework for benchmarking different feature extraction and classification\ntools in a reproducible manner. We demonstrate its use on all (1519) baseline\nT1 MR images and all (1102) baseline FDG PET images from ADNI 1, GO and 2 with\nSPM-based feature extraction pipelines and three different classification\ntechniques (linear SVM, anatomically regularized SVM and multiple kernel\nlearning SVM). The highest accuracies achieved were: 91% for AD vs CN, 83% for\nMCIc vs CN, 75% for MCIc vs MCInc, 94% for AD-A$\\beta$+ vs CN-A$\\beta$- and 72%\nfor MCIc-A$\\beta$+ vs MCInc-A$\\beta$+. The code is publicly available at\nhttps://gitlab.icm-institute.org/aramislab/AD-ML (depends on the Clinica\nsoftware platform, publicly available at http://www.clinica.run).\n", "versions": [{"version": "v1", "created": "Thu, 21 Sep 2017 11:37:01 GMT"}], "update_date": "2017-09-22", "authors_parsed": [["Samper-Gonz\u00e1lez", "Jorge", ""], ["Burgos", "Ninon", ""], ["Fontanella", "Sabrina", ""], ["Bertin", "Hugo", ""], ["Habert", "Marie-Odile", ""], ["Durrleman", "Stanley", ""], ["Evgeniou", "Theodoros", ""], ["Colliot", "Olivier", ""]]}, {"id": "1709.07285", "submitter": "Carel F.W. Peeters", "authors": "Francisca A. de Leeuw, Carel F.W. Peeters, Maartje I. Kester, Amy C.\n  Harms, Eduard A. Struys, Thomas Hankemeier, Herman W.T. van Vlijmen, Sven J.\n  van der Lee, Cornelia M. van Duijn, Philip Scheltens, Ay\\c{s}e Demirkan, Mark\n  A. van de Wiel, Wiesje M. van der Flier, Charlotte E. Teunissen", "title": "Blood-based metabolic signatures in Alzheimer's disease", "comments": "Postprint, 76 pages, 32 figures, includes supplementary material", "journal-ref": "Alzheimer's & Dementia: Diagnosis, Assessment & Disease\n  Monitoring, 8 (2017): 196-207", "doi": "10.1016/j.dadm.2017.07.006", "report-no": null, "categories": "q-bio.NC q-bio.MN stat.AP", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Introduction: Identification of blood-based metabolic changes might provide\nearly and easy-to-obtain biomarkers.\n  Methods: We included 127 AD patients and 121 controls with\nCSF-biomarker-confirmed diagnosis (cut-off tau/A$\\beta_{42}$: 0.52). Mass\nspectrometry platforms determined the concentrations of 53 amine, 22 organic\nacid, 120 lipid, and 40 oxidative stress compounds. Multiple signatures were\nassessed: differential expression (nested linear models), classification\n(logistic regression), and regulatory (network extraction).\n  Results: Twenty-six metabolites were differentially expressed. Metabolites\nimproved the classification performance of clinical variables from 74% to 79%.\nNetwork models identified 5 hubs of metabolic dysregulation: Tyrosine,\nglycylglycine, glutamine, lysophosphatic acid C18:2 and platelet activating\nfactor C16:0. The metabolite network for APOE $\\epsilon$4 negative AD patients\nwas less cohesive compared to the network for APOE $\\epsilon$4 positive AD\npatients.\n  Discussion: Multiple signatures point to various promising peripheral markers\nfor further validation. The network differences in AD patients according to\nAPOE genotype may reflect different pathways to AD.\n", "versions": [{"version": "v1", "created": "Thu, 21 Sep 2017 12:47:43 GMT"}], "update_date": "2017-09-22", "authors_parsed": [["de Leeuw", "Francisca A.", ""], ["Peeters", "Carel F. W.", ""], ["Kester", "Maartje I.", ""], ["Harms", "Amy C.", ""], ["Struys", "Eduard A.", ""], ["Hankemeier", "Thomas", ""], ["van Vlijmen", "Herman W. T.", ""], ["van der Lee", "Sven J.", ""], ["van Duijn", "Cornelia M.", ""], ["Scheltens", "Philip", ""], ["Demirkan", "Ay\u015fe", ""], ["van de Wiel", "Mark A.", ""], ["van der Flier", "Wiesje M.", ""], ["Teunissen", "Charlotte E.", ""]]}, {"id": "1709.08166", "submitter": "Luziwei Leng", "authors": "Luziwei Leng, Roman Martel, Oliver Breitwieser, Ilja Bytschok, Walter\n  Senn, Johannes Schemmel, Karlheinz Meier and Mihai A. Petrovici", "title": "Spiking neurons with short-term synaptic plasticity form superior\n  generative networks", "comments": "corrected typo in abstract", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NE physics.bio-ph q-bio.NC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Spiking networks that perform probabilistic inference have been proposed both\nas models of cortical computation and as candidates for solving problems in\nmachine learning. However, the evidence for spike-based computation being in\nany way superior to non-spiking alternatives remains scarce. We propose that\nshort-term plasticity can provide spiking networks with distinct computational\nadvantages compared to their classical counterparts. In this work, we use\nnetworks of leaky integrate-and-fire neurons that are trained to perform both\ndiscriminative and generative tasks in their forward and backward information\nprocessing paths, respectively. During training, the energy landscape\nassociated with their dynamics becomes highly diverse, with deep attractor\nbasins separated by high barriers. Classical algorithms solve this problem by\nemploying various tempering techniques, which are both computationally\ndemanding and require global state updates. We demonstrate how similar results\ncan be achieved in spiking networks endowed with local short-term synaptic\nplasticity. Additionally, we discuss how these networks can even outperform\ntempering-based approaches when the training data is imbalanced. We thereby\nshow how biologically inspired, local, spike-triggered synaptic dynamics based\nsimply on a limited pool of synaptic resources can allow spiking networks to\noutperform their non-spiking relatives.\n", "versions": [{"version": "v1", "created": "Sun, 24 Sep 2017 09:15:39 GMT"}, {"version": "v2", "created": "Wed, 27 Sep 2017 03:23:56 GMT"}, {"version": "v3", "created": "Tue, 10 Oct 2017 20:45:49 GMT"}], "update_date": "2017-10-12", "authors_parsed": [["Leng", "Luziwei", ""], ["Martel", "Roman", ""], ["Breitwieser", "Oliver", ""], ["Bytschok", "Ilja", ""], ["Senn", "Walter", ""], ["Schemmel", "Johannes", ""], ["Meier", "Karlheinz", ""], ["Petrovici", "Mihai A.", ""]]}, {"id": "1709.08491", "submitter": "Olivier Colliot", "authors": "Igor Koval, Jean-Baptiste Schiratti, Alexandre Routier, Michael Bacci,\n  Olivier Colliot, St\\'ephanie Allassonni\\`ere, Stanley Durrleman", "title": "Statistical learning of spatiotemporal patterns from longitudinal\n  manifold-valued networks", "comments": null, "journal-ref": "Proc. Medical Image Computing and Computer-Assisted Intervention,\n  MICCAI 2017, Lecture Notes in Computer Science, volume 10433, pp 451-459,\n  Springer", "doi": "10.1007/978-3-319-66182-7_52", "report-no": null, "categories": "stat.ML cs.CV q-bio.NC q-bio.QM", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  We introduce a mixed-effects model to learn spatiotempo-ral patterns on a\nnetwork by considering longitudinal measures distributed on a fixed graph. The\ndata come from repeated observations of subjects at different time points which\ntake the form of measurement maps distributed on a graph such as an image or a\nmesh. The model learns a typical group-average trajectory characterizing the\npropagation of measurement changes across the graph nodes. The subject-specific\ntrajectories are defined via spatial and temporal transformations of the\ngroup-average scenario, thus estimating the variability of spatiotemporal\npatterns within the group. To estimate population and individual model\nparameters, we adapted a stochastic version of the Expectation-Maximization\nalgorithm, the MCMC-SAEM. The model is used to describe the propagation of\ncortical atrophy during the course of Alzheimer's Disease. Model parameters\nshow the variability of this average pattern of atrophy in terms of\ntrajectories across brain regions, age at disease onset and pace of\npropagation. We show that the personalization of this model yields accurate\nprediction of maps of cortical thickness in patients.\n", "versions": [{"version": "v1", "created": "Mon, 25 Sep 2017 13:57:08 GMT"}], "update_date": "2017-09-26", "authors_parsed": [["Koval", "Igor", ""], ["Schiratti", "Jean-Baptiste", ""], ["Routier", "Alexandre", ""], ["Bacci", "Michael", ""], ["Colliot", "Olivier", ""], ["Allassonni\u00e8re", "St\u00e9phanie", ""], ["Durrleman", "Stanley", ""]]}, {"id": "1709.08578", "submitter": "Bhim Adhikari", "authors": "Bhim M. Adhikari, Neda Jahanshad, Dinesh Shukla, Dinesh Shukla,\n  Richard C. Reynolds, Robert W. Cox, Els Fieremans, Jelle Veraart, Dmitry S.\n  Novikov, L. Elliot Hong, Paul M. Thompson, Peter Kochunov", "title": "Heritability estimates on resting state fMRI data using the ENIGMA\n  analysis pipeline", "comments": "12 pages, 3 figures, PSB 2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "q-bio.NC q-bio.PE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Big data initiatives such as the Enhancing NeuroImaging Genetics through\nMeta-Analysis consortium (ENIGMA), combine data collected by independent\nstudies worldwide to achieve more accurate estimates of effect sizes and more\nreliable and reproducible outcomes. Such efforts require harmonized analyses\nprotocols to consistently extract phenotypes. Even so, challenges include wide\nvariability of fMRI protocols and scanner platforms; this leads to site-to-site\nvariance in quality, resolution and temporal signal-to-noise ratio (tSNR). An\neffective harmonization should provide optimal measures for data of different\nqualities. We developed a multi-site rsfMRI analysis pipeline to allow research\ngroups around the world to process rsfMRI scans in a harmonized way, to extract\nconsistent and quantitative measurements of connectivity and to perform\ncoordinated statistical tests. We used the single-modality ENIGMA rsfMRI\npipeline based on model-free Marchenko-Pastor PCA based denoising to verify and\nreplicate findings of significant heritability of measures from resting state\nnetworks. We analyzed two independent cohorts, GOBS (Genetics of Brain\nStructure) and HCP (the Human Connectome Project), which collected data using\nconventional and connectomics oriented fMRI protocols. We used seed-based\nconnectivity and dual-regression approaches to show that rsfMRI signal is\nconsistently heritable across twenty major functional network measures.\nHeritability values of 20-40% were observed across both cohorts.\n", "versions": [{"version": "v1", "created": "Wed, 13 Sep 2017 14:12:11 GMT"}], "update_date": "2017-09-26", "authors_parsed": [["Adhikari", "Bhim M.", ""], ["Jahanshad", "Neda", ""], ["Shukla", "Dinesh", ""], ["Shukla", "Dinesh", ""], ["Reynolds", "Richard C.", ""], ["Cox", "Robert W.", ""], ["Fieremans", "Els", ""], ["Veraart", "Jelle", ""], ["Novikov", "Dmitry S.", ""], ["Hong", "L. Elliot", ""], ["Thompson", "Paul M.", ""], ["Kochunov", "Peter", ""]]}, {"id": "1709.08591", "submitter": "Chris Antonopoulos Dr", "authors": "Chris G. Antonopoulos, Ezequiel Bianco-Martinez, Murilo S. Baptista", "title": "Evaluating performance of neural codes in model neural communication\n  networks", "comments": "20 pages, 5 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "q-bio.NC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Information needs to be appropriately encoded to be reliably transmitted over\nphysical media. Similarly, neurons have their own codes to convey information\nin the brain. Even though it is well-known that neurons exchange information\nusing a pool of several protocols of spatio-temporal encodings, the suitability\nof each code and their performance as a function of network parameters and\nexternal stimuli is still one of the great mysteries in neuroscience. This\npaper sheds light on this by modeling small-size networks of chemically and\nelectrically coupled Hindmarsh-Rose spiking neurons. We focus on a class of\ntemporal and firing-rate codes that result from neurons' membrane-potentials\nand phases, and quantify numerically their performance estimating the Mutual\nInformation Rate, aka the rate of information exchange. Our results suggest\nthat the firing-rate and interspike-intervals codes are more robust to additive\nGaussian white noise. In a network of four interconnected neurons and in the\nabsence of such noise, pairs of neurons that have the largest rate of\ninformation exchange using the interspike-intervals and firing-rate codes are\nnot adjacent in the network, whereas spike-timings and phase codes (temporal)\npromote large rate of information exchange for adjacent neurons. If that result\nwould have been possible to extend to larger neural networks, it would suggest\nthat small microcircuits would preferably exchange information using temporal\ncodes (spike-timings and phase codes), whereas on the macroscopic scale, where\nthere would be typically pairs of neurons not directly connected due to the\nbrain's sparsity, firing-rate and interspike-intervals codes would be the most\nefficient codes.\n", "versions": [{"version": "v1", "created": "Mon, 25 Sep 2017 16:44:49 GMT"}, {"version": "v2", "created": "Fri, 5 Oct 2018 09:58:34 GMT"}, {"version": "v3", "created": "Wed, 17 Oct 2018 15:50:50 GMT"}], "update_date": "2018-10-18", "authors_parsed": [["Antonopoulos", "Chris G.", ""], ["Bianco-Martinez", "Ezequiel", ""], ["Baptista", "Murilo S.", ""]]}, {"id": "1709.09541", "submitter": "R. Ozgur Doruk", "authors": "R.Ozgur Doruk, Kechen Zhang", "title": "Fitting of dynamic recurrent neural network models to sensory\n  stimulus-response data", "comments": "arXiv admin note: text overlap with arXiv:1610.05561", "journal-ref": null, "doi": null, "report-no": null, "categories": "q-bio.NC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a theoretical study aiming at model fitting for sensory neurons.\nConventional neural network training approaches are not applicable to this\nproblem due to lack of continuous data. Although the stimulus can be considered\nas a smooth time dependent variable, the associated response will be a set of\nneural spike timings (roughly the instants of successive action potential\npeaks) which have no amplitude information. A recurrent neural network model\ncan be fitted to such a stimulus-response data pair by using maximum likelihood\nestimation method where the likelihood function is derived from Poisson\nstatistics of neural spiking. The universal approximation feature of the\nrecurrent dynamical neuron network models allow us to describe\nexcitatory-inhibitory characteristics of an actual sensory neural network with\nany desired number of neurons. The stimulus data is generated by a Phased\nCosine Fourier series having fixed amplitude and frequency but a randomly shot\nphase. Various values of amplitude, stimulus component size and sample size are\napplied in order to examine the effect of stimulus to the identification\nprocess. Results are presented in tabular form at the end of this text.\n", "versions": [{"version": "v1", "created": "Tue, 26 Sep 2017 08:35:35 GMT"}], "update_date": "2017-09-28", "authors_parsed": [["Doruk", "R. Ozgur", ""], ["Zhang", "Kechen", ""]]}, {"id": "1709.09645", "submitter": "Moo K. Chung", "authors": "Moo K. Chung", "title": "Statistical Challenges of Big Brain Network Data", "comments": "8 pages, 2 figure", "journal-ref": null, "doi": null, "report-no": null, "categories": "q-bio.NC stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We explore the main characteristics of big brain network data that offer\nunique statistical challenges. The brain networks are biologically expected to\nbe both sparse and hierarchical. Such unique characterizations put specific\ntopological constraints onto statistical approaches and models we can use\neffectively. We explore the limitations of the current models used in the field\nand offer alternative approaches and explain new challenges.\n", "versions": [{"version": "v1", "created": "Wed, 27 Sep 2017 17:30:10 GMT"}, {"version": "v2", "created": "Sat, 23 Dec 2017 07:31:43 GMT"}], "update_date": "2017-12-27", "authors_parsed": [["Chung", "Moo K.", ""]]}, {"id": "1709.09748", "submitter": "Rodrigo Felipe de Oliveira Pena", "authors": "Rodrigo F.O. Pena, Michael A. Zaks, Antonio C. Roque", "title": "Dynamics of spontaneous activity in random networks with multiple neuron\n  subtypes and synaptic noise", "comments": "30 pages, 19 figures", "journal-ref": "Journal of Computational Neuroscience (2018)", "doi": "10.1007/s10827-018-0688-6", "report-no": null, "categories": "q-bio.NC nlin.AO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Spontaneous cortical population activity exhibits a multitude of oscillatory\npatterns, which often display synchrony during slow-wave sleep or under certain\nanesthetics and stay asynchronous during quiet wakefulness. The mechanisms\nbehind these cortical states and transitions among them are not completely\nunderstood. Here we study spontaneous population activity patterns in random\nnetworks of spiking neurons of mixed types modeled by Izhikevich equations.\nNeurons are coupled by conductance-based synapses subject to synaptic noise. We\nlocalize the population activity patterns on the parameter diagram spanned by\nthe relative inhibitory synaptic strength and the magnitude of synaptic noise.\nIn absence of noise, networks display transient activity patterns, either\noscillatory or at constant level. The effect of noise is to turn transient\npatterns into persistent ones: for weak noise, all activity patterns are\nasynchronous non-oscillatory independently of synaptic strengths; for stronger\nnoise, patterns have oscillatory and synchrony characteristics that depend on\nthe relative inhibitory synaptic strength. In the region of parameter space\nwhere inhibitory synaptic strength exceeds the excitatory synaptic strength and\nfor moderate noise magnitudes networks feature intermittent switches between\noscillatory and quiescent states with characteristics similar to those of\nsynchronous and asynchronous cortical states, respectively. We explain these\noscillatory and quiescent patterns by combining a phenomenological global\ndescription of the network state with local descriptions of individual neurons\nin their partial phase spaces. Our results point to a bridge from events at the\nmolecular scale of synapses to the cellular scale of individual neurons to the\ncollective scale of neuronal populations.\n", "versions": [{"version": "v1", "created": "Wed, 27 Sep 2017 22:02:38 GMT"}, {"version": "v2", "created": "Sun, 20 May 2018 01:16:06 GMT"}], "update_date": "2018-06-20", "authors_parsed": [["Pena", "Rodrigo F. O.", ""], ["Zaks", "Michael A.", ""], ["Roque", "Antonio C.", ""]]}, {"id": "1709.09761", "submitter": "Pegah Fakhari", "authors": "Pegah Fakhari, Arash Khodadadi, Jerome Busemeyer", "title": "The detour problem in a stochastic environment: Tolman revisited", "comments": "44 pages, 7 figures, 5 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML q-bio.NC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We designed a grid world task to study human planning and re-planning\nbehavior in an unknown stochastic environment. In our grid world, participants\nwere asked to travel from a random starting point to a random goal position\nwhile maximizing their reward. Because they were not familiar with the\nenvironment, they needed to learn its characteristics from experience to plan\noptimally. Later in the task, we randomly blocked the optimal path to\ninvestigate whether and how people adjust their original plans to find a\ndetour. To this end, we developed and compared 12 different models. These\nmodels were different on how they learned and represented the environment and\nhow they planned to catch the goal. The majority of our participants were able\nto plan optimally. We also showed that people were capable of revising their\nplans when an unexpected event occurred. The result from the model comparison\nshowed that the model-based reinforcement learning approach provided the best\naccount for the data and outperformed heuristics in explaining the behavioral\ndata in the re-planning trials.\n", "versions": [{"version": "v1", "created": "Wed, 27 Sep 2017 23:22:06 GMT"}], "update_date": "2017-09-29", "authors_parsed": [["Fakhari", "Pegah", ""], ["Khodadadi", "Arash", ""], ["Busemeyer", "Jerome", ""]]}, {"id": "1709.10045", "submitter": "Evelyn Tang", "authors": "Evelyn Tang, Marcelo G. Mattar, Chad Giusti, Sharon L.\n  Thompson-Schill, and Danielle S. Bassett", "title": "Effective learning is accompanied by high dimensional and efficient\n  representations of neural activity", "comments": null, "journal-ref": "Nature Neuroscience 22, 1000-1009 (2019)", "doi": "10.1038/s41593-019-0400-9", "report-no": null, "categories": "q-bio.NC q-bio.QM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A fundamental cognitive process is the ability to map value and identity onto\nobjects as we learn about them. Exactly how such mental constructs emerge and\nwhat kind of space best embeds this mapping remains incompletely understood.\nHere we develop tools to quantify the space and organization of such a mapping,\nthereby providing a framework for studying the geometric representations of\nneural responses as reflected in functional MRI. Considering how human subjects\nlearn the values of novel objects, we show that quick learners have a higher\ndimensional geometric representation than slow learners, and hence more easily\ndistinguishable whole-brain responses to objects of different value.\nFurthermore, we find that quick learners display a more compact embedding of\ntheir neural responses and hence have a higher ratio of their task-based\ndimension to their embedding dimension -- consistent with a greater efficiency\nof cognitive coding. Lastly, we investigate the neurophysiological drivers of\nhigh dimensional patterns at both regional and voxel levels, and we complete\nour study with a complementary test of the distinguishability of associated\nwhole-brain responses. Our results demonstrate a spatial organization of neural\nresponses characteristic of learning, and offer a suite of geometric measures\napplicable to the study of efficient coding in higher-order cognitive processes\nmore broadly.\n", "versions": [{"version": "v1", "created": "Thu, 28 Sep 2017 16:24:13 GMT"}, {"version": "v2", "created": "Mon, 9 Oct 2017 20:44:57 GMT"}, {"version": "v3", "created": "Thu, 7 Dec 2017 02:38:29 GMT"}, {"version": "v4", "created": "Fri, 14 Sep 2018 16:23:54 GMT"}], "update_date": "2019-05-31", "authors_parsed": [["Tang", "Evelyn", ""], ["Mattar", "Marcelo G.", ""], ["Giusti", "Chad", ""], ["Thompson-Schill", "Sharon L.", ""], ["Bassett", "Danielle S.", ""]]}, {"id": "1709.10138", "submitter": "Kelly Iarosz", "authors": "F. S. Borges, E. L. Lameu, K. C. Iarosz, P. R. Protachevicz, I. L.\n  Caldas, R. L. Viana, E. E. N. Macau, A. M. Batista, M. S. Baptista", "title": "Inference of topology and the nature of synapses, and the flow of\n  information in neuronal networks", "comments": null, "journal-ref": "Phys. Rev. E 97, 022303 (2018)", "doi": "10.1103/PhysRevE.97.022303", "report-no": null, "categories": "q-bio.NC physics.bio-ph", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The characterisation of neuronal connectivity is one of the most important\nmatters in neuroscience. In this work, we show that a recently proposed\ninformational quantity, the causal mutual information, employed with an\nappropriate methodology, can be used not only to correctly infer the direction\nof the underlying physical synapses, but also to identify their excitatory or\ninhibitory nature, considering easy to handle and measure bivariate\ntime-series. The success of our approach relies on a surprising property found\nin neuronal networks by which non-adjacent neurons do \"understand\" each other\n(positive mutual information), however this exchange of information is not\ncapable of causing effect (zero transfer entropy). Remarkably, inhibitory\nconnections, responsible for enhancing synchronisation, transfer more\ninformation than excitatory connections, known to enhance entropy in the\nnetwork. We also demonstrate that our methodology can be used to correctly\ninfer directionality of synapses even in the presence of dynamic and\nobservational Gaussian noise, and is also successful in providing the effective\ndirectionality of inter modular connectivity, when only mean fields can be\nmeasured.\n", "versions": [{"version": "v1", "created": "Thu, 28 Sep 2017 19:21:37 GMT"}], "update_date": "2018-02-14", "authors_parsed": [["Borges", "F. S.", ""], ["Lameu", "E. L.", ""], ["Iarosz", "K. C.", ""], ["Protachevicz", "P. R.", ""], ["Caldas", "I. L.", ""], ["Viana", "R. L.", ""], ["Macau", "E. E. N.", ""], ["Batista", "A. M.", ""], ["Baptista", "M. S.", ""]]}, {"id": "1709.10248", "submitter": "Patrick Purdon", "authors": "Patrick A. Stokes and Patrick L. Purdon", "title": "In reply to Faes et al. and Barnett et al. regarding \"A study of\n  problems encountered in Granger causality analysis from a neuroscience\n  perspective\"", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME q-bio.NC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This reply is in response to commentaries by Barnett, Barrett, and Seth\n(arXiv:1708.08001) and Faes, Stramaglia, and Marinazzo (arXiv:1708.06990) on\nour paper entitled \"A study of problems encountered in Granger causality\nanalysis from a neuroscience perspective.\" (PNAS 114(34):7063-7072. 2017). In\nour paper, we analyzed several properties of Granger-Geweke causality (GGC) and\ndiscussed potential problems in neuroscience applications. We demonstrated: (i)\nthat GGC, estimated using separate model fits, is either severely biased,\nparticularly when the true model is known, or a high variance is introduced to\novercome the bias; and (ii) that GGC does not reflect some component dynamics\nof the system. The commentaries by both Faes et al. and Barnett et al. point\nout that the computational problems of (i) are resolved by using recent\ncomputational methods. We acknowledge that these problems are indeed resolved\nby these methods. However, the traditional computation using separate model\nfits continues to be presented and applied. More fundamentally, the\ninterpretational problems stemming from (ii) are not in anyway addressed by the\nimproved methods because they are inherent to the definition of GGC. These\nproperties are indeed acknowledged by both commentaries. We have no\nmisconception of the GGC measure and do not claim that these properties are\nfacially wrong. But we do discuss at length how these properties make it\ninappropriate and misleading for common types of scientific questions, how\npresentation of GGC results without model estimates are not decipherable, and\nhow the absence of clear statements of questions of interest present further\nopportunities for misinterpretation.\n", "versions": [{"version": "v1", "created": "Fri, 29 Sep 2017 06:19:45 GMT"}], "update_date": "2017-10-02", "authors_parsed": [["Stokes", "Patrick A.", ""], ["Purdon", "Patrick L.", ""]]}, {"id": "1709.10483", "submitter": "Alexandra Badea", "authors": "Robert J Anderson, James J Cook, Natalie A Delpratt, John C Nouls, Bin\n  Gu, James O McNamara, Brian B Avants, G Allan Johnson, Alexandra Badea", "title": "Small Animal Multivariate Brain Analysis (SAMBA): A High Throughput\n  Pipeline with a Validation Framework", "comments": "48 pages, 9 Figures, 3 Tables, 1 Suppl Table, 7 SupplementaryTables", "journal-ref": null, "doi": null, "report-no": null, "categories": "q-bio.QM q-bio.NC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  While many neuroscience questions aim to understand the human brain, much\ncurrent knowledge has been gained using animal models, which replicate genetic,\nstructural, and connectivity aspects of the human brain. While voxel-based\nanalysis (VBA) of preclinical magnetic resonance images is widely-used, a\nthorough examination of the statistical robustness, stability, and error rates\nis hindered by high computational demands of processing large arrays, and the\nmany parameters involved. Thus, workflows are often based on intuition or\nexperience, while preclinical validation studies remain scarce. To increase\nthroughput and reproducibility of quantitative small animal brain studies, we\nhave developed a publicly shared, high throughput VBA pipeline in a\nhigh-performance computing environment, called SAMBA. The increased\ncomputational efficiency allowed large multidimensional arrays to be processed\nin 1-3 days, a task that previously took ~1 month. To quantify the variability\nand reliability of preclinical VBA in rodent models, we propose a validation\nframework consisting of morphological phantoms, and four metrics. This\naddresses several sources that impact VBA results, including registration and\ntemplate construction strategies. We have used this framework to inform the VBA\nworkflow parameters in a VBA study for a mouse model of epilepsy. We also\npresent initial efforts towards standardizing small animal neuroimaging data in\na similar fashion with human neuroimaging. We conclude that verifying the\naccuracy of VBA merits attention, and should be the focus of a broader effort\nwithin the community. The proposed framework promotes consistent quality\nassurance of VBA in preclinical neuroimaging; facilitating the creation and\ncommunication of robust results.\n", "versions": [{"version": "v1", "created": "Fri, 29 Sep 2017 16:28:46 GMT"}, {"version": "v2", "created": "Mon, 21 May 2018 13:50:16 GMT"}], "update_date": "2018-05-22", "authors_parsed": [["Anderson", "Robert J", ""], ["Cook", "James J", ""], ["Delpratt", "Natalie A", ""], ["Nouls", "John C", ""], ["Gu", "Bin", ""], ["McNamara", "James O", ""], ["Avants", "Brian B", ""], ["Johnson", "G Allan", ""], ["Badea", "Alexandra", ""]]}]