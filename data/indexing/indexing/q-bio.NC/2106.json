[{"id": "2106.00172", "submitter": "Sang-Yoon  Kim", "authors": "Sang-Yoon Kim and Woochang Lim", "title": "Population and Individual Firing Behaviors in Sparsely Synchronized\n  Rhythms in The Hippocampal Dentate Gyrus", "comments": "arXiv admin note: text overlap with arXiv:2105.06057", "journal-ref": null, "doi": null, "report-no": null, "categories": "q-bio.NC physics.bio-ph", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We investigate population and individual firing behaviors in sparsely\nsynchronized rhythms (SSRs) in a spiking neural network of the hippocampal\ndentate gyrus (DG). The main encoding granule cells (GCs) are grouped into\nlamellar clusters. In each GC cluster, there is one inhibitory (I) basket cell\n(BC) along with excitatory (E) GCs, and they form the E-I loop. Winner-take-all\ncompetition, leading to sparse activation of the GCs, occurs in each GC\ncluster. Such sparsity has been thought to enhance pattern separation performed\nin the DG. During the winner-take-all competition, SSRs are found to appear in\neach population of the GCs and the BCs through interaction of excitation of the\nGCs with inhibition of the BCs. Sparsely synchronized spiking stripes appear\nsuccessively with the population frequency $f_p~ (= 13$ Hz) in the raster plots\nof spikes. We also note that excitatory hilar mossy cells (MCs) control the\nfiring activity of the GC-BC loop by providing excitation to both the GCs and\nthe BCs. SSR also appears in the population of MCs via interaction with the GCs\n(i.e., GC-MC loop). Population behaviors in the SSRs are quantitatively\ncharacterized in terms of the synchronization measures. In addition, we\ninvestigate individual firing activity of GCs, BCs, and MCs in the SSRs.\nIndividual GCs exhibit random spike skipping, leading to a multi-peaked\ninter-spike-interval histogram, which is well characterized in terms of the\nrandom phase-locking degree. On the other hand, both BCs and MCs show\n\"intrastripe\" burstings within stripes, together with \"interstripe\" random\nspike skipping. MC loss may occur during epileptogenesis. With decreasing the\nfraction of the MCs, changes in the population and individual firings in the\nSSRs are also studied. Finally, quantitative association between the\npopulation/individual firing behaviors in the SSRs and the winner-take-all\ncompetition is discussed.\n", "versions": [{"version": "v1", "created": "Tue, 1 Jun 2021 01:42:34 GMT"}], "update_date": "2021-06-02", "authors_parsed": [["Kim", "Sang-Yoon", ""], ["Lim", "Woochang", ""]]}, {"id": "2106.00637", "submitter": "Emmanuelle Tognoli", "authors": "Emmanuelle Tognoli, Daniela Benites, J. A. Scott Kelso", "title": "A Blueprint for the Study of the Brain's Spatiotemporal Patterns", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "q-bio.NC", "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/", "abstract": "  The functioning of an organ such as the brain emerges from interactions\nbetween its constituent parts. Further, this interaction is not immutable in\ntime but rather unfolds in a succession of patterns, thereby allowing the brain\nto adapt to constantly changing exterior and interior milieus. This calls for a\nframework able to study patterned spatiotemporal interactions between\ncomponents of the brain. A theoretical and methodological framework is\ndeveloped to study the brain's coordination dynamics. Here we present a toolset\ndesigned to decipher the continuous dynamics of electrophysiological data and\nits relation to (dys-) function. Understanding the spatiotemporal organization\nof brain patterns and their association with behavioral, cognitive and\nclinically-relevant variables is an important challenge for the fields of\nneuroscience and biologically-inspired engineering. It is hoped that such a\ncomprehensive framework will shed light not only on human behavior and the\nhuman mind but also help in understanding the growing number of pathologies\nthat are linked to disorders of brain connectivity.\n", "versions": [{"version": "v1", "created": "Tue, 1 Jun 2021 17:09:37 GMT"}], "update_date": "2021-06-02", "authors_parsed": [["Tognoli", "Emmanuelle", ""], ["Benites", "Daniela", ""], ["Kelso", "J. A. Scott", ""]]}, {"id": "2106.00790", "submitter": "SueYeon Chung", "authors": "SueYeon Chung", "title": "Statistical Mechanics of Neural Processing of Object Manifolds", "comments": "PhD thesis, Harvard University, Cambridge, Massachusetts, USA. 2017.\n  Some chapters report joint work", "journal-ref": null, "doi": null, "report-no": null, "categories": "q-bio.NC cond-mat.dis-nn cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Invariant object recognition is one of the most fundamental cognitive tasks\nperformed by the brain. In the neural state space, different objects with\nstimulus variabilities are represented as different manifolds. In this\ngeometrical perspective, object recognition becomes the problem of linearly\nseparating different object manifolds. In feedforward visual hierarchy, it has\nbeen suggested that the object manifold representations are reformatted across\nthe layers, to become more linearly separable. Thus, a complete theory of\nperception requires characterizing the ability of linear readout networks to\nclassify object manifolds from variable neural responses.\n  A theory of the perceptron of isolated points was pioneered by E. Gardner who\nformulated it as a statistical mechanics problem and analyzed it using replica\ntheory. In this thesis, we generalize Gardner's analysis and establish a theory\nof linear classification of manifolds synthesizing statistical and geometric\nproperties of high dimensional signals. [..] Next, we generalize our theory\nfurther to linear classification of general perceptual manifolds, such as point\nclouds. We identify that the capacity of a manifold is determined that\neffective radius, R_M, and effective dimension, D_M. Finally, we show\nextensions relevant for applications to real data, incorporating correlated\nmanifolds, heterogenous manifold geometries, sparse labels and nonlinear\nclassifications. Then, we demonstrate how object-based manifolds transform in\nstandard deep networks.\n  This thesis lays the groundwork for a computational theory of neuronal\nprocessing of objects, providing quantitative measures for linear separability\nof object manifolds. We hope this theory will provide new insights into the\ncomputational principles underlying processing of sensory representations in\nbiological and artificial neural networks.\n", "versions": [{"version": "v1", "created": "Tue, 1 Jun 2021 20:49:14 GMT"}], "update_date": "2021-06-03", "authors_parsed": [["Chung", "SueYeon", ""]]}, {"id": "2106.01306", "submitter": "Nataliya Stankevich", "authors": "Nataliya Stankevich", "title": "Stabilization of steady state in Multiplex heterogeneous networks of\n  neuron-like models with bistability between silent state and bursting\n  attractor", "comments": "6 pages, 5 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cond-mat.dis-nn nlin.AO q-bio.NC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The dynamics of a multiplex heterogeneous network of oscillators is studied.\nTwo types of similar models based on the Hodgkin-Huxley formalism are used as\nthe basic elements of the networks. The first type model demonstrates bursting\noscillations. The second model demonstrates bistability between bursting\nattractor and stable steady state. Basin of attraction of the stable\nequilibrium in the model is very small. Bistabilty is a result taking into\naccount an additional ion channel, which has a non-monotonic characteristic and\ncan be interpreted as a channel with a communication defect. Suggested\nmultiplex networks assumed more active communication between models with a\ndefect as a result in such networks it is enough to have one element with a\ncommunication defect in the subnetworks in order to stabilize the state of\nequilibrium in the entire network.\n", "versions": [{"version": "v1", "created": "Fri, 7 May 2021 20:02:23 GMT"}], "update_date": "2021-06-03", "authors_parsed": [["Stankevich", "Nataliya", ""]]}, {"id": "2106.01329", "submitter": "Giacomo Indiveri", "authors": "Giacomo Indiveri", "title": "Introducing \"Neuromorphic Computing and Engineering\"", "comments": "NCE Editorial", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.AR cs.CE cs.NE q-bio.NC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The standard nature of computing is currently being challenged by a range of\nproblems that start to hinder technological progress. One of the strategies\nbeing proposed to address some of these problems is to develop novel\nbrain-inspired processing methods and technologies, and apply them to a wide\nrange of application scenarios. This is an extremely challenging endeavor that\nrequires researchers in multiple disciplines to combine their efforts and\nco-design at the same time the processing methods, the supporting computing\narchitectures, and their underlying technologies. The journal ``Neuromorphic\nComputing and Engineering'' (NCE) has been launched to support this new\ncommunity in this effort and provide a forum and repository for presenting and\ndiscussing its latest advances. Through close collaboration with our colleagues\non the editorial team, the scope and characteristics of NCE have been designed\nto ensure it serves a growing transdisciplinary and dynamic community across\nacademia and industry.\n", "versions": [{"version": "v1", "created": "Sun, 30 May 2021 20:12:27 GMT"}], "update_date": "2021-06-03", "authors_parsed": [["Indiveri", "Giacomo", ""]]}, {"id": "2106.01683", "submitter": "Udaya B Rongala", "authors": "Udaya B. Rongala and Henrik J\\\"orntell", "title": "Rich dynamics caused by known biological brain network features\n  resulting in stateful networks", "comments": "13 pages, 5 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "q-bio.NC cs.AI cs.NE", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  The mammalian brain could contain dense and sparse network connectivity\nstructures, including both excitatory and inhibitory neurons, but is without\nany clearly defined output layer. The neurons have time constants, which mean\nthat the integrated network structure has state memory. The network structure\ncontains complex mutual interactions between the neurons under different\nconditions, which depend on the internal state of the network. The internal\nstate can be defined as the distribution of activity across all individual\nneurons across the network. Therefore, the state of a neuron/network becomes a\ndefining factor for how information is represented within the network. Towards\nthis study, we constructed a fully connected (with dense/sparse coding\nstrategies) recurrent network comprising of both excitatory and inhibitory\nneurons, driven by pseudo-random inputs of varying frequencies. In this study\nwe assessed the impact of varying specific intrinsic parameters of the neurons\nthat enriched network state dynamics, such as initial neuron activity, amount\nof inhibition in combination with thresholded neurons and conduction delays.\nThe impact was assessed by quantifying the changes in mutual interactions\nbetween the neurons within the network for each given input. We found such\neffects were more profound in sparsely connected networks than in densely\nconnected networks. However, also densely connected networks could make use of\nsuch dynamic changes in the mutual interactions between neurons, as a given\ninput could induce multiple different network states.\n", "versions": [{"version": "v1", "created": "Thu, 3 Jun 2021 08:32:43 GMT"}], "update_date": "2021-06-04", "authors_parsed": [["Rongala", "Udaya B.", ""], ["J\u00f6rntell", "Henrik", ""]]}, {"id": "2106.01684", "submitter": "Susmita Bhaduri", "authors": "Susmita Bhaduri, Anirban Bhaduri, Rajib Sarkar", "title": "Language Independent Speech Emotion and Non-invasive Early Detection of\n  Neurocognitive Disorder", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.SD eess.AS q-bio.NC", "license": "http://creativecommons.org/publicdomain/zero/1.0/", "abstract": "  Emotions(like fear,anger,sadness,happiness etc.) are the fundamental features\nof human behavior and governs his/her mental health. The subtlety of emotional\nfluctuations can be examined through perturbation in conversations or speech.\nAnalysis of emotional state of a person from acoustical features of speech\nsignal leads to discovery of vital cues determining his or her mental health.\nHence, it's an important field of research in the area of Human Computer\nInteraction(HCI). In a recent work we have shown that how the contrast in\nHurst-Exponent calculated from the non-stationary and nonlinear aspects of\n\"angry\" and \"sad\" speech(spoken in English language) recordings in the\nToronto-Emotional-Speech-Set(TESS) can be used for early detection and\ndiagnosis of Alzheimer's Disease. In this work we have extended the work and\nextracted Hurst-exponent for the speech-signals of similar emotions but spoken\nin German language. It has been observed that the Hurst-exponent efficiently\nsegregates the contrasting emotions of \"anger\" and \"sadness\" in the speech\nspoken in German language, in similar fashion it has been doing for English\nspeech. Hence it can be concluded that the Hurst-exponent can differentiate\namong speech spoken out of different emotions in language-independent manner.\nWe propose algorithm for a language-independent application for early\nnon-invasive detection of various severe neurocognitive-disorders like\nAlzheimer's Disease, MND(motor-neuron-disorder), ASD(autism-spectrum-disorder),\ndepression, suicidal-tendency etc. which is not possible with the state of the\nart medical science.\n", "versions": [{"version": "v1", "created": "Thu, 3 Jun 2021 08:38:22 GMT"}], "update_date": "2021-06-04", "authors_parsed": [["Bhaduri", "Susmita", ""], ["Bhaduri", "Anirban", ""], ["Sarkar", "Rajib", ""]]}, {"id": "2106.01951", "submitter": "Ashish Runthala", "authors": "Prabha Sankara Narayanan, Ashish Runthala", "title": "Accurate computational evolution of proteins and its dependence on deep\n  learning", "comments": "24 pages, 2 fgures", "journal-ref": null, "doi": null, "report-no": null, "categories": "q-bio.BM q-bio.NC", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Enzyme is the major workhorse to carry out the diverse cellular functions. It\ncatalyzes the biological reactions with a high specificity, with its topology\nplaying a crucial role. For ecologically safe production of numerous\nbioproducts including drugs and chemicals, we have been striving to design the\nindustrially useful enzyme molecules with highly improved catalytic capability.\nAs the sequence space is enormous for an enzyme, its quick and effective\nexploration is quite improbable for the mutagenesis studies whose accuracy is\ngreatly reliant on the prior information of the mutated sites and the extent of\nrigorous screening of the mutant libraries. Although directed evolution methods\nsignificantly aid the construction of a functionally improved molecule, their\ncredibility depends on the successful excavation of the functionally similar\nsequence space in the available databases, encompassing billions of proteins.\nAs deep learning methods aid us to extensively uncover the underlying network\nof all the key catalytic positions without any experimental data, their\nimplementation has reliably increased the accuracy of directed evolution. The\nchapter comprehensively explains data mining and deep learning methods to\nfurther showcase their importance in enzyme engineering methods. The key\nbiological and algorithmic limitations of these deep learning methodologies are\nlastly highlighted.\n", "versions": [{"version": "v1", "created": "Thu, 3 Jun 2021 15:58:59 GMT"}], "update_date": "2021-06-04", "authors_parsed": [["Narayanan", "Prabha Sankara", ""], ["Runthala", "Ashish", ""]]}, {"id": "2106.02387", "submitter": "Aleksandar Miladinovic", "authors": "Aleksandar Miladinovi\\'c, Milo\\v{s} Aj\\v{c}evi\\'c, Pierpaolo Busan,\n  Joanna Jarmolowska, Manuela Deodatod, Susanna Mezzarobba, Piero Paolo\n  Battaglini, Agostino Accardo", "title": "EEG changes and motor deficits in Parkinson's disease patients:\n  Correlation of motor scales and EEG power bands", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.SP q-bio.NC", "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/", "abstract": "  Over the years motor deficit in Parkinson's Disease (PD) patients was largely\nstudied, however, no consistent pattern of relations between quantitative\nelectroencephalography (qEEG) and motor scales emerged. There is a general lack\nof information on the relation between EEG changes and scales related to\nspecific motor deficits. Therefore, the study aimed to investigate the relation\nbetween brain oscillatory activity alterations (EEG power bands) and most used\nPD-related motor deficit scales. A positive correlation was found between the\nfreezing of the gait questionnaire (FOGQ) and delta spectral power band\n(rho=0.67; p=0.008), while a negative correlation with the same scale was\nobserved in the alpha spectral power band (rho=-0.59, p=0.027). Additionally,\nmotor scores measure by motor part of Unified Parkinson's Disease Rating Scale\n(UPDRS) correlated directly with theta (rho=0.55, p=0.040) and inversely with\nbeta EEG power band (rho=-0.77, p=0.001). No significant correlation was found\nbetween spectral powers and Hoehn and Yahr (H&Y), BERG (Berg K. et. al. 1995),\nModified Parkinson Activity Scale (MPAS), Six-Minute Walk Test (6MWT) and Timed\nUp and Go Test (TUG). In conclusion, our study supports the earlier findings\nsuggesting a link between EEG slowing and motor decline, providing more insight\ninto the relation between EEG alteration and deficits in different motor\ndomains. These findings indicate that EEG assessment may be a useful biomarker\nfor objective monitoring of progression and neurophysiological effect of\nrehabilitation approaches in PD's.\n", "versions": [{"version": "v1", "created": "Fri, 4 Jun 2021 09:59:32 GMT"}], "update_date": "2021-06-07", "authors_parsed": [["Miladinovi\u0107", "Aleksandar", ""], ["Aj\u010devi\u0107", "Milo\u0161", ""], ["Busan", "Pierpaolo", ""], ["Jarmolowska", "Joanna", ""], ["Deodatod", "Manuela", ""], ["Mezzarobba", "Susanna", ""], ["Battaglini", "Piero Paolo", ""], ["Accardo", "Agostino", ""]]}, {"id": "2106.02583", "submitter": "Alessio Franci", "authors": "Alessio Franci", "title": "Feedback design of spatially-distributed filters with tunable resolution", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.OC nlin.PS q-bio.NC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We derive gain-tuning rules for the positive and negative spatial-feedback\nloops of a spatially-distributed filter to change the resolution of its spatial\nband-pass characteristic accordingly to a wavelet zoom, while preserving\ntemporal stability. The filter design is inspired by the canonical spatial\nfeedback structure of the primary visual cortex and is motivated by\nunderstanding attentional control of visual resolution. Besides biology, our\ncontrol-theoretical design strategy is relevant for the development of\nneuromorphic multiresolution distributed sensors through the feedback\ninterconnection of elementary spatial transfer functions and gain tuning.\n", "versions": [{"version": "v1", "created": "Fri, 4 Jun 2021 16:29:42 GMT"}], "update_date": "2021-06-07", "authors_parsed": [["Franci", "Alessio", ""]]}, {"id": "2106.02626", "submitter": "Gabriel B\\'ena", "authors": "Gabriel B\\'ena, Dan F. M. Goodman", "title": "Extreme sparsity gives rise to functional specialization", "comments": "12 pages, 4 figures, Preprint (submitted to Neurips 2021)", "journal-ref": null, "doi": null, "report-no": null, "categories": "q-bio.NC cs.AI cs.LG cs.NE", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Modularity of neural networks -- both biological and artificial -- can be\nthought of either structurally or functionally, and the relationship between\nthese is an open question. We show that enforcing structural modularity via\nsparse connectivity between two dense sub-networks which need to communicate to\nsolve the task leads to functional specialization of the sub-networks, but only\nat extreme levels of sparsity. With even a moderate number of interconnections,\nthe sub-networks become functionally entangled. Defining functional\nspecialization is in itself a challenging problem without a universally agreed\nsolution. To address this, we designed three different measures of\nspecialization (based on weight masks, retraining and correlation) and found\nthem to qualitatively agree. Our results have implications in both neuroscience\nand machine learning. For neuroscience, it shows that we cannot conclude that\nthere is functional modularity simply by observing moderate levels of\nstructural modularity: knowing the brain's connectome is not sufficient for\nunderstanding how it breaks down into functional modules. For machine learning,\nusing structure to promote functional modularity -- which may be important for\nrobustness and generalization -- may require extremely narrow bottlenecks\nbetween modules.\n", "versions": [{"version": "v1", "created": "Fri, 4 Jun 2021 17:39:36 GMT"}], "update_date": "2021-06-07", "authors_parsed": [["B\u00e9na", "Gabriel", ""], ["Goodman", "Dan F. M.", ""]]}, {"id": "2106.02726", "submitter": "Mason A. Porter", "authors": "Elisa C. Baek, Ryan Hyon, Karina L\\'opez, Emily S. Finn, Mason A.\n  Porter, and Carolyn Parkinson", "title": "Popularity is linked to neural coordination: Neural evidence for an Anna\n  Karenina principle in social networks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.SI physics.soc-ph q-bio.NC stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  People differ in how they attend to, interpret, and respond to their\nsurroundings. Convergent processing of the world may be one factor that\ncontributes to social connections between individuals. We used neuroimaging and\nnetwork analysis to investigate whether the most central individuals in their\ncommunities (as measured by in-degree centrality, a notion of popularity)\nprocess the world in a particularly normative way. More central individuals had\nexceptionally similar neural responses to their peers and especially to each\nother in brain regions associated with high-level interpretations and social\ncognition (e.g., in the default-mode network), whereas less-central individuals\nexhibited more idiosyncratic responses. Self-reported enjoyment of and interest\nin stimuli followed a similar pattern, but accounting for these data did not\nchange our main results. These findings suggest an \"Anna Karenina principle\" in\nsocial networks: Highly-central individuals process the world in exceptionally\nsimilar ways, whereas less-central individuals process the world in\nidiosyncratic ways.\n", "versions": [{"version": "v1", "created": "Fri, 4 Jun 2021 21:24:08 GMT"}], "update_date": "2021-06-09", "authors_parsed": [["Baek", "Elisa C.", ""], ["Hyon", "Ryan", ""], ["L\u00f3pez", "Karina", ""], ["Finn", "Emily S.", ""], ["Porter", "Mason A.", ""], ["Parkinson", "Carolyn", ""]]}, {"id": "2106.02749", "submitter": "Bhavin Choksi", "authors": "Bhavin Choksi, Milad Mozafari, Callum Biggs O'May, Benjamin Ador,\n  Andrea Alamia, Rufin VanRullen", "title": "Predify: Augmenting deep neural networks with brain-inspired predictive\n  coding dynamics", "comments": "Preprint under review", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV q-bio.NC", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Deep neural networks excel at image classification, but their performance is\nfar less robust to input perturbations than human perception. In this work we\nexplore whether this shortcoming may be partly addressed by incorporating\nbrain-inspired recurrent dynamics in deep convolutional networks. We take\ninspiration from a popular framework in neuroscience: 'predictive coding'. At\neach layer of the hierarchical model, generative feedback 'predicts' (i.e.,\nreconstructs) the pattern of activity in the previous layer. The reconstruction\nerrors are used to iteratively update the network's representations across\ntimesteps, and to optimize the network's feedback weights over the natural\nimage dataset-a form of unsupervised training. We show that implementing this\nstrategy into two popular networks, VGG16 and EfficientNetB0, improves their\nrobustness against various corruptions. We hypothesize that other feedforward\nnetworks could similarly benefit from the proposed framework. To promote\nresearch in this direction, we provide an open-sourced PyTorch-based package\ncalled Predify, which can be used to implement and investigate the impacts of\nthe predictive coding dynamics in any convolutional neural network.\n", "versions": [{"version": "v1", "created": "Fri, 4 Jun 2021 22:48:13 GMT"}], "update_date": "2021-06-08", "authors_parsed": [["Choksi", "Bhavin", ""], ["Mozafari", "Milad", ""], ["O'May", "Callum Biggs", ""], ["Ador", "Benjamin", ""], ["Alamia", "Andrea", ""], ["VanRullen", "Rufin", ""]]}, {"id": "2106.02785", "submitter": "Kenji Doya", "authors": "Kenji Doya", "title": "Canonical Cortical Circuits and the Duality of Bayesian Inference and\n  Optimal Control", "comments": "13 pages, 3 figure", "journal-ref": null, "doi": null, "report-no": null, "categories": "q-bio.NC", "license": "http://creativecommons.org/licenses/by-sa/4.0/", "abstract": "  The duality of sensory inference and motor control has been known since the\n1960s and has recently been recognized as the commonality in computations\nrequired for the posterior distributions in Bayesian inference and the value\nfunctions in optimal control. Meanwhile, an intriguing question about the brain\nis why the entire neocortex shares a canonical six-layer architecture while its\nposterior and anterior halves are engaged in sensory processing and motor\ncontrol, respectively. Here we consider the hypothesis that the sensory and\nmotor cortical circuits implement the dual computations for Bayesian inference\nand optimal control, or perceptual and value-based decision making,\nrespectively. We first review the classic duality of inference and control in\nlinear quadratic systems and then review the correspondence between dynamic\nBayesian inference and optimal control. Based on the architecture of the\ncanonical cortical circuit, we explore how different cortical neurons may\nrepresent variables and implement computations.\n", "versions": [{"version": "v1", "created": "Sat, 5 Jun 2021 03:23:13 GMT"}, {"version": "v2", "created": "Sat, 3 Jul 2021 22:13:31 GMT"}], "update_date": "2021-07-06", "authors_parsed": [["Doya", "Kenji", ""]]}, {"id": "2106.02948", "submitter": "Yu Takagi", "authors": "Yu Takagi, Laurence T. Hunt, Ryu Ohata, Hiroshi Imamizu, Jun-ichiro\n  Hirayama", "title": "Neural dSCA: demixing multimodal interaction among brain areas during\n  naturalistic experiments", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "q-bio.NC cs.AI cs.LG", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Multi-regional interaction among neuronal populations underlies the brain's\nprocessing of rich sensory information in our daily lives. Recent neuroscience\nand neuroimaging studies have increasingly used naturalistic stimuli and\nexperimental design to identify such realistic sensory computation in the\nbrain. However, existing methods for cross-areal interaction analysis with\ndimensionality reduction, such as reduced-rank regression and canonical\ncorrelation analysis, have limited applicability and interpretability in\nnaturalistic settings because they usually do not appropriately 'demix' neural\ninteractions into those associated with different types of task parameters or\nstimulus features (e.g., visual or audio). In this paper, we develop a new\nmethod for cross-areal interaction analysis that uses the rich task or stimulus\nparameters to reveal how and what types of information are shared by different\nneural populations. The proposed neural demixed shared component analysis\ncombines existing dimensionality reduction methods with a practical neural\nnetwork implementation of functional analysis of variance with latent\nvariables, thereby efficiently demixing nonlinear effects of continuous and\nmultimodal stimuli. We also propose a simplifying alternative under the\nassumptions of linear effects and unimodal stimuli. To demonstrate our methods,\nwe analyzed two human neuroimaging datasets of participants watching\nnaturalistic videos of movies and dance movements. The results demonstrate that\nour methods provide new insights into multi-regional interaction in the brain\nduring naturalistic sensory inputs, which cannot be captured by conventional\ntechniques.\n", "versions": [{"version": "v1", "created": "Sat, 5 Jun 2021 19:16:21 GMT"}], "update_date": "2021-06-08", "authors_parsed": [["Takagi", "Yu", ""], ["Hunt", "Laurence T.", ""], ["Ohata", "Ryu", ""], ["Imamizu", "Hiroshi", ""], ["Hirayama", "Jun-ichiro", ""]]}, {"id": "2106.02953", "submitter": "Shashi Kant Gupta", "authors": "Shashi Kant Gupta, Mengmi Zhang, Chia-Chien Wu, Jeremy M. Wolfe,\n  Gabriel Kreiman", "title": "Visual Search Asymmetry: Deep Nets and Humans Share Similar Inherent\n  Biases", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV q-bio.NC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Visual search is a ubiquitous and often challenging daily task, exemplified\nby looking for the car keys at home or a friend in a crowd. An intriguing\nproperty of some classical search tasks is an asymmetry such that finding a\ntarget A among distractors B can be easier than finding B among A. To elucidate\nthe mechanisms responsible for asymmetry in visual search, we propose a\ncomputational model that takes a target and a search image as inputs and\nproduces a sequence of eye movements until the target is found. The model\nintegrates eccentricity-dependent visual recognition with target-dependent\ntop-down cues. We compared the model against human behavior in six paradigmatic\nsearch tasks that show asymmetry in humans. Without prior exposure to the\nstimuli or task-specific training, the model provides a plausible mechanism for\nsearch asymmetry. We hypothesized that the polarity of search asymmetry arises\nfrom experience with the natural environment. We tested this hypothesis by\ntraining the model on an augmented version of ImageNet where the biases of\nnatural images were either removed or reversed. The polarity of search\nasymmetry disappeared or was altered depending on the training protocol. This\nstudy highlights how classical perceptual properties can emerge in neural\nnetwork models, without the need for task-specific training, but rather as a\nconsequence of the statistical properties of the developmental diet fed to the\nmodel. All source code and stimuli are publicly available\nhttps://github.com/kreimanlab/VisualSearchAsymmetry\n", "versions": [{"version": "v1", "created": "Sat, 5 Jun 2021 19:46:42 GMT"}], "update_date": "2021-06-08", "authors_parsed": [["Gupta", "Shashi Kant", ""], ["Zhang", "Mengmi", ""], ["Wu", "Chia-Chien", ""], ["Wolfe", "Jeremy M.", ""], ["Kreiman", "Gabriel", ""]]}, {"id": "2106.03523", "submitter": "Marc Huett", "authors": "Venetia Voutsa, Demian Battaglia, Louise J. Bracken, Andrea Brovelli,\n  Julia Costescu, Mario Diaz Munoz, Brian D. Fath, Andrea Funk, Mel Guirro,\n  Thomas Hein, Christian Kerschner, Christian Kimmich, Vinicius Lima, Arnaud\n  Messe, Anthony J. Parsons, John Perez, Ronald P\\\"oppl, Christina Prell, Sonia\n  Recinos, Yanhua Shi, Shubham Tiwari, Laura Turnbull, John Wainwright, Harald\n  Waxenecker, Marc-Thorsten H\\\"utt", "title": "A stylised view on structural and functional connectivity in dynamical\n  processes in networks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "physics.soc-ph q-bio.MN q-bio.NC", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  The relationship of network structure and dynamics is one of most extensively\ninvestigated problems in the theory of complex systems of the last years.\nUnderstanding this relationship is of relevance to a range of disciplines --\nfrom Neuroscience to Geomorphology. A major strategy of investigating this\nrelationship is the quantitative comparison of a representation of network\narchitecture (structural connectivity) with a (network) representation of the\ndynamics (functional connectivity). Analysing such SC/FC relationships has over\nthe past years contributed substantially to our understanding of the functional\nrole of network properties, such as modularity, hierarchical organization, hubs\nand cycles.\n  Here, we show that one can distinguish two classes of functional connectivity\n-- one based on simultaneous activity (co-activity) of nodes the other based on\nsequential activity of nodes. We delineate these two classes in different\ncategories of dynamical processes -- excitations, regular and chaotic\noscillators -- and provide examples for SC/FC correlations of both classes in\neach of these models. We expand the theoretical view of the SC/FC\nrelationships, with conceptual instances of the SC and the two classes of FC\nfor various application scenarios in Geomorphology, Freshwater Ecology, Systems\nBiology, Neuroscience and Social-Ecological Systems.\n  Seeing the organization of a dynamical processes in a network either as\ngoverned by co-activity or by sequential activity allows us to bring some order\nin the myriad of observations relating structure and function of complex\nnetworks.\n", "versions": [{"version": "v1", "created": "Mon, 7 Jun 2021 11:29:57 GMT"}], "update_date": "2021-06-08", "authors_parsed": [["Voutsa", "Venetia", ""], ["Battaglia", "Demian", ""], ["Bracken", "Louise J.", ""], ["Brovelli", "Andrea", ""], ["Costescu", "Julia", ""], ["Munoz", "Mario Diaz", ""], ["Fath", "Brian D.", ""], ["Funk", "Andrea", ""], ["Guirro", "Mel", ""], ["Hein", "Thomas", ""], ["Kerschner", "Christian", ""], ["Kimmich", "Christian", ""], ["Lima", "Vinicius", ""], ["Messe", "Arnaud", ""], ["Parsons", "Anthony J.", ""], ["Perez", "John", ""], ["P\u00f6ppl", "Ronald", ""], ["Prell", "Christina", ""], ["Recinos", "Sonia", ""], ["Shi", "Yanhua", ""], ["Tiwari", "Shubham", ""], ["Turnbull", "Laura", ""], ["Wainwright", "John", ""], ["Waxenecker", "Harald", ""], ["H\u00fctt", "Marc-Thorsten", ""]]}, {"id": "2106.03535", "submitter": "Islem Rekik", "authors": "Alaa Bessadok, Mohamed Ali Mahjoub and Islem Rekik", "title": "Graph Neural Networks in Network Neuroscience", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG q-bio.NC", "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/", "abstract": "  Noninvasive medical neuroimaging has yielded many discoveries about the brain\nconnectivity. Several substantial techniques mapping morphological, structural\nand functional brain connectivities were developed to create a comprehensive\nroad map of neuronal activities in the human brain -namely brain graph. Relying\non its non-Euclidean data type, graph neural network (GNN) provides a clever\nway of learning the deep graph structure and it is rapidly becoming the\nstate-of-the-art leading to enhanced performance in various network\nneuroscience tasks. Here we review current GNN-based methods, highlighting the\nways that they have been used in several applications related to brain graphs\nsuch as missing brain graph synthesis and disease classification. We conclude\nby charting a path toward a better application of GNN models in network\nneuroscience field for neurological disorder diagnosis and population graph\nintegration. The list of papers cited in our work is available at\nhttps://github.com/basiralab/GNNs-in-Network-Neuroscience.\n", "versions": [{"version": "v1", "created": "Mon, 7 Jun 2021 11:49:57 GMT"}], "update_date": "2021-06-08", "authors_parsed": [["Bessadok", "Alaa", ""], ["Mahjoub", "Mohamed Ali", ""], ["Rekik", "Islem", ""]]}, {"id": "2106.03580", "submitter": "M Ganesh Kumar", "authors": "M Ganesh Kumar, Cheston Tan, Camilo Libedinsky, Shih-Cheng Yen, Andrew\n  Yong-Yi Tan", "title": "One-shot learning of paired associations by a reservoir computing model\n  with Hebbian plasticity", "comments": "16 pages, 6 figures. Code can be accessed at\n  https://github.com/mgkumar138/Oneshot_Reservoir", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NE q-bio.NC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  One-shot learning can be achieved by algorithms and animals, but how the\nlatter do it is poorly understood as most of the algorithms are not\nbiologically plausible. Experiments studying one-shot learning in rodents have\nshown that after initial gradual learning of associations between cues and\nlocations, new associations can be learned with just a single exposure to each\nnew cue-location pair. Foster, Morris and Dayan (2000) developed a hybrid\ntemporal difference - symbolic model that exhibited one-shot learning for dead\nreckoning to displaced single locations. While the temporal difference rule for\nlearning the agent's actual coordinates was biologically plausible, the model's\nsymbolic mechanism for learning target coordinates was not, and one-shot\nlearning for multiple target locations was not addressed. Here we extend the\nmodel by replacing the symbolic mechanism with a reservoir of recurrently\nconnected neurons resembling cortical microcircuitry. Biologically plausible\nlearning of target coordinates was achieved by subjecting the reservoir's\noutput weights to synaptic plasticity governed by a novel 4-factor variant of\nthe exploratory Hebbian (EH) rule. As with rodents, the reservoir model\nexhibited one-shot learning for multiple paired associations.\n", "versions": [{"version": "v1", "created": "Mon, 7 Jun 2021 13:03:51 GMT"}], "update_date": "2021-06-08", "authors_parsed": [["Kumar", "M Ganesh", ""], ["Tan", "Cheston", ""], ["Libedinsky", "Camilo", ""], ["Yen", "Shih-Cheng", ""], ["Tan", "Andrew Yong-Yi", ""]]}, {"id": "2106.03610", "submitter": "Rodrigo Felipe De Oliveira Pena", "authors": "Vinicius Lima, Rodrigo F. O. Pena, Renan O. Shimoura, Nilton L.\n  Kamiji, Cesar C. Ceballos, Fernando S. Borges, Guilherme S. V. Higa, Roberto\n  de Pasquale and Antonio C. Roque", "title": "Modeling and characterizing stochastic neurons based on in vitro\n  voltage-dependent spike probability functions", "comments": "15 pages, 5 figures", "journal-ref": null, "doi": "10.1140/epjs/s11734-021-00160-7", "report-no": null, "categories": "q-bio.NC", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Neurons in the nervous system are submitted to distinct sources of noise,\nsuch as ionic-channel and synaptic noise, which introduces variability in their\nresponses to repeated presentations of identical stimuli. This motivates the\nuse of stochastic models to describe neuronal behavior. In this work, we\ncharacterize an intrinsically stochastic neuron model based on a\nvoltage-dependent spike probability function. We determine the effect of the\nintrinsic noise in single neurons by measuring the spike time reliability and\nstudy the stochastic resonance phenomenon. The model was able to show increased\nreliability for non-zero intrinsic noise values, according to what is known\nfrom the literature, and the addition of intrinsic stochasticity in it enhanced\nthe region in which stochastic resonance is present. We proceeded to the study\nat the network level where we investigated the behavior of a random network\ncomposed of stochastic neurons. In this case, the addition of an extra\ndimension, represented by the intrinsic noise, revealed dynamic states of the\nsystem that could not be found otherwise. Finally, we propose a method to\nestimate the spike probability curve from in vitro electrophysiological data.\n", "versions": [{"version": "v1", "created": "Mon, 7 Jun 2021 13:37:14 GMT"}, {"version": "v2", "created": "Tue, 8 Jun 2021 18:46:10 GMT"}], "update_date": "2021-06-10", "authors_parsed": [["Lima", "Vinicius", ""], ["Pena", "Rodrigo F. O.", ""], ["Shimoura", "Renan O.", ""], ["Kamiji", "Nilton L.", ""], ["Ceballos", "Cesar C.", ""], ["Borges", "Fernando S.", ""], ["Higa", "Guilherme S. V.", ""], ["de Pasquale", "Roberto", ""], ["Roque", "Antonio C.", ""]]}, {"id": "2106.03688", "submitter": "Giovanni Granato G", "authors": "Giovanni Granato, Emilio Cartoni, Federico Da Rold, Andrea Mattera,\n  Gianluca Baldassarre", "title": "A Computational Model of Representation Learning in the Brain Cortex,\n  Integrating Unsupervised and Reinforcement Learning", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "q-bio.NC cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A common view on the brain learning processes proposes that the three classic\nlearning paradigms -- unsupervised, reinforcement, and supervised -- take place\nin respectively the cortex, the basal-ganglia, and the cerebellum. However,\ndopamine outbursts, usually assumed to encode reward, are not limited to the\nbasal ganglia but also reach prefrontal, motor, and higher sensory cortices. We\npropose that in the cortex the same reward-based trial-and-error processes\nmight support not only the acquisition of motor representations but also of\nsensory representations. In particular, reward signals might guide\ntrial-and-error processes that mix with associative learning processes to\nsupport the acquisition of representations better serving downstream action\nselection. We tested the soundness of this hypothesis with a computational\nmodel that integrates unsupervised learning (Contrastive Divergence) and\nreinforcement learning (REINFORCE). The model was tested with a task requiring\ndifferent responses to different visual images grouped in categories involving\neither colour, shape, or size. Results show that a balanced mix of unsupervised\nand reinforcement learning processes leads to the best performance. Indeed,\nexcessive unsupervised learning tends to under-represent task-relevant features\nwhile excessive reinforcement learning tends to initially learn slowly and then\nto incur in local minima. These results stimulate future empirical studies on\ncategory learning directed to investigate similar effects in the extrastriate\nvisual cortices. Moreover, they prompt further computational investigations\ndirected to study the possible advantages of integrating unsupervised and\nreinforcement learning processes.\n", "versions": [{"version": "v1", "created": "Mon, 7 Jun 2021 15:03:02 GMT"}], "update_date": "2021-06-08", "authors_parsed": [["Granato", "Giovanni", ""], ["Cartoni", "Emilio", ""], ["Da Rold", "Federico", ""], ["Mattera", "Andrea", ""], ["Baldassarre", "Gianluca", ""]]}, {"id": "2106.03902", "submitter": "Rodrigo Felipe De Oliveira Pena", "authors": "Cesar C. Ceballos, Rodrigo F.O. Pena, and Antonio C. Roque", "title": "Impact of the activation rate of the hyperpolarization-activated current\n  $I_{\\rm h}$ on the neuronal membrane time constant and synaptic potential\n  duration", "comments": "15 pages, 7 figures", "journal-ref": "Eur. Phys. J. Spec. Top. (2021)", "doi": "10.1140/epjs/s11734-021-00176-z", "report-no": null, "categories": "q-bio.NC q-bio.QM q-bio.SC", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  The temporal dynamics of membrane voltage changes in neurons is controlled by\nionic currents. These currents are characterized by two main properties:\nconductance and kinetics. The hyperpolarization-activated current ($I_{\\rm h}$)\nstrongly modulates subthreshold potential changes by shortening the excitatory\npostsynaptic potentials and decreasing their temporal summation. Whereas the\nshortening of the synaptic potentials caused by the $I_{\\rm h}$ conductance is\nwell understood, the role of the $I_{\\rm h}$ kinetics remains unclear. Here, we\nuse a model of the $I_{\\rm h}$ current model with either fast or slow kinetics\nto determine its influence on the membrane time constant ($\\tau_m$) of a CA1\npyramidal cell model. Our simulation results show that the $I_{\\rm h}$ with\nfast kinetics decreases $\\tau_m$ and attenuates and shortens the excitatory\npostsynaptic potentials more than the slow $I_{\\rm h}$. We conclude that the\n$I_{\\rm h}$ activation kinetics is able to modulate $\\tau_m$ and the temporal\nproperties of excitatory postsynaptic potentials (EPSPs) in CA1 pyramidal\ncells. In order to elucidate the mechanisms by which $I_{\\rm h}$ kinetics\ncontrols $\\tau_m$, we propose a new concept called \"time scaling factor\". Our\nmain finding is that the $I_{\\rm h}$ kinetics influences $\\tau_m$ by modulating\nthe contribution of the $I_{\\rm h}$ derivative conductance to $\\tau_m$.\n", "versions": [{"version": "v1", "created": "Mon, 7 Jun 2021 18:23:14 GMT"}, {"version": "v2", "created": "Sat, 12 Jun 2021 13:58:20 GMT"}], "update_date": "2021-06-15", "authors_parsed": [["Ceballos", "Cesar C.", ""], ["Pena", "Rodrigo F. O.", ""], ["Roque", "Antonio C.", ""]]}, {"id": "2106.03995", "submitter": "Mauricio Girardi-Schappo", "authors": "Renan Oliveira Shimoura, Rodrigo F. O. Pena, Vinicius Lima, Nilton L.\n  Kamiji, Mauricio Girardi-Schappo, Antonio C. Roque", "title": "Building a model of the brain: from detailed connectivity maps to\n  network organization", "comments": "35 pages, 5 figures", "journal-ref": "Eur. Phys. J. Spec. Top. (2021)", "doi": "10.1140/epjs/s11734-021-00152-7", "report-no": null, "categories": "physics.bio-ph cond-mat.dis-nn nlin.AO q-bio.NC", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  The field of computational modeling of the brain is advancing so rapidly that\nnow it is possible to model large scale networks representing different brain\nregions with a high level of biological detail in terms of numbers and\nsynapses. For a theoretician approaching a neurobiological question, it is\nimportant to analyze the pros and cons of each of the models available. Here,\nwe provide a tutorial review on recent models for different brain circuits,\nwhich are based on experimentally obtained connectivity maps. We discuss\nparticularities that may be relevant to the modeler when choosing one of the\nreviewed models. The objective of this review is to give the reader a fair\nnotion of the computational models covered, with emphasis on the corresponding\nconnectivity maps, and how to use them.\n", "versions": [{"version": "v1", "created": "Mon, 7 Jun 2021 22:37:58 GMT"}], "update_date": "2021-06-15", "authors_parsed": [["Shimoura", "Renan Oliveira", ""], ["Pena", "Rodrigo F. O.", ""], ["Lima", "Vinicius", ""], ["Kamiji", "Nilton L.", ""], ["Girardi-Schappo", "Mauricio", ""], ["Roque", "Antonio C.", ""]]}, {"id": "2106.04026", "submitter": "Dae-Hyeok Lee", "authors": "Dae-Hyeok Lee, Dong-Kyun Han, Sung-Jin Kim, Ji-Hoon Jeong, and\n  Seong-Whan Lee", "title": "Subject-Independent Brain-Computer Interface for Decoding High-Level\n  Visual Imagery Tasks", "comments": "6 pages, 3 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI eess.IV q-bio.NC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Brain-computer interface (BCI) is used for communication between humans and\ndevices by recognizing status and intention of humans. Communication between\nhumans and a drone using electroencephalogram (EEG) signals is one of the most\nchallenging issues in the BCI domain. In particular, the control of drone\nswarms (the direction and formation) has more advantages compared to the\ncontrol of a drone. The visual imagery (VI) paradigm is that subjects visually\nimagine specific objects or scenes. Reduction of the variability among EEG\nsignals of subjects is essential for practical BCI-based systems. In this\nstudy, we proposed the subepoch-wise feature encoder (SEFE) to improve the\nperformances in the subject-independent tasks by using the VI dataset. This\nstudy is the first attempt to demonstrate the possibility of generalization\namong subjects in the VI-based BCI. We used the leave-one-subject-out\ncross-validation for evaluating the performances. We obtained higher\nperformances when including our proposed module than excluding our proposed\nmodule. The DeepConvNet with SEFE showed the highest performance of 0.72 among\nsix different decoding models. Hence, we demonstrated the feasibility of\ndecoding the VI dataset in the subject-independent task with robust\nperformances by using our proposed module.\n", "versions": [{"version": "v1", "created": "Tue, 8 Jun 2021 00:39:31 GMT"}], "update_date": "2021-06-09", "authors_parsed": [["Lee", "Dae-Hyeok", ""], ["Han", "Dong-Kyun", ""], ["Kim", "Sung-Jin", ""], ["Jeong", "Ji-Hoon", ""], ["Lee", "Seong-Whan", ""]]}, {"id": "2106.04089", "submitter": "David Clark", "authors": "David G. Clark, L. F. Abbott, SueYeon Chung", "title": "Credit Assignment Through Broadcasting a Global Error Vector", "comments": "18 pages, 6 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "q-bio.NC cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Backpropagation (BP) uses detailed, unit-specific feedback to train deep\nneural networks (DNNs) with remarkable success. That biological neural circuits\nappear to perform credit assignment, but cannot implement BP, implies the\nexistence of other powerful learning algorithms. Here, we explore the extent to\nwhich a globally broadcast learning signal, coupled with local weight updates,\nenables training of DNNs. We present both a learning rule, called global\nerror-vector broadcasting (GEVB), and a class of DNNs, called vectorized\nnonnegative networks (VNNs), in which this learning rule operates. VNNs have\nvector-valued units and nonnegative weights past the first layer. The GEVB\nlearning rule generalizes three-factor Hebbian learning, updating each weight\nby an amount proportional to the inner product of the presynaptic activation\nand a globally broadcast error vector when the postsynaptic unit is active. We\nprove that these weight updates are matched in sign to the gradient, enabling\naccurate credit assignment. Moreover, at initialization, these updates are\nexactly proportional to the gradient in the limit of infinite network width.\nGEVB matches the performance of BP in VNNs, and in some cases outperforms\ndirect feedback alignment (DFA) applied in conventional networks. Unlike DFA,\nGEVB successfully trains convolutional layers. Altogether, our theoretical and\nempirical results point to a surprisingly powerful role for a global learning\nsignal in training DNNs.\n", "versions": [{"version": "v1", "created": "Tue, 8 Jun 2021 04:08:46 GMT"}], "update_date": "2021-06-09", "authors_parsed": [["Clark", "David G.", ""], ["Abbott", "L. F.", ""], ["Chung", "SueYeon", ""]]}, {"id": "2106.04123", "submitter": "Giorgio Papitto", "authors": "Giorgio Papitto, Luisa Lugli, Anna M. Borghi, Antonello Pellicano and\n  Ferdinand Binkofski", "title": "Embodied negation and levels of concreteness: A TMS Study on German and\n  Italian language processing", "comments": "30 pages, 3 figures, 1 table, research paper", "journal-ref": null, "doi": "10.1016/j.brainres.2021.147523", "report-no": null, "categories": "q-bio.NC", "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/", "abstract": "  According to the embodied cognition perspective, linguistic negation may\nblock the motor simulations induced by language processing. Transcranial\nmagnetic stimulation (TMS) was applied to the left primary motor cortex (hand\narea) of monolingual Italian and German healthy participants during a rapid\nserial visual presentation of sentences from their own language. In these\nlanguages, the negative particle is located at the beginning and at the end of\nthe sentence, respectively. The study investigated whether the interruption of\nthe motor simulation processes, accounted for by reduced motor evoked\npotentials (MEPs), takes place similarly in two languages differing on the\nposition of the negative marker. Different levels of sentence concreteness were\nalso manipulated to investigate if negation exerts generalized effects or if it\nis affected by the semantic features of the sentence. Our findings indicate\nthat negation acts as a block on motor representations, but independently from\nthe language and words concreteness level.\n", "versions": [{"version": "v1", "created": "Tue, 8 Jun 2021 06:19:47 GMT"}], "update_date": "2021-06-09", "authors_parsed": [["Papitto", "Giorgio", ""], ["Lugli", "Luisa", ""], ["Borghi", "Anna M.", ""], ["Pellicano", "Antonello", ""], ["Binkofski", "Ferdinand", ""]]}, {"id": "2106.04225", "submitter": "Andrea Alamia", "authors": "Andrea Alamia, Milad Mozafari, Bhavin Choksi and Rufin VanRullen", "title": "On the role of feedback in visual processing: a predictive coding\n  perspective", "comments": "'Andrea Alamia' and 'Milad Mozafari' contributed equally to this work", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV q-bio.NC", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Brain-inspired machine learning is gaining increasing consideration,\nparticularly in computer vision. Several studies investigated the inclusion of\ntop-down feedback connections in convolutional networks; however, it remains\nunclear how and when these connections are functionally helpful. Here we\naddress this question in the context of object recognition under noisy\nconditions. We consider deep convolutional networks (CNNs) as models of\nfeed-forward visual processing and implement Predictive Coding (PC) dynamics\nthrough feedback connections (predictive feedback) trained for reconstruction\nor classification of clean images. To directly assess the computational role of\npredictive feedback in various experimental situations, we optimize and\ninterpret the hyper-parameters controlling the network's recurrent dynamics.\nThat is, we let the optimization process determine whether top-down connections\nand predictive coding dynamics are functionally beneficial. Across different\nmodel depths and architectures (3-layer CNN, ResNet18, and EfficientNetB0) and\nagainst various types of noise (CIFAR100-C), we find that the network\nincreasingly relies on top-down predictions as the noise level increases; in\ndeeper networks, this effect is most prominent at lower layers. In addition,\nthe accuracy of the network implementing PC dynamics significantly increases\nover time-steps, compared to its equivalent forward network. All in all, our\nresults provide novel insights relevant to Neuroscience by confirming the\ncomputational role of feedback connections in sensory systems, and to Machine\nLearning by revealing how these can improve the robustness of current vision\nmodels.\n", "versions": [{"version": "v1", "created": "Tue, 8 Jun 2021 10:07:23 GMT"}], "update_date": "2021-06-09", "authors_parsed": [["Alamia", "Andrea", ""], ["Mozafari", "Milad", ""], ["Choksi", "Bhavin", ""], ["VanRullen", "Rufin", ""]]}, {"id": "2106.04316", "submitter": "Noor Sajid", "authors": "Noor Sajid, Panagiotis Tigas, Alexey Zakharov, Zafeirios Fountas and\n  Karl Friston", "title": "Exploration and preference satisfaction trade-off in reward-free\n  learning", "comments": "23 pages, 15 figures", "journal-ref": "Proceedings of the Unsupervised Reinforcement Learning Workshop\n  ICML 2021", "doi": null, "report-no": null, "categories": "cs.AI q-bio.NC", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Biological agents have meaningful interactions with their environment despite\nthe absence of immediate reward signals. In such instances, the agent can learn\npreferred modes of behaviour that lead to predictable states -- necessary for\nsurvival. In this paper, we pursue the notion that this learnt behaviour can be\na consequence of reward-free preference learning that ensures an appropriate\ntrade-off between exploration and preference satisfaction. For this, we\nintroduce a model-based Bayesian agent equipped with a preference learning\nmechanism (pepper) using conjugate priors. These conjugate priors are used to\naugment the expected free energy planner for learning preferences over states\n(or outcomes) across time. Importantly, our approach enables the agent to learn\npreferences that encourage adaptive behaviour at test time. We illustrate this\nin the OpenAI Gym FrozenLake and the 3D mini-world environments -- with and\nwithout volatility. Given a constant environment, these agents learn confident\n(i.e., precise) preferences and act to satisfy them. Conversely, in a volatile\nsetting, perpetual preference uncertainty maintains exploratory behaviour. Our\nexperiments suggest that learnable (reward-free) preferences entail a trade-off\nbetween exploration and preference satisfaction. Pepper offers a\nstraightforward framework suitable for designing adaptive agents when reward\nfunctions cannot be predefined as in real environments.\n", "versions": [{"version": "v1", "created": "Tue, 8 Jun 2021 13:24:58 GMT"}, {"version": "v2", "created": "Sun, 18 Jul 2021 18:41:52 GMT"}], "update_date": "2021-07-20", "authors_parsed": [["Sajid", "Noor", ""], ["Tigas", "Panagiotis", ""], ["Zakharov", "Alexey", ""], ["Fountas", "Zafeirios", ""], ["Friston", "Karl", ""]]}, {"id": "2106.04366", "submitter": "Rodrigo Echeveste", "authors": "Rodrigo Echeveste, Enzo Ferrante, Diego Milone, In\\'es Samengo", "title": "A bridge between physiological and perceptual views of autism by means\n  of sampling-based Bayesian inference", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "q-bio.NC", "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/", "abstract": "  Theories for autism spectrum disorder (ASD) have been formulated at different\nlevels: ranging from physiological observations to perceptual and behavioural\ndescriptions. Understanding the physiological underpinnings of perceptual\ntraits in ASD remains a significant challenge in the field. Here we show how a\nrecurrent neural circuit model which had been optimized to perform\nsampling-based inference and displays characteristic features of cortical\ndynamics can help bridge this gap. The model was able to establish a\nmechanistic link between two descriptive levels for ASD: a physiological level,\nin terms of inhibitory dysfunction, neural variability and oscillations, and a\nperceptual level, in terms of hypopriors in Bayesian computations. We took two\nparallel paths: inducing hypopriors in the probabilistic model, and an\ninhibitory dysfunction in the network model, which lead to consistent results\nin terms of the represented posteriors, providing support for the view that\nboth descriptions might constitute two sides of the same coin.\n", "versions": [{"version": "v1", "created": "Tue, 8 Jun 2021 14:06:13 GMT"}], "update_date": "2021-06-09", "authors_parsed": [["Echeveste", "Rodrigo", ""], ["Ferrante", "Enzo", ""], ["Milone", "Diego", ""], ["Samengo", "In\u00e9s", ""]]}, {"id": "2106.04427", "submitter": "Alexander Hepburn", "authors": "Alexander Hepburn and Valero Laparra and Raul Santos-Rodriguez and\n  Johannes Ball\\'e and Jes\\'us Malo", "title": "On the relation between statistical learning and perceptual distances", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV eess.IV q-bio.NC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  It has been demonstrated many times that the behavior of the human visual\nsystem is connected to the statistics of natural images. Since machine learning\nrelies on the statistics of training data as well, the above connection has\ninteresting implications when using perceptual distances (which mimic the\nbehavior of the human visual system) as a loss function. In this paper, we aim\nto unravel the non-trivial relationship between the probability distribution of\nthe data, perceptual distances, and unsupervised machine learning. To this end,\nwe show that perceptual sensitivity is correlated with the probability of an\nimage in its close neighborhood. We also explore the relation between distances\ninduced by autoencoders and the probability distribution of the data used for\ntraining them, as well as how these induced distances are correlated with human\nperception. Finally, we discuss why perceptual distances might not lead to\nnoticeable gains in performance over standard Euclidean distances in common\nimage processing tasks except when data is scarce and the perceptual distance\nprovides regularization.\n", "versions": [{"version": "v1", "created": "Tue, 8 Jun 2021 14:56:56 GMT"}], "update_date": "2021-06-09", "authors_parsed": [["Hepburn", "Alexander", ""], ["Laparra", "Valero", ""], ["Santos-Rodriguez", "Raul", ""], ["Ball\u00e9", "Johannes", ""], ["Malo", "Jes\u00fas", ""]]}, {"id": "2106.04540", "submitter": "Jordan Lei", "authors": "Jordan Lei, Ari S. Benjamin, Konrad P. Kording", "title": "Object Based Attention Through Internal Gating", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "q-bio.NC cs.AI cs.CV cs.LG cs.NE", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Object-based attention is a key component of the visual system, relevant for\nperception, learning, and memory. Neurons tuned to features of attended objects\ntend to be more active than those associated with non-attended objects. There\nis a rich set of models of this phenomenon in computational neuroscience.\nHowever, there is currently a divide between models that successfully match\nphysiological data but can only deal with extremely simple problems and models\nof attention used in computer vision. For example, attention in the brain is\nknown to depend on top-down processing, whereas self-attention in deep learning\ndoes not. Here, we propose an artificial neural network model of object-based\nattention that captures the way in which attention is both top-down and\nrecurrent. Our attention model works well both on simple test stimuli, such as\nthose using images of handwritten digits, and on more complex stimuli, such as\nnatural images drawn from the COCO dataset. We find that our model replicates a\nrange of findings from neuroscience, including attention-invariant tuning,\ninhibition of return, and attention-mediated scaling of activity. Understanding\nobject based attention is both computationally interesting and a key problem\nfor computational neuroscience.\n", "versions": [{"version": "v1", "created": "Tue, 8 Jun 2021 17:20:50 GMT"}], "update_date": "2021-06-09", "authors_parsed": [["Lei", "Jordan", ""], ["Benjamin", "Ari S.", ""], ["Kording", "Konrad P.", ""]]}, {"id": "2106.04651", "submitter": "Benjamin Hayden", "authors": "Justin M. Fine and Benjamin Y. Hayden", "title": "The whole prefrontal cortex is premotor cortex", "comments": "1 figure", "journal-ref": null, "doi": null, "report-no": null, "categories": "q-bio.NC", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  We propose that the entirety of the prefrontal cortex can be seen as\nfundamentally premotor in nature. By this, we mean that the prefrontal cortex\nconsists of an action abstraction hierarchy whose core function is the\npotentiation and depotentiation of possible action plans at different levels of\ngranularity. We argue that the apex of the hierarchy should revolve around the\nprocess of goal selection, which we posit is inherently a form of abstract\naction optimization. Anatomical and functional evidence supports the idea that\nthis hierarchy originates on the orbital surface of the brain and extends\ndorsally to motor cortex. Our view, therefore, positions the orbitofrontal\ncortex as the central site for the optimization of goal selection policies, and\nsuggests that other proposed roles are aspects of this more general function.\nWe conclude by proposing that the dynamical systems approach, which works well\nin motor systems, can be extended to the rest of prefrontal cortex. Our\nproposed perspective will reframe outstanding questions, open up new areas of\ninquiry, and will align theories of prefrontal function with evolutionary\nprinciples.\n", "versions": [{"version": "v1", "created": "Tue, 8 Jun 2021 19:29:05 GMT"}], "update_date": "2021-06-10", "authors_parsed": [["Fine", "Justin M.", ""], ["Hayden", "Benjamin Y.", ""]]}, {"id": "2106.04845", "submitter": "Philippe Robert S.", "authors": "Philippe Robert and Gaetan Vignoud", "title": "Stochastic Models of Neural Plasticity: A Scaling Approach", "comments": "arXiv admin note: substantial text overlap with arXiv:2010.08195", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.PR q-bio.NC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In neuroscience, synaptic plasticity refers to the set of mechanisms driving\nthe dynamics of neuronal connections, called synapses and represented by a\nscalar value, the synaptic weight. A Spike-Timing Dependent Plasticity (STDP)\nrule is a biologically-based model representing the time evolution of the\nsynaptic weight as a functional of the past spiking activity of adjacent\nneurons. A general mathematical framework has been introduced in [37].\n  In this paper we develop and investigate a scaling approach of these models\nbased on several biological assumptions. Experiments show that long-term\nsynaptic plasticity evolves on a much slower timescale than the cellular\nmechanisms driving the activity of neuronal cells, like their spiking activity\nor the concentration of various chemical components created/suppressed by this\nspiking activity. For this reason, a scaled version of the stochastic model of\n[37] is introduced and a limit theorem, an averaging principle, is stated for a\nlarge class of plasticity kernels. A companion paper [36] is entirely devoted\nto the tightness properties used to prove these convergence results.\n  These averaging principles are used to study two important STDP models:\npair-based rules and calcium-based rules. Our results are compared with the\napproximations of neuroscience STDP models. A class of discrete models of STDP\nrules is also investigated for the analytical tractability of its limiting\ndynamical system.\n", "versions": [{"version": "v1", "created": "Wed, 9 Jun 2021 07:08:45 GMT"}], "update_date": "2021-06-10", "authors_parsed": [["Robert", "Philippe", ""], ["Vignoud", "Gaetan", ""]]}, {"id": "2106.05113", "submitter": "Guy Gaziv", "authors": "Guy Gaziv, Michal Irani", "title": "More than meets the eye: Self-supervised depth reconstruction from brain\n  activity", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG q-bio.NC", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  In the past few years, significant advancements were made in reconstruction\nof observed natural images from fMRI brain recordings using deep-learning\ntools. Here, for the first time, we show that dense 3D depth maps of observed\n2D natural images can also be recovered directly from fMRI brain recordings. We\nuse an off-the-shelf method to estimate the unknown depth maps of natural\nimages. This is applied to both: (i) the small number of images presented to\nsubjects in an fMRI scanner (images for which we have fMRI recordings -\nreferred to as \"paired\" data), and (ii) a very large number of natural images\nwith no fMRI recordings (\"unpaired data\"). The estimated depth maps are then\nused as an auxiliary reconstruction criterion to train for depth reconstruction\ndirectly from fMRI. We propose two main approaches: Depth-only recovery and\njoint image-depth RGBD recovery. Because the number of available \"paired\"\ntraining data (images with fMRI) is small, we enrich the training data via\nself-supervised cycle-consistent training on many \"unpaired\" data (natural\nimages & depth maps without fMRI). This is achieved using our newly defined and\ntrained Depth-based Perceptual Similarity metric as a reconstruction criterion.\nWe show that predicting the depth map directly from fMRI outperforms its\nindirect sequential recovery from the reconstructed images. We further show\nthat activations from early cortical visual areas dominate our depth\nreconstruction results, and propose means to characterize fMRI voxels by their\ndegree of depth-information tuning. This work adds an important layer of\ndecoded information, extending the current envelope of visual brain decoding\ncapabilities.\n", "versions": [{"version": "v1", "created": "Wed, 9 Jun 2021 14:46:09 GMT"}], "update_date": "2021-06-10", "authors_parsed": [["Gaziv", "Guy", ""], ["Irani", "Michal", ""]]}, {"id": "2106.05180", "submitter": "Jiajia Li", "authors": "JiaJia Li, Peihua Feng, Liang Zhao, Junying Chen, Mengmeng Du,\n  Yangyang Yu, Ying Wu", "title": "Theoretical Implementation of Stochastic Epileptic Oscillator Using a\n  Tripartite Synaptic Neuronal Network Model", "comments": "21 pages, 8 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "q-bio.NC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Epilepsy is a neurological disorder, always coming with abnormal brain\nseizures activity of complex randomness and unpredictability, which have\nbrought obstacles for the improvement of epileptic treatment. However, the\nmechanism of the epileptic randomness has not been unfolded. Inspired by the\nrecent experimental finding that astrocyte G protein coupled receptor involves\nin the stochastic epileptic seizures, we proposed a cortical tripartite\nsynaptic network of neurons and astrocytes with the G protein noise, which is\ncapable to explain the stochastic process of epileptic seizures in the\ninvolvement of the G protein noise, a Gaussian distributed noise which explains\nthe heterogeneity of the G protein types and the nonuniform environmental\neffect. Based on this model, we have discussed the dynamical stochastic\ninduction process of the epileptic seizures statistically by performing totally\n60 simulation trials. Our simulation results showed that the increase of the\nnoise intensity could induce the epileptic seizures state coexisting with the\nincrease of frequency and in vitro epileptic depolarization blocks. Meanwhile,\nthere has been a bistable state of the noise intensity for the neurons\nswitching among the regular sparse spiking and the epileptic seizure state.\nThis random presence of epileptic seizure state would be absent when the noise\nintensity continues to increase, then the neurons start to stay in epileptic\nseizures steadily, but with an increase of the epileptic depolarization block\nduration. The simulation results also shed light on the fact that the calcium\nsignals in astrocytes have played significant roles in the pattern formation of\nboth the random and steady epileptic seizure state. Our results on the\nstochastic process of the epileptic seizures could provide potential theory for\nimprovement for the epileptic prediction and treatment.\n", "versions": [{"version": "v1", "created": "Wed, 26 May 2021 15:42:06 GMT"}], "update_date": "2021-06-10", "authors_parsed": [["Li", "JiaJia", ""], ["Feng", "Peihua", ""], ["Zhao", "Liang", ""], ["Chen", "Junying", ""], ["Du", "Mengmeng", ""], ["Yu", "Yangyang", ""], ["Wu", "Ying", ""]]}, {"id": "2106.05181", "submitter": "Cheng Qian", "authors": "Cheng Qian", "title": "Condition Integration Memory Network: An Interpretation of the Meaning\n  of the Neuronal Design", "comments": "39 pages, 6 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "q-bio.NC cs.NE", "license": "http://creativecommons.org/publicdomain/zero/1.0/", "abstract": "  This document introduces a hypothesized framework on the functional nature of\nprimitive neural network. It discusses such an idea that the activity of\nneurons and synapses can symbolically reenact the dynamic changes in the world\nand enable an adaptive system of behavior. More specifically, the network\nachieves these without participating in an algorithmic structure. When a\nneuron's activation represents some symbolic element in the environment, each\nof its synapses can indicate a potential change to the element and its future\nstate. The efficacy of a synaptic connection further specifies the element's\nparticular probability for, or contribution to, such a change. A neuron's\nactivation is transformed to its postsynaptic targets as it fires, resulting in\na chronological shift of the represented elements. As the inherent function of\nsummation in a neuron integrates the various presynaptic contributions, the\nneural network mimics the collective causal relationship of events in the\nobserved environment.\n", "versions": [{"version": "v1", "created": "Fri, 21 May 2021 05:59:27 GMT"}], "update_date": "2021-06-10", "authors_parsed": [["Qian", "Cheng", ""]]}, {"id": "2106.05186", "submitter": "Madhavun Candadai", "authors": "Madhavun Candadai", "title": "Information theoretic analysis of computational models as a tool to\n  understand the neural basis of behaviors", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "q-bio.NC cs.IT cs.NE math.IT", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  One of the greatest research challenges of this century is to understand the\nneural basis for how behavior emerges in brain-body-environment systems. To\nthis end, research has flourished along several directions but have\npredominantly focused on the brain. While there is in an increasing acceptance\nand focus on including the body and environment in studying the neural basis of\nbehavior, animal researchers are often limited by technology or tools.\nComputational models provide an alternative framework within which one can\nstudy model systems where ground-truth can be measured and interfered with.\nThese models act as a hypothesis generation framework that would in turn guide\nexperimentation. Furthermore, the ability to intervene as we please, allows us\nto conduct in-depth analysis of these models in a way that cannot be performed\nin natural systems. For this purpose, information theory is emerging as a\npowerful tool that can provide insights into the operation of these\nbrain-body-environment models. In this work, I provide an introduction, a\nreview and discussion to make a case for how information theoretic analysis of\ncomputational models is a potent research methodology to help us better\nunderstand the neural basis of behavior.\n", "versions": [{"version": "v1", "created": "Wed, 2 Jun 2021 02:08:18 GMT"}], "update_date": "2021-06-10", "authors_parsed": [["Candadai", "Madhavun", ""]]}, {"id": "2106.05191", "submitter": "Andrei Khrennikov Yu", "authors": "Andrei Khrennikov", "title": "Quantum-like model for unconscious-conscious interaction and emotional\n  coloring of perceptions and other conscious experiences", "comments": "submitted to BioSystems", "journal-ref": null, "doi": null, "report-no": null, "categories": "q-bio.NC quant-ph", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Quantum measurement theory is applied to quantum-like modeling of coherent\ngeneration of perceptions and emotions and generally for emotional coloring of\nconscious experiences. In quantum theory, a system should be separated from an\nobserver. The brain performs self-measurements. To model them, we split the\nbrain into two subsystems, unconsciousness and consciousness. They correspond\nto a system and an observer. The states of perceptions and emotions are\ndescribed through the tensor product decomposition of the unconscious state\nspace; similarly, there are two classes of observables, for conscious\nexperiencing of perceptions and emotions, respectively. Emotional coloring is\ncoupled to quantum contextuality: emotional observables determine contexts.\nSuch contextualization reduces degeneration of unconscious states. The\nquantum-like approach should be distinguished from consideration of the genuine\nquantum physical processes in the brain (cf. Penrose and Hameroff). In our\napproach the brain is a macroscopic system which information processing can be\ndescribed by the formalism of quantum theory.\n", "versions": [{"version": "v1", "created": "Sun, 6 Jun 2021 17:40:07 GMT"}], "update_date": "2021-06-10", "authors_parsed": [["Khrennikov", "Andrei", ""]]}, {"id": "2106.05388", "submitter": "Jiabin Tang", "authors": "Jiabin Tang, Shivani Patel, Steve Gentleman, Paul Matthews", "title": "Neurological Consequences of COVID-19 Infection", "comments": "19 pages, 4 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "q-bio.NC q-bio.MN", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  COVID-19 infections have well described systemic manifestations, especially\nrespiratory problems. There are currently no specific treatments or vaccines\nagainst the current strain. With higher case numbers, a range of neurological\nsymptoms are becoming apparent. The mechanisms responsible for these are not\nwell defined, other than those related to hypoxia and microthrombi. We\nspeculate that sustained systemic immune activation seen with SARS-CoV-2 may\nalso cause secondary autoimmune activation in the CNS. Patients with chronic\nneurological diseases may be at higher risk because of chronic secondary\nrespiratory disease and potentially poor nutritional status. Here, we review\nthe impact of COVID-19 on people with chronic neurological diseases and\npotential mechanisms. We believe special attention to protecting people with\nneurodegenerative disease is warranted. We are concerned about a possible\ndelayed pandemic in the form of an increased burden of neurodegenerative\ndisease after acceleration of pathology by systemic COVID-19 infections.\n", "versions": [{"version": "v1", "created": "Wed, 9 Jun 2021 21:02:12 GMT"}], "update_date": "2021-06-11", "authors_parsed": [["Tang", "Jiabin", ""], ["Patel", "Shivani", ""], ["Gentleman", "Steve", ""], ["Matthews", "Paul", ""]]}, {"id": "2106.05538", "submitter": "William Winlow Professor", "authors": "William Winlow and Andrew Simon Johnson", "title": "Nerve Impulses Have Three Interdependent Functions: Communication,\n  Modulation And Computation", "comments": "Keywords: Nerve impulse, Physiological Action potential, Soliton,\n  Action potential pulse Computational action potential.10 pages, 4 figures, 1\n  table", "journal-ref": null, "doi": null, "report-no": null, "categories": "q-bio.NC", "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/", "abstract": "  Comprehending the nature of action potentials is fundamental to our\nunderstanding of the functioning of nervous systems in general. Here we\nconsider their evolution and describe their functions of communication,\nmodulation and computation within nervous systems. The ionic mechanisms\nunderlying action potentials in the squid giant axon were first described by\nHodgkin and Huxley in 1952 and their findings have formed our orthodox view of\nhow the physiological action potential functions. However, substantial evidence\nhas now accumulated to show that the action potential is accompanied by a\nsynchronized coupled soliton pressure pulse in the cell membrane, the action\npotential pulse (APPulse). Here we explore the interactions between the soliton\nand the ionic mechanisms known to be associated with the action potential.\nComputational models of the action potential usually describe it as a binary\nevent, but we suggest that it is quantum ternary event known as the\ncomputational action potential (CAP), whose temporal fixed point is threshold,\nrather than the rather plastic action potential peak used in other models. The\nCAP accompanies the APPulse and the Physiological action potential. Therefore,\nwe conclude that nerve impulses appear to be an ensemble of three inseparable,\ninterdependent, concurrent states: the physiological action potential, the\nAPPulse and the CAP.\n", "versions": [{"version": "v1", "created": "Thu, 10 Jun 2021 06:59:06 GMT"}], "update_date": "2021-06-11", "authors_parsed": [["Winlow", "William", ""], ["Johnson", "Andrew Simon", ""]]}, {"id": "2106.05591", "submitter": "Kohei Ichikawa", "authors": "Kohei Ichikawa, Asaki Kataoka", "title": "Dynamical Mechanism of Sampling-based Stochastic Inference under\n  Probabilistic Population Codes", "comments": "22 pages, 12 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "q-bio.NC", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Animals are known to make efficient probabilistic inferences based on\nuncertain and noisy information from the outside world. Although it is known\nthat generic neural networks can perform near-optimal point estimation by\nprobabilistic population codes which has been proposed as a neural basis for\nencoding of probability distribution, the mechanisms of sampling-based\ninference has not been clarified. In this study, we trained two types of\nartificial neural networks: feedforward neural networks (FFNNs) and recurrent\nneural networks (RNNs) to perform sampling based probabilistic inference. Then,\nwe analyzed and compared the mechanisms of sampling in the RNN with those in\nthe FFNN. As a result, it was found that sampling in RNN is performed by a\nmechanism that efficiently utilizes the properties of dynamical systems, unlike\nFFNN. It was also found that sampling in RNNs acts as an inductive bias,\nenabling more accurate estimation than in MAP estimation. These results will\nprovide important implications for the discussion of the relationship between\ndynamical systems and information processing in neural networks.\n", "versions": [{"version": "v1", "created": "Thu, 10 Jun 2021 08:53:05 GMT"}], "update_date": "2021-06-11", "authors_parsed": [["Ichikawa", "Kohei", ""], ["Kataoka", "Asaki", ""]]}, {"id": "2106.05783", "submitter": "Leonardo Dalla Porta", "authors": "Leonardo Dalla Porta, Daniel M. Castro, Mauro Copelli, Pedro V.\n  Carelli, and Fernanda S. Matias", "title": "Feedforward and feedback influences through distinct frequency bands\n  between two spiking-neuron networks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "q-bio.NC", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Several studies with brain signals suggested that bottom-up and top-down\ninfluences are exerted through distinct frequency bands among visual cortical\nareas. It has been recently shown that theta and gamma rhythms subserve\nfeedforward, whereas the feedback influence is dominated by the alpha-beta\nrhythm in primates. A few theoretical models for reproducing these effects have\nbeen proposed so far. Here we show that a simple but biophysically plausible\ntwo-network motif composed of spiking-neuron models and chemical synapses can\nexhibit feedforward and feedback influences through distinct frequency bands.\nDifferently from previous studies, this kind of model allows us to study\ndirected influences not only at the population level, by using a proxy for the\nlocal field potential, but also at the cellular level, by using the neuronal\nspiking series.\n", "versions": [{"version": "v1", "created": "Thu, 10 Jun 2021 14:34:03 GMT"}], "update_date": "2021-06-11", "authors_parsed": [["Porta", "Leonardo Dalla", ""], ["Castro", "Daniel M.", ""], ["Copelli", "Mauro", ""], ["Carelli", "Pedro V.", ""], ["Matias", "Fernanda S.", ""]]}, {"id": "2106.06029", "submitter": "Pedro F da Costa", "authors": "Pedro F. da Costa, Rianne Haartsen, Elena Throm, Luke Mason, Anna Gui,\n  Robert Leech, Emily J.H. Jones", "title": "Neuroadaptive electroencephalography: a proof-of-principle study in\n  infants", "comments": "25 pages, 4 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "q-bio.NC q-bio.QM", "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/", "abstract": "  A core goal of functional neuroimaging is to study how the environment is\nprocessed in the brain. The mainstream paradigm involves concurrently measuring\na broad spectrum of brain responses to a small set of environmental features\npreselected with reference to previous studies or a theoretical framework. As a\ncomplement, we invert this approach by allowing the investigator to record the\nmodulation of a preselected brain response by a broad spectrum of environmental\nfeatures. Our approach is optimal when theoretical frameworks or previous\nempirical data are impoverished. By using a prespecified closed-loop design,\nthe approach addresses fundamental challenges of reproducibility and\ngeneralisability in brain research. These conditions are particularly acute\nwhen studying the developing brain, where our theories based on adult brain\nfunction may fundamentally misrepresent the topography of infant cognition and\nwhere there are substantial practical challenges to data acquisition. Our\nmethodology employs machine learning to map modulation of a neural feature\nacross a space of experimental stimuli. Our method collects, processes and\nanalyses EEG brain data in real-time; and uses a neuro-adaptive Bayesian\noptimisation algorithm to adjust the stimulus presented depending on the prior\nsamples of a given participant. Unsampled stimuli can be interpolated by\nfitting a Gaussian process regression along the dataset. We show that our\nmethod can automatically identify the face of the infant's mother through\nonline recording of their Nc brain response to a face continuum. We can\nretrieve model statistics of individualised responses for each participant,\nopening the door for early identification of atypical development. This\napproach has substantial potential in infancy research and beyond for improving\npower and generalisability of mapping the individual cognitive topography of\nbrain function.\n", "versions": [{"version": "v1", "created": "Thu, 10 Jun 2021 20:13:04 GMT"}], "update_date": "2021-06-14", "authors_parsed": [["da Costa", "Pedro F.", ""], ["Haartsen", "Rianne", ""], ["Throm", "Elena", ""], ["Mason", "Luke", ""], ["Gui", "Anna", ""], ["Leech", "Robert", ""], ["Jones", "Emily J. H.", ""]]}, {"id": "2106.06191", "submitter": "Matteo Cucchi", "authors": "Matteo Cucchi, Hans Kleemann, Hsin Tseng, Alexander Lee, Karl Leo", "title": "Structural evolution and on-demand growth of artificial synapses via\n  field-directed polymerization", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.ET cond-mat.soft physics.app-ph q-bio.NC", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Interconnectivity, fault tolerance, and dynamic evolution of the circuitry\nare long sought-after objectives of bio-inspired engineering. Here, we propose\ndendritic transistors composed of organic semiconductors as building blocks for\nneuromorphic computing. These devices, owning to their voltage-triggered growth\nand resemblance to neural structures, respond to action potentials to achieve\ncomplex brain-like features, such as Pavlovian learning, pattern recognition,\nand spike-timing-dependent plasticity. The dynamic formation of the connections\nis reminiscent of a biological learning mechanism known as synaptogenesis, and\nit is carried out by an electrochemical reaction that we name field-directed\npolymerization. We employ it to dendritic connections and, by modulating the\ngrowth parameters, control material properties such as the resistance and the\ntime constants relevant for plasticity. We believe these results will inspire\nfurther research towards the complex integration of polymerized synapses for\nbrain-inspired computing.\n", "versions": [{"version": "v1", "created": "Fri, 11 Jun 2021 06:47:01 GMT"}], "update_date": "2021-06-14", "authors_parsed": [["Cucchi", "Matteo", ""], ["Kleemann", "Hans", ""], ["Tseng", "Hsin", ""], ["Lee", "Alexander", ""], ["Leo", "Karl", ""]]}, {"id": "2106.06537", "submitter": "Johan Broekaert M.", "authors": "Johan M. Broekaert", "title": "The Auditory Tuning of a Keyboard", "comments": "6 pages, 5 figures, 5 tables, submitted to MTO, Nature of replacement\n  : Addition of an ACKNOWLEDGEMENT, citations, and a list of WORKS CITED. Note:\n  the initial submission was seen as a kind of \"announcement\" only, and did\n  therefore not meet requirements that are normal for articles. The goal was to\n  have a very readable text only, without elements enforcing the content of the\n  text", "journal-ref": null, "doi": null, "report-no": null, "categories": "q-bio.NC physics.hist-ph", "license": "http://creativecommons.org/publicdomain/zero/1.0/", "abstract": "  An optimal auditory tunable well (circular) temperament is determined. A\ntemperament that is applicable in practice is derived from this optimum. No\nother historical temperament fits as well, with this optimum. A brief\ncomparison of temperaments is worked out.\n", "versions": [{"version": "v1", "created": "Fri, 11 Jun 2021 19:25:13 GMT"}], "update_date": "2021-06-15", "authors_parsed": [["Broekaert", "Johan M.", ""]]}, {"id": "2106.06743", "submitter": "Saber Malekzadeh", "authors": "Hossein Yousefi-Banaem, Saber Malekzadeh", "title": "Hippocampus segmentation in magnetic resonance images of Alzheimer's\n  patients using Deep machine learning", "comments": null, "journal-ref": null, "doi": "10.13140/RG.2.2.17986.91843", "report-no": null, "categories": "eess.IV cs.CV cs.LG q-bio.NC", "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/", "abstract": "  Background: Alzheimers disease is a progressive neurodegenerative disorder\nand the main cause of dementia in aging. Hippocampus is prone to changes in the\nearly stages of Alzheimers disease. Detection and observation of the\nhippocampus changes using magnetic resonance imaging (MRI) before the onset of\nAlzheimers disease leads to the faster preventive and therapeutic measures.\nObjective: The aim of this study was the segmentation of the hippocampus in\nmagnetic resonance (MR) images of Alzheimers patients using deep machine\nlearning method. Methods: U-Net architecture of convolutional neural network\nwas proposed to segment the hippocampus in the real MRI data. The MR images of\nthe 100 and 35 patients available in Alzheimers disease Neuroimaging Initiative\n(ADNI) dataset, was used for the train and test of the model, respectively. The\nperformance of the proposed method was compared with manual segmentation by\nmeasuring the similarity metrics. Results: The desired segmentation achieved\nafter 10 iterations. A Dice similarity coefficient (DSC) = 92.3%, sensitivity =\n96.5%, positive predicted value (PPV) = 90.4%, and Intersection over Union\n(IoU) value for the train 92.94 and test 92.93 sets were obtained which are\nacceptable. Conclusion: The proposed approach is promising and can be extended\nin the prognosis of Alzheimers disease by the prediction of the hippocampus\nvolume changes in the early stage of the disease.\n", "versions": [{"version": "v1", "created": "Sat, 12 Jun 2021 11:00:29 GMT"}, {"version": "v2", "created": "Wed, 16 Jun 2021 06:07:08 GMT"}], "update_date": "2021-06-17", "authors_parsed": [["Yousefi-Banaem", "Hossein", ""], ["Malekzadeh", "Saber", ""]]}, {"id": "2106.06752", "submitter": "Sotirios Panagiotou", "authors": "Sotirios Panagiotou, Harry Sidiropoulos, Mario Negrello, Dimitrios\n  Soudris, Christos Strydis", "title": "EDEN: A high-performance, general-purpose, NeuroML-based neural\n  simulator", "comments": "29 pages, 9 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "q-bio.NC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Modern neuroscience employs in silico experimentation on ever-increasing and\nmore detailed neural networks. The high modelling detail goes hand in hand with\nthe need for high model reproducibility, reusability and transparency. Besides,\nthe size of the models and the long timescales under study mandate the use of a\nsimulation system with high computational performance, so as to provide an\nacceptable time to result. In this work, we present EDEN (Extensible Dynamics\nEngine for Networks), a new general-purpose, NeuroML-based neural simulator\nthat achieves both high model flexibility and high computational performance,\nthrough an innovative model-analysis and code-generation technique. The\nsimulator runs NeuroML v2 models directly, eliminating the need for users to\nlearn yet another simulator-specific, model-specification language. EDEN's\nfunctional correctness and computational performance were assessed through\nNeuroML models available on the NeuroML-DB and Open Source Brain model\nrepositories. In qualitative experiments, the results produced by EDEN were\nverified against the established NEURON simulator, for a wide range of models.\nAt the same time, computational-performance benchmarks reveal that EDEN runs up\nto 2 orders-of-magnitude faster than NEURON on a typical desktop computer, and\ndoes so without additional effort from the user. Finally, and without added\nuser effort, EDEN has been built from scratch to scale seamlessly over multiple\nCPUs and across computer clusters, when available.\n", "versions": [{"version": "v1", "created": "Sat, 12 Jun 2021 11:41:43 GMT"}], "update_date": "2021-06-15", "authors_parsed": [["Panagiotou", "Sotirios", ""], ["Sidiropoulos", "Harry", ""], ["Negrello", "Mario", ""], ["Soudris", "Dimitrios", ""], ["Strydis", "Christos", ""]]}, {"id": "2106.07030", "submitter": "Alpha Renner", "authors": "Alpha Renner, Forrest Sheldon, Anatoly Zlotnik, Louis Tao, Andrew\n  Sornborger", "title": "The Backpropagation Algorithm Implemented on Spiking Neuromorphic\n  Hardware", "comments": "20 pages, 5 figures", "journal-ref": null, "doi": null, "report-no": "LA-UR-21-24457", "categories": "cs.NE cs.AI cs.LG q-bio.NC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The capabilities of natural neural systems have inspired new generations of\nmachine learning algorithms as well as neuromorphic very large-scale integrated\n(VLSI) circuits capable of fast, low-power information processing. However,\nmost modern machine learning algorithms are not neurophysiologically plausible\nand thus are not directly implementable in neuromorphic hardware. In\nparticular, the workhorse of modern deep learning, the backpropagation\nalgorithm, has proven difficult to translate to neuromorphic hardware. In this\nstudy, we present a neuromorphic, spiking backpropagation algorithm based on\npulse-gated dynamical information coordination and processing, implemented on\nIntel's Loihi neuromorphic research processor. We demonstrate a\nproof-of-principle three-layer circuit that learns to classify digits from the\nMNIST dataset. This implementation shows a path for using massively parallel,\nlow-power, low-latency neuromorphic processors in modern deep learning\napplications.\n", "versions": [{"version": "v1", "created": "Sun, 13 Jun 2021 15:56:40 GMT"}], "update_date": "2021-06-15", "authors_parsed": [["Renner", "Alpha", ""], ["Sheldon", "Forrest", ""], ["Zlotnik", "Anatoly", ""], ["Tao", "Louis", ""], ["Sornborger", "Andrew", ""]]}, {"id": "2106.07096", "submitter": "Kenneth Harris", "authors": "Kenneth D. Harris", "title": "A test for partial correlation between repeatedly observed nonstationary\n  nonlinear timeseries", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME q-bio.NC stat.AP", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  We describe a family of statistical tests to measure partial correlation in\nvectorial timeseries. The test measures whether an observed timeseries Y can be\npredicted from a second series X, even after accounting for a third series Z\nwhich may correlate with X. It does not make any assumptions on the nature of\nthese timeseries, such as stationarity or linearity, but it does require that\nmultiple statistically independent recordings of the 3 series are available.\nIntuitively, the test works by asking if the series Y recorded on one\nexperiment can be better predicted from X recorded on the same experiment than\non a different experiment, after accounting for the prediction from Z recorded\non both experiments.\n", "versions": [{"version": "v1", "created": "Sun, 13 Jun 2021 21:35:10 GMT"}], "update_date": "2021-06-15", "authors_parsed": [["Harris", "Kenneth D.", ""]]}, {"id": "2106.07185", "submitter": "Donsuk Lee", "authors": "Donsuk Lee, Denizhan Pak, Justin N. Wood", "title": "Modeling Object Recognition in Newborn Chicks using Deep Neural Networks", "comments": "Presented at CogSci 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI q-bio.NC", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  In recent years, the brain and cognitive sciences have made great strides\ndeveloping a mechanistic understanding of object recognition in mature brains.\nDespite this progress, fundamental questions remain about the origins and\ncomputational foundations of object recognition. What learning algorithms\nunderlie object recognition in newborn brains? Since newborn animals learn\nlargely through unsupervised learning, we explored whether unsupervised\nlearning algorithms can be used to predict the view-invariant object\nrecognition behavior of newborn chicks. Specifically, we used feature\nrepresentations derived from unsupervised deep neural networks (DNNs) as inputs\nto cognitive models of categorization. We show that features derived from\nunsupervised DNNs make competitive predictions about chick behavior compared to\nsupervised features. More generally, we argue that linking controlled-rearing\nstudies to image-computable DNN models opens new experimental avenues for\nstudying the origins and computational basis of object recognition in newborn\nanimals.\n", "versions": [{"version": "v1", "created": "Mon, 14 Jun 2021 06:24:49 GMT"}], "update_date": "2021-06-15", "authors_parsed": [["Lee", "Donsuk", ""], ["Pak", "Denizhan", ""], ["Wood", "Justin N.", ""]]}, {"id": "2106.07355", "submitter": "Krishna Prasad Miyapuram", "authors": "Krishna Prasad Miyapuram, Wolfram Schultz, Philippe N. Tobler", "title": "Predicting the imagined contents using brain activation", "comments": "Published In 2013 Fourth National Conference on Computer Vision,\n  Pattern Recognition, Image Processing and Graphics (NCVPRIPG) (pp. 1-3)", "journal-ref": null, "doi": null, "report-no": null, "categories": "q-bio.NC cs.LG", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Mental imagery refers to percept-like experiences in the absence of sensory\ninput. Brain imaging studies suggest common, modality-specific, neural\ncorrelates imagery and perception. We associated abstract visual stimuli with\neither visually presented or imagined monetary rewards and scrambled pictures.\nBrain images for a group of 12 participants were collected using functional\nmagnetic resonance imaging. Statistical analysis showed that human midbrain\nregions were activated irrespective of the monetary rewards being imagined or\nvisually present. A support vector machine trained on the midbrain activation\npatterns to the visually presented rewards predicted with 75% accuracy whether\nthe participants imagined the monetary reward or the scrambled picture during\nimagination trials. Training samples were drawn from visually presented trials\nand classification accuracy was assessed for imagination trials. These results\nsuggest the use of machine learning technique for classification of underlying\ncognitive states from brain imaging data.\n", "versions": [{"version": "v1", "created": "Mon, 14 Jun 2021 12:34:37 GMT"}], "update_date": "2021-06-15", "authors_parsed": [["Miyapuram", "Krishna Prasad", ""], ["Schultz", "Wolfram", ""], ["Tobler", "Philippe N.", ""]]}, {"id": "2106.07369", "submitter": "Simon Segert", "authors": "Simon N. Segert, Jonathan D. Cohen", "title": "A Self-Supervised Framework for Function Learning and Extrapolation", "comments": "15 pages, 4 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG q-bio.NC", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Understanding how agents learn to generalize -- and, in particular, to\nextrapolate -- in high-dimensional, naturalistic environments remains a\nchallenge for both machine learning and the study of biological agents. One\napproach to this has been the use of function learning paradigms, which allow\npeoples' empirical patterns of generalization for smooth scalar functions to be\ndescribed precisely. However, to date, such work has not succeeded in\nidentifying mechanisms that acquire the kinds of general purpose\nrepresentations over which function learning can operate to exhibit the\npatterns of generalization observed in human empirical studies. Here, we\npresent a framework for how a learner may acquire such representations, that\nthen support generalization -- and extrapolation in particular -- in a few-shot\nfashion. Taking inspiration from a classic theory of visual processing, we\nconstruct a self-supervised encoder that implements the basic inductive bias of\ninvariance under topological distortions. We show the resulting representations\noutperform those from other models for unsupervised time series learning in\nseveral downstream function learning tasks, including extrapolation.\n", "versions": [{"version": "v1", "created": "Mon, 14 Jun 2021 12:41:03 GMT"}], "update_date": "2021-06-15", "authors_parsed": [["Segert", "Simon N.", ""], ["Cohen", "Jonathan D.", ""]]}, {"id": "2106.07411", "submitter": "Robert Geirhos", "authors": "Robert Geirhos, Kantharaju Narayanappa, Benjamin Mitzkus, Tizian\n  Thieringer, Matthias Bethge, Felix A. Wichmann, Wieland Brendel", "title": "Partial success in closing the gap between human and machine vision", "comments": "A preliminary version of this work was presented as Oral at the 2020\n  NeurIPS workshop on \"Shared Visual Representations in Human & Machine\n  Intelligence\" (arXiv:2010.08377)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI cs.LG q-bio.NC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A few years ago, the first CNN surpassed human performance on ImageNet.\nHowever, it soon became clear that machines lack robustness on more challenging\ntest cases, a major obstacle towards deploying machines \"in the wild\" and\ntowards obtaining better computational models of human visual perception. Here\nwe ask: Are we making progress in closing the gap between human and machine\nvision? To answer this question, we tested human observers on a broad range of\nout-of-distribution (OOD) datasets, adding the \"missing human baseline\" by\nrecording 85,120 psychophysical trials across 90 participants. We then\ninvestigated a range of promising machine learning developments that crucially\ndeviate from standard supervised CNNs along three axes: objective function\n(self-supervised, adversarially trained, CLIP language-image training),\narchitecture (e.g. vision transformers), and dataset size (ranging from 1M to\n1B). Our findings are threefold. (1.) The longstanding robustness gap between\nhumans and CNNs is closing, with the best models now matching or exceeding\nhuman performance on most OOD datasets. (2.) There is still a substantial\nimage-level consistency gap, meaning that humans make different errors than\nmodels. In contrast, most models systematically agree in their categorisation\nerrors, even substantially different ones like contrastive self-supervised vs.\nstandard supervised models. (3.) In many cases, human-to-model consistency\nimproves when training dataset size is increased by one to three orders of\nmagnitude. Our results give reason for cautious optimism: While there is still\nmuch room for improvement, the behavioural difference between human and machine\nvision is narrowing. In order to measure future progress, 17 OOD datasets with\nimage-level human behavioural data are provided as a benchmark here:\nhttps://github.com/bethgelab/model-vs-human/\n", "versions": [{"version": "v1", "created": "Mon, 14 Jun 2021 13:23:35 GMT"}], "update_date": "2021-06-15", "authors_parsed": [["Geirhos", "Robert", ""], ["Narayanappa", "Kantharaju", ""], ["Mitzkus", "Benjamin", ""], ["Thieringer", "Tizian", ""], ["Bethge", "Matthias", ""], ["Wichmann", "Felix A.", ""], ["Brendel", "Wieland", ""]]}, {"id": "2106.07490", "submitter": "Spyridon Chavlis Ph.D.", "authors": "Spyridon Chavlis and Panayiota Poirazi", "title": "Drawing Inspiration from Biological Dendrites to Empower Artificial\n  Neural Networks", "comments": "12 pages, 1 figure, opinion article", "journal-ref": "Current Opinion in Neurobiology 70 (2021): 1-10", "doi": "10.1016/j.conb.2021.04.007", "report-no": null, "categories": "q-bio.NC", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  This article highlights specific features of biological neurons and their\ndendritic trees, whose adoption may help advance artificial neural networks\nused in various machine learning applications. Advancements could take the form\nof increased computational capabilities and/or reduced power consumption.\nProposed features include dendritic anatomy, dendritic nonlinearities, and\ncompartmentalized plasticity rules, all of which shape learning and information\nprocessing in biological networks. We discuss the computational benefits\nprovided by these features in biological neurons and suggest ways to adopt them\nin artificial neurons in order to exploit the respective benefits in machine\nlearning.\n", "versions": [{"version": "v1", "created": "Mon, 14 Jun 2021 15:20:24 GMT"}], "update_date": "2021-06-15", "authors_parsed": [["Chavlis", "Spyridon", ""], ["Poirazi", "Panayiota", ""]]}, {"id": "2106.07622", "submitter": "Lu Zhang", "authors": "Lu Zhang, Xiaowei Yu, Yanjun Lyu, Li Wang, Dajiang Zhu", "title": "Representative Functional Connectivity Learning for Multiple Clinical\n  groups in Alzheimer's Disease", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "q-bio.NC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Mild cognitive impairment (MCI) is a high-risk dementia condition which\nprogresses to probable Alzheimer's disease (AD) at approximately 10% to 15% per\nyear. Characterization of group-level differences between two subtypes of MCI -\nstable MCI (sMCI) and progressive MCI (pMCI) is the key step to understand the\nmechanisms of MCI progression and enable possible delay of transition from MCI\nto AD. Functional connectivity (FC) is considered as a promising way to study\nMCI progression since which may show alterations even in preclinical stages and\nprovide substrates for AD progression. However, the representative FC patterns\nduring AD development for different clinical groups, especially for sMCI and\npMCI, have been understudied. In this work, we integrated autoencoder and\nmulti-class classification into a single deep model and successfully learned a\nset of clinical group related feature vectors. Specifically, we trained two\nnon-linear mappings which realized the mutual transformations between original\nFC space and the feature space. By mapping the learned clinical group related\nfeature vectors to the original FC space, representative FCs were constructed\nfor each group. Moreover, based on these feature vectors, our model achieves a\nhigh classification accuracy - 68% for multi-class classification (NC vs SMC vs\nsMCI vs pMCI vs AD).\n", "versions": [{"version": "v1", "created": "Mon, 14 Jun 2021 17:27:54 GMT"}], "update_date": "2021-06-15", "authors_parsed": [["Zhang", "Lu", ""], ["Yu", "Xiaowei", ""], ["Lyu", "Yanjun", ""], ["Wang", "Li", ""], ["Zhu", "Dajiang", ""]]}, {"id": "2106.07713", "submitter": "Jason Zwicker", "authors": "Jason Zwicker, Francois Rivest", "title": "Interval Timing: Modeling the break-run-break pattern using start/stop\n  threshold-less drift-diffusion model", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "q-bio.NC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Animal interval timing is often studied through the peak interval (PI)\nprocedure. In this procedure, the animal is rewarded for the first response\nafter a fixed delay from the stimulus onset, but on some trials, the stimulus\nremains and no reward is given. The common methods and models to analyse the\nresponse pattern describe it as break-run-break, a period of low rate response\nfollowed by rapid responding, followed by a low rate of response. The study of\nthe pattern has found correlations between start, stop, and duration of the run\nperiod that hold across species and experiment.\n  It is commonly assumed that in order to achieve the statistics with a\npacemaker accumulator model it is necessary to have start and stop thresholds.\nIn this paper we will develop a new model that varies response rate in relation\nto the likelihood of event occurrence, as opposed to a threshold, for changing\nthe response rate. The new model reproduced the start and stop statistics that\nhave been observed in 14 different PI experiments from 3 different papers. The\ndeveloped model is also compared to the Time-adaptive Drift-diffusion Model\n(TDDM), the latest accumulator model subsuming the scalar expectancy theory\n(SET), on all 14 data-sets. The results show that it is unnecessary to have\nexplicit start and stop thresholds or an internal equivalent to break-run-break\nstates to reproduce the individual trials statistics and population behaviour\nand get the same break-run-break analysis results. The new model also produces\nmore realistic individual trials compared to TDDM.\n", "versions": [{"version": "v1", "created": "Mon, 14 Jun 2021 19:07:21 GMT"}], "update_date": "2021-06-16", "authors_parsed": [["Zwicker", "Jason", ""], ["Rivest", "Francois", ""]]}, {"id": "2106.07865", "submitter": "Brandon Munn", "authors": "Brandon R. Munn, Eli J. M\\\"uller, and James M. Shine", "title": "Neuromodulatory control over nonlinear spiking of layer V pyramidal\n  neurons mediates adaptive slightly subcritical network dynamics", "comments": "14 pages, 4 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "q-bio.NC", "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/", "abstract": "  To remain adaptable to a dynamic environment, coordinated neural activity in\nthe brain must be simultaneously flexible and reliable. There is evidence that\nthe brain facilitates this adaptive response using highly-conserved\nmetabotropic neuromodulatory neurotransmitters, such as noradrenaline and\nacetylcholine. While we understand how these neuromodulators alter individual\nneuronal dynamics, precisely how neuromodulation operates in networks of\nneurons to give rise to observed large-scale dynamics remains unknown. Here, we\ninvestigate this disparity by demonstrating correspondence between adaptive\ninformation processing modes - calculated on in vivo electrophysiological\nrecordings of bursting layer V neurons in awake mice - and fluctuations in\nneuromodulatory tone - assessed by dynamic changes in pupil diameter. We\ntheoretically validate these results by creating a novel, biologically\nplausible dual-compartment model of nonlinear layer V pyramidal neurons -\ncapable of both regular spike and bursting modes - that reproduce our main\nempirical findings. We then demonstrate that the adrenergic and cholinergic\nneuromodulatory systems shift the brain into a neuroprotective (i.e., slightly\nsubcritical) regime - assessed by the branching parameter - while facilitating\nflexible and reliable dynamics, respectively. This unexpected result\ndemonstrates that the brain has circumvented the necessity to shift the system\ncloser to criticality for variability by differentially augmenting an intrinsic\nneuronal nonlinearity. Our analyses establish that these distinct arms of the\nascending arousal system modulate ensembles of layer V pyramidal neurons to\naugment critical dynamics and facilitate distinct adaptive information\nprocessing modes within the brain.\n", "versions": [{"version": "v1", "created": "Tue, 15 Jun 2021 03:59:16 GMT"}], "update_date": "2021-06-16", "authors_parsed": [["Munn", "Brandon R.", ""], ["M\u00fcller", "Eli J.", ""], ["Shine", "James M.", ""]]}, {"id": "2106.07949", "submitter": "Yanshuai Tu", "authors": "Yanshuai Tu, Duyan Ta, Zhong-Lin Lu, Yalin Wang", "title": "Topological Receptive Field Model for Human Retinotopic Mapping", "comments": "Submitted to MICCAI 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "q-bio.NC", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  The mapping between visual inputs on the retina and neuronal activations in\nthe visual cortex, i.e., retinotopic map, is an essential topic in vision\nscience and neuroscience. Human retinotopic maps can be revealed by analyzing\nthe functional magnetic resonance imaging (fMRI) signal responses to designed\nvisual stimuli in vivo. Neurophysiology studies summarized that visual areas\nare topological (i.e., nearby neurons have receptive fields at nearby locations\nin the image). However, conventional fMRI-based analyses frequently generate\nnon-topological results because they process fMRI signals on a voxel-wise\nbasis, without considering the neighbor relations on the surface. Here we\npropose a topological receptive field (tRF) model which imposes the topological\ncondition when decoding retinotopic fMRI signals. More specifically, we\nparametrized the cortical surface to a unit disk, characterized the topological\ncondition by tRF, and employed an efficient scheme to solve the tRF model. We\ntested our framework on both synthetic and human fMRI data. Experimental\nresults showed that the tRF model could remove the topological violations,\nimprove model explaining power, and generate biologically plausible retinotopic\nmaps. The proposed framework is general and can be applied to other sensory\nmaps.\n", "versions": [{"version": "v1", "created": "Tue, 15 Jun 2021 08:04:35 GMT"}, {"version": "v2", "created": "Wed, 16 Jun 2021 00:46:07 GMT"}], "update_date": "2021-06-17", "authors_parsed": [["Tu", "Yanshuai", ""], ["Ta", "Duyan", ""], ["Lu", "Zhong-Lin", ""], ["Wang", "Yalin", ""]]}, {"id": "2106.08085", "submitter": "Kristopher Jensen", "authors": "Ta-Chu Kao, Kristopher T. Jensen, Alberto Bernacchia, Guillaume\n  Hennequin", "title": "Natural continual learning: success is a journey, not (just) a\n  destination", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG q-bio.NC", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Biological agents are known to learn many different tasks over the course of\ntheir lives, and to be able to revisit previous tasks and behaviors with little\nto no loss in performance. In contrast, artificial agents are prone to\n'catastrophic forgetting' whereby performance on previous tasks deteriorates\nrapidly as new ones are acquired. This shortcoming has recently been addressed\nusing methods that encourage parameters to stay close to those used for\nprevious tasks. This can be done by (i) using specific parameter regularizers\nthat map out suitable destinations in parameter space, or (ii) guiding the\noptimization journey by projecting gradients into subspaces that do not\ninterfere with previous tasks. However, parameter regularization has been shown\nto be relatively ineffective in recurrent neural networks (RNNs), a setting\nrelevant to the study of neural dynamics supporting biological continual\nlearning. Similarly, projection based methods can reach capacity and fail to\nlearn any further as the number of tasks increases. To address these\nlimitations, we propose Natural Continual Learning (NCL), a new method that\nunifies weight regularization and projected gradient descent. NCL uses Bayesian\nweight regularization to encourage good performance on all tasks at convergence\nand combines this with gradient projections designed to prevent catastrophic\nforgetting during optimization. NCL formalizes gradient projection as a trust\nregion algorithm based on the Fisher information metric, and achieves\nscalability via a novel Kronecker-factored approximation strategy. Our method\noutperforms both standard weight regularization techniques and projection based\napproaches when applied to continual learning problems in RNNs. The trained\nnetworks evolve task-specific dynamics that are strongly preserved as new tasks\nare learned, similar to experimental findings in biological circuits.\n", "versions": [{"version": "v1", "created": "Tue, 15 Jun 2021 12:24:53 GMT"}], "update_date": "2021-06-16", "authors_parsed": [["Kao", "Ta-Chu", ""], ["Jensen", "Kristopher T.", ""], ["Bernacchia", "Alberto", ""], ["Hennequin", "Guillaume", ""]]}, {"id": "2106.08928", "submitter": "Leo Kozachkov", "authors": "Michaela Ennis, Leo Kozachkov, Jean-Jacques Slotine", "title": "Recursive Construction of Stable Assemblies of Recurrent Neural Networks", "comments": "23 pages, 3 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG math.DS q-bio.NC", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Advanced applications of modern machine learning will likely involve\ncombinations of trained networks, as are already used in spectacular systems\nsuch as DeepMind's AlphaGo. Recursively building such combinations in an\neffective and stable fashion while also allowing for continual refinement of\nthe individual networks - as nature does for biological networks - will require\nnew analysis tools. This paper takes a step in this direction by establishing\ncontraction properties of broad classes of nonlinear recurrent networks and\nneural ODEs, and showing how these quantified properties allow in turn to\nrecursively construct stable networks of networks in a systematic fashion. The\nresults can also be used to stably combine recurrent networks and physical\nsystems with quantified contraction properties. Similarly, they may be applied\nto modular computational models of cognition.\n", "versions": [{"version": "v1", "created": "Wed, 16 Jun 2021 16:35:50 GMT"}], "update_date": "2021-06-17", "authors_parsed": [["Ennis", "Michaela", ""], ["Kozachkov", "Leo", ""], ["Slotine", "Jean-Jacques", ""]]}, {"id": "2106.08995", "submitter": "Birgitta Dresp-Langley", "authors": "Birgitta Dresp-Langley, Rongrong Liu, John M. Wandeto", "title": "Surgical task expertise detected by a self-organizing neural network map", "comments": "Conference on Automation in Medical Engineering AUTOMED21, University\n  Hospital Basel, Switzerland, 2021, June 8-9", "journal-ref": null, "doi": null, "report-no": null, "categories": "q-bio.NC cs.AI cs.RO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Individual grip force profiling of bimanual simulator task performance of\nexperts and novices using a robotic control device designed for endoscopic\nsurgery permits defining benchmark criteria that tell true expert task skills\nfrom the skills of novices or trainee surgeons. Grip force variability in a\ntrue expert and a complete novice executing a robot assisted surgical simulator\ntask reveal statistically significant differences as a function of task\nexpertise. Here we show that the skill specific differences in local grip\nforces are predicted by the output metric of a Self Organizing neural network\nMap (SOM) with a bio inspired functional architecture that maps the functional\nconnectivity of somatosensory neural networks in the primate brain.\n", "versions": [{"version": "v1", "created": "Thu, 3 Jun 2021 10:48:10 GMT"}], "update_date": "2021-06-17", "authors_parsed": [["Dresp-Langley", "Birgitta", ""], ["Liu", "Rongrong", ""], ["Wandeto", "John M.", ""]]}, {"id": "2106.09000", "submitter": "Xin Yang", "authors": "Xin Yang, Ning Zhang, Donglin Wang", "title": "Deriving Autism Spectrum Disorder Functional Networks from RS-FMRI Data\n  using Group ICA and Dictionary Learning", "comments": "Conference", "journal-ref": null, "doi": "10.5121/csit.2021.110714", "report-no": null, "categories": "q-bio.NC cs.LG eess.IV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The objective of this study is to derive functional networks for the autism\nspectrum disorder (ASD) population using the group ICA and dictionary learning\nmodel together and to classify ASD and typically developing (TD) participants\nusing the functional connectivity calculated from the derived functional\nnetworks. In our experiments, the ASD functional networks were derived from\nresting-state functional magnetic resonance imaging (rs-fMRI) data. We\ndownloaded a total of 120 training samples, including 58 ASD and 62 TD\nparticipants, which were obtained from the public repository: Autism Brain\nImaging Data Exchange I (ABIDE I). Our methodology and results have five main\nparts. First, we utilize a group ICA model to extract functional networks from\nthe ASD group and rank the top 20 regions of interest (ROIs). Second, we\nutilize a dictionary learning model to extract functional networks from the ASD\ngroup and rank the top 20 ROIs. Third, we merged the 40 selected ROIs from the\ntwo models together as the ASD functional networks. Fourth, we generate three\ncorresponding masks based on the 20 selected ROIs from group ICA, the 20 ROIs\nselected from dictionary learning, and the 40 combined ROIs selected from both.\nFinally, we extract ROIs for all training samples using the above three masks,\nand the calculated functional connectivity was used as features for ASD and TD\nclassification. The classification results showed that the functional networks\nderived from ICA and dictionary learning together outperform those derived from\na single ICA model or a single dictionary learning model.\n", "versions": [{"version": "v1", "created": "Mon, 7 Jun 2021 07:58:52 GMT"}], "update_date": "2021-06-17", "authors_parsed": [["Yang", "Xin", ""], ["Zhang", "Ning", ""], ["Wang", "Donglin", ""]]}, {"id": "2106.09007", "submitter": "Andrew Adamatzky", "authors": "Andrew Adamatzky and Antoni Gandia", "title": "Fungi anaesthesia", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "q-bio.NC cs.ET physics.bio-ph", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Electrical activity of fungus \\emph{Pleurotus ostreatus} is characterised by\nslow (hours) irregular waves of baseline potential drift and fast (minutes)\naction potential likes spikes of the electrical potential. An exposure of the\nmycelium colonised substrate to a chloroform vapour lead to several fold\ndecrease of the baseline potential waves and increase of their duration. The\nchloroform vapour also causes either complete cessation of spiking activity or\nsubstantial reduction of the spiking frequency. Removal of the chloroform\nvapour from the growth containers leads to a gradual restoration of the\nmycelium electrical activity.\n", "versions": [{"version": "v1", "created": "Sun, 13 Jun 2021 19:21:53 GMT"}], "update_date": "2021-06-17", "authors_parsed": [["Adamatzky", "Andrew", ""], ["Gandia", "Antoni", ""]]}, {"id": "2106.09389", "submitter": "Henrik Jeldtoft Jensen", "authors": "Henrik Jeldtoft Jensen", "title": "Brain, Rain and Forest Fires -- What is critical about criticality: In\n  praise of the correlation function", "comments": "11 pages, 2 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cond-mat.stat-mech q-bio.NC", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  We present a brief review of power laws and correlation functions as measures\nof criticality and the relation between them. By comparing phenomenology from\nrain, brain and the forest fire model we discuss the relevant features of\nself-organisation to the vicinity about a critical state. We conclude that\norganisation to a region of extended correlations and approximate power laws\nmay be behaviour of interest shared between the three considered systems.\n", "versions": [{"version": "v1", "created": "Thu, 17 Jun 2021 11:07:53 GMT"}], "update_date": "2021-06-18", "authors_parsed": [["Jensen", "Henrik Jeldtoft", ""]]}, {"id": "2106.09408", "submitter": "Martin Hanik", "authors": "Martin Hanik, Mehmet Arif Demirta\\c{s}, Mohammed Amine Gharsallaoui,\n  Islem Rekik", "title": "Predicting cognitive scores with graph neural networks through sample\n  selection learning", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG q-bio.NC", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Analyzing the relation between intelligence and neural activity is of the\nutmost importance in understanding the working principles of the human brain in\nhealth and disease. In existing literature, functional brain connectomes have\nbeen used successfully to predict cognitive measures such as intelligence\nquotient (IQ) scores in both healthy and disordered cohorts using machine\nlearning models. However, existing methods resort to flattening the brain\nconnectome (i.e., graph) through vectorization which overlooks its topological\nproperties. To address this limitation and inspired from the emerging graph\nneural networks (GNNs), we design a novel regression GNN model (namely RegGNN)\nfor predicting IQ scores from brain connectivity. On top of that, we introduce\na novel, fully modular sample selection method to select the best samples to\nlearn from for our target prediction task. However, since such deep learning\narchitectures are computationally expensive to train, we further propose a\n\\emph{learning-based sample selection} method that learns how to choose the\ntraining samples with the highest expected predictive power on unseen samples.\nFor this, we capitalize on the fact that connectomes (i.e., their adjacency\nmatrices) lie in the symmetric positive definite (SPD) matrix cone. Our results\non full-scale and verbal IQ prediction outperforms comparison methods in autism\nspectrum disorder cohorts and achieves a competitive performance for\nneurotypical subjects using 3-fold cross-validation. Furthermore, we show that\nour sample selection approach generalizes to other learning-based methods,\nwhich shows its usefulness beyond our GNN architecture.\n", "versions": [{"version": "v1", "created": "Thu, 17 Jun 2021 11:45:39 GMT"}], "update_date": "2021-06-18", "authors_parsed": [["Hanik", "Martin", ""], ["Demirta\u015f", "Mehmet Arif", ""], ["Gharsallaoui", "Mohammed Amine", ""], ["Rekik", "Islem", ""]]}, {"id": "2106.09639", "submitter": "Denis Turcu", "authors": "Denis Turcu and Christos Papadimitriou", "title": "Implementing Permutations in the Brain and SVO Frequencies of Languages", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "q-bio.NC", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  The subject-verb-object (SVO) word order prevalent in English is shared by\nabout $42\\%$ of world languages. Another $45\\%$ of all languages follow the SOV\norder, $9\\%$ the VSO order, and fewer languages use the three remaining\npermutations. None of the many extant explanations of this phenomenon take into\naccount the difficulty of implementing these permutations in the brain. We\npropose a plausible model of sentence generation inspired by the recently\nproposed Assembly Calculus framework of brain function. Our model results in a\nnatural explanation of the uneven frequencies. Estimating the parameters of\nthis model yields predictions of the relative difficulty of dis-inhibiting one\nbrain area from another. Our model is based on the standard syntax tree, a\nsimple binary tree with three leaves. Each leaf corresponds to one of the three\nparts of a basic sentence. The leaves can be activated through lock and unlock\noperations and the sequence of activation of the leaves implements a specific\nword order. More generally, we also formulate and algorithmically solve the\nproblems of implementing a permutation of the leaves of any binary tree, and of\nselecting the permutation that is easiest to implement on a given binary tree.\n", "versions": [{"version": "v1", "created": "Thu, 17 Jun 2021 16:36:37 GMT"}], "update_date": "2021-06-18", "authors_parsed": [["Turcu", "Denis", ""], ["Papadimitriou", "Christos", ""]]}, {"id": "2106.09979", "submitter": "Taisuke Kobayashi", "authors": "Taisuke Kobayashi, Eiji Watanabe", "title": "Artificial Perception Meets Psychophysics, Revealing a Fundamental Law\n  of Illusory Motion", "comments": "15 pages, 11 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "q-bio.NC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Rotating Snakes is a visual illusion in which a stationary design is\nperceived to move dramatically. In the current study, the mechanism that\ngenerates perception of motion was analyzed using a combination of\npsychophysics experiments and deep neural network models that mimic human\nvision. We prepared three- and four-color illusion-like designs with a wide\nrange of luminance and measured their strength of induced rotational motion. As\na result, we discovered the fundamental law that the effect of the four-color\nsnake rotation illusion was successfully enhanced by the combination of two\nperceptual motion vectors produced by the two three-color designs. In years to\ncome, deep neural network technology will be one of the most effective tools\nnot only for engineering applications but also for human perception research.\n", "versions": [{"version": "v1", "created": "Fri, 18 Jun 2021 07:59:18 GMT"}, {"version": "v2", "created": "Mon, 21 Jun 2021 06:06:02 GMT"}, {"version": "v3", "created": "Thu, 24 Jun 2021 05:26:51 GMT"}], "update_date": "2021-06-25", "authors_parsed": [["Kobayashi", "Taisuke", ""], ["Watanabe", "Eiji", ""]]}, {"id": "2106.10064", "submitter": "Guillaume Bellec", "authors": "Guillaume Bellec, Shuqi Wang, Alireza Modirshanechi, Johanni Brea,\n  Wulfram Gerstner", "title": "Fitting summary statistics of neural data with a differentiable spiking\n  network simulator", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG q-bio.NC", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Fitting network models to neural activity is becoming an important tool in\nneuroscience. A popular approach is to model a brain area with a probabilistic\nrecurrent spiking network whose parameters maximize the likelihood of the\nrecorded activity. Although this is widely used, we show that the resulting\nmodel does not produce realistic neural activity and wrongly estimates the\nconnectivity matrix when neurons that are not recorded have a substantial\nimpact on the recorded network. To correct for this, we suggest to augment the\nlog-likelihood with terms that measure the dissimilarity between simulated and\nrecorded activity. This dissimilarity is defined via summary statistics\ncommonly used in neuroscience, and the optimization is efficient because it\nrelies on back-propagation through the stochastically simulated spike trains.\nWe analyze this method theoretically and show empirically that it generates\nmore realistic activity statistics and recovers the connectivity matrix better\nthan other methods.\n", "versions": [{"version": "v1", "created": "Fri, 18 Jun 2021 11:21:30 GMT"}], "update_date": "2021-06-21", "authors_parsed": [["Bellec", "Guillaume", ""], ["Wang", "Shuqi", ""], ["Modirshanechi", "Alireza", ""], ["Brea", "Johanni", ""], ["Gerstner", "Wulfram", ""]]}, {"id": "2106.10112", "submitter": "Maytus Piriyajitakonkij", "authors": "Maytus Piriyajitakonkij, Sirawaj Itthipuripat, Theerawit\n  Wilaiprasitporn, Nat Dilokthanakul", "title": "Deep Reinforcement Learning Models Predict Visual Responses in the\n  Brain: A Preliminary Result", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG q-bio.NC", "license": "http://creativecommons.org/publicdomain/zero/1.0/", "abstract": "  Supervised deep convolutional neural networks (DCNNs) are currently one of\nthe best computational models that can explain how the primate ventral visual\nstream solves object recognition. However, embodied cognition has not been\nconsidered in the existing visual processing models. From the ecological\nstandpoint, humans learn to recognize objects by interacting with them,\nallowing better classification, specialization, and generalization. Here, we\nask if computational models under the embodied learning framework can explain\nmechanisms underlying object recognition in the primate visual system better\nthan the existing supervised models? To address this question, we use\nreinforcement learning to train neural network models to play a 3D computer\ngame and we find that these reinforcement learning models achieve neural\nresponse prediction accuracy scores in the early visual areas (e.g., V1 and V2)\nin the levels that are comparable to those accomplished by the supervised\nneural network model. In contrast, the supervised neural network models yield\nbetter neural response predictions in the higher visual areas, compared to the\nreinforcement learning models. Our preliminary results suggest the future\ndirection of visual neuroscience in which deep reinforcement learning should be\nincluded to fill the missing embodiment concept.\n", "versions": [{"version": "v1", "created": "Fri, 18 Jun 2021 13:10:06 GMT"}], "update_date": "2021-06-21", "authors_parsed": [["Piriyajitakonkij", "Maytus", ""], ["Itthipuripat", "Sirawaj", ""], ["Wilaiprasitporn", "Theerawit", ""], ["Dilokthanakul", "Nat", ""]]}, {"id": "2106.10211", "submitter": "Pedro Mediano", "authors": "Pedro A.M. Mediano, Fernando E. Rosas, Juan Carlos Farah, Murray\n  Shanahan, Daniel Bor and Adam B. Barrett", "title": "Integrated information as a common signature of dynamical and\n  information-processing complexity", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "q-bio.NC nlin.AO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The apparent dichotomy between information-processing and dynamical\napproaches to complexity science forces researchers to choose between two\ndiverging sets of tools and explanations, creating conflict and often hindering\nscientific progress. Nonetheless, given the shared theoretical goals between\nboth approaches, it is reasonable to conjecture the existence of underlying\ncommon signatures that capture interesting behaviour in both dynamical and\ninformation-processing systems. Here we argue that a pragmatic use of\nIntegrated Information Theory (IIT), originally conceived in theoretical\nneuroscience, can provide a potential unifying framework to study complexity in\ngeneral multivariate systems. Furthermore, by leveraging metrics put forward by\nthe integrated information decomposition ($\\Phi$ID) framework, our results\nreveal that integrated information can effectively capture surprisingly\nheterogeneous signatures of complexity -- including metastability and\ncriticality in networks of coupled oscillators as well as distributed\ncomputation and emergent stable particles in cellular automata -- without\nrelying on idiosyncratic, ad-hoc criteria. These results show how an agnostic\nuse of IIT can provide important steps towards bridging the gap between\ninformational and dynamical approaches to complex systems.\n", "versions": [{"version": "v1", "created": "Fri, 18 Jun 2021 16:31:01 GMT"}], "update_date": "2021-06-21", "authors_parsed": [["Mediano", "Pedro A. M.", ""], ["Rosas", "Fernando E.", ""], ["Farah", "Juan Carlos", ""], ["Shanahan", "Murray", ""], ["Bor", "Daniel", ""], ["Barrett", "Adam B.", ""]]}, {"id": "2106.10344", "submitter": "Hamidreza Ramezanpour", "authors": "Hamidreza Ramezanpour and Mazyar Fallah", "title": "The role of temporal cortex in the control of attention", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "q-bio.NC", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Attention is an indispensable component of active vision. Contrary to the\nwidely accepted notion that temporal cortex processing primarily focusses on\npassive object recognition, a series of very recent studies emphasize the role\nof temporal cortex structures, specifically the superior temporal sulcus (STS)\nand inferotemporal (IT) cortex, in guiding attention and implementing cognitive\nprograms relevant for behavioral tasks. The goal of this review is to advance\nthe hypothesis that the temporal cortex, classically thought to be only\ninvolved in object recognition, entails necessary components to actively\nparticipate in attentional control in a flexible task-dependent manner. First,\nwe will briefly discuss the general architecture of the temporal cortex with a\nfocus on the STS and IT cortex and their modulation with attention. Then we\nwill review evidence from behavioral and neurophysiological studies that\nsupport their guidance of attention in the presence of cognitive control\nsignals. Next, we propose a mechanistic framework for executive control of\nattention in the temporal cortex. Finally, we summarize the role of temporal\ncortex in implementing cognitive programs and discuss how they contribute to\nthe dynamic nature of visual attention to ensure flexible behavior.\n", "versions": [{"version": "v1", "created": "Fri, 18 Jun 2021 20:17:15 GMT"}], "update_date": "2021-06-22", "authors_parsed": [["Ramezanpour", "Hamidreza", ""], ["Fallah", "Mazyar", ""]]}, {"id": "2106.10627", "submitter": "Gerald Pao", "authors": "Gerald M Pao, Cameron Smith, Joseph Park, Keichi Takahashi, Wassapon\n  Watanakeesuntorn, Hiroaki Natsukawa, Sreekanth H Chalasani, Tom Lorimer,\n  Ryousei Takano, Nuttida Rungratsameetaweemana, George Sugihara", "title": "Experimentally testable whole brain manifolds that recapitulate behavior", "comments": "20 pages, 15 figures; corresponding author: Gerald Pao\n  geraldpao@gmail.com", "journal-ref": null, "doi": null, "report-no": null, "categories": "q-bio.NC math.DS", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  We propose an algorithm grounded in dynamical systems theory that generalizes\nmanifold learning from a global state representation, to a network of local\ninteracting manifolds termed a Generative Manifold Network (GMN). Manifolds are\ndiscovered using the convergent cross mapping (CCM) causal inference algorithm\nwhich are then compressed into a reduced redundancy network. The representation\nis a network of manifolds embedded from observational data where each\northogonal axis of a local manifold is an embedding of a individually\nidentifiable neuron or brain area that has exact correspondence in the real\nworld. As such these can be experimentally manipulated to test hypotheses\nderived from theory and data analysis. Here we demonstrate that this\nrepresentation preserves the essential features of the brain of flies,larval\nzebrafish and humans. In addition to accurate near-term prediction, the GMN\nmodel can be used to synthesize realistic time series of whole brain neuronal\nactivity and locomotion viewed over the long term. Thus, as a final validation\nof how well GMN captures essential dynamic information, we show that the\nartificially generated time series can be used as a training set to predict\nout-of-sample observed fly locomotion, as well as brain activity in out of\nsample withheld data not used in model building. Remarkably, the artificially\ngenerated time series show realistic novel behaviors that do not exist in the\ntraining data, but that do exist in the out-of-sample observational data. This\nsuggests that GMN captures inherently emergent properties of the network. We\nsuggest our approach may be a generic recipe for mapping time series\nobservations of any complex nonlinear network into a model that is able to\ngenerate naturalistic system behaviors that identifies variables that have real\nworld correspondence and can be experimentally manipulated.\n", "versions": [{"version": "v1", "created": "Sun, 20 Jun 2021 05:17:05 GMT"}], "update_date": "2021-06-22", "authors_parsed": [["Pao", "Gerald M", ""], ["Smith", "Cameron", ""], ["Park", "Joseph", ""], ["Takahashi", "Keichi", ""], ["Watanakeesuntorn", "Wassapon", ""], ["Natsukawa", "Hiroaki", ""], ["Chalasani", "Sreekanth H", ""], ["Lorimer", "Tom", ""], ["Takano", "Ryousei", ""], ["Rungratsameetaweemana", "Nuttida", ""], ["Sugihara", "George", ""]]}, {"id": "2106.10631", "submitter": "Leonardo Novelli", "authors": "Leonardo Novelli, Adeel Razi", "title": "A mathematical perspective on edge-centric functional connectivity", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "q-bio.NC physics.data-an", "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/", "abstract": "  Edge-centric functional connectivity (eFC) has recently been proposed to\ncharacterise the finest time resolution on the FC dynamics without the\nconcomitant assumptions of sliding-window approaches. Here, we lay the\nmathematical foundations for the edge-centric analysis and examine its main\nfindings from a quantitative perspective. The proposed framework provides a\ntheoretical explanation for the observed occurrence of high-amplitude edge\ncofluctuations across datasets and clarifies why a few large events drive the\nnode-centric FC (nFC). Our exposition also constitutes a critique of the\nedge-centric approach as currently applied to functional MRI (fMRI) time\nseries. The central argument is that the existing findings based on edge time\nseries can be derived from the static nFC under a null hypothesis that only\naccounts for the observed static spatial correlations and not the temporal\nones. Challenging our analytic predictions against fMRI data from the Human\nConnectome Project confirms that the nFC is sufficient to replicate the eFC\nmatrix, the edge communities, the large cofluctuations, and the corresponding\nbrain activity mode. We conclude that the temporal structure of the edge time\nseries has not so far been exploited sufficiently and encourage further work to\nexplore features that cannot be explained by the presented static null model.\n", "versions": [{"version": "v1", "created": "Sun, 20 Jun 2021 05:53:12 GMT"}], "update_date": "2021-06-22", "authors_parsed": [["Novelli", "Leonardo", ""], ["Razi", "Adeel", ""]]}, {"id": "2106.10851", "submitter": "Jian Zhai", "authors": "Jian Zhai, Chaojun Yu, You Zhai", "title": "Witten-type topological field theory of self-organized criticality for\n  stochastic neural networks", "comments": "4 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "q-bio.NC quant-ph", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  We study the Witten-type topological field theory(W-TFT) of self-organized\ncriticality(SOC) for stochastic neural networks. The Parisi-Sourlas-Wu\nquantization of general stochastic differential equations (SDEs) for neural\nnetworks, the Becchi-Rouet-Stora-Tyutin(BRST)-symmetry of the diffusion system\nand the relation between spontaneous breaking and instantons connecting steady\nstates of the SDEs, as well as the sufficient and necessary condition on\npseudo-supersymmetric stochastic neural networks are obtained. Suppose neuronal\navalanche is a mechanism of cortical information processing and storage\n\\cite{Beggs}\\cite{Plenz1}\\cite{Plenz2} and the model of stochastic neural\nnetworks\\cite{Dayan} is correct, as well as the SOC system can be looked upon\nas a W-TFT with spontaneously broken BRST symmetry. Then we should recover the\nneuronal avalanches and spontaneously broken BRST symmetry from the model of\nstochastic neural networks. We find that, provided the divergence of drift\ncoefficients is small and non-constant, the BRST symmetry for the model of\nstochastic neural networks is spontaneously broken. That is, if the SOC of\nbrain neural networks system can be looked upon as a W-TFT with spontaneously\nbroken BRST symmetry, then the general model of stochastic neural networks\nwhich be extensively used in neuroscience \\cite{Dayan} is enough to describe\nthe SOC. On the other hand, using the Fokker-Planck equation, we show the\nsufficient condition on diffusion so that the behavior of the stochastic neural\nnetworks approximate to a stationary Markov process. Rhythms of the firing\nrates of the neuronal networks arise from the process, meanwhile some\nbiological laws are conserved.\n", "versions": [{"version": "v1", "created": "Mon, 21 Jun 2021 04:37:29 GMT"}], "update_date": "2021-06-22", "authors_parsed": [["Zhai", "Jian", ""], ["Yu", "Chaojun", ""], ["Zhai", "You", ""]]}, {"id": "2106.11951", "submitter": "Omer Ashmaig", "authors": "Omer E Ashmaig, Liberty S Hamilton, Pradeep Modur, Robert J Buchanan,\n  Alison R Preston, Andrew J Watrous", "title": "A platform for cognitive monitoring of neurosurgical patients during\n  hospitalization", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "q-bio.NC", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Intracranial recordings in epilepsy patients are increasingly utilized to\ngain insight into the electrophysiological mechanisms of human cognition. There\nare currently several practical limitations to conducting research with these\npatients, including patient and researcher availability and the cognitive\nabilities of patients, which limit the amount of task-related data that can be\ncollected. Prior studies have synchronized clinical audio, video, and neural\nrecordings to understand naturalistic behaviors, but these recordings are\ncentered on the patient to understand their seizure semiology and thus do not\ncapture and synchronize audiovisual stimuli from tasks. Here, we describe a\nplatform for cognitive monitoring of neurosurgical patients during their\nhospitalization that benefits both patients and researchers alike. We provide\nthe full specifications for this system and describe some example use cases in\nperception, memory, and sleep research. Our system opens up new avenues to\ncollect more data per patient using real-world tasks, affording new\npossibilities to conduct longitudinal studies of the electrophysiological basis\nof human cognition under naturalistic conditions.\n", "versions": [{"version": "v1", "created": "Tue, 22 Jun 2021 17:49:38 GMT"}], "update_date": "2021-06-23", "authors_parsed": [["Ashmaig", "Omer E", ""], ["Hamilton", "Liberty S", ""], ["Modur", "Pradeep", ""], ["Buchanan", "Robert J", ""], ["Preston", "Alison R", ""], ["Watrous", "Andrew J", ""]]}, {"id": "2106.12248", "submitter": "Louis Rouillard", "authors": "Louis Rouillard (PARIETAL, Inria, CEA), Demian Wassermann (PARIETAL,\n  Inria, CEA)", "title": "ADAVI: Automatic Dual Amortized Variational Inference Applied To\n  Pyramidal Bayesian Models", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI q-bio.NC stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Frequently, population studies feature pyramidally-organized data represented\nusing Hierarchical Bayesian Models (HBM) enriched with plates. These models can\nbecome prohibitively large in settings such as neuroimaging, where a sample is\ncomposed of a functional MRI signal measured on 64 thousand brain locations,\nacross 4 measurement sessions, and at least tens of subjects. Even a reduced\nexample on a specific cortical region of 300 brain locations features around 1\nmillion parameters, hampering the usage of modern density estimation techniques\nsuch as Simulation-Based Inference (SBI). To infer parameter posterior\ndistributions in this challenging class of problems, we designed a novel\nmethodology that automatically produces a variational family dual to a target\nHBM. This variatonal family, represented as a neural network, consists in the\ncombination of an attention-based hierarchical encoder feeding summary\nstatistics to a set of normalizing flows. Our automatically-derived neural\nnetwork exploits exchangeability in the plate-enriched HBM and factorizes its\nparameter space. The resulting architecture reduces by orders of magnitude its\nparameterization with respect to that of a typical SBI representation, while\nmaintaining expressivity. Our method performs inference on the specified HBM in\nan amortized setup: once trained, it can readily be applied to a new data\nsample to compute the parameters' full posterior. We demonstrate the capability\nof our method on simulated data, as well as a challenging high-dimensional\nbrain parcellation experiment. We also open up several questions that lie at\nthe intersection between SBI techniques and structured Variational Inference.\n", "versions": [{"version": "v1", "created": "Wed, 23 Jun 2021 09:09:01 GMT"}], "update_date": "2021-06-25", "authors_parsed": [["Rouillard", "Louis", "", "PARIETAL, Inria, CEA"], ["Wassermann", "Demian", "", "PARIETAL,\n  Inria, CEA"]]}, {"id": "2106.12254", "submitter": "Taisuke Kobayashi", "authors": "Taisuke Kobayashi, Akiyoshi Kitaoka, Manabu Kosaka, Kenta Tanaka, and\n  Eiji Watanabe", "title": "Motion Illusion-like Patterns Extracted from Photo and Art Images Using\n  Predictive Deep Neural Networks", "comments": "13 pages, 7 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "q-bio.NC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In our previous study, we successfully reproduced the illusory motion of the\nrotating snake illusion using deep neural networks incorporating predictive\ncoding theory. In the present study, we further examined the properties of the\nnetworks using a set of 1500 images, including ordinary static images of\npaintings and photographs and images of various types of motion illusions.\nResults showed that the networks clearly classified illusory images and others\nand reproduced illusory motions against various types of illusions similar to\nhuman perception. Notably, the networks occasionally detected anomalous motion\nvectors, even in ordinally static images where humans were unable to perceive\nany illusory motion. Additionally, illusion-like designs with repeating\npatterns were generated using areas where anomalous vectors were detected, and\npsychophysical experiments were conducted, in which illusory motion perception\nin the generated designs was detected. The observed inaccuracy of the networks\nwill provide useful information for further understanding information\nprocessing associated with human vision.\n", "versions": [{"version": "v1", "created": "Wed, 23 Jun 2021 09:21:12 GMT"}], "update_date": "2021-06-24", "authors_parsed": [["Kobayashi", "Taisuke", ""], ["Kitaoka", "Akiyoshi", ""], ["Kosaka", "Manabu", ""], ["Tanaka", "Kenta", ""], ["Watanabe", "Eiji", ""]]}, {"id": "2106.12993", "submitter": "Pavol Bauer", "authors": "Indrani Sarkar, Indranil Maji, Charitha Omprakash, Sebastian Stober,\n  Sanja Mikulovic, Pavol Bauer", "title": "Evaluation of deep lift pose models for 3D rodent pose estimation based\n  on geometrically triangulated data", "comments": "5 pages, 6 figures, Accepted at the CVPR 2021 CV4Animals workshop", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV q-bio.NC q-bio.QM", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  The assessment of laboratory animal behavior is of central interest in modern\nneuroscience research. Behavior is typically studied in terms of pose changes,\nwhich are ideally captured in three dimensions. This requires triangulation\nover a multi-camera system which view the animal from different angles.\nHowever, this is challenging in realistic laboratory setups due to occlusions\nand other technical constrains. Here we propose the usage of lift-pose models\nthat allow for robust 3D pose estimation of freely moving rodents from a single\nview camera view. To obtain high-quality training data for the pose-lifting, we\nfirst perform geometric calibration in a camera setup involving bottom as well\nas side views of the behaving animal. We then evaluate the performance of two\npreviously proposed model architectures under given inference perspectives and\nconclude that reliable 3D pose inference can be obtained using temporal\nconvolutions. With this work we would like to contribute to a more robust and\ndiverse behavior tracking of freely moving rodents for a wide range of\nexperiments and setups in the neuroscience community.\n", "versions": [{"version": "v1", "created": "Thu, 24 Jun 2021 13:08:33 GMT"}], "update_date": "2021-06-25", "authors_parsed": [["Sarkar", "Indrani", ""], ["Maji", "Indranil", ""], ["Omprakash", "Charitha", ""], ["Stober", "Sebastian", ""], ["Mikulovic", "Sanja", ""], ["Bauer", "Pavol", ""]]}, {"id": "2106.13031", "submitter": "Roman Pogodin", "authors": "Roman Pogodin, Yash Mehta, Timothy P. Lillicrap, Peter E. Latham", "title": "Towards Biologically Plausible Convolutional Networks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.NE q-bio.NC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Convolutional networks are ubiquitous in deep learning. They are particularly\nuseful for images, as they reduce the number of parameters, reduce training\ntime, and increase accuracy. However, as a model of the brain they are\nseriously problematic, since they require weight sharing - something real\nneurons simply cannot do. Consequently, while neurons in the brain can be\nlocally connected (one of the features of convolutional networks), they cannot\nbe convolutional. Locally connected but non-convolutional networks, however,\nsignificantly underperform convolutional ones. This is troublesome for studies\nthat use convolutional networks to explain activity in the visual system. Here\nwe study plausible alternatives to weight sharing that aim at the same\nregularization principle, which is to make each neuron within a pool react\nsimilarly to identical inputs. The most natural way to do that is by showing\nthe network multiple translations of the same image, akin to saccades in animal\nvision. However, this approach requires many translations, and doesn't remove\nthe performance gap. We propose instead to add lateral connectivity to a\nlocally connected network, and allow learning via Hebbian plasticity. This\nrequires the network to pause occasionally for a sleep-like phase of \"weight\nsharing\". This method enables locally connected networks to achieve nearly\nconvolutional performance on ImageNet, thus supporting convolutional networks\nas a model of the visual stream.\n", "versions": [{"version": "v1", "created": "Tue, 22 Jun 2021 17:01:58 GMT"}], "update_date": "2021-06-25", "authors_parsed": [["Pogodin", "Roman", ""], ["Mehta", "Yash", ""], ["Lillicrap", "Timothy P.", ""], ["Latham", "Peter E.", ""]]}, {"id": "2106.13082", "submitter": "Robert Rosenbaum", "authors": "Robert Rosenbaum", "title": "On the relationship between predictive coding and backpropagation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "q-bio.NC cs.LG cs.NE", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Artificial neural networks are often interpreted as abstract models of\nbiological neuronal networks, but they are typically trained using the\nbiologically unrealistic backpropagation algorithm and its variants. Predictive\ncoding has been offered as a potentially more biologically realistic\nalternative to backpropagation for training neural networks. In this\nmanuscript, I review and extend recent work on the mathematical relationship\nbetween predictive coding and backpropagation for training feedforward\nartificial neural networks on supervised learning tasks. I discuss some\nimplications of these results for the interpretation of predictive coding and\ndeep neural networks as models of biological learning and I describe a\nrepository of functions, Torch2PC, for performing predictive coding with\nPyTorch neural network models.\n", "versions": [{"version": "v1", "created": "Sun, 20 Jun 2021 18:22:50 GMT"}, {"version": "v2", "created": "Fri, 25 Jun 2021 11:13:15 GMT"}], "update_date": "2021-06-28", "authors_parsed": [["Rosenbaum", "Robert", ""]]}, {"id": "2106.13249", "submitter": "Tan Zhi-Xuan", "authors": "Arwa Alanqary, Gloria Z. Lin, Joie Le, Tan Zhi-Xuan, Vikash K.\n  Mansinghka, Joshua B. Tenenbaum", "title": "Modeling the Mistakes of Boundedly Rational Agents Within a Bayesian\n  Theory of Mind", "comments": "Accepted to CogSci 2021. 6 pages, 5 figures. (Appendix: 1 page, 1\n  figure)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI q-bio.NC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  When inferring the goals that others are trying to achieve, people\nintuitively understand that others might make mistakes along the way. This is\ncrucial for activities such as teaching, offering assistance, and deciding\nbetween blame or forgiveness. However, Bayesian models of theory of mind have\ngenerally not accounted for these mistakes, instead modeling agents as mostly\noptimal in achieving their goals. As a result, they are unable to explain\nphenomena like locking oneself out of one's house, or losing a game of chess.\nHere, we extend the Bayesian Theory of Mind framework to model boundedly\nrational agents who may have mistaken goals, plans, and actions. We formalize\nthis by modeling agents as probabilistic programs, where goals may be confused\nwith semantically similar states, plans may be misguided due to\nresource-bounded planning, and actions may be unintended due to execution\nerrors. We present experiments eliciting human goal inferences in two domains:\n(i) a gridworld puzzle with gems locked behind doors, and (ii) a block-stacking\ndomain. Our model better explains human inferences than alternatives, while\ngeneralizing across domains. These findings indicate the importance of modeling\nothers as bounded agents, in order to account for the full richness of human\nintuitive psychology.\n", "versions": [{"version": "v1", "created": "Thu, 24 Jun 2021 18:00:03 GMT"}], "update_date": "2021-06-28", "authors_parsed": [["Alanqary", "Arwa", ""], ["Lin", "Gloria Z.", ""], ["Le", "Joie", ""], ["Zhi-Xuan", "Tan", ""], ["Mansinghka", "Vikash K.", ""], ["Tenenbaum", "Joshua B.", ""]]}, {"id": "2106.13541", "submitter": "M Ganesh Kumar", "authors": "M Ganesh Kumar, Cheston Tan, Camilo Libedinsky, Shih-Cheng Yen, Andrew\n  Yong-Yi Tan", "title": "A nonlinear hidden layer enables actor-critic agents to learn multiple\n  paired association navigation", "comments": "31 pages, 8 figures. Acknowledgements revised", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NE q-bio.NC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Navigation to multiple cued reward locations has been increasingly used to\nstudy rodent learning. Though deep reinforcement learning agents have been\nshown to be able to learn the task, they are not biologically plausible.\nBiologically plausible classic actor-critic agents have been shown to learn to\nnavigate to single reward locations, but which biologically plausible agents\nare able to learn multiple cue-reward location tasks has remained unclear. In\nthis computational study, we show versions of classic agents that learn to\nnavigate to a single reward location, and adapt to reward location\ndisplacement, but are not able to learn multiple paired association navigation.\nThe limitation is overcome by an agent in which place cell and cue information\nare first processed by a feedforward nonlinear hidden layer with synapses to\nthe actor and critic subject to temporal difference error-modulated plasticity.\nFaster learning is obtained when the feedforward layer is replaced by a\nrecurrent reservoir network.\n", "versions": [{"version": "v1", "created": "Fri, 25 Jun 2021 10:23:05 GMT"}, {"version": "v2", "created": "Fri, 16 Jul 2021 03:30:13 GMT"}], "update_date": "2021-07-19", "authors_parsed": [["Kumar", "M Ganesh", ""], ["Tan", "Cheston", ""], ["Libedinsky", "Camilo", ""], ["Yen", "Shih-Cheng", ""], ["Tan", "Andrew Yong-Yi", ""]]}, {"id": "2106.13830", "submitter": "Lancelot Da Costa", "authors": "Lancelot Da Costa, Karl Friston, Conor Heins and Grigorios A.\n  Pavliotis", "title": "Bayesian Mechanics for Stationary Processes", "comments": "16 pages, 10 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "math-ph math.MP math.OC nlin.AO q-bio.NC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper develops a Bayesian mechanics for adaptive systems.\n  Firstly, we model the interface between a system and its environment with a\nMarkov blanket. This affords conditions under which states internal to the\nblanket encode information about external states.\n  Second, we introduce dynamics and represent adaptive systems as Markov\nblankets at steady-state. This allows us to identify a wide class of systems\nwhose internal states appear to infer external states, consistent with\nvariational inference in Bayesian statistics and theoretical neuroscience.\n  Finally, we partition the blanket into sensory and active states. It follows\nthat active states can be seen as performing active inference and well-known\nforms of stochastic control (such as PID control), which are prominent\nformulations of adaptive behaviour in theoretical biology and engineering.\n", "versions": [{"version": "v1", "created": "Fri, 25 Jun 2021 18:10:28 GMT"}], "update_date": "2021-06-29", "authors_parsed": [["Da Costa", "Lancelot", ""], ["Friston", "Karl", ""], ["Heins", "Conor", ""], ["Pavliotis", "Grigorios A.", ""]]}, {"id": "2106.14362", "submitter": "Nan Zheng", "authors": "Nan Zheng, Vincent Fitzpatrick, Ran Cheng, Linli Shi, David L. Kaplan,\n  Chen Yang", "title": "Photoacoustic Silk Scaffolds for Neural stimulation and Regeneration", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "q-bio.NC q-bio.TO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Neural interfaces using biocompatible scaffolds provide crucial properties\nfor the functional repair of nerve injuries and neurodegenerative diseases,\nincluding cell adhesion, structural support, and mass transport. Neural\nstimulation has also been found to be effective in promoting neural\nregeneration. This work provides a new strategy to integrate photoacoustic (PA)\nneural stimulation into hydrogel scaffolds using a nanocomposite hydrogel\napproach. Specifically, polyethylene glycol (PEG)-functionalized carbon\nnanotubes (CNT), highly efficient photoacoustic agents, are embedded into silk\nfibroin to form biocompatible and soft photoacoustic materials. We show that\nthese photoacoustic functional scaffolds enable non-genetic activation of\nneurons with a spatial precision defined by the area of light illumination,\npromoting neuron regeneration. These CNT/silk scaffolds offered reliable and\nrepeatable photoacoustic neural stimulation. 94% of photoacoustic stimulated\nneurons exhibit a fluorescence change larger than 10% in calcium imaging in the\nlight illuminated area. The on-demand photoacoustic stimulation increased\nneurite outgrowth by 1.74-fold in a dorsal root ganglion model, when compared\nto the unstimulated group. We also confirmed that photoacoustic neural\nstimulation promoted neurite outgrowth by impacting the brain-derived\nneurotrophic factor (BDNF) pathway. As a multifunctional neural scaffold,\nCNT/silk scaffolds demonstrated non-genetic PA neural stimulation functions and\npromoted neurite outgrowth, providing a new method for non-pharmacological\nneural regeneration.\n", "versions": [{"version": "v1", "created": "Mon, 28 Jun 2021 01:38:39 GMT"}], "update_date": "2021-06-29", "authors_parsed": [["Zheng", "Nan", ""], ["Fitzpatrick", "Vincent", ""], ["Cheng", "Ran", ""], ["Shi", "Linli", ""], ["Kaplan", "David L.", ""], ["Yang", "Chen", ""]]}, {"id": "2106.14612", "submitter": "Robert Worden", "authors": "Robert Worden", "title": "A Theory of Language Learning", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL q-bio.NC", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  A theory of language learning is described, which uses Bayesian induction of\nfeature structures (scripts) and script functions. Each word sense in a\nlanguage is mentally represented by an m-script, a script function which\nembodies all the syntax and semantics of the word. M-scripts form a\nfully-lexicalised unification grammar, which can support adult language. Each\nword m-script can be learnt robustly from about six learning examples. The\ntheory has been implemented as a computer model, which can bootstrap-learn a\nlanguage from zero vocabulary. The Bayesian learning mechanism is (1) Capable:\nto learn arbitrarily complex meanings and syntactic structures; (2) Fast:\nlearning these structures from a few examples each; (3) Robust: learning in the\npresence of much irrelevant noise, and (4) Self-repairing: able to acquire\nimplicit negative evidence, using it to learn exceptions. Children learning\nlanguage are clearly all of (1) - (4), whereas connectionist theories fail on\n(1) and (2), and symbolic theories fail on (3) and (4). The theory is in good\nagreement with many key facts of language acquisition, including facts which\nare problematic for other theories. It is compared with over 100 key\ncross-linguistic findings about acquisition of the lexicon, phrase structure,\nmorphology, complementation and control, auxiliaries, verb argument structures,\ngaps and movement - in nearly all cases giving unforced agreement without extra\nassumptions.\n", "versions": [{"version": "v1", "created": "Sun, 6 Jun 2021 11:06:42 GMT"}], "update_date": "2021-06-29", "authors_parsed": [["Worden", "Robert", ""]]}, {"id": "2106.14675", "submitter": "Nicol\\'as J. Gallego-Molina", "authors": "Nicol\\'as J. Gallego-Molina, Andr\\'es Ortiz, Francisco J.\n  Mart\\'inez-Murcia, Marco Formoso and Almudena Gim\\'enez", "title": "Complex network modelling of EEG band coupling in dyslexia: an\n  exploratory analysis of auditory processing and diagnosis", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "q-bio.NC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Complex network analysis has an increasing relevance in the study of\nneurological disorders, enhancing the knowledge of brain's structural and\nfunctional organization. Network structure and efficiency reveal different\nbrain states along with different ways of processing the information. This work\nis structured around the exploratory analysis of the brain processes involved\nin low-level auditory processing. A complex network analysis was performed on\nthe basis of brain coupling obtained from Electroencephalography (EEG) data,\nwhile different auditory stimuli were presented to the subjects. This coupling\nis inferred from the Phase-Amplitude coupling (PAC) from different EEG\nelectrodes to explore differences between controls and dyslexic subjects.\nCoupling data allows the construction of a graph, and then, graph theory is\nused to study the characteristics of the complex networks throughout time for\ncontrols and dyslexics. This results in a set of metrics including clustering\ncoefficient, path length and small-worldness. From this, different\ncharacteristics linked to the temporal evolution of networks and coupling are\npointed out for dyslexics. Our study revealed patterns related to Dyslexia as\nlosing the small-world topology. Finally, these graph-based features are used\nto classify between controls and dyslexic subjects by means of a Support Vector\nMachine (SVM).\n", "versions": [{"version": "v1", "created": "Mon, 28 Jun 2021 12:57:25 GMT"}, {"version": "v2", "created": "Wed, 21 Jul 2021 11:37:26 GMT"}], "update_date": "2021-07-22", "authors_parsed": [["Gallego-Molina", "Nicol\u00e1s J.", ""], ["Ortiz", "Andr\u00e9s", ""], ["Mart\u00ednez-Murcia", "Francisco J.", ""], ["Formoso", "Marco", ""], ["Gim\u00e9nez", "Almudena", ""]]}, {"id": "2106.15420", "submitter": "Vineet Kotariya", "authors": "Vineet Kotariya, Udayan Ganguly", "title": "Spiking-GAN: A Spiking Generative Adversarial Network Using\n  Time-To-First-Spike Coding", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NE cs.CV q-bio.NC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Spiking Neural Networks (SNNs) have shown great potential in solving deep\nlearning problems in an energy-efficient manner. However, they are still\nlimited to simple classification tasks. In this paper, we propose Spiking-GAN,\nthe first spike-based Generative Adversarial Network (GAN). It employs a kind\nof temporal coding scheme called time-to-first-spike coding. We train it using\napproximate backpropagation in the temporal domain. We use simple\nintegrate-and-fire (IF) neurons with very high refractory period for our\nnetwork which ensures a maximum of one spike per neuron. This makes the model\nmuch sparser than a spike rate-based system. Our modified temporal loss\nfunction called 'Aggressive TTFS' improves the inference time of the network by\nover 33% and reduces the number of spikes in the network by more than 11%\ncompared to previous works. Our experiments show that on training the network\non the MNIST dataset using this approach, we can generate high quality samples.\nThereby demonstrating the potential of this framework for solving such problems\nin the spiking domain.\n", "versions": [{"version": "v1", "created": "Tue, 29 Jun 2021 13:43:07 GMT"}], "update_date": "2021-06-30", "authors_parsed": [["Kotariya", "Vineet", ""], ["Ganguly", "Udayan", ""]]}, {"id": "2106.15428", "submitter": "Richard Betzel", "authors": "Farnaz Zamani Esfahlani, Youngheun Jo, Maria Grazia Puxeddu, Haily\n  Merritt, Jacob C. Tanner, Sarah Greenwell, Riya Patel, Joshua Faskowitz,\n  Richard F. Betzel", "title": "Modularity maximization as a flexible and generic framework for brain\n  network exploratory analysis", "comments": "18 pages, 4 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "q-bio.NC", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  The modular structure of brain networks supports specialized information\nprocessing, complex dynamics, and cost-efficient spatial embedding.\nInter-individual variation in modular structure has been linked to differences\nin performance, disease, and development. There exist many data-driven methods\nfor detecting and comparing modular structure, the most popular of which is\nmodularity maximization. Although modularity maximization is a general\nframework that can be modified and reparamaterized to address domain-specific\nresearch questions, its application to neuroscientific datasets has, thus far,\nbeen narrow. Here, we highlight several strategies in which the\n``out-of-the-box'' version of modularity maximization can be extended to\naddress questions specific to neuroscience. First, we present approaches for\ndetecting ``space-independent'' modules and for applying modularity\nmaximization to signed matrices. Next, we show that the modularity maximization\nframe is well-suited for detecting task- and condition-specific modules.\nFinally, we highlight the role of multi-layer models in detecting and tracking\nmodules across time, tasks, subjects, and modalities. In summary, modularity\nmaximization is a flexible and general framework that can be adapted to detect\nmodular structure resulting from a wide range of hypotheses. This article\nhighlights opens multiple frontiers for future research and applications.\n", "versions": [{"version": "v1", "created": "Tue, 29 Jun 2021 13:56:21 GMT"}], "update_date": "2021-06-30", "authors_parsed": [["Esfahlani", "Farnaz Zamani", ""], ["Jo", "Youngheun", ""], ["Puxeddu", "Maria Grazia", ""], ["Merritt", "Haily", ""], ["Tanner", "Jacob C.", ""], ["Greenwell", "Sarah", ""], ["Patel", "Riya", ""], ["Faskowitz", "Joshua", ""], ["Betzel", "Richard F.", ""]]}, {"id": "2106.15685", "submitter": "Gabriel Ocker", "authors": "Gabriel Koch Ocker and Michael A. Buice", "title": "Tensor decomposition of higher-order correlations by nonlinear Hebbian\n  plasticity", "comments": "9 pages, 3 figures + supplemental 20 pages, 4 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "q-bio.NC stat.ML", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Biological synaptic plasticity exhibits nonlinearities that are not accounted\nfor by classic Hebbian learning rules. Here, we introduce a simple family of\ngeneralized, nonlinear Hebbian learning rules. We study the computations\nimplemented by their dynamics in the simple setting of a neuron receiving\nfeedforward inputs. We show that these nonlinear Hebbian rules allow a neuron\nto learn tensor decompositions of its higher-order input correlations. The\nparticular input correlation decomposed, and the form of the decomposition,\ndepend on the location of nonlinearities in the plasticity rule. For simple,\nbiologically motivated parameters, the neuron learns tensor eigenvectors of\nhigher-order input correlations. We prove that each tensor eigenvector is an\nattractor and determine their basins of attraction. We calculate the volume of\nthose basins, showing that the dominant eigenvector has the largest basin of\nattraction. We then study arbitrary learning rules, and find that any learning\nrule that admits a finite Taylor expansion into the neural input and output\nalso has stable equilibria at tensor eigenvectors of its higher-order input\ncorrelations. Nonlinearities in synaptic plasticity thus allow a neuron to\nencode higher-order input correlations in a simple fashion.\n", "versions": [{"version": "v1", "created": "Tue, 29 Jun 2021 19:24:35 GMT"}], "update_date": "2021-07-01", "authors_parsed": [["Ocker", "Gabriel Koch", ""], ["Buice", "Michael A.", ""]]}, {"id": "2106.16059", "submitter": "Thomas Shultz", "authors": "Thomas R Shultz, Ardavan S Nobandegani", "title": "A Computational Model of Infant Learning and Reasoning with\n  Probabilities", "comments": "To be published in Psychological Review", "journal-ref": null, "doi": null, "report-no": null, "categories": "q-bio.NC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recent experiments reveal that 6- to 12-month-old infants can learn\nprobabilities and reason with them. In this work, we present a novel\ncomputational system called Neural Probability Learner and Sampler (NPLS) that\nlearns and reasons with probabilities, providing a computationally sufficient\nmechanism to explain infant probabilistic learning and inference. In 24\ncomputer simulations, NPLS simulations show how probability distributions can\nemerge naturally from neural-network learning of event sequences, providing a\nnovel explanation of infant probabilistic learning and reasoning. Three\nmathematical proofs show how and why NPLS simulates the infant results so\naccurately. The results are situated in relation to seven other active research\nlines. This work provides an effective way to integrate Bayesian and\nneural-network approaches to cognition.\n", "versions": [{"version": "v1", "created": "Wed, 30 Jun 2021 13:34:37 GMT"}], "update_date": "2021-07-01", "authors_parsed": [["Shultz", "Thomas R", ""], ["Nobandegani", "Ardavan S", ""]]}, {"id": "2106.16061", "submitter": "Quanlong Wang", "authors": "Camilo Miguel Signorelli, Quanlong Wang, Bob Coecke", "title": "Reasoning about conscious experience with axiomatic and graphical\n  mathematics", "comments": "20 pages, accepted to Consciousness and Cognition", "journal-ref": null, "doi": null, "report-no": null, "categories": "q-bio.NC cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We cast aspects of consciousness in axiomatic mathematical terms, using the\ngraphical calculus of general process theories (a.k.a symmetric monoidal\ncategories and Frobenius algebras therein). This calculus exploits the\nontological neutrality of process theories. A toy example using the axiomatic\ncalculus is given to show the power of this approach, recovering other aspects\nof conscious experience, such as external and internal subjective distinction,\nprivacy or unreadability of personal subjective experience, and phenomenal\nunity, one of the main issues for scientific studies of consciousness. In fact,\nthese features naturally arise from the compositional nature of axiomatic\ncalculus.\n", "versions": [{"version": "v1", "created": "Wed, 30 Jun 2021 13:39:02 GMT"}], "update_date": "2021-07-01", "authors_parsed": [["Signorelli", "Camilo Miguel", ""], ["Wang", "Quanlong", ""], ["Coecke", "Bob", ""]]}, {"id": "2106.16154", "submitter": "Jihwan Lee", "authors": "Ren Liu, Jihwan Lee, Youngbin Tchoe, Deborah Pre, Andrew M. Bourhis,\n  Agnieszka D'Antonio-Chronowska, Gaelle Robin, Sang Heon Lee, Yun Goo Ro,\n  Ritwik Vatsyayan, Karen J. Tonsfeldt, Lorraine A. Hossain, M. Lisa Phipps,\n  Jinkyoung Yoo, John Nogan, Jennifer S. Martinez, Kelly A. Frazer, Anne G.\n  Bang, Shadi A. Dayeh", "title": "Ultra-Sharp Nanowire Arrays Natively Permeate, Record, and Stimulate\n  Intracellular Activity in Neuronal and Cardiac Networks", "comments": "Main manuscript: 33 pages, 4 figures, Supporting information: 43\n  pages, 27 figures, Submitted to Advanced Materials", "journal-ref": null, "doi": null, "report-no": null, "categories": "cond-mat.mtrl-sci physics.bio-ph physics.med-ph q-bio.CB q-bio.NC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Intracellular access with high spatiotemporal resolution can enhance our\nunderstanding of how neurons or cardiomyocytes regulate and orchestrate network\nactivity, and how this activity can be affected with pharmacology or other\ninterventional modalities. Nanoscale devices often employ electroporation to\ntransiently permeate the cell membrane and record intracellular potentials,\nwhich tend to decrease rapidly to extracellular potential amplitudes with time.\nHere, we report innovative scalable, vertical, ultra-sharp nanowire arrays that\nare individually addressable to enable long-term, native recordings of\nintracellular potentials. We report large action potential amplitudes that are\nindicative of intracellular access from 3D tissue-like networks of neurons and\ncardiomyocytes across recording days and that do not decrease to extracellular\namplitudes for the duration of the recording of several minutes. Our findings\nare validated with cross-sectional microscopy, pharmacology, and electrical\ninterventions. Our experiments and simulations demonstrate that individual\nelectrical addressability of nanowires is necessary for high-fidelity\nintracellular electrophysiological recordings. This study advances our\nunderstanding of and control over high-quality multi-channel intracellular\nrecordings, and paves the way toward predictive, high-throughput, and low-cost\nelectrophysiological drug screening platforms.\n", "versions": [{"version": "v1", "created": "Wed, 30 Jun 2021 15:51:22 GMT"}, {"version": "v2", "created": "Mon, 5 Jul 2021 21:47:13 GMT"}], "update_date": "2021-07-07", "authors_parsed": [["Liu", "Ren", ""], ["Lee", "Jihwan", ""], ["Tchoe", "Youngbin", ""], ["Pre", "Deborah", ""], ["Bourhis", "Andrew M.", ""], ["D'Antonio-Chronowska", "Agnieszka", ""], ["Robin", "Gaelle", ""], ["Lee", "Sang Heon", ""], ["Ro", "Yun Goo", ""], ["Vatsyayan", "Ritwik", ""], ["Tonsfeldt", "Karen J.", ""], ["Hossain", "Lorraine A.", ""], ["Phipps", "M. Lisa", ""], ["Yoo", "Jinkyoung", ""], ["Nogan", "John", ""], ["Martinez", "Jennifer S.", ""], ["Frazer", "Kelly A.", ""], ["Bang", "Anne G.", ""], ["Dayeh", "Shadi A.", ""]]}, {"id": "2106.16219", "submitter": "Seongmin Park", "authors": "Linda Q. Yu, Seongmin A. Park, Sarah C. Sweigart, Erie D. Boorman,\n  Matthew R. Nassar", "title": "Do grid codes afford generalization and flexible decision-making?", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "q-bio.NC", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Behavioral flexibility is learning from previous experiences and planning\nappropriate actions in a changing or novel environment. Successful behavioral\nadaptation depends on internal models the brain builds to represent the\nrelational structure of an abstract task. Emerging evidence suggests that the\nwell-known roles of the hippocampus and entorhinal cortex (HC-EC) in\nintegrating spatial relationships into cognitive maps can be extended to map\nthe transition structure between states in non-spatial abstract tasks. However,\nwhat the EC grid-codes actually compute to afford generalization remains\nelusive. We introduce two non-exclusive ideas regarding what grid-codes may\nrepresent to afford higher-level cognition. One idea is that grid-codes are\neigenvectors of the successor representation (SR) learned online during a task.\nThis view assumes that the grid codes serve as an efficient basis function for\nlearning and representing experienced relationships between entities.\nSubsequently, the grid codes facilitate generalization in novel contexts such\nas when the goal changes. The second idea is that the grid-codes reflect the\ninferred global task structure. This view assumes that the grid-code represents\na structural code that is factorized from specific sensory content, enabling\nstructural information to be transferred across tasks. Subsequently, the brain\ncould afford one-shot inferences without requiring experience. The ability to\ngeneralize experiences and make appropriate decisions in novel situations is\ncritical for both animals and machines. Here we review proposed computations of\nthe grid-code in the brain, which is potentially critical to behavioral\nflexibility.\n", "versions": [{"version": "v1", "created": "Wed, 30 Jun 2021 17:18:09 GMT"}], "update_date": "2021-07-01", "authors_parsed": [["Yu", "Linda Q.", ""], ["Park", "Seongmin A.", ""], ["Sweigart", "Sarah C.", ""], ["Boorman", "Erie D.", ""], ["Nassar", "Matthew R.", ""]]}]