[{"id": "0712.0036", "submitter": "Sebastian Brandt", "authors": "Ulrike Meyer, Jing Shao, Saurish Chakrabarty, Sebastian F. Brandt,\n  Harald Luksch, Ralf Wessel", "title": "Distributed delays stabilize neural feedback systems", "comments": "7 pages, 4 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "physics.bio-ph q-bio.NC", "license": null, "abstract": "  We consider the effect of distributed delays in neural feedback systems. The\navian optic tectum is reciprocally connected with the nucleus isthmi.\nExtracellular stimulation combined with intracellular recordings reveal a range\nof signal delays from 4 to 9 ms between isthmotectal elements. This observation\ntogether with prior mathematical analysis concerning the influence of a delay\ndistribution on system dynamics raises the question whether a broad delay\ndistribution can impact the dynamics of neural feedback loops. For a system of\nreciprocally connected model neurons, we found that distributed delays enhance\nsystem stability in the following sense. With increased distribution of delays,\nthe system converges faster to a fixed point and converges slower toward a\nlimit cycle. Further, the introduction of distributed delays leads to an\nincreased range of the average delay value for which the system's equilibrium\npoint is stable. The enhancement of stability with increasing delay\ndistribution is caused by the introduction of smaller delays rather than the\ndistribution per se.\n", "versions": [{"version": "v1", "created": "Mon, 3 Dec 2007 19:30:36 GMT"}], "update_date": "2007-12-04", "authors_parsed": [["Meyer", "Ulrike", ""], ["Shao", "Jing", ""], ["Chakrabarty", "Saurish", ""], ["Brandt", "Sebastian F.", ""], ["Luksch", "Harald", ""], ["Wessel", "Ralf", ""]]}, {"id": "0712.0148", "submitter": "Shengrong Zou", "authors": "Sheng-Rong Zou, Zhong-Wei Guo, Yu-Jing Peng, Ta Zhou, Chang-Gui Gu,\n  Da-Ren He", "title": "A Brand-new Research Method of Neuroendocrine System", "comments": "9 pages with 3 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "nlin.AO q-bio.NC", "license": null, "abstract": "  In this paper, we present the empirical investigation results on the\nneuroendocrine system by bipartite graphs. This neuroendocrine network model\ncan describe the structural characteristic of neuroendocrine system. The act\ndegree distribution and cumulate act degree distribution show so-called shifted\npower law-SPL function forms. In neuroendocrine network, the act degree stands\nfor the number of the cells that secretes a single mediator, in which\nbFGF(basic fibroblast growth factor) is the largest node act degree. It is an\nimportant mitogenic cytokine, followed by TGF-beta, IL-6, IL1-beta, VEGF,\nIGF-1and so on. They are critical in neuroendocrine system to maintain bodily\nhealthiness, emotional stabilization and endocrine harmony. The average act\ndegree of neuroendocrine network is h = 3.01, It means each mediator is\nsecreted by three cells on an average . The similarity that stand for the\naverage probability of secreting the same mediators by all the neuroendocrine\ncells is s = 0.14. Our results may be used in the research of the medical\ntreatment of neuroendocrine diseases.\n", "versions": [{"version": "v1", "created": "Sun, 2 Dec 2007 14:45:14 GMT"}], "update_date": "2007-12-04", "authors_parsed": [["Zou", "Sheng-Rong", ""], ["Guo", "Zhong-Wei", ""], ["Peng", "Yu-Jing", ""], ["Zhou", "Ta", ""], ["Gu", "Chang-Gui", ""], ["He", "Da-Ren", ""]]}, {"id": "0712.0846", "submitter": "Kai Miller", "authors": "Kai J. Miller, Larry B. Sorensen, Jeffrey G. Ojemann, Marcel den Nijs", "title": "ECoG observations of power-law scaling in the human cortex", "comments": "4 pages, 4 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "q-bio.NC cond-mat.other", "license": null, "abstract": "  We report the results of our search for power-law electrical signals in the\nhuman brain, using subdural electrocorticographic recordings from the surface\nof the cortex. The power spectral density (PSD) of these signals has the\npower-law form $ P(f)\\sim f^{-\\chi} $ from 80 to 500 Hz. This scaling index\n$\\chi = 4.0\\pm 0.1$ is universal, across subjects, area in the cortex, and\nlocal neural activity levels. The shape of the PSD does not change with local\ncortex activity, only its amplitude increases. We observe a knee in the spectra\nat $f_0\\simeq 70$ Hz, implying the existence of a characteristic time scale\n$\\tau=(2\\pi f_0)^{-1}\\simeq 2-4$ msec. For $f<f_0$ we find evidence for a\npower-law with $\\chi_L\\simeq 2.0\\pm 0.4$.\n", "versions": [{"version": "v1", "created": "Wed, 5 Dec 2007 23:04:04 GMT"}], "update_date": "2007-12-11", "authors_parsed": [["Miller", "Kai J.", ""], ["Sorensen", "Larry B.", ""], ["Ojemann", "Jeffrey G.", ""], ["Nijs", "Marcel den", ""]]}, {"id": "0712.1003", "submitter": "Anna Levina", "authors": "Anna Levina, J. Michael Herrmann, Theo Geisel", "title": "Dynamical synapses causing self-organized criticality in neural networks", "comments": "9 pages, 4 figures", "journal-ref": "A. Levina, J. M. Herrmann, T. Geisel. Dynamical synapses causing\n  self-organized criticality in neural networks, Nature Phys. 3, 857-860 (2007)", "doi": "10.1038/nphys758", "report-no": null, "categories": "cond-mat.stat-mech cond-mat.dis-nn q-bio.NC", "license": null, "abstract": "  We show that a network of spiking neurons exhibits robust self-organized\ncriticality if the synaptic efficacies follow realistic dynamics. Deriving\nanalytical expressions for the average coupling strengths and inter-spike\nintervals, we demonstrate that networks with dynamical synapses exhibit\ncritical avalanche dynamics for a wide range of interaction parameters. We\nprove that in the thermodynamical limit the network becomes critical for all\nlarge enough coupling parameters. We thereby explain experimental observations\nin which cortical neurons show avalanche activity with the total intensity of\nfiring events being distributed as a power-law.\n", "versions": [{"version": "v1", "created": "Thu, 6 Dec 2007 18:04:19 GMT"}], "update_date": "2007-12-07", "authors_parsed": [["Levina", "Anna", ""], ["Herrmann", "J. Michael", ""], ["Geisel", "Theo", ""]]}, {"id": "0712.1126", "submitter": "Ricard Sole", "authors": "Ricard V. Sole", "title": "Consciousness, brains and the replica problem", "comments": "4 pages, 1 figure, preprint", "journal-ref": null, "doi": null, "report-no": null, "categories": "nlin.AO q-bio.NC", "license": null, "abstract": "  Although the conscious state is considered an emergent property of the\nunderlying brain activity and thus somehow resides on brain hardware, there is\na non-univocal mapping between both. Given a neural hardware, multiple\nconscious patterns are consistent with it. Here we show, by means of a simple\n{\\em gedankenexperiment} that this has an important logic consequence: any\nscenario involving the transient shutdown of brain activity leads to the\nirreversible death of the conscious experience. In a fundamental way, unless\nthe continuous stream of consciousness is guaranteed, the previous self\nvanishes and is replaced by a new one.\n", "versions": [{"version": "v1", "created": "Fri, 7 Dec 2007 12:27:38 GMT"}], "update_date": "2007-12-10", "authors_parsed": [["Sole", "Ricard V.", ""]]}, {"id": "0712.1219", "submitter": "Francois Meyer", "authors": "Francois G. Meyer and Greg J. Stephens", "title": "Locality and low-dimensions in the prediction of natural experience from\n  fMRI", "comments": "To appear in: Advances in Neural Information Processing Systems 20,\n  Scholkopf B., Platt J. and Hofmann T. (Editors), MIT Press, 2008", "journal-ref": null, "doi": null, "report-no": null, "categories": "q-bio.NC stat.ML", "license": null, "abstract": "  Functional Magnetic Resonance Imaging (fMRI) provides dynamical access into\nthe complex functioning of the human brain, detailing the hemodynamic activity\nof thousands of voxels during hundreds of sequential time points. One approach\ntowards illuminating the connection between fMRI and cognitive function is\nthrough decoding; how do the time series of voxel activities combine to provide\ninformation about internal and external experience? Here we seek models of fMRI\ndecoding which are balanced between the simplicity of their interpretation and\nthe effectiveness of their prediction. We use signals from a subject immersed\nin virtual reality to compare global and local methods of prediction applying\nboth linear and nonlinear techniques of dimensionality reduction. We find that\nthe prediction of complex stimuli is remarkably low-dimensional, saturating\nwith less than 100 features. In particular, we build effective models based on\nthe decorrelated components of cognitive activity in the classically-defined\nBrodmann areas. For some of the stimuli, the top predictive areas were\nsurprisingly transparent, including Wernicke's area for verbal instructions,\nvisual cortex for facial and body features, and visual-temporal regions for\nvelocity. Direct sensory experience resulted in the most robust predictions,\nwith the highest correlation ($c \\sim 0.8$) between the predicted and\nexperienced time series of verbal instructions. Techniques based on non-linear\ndimensionality reduction (Laplacian eigenmaps) performed similarly. The\ninterpretability and relative simplicity of our approach provides a conceptual\nbasis upon which to build more sophisticated techniques for fMRI decoding and\noffers a window into cognitive function during dynamic, natural experience.\n", "versions": [{"version": "v1", "created": "Fri, 7 Dec 2007 20:21:18 GMT"}, {"version": "v2", "created": "Sat, 12 Jan 2008 01:00:50 GMT"}], "update_date": "2008-01-16", "authors_parsed": [["Meyer", "Francois G.", ""], ["Stephens", "Greg J.", ""]]}, {"id": "0712.1845", "submitter": "Andrew Inglis", "authors": "Andrew Inglis, Luis Cruz, Dan L. Roe, H.E. Stanley, Douglas L. Rosene,\n  Brigita Urbanc", "title": "Automated identification of neurons and their locations", "comments": "38 pages. Formatted for two-sided printing. Supplemental material and\n  software available at http://physics.bu.edu/~ainglis/ANRA/", "journal-ref": null, "doi": null, "report-no": null, "categories": "physics.bio-ph q-bio.NC", "license": null, "abstract": "  Individual locations of many neuronal cell bodies (>10^4) are needed to\nenable statistically significant measurements of spatial organization within\nthe brain such as nearest-neighbor and microcolumnarity measurements. In this\npaper, we introduce an Automated Neuron Recognition Algorithm (ANRA) which\nobtains the (x,y) location of individual neurons within digitized images of\nNissl-stained, 30 micron thick, frozen sections of the cerebral cortex of the\nRhesus monkey. Identification of neurons within such Nissl-stained sections is\ninherently difficult due to the variability in neuron staining, the overlap of\nneurons, the presence of partial or damaged neurons at tissue surfaces, and the\npresence of non-neuron objects, such as glial cells, blood vessels, and random\nartifacts. To overcome these challenges and identify neurons, ANRA applies a\ncombination of image segmentation and machine learning. The steps involve\nactive contour segmentation to find outlines of potential neuron cell bodies\nfollowed by artificial neural network training using the segmentation\nproperties (size, optical density, gyration, etc.) to distinguish between\nneuron and non-neuron segmentations. ANRA positively identifies 86[5]% neurons\nwith 15[8]% error (mean[st.dev.]) on a wide range of Nissl-stained images,\nwhereas semi-automatic methods obtain 80[7]%/17[12]%. A further advantage of\nANRA is that it affords an unlimited increase in speed from semi-automatic\nmethods, and is computationally efficient, with the ability to recognize ~100\nneurons per minute using a standard personal computer. ANRA is amenable to\nanalysis of huge photo-montages of Nissl-stained tissue, thereby opening the\ndoor to fast, efficient and quantitative analysis of vast stores of archival\nmaterial that exist in laboratories and research collections around the world.\n", "versions": [{"version": "v1", "created": "Wed, 12 Dec 2007 00:17:17 GMT"}, {"version": "v2", "created": "Wed, 30 Apr 2008 21:23:19 GMT"}], "update_date": "2008-05-01", "authors_parsed": [["Inglis", "Andrew", ""], ["Cruz", "Luis", ""], ["Roe", "Dan L.", ""], ["Stanley", "H. E.", ""], ["Rosene", "Douglas L.", ""], ["Urbanc", "Brigita", ""]]}, {"id": "0712.2097", "submitter": "Maurice Courbage", "authors": "Maurice Courbage (MSC), V.I. Nekorkin (IAPRAS), L.V. Vdovin (IAPRAS)", "title": "Chaotic oscillations in a map-based model of neural activity", "comments": null, "journal-ref": null, "doi": "10.1063/1.2795435", "report-no": null, "categories": "q-bio.NC nlin.CD", "license": null, "abstract": "  We propose a discrete time dynamical system (a map) as phenomenological model\nof excitable and spiking-bursting neurons. The model is a discontinuous\ntwo-dimensional map. We find condition under which this map has an invariant\nregion on the phase plane, containing chaotic attractor. This attractor creates\nchaotic spiking-bursting oscillations of the model. We also show various\nregimes of other neural activities (subthreshold oscillations, phasic spiking\netc.) derived from the proposed model.\n", "versions": [{"version": "v1", "created": "Thu, 13 Dec 2007 06:33:59 GMT"}], "update_date": "2009-11-13", "authors_parsed": [["Courbage", "Maurice", "", "MSC"], ["Nekorkin", "V. I.", "", "IAPRAS"], ["Vdovin", "L. V.", "", "IAPRAS"]]}, {"id": "0712.2108", "submitter": "Yong Fu", "authors": "Simon Fu", "title": "An Illustrated Introduction to the Basic Biological Principles", "comments": "Content changed", "journal-ref": null, "doi": null, "report-no": null, "categories": "q-bio.PE q-bio.GN q-bio.NC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Both external environmental selection and internal lower-level evolution are\nessential for an integral picture of evolution. This paper proposes that the\ndivision of internal evolution into DNA/RNA pattern formation (genotype) and\nprotein functional action (phenotype) resolves a universal conflict between\nfitness and evolvability. Specifically, this paper explains how this universal\nconflict drove the emergence of genotype-phenotype division, why this labor\ndivision is responsible for the extraordinary complexity of life, and how the\nspecific ways of genotype-phenotype mapping in the labor division determine the\npaths and forms of evolution and development.\n", "versions": [{"version": "v1", "created": "Thu, 13 Dec 2007 08:23:38 GMT"}, {"version": "v2", "created": "Sat, 15 Dec 2007 08:20:21 GMT"}, {"version": "v3", "created": "Wed, 2 Jan 2008 08:20:01 GMT"}, {"version": "v4", "created": "Mon, 16 Mar 2009 02:56:16 GMT"}, {"version": "v5", "created": "Mon, 14 Dec 2009 01:18:52 GMT"}], "update_date": "2009-12-14", "authors_parsed": [["Fu", "Simon", ""]]}, {"id": "0712.2218", "submitter": "Vadas Gintautas", "authors": "Luis M. A. Bettencourt, Vadas Gintautas, Michael I. Ham", "title": "Identification of functional information subgraphs in complex networks", "comments": "4 pages, 4 figures", "journal-ref": "Phys. Rev. Lett. 100, 238701 (2008)", "doi": "10.1103/PhysRevLett.100.238701", "report-no": null, "categories": "q-bio.NC cond-mat.dis-nn", "license": null, "abstract": "  We present a general information theoretic approach for identifying\nfunctional subgraphs in complex networks where the dynamics of each node are\nobservable. We show that the uncertainty in the state of each node can be\nexpressed as a sum of information quantities involving a growing number of\ncorrelated variables at other nodes. We demonstrate that each term in this sum\nis generated by successively conditioning mutual informations on new measured\nvariables, in a way analogous to a discrete differential calculus. The analogy\nto a Taylor series suggests efficient search algorithms for determining the\nstate of a target variable in terms of functional groups of other degrees of\nfreedom. We apply this methodology to electrophysiological recordings of\nnetworks of cortical neurons grown it in vitro. Despite strong stochasticity,\nwe show that each cell's patterns of firing are generally explained by the\nactivity of a small number of other neurons. We identify these neuronal\nsubgraphs in terms of their mutually redundant or synergetic character and\nreconstruct neuronal circuits that account for the state of each target cell.\n", "versions": [{"version": "v1", "created": "Thu, 13 Dec 2007 19:28:55 GMT"}], "update_date": "2008-07-31", "authors_parsed": [["Bettencourt", "Luis M. A.", ""], ["Gintautas", "Vadas", ""], ["Ham", "Michael I.", ""]]}, {"id": "0712.2437", "submitter": "William Bialek", "authors": "Tamara Broderick, Miroslav Dudik, Gasper Tkacik, Robert E. Schapire\n  and William Bialek", "title": "Faster solutions of the inverse pairwise Ising problem", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "q-bio.QM cond-mat.dis-nn q-bio.NC", "license": null, "abstract": "  Recent work has shown that probabilistic models based on pairwise\ninteractions-in the simplest case, the Ising model-provide surprisingly\naccurate descriptions of experiments on real biological networks ranging from\nneurons to genes. Finding these models requires us to solve an inverse problem:\ngiven experimentally measured expectation values, what are the parameters of\nthe underlying Hamiltonian? This problem sits at the intersection of\nstatistical physics and machine learning, and we suggest that more efficient\nsolutions are possible by merging ideas from the two fields. We use a\ncombination of recent coordinate descent algorithms with an adaptation of the\nhistogram Monte Carlo method, and implement these techniques to take advantage\nof the sparseness found in data on real neurons. The resulting algorithm learns\nthe parameters of an Ising model describing a network of forty neurons within a\nfew minutes. This opens the possibility of analyzing much larger data sets now\nemerging, and thus testing hypotheses about the collective behaviors of these\nnetworks.\n", "versions": [{"version": "v1", "created": "Fri, 14 Dec 2007 19:31:32 GMT"}, {"version": "v2", "created": "Sat, 15 Dec 2007 20:50:50 GMT"}], "update_date": "2007-12-18", "authors_parsed": [["Broderick", "Tamara", ""], ["Dudik", "Miroslav", ""], ["Tkacik", "Gasper", ""], ["Schapire", "Robert E.", ""], ["Bialek", "William", ""]]}, {"id": "0712.2854", "submitter": "Geoffrey Hoffmann PhD", "authors": "Geoffrey W. Hoffmann", "title": "Perception and recognition in a neural network theory in which neurons\n  exhibit hysteresis", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "q-bio.NC", "license": null, "abstract": "  A neural network theory of visual perception and recognition is presented.\nInformation flows both from the retina to the brain and from the brain to the\nretina. A report that when a scene is perceived 50 retinal cells are much more\nactive than any of the other retinal cells is ascribed significance in the\ntheory. The theory involves neurons that exhibit hysteresis, without the need\nfor any changes in synaptic connection strengths during learning. The fact that\nthe brain is able to recognize faces and other objects very rapidly is\ndiscussed in the context of the theory. The theory can be epitomized as \"We see\nwith our eyes and remember with our brains\".\n", "versions": [{"version": "v1", "created": "Mon, 17 Dec 2007 23:49:07 GMT"}, {"version": "v2", "created": "Sun, 23 Dec 2007 18:57:41 GMT"}, {"version": "v3", "created": "Mon, 31 Dec 2007 04:10:33 GMT"}], "update_date": "2007-12-31", "authors_parsed": [["Hoffmann", "Geoffrey W.", ""]]}, {"id": "0712.3770", "submitter": "Murilo Baptista S.", "authors": "M. S. Baptista and J. Kurths", "title": "Transmission of Information in Active Networks", "comments": "15 pages, 5 figures. submitted for publication. to appear in Phys.\n  Rev. E", "journal-ref": null, "doi": "10.1103/PhysRevE.77.026205", "report-no": null, "categories": "nlin.CD nlin.SI q-bio.NC", "license": null, "abstract": "  Shannon's Capacity Theorem is the main concept behind the Theory of\nCommunication. It says that if the amount of information contained in a signal\nis smaller than the channel capacity of a physical media of communication, it\ncan be transmitted with arbitrarily small probability of error. This theorem is\nusually applicable to ideal channels of communication in which the information\nto be transmitted does not alter the passive characteristics of the channel\nthat basically tries to reproduce the source of information. For an {\\it active\nchannel}, a network formed by elements that are dynamical systems (such as\nneurons, chaotic or periodic oscillators), it is unclear if such theorem is\napplicable, once an active channel can adapt to the input of a signal, altering\nits capacity. To shed light into this matter, we show, among other results, how\nto calculate the information capacity of an active channel of communication.\nThen, we show that the {\\it channel capacity} depends on whether the active\nchannel is self-excitable or not and that, contrary to a current belief,\ndesynchronization can provide an environment in which large amounts of\ninformation can be transmitted in a channel that is self-excitable. An\ninteresting case of a self-excitable active channel is a network of\nelectrically connected Hindmarsh-Rose chaotic neurons.\n", "versions": [{"version": "v1", "created": "Fri, 21 Dec 2007 18:40:49 GMT"}, {"version": "v2", "created": "Wed, 16 Jan 2008 14:01:23 GMT"}], "update_date": "2009-11-13", "authors_parsed": [["Baptista", "M. S.", ""], ["Kurths", "J.", ""]]}, {"id": "0712.3897", "submitter": "Yuri A. Dabaghian", "authors": "Yu. Dabaghian, A. G. Cohn, L. Frank", "title": "Topological Maps from Signals", "comments": "posted by permission of ACM for personal use. The definitive version\n  was published in (ACMGIS .07, November 7-9, 2007, Seattle, WA) ISBN\n  978-1-59593-914-2/07/11. 11 pages, 4 figures", "journal-ref": "proceedings of the 15th ACM International Symposium ACM GIS 2007,\n  pp. 392-395", "doi": null, "report-no": null, "categories": "q-bio.QM q-bio.NC", "license": null, "abstract": "  We discuss the task of reconstructing the topological map of an environment\nbased on the sequences of locations visited by a mobile agent -- this occurs in\nsystems neuroscience, where one runs into the task of reconstructing the global\ntopological map of the environment based on activation patterns of the place\ncoding cells in hippocampus area of the brain. A similar task appears in the\ncontext of establishing wifi connectivity maps.\n", "versions": [{"version": "v1", "created": "Sun, 23 Dec 2007 06:00:20 GMT"}], "update_date": "2007-12-27", "authors_parsed": [["Dabaghian", "Yu.", ""], ["Cohn", "A. G.", ""], ["Frank", "L.", ""]]}, {"id": "0712.4381", "submitter": "William Bialek", "authors": "William Bialek, Rob R. de Ruyter van Steveninck and Naftali Tishby", "title": "Efficient representation as a design principle for neural coding and\n  computation", "comments": "Based on a presentation at the International Symposium on Information\n  Theory 2006", "journal-ref": null, "doi": null, "report-no": null, "categories": "q-bio.NC", "license": null, "abstract": "  Does the brain construct an efficient representation of the sensory world? We\nreview progress on this question, focusing on a series of experiments in the\nlast decade which use fly vision as a model system in which theory and\nexperiment can confront each other. Although the idea of efficient\nrepresentation has been productive, clearly it is incomplete since it doesn't\ntell us which bits of sensory information are most valuable to the organism. We\nsuggest that an organism which maximizes the (biologically meaningful) adaptive\nvalue of its actions given fixed resources should have internal representations\nof the outside world that are optimal in a very specific information theoretic\nsense: they maximize the information about the future of sensory inputs at a\nfixed value of the information about their past. This principle contains as\nspecial cases computations which the brain seems to carry out, and it should be\npossible to test this optimization directly. We return to the fly visual system\nand report the results of preliminary experiments that are in encouraging\nagreement with theory.\n", "versions": [{"version": "v1", "created": "Fri, 28 Dec 2007 17:46:41 GMT"}], "update_date": "2007-12-31", "authors_parsed": [["Bialek", "William", ""], ["van Steveninck", "Rob R. de Ruyter", ""], ["Tishby", "Naftali", ""]]}]