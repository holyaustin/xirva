[{"id": "1803.00044", "submitter": "Rok Cestnik", "authors": "Rok Cestnik, Michael Rosenblum", "title": "Inferring the phase response curve from observation of a continuously\n  perturbed oscillator", "comments": "11 pages (+6 supplementary), 7 figures (+8 supplementary)", "journal-ref": "Scientific Reports, volume 8, Article number: 13606 (2018)", "doi": "10.1038/s41598-018-32069-y", "report-no": null, "categories": "nlin.AO q-bio.NC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Phase response curves are important for analysis and modeling of oscillatory\ndynamics in various applications, particularly in neuroscience. Standard\nexperimental technique for determining them requires isolation of the system\nand application of a specifically designed input. However, isolation is not\nalways feasible and we are compelled to observe the system in its natural\nenvironment under free-running conditions. To that end we propose an approach\nrelying only on passive observations of the system and its input. We illustrate\nit with simulation results of an oscillator driven by a stochastic force.\n", "versions": [{"version": "v1", "created": "Wed, 28 Feb 2018 19:25:22 GMT"}, {"version": "v2", "created": "Tue, 18 Sep 2018 21:32:37 GMT"}], "update_date": "2018-09-20", "authors_parsed": [["Cestnik", "Rok", ""], ["Rosenblum", "Michael", ""]]}, {"id": "1803.00338", "submitter": "Manuel Molano-Mazon", "authors": "Manuel Molano-Mazon, Arno Onken, Eugenio Piasini, Stefano Panzeri", "title": "Synthesizing realistic neural population activity patterns using\n  Generative Adversarial Networks", "comments": "Published as a conference paper at ICLR 2018 V2: minor changes in\n  supp. material", "journal-ref": null, "doi": null, "report-no": null, "categories": "q-bio.NC cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The ability to synthesize realistic patterns of neural activity is crucial\nfor studying neural information processing. Here we used the Generative\nAdversarial Networks (GANs) framework to simulate the concerted activity of a\npopulation of neurons. We adapted the Wasserstein-GAN variant to facilitate the\ngeneration of unconstrained neural population activity patterns while still\nbenefiting from parameter sharing in the temporal domain. We demonstrate that\nour proposed GAN, which we termed Spike-GAN, generates spike trains that match\naccurately the first- and second-order statistics of datasets of tens of\nneurons and also approximates well their higher-order statistics. We applied\nSpike-GAN to a real dataset recorded from salamander retina and showed that it\nperforms as well as state-of-the-art approaches based on the maximum entropy\nand the dichotomized Gaussian frameworks. Importantly, Spike-GAN does not\nrequire to specify a priori the statistics to be matched by the model, and so\nconstitutes a more flexible method than these alternative approaches. Finally,\nwe show how to exploit a trained Spike-GAN to construct 'importance maps' to\ndetect the most relevant statistical structures present in a spike train.\nSpike-GAN provides a powerful, easy-to-use technique for generating realistic\nspiking neural activity and for describing the most relevant features of the\nlarge-scale neural population recordings studied in modern systems\nneuroscience.\n", "versions": [{"version": "v1", "created": "Thu, 1 Mar 2018 12:30:22 GMT"}, {"version": "v2", "created": "Tue, 17 Apr 2018 08:26:53 GMT"}], "update_date": "2018-04-18", "authors_parsed": [["Molano-Mazon", "Manuel", ""], ["Onken", "Arno", ""], ["Piasini", "Eugenio", ""], ["Panzeri", "Stefano", ""]]}, {"id": "1803.00643", "submitter": "Ran Darshan", "authors": "Ran Darshan, Carl van Vreeswijk and David Hansel", "title": "How strong are correlations in strongly recurrent neuronal networks?", "comments": null, "journal-ref": "Phys. Rev. X 8, 031072 (2018)", "doi": "10.1103/PhysRevX.8.031072", "report-no": null, "categories": "q-bio.NC cond-mat.dis-nn nlin.CD physics.bio-ph", "license": "http://creativecommons.org/licenses/by-sa/4.0/", "abstract": "  Cross-correlations in the activity in neural networks are commonly used to\ncharacterize their dynamical states and their anatomical and functional\norganizations. Yet, how these latter network features affect the spatiotemporal\nstructure of the correlations in recurrent networks is not fully understood.\nHere, we develop a general theory for the emergence of correlated neuronal\nactivity from the dynamics in strongly recurrent networks consisting of several\npopulations of binary neurons. We apply this theory to the case in which the\nconnectivity depends on the anatomical or functional distance between the\nneurons. We establish the architectural conditions under which the system\nsettles into a dynamical state where correlations are strong, highly robust and\nspatially modulated. We show that such strong correlations arise if the network\nexhibits an effective feedforward structure. We establish how this feedforward\nstructure determines the way correlations scale with the network size and the\ndegree of the connectivity. In networks lacking an effective feedforward\nstructure correlations are extremely small and only weakly depend on the number\nof connections per neuron. Our work shows how strong correlations can be\nconsistent with highly irregular activity in recurrent networks, two key\nfeatures of neuronal dynamics in the central nervous system.\n", "versions": [{"version": "v1", "created": "Thu, 1 Mar 2018 21:51:39 GMT"}], "update_date": "2018-09-26", "authors_parsed": [["Darshan", "Ran", ""], ["van Vreeswijk", "Carl", ""], ["Hansel", "David", ""]]}, {"id": "1803.00795", "submitter": "Kevin Berlemont", "authors": "Kevin Berlemont (LPS), Jean-Pierre Nadal (LPS, CAMS)", "title": "Perceptual decision making: Biases in post-error reaction times\n  explained by attractor network dynamics", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "physics.bio-ph cond-mat.dis-nn q-bio.NC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Perceptual decision making is the subject of many experimental and\ntheoretical studies. Whereas most modeling analysis are based on statistical\nprocesses of accumulation of evidence, less attention is being devoted to the\nmodeling with attractor network dynamics, even though they describe well\npsychophysical and neurophysiological data. In particular, very few works\nconfront attractor models predictions with data from continuous sequences of\ntrials. Recently however, a biophysical competitive attractor network model has\nbeen used to describe such sequences of decision trials, and has been shown to\nreproduce repetition biases observed in perceptual decision experiments. Here\nwe propose an extension of the reduced attractor network model of Wong and Wang\n(2006) to get more insights into such effects. We make explicit the conditions\nunder which such network can perform a succession of decisions, and show that\nthe model provides a mathematical framework for studying the effects of a trial\non the decision made on the next one. We study in details the reaction times\nproperties during a sequence of decision trials, and show that the model\nreproduces behavioral data, both qualitatively and quantitatively. In\nparticular, we find that the decision made on the current trial is biased\ntoward the one made on the previous trial. More remarkably, we show that, in\nthe absence of any feedback about the correctness of the decision, the network\nexhibits post-error slowing, a subtle effect in agreement with empirical data.\n", "versions": [{"version": "v1", "created": "Fri, 2 Mar 2018 10:29:08 GMT"}, {"version": "v2", "created": "Mon, 23 Apr 2018 12:31:01 GMT"}, {"version": "v3", "created": "Mon, 13 Aug 2018 17:17:18 GMT"}, {"version": "v4", "created": "Wed, 21 Nov 2018 15:43:41 GMT"}], "update_date": "2018-11-22", "authors_parsed": [["Berlemont", "Kevin", "", "LPS"], ["Nadal", "Jean-Pierre", "", "LPS, CAMS"]]}, {"id": "1803.00871", "submitter": "Paolo Olivero", "authors": "L. Guarina, C. Calorio, D. Gavello, E. Moreva, P. Traina, A. Battiato,\n  S. Ditalia Tchernij, J. Forneris, M. Gai, F. Picollo, P. Olivero, M.\n  Genovese, E. Carbone, A. Marcantoni, V. Carabelli", "title": "Nanodiamonds-induced effects on neuronal firing of mouse hippocampal\n  microcircuits", "comments": "34 pages, 9 figures", "journal-ref": "Scientific Reports 8, 2221 (2018)", "doi": "10.1038/s41598-018-20528-5", "report-no": null, "categories": "q-bio.NC physics.bio-ph", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Fluorescent nanodiamonds (FND) are carbon-based nanomaterials that can\nefficiently incorporate optically active photoluminescent centers such as the\nnitrogen-vacancy complex, thus making them promising candidates as optical\nbiolabels and drug-delivery agents. FNDs exhibit bright fluorescence without\nphotobleaching combined with high uptake rate and low cytotoxicity. Focusing on\nFNDs interference with neuronal function, here we examined their effect on\ncultured hippocampal neurons, monitoring the whole network development as well\nas the electrophysiological properties of single neurons. We observed that FNDs\ndrastically decreased the frequency of inhibitory (from 1.81 Hz to 0.86 Hz) and\nexcitatory (from 1.61 Hz to 0.68 Hz) miniature postsynaptic currents, and\nconsistently reduced action potential (AP) firing frequency (by 36%), as\nmeasured by microelectrode arrays. On the contrary, bursts synchronization was\npreserved, as well as the amplitude of spontaneous inhibitory and excitatory\nevents. Current-clamp recordings revealed that the ratio of neurons responding\nwith AP trains of high-frequency (fast-spiking) versus neurons responding with\ntrains of low-frequency (slow-spiking) was unaltered, suggesting that FNDs\nexerted a comparable action on neuronal subpopulations. At the single cell\nlevel, rapid onset of the somatic AP (\"kink\") was drastically reduced in\nFND-treated neurons, suggesting a reduced contribution of axonal and dendritic\ncomponents while preserving neuronal excitability.\n", "versions": [{"version": "v1", "created": "Fri, 2 Mar 2018 14:50:56 GMT"}], "update_date": "2018-03-05", "authors_parsed": [["Guarina", "L.", ""], ["Calorio", "C.", ""], ["Gavello", "D.", ""], ["Moreva", "E.", ""], ["Traina", "P.", ""], ["Battiato", "A.", ""], ["Tchernij", "S. Ditalia", ""], ["Forneris", "J.", ""], ["Gai", "M.", ""], ["Picollo", "F.", ""], ["Olivero", "P.", ""], ["Genovese", "M.", ""], ["Carbone", "E.", ""], ["Marcantoni", "A.", ""], ["Carabelli", "V.", ""]]}, {"id": "1803.00905", "submitter": "V\\'ictor J. L\\'opez-Madrona", "authors": "V\\'ictor J. L\\'opez-Madrona, Fernanda Matias, Claudio Mirasso,\n  Santiago Canals and Ernesto Pereda", "title": "Inferring correlations associated to causal interactions in brain\n  signals using autoregressive models", "comments": null, "journal-ref": "Lopez-Madrona, V.J., Matias, F.S., Mirasso, C.R. et al. Inferring\n  correlations associated to causal interactions in brain signals using\n  autoregressive models. Sci Rep 9, 17041 (2019) doi:10.1038/s41598-019-53453-2", "doi": "10.1038/s41598-019-53453-2", "report-no": null, "categories": "q-bio.QM q-bio.NC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The specific connectivity of a neuronal network is reflected in the dynamics\nof the signals recorded on its nodes. The analysis of how the activity in one\nnode predicts the behaviour of another gives the directionality in their\nrelationship. However, each node is composed of many different elements which\ndefine the properties of the links. For instance, excitatory and inhibitory\nneuronal subtypes determine the functionality of the connection. Classic\nindexes such as the Granger causality (GC) quantifies these interactions, but\nthey do not infer into the mechanism behind them. Here, we introduce an\nextension of the well-known GC that analyses the correlation associated to the\nspecific influence that a transmitter node has over the receiver. This way, the\nG-causal link has a positive or negative effect if the predicted activity\nfollows directly or inversely, respectively, the dynamics of the sender. The\nmethod is validated in a neuronal population model, testing the paradigm that\nexcitatory and inhibitory neurons have a differential effect in the\nconnectivity. Our approach correctly infers the positive or negative coupling\nproduced by different types of neurons. Our results suggest that the proposed\napproach provides additional information on the characterization of G-causal\nconnections, which is potentially relevant when it comes to understanding\ninteractions in the brain circuits.\n", "versions": [{"version": "v1", "created": "Fri, 2 Mar 2018 15:44:53 GMT"}, {"version": "v2", "created": "Fri, 9 Mar 2018 14:01:43 GMT"}, {"version": "v3", "created": "Wed, 14 Mar 2018 10:19:08 GMT"}, {"version": "v4", "created": "Wed, 20 Nov 2019 13:01:16 GMT"}], "update_date": "2019-11-21", "authors_parsed": [["L\u00f3pez-Madrona", "V\u00edctor J.", ""], ["Matias", "Fernanda", ""], ["Mirasso", "Claudio", ""], ["Canals", "Santiago", ""], ["Pereda", "Ernesto", ""]]}, {"id": "1803.00997", "submitter": "Nima Dehghani", "authors": "Nima Dehghani, Ralf D. Wimmer", "title": "A computational perspective of the role of Thalamus in cognition", "comments": "A theoretical perspective on thalamic computation and its role in\n  cognition", "journal-ref": null, "doi": null, "report-no": null, "categories": "q-bio.NC cs.NE", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Thalamus has traditionally been considered as only a relay source of cortical\ninputs, with hierarchically organized cortical circuits serially transforming\nthalamic signals to cognitively-relevant representations. Given the absence of\nlocal excitatory connections within the thalamus, the notion of thalamic\n`relay' seemed like a reasonable description over the last several decades.\nRecent advances in experimental approaches and theory provide a broader\nperspective on the role of the thalamus in cognitively-relevant cortical\ncomputations, and suggest that only a subset of thalamic circuit motifs fit the\nrelay description. Here, we discuss this perspective and highlight the\npotential role for the thalamus -- and specifically mediodorsal (MD) nucleus --\nin dynamic selection of cortical representations through a combination of\nintrinsic thalamic computations and output signals that change cortical network\nfunctional parameters. We suggest that through the contextual modulation of\ncortical computation, thalamus and cortex jointly optimize the information/cost\ntradeoff in an emergent fashion. We emphasize that coordinated experimental and\ntheoretical efforts will provide a path to understanding the role of the\nthalamus in cognition, along with an understanding to augment cognitive\ncapacity in health and disease.\n", "versions": [{"version": "v1", "created": "Fri, 2 Mar 2018 18:58:41 GMT"}, {"version": "v2", "created": "Mon, 16 Apr 2018 05:10:17 GMT"}, {"version": "v3", "created": "Wed, 17 Oct 2018 00:48:10 GMT"}], "update_date": "2018-10-18", "authors_parsed": [["Dehghani", "Nima", ""], ["Wimmer", "Ralf D.", ""]]}, {"id": "1803.01236", "submitter": "Marcos Trevisan Dr.", "authors": "Alan Taitz, Diego E Shalom, Marcos A Trevisan", "title": "Vocal effort modulates the motor planning of short speech structures", "comments": "17 pages, 3 figures", "journal-ref": "Phys. Rev. E 97, 052406 (2018)", "doi": "10.1103/PhysRevE.97.052406", "report-no": null, "categories": "q-bio.NC physics.bio-ph", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Speech requires programming the sequence of vocal gestures that produce the\nsounds of words. Here we explored the timing of this program by asking our\nparticipants to pronounce, as quickly as possible, a sequence of\nconsonant-consonant-vowel (CCV) structures appearing on screen. We measured the\ndelay between visual presentation and voice onset. In the case of plosive\nconsonants, produced by sharp and well defined movements of the vocal tract, we\nfound that delays are positively correlated with the duration of the transition\nbetween consonants. We then used a battery of statistical tests and\nmathematical vocal models to show that delays reflect the motor planning of\nCCVs and transitions are proxy indicators of the vocal effort needed to produce\nthem. These results support that the effort required to produce the sequence of\nmovements of a vocal gesture modulates the onset of the motor plan.\n", "versions": [{"version": "v1", "created": "Sat, 3 Mar 2018 20:45:33 GMT"}], "update_date": "2018-05-23", "authors_parsed": [["Taitz", "Alan", ""], ["Shalom", "Diego E", ""], ["Trevisan", "Marcos A", ""]]}, {"id": "1803.01325", "submitter": "Huilin Zhu", "authors": "Wei Cao, Wenxu Song, Xinge Li, Sixiao Zheng, Ge Zhang, Yanting Wu,\n  Sailing He, Huilin Zhu, Jiajia Chen", "title": "Could Interaction with Social Robots Facilitate Joint Attention of\n  Children with Autism Spectrum Disorder?", "comments": "First author: Wei Cao and Wenxu Song; Corresponding author: Huilin\n  Zhu(huilin.zhu@m.scnu.edu.cn)ans Jiajia Chen(jiajiac@kth.se)", "journal-ref": null, "doi": null, "report-no": null, "categories": "q-bio.NC cs.HC", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  This research addressed whether interactions with social robots could\nfacilitate joint attention of the autism spectrum disorder (ASD). Two\nconditions of initiators, namely 'Human' vs. 'Robot' were measured with 15\nchildren with ASD and 15 age-matched typically developing (TD) children. Apart\nfrom fixation and gaze transition, a new longest common subsequence (LCS)\napproach was proposed to analyze eye-movement traces. Results revealed that\nchildren with ASD showed deficits of joint attention. Compared to the human\nagent, robot facilitate less fixations towards the targets, but it attracted\nmore attention and allowed the children to show gaze transition and to follow\njoint attention logic. This results highlight both potential application of LCS\nanalysis on eye-tracking studies and of social robot to intervention.\n", "versions": [{"version": "v1", "created": "Sun, 4 Mar 2018 09:12:34 GMT"}], "update_date": "2018-03-06", "authors_parsed": [["Cao", "Wei", ""], ["Song", "Wenxu", ""], ["Li", "Xinge", ""], ["Zheng", "Sixiao", ""], ["Zhang", "Ge", ""], ["Wu", "Yanting", ""], ["He", "Sailing", ""], ["Zhu", "Huilin", ""], ["Chen", "Jiajia", ""]]}, {"id": "1803.02598", "submitter": "Adrien Wohrer", "authors": "Adrien Wohrer", "title": "Ising distribution as a latent variable model", "comments": "19 pages, 7 figures", "journal-ref": "Phys. Rev. E 99, 042147 (2019)", "doi": "10.1103/PhysRevE.99.042147", "report-no": null, "categories": "cond-mat.dis-nn q-bio.NC stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  During the past decades, the Ising distribution has attracted interest in\nmany applied disciplines, as the maximum entropy distribution associated to any\nset of correlated binary (`spin') variables with observed means and\ncovariances. However, numerically speaking, the Ising distribution is\nunpractical, so alternative models are often preferred to handle correlated\nbinary data. One popular alternative, especially in life sciences, is the Cox\ndistribution (or the closely related dichotomized Gaussian distribution and\nlog-normal Cox point process), where the spins are generated independently\nconditioned on the drawing of a latent variable with a multivariate normal\ndistribution. This article explores the conditions for a principled replacement\nof the Ising distribution by a Cox distribution. It shows that the Ising\ndistribution itself can be treated as a latent variable model, and it explores\nwhen this latent variable has a quasi-normal distribution. A variational\napproach to this question reveals a formal link with classic mean-field\nmethods, especially Opper and Winther's adaptive TAP approximation. This link\nis confirmed by weak coupling (Plefka) expansions of the different\napproximations and then by numerical tests. Overall, this study suggests that\nan Ising distribution can be replaced by a Cox distribution in practical\napplications, precisely when its parameters lie in the `mean-field domain'.\n", "versions": [{"version": "v1", "created": "Wed, 7 Mar 2018 11:20:29 GMT"}, {"version": "v2", "created": "Sun, 14 Oct 2018 21:37:30 GMT"}, {"version": "v3", "created": "Tue, 7 May 2019 15:40:24 GMT"}, {"version": "v4", "created": "Fri, 10 May 2019 08:47:05 GMT"}], "update_date": "2019-05-13", "authors_parsed": [["Wohrer", "Adrien", ""]]}, {"id": "1803.02626", "submitter": "PierGianLuca Porta Mana", "authors": "PierGianLuca Porta Mana, Claudia Bachmann, Abigail Morrison", "title": "Inferring health conditions from fMRI-graph data", "comments": "V1: 35 pages, 5 figures, 2 tables. V2: 36 pages, 5 figures, 2 tables;\n  partially rewritten all sections and added references. V3: Rewritten\n  introduction", "journal-ref": null, "doi": null, "report-no": null, "categories": "q-bio.QM q-bio.NC stat.AP", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Automated classification methods for disease diagnosis are currently in the\nlimelight, especially for imaging data. Classification does not fully meet a\nclinician's needs, however: in order to combine the results of multiple tests\nand decide on a course of treatment, a clinician needs the likelihood of a\ngiven health condition rather than binary classification yielded by such\nmethods. We illustrate how likelihoods can be derived step by step from first\nprinciples and approximations, and how they can be assessed and selected,\nillustrating our approach using fMRI data from a publicly available data set\ncontaining schizophrenic and healthy control subjects. We start from the basic\nassumption of partial exchangeability, and then the notion of sufficient\nstatistics and the \"method of translation\" (Edgeworth, 1898) combined with\nconjugate priors. This method can be used to construct a likelihood that can be\nused to compare different data-reduction algorithms. Despite the\nsimplifications and possibly unrealistic assumptions used to illustrate the\nmethod, we obtain classification results comparable to previous, more realistic\nstudies about schizophrenia, whilst yielding likelihoods that can naturally be\ncombined with the results of other diagnostic tests.\n", "versions": [{"version": "v1", "created": "Wed, 7 Mar 2018 12:58:46 GMT"}, {"version": "v2", "created": "Mon, 19 Mar 2018 14:42:08 GMT"}, {"version": "v3", "created": "Fri, 4 May 2018 10:48:02 GMT"}], "update_date": "2018-05-07", "authors_parsed": [["Mana", "PierGianLuca Porta", ""], ["Bachmann", "Claudia", ""], ["Morrison", "Abigail", ""]]}, {"id": "1803.02647", "submitter": "Mazza", "authors": "Corina Ciobotaru, Linard Hoessly, Christian Mazza, Xavier Richard", "title": "Mean field repulsive Kuramoto models: Phase locking and spatial signs", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "nlin.AO math.DS math.PR math.ST q-bio.NC stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The phenomenon of self-synchronization in populations of oscillatory units\nappears naturally in neurosciences. However, in some situations, the formation\nof a coherent state is damaging. In this article we study a repulsive\nmean-field Kuramoto model that describes the time evolution of n points on the\nunit circle, which are transformed into incoherent phase-locked states. It has\nbeen recently shown that such systems can be reduced to a three-dimensional\nsystem of ordinary differential equations, whose mathematical structure is\nstrongly related to hyperbolic geometry. The orbits of the Kuramoto dynamical\nsystem are then described by a ow of M\\\"obius transformations. We show this\nunderlying dynamic performs statistical inference by computing dynamically\nM-estimates of scatter matrices. We also describe the limiting phase-locked\nstates for random initial conditions using Tyler's transformation matrix.\nMoreover, we show the repulsive Kuramoto model performs dynamically not only\nrobust covariance matrix estimation, but also data processing: the initial\nconfiguration of the n points is transformed by the dynamic into a limiting\nphase-locked state that surprisingly equals the spatial signs from\nnonparametric statistics. That makes the sign empirical covariance matrix to\nequal 1 2 id2, the variance-covariance matrix of a random vector that is\nuniformly distributed on the unit circle.\n", "versions": [{"version": "v1", "created": "Wed, 7 Mar 2018 13:44:45 GMT"}], "update_date": "2018-03-08", "authors_parsed": [["Ciobotaru", "Corina", ""], ["Hoessly", "Linard", ""], ["Mazza", "Christian", ""], ["Richard", "Xavier", ""]]}, {"id": "1803.02765", "submitter": "Paul Yaworsky", "authors": "Paul Yaworsky", "title": "Realizing Intelligence", "comments": "8 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "q-bio.NC cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Order exists in the world. The intelligence process enables us to realize\nthat order, to some extent. We provide a high level description of intelligence\nusing simple definitions, basic building blocks, a conceptual framework and\ngeneral hierarchy. This perspective includes multiple levels of abstraction\noccurring in space and in time. The resulting model offers simple, useful ways\nto help realize the essence of intelligence.\n", "versions": [{"version": "v1", "created": "Wed, 7 Mar 2018 17:40:06 GMT"}], "update_date": "2018-03-09", "authors_parsed": [["Yaworsky", "Paul", ""]]}, {"id": "1803.02905", "submitter": "Ahmad Maqboul", "authors": "Ahmad Maqboul and Bakheet Elsadek", "title": "A Novel Model of Cancer-Induced Peripheral Neuropathy and the Role of\n  TRPA1 in Pain Transduction", "comments": "12 pages", "journal-ref": "Pain Research and Management, Volume 2017, Article ID 3517207", "doi": "10.1155/2017/3517207", "report-no": null, "categories": "q-bio.TO q-bio.NC", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Background. Models of cancer-induced neuropathy are designed by injecting\ncancer cells near the peripheral nerves. The interference of tissue-resident\nimmune cells does not allow a direct contact with nerve fibres which affects\nthe tumor microenvironment and the invasion process. Methods. Anaplastic\ntumor-1 (AT-1) cells were inoculated within the sciatic nerves (SNs) of male\nCopenhagen rats. Lumbar dorsal root ganglia (DRGs) and the SNs were collected\non days 3, 7, 14, and 21. SN tissues were examined for morphological changes\nand DRG tissues for immunofluorescence, electrophoretic tendency, and mRNA\nquantification. Hypersensitivities to cold, mechanical, and thermal stimuli\nwere determined. HC-030031, a selective TRPA1 antagonist, was used to treat\ncold allodynia. Results. Nociception thresholds were identified on day 6.\nImmunofluorescent micrographs showed overexpression of TRPA1 on days 7 and 14\nand of CGRP on day 14 until day 21. Both TRPA1 and CGRP were coexpressed on the\nsame cells. Immunoblots exhibited an increase in TRPA1 expression on day 14.\nTRPA1 mRNA underwent an increase on day 7 (normalized to 18S). Injection of\nHC-030031 transiently reversed the cold allodynia. Conclusion. A novel and a\npromising model of cancer-induced neuropathy was established, and the role of\nTRPA1 and CGRP in pain transduction was examined.\n", "versions": [{"version": "v1", "created": "Wed, 7 Mar 2018 22:43:41 GMT"}], "update_date": "2018-03-09", "authors_parsed": [["Maqboul", "Ahmad", ""], ["Elsadek", "Bakheet", ""]]}, {"id": "1803.02917", "submitter": "Lawrence Ward", "authors": "Priscilla E. Greenwood and Lawrence M. Ward", "title": "Rapidly forming, slowly evolving, spatial patterns from quasi-cycle\n  Mexican Hat coupling", "comments": "24 pages, 7 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "q-bio.NC nlin.AO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A lattice-indexed family of stochastic processes has quasi-cycle oscillations\nif its otherwise-damped oscillations are sustained by noise. Such a family\nperforms the reaction part of a discrete stochastic reaction-diffusion system\nwhen we insert a local Mexican Hat-type, difference of Gaussians, coupling on a\none-dimensional and on a two-dimensional lattice. Quasi-cycles are a proposed\nmechanism for the production of neural oscillations, and Mexican Hat coupling\nis ubiquitous in the brain. Thus this combination might provide insight into\nthe function of neural oscillations in the brain. Importantly, we study this\nsystem only in the transient case, on time intervals before saturation occurs.\nIn one dimension, for weak coupling, we find that the phases of the coupled\nquasi-cycles synchronize (establish a relatively constant relationship, or\nphase lock) rapidly at coupling strengths lower than those required to produce\nspatial patterns of their amplitudes. In two dimensions the amplitude patterns\nform more quickly, but there remain parameter regimes in which phase\nsynchronization patterns form without being accompanied by clear amplitude\npatterns. At higher coupling strengths we find patterns both of phase\nsynchronization and of amplitude (resembling Turing patterns) corresponding to\nthe patterns of phase synchronization. Specific properties of these patterns\nare controlled by the parameters of the reaction and of the Mexican Hat\ncoupling.\n", "versions": [{"version": "v1", "created": "Wed, 7 Mar 2018 23:43:33 GMT"}, {"version": "v2", "created": "Mon, 23 Sep 2019 22:05:49 GMT"}], "update_date": "2019-09-25", "authors_parsed": [["Greenwood", "Priscilla E.", ""], ["Ward", "Lawrence M.", ""]]}, {"id": "1803.03204", "submitter": "Angelique Morvant", "authors": "Angelique Morvant", "title": "Strengthening Relationships between Neural Ideals and Receptive Fields", "comments": "16 pages, 3 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.AC q-bio.NC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Neural codes are collections of binary vectors that represent the firing\npatterns of neurons. The information given by a neural code $C$ can be\nrepresented by its neural ideal $J_C$. In turn, the polynomials in $J_C$ can be\nused to determine the relationships among the receptive fields of the neurons.\nIn a paper by Curto et al., three such relationships, known as the Type 1-3\nrelations, were linked to the neural ideal by three if-and-only-if statements.\nLater, Garcia et al. discovered the Type 4-6 relations. These new relations\ndiffered from the first three in that they were related to $J_C$ by one-way\nimplications. In this paper, we first show that the converses of these new\nimplications are false at the level of both the neural ideal $J_C$ and the\nlarger ideal $I(C)$ of a code. We then present modified statements of these\nrelations that, like the first three, can be related by if-and-only-if\nstatements to both $J_C$ and $I(C)$. Using the modified relations, we uncover a\nnew relationship involving $J_C$, $I(C)$, and the Type 1-6 relations.\n", "versions": [{"version": "v1", "created": "Thu, 8 Mar 2018 17:09:15 GMT"}], "update_date": "2018-03-09", "authors_parsed": [["Morvant", "Angelique", ""]]}, {"id": "1803.03304", "submitter": "Ryan Pyle", "authors": "Ryan Pyle, Robert Rosenbaum", "title": "A model of reward-modulated motor learning with parallelcortical and\n  basal ganglia pathways", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "q-bio.NC cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Many recent studies of the motor system are divided into two distinct\napproaches: Those that investigate how motor responses are encoded in cortical\nneurons' firing rate dynamics and those that study the learning rules by which\nmammals and songbirds develop reliable motor responses. Computationally, the\nfirst approach is encapsulated by reservoir computing models, which can learn\nintricate motor tasks and produce internal dynamics strikingly similar to those\nof motor cortical neurons, but rely on biologically unrealistic learning rules.\nThe more realistic learning rules developed by the second approach are often\nderived for simplified, discrete tasks in contrast to the intricate dynamics\nthat characterize real motor responses. We bridge these two approaches to\ndevelop a biologically realistic learning rule for reservoir computing. Our\nalgorithm learns simulated motor tasks on which previous reservoir computing\nalgorithms fail, and reproduces experimental findings including those that\nrelate motor learning to Parkinson's disease and its treatment.\n", "versions": [{"version": "v1", "created": "Thu, 8 Mar 2018 21:01:02 GMT"}, {"version": "v2", "created": "Fri, 1 Mar 2019 23:25:16 GMT"}], "update_date": "2019-03-05", "authors_parsed": [["Pyle", "Ryan", ""], ["Rosenbaum", "Robert", ""]]}, {"id": "1803.03692", "submitter": "Zhinus Marzi", "authors": "Zhinus Marzi, Joao Hespanha and Upamanyu Madhow", "title": "On the information in spike timing: neural codes derived from\n  polychronous groups", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "q-bio.NC cs.NE stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  There is growing evidence regarding the importance of spike timing in neural\ninformation processing, with even a small number of spikes carrying\ninformation, but computational models lag significantly behind those for rate\ncoding. Experimental evidence on neuronal behavior is consistent with the\ndynamical and state dependent behavior provided by recurrent connections. This\nmotivates the minimalistic abstraction investigated in this paper, aimed at\nproviding insight into information encoding in spike timing via recurrent\nconnections. We employ information-theoretic techniques for a simple reservoir\nmodel which encodes input spatiotemporal patterns into a sparse neural code,\ntranslating the polychronous groups introduced by Izhikevich into codewords on\nwhich we can perform standard vector operations. We show that the distance\nproperties of the code are similar to those for (optimal) random codes. In\nparticular, the code meets benchmarks associated with both linear\nclassification and capacity, with the latter scaling exponentially with\nreservoir size.\n", "versions": [{"version": "v1", "created": "Fri, 9 Mar 2018 20:53:31 GMT"}], "update_date": "2018-03-13", "authors_parsed": [["Marzi", "Zhinus", ""], ["Hespanha", "Joao", ""], ["Madhow", "Upamanyu", ""]]}, {"id": "1803.03742", "submitter": "Nir Lahav", "authors": "Nir Lahav, Baruch Ksherim, Eti Ben-Simon, Adi Maron-Katz, Reuven Cohen\n  and Shlomo Havlin", "title": "K-shell decomposition reveals hierarchical cortical organization of the\n  human brain", "comments": "New Journal of Physics, Volume 18, August 2016", "journal-ref": null, "doi": "10.1088/1367-2630/18/8/083013", "report-no": null, "categories": "q-bio.NC", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  In recent years numerous attempts to understand the human brain were\nundertaken from a network point of view. A network framework takes into account\nthe relationships between the different parts of the system and enables to\nexamine how global and complex functions might emerge from network topology.\nPrevious work revealed that the human brain features 'small world'\ncharacteristics and that cortical hubs tend to interconnect among themselves.\nHowever, in order to fully understand the topological structure of hubs one\nneeds to go beyond the properties of a specific hub and examine the various\nstructural layers of the network. To address this topic further, we applied an\nanalysis known in statistical physics and network theory as k-shell\ndecomposition analysis. The analysis was applied on a human cortical network,\nderived from MRI\\DSI data of six participants. Such analysis enables us to\nportray a detailed account of cortical connectivity focusing on different\nneighborhoods of interconnected layers across the cortex. Our findings reveal\nthat the human cortex is highly connected and efficient, and unlike the\ninternet network contains no isolated nodes. The cortical network is comprised\nof a nucleus alongside shells of increasing connectivity that formed one\nconnected giant component. All these components were further categorized into\nthree hierarchies in accordance with their connectivity profile, with each\nhierarchy reflecting different functional roles. Such a model may explain an\nefficient flow of information from the lowest hierarchy to the highest one,\nwith each step enabling increased data integration. At the top, the highest\nhierarchy (the nucleus) serves as a global interconnected collective and\ndemonstrates high correlation with consciousness related regions, suggesting\nthat the nucleus might serve as a platform for consciousness to emerge.\n", "versions": [{"version": "v1", "created": "Sat, 10 Mar 2018 02:19:09 GMT"}], "update_date": "2018-03-13", "authors_parsed": [["Lahav", "Nir", ""], ["Ksherim", "Baruch", ""], ["Ben-Simon", "Eti", ""], ["Maron-Katz", "Adi", ""], ["Cohen", "Reuven", ""], ["Havlin", "Shlomo", ""]]}, {"id": "1803.03940", "submitter": "David Holcman", "authors": "J. Cartailler and D. Holcman", "title": "Electrical transient laws in neuronal microdomains based on\n  electro-diffusion", "comments": "3 figures;submitted to PRL", "journal-ref": null, "doi": "10.1039/C8CP02593B", "report-no": null, "categories": "q-bio.NC math.AP physics.bio-ph", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The current-voltage (I-V) conversion characterizes the physiology of cellular\nmicrodomains and reflects cellular communication, excitability, and electrical\ntransduction. Yet deriving such I-V laws remains a major challenge in most\ncellular microdomains due to their small sizes and the difficulty of accessing\nvoltage with a high nanometer precision. We present here novel analytical\nrelations derived for different numbers of ionic species inside a neuronal\nmicro/nano-domains, such as dendritic spines. When a steady-state current is\ninjected, we find a large deviation from the classical Ohm's law, showing that\nthe spine neck resistance is insuficent to characterize electrical properties.\nFor a constricted spine neck, modeled by a hyperboloid, we obtain a new I-V law\nthat illustrates the consequences of narrow passages on electrical conduction.\nFinally, during a fast current transient, the local voltage is modulated by the\ndistance between activated voltage-gated channels. To conclude,\nelectro-diffusion laws can now be used to interpret voltage distribution in\nneuronal microdomains.\n", "versions": [{"version": "v1", "created": "Sun, 11 Mar 2018 10:53:26 GMT"}], "update_date": "2018-08-29", "authors_parsed": [["Cartailler", "J.", ""], ["Holcman", "D.", ""]]}, {"id": "1803.04085", "submitter": "Andrew Leifer", "authors": "Mochi Liu, Anuj K Sharma, Joshua W Shaevitz, Andrew M Leifer", "title": "Temporal processing and context dependency in C. elegans\n  mechanosensation", "comments": "40 pages, 8 main figures, 19 supplementary figures", "journal-ref": "eLife 2018;7:e36419", "doi": "10.7554/eLife.36419", "report-no": null, "categories": "q-bio.NC physics.bio-ph", "license": "http://creativecommons.org/licenses/by-sa/4.0/", "abstract": "  A quantitative understanding of how sensory signals are transformed into\nmotor outputs places useful constraints on brain function and helps reveal the\nbrain's underlying computations. We investigate how the nematode C. elegans\nresponds to time-varying mechanosensory signals using a high-throughput\noptogenetic assay and automated behavior quantification. In the prevailing\npicture of the touch circuit, the animal's behavior is determined by which\nneurons are stimulated and by the stimulus amplitude. In contrast, we find that\nthe behavioral response is tuned to temporal properties of mechanosensory\nsignals, like its integral and derivative, that extend over many seconds.\nMechanosensory signals, even in the same neurons, can be tailored to elicit\ndifferent behavioral responses. Moreover, we find that the animal's response\nalso depends on its behavioral context. Most dramatically, the animal ignores\nall tested mechanosensory stimuli during turns. Finally, we present a\nlinear-nonlinear model that predicts the animal's behavioral response to\nstimulus.\n", "versions": [{"version": "v1", "created": "Mon, 12 Mar 2018 01:42:26 GMT"}, {"version": "v2", "created": "Tue, 20 Mar 2018 16:30:34 GMT"}], "update_date": "2020-08-18", "authors_parsed": [["Liu", "Mochi", ""], ["Sharma", "Anuj K", ""], ["Shaevitz", "Joshua W", ""], ["Leifer", "Andrew M", ""]]}, {"id": "1803.04236", "submitter": "Amirhossein Jabalameli", "authors": "Amirhossein Jabalameli, Aman Behal", "title": "System Identification of a Multi-timescale Adaptive Threshold Neuronal\n  Model", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "q-bio.NC cs.SY", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, the parameter estimation problem for a multi-timescale\nadaptive threshold (MAT) neuronal model is investigated. By manipulating the\nsystem dynamics, which comprise of a non-resetting leaky integrator coupled\nwith an adaptive threshold, the threshold voltage can be obtained as a\nrealizable model that is linear in the unknown parameters. This linearly\nparametrized realizable model is then utilized inside a prediction error based\nframework to identify the threshold parameters with the purpose of predicting\nsingle neuron precise firing times. The iterative linear least squares\nestimation scheme is evaluated using both synthetic data obtained from an exact\nmodel as well as experimental data obtained from in vitro rat somatosensory\ncortical neurons. Results show the ability of this approach to fit the MAT\nmodel to different types of fluctuating reference data. The performance of the\nproposed approach is seen to be superior when comparing with existing\nidentification approaches used by the neuronal community.\n", "versions": [{"version": "v1", "created": "Fri, 23 Feb 2018 23:06:31 GMT"}], "update_date": "2018-03-13", "authors_parsed": [["Jabalameli", "Amirhossein", ""], ["Behal", "Aman", ""]]}, {"id": "1803.04363", "submitter": "Jingyi Zheng", "authors": "Jingyi Zheng, Fushing Hsieh", "title": "Information of Epileptic Mechanism and its Systemic Change-points in a\n  Zebrafish's Brain-wide Calcium Imaging Video Data", "comments": "8 Pages, 11 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "q-bio.NC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The epileptic mechanism is postulated as that an animal's neurons gradually\ndiminish their inhibition function coupled with enhanced excitation when an\nepileptic event is approaching. Calcium imaging technique is designed to\ndirectly record brain-wide neurons activity in order to discover the underlying\nepileptic mechanism. In this paper, using one brain-wide calcium imaging video\nof Zebrafish, we compute dynamic pattern information of the epileptic\nmechanism, and devise three graphical displays to show the visible functional\naspect of epileptic mechanism over five inter-ictal periods. The foundation of\nour data-driven computations for such dynamic patterns relies on one universal\nphenomenon discovered across 696 informative pixels. This universality is that\neach pixel's progressive 5-percentile process oscillates in an irregular\nfashion at first, but, after the middle point of inter-ictal period, the\noscillation is replaced by a steady increasing trend. Such dynamic patterns are\ncollectively transformed into a visible systemic change-point as an early\nwarning signal (EWS) of an incoming epileptic event. We conclude through the\ngraphic displays that pattern information extracted from the calcium imaging\nvideo realistically reveals the Zebrafish's authentic epileptic mechanism.\n", "versions": [{"version": "v1", "created": "Wed, 14 Feb 2018 01:27:03 GMT"}], "update_date": "2018-03-13", "authors_parsed": [["Zheng", "Jingyi", ""], ["Hsieh", "Fushing", ""]]}, {"id": "1803.04364", "submitter": "Sheraz Khan", "authors": "Sheraz Khan, Javeria Hashmi, Fahimeh Mamashli, Konstantinos Michmizos,\n  Manfred Kitzbichler, Hari Bharadwaj, Yousra Bekhti, Santosh Ganesan, Keri A\n  Garel, Susan Whitfield-Gabrieli, Randy Gollub, Jian Kong, Lucia M Vaina,\n  Kunjan Rana, Steven Stufflebeam, Matti Hamalainen, and Tal Kenet", "title": "Maturation Trajectories of Cortical Resting-State Networks Depend on the\n  Mediating Frequency Band", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "q-bio.NC cs.DM stat.ML", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  The functional significance of resting state networks and their abnormal\nmanifestations in psychiatric disorders are firmly established, as is the\nimportance of the cortical rhythms in mediating these networks. Resting state\nnetworks are known to undergo substantial reorganization from childhood to\nadulthood, but whether distinct cortical rhythms, which are generated by\nseparable neural mechanisms and are often manifested abnormally in psychiatric\nconditions, mediate maturation differentially, remains unknown. Using\nmagnetoencephalography (MEG) to map frequency band specific maturation of\nresting state networks from age 7 to 29 in 162 participants (31 independent),\nwe found significant changes with age in networks mediated by the beta\n(13-30Hz) and gamma (31-80Hz) bands. More specifically, gamma band mediated\nnetworks followed an expected asymptotic trajectory, but beta band mediated\nnetworks followed a linear trajectory. Network integration increased with age\nin gamma band mediated networks, while local segregation increased with age in\nbeta band mediated networks. Spatially, the hubs that changed in importance\nwith age in the beta band mediated networks had relatively little overlap with\nthose that showed the greatest changes in the gamma band mediated networks.\nThese findings are relevant for our understanding of the neural mechanisms of\ncortical maturation, in both typical and atypical development.\n", "versions": [{"version": "v1", "created": "Tue, 13 Feb 2018 01:04:40 GMT"}], "update_date": "2018-03-13", "authors_parsed": [["Khan", "Sheraz", ""], ["Hashmi", "Javeria", ""], ["Mamashli", "Fahimeh", ""], ["Michmizos", "Konstantinos", ""], ["Kitzbichler", "Manfred", ""], ["Bharadwaj", "Hari", ""], ["Bekhti", "Yousra", ""], ["Ganesan", "Santosh", ""], ["Garel", "Keri A", ""], ["Whitfield-Gabrieli", "Susan", ""], ["Gollub", "Randy", ""], ["Kong", "Jian", ""], ["Vaina", "Lucia M", ""], ["Rana", "Kunjan", ""], ["Stufflebeam", "Steven", ""], ["Hamalainen", "Matti", ""], ["Kenet", "Tal", ""]]}, {"id": "1803.04566", "submitter": "Nicholas Waytowich", "authors": "Nicholas R. Waytowich, Vernon Lawhern, Javier O. Garcia, Jennifer\n  Cummings, Josef Faller, Paul Sajda, Jean M. Vettel", "title": "Compact Convolutional Neural Networks for Classification of Asynchronous\n  Steady-state Visual Evoked Potentials", "comments": "Accepted for publication at the Journal of Neural Engineering", "journal-ref": null, "doi": "10.1088/1741-2552/aae5d8", "report-no": null, "categories": "cs.LG q-bio.NC stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Steady-State Visual Evoked Potentials (SSVEPs) are neural oscillations from\nthe parietal and occipital regions of the brain that are evoked from flickering\nvisual stimuli. SSVEPs are robust signals measurable in the\nelectroencephalogram (EEG) and are commonly used in brain-computer interfaces\n(BCIs). However, methods for high-accuracy decoding of SSVEPs usually require\nhand-crafted approaches that leverage domain-specific knowledge of the stimulus\nsignals, such as specific temporal frequencies in the visual stimuli and their\nrelative spatial arrangement. When this knowledge is unavailable, such as when\nSSVEP signals are acquired asynchronously, such approaches tend to fail. In\nthis paper, we show how a compact convolutional neural network (Compact-CNN),\nwhich only requires raw EEG signals for automatic feature extraction, can be\nused to decode signals from a 12-class SSVEP dataset without the need for any\ndomain-specific knowledge or calibration data. We report across subject mean\naccuracy of approximately 80% (chance being 8.3%) and show this is\nsubstantially better than current state-of-the-art hand-crafted approaches\nusing canonical correlation analysis (CCA) and Combined-CCA. Furthermore, we\nanalyze our Compact-CNN to examine the underlying feature representation,\ndiscovering that the deep learner extracts additional phase and amplitude\nrelated features associated with the structure of the dataset. We discuss how\nour Compact-CNN shows promise for BCI applications that allow users to freely\ngaze/attend to any stimulus at any time (e.g., asynchronous BCI) as well as\nprovides a method for analyzing SSVEP signals in a way that might augment our\nunderstanding about the basic processing in the visual cortex.\n", "versions": [{"version": "v1", "created": "Mon, 12 Mar 2018 23:03:44 GMT"}, {"version": "v2", "created": "Tue, 9 Oct 2018 16:53:26 GMT"}], "update_date": "2018-10-10", "authors_parsed": [["Waytowich", "Nicholas R.", ""], ["Lawhern", "Vernon", ""], ["Garcia", "Javier O.", ""], ["Cummings", "Jennifer", ""], ["Faller", "Josef", ""], ["Sajda", "Paul", ""], ["Vettel", "Jean M.", ""]]}, {"id": "1803.04738", "submitter": "Yu Terada", "authors": "Yu Terada, Tomoyuki Obuchi, Takuya Isomura, Yoshiyuki Kabashima", "title": "Inferring neuronal couplings from spiking data using a systematic\n  procedure with a statistical criterion", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cond-mat.dis-nn cond-mat.stat-mech q-bio.NC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recent remarkable advances in the experimental techniques have provided a\nbackground for inferring neuronal couplings from point process data that\nincludes a great number of neurons. Here, we propose a systematic procedure for\npre- and post-processing generic point process data in an objective manner, to\nhandle data in the framework of a binary simple statistical model, the Ising or\ngeneralized McCulloch--Pitts model. The procedure involves two steps: (1)\ndetermining time-bin size for transforming the point-process data into\ndiscrete-time binary data and (2) screening relevant couplings from the\nestimated couplings. For the first step, we decide the optimal time-bin size by\nintroducing the null hypothesis that all neurons would fire independently, then\nchoosing a time-bin size so that the null hypothesis is rejected with the most\nstrict criterion. The likelihood associated with the null hypothesis is\nanalytically evaluated and used for the rejection process. For the second\npost-processing step, after a certain estimator of coupling is obtained based\non the pre-processed dataset, the estimate is compared with many other\nestimates derived from datasets obtained by randomizing the original dataset in\nthe time direction. We accept the original estimate as relevant only if its\nabsolute value is sufficiently larger than them of randomized datasets. These\nmanipulations suppress false positive couplings induced by statistical noise.\nWe apply this inference procedure to spiking data from synthetic and in vitro\nneuronal networks. The results show that the proposed procedure identifies the\npresence/absence of synaptic couplings fairly well including their signs, for\nthe synthetic and experimental data. In particular, the results support that we\ncan infer the physical connections of underlying systems in favorable\nsituations, even when using the simple statistical model.\n", "versions": [{"version": "v1", "created": "Tue, 13 Mar 2018 11:44:09 GMT"}, {"version": "v2", "created": "Tue, 7 Jul 2020 10:16:18 GMT"}], "update_date": "2020-07-08", "authors_parsed": [["Terada", "Yu", ""], ["Obuchi", "Tomoyuki", ""], ["Isomura", "Takuya", ""], ["Kabashima", "Yoshiyuki", ""]]}, {"id": "1803.05048", "submitter": "Jeremi K. Ochab", "authors": "Jeremi K. Ochab and Wojciech Tarnowski and and Maciej A. Nowak and\n  Dante R. Chialvo", "title": "On the pros and cons of using temporal derivatives to assess brain\n  functional connectivity", "comments": "12 pages, 7 figures", "journal-ref": "NeuroImage 184 (2019), pp. 577-585", "doi": "10.1016/j.neuroimage.2018.09.063", "report-no": null, "categories": "q-bio.NC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The study of correlations between brain regions is an important chapter of\nthe analysis of large-scale brain spatiotemporal dynamics. In particular, novel\nmethods suited to extract dynamic changes in mutual correlations are needed.\nHere we scrutinize a recently reported metric dubbed \"Multiplication of\nTemporal Derivatives\" (MTD) which is based on the temporal derivative of each\ntime series. The formal comparison of the MTD formula with the Pearson\ncorrelation of the derivatives reveals only minor differences, which we find\nnegligible in practice. A comparison with the sliding window Pearson\ncorrelation of the raw time series in several stationary and non-stationary\nset-ups, including a realistic stationary network detection, reveals lower\nsensitivity of derivatives to low frequency drifts and to autocorrelations but\nalso lower signal-to-noise ratio. It does not indicate any evident mathematical\nadvantages of the proposed metric over commonly used correlation methods.\n", "versions": [{"version": "v1", "created": "Tue, 13 Mar 2018 21:16:41 GMT"}, {"version": "v2", "created": "Thu, 7 Jun 2018 13:05:04 GMT"}], "update_date": "2018-10-11", "authors_parsed": [["Ochab", "Jeremi K.", ""], ["Tarnowski", "Wojciech", ""], ["Nowak", "and Maciej A.", ""], ["Chialvo", "Dante R.", ""]]}, {"id": "1803.05109", "submitter": "Tao Liu", "authors": "Tao Liu, Lei Jiang, Yier Jin, Gang Quan, Wujie Wen", "title": "PT-Spike: A Precise-Time-Dependent Single Spike Neuromorphic\n  Architecture with Efficient Supervised Learning", "comments": "23rd Asia and South Pacific Design Automation Conference (ASP-DAC\n  2018)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NE q-bio.NC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  One of the most exciting advancements in AI over the last decade is the wide\nadoption of ANNs, such as DNN and CNN, in many real-world applications.\nHowever, the underlying massive amounts of computation and storage requirement\ngreatly challenge their applicability in resource-limited platforms like the\ndrone, mobile phone, and IoT devices etc. The third generation of neural\nnetwork model--Spiking Neural Network (SNN), inspired by the working mechanism\nand efficiency of human brain, has emerged as a promising solution for\nachieving more impressive computing and power efficiency within light-weighted\ndevices (e.g. single chip). However, the relevant research activities have been\nnarrowly carried out on conventional rate-based spiking system designs for\nfulfilling the practical cognitive tasks, underestimating SNN's energy\nefficiency, throughput, and system flexibility. Although the time-based SNN can\nbe more attractive conceptually, its potentials are not unleashed in realistic\napplications due to lack of efficient coding and practical learning schemes. In\nthis work, a Precise-Time-Dependent Single Spike Neuromorphic Architecture,\nnamely \"PT-Spike\", is developed to bridge this gap. Three constituent\nhardware-favorable techniques: precise single-spike temporal encoding,\nefficient supervised temporal learning, and fast asymmetric decoding are\nproposed accordingly to boost the energy efficiency and data processing\ncapability of the time-based SNN at a more compact neural network model size\nwhen executing real cognitive tasks. Simulation results show that \"PT-Spike\"\ndemonstrates significant improvements in network size, processing efficiency\nand power consumption with marginal classification accuracy degradation when\ncompared with the rate-based SNN and ANN under the similar network\nconfiguration.\n", "versions": [{"version": "v1", "created": "Wed, 14 Mar 2018 02:37:42 GMT"}], "update_date": "2018-03-15", "authors_parsed": [["Liu", "Tao", ""], ["Jiang", "Lei", ""], ["Jin", "Yier", ""], ["Quan", "Gang", ""], ["Wen", "Wujie", ""]]}, {"id": "1803.05117", "submitter": "Tao Liu", "authors": "Tao Liu, Zihao Liu, Fuhong Lin, Yier Jin, Gang Quan, Wujie Wen", "title": "MT-Spike: A Multilayer Time-based Spiking Neuromorphic Architecture with\n  Temporal Error Backpropagation", "comments": "36th International Conference On Computer Aided Design (ICCAD 2017)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NE q-bio.NC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Modern deep learning enabled artificial neural networks, such as Deep Neural\nNetwork (DNN) and Convolutional Neural Network (CNN), have achieved a series of\nbreaking records on a broad spectrum of recognition applications. However, the\nenormous computation and storage requirements associated with such deep and\ncomplex neural network models greatly challenge their implementations on\nresource-limited platforms. Time-based spiking neural network has recently\nemerged as a promising solution in Neuromorphic Computing System designs for\nachieving remarkable computing and power efficiency within a single chip.\nHowever, the relevant research activities have been narrowly concentrated on\nthe biological plausibility and theoretical learning approaches, causing\ninefficient neural processing and impracticable multilayer extension thus\nsignificantly limitations on speed and accuracy when handling the realistic\ncognitive tasks. In this work, a practical multilayer time-based spiking\nneuromorphic architecture, namely \"MT-Spike\", is developed to fill this gap.\nWith the proposed practical time-coding scheme, average delay response model,\ntemporal error backpropagation algorithm, and heuristic loss function,\n\"MT-Spike\" achieves more efficient neural processing through flexible neural\nmodel size reduction while offering very competitive classification accuracy\nfor realistic recognition tasks. Simulation results well validated that the\nalgorithmic power of deep multi-layer learning can be seamlessly merged with\nthe efficiency of time-based spiking neuromorphic architecture, demonstrating\ngreat potentials of \"MT-Spike\" in resource and power constrained embedded\nplatforms.\n", "versions": [{"version": "v1", "created": "Wed, 14 Mar 2018 03:01:19 GMT"}], "update_date": "2018-03-15", "authors_parsed": [["Liu", "Tao", ""], ["Liu", "Zihao", ""], ["Lin", "Fuhong", ""], ["Jin", "Yier", ""], ["Quan", "Gang", ""], ["Wen", "Wujie", ""]]}, {"id": "1803.05239", "submitter": "Gemma Rosell-Tarrag\\'o", "authors": "Gemma Rosell-Tarrag\\'o, Emanuele Cozzo and Albert D\\'iaz-Guilera", "title": "A complex network framework to model cognition: unveiling correlation\n  structures from connectivity", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "physics.soc-ph cond-mat.dis-nn q-bio.NC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Several approaches to cognition and intelligence research rely on\nstatistics-based models testing, namely factor analysis. In the present work we\nexploit the emerging dynamical systems perspective putting the focus on the\nrole of the network topology underlying the relationships between cognitive\nprocesses. We go through a couple of models of distinct cognitive phenomena and\nyet find the conditions for them to be mathematically equivalent. We find a\nnon-trivial attractor of the system that corresponds to the exact definition of\na well-known network centrality and hence stress the interplay between the\ndynamics and the underlying network connectivity, showing that both of the two\nare relevant. The connectivity structure between cognitive processes is not\nknown but yet it is not any. Regardless of the network considered, it is always\npossible to recover a positive manifold of correlations. However, we show that\ndifferent network topologies lead to different plausible statistical models\nconcerning correlations structure, ranging from one to multiple factors models\nand richer correlation structures.\n", "versions": [{"version": "v1", "created": "Wed, 14 Mar 2018 12:33:37 GMT"}], "update_date": "2018-03-15", "authors_parsed": [["Rosell-Tarrag\u00f3", "Gemma", ""], ["Cozzo", "Emanuele", ""], ["D\u00edaz-Guilera", "Albert", ""]]}, {"id": "1803.05369", "submitter": "Bulcs\\'u S\\'andor", "authors": "Bulcs\\'u S\\'andor and Claudius Gros", "title": "Complex activity patterns generated by short-term synaptic plasticity", "comments": "6 pages, 4 figures, ESANN conference", "journal-ref": "ESANN 2017 proceedings, Bruges (Belgium), 26-28 April 2017,\n  Available from: Available from\n  https://www.elen.ucl.ac.be/Proceedings/esann/esannpdf/es2017-47.pdf", "doi": null, "report-no": null, "categories": "q-bio.NC cond-mat.dis-nn nlin.CD", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Short-term synaptic plasticity (STSP) affects the efficiency of synaptic\ntransmission for persistent presynaptic activities. We consider attractor\nneural networks, for which the attractors are given, in the absence of STSP, by\ncell assemblies of excitatory cliques. We show that STSP may transform these\nattracting states into attractor relics, inducing ongoing transient-state\ndynamics in terms of sequences of transiently activated cell assemblies, the\nformer attractors. Subsequent cell assemblies may be both disjoint or partially\noverlapping. It may hence be possible to use the resulting dynamics for the\ngeneration of motor control sequences.\n", "versions": [{"version": "v1", "created": "Wed, 14 Mar 2018 15:47:44 GMT"}], "update_date": "2019-04-09", "authors_parsed": [["S\u00e1ndor", "Bulcs\u00fa", ""], ["Gros", "Claudius", ""]]}, {"id": "1803.05783", "submitter": "Noemi Montobbio", "authors": "Noemi Montobbio, Giovanna Citti, Alessandro Sarti", "title": "From receptive profiles to a metric model of V1", "comments": "25 pages, 18 figures. Added acknowledgements", "journal-ref": null, "doi": "10.1007/s10827-019-00716-6", "report-no": null, "categories": "math.MG q-bio.NC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this work we show how to construct connectivity kernels induced by the\nreceptive profiles of simple cells of the primary visual cortex (V1). These\nkernels are directly defined by the shape of such profiles: this provides a\nmetric model for the functional architecture of V1, whose global geometry is\ndetermined by the reciprocal interactions between local elements. Our\nconstruction adapts to any bank of filters chosen to represent a set of\nreceptive profiles, since it does not require any structure on the\nparameterization of the family. The connectivity kernel that we define carries\na geometrical structure consistent with the well-known properties of long-range\nhorizontal connections in V1, and it is compatible with the perceptual rules\nsynthesized by the concept of association field. These characteristics are\nstill present when the kernel is constructed from a bank of filters arising\nfrom an unsupervised learning algorithm.\n", "versions": [{"version": "v1", "created": "Thu, 15 Mar 2018 14:42:50 GMT"}, {"version": "v2", "created": "Thu, 21 Feb 2019 16:07:11 GMT"}, {"version": "v3", "created": "Fri, 11 Oct 2019 12:31:07 GMT"}], "update_date": "2019-10-14", "authors_parsed": [["Montobbio", "Noemi", ""], ["Citti", "Giovanna", ""], ["Sarti", "Alessandro", ""]]}, {"id": "1803.05840", "submitter": "Hans-Christian Ruiz Dipl-Phys", "authors": "H.C. Ruiz-Euler, H.J. Kappen", "title": "Effective Connectivity from Single Trial fMRI Data by Sampling\n  Biologically Plausible Models", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "q-bio.NC physics.data-an", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The estimation of causal network architectures in the brain is fundamental\nfor understanding cognitive information processes. However, access to the\ndynamic processes underlying cognition is limited to indirect measurements of\nthe hidden neuronal activity, for instance through fMRI data. Thus, estimating\nthe network structure of the underlying process is challenging. In this\narticle, we embed an adaptive importance sampler called Adaptive Path Integral\nSmoother (APIS) into the Expectation-Maximization algorithm to obtain point\nestimates of causal connectivity. We demonstrate on synthetic data that this\nprocedure finds not only the correct network structure but also the direction\nof effective connections from random initializations of the connectivity\nmatrix. In addition--motivated by contradictory claims in the literature--we\nexamine the effect of the neuronal timescale on the sensitivity of the BOLD\nsignal to changes in the connectivity and on the maximum likelihood solutions\nof the connectivity. We conclude with two warnings: First, the connectivity\nestimates under the assumption of slow dynamics can be extremely biased if the\ndata was generated by fast neuronal processes. Second, the faster the time\nscale, the less sensitive the BOLD signal is to changes in the incoming\nconnections to a node. Hence, connectivity estimation using realistic neural\ndynamics timescale requires extremely high-quality data and seems infeasible in\nmany practical data sets.\n", "versions": [{"version": "v1", "created": "Thu, 15 Mar 2018 16:25:47 GMT"}], "update_date": "2020-08-17", "authors_parsed": [["Ruiz-Euler", "H. C.", ""], ["Kappen", "H. J.", ""]]}, {"id": "1803.05897", "submitter": "Jim Kay", "authors": "Jim W. Kay and William A. Phillips", "title": "Contrasting information theoretic decompositions of modulatory and\n  arithmetic interactions in neural information processing systems", "comments": "23 pages, 6 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IT math.IT q-bio.NC q-bio.QM stat.ML", "license": "http://creativecommons.org/licenses/by-sa/4.0/", "abstract": "  Biological and artificial neural systems are composed of many local\nprocessors, and their capabilities depend upon the transfer function that\nrelates each local processor's outputs to its inputs. This paper uses a recent\nadvance in the foundations of information theory to study the properties of\nlocal processors that use contextual input to amplify or attenuate transmission\nof information about their driving inputs. This advance enables the information\ntransmitted by processors with two distinct inputs to be decomposed into those\ncomponents unique to each input, that shared between the two inputs, and that\nwhich depends on both though it is in neither, i.e. synergy. The decompositions\nthat we report here show that contextual modulation has information processing\nproperties that contrast with those of all four simple arithmetic operators,\nthat it can take various forms, and that the form used in our previous studies\nof artificial neural nets composed of local processors with both driving and\ncontextual inputs is particularly well-suited to provide the distinctive\ncapabilities of contextual modulation under a wide range of conditions. We\nargue that the decompositions reported here could be compared with those\nobtained from empirical neurobiological and psychophysical data under\nconditions thought to reflect contextual modulation. That would then shed new\nlight on the underlying processes involved. Finally, we suggest that such\ndecompositions could aid the design of context-sensitive machine learning\nalgorithms.\n", "versions": [{"version": "v1", "created": "Thu, 15 Mar 2018 17:51:21 GMT"}], "update_date": "2018-03-16", "authors_parsed": [["Kay", "Jim W.", ""], ["Phillips", "William A.", ""]]}, {"id": "1803.05938", "submitter": "Steven Baete", "authors": "Steven H. Baete, Jingyun Chen, Ying-Chia Lin, Xiuyuan Wang, Ricardo\n  Otazo, Fernando E. Boada", "title": "Low Rank plus Sparse Decomposition of ODFs for Improved Detection of\n  Group-level Differences and Variable Correlations in White Matter", "comments": "20 pages, 11 figures, 5 supplementary figures", "journal-ref": "NeuroImage, 174:138-152, 2018", "doi": "10.1016/j.neuroimage.2018.03.014", "report-no": null, "categories": "physics.med-ph physics.bio-ph q-bio.NC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A novel approach is presented for group statistical analysis of diffusion\nweighted MRI datasets through voxelwise Orientation Distribution Functions\n(ODF). Recent advances in MRI acquisition make it possible to use high quality\ndiffusion weighted protocols (multi-shell, large number of gradient directions)\nfor routine in vivo study of white matter architecture. The dimensionality of\nthese data sets is however often reduced to simplify statistical analysis.\nWhile these approaches may detect large group differences, they do not fully\ncapitalize on all acquired image volumes. Incorporation of all available\ndiffusion information in the analysis however risks biasing the outcome by\noutliers. Here we propose a statistical analysis method operating on the ODF,\neither the diffusion ODF or fiber ODF. To avoid outlier bias and reliably\ndetect voxelwise group differences and correlations with demographic or\nbehavioral variables, we apply the Low-Rank plus Sparse (L + S) matrix\ndecomposition on the voxelwise ODFs which separates the sparse individual\nvariability in the sparse matrix S whilst recovering the essential ODF features\nin the low-rank matrix L. We demonstrate the performance of this ODF L + S\napproach by replicating the established negative association between global\nwhite matter integrity and physical obesity in the Human Connectome dataset.\nThe volume of positive findings agrees with and expands on the volume found by\nTBSS, Connectivity based fixel enhancement and Connectometry. In the same\ndataset we further localize the correlations of brain structure with\nneurocognitive measures such as fluid intelligence and episodic memory. The\npresented ODF L + S approach will aid in the full utilization of all acquired\ndiffusion weightings leading to the detection of smaller group differences in\nclinically relevant settings as well as in neuroscience applications.\n", "versions": [{"version": "v1", "created": "Thu, 15 Mar 2018 18:33:54 GMT"}], "update_date": "2019-05-13", "authors_parsed": [["Baete", "Steven H.", ""], ["Chen", "Jingyun", ""], ["Lin", "Ying-Chia", ""], ["Wang", "Xiuyuan", ""], ["Otazo", "Ricardo", ""], ["Boada", "Fernando E.", ""]]}, {"id": "1803.05985", "submitter": "Milena \\v{C}uki\\'c Dr", "authors": "Milena Cukic, David Pokrajac, Miodrag Stokic, slobodan Simic, Vlada\n  Radivojevic and Milos Ljubisavljevic", "title": "EEG machine learning with Higuchi fractal dimension and Sample Entropy\n  as features for successful detection of depression", "comments": "34 pages, 4 Figures, 2 tables", "journal-ref": "Cognitive Neurodynamics Springer Nature March 2020", "doi": "10.1007/s11571-020-09581-x", "report-no": null, "categories": "stat.ML cs.LG q-bio.NC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Reliable diagnosis of depressive disorder is essential for both optimal\ntreatment and prevention of fatal outcomes. In this study, we aimed to\nelucidate the effectiveness of two non-linear measures, Higuchi Fractal\nDimension (HFD) and Sample Entropy (SampEn), in detecting depressive disorders\nwhen applied on EEG. HFD and SampEn of EEG signals were used as features for\nseven machine learning algorithms including Multilayer Perceptron, Logistic\nRegression, Support Vector Machines with the linear and polynomial kernel,\nDecision Tree, Random Forest, and Naive Bayes classifier, discriminating EEG\nbetween healthy control subjects and patients diagnosed with depression. We\nconfirmed earlier observations that both non-linear measures can discriminate\nEEG signals of patients from healthy control subjects. The results suggest that\ngood classification is possible even with a small number of principal\ncomponents. Average accuracy among classifiers ranged from 90.24% to 97.56%.\nAmong the two measures, SampEn had better performance. Using HFD and SampEn and\na variety of machine learning techniques we can accurately discriminate\npatients diagnosed with depression vs controls which can serve as a highly\nsensitive, clinically relevant marker for the diagnosis of depressive\ndisorders.\n", "versions": [{"version": "v1", "created": "Thu, 15 Mar 2018 20:13:38 GMT"}], "update_date": "2020-03-27", "authors_parsed": [["Cukic", "Milena", ""], ["Pokrajac", "David", ""], ["Stokic", "Miodrag", ""], ["Simic", "slobodan", ""], ["Radivojevic", "Vlada", ""], ["Ljubisavljevic", "Milos", ""]]}, {"id": "1803.06180", "submitter": "Daqing Guo", "authors": "Shengdun Wu, Yangsong Zhang, Yan Cui, Heng Li, Jiakang Wang, Lijun\n  Guo, Yang Xia, Dezhong Yao, Peng Xu, Daqing Guo", "title": "Heterogeneity of Synaptic Input Connectivity Regulates Spike-based\n  Neuronal Avalanches", "comments": "37 pages, 9 figures, 1 table", "journal-ref": null, "doi": null, "report-no": null, "categories": "q-bio.NC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Our mysterious brain is believed to operate near a non-equilibrium point and\ngenerate critical self-organized avalanches in neuronal activity. Recent\nexperimental evidence has revealed significant heterogeneity in both synaptic\ninput and output connectivity, but whether the structural heterogeneity\nparticipates in the regulation of neuronal avalanches remains poorly\nunderstood. By computational modelling, we predict that different types of\nstructural heterogeneity contribute distinct effects on avalanche\nneurodynamics. In particular, neuronal avalanches can be triggered at an\nintermediate level of input heterogeneity, but heterogeneous output\nconnectivity cannot evoke avalanche dynamics. In the criticality region, the\nco-emergence of multi-scale cortical activities is observed, and both the\navalanche dynamics and neuronal oscillations are modulated by the input\nheterogeneity. Remarkably, we show similar results can be reproduced in\nnetworks with various types of in- and out-degree distributions. Overall, these\nfindings not only provide details on the underlying circuitry mechanisms of\nnonrandom synaptic connectivity in the regulation of neuronal avalanches, but\nalso inspire testable hypotheses for future experimental studies.\n", "versions": [{"version": "v1", "created": "Fri, 16 Mar 2018 11:54:38 GMT"}, {"version": "v2", "created": "Wed, 11 Jul 2018 13:26:39 GMT"}], "update_date": "2018-07-12", "authors_parsed": [["Wu", "Shengdun", ""], ["Zhang", "Yangsong", ""], ["Cui", "Yan", ""], ["Li", "Heng", ""], ["Wang", "Jiakang", ""], ["Guo", "Lijun", ""], ["Xia", "Yang", ""], ["Yao", "Dezhong", ""], ["Xu", "Peng", ""], ["Guo", "Daqing", ""]]}, {"id": "1803.06288", "submitter": "David Heeger", "authors": "David J. Heeger and Wayne E. Mackey", "title": "ORGaNICs: A Theory of Working Memory in Brains and Machines", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI q-bio.NC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Working memory is a cognitive process that is responsible for temporarily\nholding and manipulating information. Most of the empirical neuroscience\nresearch on working memory has focused on measuring sustained activity in\nprefrontal cortex (PFC) and/or parietal cortex during simple delayed-response\ntasks, and most of the models of working memory have been based on neural\nintegrators. But working memory means much more than just holding a piece of\ninformation online. We describe a new theory of working memory, based on a\nrecurrent neural circuit that we call ORGaNICs (Oscillatory Recurrent GAted\nNeural Integrator Circuits). ORGaNICs are a variety of Long Short Term Memory\nunits (LSTMs), imported from machine learning and artificial intelligence.\nORGaNICs can be used to explain the complex dynamics of delay-period activity\nin prefrontal cortex (PFC) during a working memory task. The theory is\nanalytically tractable so that we can characterize the dynamics, and the theory\nprovides a means for reading out information from the dynamically varying\nresponses at any point in time, in spite of the complex dynamics. ORGaNICs can\nbe implemented with a biophysical (electrical circuit) model of pyramidal\ncells, combined with shunting inhibition via a thalamocortical loop. Although\nintroduced as a computational theory of working memory, ORGaNICs are also\napplicable to models of sensory processing, motor preparation and motor\ncontrol. ORGaNICs offer computational advantages compared to other varieties of\nLSTMs that are commonly used in AI applications. Consequently, ORGaNICs are a\nframework for canonical computation in brains and machines.\n", "versions": [{"version": "v1", "created": "Fri, 16 Mar 2018 16:04:09 GMT"}, {"version": "v2", "created": "Wed, 21 Mar 2018 20:04:04 GMT"}, {"version": "v3", "created": "Sun, 22 Apr 2018 15:55:23 GMT"}, {"version": "v4", "created": "Fri, 25 May 2018 18:25:38 GMT"}], "update_date": "2018-05-29", "authors_parsed": [["Heeger", "David J.", ""], ["Mackey", "Wayne E.", ""]]}, {"id": "1803.06622", "submitter": "Christopher Kim", "authors": "Christopher Kim and Carson Chow", "title": "Learning recurrent dynamics in spiking networks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "q-bio.NC cs.AI cs.LG cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Spiking activity of neurons engaged in learning and performing a task show\ncomplex spatiotemporal dynamics. While the output of recurrent network models\ncan learn to perform various tasks, the possible range of recurrent dynamics\nthat emerge after learning remains unknown. Here we show that modifying the\nrecurrent connectivity with a recursive least squares algorithm provides\nsufficient flexibility for synaptic and spiking rate dynamics of spiking\nnetworks to produce a wide range of spatiotemporal activity. We apply the\ntraining method to learn arbitrary firing patterns, stabilize irregular spiking\nactivity of a balanced network, and reproduce the heterogeneous spiking rate\npatterns of cortical neurons engaged in motor planning and movement. We\nidentify sufficient conditions for successful learning, characterize two types\nof learning errors, and assess the network capacity. Our findings show that\nsynaptically-coupled recurrent spiking networks possess a vast computational\ncapability that can support the diverse activity patterns in the brain.\n", "versions": [{"version": "v1", "created": "Sun, 18 Mar 2018 07:51:19 GMT"}, {"version": "v2", "created": "Sat, 18 Aug 2018 14:13:37 GMT"}], "update_date": "2018-08-21", "authors_parsed": [["Kim", "Christopher", ""], ["Chow", "Carson", ""]]}, {"id": "1803.07193", "submitter": "Lior Lebovich", "authors": "Lior Lebovich, Ran Darshan, Yoni Lavi, David Hansel and Yonatan\n  Loewenstein", "title": "Idiosyncratic choice bias in decision tasks naturally emerges from\n  neuronal network dynamics", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "q-bio.NC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Idiosyncratic tendency to choose one alternative over others in the absence\nof an identified reason, is a common observation in two-alternative\nforced-choice experiments. It is tempting to account for it as resulting from\nthe (unknown) participant-specific history and thus treat it as a measurement\nnoise. Indeed, idiosyncratic choice biases are typically considered as\nnuisance. Care is taken to account for them by adding an ad-hoc bias parameter\nor by counterbalancing the choices to average them out. Here we quantify\nidiosyncratic choice biases in a perceptual discrimination task and a motor\ntask. We report substantial and significant biases in both cases. Then, we\npresent theoretical evidence that even in idealized experiments, in which the\nsettings are symmetric, idiosyncratic choice bias is expected to emerge from\nthe dynamics of competing neuronal networks. We thus argue that idiosyncratic\nchoice bias reflects the microscopic dynamics of choice and therefore is\nvirtually inevitable in any comparison or decision task.\n", "versions": [{"version": "v1", "created": "Mon, 19 Mar 2018 23:10:51 GMT"}], "update_date": "2018-03-21", "authors_parsed": [["Lebovich", "Lior", ""], ["Darshan", "Ran", ""], ["Lavi", "Yoni", ""], ["Hansel", "David", ""], ["Loewenstein", "Yonatan", ""]]}, {"id": "1803.07256", "submitter": "Sang-Yoon  Kim", "authors": "Sang-Yoon Kim and Woochang Lim", "title": "Burst Synchronization in A Scale-Free Neuronal Network with Inhibitory\n  Spike-Timing-Dependent Plasticity", "comments": "arXiv admin note: substantial text overlap with arXiv:1708.04543,\n  arXiv:1801.01385", "journal-ref": null, "doi": null, "report-no": null, "categories": "q-bio.NC physics.bio-ph", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We are concerned about burst synchronization (BS), related to neural\ninformation processes in health and disease, in the Barab\\'{a}si-Albert\nscale-free network (SFN) composed of inhibitory bursting Hindmarsh-Rose\nneurons. This inhibitory neuronal population has adaptive dynamic synaptic\nstrengths governed by the inhibitory spike-timing-dependent plasticity (iSTDP).\nIn previous works without considering iSTDP, BS was found to appear in a range\nof noise intensities for fixed synaptic inhibition strengths. In contrast, in\nour present work, we take into consideration iSTDP and investigate its effect\non BS by varying the noise intensity. Our new main result is to find occurrence\nof a Matthew effect in inhibitory synaptic plasticity: good BS gets better via\nLTD, while bad BS get worse via LTP. This kind of Matthew effect in inhibitory\nsynaptic plasticity is in contrast to that in excitatory synaptic plasticity\nwhere good (bad) synchronization gets better (worse) via LTP (LTD). We note\nthat, due to inhibition, the roles of LTD and LTP in inhibitory synaptic\nplasticity are reversed in comparison with those in excitatory synaptic\nplasticity. Moreover, emergences of LTD and LTP of synaptic inhibition\nstrengths are intensively investigated via a microscopic method based on the\ndistributions of time delays between the pre- and the post-synaptic burst onset\ntimes. Finally, in the presence of iSTDP we investigate the effects of network\narchitecture on BS by varying the symmetric attachment degree $l^*$ and the\nasymmetry parameter $\\Delta l$ in the SFN.\n", "versions": [{"version": "v1", "created": "Tue, 20 Mar 2018 04:52:01 GMT"}, {"version": "v2", "created": "Wed, 21 Mar 2018 02:16:06 GMT"}, {"version": "v3", "created": "Fri, 6 Apr 2018 05:23:12 GMT"}, {"version": "v4", "created": "Mon, 20 Aug 2018 07:23:05 GMT"}], "update_date": "2018-08-21", "authors_parsed": [["Kim", "Sang-Yoon", ""], ["Lim", "Woochang", ""]]}, {"id": "1803.07352", "submitter": "Heiko Sch\\\"utt", "authors": "Heiko H. Sch\\\"utt, Lars O. M. Rothkegel, Hans A. Trukenbrod, Ralf\n  Engbert, Felix A. Wichmann", "title": "Disentangling top-down vs. bottom-up and low-level vs. high-level\n  influences on eye movements over time", "comments": "Submitted to Journal of Vision", "journal-ref": null, "doi": null, "report-no": null, "categories": "q-bio.NC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Bottom-up and top-down, as well as low-level and high-level factors influence\nwhere we fixate when viewing natural scenes. However, the importance of each of\nthese factors and how they interact remains a matter of debate. Here, we\ndisentangle these factors by analysing their influence over time. For this\npurpose we develop a saliency model which is based on the internal\nrepresentation of a recent early spatial vision model to measure the low-level\nbottom-up factor. To measure the influence of high-level bottom-up features, we\nuse a recent DNN-based saliency model. To account for top-down influences, we\nevaluate the models on two large datasets with different tasks: first, a\nmemorisation task and, second, a search task. Our results lend support to a\nseparation of visual scene exploration into three phases: The first saccade, an\ninitial guided exploration characterised by a gradual broadening of the\nfixation density, and an steady state which is reached after roughly 10\nfixations. Saccade target selection during the initial exploration and in the\nsteady state are related to similar areas of interest, which are better\npredicted when including high-level features. In the search dataset, fixation\nlocations are determined predominantly by top-down processes. In contrast, the\nfirst fixation follows a different fixation density and contains a strong\ncentral fixation bias. Nonetheless, first fixations are guided strongly by\nimage properties and as early as 200 ms after image onset, fixations are better\npredicted by high-level information. We conclude that any low-level bottom-up\nfactors are mainly limited to the generation of the first saccade. All saccades\nare better explained when high-level features are considered, and later this\nhigh-level bottom-up control can be overruled by top-down influences.\n", "versions": [{"version": "v1", "created": "Tue, 20 Mar 2018 10:33:44 GMT"}, {"version": "v2", "created": "Thu, 17 May 2018 03:45:47 GMT"}], "update_date": "2018-05-18", "authors_parsed": [["Sch\u00fctt", "Heiko H.", ""], ["Rothkegel", "Lars O. M.", ""], ["Trukenbrod", "Hans A.", ""], ["Engbert", "Ralf", ""], ["Wichmann", "Felix A.", ""]]}, {"id": "1803.07770", "submitter": "Christopher J. Cueva", "authors": "Christopher J. Cueva and Xue-Xin Wei", "title": "Emergence of grid-like representations by training recurrent neural\n  networks to perform spatial localization", "comments": null, "journal-ref": "International Conference on Learning Representations (ICLR) 2018", "doi": null, "report-no": null, "categories": "q-bio.NC cs.AI cs.NE stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Decades of research on the neural code underlying spatial navigation have\nrevealed a diverse set of neural response properties. The Entorhinal Cortex\n(EC) of the mammalian brain contains a rich set of spatial correlates,\nincluding grid cells which encode space using tessellating patterns. However,\nthe mechanisms and functional significance of these spatial representations\nremain largely mysterious. As a new way to understand these neural\nrepresentations, we trained recurrent neural networks (RNNs) to perform\nnavigation tasks in 2D arenas based on velocity inputs. Surprisingly, we find\nthat grid-like spatial response patterns emerge in trained networks, along with\nunits that exhibit other spatial correlates, including border cells and\nband-like cells. All these different functional types of neurons have been\nobserved experimentally. The order of the emergence of grid-like and border\ncells is also consistent with observations from developmental studies.\nTogether, our results suggest that grid cells, border cells and others as\nobserved in EC may be a natural solution for representing space efficiently\ngiven the predominant recurrent connections in the neural circuits.\n", "versions": [{"version": "v1", "created": "Wed, 21 Mar 2018 07:09:57 GMT"}], "update_date": "2018-05-11", "authors_parsed": [["Cueva", "Christopher J.", ""], ["Wei", "Xue-Xin", ""]]}, {"id": "1803.07858", "submitter": "Pablo Villegas G\\'ongora", "authors": "Serena di Santo, Pablo Villegas, Raffaella Burioni, Miguel A. Mu\\~noz", "title": "Non-normality, reactivity, and intrinsic stochasticity in neural\n  dynamics: a non-equilibrium potential approach", "comments": "26 pages, 11 figures", "journal-ref": "J. Stat. Mech. (2018) 073402", "doi": "10.1088/1742-5468/aacda3", "report-no": null, "categories": "cond-mat.dis-nn cond-mat.stat-mech q-bio.NC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Intrinsic stochasticity can induce highly non-trivial effects on dynamical\nsystems, including stochastic and coherence resonance, noise induced\nbistability, noise-induced oscillations, to name but a few. In this paper we\nrevisit a mechanism first investigated in the context of neuroscience by which\nrelatively small demographic (intrinsic) fluctuations can lead to the emergence\nof avalanching behavior in systems that are deterministically characterized by\na single stable fixed point (up state). The anomalously large response of such\nsystems to stochasticity stems (or is strongly associated with) the existence\nof a \"non-normal\" stability matrix at the deterministic fixed point, which may\ninduce the system to be \"reactive\". Here, we further investigate this mechanism\nby exploring the interplay between non-normality and intrinsic (demographic)\nstochasticity, by employing a number of analytical and computational\napproaches. We establish, in particular, that the resulting dynamics in this\ntype of systems cannot be simply derived from a scalar potential but,\nadditionally, one needs to consider a curl flux which describes the essential\nnon-equilibrium nature of this type of noisy non-normal systems. Moreover, we\nshed further light on the origin of the phenomenon, introduce the novel concept\nof \"non-linear reactivity\", and rationalize of the observed the value of the\nemerging avalanche exponents.\n", "versions": [{"version": "v1", "created": "Wed, 21 Mar 2018 11:11:00 GMT"}, {"version": "v2", "created": "Wed, 6 Jun 2018 08:42:41 GMT"}], "update_date": "2018-07-17", "authors_parsed": [["di Santo", "Serena", ""], ["Villegas", "Pablo", ""], ["Burioni", "Raffaella", ""], ["Mu\u00f1oz", "Miguel A.", ""]]}, {"id": "1803.07883", "submitter": "Ernest Greene", "authors": "Ernest Greene, Yash Patel", "title": "Scan transcription of two-dimensional shapes as an alternative\n  neuromorphic concept", "comments": "7 pages, 4 figures, 53 references", "journal-ref": "Trends in Artificial Intelligence, 2018, 1, 27-33", "doi": null, "report-no": null, "categories": "q-bio.NC", "license": "http://creativecommons.org/publicdomain/zero/1.0/", "abstract": "  Selfridge, along with Sutherland and Marr provided some of the earliest\nproposals for how to program computers to recognize shapes. Their emphasis on\nfiltering for contour features, especially the orientation of boundary\nsegments, was reinforced by the Nobel Prize winning work of Hubel & Wiesel who\ndiscovered that neurons in primary visual cortex selectively respond as a\nfunction of contour orientation. Countless investigators and theorists have\ncontinued to build on this approach. These models are often described as\nneuromorphic, which implies that the computational methods are based on\nbiologically plausible principles. Recent work from the present lab has\nchallenged the emphasis on orientation selectivity and the use of neural\nnetwork principles. The goal of the present report is not to relitigate those\nissues, but to provide an alternative concept for encoding of shape information\nthat may be useful to neuromorphic modelers.\n", "versions": [{"version": "v1", "created": "Wed, 21 Mar 2018 12:27:53 GMT"}], "update_date": "2018-03-22", "authors_parsed": [["Greene", "Ernest", ""], ["Patel", "Yash", ""]]}, {"id": "1803.08109", "submitter": "Maxwell Bertolero Dr", "authors": "Maxwell A. Bertolero, B.T.T. Yeo, Danielle S. Bassett, Mark D'Esposito", "title": "A mechanistic model of connector hubs, modularity, and cognition", "comments": null, "journal-ref": "Bertolero, M. A., Yeo, B. T. T., Bassett, D. S., & D'Esposito, M.\n  (2018). A mechanistic model of connector hubs, modularity and cognition.\n  Nature Human Behaviour, 2(10), 765-777", "doi": "10.1038/s41562-018-0420-6", "report-no": null, "categories": "q-bio.NC q-bio.QM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The human brain network is modular--comprised of communities of tightly\ninterconnected nodes. This network contains local hubs, which have many\nconnections within their own communities, and connector hubs, which have\nconnections diversely distributed across communities. A mechanistic\nunderstanding of these hubs and how they support cognition has not been\ndemonstrated. Here, we leveraged individual differences in hub connectivity and\ncognition. We show that a model of hub connectivity accurately predicts the\ncognitive performance of 476 individuals in four distinct tasks. Moreover,\nthere is a general optimal network structure for cognitive\nperformance--individuals with diversely connected hubs and consequent modular\nbrain networks exhibit increased cognitive performance, regardless of the task.\nCritically, we find evidence consistent with a mechanistic model in which\nconnector hubs tune the connectivity of their neighbors to be more modular\nwhile allowing for task appropriate information integration across communities,\nwhich increases global modularity and cognitive performance.\n", "versions": [{"version": "v1", "created": "Wed, 21 Mar 2018 19:55:37 GMT"}, {"version": "v2", "created": "Wed, 30 May 2018 19:07:38 GMT"}, {"version": "v3", "created": "Thu, 5 Jul 2018 14:19:36 GMT"}, {"version": "v4", "created": "Thu, 9 Aug 2018 15:56:54 GMT"}], "update_date": "2018-10-10", "authors_parsed": [["Bertolero", "Maxwell A.", ""], ["Yeo", "B. T. T.", ""], ["Bassett", "Danielle S.", ""], ["D'Esposito", "Mark", ""]]}, {"id": "1803.08362", "submitter": "Jahan Schad", "authors": "Jahan N. Schad", "title": "Consciousness: From the Perspective of the Dynamical Systems Theory", "comments": "16 Pages, 1 Figure", "journal-ref": "J Neurol Stroke. 2019; 9 (3): 133\u00e2\u0080\u0092138", "doi": null, "report-no": null, "categories": "q-bio.NC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Beings, animate or inanimate, are dynamical systems which continuously\ninteract with the (external and /or internal) environment through the physical\nor physiologic interfaces of their Kantian (representational) realities. And\nthe nature of their interactions is determined by the inner workings of their\nsystems. It is from this perspective that this work attempts to address some of\nthe long-held philosophical questions, major one among them consciousness-- in\nthe context of the physicality of the dynamic systems.\n", "versions": [{"version": "v1", "created": "Sun, 4 Mar 2018 17:10:03 GMT"}], "update_date": "2019-09-03", "authors_parsed": [["Schad", "Jahan N.", ""]]}, {"id": "1803.08541", "submitter": "Andrea Avena-Koenigsberger", "authors": "Andrea Avena-Koenigsberger, Xiaoran Yan, Artemy Kolchinsky, Martijn\n  van den Heuvel, Patric Hagmann and Olaf Sporns", "title": "A spectrum of routing strategies for brain networks", "comments": null, "journal-ref": null, "doi": "10.1371/journal.pcbi.1006833", "report-no": null, "categories": "q-bio.NC", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Communication of signals among nodes in a complex network poses fundamental\nproblems of efficiency and cost. Routing of messages along shortest paths\nrequires global information about the topology, while spreading by diffusion,\nwhich operates according to local topological features, is informationally\n\"cheap\" but inefficient. We introduce a stochastic model for network\ncommunication that combines varying amounts of local and global information\nabout the network topology. The model generates a continuous spectrum of\ndynamics that converge onto shortest-path and random-walk (diffusion)\ncommunication processes at the limiting extremes. We implement the model on two\ncohorts of human connectome networks and investigate the effects of varying\namounts of local and global information on the network's communication cost. We\nidentify routing strategies that approach a (highly efficient) shortest-path\ncommunication process with a relatively small amount of global information.\nMoreover, we show that the cost of routing messages from and to hub nodes\nvaries as a function of the amount of global information driving the system's\ndynamics. Finally, we implement the model to identify individual subject\ndifferences from a communication dynamics point of view. The present framework\ndeparts from the classical shortest paths vs. diffusion dichotomy, suggesting\ninstead that brain networks may exhibit different types of communication\ndynamics depending on varying functional demands and the availability of\nresources.\n", "versions": [{"version": "v1", "created": "Thu, 22 Mar 2018 18:46:21 GMT"}], "update_date": "2019-06-19", "authors_parsed": [["Avena-Koenigsberger", "Andrea", ""], ["Yan", "Xiaoran", ""], ["Kolchinsky", "Artemy", ""], ["Heuvel", "Martijn van den", ""], ["Hagmann", "Patric", ""], ["Sporns", "Olaf", ""]]}, {"id": "1803.08554", "submitter": "Ramin M. Hasani", "authors": "Mathias Lechner, Ramin M. Hasani, Radu Grosu", "title": "Neuronal Circuit Policies", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "q-bio.NC cs.AI cs.LG cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose an effective way to create interpretable control agents, by\nre-purposing the function of a biological neural circuit model, to govern\nsimulated and real world reinforcement learning (RL) test-beds. We model the\ntap-withdrawal (TW) neural circuit of the nematode, C. elegans, a circuit\nresponsible for the worm's reflexive response to external mechanical touch\nstimulations, and learn its synaptic and neuronal parameters as a policy for\ncontrolling basic RL tasks. We also autonomously park a real rover robot on a\npre-defined trajectory, by deploying such neuronal circuit policies learned in\na simulated environment. For reconfiguration of the purpose of the TW neural\ncircuit, we adopt a search-based RL algorithm. We show that our neuronal\npolicies perform as good as deep neural network policies with the advantage of\nrealizing interpretable dynamics at the cell level.\n", "versions": [{"version": "v1", "created": "Thu, 22 Mar 2018 19:23:32 GMT"}], "update_date": "2018-03-26", "authors_parsed": [["Lechner", "Mathias", ""], ["Hasani", "Ramin M.", ""], ["Grosu", "Radu", ""]]}, {"id": "1803.08573", "submitter": "Katherine Medina", "authors": "Victor Flores and Katherine Medina", "title": "The effects of cyclical simulation on the axon hillock diameter of\n  murine intracortical neurons", "comments": "12 pages; submitted to PlosOne", "journal-ref": null, "doi": null, "report-no": null, "categories": "q-bio.NC", "license": "http://creativecommons.org/publicdomain/zero/1.0/", "abstract": "  Changes to the axon hillock in frequently firing neurons are known to be\nimportant predictors of early disease states. Studying this phenomenon is\ncritical to understanding the first insult implicated in multiple\nneuro-degenerative disorders. To study these changes we used cyclical\nstimulations using micro-electrodes to the axon hillock of mouse intracortical\nneurons. Numerical simulation results indicate that axon hillock water\npotential fluctuated sinusoidally on high voltage only. Fluctuations in the\namplitude and trend were caused by calcium flow and storage resistance,\nrespectively. The change in axon hillock-stored water was proportional to the\nchange rate in water potential. Axon hillock diameter increased with\nfluctuations in calcium free media; moreover, it varied slightly under low\nvoltage conditions. Changes in axon hillock diameter were caused by changes in\nwater potential, which was determined by subcellular gated channels, media\ncalcium potential, and other baseline characteristics of neurons.\n", "versions": [{"version": "v1", "created": "Thu, 22 Mar 2018 20:18:15 GMT"}], "update_date": "2018-03-26", "authors_parsed": [["Flores", "Victor", ""], ["Medina", "Katherine", ""]]}, {"id": "1803.08797", "submitter": "Hans-Christian Ruiz Dipl-Phys", "authors": "Hans-Christian Ruiz-Euler, Jose R. Ferreira Marques, Hilbert J. Kappen", "title": "Nonlinear Deconvolution by Sampling Biophysically Plausible Hemodynamic\n  Models", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "q-bio.NC physics.data-an", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Non-invasive methods to measure brain activity are important to understand\ncognitive processes in the human brain. A prominent example is functional\nmagnetic resonance imaging (fMRI), which is a noisy measurement of a delayed\nsignal that depends non-linearly on the neuronal activity through the\nneurovascular coupling. These characteristics make the inference of neuronal\nactivity from fMRI a difficult but important step in fMRI studies that require\ninformation at the neuronal level. In this article, we address this inference\nproblem using a Bayesian approach where we model the latent neural activity as\na stochastic process and assume that the observed BOLD signal results from a\nrealistic physiological (Balloon) model. We apply a recently developed\nsmoothing method called APIS to efficiently sample the posterior given single\nevent fMRI time series. To infer neuronal signals with high likelihood for\nmultiple time series efficiently, a modification of the original algorithm is\nintroduced. We demonstrate that our adaptive procedure is able to compensate\nthe lacking of inputs in the model to infer the neuronal activity and that it\noutperforms dramatically the standard bootstrap particle filter-smoother in\nthis setting. This makes the proposed procedure especially attractive to\ndeconvolve resting state fMRI data. To validate the method, we evaluate the\nquality of the signals inferred using the timing information contained in them.\nAPIS obtains reliable event timing estimates based on fMRI data gathered during\na reaction time experiment with short stimuli. Hence, we show for the first\ntime that one can obtain accurate absolute timing of neuronal activity by\nreconstructing the latent neural signal.\n", "versions": [{"version": "v1", "created": "Fri, 23 Mar 2018 14:04:18 GMT"}], "update_date": "2020-08-17", "authors_parsed": [["Ruiz-Euler", "Hans-Christian", ""], ["Marques", "Jose R. Ferreira", ""], ["Kappen", "Hilbert J.", ""]]}, {"id": "1803.08833", "submitter": "Elena Pastorelli", "authors": "Elena Pastorelli, Pier Stanislao Paolucci, Francesco Simula, Andrea\n  Biagioni, Fabrizio Capuani, Paolo Cretaro, Giulia De Bonis, Francesca Lo\n  Cicero, Alessandro Lonardo, Michele Martinelli, Luca Pontisso, Piero Vicini,\n  Roberto Ammendola", "title": "Gaussian and exponential lateral connectivity on distributed spiking\n  neural network simulation", "comments": "9 pages, 9 figures, added reference to final peer reviewed version on\n  conference paper and DOI", "journal-ref": null, "doi": "10.1109/PDP2018.2018.00110", "report-no": null, "categories": "cs.DC cs.NE q-bio.NC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We measured the impact of long-range exponentially decaying intra-areal\nlateral connectivity on the scaling and memory occupation of a distributed\nspiking neural network simulator compared to that of short-range Gaussian\ndecays. While previous studies adopted short-range connectivity, recent\nexperimental neurosciences studies are pointing out the role of longer-range\nintra-areal connectivity with implications on neural simulation platforms.\nTwo-dimensional grids of cortical columns composed by up to 11 M point-like\nspiking neurons with spike frequency adaption were connected by up to 30 G\nsynapses using short- and long-range connectivity models. The MPI processes\ncomposing the distributed simulator were run on up to 1024 hardware cores,\nhosted on a 64 nodes server platform. The hardware platform was a cluster of\nIBM NX360 M5 16-core compute nodes, each one containing two Intel Xeon Haswell\n8-core E5-2630 v3 processors, with a clock of 2.40 G Hz, interconnected through\nan InfiniBand network, equipped with 4x QDR switches.\n", "versions": [{"version": "v1", "created": "Fri, 23 Mar 2018 15:21:42 GMT"}, {"version": "v2", "created": "Tue, 19 Feb 2019 14:54:34 GMT"}], "update_date": "2019-02-20", "authors_parsed": [["Pastorelli", "Elena", ""], ["Paolucci", "Pier Stanislao", ""], ["Simula", "Francesco", ""], ["Biagioni", "Andrea", ""], ["Capuani", "Fabrizio", ""], ["Cretaro", "Paolo", ""], ["De Bonis", "Giulia", ""], ["Cicero", "Francesca Lo", ""], ["Lonardo", "Alessandro", ""], ["Martinelli", "Michele", ""], ["Pontisso", "Luca", ""], ["Vicini", "Piero", ""], ["Ammendola", "Roberto", ""]]}, {"id": "1803.09018", "submitter": "Abraham Nunes", "authors": "Abraham Nunes and Alexander Rudiuk", "title": "The Importance of Constraint Smoothness for Parameter Estimation in\n  Computational Cognitive Modeling", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "q-bio.QM cs.LG q-bio.NC stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Psychiatric neuroscience is increasingly aware of the need to define\npsychopathology in terms of abnormal neural computation. The central tool in\nthis endeavour is the fitting of computational models to behavioural data. The\nmost prominent example of this procedure is fitting reinforcement learning (RL)\nmodels to decision-making data collected from mentally ill and healthy subject\npopulations. These models are generative models of the decision-making data\nthemselves, and the parameters we seek to infer can be psychologically and\nneurobiologically meaningful. Currently, the gold standard approach to this\ninference procedure involves Monte-Carlo sampling, which is robust but\ncomputationally intensive---rendering additional procedures, such as\ncross-validation, impractical. Searching for point estimates of model\nparameters using optimization procedures remains a popular and interesting\noption. On a novel testbed simulating parameter estimation from a common RL\ntask, we investigated the effects of smooth vs. boundary constraints on\nparameter estimation using interior point and deterministic direct search\nalgorithms for optimization. Ultimately, we show that the use of boundary\nconstraints can lead to substantial truncation effects. Our results discourage\nthe use of boundary constraints for these applications.\n", "versions": [{"version": "v1", "created": "Sat, 24 Mar 2018 00:25:20 GMT"}], "update_date": "2018-03-28", "authors_parsed": [["Nunes", "Abraham", ""], ["Rudiuk", "Alexander", ""]]}, {"id": "1803.09107", "submitter": "Moti Salti", "authors": "Moti Salti, Asaf Harel, Sebastien Marti", "title": "Conscious Perception: Time for an Update?", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "q-bio.NC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Understanding the neural mechanism underlying subjective representation has\nbecome a central endeavor in cognitive-neuroscience. In theories of conscious\nperception, stimulus gaining conscious access is usually considered as a\ndiscrete neuronal event to be characterized in time or space, sometimes refer\nto as a 'conscious episode'. Surprisingly, the alternative hypothesis according\nto which conscious perception is a dynamic process has been rarely considered.\nHere, we discuss this hypothesis and envisage its implications. We show how it\ncan reconcile inconsistent empirical findings on the timing of the neural\ncorrelates of consciousness (NCCs), and make testable predictions. According to\nthis hypothesis, a stimulus is consciously perceived for as long as it is\nrecoded to fit an ongoing stream composed of all other perceived stimuli. We\nsuggest that this 'updating' process is governed by at least three factors (1)\ncontext, (2) stimulus saliency and (3) observer's goal. Finally, this framework\nforces us to reconsider the typical distinction between conscious and\nunconscious information processing.\n", "versions": [{"version": "v1", "created": "Sat, 24 Mar 2018 13:25:26 GMT"}], "update_date": "2018-03-28", "authors_parsed": [["Salti", "Moti", ""], ["Harel", "Asaf", ""], ["Marti", "Sebastien", ""]]}, {"id": "1803.09574", "submitter": "Darjan Salaj", "authors": "Guillaume Bellec, Darjan Salaj, Anand Subramoney, Robert Legenstein,\n  Wolfgang Maass", "title": "Long short-term memory and learning-to-learn in networks of spiking\n  neurons", "comments": "First three authors contributed equally; Paper accepted at NIPS 2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NE q-bio.NC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recurrent networks of spiking neurons (RSNNs) underlie the astounding\ncomputing and learning capabilities of the brain. But computing and learning\ncapabilities of RSNN models have remained poor, at least in comparison with\nartificial neural networks (ANNs). We address two possible reasons for that.\nOne is that RSNNs in the brain are not randomly connected or designed according\nto simple rules, and they do not start learning as a tabula rasa network.\nRather, RSNNs in the brain were optimized for their tasks through evolution,\ndevelopment, and prior experience. Details of these optimization processes are\nlargely unknown. But their functional contribution can be approximated through\npowerful optimization methods, such as backpropagation through time (BPTT).\n  A second major mismatch between RSNNs in the brain and models is that the\nlatter only show a small fraction of the dynamics of neurons and synapses in\nthe brain. We include neurons in our RSNN model that reproduce one prominent\ndynamical process of biological neurons that takes place at the behaviourally\nrelevant time scale of seconds: neuronal adaptation. We denote these networks\nas LSNNs because of their Long short-term memory. The inclusion of adapting\nneurons drastically increases the computing and learning capability of RSNNs if\nthey are trained and configured by deep learning (BPTT combined with a rewiring\nalgorithm that optimizes the network architecture). In fact, the computational\nperformance of these RSNNs approaches for the first time that of LSTM networks.\nIn addition RSNNs with adapting neurons can acquire abstract knowledge from\nprior learning in a Learning-to-Learn (L2L) scheme, and transfer that knowledge\nin order to learn new but related tasks from very few examples. We demonstrate\nthis for supervised learning and reinforcement learning.\n", "versions": [{"version": "v1", "created": "Mon, 26 Mar 2018 13:25:43 GMT"}, {"version": "v2", "created": "Sat, 19 May 2018 16:29:36 GMT"}, {"version": "v3", "created": "Fri, 2 Nov 2018 12:26:32 GMT"}, {"version": "v4", "created": "Tue, 25 Dec 2018 11:17:22 GMT"}], "update_date": "2018-12-27", "authors_parsed": [["Bellec", "Guillaume", ""], ["Salaj", "Darjan", ""], ["Subramoney", "Anand", ""], ["Legenstein", "Robert", ""], ["Maass", "Wolfgang", ""]]}, {"id": "1803.09807", "submitter": "Jesse Livezey", "authors": "Jesse A. Livezey, Kristofer E. Bouchard, Edward F. Chang", "title": "Deep learning as a tool for neural data analysis: speech classification\n  and cross-frequency coupling in human sensorimotor cortex", "comments": "23 pages, 9 figures", "journal-ref": null, "doi": "10.1371/journal.pcbi.1007091", "report-no": null, "categories": "cs.NE q-bio.NC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A fundamental challenge in neuroscience is to understand what structure in\nthe world is represented in spatially distributed patterns of neural activity\nfrom multiple single-trial measurements. This is often accomplished by learning\na simple, linear transformations between neural features and features of the\nsensory stimuli or motor task. While successful in some early sensory\nprocessing areas, linear mappings are unlikely to be ideal tools for\nelucidating nonlinear, hierarchical representations of higher-order brain areas\nduring complex tasks, such as the production of speech by humans. Here, we\napply deep networks to predict produced speech syllables from cortical surface\nelectric potentials recorded from human sensorimotor cortex. We found that deep\nnetworks had higher decoding prediction accuracy compared to baseline models,\nand also exhibited greater improvements in accuracy with increasing dataset\nsize. We further demonstrate that deep network's confusions revealed\nhierarchical latent structure in the neural data, which recapitulated the\nunderlying articulatory nature of speech motor control. Finally, we used deep\nnetworks to compare task-relevant information in different neural frequency\nbands, and found that the high-gamma band contains the vast majority of\ninformation relevant for the speech prediction task, with little-to-no\nadditional contribution from lower-frequencies. Together, these results\ndemonstrate the utility of deep networks as a data analysis tool for\nneuroscience.\n", "versions": [{"version": "v1", "created": "Mon, 26 Mar 2018 19:26:44 GMT"}], "update_date": "2020-07-01", "authors_parsed": [["Livezey", "Jesse A.", ""], ["Bouchard", "Kristofer E.", ""], ["Chang", "Edward F.", ""]]}, {"id": "1803.09974", "submitter": "Dimitra Maoutsa", "authors": "Jose Casadiego, Dimitra Maoutsa, Marc Timme", "title": "Inferring network connectivity from event timing patterns", "comments": "6 pages, 5 figures, The first two authors contributed equally to this\n  paper, and should be regarded as co-first authors. [v2: metadata update]", "journal-ref": "Phys. Rev. Lett. 121, 054101 (2018)", "doi": "10.1103/PhysRevLett.121.054101", "report-no": null, "categories": "q-bio.NC nlin.CD physics.data-an stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Reconstructing network connectivity from the collective dynamics of a system\ntypically requires access to its complete continuous-time evolution although\nthese are often experimentally inaccessible. Here we propose a theory for\nrevealing physical connectivity of networked systems only from the event time\nseries their intrinsic collective dynamics generate. Representing the patterns\nof event timings in an event space spanned by inter-event and cross-event\nintervals, we reveal which other units directly influence the inter-event times\nof any given unit. For illustration, we linearize an event space mapping\nconstructed from the spiking patterns in model neural circuits to reveal the\npresence or absence of synapses between any pair of neurons as well as whether\nthe coupling acts in an inhibiting or activating (excitatory) manner. The\nproposed model-independent reconstruction theory is scalable to larger networks\nand may thus play an important role in the reconstruction of networks from\nbiology to social science and engineering.\n", "versions": [{"version": "v1", "created": "Tue, 27 Mar 2018 09:15:04 GMT"}, {"version": "v2", "created": "Thu, 29 Mar 2018 12:56:06 GMT"}], "update_date": "2018-08-08", "authors_parsed": [["Casadiego", "Jose", ""], ["Maoutsa", "Dimitra", ""], ["Timme", "Marc", ""]]}, {"id": "1803.10205", "submitter": "Johanna Senk", "authors": "Johanna Senk, Corto Carde, Espen Hagen, Torsten W. Kuhlen, Markus\n  Diesmann, Benjamin Weyers", "title": "VIOLA - A multi-purpose and web-based visualization tool for\n  neuronal-network simulation output", "comments": "38 pages, 10 figures, 3 tables", "journal-ref": null, "doi": "10.3389/fninf.2018.00075", "report-no": null, "categories": "q-bio.NC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Neuronal network models and corresponding computer simulations are invaluable\ntools to aid the interpretation of the relationship between neuron properties,\nconnectivity and measured activity in cortical tissue. Spatiotemporal patterns\nof activity propagating across the cortical surface as observed experimentally\ncan for example be described by neuronal network models with layered geometry\nand distance-dependent connectivity. The interpretation of the resulting stream\nof multi-modal and multi-dimensional simulation data calls for integrating\ninteractive visualization steps into existing simulation-analysis workflows.\nHere, we present a set of interactive visualization concepts called views for\nthe visual analysis of activity data in topological network models, and a\ncorresponding reference implementation VIOLA (VIsualization Of Layer Activity).\nThe software is a lightweight, open-source, web-based and platform-independent\napplication combining and adapting modern interactive visualization paradigms,\nsuch as coordinated multiple views, for massively parallel neurophysiological\ndata. For a use-case demonstration we consider spiking activity data of a\ntwo-population, layered point-neuron network model subject to a spatially\nconfined excitation originating from an external population. With the multiple\ncoordinated views, an explorative and qualitative assessment of the\nspatiotemporal features of neuronal activity can be performed upfront of a\ndetailed quantitative data analysis of specific aspects of the data.\nFurthermore, ongoing efforts including the European Human Brain Project aim at\nproviding online user portals for integrated model development, simulation,\nanalysis and provenance tracking, wherein interactive visual analysis tools are\none component. Browser-compatible, web-technology based solutions are therefore\nrequired. Within this scope, with VIOLA we provide a first prototype.\n", "versions": [{"version": "v1", "created": "Tue, 27 Mar 2018 17:43:06 GMT"}], "update_date": "2019-09-23", "authors_parsed": [["Senk", "Johanna", ""], ["Carde", "Corto", ""], ["Hagen", "Espen", ""], ["Kuhlen", "Torsten W.", ""], ["Diesmann", "Markus", ""], ["Weyers", "Benjamin", ""]]}, {"id": "1803.10221", "submitter": "Mojtaba Aliakbarzadeh", "authors": "Mojtaba Aliakbarzadeh and Kirsty Kitto", "title": "Preparation and Measurement in Quantum Memory Models", "comments": "published in Journal of Mathematical Psychology", "journal-ref": null, "doi": "10.1016/j.jmp.2018.03.002", "report-no": null, "categories": "q-bio.NC quant-ph", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Quantum Cognition has delivered a number of models for semantic memory, but\nto date these have tended to assume pure states and projective measurement.\nHere we relax these assumptions. A quantum inspired model of human word\nassociation experiments will be extended using a density matrix representation\nof human memory and a POVM based upon non-ideal measurements. Our formulation\nallows for a consideration of key terms like measurement and contextuality\nwithin a rigorous modern approach. This approach both provides new conceptual\nadvances and suggests new experimental protocols.\n", "versions": [{"version": "v1", "created": "Mon, 26 Mar 2018 08:14:08 GMT"}], "update_date": "2018-03-29", "authors_parsed": [["Aliakbarzadeh", "Mojtaba", ""], ["Kitto", "Kirsty", ""]]}, {"id": "1803.10318", "submitter": "Gaurav Gupta", "authors": "Gaurav Gupta, Sergio Pequito, Paul Bogdan", "title": "Re-thinking EEG-based non-invasive brain interfaces: modeling and\n  analysis", "comments": "12 pages, 16 figures, ICCPS-18", "journal-ref": null, "doi": null, "report-no": null, "categories": "q-bio.NC cs.HC cs.SY", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Brain interfaces are cyber-physical systems that aim to harvest information\nfrom the (physical) brain through sensing mechanisms, extract information about\nthe underlying processes, and decide/actuate accordingly. Nonetheless, the\nbrain interfaces are still in their infancy, but reaching to their maturity\nquickly as several initiatives are released to push forward their development\n(e.g., NeuraLink by Elon Musk and `typing-by-brain' by Facebook). This has\nmotivated us to revisit the design of EEG-based non-invasive brain interfaces.\nSpecifically, current methodologies entail a highly skilled neuro-functional\napproach and evidence-based \\emph{a priori} knowledge about specific signal\nfeatures and their interpretation from a neuro-physiological point of view.\nHereafter, we propose to demystify such approaches, as we propose to leverage\nnew time-varying complex network models that equip us with a fractal dynamical\ncharacterization of the underlying processes. Subsequently, the parameters of\nthe proposed complex network models can be explained from a system's\nperspective, and, consecutively, used for classification using machine learning\nalgorithms and/or actuation laws determined using control system's theory.\nBesides, the proposed system identification methods and techniques have\ncomputational complexities comparable with those currently used in EEG-based\nbrain interfaces, which enable comparable online performances. Furthermore, we\nforesee that the proposed models and approaches are also valid using other\ninvasive and non-invasive technologies. Finally, we illustrate and\nexperimentally evaluate this approach on real EEG-datasets to assess and\nvalidate the proposed methodology. The classification accuracies are high even\non having less number of training samples.\n", "versions": [{"version": "v1", "created": "Tue, 27 Mar 2018 20:54:29 GMT"}], "update_date": "2018-03-29", "authors_parsed": [["Gupta", "Gaurav", ""], ["Pequito", "Sergio", ""], ["Bogdan", "Paul", ""]]}, {"id": "1803.10470", "submitter": "Jaan Aru", "authors": "Jaan Aru and Raul Vicente", "title": "What deep learning can tell us about higher cognitive functions like\n  mindreading?", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.LG q-bio.NC", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Can deep learning (DL) guide our understanding of computations happening in\nbiological brain? We will first briefly consider how DL has contributed to the\nresearch on visual object recognition. In the main part we will assess whether\nDL could also help us to clarify the computations underlying higher cognitive\nfunctions such as Theory of Mind. In addition, we will compare the objectives\nand learning signals of brains and machines, leading us to conclude that simply\nscaling up the current DL algorithms will most likely not lead to human level\nTheory of Mind.\n", "versions": [{"version": "v1", "created": "Wed, 28 Mar 2018 08:58:49 GMT"}, {"version": "v2", "created": "Thu, 4 Jul 2019 05:44:47 GMT"}], "update_date": "2019-07-05", "authors_parsed": [["Aru", "Jaan", ""], ["Vicente", "Raul", ""]]}, {"id": "1803.10597", "submitter": "Ru Zhang", "authors": "Ru Zhang and Yanjun Liu and James T Townsend", "title": "A Theoretical Study of Process Dependence for Critical Statistics in\n  Standard Serial Models and Standard Parallel Models", "comments": "arXiv admin note: substantial text overlap with arXiv:1712.00528", "journal-ref": null, "doi": null, "report-no": null, "categories": "q-bio.NC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Critical parts of the definitions of standard serial and standard parallel\nmodes refer to stochastic independence. Standard serial models are defined by\nstochastic independence and identical distributions of their processing times.\nProcessing times in the serial models are identical to the intercompletion time\nstatistics. Similarly, standard parallel models assume stochastically\nindependent and identical processing times. Their processing times are\nequivalent to the statistic known as total completion times. Little is known\nabout what standard serial models can predict for the total completion time or\nwhat standard parallel models can predict for the intercompletion times. Here\nwe demonstrate that standard serial models possess a tendency to predict a\npositive dependence for the total completion times with that always being true\nin the case of a single processing order. However, with mixtures of processing\norders, standard serial models may predict negative dependence of the total\ncompletion times. Comparably, standard parallel models typically predict\nneither independence of the intercompletion times nor identical distributions.\nIn fact, standard parallel models predict increasing intercompletion times as\nthe individual channels continue to finish. Nevertheless, dramatically\nincreasing hazard functions of the channels can defeat that tendency. And,\nstandard parallel models can predict intercompletion time independence but only\nwhen individual channel distributions are exponential. Finally, we use these\nand ancillary mathematical results to conclude that standard serial and\nstandard parallel models can never perfectly mimic one another. Therefore, our\nfindings set the stage for explicit model testing between these classes.\n", "versions": [{"version": "v1", "created": "Mon, 19 Mar 2018 07:02:09 GMT"}, {"version": "v2", "created": "Mon, 11 Feb 2019 20:22:00 GMT"}], "update_date": "2019-02-13", "authors_parsed": [["Zhang", "Ru", ""], ["Liu", "Yanjun", ""], ["Townsend", "James T", ""]]}, {"id": "1803.10753", "submitter": "Milena \\v{C}uki\\'c Dr", "authors": "Milena B. Cukic, Mirjana M. Platisa, Aleksandar Kalauzi, Joji Oommen,\n  Milos R. Ljubisavljevic", "title": "The comparison of Higuchi fractal dimension and Sample Entropy analysis\n  of sEMG: effects of muscle contraction intensity and TMS", "comments": "21 pages, 3 Figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "q-bio.NC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The aim of the study was to examine how the complexity of surface\nelectromyogram (sEMG) signal, estimated by Higuchi fractal dimension (HFD) and\nSample Entropy (SampEn), change depending on muscle contraction intensity and\nexternal perturbation of the corticospinal activity during muscle contraction\ninduced by single-pulse Transcranial Magnetic Stimulation (spTMS). HFD and\nSampEn were computed from sEMG signal recorded at three various levels of\nvoluntary contraction before and after spTMS. After spTMS, both HFD and SampEn\ndecreased at medium compared to the mild contraction. SampEn increased, while\nHFD did not change significantly at strong compared to medium contraction.\nspTMS significantly decreased both parameters at all contraction levels. When\nsame parameters were computed from the mathematically generated sine-wave\ncalibration curves, the results show that SampEn has better accuracy at lower\n(0-40 Hz) and HFD at higher (60-120 Hz) frequencies. Changes in the sEMG\ncomplexity associated with increased muscle contraction intensity cannot be\naccurately depicted by a single complexity measure. Examination of sEMG should\nentail both SampEn and HFD as they provide complementary information about\ndifferent frequency components of sEMG. Further studies are needed to explain\nthe implication of changes in nonlinear parameters and their relation to\nunderlying sEMG physiological processes.\n", "versions": [{"version": "v1", "created": "Wed, 28 Mar 2018 17:44:44 GMT"}], "update_date": "2018-03-29", "authors_parsed": [["Cukic", "Milena B.", ""], ["Platisa", "Mirjana M.", ""], ["Kalauzi", "Aleksandar", ""], ["Oommen", "Joji", ""], ["Ljubisavljevic", "Milos R.", ""]]}, {"id": "1803.11516", "submitter": "Aaron Chen", "authors": "Aaron Chen, Florian Frick, and Anne Shiu", "title": "Neural codes, decidability, and a new local obstruction to convexity", "comments": null, "journal-ref": "SIAM J. Appl. Algebra Geom. 3 (1), 44-66 (2019)", "doi": null, "report-no": null, "categories": "math.CO cs.CG q-bio.NC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Given an intersection pattern of arbitrary sets in Euclidean space, is there\nan arrangement of convex open sets in Euclidean space that exhibits the same\nintersections? This question is combinatorial and topological in nature, but is\nmotivated by neuroscience. Specifically, we are interested in a type of neuron\ncalled a place cell, which fires precisely when an organism is in a certain\nregion, usually convex, called a place field. The earlier question, therefore,\ncan be rephrased as follows: Which neural codes, that is, patterns of neural\nactivity, can arise from a collection of convex open sets? To address this\nquestion, Giusti and Itskov proved that convex neural codes have no \"local\nobstructions,\" which are defined via the topology of a code's simplicial\ncomplex. Codes without local obstructions are called locally good, because the\nobstruction precludes the code from encoding the intersections of open sets\nthat form a good cover. In other words, every good-cover code is locally good.\nHere we prove the converse: Every locally good code is a good-cover code. We\nalso prove that the good-cover decision problem is undecidable. Finally, we\nreveal a stronger type of local obstruction that prevents a code from being\nconvex, and prove that the corresponding decision problem is NP-hard. Our\nproofs use combinatorial and topological methods.\n", "versions": [{"version": "v1", "created": "Fri, 30 Mar 2018 15:35:29 GMT"}, {"version": "v2", "created": "Wed, 26 Sep 2018 05:25:24 GMT"}], "update_date": "2019-02-22", "authors_parsed": [["Chen", "Aaron", ""], ["Frick", "Florian", ""], ["Shiu", "Anne", ""]]}]