[{"id": "1505.00041", "submitter": "Joaquin Rapela", "authors": "Joaquin Rapela and Mark Kostuk and Peter F. Rowat and Tim Mullen and\n  Edward F. Chang and Kristofer Bouchard", "title": "Modeling neural activity at the ensemble level", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "q-bio.NC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Here we demonstrate that the activity of neural ensembles can be\nquantitatively modeled. We first show that an ensemble dynamical model (EDM)\naccurately approximates the distribution of voltages and average firing rate\nper neuron of a population of simulated integrate-and-fire neurons. EDMs are\nhigh-dimensional nonlinear dynamical models. To faciliate the estimation of\ntheir parameters we present a dimensionality reduction method and study its\nperformance with simulated data. We then introduce and evaluate a\nmaximum-likelihood method to estimate connectivity parameters in networks of\nEDMS. Finally, we show that this model an methods accurately approximate the\nhigh-gamma power evoked by pure tones in the auditory cortex of rodents.\nOverall, this article demonstrates that quantitatively modeling brain activity\nat the ensemble level is indeed possible, and opens the way to understanding\nthe computations performed by neural ensembles, which could revolutionarize our\nunderstanding of brain function.\n", "versions": [{"version": "v1", "created": "Thu, 30 Apr 2015 21:54:11 GMT"}, {"version": "v2", "created": "Fri, 22 May 2015 03:19:02 GMT"}, {"version": "v3", "created": "Thu, 3 Sep 2015 20:14:21 GMT"}], "update_date": "2015-09-07", "authors_parsed": [["Rapela", "Joaquin", ""], ["Kostuk", "Mark", ""], ["Rowat", "Peter F.", ""], ["Mullen", "Tim", ""], ["Chang", "Edward F.", ""], ["Bouchard", "Kristofer", ""]]}, {"id": "1505.00045", "submitter": "Karina Yaginuma Yuriko", "authors": "Karina Y. Yaginuma", "title": "A stochastic system with infinite interacting components to model the\n  time evolution of the membrane potentials of a population of neurons", "comments": "arXiv admin note: text overlap with arXiv:0904.1845 by other authors", "journal-ref": null, "doi": "10.1007/s10955-016-1490-3", "report-no": null, "categories": "stat.ME q-bio.NC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider a new class of interacting particle systems with a countable\nnumber of interacting components. The system represents the time evolution of\nthe membrane potentials of an infinite set of interacting neurons. We prove the\nexistence and uniqueness of the process, using a perfect simulation procedure.\nWe show that this algorithm is successful, that is, we show that the number of\nsteps of the algorithm is almost surely finite. We also construct a perfect\nsimulation procedure for the coupling of a process with a finite number of\nneurons and the process with a infinite number of neurons. As a consequence, we\nobtain an upper bound for the error that we make when sampling from a finite\nset of neurons instead of the infinite set of neurons.\n", "versions": [{"version": "v1", "created": "Thu, 30 Apr 2015 22:06:57 GMT"}], "update_date": "2016-03-23", "authors_parsed": [["Yaginuma", "Karina Y.", ""]]}, {"id": "1505.00351", "submitter": "Sarine Babikian", "authors": "Sarine Babikian, Francisco J. Valero-Cuevas, Eva Kanso", "title": "Slow Limb Movements Require Precise Control of Muscle Stiffness", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "q-bio.TO q-bio.NC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Slow and accurate finger and limb movements are essential to daily\nactivities, but their neural control and governing mechanics are relatively\nunexplored. We consider neuromechanical systems where slow movements are\nproduced by neural commands that modulate muscle stiffness. This formulation\nbased on strain-energy equilibria is in agreement with prior work on neural\ncontrol of muscle and limb impedance. Slow limb movements are driftless in the\nsense that movement stops when neural commands stop. We demonstrate, in the\ncontext of two planar tendon-driven systems representing a finger and a leg,\nthat the control of muscle stiffness suffices to produce stable and accurate\nlimb postures and quasi-static (slow) transitions among them. We prove,\nhowever, that stable postures are achievable only when muscles are\npre-tensioned, as is the case for natural muscle tone. Our results further\nindicate, in accordance with experimental findings, that slow movements are\nnon-smooth. The non-smoothness arises because the precision with which\nindividual muscle stiffnesses need to be controlled changes substantially\nthroughout the limb's motion. These results underscore the fundamental roles of\nmuscle tone and accurate neural control of muscle stiffness in producing stable\nlimb postures and slow movements.\n", "versions": [{"version": "v1", "created": "Sat, 2 May 2015 16:32:34 GMT"}, {"version": "v2", "created": "Tue, 5 May 2015 17:18:39 GMT"}], "update_date": "2015-05-06", "authors_parsed": [["Babikian", "Sarine", ""], ["Valero-Cuevas", "Francisco J.", ""], ["Kanso", "Eva", ""]]}, {"id": "1505.00655", "submitter": "Maurizio De Pitta'", "authors": "Maurizio De Pitt\\`a, Nicolas Brunel and Andrea Volterra", "title": "Astrocytes: orchestrating synaptic plasticity?", "comments": "63 pages, 4 figures", "journal-ref": null, "doi": "10.1016/j.neuroscience.2015.04.001", "report-no": null, "categories": "q-bio.NC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Synaptic plasticity is the capacity of a preexisting connection between two\nneurons to change in strength as a function of neural activity. Because\nsynaptic plasticity is the major candidate mechanism for learning and memory,\nthe elucidation of its constituting mechanisms is of crucial importance in many\naspects of normal and pathological brain function. In particular, a prominent\naspect that remains debated is how the plasticity mechanisms, that encompass a\nbroad spectrum of temporal and spatial scales, come to play together in a\nconcerted fashion. Here we review and discuss evidence that pinpoints to a\npossible non-neuronal, glial candidate for such orchestration: the regulation\nof synaptic plasticity by astrocytes.\n", "versions": [{"version": "v1", "created": "Mon, 4 May 2015 14:32:54 GMT"}], "update_date": "2015-05-05", "authors_parsed": [["De Pitt\u00e0", "Maurizio", ""], ["Brunel", "Nicolas", ""], ["Volterra", "Andrea", ""]]}, {"id": "1505.00774", "submitter": "Vishal Sahni Dr", "authors": "Dayal Pyari Srivastava, Vishal Sahni, Prem Saran Satsangi", "title": "Modelling Microtubules in the Brain as n-qudit Quantum Hopfield Network\n  and Beyond", "comments": null, "journal-ref": "International Journal of General Systems, Vol. 45, Issue 1, 2016", "doi": "10.1080/03081079.2015.1076405", "report-no": null, "categories": "q-bio.NC quant-ph", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The scientific approach to understand the nature of consciousness revolves\naround the study of human brain. Neurobiological studies that compare the\nnervous system of different species have accorded highest place to the humans\non account of various factors that include a highly developed cortical area\ncomprising of approximately 100 billion neurons, that are intrinsically\nconnected to form a highly complex network. Quantum theories of consciousness\nare based on mathematical abstraction and Penrose-Hameroff Orch-OR Theory is\none of the most promising ones. Inspired by Penrose-Hameroff Orch-OR Theory,\nBehrman et. al. (Behrman, 2006) have simulated a quantum Hopfield neural\nnetwork with the structure of a microtubule. They have used an extremely\nsimplified model of the tubulin dimers with each dimer represented simply as a\nqubit, a single quantum two-state system. The extension of this model to\nn-dimensional quantum states, or n-qudits presented in this work holds\nconsiderable promise for even higher mathematical abstraction in modelling\nconsciousness systems.\n", "versions": [{"version": "v1", "created": "Sat, 2 May 2015 11:54:31 GMT"}], "update_date": "2017-03-28", "authors_parsed": [["Srivastava", "Dayal Pyari", ""], ["Sahni", "Vishal", ""], ["Satsangi", "Prem Saran", ""]]}, {"id": "1505.00775", "submitter": "Danko Nikolic", "authors": "Danko Nikoli\\'c", "title": "Only T3-AI can reach human-level intelligence: A variety argument", "comments": "18 page, 6800 words, no figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "q-bio.NC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The recently introduced theory of practopoiesis offers an account on how\nadaptive intelligent systems are organized. According to that theory biological\nagents adapt at three levels of organization and this structure applies also to\nour brains. This is referred to as tri-traversal theory of the organization of\nmind or for short, a T3-structure. To implement a similar T3-organization in an\nartificially intelligent agent, it is necessary to have multiple policies, as\nusually used as a concept in the theory of reinforcement learning. These\npolicies have to form a hierarchy. We define adaptive practopoietic systems in\nterms of hierarchy of policies and calculate whether the total variety of\nbehavior required by real-life conditions of an adult human can be\nsatisfactorily accounted for by a traditional approach to artificial\nintelligence based on T2-agents, or whether a T3-agent is needed instead. We\nconclude that the complexity of real life can be dealt with appropriately only\nby a T3-agent.\n", "versions": [{"version": "v1", "created": "Sat, 2 May 2015 12:58:02 GMT"}, {"version": "v2", "created": "Thu, 16 Jul 2015 09:29:50 GMT"}], "update_date": "2015-07-17", "authors_parsed": [["Nikoli\u0107", "Danko", ""]]}, {"id": "1505.00835", "submitter": "Georg Martius", "authors": "Ralf Der and Georg Martius", "title": "A novel plasticity rule can explain the development of sensorimotor\n  intelligence", "comments": "18 pages, 5 figures, 7 videos", "journal-ref": "PNAS November 10, 2015 vol. 112 no. 45 E6224-E6232", "doi": "10.1073/pnas.1508400112", "report-no": null, "categories": "cs.RO cs.LG q-bio.NC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Grounding autonomous behavior in the nervous system is a fundamental\nchallenge for neuroscience. In particular, the self-organized behavioral\ndevelopment provides more questions than answers. Are there special functional\nunits for curiosity, motivation, and creativity? This paper argues that these\nfeatures can be grounded in synaptic plasticity itself, without requiring any\nhigher level constructs. We propose differential extrinsic plasticity (DEP) as\na new synaptic rule for self-learning systems and apply it to a number of\ncomplex robotic systems as a test case. Without specifying any purpose or goal,\nseemingly purposeful and adaptive behavior is developed, displaying a certain\nlevel of sensorimotor intelligence. These surprising results require no system\nspecific modifications of the DEP rule but arise rather from the underlying\nmechanism of spontaneous symmetry breaking due to the tight\nbrain-body-environment coupling. The new synaptic rule is biologically\nplausible and it would be an interesting target for a neurobiolocal\ninvestigation. We also argue that this neuronal mechanism may have been a\ncatalyst in natural evolution.\n", "versions": [{"version": "v1", "created": "Mon, 4 May 2015 22:48:25 GMT"}], "update_date": "2016-06-16", "authors_parsed": [["Der", "Ralf", ""], ["Martius", "Georg", ""]]}, {"id": "1505.01228", "submitter": "Rogerio Normand", "authors": "Rogerio Normand and Hugo Alexandre Ferreira", "title": "Superchords: the atoms of thought", "comments": "5 pages, 3 figures with left/right images", "journal-ref": null, "doi": null, "report-no": null, "categories": "q-bio.NC q-bio.QM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Electroencephalography (EEG) signals' interpretation is based on waveform\nanalysis, where meaningful information should emerge from a plethora of data.\nNonetheless, the continuous increase in computational power and the development\nof new data processing algorithms in the recent years have put into reach the\npossibility of analysing raw EEG signals. Bearing that motivation, the authors\npropose a new approach using raw data EEG signals and deep learning neural\nnetworks, for the classification of motor activities (executed and imagery).\nThe hypothesis to be presented here is: each instantaneous measurement of the\nraw signal of all EEG channels (superchord) is unique per motor activity\nregardless the moment of measurement. This study has confirmed the hypothesis\n(results with accuracy over 80%, mean for 109 subjects), reinforcing the need\nof further research for the understanding of mental processes.\n", "versions": [{"version": "v1", "created": "Wed, 6 May 2015 00:24:39 GMT"}, {"version": "v2", "created": "Thu, 7 May 2015 02:41:07 GMT"}], "update_date": "2015-05-08", "authors_parsed": [["Normand", "Rogerio", ""], ["Ferreira", "Hugo Alexandre", ""]]}, {"id": "1505.02142", "submitter": "Sebastian Billaudelle", "authors": "Sebastian Billaudelle, Subutai Ahmad", "title": "Porting HTM Models to the Heidelberg Neuromorphic Computing Platform", "comments": "10 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "q-bio.NC cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Hierarchical Temporal Memory (HTM) is a computational theory of machine\nintelligence based on a detailed study of the neocortex. The Heidelberg\nNeuromorphic Computing Platform, developed as part of the Human Brain Project\n(HBP), is a mixed-signal (analog and digital) large-scale platform for modeling\nnetworks of spiking neurons. In this paper we present the first effort in\nporting HTM networks to this platform. We describe a framework for simulating\nkey HTM operations using spiking network models. We then describe specific\nspatial pooling and temporal memory implementations, as well as simulations\ndemonstrating that the fundamental properties are maintained. We discuss issues\nin implementing the full set of plasticity rules using Spike-Timing Dependent\nPlasticity (STDP), and rough place and route calculations. Although further\nwork is required, our initial studies indicate that it should be possible to\nrun large-scale HTM networks (including plasticity rules) efficiently on the\nHeidelberg platform. More generally the exercise of porting high level HTM\nalgorithms to biophysical neuron models promises to be a fruitful area of\ninvestigation for future studies.\n", "versions": [{"version": "v1", "created": "Fri, 8 May 2015 19:18:07 GMT"}, {"version": "v2", "created": "Tue, 9 Feb 2016 13:23:40 GMT"}], "update_date": "2016-02-10", "authors_parsed": [["Billaudelle", "Sebastian", ""], ["Ahmad", "Subutai", ""]]}, {"id": "1505.02194", "submitter": "Sarah Muldoon", "authors": "Sarah Feldt Muldoon, Eric W. Bridgeford, and Danielle S. Bassett", "title": "Small-World Propensity in Weighted, Real-World Networks", "comments": "Manuscript and SI; 13 pages, 7 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "q-bio.NC physics.data-an", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Quantitative descriptions of network structure in big data can provide\nfundamental insights into the function of interconnected complex systems.\nSmall-world structure, commonly diagnosed by high local clustering yet short\naverage path length between any two nodes, directly enables information flow in\ncoupled systems, a key function that can differ across conditions or between\ngroups. However, current techniques to quantify small-world structure are\ndependent on nuisance variables such as density and agnostic to critical\nvariables such as the strengths of connections between nodes, thereby hampering\naccurate and comparable assessments of small-world structure in different\nnetworks. Here, we address both limitations with a novel metric called the\nSmall-World Propensity (SWP). In its binary instantiation, the SWP provides an\nunbiased assessment of small-world structure in networks of varying densities.\nWe extend this concept to the case of weighted networks by developing (i) a\nstandardized procedure for generating weighted small-world networks, (ii) a\nweighted extension of the SWP, and (iii) a stringent and generalizable method\nfor mapping real-world data onto the theoretical model. In applying these\ntechniques to real world brain networks, we uncover the surprising fact that\nthe canonical example of a biological small-world network, the C. elegans\nneuronal network, has strikingly low SWP in comparison to other examined brain\nnetworks. These metrics, models, and maps form a coherent toolbox for the\nassessment of architectural properties in real-world networks and their\nstatistical comparison across conditions.\n", "versions": [{"version": "v1", "created": "Fri, 8 May 2015 21:53:05 GMT"}], "update_date": "2015-05-12", "authors_parsed": [["Muldoon", "Sarah Feldt", ""], ["Bridgeford", "Eric W.", ""], ["Bassett", "Danielle S.", ""]]}, {"id": "1505.02963", "submitter": "Daniele De Martino", "authors": "Daniele De Martino", "title": "The dual of the space of interactions in neural network models", "comments": "13 pages, 4 figures, 1 table", "journal-ref": null, "doi": "10.1142/S0129183116500674", "report-no": null, "categories": "cond-mat.dis-nn cond-mat.stat-mech q-bio.NC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this work the Gardner problem of inferring interactions and fields for an\nIsing neural network from given patterns under a local stability hypothesis is\naddressed under a dual perspective. By means of duality arguments an integer\nlinear system is defined whose solution space is the dual of the Gardner space\nand whose solutions represent mutually unstable patterns. We propose and\ndiscuss Monte Carlo methods in order to find and remove unstable patterns and\nuniformly sample the space of interactions thereafter. We illustrate the\nproblem on a set of real data and perform ensemble calculation that shows how\nthe emergence of phase dominated by unstable patterns can be triggered in a\nnon-linear discontinuous way.\n", "versions": [{"version": "v1", "created": "Tue, 12 May 2015 11:27:26 GMT"}, {"version": "v2", "created": "Tue, 27 Oct 2015 09:08:41 GMT"}], "update_date": "2016-05-25", "authors_parsed": [["De Martino", "Daniele", ""]]}, {"id": "1505.03015", "submitter": "Pier Stanislao Paolucci", "authors": "Pier Stanislao Paolucci, Roberto Ammendola, Andrea Biagioni, Ottorino\n  Frezza, Francesca Lo Cicero, Alessandro Lonardo, Michele Martinelli, Elena\n  Pastorelli, Francesco Simula, Piero Vicini", "title": "Power, Energy and Speed of Embedded and Server Multi-Cores applied to\n  Distributed Simulation of Spiking Neural Networks: ARM in NVIDIA Tegra vs\n  Intel Xeon quad-cores", "comments": "4 pages, 1 table", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC q-bio.NC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This short note regards a comparison of instantaneous power, total energy\nconsumption, execution time and energetic cost per synaptic event of a spiking\nneural network simulator (DPSNN-STDP) distributed on MPI processes when\nexecuted either on an embedded platform (based on a dual socket quad-core ARM\nplatform) or a server platform (INTEL-based quad-core dual socket platform). We\nalso compare the measure with those reported by leading custom and semi-custom\ndesigns: TrueNorth and SpiNNaker. In summary, we observed that: 1- we spent 2.2\nmicro-Joule per simulated event on the \"embedded platform\", approx. 4.4 times\nlower than what was spent by the \"server platform\"; 2- the instantaneous power\nconsumption of the \"embedded platform\" was 14.4 times better than the \"server\"\none; 3- the server platform is a factor 3.3 faster. The \"embedded platform\" is\nmade of NVIDIA Jetson TK1 boards, interconnected by Ethernet, each mounting a\nTegra K1 chip including a quad-core ARM Cortex-A15 at 2.3GHz. The \"server\nplatform\" is based on dual-socket quad-core Intel Xeon CPUs (E5620 at 2.4GHz).\nThe measures were obtained with the DPSNN-STDP simulator (Distributed Simulator\nof Polychronous Spiking Neural Network with synaptic Spike Timing Dependent\nPlasticity) developed by INFN, that already proved its efficient scalability\nand execution speed-up on hundreds of similar \"server\" cores and MPI processes,\napplied to neural nets composed of several billions of synapses.\n", "versions": [{"version": "v1", "created": "Tue, 12 May 2015 14:14:15 GMT"}], "update_date": "2015-05-13", "authors_parsed": [["Paolucci", "Pier Stanislao", ""], ["Ammendola", "Roberto", ""], ["Biagioni", "Andrea", ""], ["Frezza", "Ottorino", ""], ["Cicero", "Francesca Lo", ""], ["Lonardo", "Alessandro", ""], ["Martinelli", "Michele", ""], ["Pastorelli", "Elena", ""], ["Simula", "Francesco", ""], ["Vicini", "Piero", ""]]}, {"id": "1505.03176", "submitter": "Cheng Ly", "authors": "Cheng Ly", "title": "Firing Rate Dynamics in Recurrent Spiking Neural Networks with Intrinsic\n  and Network Heterogeneity", "comments": "21 pages, 5 figures", "journal-ref": null, "doi": "10.1007/s10827-015-0578-0", "report-no": null, "categories": "q-bio.NC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Heterogeneity of neural attributes has recently gained a lot of attention and\nis increasing recognized as a crucial feature in neural processing. Despite its\nimportance, this physiological feature has traditionally been neglected in\ntheoretical studies of cortical neural networks. Thus, there is still a lot\nunknown about the consequences of cellular and circuit heterogeneity in spiking\nneural networks. In particular, combining network or synaptic heterogeneity and\nintrinsic heterogeneity has yet to be considered systematically despite the\nfact that both are known to exist and likely have significant roles in neural\nnetwork dynamics. In a canonical recurrent spiking neural network model, we\nstudy how these two forms of heterogeneity lead to different distributions of\nexcitatory firing rates. To analytically characterize how these types of\nheterogeneities affect the network, we employ a dimension reduction method that\nrelies on a combination of Monte Carlo simulations and probability density\nfunction equations. We find that the relationship between intrinsic and network\nheterogeneity has a strong effect on the overall level of heterogeneity of the\nfiring rates. Specifically, this relationship can lead to amplification or\nattenuation of firing rate heterogeneity, and these effects depend on whether\nthe recurrent network is firing asynchronously or rhythmically firing. These\nobservations are captured with the aforementioned reduction method, and\nfurthermore simpler analytic descriptions based on this dimension reduction\nmethod are developed. The final analytic descriptions provide compact and\ndescriptive formulas for how the relationship between intrinsic and network\nheterogeneity determines the firing rate heterogeneity dynamics in various\nsettings.\n", "versions": [{"version": "v1", "created": "Tue, 12 May 2015 21:49:36 GMT"}, {"version": "v2", "created": "Mon, 21 Nov 2016 16:22:35 GMT"}], "update_date": "2016-11-22", "authors_parsed": [["Ly", "Cheng", ""]]}, {"id": "1505.03242", "submitter": "Bo Deng", "authors": "Bo Deng", "title": "Mechanistic Model to Replace Hodgkin-Huxley Equations", "comments": "23 pages, 10 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "q-bio.NC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper we construct a mathematical model for excitable membranes by\nintroducing circuit characteristics for ion pump, ion current activation, and\nvoltage-gating. The model is capable of reestablishing the Nernst resting\npotentials, all-or-nothing action potentials, absolute refraction, anode break\nexcitation, and spike bursts. We propose to replace the Hodgkin-Huxley model by\nour model as the basis template for neurons and excitable membranes.\n", "versions": [{"version": "v1", "created": "Wed, 13 May 2015 04:52:43 GMT"}], "update_date": "2015-05-14", "authors_parsed": [["Deng", "Bo", ""]]}, {"id": "1505.03342", "submitter": "Drew Fudenberg", "authors": "Drew Fudenberg, Philipp Strack, and Tomasz Strzalecki", "title": "Stochastic Choice and Optimal Sequential Sampling", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "q-bio.NC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We model the joint distribution of choice probabilities and decision times in\nbinary choice tasks as the solution to a problem of optimal sequential\nsampling, where the agent is uncertain of the utility of each action and pays a\nconstant cost per unit time for gathering information. In the resulting optimal\npolicy, the agent's choices are more likely to be correct when the agent\nchooses to decide quickly, provided that the agent's prior beliefs are correct.\nFor this reason it better matches the observed correlation between decision\ntime and choice probability than does the classical drift-diffusion model,\nwhere the agent is uncertain which of two actions is best but knows the utility\ndifference between them\n", "versions": [{"version": "v1", "created": "Wed, 13 May 2015 11:54:11 GMT"}], "update_date": "2015-05-14", "authors_parsed": [["Fudenberg", "Drew", ""], ["Strack", "Philipp", ""], ["Strzalecki", "Tomasz", ""]]}, {"id": "1505.03560", "submitter": "Sebastiano Stramaglia", "authors": "Ibai Diez, Asier Erramuzpe, Inaki Escudero, Beatriz Mateos, Alberto\n  Cabrera, Daniele Marinazzo, Ernesto J. Sanz-Arigita, Sebastiano Stramaglia\n  and Jesus M. Cortes", "title": "Information flow between resting state networks", "comments": "47 pages, 5 figures, 4 tables, 3 supplementary figures. Accepted for\n  publication in Brain Connectivity in its current form", "journal-ref": null, "doi": null, "report-no": null, "categories": "q-bio.NC physics.data-an q-bio.QM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The resting brain dynamics self-organizes into a finite number of correlated\npatterns known as resting state networks (RSNs). It is well known that\ntechniques like independent component analysis can separate the brain activity\nat rest to provide such RSNs, but the specific pattern of interaction between\nRSNs is not yet fully understood. To this aim, we propose here a novel method\nto compute the information flow (IF) between different RSNs from resting state\nmagnetic resonance imaging. After haemodynamic response function blind\ndeconvolution of all voxel signals, and under the hypothesis that RSNs define\nregions of interest, our method first uses principal component analysis to\nreduce dimensionality in each RSN to next compute IF (estimated here in terms\nof Transfer Entropy) between the different RSNs by systematically increasing k\n(the number of principal components used in the calculation). When k = 1, this\nmethod is equivalent to computing IF using the average of all voxel activities\nin each RSN. For k greater than one our method calculates the k-multivariate IF\nbetween the different RSNs. We find that the average IF among RSNs is\ndimension-dependent, increasing from k =1 (i.e., the average voxels activity)\nup to a maximum occurring at k =5 to finally decay to zero for k greater than\n10. This suggests that a small number of components (close to 5) is sufficient\nto describe the IF pattern between RSNs. Our method - addressing differences in\nIF between RSNs for any generic data - can be used for group comparison in\nhealth or disease. To illustrate this, we have calculated the interRSNs IF in a\ndataset of Alzheimer's Disease (AD) to find that the most significant\ndifferences between AD and controls occurred for k =2, in addition to AD\nshowing increased IF w.r.t. controls.\n", "versions": [{"version": "v1", "created": "Wed, 13 May 2015 21:45:44 GMT"}], "update_date": "2015-05-15", "authors_parsed": [["Diez", "Ibai", ""], ["Erramuzpe", "Asier", ""], ["Escudero", "Inaki", ""], ["Mateos", "Beatriz", ""], ["Cabrera", "Alberto", ""], ["Marinazzo", "Daniele", ""], ["Sanz-Arigita", "Ernesto J.", ""], ["Stramaglia", "Sebastiano", ""], ["Cortes", "Jesus M.", ""]]}, {"id": "1505.03711", "submitter": "Yedidyah Dordek", "authors": "Yedidyah Dordek, Ron Meir, Dori Derdikman", "title": "Extracting grid characteristics from spatially distributed place cell\n  inputs using non-negative PCA", "comments": "35 pages, 14 figures, 1 table", "journal-ref": null, "doi": null, "report-no": null, "categories": "q-bio.NC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Many recent models study the downstream projection from grid cells to place\ncells, while recent data has pointed out the importance of the feedback\nprojection. We thus asked how grid cells are affected by the nature of the\ninput from the place cells. We propose a two-layered neural network with\nfeedforward weights connecting place-like input cells to grid cell outputs.\nPlace-to-grid weights were learned via a generalized Hebbian rule. The\narchitecture of this network highly resembles neural networks used to perform\nPrincipal Component Analysis (PCA). Our results indicate that if the components\nof the feedforward neural network were non-negative, the output converged to a\nhexagonal lattice. Without the non-negativity constraint the output converged\nto a square lattice. Consistent with experiments, grid alignment to walls was\n~7{\\deg} and grid spacing ratio between consecutive modules was ~1.4. Our\nresults express a possible linkage between place-cell to grid-cell interactions\nand PCA, suggesting that grid cells represent a process of constrained\ndimensionality reduction that can be viewed also as a process of variance\nmaximization of the information from place-cells.\n", "versions": [{"version": "v1", "created": "Thu, 14 May 2015 13:08:37 GMT"}, {"version": "v2", "created": "Fri, 19 Jun 2015 15:29:14 GMT"}, {"version": "v3", "created": "Tue, 14 Jul 2015 21:26:27 GMT"}], "update_date": "2015-07-16", "authors_parsed": [["Dordek", "Yedidyah", ""], ["Meir", "Ron", ""], ["Derdikman", "Dori", ""]]}, {"id": "1505.03964", "submitter": "Gabriel Silva", "authors": "Marius Buibas and Gabriel A. Silva", "title": "Algebraic identification of the effective connectivity of constrained\n  geometric network models of neural signaling", "comments": "9 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "q-bio.NC physics.data-an", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Cellular neural circuit and networks consisting of interconnected neurons and\nglia are ulti- mately responsible for the information processing associated\nwith information processing in the brain. While there are major efforts aimed\nat mapping the structural and (electro)physiological connectivity of brain\nnetworks, such as the White House BRAIN Initiative aimed at the devel- opment\nof neurotechnologies capable of high density neural recordings, theoretical and\ncompu- tational methods for analyzing and making sense of all this data seem to\nbe further behind. Here, we propose and provide a summary of an approach for\ncalculating effective connectivity from experimental observations of neuronal\nnetwork activity. The proposed method operates on network-level data, makes use\nof all relevant prior knowledge, such as dynamical models of individual cells\nin the network and the physical structural connectivity of the network, and is\nbroadly applicable to large classes of biological and non-biological networks.\n", "versions": [{"version": "v1", "created": "Fri, 15 May 2015 05:53:45 GMT"}], "update_date": "2015-05-18", "authors_parsed": [["Buibas", "Marius", ""], ["Silva", "Gabriel A.", ""]]}, {"id": "1505.04010", "submitter": "Rodrigo Echeveste", "authors": "Rodrigo Echeveste and Claudius Gros", "title": "An objective function for self-limiting neural plasticity rules", "comments": "To appear in the Proceedings of ESANN 2015", "journal-ref": null, "doi": null, "report-no": null, "categories": "q-bio.NC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Self-organization provides a framework for the study of systems in which\ncomplex patterns emerge from simple rules, without the guidance of external\nagents or fine tuning of parameters. Within this framework, one can formulate a\nguiding principle for plasticity in the context of unsupervised learning, in\nterms of an objective function. In this work we derive Hebbian, self-limiting\nsynaptic plasticity rules from such an objective function and then apply the\nrules to the non-linear bars problem.\n", "versions": [{"version": "v1", "created": "Fri, 15 May 2015 10:13:23 GMT"}], "update_date": "2015-05-18", "authors_parsed": [["Echeveste", "Rodrigo", ""], ["Gros", "Claudius", ""]]}, {"id": "1505.04195", "submitter": "Zachary Kilpatrick PhD", "authors": "Alan Veliz-Cuba, Zachary P. Kilpatrick, and Kresimir Josic", "title": "Stochastic models of evidence accumulation in changing environments", "comments": "26 pages, 7 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "q-bio.NC math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Organisms and ecological groups accumulate evidence to make decisions.\nClassic experiments and theoretical studies have explored this process when the\ncorrect choice is fixed during each trial. However, we live in a constantly\nchanging world. What effect does such impermanence have on classical results\nabout decision making? To address this question we use sequential analysis to\nderive a tractable model of evidence accumulation when the correct option\nchanges in time. Our analysis shows that ideal observers discount prior\nevidence at a rate determined by the volatility of the environment, and the\ndynamics of evidence accumulation is governed by the information gained over an\naverage environmental epoch. A plausible neural implementation of an optimal\nobserver in a changing environment shows that, in contrast to previous models,\nneural populations representing alternate choices are coupled through\nexcitation. Our work builds a bridge between statistical decision making in\nvolatile environments and stochastic nonlinear dynamics.\n", "versions": [{"version": "v1", "created": "Fri, 15 May 2015 20:07:28 GMT"}, {"version": "v2", "created": "Sat, 13 Jun 2015 17:38:03 GMT"}, {"version": "v3", "created": "Tue, 8 Sep 2015 18:41:42 GMT"}, {"version": "v4", "created": "Wed, 30 Sep 2015 14:40:45 GMT"}], "update_date": "2015-10-01", "authors_parsed": [["Veliz-Cuba", "Alan", ""], ["Kilpatrick", "Zachary P.", ""], ["Josic", "Kresimir", ""]]}, {"id": "1505.04368", "submitter": "Masafumi Oizumi", "authors": "Masafumi Oizumi, Shun-ichi Amari, Toru Yanagawa, Naotaka Fujii,\n  Naotsugu Tsuchiya", "title": "Measuring integrated information from the decoding perspective", "comments": null, "journal-ref": "PLoS Comput Biol 12(1), e1004654, 2016", "doi": "10.1371/journal.pcbi.1004654", "report-no": null, "categories": "q-bio.NC cs.IT math.IT", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Accumulating evidence indicates that the capacity to integrate information in\nthe brain is a prerequisite for consciousness. Integrated Information Theory\n(IIT) of consciousness provides a mathematical approach to quantifying the\ninformation integrated in a system, called integrated information, $\\Phi$.\nIntegrated information is defined theoretically as the amount of information a\nsystem generates as a whole, above and beyond the sum of the amount of\ninformation its parts independently generate. IIT predicts that the amount of\nintegrated information in the brain should reflect levels of consciousness.\nEmpirical evaluation of this theory requires computing integrated information\nfrom neural data acquired from experiments, although difficulties with using\nthe original measure $\\Phi$ precludes such computations. Although some\npractical measures have been previously proposed, we found that these measures\nfail to satisfy the theoretical requirements as a measure of integrated\ninformation. Measures of integrated information should satisfy the lower and\nupper bounds as follows: The lower bound of integrated information should be 0\nwhen the system does not generate information (no information) or when the\nsystem comprises independent parts (no integration). The upper bound of\nintegrated information is the amount of information generated by the whole\nsystem and is realized when the amount of information generated independently\nby its parts equals to 0. Here we derive the novel practical measure $\\Phi^*$\nby introducing a concept of mismatched decoding developed from information\ntheory. We show that $\\Phi^*$ is properly bounded from below and above, as\nrequired, as a measure of integrated information. We derive the analytical\nexpression $\\Phi^*$ under the Gaussian assumption, which makes it readily\napplicable to experimental data.\n", "versions": [{"version": "v1", "created": "Sun, 17 May 2015 08:00:35 GMT"}], "update_date": "2016-02-22", "authors_parsed": [["Oizumi", "Masafumi", ""], ["Amari", "Shun-ichi", ""], ["Yanagawa", "Toru", ""], ["Fujii", "Naotaka", ""], ["Tsuchiya", "Naotsugu", ""]]}, {"id": "1505.04397", "submitter": "Dominique Vuillaume", "authors": "Selina La Barbera, Dominique Vuillaume, and Fabien Alibart", "title": "Filamentary Switching: Synaptic Plasticity through Device Volatility", "comments": null, "journal-ref": "ACS Nano, 9, 941-949 (2015)", "doi": "10.1021/nn506735m", "report-no": null, "categories": "physics.bio-ph cond-mat.mes-hall q-bio.NC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Replicating the computational functionalities and performances of the brain\nremains one of the biggest challenges for the future of information and\ncommunication technologies. Such an ambitious goal requires research efforts\nfrom the architecture level to the basic device level (i.e., investigating the\nopportunities offered by emerging nanotechnologies to build such systems).\nNanodevices, or, more precisely, memory or memristive devices, have been\nproposed for the implementation of synaptic functions, offering the required\nfeatures and integration in a single component. In this paper, we demonstrate\nthat the basic physics involved in the filamentary switching of electrochemical\nmetallization cells can reproduce important biological synaptic functions that\nare key mechanisms for information processing and storage. The transition from\nshort- to long-term plasticity has been reported as a direct consequence of\nfilament growth (i.e., increased conductance) in filamentary memory devices. In\nthis paper, we show that a more complex filament shape, such as dendritic paths\nof variable density and width, can permit the short- and long-term processes to\nbe controlled independently. Our solid-state device is strongly analogous to\nbiological synapses, as indicated by the interpretation of the results from the\nframework of a phenomenological model developed for biological synapses. We\ndescribe a single memristive element containing a rich panel of features, which\nwill be of benefit to future neuromorphic hardware systems.\n", "versions": [{"version": "v1", "created": "Sun, 17 May 2015 13:57:42 GMT"}], "update_date": "2015-05-20", "authors_parsed": [["La Barbera", "Selina", ""], ["Vuillaume", "Dominique", ""], ["Alibart", "Fabien", ""]]}, {"id": "1505.04432", "submitter": "Harold P. de Vladar", "authors": "Harold P. de Vladar and E\\\"ors Szathm\\'ary", "title": "Coupled Hebbian learning and evolutionary dynamics in a formal model for\n  structural synaptic plasticity", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "q-bio.NC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Theoretical models of neuronal function consider different mechanisms through\nwhich networks learn, classify and discern inputs. A central focus of these\nmodels is to understand how associations are established amongst neurons, in\norder to predict spiking patterns that are compatible with empirical\nobservations. Although these models have led to major insights and advances,\nthey still do not account for the astonishing velocity with which the brain\nsolves certain problems and what lies behind its creativity, amongst others\nfeatures. We examine two important components that may crucially aid\ncomprehensive understanding of said neurodynamical processes. First, we argue\nthat once presented with a problem, different putative solutions are generated\nin parallel by different groups or local neuronal complexes, with the\nsubsequent stabilization and spread of the best solutions. Using mathematical\nmodels we show that this mechanism accelerates finding the right solutions.\nThis formalism is analogous to standard replicator-mutator models of evolution\nwhere mutation is analogous to the probability of neuron state switching\n(on/off). The second factor that we incorporate is structural synaptic\nplasticity, i.e. the making of new and disbanding of old synapses, which we\napply as a dynamical reorganization of synaptic connections. We show that\nHebbian learning alone does not suffice to reach optimal solutions. However,\ncombining it with parallel evaluation and structural plasticity opens up\npossibilities for efficient problem solving. In the resulting networks,\ntopologies converge to subsets of fully connected components. Imposing costs on\nsynapses reduces the connectivity, although the number of connected components\nremains robust. The average lifetime of synapses is longer for connections that\nare established early, and diminishes with synaptic cost.\n", "versions": [{"version": "v1", "created": "Sun, 17 May 2015 18:34:12 GMT"}], "update_date": "2015-05-19", "authors_parsed": [["de Vladar", "Harold P.", ""], ["Szathm\u00e1ry", "E\u00f6rs", ""]]}, {"id": "1505.04544", "submitter": "Laurence Aitchison", "authors": "Laurence Aitchison and Peter E. Latham", "title": "Synaptic sampling: A connection between PSP variability and uncertainty\n  explains neurophysiological observations", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "q-bio.NC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  When an action potential is transmitted to a postsynaptic neuron, a small\nchange in the postsynaptic neuron's membrane potential occurs. These small\nchanges, known as a postsynaptic potentials (PSPs), are highly variable, and\ncurrent models assume that this variability is corrupting noise. In contrast,\nwe show that this variability could have an important computational role:\nrepresenting a synapse's uncertainty about the optimal synaptic weight (i.e.\nthe best possible setting for the synaptic weight). We show that this link\nbetween uncertainty and variability, that we call synaptic sampling, leads to\nmore accurate estimates of the uncertainty in task relevant quantities, leading\nto more effective decision making. Synaptic sampling makes four predictions,\nall of which have some experimental support. First the more variable a synapse\nis, the more it should change during LTP protocols. Second, variability should\nincrease as the presynpatic firing rate falls. Third, PSP variance should be\nproportional to PSP mean. Fourth, variability should increase with distance\nfrom the cell soma. We provide support for the first two predictions by\nreanalysing existing datasets, and we find preexisting data in support of the\nlast two predictions.\n", "versions": [{"version": "v1", "created": "Mon, 18 May 2015 08:10:09 GMT"}, {"version": "v2", "created": "Sun, 12 Jul 2015 08:46:55 GMT"}], "update_date": "2015-07-14", "authors_parsed": [["Aitchison", "Laurence", ""], ["Latham", "Peter E.", ""]]}, {"id": "1505.04578", "submitter": "Emanuel Diamant", "authors": "Emanuel Diamant", "title": "Advances in Artificial Intelligence: Deep Intentions, Shallow\n  Achievements", "comments": "The paper was submitted to the ICAI'15 conference (Las Vegas, Nevada,\n  USA, July 27-30, 2015) and was accepted as a poster presentation. arXiv admin\n  note: substantial text overlap with arXiv:1502.04791", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI q-bio.NC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Over the past decade, AI has made a remarkable progress due to recently\nrevived Deep Learning technology. Deep Learning enables to process large\namounts of data using simplified neuron networks that simulate the way in which\nthe brain works. At the same time, there is another point of view that posits\nthat brain is processing information, not data. This duality hampered AI\nprogress for years. To provide a remedy for this situation, I propose a new\ndefinition of information that considers it as a coupling between two separate\nentities - physical information (that implies data processing) and semantic\ninformation (that provides physical information interpretation). In such a\ncase, intelligence arises as a result of information processing. The paper\npoints on the consequences of this turn for the AI design philosophy.\n", "versions": [{"version": "v1", "created": "Mon, 18 May 2015 10:05:24 GMT"}], "update_date": "2015-05-26", "authors_parsed": [["Diamant", "Emanuel", ""]]}, {"id": "1505.04600", "submitter": "Zedong Bi", "authors": "Zedong Bi, Changsong Zhou, Hai-Jun Zhou", "title": "Spike Pattern Structure Influences Efficacy Variability under STDP and\n  Synaptic Homeostasis", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "q-bio.NC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In neural systems, synaptic plasticity is usually driven by spike trains. Due\nto the inherent noises of neurons, synapses and networks, spike trains\ntypically exhibit externally uncontrollable variability such as spatial\nheterogeneity and temporal stochasticity, resulting in variability of synapses,\nwhich we call efficacy variability. Spike patterns with the same population\nrate but inducing different efficacy variability may result in neuronal\nnetworks with sharply different structures and functions. However, how the\nvariability of spike trains influences the efficacy variability remains\nunclear. Here, we systematically study this influence when spike patterns\npossess four aspects of statistical features, i.e. synchronous firing,\nauto-temporal structure, heterogeneity of rates and heterogeneity of\ncross-correlations, under spike-timing dependent plasticity (STDP) after\ndynamically bounding the mean strength of plastic synapses into or out of a\nneuron (synaptic homeostasis). We then show the functional importance of\nefficacy variability on the encoding and maintenance of connection patterns and\non the early development of primary visual systems driven by retinal waves. We\nanticipate our work brings a fresh perspective to the understanding of the\ninteraction between synaptic plasticity and dynamical spike patterns in\nfunctional processes of neural systems.\n", "versions": [{"version": "v1", "created": "Mon, 18 May 2015 11:36:18 GMT"}, {"version": "v2", "created": "Fri, 29 May 2015 02:50:46 GMT"}, {"version": "v3", "created": "Wed, 17 Jun 2015 14:51:59 GMT"}], "update_date": "2015-06-18", "authors_parsed": [["Bi", "Zedong", ""], ["Zhou", "Changsong", ""], ["Zhou", "Hai-Jun", ""]]}, {"id": "1505.05179", "submitter": "Xiao-Jun Tian", "authors": "Xiao-Jun Tian, Hang Zhang, Jens Sannerud, and Jianhua Xing", "title": "Achieving Diverse and Monoallelic Olfactory Receptor Selection Through\n  Dual-Objective Optimization Design", "comments": "10 pages, 6 figures, Proceedings of the National Academy of Sciences\n  2016", "journal-ref": null, "doi": "10.1073/pnas.1601722113", "report-no": null, "categories": "q-bio.MN q-bio.NC", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Multiple-objective optimization is common in biological systems. In the\nmammalian olfactory system, each sensory neuron stochastically expresses only\none out of up to thousands of olfactory receptor (OR) gene alleles; at organism\nlevel the types of expressed ORs need to be maximized. Existing models focus\nonly on monoallele activation, and cannot explain recent observations in\nmutants, especially the reduced global diversity of expressed ORs in G9a/GLP\nknockouts. In this work we integrated existing information on OR expression,\nand constructed a comprehensive model that has all its components based on\nphysical interactions. Analyzing the model reveals an evolutionarily optimized\nthree-layer regulation mechanism, which includes zonal segregation, epigenetic\nbarrier crossing coupled to a negative feedback loop that mechanistically\ndiffers from previous theoretical proposals, and a previously unidentified\nenhancer competition step. This model not only recapitulates monoallelic OR\nexpression, but also elucidates how the olfactory system maximizes and\nmaintains the diversity of OR expression, and has multiple predictions\nvalidated by existing experimental results. Through making analogy to a\nphysical system with thermally activated barrier crossing and comparative\nreverse engineering analyses, the study reveals that the olfactory receptor\nselection system is optimally designed, and particularly underscores\ncooperativity and synergy as a general design principle for multi-objective\noptimization in biology.\n", "versions": [{"version": "v1", "created": "Tue, 19 May 2015 20:41:43 GMT"}, {"version": "v2", "created": "Tue, 2 Jun 2015 18:49:41 GMT"}, {"version": "v3", "created": "Wed, 3 Jun 2015 19:40:42 GMT"}, {"version": "v4", "created": "Sat, 16 Jan 2016 14:08:04 GMT"}, {"version": "v5", "created": "Mon, 9 May 2016 20:41:02 GMT"}], "update_date": "2016-05-11", "authors_parsed": [["Tian", "Xiao-Jun", ""], ["Zhang", "Hang", ""], ["Sannerud", "Jens", ""], ["Xing", "Jianhua", ""]]}, {"id": "1505.06033", "submitter": "Geir Halnes", "authors": "Geir Halnes, Tuomo M\\\"aki-Marttunen, Daniel Keller, Klas H. Pettersen,\n  Gaute T. Einevoll", "title": "The effect of ionic diffusion on extracellular potentials in neural\n  tissue", "comments": "27 pages, 8 figures", "journal-ref": null, "doi": "10.1371/journal.pcbi.1005193", "report-no": null, "categories": "physics.bio-ph physics.comp-ph q-bio.NC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In computational neuroscience, it is common to use the simplifying assumption\nthat diffusive currents are negligible compared to Ohmic currents. However,\nendured periods of intense neural signaling may cause local ion concentration\nchanges in the millimolar range. Theoretical studies have identified scenarios\nwhere steep concentration gradients give rise to diffusive currents that are of\ncomparable magnitude with Ohmic currents, and where the simplifying assumption\nthat diffusion can be neglected does not hold. We here propose a novel\nformalism for computing (1) the ion concentration dynamics and (2) the\nelectrical potential in the extracellular space surrounding multi-compartmental\nneuron models or networks of such (e.g., the Blue-Brain simulator). We use this\nformalism to explore the effects that diffusive currents can have on the\nextracellular (ECS) potential surrounding a small population of active cortical\nneurons. Our key findings are: (i) Sustained periods of neuronal output\n(simulations were run for 84 s) could change local ECS ion concentrations by\nseveral mM, as observed experimentally. (ii) For large, but realistic,\nconcentration gradients, diffusive currents in the ECS were of the same\nmagnitude as Ohmic currents. (iii) Neuronal current sources could induce local\nchanges in the ECS potential by a few mV, whereas diffusive currents could\ncould induce local changes in the ECS potential by a few tens of a mV.\nDiffusive currents could thus have a quite significant impact on ECS\npotentials. (v) Potential variations caused by diffusive currents were quite\nslow, but could influence the comparable to those induced by Ohmic currents up\nto frequencies as high as 7Hz.\n", "versions": [{"version": "v1", "created": "Fri, 22 May 2015 11:24:55 GMT"}, {"version": "v2", "created": "Mon, 25 May 2015 00:14:23 GMT"}], "update_date": "2017-02-08", "authors_parsed": [["Halnes", "Geir", ""], ["M\u00e4ki-Marttunen", "Tuomo", ""], ["Keller", "Daniel", ""], ["Pettersen", "Klas H.", ""], ["Einevoll", "Gaute T.", ""]]}, {"id": "1505.06257", "submitter": "Zachary Kilpatrick PhD", "authors": "Zachary P. Kilpatrick", "title": "Ghosts of bump attractors in stochastic neural fields: Bottlenecks and\n  extinction", "comments": "21 pages, 5 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "nlin.PS q-bio.NC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study the effects of additive noise on stationary bump solutions to\nspatially extended neural fields near a saddle-node bifurcation. The integral\nterms of these evolution equations have a weight kernel describing synaptic\ninteractions between neurons at different locations of the network. Excited\nregions of the neural field correspond to parts of the domain whose fraction of\nactive neurons exceeds a sharp threshold of a firing rate nonlinearity. For\nsufficiently low firing threshold, a stable bump coexists with an unstable bump\nand a homogeneous quiescent state. As the threshold is increased, the stable\nand unstable branch of bump solutions annihilate in a saddle node bifurcation.\nNear this criticality, we derive a quadratic amplitude equation that describes\nthe slow evolution of the even mode (bump contractions) as it depends on the\ndistance from the bifurcation. Beyond the bifurcation, bumps eventually become\nextinct, and the time it takes for this to occur increases for systems nearer\nthe bifurcation. When noise is incorporated, a stochastic amplitude equation\nfor the even mode can be derived, which can be analyzed to reveal bump\nextinction time both below and above the saddle-node.\n", "versions": [{"version": "v1", "created": "Sat, 23 May 2015 01:17:41 GMT"}, {"version": "v2", "created": "Tue, 25 Aug 2015 03:17:04 GMT"}], "update_date": "2015-08-26", "authors_parsed": [["Kilpatrick", "Zachary P.", ""]]}, {"id": "1505.06434", "submitter": "Pascal Grange", "authors": "Pascal Grange", "title": "Cell-type-specific computational neuroanatomy, simulations from the\n  sagittal and coronal Allen Brain Atlas", "comments": "prepared for the International Conference on Mathematical Modeling in\n  Physical Sciences June 5-8, 2015, Mykonos Island, Greece", "journal-ref": null, "doi": null, "report-no": null, "categories": "q-bio.NC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The Allen Atlas of the adult mouse brain is a brain-wide, genome-wide data\nset that has been made available online, triggering a renaissance in\nneuroanatomy. In particular, it has been used to define brain regions in a\ncomputational, data-driven way, and to estimate the region-specificity of cell\ntypes characterized independently by their transcriptional activity. However,\nthese results were based on one series of co-registered (coronal) ISH image\nseries per gene, whereas the online ABA contains several image series per\ngenes, including sagittal ones. Since the sagittal series cover mostly the left\nhemisphere, we can simulate the variability of results by repeatedly drawing a\nrandom image series for each gene and restricting the computation to the left\nhemisphere. This gives rise to an estimate of error bars on the results of\ncomputational neuroanatomy.\n", "versions": [{"version": "v1", "created": "Sun, 24 May 2015 13:03:47 GMT"}], "update_date": "2015-05-26", "authors_parsed": [["Grange", "Pascal", ""]]}, {"id": "1505.06603", "submitter": "Alexander K. Vidybida", "authors": "A. K. Vidybida", "title": "Simulating leaky integrate and fire neuron with integers", "comments": "11 pages. The testing program used is included as ancillary material", "journal-ref": null, "doi": null, "report-no": null, "categories": "q-bio.NC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The leaky integrate and fire (LIF) neuron represents standard neuronal model\nused for numerical simulations. The leakage is implemented in the model as\nexponential decay of trans-membrane voltage towards its resting value. This\nmakes inevitable the usage of machine floating point numbers in the course of\nsimulation. It is known that machine floating point arithmetic is subjected to\nsmall inaccuracies, which prevent from exact comparison of floating point\nquantities. In particular, it is incorrect to decide whether two separate in\ntime states of a simulated system composed of LIF neurons are exactly\nidentical. However, decision of this type is necessary, e.g. to figure periodic\ndynamical regimes in a reverberating network. Here we offer a simulation\nparadigm of a LIF neuron, in which neuronal states are described by whole\nnumbers. Within this paradigm, the LIF neuron behaves exactly the same way as\ndoes the standard floating point simulated LIF, although exact comparison of\nstates becomes correctly defined.\n", "versions": [{"version": "v1", "created": "Mon, 25 May 2015 12:03:40 GMT"}], "update_date": "2015-05-26", "authors_parsed": [["Vidybida", "A. K.", ""]]}, {"id": "1505.06670", "submitter": "Barak Pearlmutter", "authors": "Andrei Barbu, N. Siddharth, Caiming Xiong, Jason J. Corso, Christiane\n  D. Fellbaum, Catherine Hanson, Stephen Jos\\'e Hanson, S\\'ebastien H\\'elie,\n  Evguenia Malaia, Barak A. Pearlmutter, Jeffrey Mark Siskind, Thomas Michael\n  Talavage, and Ronnie B. Wilbur", "title": "The Compositional Nature of Event Representations in the Human Brain", "comments": "28 pages; 8 figures; 8 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "q-bio.NC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  How does the human brain represent simple compositions of constituents:\nactors, verbs, objects, directions, and locations? Subjects viewed videos\nduring neuroimaging (fMRI) sessions from which sentential descriptions of those\nvideos were identified by decoding the brain representations based only on\ntheir fMRI activation patterns. Constituents (e.g., \"fold\" and \"shirt\") were\nindependently decoded from a single presentation. Independent constituent\nclassification was then compared to joint classification of aggregate concepts\n(e.g., \"fold-shirt\"); results were similar as measured by accuracy and\ncorrelation. The brain regions used for independent constituent classification\nare largely disjoint and largely cover those used for joint classification.\nThis allows recovery of sentential descriptions of stimulus videos by composing\nthe results of the independent constituent classifiers. Furthermore,\nclassifiers trained on the words one set of subjects think of when watching a\nvideo can recognise sentences a different subject thinks of when watching a\ndifferent video.\n", "versions": [{"version": "v1", "created": "Mon, 25 May 2015 15:49:03 GMT"}], "update_date": "2015-05-26", "authors_parsed": [["Barbu", "Andrei", ""], ["Siddharth", "N.", ""], ["Xiong", "Caiming", ""], ["Corso", "Jason J.", ""], ["Fellbaum", "Christiane D.", ""], ["Hanson", "Catherine", ""], ["Hanson", "Stephen Jos\u00e9", ""], ["H\u00e9lie", "S\u00e9bastien", ""], ["Malaia", "Evguenia", ""], ["Pearlmutter", "Barak A.", ""], ["Siskind", "Jeffrey Mark", ""], ["Talavage", "Thomas Michael", ""], ["Wilbur", "Ronnie B.", ""]]}, {"id": "1505.07376", "submitter": "Leon Gatys", "authors": "Leon A. Gatys, Alexander S. Ecker and Matthias Bethge", "title": "Texture Synthesis Using Convolutional Neural Networks", "comments": "Revision for NIPS 2015 Camera Ready. In line with reviewer's comments\n  we now focus on the texture model and texture synthesis performance. We limit\n  the relationship of our texture model to the ventral stream and its potential\n  use for neuroscience to the discussion of the paper", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.NE q-bio.NC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Here we introduce a new model of natural textures based on the feature spaces\nof convolutional neural networks optimised for object recognition. Samples from\nthe model are of high perceptual quality demonstrating the generative power of\nneural networks trained in a purely discriminative fashion. Within the model,\ntextures are represented by the correlations between feature maps in several\nlayers of the network. We show that across layers the texture representations\nincreasingly capture the statistical properties of natural images while making\nobject information more and more explicit. The model provides a new tool to\ngenerate stimuli for neuroscience and might offer insights into the deep\nrepresentations learned by convolutional neural networks.\n", "versions": [{"version": "v1", "created": "Wed, 27 May 2015 15:29:52 GMT"}, {"version": "v2", "created": "Thu, 29 Oct 2015 23:10:38 GMT"}, {"version": "v3", "created": "Fri, 6 Nov 2015 13:55:09 GMT"}], "update_date": "2015-11-09", "authors_parsed": [["Gatys", "Leon A.", ""], ["Ecker", "Alexander S.", ""], ["Bethge", "Matthias", ""]]}, {"id": "1505.07866", "submitter": "Dominik Thalmeier", "authors": "Dominik Thalmeier, Marvin Uhlmann, Hilbert J. Kappen, Raoul-Martin\n  Memmesheimer", "title": "Learning universal computations with spikes", "comments": null, "journal-ref": "PLoS Comput Biol 12(6): e1004895 (2016)", "doi": "10.1371/journal.pcbi.1004895", "report-no": null, "categories": "q-bio.NC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Providing the neurobiological basis of information processing in higher\nanimals, spiking neural networks must be able to learn a variety of complicated\ncomputations, including the generation of appropriate, possibly delayed\nreactions to inputs and the self-sustained generation of complex activity\npatterns, e.g.~for locomotion. Many such computations require previous building\nof intrinsic world models. Here we show how spiking neural networks may solve\nthese different tasks. Firstly, we derive constraints under which classes of\nspiking neural networks lend themselves to substrates of powerful general\npurpose computing. The networks contain dendritic or synaptic nonlinearities\nand have a constrained connectivity. We then combine such networks with\nlearning rules for outputs or recurrent connections. We show that this allows\nto learn even difficult benchmark tasks such as the self-sustained generation\nof desired low-dimensional chaotic dynamics or memory-dependent computations.\nFurthermore, we show how spiking networks can build models of external world\nsystems and use the acquired knowledge to control them.\n", "versions": [{"version": "v1", "created": "Thu, 28 May 2015 21:34:57 GMT"}, {"version": "v2", "created": "Wed, 29 Jun 2016 14:11:07 GMT"}], "update_date": "2016-06-30", "authors_parsed": [["Thalmeier", "Dominik", ""], ["Uhlmann", "Marvin", ""], ["Kappen", "Hilbert J.", ""], ["Memmesheimer", "Raoul-Martin", ""]]}]