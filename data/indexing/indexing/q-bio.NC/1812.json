[{"id": "1812.00105", "submitter": "Ehtibar Dzhafarov", "authors": "Victor H. Cervantes and Ehtibar N. Dzhafarov", "title": "True Contextuality in a Psychophysical Experiment", "comments": "version 2 is a minor revision", "journal-ref": null, "doi": null, "report-no": null, "categories": "q-bio.NC math.PR quant-ph", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recent crowdsourcing experiments have shown that true contextuality of the\nkind found in quantum mechanics can also be present in human behavior. In these\nexperiments simple human choices were aggregated over large numbers of\nrespondents, with each respondent dealing with a single context (set of\nquestions asked). In this paper we present experimental evidence of\ncontextuality in individual human behavior, in a psychophysical experiment with\nrepeated presentations of visual stimuli in randomly varying contexts\n(arrangements of stimuli). The analysis is based on the\nContextuality-by-Default (CbD) theory whose relevant aspects are reviewed in\nthe paper. CbD allows one to detect contextuality in the presence of direct\ninfluences, i.e., when responses to the same stimuli have different\ndistributions in different contexts. The experiment presented is also the first\none in which contextuality is demonstrated for responses that are not\ndichotomous, with five options to choose among. CbD requires that random\nvariables representing such responses be dichotomized before they are subjected\nto contextuality analysis. A theorem says that a system consisting of all\npossible dichotomizations of responses has to be contextual if these responses\nviolate a certain condition, called nominal dominance. In our experiment\nnominal dominance was violated in all data sets, with very high statistical\nreliability established by bootstrapping.\n", "versions": [{"version": "v1", "created": "Sat, 1 Dec 2018 00:28:10 GMT"}, {"version": "v2", "created": "Fri, 22 Feb 2019 22:15:31 GMT"}], "update_date": "2019-02-26", "authors_parsed": [["Cervantes", "Victor H.", ""], ["Dzhafarov", "Ehtibar N.", ""]]}, {"id": "1812.00278", "submitter": "Pamela Douglas", "authors": "Nikolaus Kriegeskorte, Pamela K. Douglas", "title": "Interpreting Encoding and Decoding Models", "comments": "19 pages, 2 figures, author preprint", "journal-ref": null, "doi": null, "report-no": null, "categories": "q-bio.NC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Encoding and decoding models are widely used in systems, cognitive, and\ncomputational neuroscience to make sense of brain-activity data. However, the\ninterpretation of their results requires care. Decoding models can help reveal\nwhether particular information is present in a brain region in a format the\ndecoder can exploit. Encoding models make comprehensive predictions about\nrepresentational spaces. In the context of sensory systems, encoding models\nenable us to test and compare brain-computational models, and thus directly\nconstrain computational theory. Encoding and decoding models typically include\nfitted linear-model components. Sometimes the weights of the fitted linear\ncombinations are interpreted as reflecting, in an encoding model, the\ncontribution of different sensory features to the representation or, in a\ndecoding model, the contribution of different measured brain responses to a\ndecoded feature. Such interpretations can be problematic when the predictor\nvariables or their noise components are correlated and when priors (or\npenalties) are used to regularize the fit. Encoding and decoding models are\nevaluated in terms of their generalization performance. The correct\ninterpretation depends on the level of generalization a model achieves (e.g. to\nnew response measurements for the same stimuli, to new stimuli from the same\npopulation, or to stimuli from a different population). Significant decoding or\nencoding performance of a single model (at whatever level of generality) does\nnot provide strong constraints for theory. Many models must be tested and\ninferentially compared for analyses to drive theoretical progress.\n", "versions": [{"version": "v1", "created": "Sat, 1 Dec 2018 22:58:55 GMT"}, {"version": "v2", "created": "Fri, 26 Apr 2019 05:13:15 GMT"}], "update_date": "2019-04-29", "authors_parsed": [["Kriegeskorte", "Nikolaus", ""], ["Douglas", "Pamela K.", ""]]}, {"id": "1812.00598", "submitter": "Hao Wang", "authors": "Jiahui Wang, Hao Wanga, Xin Yuan Thow, Nitish V. Thakor", "title": "A Static Distributed-parameter Circuit Model Explains Electrical\n  Stimulation on the Neuromuscular System", "comments": "This manuscript is not completed yet", "journal-ref": null, "doi": null, "report-no": null, "categories": "q-bio.NC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Finite Element Modeling (FEM) has been widely used to model the electric\nfield distribution, to study the interaction between stimulation electrodes and\nneural tissue. However, due to the insufficient computational capability to\nrepresent neural tissue down to an atom-level, the existing FEM fails to model\nthe real electric field that is perpendicular to neuron membrane to initiate an\naction potential. Thus, to reveal the real electrode-tissue interactions, we\ndeveloped a circuit to model transmembrane voltage waveforms. Here, we show a\ndistributed-parameter circuit model to systematically study how\nelectrode-tissue interaction is affected by electrode position, input current\nwaveform, and biological structures in the neuromuscular system. Our model\nexplains and predicts various phenomena in neuromuscular stimulation, guides\nnew stimulation electrode and method design, and more importantly, facilitates\na fundamental understanding of the physical process during electrode-tissue\ninteraction. In our model, myelin is assumed to be inductive. The voltage\nwaveform resonance caused by this inductive myelin accounts for the much lower\nstimulation threshold to activate motoneurons than muscle fibers, which is\nobserved with in vivo measurements. These findings confirmed the feasibility of\nstudying electrode-tissue interaction using a proper distributed-parameter\ncircuit. Our current application on the neuromuscular system also raises the\npossibility that this distributed-parameter circuit model could potentially be\napplied to study other neural tissues, including the Peripheral Nervous System\n(PNS) and the Central Nervous System (CNS).\n", "versions": [{"version": "v1", "created": "Mon, 3 Dec 2018 08:26:34 GMT"}, {"version": "v2", "created": "Mon, 10 Dec 2018 11:55:44 GMT"}, {"version": "v3", "created": "Wed, 21 Aug 2019 04:07:18 GMT"}], "update_date": "2019-08-22", "authors_parsed": [["Wang", "Jiahui", ""], ["Wanga", "Hao", ""], ["Thow", "Xin Yuan", ""], ["Thakor", "Nitish V.", ""]]}, {"id": "1812.00625", "submitter": "Gestionnaire Hal-Upmc", "authors": "Kevin Fidelin (ICM, INSERM, CNRS, UPMC), Lydia Djenoune (ICM, INSERM,\n  CNRS, UPMC, MNHN), Caleb Stokes (ICM, INSERM, CNRS, UPMC), Andrew Prendergast\n  (ICM, INSERM, CNRS, UPMC), Johanna G\\'omez (ICM, INSERM, CNRS, UPMC), Audrey\n  Baradel (ICM, INSERM, CNRS, UPMC), Filippo Del\\^A bene (UPMC), Claire Wyart\n  (ICM, INSERM, CNRS, UPMC)", "title": "State-Dependent Modulation of Locomotion by GABAergic Spinal Sensory\n  Neurons", "comments": null, "journal-ref": "Current Biology - CB, Elsevier, 2015, 25 (23), pp.3035-3047", "doi": null, "report-no": null, "categories": "q-bio.NC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The cerebrospinal fluid (CSF) constitutes an interface through which chemical\ncues can reach and modulate the activity of neurons located at the epithelial\nboundary within the entire nervous system. Here, we investigate the role and\nfunctional connectivity of a class of GABAergic sensory neurons contacting the\nCSF in the vertebrate spinal cord and referred to as CSF-cNs. The remote\nactivation of CSF-cNs was shown to trigger delayed slow locomotion in the\nzebrafish larva, suggesting that these cells modulate components of locomotor\ncentral pattern generators (CPGs). Combining anatomy, electrophysiology, and\noptogenetics in vivo, we show that CSF-cNs form active GABAergic synapses onto\nV0-v glutamatergic interneurons, an essential component of locomotor CPGs. We\nconfirmed that activating CSF-cNs at rest induced delayed slow locomotion in\nthe fictive preparation. In contrast, the activation of CSF-cNs promptly\ninhibited ongoing slow locomotion. Moreover, selective activation of rostral\nCSF-cNs during ongoing activity disrupted rostrocaudal propagation of\ndescending excitation along the spinal cord, indicating that CSF-cNs primarily\nact at the premotor level. Altogether, our results demonstrate how a spinal\nGABAergic sensory neuron can tune the excitability of locomotor CPGs in a\nstate-dependent manner by projecting onto essential components of the\nexcitatory premotor pool.\n", "versions": [{"version": "v1", "created": "Mon, 3 Dec 2018 09:35:14 GMT"}], "update_date": "2018-12-04", "authors_parsed": [["Fidelin", "Kevin", "", "ICM, INSERM, CNRS, UPMC"], ["Djenoune", "Lydia", "", "ICM, INSERM,\n  CNRS, UPMC, MNHN"], ["Stokes", "Caleb", "", "ICM, INSERM, CNRS, UPMC"], ["Prendergast", "Andrew", "", "ICM, INSERM, CNRS, UPMC"], ["G\u00f3mez", "Johanna", "", "ICM, INSERM, CNRS, UPMC"], ["Baradel", "Audrey", "", "ICM, INSERM, CNRS, UPMC"], ["bene", "Filippo Del\u00c2", "", "UPMC"], ["Wyart", "Claire", "", "ICM, INSERM, CNRS, UPMC"]]}, {"id": "1812.01084", "submitter": "Felipe Gewers B.Sc.", "authors": "Felipe L. Gewers, Luciano da F. Costa", "title": "Numerical Frequency Transfer Function Analysis of a Leaky\n  Integrate-and-Fire Neuron", "comments": "13 pages, 12 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "q-bio.NC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This work reports a transfer function-based approach to characterizing the\noperation of single neuronal cells in terms of the instantaneous frequency of\nthe input and output signals. We adopt the leaky integrate-and-fire model. The\ntransfer relationship is obtained by performing successive\nnumeric-computational simulations and statistical regressions. Several\ninteresting results are reported, including the identification and\ncharacterization of linearity in the transfer relationship, as well as the\nidentification of regions in the parameter space characterized by sharper\ntransfer functions. These properties can be used to facilitate simulations\nunder certain circumstances and to validate the model by comparison with\nbiological results.\n", "versions": [{"version": "v1", "created": "Mon, 3 Dec 2018 21:23:40 GMT"}, {"version": "v2", "created": "Tue, 22 Oct 2019 03:41:52 GMT"}], "update_date": "2019-10-23", "authors_parsed": [["Gewers", "Felipe L.", ""], ["Costa", "Luciano da F.", ""]]}, {"id": "1812.01179", "submitter": "Hao Wang", "authors": "Jiahui Wang, Hao Wang, Xin Yuan Thow, Nitish V. Thakor", "title": "The Dynamic Shift of Neuron Excitability Observed with Enveloped High\n  Frequency Stimulation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "q-bio.NC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Neuronal excitability is known to be affected by transcranial electrical\nstimulation. However, due to the existence of both excitatory and inhibitory\nneurons in the cortex, the mechanism of neuronal excitability shift is still\nnot clear. Here, we study electrical stimulation disturbance on neuronal\nexcitability in the neuromuscular system on an acute rat model. We design a\nspecial stimulation waveform of enveloped high frequency stimulation (EHFS).\nWith modeling and in vivo measurements, EHFS is confirmed to synchronize the\nneuronal recruitment at the positive and negative electrode. This unique\nsynchronization property of EHFS amplifies the electrical stimulation\ndisturbance on neuronal excitability, to enable the macroscopic observations of\nlarge increase and decrease of force output. By eliminating the complicated\ninhibitory interneuron effects in the cortex, our observations on the\nneuromuscular system supports the argument that electrical stimulation\ndynamically shifts neuronal excitability, by inducing depolarization during\nanodal stimulation and hyperpolarization during cathodal stimulation.\n", "versions": [{"version": "v1", "created": "Tue, 4 Dec 2018 02:54:30 GMT"}, {"version": "v2", "created": "Mon, 10 Dec 2018 11:54:14 GMT"}, {"version": "v3", "created": "Thu, 22 Aug 2019 03:46:50 GMT"}], "update_date": "2019-08-23", "authors_parsed": [["Wang", "Jiahui", ""], ["Wang", "Hao", ""], ["Thow", "Xin Yuan", ""], ["Thakor", "Nitish V.", ""]]}, {"id": "1812.01342", "submitter": "Milena \\v{C}uki\\'c Dr", "authors": "Milena Cukic, Miodrag Stokic, Slavoljub Radenkovic, Milos\n  Ljubisavljevic and Dragoljub Donald Pokrajac", "title": "The Shift in brain-state induced by tDCS: an EEG study", "comments": "23 pages, 6 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "q-bio.NC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Transcranial direct current stimulation (tDCS) is known to have a modulatory\neffect on neural tissue and that it is polarity specific. It is also shown that\ntDCS demonstrated the lasting effect in therapeutic applications. The main aim\nof the study was to examine the effects of tDCS on cortical dynamics by\nanalyzing EEG recordings. We applied here measures taken from Recurrence\nQuantification Analysis, Mean State Shift (MSS) and State Variance (SV) which\nwere previously used to detect changes in brain-state dynamics after TMS. The\nstudied cohort comprised of 16 healthy subjects; all subjects received anodal\nand cathodal tDCS, which were given in two separate sessions on the same day.\nThe EEG was recorded from 10 electrodes, positioned over left motor cortex and\nmirroring right cortex corresponding to 10/20 standard. From three traces of\nrecordings (pre, post1 and post2/before the stimulation, immediately after and\n30min after tDCS) we extracted five different intervals (T1-T5) comprising of\n500 samples. After calculating MSS and SV on those epochs and statistical\ntesting for a significant difference, we applied Principal Component Analysis\n(PCA) on the same time series to check whether the data are separable. The\nresults show that tDCS exert polarity specific effects on the MSS as shown by\nsignificantly lower MSS values after cathodal stimulation compared to anodal\nstimulation. Cathodal stimulation affected the SV, as compared to anodal\nstimulation, which did not lead to detectable changes. We are offering here for\nthe first time an informative PCA visualization of a time-effect of a tDCS\nstimulation on brain state shift. Further research is needed to elucidate for\nhow long that change can be detected and what neurobiological changes are\nintroduced by that phenomena.\n", "versions": [{"version": "v1", "created": "Tue, 4 Dec 2018 11:17:13 GMT"}], "update_date": "2018-12-05", "authors_parsed": [["Cukic", "Milena", ""], ["Stokic", "Miodrag", ""], ["Radenkovic", "Slavoljub", ""], ["Ljubisavljevic", "Milos", ""], ["Pokrajac", "Dragoljub Donald", ""]]}, {"id": "1812.01475", "submitter": "Michal Hled\\'ik", "authors": "Michal Hled\\'ik, Thomas R. Sokolowski and Ga\\v{s}per Tka\\v{c}ik", "title": "A Tight Upper Bound on Mutual Information", "comments": "6 pages, 3 figures; proof illustration added", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IT math.IT q-bio.NC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We derive a tight lower bound on equivocation (conditional entropy), or\nequivalently a tight upper bound on mutual information between a signal\nvariable and channel outputs. The bound is in terms of the joint distribution\nof the signals and maximum a posteriori decodes (most probable signals given\nchannel output). As part of our derivation, we describe the key properties of\nthe distribution of signals, channel outputs and decodes, that minimizes\nequivocation and maximizes mutual information. This work addresses a problem in\ndata analysis, where mutual information between signals and decodes is\nsometimes used to lower bound the mutual information between signals and\nchannel outputs. Our result provides a corresponding upper bound.\n", "versions": [{"version": "v1", "created": "Tue, 4 Dec 2018 15:07:57 GMT"}, {"version": "v2", "created": "Fri, 11 Jan 2019 09:39:58 GMT"}], "update_date": "2019-01-14", "authors_parsed": [["Hled\u00edk", "Michal", ""], ["Sokolowski", "Thomas R.", ""], ["Tka\u010dik", "Ga\u0161per", ""]]}, {"id": "1812.01727", "submitter": "Zachary Kilpatrick PhD", "authors": "Zachary P. Kilpatrick, William R. Holmes, Tahra L. Eissa, Kre\\v{s}imir\n  Josi\\'c", "title": "Optimal models of decision-making in dynamic environments", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "q-bio.NC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Nature is in constant flux, so animals must account for changes in their\nenvironment when making decisions. How animals learn the timescale of such\nchanges and adapt their decision strategies accordingly is not well understood.\nRecent psychophysical experiments have shown humans and other animals can\nachieve near-optimal performance at two alternative forced choice (2AFC) tasks\nin dynamically changing environments. Characterization of performance requires\nthe derivation and analysis of computational models of optimal decision-making\npolicies on such tasks. We review recent theoretical work in this area, and\ndiscuss how models compare with subjects' behavior in tasks where the correct\nchoice or evidence quality changes in dynamic, but predictable, ways.\n", "versions": [{"version": "v1", "created": "Tue, 4 Dec 2018 22:24:03 GMT"}, {"version": "v2", "created": "Fri, 21 Dec 2018 05:39:45 GMT"}], "update_date": "2018-12-24", "authors_parsed": [["Kilpatrick", "Zachary P.", ""], ["Holmes", "William R.", ""], ["Eissa", "Tahra L.", ""], ["Josi\u0107", "Kre\u0161imir", ""]]}, {"id": "1812.02297", "submitter": "Yousef Jamali", "authors": "Arefeh Mazarei, Mohammad Amirian Matlob, Gholamhossein Riazi, and\n  Yousef Jamali", "title": "The Role of Topology in the Synchronization of Neuronal Networks Based\n  on the Hodgkin-Huxley Model", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "physics.bio-ph q-bio.NC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Complex systems in the real world can be modeled as a network of connected\ncomponents. The human brain, as a network of neurons among which the\ninteractions cause perception, is a complex network. Synchronization is a\ndynamical phenomenon that can be seen in the brain. The network topology has a\nremarkable impact on both the function and the dynamics of neural networks. In\nthis research, synchronization of neural networks is scrutinized through\ncreating various topologies. These networks include both excitatory and\ninhibitory neurons. We investigate the dynamics of different networks by random\nrewiring of the synaptic connections. In this manner, a regular network\ntransforms into a small-world network and then becomes a random network.\nCoherence level which is measured and utilized as the criteria to analyze\nsynchronicity, experiencing a sharp increase as the network changes into the\nsmall-world network and growing steadily by the end. On the other hand, a\ndecreasing trend of coherence level is revealed starting from a complete\nexcitatory network and gradually increasing of inhibitory neurons. Thus, the\ncoherence level reaches approximately zero in a complete inhibitory network. By\nincreasing the number of neurons in the network, the degree of synchronization\nfollows a power-law distribution; however, the number of synaptic connections\nof each neuron and their conductance have a positive impact on synchronization.\nBy applying the model to a C-elegance neural network, not only the mentioned\nparameters but also the role of the degree distribution are highlighted.\n", "versions": [{"version": "v1", "created": "Sat, 1 Dec 2018 05:56:09 GMT"}, {"version": "v2", "created": "Mon, 29 Apr 2019 05:35:01 GMT"}], "update_date": "2019-04-30", "authors_parsed": [["Mazarei", "Arefeh", ""], ["Matlob", "Mohammad Amirian", ""], ["Riazi", "Gholamhossein", ""], ["Jamali", "Yousef", ""]]}, {"id": "1812.02338", "submitter": "Liane Gabora", "authors": "Alexandra Maland and Liane Gabora", "title": "Educational Implications of the 'Self-Made Worldview' Concept", "comments": "18 pages", "journal-ref": "In Beghetto, R., & Coraza, G. S. (Eds.) Dynamic Perspectives on\n  Creativity: New Directions for Theory, Research, and Practice in Education\n  (pp. 117-136). Berlin: Springer (2019)", "doi": null, "report-no": null, "categories": "q-bio.NC nlin.AO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Immersion in a creative task can be an intimate experience. It can feel like\na mystery: intangible, inexplicable, and beyond the reach of science. However,\nscience is making exciting headway into understanding creativity. While the\nmind of a highly uncreative individual consists of a collection of items\naccumulated through direct experience and enculturation, the mind of a creative\nindividual is self-organizing and self-mending; thus, experiences and items of\ncultural knowledge are thought through from different perspectives such that\nthey cohere together into a loosely integrated whole. The reweaving of items in\nmemory is elicited by perturbations: experiences that increase psychological\nentropy because they are inconsistent with one's web of understandings. The\nprocess of responding to one perturbation often leads to other perturbations,\ni.e., other inconsistencies in one's web of understandings. Creative thinking\noften requires the capacity to shift between divergent and convergent modes of\nthought in response to the ever-changing demands of the creative task. Since\nuncreative individuals can reap the benefits of creativity by imitating\ncreators, using their inventions, or purchasing their artworks, it is not\nnecessary that everyone be creative. Agent based computer models of cultural\nevolution suggest that society functions best with a mixture of creative and\nuncreative individuals. The ideal ratio of creativity to imitation increases in\ntimes of change, such as we are experiencing now. Therefore it is important to\neducate the next generation in ways that foster creativity. The chapter\nconcludes with suggestions for how educational systems can cultivate\ncreativity.\n", "versions": [{"version": "v1", "created": "Thu, 6 Dec 2018 03:46:41 GMT"}, {"version": "v2", "created": "Fri, 5 Jul 2019 21:25:29 GMT"}], "update_date": "2019-07-09", "authors_parsed": [["Maland", "Alexandra", ""], ["Gabora", "Liane", ""]]}, {"id": "1812.02478", "submitter": "Jochen Braun", "authors": "Stepan Aleshin, Gergo Ziman, Ilona Kovacs, Jochen Braun", "title": "Perceptual reversals in binocular rivalry: improved detection from OKN", "comments": "37 pages, 9 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "q-bio.NC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  When binocular rivalry is induced by opponent motion displays, perceptual\nreversals are often associated with changed oculomotor behaviour (Frassle et\nal., 2014; Fujiwara et al., 2017). Specifically, the direction of smooth\npursuit phases in optokinetic nystagmus (OKN) typically corresponds to the\ndirection of motion that dominates perceptual appearance at any given time.\nHere we report an improved analysis that continuously estimates perceived\nmotion in terms of `cumulative smooth pursuit'. In essence, smooth pursuit\nsegments are identified, interpolated where necessary, and joined\nprobabilistically into a continuous record of `cumulative smooth pursuit'\n(i.e., a probability of eye position disregarding blinks, saccades, signal\nlosses, and artefacts). The analysis is fully automated and robust in healthy,\ndevelopmental, and patient populations. To validate reliability, we compare\nvolitional reports of perceptual reversals in rivalry displays, and of physical\nreversals in non-rivalrous control displays. `Cumulative smooth pursuit'\ndetects physical reversals and estimates eye velocity more accurately than\nexisting methods do (Frassle et al., 2014). It also appears to distinguish\ndominant and transitional perceptual states, detecting changes with a precision\nof $\\pm100\\,\\mathit{ms}$. We conclude that `cumulative smooth pursuit'\nsignificantly improves the monitoring of binocular rivalry by means of\nrecording OKN.\n", "versions": [{"version": "v1", "created": "Thu, 6 Dec 2018 11:45:27 GMT"}], "update_date": "2018-12-07", "authors_parsed": [["Aleshin", "Stepan", ""], ["Ziman", "Gergo", ""], ["Kovacs", "Ilona", ""], ["Braun", "Jochen", ""]]}, {"id": "1812.02870", "submitter": "Liane Gabora", "authors": "Liane Gabora and Mike Unrau", "title": "The Role of Engagement, Honing, and Mindfulness in Creativity", "comments": "22 pages;", "journal-ref": "in Mullen, C. (Ed.) Creativity under duress in education?\n  Resistive theories, practices, and actions, Creativity Theory and Action in\n  Education Vol. 3.(2019)", "doi": null, "report-no": null, "categories": "q-bio.NC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  As both our external world and inner worlds become more complex, we are faced\nwith more novel challenges, hardships, and duress. Creative thinking is needed\nto provide fresh perspectives and solve new problems.Because creativity can be\nconducive to accessing and reliving traumatic memories, emotional scars may be\nexacerbated by creative practices before these are transformed and released.\nTherefore, in preparing our youth to thrive in an increasingly unpredictable\nworld, it could be helpful to cultivate in them an understanding of the\ncreative process and its relationship to hardship, as well as tools and\ntechniques for fostering not just creativity but self-awareness and\nmindfulness. This chapter is a review of theories of creativity through the\nlens of their capacity to account for the relationship between creativity and\nhardship, as well as the therapeutic effects of creativity. We also review\ntheories and research on aspects of mindfulness attending to potential\ntherapeutic effects of creativity. Drawing upon the creativity and mindfulness\nliteratures, we sketch out what an introductory 'creativity and mindfulness'\nmodule might look like as part of an educational curriculum designed to address\nthe unique challenges of the 21st Century.\n", "versions": [{"version": "v1", "created": "Fri, 7 Dec 2018 01:55:30 GMT"}, {"version": "v2", "created": "Wed, 13 Mar 2019 21:31:36 GMT"}, {"version": "v3", "created": "Fri, 5 Jul 2019 21:18:11 GMT"}], "update_date": "2019-07-09", "authors_parsed": [["Gabora", "Liane", ""], ["Unrau", "Mike", ""]]}, {"id": "1812.03122", "submitter": "Alexander K. Vidybida", "authors": "Alexander Vidybida", "title": "Trade-off between sensitivity and selectivity in olfactory receptor\n  neuron", "comments": "4 pages, 2 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "q-bio.NC physics.bio-ph", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  It was observed before that due to convergence in the olfactory system a\npossible amplification can be as large as the degree of convergence. This is in\nthe case when a single impulse from the converging inputs is enough to trigger\nthe secondary neuron. On the other hand, if a number of impulses are required\nfor triggering, a gain in discriminating ability may be obtained along with\ndecrease in sensitivity gained due to the convergence. We discuss this\ntrade-off in terms of concrete estimates using olfactory sensory neuron and the\nset of its receptor proteins as an example of system with convergence.\n", "versions": [{"version": "v1", "created": "Fri, 7 Dec 2018 17:36:33 GMT"}, {"version": "v2", "created": "Fri, 25 Jan 2019 14:58:38 GMT"}], "update_date": "2019-01-28", "authors_parsed": [["Vidybida", "Alexander", ""]]}, {"id": "1812.03363", "submitter": "David Mehler", "authors": "David Marc Anton Mehler, Konrad Paul Kording", "title": "The lure of misleading causal statements in functional connectivity\n  research", "comments": "37 pages, 2 figures. Code and simulated data available on:\n  https://osf.io/9cs8p/", "journal-ref": null, "doi": null, "report-no": null, "categories": "q-bio.NC", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  As neuroscientists we want to understand how causal interactions or\nmechanisms within the brain give rise to perception, cognition, and behavior.\nIt is typical to estimate interaction effects from measured activity using\nstatistical techniques such as functional connectivity, Granger Causality, or\ninformation flow, whose outcomes are often falsely treated as revealing\nmechanistic insight. Since these statistical techniques fit models to\nlow-dimensional measurements from brains, they ignore the fact that brain\nactivity is high-dimensional. Here we focus on the obvious confound of common\ninputs: the countless unobserved variables likely have more influence than the\nfew observed ones. Any given observed correlation can be explained by an\ninfinite set of causal models that take into account the unobserved variables.\nTherefore, correlations within massively undersampled measurements tell us\nlittle about mechanisms. We argue that these mis-inferences of causality from\ncorrelation are augmented by an implicit redefinition of words that suggest\nmechanisms, such as connectivity, causality, and flow.\n", "versions": [{"version": "v1", "created": "Sat, 8 Dec 2018 18:21:07 GMT"}, {"version": "v2", "created": "Wed, 21 Oct 2020 11:21:23 GMT"}, {"version": "v3", "created": "Fri, 23 Oct 2020 09:50:39 GMT"}], "update_date": "2020-10-26", "authors_parsed": [["Mehler", "David Marc Anton", ""], ["Kording", "Konrad Paul", ""]]}, {"id": "1812.03455", "submitter": "Gr\\'egory Dumont", "authors": "Gregory Dumont and Boris Gutkin", "title": "Macroscopic phase resetting-curves determine oscillatory coherence and\n  signal transfer in inter-coupled neural circuits", "comments": "35 pages, 9 figures", "journal-ref": null, "doi": "10.1371/journal.pcbi.1007019", "report-no": null, "categories": "q-bio.NC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Macroscopic oscillations of different brain regions show multiple phase\nrelationships that are persistent across time and have been implicated routing\ninformation. Various cellular level mechanisms influence the network dynamics\nand structure the macroscopic firing patterns. Key question is to identify the\nbiophysical neuronal and synaptic properties that permit such motifs to arise\nand how the different coherence states determine the communication between the\nneural circuits. We analyse the emergence of phase locking within\nbidirectionally delayed-coupled spiking circuits showing global gamma band\noscillations. We consider both the interneuronal (ING) and the\npyramidal-interneuronal (PING) gamma rhythms and the inter coupling targeting\nthe pyramidal or the inhibitory interneurons. Using a mean-field approach\ntogether with an exact reduction method, we break down each spiking network\ninto a low dimensional nonlinear system and derive the macroscopic phase\nresetting-curves (mPRCs) that determine how the phase of the global oscillation\nresponds to incoming perturbations. Depending on the type of gamma oscillation,\nwe show that incoming excitatory inputs can either only speed up the\noscillation (phase advance; type I PRC) or induce both an advance and a delay\nthe macroscopic oscillation (phase delay; type II PRC). From there we determine\nthe structure of macroscopic coherence states (phase locking) of two weakly\nsynaptically-coupled networks. To do so we derive a phase equation for the\ncoupled system which links the synaptic mechanisms to the coherence state of\nthe system. We show that the transmission delay is a necessary condition for\nsymmetry breaking, i.e. a non-symmetric phase lag between the macroscopic\noscillations, potentially giving an explanation to the experimentally observed\nvariety of gamma phase-locking modes.\n", "versions": [{"version": "v1", "created": "Sun, 9 Dec 2018 10:26:18 GMT"}], "update_date": "2019-06-19", "authors_parsed": [["Dumont", "Gregory", ""], ["Gutkin", "Boris", ""]]}, {"id": "1812.03609", "submitter": "Nikita Novikov", "authors": "Nikita Novikov, Boris Gutkin", "title": "Role of NMDA conductance in average firing rate shifts caused by\n  external periodic forcing", "comments": null, "journal-ref": "Phys. Rev. E 101, 052408 (2020)", "doi": "10.1103/PhysRevE.101.052408", "report-no": null, "categories": "q-bio.NC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A widely accepted view of computations in the brain relies on population\ncoding, where the neural ensemble firing rate is modulated in a stable manner\nto transmit information and perform various cognitive tasks. At the same time,\noscillatory neural activity is specifically modulated in frequency, coherence\nand power during cognitive performance. How the firing rate and oscillations\ninteract remains a salient question. In this paper, we develop a theory for the\ninteractions between oscillatory signals and the firing rate of neural\npopulations based on activity of non-linear voltage-dependent NMDA synapses.\nNotably, we show under which conditions oscillatory inputs can control the mean\nfiring rate without loss of stability. Using mathematical analysis and\nsimulations of mean-field models, we demonstrate that presence of NMDA synapses\non both the excitatory and the inhibitory neurons is critical for sinusoidal\noscillations to significantly and stably increase the firing rate. We\ncharacterize the oscillation-induced mean firing rate shift as a function of\nthe fast and slow synaptic weights and demonstrate the parameter region, in\nwhich the effect under investigation is mostly pronounced. Results of our work\nmay help identify the properties of neural circuits that allow for constructive\ncontrol of the firing rate codes by large-scale neural oscillations.\n", "versions": [{"version": "v1", "created": "Mon, 10 Dec 2018 03:42:35 GMT"}], "update_date": "2020-05-27", "authors_parsed": [["Novikov", "Nikita", ""], ["Gutkin", "Boris", ""]]}, {"id": "1812.03684", "submitter": "Miljan Petrovi\\'c", "authors": "Miljan Petrovi\\'c (1 and 2), Thomas A.W. Bolton (1 and 2), Maria\n  Giulia Preti (1 and 2), Rapha\\\"el Li\\'egeois (1 and 2) and Dimitri Van De\n  Ville (1 and 2) ((1) Institute of Bioengineering, \\'Ecole Polytechnique\n  F\\'ed\\'erale de Lausanne, Campus Biotech, Geneva, Switzerland, (2) Department\n  of Radiology and Medical Informatics, University of Geneva, Geneva,\n  Switzerland)", "title": "Guided Graph Spectral Embedding: Application to the C. elegans\n  Connectome", "comments": "43 pages, 7 figures, submitted to Network Neuroscience", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG q-bio.NC stat.ML", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Graph spectral analysis can yield meaningful embeddings of graphs by\nproviding insight into distributed features not directly accessible in nodal\ndomain. Recent efforts in graph signal processing have proposed new\ndecompositions-e.g., based on wavelets and Slepians-that can be applied to\nfilter signals defined on the graph. In this work, we take inspiration from\nthese constructions to define a new guided spectral embedding that combines\nmaximizing energy concentration with minimizing modified embedded distance for\na given importance weighting of the nodes. We show these optimization goals are\nintrinsically opposite, leading to a well-defined and stable spectral\ndecomposition. The importance weighting allows to put the focus on particular\nnodes and tune the trade-off between global and local effects. Following the\nderivation of our new optimization criterion and its linear approximation, we\nexemplify the methodology on the C. elegans structural connectome. The results\nof our analyses confirm known observations on the nematode's neural network in\nterms of functionality and importance of cells. Compared to Laplacian\nembedding, the guided approach, focused on a certain class of cells (sensory,\ninter- and motoneurons), provides more biological insights, such as the\ndistinction between somatic positions of cells, and their involvement in low or\nhigh order processing functions.\n", "versions": [{"version": "v1", "created": "Mon, 10 Dec 2018 09:16:21 GMT"}, {"version": "v2", "created": "Fri, 1 Feb 2019 09:12:06 GMT"}, {"version": "v3", "created": "Wed, 6 Mar 2019 12:49:14 GMT"}], "update_date": "2019-03-07", "authors_parsed": [["Petrovi\u0107", "Miljan", "", "1 and 2"], ["Bolton", "Thomas A. W.", "", "1 and 2"], ["Preti", "Maria Giulia", "", "1 and 2"], ["Li\u00e9geois", "Rapha\u00ebl", "", "1 and 2"], ["Van De Ville", "Dimitri", "", "1 and 2"]]}, {"id": "1812.03796", "submitter": "Richard Betzel", "authors": "Richard F. Betzel, Katherine C. Wood, Christopher Angeloni, Maria\n  Neimark Geffen, Danielle S. Bassett", "title": "Stability of spontaneous, correlated activity in mouse auditory cortex", "comments": "15 pages, 3 figures", "journal-ref": null, "doi": "10.1371/journal.pcbi.1007360", "report-no": null, "categories": "q-bio.NC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Neural systems can be modeled as networks of functionally connected neural\nelements. The resulting network can be analyzed using mathematical tools from\nnetwork science and graph theory to quantify the system's topological\norganization and to better understand its function. While the network-based\napproach is common in the analysis of large-scale neural systems probed by\nnon-invasive neuroimaging, few studies have used network science to study the\norganization of networks reconstructed at the cellular level, and thus many\nvery basic and fundamental questions remain unanswered. Here, we used\ntwo-photon calcium imaging to record spontaneous activity from the same set of\ncells in mouse auditory cortex over the course of several weeks. We reconstruct\nfunctional networks in which cells are linked to one another by edges weighted\naccording to the correlation of their fluorescence traces. We show that the\nnetworks exhibit modular structure across multiple topological scales and that\nthese multi-scale modules unfold as part of a hierarchy. We also show that, on\naverage, network architecture becomes increasingly dissimilar over time, with\nsimilarity decaying monotonically with the distance (in time) between sessions.\nFinally, we show that a small fraction of cells maintain strongly-correlated\nactivity over multiple days, forming a stable temporal core surrounded by a\nfluctuating and variable periphery. Our work provides a careful methodological\nblueprint for future studies of spontaneous activity measured by two-photon\ncalcium imaging using cutting-edge computational methods and machine learning\nalgorithms informed by explicit graphical models from network science. The\nmethods are easily extended to additional datasets, opening the possibility of\nstudying cellular level network organization of neural systems and how that\norganization is modulated by stimuli or altered in models of disease.\n", "versions": [{"version": "v1", "created": "Mon, 10 Dec 2018 13:53:10 GMT"}], "update_date": "2020-07-01", "authors_parsed": [["Betzel", "Richard F.", ""], ["Wood", "Katherine C.", ""], ["Angeloni", "Christopher", ""], ["Geffen", "Maria Neimark", ""], ["Bassett", "Danielle S.", ""]]}, {"id": "1812.04533", "submitter": "Rasoul Hekmati", "authors": "Rasoul Hekmati, Robert Azencott, Wei Zhang, Zili D. Chu, Michael J.\n  Paldino", "title": "Localization of Epileptic Seizure Focus by Computerized Analysis of fMRI\n  Recordings", "comments": "25 pages,5 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "q-bio.QM q-bio.NC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  By computerized analysis of cortical activity recorded via fMRI for pediatric\nepilepsy patients, we implement algorithmic localization of epileptic seizure\nfocus within one of eight cortical lobes. Our innovative machine learning\ntechniques involve intensive analysis of large matrices of mutual information\ncoefficients between pairs of anatomically identified cortical regions. Drastic\nselection of pairs of regions with significant inter-connectivity provide\nefficient inputs for our Multi-Layer Perceptron (MLP) classifier. By imposing\nrigorous parameter parsimony to avoid over fitting we construct a small size\nMLP with very good percentages of successful classification.\n", "versions": [{"version": "v1", "created": "Tue, 11 Dec 2018 16:47:33 GMT"}], "update_date": "2018-12-12", "authors_parsed": [["Hekmati", "Rasoul", ""], ["Azencott", "Robert", ""], ["Zhang", "Wei", ""], ["Chu", "Zili D.", ""], ["Paldino", "Michael J.", ""]]}, {"id": "1812.04723", "submitter": "Cengiz Kaygusuz", "authors": "Cengiz Kaygusuz, Julian Zuluaga", "title": "Impact of Intervals on the Emotional Effect in Western Music", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "q-bio.NC cs.LG cs.SD eess.AS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Every art form ultimately aims to invoke an emotional response over the\naudience, and music is no different. While the precise perception of music is a\nhighly subjective topic, there is an agreement in the \"feeling\" of a piece of\nmusic in broad terms. Based on this observation, in this study, we aimed to\ndetermine the emotional feeling associated with short passages of music;\nspecifically by analyzing the melodic aspects. We have used the dataset put\ntogether by Eerola et. al. which is comprised of labeled short passages of film\nmusic. Our initial survey of the dataset indicated that other than \"happy\" and\n\"sad\" labels do not possess a melodic structure. We transcribed the main melody\nof the happy and sad tracks and used the intervals between the notes to\nclassify them. Our experiments have shown that treating a melody as a\nbag-of-intervals do not possess any predictive power whatsoever, whereas\ncounting intervals with respect to the key of the melody yielded a classifier\nwith 85% accuracy.\n", "versions": [{"version": "v1", "created": "Mon, 10 Dec 2018 17:49:30 GMT"}], "update_date": "2018-12-13", "authors_parsed": [["Kaygusuz", "Cengiz", ""], ["Zuluaga", "Julian", ""]]}, {"id": "1812.04769", "submitter": "Emanuel Diamant", "authors": "Emanuel Diamant", "title": "Designing Artificial Cognitive Architectures: Brain Inspired or\n  Biologically Inspired?", "comments": null, "journal-ref": null, "doi": "10.1016/j.procs.2018.11.023", "report-no": null, "categories": "cs.AI q-bio.NC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Artificial Neural Networks (ANNs) were devised as a tool for Artificial\nIntelligence design implementations. However, it was soon became obvious that\nthey are unable to fulfill their duties. The fully autonomous way of ANNs\nworking, precluded from any human intervention or supervision, deprived of any\ntheoretical underpinning, leads to a strange state of affairs, when ANN\ndesigners cannot explain why and how they achieve their amazing and remarkable\nresults. Therefore, contemporary Artificial Intelligence R&D looks more like a\nModern Alchemy enterprise rather than a respected scientific or technological\nundertaking. On the other hand, modern biological science posits that\nintelligence can be distinguished not only in human brains. Intelligence today\nis considered as a fundamental property of each and every living being.\nTherefore, lower simplified forms of natural intelligence are more suitable for\ninvestigation and further replication in artificial cognitive architectures.\n", "versions": [{"version": "v1", "created": "Wed, 12 Dec 2018 01:40:51 GMT"}], "update_date": "2018-12-13", "authors_parsed": [["Diamant", "Emanuel", ""]]}, {"id": "1812.04786", "submitter": "Shoeb Shaikh", "authors": "Shoeb Shaikh, Rosa So, Camilo Libedinsky and Arindam Basu", "title": "Experimental Comparison of Hardware-Amenable Spike Detection Algorithms\n  for iBMIs", "comments": "accepted at NER (Neural Engineering Conference) - 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "q-bio.NC", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  This paper presents an experiment based comparison of absolute threshold (AT)\nand non-linear energy operator (NEO) spike detection algorithms in\nIntra-cortical Brain Machine Interfaces (iBMIs). Results show an average\nincrease in decoding performance of approx. 5% in monkey A across 28 sessions\nrecorded over 6 days and approx. 2% in monkey B across 35 sessions recorded\nover 8 days when using NEO over AT. To the best of our knowledge, this is the\nfirst ever reported comparison of spike detection algorithms in an iBMI\nexperimental framework involving two monkeys. Based on the improvements\nobserved in an experimental setting backed by previously reported improvements\nin simulation studies, we advocate switching from state of the art spike\ndetection technique - AT to NEO.\n", "versions": [{"version": "v1", "created": "Wed, 12 Dec 2018 02:50:43 GMT"}], "update_date": "2018-12-13", "authors_parsed": [["Shaikh", "Shoeb", ""], ["So", "Rosa", ""], ["Libedinsky", "Camilo", ""], ["Basu", "Arindam", ""]]}, {"id": "1812.04974", "submitter": "Elena Pastorelli", "authors": "Francesco Simula, Elena Pastorelli, Pier Stanislao Paolucci, Michele\n  Martinelli, Alessandro Lonardo, Andrea Biagioni, Cristiano Capone, Fabrizio\n  Capuani, Paolo Cretaro, Giulia De Bonis, Francesca Lo Cicero, Luca Pontisso,\n  Piero Vicini, Roberto Ammendola", "title": "Real-time cortical simulations: energy and interconnect scaling on\n  distributed systems", "comments": "8 pages, 8 figures, 4 tables, submitted after final publication on\n  PDP2019 proceedings, corrected final DOI. arXiv admin note: text overlap with\n  arXiv:1812.04974, arXiv:1804.03441", "journal-ref": "27th Euromicro International Conference on Parallel, Distributed\n  and Network-based Processing (PDP), Pavia, Italy, February 13-15, 2019, pp.\n  283-290", "doi": "10.1109/EMPDP.2019.8671627", "report-no": null, "categories": "cs.DC cs.NE q-bio.NC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We profile the impact of computation and inter-processor communication on the\nenergy consumption and on the scaling of cortical simulations approaching the\nreal-time regime on distributed computing platforms. Also, the speed and energy\nconsumption of processor architectures typical of standard HPC and embedded\nplatforms are compared. We demonstrate the importance of the design of\nlow-latency interconnect for speed and energy consumption. The cost of cortical\nsimulations is quantified using the Joule per synaptic event metric on both\narchitectures. Reaching efficient real-time on large scale cortical simulations\nis of increasing relevance for both future bio-inspired artificial intelligence\napplications and for understanding the cognitive functions of the brain, a\nscientific quest that will require to embed large scale simulations into highly\ncomplex virtual or real worlds. This work stands at the crossroads between the\nWaveScalES experiment in the Human Brain Project (HBP), which includes the\nobjective of large scale thalamo-cortical simulations of brain states and their\ntransitions, and the ExaNeSt and EuroExa projects, that investigate the design\nof an ARM-based, low-power High Performance Computing (HPC) architecture with a\ndedicated interconnect scalable to million of cores; simulation of deep sleep\nSlow Wave Activity (SWA) and Asynchronous aWake (AW) regimes expressed by\nthalamo-cortical models are among their benchmarks.\n", "versions": [{"version": "v1", "created": "Wed, 12 Dec 2018 14:42:10 GMT"}, {"version": "v2", "created": "Thu, 27 Dec 2018 13:16:02 GMT"}, {"version": "v3", "created": "Tue, 19 Feb 2019 15:33:39 GMT"}, {"version": "v4", "created": "Tue, 26 Nov 2019 11:18:26 GMT"}], "update_date": "2019-11-27", "authors_parsed": [["Simula", "Francesco", ""], ["Pastorelli", "Elena", ""], ["Paolucci", "Pier Stanislao", ""], ["Martinelli", "Michele", ""], ["Lonardo", "Alessandro", ""], ["Biagioni", "Andrea", ""], ["Capone", "Cristiano", ""], ["Capuani", "Fabrizio", ""], ["Cretaro", "Paolo", ""], ["De Bonis", "Giulia", ""], ["Cicero", "Francesca Lo", ""], ["Pontisso", "Luca", ""], ["Vicini", "Piero", ""], ["Ammendola", "Roberto", ""]]}, {"id": "1812.04994", "submitter": "Wolfgang Fr\\\"uhwirt", "authors": "Wolfgang Fruehwirt, Adam D. Cobb, Martin Mairhofer, Leonard Weydemann,\n  Heinrich Garn, Reinhold Schmidt, Thomas Benke, Peter Dal-Bianco, Gerhard\n  Ransmayr, Markus Waser, Dieter Grossegger, Pengfei Zhang, Georg Dorffner,\n  Stephen Roberts", "title": "Bayesian deep neural networks for low-cost neurophysiological markers of\n  Alzheimer's disease severity", "comments": "Machine Learning for Health (ML4H) Workshop at NeurIPS 2018\n  arXiv:1811.07216", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG eess.SP q-bio.NC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  As societies around the world are ageing, the number of Alzheimer's disease\n(AD) patients is rapidly increasing. To date, no low-cost, non-invasive\nbiomarkers have been established to advance the objectivization of AD diagnosis\nand progression assessment. Here, we utilize Bayesian neural networks to\ndevelop a multivariate predictor for AD severity using a wide range of\nquantitative EEG (QEEG) markers. The Bayesian treatment of neural networks both\nautomatically controls model complexity and provides a predictive distribution\nover the target function, giving uncertainty bounds for our regression task. It\nis therefore well suited to clinical neuroscience, where data sets are\ntypically sparse and practitioners require a precise assessment of the\npredictive uncertainty. We use data of one of the largest prospective AD EEG\ntrials ever conducted to demonstrate the potential of Bayesian deep learning in\nthis domain, while comparing two distinct Bayesian neural network approaches,\ni.e., Monte Carlo dropout and Hamiltonian Monte Carlo.\n", "versions": [{"version": "v1", "created": "Wed, 12 Dec 2018 15:46:12 GMT"}, {"version": "v2", "created": "Thu, 13 Dec 2018 16:05:37 GMT"}], "update_date": "2018-12-14", "authors_parsed": [["Fruehwirt", "Wolfgang", ""], ["Cobb", "Adam D.", ""], ["Mairhofer", "Martin", ""], ["Weydemann", "Leonard", ""], ["Garn", "Heinrich", ""], ["Schmidt", "Reinhold", ""], ["Benke", "Thomas", ""], ["Dal-Bianco", "Peter", ""], ["Ransmayr", "Gerhard", ""], ["Waser", "Markus", ""], ["Grossegger", "Dieter", ""], ["Zhang", "Pengfei", ""], ["Dorffner", "Georg", ""], ["Roberts", "Stephen", ""]]}, {"id": "1812.05156", "submitter": "Justin Faber", "authors": "Justin Faber and Dolores Bozovic", "title": "Chaotic Dynamics Enhance the Sensitivity of Inner Ear Hair Cells", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "q-bio.NC physics.bio-ph", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Hair cells of the auditory and vestibular systems are capable of detecting\nsounds that induce sub-nanometer vibrations of the hair bundle, below the\nstochastic noise levels of the surrounding fluid. Hair bundles of certain\nspecies are also known to oscillate without external stimulation, indicating\nthe presence of an underlying active mechanism. We propose that chaotic\ndynamics enhance the sensitivity and temporal resolution of the hair bundle\nresponse, and provide experimental and theoretical evidence for this effect. By\nvarying the viscosity and ionic composition of the surrounding fluid, we are\nable to modulate the degree of chaos observed in the hair bundle dynamics in\nvitro. We consistently find that the hair bundle is most sensitive to a\nstimulus of small amplitude when it is poised in the weakly chaotic regime.\nFurther, we show that the response time to a force step decreases with\nincreasing levels of chaos. These results agree well with our numerical\nsimulations of a chaotic Hopf oscillator and suggest that chaos may be\nresponsible for the sensitivity and temporal resolution of hair cells.\n", "versions": [{"version": "v1", "created": "Wed, 28 Nov 2018 22:09:58 GMT"}, {"version": "v2", "created": "Tue, 21 May 2019 20:57:54 GMT"}], "update_date": "2019-05-23", "authors_parsed": [["Faber", "Justin", ""], ["Bozovic", "Dolores", ""]]}, {"id": "1812.05556", "submitter": "Liane Gabora", "authors": "Steve DiPaola, Liane Gabora, and Graeme McCaig", "title": "Informing Artificial Intelligence Generative Techniques using Cognitive\n  Theories of Human Creativity", "comments": "18 pages; 6 figures. arXiv admin note: substantial text overlap with\n  arXiv:1610.02478", "journal-ref": "Procedia Computer Science, vol. 145 (pp. 158-168). Amsterdam:\n  Elsevier (2018)", "doi": "10.1016/j.procs.2018.11.024", "report-no": null, "categories": "cs.AI cs.LG q-bio.NC stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The common view that our creativity is what makes us uniquely human suggests\nthat incorporating research on human creativity into generative deep learning\ntechniques might be a fruitful avenue for making their outputs more compelling\nand human-like. Using an original synthesis of Deep Dream-based convolutional\nneural networks and cognitive based computational art rendering systems, we\nshow how honing theory, intrinsic motivation, and the notion of a 'seed\nincident' can be implemented computationally, and demonstrate their impact on\nthe resulting generative art. Conversely, we discuss how explorations in deep\nlearn-ing convolutional neural net generative systems can inform our\nunderstanding of human creativity. We conclude with ideas for further\ncross-fertilization between AI based computational creativity and psychology of\ncreativity.\n", "versions": [{"version": "v1", "created": "Tue, 11 Dec 2018 18:12:44 GMT"}, {"version": "v2", "created": "Fri, 5 Jul 2019 21:00:09 GMT"}], "update_date": "2019-07-09", "authors_parsed": [["DiPaola", "Steve", ""], ["Gabora", "Liane", ""], ["McCaig", "Graeme", ""]]}, {"id": "1812.05668", "submitter": "Jiansheng Wu", "authors": "Hang Yu, Ziyi Liu, Jiansheng Wu", "title": "Forgetting in order to Remember Better", "comments": "4 pages, 2 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "q-bio.NC physics.bio-ph", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In human memory, forgetting occur rapidly after the remembering and the rate\nof forgetting slowed down as time went. This is so-called the Ebbinghaus\nforgetting curve. There are many explanations of how this curve occur based on\nthe properties of the brains. In this article, we use a simple mathematical\nmodel to explain the mechanism of forgetting based on rearrangement inequality\nand get a general formalism for short-term and long-term memory and use it to\nfit the Ebbinghaus forgetting curve. We also find out that forgetting is not a\nflaw, instead it is help to improve the efficiency of remembering when human\nconfront different situations by reducing the interference of information and\nreducing the number of retrievals. Furthurmove, we find that the interference\nof informations limits the capacity of human memory, which is the \"magic number\nseven\".\n", "versions": [{"version": "v1", "created": "Wed, 12 Dec 2018 18:54:27 GMT"}], "update_date": "2018-12-17", "authors_parsed": [["Yu", "Hang", ""], ["Liu", "Ziyi", ""], ["Wu", "Jiansheng", ""]]}, {"id": "1812.06234", "submitter": "Yogatheesan Varatharajah", "authors": "Yogatheesan Varatharajah, Brent Berry, Jan Cimbalnik, Vaclav Kremen,\n  Jamie Van Gompel, Matt Stead, Benjamin Brinkmann, Ravishankar Iyer, and\n  Gregory Worrell", "title": "Integrating Artificial Intelligence with Real-time Intracranial EEG\n  Monitoring to Automate Interictal Identification of Seizure Onset Zones in\n  Focal Epilepsy", "comments": "25 pages, Journal of neural engineering (2018)", "journal-ref": null, "doi": "10.1088/1741-2552/aac960", "report-no": null, "categories": "q-bio.NC cs.AI q-bio.QM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  An ability to map seizure-generating brain tissue, i.e., the seizure onset\nzone (SOZ), without recording actual seizures could reduce the duration of\ninvasive EEG monitoring for patients with drug-resistant epilepsy. A\nwidely-adopted practice in the literature is to compare the incidence\n(events/time) of putative pathological electrophysiological biomarkers\nassociated with epileptic brain tissue with the SOZ determined from spontaneous\nseizures recorded with intracranial EEG, primarily using a single biomarker.\nClinical translation of the previous efforts suffers from their inability to\ngeneralize across multiple patients because of (a) the inter-patient\nvariability and (b) the temporal variability in the epileptogenic activity.\nHere, we report an artificial intelligence-based approach for combining\nmultiple interictal electrophysiological biomarkers and their temporal\ncharacteristics as a way of accounting for the above barriers and show that it\ncan reliably identify seizure onset zones in a study cohort of 82 patients who\nunderwent evaluation for drug-resistant epilepsy. Our investigation provides\nevidence that utilizing the complementary information provided by multiple\nelectrophysiological biomarkers and their temporal characteristics can\nsignificantly improve the localization potential compared to previously\npublished single-biomarker incidence-based approaches, resulting in an average\narea under ROC curve (AUC) value of 0.73 in a cohort of 82 patients. Our\nresults also suggest that recording durations between ninety minutes and two\nhours are sufficient to localize SOZs with accuracies that may prove clinically\nrelevant. The successful validation of our approach on a large cohort of 82\npatients warrants future investigation on the feasibility of utilizing\nintra-operative EEG monitoring and artificial intelligence to localize\nepileptogenic brain tissue.\n", "versions": [{"version": "v1", "created": "Sat, 15 Dec 2018 05:15:40 GMT"}], "update_date": "2018-12-18", "authors_parsed": [["Varatharajah", "Yogatheesan", ""], ["Berry", "Brent", ""], ["Cimbalnik", "Jan", ""], ["Kremen", "Vaclav", ""], ["Van Gompel", "Jamie", ""], ["Stead", "Matt", ""], ["Brinkmann", "Benjamin", ""], ["Iyer", "Ravishankar", ""], ["Worrell", "Gregory", ""]]}, {"id": "1812.06259", "submitter": "Geza Odor", "authors": "G\\'eza \\'Odor", "title": "Robustness of Griffiths effects in homeostatic connectome models", "comments": "9 pages, 10 figures, accepted version in PRE", "journal-ref": "Phys. Rev. E 99, 012113 (2019)", "doi": "10.1103/PhysRevE.99.012113", "report-no": null, "categories": "cond-mat.dis-nn cond-mat.stat-mech physics.bio-ph q-bio.NC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  I provide numerical evidence for the robustness of the Griffiths phase (GP)\nreported previously in dynamical threshold model simulations on a large human\nbrain network with N=836733 connected nodes. The model, with equalized network\nsensitivity, is extended in two ways: introduction of refractory states or by\nrandomized time dependent thresholds. The non-universal power-law dynamics in\nan extended control parameter region survives these modifications for a short\nrefractory state and weak disorder. In case of temporal disorder the GP shrinks\nand for stronger heterogeneity disappears, leaving behind a mean-field type of\ncritical transition. Activity avalanche size distributions below the critical\npoint decay faster than in the original model, but the addition of inhibitory\ninteractions sets it back to the range of experimental values.\n", "versions": [{"version": "v1", "created": "Sat, 15 Dec 2018 09:12:19 GMT"}, {"version": "v2", "created": "Thu, 10 Jan 2019 14:32:07 GMT"}], "update_date": "2019-03-27", "authors_parsed": [["\u00d3dor", "G\u00e9za", ""]]}, {"id": "1812.06317", "submitter": "Nikita Pospelov", "authors": "K.Anokhin, V.Avetisov, A.Gorsky, S.Nechaev, N.Pospelov, O.Valba", "title": "Spectral peculiarity and criticality of the human connectome", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "q-bio.NC cond-mat.dis-nn physics.soc-ph", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We have performed the comparative spectral analysis of structural connectomes\nfor various organisms using open-access data. Our analysis indicates several\nnew peculiar features of the human connectome. We found that the spectral\ndensity of human connectome has the maximal deviation from the spectral density\nof the randomized network compared to all other organisms. For many animals\nexcept human structural peculiarities of connectomes are well reproduced in the\nnetwork evolution induced by the preference of 3-cycles formation. To get the\nreliable fit , we discovered the crucial role of the conservation of local\nclusterization in human connectome evolution. We investigated for the first\ntime the level spacing distribution in the spectrum of human connectome graph\nLaplacian. It turns out that the spectral statistics of human connectome\ncorresponds exactly to the critical regime familiar in the condensed matter\nphysics which is hybrid of Wigner-Dyson and Poisson distributions. This\nobservation provides the strong support for the much debated statement of the\nbrain criticality.\n", "versions": [{"version": "v1", "created": "Sat, 15 Dec 2018 16:28:02 GMT"}], "update_date": "2018-12-18", "authors_parsed": [["Anokhin", "K.", ""], ["Avetisov", "V.", ""], ["Gorsky", "A.", ""], ["Nechaev", "S.", ""], ["Pospelov", "N.", ""], ["Valba", "O.", ""]]}, {"id": "1812.06562", "submitter": "Xinghua Yao", "authors": "X. Yao, X. Li, Q. Ye, Y. Huang, Q. Cheng, and G.-Q. Zhang", "title": "A Robust Deep Learning Approach for Automatic Classification of Seizures\n  Against Non-seizures", "comments": "13 pages, 10 figures, submitted to Biomedical Signal Processing and\n  Control", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG q-bio.NC stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Identifying epileptic seizures through analysis of the electroencephalography\n(EEG) signal becomes a standard method for the diagnosis of epilepsy. Manual\nseizure identification on EEG by trained neurologists is time-consuming,\nlabor-intensive and error-prone, and a reliable automatic seizure/non-seizure\nclassification method is needed. One of the challenges in automatic\nseizure/non-seizure classification is that seizure morphologies exhibit\nconsiderable variabilities. In order to capture essential seizure patterns,\nthis paper leverages an attention mechanism and a bidirectional long short-term\nmemory (BiLSTM) to exploit both spatial and temporal discriminating features\nand overcome seizure variabilities. The attention mechanism is to capture\nspatial features according to the contributions of different brain regions to\nseizures. The BiLSTM is to extract discriminating temporal features in the\nforward and the backward directions. Cross-validation experiments and\ncross-patient experiments over the noisy data of CHB-MIT are performed to\nevaluate our proposed approach. The obtained average sensitivity of 87.00%,\nspecificity of 88.60% and precision of 88.63% in cross-validation experiments\nare higher than using the current state-of-the-art methods, and the standard\ndeviations of our approach are lower. The evaluation results of cross-patient\nexperiments indicate that, our approach has better performance compared with\nthe current state-of-the-art methods and is more robust across patients.\n", "versions": [{"version": "v1", "created": "Mon, 17 Dec 2018 00:03:13 GMT"}, {"version": "v2", "created": "Thu, 6 Jun 2019 00:43:44 GMT"}], "update_date": "2019-06-07", "authors_parsed": [["Yao", "X.", ""], ["Li", "X.", ""], ["Ye", "Q.", ""], ["Huang", "Y.", ""], ["Cheng", "Q.", ""], ["Zhang", "G. -Q.", ""]]}, {"id": "1812.06574", "submitter": "Yunzhe Hao", "authors": "Yunzhe Hao, Xuhui Huang, Meng Dong, Bo Xu", "title": "A Biologically Plausible Supervised Learning Method for Spiking Neural\n  Networks Using the Symmetric STDP Rule", "comments": "29 pages, 6 figures", "journal-ref": "Neural Networks 121C (2020) pp. 387-395", "doi": "10.1016/j.neunet.2019.09.007", "report-no": null, "categories": "cs.NE cs.AI cs.LG q-bio.NC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Spiking neural networks (SNNs) possess energy-efficient potential due to\nevent-based computation. However, supervised training of SNNs remains a\nchallenge as spike activities are non-differentiable. Previous SNNs training\nmethods can be generally categorized into two basic classes, i.e.,\nbackpropagation-like training methods and plasticity-based learning methods.\nThe former methods are dependent on energy-inefficient real-valued computation\nand non-local transmission, as also required in artificial neural networks\n(ANNs), whereas the latter are either considered to be biologically implausible\nor exhibit poor performance. Hence, biologically plausible (bio-plausible)\nhigh-performance supervised learning (SL) methods for SNNs remain deficient. In\nthis paper, we proposed a novel bio-plausible SNN model for SL based on the\nsymmetric spike-timing dependent plasticity (sym-STDP) rule found in\nneuroscience. By combining the sym-STDP rule with bio-plausible synaptic\nscaling and intrinsic plasticity of the dynamic threshold, our SNN model\nimplemented SL well and achieved good performance in the benchmark recognition\ntask (MNIST dataset). To reveal the underlying mechanism of our SL model, we\nvisualized both layer-based activities and synaptic weights using the\nt-distributed stochastic neighbor embedding (t-SNE) method after training and\nfound that they were well clustered, thereby demonstrating excellent\nclassification ability. Furthermore, to verify the robustness of our model, we\ntrained it on another more realistic dataset (Fashion-MNIST), which also showed\ngood performance. As the learning rules were bio-plausible and based purely on\nlocal spike events, our model could be easily applied to neuromorphic hardware\nfor online training and may be helpful for understanding SL information\nprocessing at the synaptic level in biological neural systems.\n", "versions": [{"version": "v1", "created": "Mon, 17 Dec 2018 01:38:14 GMT"}, {"version": "v2", "created": "Mon, 1 Jul 2019 03:33:43 GMT"}, {"version": "v3", "created": "Sun, 6 Oct 2019 09:27:27 GMT"}], "update_date": "2019-10-08", "authors_parsed": [["Hao", "Yunzhe", ""], ["Huang", "Xuhui", ""], ["Dong", "Meng", ""], ["Xu", "Bo", ""]]}, {"id": "1812.06590", "submitter": "Liane Gabora", "authors": "Liane Gabora and Cameron M. Smith", "title": "Exploring the Psychological Basis for Transitions in the Archaeological\n  Record", "comments": "20 pages. arXiv admin note: substantial text overlap with\n  arXiv:1811.10431", "journal-ref": "In Tracy B. Henley, Matthew Rossano, and Edward P. Kardas (Eds.)\n  Handbook of Cognitive Archaeology: A Psychological Framework. (2019)", "doi": null, "report-no": null, "categories": "q-bio.NC q-bio.PE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In lieu of an abstract here is the first paragraph: No other species remotely\napproaches the human capacity for the cultural evolution of novelty that is\naccumulative, adaptive, and open-ended (i.e., with no a priori limit on the\nsize or scope of possibilities). By culture we mean extrasomatic\nadaptations--including behavior and technology--that are socially rather than\nsexually transmitted. This chapter synthesizes research from anthropology,\npsychology, archaeology, and agent-based modeling into a speculative yet\ncoherent account of two fundamental cognitive transitions underlying human\ncultural evolution that is consistent with contemporary psychology. While the\nchapter overlaps with a more technical paper on this topic (Gabora & Smith\n2018), it incorporates new research and elaborates a genetic component to our\noverall argument. The ideas in this chapter grew out of a non-Darwinian\nframework for cultural evolution, referred to as the Self-other Reorganization\n(SOR) theory of cultural evolution (Gabora, 2013, in press; Smith, 2013), which\nwas inspired by research on the origin and earliest stage in the evolution of\nlife (Cornish-Bowden & C\\'ardenas 2017; Goldenfeld, Biancalani, & Jafarpour,\n2017, Vetsigian, Woese, & Goldenfeld 2006; Woese, 2002). SOR bridges\npsychological research on fundamental aspects of our human nature such as\ncreativity and our proclivity to reflect on ideas from different perspectives,\nwith the literature on evolutionary approaches to cultural evolution that\naspire to synthesize the behavioral sciences much as has been done for the\nbiological scientists. The current chapter is complementary to this effort, but\nless abstract; it attempts to ground the theory of cultural evolution in terms\nof cognitive transitions as suggested by archaeological evidence.\n", "versions": [{"version": "v1", "created": "Thu, 13 Dec 2018 23:09:31 GMT"}, {"version": "v2", "created": "Fri, 5 Jul 2019 20:53:47 GMT"}], "update_date": "2021-07-22", "authors_parsed": [["Gabora", "Liane", ""], ["Smith", "Cameron M.", ""]]}, {"id": "1812.06594", "submitter": "Sebastian Mathias Keller", "authors": "Sebastian Mathias Keller, Maxim Samarin, Antonia Meyer, Vitalii Kosak\n  (Cozak), Ute Gschwandtner, Peter Fuhr, Volker Roth", "title": "Computational EEG in Personalized Medicine: A study in Parkinson's\n  Disease", "comments": "Machine Learning for Health (ML4H) Workshop at NeurIPS 2018\n  arXiv:811.07216", "journal-ref": null, "doi": null, "report-no": null, "categories": "q-bio.NC cs.LG eess.SP q-bio.QM stat.ML", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Recordings of electrical brain activity carry information about a person's\ncognitive health. For recording EEG signals, a very common setting is for a\nsubject to be at rest with its eyes closed. Analysis of these recordings often\ninvolve a dimensionality reduction step in which electrodes are grouped into 10\nor more regions (depending on the number of electrodes available). Then an\naverage over each group is taken which serves as a feature in subsequent\nevaluation. Currently, the most prominent features used in clinical practice\nare based on spectral power densities. In our work we consider a simplified\ngrouping of electrodes into two regions only. In addition to spectral features\nwe introduce a secondary, non-redundant view on brain activity through the lens\nof Tsallis Entropy $S_{q=2}$. We further take EEG measurements not only in an\neyes closed (ec) but also in an eyes open (eo) state. For our cohort of healthy\ncontrols (HC) and individuals suffering from Parkinson's disease (PD), the\nquestion we are asking is the following: How well can one discriminate between\nHC and PD within this simplified, binary grouping? This question is motivated\nby the commercial availability of inexpensive and easy to use portable EEG\ndevices. If enough information is retained in this binary grouping, then such\nsimple devices could potentially be used as personal monitoring tools, as\nstandard screening tools by general practitioners or as digital biomarkers for\neasy long term monitoring during neurological studies.\n", "versions": [{"version": "v1", "created": "Sun, 2 Dec 2018 15:15:44 GMT"}], "update_date": "2018-12-18", "authors_parsed": [["Keller", "Sebastian Mathias", "", "Cozak"], ["Samarin", "Maxim", "", "Cozak"], ["Meyer", "Antonia", "", "Cozak"], ["Kosak", "Vitalii", "", "Cozak"], ["Gschwandtner", "Ute", ""], ["Fuhr", "Peter", ""], ["Roth", "Volker", ""]]}, {"id": "1812.06919", "submitter": "Manuel Beiran", "authors": "Manuel Beiran, Srdjan Ostojic", "title": "Contrasting the effects of adaptation and synaptic filtering on the\n  timescales of dynamics in recurrent networks", "comments": null, "journal-ref": "PLOS Computational Biology 15(3): e1006893 (2019)", "doi": "10.1371/journal.pcbi.1006893", "report-no": null, "categories": "q-bio.NC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Neural activity exhibits a vast range of timescales that can be several fold\nlarger than the membrane time constant of individual neurons. Two types of\nmechanisms have been proposed to explain this conundrum. One possibility is\nthat large timescales are generated by a network mechanism based on positive\nfeedback, but this hypothesis requires fine-tuning of the synaptic connections.\nA second possibility is that large timescales in the neural dynamics are\ninherited from large timescales of underlying biophysical processes, two\nprominent candidates being adaptive ionic currents and synaptic transmission.\nHow the timescales of these processes influence the timescale of the network\ndynamics has however not been fully explored. To address this question, we\nanalyze large networks of randomly connected excitatory and inhibitory units\nwith additional degrees of freedom that correspond to adaptation or synaptic\nfiltering. We determine the fixed points of the systems, their stability to\nperturbations and the corresponding dynamical timescales. Furthermore, we apply\ndynamical mean field theory to study the temporal statistics of the activity in\nthe fluctuating regime, and examine how the adaptation and synaptic timescales\ntransfer from individual units to the whole population. Our overarching finding\nis that synaptic filtering and adaptation in single neurons have very different\neffects at the network level. Unexpectedly, the macroscopic network dynamics do\nnot inherit the large timescale present in adaptive currents. In contrast, the\ntimescales of network activity increase proportionally to the time constant of\nthe synaptic filter. Altogether, our study demonstrates that the timescales of\ndifferent biophysical processes have different effects on the network level, so\nthat the slow processes within individual neurons do not necessarily induce\nslow activity in large recurrent neural networks.\n", "versions": [{"version": "v1", "created": "Mon, 17 Dec 2018 17:42:22 GMT"}, {"version": "v2", "created": "Mon, 25 Mar 2019 16:09:58 GMT"}], "update_date": "2019-03-26", "authors_parsed": [["Beiran", "Manuel", ""], ["Ostojic", "Srdjan", ""]]}, {"id": "1812.06925", "submitter": "Samuel Muscinelli", "authors": "Samuel P. Muscinelli, Wulfram Gerstner, Tilo Schwalger", "title": "How single neuron properties shape chaotic dynamics and signal\n  transmission in random neural networks", "comments": null, "journal-ref": null, "doi": "10.1371/journal.pcbi.1007122", "report-no": null, "categories": "cond-mat.dis-nn q-bio.NC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  While most models of randomly connected networks assume nodes with simple\ndynamics, nodes in realistic highly connected networks, such as neurons in the\nbrain, exhibit intrinsic dynamics over multiple timescales. We analyze how the\ndynamical properties of nodes (such as single neurons) and recurrent\nconnections interact to shape the effective dynamics in large randomly\nconnected networks. A novel dynamical mean-field theory for strongly connected\nnetworks of multi-dimensional rate units shows that the power spectrum of the\nnetwork activity in the chaotic phase emerges from a nonlinear sharpening of\nthe frequency response function of single units. For the case of\ntwo-dimensional rate units with strong adaptation, we find that the network\nexhibits a state of \"resonant chaos\", characterized by robust, narrow-band\nstochastic oscillations. The coherence of stochastic oscillations is maximal at\nthe onset of chaos and their correlation time scales with the adaptation\ntimescale of single units. Surprisingly, the resonance frequency can be\npredicted from the properties of isolated units, even in the presence of\nheterogeneity in the adaptation parameters. In the presence of these\ninternally-generated chaotic fluctuations, the transmission of weak,\nlow-frequency signals is strongly enhanced by adaptation, whereas signal\ntransmission is not influenced by adaptation in the non-chaotic regime. Our\ntheoretical framework can be applied to other mechanisms at the level of single\nnodes, such as synaptic filtering, refractoriness or spike synchronization.\nThese results advance our understanding of the interaction between the dynamics\nof single units and recurrent connectivity, which is a fundamental step toward\nthe description of biologically realistic network models in the brain, or, more\ngenerally, networks of other physical or man-made complex dynamical units.\n", "versions": [{"version": "v1", "created": "Mon, 17 Dec 2018 17:59:23 GMT"}, {"version": "v2", "created": "Thu, 28 Feb 2019 18:34:20 GMT"}], "update_date": "2019-09-11", "authors_parsed": [["Muscinelli", "Samuel P.", ""], ["Gerstner", "Wulfram", ""], ["Schwalger", "Tilo", ""]]}, {"id": "1812.07214", "submitter": "Lauriane Veron-Delor", "authors": "Lauriane V\\'eron-Delor (LNC, LPL), Serge Pinto (LPL), Alexandre\n  Eusebio (TIMONE), Jean-Luc Velay (LNC), J\\'er\\'emy Danna (LNC)", "title": "Music and musical sonification for the rehabilitation of Parkinsonian\n  dysgraphia: Conceptual framework", "comments": null, "journal-ref": "Music Technology with Swing, pp.312-326, 2018", "doi": null, "report-no": null, "categories": "q-bio.NC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Music has been shown to enhance motor control in patients with Parkinson's\ndisease (PD). Notably, musical rhythm is perceived as an external auditory cue\nthat helps PD patients to better control movements. The rationale of such\neffects is that motor control based on auditory guidance would activate a\ncompensatory brain network that minimizes the recruitment of the defective\npathway involving the basal ganglia. Would associating music to movement\nimprove its perception and control in PD? Musical sonification consists in\nmodifying in real-time the playback of a preselected music according to some\nmovement parameters. The validation of such a method is underway for\nhandwriting in PD patients. When confirmed, this study will strengthen the\nclinical interest of musical sonification in motor control and (re)learning in\nPD.\n", "versions": [{"version": "v1", "created": "Tue, 18 Dec 2018 07:41:58 GMT"}], "update_date": "2018-12-19", "authors_parsed": [["V\u00e9ron-Delor", "Lauriane", "", "LNC, LPL"], ["Pinto", "Serge", "", "LPL"], ["Eusebio", "Alexandre", "", "TIMONE"], ["Velay", "Jean-Luc", "", "LNC"], ["Danna", "J\u00e9r\u00e9my", "", "LNC"]]}, {"id": "1812.07328", "submitter": "Gestionnaire Hal-Su", "authors": "Claire Meyniel, Dalila Samri (IM2A), Farah Stefano (CH St Joseph),\n  Joel Crevoisier, Florence Bont\\'e (SAPPH), Raffaella Migliaccio (ICM, IM2A,\n  UPMC), Laure Delaby (IM2A), Anne Bertrand (ARAMIS, UPMC, ICM), Marie Odile\n  Habert (CATI), Bruno Dubois (UPMC, ICM, IM2A), Bahram Bodaghi, St\\'ephane\n  Epelbaum (IM2A, ARAMIS, UPMC, ICM)", "title": "COGEVIS: A New Scale to Evaluate Cognition in Patients with Visual\n  Deficiency", "comments": null, "journal-ref": "Behavioural Neurology, IOS Press, 2018, 2018, pp.1-7", "doi": "10.1155/2018/4295184", "report-no": null, "categories": "q-bio.NC cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We evaluated the cognitive status of visually impaired patients referred to\nlow vision rehabilitation (LVR) based on a standard cognitive battery and a new\nevaluation tool, named the COGEVIS, which can be used to assess patients with\nsevere visual deficits. We studied patients aged 60 and above, referred to the\nLVR Hospital in Paris. Neurological and cognitive evaluations were performed in\nan expert memory center. Thirty-eight individuals, 17 women and 21 men with a\nmean age of 70.3 $\\pm$ 1.3 years and a mean visual acuity of 0.12 $\\pm$ 0.02,\nwere recruited over a one-year period. Sixty-three percent of participants had\nnormal cognitive status. Cognitive impairment was diagnosed in 37.5% of\nparticipants. The COGEVIS score cutoff point to screen for cognitive impairment\nwas 24 (maximum score of 30) with a sensitivity of 66.7% and a specificity of\n95%. Evaluation following 4 months of visual rehabilitation showed an\nimprovement of Instrumental Activities of Daily Living (p = 0 004), National\nEye Institute Visual Functioning Questionnaire (p = 0 035), and\nMontgomery-{\\AA}sberg Depression Rating Scale (p = 0 037). This study\nintroduces a new short test to screen for cognitive impairment in visually\nimpaired patients.\n", "versions": [{"version": "v1", "created": "Tue, 18 Dec 2018 12:28:48 GMT"}], "update_date": "2018-12-19", "authors_parsed": [["Meyniel", "Claire", "", "IM2A"], ["Samri", "Dalila", "", "IM2A"], ["Stefano", "Farah", "", "CH St Joseph"], ["Crevoisier", "Joel", "", "SAPPH"], ["Bont\u00e9", "Florence", "", "SAPPH"], ["Migliaccio", "Raffaella", "", "ICM, IM2A,\n  UPMC"], ["Delaby", "Laure", "", "IM2A"], ["Bertrand", "Anne", "", "ARAMIS, UPMC, ICM"], ["Habert", "Marie Odile", "", "CATI"], ["Dubois", "Bruno", "", "UPMC, ICM, IM2A"], ["Bodaghi", "Bahram", "", "IM2A, ARAMIS, UPMC, ICM"], ["Epelbaum", "St\u00e9phane", "", "IM2A, ARAMIS, UPMC, ICM"]]}, {"id": "1812.07697", "submitter": "Hamad Ahmed", "authors": "Ren Li, Jared S. Johansen, Hamad Ahmed, Thomas V. Ilyevsky, Ronnie B\n  Wilbur, Hari M Bharadwaj, and Jeffrey Mark Siskind", "title": "Training on the test set? An analysis of Spampinato et al. [31]", "comments": "18 Pages, 4 Figures, 10 Tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG q-bio.NC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A recent paper [31] claims to classify brain processing evoked in subjects\nwatching ImageNet stimuli as measured with EEG and to use a representation\nderived from this processing to create a novel object classifier. That paper,\ntogether with a series of subsequent papers [8, 15, 17, 20, 21, 30, 35], claims\nto revolutionize the field by achieving extremely successful results on several\ncomputer-vision tasks, including object classification, transfer learning, and\ngeneration of images depicting human perception and thought using brain-derived\nrepresentations measured through EEG. Our novel experiments and analyses\ndemonstrate that their results crucially depend on the block design that they\nuse, where all stimuli of a given class are presented together, and fail with a\nrapid-event design, where stimuli of different classes are randomly intermixed.\nThe block design leads to classification of arbitrary brain states based on\nblock-level temporal correlations that tend to exist in all EEG data, rather\nthan stimulus-related activity. Because every trial in their test sets comes\nfrom the same block as many trials in the corresponding training sets, their\nblock design thus leads to surreptitiously training on the test set. This\ninvalidates all subsequent analyses performed on this data in multiple\npublished papers and calls into question all of the purported results. We\nfurther show that a novel object classifier constructed with a random codebook\nperforms as well as or better than a novel object classifier constructed with\nthe representation extracted from EEG data, suggesting that the performance of\ntheir classifier constructed with a representation extracted from EEG data does\nnot benefit at all from the brain-derived representation. Our results calibrate\nthe underlying difficulty of the tasks involved and caution against sensational\nand overly optimistic, but false, claims to the contrary.\n", "versions": [{"version": "v1", "created": "Tue, 18 Dec 2018 23:38:28 GMT"}], "update_date": "2018-12-20", "authors_parsed": [["Li", "Ren", ""], ["Johansen", "Jared S.", ""], ["Ahmed", "Hamad", ""], ["Ilyevsky", "Thomas V.", ""], ["Wilbur", "Ronnie B", ""], ["Bharadwaj", "Hari M", ""], ["Siskind", "Jeffrey Mark", ""]]}, {"id": "1812.07965", "submitter": "Yali Amit", "authors": "Yali Amit", "title": "Deep learning with asymmetric connections and Hebbian updates", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NE cs.LG q-bio.NC stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We show that deep networks can be trained using Hebbian updates yielding\nsimilar performance to ordinary back-propagation on challenging image datasets.\nTo overcome the unrealistic symmetry in connections between layers, implicit in\nback-propagation, the feedback weights are separate from the feedforward\nweights. The feedback weights are also updated with a local rule, the same as\nthe feedforward weights - a weight is updated solely based on the product of\nactivity of the units it connects. With fixed feedback weights as proposed in\nLillicrap et. al (2016) performance degrades quickly as the depth of the\nnetwork increases. If the feedforward and feedback weights are initialized with\nthe same values, as proposed in Zipser and Rumelhart (1990), they remain the\nsame throughout training thus precisely implementing back-propagation. We show\nthat even when the weights are initialized differently and at random, and the\nalgorithm is no longer performing back-propagation, performance is comparable\non challenging datasets. We also propose a cost function whose derivative can\nbe represented as a local Hebbian update on the last layer. Convolutional\nlayers are updated with tied weights across space, which is not biologically\nplausible. We show that similar performance is achieved with untied layers,\nalso known as locally connected layers, corresponding to the connectivity\nimplied by the convolutional layers, but where weights are untied and updated\nseparately. In the linear case we show theoretically that the convergence of\nthe error to zero is accelerated by the update of the feedback weights.\n", "versions": [{"version": "v1", "created": "Mon, 19 Nov 2018 20:40:29 GMT"}, {"version": "v2", "created": "Tue, 12 Mar 2019 21:05:57 GMT"}], "update_date": "2019-03-14", "authors_parsed": [["Amit", "Yali", ""]]}, {"id": "1812.08031", "submitter": "Gestionnaire Hal-Su", "authors": "St\\'ephane Epelbaum (ICM, IM2A, UPMC), Vincent Bouteloup (ISPED, BPH),\n  Jean Mangin (CATI, NEUROSPIN), Valentina La Corte (UPD5 Psychologie, CPN -\n  U894), Raffaela Migliaccio (ICM, IM2A, UPMC), Hugo Bertin (LIB, CATI, UPMC),\n  Marie O. Habert (LIB, CATI, UPMC), Clara Fischer (CATI, NEUROSPIN), Chabha\n  Azouani (CATI), Ludovic Fillon (CATI), Marie Chupin (CATI), Bruno Vellas,\n  Florence Pasquier, Jean Dartigues (BPH, ISPED), Fr\\'ed\\'eric Blanc, Audrey\n  Gabelle (CHRU Montpellier), Mathieu Ceccaldi (INS, TIMONE), Pierre\n  Krolak-Salmon (HCL), Jacques Hugon (UPD7), Olivier Hanon (UPD5), Olivier\n  Rouaud (CHU Dijon), Renaud David (CMRR Nice), Genevi\\`eve Ch\\^ene (BPH,\n  ISPED), Bruno Dubois (IM2A, ICM, UPMC), Carole Dufouil (BPH, ISPED)", "title": "Neural correlates of episodic memory in the Memento cohort", "comments": null, "journal-ref": "Alzheimer's & Dementia: Translational Research & Clinical\n  Interventions, 2018, 4, pp.224-233", "doi": "10.1016/j.trci.2018.03.010", "report-no": null, "categories": "q-bio.NC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  IntroductionThe free and cued selective reminding test is used to identify\nmemory deficits in mild cognitive impairment and demented patients. It allows\nassessing three processes: encoding, storage, and recollection of verbal\nepisodic memory.MethodsWe investigated the neural correlates of these three\nmemory processes in a large cohort study. The Memento cohort enrolled 2323\noutpatients presenting either with subjective cognitive decline or mild\ncognitive impairment who underwent cognitive, structural MRI and, for a subset,\nfluorodeoxyglucose--positron emission tomography evaluations.ResultsEncoding\nwas associated with a network including parietal and temporal cortices; storage\nwas mainly associated with entorhinal and parahippocampal regions, bilaterally;\nretrieval was associated with a widespread network encompassing frontal\nregions.DiscussionThe neural correlates of episodic memory processes can be\nassessed in large and standardized cohorts of patients at risk for Alzheimer's\ndisease. Their relation to pathophysiological markers of Alzheimer's disease\nremains to be studied.\n", "versions": [{"version": "v1", "created": "Wed, 19 Dec 2018 15:44:51 GMT"}], "update_date": "2018-12-20", "authors_parsed": [["Epelbaum", "St\u00e9phane", "", "ICM, IM2A, UPMC"], ["Bouteloup", "Vincent", "", "ISPED, BPH"], ["Mangin", "Jean", "", "CATI, NEUROSPIN"], ["La Corte", "Valentina", "", "UPD5 Psychologie, CPN -\n  U894"], ["Migliaccio", "Raffaela", "", "ICM, IM2A, UPMC"], ["Bertin", "Hugo", "", "LIB, CATI, UPMC"], ["Habert", "Marie O.", "", "LIB, CATI, UPMC"], ["Fischer", "Clara", "", "CATI, NEUROSPIN"], ["Azouani", "Chabha", "", "CATI"], ["Fillon", "Ludovic", "", "CATI"], ["Chupin", "Marie", "", "CATI"], ["Vellas", "Bruno", "", "BPH, ISPED"], ["Pasquier", "Florence", "", "BPH, ISPED"], ["Dartigues", "Jean", "", "BPH, ISPED"], ["Blanc", "Fr\u00e9d\u00e9ric", "", "CHRU Montpellier"], ["Gabelle", "Audrey", "", "CHRU Montpellier"], ["Ceccaldi", "Mathieu", "", "INS, TIMONE"], ["Krolak-Salmon", "Pierre", "", "HCL"], ["Hugon", "Jacques", "", "UPD7"], ["Hanon", "Olivier", "", "UPD5"], ["Rouaud", "Olivier", "", "CHU Dijon"], ["David", "Renaud", "", "CMRR Nice"], ["Ch\u00eane", "Genevi\u00e8ve", "", "BPH,\n  ISPED"], ["Dubois", "Bruno", "", "IM2A, ICM, UPMC"], ["Dufouil", "Carole", "", "BPH, ISPED"]]}, {"id": "1812.08238", "submitter": "Kenneth Latimer", "authors": "Kenneth W. Latimer", "title": "Nonlinear demixed component analysis for neural population data as a\n  low-rank kernel regression problem", "comments": "Accepted for publication in Neurons, Behavior, Data Analysis, and\n  Theory", "journal-ref": "Neurons, Behavior, Data Analysis, and Theory, 2019", "doi": null, "report-no": null, "categories": "q-bio.NC", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Many studies of neural activity in behaving animals aim to discover\ninterpretable low-dimensional structure in large-scale neural population\nrecordings. One approach to this problem is demixed principal component\nanalysis (dPCA), a supervised linear dimensionality reduction technique to find\ncomponents that depend on particular experimental parameters. Here, I introduce\nkernel dPCA (kdPCA) as a nonlinear extension of dPCA by applying kernel\nleast-squares regression to the demixing problem. I consider simulated examples\nof neural populations with low-dimensional activity to compare the components\nrecovered from dPCA and kdPCA. These simulations demonstrate that neurally\nrelevant nonlinearities, such as stimulus-dependent gain and rotation,\ninterfere with linear demixing of neural activity into components that\nrepresent to individual experimental parameters. However, kdPCA can still\nrecover interpretable components in these examples. Finally, I demonstrate\nkdPCA using two examples of neural populations recorded during perceptual\ndecision-making tasks.\n", "versions": [{"version": "v1", "created": "Wed, 19 Dec 2018 20:53:46 GMT"}, {"version": "v2", "created": "Tue, 26 Feb 2019 21:29:38 GMT"}, {"version": "v3", "created": "Mon, 9 Dec 2019 20:27:52 GMT"}], "update_date": "2019-12-11", "authors_parsed": [["Latimer", "Kenneth W.", ""]]}, {"id": "1812.08308", "submitter": "Benjamin Smith", "authors": "Benjamin J. Smith, Stephen J. Read", "title": "A multiple attribute model resolves a conflict between additive and\n  multiplicative models of incentive salience", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "q-bio.NC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A model of incentive salience as a function of stimulus value and\ninteroceptive state has been previously proposed. In that model, the function\ndiffers depending on whether the stimulus is appetitive or aversive; it is\nmultiplicative for appetitive stimuli and additive for aversive stimuli. The\nauthors argued it was necessary to capture data on how extreme changes in salt\nappetite could move evaluation of an extreme salt solution from negative to\npositive. We demonstrate that arbitrarily varying this function is unnecessary,\nand that a multiplicative function is sufficient if one assumes the incentive\nsalience function for an incentive (such as salt) is comprised of multiple\nstimulus features and multiple interoceptive signals. We show that it is also\nunnecessary considering the dual-structure approach-aversive nature of the\nreward system, which results in separate weighting of appetitive and aversive\nstimulus features.\n", "versions": [{"version": "v1", "created": "Thu, 20 Dec 2018 01:35:14 GMT"}], "update_date": "2018-12-21", "authors_parsed": [["Smith", "Benjamin J.", ""], ["Read", "Stephen J.", ""]]}, {"id": "1812.08430", "submitter": "Hamid Reza Mahdiani", "authors": "Hamid Reza Mahdiani (Computer Science and Engineering Department,\n  Shahid Beheshti University, Tehran, IRAN-Institute for Cognitive and Brain\n  Science, Shahid Beheshti University, Tehran, Iran), Mahdi Nazm Bojnordi\n  (School of Computing, University of Utah, Salt Lake City, USA), Sied Mehdi\n  Fakhraie (Electrical and Computer Engineering Department, University of\n  Tehran, Tehran, IRAN)", "title": "Soft Realization: a Bio-inspired Implementation Paradigm", "comments": "The Imprecise (Approximate) computing and Relaxed Fault Tolerance\n  concept are some but not all instances of the Soft Realization. The soft\n  realization and imprecise computing are first introduced around 2005 as H.R.\n  Mahdiani Phd Thesis proposal. The first imprecise computing paper is\n  published in 2010. This manuscript is written in 2012, submitted to Nature in\n  2017 and rejected by the editors", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.ET eess.SP q-bio.NC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Researchers traditionally solve the computational problems through rigorous\nand deterministic algorithms called as Hard Computing. These precise algorithms\nhave widely been realized using digital technology as an inherently reliable\nand accurate implementation platform, either in hardware or software forms.\nThis rigid form of implementation which we refer as Hard Realization relies on\nstrict algorithmic accuracy constraints dictated to digital design engineers.\nHard realization admits paying as much as necessary implementation costs to\npreserve computation precision and determinism throughout all the design and\nimplementation steps. Despite its prior accomplishments, this conventional\nparadigm has encountered serious challenges with today's emerging applications\nand implementation technologies. Unlike traditional hard computing, the\nemerging soft and bio-inspired algorithms do not rely on fully precise and\ndeterministic computation. Moreover, the incoming nanotechnologies face\nincreasing reliability issues that prevent them from being efficiently\nexploited in hard realization of applications. This article examines Soft\nRealization, a novel bio-inspired approach to design and implementation of an\nimportant category of applications noticing the internal brain structure. The\nproposed paradigm mitigates major weaknesses of hard realization by (1)\nalleviating incompatibilities with today's soft and bio-inspired algorithms\nsuch as artificial neural networks, fuzzy systems, and human sense signal\nprocessing applications, and (2) resolving the destructive inconsistency with\nunreliable nanotechnologies. Our experimental results on a set of well-known\nsoft applications implemented using the proposed soft realization paradigm in\nboth reliable and unreliable technologies indicate that significant energy,\ndelay, and area savings can be obtained compared to the conventional\nimplementation.\n", "versions": [{"version": "v1", "created": "Thu, 20 Dec 2018 09:23:32 GMT"}], "update_date": "2018-12-21", "authors_parsed": [["Mahdiani", "Hamid Reza", "", "Computer Science and Engineering Department,\n  Shahid Beheshti University, Tehran, IRAN-Institute for Cognitive and Brain\n  Science, Shahid Beheshti University, Tehran, Iran"], ["Bojnordi", "Mahdi Nazm", "", "School of Computing, University of Utah, Salt Lake City, USA"], ["Fakhraie", "Sied Mehdi", "", "Electrical and Computer Engineering Department, University of\n  Tehran, Tehran, IRAN"]]}, {"id": "1812.08857", "submitter": "Dionysios Georgiadis", "authors": "Dionysios Georgiadis and Didier Sornette", "title": "Pattern phase diagram of spiking neurons on spatial networks", "comments": null, "journal-ref": "Phys. Rev. E 99, 042410 (2019)", "doi": "10.1103/PhysRevE.99.042410", "report-no": null, "categories": "q-bio.NC nlin.AO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study an abstracted model of neuronal activity via numerical simulation,\nand report spatiotemporal pattern formation and critical like dynamics. A\npopulation of pulse coupled, discretised, relaxation oscillators is simulated\nover networks with varying edge density and spatial embedded ness. For\nintermediate edge density and sufficiently strong spatial embeddedness, we\nobserve a novel spatiotemporal pattern in the field of oscillator phases,\nvisually resembling the surface of a frothing liquid. Increasing the edge\ndensity results in critical dynamics, with the distribution of neuronal\navalanche sizes following a power law with exponent one. Further increase of\nthe edge density results in metastable behaviour between pattern formation and\nsynchronisation, before transitioning the system entirely into synchrony.\n", "versions": [{"version": "v1", "created": "Wed, 5 Dec 2018 08:14:17 GMT"}], "update_date": "2019-04-24", "authors_parsed": [["Georgiadis", "Dionysios", ""], ["Sornette", "Didier", ""]]}, {"id": "1812.09123", "submitter": "Martino Sorbaro Sindaci", "authors": "Martino Sorbaro and J. Michael Herrmann and Matthias H. Hennig", "title": "Statistical models of neural activity, criticality, and Zipf's law", "comments": "23 pages, 7 figures. Originally prepared as a book chapter for the\n  volume \"The Functional Role of Critical Dynamics in Neural Systems\" (Edited\n  by Udo Ernst, Nergis Tomen and Michael Herrmann)", "journal-ref": null, "doi": null, "report-no": null, "categories": "q-bio.NC cond-mat.dis-nn nlin.AO", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  In this overview, we discuss the connections between the observations of\ncritical dynamics in neuronal networks and the maximum entropy models that are\noften used as statistical models of neural activity, focusing in particular on\nthe relation between \"statistical\" and \"dynamical\" criticality. We present\nexamples of systems that are critical in one way, but not in the other,\nexemplifying thus the difference of the two concepts. We then discuss the\nemergence of Zipf laws in neural activity, verifying their presence in retinal\nactivity under a number of different conditions. In the second part of the\nchapter we review connections between statistical criticality and the structure\nof the parameter space, as described by Fisher information. We note that the\nmodel-based signature of criticality, namely the divergence of specific heat,\nemerges independently of the dataset studied; we suggest this is compatible\nwith previous theoretical findings.\n", "versions": [{"version": "v1", "created": "Fri, 21 Dec 2018 14:03:01 GMT"}], "update_date": "2018-12-24", "authors_parsed": [["Sorbaro", "Martino", ""], ["Herrmann", "J. Michael", ""], ["Hennig", "Matthias H.", ""]]}, {"id": "1812.09218", "submitter": "Paul Manz", "authors": "Paul Manz, Sven Goedeke, Raoul-Martin Memmesheimer", "title": "Dynamics and computation in mixed networks containing neurons that\n  accelerate towards spiking", "comments": "v2, 21 pages, 11 figures", "journal-ref": "Phys. Rev. E 100, 042404 (2019)", "doi": "10.1103/PhysRevE.100.042404", "report-no": null, "categories": "q-bio.NC cond-mat.dis-nn nlin.CD", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Networks in the brain consist of different types of neurons. Here we\ninvestigate the influence of neuron diversity on the dynamics, phase space\nstructure and computational capabilities of spiking neural networks. We find\nthat already a single neuron of a different type can qualitatively change the\nnetwork dynamics and that mixed networks may combine the computational\ncapabilities of ones with a single neuron type. We study inhibitory networks of\nconcave leaky (LIF) and convex \"anti-leaky\" (XIF) integrate-and-fire neurons\nthat generalize irregularly spiking non-chaotic LIF neuron networks. Endowed\nwith simple conductance-based synapses for XIF neurons, our networks can\ngenerate a balanced state of irregular asynchronous spiking as well. We\ndetermine the voltage probability distributions and self-consistent firing\nrates assuming Poisson input with finite size spike impacts. Further, we\ncompute the full spectrum of Lyapunov exponents (LEs) and the covariant\nLyapunov vectors (CLVs) specifying the corresponding perturbation directions.\nWe find that there is approximately one positive LE for each XIF neuron. This\nindicates in particular that a single XIF neuron renders the network dynamics\nchaotic. A simple mean-field approach, which can be justified by properties of\nthe CLVs, explains the finding. As an application, we propose a spike-based\ncomputing scheme where our networks serve as computational reservoirs and their\ndifferent stability properties yield different computational capabilities.\n", "versions": [{"version": "v1", "created": "Fri, 21 Dec 2018 16:02:32 GMT"}, {"version": "v2", "created": "Thu, 15 Aug 2019 16:43:33 GMT"}], "update_date": "2019-10-09", "authors_parsed": [["Manz", "Paul", ""], ["Goedeke", "Sven", ""], ["Memmesheimer", "Raoul-Martin", ""]]}, {"id": "1812.09361", "submitter": "Harang Ju", "authors": "Harang Ju, Jason Z. Kim, Danielle S. Bassett", "title": "Network structure of cascading neural systems predicts stimulus\n  propagation and recovery", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "q-bio.NC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Many neural systems display cascading behavior characterized by uninterrupted\nsequences of neuronal firing. This gap precludes an understanding of how\nvariations in network structure manifest in neural dynamics and either support\nor impinge upon information processing. Here, we develop a theoretical\nunderstanding of how network structure supports information processing through\nnetwork dynamics, and we validate our theory with empirical data. Using a\ngeneralized spiking model and mathematical tools from linear systems theory,\nnetwork control theory, and information theory, we show how network structure\ncan be designed to temporally extend the propagation and recovery of certain\nstimulus patterns. Moreover, we observe cycles as structural and dynamic motifs\nthat are prevalent in such networks. Broadly, our results demonstrate how\ncascading neural networks could contribute to cognitive faculties that require\nlasting activation of neuronal patterns, such as working memory or attention.\n", "versions": [{"version": "v1", "created": "Fri, 21 Dec 2018 20:24:57 GMT"}, {"version": "v2", "created": "Sun, 10 Nov 2019 21:51:36 GMT"}], "update_date": "2019-11-12", "authors_parsed": [["Ju", "Harang", ""], ["Kim", "Jason Z.", ""], ["Bassett", "Danielle S.", ""]]}, {"id": "1812.09362", "submitter": "Logan Thrasher Collins", "authors": "Logan Thrasher Collins", "title": "The case for emulating insect brains using anatomical \"wiring diagrams\"\n  equipped with biophysical models of neuronal activity", "comments": "25 pages, 2 figures. Biological Cybernetics", "journal-ref": null, "doi": "10.1007/s00422-019-00810-z", "report-no": null, "categories": "q-bio.NC", "license": "http://creativecommons.org/licenses/by-sa/4.0/", "abstract": "  Developing whole-brain emulation (WBE) technology would provide immense\nbenefits across neuroscience, biomedicine, artificial intelligence, and\nrobotics. At this time, constructing a simulated human brain lacks feasibility\ndue to limited experimental data and limited computational resources. However,\nI suggest that progress towards this goal might be accelerated by working\ntowards an intermediate objective, namely insect brain emulation (IBE). More\nspecifically, this would entail creating biologically realistic simulations of\nentire insect nervous systems along with more approximate simulations of\nnon-neuronal insect physiology to make \"virtual insects.\" I argue that this\ncould be realistically achievable within the next 20 years. I propose that\ndeveloping emulations of insect brains will galvanize the global community of\nscientists, businesspeople, and policymakers towards pursuing the loftier goal\nof emulating the human brain. By demonstrating that WBE is possible via IBE,\nsimulating mammalian brains and eventually the human brain may no longer be\nviewed as too radically ambitious to deserve substantial funding and resources.\nFurthermore, IBE will facilitate dramatic advances in cognitive neuroscience,\nartificial intelligence, and robotics through studies performed using virtual\ninsects.\n", "versions": [{"version": "v1", "created": "Fri, 7 Dec 2018 06:00:34 GMT"}, {"version": "v2", "created": "Wed, 1 May 2019 18:15:04 GMT"}, {"version": "v3", "created": "Sun, 27 Oct 2019 19:30:14 GMT"}], "update_date": "2019-11-07", "authors_parsed": [["Collins", "Logan Thrasher", ""]]}, {"id": "1812.09414", "submitter": "Tilo Schwalger", "authors": "Valentin Schmutz, Wulfram Gerstner and Tilo Schwalger", "title": "Mesoscopic population equations for spiking neural networks with\n  synaptic short-term plasticity", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "q-bio.NC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Coarse-graining microscopic models of biological neural networks to obtain\nmesoscopic models of neural activities is an essential step towards multi-scale\nmodels of the brain. Here, we extend a recent theory for mesoscopic population\ndynamics with static synapses to the case of dynamic synapses exhibiting\nshort-term plasticity (STP). Under the assumption that spike arrivals at\nsynapses have Poisson statistics, we derive analytically stochastic mean-field\ndynamics for the effective synaptic coupling between finite-size populations\nundergoing Tsodyks-Markram STP. The novel mean-field equations account for both\nfinite number of synapses and correlations between the neurotransmitter release\nprobability and the fraction of available synaptic resources. Comparisons with\nMonte Carlo simulations of the microscopic model show that in both feedforward\nand recurrent networks the mesoscopic mean-field model accurately reproduces\nstochastic realizations of the total synaptic input into a postsynaptic neuron\nand accounts for stochastic switches between Up and Down states as well as for\npopulation spikes. The extended mesoscopic population theory of spiking neural\nnetworks with STP may be useful for a systematic reduction of detailed\nbiophysical models of cortical microcircuits to efficient and mathematically\ntractable mean-field models.\n", "versions": [{"version": "v1", "created": "Fri, 21 Dec 2018 23:36:11 GMT"}], "update_date": "2018-12-27", "authors_parsed": [["Schmutz", "Valentin", ""], ["Gerstner", "Wulfram", ""], ["Schwalger", "Tilo", ""]]}, {"id": "1812.09431", "submitter": "Chi Zhang", "authors": "Chi Zhang, Xiaohan Duan, Linyuan Wang, Yongli Li, Bin Yan, Guoen Hu,\n  Ruyuan Zhang, Li Tong", "title": "Dissociable neural representations of adversarially perturbed images in\n  convolutional neural networks and the human brain", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV q-bio.NC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Despite the remarkable similarities between convolutional neural networks\n(CNN) and the human brain, CNNs still fall behind humans in many visual tasks,\nindicating that there still exist considerable differences between the two\nsystems. Here, we leverage adversarial noise (AN) and adversarial interference\n(AI) images to quantify the consistency between neural representations and\nperceptual outcomes in the two systems. Humans can successfully recognize AI\nimages as corresponding categories but perceive AN images as meaningless noise.\nIn contrast, CNNs can correctly recognize AN images but mistakenly classify AI\nimages into wrong categories with surprisingly high confidence. We use\nfunctional magnetic resonance imaging to measure brain activity evoked by\nregular and adversarial images in the human brain, and compare it to the\nactivity of artificial neurons in a prototypical CNN-AlexNet. In the human\nbrain, we find that the representational similarity between regular and\nadversarial images largely echoes their perceptual similarity in all early\nvisual areas. In AlexNet, however, the neural representations of adversarial\nimages are inconsistent with network outputs in all intermediate processing\nlayers, providing no neural foundations for perceptual similarity. Furthermore,\nwe show that voxel-encoding models trained on regular images can successfully\ngeneralize to the neural responses to AI images but not AN images. These\nremarkable differences between the human brain and AlexNet in the\nrepresentation-perception relation suggest that future CNNs should emulate both\nbehavior and the internal neural presentations of the human brain.\n", "versions": [{"version": "v1", "created": "Sat, 22 Dec 2018 01:56:04 GMT"}, {"version": "v2", "created": "Fri, 17 Jul 2020 04:47:10 GMT"}, {"version": "v3", "created": "Mon, 20 Jul 2020 01:28:40 GMT"}], "update_date": "2020-07-21", "authors_parsed": [["Zhang", "Chi", ""], ["Duan", "Xiaohan", ""], ["Wang", "Linyuan", ""], ["Li", "Yongli", ""], ["Yan", "Bin", ""], ["Hu", "Guoen", ""], ["Zhang", "Ruyuan", ""], ["Tong", "Li", ""]]}, {"id": "1812.09897", "submitter": "Matjaz Perc", "authors": "Daqing Guo, Matjaz Perc, Tiejun Liu, Dezhong Yao", "title": "Functional importance of noise in neuronal information processing", "comments": "7 two-column pages, 4 figures; Perspective accepted for publication\n  in EPL", "journal-ref": "EPL 124, 50001 (2018)", "doi": "10.1209/0295-5075/124/50001", "report-no": null, "categories": "q-bio.NC nlin.AO physics.bio-ph", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Noise is an inherent part of neuronal dynamics, and thus of the brain. It can\nbe observed in neuronal activity at different spatiotemporal scales, including\nin neuronal membrane potentials, local field potentials,\nelectroencephalography, and magnetoencephalography. A central research topic in\ncontemporary neuroscience is to elucidate the functional role of noise in\nneuronal information processing. Experimental studies have shown that a\nsuitable level of noise may enhance the detection of weak neuronal signals by\nmeans of stochastic resonance. In response, theoretical research, based on the\ntheory of stochastic processes, nonlinear dynamics, and statistical physics,\nhas made great strides in elucidating the mechanism and the many benefits of\nstochastic resonance in neuronal systems. In this perspective, we review recent\nresearch dedicated to neuronal stochastic resonance in biophysical mathematical\nmodels. We also explore the regulation of neuronal stochastic resonance, and we\noutline important open questions and directions for future research. A deeper\nunderstanding of neuronal stochastic resonance may afford us new insights into\nthe highly impressive information processing in the brain.\n", "versions": [{"version": "v1", "created": "Mon, 24 Dec 2018 11:36:03 GMT"}, {"version": "v2", "created": "Fri, 28 Dec 2018 16:26:46 GMT"}], "update_date": "2019-01-03", "authors_parsed": [["Guo", "Daqing", ""], ["Perc", "Matjaz", ""], ["Liu", "Tiejun", ""], ["Yao", "Dezhong", ""]]}, {"id": "1812.10050", "submitter": "Moo K. Chung", "authors": "Shih-Gu Huang, S. Balqis Samdin, Chee-Ming Ting, Hernando Ombao, Moo\n  K. Chung", "title": "Statistical Model for Dynamically-Changing Correlation Matrices with\n  Application to Brain Connectivity", "comments": "Accepted for publication in Journal of Neuroscience Methods", "journal-ref": null, "doi": null, "report-no": null, "categories": "q-bio.NC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Background: Recent studies have indicated that functional connectivity is\ndynamic even during rest. A common approach to modeling the dynamic functional\nconnectivity in whole-brain resting-state fMRI is to compute the correlation\nbetween anatomical regions via sliding time windows. However, the direct use of\nthe sample correlation matrices is not reliable due to the image acquisition\nand processing noises in resting-sate fMRI.\n  New method: To overcome these limitations, we propose a new statistical model\nthat smooths out the noise by exploiting the geometric structure of correlation\nmatrices. The dynamic correlation matrix is modeled as a linear combination of\nsymmetric positive-definite matrices combined with cosine series\nrepresentation. The resulting smoothed dynamic correlation matrices are\nclustered into disjoint brain connectivity states using the k-means clustering\nalgorithm.\n  Results: The proposed model preserves the geometric structure of underlying\nphysiological dynamic correlation, eliminates unwanted noise in connectivity\nand obtains more accurate state spaces. The difference in the estimated dynamic\nconnectivity states between males and females is identified.\n  Comparison with existing methods: We demonstrate that the proposed\nstatistical model has less rapid state changes caused by noise and improves the\naccuracy in identifying and discriminating different states.\n  Conclusions: We propose a new regression model on dynamically changing\ncorrelation matrices that provides better performance over existing windowed\ncorrelation and is more reliable for the modeling of dynamic connectivity.\n", "versions": [{"version": "v1", "created": "Tue, 25 Dec 2018 06:23:33 GMT"}, {"version": "v2", "created": "Sun, 3 Nov 2019 16:28:17 GMT"}], "update_date": "2019-11-05", "authors_parsed": [["Huang", "Shih-Gu", ""], ["Samdin", "S. Balqis", ""], ["Ting", "Chee-Ming", ""], ["Ombao", "Hernando", ""], ["Chung", "Moo K.", ""]]}, {"id": "1812.10227", "submitter": "Yangsong Zhang", "authors": "Yangsong Zhang, Erwei Yin, Fali Li, Yu Zhang, Daqing Guo, Dezhong Yao,\n  Peng Xu", "title": "Hierarchical feature fusion framework for frequency recognition in\n  SSVEP-based BCIs", "comments": "25 pages, 9 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "q-bio.NC eess.SP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Effective frequency recognition algorithms are critical in steady-state\nvisual evoked potential (SSVEP) based brain-computer interfaces (BCIs). In this\nstudy, we present a hierarchical feature fusion framework which can be used to\ndesign high-performance frequency recognition methods. The proposed framework\nincludes two primary technique for fusing features: spatial dimension fusion\n(SD) and frequency dimension fusion (FD). Both SD and FD fusions are obtained\nusing a weighted strategy with a nonlinear function. To assess our novel\nmethods, we used the correlated component analysis (CORRCA) method to\ninvestigate the efficiency and effectiveness of the proposed framework.\nExperimental results were obtained from a benchmark dataset of thirty-five\nsubjects and indicate that the extended CORRCA method used within the framework\nsignificantly outperforms the original CORCCA method. Accordingly, the proposed\nframework holds promise to enhance the performance of frequency recognition\nmethods in SSVEP-based BCIs.\n", "versions": [{"version": "v1", "created": "Wed, 26 Dec 2018 05:06:16 GMT"}, {"version": "v2", "created": "Thu, 21 Mar 2019 14:41:35 GMT"}], "update_date": "2019-03-22", "authors_parsed": [["Zhang", "Yangsong", ""], ["Yin", "Erwei", ""], ["Li", "Fali", ""], ["Zhang", "Yu", ""], ["Guo", "Daqing", ""], ["Yao", "Dezhong", ""], ["Xu", "Peng", ""]]}, {"id": "1812.10622", "submitter": "Alex Frid", "authors": "Alex Frid and Larry M. Manevitz", "title": "Features and Machine Learning for Correlating and Classifying between\n  Brain Areas and Dyslexia", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG q-bio.NC stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We develop a method that is based on processing gathered Event Related\nPotentials (ERP) signals and the use of machine learning technique for\nmultivariate analysis (i.e. classification) that we apply in order to analyze\nthe differences between Dyslexic and Skilled readers.\n  No human intervention is needed in the analysis process. This is the state of\nthe art results for automatic identification of Dyslexic readers using a\nLexical Decision Task. We use mathematical and machine learning based\ntechniques to automatically discover novel complex features that (i) allow for\nreliable distinction between Dyslexic and Normal Control Skilled readers and\n(ii) to validate the assumption that the most of the differences between\nDyslexic and Skilled readers located in the left hemisphere.\n  Interestingly, these tools also pointed to the fact that High Pass signals\n(typically considered as \"noise\" during ERP/EEG analyses) in fact contains\nsignificant relevant information. Finally, the proposed scheme can be used for\nanalysis of any ERP based studies.\n", "versions": [{"version": "v1", "created": "Thu, 27 Dec 2018 04:50:33 GMT"}, {"version": "v2", "created": "Thu, 17 Jan 2019 22:40:16 GMT"}], "update_date": "2019-01-21", "authors_parsed": [["Frid", "Alex", ""], ["Manevitz", "Larry M.", ""]]}, {"id": "1812.11001", "submitter": "Alexandra Badea", "authors": "Alexandra Badea, Natalie A Delpratt, RJ Anderson, Russell Dibb, Yi Qi,\n  Hongjiang Wei, Chunlei Liu, William C Wetsel, Brian B Avants, Carol Colton", "title": "Multivariate MR Biomarkers Better Predict Cognitive Dysfunction in Mouse\n  Models of Alzheimers Disease", "comments": "23 pages, 3 Tables, 6 Figures; submitted for publication", "journal-ref": null, "doi": null, "report-no": null, "categories": "q-bio.QM q-bio.NC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  To understand multifactorial conditions such as Alzheimers disease (AD) we\nneed brain signatures that predict the impact of multiple pathologies and their\ninteractions. To help uncover the relationships between brain circuits and\ncognitive markers we have used mouse models that represent, at least in part,\nthe complex interactions altered in AD. In particular, we aimed to understand\nthe relationship between vulnerable brain circuits and memory deficits measured\nin the Morris water maze, and we tested several predictive modeling approaches.\nWe used in vivo manganese enhanced MRI voxel based analyses to reveal regional\ndifferences in volume (morphometry), signal intensity (activity), and magnetic\nsusceptibility (iron deposition, demyelination). These regions included the\nhippocampus, olfactory areas, entorhinal cortex and cerebellum. The image based\nproperties of these regions were used to predict spatial memory. We next used\neigenanatomy, which reduces dimensionality to produce sets of regions that\nexplain the variance in the data. For each imaging marker, eigenanatomy\nrevealed networks underpinning a range of cognitive functions including memory,\nmotor function, and associative learning. Finally, the integration of\nmultivariate markers in a supervised sparse canonical correlation approach\noutperformed single predictor models and had significant correlates to spatial\nmemory. Among a priori selected regions, the fornix also provided good\npredictors, raising the possibility of investigating how disease propagation\nwithin brain networks leads to cognitive deterioration. Our results support\nthat modeling approaches integrating multivariate imaging markers provide\nsensitive predictors of AD-like behaviors. Such strategies for mapping brain\ncircuits responsible for behaviors may help in the future predict disease\nprogression, or response to interventions.\n", "versions": [{"version": "v1", "created": "Fri, 28 Dec 2018 14:22:48 GMT"}], "update_date": "2018-12-31", "authors_parsed": [["Badea", "Alexandra", ""], ["Delpratt", "Natalie A", ""], ["Anderson", "RJ", ""], ["Dibb", "Russell", ""], ["Qi", "Yi", ""], ["Wei", "Hongjiang", ""], ["Liu", "Chunlei", ""], ["Wetsel", "William C", ""], ["Avants", "Brian B", ""], ["Colton", "Carol", ""]]}, {"id": "1812.11424", "submitter": "Alessandro Ingrosso", "authors": "Alessandro Ingrosso and L.F. Abbott", "title": "Training dynamically balanced excitatory-inhibitory networks", "comments": "12 pages, 7 figures", "journal-ref": null, "doi": "10.1371/journal.pone.0220547", "report-no": null, "categories": "cond-mat.dis-nn cs.NE q-bio.NC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The construction of biologically plausible models of neural circuits is\ncrucial for understanding the computational properties of the nervous system.\nConstructing functional networks composed of separate excitatory and inhibitory\nneurons obeying Dale's law presents a number of challenges. We show how a\ntarget-based approach, when combined with a fast online constrained\noptimization technique, is capable of building functional models of rate and\nspiking recurrent neural networks in which excitation and inhibition are\nbalanced. Balanced networks can be trained to produce complicated temporal\npatterns and to solve input-output tasks while retaining biologically desirable\nfeatures such as Dale's law and response variability.\n", "versions": [{"version": "v1", "created": "Sat, 29 Dec 2018 18:58:52 GMT"}], "update_date": "2020-07-01", "authors_parsed": [["Ingrosso", "Alessandro", ""], ["Abbott", "L. F.", ""]]}, {"id": "1812.11581", "submitter": "H. Sebastian Seung", "authors": "H. Sebastian Seung", "title": "Unsupervised learning by a nonlinear network with Hebbian excitatory and\n  anti-Hebbian inhibitory neurons", "comments": "10 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "q-bio.NC cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper introduces a rate-based nonlinear neural network in which\nexcitatory (E) neurons receive feedforward excitation from sensory (S) neurons,\nand inhibit each other through disynaptic pathways mediated by inhibitory (I)\ninterneurons. Correlation-based plasticity of disynaptic inhibition serves to\nincompletely decorrelate E neuron activity, pushing the E neurons to learn\ndistinct sensory features. The plasticity equations additionally contain\n\"extra\" terms fostering competition between excitatory synapses converging onto\nthe same postsynaptic neuron and inhibitory synapses diverging from the same\npresynaptic neuron. The parameters of competition between S$\\to$E connections\ncan be adjusted to make learned features look more like \"parts\" or \"wholes.\"\nThe parameters of competition between I-E connections can be adjusted to set\nthe typical decorrelatedness and sparsity of E neuron activity. Numerical\nsimulations of unsupervised learning show that relatively few I neurons can be\nsufficient for achieving good decorrelation, and increasing the number of I\nneurons makes decorrelation more complete. Excitatory and inhibitory inputs to\nactive E neurons are approximately balanced as a result of learning.\n", "versions": [{"version": "v1", "created": "Sun, 30 Dec 2018 18:19:23 GMT"}], "update_date": "2019-01-01", "authors_parsed": [["Seung", "H. Sebastian", ""]]}, {"id": "1812.11680", "submitter": "Varun Shankar", "authors": "Sean D. Lawley and Varun Shankar", "title": "Asymptotic and numerical analysis of a stochastic PDE model of volume\n  transmission", "comments": "29 pages, 4 figures. Accepted to SIAM Multiscale Modeling and\n  Simulation", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.PR cs.NA math.NA q-bio.NC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Volume transmission is an important neural communication pathway in which\nneurons in one brain region influence the neurotransmitter concentration in the\nextracellular space of a distant brain region. In this paper, we apply\nasymptotic analysis to a stochastic partial differential equation model of\nvolume transmission to calculate the neurotransmitter concentration in the\nextracellular space. Our model involves the diffusion equation in a\nthree-dimensional domain with interior holes that randomly switch between being\neither sources or sinks. These holes model nerve varicosities that alternate\nbetween releasing and absorbing neurotransmitter, according to when they fire\naction potentials. In the case that the holes are small, we compute\nanalytically the first two nonzero terms in an asymptotic expansion of the\naverage neurotransmitter concentration. The first term shows that the\nconcentration is spatially constant to leading order and that this constant is\nindependent of many details in the problem. Specifically, this constant first\nterm is independent of the number and location of nerve varicosities, neural\nfiring correlations, and the size and geometry of the extracellular space. The\nsecond term shows how these factors affect the concentration at second order.\nInterestingly, the second term is also spatially constant under some mild\nassumptions. We verify our asymptotic results by high-order numerical\nsimulation using radial basis function-generated finite differences.\n", "versions": [{"version": "v1", "created": "Mon, 31 Dec 2018 03:00:26 GMT"}, {"version": "v2", "created": "Sat, 25 Apr 2020 04:31:26 GMT"}], "update_date": "2020-04-28", "authors_parsed": [["Lawley", "Sean D.", ""], ["Shankar", "Varun", ""]]}, {"id": "1812.11758", "submitter": "Timothy O'Leary", "authors": "Dhruva V Raman, Timothy O'Leary", "title": "Fundamental bounds on learning performance in neural circuits", "comments": null, "journal-ref": "Proceedings of the National Academy of Sciences May 2019,\n  201813416", "doi": "10.1073/pnas.1813416116", "report-no": null, "categories": "q-bio.NC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  How does the size of a neural circuit influence its learning performance?\nIntuitively, we expect the learning capacity of a neural circuit to grow with\nthe number of neurons and synapses. Larger brains tend to be found in species\nwith higher cognitive function and learning ability. Similarly, adding\nconnections and units to artificial neural networks can allow them to solve\nmore complex tasks. However, we show that in a biologically relevant setting\nwhere synapses introduce an unavoidable amount of noise, there is an optimal\nsize of network for a given task. Beneath this optimal size, our analysis shows\nhow adding apparently redundant neurons and connections can make tasks more\nlearnable. Therefore large neural circuits can either devote connectivity to\ngenerating complex behaviors, or exploit this connectivity to achieve faster\nand more precise learning of simpler behaviors. Above the optimal network size,\nthe addition of neurons and synaptic connections starts to impede learning\nperformance. This suggests that overall brain size may be constrained by the\nneed to learn efficiently with unreliable synapses, and may explain why some\nneurological learning deficits are associated with hyperconnectivity. Our\nanalysis is independent of specific learning rules and uncovers fundamental\nrelationships between learning rate, task performance, network size and\nintrinsic noise in neural circuits.\n", "versions": [{"version": "v1", "created": "Mon, 31 Dec 2018 10:59:17 GMT"}], "update_date": "2019-05-09", "authors_parsed": [["Raman", "Dhruva V", ""], ["O'Leary", "Timothy", ""]]}, {"id": "1812.11878", "submitter": "William Bialek", "authors": "Shiva R. Sinha, William Bialek, and Rob R. de Ruyter van Steveninck", "title": "Optimal local estimates of visual motion in a natural environment", "comments": null, "journal-ref": "Phys. Rev. Lett. 126, 018101 (2021)", "doi": "10.1103/PhysRevLett.126.018101", "report-no": null, "categories": "q-bio.NC cond-mat.dis-nn physics.bio-ph", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Many organisms, from flies to humans, use visual signals to estimate their\nmotion through the world. To explore the motion estimation problem, we have\nconstructed a camera/gyroscope system that allows us to sample, at high\ntemporal resolution, the joint distribution of input images and rotational\nmotions during a long walk in the woods. From these data we construct the\noptimal estimator of velocity based on spatial and temporal derivatives of\nimage intensity in small patches of the visual world. Over the bulk of the\nnaturally occurring dynamic range, the optimal estimator exhibits the same\nsystematic errors seen in neural and behavioral responses, including the\nconfounding of velocity and contrast. These results suggest that apparent\nerrors of sensory processing may reflect an optimal response to the physical\nsignals in the environment.\n", "versions": [{"version": "v1", "created": "Mon, 31 Dec 2018 16:20:12 GMT"}], "update_date": "2021-01-13", "authors_parsed": [["Sinha", "Shiva R.", ""], ["Bialek", "William", ""], ["van Steveninck", "Rob R. de Ruyter", ""]]}, {"id": "1812.11904", "submitter": "Leenoy Meshulam", "authors": "Leenoy Meshulam, Jeffrey L. Gauthier, Carlos D. Brody, David W. Tank,\n  and William Bialek", "title": "Coarse--graining and hints of scaling in a population of 1000+ neurons", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "physics.bio-ph q-bio.NC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In many systems we can describe emergent macroscopic behaviors,\nquantitatively, using models that are much simpler than the underlying\nmicroscopic interactions; we understand the success of this simplification\nthrough the renormalization group. Could similar simplifications succeed in\ncomplex biological systems? We develop explicit coarse-graining procedures that\nwe apply to experimental data on the electrical activity in large populations\nof neurons in the mouse hippocampus. Probability distributions of\ncoarse-grained variables seem to approach a fixed non-Gaussian form, and we see\nevidence of power-law dependencies in both static and dynamic quantities as we\nvary the coarse-graining scale over two decades. Taken together, these results\nsuggest that the collective behavior of the network is described by a\nnon-trivial fixed point.\n", "versions": [{"version": "v1", "created": "Mon, 31 Dec 2018 17:20:24 GMT"}], "update_date": "2019-01-01", "authors_parsed": [["Meshulam", "Leenoy", ""], ["Gauthier", "Jeffrey L.", ""], ["Brody", "Carlos D.", ""], ["Tank", "David W.", ""], ["Bialek", "William", ""]]}, {"id": "1812.11937", "submitter": "H. Sebastian Seung", "authors": "H. Sebastian Seung", "title": "Two \"correlation games\" for a nonlinear network with Hebbian excitatory\n  neurons and anti-Hebbian inhibitory neurons", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "q-bio.NC cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A companion paper introduces a nonlinear network with Hebbian excitatory (E)\nneurons that are reciprocally coupled with anti-Hebbian inhibitory (I) neurons\nand also receive Hebbian feedforward excitation from sensory (S) afferents. The\npresent paper derives the network from two normative principles that are\nmathematically equivalent but conceptually different. The first principle\nformulates unsupervised learning as a constrained optimization problem:\nmaximization of S-E correlations subject to a copositivity constraint on E-E\ncorrelations. A combination of Legendre and Lagrangian duality yields a\nzero-sum continuous game between excitatory and inhibitory connections that is\nsolved by the neural network. The second principle defines a zero-sum game\nbetween E and I cells. E cells want to maximize S-E correlations and minimize\nE-I correlations, while I cells want to maximize I-E correlations and minimize\npower. The conflict between I and E objectives effectively forces the E cells\nto decorrelate from each other, although only incompletely. Legendre duality\nyields the neural network.\n", "versions": [{"version": "v1", "created": "Mon, 31 Dec 2018 18:18:46 GMT"}], "update_date": "2019-01-01", "authors_parsed": [["Seung", "H. Sebastian", ""]]}]