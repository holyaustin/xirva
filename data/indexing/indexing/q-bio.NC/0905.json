[{"id": "0905.0149", "submitter": "Ivan Yu. Tyukin", "authors": "David Fairhurst, Ivan Tyukin, Henk Nijmeijer, and Cees van Leeuwen", "title": "Observers for canonic models of neural oscillators", "comments": null, "journal-ref": "Mathematical Modelling of Natural Phenomena, 5(2): 146-184, 2010", "doi": "10.1051/mmnp/20105206", "report-no": null, "categories": "q-bio.NC math.OC q-bio.QM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider the problem of state and parameter estimation for a wide class of\nnonlinear oscillators. Observable variables are limited to a few components of\nstate vector and an input signal. The problem of state and parameter\nreconstruction is viewed within the classical framework of observer design.\nThis framework offers computationally-efficient solutions to the problem of\nstate and parameter reconstruction of a system of nonlinear differential\nequations, provided that these equations are in the so-called adaptive observer\ncanonic form. We show that despite typical neural oscillators being locally\nobservable they are not in the adaptive canonic observer form. Furthermore, we\nshow that no parameter-independent diffeomorphism exists such that the original\nequations of these models can be transformed into the adaptive canonic observer\nform. We demonstrate, however, that for the class of Hindmarsh-Rose and\nFitzHugh-Nagumo models, parameter-dependent coordinate transformations can be\nused to render these systems into the adaptive observer canonical form. This\nallows reconstruction, at least partially and up to a (bi)linear\ntransformation, of unknown state and parameter values with exponential rate of\nconvergence. In order to avoid the problem of only partial reconstruction and\nto deal with more general nonlinear models in which the unknown parameters\nenter the system nonlinearly, we present a new method for state and parameter\nreconstruction for these systems. The method combines advantages of standard\nLyapunov-based design with more flexible design and analysis techniques based\non the non-uniform small-gain theorems. Effectiveness of the method is\nillustrated with simple numerical examples.\n", "versions": [{"version": "v1", "created": "Fri, 1 May 2009 21:03:48 GMT"}, {"version": "v2", "created": "Thu, 10 Jun 2010 09:30:56 GMT"}, {"version": "v3", "created": "Fri, 18 Jun 2010 13:19:12 GMT"}], "update_date": "2015-03-13", "authors_parsed": [["Fairhurst", "David", ""], ["Tyukin", "Ivan", ""], ["Nijmeijer", "Henk", ""], ["van Leeuwen", "Cees", ""]]}, {"id": "0905.0701", "submitter": "Ernest Montbrio", "authors": "Alex Roxin, Ernest Montbrio", "title": "How effective delays shape oscillatory dynamics in neuronal networks", "comments": "59 pages, 25 figures", "journal-ref": "Physica D 240, (2011) 323-345", "doi": "10.1016/j.physd.2010.09.009", "report-no": null, "categories": "q-bio.NC nlin.PS q-bio.QM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Synaptic, dendritic and single-cell kinetics generate significant time delays\nthat shape the dynamics of large networks of spiking neurons. Previous work has\nshown that such effective delays can be taken into account with a rate model\nthrough the addition of an explicit, fixed delay [Roxin et al. PRL 238103\n(2005)]. Here we extend this work to account for arbitrary symmetric patterns\nof synaptic connectivity and generic nonlinear transfer functions.\nSpecifically, we conduct a weakly nonlinear analysis of the dynamical states\narising via primary instabilities of the asynchronous state. In this way we\ndetermine analytically how the nature and stability of these states depend on\nthe choice of transfer function and connectivity. We arrive at two general\nobservations of physiological relevance that could not be explained in previous\nworks. These are: 1 - Fast oscillations are always supercritical for realistic\ntransfer functions. 2 - Traveling waves are preferred over standing waves given\nplausible patterns of local connectivity. We finally demonstrate that these\nresults show a good agreement with those obtained performing numerical\nsimulations of a network of Hodgkin-Huxley neurons.\n", "versions": [{"version": "v1", "created": "Wed, 6 May 2009 16:15:10 GMT"}, {"version": "v2", "created": "Thu, 13 Aug 2009 21:49:46 GMT"}, {"version": "v3", "created": "Tue, 17 May 2011 14:56:45 GMT"}], "update_date": "2014-01-31", "authors_parsed": [["Roxin", "Alex", ""], ["Montbrio", "Ernest", ""]]}, {"id": "0905.1410", "submitter": "Yasser Roudi", "authors": "Yasser Roudi, Erik Aurell, John Hertz", "title": "Statistical physics of pairwise probability models", "comments": "25 pages, 3 figures", "journal-ref": "Front. Comput. Neurosci (2009) 3:22", "doi": "10.3389/neuro.10.022.2009", "report-no": "NORDITA-2009-25", "categories": "q-bio.QM cond-mat.dis-nn q-bio.NC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Statistical models for describing the probability distribution over the\nstates of biological systems are commonly used for dimensional reduction. Among\nthese models, pairwise models are very attractive in part because they can be\nfit using a reasonable amount of data: knowledge of the means and correlations\nbetween pairs of elements in the system is sufficient. Not surprisingly, then,\nusing pairwise models for studying neural data has been the focus of many\nstudies in recent years. In this paper, we describe how tools from statistical\nphysics can be employed for studying and using pairwise models. We build on our\nprevious work on the subject and study the relation between different methods\nfor fitting these models and evaluating their quality. In particular, using\ndata from simulated cortical networks we study how the quality of various\napproximate methods for inferring the parameters in a pairwise model depends on\nthe time bin chosen for binning the data. We also study the effect of the size\nof the time bin on the model quality itself, again using simulated data. We\nshow that using finer time bins increases the quality of the pairwise model. We\noffer new ways of deriving the expressions reported in our previous work for\nassessing the quality of pairwise models.\n", "versions": [{"version": "v1", "created": "Sat, 9 May 2009 14:10:37 GMT"}], "update_date": "2009-11-30", "authors_parsed": [["Roudi", "Yasser", ""], ["Aurell", "Erik", ""], ["Hertz", "John", ""]]}, {"id": "0905.1458", "submitter": "Michael Krumin", "authors": "Michael Krumin, Avner Shimron and Shy Shoham", "title": "Correlation-distortion based identification of Linear-Nonlinear-Poisson\n  models", "comments": null, "journal-ref": null, "doi": "10.1007/s10827-009-0184-0", "report-no": null, "categories": "q-bio.NC q-bio.QM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Linear-Nonlinear-Poisson (LNP) models are a popular and powerful tool for\ndescribing encoding (stimulus-response) transformations by single sensory as\nwell as motor neurons. Recently, there has been rising interest in the second-\nand higher-order correlation structure of neural spike trains, and how it may\nbe related to specific encoding relationships. The distortion of signal\ncorrelations as they are transformed through particular LNP models is\npredictable and in some cases analytically tractable and invertible. Here, we\npropose that LNP encoding models can potentially be identified strictly from\nthe correlation transformations they induce, and develop a computational method\nfor identifying minimum-phase single-neuron temporal kernels under white and\ncolored- random Gaussian excitation. Unlike reverse-correlation or\nmaximum-likelihood, correlation-distortion based identification does not\nrequire the simultaneous observation of stimulus-response pairs - only their\nrespective second order statistics. Although in principle filter kernels are\nnot necessarily minimum-phase, and only their spectral amplitude can be\nuniquely determined from output correlations, we show that in practice this\nmethod provides excellent estimates of kernels from a range of parametric\nmodels of neural systems. We conclude by discussing how this approach could\npotentially enable neural models to be estimated from a much wider variety of\nexperimental conditions and systems, and its limitations.\n", "versions": [{"version": "v1", "created": "Sun, 10 May 2009 09:01:18 GMT"}, {"version": "v2", "created": "Mon, 19 Oct 2009 01:00:23 GMT"}], "update_date": "2016-09-08", "authors_parsed": [["Krumin", "Michael", ""], ["Shimron", "Avner", ""], ["Shoham", "Shy", ""]]}, {"id": "0905.2125", "submitter": "Jenia Jitsev", "authors": "Jenia Jitsev, Christoph von der Malsburg", "title": "Experience-driven formation of parts-based representations in a model of\n  layered visual memory", "comments": "34 pages, 12 Figures, 1 Table, published in Frontiers in\n  Computational Neuroscience (Special Issue on Complex Systems Science and\n  Brain Dynamics),\n  http://www.frontiersin.org/neuroscience/computationalneuroscience/paper/10.3389/neuro.10/015.2009/", "journal-ref": "Front. Comput. Neurosci. 3:15 (2009)", "doi": "10.3389/neuro.10.015.2009", "report-no": null, "categories": "q-bio.NC cs.LG nlin.AO", "license": "http://creativecommons.org/licenses/by/3.0/", "abstract": "  Growing neuropsychological and neurophysiological evidence suggests that the\nvisual cortex uses parts-based representations to encode, store and retrieve\nrelevant objects. In such a scheme, objects are represented as a set of\nspatially distributed local features, or parts, arranged in stereotypical\nfashion. To encode the local appearance and to represent the relations between\nthe constituent parts, there has to be an appropriate memory structure formed\nby previous experience with visual objects. Here, we propose a model how a\nhierarchical memory structure supporting efficient storage and rapid recall of\nparts-based representations can be established by an experience-driven process\nof self-organization. The process is based on the collaboration of slow\nbidirectional synaptic plasticity and homeostatic unit activity regulation,\nboth running at the top of fast activity dynamics with winner-take-all\ncharacter modulated by an oscillatory rhythm. These neural mechanisms lay down\nthe basis for cooperation and competition between the distributed units and\ntheir synaptic connections. Choosing human face recognition as a test task, we\nshow that, under the condition of open-ended, unsupervised incremental\nlearning, the system is able to form memory traces for individual faces in a\nparts-based fashion. On a lower memory layer the synaptic structure is\ndeveloped to represent local facial features and their interrelations, while\nthe identities of different persons are captured explicitly on a higher layer.\nAn additional property of the resulting representations is the sparseness of\nboth the activity during the recall and the synaptic patterns comprising the\nmemory traces.\n", "versions": [{"version": "v1", "created": "Wed, 13 May 2009 14:23:36 GMT"}, {"version": "v2", "created": "Thu, 16 Jul 2009 12:58:02 GMT"}, {"version": "v3", "created": "Mon, 22 Mar 2010 15:38:57 GMT"}], "update_date": "2015-03-13", "authors_parsed": [["Jitsev", "Jenia", ""], ["von der Malsburg", "Christoph", ""]]}, {"id": "0905.2836", "submitter": "Charles Ross FBCS FIAP FIMIS", "authors": "Charles Ross and Shirley Redpath", "title": "Role of Glia cells in the formation of memory in the brain", "comments": "5 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "q-bio.NC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We build on progress in understanding the role of glia cells in building the\ninitial networks in the foetal brain. This has led us to make three significant\npostulates. Short term memory results from glia cells forming speculative links\ndirectly and solely as a result of neural activity generated by life\nexperiences. These temporary 'glia bridges' create long term memory by\nstimulating the growth of axons, dendrites and synapses and providing the\npathways enabling permanent neural structures to be created. Problem solving,\nidea creation and memory maintenance results from this fundamental algorithm\nfor how the brain generates new links.\n", "versions": [{"version": "v1", "created": "Mon, 18 May 2009 09:07:26 GMT"}], "update_date": "2009-05-19", "authors_parsed": [["Ross", "Charles", ""], ["Redpath", "Shirley", ""]]}, {"id": "0905.2843", "submitter": "Wolfgang Keil", "authors": "Wolfgang Keil, Karl-Friedrich Schmidt, Siegrid Loewel, Matthias\n  Kaschube", "title": "Reorganization of columnar architecture in the growing visual cortex", "comments": "8+13 pages, 4+8 figures, paper + supplementary material", "journal-ref": "PNAS July 6, 2010 vol. 107 no. 27 12293-12298", "doi": "10.1073/pnas.0913020107", "report-no": null, "categories": "q-bio.NC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Many cortical areas increase in size considerably during postnatal\ndevelopment, progressively displacing neuronal cell bodies from each other. At\npresent, little is known about how cortical growth affects the development of\nneuronal circuits. Here, in acute and chronic experiments, we study the layout\nof ocular dominance (OD) columns in cat primary visual cortex (V1) during a\nperiod of substantial postnatal growth. We find that despite a considerable\nsize increase of V1, the spacing between columns is largely preserved. In\ncontrast, their spatial arrangement changes systematically over this period.\nWhile in young animals columns are more band-like, layouts become more\nisotropic in mature animals. We propose a novel mechanism of growth-induced\nreorganization that is based on the `zigzag instability', a dynamical\ninstability observed in several inanimate pattern forming systems. We argue\nthat this mechanism is inherent to a wide class of models for the\nactivity-dependent formation of OD columns. Analyzing one member of this class,\nthe Elastic Network model, we show that this mechanism can account for the\npreservation of column spacing and the specific mode of reorganization of OD\ncolumns that we observe. We conclude that neurons systematically shift their\nselectivities during normal development and that this reorganization is induced\nby the cortical expansion during growth. Our work suggests that cortical\ncircuits remain plastic for an extended period in development in order to\nfacilitate the modification of neuronal circuits to adjust for cortical growth.\n", "versions": [{"version": "v1", "created": "Mon, 18 May 2009 11:51:34 GMT"}, {"version": "v2", "created": "Mon, 11 Apr 2011 15:04:59 GMT"}], "update_date": "2011-04-12", "authors_parsed": [["Keil", "Wolfgang", ""], ["Schmidt", "Karl-Friedrich", ""], ["Loewel", "Siegrid", ""], ["Kaschube", "Matthias", ""]]}, {"id": "0905.2935", "submitter": "Yuriy Pershin", "authors": "Yuriy V. Pershin and Massimiliano Di Ventra", "title": "Experimental demonstration of associative memory with memristive neural\n  networks", "comments": null, "journal-ref": "Neural Networks 23, 881 (2010)", "doi": null, "report-no": null, "categories": "q-bio.NC cond-mat.mes-hall q-bio.MN", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  When someone mentions the name of a known person we immediately recall her\nface and possibly many other traits. This is because we possess the so-called\nassociative memory, that is the ability to correlate different memories to the\nsame fact or event. Associative memory is such a fundamental and encompassing\nhuman ability (and not just human) that the network of neurons in our brain\nmust perform it quite easily. The question is then whether electronic neural\nnetworks (electronic schemes that act somewhat similarly to human brains) can\nbe built to perform this type of function. Although the field of neural\nnetworks has developed for many years, a key element, namely the synapses\nbetween adjacent neurons, has been lacking a satisfactory electronic\nrepresentation. The reason for this is that a passive circuit element able to\nreproduce the synapse behaviour needs to remember its past dynamical history,\nstore a continuous set of states, and be \"plastic\" according to the\npre-synaptic and post-synaptic neuronal activity. Here we show that all this\ncan be accomplished by a memory-resistor (memristor for short). In particular,\nby using simple and inexpensive off-the-shelf components we have built a\nmemristor emulator which realizes all required synaptic properties. Most\nimportantly, we have demonstrated experimentally the formation of associative\nmemory in a simple neural network consisting of three electronic neurons\nconnected by two memristor-emulator synapses. This experimental demonstration\nopens up new possibilities in the understanding of neural processes using\nmemory devices, an important step forward to reproduce complex learning,\nadaptive and spontaneous behaviour with electronic neural networks.\n", "versions": [{"version": "v1", "created": "Mon, 18 May 2009 17:16:03 GMT"}, {"version": "v2", "created": "Fri, 18 Sep 2009 16:00:11 GMT"}, {"version": "v3", "created": "Mon, 18 Jan 2010 02:39:52 GMT"}], "update_date": "2010-08-26", "authors_parsed": [["Pershin", "Yuriy V.", ""], ["Di Ventra", "Massimiliano", ""]]}, {"id": "0905.3084", "submitter": "Matjaz Perc", "authors": "Mahmut Ozer, Matjaz Perc, Muhammet Uzuntarla", "title": "Controlling the spontaneous spiking regularity via channel blocking on\n  Newman-Watts networks of Hodgkin-Huxley neurons", "comments": "6 two-column pages, 5 figures; accepted for publication in\n  Europhysics Letters", "journal-ref": "EPL 86 (2009) 40008", "doi": "10.1209/0295-5075/86/40008", "report-no": null, "categories": "physics.bio-ph cond-mat.dis-nn q-bio.NC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We investigate the regularity of spontaneous spiking activity on Newman-Watts\nsmall-world networks consisting of biophysically realistic Hodgkin-Huxley\nneurons with a tunable intensity of intrinsic noise and fraction of blocked\nvoltage-gated sodium and potassium ion channels embedded in neuronal membranes.\nWe show that there exists an optimal fraction of shortcut links between\nphysically distant neurons, as well as an optimal intensity of intrinsic noise,\nwhich warrant an optimally ordered spontaneous spiking activity. This doubly\ncoherence resonance-like phenomenon depends significantly, and can be\ncontrolled via the fraction of closed sodium and potassium ion channels,\nwhereby the impacts can be understood via the analysis of the firing rate\nfunction as well as the deterministic system dynamics. Potential biological\nimplications of our findings for information propagation across neural networks\nare also discussed.\n", "versions": [{"version": "v1", "created": "Tue, 19 May 2009 12:50:30 GMT"}], "update_date": "2009-06-05", "authors_parsed": [["Ozer", "Mahmut", ""], ["Perc", "Matjaz", ""], ["Uzuntarla", "Muhammet", ""]]}, {"id": "0905.3154", "submitter": "Patrick Crotty", "authors": "Jeffrey Seely, Patrick Crotty", "title": "Optimization of the leak conductance in the squid giant axon", "comments": "9 pages; 9 figures; accepted for publication in Physical Review E", "journal-ref": null, "doi": "10.1103/PhysRevE.82.021906", "report-no": null, "categories": "q-bio.NC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We report on a theoretical study showing that the leak conductance density,\n$\\GL$, in the squid giant axon appears to be optimal for the action potential\nfiring frequency. More precisely, the standard assumption that the leak current\nis composed of chloride ions leads to the result that the experimental value\nfor $\\GL$ is very close to the optimal value in the Hodgkin-Huxley model which\nminimizes the absolute refractory period of the action potential, thereby\nmaximizing the maximum firing frequency under stimulation by sharp, brief input\ncurrent spikes to one end of the axon. The measured value of $\\GL$ also appears\nto be close to optimal for the frequency of repetitive firing caused by a\nconstant current input to one end of the axon, especially when temperature\nvariations are taken into account. If, by contrast, the leak current is assumed\nto be composed of separate voltage-independent sodium and potassium currents,\nthen these optimizations are not observed.\n", "versions": [{"version": "v1", "created": "Tue, 19 May 2009 17:50:29 GMT"}, {"version": "v2", "created": "Mon, 26 Jul 2010 14:14:32 GMT"}], "update_date": "2015-05-13", "authors_parsed": [["Seely", "Jeffrey", ""], ["Crotty", "Patrick", ""]]}, {"id": "0905.3678", "submitter": "Vadim Madgazin", "authors": "Vadim R. Madgazin", "title": "Major and minor. The formula of musical emotions", "comments": "22 pages, 3 figures, in Russian, added figures, changed content", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.SD q-bio.NC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The new formulas, which determine sign and amplitude of utilitarian emotions,\nare proposed on the basis of the information theory of emotions. In area of\nperception of musical chords the force of emotions depends on the relative\npitch of sounds of major and minor chords.\n  Is advanced hypothesis that in the perception of a musical chord in the\npsyche caused by the subject value of some objective function L. This function\nis expressed directly through the proportion of the pitch of chord. Major\nchords are expressed as the straight proportions, which generate idea about an\nincrease in the objective function (L>1) and are caused positive utilitarian\nemotions. Minor chords are expressed as the inverse proportion, which generate\nidea about the decrease of objective function (L<1) and are caused negative\nutilitarian emotions.\n  The formula of musical emotions is advanced: Pwe = log(L) =\n(1/M)*log(n1*n2*n3* ... *nM), where M is a quantity of voices of chord, ni -\ninteger number (or reciprocal fraction) from the pitch proportion, which\ncorresponds to the i-th voice of chord.\n  Confined experimental check is produced. The limits of the applicability of\nthe formula of musical emotions are investigated.\n  Keywords: sound, music, chord, major, minor, emotions, the formula of musical\nemotions, the information theory of emotions.\n", "versions": [{"version": "v1", "created": "Fri, 22 May 2009 13:40:36 GMT"}, {"version": "v2", "created": "Wed, 15 Jul 2009 10:57:50 GMT"}, {"version": "v3", "created": "Tue, 1 Sep 2009 11:19:59 GMT"}], "update_date": "2009-09-01", "authors_parsed": [["Madgazin", "Vadim R.", ""]]}, {"id": "0905.3690", "submitter": "Jan Karbowski", "authors": "Jan Karbowski", "title": "Thermodynamic constraints on neural dimensions, firing rates, brain\n  temperature and size", "comments": "Journal of Computational Neuroscience - in press (2009)", "journal-ref": null, "doi": "10.1007/s10827-009-0153-7", "report-no": null, "categories": "q-bio.NC q-bio.CB q-bio.SC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  There have been suggestions that heat caused by cerebral metabolic activity\nmay constrain mammalian brain evolution, architecture, and function. This\narticle investigates physical limits on brain wiring and corresponding changes\nin brain temperature that are imposed by thermodynamics of heat balance\ndetermined mainly by Na$^{+}$/K$^{+}$-ATPase, cerebral blood flow, and heat\nconduction. It is found that even moderate firing rates cause significant\nintracellular Na$^{+}$ build-up, and the ATP consumption rate associated with\npumping out these ions grows nonlinearly with frequency. Surprisingly, the\npower dissipated by the Na$^{+}$/K$^{+}$ pump depends biphasically on\nfrequency, which can lead to the biphasic dependence of brain temperature on\nfrequency as well. Both the total power of sodium pumps and brain temperature\ndiverge for very small fiber diameters, indicating that too thin fibers are not\nbeneficial for thermal balance. For very small brains blood flow is not a\nsufficient cooling mechanism deep in the brain. The theoretical lower bound on\nfiber diameter above which brain temperature is in the operational regime is\nstrongly frequency dependent but finite due to synaptic depression. For normal\nneurophysiological conditions this bound is at least an order of magnitude\nsmaller than average values of empirical fiber diameters, suggesting that\nneuroanatomy of the mammalian brains operates in the thermodynamically safe\nregime. Analytical formulas presented can be used to estimate average firing\nrates in mammals, and relate their changes to changes in brain temperature,\nwhich can have important practical applications. In general, activity in larger\nbrains is found to be slower than in smaller brains.\n", "versions": [{"version": "v1", "created": "Fri, 22 May 2009 14:31:55 GMT"}], "update_date": "2014-05-19", "authors_parsed": [["Karbowski", "Jan", ""]]}, {"id": "0905.3759", "submitter": "Samuel Johnson", "authors": "Samuel Johnson, J. Marro, and Joaquin J. Torres", "title": "Evolving Networks and the Development of Neural Systems", "comments": "16 pages, 4 figures. Accepted for publication in J. Stat. Mech", "journal-ref": "J. Stat. Mech. (2010) P03003", "doi": "10.1088/1742-5468/2010/03/P03003", "report-no": null, "categories": "nlin.AO cond-mat.dis-nn q-bio.NC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  It is now generally assumed that the heterogeneity of most networks in nature\nprobably arises via preferential attachment of some sort. However, the origin\nof various other topological features, such as degree-degree correlations and\nrelated characteristics, is often not clear and attributed to specific\nfunctional requirements. We show how it is possible to analyse a very general\nscenario in which nodes gain or lose edges according to any (e.g., nonlinear)\nfunctions of local and/or global degree information. Applying our method to two\nrather different examples of brain development -- synaptic pruning in humans\nand the neural network of the worm C. Elegans -- we find that simple\nbiologically motivated assumptions lead to very good agreement with\nexperimental data. In particular, many nontrivial topological features of the\nworm's brain arise naturally at a critical point.\n", "versions": [{"version": "v1", "created": "Sun, 24 May 2009 20:33:45 GMT"}, {"version": "v2", "created": "Sun, 13 Sep 2009 01:01:25 GMT"}, {"version": "v3", "created": "Wed, 27 Jan 2010 16:57:14 GMT"}], "update_date": "2010-03-05", "authors_parsed": [["Johnson", "Samuel", ""], ["Marro", "J.", ""], ["Torres", "Joaquin J.", ""]]}, {"id": "0905.3771", "submitter": "Subhash Kak", "authors": "Subhash Kak", "title": "Memory Retrieved from Single Neurons", "comments": "12 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NE q-bio.NC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The paper examines the problem of accessing a vector memory from a single\nneuron in a Hebbian neural network. It begins with the review of the author's\nearlier method, which is different from the Hopfield model in that it recruits\nneighboring neurons by spreading activity, making it possible for single or\ngroup of neurons to become associated with vector memories. Some open issues\nassociated with this approach are identified. It is suggested that fragments\nthat generate stored memories could be associated with single neurons through\nlocal spreading activity.\n", "versions": [{"version": "v1", "created": "Fri, 22 May 2009 22:09:59 GMT"}], "update_date": "2009-05-26", "authors_parsed": [["Kak", "Subhash", ""]]}, {"id": "0905.3887", "submitter": "Raj Kumar Pan", "authors": "Raj Kumar Pan and Nivedita Chatterjee and Sitabhra Sinha", "title": "Mesoscopic organization reveals the constraints governing C. elegans\n  nervous system", "comments": "Published version, Minor modifications, 16 pages, 9 figures", "journal-ref": "PLoS ONE 5(2): e9240 (2010)", "doi": "10.1371/journal.pone.0009240", "report-no": null, "categories": "q-bio.NC cond-mat.dis-nn physics.bio-ph", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  One of the biggest challenges in biology is to understand how activity at the\ncellular level of neurons, as a result of their mutual interactions, leads to\nthe observed behavior of an organism responding to a variety of environmental\nstimuli. Investigating the intermediate or mesoscopic level of organization in\nthe nervous system is a vital step towards understanding how the integration of\nmicro-level dynamics results in macro-level functioning. In this paper, we have\nconsidered the somatic nervous system of the nematode Caenorhabditis elegans,\nfor which the entire neuronal connectivity diagram is known. We focus on the\norganization of the system into modules, i.e., neuronal groups having\nrelatively higher connection density compared to that of the overall network.\nWe show that this mesoscopic feature cannot be explained exclusively in terms\nof considerations, such as optimizing for resource constraints (viz., total\nwiring cost) and communication efficiency (i.e., network path length).\nComparison with other complex networks designed for efficient transport (of\nsignals or resources) implies that neuronal networks form a distinct class.\nThis suggests that the principal function of the network, viz., processing of\nsensory information resulting in appropriate motor response, may be playing a\nvital role in determining the connection topology. Using modular spectral\nanalysis, we make explicit the intimate relation between function and structure\nin the nervous system. This is further brought out by identifying functionally\ncritical neurons purely on the basis of patterns of intra- and inter-modular\nconnections. Our study reveals how the design of the nervous system reflects\nseveral constraints, including its key functional role as a processor of\ninformation.\n", "versions": [{"version": "v1", "created": "Sun, 24 May 2009 10:21:52 GMT"}, {"version": "v2", "created": "Mon, 8 Mar 2010 16:41:07 GMT"}], "update_date": "2010-03-09", "authors_parsed": [["Pan", "Raj Kumar", ""], ["Chatterjee", "Nivedita", ""], ["Sinha", "Sitabhra", ""]]}, {"id": "0905.4670", "submitter": "Felix M\\\"uller", "authors": "D. E. Postnov, F. M\\\"uller, R. B. Schuppner and L. Schimansky-Geier", "title": "Dynamical structures in binary media of potassium-driven neurons", "comments": "27 pages, 14 figures", "journal-ref": null, "doi": "10.1103/PhysRevE.80.031921", "report-no": null, "categories": "cond-mat.dis-nn cond-mat.stat-mech q-bio.NC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  According to the conventional approach to model neural ensembles the\nextracellular environment has fixed ionic concentrations. However, in many\ncases the extracellular concentration of potassium ions can not be regarded as\nconstant. That represents specific chemical pathway for neurons to interact and\ncan influence strongly the behavior of a single neuron as well as large\nensembles. The released chemical agent follows a diffusive dynamics in the\nexternal medium, that lowers the threshold of individual excitable units. We\naddress this problem by studying simplified excitable units given by a modified\nFitzHugh-Nagumo dynamics. In our model neurons interact only chemically via the\nreleased and diffusing potassium in the surrounding non-active medium. We study\nthe dynamics of a single excitable unit embedded in the extracellular matter.\nThat leads to a number of noise-induced effects, like self-modulation of firing\nrate in an individual neuron. In the spatially extended situation various\npatterns appear ranging from spirals and traveling waves to oscillons and\ninverted structures depending on the parameters of the medium.\n", "versions": [{"version": "v1", "created": "Thu, 28 May 2009 14:48:47 GMT"}, {"version": "v2", "created": "Mon, 3 Aug 2009 14:51:16 GMT"}, {"version": "v3", "created": "Wed, 9 Sep 2009 11:45:23 GMT"}], "update_date": "2015-05-13", "authors_parsed": [["Postnov", "D. E.", ""], ["M\u00fcller", "F.", ""], ["Schuppner", "R. B.", ""], ["Schimansky-Geier", "L.", ""]]}]