[{"id": "1906.00092", "submitter": "Anik Chattopadhyay", "authors": "Anik Chattopadhyay, Arunava Banerjee", "title": "Signal Coding and Perfect Reconstruction using Spike Trains", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "q-bio.NC cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In many animal sensory pathways, the transformation from external stimuli to\nspike trains is essentially deterministic. In this context, a new mathematical\nframework for coding and reconstruction, based on a biologically plausible\nmodel of the spiking neuron, is presented. The framework considers encoding of\na signal through spike trains generated by an ensemble of neurons via a\nstandard convolve-then-threshold mechanism. Neurons are distinguished by their\nconvolution kernels and threshold values. Reconstruction is posited as a convex\noptimization minimizing energy. Formal conditions under which perfect\nreconstruction of the signal from the spike trains is possible are then\nidentified in this setup. Finally, a stochastic gradient descent mechanism is\nproposed to achieve these conditions. Simulation experiments are presented to\ndemonstrate the strength and efficacy of the framework\n", "versions": [{"version": "v1", "created": "Fri, 31 May 2019 21:53:42 GMT"}, {"version": "v2", "created": "Tue, 30 Jul 2019 19:40:29 GMT"}], "update_date": "2019-08-01", "authors_parsed": [["Chattopadhyay", "Anik", ""], ["Banerjee", "Arunava", ""]]}, {"id": "1906.00511", "submitter": "Christian Meisel", "authors": "Christian Meisel, Rima El Atrache, Michele Jackson, Sarah Schubach,\n  Claire Ufongene, Tobias Loddenkemper", "title": "Deep learning from wristband sensor data: towards wearable, non-invasive\n  seizure forecasting", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "q-bio.NC eess.SP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Seizure forecasting may provide patients with timely warnings to adapt their\ndaily activities and help clinicians deliver more objective, personalized\ntreatments. While recent work has convincingly demonstrated that seizure risk\nassessment is possible, these early approaches relied largely on complex, often\ninvasive setups including intracranial electrocorticography, implanted devices\nand multi-channel EEG, which limits translation of these methods to broad\nclinical application. To facilitate broader adaptation of seizure forecasting\nin clinical practice, non-invasive, easily applicable techniques that reliably\nassess seizure risk, in combination with clinical information, are crucial.\nWristbands that continuously record physiological parameters, including\nelectrodermal activity, body temperature, blood volume pressure and actigraphy,\nmay afford monitoring of autonomous nervous system function and movement\nrelevant for such a task, hence minimizing potential complications associated\nwith invasive monitoring, and avoiding stigma associated with bulky external\nmonitoring devices on the head. Here, we use deep learning to analyze\nlong-term, multi-modal wristband sensor data from 50 patients with epilepsy\n(total duration $>$1400 hours) to assess its capability to distinguish preictal\nfrom interictal states. Prediction performance is assessed using area under the\nreceiver operating charateristic (AUC) and improvement over chance (IoC) based\non F1 scores. Using one- and two-dimensional convolutional neural networks, we\nidentified better-than-chance predictability in out-of-sample test data in 60\\%\nof the patients in leave-one-out and 43\\% of patients in pseudo-prospective\napproaches. These results provide a step towards developing easier to apply,\nnon-invasive methods for seizure risk assessments in patients with epilepsy.\n", "versions": [{"version": "v1", "created": "Mon, 3 Jun 2019 00:34:55 GMT"}, {"version": "v2", "created": "Fri, 7 Jun 2019 03:13:24 GMT"}], "update_date": "2019-06-10", "authors_parsed": [["Meisel", "Christian", ""], ["Atrache", "Rima El", ""], ["Jackson", "Michele", ""], ["Schubach", "Sarah", ""], ["Ufongene", "Claire", ""], ["Loddenkemper", "Tobias", ""]]}, {"id": "1906.00676", "submitter": "Caglar Cakan", "authors": "Caglar Cakan (1, 2) and Klaus Obermayer (1, 2) ((1) Department of\n  Software Engineering and Theoretical Computer Science, Technische\n  Universit\\\"at Berlin, Germany, (2) Bernstein Center for Computational\n  Neuroscience Berlin, Germany)", "title": "Biophysically grounded mean-field models of neural populations under\n  electrical stimulation", "comments": "A Python package with an implementation of the AdEx mean-field model\n  can be found at https://github.com/neurolib-dev/neurolib - code for\n  simulation and data analysis can be found at\n  https://github.com/caglarcakan/stimulus_neural_populations", "journal-ref": "PLOS Comput. Biol. 16, e1007822 (2020)", "doi": "10.1371/journal.pcbi.1007822", "report-no": null, "categories": "q-bio.NC cond-mat.dis-nn nlin.AO", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Electrical stimulation of neural systems is a key tool for understanding\nneural dynamics and ultimately for developing clinical treatments. Many\napplications of electrical stimulation affect large populations of neurons.\nHowever, computational models of large networks of spiking neurons are\ninherently hard to simulate and analyze. We evaluate a reduced mean-field model\nof excitatory and inhibitory adaptive exponential integrate-and-fire (AdEx)\nneurons which can be used to efficiently study the effects of electrical\nstimulation on large neural populations. The rich dynamical properties of this\nbasic cortical model are described in detail and validated using large network\nsimulations. Bifurcation diagrams reflecting the network's state reveal\nasynchronous up- and down-states, bistable regimes, and oscillatory regions\ncorresponding to fast excitation-inhibition and slow excitation-adaptation\nfeedback loops. The biophysical parameters of the AdEx neuron can be coupled to\nan electric field with realistic field strengths which then can be propagated\nup to the population description.We show how on the edge of bifurcation, direct\nelectrical inputs cause network state transitions, such as turning on and off\noscillations of the population rate. Oscillatory input can frequency-entrain\nand phase-lock endogenous oscillations. Relatively weak electric field\nstrengths on the order of 1 V/m are able to produce these effects, indicating\nthat field effects are strongly amplified in the network. The effects of\ntime-varying external stimulation are well-predicted by the mean-field model,\nfurther underpinning the utility of low-dimensional neural mass models.\n", "versions": [{"version": "v1", "created": "Mon, 3 Jun 2019 09:57:04 GMT"}, {"version": "v2", "created": "Thu, 25 Jul 2019 10:34:26 GMT"}, {"version": "v3", "created": "Sun, 26 Jan 2020 12:51:26 GMT"}, {"version": "v4", "created": "Wed, 29 Jan 2020 10:44:05 GMT"}, {"version": "v5", "created": "Wed, 10 Jun 2020 10:09:41 GMT"}, {"version": "v6", "created": "Thu, 9 Jul 2020 07:59:31 GMT"}, {"version": "v7", "created": "Tue, 17 Nov 2020 14:54:26 GMT"}], "update_date": "2020-11-18", "authors_parsed": [["Cakan", "Caglar", ""], ["Obermayer", "Klaus", ""]]}, {"id": "1906.00728", "submitter": "Stephen Fleming", "authors": "Stephen M. Fleming", "title": "Awareness as inference in a higher-order state space", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "q-bio.NC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Humans have the ability to report the contents of their subjective experience\n- we can say to each other, \"I am aware of X\". The decision processes that\nsupport these reports about mental contents remain poorly understood. In this\narticle I propose a computational framework that characterises awareness\nreports as metacognitive decisions (inference) about a generative model of\nperceptual content. This account is motivated from the perspective of how\nflexible hierarchical state spaces are built during learning and\ndecision-making. Internal states supporting awareness reports, unlike those\ncovarying with perceptual contents, are simple and abstract, varying along a\none-dimensional continuum from absent to present. A critical feature of this\narchitecture is that it is both higher-order and asymmetric: a vast number of\nperceptual states is nested under \"present\", but a much smaller number of\npossible states nested under \"absent\". Via simulations I show that this\nasymmetry provides a natural account of observations of \"global ignition\" in\nbrain imaging studies of awareness reports.\n", "versions": [{"version": "v1", "created": "Fri, 31 May 2019 10:26:26 GMT"}, {"version": "v2", "created": "Mon, 30 Sep 2019 13:21:13 GMT"}, {"version": "v3", "created": "Tue, 3 Dec 2019 11:24:09 GMT"}], "update_date": "2019-12-04", "authors_parsed": [["Fleming", "Stephen M.", ""]]}, {"id": "1906.00889", "submitter": "Benjamin Lansdell", "authors": "Benjamin James Lansdell, Prashanth Ravi Prakash, Konrad Paul Kording", "title": "Learning to solve the credit assignment problem", "comments": "18 pages; 4 figures. (ICLR 2020 version)", "journal-ref": null, "doi": null, "report-no": null, "categories": "q-bio.NC cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Backpropagation is driving today's artificial neural networks (ANNs).\nHowever, despite extensive research, it remains unclear if the brain implements\nthis algorithm. Among neuroscientists, reinforcement learning (RL) algorithms\nare often seen as a realistic alternative: neurons can randomly introduce\nchange, and use unspecific feedback signals to observe their effect on the cost\nand thus approximate their gradient. However, the convergence rate of such\nlearning scales poorly with the number of involved neurons. Here we propose a\nhybrid learning approach. Each neuron uses an RL-type strategy to learn how to\napproximate the gradients that backpropagation would provide. We provide proof\nthat our approach converges to the true gradient for certain classes of\nnetworks. In both feedforward and convolutional networks, we empirically show\nthat our approach learns to approximate the gradient, and can match or the\nperformance of exact gradient-based learning. Learning feedback weights\nprovides a biologically plausible mechanism of achieving good performance,\nwithout the need for precise, pre-specified learning rules.\n", "versions": [{"version": "v1", "created": "Mon, 3 Jun 2019 15:48:38 GMT"}, {"version": "v2", "created": "Wed, 5 Jun 2019 12:06:38 GMT"}, {"version": "v3", "created": "Tue, 15 Oct 2019 14:11:01 GMT"}, {"version": "v4", "created": "Wed, 22 Apr 2020 20:19:19 GMT"}], "update_date": "2020-04-24", "authors_parsed": [["Lansdell", "Benjamin James", ""], ["Prakash", "Prashanth Ravi", ""], ["Kording", "Konrad Paul", ""]]}, {"id": "1906.00905", "submitter": "Quanying Liu", "authors": "Yorie Nakahira, Quanying Liu, Terrence J. Sejnowski, John C. Doyle", "title": "Fitts' Law for speed-accuracy trade-off describes a diversity-enabled\n  sweet spot in sensorimotor control", "comments": "23 pages, 4 figures, Supplementary Material", "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.SP q-bio.NC", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Human sensorimotor control exhibits remarkable speed and accuracy, and the\ntradeoff between them is encapsulated in Fitts' Law for reaching and pointing.\nWhile Fitts related this to Shannon's channel capacity theorem, despite\nwidespread study of Fitts' Law, a theory that connects implementation of\nsensorimotor control at the system and hardware level has not emerged. Here we\ndescribe a theory that connects hardware (neurons and muscles with inherent\nsevere speed-accuracy tradeoffs) with system level control to explain Fitts'\nLaw for reaching and related laws. The results supporting the theory show that\ndiversity between hardware components is exploited to achieve both fast and\naccurate control performance despite slow or inaccurate hardware. Such\n\"diversity-enabled sweet spots\" (DESSs) are ubiquitous in biology and\ntechnology, and explain why large heterogeneities exist in biological and\ntechnical components and how both engineers and natural selection routinely\nevolve fast and accurate systems using imperfect hardware.\n", "versions": [{"version": "v1", "created": "Mon, 3 Jun 2019 16:16:29 GMT"}, {"version": "v2", "created": "Sun, 9 Jun 2019 00:17:42 GMT"}, {"version": "v3", "created": "Thu, 13 Jun 2019 17:38:47 GMT"}, {"version": "v4", "created": "Fri, 6 Sep 2019 00:51:34 GMT"}, {"version": "v5", "created": "Wed, 18 Sep 2019 17:54:45 GMT"}], "update_date": "2019-09-19", "authors_parsed": [["Nakahira", "Yorie", ""], ["Liu", "Quanying", ""], ["Sejnowski", "Terrence J.", ""], ["Doyle", "John C.", ""]]}, {"id": "1906.00926", "submitter": "Christopher Lynn", "authors": "Christopher W. Lynn, Lia Papadopoulos, Ari E. Kahn, and Danielle S.\n  Bassett", "title": "Human information processing in complex networks", "comments": "87 pages, 26 figures, 13 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "physics.soc-ph physics.bio-ph q-bio.NC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Humans communicate using systems of interconnected stimuli or concepts --\nfrom language and music to literature and science -- yet it remains unclear\nhow, if at all, the structure of these networks supports the communication of\ninformation. Although information theory provides tools to quantify the\ninformation produced by a system, traditional metrics do not account for the\ninefficient ways that humans process this information. Here we develop an\nanalytical framework to study the information generated by a system as\nperceived by a human observer. We demonstrate experimentally that this\nperceived information depends critically on a system's network topology.\nApplying our framework to several real networks, we find that they communicate\na large amount of information (having high entropy) and do so efficiently\n(maintaining low divergence from human expectations). Moreover, we show that\nsuch efficient communication arises in networks that are simultaneously\nheterogeneous, with high-degree hubs, and clustered, with tightly-connected\nmodules -- the two defining features of hierarchical organization. Together,\nthese results suggest that many communication networks are constrained by the\npressures of information transmission, and that these pressures select for\nspecific structural features.\n", "versions": [{"version": "v1", "created": "Mon, 3 Jun 2019 16:51:43 GMT"}, {"version": "v2", "created": "Wed, 25 Mar 2020 18:38:56 GMT"}], "update_date": "2020-03-27", "authors_parsed": [["Lynn", "Christopher W.", ""], ["Papadopoulos", "Lia", ""], ["Kahn", "Ari E.", ""], ["Bassett", "Danielle S.", ""]]}, {"id": "1906.01039", "submitter": "Guruprasad Raghavan", "authors": "Guruprasad Raghavan, Matt Thomson", "title": "Neural networks grown and self-organized by noise", "comments": "21 pages (including 11 pages of appendix)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NE cs.AI nlin.AO q-bio.NC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Living neural networks emerge through a process of growth and\nself-organization that begins with a single cell and results in a brain, an\norganized and functional computational device. Artificial neural networks,\nhowever, rely on human-designed, hand-programmed architectures for their\nremarkable performance. Can we develop artificial computational devices that\ncan grow and self-organize without human intervention? In this paper, we\npropose a biologically inspired developmental algorithm that can 'grow' a\nfunctional, layered neural network from a single initial cell. The algorithm\norganizes inter-layer connections to construct a convolutional pooling layer, a\nkey constituent of convolutional neural networks (CNN's). Our approach is\ninspired by the mechanisms employed by the early visual system to wire the\nretina to the lateral geniculate nucleus (LGN), days before animals open their\neyes. The key ingredients for robust self-organization are an emergent\nspontaneous spatiotemporal activity wave in the first layer and a local\nlearning rule in the second layer that 'learns' the underlying activity pattern\nin the first layer. The algorithm is adaptable to a wide-range of input-layer\ngeometries, robust to malfunctioning units in the first layer, and so can be\nused to successfully grow and self-organize pooling architectures of different\npool-sizes and shapes. The algorithm provides a primitive procedure for\nconstructing layered neural networks through growth and self-organization.\nBroadly, our work shows that biologically inspired developmental algorithms can\nbe applied to autonomously grow functional 'brains' in-silico.\n", "versions": [{"version": "v1", "created": "Mon, 3 Jun 2019 19:33:39 GMT"}], "update_date": "2019-06-05", "authors_parsed": [["Raghavan", "Guruprasad", ""], ["Thomson", "Matt", ""]]}, {"id": "1906.01094", "submitter": "Cecilia Jarne", "authors": "C. Jarne and R. Laje", "title": "Graceful degradation of recurrent neural networks as a function of\n  network size, memory length, and connectivity damage", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "q-bio.NC cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recurrent Neural Networks (RNNs) are frequently used to model aspects of\nbrain function and structure. In this work, we trained small fully-connected\nRNNs to perform temporal and flow control tasks with time-varying stimuli. Our\nresults show that different RNNs can solve the same task by converging to\ndifferent underlying dynamics and that the performance gracefully degrades\neither as network size is decreased, interval duration is increased, or\nconnectivity is damaged. Our results are useful to quantify different aspects\nof the models, which are normally used as black boxes and need to be understood\nin advance to modeling the biological response of cerebral cortex areas.\n", "versions": [{"version": "v1", "created": "Mon, 3 Jun 2019 21:56:48 GMT"}, {"version": "v2", "created": "Fri, 13 Sep 2019 16:23:44 GMT"}, {"version": "v3", "created": "Tue, 1 Oct 2019 14:18:27 GMT"}, {"version": "v4", "created": "Mon, 21 Dec 2020 14:04:01 GMT"}], "update_date": "2020-12-22", "authors_parsed": [["Jarne", "C.", ""], ["Laje", "R.", ""]]}, {"id": "1906.01539", "submitter": "Samira Abnar", "authors": "Samira Abnar, Lisa Beinborn, Rochelle Choenni, Willem Zuidema", "title": "Blackbox meets blackbox: Representational Similarity and Stability\n  Analysis of Neural Language Models and Brains", "comments": null, "journal-ref": "2nd BlackBoxNLP workshop @ACL2019", "doi": null, "report-no": null, "categories": "cs.AI cs.CL q-bio.NC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we define and apply representational stability analysis\n(ReStA), an intuitive way of analyzing neural language models. ReStA is a\nvariant of the popular representational similarity analysis (RSA) in cognitive\nneuroscience. While RSA can be used to compare representations in models, model\ncomponents, and human brains, ReStA compares instances of the same model, while\nsystematically varying single model parameter. Using ReStA, we study four\nrecent and successful neural language models, and evaluate how sensitive their\ninternal representations are to the amount of prior context. Using RSA, we\nperform a systematic study of how similar the representational spaces in the\nfirst and second (or higher) layers of these models are to each other and to\npatterns of activation in the human brain. Our results reveal surprisingly\nstrong differences between language models, and give insights into where the\ndeep linguistic processing, that integrates information over multiple\nsentences, is happening in these models. The combination of ReStA and RSA on\nmodels and brains allows us to start addressing the important question of what\nkind of linguistic processes we can hope to observe in fMRI brain imaging data.\nIn particular, our results suggest that the data on story reading from Wehbe et\nal. (2014) contains a signal of shallow linguistic processing, but show no\nevidence on the more interesting deep linguistic processing.\n", "versions": [{"version": "v1", "created": "Tue, 4 Jun 2019 15:52:46 GMT"}, {"version": "v2", "created": "Wed, 5 Jun 2019 09:58:34 GMT"}], "update_date": "2019-06-06", "authors_parsed": [["Abnar", "Samira", ""], ["Beinborn", "Lisa", ""], ["Choenni", "Rochelle", ""], ["Zuidema", "Willem", ""]]}, {"id": "1906.01678", "submitter": "Todd Hylton", "authors": "Todd Hylton", "title": "Thermodynamic Neural Network", "comments": "26 pages, 16 figures", "journal-ref": null, "doi": "10.3390/e22030256", "report-no": null, "categories": "q-bio.NC physics.soc-ph", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A thermodynamically motivated neural network model is described that\nself-organizes to transport charge associated with internal and external\npotentials while in contact with a thermal reservoir. The model integrates\ntechniques for rapid, large-scale, reversible, conservative equilibration of\nnode states and slow, small-scale, irreversible, dissipative adaptation of the\nedge states as a means to create multiscale order. All interactions in the\nnetwork are local and the network structures can be generic and recurrent.\nIsolated networks show multiscale dynamics, and externally driven networks\nevolve to efficiently connect external positive and negative potentials. The\nmodel integrates concepts of conservation, potentiation, fluctuation,\ndissipation, adaptation, equilibration and causation to illustrate the\nthermodynamic evolution of organization in open systems. A key conclusion of\nthe work is that the transport and dissipation of conserved physical quantities\ndrives the self-organization of open thermodynamic systems.\n", "versions": [{"version": "v1", "created": "Fri, 31 May 2019 01:18:30 GMT"}, {"version": "v2", "created": "Sun, 13 Oct 2019 20:32:47 GMT"}, {"version": "v3", "created": "Tue, 5 Nov 2019 22:49:05 GMT"}, {"version": "v4", "created": "Thu, 30 Jan 2020 21:09:27 GMT"}], "update_date": "2020-04-22", "authors_parsed": [["Hylton", "Todd", ""]]}, {"id": "1906.01700", "submitter": "Andreas Brink-Kj{\\ae}r", "authors": "Andreas Brink-Kjaer, Alexander Neergaard Olesen, Paul E. Peppard,\n  Katie L. Stone, Poul Jennum, Emmanuel Mignot, Helge B.D. Sorensen", "title": "Automatic Detection of Cortical Arousals in Sleep and their Contribution\n  to Daytime Sleepiness", "comments": "40 pages, 13 figures, 9 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "q-bio.NC cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Cortical arousals are transient events of disturbed sleep that occur\nspontaneously or in response to stimuli such as apneic events. The gold\nstandard for arousal detection in human polysomnographic recordings (PSGs) is\nmanual annotation by expert human scorers, a method with significant\ninterscorer variability. In this study, we developed an automated method, the\nMultimodal Arousal Detector (MAD), to detect arousals using deep learning\nmethods. The MAD was trained on 2,889 PSGs to detect both cortical arousals and\nwakefulness in 1 second intervals. Furthermore, the relationship between\nMAD-predicted labels on PSGs and next day mean sleep latency (MSL) on a\nmultiple sleep latency test (MSLT), a reflection of daytime sleepiness, was\nanalyzed in 1447 MSLT instances in 873 subjects. In a dataset of 1,026 PSGs,\nthe MAD achieved a F1 score of 0.76 for arousal detection, while wakefulness\nwas predicted with an accuracy of 0.95. In 60 PSGs scored by multiple human\nexpert technicians, the MAD significantly outperformed the average human scorer\nfor arousal detection with a difference in F1 score of 0.09. After controlling\nfor other known covariates, a doubling of the arousal index was associated with\nan average decrease in MSL of 40 seconds ($\\beta$ = -0.67, p = 0.0075). The MAD\noutperformed the average human expert and the MAD-predicted arousals were shown\nto be significant predictors of MSL, which demonstrate clinical validity the\nMAD.\n", "versions": [{"version": "v1", "created": "Fri, 15 Mar 2019 10:11:32 GMT"}], "update_date": "2019-06-06", "authors_parsed": [["Brink-Kjaer", "Andreas", ""], ["Olesen", "Alexander Neergaard", ""], ["Peppard", "Paul E.", ""], ["Stone", "Katie L.", ""], ["Jennum", "Poul", ""], ["Mignot", "Emmanuel", ""], ["Sorensen", "Helge B. D.", ""]]}, {"id": "1906.01703", "submitter": "Jiawei Zhang", "authors": "Jiawei Zhang", "title": "Basic Neural Units of the Brain: Neurons, Synapses and Action Potential", "comments": "38 pages, 23 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "q-bio.NC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  As a follow-up tutorial article of [29], in this paper, we will introduce the\nbasic compositional units of the human brain, which will further illustrate the\ncell-level bio-structure of the brain. On average, the human brain contains\nabout 100 billion neurons and many more neuroglia which serve to support and\nprotect the neurons. Each neuron may be connected to up to 10,000 other\nneurons, passing signals to each other via as many as 1,000 trillion synapses.\nIn the nervous system, a synapse is a structure that permits a neuron to pass\nan electrical or chemical signal to another neuron or to the target effector\ncell. Such signals will be accumulated as the membrane potential of the\nneurons, and it will trigger and pass the signal pulse (i.e., action potential)\nto other neurons when the membrane potential is greater than a precisely\ndefined threshold voltage. To be more specific, in this paper, we will talk\nabout the neurons, synapses and the action potential concepts in detail. Many\nof the materials used in this paper are from wikipedia and several other\nneuroscience introductory articles, which will be properly cited in this paper.\nThis is the second of the three tutorial articles about the brain (the other\ntwo are [29] and [28]). The readers are suggested to read the previous tutorial\narticle [29] to get more background information about the brain structure and\nfunctions prior to reading this paper.\n", "versions": [{"version": "v1", "created": "Thu, 30 May 2019 23:11:57 GMT"}], "update_date": "2019-06-06", "authors_parsed": [["Zhang", "Jiawei", ""]]}, {"id": "1906.01704", "submitter": "Yang Li", "authors": "Yang Li, Wenming Zheng, Lei Wang, Yuan Zong, Lei Qi, Zhen Cui, Tong\n  Zhang, and Tengfei Song", "title": "A Novel Bi-hemispheric Discrepancy Model for EEG Emotion Recognition", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "q-bio.NC eess.SP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The neuroscience study has revealed the discrepancy of emotion expression\nbetween left and right hemispheres of human brain. Inspired by this study, in\nthis paper, we propose a novel bi-hemispheric discrepancy model (BiHDM) to\nlearn the asymmetric differences between two hemispheres for\nelectroencephalograph (EEG) emotion recognition. Concretely, we first employ\nfour directed recurrent neural networks (RNNs) based on two spatial\norientations to traverse electrode signals on two separate brain regions, which\nenables the model to obtain the deep representations of all the EEG electrodes'\nsignals while keeping the intrinsic spatial dependence. Then we design a\npairwise subnetwork to capture the discrepancy information between two\nhemispheres and extract higher-level features for final classification.\nBesides, in order to reduce the domain shift between training and testing data,\nwe use a domain discriminator that adversarially induces the overall feature\nlearning module to generate emotion-related but domain-invariant feature, which\ncan further promote EEG emotion recognition. We conduct experiments on three\npublic EEG emotional datasets, and the experiments show that the new\nstate-of-the-art results can be achieved.\n", "versions": [{"version": "v1", "created": "Sat, 11 May 2019 01:17:22 GMT"}], "update_date": "2019-06-06", "authors_parsed": [["Li", "Yang", ""], ["Zheng", "Wenming", ""], ["Wang", "Lei", ""], ["Zong", "Yuan", ""], ["Qi", "Lei", ""], ["Cui", "Zhen", ""], ["Zhang", "Tong", ""], ["Song", "Tengfei", ""]]}, {"id": "1906.01767", "submitter": "Stevan Harnad", "authors": "Stevan Harnad", "title": "Codes, communication and cognition", "comments": "4 pages, no figures, 7 references", "journal-ref": "Behav Brain Sci 42 (2019) e231", "doi": "10.1017/S0140525X19001481", "report-no": null, "categories": "q-bio.NC", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Brette (2019) criticizes the notion of neural coding because it seems to\nentail that neural signals need to be decoded by or for some receiver in the\nhead. If that were so, then neural coding would indeed be homuncular (Brette\ncalls it dualistic), requiring an entity to decipher the code. But I think the\nplea of Brett to think instead in terms of complex, interactive causal\nthroughput is preaching to the converted. Turing (not Shannon) has already\nshown the way. In any case, the metaphor of neural coding has little to do with\nthe symbol grounding problem.\n", "versions": [{"version": "v1", "created": "Wed, 5 Jun 2019 00:41:06 GMT"}], "update_date": "2019-12-04", "authors_parsed": [["Harnad", "Stevan", ""]]}, {"id": "1906.01862", "submitter": "Pierrick Coupe", "authors": "Pierrick Coup\\'e, Boris Mansencal, Micha\\\"el Cl\\'ement, R\\'emi Giraud,\n  Baudouin Denis de Senneville, Vinh-Thong Ta, Vincent Lepetit, Jos\\'e V.\n  Manjon", "title": "AssemblyNet: A Novel Deep Decision-Making Process for Whole Brain MRI\n  Segmentation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV cs.LG q-bio.NC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Whole brain segmentation using deep learning (DL) is a very challenging task\nsince the number of anatomical labels is very high compared to the number of\navailable training images. To address this problem, previous DL methods\nproposed to use a global convolution neural network (CNN) or few independent\nCNNs. In this paper, we present a novel ensemble method based on a large number\nof CNNs processing different overlapping brain areas. Inspired by parliamentary\ndecision-making systems, we propose a framework called AssemblyNet, made of two\n\"assemblies\" of U-Nets. Such a parliamentary system is capable of dealing with\ncomplex decisions and reaching a consensus quickly. AssemblyNet introduces\nsharing of knowledge among neighboring U-Nets, an \"amendment\" procedure made by\nthe second assembly at higher-resolution to refine the decision taken by the\nfirst one, and a final decision obtained by majority voting. When using the\nsame 45 training images, AssemblyNet outperforms global U-Net by 28% in terms\nof the Dice metric, patch-based joint label fusion by 15% and SLANT-27 by 10%.\nFinally, AssemblyNet demonstrates high capacity to deal with limited training\ndata to achieve whole brain segmentation in practical training and testing\ntimes.\n", "versions": [{"version": "v1", "created": "Wed, 5 Jun 2019 07:35:37 GMT"}], "update_date": "2019-06-06", "authors_parsed": [["Coup\u00e9", "Pierrick", ""], ["Mansencal", "Boris", ""], ["Cl\u00e9ment", "Micha\u00ebl", ""], ["Giraud", "R\u00e9mi", ""], ["de Senneville", "Baudouin Denis", ""], ["Ta", "Vinh-Thong", ""], ["Lepetit", "Vincent", ""], ["Manjon", "Jos\u00e9 V.", ""]]}, {"id": "1906.02076", "submitter": "David Calhas", "authors": "David Calhas, Enrique Romero, Rui Henriques", "title": "On the use of Pairwise Distance Learning for Brain Signal Classification\n  with Limited Observations", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV cs.NE eess.SP q-bio.NC stat.ML", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  The increasing access to brain signal data using electroencephalography\ncreates new opportunities to study electrophysiological brain activity and\nperform ambulatory diagnoses of neuronal diseases. This work proposes a\npairwise distance learning approach for Schizophrenia classification relying on\nthe spectral properties of the signal. Given the limited number of observations\n(i.e. the case and/or control individuals) in clinical trials, we propose a\nSiamese neural network architecture to learn a discriminative feature space\nfrom pairwise combinations of observations per channel. In this way, the\nmultivariate order of the signal is used as a form of data augmentation,\nfurther supporting the network generalization ability. Convolutional layers\nwith parameters learned under a cosine contrastive loss are proposed to\nadequately explore spectral images derived from the brain signal. Results on a\ncase-control population show that the features extracted using the proposed\nneural network lead to an improved Schizophrenia diagnosis (+10pp in accuracy\nand sensitivity) against spectral features, thus suggesting the existence of\nnon-trivial, discriminative electrophysiological brain patterns.\n", "versions": [{"version": "v1", "created": "Wed, 5 Jun 2019 15:36:57 GMT"}, {"version": "v2", "created": "Fri, 6 Sep 2019 11:40:27 GMT"}], "update_date": "2019-09-09", "authors_parsed": [["Calhas", "David", ""], ["Romero", "Enrique", ""], ["Henriques", "Rui", ""]]}, {"id": "1906.02241", "submitter": "Yun Zhao", "authors": "Yun Zhao, Elmer Guzman, Morgane Audouard, Zhuowei Cheng, PaulK.\n  Hansma, Kenneth S. Kosik, and Linda Petzold", "title": "A Deep Learning Framework for Classification of in vitro Multi-Electrode\n  Array Recordings", "comments": "14 pages, in ICDM 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "q-bio.NC cs.LG eess.SP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Multi-Electrode Arrays (MEAs) have been widely used to record neuronal\nactivities, which could be used in the diagnosis of gene defects and drug\neffects. In this paper, we address the problem of classifying in vitro MEA\nrecordings of mouse and human neuronal cultures from different genotypes, where\nthere is no easy way to directly utilize raw sequences as inputs to train an\nend-to-end classification model. While carefully extracting some features by\nhand could partially solve the problem, this approach suffers from obvious\ndrawbacks such as difficulty of generalizing. We propose a deep learning\nframework to address this challenge. Our approach correctly classifies neuronal\nculture data prepared from two different genotypes -- a mouse Knockout of the\ndelta-catenin gene and human induced Pluripotent Stem Cell-derived neurons from\nWilliams syndrome. By splitting the long recordings into short slices for\ntraining, and applying Consensus Prediction during testing, our deep learning\napproach improves the prediction accuracy by 16.69% compared with feature based\nLogistic Regression for mouse MEA recordings. We further achieve an accuracy of\n95.91% using Consensus Prediction in one subset of mouse MEA recording data,\nwhich were all recorded at six days in vitro. As high-density MEA recordings\nbecome more widely available, this approach could be generalized for\nclassification of neurons carrying different mutations and classification of\ndrug responses.\n", "versions": [{"version": "v1", "created": "Wed, 5 Jun 2019 18:36:31 GMT"}], "update_date": "2019-06-07", "authors_parsed": [["Zhao", "Yun", ""], ["Guzman", "Elmer", ""], ["Audouard", "Morgane", ""], ["Cheng", "Zhuowei", ""], ["Hansma", "PaulK.", ""], ["Kosik", "Kenneth S.", ""], ["Petzold", "Linda", ""]]}, {"id": "1906.02487", "submitter": "Qiulei Dong", "authors": "Qiulei Dong and Bo Liu and Zhanyi Hu", "title": "Non-uniqueness phenomenon of object representation in modelling IT\n  cortex by deep convolutional neural network (DCNN)", "comments": "24 pages, 5 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "q-bio.NC cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recently DCNN (Deep Convolutional Neural Network) has been advocated as a\ngeneral and promising modelling approach for neural object representation in\nprimate inferotemporal cortex. In this work, we show that some inherent\nnon-uniqueness problem exists in the DCNN-based modelling of image object\nrepresentations. This non-uniqueness phenomenon reveals to some extent the\ntheoretical limitation of this general modelling approach, and invites due\nattention to be taken in practice.\n", "versions": [{"version": "v1", "created": "Thu, 6 Jun 2019 09:08:21 GMT"}], "update_date": "2019-06-07", "authors_parsed": [["Dong", "Qiulei", ""], ["Liu", "Bo", ""], ["Hu", "Zhanyi", ""]]}, {"id": "1906.02558", "submitter": "Mohammad Nami", "authors": "Iman Ghodratitoostani, Ali-Mohammad Kamali, Mahshid Tahamtan, Neda\n  Mohammadi, Hadi Aligholi, Mohammad Nami", "title": "The Substrates of Integrated Neurocognitive Rehabilitation Platforms\n  (INCRPs)", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "q-bio.NC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The integrated neurocognitive rehabilitation platforms (INCRPs) refer to\ninfrastructures and teams integrated for set of interventions which aim to\nrestore, or compensate for cognitive deficits. Cognitive skills may be lost or\naltered due to brain damage resulting from diseases or injury. The INCRP is a\ntwo-way interactive process whereby people with neurological impairments work\nwith specialists, professional staff, families, and community members to\nalleviate the impact of cognitive deficits. This perspective paper would\nhighlight key elements required in INCRPs.\n", "versions": [{"version": "v1", "created": "Thu, 6 Jun 2019 12:58:53 GMT"}], "update_date": "2019-06-07", "authors_parsed": [["Ghodratitoostani", "Iman", ""], ["Kamali", "Ali-Mohammad", ""], ["Tahamtan", "Mahshid", ""], ["Mohammadi", "Neda", ""], ["Aligholi", "Hadi", ""], ["Nami", "Mohammad", ""]]}, {"id": "1906.02757", "submitter": "Francesco Cremonesi", "authors": "Francesco Cremonesi and Felix Sch\\\"urmann", "title": "Telling neuronal apples from oranges: analytical performance modeling of\n  neural tissue simulations", "comments": "44 pages, 9 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "q-bio.NC physics.comp-ph", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Computational modeling and simulation have become essential tools in the\nquest to better understand the brain's makeup and to decipher the causal\ninterrelations of its components. The breadth of biochemical and biophysical\nprocesses and structures in the brain has led to the development of a large\nvariety of model abstractions and specialized tools, often times requiring high\nperformance computing resource for their timely execution. What has been\nmissing so far was an in-depth analysis of the complexity of the computational\nkernels, hindering a systematic approach to identifying bottlenecks of\nalgorithms and hardware, and their combinations. If whole brain models are to\nbe achieved on emerging computer generations, models and simulation engines\nwill have to be carefully co-designed for the intrinsic hardware tradeoffs. For\nthe first time, we present a systematic exploration based on analytic\nperformance modeling. We base our analysis on three in silico models, chosen as\nrepresentative examples of the most widely employed modeling abstractions. We\nidentify that the synaptic formalism, i.e. current or conductance based\nrepresentations, and not the level of morphological detail, is the most\nsignificant factor in determining the properties of memory bandwidth saturation\nand shared-memory scaling of in silico models. Even though general purpose\ncomputing has, until now, largely been able to deliver high performance, we\nfind that for all types of abstractions, network latency and memory bandwidth\nwill become severe bottlenecks as the number of neurons to be simulated grows.\nBy adapting and extending a performance modeling approach, we deliver a first\ncharacterization of the performance landscape of brain tissue simulations,\nallowing us to pinpoint current bottlenecks in state-of-the-art in silico\nmodels, and make projections for future hardware and software requirements.\n", "versions": [{"version": "v1", "created": "Thu, 6 Jun 2019 18:00:53 GMT"}], "update_date": "2019-06-10", "authors_parsed": [["Cremonesi", "Francesco", ""], ["Sch\u00fcrmann", "Felix", ""]]}, {"id": "1906.02796", "submitter": "Wilkie Olin-Ammentorp", "authors": "Wilkie Olin-Ammentorp, Karsten Beckmann, Catherine D. Schuman, James\n  S. Plank, Nathaniel C. Cady", "title": "Stochasticity and Robustness in Spiking Neural Networks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NE q-bio.NC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Artificial neural networks normally require precise weights to operate,\ndespite their origins in biological systems, which can be highly variable and\nnoisy. When implementing artificial networks which utilize analog 'synaptic'\ndevices to encode weights, however, inherent limits are placed on the accuracy\nand precision with which these values can be encoded. In this work, we\ninvestigate the effects that inaccurate synapses have on spiking neurons and\nspiking neural networks. Starting with a mathematical analysis of\nintegrate-and-fire (IF) neurons, including different non-idealities (such as\nleakage and channel noise), we demonstrate that noise can be used to make the\nbehavior of IF neurons more robust to synaptic inaccuracy. We then train\nspiking networks which utilize IF neurons with and without noise and leakage,\nand experimentally confirm that the noisy networks are more robust. Lastly, we\nshow that a noisy network can tolerate the inaccuracy expected when\nhafnium-oxide based resistive random-access memory is used to encode synaptic\nweights.\n", "versions": [{"version": "v1", "created": "Thu, 6 Jun 2019 20:16:46 GMT"}], "update_date": "2019-06-10", "authors_parsed": [["Olin-Ammentorp", "Wilkie", ""], ["Beckmann", "Karsten", ""], ["Schuman", "Catherine D.", ""], ["Plank", "James S.", ""], ["Cady", "Nathaniel C.", ""]]}, {"id": "1906.02863", "submitter": "Qi Zhao", "authors": "Qi Zhao, Lingli Zhang, Chun Shen, Jie Zhang, Jianfeng Feng", "title": "Double Generalized Linear Model Reveals Those with High Intelligence are\n  More Similar in Cortical Thickness", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "q-bio.NC stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Most studies indicate that intelligence (g) is positively correlated with\ncortical thickness. However, the interindividual variability of cortical\nthickness has not been taken into account. In this study, we aimed to identify\nthe association between intelligence and cortical thickness in adolescents from\nboth the group's mean and dispersion point of view, utilizing the structural\nbrain imaging from the Adolescent Brain and Cognitive Development (ABCD)\nConsortium, the largest cohort in early adolescents around 10 years old. The\nmean and dispersion parameters of cortical thickness and their association with\nintelligence were estimated using double generalized linear models(DGLM). We\nfound that for the mean model part, the thickness of the frontal lobe like\nsuperior frontal gyrus was negatively related to intelligence, while the\nsurface area was most positively associated with intelligence in the frontal\nlobe. In the dispersion part, intelligence was negatively correlated with the\ndispersion of cortical thickness in widespread areas, but not with the surface\narea. These results suggested that people with higher IQ are more similar in\ncortical thickness, which may be related to less differentiation or\nheterogeneity in cortical columns.\n", "versions": [{"version": "v1", "created": "Fri, 7 Jun 2019 02:11:21 GMT"}, {"version": "v2", "created": "Fri, 22 Nov 2019 10:34:16 GMT"}], "update_date": "2019-11-25", "authors_parsed": [["Zhao", "Qi", ""], ["Zhang", "Lingli", ""], ["Shen", "Chun", ""], ["Zhang", "Jie", ""], ["Feng", "Jianfeng", ""]]}, {"id": "1906.03078", "submitter": "Jurgis Pods", "authors": "Jurgis Pods", "title": "Electrodiffusion Models of Axon and Extracellular Space Using the\n  Poisson-Nernst-Planck Equations", "comments": "PhD thesis, 2014, University of Heidelberg, permalink to university\n  library open access publication: http://www.ub.uni-heidelberg.de/archiv/17128", "journal-ref": null, "doi": "10.11588/heidok.00017128", "report-no": null, "categories": "q-bio.NC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In studies of the brain and the nervous system, extracellular signals - as\nmeasured by local field potentials (LFPs) or electroencephalography (EEG) - are\nof capital importance, as they allow to simultaneously obtain data from\nmultiple neurons. The exact biophysical basis of these signals is, however,\nstill not fully understood. Most models for the extracellular potential today\nare based on volume conductor theory, which assumes that the extracellular\nfluid is electroneutral and that the only contributions to the electric field\nare given by membrane currents, which can be imposed as boundary conditions in\nthe mathematical model. This neglects a second, possibly important contributor\nto the extracellular field: the time- and position-dependent concentrations of\nions in the intra- and extracellular fluids. In this thesis, a 3D model of a\nsingle axon in extracellular fluid is presented based on the\nPoisson-Nernst-Planck (PNP) equations of electrodiffusion. This fundamental\nmodel includes not only the potential, but also the concentrations of all\nparticipating ion concentrations in a self-consistent way. This enables us to\nstudy the propagation of an action potential (AP) along the axonal membrane\nbased on first principles by means of numerical simulations. By exploiting the\ncylinder symmetry of this geometry, the problem can be reduced to two\ndimensions. The numerical solution is implemented in a flexible and efficient\nway, using the DUNE framework. A suitable mesh generation strategy and a\nparallelization of the algorithm allow to solve the problem in reasonable time,\nwith a high spatial and temporal resolution. The methods and programming\ntechniques used to deal with the numerical challenges of this multi-scale\nproblem are presented in detail.\n", "versions": [{"version": "v1", "created": "Fri, 7 Jun 2019 13:16:24 GMT"}], "update_date": "2019-06-10", "authors_parsed": [["Pods", "Jurgis", ""]]}, {"id": "1906.03314", "submitter": "Jiawei Zhang", "authors": "Jiawei Zhang", "title": "Secrets of the Brain: An Introduction to the Brain Anatomical Structure\n  and Biological Function", "comments": "34 pages, 19 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "q-bio.NC cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we will provide an introduction to the brain structure and\nfunction. Brain is an astonishing living organ inside our heads, weighing about\n1.5kg, consisting of billions of tiny cells. The brain enables us to sense the\nworld around us (to touch, to smell, to see and to hear, etc.), to think and to\nrespond to the world as well. The main obstacles that prevent us from creating\na machine which can behavior like real-world creatures are due to our limited\nknowledge about the brain in both its structure and its function. In this\npaper, we will focus introducing the brain anatomical structure and biological\nfunction, as well as its surrounding sensory systems. Many of the materials\nused in this paper are from wikipedia and several other neuroscience\nintroductory articles, which will be properly cited in this article. This is\nthe first of the three tutorial articles about the brain (the other two are\n[26] and [27]). In the follow-up two articles, we will further introduce the\nlow-level composition basis structures (e.g., neuron, synapse and action\npotential) and the high-level cognitive functions (e.g., consciousness,\nattention, learning and memory) of the brain, respectively.\n", "versions": [{"version": "v1", "created": "Fri, 31 May 2019 03:07:33 GMT"}], "update_date": "2019-06-11", "authors_parsed": [["Zhang", "Jiawei", ""]]}, {"id": "1906.03958", "submitter": "Collins Assisi", "authors": "Saptarshi Soham Mohanta and Collins Assisi", "title": "Parallel scalable simulations of biological neural networks using\n  TensorFlow: A beginner's guide", "comments": "Download the associated code from https://github.com/technosap/PSST", "journal-ref": null, "doi": null, "report-no": null, "categories": "q-bio.NC q-bio.QM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Neuronal networks are often modeled as systems of coupled, nonlinear,\nordinary or partial differential equations. The number of differential\nequations used to model a network increases with the size of the network and\nthe level of detail used to model individual neurons and synapses. As one\nscales up the size of the simulation it becomes important to use powerful\ncomputing platforms. Many tools exist that solve these equations numerically.\nHowever, these tools are often platform specific. There is a high barrier of\nentry to developing flexible general purpose code that is platform independent\nand supports hardware acceleration on modern computing architectures such as\nGPUs/TPUs and Distributed Platforms. TensorFlow is a Python-based open-source\npackage initially designed for machine learning algorithms, but it presents a\nscalable environment for a variety of computations including solving\ndifferential equations using iterative algorithms such as Runge Kutta methods.\nIn this article, organized as a series of tutorials, we present a simple\nexposition of numerical methods to solve ordinary differential equations using\nPython and TensorFlow. It consists of a series of Python notebooks that, over\nthe course of five sessions, will lead novice programmers from writing programs\nto integrate simple 1-dimensional differential equations using Python, to\nsolving a large system (1000's of differential equations) of conductance-based\nneurons using a highly parallel and scalable framework. Embedded within the\ntutorial is a physiologically realistic implementation of a network in the\ninsect olfactory system. This system, consisting of multiple neuron and synapse\ntypes, can serve as a template to simulate other networks.\n", "versions": [{"version": "v1", "created": "Mon, 10 Jun 2019 13:01:57 GMT"}], "update_date": "2019-06-11", "authors_parsed": [["Mohanta", "Saptarshi Soham", ""], ["Assisi", "Collins", ""]]}, {"id": "1906.05166", "submitter": "Antonio de Candia", "authors": "S. Scarpetta, I. Apicella, L. Minati, A. de Candia", "title": "Hysteresis, neural avalanches and critical behaviour near a first-order\n  transition of a spiking neural network", "comments": null, "journal-ref": "Phys. Rev. E 97, 062305 (2018)", "doi": "10.1103/PhysRevE.97.062305", "report-no": null, "categories": "q-bio.NC cond-mat.dis-nn", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Many experimental results, both in-vivo and in-vitro, support the idea that\nthe brain cortex operates near a critical point, and at the same time works as\na reservoir of precise spatio-temporal patterns. However the mechanism at the\nbasis of these observations is still not clear. In this paper we introduce a\nmodel which combines both these features, showing that scale-free avalanches\nare the signature of a system posed near the spinodal line of a first order\ntransition, with many spatio-temporal patterns stored as dynamical metastable\nattractors. Specifically, we studied a network of leaky integrate and fire\nneurons, whose connections are the result of the learning of multiple\nspatio-temporal dynamical patterns, each with a randomly chosen ordering of the\nneurons. We found that the network shows a first order transition between a low\nspiking rate disordered state (down), and a high rate state characterized by\nthe emergence of collective activity and the replay of one of the stored\npatterns (up). The transition is characterized by hysteresis, or alternation of\nup and down states, depending on the lifetime of the metastable states. In both\ncases, critical features and neural avalanches are observed. Notably, critical\nphenomena occur at the edge of a discontinuous phase transition, as recently\nobserved in a network of glow lamps.\n", "versions": [{"version": "v1", "created": "Wed, 12 Jun 2019 14:26:43 GMT"}], "update_date": "2019-06-14", "authors_parsed": [["Scarpetta", "S.", ""], ["Apicella", "I.", ""], ["Minati", "L.", ""], ["de Candia", "A.", ""]]}, {"id": "1906.05369", "submitter": "Eduardo Mart\\'inez-Montes", "authors": "Julio A. Peraza-Goicolea, Eduardo Mart\\'inez-Montes, Eduardo Aubert,\n  Pedro A. Vald\\'es-Hern\\'andez, Roberto Mulet", "title": "Modeling functional resting-state brain networks through neural message\n  passing on the human connectome", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "q-bio.NC cond-mat.dis-nn cond-mat.stat-mech nlin.AO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Understanding the relationship between the structure and function of the\nhuman brain is one of the most important open questions in Neurosciences. In\nparticular, Resting State Networks (RSN) and more specifically the Default Mode\nNetwork (DMN) of the brain, which are defined from the analysis of functional\ndata lack a definitive justification consistent with the anatomical structure\nof the brain. In this work, we show that a possible connection may naturally\nrest on the idea that information flows in the brain through a neural\nmessage-passing dynamics between macroscopic structures, like those defined by\nthe human connectome (HC). In our model, each brain region in the HC is assumed\nto have a binary behavior (active or not), the strength of interactions among\nthem is encoded in the anatomical connectivity matrix defined by the HC, and\nthe dynamics of the system is defined by a neural message-passing algorithm,\nBelief Propagation (BP), working near the critical point of the human\nconnectome. We show that in the absence of direct external stimuli the BP\nalgorithm converges to a spatial map of activations that is similar to the DMN.\nMoreover, we computed, using Susceptibility Propagation (SP), the matrix of\ncorrelations between the different regions and show that the modules defined by\na clustering of this matrix resemble several Resting States Networks determined\nexperimentally. Both results suggest that the functional DMN and RSNs can be\nseen as simple consequences of the anatomical structure of the brain and a\nneural message-passing dynamics between macroscopic regions. We then show\npreliminary results indicating our predictions on how functional DMN maps\nchange when the anatomical brain network suffers structural anomalies, like in\nAlzheimers Disease and in lesions of the Corpus Callosum.\n", "versions": [{"version": "v1", "created": "Wed, 12 Jun 2019 20:32:57 GMT"}], "update_date": "2019-06-14", "authors_parsed": [["Peraza-Goicolea", "Julio A.", ""], ["Mart\u00ednez-Montes", "Eduardo", ""], ["Aubert", "Eduardo", ""], ["Vald\u00e9s-Hern\u00e1ndez", "Pedro A.", ""], ["Mulet", "Roberto", ""]]}, {"id": "1906.05584", "submitter": "Antonio de Candia", "authors": "S. Scarpetta, A. de Candia", "title": "Information capacity of a network of spiking neurons", "comments": "Accepted for publication in Physica A", "journal-ref": null, "doi": "10.1016/j.physa.2019.123681", "report-no": null, "categories": "q-bio.NC cond-mat.dis-nn", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study a model of spiking neurons, with recurrent connections that result\nfrom learning a set of spatio-temporal patterns with a spike-timing dependent\nplasticity rule and a global inhibition. We investigate the ability of the\nnetwork to store and selectively replay multiple patterns of spikes, with a\ncombination of spatial population and phase-of-spike code. Each neuron in a\npattern is characterized by a binary variable determining if the neuron is\nactive in the pattern, and a phase-lag variable representing the spike-timing\norder among the active units. After the learning stage, we study the dynamics\nof the network induced by a brief cue stimulation, and verify that the network\nis able to selectively replay the pattern correctly and persistently. We\ncalculate the information capacity of the network, defined as the maximum\nnumber of patterns that can be encoded in the network times the number of bits\ncarried by each pattern, normalized by the number of synapses, and find that it\ncan reach a value $\\alpha_\\text{max}\\simeq 0.27$, similar to the one of\nsequence processing neural networks, and almost double of the capacity of the\nstatic Hopfield model. We study the dependence of the capacity on the global\ninhibition, connection strength (or neuron threshold) and fraction of neurons\nparticipating to the patterns. The results show that a dual population and\ntemporal coding can be optimal for the capacity of an associative memory.\n", "versions": [{"version": "v1", "created": "Thu, 13 Jun 2019 09:56:33 GMT"}, {"version": "v2", "created": "Sun, 24 Nov 2019 21:34:39 GMT"}], "update_date": "2020-04-22", "authors_parsed": [["Scarpetta", "S.", ""], ["de Candia", "A.", ""]]}, {"id": "1906.05624", "submitter": "Mauricio Girardi-Schappo", "authors": "Mauricio Girardi-Schappo, Ludmila Brochini, Ariadne A. Costa, Tawan T.\n  A. Carvalho, Osame Kinouchi", "title": "Self-organized critical balanced networks: a unified framework", "comments": "30 pages, 5 figures", "journal-ref": "Phys. Rev. Research 2, 012042(R) (2020)", "doi": "10.1103/PhysRevResearch.2.012042", "report-no": null, "categories": "nlin.AO nlin.CD physics.bio-ph q-bio.NC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Asynchronous irregular (AI) and critical states are two competing frameworks\nproposed to explain spontaneous neuronal activity. Here, we propose a\nmean-field model with simple stochastic neurons that generalizes the\nintegrate-and-fire network of Brunel (2000). We show that the point with\nbalanced inhibitory/excitatory synaptic weight ratio $g_c \\approx 4$\ncorresponds to a second order absorbing phase transition usual in\nself-organized critical (SOC) models. At the synaptic balance point $g_c$, the\nnetwork exhibits power-law neuronal avalanches with the usual exponents,\nwhereas for nonzero external field the system displays the four usual\nsynchronicity states of balanced networks. We add homeostatic inhibition and\nfiring rate adaption and obtain a self-organized quasi-critical balanced state\nwith avalanches and AI-like activity. Our model might explain why different\ninhibition levels are obtained in different experimental conditions and for\ndifferent regions of the brain, since at least two dynamical mechanisms are\nnecessary to obtain a truly balanced state, without which the network may hover\nin different regions of the presented theoretical phase diagram.\n", "versions": [{"version": "v1", "created": "Thu, 13 Jun 2019 12:17:52 GMT"}], "update_date": "2020-02-24", "authors_parsed": [["Girardi-Schappo", "Mauricio", ""], ["Brochini", "Ludmila", ""], ["Costa", "Ariadne A.", ""], ["Carvalho", "Tawan T. A.", ""], ["Kinouchi", "Osame", ""]]}, {"id": "1906.05803", "submitter": "Anqi Liu", "authors": "Quanying Liu, Haiyan Wu, Anqi Liu", "title": "Modeling and Interpreting Real-world Human Risk Decision Making with\n  Inverse Reinforcement Learning", "comments": "Real-world Sequential Decision Making Workshop at ICML 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG q-bio.NC stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We model human decision-making behaviors in a risk-taking task using inverse\nreinforcement learning (IRL) for the purposes of understanding real human\ndecision making under risk. To the best of our knowledge, this is the first\nwork applying IRL to reveal the implicit reward function in human risk-taking\ndecision making and to interpret risk-prone and risk-averse decision-making\npolicies. We hypothesize that the state history (e.g. rewards and decisions in\nprevious trials) are related to the human reward function, which leads to\nrisk-averse and risk-prone decisions. We design features that reflect these\nfactors in the reward function of IRL and learn the corresponding weight that\nis interpretable as the importance of features. The results confirm the\nsub-optimal risk-related decisions of human-driven by the personalized reward\nfunction. In particular, the risk-prone person tends to decide based on the\ncurrent pump number, while the risk-averse person relies on burst information\nfrom the previous trial and the average end status. Our results demonstrate\nthat IRL is an effective tool to model human decision-making behavior, as well\nas to help interpret the human psychological process in risk decision-making.\n", "versions": [{"version": "v1", "created": "Thu, 13 Jun 2019 16:35:32 GMT"}], "update_date": "2019-06-14", "authors_parsed": [["Liu", "Quanying", ""], ["Wu", "Haiyan", ""], ["Liu", "Anqi", ""]]}, {"id": "1906.06006", "submitter": "Carina Curto", "authors": "Lindsey S. Brown, Carina Curto", "title": "Periodic Neural Codes and Sound Localization in Barn Owls", "comments": "31 pages, 8 figures, new title. To appear in Involve", "journal-ref": null, "doi": null, "report-no": null, "categories": "q-bio.NC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Inspired by the sound localization system of the barn owl, we define a new\nclass of neural codes, called periodic codes, and study their basic properties.\nPeriodic codes are binary codes with a special patterned form that reflects the\nperiodicity of the stimulus. Because these codes can be used by the owl to\nlocalize sounds within a convex set of angles, we investigate whether they are\nexamples of convex codes, which have previously been studied for hippocampal\nplace cells. We find that periodic codes are typically not convex, but can be\ncompleted to convex codes in the presence of noise. We introduce the convex\nclosure and Hamming distance completion as ways of adding codewords to make a\ncode convex, and describe the convex closure of a periodic code. We also find\nthat the probability of the convex closure arising stochastically is greater\nfor sparser codes. Finally, we provide an algebraic method using the neural\nideal to detect if a code is periodic. We find that properties of periodic\ncodes help to explain several aspects of the behavior observed in the sound\nlocalization system of the barn owl, including common errors in localizing pure\ntones.\n", "versions": [{"version": "v1", "created": "Fri, 14 Jun 2019 03:39:22 GMT"}, {"version": "v2", "created": "Wed, 21 Jul 2021 15:23:27 GMT"}], "update_date": "2021-07-22", "authors_parsed": [["Brown", "Lindsey S.", ""], ["Curto", "Carina", ""]]}, {"id": "1906.06171", "submitter": "Tsvi Tlusty", "authors": "John M. McBride and Tsvi Tlusty", "title": "Cross-cultural data shows musical scales evolved to maximise imperfect\n  fifths", "comments": "including SI", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.SD eess.AS q-bio.NC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Musical scales are used throughout the world, but the question of how they\nevolved remains open. Some suggest that scales based on the harmonic series are\ninherently pleasant, while others propose that scales are chosen that are easy\nto communicate. However, testing these theories has been hindered by the\nsparseness of empirical evidence. Here, we assimilate data from diverse\nethnomusicological sources into a cross-cultural database of scales. We\ngenerate populations of scales based on multiple theories and assess their\nsimilarity to empirical distributions from the database. Most scales tend to\ninclude intervals which are close in size to perfect fifths (``imperfect\nfifths''), and packing arguments explain the salient features of the\ndistributions. Scales are also preferred if their intervals are compressible,\nwhich may facilitate efficient communication and memory of melodies. While\nscales appear to evolve according to various selection pressures, the simplest\nh imperfect-fifths packing model best fits the empirical data.\n", "versions": [{"version": "v1", "created": "Thu, 13 Jun 2019 04:29:19 GMT"}, {"version": "v2", "created": "Mon, 1 Jun 2020 11:13:47 GMT"}], "update_date": "2020-06-02", "authors_parsed": [["McBride", "John M.", ""], ["Tlusty", "Tsvi", ""]]}, {"id": "1906.06189", "submitter": "Torbj{\\o}rn V Ness", "authors": "Gaute T. Einevoll, Alain Destexhe, Markus Diesmann, Sonja Gr\\\"un,\n  Viktor Jirsa, Marc de Kamps, Michele Migliore, Torbj{\\o}rn V. Ness, Hans E.\n  Plesser, Felix Sch\\\"urmann", "title": "The scientific case for brain simulations", "comments": null, "journal-ref": "Einevoll et al. (2019) The Scientific Case for Brain Simulations.\n  Neuron 102:735-744", "doi": "10.1016/j.neuron.2019.03.027", "report-no": null, "categories": "q-bio.NC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A key element of the European Union's Human Brain Project (HBP) and other\nlarge-scale brain research projects is simulation of large-scale model networks\nof neurons. Here we argue why such simulations will likely be indispensable for\nbridging the scales between the neuron and system levels in the brain, and a\nset of brain simulators based on neuron models at different levels of\nbiological detail should thus be developed. To allow for systematic refinement\nof candidate network models by comparison with experiments, the simulations\nshould be multimodal in the sense that they should not only predict action\npotentials, but also electric, magnetic, and optical signals measured at the\npopulation and system levels.\n", "versions": [{"version": "v1", "created": "Fri, 14 Jun 2019 13:06:31 GMT"}], "update_date": "2019-06-17", "authors_parsed": [["Einevoll", "Gaute T.", ""], ["Destexhe", "Alain", ""], ["Diesmann", "Markus", ""], ["Gr\u00fcn", "Sonja", ""], ["Jirsa", "Viktor", ""], ["de Kamps", "Marc", ""], ["Migliore", "Michele", ""], ["Ness", "Torbj\u00f8rn V.", ""], ["Plesser", "Hans E.", ""], ["Sch\u00fcrmann", "Felix", ""]]}, {"id": "1906.06424", "submitter": "Paola Sessa", "authors": "Arianna Schiano Lomoriello, Antonio Maffei, Sabrina Brigadoi, and\n  Paola Sessa", "title": "Altering sensorimotor simulation impacts early stages of facial\n  expression processing depending on individual differences in alexithymic\n  traits", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "q-bio.NC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Simulation models of facial expressions suggest that posterior visual areas\nand brain areas underpinning sensorimotor simulations might interact to improve\nfacial expression processing. According to these models, facial mimicry may\ncontribute to the visual processing of facial expressions by influencing early\nstages. The aim of the present study was to assess whether/how early\nsensorimotor simulation influences early stages of face processing. A secondary\naim was to investigate the relationship between alexithymic traits and\nsensorimotor simulation. We monitored P1 and N170 components of the\nevent-related potentials (ERP) in participants performing a fine discrimination\ntask of facial expressions while implementing an animal discrimination task as\ncontrol condition. In half of the experiment, participants could freely use\ntheir facial mimicry whereas in the other half, they had their facial mimicry\nblocked by a gel. Our results revealed that, on average, both P1 and N170 ERP\ncomponents were not sensitive to mimicry manipulation. However, when taking\ninto account alexithymic traits, a scenario corroborating sensorimotor\nsimulation models emerged, with two dissociable temporal windows affected by\nmimicry manipulation as a function of alexithymia levels. Specifically, as a\nfunction of mimicry manipulation, individuals with lower alexithymic traits\nshowed modulations of the P1 amplitude, while individuals with higher\nalexithymic traits showed modulations of the later N170). Furthermore,\nconnectivity analysis at the scalp level suggested increased connectivity\nbetween sensorimotor and extrastriate visual regions in individuals with lower\nalexithymic traits compared to individuals with higher alexithymic traits.\nOverall, we interpreted these ERPs modulations as compensative visual\nprocessing under conditions of interference on the sensorimotor processing.\n", "versions": [{"version": "v1", "created": "Fri, 14 Jun 2019 22:28:31 GMT"}, {"version": "v2", "created": "Thu, 14 May 2020 08:39:39 GMT"}], "update_date": "2020-05-15", "authors_parsed": [["Lomoriello", "Arianna Schiano", ""], ["Maffei", "Antonio", ""], ["Brigadoi", "Sabrina", ""], ["Sessa", "Paola", ""]]}, {"id": "1906.06445", "submitter": "Thomas Bolton", "authors": "Thomas A.W. Bolton, Daniela Z\\\"oller, C\\'esar Caballero-Gaudes,\n  Valeria Kebets, Enrico Glerean, Dimitri Van De Ville", "title": "Agito ergo sum: correlates of spatiotemporal motion characteristics\n  during fMRI", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "q-bio.NC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The impact of in-scanner motion on functional magnetic resonance imaging\n(fMRI) data has a notorious reputation in the neuroimaging community.\nState-ofthe-art guidelines advise to scrub out excessively corrupted frames as\nassessed by a composite framewise displacement (FD) score, to regress out\nmodels of nuisance variables, and to include average FD as a covariate in\ngroup-level analyses.\n  Here, we studied individual motion time courses at time points typically\nretained in fMRI analyses. We observed that even in this set of putatively\nclean time points, motion exhibited a very clear spatiotemporal structure, so\nthat we could distinguish subjects into four groups of movers with varying\ncharacteristics\n  Then, we showed that this spatiotemporal motion cartography tightly relates\nto a broad array of anthropometric, behavioral and clinical factors. Convergent\nresults were obtained from two different analytical perspectives: univariate\nassessment of behavioral differences across mover subgroups unraveled defining\nmarkers, while subsequent multivariate analysis broadened the range of involved\nfactors and clarified that multiple motion/behavior modes of covariance overlap\nin the data.\n  Our results demonstrate that even the smaller episodes of motion typically\nretained in fMRI analyses carry structured, behaviorally relevant information.\nThey call for further examinations of possible biases in current\nregression-based motion correction strategies.\n", "versions": [{"version": "v1", "created": "Sat, 15 Jun 2019 01:02:00 GMT"}], "update_date": "2019-06-18", "authors_parsed": [["Bolton", "Thomas A. W.", ""], ["Z\u00f6ller", "Daniela", ""], ["Caballero-Gaudes", "C\u00e9sar", ""], ["Kebets", "Valeria", ""], ["Glerean", "Enrico", ""], ["Van De Ville", "Dimitri", ""]]}, {"id": "1906.06778", "submitter": "Samir Suweis Dr.", "authors": "Samir Suweis, Chengyi Tu, Rodrigo P. Rocha, Sandro Zampieri, Marzo\n  Zorzi and, Maurizio Corbetta", "title": "Brain Controllability: not a slam dunk yet", "comments": "3 pages, 1 figure", "journal-ref": null, "doi": null, "report-no": null, "categories": "q-bio.QM q-bio.NC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In our recent article (Tu et al., Warnings and caveats in brain\ncontrollability, arXiv:1705.08261) we provided quantitative evidence to show\nthat there are warnings and caveats in the way Gu and collaborators (Gu et al.\nControllability of structural brain networks. Nature communications 6 (2015):\n8414) define brain controllability. The comment by Pasqualetti et al.\n(Pasqualetti et al. RE: Warnings and Caveats in Brain Controllability.\nNeuroImage 297 (2019), 586-588) confirms the need to go beyond the methodology\nand approach presented in Gu et al. original work. In fact, they recognize that\nthe source of confusion is due to the fact that assessing controllability via\nnumerical analysis typically leads to ill-conditioned problems, and thus often\ngenerates results that are difficult to interpret. This is indeed the first\nwarning we discussed: our work was not meant to prove that brain networks are\nnot controllable from one node, rather we wished to highlight that the one node\ncontrollability framework and all consequent results were not properly\njustified based on the methodology presented in Gu et al. We used in our work\nthe same method of Gu et al. not because we believe it is the best methodology,\nbut because we extensively investigated it with the aim of replicating, testing\nand extending their results. And the warning and caveats we have proposed are\nthe results of this investigation.\n", "versions": [{"version": "v1", "created": "Sun, 16 Jun 2019 21:44:21 GMT"}], "update_date": "2019-06-18", "authors_parsed": [["Suweis", "Samir", ""], ["Tu", "Chengyi", ""], ["Rocha", "Rodrigo P.", ""], ["Zampieri", "Sandro", ""], ["and", "Marzo Zorzi", ""], ["Corbetta", "Maurizio", ""]]}, {"id": "1906.07067", "submitter": "Thomas Cleland", "authors": "Nabil Imam, Thomas A. Cleland", "title": "Rapid online learning and robust recall in a neuromorphic olfactory\n  circuit", "comments": "52 text pages; 8 figures. Version 3 includes a new figure and\n  additional details", "journal-ref": "Nature Machine Intelligence 2 (2020): 181-191", "doi": "10.1038/s42256-020-0159-4", "report-no": null, "categories": "cs.NE q-bio.NC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a neural algorithm for the rapid online learning and\nidentification of odorant samples under noise, based on the architecture of the\nmammalian olfactory bulb and implemented on the Intel Loihi neuromorphic\nsystem. As with biological olfaction, the spike timing-based algorithm utilizes\ndistributed, event-driven computations and rapid (one-shot) online learning.\nSpike timing-dependent plasticity rules operate iteratively over sequential\ngamma-frequency packets to construct odor representations from the activity of\nchemosensor arrays mounted in a wind tunnel. Learned odorants then are reliably\nidentified despite strong destructive interference. Noise resistance is further\nenhanced by neuromodulation and contextual priming. Lifelong learning\ncapabilities are enabled by adult neurogenesis. The algorithm is applicable to\nany signal identification problem in which high-dimensional signals are\nembedded in unknown backgrounds.\n", "versions": [{"version": "v1", "created": "Mon, 17 Jun 2019 14:51:06 GMT"}, {"version": "v2", "created": "Mon, 11 Nov 2019 14:05:11 GMT"}, {"version": "v3", "created": "Thu, 23 Jan 2020 10:32:54 GMT"}], "update_date": "2020-05-26", "authors_parsed": [["Imam", "Nabil", ""], ["Cleland", "Thomas A.", ""]]}, {"id": "1906.07211", "submitter": "Junqi Wang", "authors": "Junqi Wang, Li Xiao, Tony W. Wilson, Julia M. Stephen, Vince D.\n  Calhoun, Yu-Ping Wang", "title": "Brain Maturation Study during Adolescence Using Graph Laplacian Learning\n  Based Fourier Transform", "comments": "10 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "q-bio.NC eess.SP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Objective: Longitudinal neuroimaging studies have demonstrated that\nadolescence is the crucial developmental epoch of continued brain growth and\nchange. A large number of researchers dedicate to uncovering the mechanisms\nabout brain maturity during adolescence. Motivated by both achievement in graph\nsignal processing and recent evidence that some brain areas act as hubs\nconnecting functionally specialized systems, we proposed an approach to detect\nthese regions from spectral analysis perspective. In particular, as human brain\nundergoes substantial development throughout adolescence, we addressed the\nchallenge by evaluating the functional network difference among age groups from\nfunctional magnetic resonance imaging (fMRI) observations. Methods: We treated\nthese observations as graph signals defined on the parcellated functional brain\nregions and applied graph Laplacian learning based Fourier Transform (GLFT) to\ntransform the original graph signals into frequency domain. Eigen-analysis was\nconducted afterwards to study the behavior of the corresponding brain regions,\nwhich enables the characterization of brain maturation. Result: We first\nevaluated our method on the synthetic data and further applied the method to\nresting and task state fMRI imaging data from Philadelphia Neurodevelopmental\nCohort (PNC) dataset, comprised of normally developing adolescents from 8 to\n22. The model provided a highest accuracy of 95.69% in distinguishing different\nadolescence stages. Conclusion: We detected 13 hubs from resting state fMRI and\n16 hubs from task state fMRI that are highly related to brain maturation\nprocess. Significance: The proposed GLFT method is powerful in extracting the\nbrain connectivity patterns and identifying hub regions with a high prediction\npower\n", "versions": [{"version": "v1", "created": "Mon, 17 Jun 2019 18:23:41 GMT"}], "update_date": "2019-06-19", "authors_parsed": [["Wang", "Junqi", ""], ["Xiao", "Li", ""], ["Wilson", "Tony W.", ""], ["Stephen", "Julia M.", ""], ["Calhoun", "Vince D.", ""], ["Wang", "Yu-Ping", ""]]}, {"id": "1906.07354", "submitter": "Huilin Wei", "authors": "Huilin Wei, Amirhossein Jafarian, Peter Zeidman, Vladimir Litvak,\n  Adeel Razi, Dewen Hu, Karl J. Friston", "title": "Bayesian fusion and multimodal DCM for EEG and fMRI", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "q-bio.QM q-bio.NC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper asks whether integrating multimodal EEG and fMRI data offers a\nbetter characterisation of functional brain architectures than either modality\nalone. This evaluation rests upon a dynamic causal model that generates both\nEEG and fMRI data from the same neuronal dynamics. We introduce the use of\nBayesian fusion to provide informative (empirical) neuronal priors - derived\nfrom dynamic causal modelling (DCM) of EEG data - for subsequent DCM of fMRI\ndata. To illustrate this procedure, we generated synthetic EEG and fMRI\ntimeseries for a mismatch negativity (or auditory oddball) paradigm, using\nbiologically plausible model parameters (i.e., posterior expectations from a\nDCM of empirical, open access, EEG data). Using model inversion, we found that\nBayesian fusion provided a substantial improvement in marginal likelihood or\nmodel evidence, indicating a more efficient estimation of model parameters, in\nrelation to inverting fMRI data alone. We quantified the benefits of multimodal\nfusion with the information gain pertaining to neuronal and haemodynamic\nparameters - as measured by the Kullback-Leibler divergence between their prior\nand posterior densities. Remarkably, this analysis suggested that EEG data can\nimprove estimates of haemodynamic parameters; thereby furnishing\nproof-of-principle that Bayesian fusion of EEG and fMRI is necessary to resolve\nconditional dependencies between neuronal and haemodynamic estimators. These\nresults suggest that Bayesian fusion may offer a useful approach that exploits\nthe complementary temporal (EEG) and spatial (fMRI) precision of different data\nmodalities. We envisage the procedure could be applied to any multimodal\ndataset that can be explained by a DCM with a common neuronal parameterisation.\n", "versions": [{"version": "v1", "created": "Tue, 18 Jun 2019 02:59:47 GMT"}], "update_date": "2019-06-19", "authors_parsed": [["Wei", "Huilin", ""], ["Jafarian", "Amirhossein", ""], ["Zeidman", "Peter", ""], ["Litvak", "Vladimir", ""], ["Razi", "Adeel", ""], ["Hu", "Dewen", ""], ["Friston", "Karl J.", ""]]}, {"id": "1906.07511", "submitter": "Anne-Sophie Herard", "authors": "Florent Letronne, Geoffroy Laumet, Anne-Marie Ayral, Julien Chapuis,\n  Florie Demiautte, Mathias Laga, Michel Vandenberghe (LMN), Nicolas Malmanche,\n  Florence Leroux, Fanny Eysert, Yoann Sottejeau, Linda Chami, Amandine Flaig,\n  Charlotte Bauer (IPMC), Pierre Dourlen (JPArc - U837 Inserm), Marie Lesaffre,\n  Charlotte Delay, Ludovic Huot (CIIL), Julie Dumont (EGID), Elisabeth\n  Werkmeister, Franck Lafont (CIIL), Tiago Mendes (Inserm U1167 - RID-AGE -\n  Institut Pasteur), Franck Hansmannel (NGERE), Bart Dermaut, Benoit Deprez,\n  Anne-Sophie Herard (LMN), Marc Dhenain (UGRA / SETA), Nicolas Souedet (LMN),\n  Florence Pasquier, David Tulasne (IBLI), Claudine Berr (UMRESTTE UMR T9405),\n  Jean-Jacques Hauw, Yves Lemoine (UPVM), Philippe Amouyel, David Mann, Rebecca\n  D\\'eprez, Fr\\'ed\\'eric Checler (IPMC), David Hot (CIIL), Thierry Delzescaux\n  (MIRCEN), Kris Gevaert, Jean-Charles Lambert (DISC)", "title": "ADAM30 Downregulates APP-Linked Defects Through Cathepsin D Activation\n  in Alzheimer's Disease", "comments": null, "journal-ref": "EBioMedicine, Elsevier, 2016, 9, pp.278-292", "doi": "10.1016/j.ebiom.2016.06.002", "report-no": null, "categories": "q-bio.NC q-bio.TO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Although several ADAMs (A disintegrin-like and metalloproteases) have been\nshown to contribute to the amy-loid precursor protein (APP) metabolism, the\nfull spectrum of metalloproteases involved in this metabolism remains to be\nestablished. Transcriptomic analyses centred on metalloprotease genes unraveled\na 50% decrease in ADAM30 expression that inversely correlates with amyloid load\nin Alzheimer's disease brains. Accordingly, in vitro down-or up-regulation of\nADAM30 expression triggered an increase/decrease in A$\\beta$ peptides levels\nwhereas expression of a biologically inactive ADAM30 (ADAM30 mut) did not\naffect A$\\beta$ secretion. Proteomics/cell-based experiments showed that\nADAM30-dependent regulation of APP metabolism required both cathepsin D (CTSD)\nactivation and APP sorting to lysosomes. Accordingly, in Alzheimer-like\ntransgenic mice, neuronal ADAM30 over-expression lowered A$\\beta$42 secretion\nin neuron primary cultures, soluble A$\\beta$42 and amyloid plaque load levels\nin the brain and concomitantly enhanced CTSD activity and finally rescued long\nterm potentiation.\n", "versions": [{"version": "v1", "created": "Tue, 18 Jun 2019 11:56:49 GMT"}], "update_date": "2019-06-19", "authors_parsed": [["Letronne", "Florent", "", "LMN"], ["Laumet", "Geoffroy", "", "LMN"], ["Ayral", "Anne-Marie", "", "LMN"], ["Chapuis", "Julien", "", "LMN"], ["Demiautte", "Florie", "", "LMN"], ["Laga", "Mathias", "", "LMN"], ["Vandenberghe", "Michel", "", "LMN"], ["Malmanche", "Nicolas", "", "IPMC"], ["Leroux", "Florence", "", "IPMC"], ["Eysert", "Fanny", "", "IPMC"], ["Sottejeau", "Yoann", "", "IPMC"], ["Chami", "Linda", "", "IPMC"], ["Flaig", "Amandine", "", "IPMC"], ["Bauer", "Charlotte", "", "IPMC"], ["Dourlen", "Pierre", "", "JPArc - U837 Inserm"], ["Lesaffre", "Marie", "", "CIIL"], ["Delay", "Charlotte", "", "CIIL"], ["Huot", "Ludovic", "", "CIIL"], ["Dumont", "Julie", "", "EGID"], ["Werkmeister", "Elisabeth", "", "CIIL"], ["Lafont", "Franck", "", "CIIL"], ["Mendes", "Tiago", "", "Inserm U1167 - RID-AGE -\n  Institut Pasteur"], ["Hansmannel", "Franck", "", "NGERE"], ["Dermaut", "Bart", "", "LMN"], ["Deprez", "Benoit", "", "LMN"], ["Herard", "Anne-Sophie", "", "LMN"], ["Dhenain", "Marc", "", "UGRA / SETA"], ["Souedet", "Nicolas", "", "LMN"], ["Pasquier", "Florence", "", "IBLI"], ["Tulasne", "David", "", "IBLI"], ["Berr", "Claudine", "", "UMRESTTE UMR T9405"], ["Hauw", "Jean-Jacques", "", "UPVM"], ["Lemoine", "Yves", "", "UPVM"], ["Amouyel", "Philippe", "", "IPMC"], ["Mann", "David", "", "IPMC"], ["D\u00e9prez", "Rebecca", "", "IPMC"], ["Checler", "Fr\u00e9d\u00e9ric", "", "IPMC"], ["Hot", "David", "", "CIIL"], ["Delzescaux", "Thierry", "", "MIRCEN"], ["Gevaert", "Kris", "", "DISC"], ["Lambert", "Jean-Charles", "", "DISC"]]}, {"id": "1906.07546", "submitter": "Qiongge Li", "authors": "Qiongge Li, Gino Del Ferraro, Luca Pasquini, Kyung K. Peck, Hernan A.\n  Makse and Andrei I. Holodny", "title": "Core language brain network for fMRI-language task used in clinical\n  applications", "comments": "14 pages, 7 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "q-bio.NC physics.bio-ph physics.med-ph physics.soc-ph", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Functional magnetic resonance imaging (fMRI) is widely used in clinical\napplications to highlight brain areas involved in specific cognitive processes.\nBrain impairments, such as tumors, suppress the fMRI activation of the\nanatomical areas they invade and, thus, brain-damaged functional networks\npresent missing links/areas of activation. The identification of the missing\ncircuitry components is of crucial importance to estimate the damage extent.\nThe study of functional networks associated to clinical tasks but performed by\nhealthy individuals becomes, therefore, of paramount concern. These `healthy'\nnetworks can, indeed, be used as control networks for clinical studies. In this\nwork we investigate the functional architecture of 20 healthy individuals\nperforming a language task designed for clinical purposes. We unveil a common\narchitecture persistent across all subjects under study, which involves Broca's\narea, Wernicke's area, the Premotor area, and the pre-Supplementary motor area.\nWe study the connectivity weight of this circuitry by using the k-core\ncentrality measure and we find that three of these areas belong to the most\nrobust structure of the functional language network for the specific task under\nstudy. Our results provide useful insight for clinical applications on\nprimarily important functional connections which, thus, should be preserved\nthrough brain surgery.\n", "versions": [{"version": "v1", "created": "Wed, 12 Jun 2019 19:15:15 GMT"}], "update_date": "2019-06-20", "authors_parsed": [["Li", "Qiongge", ""], ["Del Ferraro", "Gino", ""], ["Pasquini", "Luca", ""], ["Peck", "Kyung K.", ""], ["Makse", "Hernan A.", ""], ["Holodny", "Andrei I.", ""]]}, {"id": "1906.07570", "submitter": "Tasio Gonzalez-Raya", "authors": "Tasio Gonzalez-Raya, Enrique Solano, Mikel Sanz", "title": "Quantized Three-Ion-Channel Neuron Model for Neural Action Potentials", "comments": null, "journal-ref": "Quantum 4, 224 (2020)", "doi": "10.22331/q-2020-01-20-224", "report-no": null, "categories": "q-bio.NC cond-mat.dis-nn quant-ph", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  The Hodgkin-Huxley model describes the conduction of the nervous impulse\nthrough the axon, whose membrane's electric response can be described employing\nmultiple connected electric circuits containing capacitors, voltage sources,\nand conductances. These conductances depend on previous depolarizing membrane\nvoltages, which can be identified with a memory resistive element called\nmemristor. Inspired by the recent quantization of the memristor, a simplified\nHodgkin-Huxley model including a single ion channel has been studied in the\nquantum regime. Here, we study the quantization of the complete Hodgkin-Huxley\nmodel, accounting for all three ion channels, and introduce a quantum source,\ntogether with an output waveguide as the connection to a subsequent neuron. Our\nsystem consists of two memristors and one resistor, describing potassium,\nsodium, and chloride ion channel conductances, respectively, and a capacitor to\naccount for the axon's membrane capacitance. We study the behavior of both ion\nchannel conductivities and the circuit voltage, and we compare the results with\nthose of the single channel, for a given quantum state of the source. It is\nremarkable that, in opposition to the single-channel model, we are able to\nreproduce the voltage spike in an adiabatic regime. Arguing that the circuit\nvoltage is a quantum variable, we find a purely quantum-mechanical contribution\nin the system voltage's second moment. This work represents a complete study of\nthe Hodgkin-Huxley model in the quantum regime, establishing a recipe for\nconstructing quantum neuron networks with quantum state inputs. This paves the\nway for advances in hardware-based neuromorphic quantum computing, as well as\nquantum machine learning, which might be more efficient resource-wise.\n", "versions": [{"version": "v1", "created": "Sun, 16 Jun 2019 20:10:45 GMT"}, {"version": "v2", "created": "Mon, 13 Jan 2020 15:42:30 GMT"}], "update_date": "2020-01-22", "authors_parsed": [["Gonzalez-Raya", "Tasio", ""], ["Solano", "Enrique", ""], ["Sanz", "Mikel", ""]]}, {"id": "1906.07663", "submitter": "Tamas Madarasz", "authors": "Tamas J. Madarasz", "title": "Better transfer learning with inferred successor maps", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI q-bio.NC stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Humans and animals show remarkable flexibility in adjusting their behaviour\nwhen their goals, or rewards in the environment change. While such flexibility\nis a hallmark of intelligent behaviour, these multi-task scenarios remain an\nimportant challenge for machine learning algorithms and neurobiological models\nalike. We investigated two approaches that could enable this flexibility:\nfactorized representations, which abstract away general aspects of a task from\nthose prone to change, and nonparametric, memory-based approaches, which can\nprovide a principled way of using similarity to past experiences to guide\ncurrent behaviour. In particular, we combine the successor representation (SR)\nthat factors the value of actions into expected outcomes and corresponding\nrewards with evaluating task similarity through clustering the space of reward\nfunctions. The proposed algorithm inverts a generative model over tasks, and\ndynamically samples from a flexible number of distinct SR maps while\naccumulating evidence about the current task context through amortized\ninference. It improves SR's transfer capabilities and outperforms competing\nalgorithms and baselines in settings with both known and unsignalled rewards\nchanges. Further, as a neurobiological model of spatial coding in the\nhippocampus, it explains important signatures of this representation, such as\nthe \"flickering\" behaviour of hippocampal maps, and trajectory-dependent place\ncells (so-called splitter cells) and their dynamics. We thus provide a novel\nalgorithmic approach for multi-task learning, as well as a common normative\nframework that links together these different characteristics of the brain's\nspatial representation.\n", "versions": [{"version": "v1", "created": "Tue, 18 Jun 2019 16:03:25 GMT"}, {"version": "v2", "created": "Tue, 2 Jul 2019 17:11:46 GMT"}, {"version": "v3", "created": "Thu, 31 Oct 2019 15:45:21 GMT"}, {"version": "v4", "created": "Fri, 6 Dec 2019 16:31:08 GMT"}, {"version": "v5", "created": "Fri, 10 Jan 2020 15:27:22 GMT"}], "update_date": "2020-01-13", "authors_parsed": [["Madarasz", "Tamas J.", ""]]}, {"id": "1906.07777", "submitter": "Luca Mazzucato", "authors": "Giancarlo La Camera, Alfredo Fontanini and Luca Mazzucato", "title": "Cortical computations via metastable activity", "comments": "15 pages, 3 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "q-bio.NC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Metastable brain dynamics are characterized by abrupt, jump-like modulations\nso that the neural activity in single trials appears to unfold as a sequence of\ndiscrete, quasi-stationary states. Evidence that cortical neural activity\nunfolds as a sequence of metastable states is accumulating at fast pace.\nMetastable activity occurs both in response to an external stimulus and during\nongoing, self-generated activity. These spontaneous metastable states are\nincreasingly found to subserve internal representations that are not locked to\nexternal triggers, including states of deliberations, attention and\nexpectation. Moreover, decoding stimuli or decisions via metastable states can\nbe carried out trial-by-trial. Focusing on metastability will allow us to shift\nour perspective on neural coding from traditional concepts based on\ntrial-averaging to models based on dynamic ensemble representations. Recent\ntheoretical work has started to characterize the mechanistic origin and\npotential roles of metastable representations. In this article we review recent\nfindings on metastable activity, how it may arise in biologically realistic\nmodels, and its potential role for representing internal states as well as\nrelevant task variables.\n", "versions": [{"version": "v1", "created": "Tue, 18 Jun 2019 19:25:08 GMT"}], "update_date": "2019-06-20", "authors_parsed": [["La Camera", "Giancarlo", ""], ["Fontanini", "Alfredo", ""], ["Mazzucato", "Luca", ""]]}, {"id": "1906.07837", "submitter": "Ran Xu", "authors": "Ran Xu, Manu Mathew Thomas, Alex Leow, Olusola Ajilore, Angus G.\n  Forbes", "title": "TempoCave: Visualizing Dynamic Connectome Datasets to Support Cognitive\n  Behavioral Therapy", "comments": null, "journal-ref": null, "doi": "10.1109/VISUAL.2019.8933544", "report-no": null, "categories": "cs.HC q-bio.NC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We introduce TempoCave, a novel visualization application for analyzing\ndynamic brain networks, or connectomes. TempoCave provides a range of\nfunctionality to explore metrics related to the activity patterns and modular\naffiliations of different regions in the brain. These patterns are calculated\nby processing raw data retrieved functional magnetic resonance imaging (fMRI)\nscans, which creates a network of weighted edges between each brain region,\nwhere the weight indicates how likely these regions are to activate\nsynchronously. In particular, we support the analysis needs of clinical\npsychologists, who examine these modular affiliations and weighted edges and\ntheir temporal dynamics, utilizing them to understand relationships between\nneurological disorders and brain activity, which could have a significant\nimpact on the way in which patients are diagnosed and treated. We summarize the\ncore functionality of TempoCave, which supports a range of comparative tasks,\nand runs both in a desktop mode and in an immersive mode. Furthermore, we\npresent a real-world use case that analyzes pre- and post-treatment connectome\ndatasets from 27 subjects in a clinical study investigating the use of\ncognitive behavior therapy to treat major depression disorder, indicating that\nTempoCave can provide new insight into the dynamic behavior of the human brain.\n", "versions": [{"version": "v1", "created": "Tue, 18 Jun 2019 22:49:42 GMT"}, {"version": "v2", "created": "Tue, 6 Aug 2019 22:01:04 GMT"}], "update_date": "2020-01-16", "authors_parsed": [["Xu", "Ran", ""], ["Thomas", "Manu Mathew", ""], ["Leow", "Alex", ""], ["Ajilore", "Olusola", ""], ["Forbes", "Angus G.", ""]]}, {"id": "1906.07899", "submitter": "Tomasz Rutkowski", "authors": "Tomasz M. Rutkowski and Marcin Koculak and Masato S. Abe and Mihoko\n  Otake-Matsuura", "title": "Brain correlates of task-load and dementia elucidation with tensor\n  machine learning using oddball BCI paradigm", "comments": "In ICASSP 2019 - 2019 IEEE International Conference on Acoustics,\n  Speech and Signal Processing (ICASSP), pp. 8578-8582, May 2019", "journal-ref": null, "doi": "10.1109/ICASSP.2019.8682387", "report-no": null, "categories": "q-bio.NC cs.LG eess.SP", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Dementia in the elderly has recently become the most usual cause of cognitive\ndecline. The proliferation of dementia cases in aging societies creates a\nremarkable economic as well as medical problems in many communities worldwide.\nA recently published report by The World Health Organization (WHO) estimates\nthat about 47 million people are suffering from dementia-related neurocognitive\ndeclines worldwide. The number of dementia cases is predicted by 2050 to\ntriple, which requires the creation of an AI-based technology application to\nsupport interventions with early screening for subsequent mental wellbeing\nchecking as well as preservation with digital-pharma (the so-called beyond a\npill) therapeutical approaches. We present an attempt and exploratory results\nof brain signal (EEG) classification to establish digital biomarkers for\ndementia stage elucidation. We discuss a comparison of various machine learning\napproaches for automatic event-related potentials (ERPs) classification of a\nhigh and low task-load sound stimulus recognition. These ERPs are similar to\nthose in dementia. The proposed winning method using tensor-based machine\nlearning in a deep fully connected neural network setting is a step forward to\ndevelop AI-based approaches for a subsequent application for subjective- and\nmild-cognitive impairment (SCI and MCI) diagnostics.\n", "versions": [{"version": "v1", "created": "Wed, 19 Jun 2019 03:43:39 GMT"}], "update_date": "2019-06-20", "authors_parsed": [["Rutkowski", "Tomasz M.", ""], ["Koculak", "Marcin", ""], ["Abe", "Masato S.", ""], ["Otake-Matsuura", "Mihoko", ""]]}, {"id": "1906.08246", "submitter": "Jesus Malo", "authors": "J. Malo, J.J. Esteve-Taboada, M. Bertalm\\'io", "title": "Divisive Normalization from Wilson-Cowan Dynamics", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "q-bio.NC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Divisive Normalization and the Wilson-Cowan equations are influential models\nof neural interaction and saturation [Carandini and Heeger Nat.Rev.Neurosci.\n2012; Wilson and Cowan Kybernetik 1973]. However, they have not been\nanalytically related yet. In this work we show that Divisive Normalization can\nbe obtained from the Wilson-Cowan model. Specifically, assuming that Divisive\nNormalization is the steady state solution of the Wilson-Cowan differential\nequation, we find that the kernel that controls neural interactions in Divisive\nNormalization depends on the Wilson-Cowan kernel but also has a\nsignal-dependent contribution. A standard stability analysis of a Wilson-Cowan\nmodel with the parameters obtained from our relation shows that the Divisive\nNormalization solution is a stable node. This stability demonstrates the\nconsistency of our steady state assumption, and is in line with the\nstraightforward use of Divisive Normalization with time-varying stimuli.\n  The proposed theory provides a physiological foundation (a relation to a\ndynamical network with fixed wiring among neurons) for the functional\nsuggestions that have been done on the need of signal-dependent Divisive\nNormalization [e.g. in Coen-Cagli et al., PLoS Comp.Biol. 2012]. Moreover, this\ntheory explains the modifications that had to be introduced ad-hoc in Gaussian\nkernels of Divisive Normalization in [Martinez et al. Front. Neurosci. 2019] to\nreproduce contrast responses. The proposed relation implies that the\nWilson-Cowan dynamics also reproduces visual masking and subjective image\ndistortion metrics, which up to now had been mainly explained via Divisive\nNormalization. Finally, this relation allows to apply to Divisive Normalization\nthe methods which up to now had been developed for dynamical systems such as\nWilson-Cowan networks.\n", "versions": [{"version": "v1", "created": "Wed, 19 Jun 2019 17:45:41 GMT"}, {"version": "v2", "created": "Mon, 5 Aug 2019 10:19:42 GMT"}], "update_date": "2019-08-06", "authors_parsed": [["Malo", "J.", ""], ["Esteve-Taboada", "J. J.", ""], ["Bertalm\u00edo", "M.", ""]]}, {"id": "1906.08365", "submitter": "Dushyant Sahoo", "authors": "Dushyant Sahoo, Theodore D. Satterthwaite and Christos Davatzikos", "title": "Extraction of hierarchical functional connectivity components in human\n  brain using resting-state fMRI", "comments": null, "journal-ref": null, "doi": "10.1109/TMI.2020.3042873", "report-no": null, "categories": "q-bio.NC cs.LG eess.IV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The study of hierarchy in networks of the human brain has been of significant\ninterest among the researchers as numerous studies have pointed out towards a\nfunctional hierarchical organization of the human brain. This paper provides a\nnovel method for the extraction of hierarchical connectivity components in the\nhuman brain using resting-state fMRI. The method builds upon prior work of\nSparse Connectivity Patterns (SCPs) by introducing a hierarchy of sparse\noverlapping patterns. The components are estimated by deep factorization of\ncorrelation matrices generated from fMRI. The goal of the paper is to extract\ninterpretable hierarchical patterns using correlation matrices where a low rank\ndecomposition is formed by a linear combination of a high rank decomposition.\nWe formulate the decomposition as a non-convex optimization problem and solve\nit using gradient descent algorithms with adaptive step size. We also provide a\nmethod for the warm start of the gradient descent using singular value\ndecomposition. We demonstrate the effectiveness of the developed method on two\ndifferent real-world datasets by showing that multi-scale hierarchical SCPs are\nreproducible between sub-samples and are more reproducible as compared to\nsingle scale patterns. We also compare our method with existing hierarchical\ncommunity detection approaches. Our method also provides novel insight into the\nfunctional organization of the human brain.\n", "versions": [{"version": "v1", "created": "Wed, 19 Jun 2019 21:29:40 GMT"}, {"version": "v2", "created": "Thu, 27 Jun 2019 18:27:57 GMT"}, {"version": "v3", "created": "Mon, 7 Dec 2020 17:24:32 GMT"}], "update_date": "2021-03-02", "authors_parsed": [["Sahoo", "Dushyant", ""], ["Satterthwaite", "Theodore D.", ""], ["Davatzikos", "Christos", ""]]}, {"id": "1906.08451", "submitter": "Behtash Babadi", "authors": "Proloy Das and Behtash Babadi", "title": "Multitaper Spectral Analysis of Neuronal Spiking Activity Driven by\n  Latent Stationary Processes", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.SP cs.IT math.IT q-bio.NC stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Investigating the spectral properties of the neural covariates that underlie\nspiking activity is an important problem in systems neuroscience, as it allows\nto study the role of brain rhythms in cognitive functions. While the spectral\nestimation of continuous time-series is a well-established domain, computing\nthe spectral representation of these neural covariates from spiking data sets\nforth various challenges due to the intrinsic non-linearities involved. In this\npaper, we address this problem by proposing a variant of the multitaper method\nspecifically tailored for point process data. To this end, we construct\nauxiliary spiking statistics from which the eigen-spectra of the underlying\nlatent process can be directly inferred using maximum likelihood estimation,\nand thereby the multitaper estimate can be efficiently computed. Comparison of\nour proposed technique to existing methods using simulated data reveals\nsignificant gains in terms of the bias-variance trade-off.\n", "versions": [{"version": "v1", "created": "Thu, 20 Jun 2019 05:35:41 GMT"}], "update_date": "2019-06-21", "authors_parsed": [["Das", "Proloy", ""], ["Babadi", "Behtash", ""]]}, {"id": "1906.08804", "submitter": "Alianna Maren", "authors": "Alianna J. Maren", "title": "Derivation of the Variational Bayes Equations", "comments": "62 pages, 7 figures (one new); typos corrected, minor corrections to\n  equations, explanatory material added/revised, references added", "journal-ref": null, "doi": null, "report-no": "Themasis Technical Report TR-2019-01v4 (ajm)", "categories": "cs.NE q-bio.NC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The derivation of key equations for the variational Bayes approach is\nwell-known in certain circles. However, translating the fundamental derivations\n(e.g., as found in Beal (2003)) to the notation of Friston (2013, 2015) is\nsomewhat delicate. Further, the notion of using variational Bayes in the\ncontext of a system with Markov blankets requires special attention. This\nTechnical Report presents the derivation in detail. It further illustrates how\nthe variational Bayes method provides a framework for a new computational\nengine, incorporating the 2-D cluster variation method (CVM), which provides a\nnecessary free energy equation that can be minimized across both the external\nand representational systems' states, respectively.\n", "versions": [{"version": "v1", "created": "Thu, 20 Jun 2019 18:43:47 GMT"}, {"version": "v2", "created": "Mon, 24 Jun 2019 23:23:45 GMT"}, {"version": "v3", "created": "Wed, 26 Jun 2019 01:41:22 GMT"}, {"version": "v4", "created": "Tue, 30 Jul 2019 02:38:14 GMT"}], "update_date": "2019-07-31", "authors_parsed": [["Maren", "Alianna J.", ""]]}, {"id": "1906.09264", "submitter": "Baihan Lin", "authors": "Baihan Lin, Marieke Mur, Tim Kietzmann, Nikolaus Kriegeskorte", "title": "Visualizing Representational Dynamics with Multidimensional Scaling\n  Alignment", "comments": "CCN 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "q-bio.NC cs.AI cs.LG cs.NE stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Representational similarity analysis (RSA) has been shown to be an effective\nframework to characterize brain-activity profiles and deep neural network\nactivations as representational geometry by computing the pairwise distances of\nthe response patterns as a representational dissimilarity matrix (RDM).\nHowever, how to properly analyze and visualize the representational geometry as\ndynamics over the time course from stimulus onset to offset is not well\nunderstood. In this work, we formulated the pipeline to understand\nrepresentational dynamics with RDM movies and Procrustes-aligned\nMultidimensional Scaling (pMDS), and applied it to neural recording of monkey\nIT cortex. Our results suggest that the the multidimensional scaling alignment\ncan genuinely capture the dynamics of the category-specific representation\nspaces with multiple visualization possibilities, and that object\ncategorization may be hierarchical, multi-staged, and oscillatory (or\nrecurrent).\n", "versions": [{"version": "v1", "created": "Fri, 21 Jun 2019 03:46:18 GMT"}, {"version": "v2", "created": "Sun, 28 Jul 2019 12:07:06 GMT"}], "update_date": "2019-07-30", "authors_parsed": [["Lin", "Baihan", ""], ["Mur", "Marieke", ""], ["Kietzmann", "Tim", ""], ["Kriegeskorte", "Nikolaus", ""]]}, {"id": "1906.09290", "submitter": "Danielle Bassett", "authors": "Urs Braun, Anais Harneit, Giulio Pergola, Tommaso Menara, Axel\n  Schaefer, Richard F. Betzel, Zhenxiang Zang, Janina I. Schweiger, Kristina\n  Schwarz, Junfang Chen, Giuseppe Blasi, Alessandro Bertolino, Daniel\n  Durstewitz, Fabio Pasqualetti, Emanuel Schwarz, Andreas Meyer-Lindenberg,\n  Danielle S. Bassett, Heike Tost", "title": "Brain state stability during working memory is explained by network\n  control theory, modulated by dopamine D1/D2 receptor function, and diminished\n  in schizophrenia", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "q-bio.NC physics.bio-ph", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Dynamical brain state transitions are critical for flexible working memory\nbut the network mechanisms are incompletely understood. Here, we show that\nworking memory entails brainwide switching between activity states. The\nstability of states relates to dopamine D1 receptor gene expression while state\ntransitions are influenced by D2 receptor expression and pharmacological\nmodulation. Schizophrenia patients show altered network control properties,\nincluding a more diverse energy landscape and decreased stability of working\nmemory representations.\n", "versions": [{"version": "v1", "created": "Fri, 21 Jun 2019 18:46:09 GMT"}], "update_date": "2019-06-25", "authors_parsed": [["Braun", "Urs", ""], ["Harneit", "Anais", ""], ["Pergola", "Giulio", ""], ["Menara", "Tommaso", ""], ["Schaefer", "Axel", ""], ["Betzel", "Richard F.", ""], ["Zang", "Zhenxiang", ""], ["Schweiger", "Janina I.", ""], ["Schwarz", "Kristina", ""], ["Chen", "Junfang", ""], ["Blasi", "Giuseppe", ""], ["Bertolino", "Alessandro", ""], ["Durstewitz", "Daniel", ""], ["Pasqualetti", "Fabio", ""], ["Schwarz", "Emanuel", ""], ["Meyer-Lindenberg", "Andreas", ""], ["Bassett", "Danielle S.", ""], ["Tost", "Heike", ""]]}, {"id": "1906.09480", "submitter": "Eszter V\\'ertes", "authors": "Eszter Vertes and Maneesh Sahani", "title": "A neurally plausible model learns successor representations in partially\n  observable environments", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG cs.NE q-bio.NC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Animals need to devise strategies to maximize returns while interacting with\ntheir environment based on incoming noisy sensory observations. Task-relevant\nstates, such as the agent's location within an environment or the presence of a\npredator, are often not directly observable but must be inferred using\navailable sensory information. Successor representations (SR) have been\nproposed as a middle-ground between model-based and model-free reinforcement\nlearning strategies, allowing for fast value computation and rapid adaptation\nto changes in the reward function or goal locations. Indeed, recent studies\nsuggest that features of neural responses are consistent with the SR framework.\nHowever, it is not clear how such representations might be learned and computed\nin partially observed, noisy environments. Here, we introduce a neurally\nplausible model using distributional successor features, which builds on the\ndistributed distributional code for the representation and computation of\nuncertainty, and which allows for efficient value function computation in\npartially observed environments via the successor representation. We show that\ndistributional successor features can support reinforcement learning in noisy\nenvironments in which direct learning of successful policies is infeasible.\n", "versions": [{"version": "v1", "created": "Sat, 22 Jun 2019 18:05:07 GMT"}], "update_date": "2019-06-25", "authors_parsed": [["Vertes", "Eszter", ""], ["Sahani", "Maneesh", ""]]}, {"id": "1906.09528", "submitter": "Sergey Shuvaev", "authors": "Sergey A. Shuvaev, Ngoc B. Tran, Marcus Stephenson-Jones, Bo Li, and\n  Alexei A. Koulakov", "title": "Neural networks with motivation", "comments": "Added the Methods section", "journal-ref": null, "doi": null, "report-no": null, "categories": "q-bio.NC cs.LG", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  How can animals behave effectively in conditions involving different\nmotivational contexts? Here, we propose how reinforcement learning neural\nnetworks can learn optimal behavior for dynamically changing motivational\nsalience vectors. First, we show that Q-learning neural networks with\nmotivation can navigate in environment with dynamic rewards. Second, we show\nthat such networks can learn complex behaviors simultaneously directed towards\nseveral goals distributed in an environment. Finally, we show that in Pavlovian\nconditioning task, the responses of the neurons in our model resemble the\nfiring patterns of neurons in the ventral pallidum (VP), a basal ganglia\nstructure involved in motivated behaviors. We show that, similarly to real\nneurons, recurrent networks with motivation are composed of two\noppositely-tuned classes of neurons, responding to positive and negative\nrewards. Our model generates predictions for the VP connectivity. We conclude\nthat networks with motivation can rapidly adapt their behavior to varying\nconditions without changes in synaptic strength when expected reward is\nmodulated by motivation. Such networks may also provide a mechanism for how\nhierarchical reinforcement learning is implemented in the brain.\n", "versions": [{"version": "v1", "created": "Sun, 23 Jun 2019 01:44:32 GMT"}, {"version": "v2", "created": "Tue, 19 Nov 2019 03:27:52 GMT"}], "update_date": "2019-11-20", "authors_parsed": [["Shuvaev", "Sergey A.", ""], ["Tran", "Ngoc B.", ""], ["Stephenson-Jones", "Marcus", ""], ["Li", "Bo", ""], ["Koulakov", "Alexei A.", ""]]}, {"id": "1906.09622", "submitter": "Anne Churchland", "authors": "Simon Musall, Anne Urai, David Sussillo, Anne Churchland", "title": "Harnessing behavioral diversity to understand circuits for cognition", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "q-bio.NC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  With the increasing acquisition of large-scale neural recordings comes the\nchallenge of inferring the computations they perform and understanding how\nthese give rise to behavior. Here, we review emerging conceptual and\ntechnological advances that begin to address this challenge, garnering insights\nfrom both biological and artificial neural networks. We argue that neural data\nshould be recorded during rich behavioral tasks, to model cognitive processes\nand estimate latent behavioral variables. Careful quantification of animal\nmovements can also provide a more complete picture of how movements shape\nneural dynamics and reflect changes in brain state, such as arousal or stress.\nArtificial neural networks (ANNs) could serve as an important tool to connect\nneural dynamics and rich behavioral data. ANNs have already begun to reveal how\nparticular behaviors can be optimally solved, generating hypotheses about how\nobserved neural activity might drive behavior and explaining diversity in\nbehavioral strategies.\n", "versions": [{"version": "v1", "created": "Sun, 23 Jun 2019 18:14:37 GMT"}], "update_date": "2019-06-25", "authors_parsed": [["Musall", "Simon", ""], ["Urai", "Anne", ""], ["Sussillo", "David", ""], ["Churchland", "Anne", ""]]}, {"id": "1906.09835", "submitter": "Yulia Sandamirskaya", "authors": "Hajar Asgari, BabakMazloom-Nezhad Maybodi, Raphaela Kreiser, and Yulia\n  Sandamirskaya", "title": "Digital Multiplier-less Event-Driven Spiking Neural Network Architecture\n  for Learning a Context-Dependent Task", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "q-bio.NC cs.SY eess.SY", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Neuromorphic engineers aim to develop event-based spiking neural networks\n(SNNs) in hardware. These SNNs closer resemble dynamics of biological neurons\nthan todays' artificial neural networks and achieve higher efficiency thanks to\nthe event-based, asynchronous nature of processing. Learning in SNNs is more\nchallenging, however. Since conventional supervised learning methods cannot be\nported on SNNs due to the non-differentiable event-based nature of their\nactivation, learning in SNNs is currently an active research topic.\nReinforcement learning (RL) is particularly promising method for neuromorphic\nimplementation, especially in the field of autonomous agents' control, and is\nin focus of this work. In particular, in this paper we propose a new digital\nmultiplier-less hardware implementation of an SNN. We show how this network can\nlearn stimulus-response associations in a context-dependent task through a RL\nmechanism. The task is inspired by biological experiments used to study RL in\nanimals. The architecture is described using the standard digital design flow\nand uses power- and space-efficient cores. We implement the behavioral\nexperiments using a robot, to show that learning in hardware also works in a\nclosed sensorimotor loop.\n", "versions": [{"version": "v1", "created": "Mon, 24 Jun 2019 10:17:16 GMT"}], "update_date": "2019-06-25", "authors_parsed": [["Asgari", "Hajar", ""], ["Maybodi", "BabakMazloom-Nezhad", ""], ["Kreiser", "Raphaela", ""], ["Sandamirskaya", "Yulia", ""]]}, {"id": "1906.09908", "submitter": "Zhen Zhou", "authors": "Zhen Zhou, Xiaobo Chen, Yu Zhang, Lishan Qiao, Renping Yu, Gang Pan,\n  Han Zhang, Dinggang Shen", "title": "Brain Network Construction and Classification Toolbox (BrainNetClass)", "comments": null, "journal-ref": null, "doi": "10.1002/hbm.24979", "report-no": null, "categories": "q-bio.NC cs.CV cs.LG", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Brain functional network has become an increasingly used approach in\nunderstanding brain functions and diseases. Many network construction methods\nhave been developed, whereas the majority of the studies still used static\npairwise Pearson's correlation-based functional connectivity. The goal of this\nwork is to introduce a toolbox namely \"Brain Network Construction and\nClassification\" (BrainNetClass) to the field to promote more advanced brain\nnetwork construction methods. It comprises various brain network construction\nmethods, including some state-of-the-art methods that were recently developed\nto capture more complex interactions among brain regions along with connectome\nfeature extraction, reduction, parameter optimization towards network-based\nindividualized classification. BrainNetClass is a MATLAB-based, open-source,\ncross-platform toolbox with graphical user-friendly interfaces for cognitive\nand clinical neuroscientists to perform rigorous computer-aided diagnosis with\ninterpretable result presentations even though they do not possess neuroimage\ncomputing and machine learning knowledge. We demonstrate the implementations of\nthis toolbox on real resting-state functional MRI datasets. BrainNetClass\n(v1.0) can be downloaded from https://github.com/zzstefan/BrainNetClass.\n", "versions": [{"version": "v1", "created": "Mon, 17 Jun 2019 18:19:18 GMT"}], "update_date": "2020-03-13", "authors_parsed": [["Zhou", "Zhen", ""], ["Chen", "Xiaobo", ""], ["Zhang", "Yu", ""], ["Qiao", "Lishan", ""], ["Yu", "Renping", ""], ["Pan", "Gang", ""], ["Zhang", "Han", ""], ["Shen", "Dinggang", ""]]}, {"id": "1906.09996", "submitter": "Unai Lopez-Novoa", "authors": "Unai Lopez-Novoa, Cyril Charron, John Evans, Leandro Beltrachini", "title": "The BIDS Toolbox: A web service to manage brain imaging datasets", "comments": "Paper for the Workshop on Data Preprocessing for Big Biomedical Data\n  2019, held in conjunction with the IEEE Smart World Congress 2019, Leicester,\n  UK", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DL q-bio.NC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Data sharing is a key factor for ensuring reproducibility and transparency of\nscientific experiments, and neuroimaging is no exception. The vast\nheterogeneity of data formats and imaging modalities utilised in the field\nmakes it a very challenging problem. In this context, the Brain Imaging Data\nStructure (BIDS) appears as a solution for organising and describing\nneuroimaging datasets. Since its publication in 2015, BIDS has gained\nwidespread attention in the field, as it provides a common way to arrange and\nshare multimodal brain images. Although the evident benefits it presents, BIDS\nhas not been widely adopted in the field of MRI yet and we believe that this is\ndue to the lack of a go-to tool to create and managed BIDS datasets. Motivated\nby this, we present the BIDS Toolbox, a web service to manage brain imaging\ndatasets in BIDS format. Different from other tools, the BIDS Toolbox allows\nthe creation and modification of BIDS-compliant datasets based on MRI data. It\nprovides both a web interface and REST endpoints for its use. In this paper we\ndescribe its design and early prototype, and provide a link to the public\nsource code repository.\n", "versions": [{"version": "v1", "created": "Mon, 24 Jun 2019 14:34:38 GMT"}], "update_date": "2019-06-25", "authors_parsed": [["Lopez-Novoa", "Unai", ""], ["Charron", "Cyril", ""], ["Evans", "John", ""], ["Beltrachini", "Leandro", ""]]}, {"id": "1906.10015", "submitter": "Pablo Lanillos", "authors": "Pablo Lanillos, Daniel Oliva, Anja Philippsen, Yuichi Yamashita, Yukie\n  Nagai, Gordon Cheng", "title": "A Review on Neural Network Models of Schizophrenia and Autism Spectrum\n  Disorder", "comments": "Preprint submitted to Neural Networks. Research not referenced in the\n  manuscript within the field of NN models of SZ and ASD are encouraged to\n  contact the corresponding authors", "journal-ref": "Neural Networks 122 (2020) 338-363", "doi": "10.1016/j.neunet.2019.10.014", "report-no": null, "categories": "q-bio.NC cs.AI cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This survey presents the most relevant neural network models of autism\nspectrum disorder and schizophrenia, from the first connectionist models to\nrecent deep network architectures. We analyzed and compared the most\nrepresentative symptoms with its neural model counterpart, detailing the\nalteration introduced in the network that generates each of the symptoms, and\nidentifying their strengths and weaknesses. We additionally cross-compared\nBayesian and free-energy approaches, as they are widely applied to modeling\npsychiatric disorders and share basic mechanisms with neural networks. Models\nof schizophrenia mainly focused on hallucinations and delusional thoughts using\nneural dysconnections or inhibitory imbalance as the predominating alteration.\nModels of autism rather focused on perceptual difficulties, mainly excessive\nattention to environment details, implemented as excessive inhibitory\nconnections or increased sensory precision. We found an excessive tight view of\nthe psychopathologies around one specific and simplified effect, usually\nconstrained to the technical idiosyncrasy of the used network architecture.\nRecent theories and evidence on sensorimotor integration and body perception\ncombined with modern neural network architectures could offer a broader and\nnovel spectrum to approach these psychopathologies. This review emphasizes the\npower of artificial neural networks for modeling some symptoms of neurological\ndisorders but also calls for further developing these techniques in the field\nof computational psychiatry.\n", "versions": [{"version": "v1", "created": "Mon, 24 Jun 2019 15:10:44 GMT"}, {"version": "v2", "created": "Wed, 23 Oct 2019 09:25:59 GMT"}], "update_date": "2019-11-22", "authors_parsed": [["Lanillos", "Pablo", ""], ["Oliva", "Daniel", ""], ["Philippsen", "Anja", ""], ["Yamashita", "Yuichi", ""], ["Nagai", "Yukie", ""], ["Cheng", "Gordon", ""]]}, {"id": "1906.10184", "submitter": "Karl Friston", "authors": "Karl Friston", "title": "A free energy principle for a particular physics", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "q-bio.NC", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  This monograph attempts a theory of every 'thing' that can be distinguished\nfrom other things in a statistical sense. The ensuing statistical\nindependencies, mediated by Markov blankets, speak to a recursive composition\nof ensembles (of things) at increasingly higher spatiotemporal scales. This\ndecomposition provides a description of small things; e.g., quantum mechanics -\nvia the Schrodinger equation, ensembles of small things - via statistical\nmechanics and related fluctuation theorems, through to big things - via\nclassical mechanics. These descriptions are complemented with a Bayesian\nmechanics for autonomous or active things. Although this work provides a\nformulation of every thing, its main contribution is to examine the\nimplications of Markov blankets for self-organisation to nonequilibrium\nsteady-state. In brief, we recover an information geometry and accompanying\nfree energy principle that allows one to interpret the internal states of\nsomething as representing or making inferences about its external states. The\nensuing Bayesian mechanics is compatible with quantum, statistical and\nclassical mechanics and may offer a formal description of lifelike particles.\n", "versions": [{"version": "v1", "created": "Mon, 24 Jun 2019 19:18:37 GMT"}], "update_date": "2019-06-26", "authors_parsed": [["Friston", "Karl", ""]]}, {"id": "1906.10458", "submitter": "Ran Levi", "authors": "Daniel Luetgehetmann, Dejan Govc, Jason Smith, Ran Levi", "title": "Computing persistent homology of directed flag complexes", "comments": "14 Pages, 12 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.AT q-bio.NC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a new computing package Flagser, designed to construct the\ndirected flag complex of a finite directed graph, and compute persistent\nhomology for flexibly defined filtrations on the graph and the resulting\ncomplex. The persistent homology computation part of Flagser is based on the\nprogram Ripser [Bau18a], but is optimized specifically for large computations.\nThe construction of the directed flag complex is done in a way that allows easy\nparallelization by arbitrarily many cores. Flagser also has the option of\nworking with undirected graphs. For homology computations Flagser has an\nApproximate option, which shortens compute time with remarkable accuracy. We\ndemonstrate the power of Flagser by applying it to the construction of the\ndirected flag complex of digital reconstructions of brain microcircuitry by the\nBlue Brain Project and several other examples. In some instances we perform\ncomputation of homology. For a more complete performance analysis, we also\napply Flagser to some other data collections. In all cases the hardware used in\nthe computation, the use of memory and the compute time are recorded.\n", "versions": [{"version": "v1", "created": "Tue, 25 Jun 2019 11:21:39 GMT"}], "update_date": "2019-06-26", "authors_parsed": [["Luetgehetmann", "Daniel", ""], ["Govc", "Dejan", ""], ["Smith", "Jason", ""], ["Levi", "Ran", ""]]}, {"id": "1906.10538", "submitter": "Jared Vasil", "authors": "Jared Vasil, Paul B. Badcock, Axel Constant, Karl Friston, Maxwell J.\n  D. Ramstead", "title": "A World unto Itself: Human Communication as Active Inference", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "q-bio.NC", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Work in developmental psychology suggests that humans are predisposed to\nalign their mental states with other individuals. This manifests principally in\ncooperative communication, that is, intentional communication geared towards\naligning mental states. This viewpoint has received ample empirical support.\nHowever, this view lacks a formal grounding, and provides no precise\nneuroscientific hypotheses. To remedy this, we suggest an active inference\napproach to cooperative communication. We suggest that humans appear to possess\nan evolved adaptive prior belief that their mental states are aligned with\nthose of conspecifics. Cooperative communication emerges as the principal means\nto gather evidence for this belief. Our approach has implications for the study\nof the usage, ontogeny, and cultural evolution of human communication.\n", "versions": [{"version": "v1", "created": "Tue, 25 Jun 2019 13:58:36 GMT"}, {"version": "v2", "created": "Sun, 23 Feb 2020 01:09:33 GMT"}], "update_date": "2020-02-25", "authors_parsed": [["Vasil", "Jared", ""], ["Badcock", "Paul B.", ""], ["Constant", "Axel", ""], ["Friston", "Karl", ""], ["Ramstead", "Maxwell J. D.", ""]]}, {"id": "1906.10592", "submitter": "Pablo Lanillos", "authors": "Michael Deistler, Yagmur Yener, Florian Bergner, Pablo Lanillos, and\n  Gordon Cheng", "title": "Tactile Hallucinations on Artificial Skin Induced by Homeostasis in a\n  Deep Boltzmann Machine", "comments": "Submitted to 2019 IEEE International Conference on Cyborg and Bionic\n  Systems", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NE cs.LG q-bio.NC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Perceptual hallucinations are present in neurological and psychiatric\ndisorders and amputees. While the hallucinations can be drug-induced, it has\nbeen described that they can even be provoked in healthy subjects.\nUnderstanding their manifestation could thus unveil how the brain processes\nsensory information and might evidence the generative nature of perception. In\nthis work, we investigate the generation of tactile hallucinations on\nbiologically inspired, artificial skin. To model tactile hallucinations, we\napply homeostasis, a change in the excitability of neurons during sensory\ndeprivation, in a Deep Boltzmann Machine (DBM). We find that homeostasis\nprompts hallucinations of previously learned patterns on the artificial skin in\nthe absence of sensory input. Moreover, we show that homeostasis is capable of\ninducing the formation of meaningful latent representations in a DBM and that\nit significantly increases the quality of the reconstruction of these latent\nstates. Through this, our work provides a possible explanation for the nature\nof tactile hallucinations and highlights homeostatic processes as a potential\nunderlying mechanism.\n", "versions": [{"version": "v1", "created": "Tue, 25 Jun 2019 15:14:49 GMT"}, {"version": "v2", "created": "Wed, 26 Jun 2019 11:20:03 GMT"}], "update_date": "2019-06-27", "authors_parsed": [["Deistler", "Michael", ""], ["Yener", "Yagmur", ""], ["Bergner", "Florian", ""], ["Lanillos", "Pablo", ""], ["Cheng", "Gordon", ""]]}, {"id": "1906.11286", "submitter": "Baihan Lin", "authors": "Baihan Lin, Guillermo Cecchi, Djallel Bouneffouf, Jenna Reinen, Irina\n  Rish", "title": "A Story of Two Streams: Reinforcement Learning Models from Human\n  Behavior and Neuropsychiatry", "comments": "Published in AAMAS 2020 as a full paper. This article supersedes our\n  work arXiv:1706.02897 into RL setting and extends extensively into RL games,\n  cognitive modeling, and gambling tasks in lifelong learning setting", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI cs.MA q-bio.NC stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Drawing an inspiration from behavioral studies of human decision making, we\npropose here a more general and flexible parametric framework for reinforcement\nlearning that extends standard Q-learning to a two-stream model for processing\npositive and negative rewards, and allows to incorporate a wide range of\nreward-processing biases -- an important component of human decision making\nwhich can help us better understand a wide spectrum of multi-agent interactions\nin complex real-world socioeconomic systems, as well as various\nneuropsychiatric conditions associated with disruptions in normal reward\nprocessing. From the computational perspective, we observe that the proposed\nSplit-QL model and its clinically inspired variants consistently outperform\nstandard Q-Learning and SARSA methods, as well as recently proposed Double\nQ-Learning approaches, on simulated tasks with particular reward distributions,\na real-world dataset capturing human decision-making in gambling tasks, and the\nPac-Man game in a lifelong learning setting across different reward\nstationarities.\n", "versions": [{"version": "v1", "created": "Fri, 21 Jun 2019 03:31:37 GMT"}, {"version": "v2", "created": "Fri, 28 Jun 2019 06:14:11 GMT"}, {"version": "v3", "created": "Mon, 18 Nov 2019 04:34:06 GMT"}, {"version": "v4", "created": "Mon, 24 Feb 2020 19:21:21 GMT"}, {"version": "v5", "created": "Wed, 26 Feb 2020 18:56:09 GMT"}, {"version": "v6", "created": "Tue, 10 Mar 2020 20:52:24 GMT"}, {"version": "v7", "created": "Tue, 14 Apr 2020 17:26:49 GMT"}], "update_date": "2020-04-15", "authors_parsed": [["Lin", "Baihan", ""], ["Cecchi", "Guillermo", ""], ["Bouneffouf", "Djallel", ""], ["Reinen", "Jenna", ""], ["Rish", "Irina", ""]]}, {"id": "1906.11398", "submitter": "James Hope Mr", "authors": "James Hope, Zaid Aqrawe, Marshall Lim, Frederique Vanholsbeeck, Andrew\n  McDaid", "title": "Increasing signal amplitude in electrical impedance tomography of neural\n  activity using a parallel resistor inductor capacitor (RLC) circuit", "comments": "18 pages, 14 figures, journal submission", "journal-ref": null, "doi": "10.1088/1741-2552/ab462b", "report-no": null, "categories": "q-bio.NC physics.ins-det", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Objective: To increase the impedance signal amplitude produced during neural\nactivity using a novel approach of implementing a parallel resistor inductor\ncapacitor (RLC) circuit across the current source used in electrical impedance\ntomography (EIT) of peripheral nerve. Approach: Experiments were performed in\nvitro on sciatic nerve of Sprague-Dawley rats. Design of the RLC circuit was\nperformed in electrical circuit modelling software, aided by in vitro impedance\nmeasurements on nerve and nerve cuff in the range 5 Hz to 50 kHz. Main results:\nThe frequency range 17 +/- 1 kHz was selected for the RLC experiment. The RLC\nexperiment was performed on four subjects using an RLC circuit designed to\nproduce a resonant frequency of 17 kHz with a bandwidth of 3.6 kHz, and\ncontaining a 22 mH inductive element and a 3.45 nF capacitive element. With the\nRLC circuit connected, relative increases in the impedance signal (+/- 3sig\nnoise) of 44 % (+/-15 %), 33 % (+/-30 %), 37 % (+/-8.6 %), and 16 % (+/-19 %)\nwere produced. Significance: The increase in impedance signal amplitude at high\nfrequencies, generated by the novel implementation of a parallel RLC circuit\nacross the drive current, improves spatial resolution by increasing the number\nof parallel drive currents which can be implemented in a frequency division\nmultiplexed (FDM) EIT system, and aids the long term goal of a real-time FDM\nEIT system by reducing the need for ensemble averaging.\n", "versions": [{"version": "v1", "created": "Thu, 27 Jun 2019 00:19:25 GMT"}], "update_date": "2019-12-06", "authors_parsed": [["Hope", "James", ""], ["Aqrawe", "Zaid", ""], ["Lim", "Marshall", ""], ["Vanholsbeeck", "Frederique", ""], ["McDaid", "Andrew", ""]]}, {"id": "1906.11573", "submitter": "Michael Forrester", "authors": "Michael Forrester, Stephen Coombes, Jonathan J. Crofts, Stamatios N.\n  Sotiropoulos, and Reuben D. O'Dea", "title": "The role of node dynamics in shaping emergent functional connectivity\n  patterns in the brain", "comments": "23 pages, 9 figures", "journal-ref": "Network Neuroscience (2020), 4(2) 467-483", "doi": "10.1162/netn_a_00130", "report-no": null, "categories": "q-bio.NC math.DS nlin.AO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The contribution of structural connectivity to functional brain states\nremains poorly understood. We present a mathematical and computational study\nsuited to assess the structure--function issue, treating a system of\nJansen--Rit neural-mass nodes with heterogeneous structural connections\nestimated from diffusion MRI data provided by the Human Connectome Project. Via\ndirect simulations we determine the similarity of functional (inferred from\ncorrelated activity between nodes) and structural connectivity matrices under\nvariation of the parameters controlling single-node dynamics, highlighting a\nnon-trivial structure--function relationship in regimes that support limit\ncycle oscillations. To determine their relationship, we firstly calculate\nnetwork instabilities giving rise to oscillations, and the so-called `false\nbifurcations' (for which a significant qualitative change in the orbit is\nobserved, without a change of stability) occurring beyond this onset. We\nhighlight that functional connectivity (FC) is inherited robustly from\nstructure when node dynamics are poised near a Hopf bifurcation, whilst near\nfalse bifurcations, structure only weakly influences FC. Secondly, we develop a\nweakly-coupled oscillator description to analyse oscillatory phase-locked\nstates and, furthermore, show how the modular structure of FC matrices can be\npredicted via linear stability analysis. This study thereby emphasises the\nsubstantial role that local dynamics can have in shaping large-scale functional\nbrain states.\n", "versions": [{"version": "v1", "created": "Thu, 27 Jun 2019 12:00:13 GMT"}, {"version": "v2", "created": "Thu, 9 Jul 2020 12:10:20 GMT"}], "update_date": "2020-07-10", "authors_parsed": [["Forrester", "Michael", ""], ["Coombes", "Stephen", ""], ["Crofts", "Jonathan J.", ""], ["Sotiropoulos", "Stamatios N.", ""], ["O'Dea", "Reuben D.", ""]]}, {"id": "1906.11741", "submitter": "Alessandro Grillini", "authors": "Alessandro Grillini, Remco J. Renken, Frans W. Cornelissen", "title": "Attentional Modulation of Visual Spatial Integration: Psychophysical\n  Evidence Supported by Population Coding Modeling", "comments": "39 pages, 10 figures, accepted for publication in Journal of\n  Cognitive Neuroscience", "journal-ref": null, "doi": null, "report-no": null, "categories": "q-bio.NC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Two prominent strategies that the human visual system uses to reduce incoming\ninformation are spatial integration and selective attention. Although spatial\nintegration summarizes and combines information over the visual field,\nselective attention can single it out for scrutiny. The way in which these\nwell-known mechanisms, with rather opposing effects, interact remains largely\nunknown. To address this, we had observers perform a gaze-contingent search\ntask that nudged them to deploy either spatial or feature-based attention to\nmaximize performance. We found that, depending on the type of attention\nemployed, visual spatial integration strength changed either in a strong and\nlocalized or a more modest and global manner compared with a baseline\ncondition. Population code modeling revealed that a single mechanism can\naccount for both observations: Attention acts beyond the neuronal encoding\nstage to tune the spatial integration weights of neural populations. Our study\nshows how attention and integration interact to optimize the information flow\nthrough the brain.\n", "versions": [{"version": "v1", "created": "Thu, 27 Jun 2019 15:41:41 GMT"}], "update_date": "2019-06-28", "authors_parsed": [["Grillini", "Alessandro", ""], ["Renken", "Remco J.", ""], ["Cornelissen", "Frans W.", ""]]}, {"id": "1906.11759", "submitter": "Francisco Afonso Raposo", "authors": "Francisco Afonso Raposo and David Martins de Matos and Ricardo Ribeiro", "title": "Low-dimensional Embodied Semantics for Music and Language", "comments": "6 pages, 1 figure, 1 table", "journal-ref": null, "doi": null, "report-no": null, "categories": "q-bio.NC cs.IR cs.LG cs.SD eess.AS stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Embodied cognition states that semantics is encoded in the brain as firing\npatterns of neural circuits, which are learned according to the statistical\nstructure of human multimodal experience. However, each human brain is\nidiosyncratically biased, according to its subjective experience history,\nmaking this biological semantic machinery noisy with respect to the overall\nsemantics inherent to media artifacts, such as music and language excerpts. We\npropose to represent shared semantics using low-dimensional vector embeddings\nby jointly modeling several brains from human subjects. We show these\nunsupervised efficient representations outperform the original high-dimensional\nfMRI voxel spaces in proxy music genre and language topic classification tasks.\nWe further show that joint modeling of several subjects increases the semantic\nrichness of the learned latent vector spaces.\n", "versions": [{"version": "v1", "created": "Thu, 20 Jun 2019 11:09:01 GMT"}], "update_date": "2019-06-28", "authors_parsed": [["Raposo", "Francisco Afonso", ""], ["de Matos", "David Martins", ""], ["Ribeiro", "Ricardo", ""]]}, {"id": "1906.11770", "submitter": "Tomoki Kurikawa", "authors": "Tomoki Kurikawa, Omri Barak, Kunihiko Kaneko", "title": "Repeated sequential learning increases memory capacity via effective\n  decorrelation in a recurrent neural network", "comments": null, "journal-ref": "Phys. Rev. Research 2, 023307 (2020)", "doi": "10.1103/PhysRevResearch.2.023307", "report-no": null, "categories": "nlin.AO cs.NE q-bio.NC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Memories in neural system are shaped through the interplay of neural and\nlearning dynamics under external inputs. By introducing a simple local learning\nrule to a neural network, we found that the memory capacity is drastically\nincreased by sequentially repeating the learning steps of input-output\nmappings. The origin of this enhancement is attributed to the generation of a\nPsuedo-inverse correlation in the connectivity. This is associated with the\nemergence of spontaneous activity that intermittently exhibits neural patterns\ncorresponding to embedded memories. Stablization of memories is achieved by a\ndistinct bifurcation from the spontaneous activity under the application of\neach input.\n", "versions": [{"version": "v1", "created": "Sat, 22 Jun 2019 13:47:33 GMT"}], "update_date": "2020-07-01", "authors_parsed": [["Kurikawa", "Tomoki", ""], ["Barak", "Omri", ""], ["Kaneko", "Kunihiko", ""]]}, {"id": "1906.12222", "submitter": "Alexander Gorban", "authors": "Alexander N. Gorban, Valeri A. Makarov, Ivan Y. Tyukin", "title": "Symphony of high-dimensional brain", "comments": null, "journal-ref": "Physics of Life Reviews, 2019", "doi": "10.1016/j.plrev.2019.06.003", "report-no": null, "categories": "q-bio.NC cs.AI", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  This paper is the final part of the scientific discussion organised by the\nJournal \"Physics of Life Rviews\" about the simplicity revolution in\nneuroscience and AI. This discussion was initiated by the review paper \"The\nunreasonable effectiveness of small neural ensembles in high-dimensional\nbrain\". Phys Life Rev 2019, doi 10.1016/j.plrev.2018.09.005, arXiv:1809.07656.\nThe topics of the discussion varied from the necessity to take into account the\ndifference between the theoretical random distributions and \"extremely\nnon-random\" real distributions and revise the common machine learning theory,\nto different forms of the curse of dimensionality and high-dimensional pitfalls\nin neuroscience. V. K{\\r{u}}rkov{\\'a}, A. Tozzi and J.F. Peters, R. Quian\nQuiroga, P. Varona, R. Barrio, G. Kreiman, L. Fortuna, C. van Leeuwen, R. Quian\nQuiroga, and V. Kreinovich, A.N. Gorban, V.A. Makarov, and I.Y. Tyukin\nparticipated in the discussion. In this paper we analyse the symphony of\nopinions and the possible outcomes of the simplicity revolution for machine\nlearning and neuroscience.\n", "versions": [{"version": "v1", "created": "Thu, 27 Jun 2019 17:46:24 GMT"}], "update_date": "2019-07-01", "authors_parsed": [["Gorban", "Alexander N.", ""], ["Makarov", "Valeri A.", ""], ["Tyukin", "Ivan Y.", ""]]}, {"id": "1906.12350", "submitter": "Baihan Lin", "authors": "Baihan Lin, Djallel Bouneffouf, Guillermo Cecchi", "title": "Split Q Learning: Reinforcement Learning with Two-Stream Rewards", "comments": "IJCAI 2019. This article supersedes our work arXiv:1706.02897 into RL\n  setting, with a different focus by applying Inverse Reinforcement Learning to\n  model human clinical behavioral bias. It also precedes our work\n  arXiv:1906.11286 which introduces extensive emphases in RL games", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI cs.MA q-bio.NC stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Drawing an inspiration from behavioral studies of human decision making, we\npropose here a general parametric framework for a reinforcement learning\nproblem, which extends the standard Q-learning approach to incorporate a\ntwo-stream framework of reward processing with biases biologically associated\nwith several neurological and psychiatric conditions, including Parkinson's and\nAlzheimer's diseases, attention-deficit/hyperactivity disorder (ADHD),\naddiction, and chronic pain. For AI community, the development of agents that\nreact differently to different types of rewards can enable us to understand a\nwide spectrum of multi-agent interactions in complex real-world socioeconomic\nsystems. Moreover, from the behavioral modeling perspective, our parametric\nframework can be viewed as a first step towards a unifying computational model\ncapturing reward processing abnormalities across multiple mental conditions and\nuser preferences in long-term recommendation systems.\n", "versions": [{"version": "v1", "created": "Fri, 21 Jun 2019 01:59:52 GMT"}, {"version": "v2", "created": "Tue, 12 Nov 2019 19:10:05 GMT"}], "update_date": "2019-11-14", "authors_parsed": [["Lin", "Baihan", ""], ["Bouneffouf", "Djallel", ""], ["Cecchi", "Guillermo", ""]]}]