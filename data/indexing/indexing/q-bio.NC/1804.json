[{"id": "1804.00049", "submitter": "Martin Dyrba", "authors": "Martin Dyrba, Reza Mohammadi, Michel J. Grothe, Thomas Kirste, Stefan\n  J. Teipel", "title": "Gaussian graphical models reveal inter-modal and inter-regional\n  conditional dependencies of brain alterations in Alzheimer's disease", "comments": "24 pages, 9 figures, 2 tables, supporting material", "journal-ref": null, "doi": "10.3389/fnagi.2020.00099", "report-no": null, "categories": "q-bio.NC stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Alzheimer's disease (AD) is characterized by a sequence of pathological\nchanges, which are commonly assessed in vivo using MRI and PET. Currently, the\nmost approaches to analyze statistical associations between brain regions rely\non Pearson correlation. However, these are prone to spurious correlations\narising from uninformative shared variance. Notably, there are no appropriate\nmultivariate statistical models available that can easily integrate dozens of\nvariables derived from such data, being able to use the additional information\nprovided from the combination of data sources. Gaussian graphical models (GGMs)\ncan estimate the conditional dependency from given data, which is expected to\nreflect the underlying causal relationships. We applied GGMs to assess\nmultimodal regional brain alterations in AD. We obtained data from N=972\nsubjects from the Alzheimer's Disease Neuroimaging Initiative. The mean amyloid\nload (AV45-PET), glucose metabolism (FDG-PET), and gray matter volume (MRI)\nwere calculated. GGMs were estimated using a Bayesian framework for the\ncombined multimodal data to obtain conditional dependency networks. Conditional\ndependency matrices were much sparser (10% density) than Pearson correlation\nmatrices (50% density). Within modalities, conditional dependency networks\nyielded clusters connecting anatomically adjacent regions. For associations\nbetween different modalities, only few region-specific connections remained.\nGraph-theoretical network statistics were significantly altered between groups,\nwith a biphasic u-shape trajectory. GGMs removed shared variance among\nmultimodal measures of regional brain alterations in MCI and AD, and yielded\nsparser matrices compared to Pearson correlation networks. Therefore, GGMs may\nbe used as alternative to thresholding-approaches typically applied to\ncorrelation networks to obtain the most informative relations between\nvariables.\n", "versions": [{"version": "v1", "created": "Fri, 30 Mar 2018 20:33:49 GMT"}, {"version": "v2", "created": "Wed, 5 Sep 2018 14:24:18 GMT"}, {"version": "v3", "created": "Fri, 27 Mar 2020 16:27:51 GMT"}], "update_date": "2020-03-30", "authors_parsed": [["Dyrba", "Martin", ""], ["Mohammadi", "Reza", ""], ["Grothe", "Michel J.", ""], ["Kirste", "Thomas", ""], ["Teipel", "Stefan J.", ""]]}, {"id": "1804.00153", "submitter": "Cristiano Capone", "authors": "Cristiano Capone, Guido Gigante, Paolo Del Giudice", "title": "Spontaneous activity emerging from an inferred network model captures\n  complex temporal dynamics of spiking data", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "q-bio.NC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The combination of new recording techniques in neuroscience and powerful\ninference methods recently held the promise to recover useful effective models,\nat the single neuron or network level, directly from observed data. The value\nof a model of course should critically depend on its ability to reproduce the\ndynamical behavior of the modeled system; however, few attempts have been made\nto inquire into the dynamics of inferred models in neuroscience, and none, to\nour knowledge, at the network level. Here we introduce a principled\nmodification of a widely used generalized linear model (GLM), and learn its\nstructural and dynamic parameters from ex-vivo spiking data. We show that the\nnew model is able to capture the most prominent features of the highly\nnon-stationary and non-linear dynamics displayed by the biological network,\nwhere the reference GLM largely fails. Two ingredients turn out to be key for\nsuccess. The first one is a bounded transfer function that makes the single\nneuron able to respond to its input in a saturating fashion; beyond its\nbiological plausibility such property, by limiting the capacity of the neuron\nto transfer information, makes the coding more robust in the face of the highly\nvariable network activity, and noise. The second ingredient is a super-Poisson\nspikes generative probabilistic mechanism; this feature, that accounts for the\nfact that observations largely undersample the network, allows the model neuron\nto more flexibly incorporate the observed activity fluctuations. Taken\ntogether, the two ingredients, without increasing complexity, allow the model\nto capture the key dynamic elements. When left free to generate its spontaneous\nactivity, the inferred model proved able to reproduce not only the\nnon-stationary population dynamics of the network, but also part of the\nfine-grained structure of the dynamics at the single neuron level.\n", "versions": [{"version": "v1", "created": "Sat, 31 Mar 2018 10:48:34 GMT"}, {"version": "v2", "created": "Fri, 6 Apr 2018 09:06:53 GMT"}], "update_date": "2018-04-09", "authors_parsed": [["Capone", "Cristiano", ""], ["Gigante", "Guido", ""], ["Del Giudice", "Paolo", ""]]}, {"id": "1804.00227", "submitter": "Milad Mozafari", "authors": "Milad Mozafari, Mohammad Ganjtabesh, Abbas Nowzari-Dalini, Simon J.\n  Thorpe, Timoth\\'ee Masquelier", "title": "Bio-inspired digit recognition using reward-modulated\n  spike-timing-dependent plasticity in deep convolutional networks", "comments": "Pattern Recognition (2019)", "journal-ref": null, "doi": "10.1016/j.patcog.2019.05.015", "report-no": null, "categories": "cs.CV q-bio.NC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The primate visual system has inspired the development of deep artificial\nneural networks, which have revolutionized the computer vision domain. Yet\nthese networks are much less energy-efficient than their biological\ncounterparts, and they are typically trained with backpropagation, which is\nextremely data-hungry. To address these limitations, we used a deep\nconvolutional spiking neural network (DCSNN) and a latency-coding scheme. We\ntrained it using a combination of spike-timing-dependent plasticity (STDP) for\nthe lower layers and reward-modulated STDP (R-STDP) for the higher ones. In\nshort, with R-STDP a correct (resp. incorrect) decision leads to STDP (resp.\nanti-STDP). This approach led to an accuracy of $97.2\\%$ on MNIST, without\nrequiring an external classifier. In addition, we demonstrated that R-STDP\nextracts features that are diagnostic for the task at hand, and discards the\nother ones, whereas STDP extracts any feature that repeats. Finally, our\napproach is biologically plausible, hardware friendly, and energy-efficient.\n", "versions": [{"version": "v1", "created": "Sat, 31 Mar 2018 23:35:12 GMT"}, {"version": "v2", "created": "Mon, 11 Feb 2019 09:26:35 GMT"}, {"version": "v3", "created": "Mon, 27 May 2019 19:33:54 GMT"}], "update_date": "2019-05-30", "authors_parsed": [["Mozafari", "Milad", ""], ["Ganjtabesh", "Mohammad", ""], ["Nowzari-Dalini", "Abbas", ""], ["Thorpe", "Simon J.", ""], ["Masquelier", "Timoth\u00e9e", ""]]}, {"id": "1804.00404", "submitter": "Ryuta Mizutani", "authors": "Ryuta Mizutani, Rino Saiga, Akihisa Takeuchi, Kentaro Uesugi, Yasuko\n  Terada, Yoshio Suzuki, Vincent De Andrade, Francesco De Carlo, Susumu\n  Takekoshi, Chie Inomoto, Naoya Nakamura, Itaru Kushima, Shuji Iritani, Norio\n  Ozaki, Soichiro Ide, Kazutaka Ikeda, Kenichi Oshima, Masanari Itokawa, and\n  Makoto Arai", "title": "Three-dimensional alteration of neurites in schizophrenia", "comments": "24 pages, 4 figures, and 1 table. Supplementary materials are\n  available from DOI link", "journal-ref": "Translational Psychiatry 9, 85 (2019)", "doi": "10.1038/s41398-019-0427-4", "report-no": null, "categories": "q-bio.NC physics.bio-ph", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  This paper reports nano-CT analysis of brain tissues of schizophrenia and\ncontrol cases. The analysis revealed that: (1) neuronal structures vary between\nindividuals, (2) the mean curvature of distal neurites of the schizophrenia\ncases was 1.5 times higher than that of the controls, and (3) dendritic spines\nwere categorized into two geometrically distinctive groups, though no\nstructural differences were observed between the disease and control cases. The\ndifferences in the neurite curvature result in differences in the spatial\ntrajectory and hence alter neuronal circuits. We suggest that the structural\nalteration of neurons in the schizophrenia cases should reflect psychiatric\nsymptoms of schizophrenia.\n", "versions": [{"version": "v1", "created": "Mon, 2 Apr 2018 05:55:03 GMT"}, {"version": "v2", "created": "Fri, 20 Jul 2018 02:37:46 GMT"}, {"version": "v3", "created": "Sat, 16 Feb 2019 06:33:45 GMT"}], "update_date": "2019-02-19", "authors_parsed": [["Mizutani", "Ryuta", ""], ["Saiga", "Rino", ""], ["Takeuchi", "Akihisa", ""], ["Uesugi", "Kentaro", ""], ["Terada", "Yasuko", ""], ["Suzuki", "Yoshio", ""], ["De Andrade", "Vincent", ""], ["De Carlo", "Francesco", ""], ["Takekoshi", "Susumu", ""], ["Inomoto", "Chie", ""], ["Nakamura", "Naoya", ""], ["Kushima", "Itaru", ""], ["Iritani", "Shuji", ""], ["Ozaki", "Norio", ""], ["Ide", "Soichiro", ""], ["Ikeda", "Kazutaka", ""], ["Oshima", "Kenichi", ""], ["Itokawa", "Masanari", ""], ["Arai", "Makoto", ""]]}, {"id": "1804.00794", "submitter": "Carina Curto", "authors": "Carina Curto, Jesse Geneson, Katherine Morrison", "title": "Fixed points of competitive threshold-linear networks", "comments": "53 pages, 22 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "q-bio.NC cs.DM math.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Threshold-linear networks (TLNs) are models of neural networks that consist\nof simple, perceptron-like neurons and exhibit nonlinear dynamics that are\ndetermined by the network's connectivity. The fixed points of a TLN, including\nboth stable and unstable equilibria, play a critical role in shaping its\nemergent dynamics. In this work, we provide two novel characterizations for the\nset of fixed points of a competitive TLN: the first is in terms of a simple\nsign condition, while the second relies on the concept of domination. We apply\nthese results to a special family of TLNs, called combinatorial\nthreshold-linear networks (CTLNs), whose connectivity matrices are defined from\ndirected graphs. This leads us to prove a series of graph rules that enable one\nto determine fixed points of a CTLN by analyzing the underlying graph.\nAdditionally, we study larger networks composed of smaller \"building block\"\nsubnetworks, and prove several theorems relating the fixed points of the full\nnetwork to those of its components. Our results provide the foundation for a\nkind of \"graphical calculus\" to infer features of the dynamics from a network's\nconnectivity.\n", "versions": [{"version": "v1", "created": "Sun, 1 Apr 2018 18:37:05 GMT"}, {"version": "v2", "created": "Fri, 3 Aug 2018 17:56:10 GMT"}], "update_date": "2018-08-06", "authors_parsed": [["Curto", "Carina", ""], ["Geneson", "Jesse", ""], ["Morrison", "Katherine", ""]]}, {"id": "1804.00970", "submitter": "Wolfgang Fuhl", "authors": "Wolfgang Fuhl, Thiago Santini, Thomas Kuebler, Nora Castner, Wolfgang\n  Rosenstiel, Enkelejda Kasneci", "title": "Eye movement simulation and detector creation to reduce laborious\n  parameter adjustments", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "q-bio.NC cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Eye movements hold information about human perception, intention and\ncognitive state. Various algorithms have been proposed to identify and\ndistinguish eye movements, particularly fixations, saccades, and smooth\npursuits. A major drawback of existing algorithms is that they rely on accurate\nand constant sampling rates, impeding straightforward adaptation to new\nmovements such as micro saccades. We propose a novel eye movement simulator\nthat i) probabilistically simulates saccade movements as gamma distributions\nconsidering different peak velocities and ii) models smooth pursuit onsets with\nthe sigmoid function. This simulator is combined with a machine learning\napproach to create detectors for general and specific velocity profiles.\nAdditionally, our approach is capable of using any sampling rate, even with\nfluctuations. The machine learning approach consists of different binary\npatterns combined using conditional distributions. The simulation is evaluated\nagainst publicly available real data using a squared error, and the detectors\nare evaluated against state-of-the-art algorithms.\n", "versions": [{"version": "v1", "created": "Wed, 28 Mar 2018 06:48:37 GMT"}], "update_date": "2018-04-04", "authors_parsed": [["Fuhl", "Wolfgang", ""], ["Santini", "Thiago", ""], ["Kuebler", "Thomas", ""], ["Castner", "Nora", ""], ["Rosenstiel", "Wolfgang", ""], ["Kasneci", "Enkelejda", ""]]}, {"id": "1804.01296", "submitter": "Benjam\\'in Guti\\'errez Becker", "authors": "Benjamin Gutierrez Becker, Tassilo Klein, Christian Wachinger", "title": "Gaussian Process Uncertainty in Age Estimation as a Measure of Brain\n  Abnormality", "comments": "Paper accepted in Neuroimage", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV q-bio.NC q-bio.QM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Multivariate regression models for age estimation are a powerful tool for\nassessing abnormal brain morphology associated to neuropathology. Age\nprediction models are built on cohorts of healthy subjects and are built to\nreflect normal aging patterns. The application of these multivariate models to\ndiseased subjects usually results in high prediction errors, under the\nhypothesis that neuropathology presents a similar degenerative pattern as that\nof accelerated aging. In this work, we propose an alternative to the idea that\npathology follows a similar trajectory than normal aging. Instead, we propose\nthe use of metrics which measure deviations from the mean aging trajectory. We\npropose to measure these deviations using two different metrics: uncertainty in\na Gaussian process regression model and a newly proposed age weighted\nuncertainty measure. Consequently, our approach assumes that pathologic brain\npatterns are different to those of normal aging. We present results for\nsubjects with autism, mild cognitive impairment and Alzheimer's disease to\nhighlight the versatility of the approach to different diseases and age ranges.\nWe evaluate volume, thickness, and VBM features for quantifying brain\nmorphology. Our evaluations are performed on a large number of images obtained\nfrom a variety of publicly available neuroimaging databases. Across all\nfeatures, our uncertainty based measurements yield a better separation between\ndiseased subjects and healthy individuals than the prediction error. Finally,\nwe illustrate differences in the disease pattern to normal aging, supporting\nthe application of uncertainty as a measure of neuropathology.\n", "versions": [{"version": "v1", "created": "Wed, 4 Apr 2018 08:38:07 GMT"}], "update_date": "2018-04-05", "authors_parsed": [["Becker", "Benjamin Gutierrez", ""], ["Klein", "Tassilo", ""], ["Wachinger", "Christian", ""]]}, {"id": "1804.01487", "submitter": "Carina Curto", "authors": "Katherine Morrison and Carina Curto", "title": "Predicting neural network dynamics via graphical analysis", "comments": "29 pages, 19 figures. A book chapter for advanced undergraduates to\n  appear in \"Algebraic and Combinatorial Computational Biology.\" R. Robeva, M.\n  Macaulay (Eds) 2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "q-bio.NC math.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Neural network models in neuroscience allow one to study how the connections\nbetween neurons shape the activity of neural circuits in the brain. In this\nchapter, we study Combinatorial Threshold-Linear Networks (CTLNs) in order to\nunderstand how the pattern of connectivity, as encoded by a directed graph,\nshapes the emergent nonlinear dynamics of the corresponding network. Important\naspects of these dynamics are controlled by the stable and unstable fixed\npoints of the network, and we show how these fixed points can be determined via\ngraph-based rules. We also present an algorithm for predicting sequences of\nneural activation from the underlying directed graph, and examine the effect of\ngraph symmetries on a network's set of attractors.\n", "versions": [{"version": "v1", "created": "Wed, 4 Apr 2018 16:05:52 GMT"}], "update_date": "2018-04-05", "authors_parsed": [["Morrison", "Katherine", ""], ["Curto", "Carina", ""]]}, {"id": "1804.01568", "submitter": "Keivan Hassani Monfared", "authors": "Keivan Hassani Monfared, Kris Vasudevan, Jordan S. Farrell, and G.\n  Campbell Teskey", "title": "Community structure detection and evaluation during the pre- and\n  post-ictal hippocampal depth recordings", "comments": "13 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.SI physics.soc-ph q-bio.NC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Detecting and evaluating regions of brain under various circumstances is one\nof the most interesting topics in computational neuroscience. However, the\nmajority of the studies on detecting communities of a functional connectivity\nnetwork of the brain is done on networks obtained from coherency attributes,\nand not from correlation. This lack of studies, in part, is due to the fact\nthat many common methods for clustering graphs require the nodes of the network\nto be `positively' linked together, a property that is guaranteed by a\ncoherency matrix, by definition. However, correlation matrices reveal more\ninformation regarding how each pair of nodes are linked together. In this\nstudy, for the first time we simultaneously examine four inherently different\nnetwork clustering methods (spectral, heuristic, and optimization methods)\napplied to the functional connectivity networks of the CA1 region of the\nhippocampus of an anaesthetized rat during pre-ictal and post-ictal states. The\nnetworks are obtained from correlation matrices, and its results are compared\nwith the ones obtained by applying the same methods to coherency matrices. The\ncorrelation matrices show a much finer community structure compared to the\ncoherency matrices. Furthermore, we examine the potential smoothing effect of\nchoosing various window sizes for computing the correlation/coherency matrices.\n", "versions": [{"version": "v1", "created": "Wed, 14 Mar 2018 20:08:53 GMT"}, {"version": "v2", "created": "Thu, 31 May 2018 18:54:00 GMT"}], "update_date": "2018-06-04", "authors_parsed": [["Monfared", "Keivan Hassani", ""], ["Vasudevan", "Kris", ""], ["Farrell", "Jordan S.", ""], ["Teskey", "G. Campbell", ""]]}, {"id": "1804.01660", "submitter": "Christoph Adami", "authors": "Arend Hintze, Douglas Kirkpatrick, and Christoph Adami (Michigan State\n  University)", "title": "The structure of evolved representations across different substrates for\n  artificial intelligence", "comments": "8 pages, 13 figures. Submitted to Artificial Life Conference (Tokyo,\n  2018)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NE cs.AI nlin.AO q-bio.NC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Artificial neural networks (ANNs), while exceptionally useful for\nclassification, are vulnerable to misdirection. Small amounts of noise can\nsignificantly affect their ability to correctly complete a task. Instead of\ngeneralizing concepts, ANNs seem to focus on surface statistical regularities\nin a given task. Here we compare how recurrent artificial neural networks, long\nshort-term memory units, and Markov Brains sense and remember their\nenvironments. We show that information in Markov Brains is localized and\nsparsely distributed, while the other neural network substrates \"smear\"\ninformation about the environment across all nodes, which makes them vulnerable\nto noise.\n", "versions": [{"version": "v1", "created": "Thu, 5 Apr 2018 03:10:37 GMT"}], "update_date": "2018-04-06", "authors_parsed": [["Hintze", "Arend", "", "Michigan State\n  University"], ["Kirkpatrick", "Douglas", "", "Michigan State\n  University"], ["Adami", "Christoph", "", "Michigan State\n  University"]]}, {"id": "1804.01704", "submitter": "Nivethika Sivakumaran", "authors": "Divyanjali Shanmuganathan and Nivethika Sivakumaran", "title": "Review: the development of neural stem cell biology and technology in\n  regenerative medicine", "comments": "15 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "q-bio.NC q-bio.TO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In the middle of the last century, it has been known that neural stem cells\n(NSCs) play a key role in regenerative medicine to cure the neurodegenerative\ndisease. This review article covers about the introduction to neural stem cell\nbiology and the isolation, differentiation and transplantation\nmethods/techniques of neural stem cells. The neural stem cells can be\ntransplanted into the human brain in the future to replace the damaged and dead\nneurons. The highly limited access to embryonic stem cells and ethical issues\nhave escalated the search for other NSC sources. The developing technologies\nare indicating that it can be achieved before the end of this century. In\naddition, the differentiation and the maturation of NSCs can artificially\naccelerate by modern methods.\n", "versions": [{"version": "v1", "created": "Thu, 5 Apr 2018 07:32:21 GMT"}], "update_date": "2018-04-06", "authors_parsed": [["Shanmuganathan", "Divyanjali", ""], ["Sivakumaran", "Nivethika", ""]]}, {"id": "1804.01840", "submitter": "Syed Ahmed Aamir", "authors": "Syed Ahmed Aamir, Paul M\\\"uller, Gerd Kiene, Laura Kriener, Yannik\n  Stradmann, Andreas Gr\\\"ubl, Johannes Schemmel, Karlheinz Meier", "title": "A Mixed-Signal Structured AdEx Neuron for Accelerated Neuromorphic Cores", "comments": "11 pages, 17 figures (including author photographs)", "journal-ref": null, "doi": "10.1109/TBCAS.2018.2848203", "report-no": null, "categories": "q-bio.NC cs.ET physics.bio-ph", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Here we describe a multi-compartment neuron circuit based on the\nAdaptive-Exponential I&F (AdEx) model, developed for the second-generation\nBrainScaleS hardware. Based on an existing modular Leaky Integrate-and-Fire\n(LIF) architecture designed in 65 nm CMOS, the circuit features exponential\nspike generation, neuronal adaptation, inter-compartmental connections as well\nas a conductance-based reset. The design reproduces a diverse set of firing\npatterns observed in cortical pyramidal neurons. Further, it enables the\nemulation of sodium and calcium spikes, as well as N-Methyl-D-Aspartate (NMDA)\nplateau potentials known from apical and thin dendrites. We characterize the\nAdEx circuit extensions and exemplify how the interplay between passive and\nnon-linear active signal processing enhances the computational capabilities of\nsingle (but structured) on-chip neurons.\n", "versions": [{"version": "v1", "created": "Thu, 5 Apr 2018 13:32:10 GMT"}, {"version": "v2", "created": "Mon, 14 May 2018 15:09:44 GMT"}, {"version": "v3", "created": "Mon, 28 May 2018 16:57:30 GMT"}, {"version": "v4", "created": "Tue, 29 May 2018 18:23:02 GMT"}], "update_date": "2019-03-28", "authors_parsed": [["Aamir", "Syed Ahmed", ""], ["M\u00fcller", "Paul", ""], ["Kiene", "Gerd", ""], ["Kriener", "Laura", ""], ["Stradmann", "Yannik", ""], ["Gr\u00fcbl", "Andreas", ""], ["Schemmel", "Johannes", ""], ["Meier", "Karlheinz", ""]]}, {"id": "1804.01906", "submitter": "Yannik Stradmann", "authors": "Syed Ahmed Aamir, Yannik Stradmann, Paul M\\\"uller, Christian Pehle,\n  Andreas Hartel, Andreas Gr\\\"ubl, Johannes Schemmel and Karlheinz Meier", "title": "An Accelerated LIF Neuronal Network Array for a Large Scale Mixed-Signal\n  Neuromorphic Architecture", "comments": "14 pages, 9 Figures, accepted for publication in IEEE Transactions on\n  Circuits and Systems I", "journal-ref": null, "doi": "10.1109/TCSI.2018.2840718", "report-no": null, "categories": "q-bio.NC cs.ET physics.bio-ph physics.comp-ph", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present an array of leaky integrate-and-fire (LIF) neuron circuits\ndesigned for the second-generation BrainScaleS mixed-signal 65-nm CMOS\nneuromorphic hardware. The neuronal array is embedded in the analog network\ncore of a scaled-down prototype HICANN-DLS chip. Designed as continuous-time\ncircuits, the neurons are highly tunable and reconfigurable elements with\naccelerated dynamics. Each neuron integrates input current from a multitude of\nincoming synapses and evokes a digital spike event output. The circuit offers a\nwide tuning range for synaptic and membrane time constants, as well as for\nrefractory periods to cover a number of computational models. We elucidate our\ndesign methodology, underlying circuit design, calibration and measurement\nresults from individual sub-circuits across multiple dies. The circuit dynamics\nmatch with the behavior of the LIF mathematical model. We further demonstrate a\nwinner-take-all network on the prototype chip as a typical element of cortical\nprocessing.\n", "versions": [{"version": "v1", "created": "Thu, 5 Apr 2018 15:19:00 GMT"}, {"version": "v2", "created": "Tue, 15 May 2018 11:44:07 GMT"}, {"version": "v3", "created": "Wed, 23 May 2018 12:40:58 GMT"}], "update_date": "2019-03-28", "authors_parsed": [["Aamir", "Syed Ahmed", ""], ["Stradmann", "Yannik", ""], ["M\u00fcller", "Paul", ""], ["Pehle", "Christian", ""], ["Hartel", "Andreas", ""], ["Gr\u00fcbl", "Andreas", ""], ["Schemmel", "Johannes", ""], ["Meier", "Karlheinz", ""]]}, {"id": "1804.01958", "submitter": "Alain Goriely", "authors": "Johannes Weickenmeier, Ellen Kuhl, Alain Goriely", "title": "The multiphysics of prion-like diseases: progression and atrophy", "comments": "5 pages/5 figures", "journal-ref": "Phys. Rev. Lett. 121, 158101 (2018)", "doi": "10.1103/PhysRevLett.121.158101", "report-no": null, "categories": "q-bio.NC q-bio.TO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Many neurodegenerative diseases are related to the propagation and\naccumulation of toxic proteins throughout the brain. The lesions created by\naggregates of these toxic proteins further lead to cell death and accelerated\ntissue atrophy. A striking feature of some of these diseases is their\ncharacteristic pattern and evolution, leading to well-codified disease stages\nvisible to neuropathology and associated with various cognitive deficits and\npathologies. Here, we simulate the anisotropic propagation and accumulation of\ntoxic proteins in full brain geometry. We show that the same model with\ndifferent initial seeding zones reproduces the characteristic evolution of\ndifferent prion-like diseases. We also recover the expected evolution of the\ntotal toxic protein load. Finally, we couple our transport model to a\nmechanical atrophy model to obtain the typical degeneration patterns found in\nneurodegenerative diseases.\n", "versions": [{"version": "v1", "created": "Thu, 5 Apr 2018 17:12:17 GMT"}], "update_date": "2018-10-17", "authors_parsed": [["Weickenmeier", "Johannes", ""], ["Kuhl", "Ellen", ""], ["Goriely", "Alain", ""]]}, {"id": "1804.01961", "submitter": "Lucia Ballerini", "authors": "Enrico Pellegrini, Lucia Ballerini, Maria del C. Valdes Hernandez,\n  Francesca M. Chappell, Victor Gonz\\'alez-Castro, Devasuda Anblagan, Samuel\n  Danso, Susana Mu\\~noz Maniega, Dominic Job, Cyril Pernet, Grant Mair, Tom\n  MacGillivray, Emanuele Trucco, Joanna Wardlaw", "title": "Machine learning of neuroimaging to diagnose cognitive impairment and\n  dementia: a systematic review and comparative analysis", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "q-bio.NC cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  INTRODUCTION: Advanced machine learning methods might help to identify\ndementia risk from neuroimaging, but their accuracy to date is unclear.\n  METHODS: We systematically reviewed the literature, 2006 to late 2016, for\nmachine learning studies differentiating healthy ageing through to dementia of\nvarious types, assessing study quality, and comparing accuracy at different\ndisease boundaries.\n  RESULTS: Of 111 relevant studies, most assessed Alzheimer's disease (AD) vs\nhealthy controls, used ADNI data, support vector machines and only T1-weighted\nsequences. Accuracy was highest for differentiating AD from healthy controls,\nand poor for differentiating healthy controls vs MCI vs AD, or MCI converters\nvs non-converters. Accuracy increased using combined data types, but not by\ndata source, sample size or machine learning method.\n  DISCUSSION: Machine learning does not differentiate clinically-relevant\ndisease categories yet. More diverse datasets, combinations of different types\nof data, and close clinical integration of machine learning would help to\nadvance the field.\n", "versions": [{"version": "v1", "created": "Thu, 5 Apr 2018 17:17:39 GMT"}, {"version": "v2", "created": "Wed, 11 Apr 2018 22:01:51 GMT"}], "update_date": "2018-04-13", "authors_parsed": [["Pellegrini", "Enrico", ""], ["Ballerini", "Lucia", ""], ["Hernandez", "Maria del C. Valdes", ""], ["Chappell", "Francesca M.", ""], ["Gonz\u00e1lez-Castro", "Victor", ""], ["Anblagan", "Devasuda", ""], ["Danso", "Samuel", ""], ["Maniega", "Susana Mu\u00f1oz", ""], ["Job", "Dominic", ""], ["Pernet", "Cyril", ""], ["Mair", "Grant", ""], ["MacGillivray", "Tom", ""], ["Trucco", "Emanuele", ""], ["Wardlaw", "Joanna", ""]]}, {"id": "1804.02115", "submitter": "Elham Bayat Mokhtari", "authors": "Elham Bayat Mokhtari, J. Josh Lawrence, Emily F Stone", "title": "Effect of Neuromodulation of Short-Term Plasticity on Information\n  Processing in Hippocampal Interneuron Synapses", "comments": "29 pages, 14 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "q-bio.NC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Neurons in a micro-circuit connected by chemical synapses can have their\nconnectivity affected by the prior activity of the cells. The number of\nsynapses available for releasing neurotransmitter can be decreased by\nrepetitive activation through depletion of readily releasable neurotransmitter\n(NT), or increased through facilitation, where the probability of release of NT\nis increased by prior activation. These competing effects can create a\ncomplicated and subtle range of time dependent connectivity. Here we\ninvestigate the probabilistic properties of facilitation and depression (FD)\nfor a presynaptic neuron that is receiving a Poisson spike train of input. We\nuse a model of FD that is parameterized with experimental data from a\nhippocampal basket cell and pyramidal cell connection, for fixed frequency\ninput spikes at frequencies in the range of theta and gamma oscillations. Hence\nour results will apply to micro-circuits in the hippocampus that are\nresponsible for the interaction of theta and gamma rhythms associated with\nlearning and memory. A control situation is compared with one in which a\npharmaceutical neuromodulator (muscarine) is employed. We apply standard\ninformation theoretic measures such as entropy and mutual information, and find\na closed form approximate expression for the probability distribution of\nrelease probability. We also use techniques that measure the dependence of the\nresponse on the exact history of stimulation the synapse has received, which\nuncovers some unexpected differences between control and muscarine-added cases.\n", "versions": [{"version": "v1", "created": "Fri, 6 Apr 2018 02:37:11 GMT"}], "update_date": "2018-04-09", "authors_parsed": [["Mokhtari", "Elham Bayat", ""], ["Lawrence", "J. Josh", ""], ["Stone", "Emily F", ""]]}, {"id": "1804.02437", "submitter": "Tatiana Levanova", "authors": "T. A. Levanova, A. O. Kazakov, A. G. Korotkov, and G. V. Osipov", "title": "The impact of electrical couplings on the sequential bursting activity\n  in the ensemble of inhibitory coupled Van der Pol elements", "comments": "7 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "q-bio.NC nlin.CD", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The new phenomenological model of the ensemble of three neurons with chemical\n(synaptic) and electrical couplings has been studied. One neuron is modeled by\na single Van der Pol oscillator. The influence of the electrical coupling\nstrength and the frequency mismatch between the elements to the regime of\nsequential activity is investigated.\n", "versions": [{"version": "v1", "created": "Thu, 29 Mar 2018 13:41:48 GMT"}], "update_date": "2018-04-10", "authors_parsed": [["Levanova", "T. A.", ""], ["Kazakov", "A. O.", ""], ["Korotkov", "A. G.", ""], ["Osipov", "G. V.", ""]]}, {"id": "1804.02835", "submitter": "Joshua Vogelstein", "authors": "Randal Burns, Eric Perlman, Alex Baden, William Gray Roncal, Ben Falk,\n  Vikram Chandrashekhar, Forrest Collman, Sharmishtaa Seshamani, Jesse\n  Patsolic, Kunal Lillaney, Michael Kazhdan, Robert Hider Jr., Derek Pryor,\n  Jordan Matelsky, Timothy Gion, Priya Manavalan, Brock Wester, Mark Chevillet,\n  Eric T. Trautman, Khaled Khairy, Eric Bridgeford, Dean M. Kleissas, Daniel J.\n  Tward, Ailey K. Crow, Matthew A. Wright, Michael I. Miller, Stephen J Smith,\n  R. Jacob Vogelstein, Karl Deisseroth, and Joshua T. Vogelstein", "title": "A Community-Developed Open-Source Computational Ecosystem for Big Neuro\n  Data", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "q-bio.NC q-bio.QM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Big imaging data is becoming more prominent in brain sciences across\nspatiotemporal scales and phylogenies. We have developed a computational\necosystem that enables storage, visualization, and analysis of these data in\nthe cloud, thusfar spanning 20+ publications and 100+ terabytes including\nnanoscale ultrastructure, microscale synaptogenetic diversity, and mesoscale\nwhole brain connectivity, making NeuroData the largest and most diverse open\nrepository of brain data.\n", "versions": [{"version": "v1", "created": "Mon, 9 Apr 2018 06:23:45 GMT"}, {"version": "v2", "created": "Tue, 10 Apr 2018 01:36:15 GMT"}], "update_date": "2018-04-11", "authors_parsed": [["Burns", "Randal", ""], ["Perlman", "Eric", ""], ["Baden", "Alex", ""], ["Roncal", "William Gray", ""], ["Falk", "Ben", ""], ["Chandrashekhar", "Vikram", ""], ["Collman", "Forrest", ""], ["Seshamani", "Sharmishtaa", ""], ["Patsolic", "Jesse", ""], ["Lillaney", "Kunal", ""], ["Kazhdan", "Michael", ""], ["Hider", "Robert", "Jr."], ["Pryor", "Derek", ""], ["Matelsky", "Jordan", ""], ["Gion", "Timothy", ""], ["Manavalan", "Priya", ""], ["Wester", "Brock", ""], ["Chevillet", "Mark", ""], ["Trautman", "Eric T.", ""], ["Khairy", "Khaled", ""], ["Bridgeford", "Eric", ""], ["Kleissas", "Dean M.", ""], ["Tward", "Daniel J.", ""], ["Crow", "Ailey K.", ""], ["Wright", "Matthew A.", ""], ["Miller", "Michael I.", ""], ["Smith", "Stephen J", ""], ["Vogelstein", "R. Jacob", ""], ["Deisseroth", "Karl", ""], ["Vogelstein", "Joshua T.", ""]]}, {"id": "1804.02885", "submitter": "Urit Gordon", "authors": "Urit Gordon, Shimon Marom and Naama Brenner", "title": "Visual detection of time-varying signals: opposing biases and their\n  timescales", "comments": "Number of pages: 31 Number of figures: 20", "journal-ref": null, "doi": "10.1371/journal.pone.0224256", "report-no": null, "categories": "q-bio.NC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Human visual perception is a complex, dynamic and fluctuating process. In\naddition to the incoming visual stimulus, it is affected by many other factors\nincluding temporal context, both external and internal to the observer. In this\nstudy we investigate the dynamic properties of psychophysical responses to a\ncontinuous stream of visual near-threshold detection tasks. We manipulate the\nincoming signals to have temporal structures with various characteristic\ntimescales. Responses of human observers to these signals are analyzed using\ntools that highlight their dynamical features as well.\n  We find that two opposing biases shape perception, and operate over distinct\ntimescales. Positive recency appears over short times, e.g. consecutive trials.\nAdaptation, entailing an increased probability of changed response, reflects\ntrends over longer times. Analysis of psychometric curves conditioned on\nvarious temporal events reveals that the balance between the two biases can\nshift depending on their interplay with the temporal properties of the input\nsignal. A simple mathematical model reproduces the experimental data in all\nstimulus regimes. Taken together, our results support the view that visual\nresponse fluctuations reflect complex internal dynamics, possibly related to\nhigher cognitive processes.\n", "versions": [{"version": "v1", "created": "Mon, 9 Apr 2018 09:46:36 GMT"}], "update_date": "2020-07-01", "authors_parsed": [["Gordon", "Urit", ""], ["Marom", "Shimon", ""], ["Brenner", "Naama", ""]]}, {"id": "1804.02952", "submitter": "J.H. van Hateren", "authors": "J. H. van Hateren", "title": "A theory of consciousness: computation, algorithm, and neurobiological\n  realization", "comments": "minor revision, 21 pages, 10 figures, 1 table", "journal-ref": "Biological Cybernetics 113, 357-372 (2019)", "doi": "10.1007/s00422-019-00803-y", "report-no": null, "categories": "q-bio.NC cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The most enigmatic aspect of consciousness is the fact that it is felt, as a\nsubjective sensation. The theory proposed here aims to explain this particular\naspect. The theory encompasses both the computation that is presumably involved\nand the way in which that computation may be realized in the brain's\nneurobiology. It is assumed that the brain makes an internal estimate of an\nindividual's own evolutionary fitness, which can be shown to produce a special,\ndistinct form of causation. Communicating components of the fitness estimate\n(either for external or internal use) requires inverting them. Such inversion\ncan be performed by the thalamocortical feedback loop in the mammalian brain,\nif that loop is operating in a switched, dual-stage mode. A first\n(nonconscious) stage produces forward estimates, whereas the second (conscious)\nstage inverts those estimates. It is argued that inversion produces another\nspecial, distinct form of causation, which is spatially localized and is\nplausibly sensed as the feeling of consciousness.\n", "versions": [{"version": "v1", "created": "Mon, 9 Apr 2018 13:02:35 GMT"}, {"version": "v2", "created": "Wed, 1 Aug 2018 09:44:18 GMT"}, {"version": "v3", "created": "Mon, 21 Jan 2019 10:39:29 GMT"}, {"version": "v4", "created": "Thu, 20 Jun 2019 09:00:48 GMT"}], "update_date": "2019-07-29", "authors_parsed": [["van Hateren", "J. H.", ""]]}, {"id": "1804.03142", "submitter": "Alexander  Mathis", "authors": "Alexander Mathis, Pranav Mamidanna, Taiga Abe, Kevin M. Cury,\n  Venkatesh N. Murthy, Mackenzie W. Mathis and Matthias Bethge", "title": "Markerless tracking of user-defined features with deep learning", "comments": "Videos at http://www.mousemotorlab.org/deeplabcut", "journal-ref": "Nature Neuroscience, Technical Report, published: 20 August 2018", "doi": null, "report-no": null, "categories": "cs.CV q-bio.NC q-bio.QM stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Quantifying behavior is crucial for many applications in neuroscience.\nVideography provides easy methods for the observation and recording of animal\nbehavior in diverse settings, yet extracting particular aspects of a behavior\nfor further analysis can be highly time consuming. In motor control studies,\nhumans or other animals are often marked with reflective markers to assist with\ncomputer-based tracking, yet markers are intrusive (especially for smaller\nanimals), and the number and location of the markers must be determined a\npriori. Here, we present a highly efficient method for markerless tracking\nbased on transfer learning with deep neural networks that achieves excellent\nresults with minimal training data. We demonstrate the versatility of this\nframework by tracking various body parts in a broad collection of experimental\nsettings: mice odor trail-tracking, egg-laying behavior in drosophila, and\nmouse hand articulation in a skilled forelimb task. For example, during the\nskilled reaching behavior, individual joints can be automatically tracked (and\na confidence score is reported). Remarkably, even when a small number of frames\nare labeled ($\\approx 200$), the algorithm achieves excellent tracking\nperformance on test frames that is comparable to human accuracy.\n", "versions": [{"version": "v1", "created": "Mon, 9 Apr 2018 17:10:39 GMT"}], "update_date": "2018-08-24", "authors_parsed": [["Mathis", "Alexander", ""], ["Mamidanna", "Pranav", ""], ["Abe", "Taiga", ""], ["Cury", "Kevin M.", ""], ["Murthy", "Venkatesh N.", ""], ["Mathis", "Mackenzie W.", ""], ["Bethge", "Matthias", ""]]}, {"id": "1804.03190", "submitter": "Hosein M. Golshan", "authors": "Hosein M. Golshan, Adam O. Hebb, Joshua Nedrud, Mohammad H. Mahoor", "title": "Studying the Effects of Deep Brain Stimulation and Medication on the\n  Dynamics of STN-LFP Signals for Human Behavior Analysis", "comments": "40th IEEE International Conference on Engineering in Medicine and\n  Biology (IEEE EMBC), Honolulu, Hawaii, July 17-21, 2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "q-bio.NC cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper presents the results of our recent work on studying the effects of\ndeep brain stimulation (DBS) and medication on the dynamics of brain local\nfield potential (LFP) signals used for behavior analysis of patients with\nParkinson s disease (PD). DBS is a technique used to alleviate the severe\nsymptoms of PD when pharmacotherapy is not very effective. Behavior recognition\nfrom the LFP signals recorded from the subthalamic nucleus (STN) has\napplication in developing closed-loop DBS systems, where the stimulation pulse\nis adaptively generated according to subjects performing behavior. Most of the\nexisting studies on behavior recognition that use STN-LFPs are based on the DBS\nbeing off. This paper discovers how the performance and accuracy of automated\nbehavior recognition from the LFP signals are affected under different\nparadigms of stimulation on/off. We first study the notion of beta power\nsuppression in LFP signals under different scenarios (stimulation on/off and\nmedication on/off). Afterward, we explore the accuracy of support vector\nmachines in predicting human actions (button press and reach) using the\nspectrogram of STN-LFP signals. Our experiments on the recorded LFP signals of\nthree subjects confirm that the beta power is suppressed significantly when the\npatients take medication (p-value<0.002) or stimulation (p-value<0.0003). The\nresults also show that we can classify different behaviors with a reasonable\naccuracy of 85% even when the high-amplitude stimulation is applied.\n", "versions": [{"version": "v1", "created": "Mon, 9 Apr 2018 19:10:31 GMT"}], "update_date": "2018-04-11", "authors_parsed": [["Golshan", "Hosein M.", ""], ["Hebb", "Adam O.", ""], ["Nedrud", "Joshua", ""], ["Mahoor", "Mohammad H.", ""]]}, {"id": "1804.03269", "submitter": "Richard Spinney", "authors": "Richard E. Spinney, Joseph T. Lizier", "title": "Characterising information-theoretic storage and transfer in continuous\n  time processes", "comments": "25 pages, 2 figures", "journal-ref": "Phys. Rev. E 98, 012314 (2018)", "doi": "10.1103/PhysRevE.98.012314", "report-no": null, "categories": "cs.IT math.IT physics.data-an q-bio.NC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The characterisation of information processing is an important task in\ncomplex systems science. Information dynamics is a quantitative methodology for\nmodelling the intrinsic information processing conducted by a process\nrepresented as a time series, but to date has only been formulated in discrete\ntime. Building on previous work which demonstrated how to formulate transfer\nentropy in continuous time, we give a total account of information processing\nin this setting, incorporating information storage. We find that a convergent\nrate of predictive capacity, comprised of the transfer entropy and active\ninformation storage, does not exist, arising through divergent rates of active\ninformation storage. We identify that active information storage can be\ndecomposed into two separate quantities that characterise predictive capacity\nstored in a process: active memory utilisation and instantaneous predictive\ncapacity. The latter involves prediction related to path regularity and so\nsolely inherits the divergent properties of the active information storage,\nwhilst the former permits definitions of pathwise and rate quantities. We\nformulate measures of memory utilisation for jump and neural spiking processes\nand illustrate measures of information processing in synthetic neural spiking\nmodels and coupled Ornstein-Uhlenbeck models. The application to synthetic\nneural spiking models demonstrates that active memory utilisation for point\nprocesses consists of discontinuous jump contributions (at spikes) interrupting\na continuously varying contribution (relating to waiting times between spikes),\ncomplementing the behaviour previously demonstrated for transfer entropy in\nthese processes.\n", "versions": [{"version": "v1", "created": "Mon, 9 Apr 2018 23:08:52 GMT"}], "update_date": "2018-08-01", "authors_parsed": [["Spinney", "Richard E.", ""], ["Lizier", "Joseph T.", ""]]}, {"id": "1804.03441", "submitter": "Andrea Biagioni", "authors": "Roberto Ammendola, Andrea Biagioni, Fabrizio Capuani, Paolo Cretaro,\n  Giulia De Bonis, Francesca Lo Cicero, Alessandro Lonardo, Michele Martinelli,\n  Pier Stanislao Paolucci, Elena Pastorelli, Luca Pontisso, Francesco Simula,\n  Piero Vicini", "title": "The Brain on Low Power Architectures - Efficient Simulation of Cortical\n  Slow Waves and Asynchronous States", "comments": null, "journal-ref": "(2018) Advances in Parallel Computing, 32, pp. 760-769", "doi": "10.3233/978-1-61499-843-3-760", "report-no": null, "categories": "cs.DC cs.NE q-bio.NC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Efficient brain simulation is a scientific grand challenge, a\nparallel/distributed coding challenge and a source of requirements and\nsuggestions for future computing architectures. Indeed, the human brain\nincludes about 10^15 synapses and 10^11 neurons activated at a mean rate of\nseveral Hz. Full brain simulation poses Exascale challenges even if simulated\nat the highest abstraction level. The WaveScalES experiment in the Human Brain\nProject (HBP) has the goal of matching experimental measures and simulations of\nslow waves during deep-sleep and anesthesia and the transition to other brain\nstates. The focus is the development of dedicated large-scale\nparallel/distributed simulation technologies. The ExaNeSt project designs an\nARM-based, low-power HPC architecture scalable to million of cores, developing\na dedicated scalable interconnect system, and SWA/AW simulations are included\namong the driving benchmarks. At the joint between both projects is the INFN\nproprietary Distributed and Plastic Spiking Neural Networks (DPSNN) simulation\nengine. DPSNN can be configured to stress either the networking or the\ncomputation features available on the execution platforms. The simulation\nstresses the networking component when the neural net - composed by a\nrelatively low number of neurons, each one projecting thousands of synapses -\nis distributed over a large number of hardware cores. When growing the number\nof neurons per core, the computation starts to be the dominating component for\nshort range connections. This paper reports about preliminary performance\nresults obtained on an ARM-based HPC prototype developed in the framework of\nthe ExaNeSt project. Furthermore, a comparison is given of instantaneous power,\ntotal energy consumption, execution time and energetic cost per synaptic event\nof SWA/AW DPSNN simulations when executed on either ARM- or Intel-based server\nplatforms.\n", "versions": [{"version": "v1", "created": "Tue, 10 Apr 2018 10:39:25 GMT"}], "update_date": "2018-04-11", "authors_parsed": [["Ammendola", "Roberto", ""], ["Biagioni", "Andrea", ""], ["Capuani", "Fabrizio", ""], ["Cretaro", "Paolo", ""], ["De Bonis", "Giulia", ""], ["Cicero", "Francesca Lo", ""], ["Lonardo", "Alessandro", ""], ["Martinelli", "Michele", ""], ["Paolucci", "Pier Stanislao", ""], ["Pastorelli", "Elena", ""], ["Pontisso", "Luca", ""], ["Simula", "Francesco", ""], ["Vicini", "Piero", ""]]}, {"id": "1804.03588", "submitter": "Simona Olmi", "authors": "Simona Olmi, Spase Petkoski, Maxime Guye, Fabrice Bartolomei and\n  Viktor Jirsa", "title": "Controlling seizure propagation in large-scale brain networks", "comments": null, "journal-ref": null, "doi": "10.1371/journal.pcbi.1006805", "report-no": null, "categories": "q-bio.NC cond-mat.dis-nn nlin.AO nlin.CD", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Information transmission in the human brain is a fundamentally dynamic\nnetwork process. In partial epilepsy, this process is perturbed and highly\nsynchronous seizures originate in a local network, the so-called epileptogenic\nzone (EZ), before recruiting other close or distant brain regions. We studied\npatient-specific brain network models of 15 drug-resistant epilepsy patients\nwith implanted stereotactic electroencephalography (SEEG) electrodes. Each\npersonalized brain model was derived from structural data of magnetic resonance\nimaging (MRI) and diffusion tensor weighted imaging (DTI), comprising 88 nodes\nequipped with region specific neural mass models capable of demonstrating a\nrange of epileptiform discharges. Each patients virtual brain was further\npersonalized through the integration of the clinically hypothesized EZ.\nSubsequent simulations and connectivity modulations were performed and\nuncovered a finite repertoire of seizure propagation patterns. Across patients,\nwe found that (i) patient-specific network connectivity is predictive for the\nsubsequent seizure propagation pattern; (ii)seizure propagation is\ncharacterized by a systematic sequence of brain states; (iii) propagation can\nbe controlled by an optimal intervention on the connectivity matrix; (iv) the\ndegree of invasiveness can be significantly reduced via the here proposed\nseizure control as compared to traditional resective surgery. To stop seizures,\nneurosurgeons typically resect the EZ completely. We showed that stability\nanalysis of the network dynamics using graph theoretical metrics estimates\nreliably the spatiotemporal properties of seizure propagation. This suggests\nnovel less invasive paradigms of surgical interventions to treat and manage\npartial epilepsy.\n", "versions": [{"version": "v1", "created": "Tue, 10 Apr 2018 15:28:01 GMT"}], "update_date": "2019-06-19", "authors_parsed": [["Olmi", "Simona", ""], ["Petkoski", "Spase", ""], ["Guye", "Maxime", ""], ["Bartolomei", "Fabrice", ""], ["Jirsa", "Viktor", ""]]}, {"id": "1804.04324", "submitter": "Makoto Naruse", "authors": "Makoto Naruse, Eiji Yamamoto, Takashi Nakao, Takuma Akimoto, Hayato\n  Saigo, Kazuya Okamura, Izumi Ojima, Georg Northoff, Hirokazu Hori", "title": "Local reservoir model for choice-based learning", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG physics.data-an physics.optics q-bio.NC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Decision making based on behavioral and neural observations of living systems\nhas been extensively studied in brain science, psychology, and other\ndisciplines. Decision-making mechanisms have also been experimentally\nimplemented in physical processes, such as single photons and chaotic lasers.\nThe findings of these experiments suggest that there is a certain common basis\nin describing decision making, regardless of its physical realizations. In this\nstudy, we propose a local reservoir model to account for choice-based learning\n(CBL). CBL describes decision consistency as a phenomenon where making a\ncertain decision increases the possibility of making that same decision again\nlater, which has been intensively investigated in neuroscience, psychology,\netc. Our proposed model is inspired by the viewpoint that a decision is\naffected by its local environment, which is referred to as a local reservoir.\nIf the size of the local reservoir is large enough, consecutive decision making\nwill not be affected by previous decisions, thus showing lower degrees of\ndecision consistency in CBL. In contrast, if the size of the local reservoir\ndecreases, a biased distribution occurs within it, which leads to higher\ndegrees of decision consistency in CBL. In this study, an analytical approach\non local reservoirs is presented, as well as several numerical demonstrations.\nFurthermore, a physical architecture for CBL based on single photons is\ndiscussed, and the effects of local reservoirs is numerically demonstrated.\nDecision consistency in human decision-making tasks and in recruiting empirical\ndata are evaluated based on local reservoir. In summary, the proposed local\nreservoir model paves a path toward establishing a foundation for computational\nmechanisms and the systematic analysis of decision making on different levels.\n", "versions": [{"version": "v1", "created": "Thu, 12 Apr 2018 05:35:14 GMT"}], "update_date": "2018-04-13", "authors_parsed": [["Naruse", "Makoto", ""], ["Yamamoto", "Eiji", ""], ["Nakao", "Takashi", ""], ["Akimoto", "Takuma", ""], ["Saigo", "Hayato", ""], ["Okamura", "Kazuya", ""], ["Ojima", "Izumi", ""], ["Northoff", "Georg", ""], ["Hori", "Hirokazu", ""]]}, {"id": "1804.04538", "submitter": "Priya  Ranjan", "authors": "Anju Mishra, Shanu Sharma, Sanjay Kumar, Priya Ranjan, and Amit\n  Ujlayan", "title": "Automated Classification of Hand-grip action on Objects using Machine\n  Learning", "comments": "This is a report on an ongoing project", "journal-ref": null, "doi": null, "report-no": null, "categories": "q-bio.NC cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Brain computer interface is the current area of research to provide\nassistance to disabled persons. To cope up with the growing needs of BCI\napplications, this paper presents an automated classification scheme for\nhandgrip actions on objects by using Electroencephalography (EEG) data. The\npresented approach focuses on investigation of classifying correct and\nincorrect handgrip responses for objects by using EEG recorded patterns. The\nmethod starts with preprocessing of data, followed by extraction of relevant\nfeatures from the epoch data in the form of discrete wavelet transform (DWT),\nand entropy measures. After computing feature vectors, artificial neural\nnetwork classifiers used to classify the patterns into correct and incorrect\nhandgrips on different objects. The proposed method was tested on real dataset,\nwhich contains EEG recordings from 14 persons. The results showed that the\nproposed approach is effective and may be useful to develop a variety of BCI\nbased devices to control hand movements.\n", "versions": [{"version": "v1", "created": "Fri, 9 Mar 2018 11:51:43 GMT"}], "update_date": "2018-04-13", "authors_parsed": [["Mishra", "Anju", ""], ["Sharma", "Shanu", ""], ["Kumar", "Sanjay", ""], ["Ranjan", "Priya", ""], ["Ujlayan", "Amit", ""]]}, {"id": "1804.04604", "submitter": "Daniel Harari", "authors": "Daniel Harari, Joshua B. Tenenbaum and Shimon Ullman", "title": "Discovery and usage of joint attention in images", "comments": "6 pages, 3 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI cs.RO q-bio.NC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Joint visual attention is characterized by two or more individuals looking at\na common target at the same time. The ability to identify joint attention in\nscenes, the people involved, and their common target, is fundamental to the\nunderstanding of social interactions, including others' intentions and goals.\nIn this work we deal with the extraction of joint attention events, and the use\nof such events for image descriptions. The work makes two novel contributions.\nFirst, our extraction algorithm is the first which identifies joint visual\nattention in single static images. It computes 3D gaze direction, identifies\nthe gaze target by combining gaze direction with a 3D depth map computed for\nthe image, and identifies the common gaze target. Second, we use a human study\nto demonstrate the sensitivity of humans to joint attention, suggesting that\nthe detection of such a configuration in an image can be useful for\nunderstanding the image, including the goals of the agents and their joint\nactivity, and therefore can contribute to image captioning and related tasks.\n", "versions": [{"version": "v1", "created": "Tue, 10 Apr 2018 07:04:19 GMT"}], "update_date": "2018-04-13", "authors_parsed": [["Harari", "Daniel", ""], ["Tenenbaum", "Joshua B.", ""], ["Ullman", "Shimon", ""]]}, {"id": "1804.04748", "submitter": "Rodrigo Felipe De Oliveira Pena", "authors": "Rodrigo F.O. Pena, Vinicius Lima, Renan O. Shimoura, Cesar C.\n  Ceballos, Horacio G. Rotstein, and Antonio C. Roque", "title": "Asymmetrical voltage response in resonant neurons shaped by\n  nonlinearities", "comments": "15 pages, 15 figures", "journal-ref": "Chaos 29, 103135 (2019)", "doi": "10.1063/1.5110033", "report-no": null, "categories": "q-bio.NC q-bio.QM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The conventional impedance profile of a neuron can identify the presence of\nresonance and other properties of the neuronal response to oscillatory inputs,\nsuch as nonlinear response amplifications, but it cannot distinguish other\nnonlinear properties such as asymmetries in the shape of the voltage response\nenvelope. Experimental observations have shown that the response of neurons to\noscillatory inputs preferentially enhances either the upper or lower part of\nthe voltage envelope in different frequency bands. These asymmetric voltage\nresponses arise in a neuron model when it is submitted to high enough amplitude\noscillatory currents of variable frequencies. We show how the nonlinearities\nassociated to different ionic currents or present in the model as captured by\nits voltage equation lead to asymmetrical response and how high amplitude\noscillatory currents emphasize this response. We propose a geometrical\nexplanation for the phenomenon where asymmetries result not only from\nnonlinearities in their activation curves but also from nonlinearites captured\nby the nullclines in the phase-plane diagram and from the system's time-scale\nseparation. In addition, we identify an unexpected frequency-dependent pattern\nwhich develops in the gating variables of these currents and is a product of\nstrong nonlinearities in the system as we show by controlling such behavior by\nmanipulating the activation curve parameters. The results reported in this\npaper shed light on the ionic mechanisms by which brain embedded neurons\nprocess oscillatory information.\n", "versions": [{"version": "v1", "created": "Thu, 12 Apr 2018 23:30:09 GMT"}, {"version": "v2", "created": "Sat, 28 Apr 2018 02:51:40 GMT"}, {"version": "v3", "created": "Tue, 14 May 2019 00:49:26 GMT"}, {"version": "v4", "created": "Fri, 4 Oct 2019 16:11:48 GMT"}, {"version": "v5", "created": "Fri, 18 Oct 2019 14:49:18 GMT"}, {"version": "v6", "created": "Wed, 23 Oct 2019 13:41:58 GMT"}], "update_date": "2019-10-24", "authors_parsed": [["Pena", "Rodrigo F. O.", ""], ["Lima", "Vinicius", ""], ["Shimoura", "Renan O.", ""], ["Ceballos", "Cesar C.", ""], ["Rotstein", "Horacio G.", ""], ["Roque", "Antonio C.", ""]]}, {"id": "1804.05266", "submitter": "Daniel Larremore", "authors": "Vidit Agrawal, Andrew B. Cowley, Qusay Alfaori, Juan G. Restrepo,\n  Daniel B. Larremore, Woodrow L. Shew", "title": "Robust entropy requires strong and balanced excitatory and inhibitory\n  synapses", "comments": null, "journal-ref": null, "doi": "10.1063/1.5043429", "report-no": null, "categories": "q-bio.NC cond-mat.dis-nn", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  It is widely appreciated that well-balanced excitation and inhibition are\nnecessary for proper function in neural networks. However, in principle, such\nbalance could be achieved by many possible configurations of excitatory and\ninhibitory strengths, and relative numbers of excitatory and inhibitory\nneurons. For instance, a given level of excitation could be balanced by either\nnumerous inhibitory neurons with weak synapses, or few inhibitory neurons with\nstrong synapses. Among the continuum of different but balanced configurations,\nwhy should any particular configuration be favored? Here we address this\nquestion in the context of the entropy of network dynamics by studying an\nanalytically tractable network of binary neurons. We find that entropy is\nhighest at the boundary between excitation-dominant and inhibition-dominant\nregimes. Entropy also varies along this boundary with a trade-off between high\nand robust entropy: weak synapse strengths yield high network entropy which is\nfragile to parameter variations, while strong synapse strengths yield a lower,\nbut more robust, network entropy. In the case where inhibitory and excitatory\nsynapses are constrained to have similar strength, we find that a small, but\nnon-zero fraction of inhibitory neurons, like that seen in mammalian cortex,\nresults in robust and relatively high entropy.\n", "versions": [{"version": "v1", "created": "Sat, 14 Apr 2018 19:08:00 GMT"}], "update_date": "2018-11-14", "authors_parsed": [["Agrawal", "Vidit", ""], ["Cowley", "Andrew B.", ""], ["Alfaori", "Qusay", ""], ["Restrepo", "Juan G.", ""], ["Larremore", "Daniel B.", ""], ["Shew", "Woodrow L.", ""]]}, {"id": "1804.05695", "submitter": "Rolf Bader", "authors": "Rolf Bader, Robert Mores", "title": "Cochlear detection of double-slip motion in cello bowing", "comments": "9 pages, 7 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "q-bio.NC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A double-slip motion of a cello sound is investigated experimentally with a\nbowing machine and analyzed using a Finite-Difference Time Domain (FDTD)\ncochlear model. A double-slip sound is investigated. Here the sawtooth motion\nof normal bowing is basically present, but within each period the bow hair\ntears off the strings once more within the period, resulting in a blurred\nsound. This additional intermediate slip appears around the middle of each\nperiod and drifts temporally around while the sound progresses. When the\ndouble-slip is perfectly in the middle of one period the sound is that of a\nregular sawtooth motion. If not, two periodicities are present around double\nthe fundamental periodicity, making the sound arbitrary. Analyzing the sound\nwith a Wavelet-transform, the expected double-peak of two periodicities around\nthe second partial cannot be found. Analyzing the tone with a cochlear FDTD\nmodel including the transfer of mechanical energy into spikes, the doubling and\neven more complex behaviour is perfectly represented in the Interspike Interval\n(ISI) of two adjacent spikes. This cochlear spike representation fits perfectly\nto an amplitude peak detection algorithm, tracking the precise time point of\nthe double-slip within the fundamental period. Therefore the ear is able to\ndetect the double-slip motion right at the transition from the basilar membrane\nmotion into electrical spikes.\n", "versions": [{"version": "v1", "created": "Mon, 16 Apr 2018 14:10:57 GMT"}], "update_date": "2018-04-17", "authors_parsed": [["Bader", "Rolf", ""], ["Mores", "Robert", ""]]}, {"id": "1804.05766", "submitter": "Kevin Brown", "authors": "Kevin S. Brown and Paul D. Allopenna and William R. Hunt and Rachael\n  Steiner and Elliot Saltzman and Ken McRae and James S. Magnuson", "title": "Universal Features in Phonological Neighbor Networks", "comments": null, "journal-ref": null, "doi": "10.3390/e20070526", "report-no": null, "categories": "q-bio.NC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Human speech perception involves transforming a countinous acoustic signal\ninto discrete linguistically meaningful units, such as phonemes, while\nsimultaneously causing a listener to activate words that are similar to the\nspoken utterance and to each other. The Neighborhood Activation Model\n(NAM~\\cite{Luce:1986,Luce:1998}) posits that phonological neighbors (two forms\n[words] that differ by one phoneme) compete significantly for recognition as a\nspoken word is heard. This definition of phonological similarity can be\nextended to an entire corpus of forms to produce a phonological neighbor\nnetwork~\\cite{Vitevitch:2008} (PNN). We study PNNs for five languages: English,\nSpanish, French, Dutch, and German. Consistent with previous work, we find that\nthe PNNs share a consistent set of topological features. Using an approach that\ngenerates random lexicons with increasing levels of phonological realism, we\nshow that even random forms with minimal relationship to any real language,\ncombined with only the empirical distribution of language-specific phonological\nform lengths, are sufficient to produce the topological properties observed in\nthe real language PNNs. The resulting pseudo-PNNs are insensitive to the level\nof lingusitic realism in the random lexicons but quite sensitive to the shape\nof the form length distribution. We therefore conclude that \"universal\"\nfeatures seen across multiple languages are really string universals, not\nlanguage universals, and arise primarily due to limitations in the kinds of\nnetworks generated by the one-step neighbor definition. Taken together, our\nresults indicate that caution is warranted when linking the dynamics of human\nspoken word recognition to the topological properties of PNNs, and that the\ninvestigation of alternative similarity metrics for phonological forms should\nbe a priority.\n", "versions": [{"version": "v1", "created": "Mon, 16 Apr 2018 16:10:00 GMT"}], "update_date": "2018-08-01", "authors_parsed": [["Brown", "Kevin S.", ""], ["Allopenna", "Paul D.", ""], ["Hunt", "William R.", ""], ["Steiner", "Rachael", ""], ["Saltzman", "Elliot", ""], ["McRae", "Ken", ""], ["Magnuson", "James S.", ""]]}, {"id": "1804.05823", "submitter": "Rodrigo Rocha Pereira", "authors": "Rodrigo P. Rocha, Loren Ko\\c{c}illari, Samir Suweis, Maurizio\n  Corbetta, and Amos Maritan", "title": "Homeostatic plasticity and emergence of functional networks in a\n  whole-brain model at criticality", "comments": "Accepted for publication in Scientific Reports", "journal-ref": "Scientific Reports 8, 15682 (2018)", "doi": "10.1038/s41598-018-33923-9", "report-no": null, "categories": "q-bio.NC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Understanding the relationship between large-scale structural and functional\nbrain networks remains a crucial issue in modern neuroscience. Recently, there\nhas been growing interest in investigating the role of homeostatic plasticity\nmechanisms, across different spatiotemporal scales, in regulating network\nactivity and brain functioning against a wide range of environmental conditions\nand brain states (e.g., during learning, development, ageing, neurological\ndiseases). In the present study, we investigate how the inclusion of\nhomeostatic plasticity in a stochastic whole-brain model, implemented as a\nnormalization of the incoming node's excitatory input, affects the macroscopic\nactivity during rest and the formation of functional networks. Importantly, we\naddress the structure-function relationship both at the group and\nindividual-based levels. In this work, we show that normalization of the node's\nexcitatory input improves the correspondence between simulated neural patterns\nof the model and various brain functional data. Indeed, we find that the best\nmatch is achieved when the model control parameter is in its critical value and\nthat normalization minimizes both the variability of the critical points and\nneuronal activity patterns among subjects. Therefore, our results suggest that\nthe inclusion of homeostatic principles lead to more realistic brain activity\nconsistent with the hallmarks of criticality. Our theoretical framework open\nnew perspectives in personalized brain modeling with potential applications to\ninvestigate the deviation from criticality due to structural lesions (e.g.\nstroke) or brain disorders.\n", "versions": [{"version": "v1", "created": "Mon, 16 Apr 2018 17:46:59 GMT"}, {"version": "v2", "created": "Thu, 27 Sep 2018 15:48:01 GMT"}], "update_date": "2018-10-25", "authors_parsed": [["Rocha", "Rodrigo P.", ""], ["Ko\u00e7illari", "Loren", ""], ["Suweis", "Samir", ""], ["Corbetta", "Maurizio", ""], ["Maritan", "Amos", ""]]}, {"id": "1804.05964", "submitter": "Jesus Malo", "authors": "Jesus Malo and Marcelo Bertalmio", "title": "Appropriate kernels for Divisive Normalization explained by Wilson-Cowan\n  equations", "comments": "MODVIS-18 and Celebration of Cowan's 50th anniv. at Univ. Chicago", "journal-ref": null, "doi": null, "report-no": null, "categories": "q-bio.NC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The interaction between wavelet-like sensors in Divisive Normalization is\nclassically described through Gaussian kernels that decay with spatial\ndistance, angular distance and frequency distance. However, simultaneous\nexplanation of (a) distortion perception in natural image databases and (b)\ncontrast perception of artificial stimuli requires very specific modifications\nin classical Divisive Normalization. First, the wavelet response has to be\nhigh-pass filtered before the Gaussian interaction is applied. Then, distinct\nweights per subband are also required after the Gaussian interaction. In\nsummary, the classical Gaussian kernel has to be left- and right-multiplied by\ntwo extra diagonal matrices.\n  In this paper we provide a lower-level justification for this specific\nempirical modification required in the Gaussian kernel of Divisive\nNormalization. Here we assume that the psychophysical behavior described by\nDivisive Normalization comes from neural interactions following the\nWilson-Cowan equations. In particular, we identify the Divisive Normalization\nresponse with the stationary regime of a Wilson-Cowan model. From this\nidentification we derive an expression for the Divisive Normalization kernel in\nterms of the interaction kernel of the Wilson-Cowan equations. It turns out\nthat the Wilson-Cowan kernel is left- and-right multiplied by diagonal matrices\nwith high-pass structure. In conclusion, symmetric Gaussian inhibitory\nrelations between wavelet-like sensors wired in the lower-level Wilson-Cowan\nmodel lead to the appropriate non-symmetric kernel that has to be empirically\nincluded in Divisive Normalization to explain a wider range of phenomena.\n", "versions": [{"version": "v1", "created": "Mon, 16 Apr 2018 22:28:58 GMT"}], "update_date": "2018-04-18", "authors_parsed": [["Malo", "Jesus", ""], ["Bertalmio", "Marcelo", ""]]}, {"id": "1804.05994", "submitter": "James Hope Mr", "authors": "J. Hope, F. Vanholsbeeck, A. McDaid", "title": "Drive and measurement electrode patterns for electrode impedance\n  tomography (EIT) imaging of neural activity in peripheral nerve", "comments": "10 pages, 5 figures. Short technical article (e.g. Note)", "journal-ref": "Hope, J., Vanholsbeeck, F., & McDaid, A. (2018). Drive and\n  measurement electrode patterns for electrode impedance tomography (EIT)\n  imaging of neural activity in peripheral nerve. Biomedical Physics and\n  Engineering Express, 4 (6)", "doi": "10.1088/2057-1976/aadff3", "report-no": null, "categories": "q-bio.NC physics.med-ph", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Objective: To establish the performance of several drive and measurement\npatterns in EIT imaging of neural activity in peripheral nerve, which involves\nlarge impedance change in the nerve's anisotropic length axis. Approach: Eight\ndrive and measurement electrode patterns are compared using a finite element\n(FE) four cylindrical shell model of a peripheral nerve and a 32 channel\ndual-ring nerve cuff. The central layer of the FE model contains impedance\nchanges representative of neural activity of -0.3 in the length axis and -8.8 x\n10-4 in the radial axis. Four of the electrode patterns generate longitudinal\ndrive current, which runs perpendicular to the anisotropic axis. Main results:\nTransverse current patterns produce higher resolution than longitudinal\npatterns but are also more susceptible to noise and errors, and exhibit poorer\nsensitivity to impedance changes in central sample locations. Three of the four\nlongitudinal current patterns considered can reconstruct fascicle level\nimpedance changes with up to 0.2 mV noise and error, which corresponds to\nbetween -5.5 and +0.18 dB of the normalised signal standard deviation. Reducing\nthe spacing between the two electrode rings in all longitudinal current\npatterns reduced the signal to error ratio across all depth locations of the\nsample. Significance: Electrode patterns which target the large impedance\nchange in the anisotropic length axis can provide improved robustness against\nnoise and errors, which is a critical step towards real time EIT imaging of\nneural activity in peripheral nerve.\n", "versions": [{"version": "v1", "created": "Tue, 17 Apr 2018 00:46:02 GMT"}], "update_date": "2019-01-09", "authors_parsed": [["Hope", "J.", ""], ["Vanholsbeeck", "F.", ""], ["McDaid", "A.", ""]]}, {"id": "1804.06758", "submitter": "Jonathan Touboul", "authors": "Cristobal Qui\\~ninao, Jonathan D. Touboul", "title": "Clamping and Synchronization in the strongly coupled FitzHugh-Nagumo\n  model", "comments": "Added reference to the recent arxiv preprint [20]", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.AP q-bio.NC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We investigate the dynamics of a limit of interacting FitzHugh-Nagumo neurons\nin the regime of large interaction coefficients. We consider the dynamics\ndescribed by a mean-field model given by a nonlinear evolution partial\ndifferential equation representing the probability distribution of one given\nneuron in a large network. The case of weak connectivity previously studied\ndisplays a unique, exponentially stable, stationary solution. Here, we consider\nthe case of strong connectivities, and exhibit the presence of possibly\nnon-unique stationary behaviors or non-stationary behaviors. To this end, using\nHopf-Cole transformation, we demonstrate that the solutions exponentially\nconcentrate around a singular Dirac measure as the connectivity parameter\ndiverges, centered at the zeros of a time-dependent continuous function. We\nnext characterize the points at which this measure concentrates, and exhibit a\nparticular solution corresponding to a Dirac measure concentrated on a\ntime-dependent point satisfying an ordinary differential equation identical to\nthe original FitzHugh-Nagumo system. This solution may thus feature multiple\nstable fixed points or periodic orbits, respectively corresponding to a\nclumping of the whole system at rest, or a synchronization of cells on a\nperiodic solution. We illustrate these results with numerical simulations of\nneural networks with a relatively modest number of neurons and finite coupling\nstrength, and show that away from the bifurcations of the limit system, the\nasymptotic equation recovers the main properties of more realistic networks.\n", "versions": [{"version": "v1", "created": "Wed, 18 Apr 2018 14:31:35 GMT"}, {"version": "v2", "created": "Tue, 8 May 2018 14:47:19 GMT"}, {"version": "v3", "created": "Fri, 30 Nov 2018 15:54:58 GMT"}], "update_date": "2018-12-03", "authors_parsed": [["Qui\u00f1inao", "Cristobal", ""], ["Touboul", "Jonathan D.", ""]]}, {"id": "1804.07609", "submitter": "Gabriel Silva", "authors": "Gabriel A. Silva", "title": "The Effect of Signaling Latencies and Node Refractory States on the\n  Dynamics of Networks", "comments": "Version 3: Accepted version in press in Neural Computation", "journal-ref": null, "doi": null, "report-no": null, "categories": "q-bio.NC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We describe the construction and theoretical analysis of a framework derived\nfrom canonical neurophysiological principles that model the competing dynamics\nof incident signals into nodes along directed edges in a network. The framework\ndescribes the dynamics between the offset in the latencies of propagating\nsignals, which reflect the geometry of the edges and conduction velocities, and\nthe internal refractory dynamics and processing times of the downstream node\nreceiving the signals. This framework naturally extends to the construction of\na perceptron model that takes into account such dynamic geometric\nconsiderations. We first describe the model in detail, culminating with the\nmodel of a geometric dynamic perceptron. We then derive upper and lower bounds\nfor a notion of optimal efficient signaling between vertex pairs based on the\nstructure of the framework. Efficient signaling in the context of the framework\nwe develop here means that there needs to be a temporal match between the\narrival time of the signals relative to how quickly nodes can internally\nprocess signals. These bounds reflect numerical constraints on the compensation\nof the timing of signaling events of upstream nodes attempting to activate\ndownstream nodes they connect into that preserve this notion of efficiency.\nWhen a mismatch between signal arrival times and the internal states of\nactivated nodes occurs, it can cause a break down in the signaling dynamics of\nthe network. In contrast to essentially all the current state of the art in\nmachine learning, this work provides a theoretical foundation for machine\nlearning and intelligence architectures based on the timing of node activations\nand their abilities to respond, rather than necessary changes in synaptic\nweights. At the same time, this work is guiding the discovery of experimentally\ntestable new structure-function principles in the biological brain.\n", "versions": [{"version": "v1", "created": "Tue, 17 Apr 2018 22:31:29 GMT"}, {"version": "v2", "created": "Wed, 11 Jul 2018 05:34:07 GMT"}, {"version": "v3", "created": "Sun, 4 Aug 2019 17:23:34 GMT"}], "update_date": "2019-08-06", "authors_parsed": [["Silva", "Gabriel A.", ""]]}, {"id": "1804.07864", "submitter": "Jens Wilting", "authors": "Jens Wilting and Viola Priesemann", "title": "Between perfectly critical and fully irregular: a reverberating model\n  captures and predicts cortical spike propagation", "comments": "27 pages + supplementary information and supplementary figures", "journal-ref": "Cerebral Cortex (2019)", "doi": "10.1093/cercor/bhz049", "report-no": null, "categories": "q-bio.NC stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Knowledge about the collective dynamics of cortical spiking is very\ninformative about the underlying coding principles. However, even most basic\nproperties are not known with certainty, because their assessment is hampered\nby spatial subsampling, i.e. the limitation that only a tiny fraction of all\nneurons can be recorded simultaneously with millisecond precision. Building on\na novel, subsampling-invariant estimator, we fit and carefully validate a\nminimal model for cortical spike propagation. The model interpolates between\ntwo prominent states: asynchronous and critical. We find neither of them in\ncortical spike recordings across various species, but instead identify a narrow\nreverberating regime. This approach enables us to predict yet unknown\nproperties from very short recordings and for every circuit individually,\nincluding responses to minimal perturbations, intrinsic network timescales, and\nthe strength of external input compared to recurrent activation - thereby\ninforming about the underlying coding principles for each circuit, area, state\nand task.\n", "versions": [{"version": "v1", "created": "Fri, 20 Apr 2018 23:46:10 GMT"}, {"version": "v2", "created": "Tue, 12 Mar 2019 17:50:23 GMT"}], "update_date": "2019-03-13", "authors_parsed": [["Wilting", "Jens", ""], ["Priesemann", "Viola", ""]]}, {"id": "1804.08154", "submitter": "Yo Joong Choe", "authors": "Yo Joong Choe, Sivaraman Balakrishnan, Aarti Singh, Jean M. Vettel,\n  Timothy Verstynen", "title": "Local White Matter Architecture Defines Functional Brain Dynamics", "comments": "Accepted to the 2018 IEEE International Conference on Systems, Man,\n  and Cybernetics (SMC 2018)", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP q-bio.NC stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Large bundles of myelinated axons, called white matter, anatomically connect\ndisparate brain regions together and compose the structural core of the human\nconnectome. We recently proposed a method of measuring the local integrity\nalong the length of each white matter fascicle, termed the local connectome. If\ncommunication efficiency is fundamentally constrained by the integrity along\nthe entire length of a white matter bundle, then variability in the functional\ndynamics of brain networks should be associated with variability in the local\nconnectome. We test this prediction using two statistical approaches that are\ncapable of handling the high dimensionality of data. First, by performing\nstatistical inference on distance-based correlations, we show that similarity\nin the local connectome between individuals is significantly correlated with\nsimilarity in their patterns of functional connectivity. Second, by employing\nvariable selection using sparse canonical correlation analysis and\ncross-validation, we show that segments of the local connectome are predictive\nof certain patterns of functional brain dynamics. These results are consistent\nwith the hypothesis that structural variability along axon bundles constrains\ncommunication between disparate brain regions.\n", "versions": [{"version": "v1", "created": "Sun, 22 Apr 2018 18:46:08 GMT"}, {"version": "v2", "created": "Sun, 16 Sep 2018 12:33:36 GMT"}], "update_date": "2018-09-18", "authors_parsed": [["Choe", "Yo Joong", ""], ["Balakrishnan", "Sivaraman", ""], ["Singh", "Aarti", ""], ["Vettel", "Jean M.", ""], ["Verstynen", "Timothy", ""]]}, {"id": "1804.08404", "submitter": "Hartmut Grote", "authors": "Hartmut Grote", "title": "Commentary: Intentional Observer Effects on Quantum Randomness: A\n  Bayesian Analysis Reveals Evidence Against Micro-Psychokinesis", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "q-bio.NC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The paper titled `Intentional Observer Effects on Quantum Randomness: A\nBayesian Analysis Reveals Evidence Against Micro-Psychokinesis', published in\nFrontiers of Psychology in March 2018, reports on a mind-matter experiment with\nthe main result of strong evidence against Micro-Psychokinesis. Despite this\nconclusion, the authors interpret the observed pattern in their data as\npossible evidence for Micro-Psychokinesis, albeit of a different kind.\nSuggesting a connection to some existing models, the authors put forward the\nhypothesis that a higher frequency of slow data variations can be observed in\ntheir experiment data than in a set of control data. This commentary analyses\nthis claim and concludes that the variation in the data motivating this\nhypothesis would show up just by chance with a probability of p=0.328 under a\nnull hypothesis. Therefore, there is no evidence for the hypothesis of faster\ndata variations, and thus for this kind of suggested Micro-Psychokinesis in\nthis experiment.\n", "versions": [{"version": "v1", "created": "Thu, 5 Apr 2018 16:49:53 GMT"}], "update_date": "2018-04-24", "authors_parsed": [["Grote", "Hartmut", ""]]}, {"id": "1804.08513", "submitter": "Thomas Schmidt", "authors": "Thomas Schmidt, Filipp Schmidt", "title": "An accumulator model for primes and targets with independent response\n  activation rates: Basic equations for average response times", "comments": "Version 2: arXiv reference number added to title page", "journal-ref": null, "doi": null, "report-no": null, "categories": "q-bio.NC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In response priming tasks, speeded responses are performed toward target\nstimuli preceded by prime stimuli. Responses are slower and error rates are\nhigher when prime and target are assigned to different responses, compared to\nassignment to the same response, and those priming effects increase with\nprime-target SOA. Here, we generalize Vorberg et al.'s (2003) accumulator model\nof response priming, where response activation is first controlled exclusively\nby the prime and then taken over by the actual target. Priming thus occurs by\nmotor conflict because a response-inconsistent prime can temporarily drive the\nprocess towards the incorrect response. While the original model assumed prime\nand target signals to be identical in strength, we allow different rates of\nresponse activation (cf. Mattler & Palmer, 2012; Schubert et al., 2012). Our\nmodel predicts that stronger primes mainly increase priming effects in response\ntimes and error rates, whereas stronger targets mainly diminish response times\nand priming effects.\n", "versions": [{"version": "v1", "created": "Mon, 23 Apr 2018 15:38:21 GMT"}, {"version": "v2", "created": "Tue, 24 Apr 2018 09:16:36 GMT"}], "update_date": "2018-04-25", "authors_parsed": [["Schmidt", "Thomas", ""], ["Schmidt", "Filipp", ""]]}, {"id": "1804.08713", "submitter": "Catherine Reason", "authors": "Catherine M Reason", "title": "A Theoretical Limit to Physicalism: A Non-Technical Explanation of the\n  Gemini Theorem", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "q-bio.NC physics.hist-ph", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The Gemini theorem asserts that, given certain reasonable assumptions, no\nphysical system can be certainly aware of its own existence. The theorem can be\nproved algorithmically, but the proof of this theorem is somewhat obscure, and\nthere exists very little literature on it. The purpose of this article is to\nprovide a brief non-technical summary of the theorem and its proof, with a view\nto stimulating critical discussion of the proof and its implications. Since the\ntheorem implies that a violation of the conservation of energy will take place\nwithin the brains of conscious human beings, it has obvious implications for\nany physical theory.\n", "versions": [{"version": "v1", "created": "Tue, 10 Apr 2018 14:33:31 GMT"}], "update_date": "2018-04-25", "authors_parsed": [["Reason", "Catherine M", ""]]}, {"id": "1804.08940", "submitter": "Dominik Fischer", "authors": "Dominik Fischer, Sanaz Mostaghim, Larissa Albantakis", "title": "How swarm size during evolution impacts the behavior, generalizability,\n  and brain complexity of animats performing a spatial navigation task", "comments": "10 pages, 14 figures, GECCO 2018, July 15-19, 2018, Kyoto, Japan", "journal-ref": null, "doi": "10.1145/3205455.3205646", "report-no": null, "categories": "cs.NE nlin.AO q-bio.NC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  While it is relatively easy to imitate and evolve natural swarm behavior in\nsimulations, less is known about the social characteristics of simulated,\nevolved swarms, such as the optimal (evolutionary) group size, why individuals\nin a swarm perform certain actions, and how behavior would change in swarms of\ndifferent sizes. To address these questions, we used a genetic algorithm to\nevolve animats equipped with Markov Brains in a spatial navigation task that\nfacilitates swarm behavior. The animats' goal was to frequently cross between\ntwo rooms without colliding with other animats. Animats were evolved in swarms\nof various sizes. We then evaluated the task performance and social behavior of\nthe final generation from each evolution when placed with swarms of different\nsizes in order to evaluate their generalizability across conditions. According\nto our experiments, we find that swarm size during evolution matters: animats\nevolved in a balanced swarm developed more flexible behavior, higher fitness\nacross conditions, and, in addition, higher brain complexity.\n", "versions": [{"version": "v1", "created": "Tue, 24 Apr 2018 10:07:34 GMT"}], "update_date": "2018-04-25", "authors_parsed": [["Fischer", "Dominik", ""], ["Mostaghim", "Sanaz", ""], ["Albantakis", "Larissa", ""]]}, {"id": "1804.09667", "submitter": "Achim Schilling", "authors": "Richard Gerum, Hinrich Rahlfs, Matthias Streb, Patrick Krauss, Claus\n  Metzner, Konstantin Tziridis, Michael G\\\"unther, Holger Schulze, Walter\n  Kellermann, Achim Schilling", "title": "Open(G)PIAS: An open source solution for the construction of a\n  high-precision Acoustic-Startle-Response (ASR) setup for tinnitus screening\n  and threshold estimation in rodents", "comments": null, "journal-ref": null, "doi": "10.3389/fnbeh.2019.00140", "report-no": null, "categories": "q-bio.NC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The acoustic startle reflex (ASR) that can be induced by a loud sound\nstimulus can be used as a versatile tool to, e.g., estimate hearing thresholds\nor identify subjective tinnitus percepts in rodents. These techniques are based\non the fact that the ASR amplitude can be suppressed by a pre-stimulus of\nlower, non-startling intensity, an effect named pre-pulse inhibition (PPI). For\nhearing threshold estimation, pure tone pre-stimuli of varying amplitudes are\npresented before an intense noise burst serving as startle stimulus. The amount\nof suppression of the ASR amplitude as a function of the pre-stimulus intensity\ncan be used as a behavioral correlate to determine the hearing ability. For\ntinnitus assessment, the pure-tone pre-stimulus is replaced by a gap of silence\nin a narrowband noise background, a paradigm termed GPIAS (gap-pre-pulse\ninhibition of the acoustic startle response). A proper application of these\nparadigms depend on a reliable measurement of the ASR amplitudes, an exact\nstimulus presentation in terms of frequency and intensity. Here we introduce a\nnovel open source solution for the construction of a low-cost ASR setup for the\nabove mentioned purpose. The complete software for data acquisition and\nstimulus presentation is written in Python 3.6 and is provided as an anaconda\npackage. Furthermore, we provide a construction plan for the sensory system\nbased on low-cost hardware components. Exemplary data show that the ratios\n(1-PPI) of the pre and post trauma ASR amplitudes can be well described by a\nlognormal distribution being in good accordance to previous studies with\nalready established setups. Hence, the open access solution described here will\nhelp to further establish the ASR method in many laboratories and thus\nfacilitate and standardize research in animal models of tinnitus or hearing\nloss.\n", "versions": [{"version": "v1", "created": "Wed, 25 Apr 2018 16:31:53 GMT"}], "update_date": "2019-10-01", "authors_parsed": [["Gerum", "Richard", ""], ["Rahlfs", "Hinrich", ""], ["Streb", "Matthias", ""], ["Krauss", "Patrick", ""], ["Metzner", "Claus", ""], ["Tziridis", "Konstantin", ""], ["G\u00fcnther", "Michael", ""], ["Schulze", "Holger", ""], ["Kellermann", "Walter", ""], ["Schilling", "Achim", ""]]}, {"id": "1804.09739", "submitter": "Anna Kutschireiter", "authors": "Anna Kutschireiter, Jean-Pascal Pfister", "title": "Particle-filtering approaches for nonlinear Bayesian decoding of\n  neuronal spike trains", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "q-bio.NC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The number of neurons that can be simultaneously recorded doubles every seven\nyears. This ever increasing number of recorded neurons opens up the possibility\nto address new questions and extract higher dimensional stimuli from the\nrecordings. Modeling neural spike trains as point processes, this task of\nextracting dynamical signals from spike trains is commonly set in the context\nof nonlinear filtering theory. Particle filter methods relying on importance\nweights are generic algorithms that solve the filtering task numerically, but\nexhibit a serious drawback when the problem dimensionality is high: they are\nknown to suffer from the 'curse of dimensionality' (COD), i.e. the number of\nparticles required for a certain performance scales exponentially with the\nobservable dimensions. Here, we first briefly review the theory on filtering\nwith point process observations in continuous time. Based on this theory, we\ninvestigate both analytically and numerically the reason for the COD of\nweighted particle filtering approaches: Similarly to particle filtering with\ncontinuous-time observations, the COD with point-process observations is due to\nthe decay of effective number of particles, an effect that is stronger when the\nnumber of observable dimensions increases. Given the success of unweighted\nparticle filtering approaches in overcoming the COD for continuous- time\nobservations, we introduce an unweighted particle filter for point-process\nobservations, the spike-based Neural Particle Filter (sNPF), and show that it\nexhibits a similar favorable scaling as the number of dimensions grows.\nFurther, we derive rules for the parameters of the sNPF from a maximum\nlikelihood approach learning. We finally employ a simple decoding task to\nillustrate the capabilities of the sNPF and to highlight one possible future\napplication of our inference and learning algorithm.\n", "versions": [{"version": "v1", "created": "Wed, 25 Apr 2018 18:20:15 GMT"}], "update_date": "2018-04-27", "authors_parsed": [["Kutschireiter", "Anna", ""], ["Pfister", "Jean-Pascal", ""]]}, {"id": "1804.09844", "submitter": "Yevgenia Kozorovitskiy", "authors": "Manish Kumar, Sandeep Kishore, Jordan Nasenbeny, David McLean,\n  Yevgenia Kozorovitskiy", "title": "Integrated one- and two-photon scanned oblique plane illumination (SOPi)\n  microscopy for rapid volumetric imaging", "comments": "15 pages, 7 figures", "journal-ref": "26 (10), 13027-13041 (2018) Optics Express", "doi": "10.1364/OE.26.013027", "report-no": null, "categories": "q-bio.NC physics.bio-ph", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Versatile, sterically accessible imaging systems capable of in vivo rapid\nvolumetric functional and structural imaging deep in the brain continue to be a\nlimiting factor in neuroscience research. Towards overcoming this obstacle, we\npresent integrated one- and two-photon scanned oblique plane illumination\n(SOPi) microscopy which uses a single front-facing microscope objective to\nprovide light-sheet scanning based rapid volumetric imaging capability at\nsubcellular resolution. Our planar scan-mirror based optimized light-sheet\narchitecture allows for non-distorted scanning of volume samples, simplifying\naccurate reconstruction of the imaged volume. Integration of both one-photon\n(1P) and two-photon (2P) light-sheet microscopy in the same system allows for\neasy selection between rapid volumetric imaging and higher resolution imaging\nin scattering media. Using SOPi, we demonstrate deep, large volume imaging\ncapability inside scattering mouse brain sections and rapid imaging speeds up\nto 10 volumes per second in zebrafish larvae expressing genetically encoded\nfluorescent proteins GFP or GCaMP6s. SOPi flexibility and steric access makes\nit adaptable for numerous imaging applications and broadly compatible with\northogonal techniques for actuating or interrogating neuronal structure and\nactivity.\n", "versions": [{"version": "v1", "created": "Thu, 26 Apr 2018 00:46:29 GMT"}], "update_date": "2018-05-10", "authors_parsed": [["Kumar", "Manish", ""], ["Kishore", "Sandeep", ""], ["Nasenbeny", "Jordan", ""], ["McLean", "David", ""], ["Kozorovitskiy", "Yevgenia", ""]]}, {"id": "1804.10090", "submitter": "Siavash Ghavami", "authors": "Siavash Ghavami, Vahid Rahmati, Farshad Lahouti, Lars Schwabe", "title": "Neuronal Synchronization Can Control the Energy Efficiency of\n  Inter-Spike Interval Coding", "comments": "47 pages, 14 figures, 2 Tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "q-bio.NC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The role of synchronous firing in sensory coding and cognition remains\ncontroversial. While studies, focusing on its mechanistic consequences in\nattentional tasks, suggest that synchronization dynamically boosts sensory\nprocessing, others failed to find significant synchronization levels in such\ntasks. We attempt to understand both lines of evidence within a coherent\ntheoretical framework. We conceptualize synchronization as an independent\ncontrol parameter to study how the postsynaptic neuron transmits the average\nfiring activity of a presynaptic population, in the presence of\nsynchronization. We apply the Berger-Levy theory of energy efficient\ninformation transmission to interpret simulations of a Hodgkin-Huxley-type\npostsynaptic neuron model, where we varied the firing rate and synchronization\nlevel in the presynaptic population independently. We find that for a fixed\npresynaptic firing rate the simulated postsynaptic interspike interval\ndistribution depends on the synchronization level and is well-described by a\ngeneralized extreme value distribution. For synchronization levels of 15% to\n50%, we find that the optimal distribution of presynaptic firing rate,\nmaximizing the mutual information per unit cost, is maximized at ~30%\nsynchronization level. These results suggest that the statistics and energy\nefficiency of neuronal communication channels, through which the input rate is\ncommunicated, can be dynamically adapted by the synchronization level.\n", "versions": [{"version": "v1", "created": "Thu, 26 Apr 2018 14:35:39 GMT"}, {"version": "v2", "created": "Tue, 8 May 2018 01:12:38 GMT"}, {"version": "v3", "created": "Sat, 4 May 2019 13:43:24 GMT"}], "update_date": "2019-05-07", "authors_parsed": [["Ghavami", "Siavash", ""], ["Rahmati", "Vahid", ""], ["Lahouti", "Farshad", ""], ["Schwabe", "Lars", ""]]}, {"id": "1804.10093", "submitter": "Katherine Medina", "authors": "Katherine Medina", "title": "A GPP algorithm for hippocampal interneuron characterization", "comments": "11 pages; 2 figures; 2 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "q-bio.NC", "license": "http://creativecommons.org/publicdomain/zero/1.0/", "abstract": "  Correctly identifying neuronal subsets is critical to multiple downstream\nmethods in several areas of neuroscience research. The hippocampal interneuron\ncharacterization technology has achieved rapid development in recent years.\nHowever, capturing true neuronal features for accurate interneuron\ncharacterization and segmentation has remained elusive. In the current study, a\nnovel global preserving estimate algorithm is used to capture the non-linearity\nin the features of hippocampal interneurons after factor Algorithm. Our results\nprovide evidence for the effective integration of the original linear and\nnonlinear neuronal features and achieves better characterization performance on\nmultiple hippocampal interneuron databases through array matching.\n", "versions": [{"version": "v1", "created": "Thu, 26 Apr 2018 14:42:03 GMT"}], "update_date": "2018-04-27", "authors_parsed": [["Medina", "Katherine", ""]]}, {"id": "1804.10454", "submitter": "Andreas Meinel", "authors": "Andreas Meinel, Henrich Kolkhorst, Michael Tangermann", "title": "Mining within-trial oscillatory brain dynamics to address the\n  variability of optimized spatial filters", "comments": null, "journal-ref": null, "doi": "10.1109/TNSRE.2019.2894914", "report-no": null, "categories": "eess.SP cs.LG q-bio.NC stat.AP stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Data-driven spatial filtering algorithms optimize scores such as the contrast\nbetween two conditions to extract oscillatory brain signal components. Most\nmachine learning approaches for filter estimation, however, disregard\nwithin-trial temporal dynamics and are extremely sensitive to changes in\ntraining data and involved hyperparameters. This leads to highly variable\nsolutions and impedes the selection of a suitable candidate for,\ne.g.,~neurotechnological applications. Fostering component introspection, we\npropose to embrace this variability by condensing the functional signatures of\na large set of oscillatory components into homogeneous clusters, each\nrepresenting specific within-trial envelope dynamics.\n  The proposed method is exemplified by and evaluated on a complex hand force\ntask with a rich within-trial structure. Based on electroencephalography data\nof 18 healthy subjects, we found that the components' distinct temporal\nenvelope dynamics are highly subject-specific. On average, we obtained seven\nclusters per subject, which were strictly confined regarding their underlying\nfrequency bands. As the analysis method is not limited to a specific spatial\nfiltering algorithm, it could be utilized for a wide range of\nneurotechnological applications, e.g., to select and monitor functionally\nrelevant features for brain-computer interface protocols in stroke\nrehabilitation.\n", "versions": [{"version": "v1", "created": "Fri, 27 Apr 2018 11:56:04 GMT"}, {"version": "v2", "created": "Mon, 21 Jan 2019 11:39:50 GMT"}], "update_date": "2019-01-30", "authors_parsed": [["Meinel", "Andreas", ""], ["Kolkhorst", "Henrich", ""], ["Tangermann", "Michael", ""]]}, {"id": "1804.10508", "submitter": "Robert Pepperell", "authors": "Robert Pepperell", "title": "Consciousness as a physical process caused by the organization of energy\n  in the brain", "comments": "22 pages, 1 figure", "journal-ref": null, "doi": null, "report-no": null, "categories": "q-bio.NC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  To explain consciousness as a physical process we must acknowledge the role\nof energy in the brain. Energetic activity is fundamental to all physical\nprocesses and causally drives biological behaviour. Recent neuroscientific\nevidence can be interpreted in a way that suggests consciousness is a product\nof the organization of energetic activity in the brain. The nature of energy\nitself, though, remains largely mysterious, and we do not fully understand how\nit contributes to brain function or consciousness. According to the principle\noutlined here, energy, along with forces and work, can be described as\nactualized differences of motion and tension. By observing physical systems, we\ncan infer there is something it is like to undergo actualized difference from\nthe intrinsic perspective of the system. Consciousness occurs because there is\nsomething it is like, intrinsically, to undergo a certain organization of\nactualized differences in the brain.\n", "versions": [{"version": "v1", "created": "Fri, 27 Apr 2018 13:53:59 GMT"}, {"version": "v2", "created": "Fri, 4 May 2018 14:18:48 GMT"}, {"version": "v3", "created": "Wed, 3 Oct 2018 17:37:19 GMT"}, {"version": "v4", "created": "Thu, 4 Oct 2018 06:08:02 GMT"}, {"version": "v5", "created": "Wed, 10 Oct 2018 17:18:02 GMT"}, {"version": "v6", "created": "Fri, 12 Oct 2018 13:41:12 GMT"}], "update_date": "2018-10-15", "authors_parsed": [["Pepperell", "Robert", ""]]}, {"id": "1804.10521", "submitter": "Bradly Alicea", "authors": "Bradly Alicea", "title": "An Integrative Introduction to Human Augmentation Science", "comments": "22 pages, 5 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "q-bio.NC cs.HC", "license": "http://creativecommons.org/licenses/by-sa/4.0/", "abstract": "  Human Augmentation (HA) spans several technical fields and methodological\napproaches, including Experimental Psychology, Human-Computer Interaction,\nPsychophysiology, and Artificial Intelligence. Augmentation involves various\nstrategies for optimizing and controlling cognitive states, which requires an\nunderstanding of biological plasticity, dynamic cognitive processes, and models\nof adaptive systems. As an instructive lesson, we will explore a few HA-related\nconcepts and outstanding issues. Next, we focus on inducing and controlling HA\nusing experimental methods by introducing three techniques for HA\nimplementation: learning augmentation, augmentation using physical media, and\nextended phenotype modeling. To conclude, we will review integrative approaches\nto augmentation, which transcend specific functions.\n", "versions": [{"version": "v1", "created": "Thu, 26 Apr 2018 14:29:27 GMT"}], "update_date": "2018-04-30", "authors_parsed": [["Alicea", "Bradly", ""]]}, {"id": "1804.10861", "submitter": "Kevin Jasberg", "authors": "Kevin Jasberg and Sergej Sizov", "title": "Neuroscientific User Models: The Source of Uncertain User Feedback and\n  Potentials for Improving Recommendation and Personalisation", "comments": "User Noise, Human Uncertainty, Collaborative Filtering, User Models,\n  Bayesian Brain, Probabilistic Population Codes, Cognitive Agency, Neural\n  Coding", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC q-bio.NC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recent research revealed a considerable lack of reliability for user feedback\nwhen interacting with adaptive systems, often denoted as user noise or human\nuncertainty. Moreover, this lack of reliability holds striking impacts for the\nassessment of adaptive systems and personalisation approaches. Whenever\nresearch on this topic is done, there is a very strong system-centric view in\nwhich user variation is something undesirable and should be modelled with the\neye to eliminate. However, the possibilities of extracting additional\ninformation were only insufficiently considered so far.\n  In this contribution we consider the neuroscientific theory of the Bayesian\nbrain in order to develop novel user models with the power of turning the\nvariability of user behaviour into additional information for improving\nrecommendation and personalisation. To this end, we first introduce an adaptive\nmodel in which populations of neurons provide an estimation for a feedback to\nbe submitted. Subsequently, we present various decoder functions with which\nneuronal activity can be translated into quantitative decisions. The interplay\nof cognition model and decoder functions lead to different model-based\nproperties of decision-making. This will help to associate users to different\nclusters on the basis of their individual neural characteristics and thinking\npatterns. By means of user experiments and simulations, we show that this\ninformation can be used to improve the standard collaborative filtering.\n", "versions": [{"version": "v1", "created": "Sun, 29 Apr 2018 02:17:15 GMT"}], "update_date": "2018-05-01", "authors_parsed": [["Jasberg", "Kevin", ""], ["Sizov", "Sergej", ""]]}, {"id": "1804.11310", "submitter": "Hao Wang", "authors": "Hao Wang, Jiahui Wang, Xin Yuan Thow, Sanghoon Lee, Wendy Yen Xian\n  Peh, Kian Ann Ng, Tianyiyi He, Nitish V. Thakor and Chengkuo Lee", "title": "Unveiling Stimulation Secrets of Electrical Excitation of Neural Tissue\n  Using a Circuit Probability Theory", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "q-bio.NC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A new theory, named the Circuit-Probability theory, is proposed to unveil the\nsecret of electrical nerve stimulation, essentially explain the nonlinear and\nresonant phenomena observed when neural and non-neural tissues are electrically\nstimulated. For the explanation of frequency dependent response, an inductor is\ninvolved in the neural circuit model. Furthermore, predicted response to varied\nstimulation strength is calculated stochastically. Based on this theory, many\nempirical models, such as strength-duration relationship and LNP model, can be\ntheoretically explained, derived, and amended. This theory can explain the\ncomplex nonlinear interactions in electrical stimulation and fit in vivo\nexperiment data on stimulation-responses of many experiments. As such, the C-P\ntheory should be able to guide novel experiments and more importantly, offer an\nin-depth physical understanding of the neural tissue. As a promising neural\nmodel, we can even further explore the more accurate circuit configuration and\nprobability equation to better describe the electrical stimulation of neural\ntissues in the future.\n", "versions": [{"version": "v1", "created": "Mon, 30 Apr 2018 16:34:08 GMT"}, {"version": "v2", "created": "Thu, 14 Jun 2018 06:01:29 GMT"}], "update_date": "2018-06-15", "authors_parsed": [["Wang", "Hao", ""], ["Wang", "Jiahui", ""], ["Thow", "Xin Yuan", ""], ["Lee", "Sanghoon", ""], ["Peh", "Wendy Yen Xian", ""], ["Ng", "Kian Ann", ""], ["He", "Tianyiyi", ""], ["Thakor", "Nitish V.", ""], ["Lee", "Chengkuo", ""]]}]