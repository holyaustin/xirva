[{"id": "1511.00083", "submitter": "Subutai Ahmad", "authors": "Jeff Hawkins and Subutai Ahmad", "title": "Why Neurons Have Thousands of Synapses, A Theory of Sequence Memory in\n  Neocortex", "comments": "Submitted for publication", "journal-ref": "Frontiers in Neural Circuits 10:23 (2016) 1-13", "doi": "10.3389/fncir.2016.00023", "report-no": null, "categories": "q-bio.NC cs.AI", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Neocortical neurons have thousands of excitatory synapses. It is a mystery\nhow neurons integrate the input from so many synapses and what kind of\nlarge-scale network behavior this enables. It has been previously proposed that\nnon-linear properties of dendrites enable neurons to recognize multiple\npatterns. In this paper we extend this idea by showing that a neuron with\nseveral thousand synapses arranged along active dendrites can learn to\naccurately and robustly recognize hundreds of unique patterns of cellular\nactivity, even in the presence of large amounts of noise and pattern variation.\nWe then propose a neuron model where some of the patterns recognized by a\nneuron lead to action potentials and define the classic receptive field of the\nneuron, whereas the majority of the patterns recognized by a neuron act as\npredictions by slightly depolarizing the neuron without immediately generating\nan action potential. We then present a network model based on neurons with\nthese properties and show that the network learns a robust model of time-based\nsequences. Given the similarity of excitatory neurons throughout the neocortex\nand the importance of sequence memory in inference and behavior, we propose\nthat this form of sequence memory is a universal property of neocortical\ntissue. We further propose that cellular layers in the neocortex implement\nvariations of the same sequence memory algorithm to achieve different aspects\nof inference and behavior. The neuron and network models we introduce are\nrobust over a wide range of parameters as long as the network uses a sparse\ndistributed code of cellular activations. The sequence capacity of the network\nscales linearly with the number of synapses on each neuron. Thus neurons need\nthousands of synapses to learn the many temporal patterns in sensory stimuli\nand motor sequences.\n", "versions": [{"version": "v1", "created": "Sat, 31 Oct 2015 06:03:05 GMT"}, {"version": "v2", "created": "Tue, 1 Dec 2015 21:20:26 GMT"}], "update_date": "2016-04-25", "authors_parsed": [["Hawkins", "Jeff", ""], ["Ahmad", "Subutai", ""]]}, {"id": "1511.00235", "submitter": "Ido Kanter", "authors": "Amir Goldental, Roni Vardi, Shira Sardi, Pinhas Sabo and Ido Kanter", "title": "Broadband Macroscopic Cortical Oscillations Emerge from Intrinsic\n  Neuronal Response Failures", "comments": "21 pages, 5 figures", "journal-ref": "Front. Neural Circuits 9:65 (2015)", "doi": "10.3389/fncir.2015.00065", "report-no": null, "categories": "q-bio.NC cond-mat.stat-mech physics.bio-ph", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Broadband spontaneous macroscopic neural oscillations are rhythmic cortical\nfiring which were extensively examined during the last century, however, their\npossible origination is still controversial. In this work we show how\nmacroscopic oscillations emerge in solely excitatory random networks and\nwithout topological constraints. We experimentally and theoretically show that\nthese oscillations stem from the counterintuitive underlying mechanism - the\nintrinsic stochastic neuronal response failures. These neuronal response\nfailures, which are characterized by short-term memory, lead to cooperation\namong neurons, resulting in sub- or several- Hertz macroscopic oscillations\nwhich coexist with high frequency gamma oscillations. A quantitative interplay\nbetween the statistical network properties and the emerging oscillations is\nsupported by simulations of large networks based on single-neuron in-vitro\nexperiments and a Langevin equation describing the network dynamics. Results\ncall for the examination of these oscillations in the presence of inhibition\nand external drives.\n", "versions": [{"version": "v1", "created": "Sun, 1 Nov 2015 11:46:33 GMT"}], "update_date": "2015-11-03", "authors_parsed": [["Goldental", "Amir", ""], ["Vardi", "Roni", ""], ["Sardi", "Shira", ""], ["Sabo", "Pinhas", ""], ["Kanter", "Ido", ""]]}, {"id": "1511.00255", "submitter": "Carina Curto", "authors": "Carina Curto and Nora Youngs", "title": "Neural ring homomorphisms and maps between neural codes", "comments": "15 pages, 2 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "q-bio.NC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Neural codes are binary codes that are used for information processing and\nrepresentation in the brain. In previous work, we have shown how an algebraic\nstructure, called the {\\it neural ring}, can be used to efficiently encode\ngeometric and combinatorial properties of a neural code [1]. In this work, we\nconsider maps between neural codes and the associated homomorphisms of their\nneural rings. In order to ensure that these maps are meaningful and preserve\nrelevant structure, we find that we need additional constraints on the ring\nhomomorphisms. This motivates us to define {\\it neural ring homomorphisms}. Our\nmain results characterize all code maps corresponding to neural ring\nhomomorphisms as compositions of 5 elementary code maps. As an application, we\nfind that neural ring homomorphisms behave nicely with respect to convexity. In\nparticular, if $\\mathcal{C}$ and $\\mathcal{D}$ are convex codes, the existence\nof a surjective code map $\\mathcal{C}\\rightarrow \\mathcal{D}$ with a\ncorresponding neural ring homomorphism implies that the minimal embedding\ndimensions satisfy $d(\\mathcal{D}) \\leq d(\\mathcal{C})$.\n", "versions": [{"version": "v1", "created": "Sun, 1 Nov 2015 14:29:03 GMT"}, {"version": "v2", "created": "Thu, 1 Nov 2018 16:40:08 GMT"}, {"version": "v3", "created": "Wed, 13 Feb 2019 18:33:31 GMT"}], "update_date": "2019-02-14", "authors_parsed": [["Curto", "Carina", ""], ["Youngs", "Nora", ""]]}, {"id": "1511.00262", "submitter": "Konrad Kording", "authors": "Konrad Paul Kording", "title": "The geometry of Tempotronlike problems", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "q-bio.NC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In the discrete Tempotron learning problem a neuron receives time varying\ninputs and for a set of such input sequences ($\\mathcal S_-$ set) the neuron\nmust be sub-threshold for all times while for some other sequences ($\\mathcal\nS_+$ set) the neuron must be super threshold for at least one time. Here we\npresent a graphical treatment of a slight reformulation of the tempotron\nproblem. We show that the problem's general form is equivalent to the question\nif a polytope, specified by a set of inequalities, is contained in the union of\na set of equally defined polytopes. Using recent results from computational\ngeometry, we show that the problem is W[1]-hard. This phrasing gives some new\ninsights into the nature of gradient based learning algorithms. A sampling\nbased approach can, under certain circumstances provide an approximation in\npolynomial time. Other problems, related to hierarchical neural networks may\nshare some topological structure.\n", "versions": [{"version": "v1", "created": "Sun, 1 Nov 2015 15:49:46 GMT"}], "update_date": "2015-11-03", "authors_parsed": [["Kording", "Konrad Paul", ""]]}, {"id": "1511.00411", "submitter": "Pete Latham", "authors": "Alexander Lerchner and Peter E. Latham", "title": "A unifying framework for understanding state-dependent network dynamics\n  in cortex", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "q-bio.NC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Activity in neocortex exhibits a range of behaviors, from irregular to\ntemporally precise, and from weakly to strongly correlated. So far there has\nbeen no single theoretical framework that could explain all these behaviors,\nleaving open the possibility that they are a signature of radically different\nmechanisms. Here, we suggest that this is not the case. Instead, we show that a\nsingle theory can account for a broad spectrum of experimental observations,\nincluding specifics such as the fine temporal details of subthreshold\ncross-correlations. For the model underlying our theory, we need only assume a\nsmall number of well-established properties common to all local cortical\nnetworks. When these assumptions are combined with realistically structured\ninput, they produce exactly the repertoire of behaviors that is observed\nexperimentally, and lead to a number of testable predictions.\n", "versions": [{"version": "v1", "created": "Mon, 2 Nov 2015 08:47:17 GMT"}], "update_date": "2015-11-03", "authors_parsed": [["Lerchner", "Alexander", ""], ["Latham", "Peter E.", ""]]}, {"id": "1511.00673", "submitter": "Karol Bacik", "authors": "Karol A. Bacik, Michael T. Schaub, Mariano Beguerisse-D\\'iaz, Yazan N.\n  Billeh, Mauricio Barahona", "title": "Flow-based network analysis of the Caenorhabditis elegans connectome", "comments": "28 pages including Supplementary Information, 8 figures and 12\n  figures in the SI", "journal-ref": "PLoS Comput Biol 12.8 (2016): e1005055", "doi": "10.1371/journal.pcbi.1005055", "report-no": null, "categories": "q-bio.NC physics.soc-ph", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We exploit flow propagation on the directed neuronal network of the nematode\nCaenorhabditis elegans to reveal dynamically relevant features of its\nconnectome. We find flow-based groupings of neurons at different levels of\ngranularity, which we relate to functional and anatomical constituents of its\nnervous system. A systematic in silico evaluation of the full set of single and\ndouble neuron ablations is used to identify deletions that induce the most\nsevere disruptions of the multi-resolution flow structure. Such ablations are\nlinked to functionally relevant neurons, and suggest potential candidates for\nfurther in vivo investigation. In addition, we use the directional patterns of\nincoming and outgoing network flows at all scales to identify flow profiles for\nthe neurons in the connectome, without pre-imposing a priori categories. The\nfour flow roles identified are linked to signal propagation motivated by\nbiological input-response scenarios.\n", "versions": [{"version": "v1", "created": "Mon, 2 Nov 2015 20:50:27 GMT"}, {"version": "v2", "created": "Thu, 10 Dec 2015 20:40:23 GMT"}, {"version": "v3", "created": "Mon, 8 Aug 2016 10:07:59 GMT"}], "update_date": "2016-08-09", "authors_parsed": [["Bacik", "Karol A.", ""], ["Schaub", "Michael T.", ""], ["Beguerisse-D\u00edaz", "Mariano", ""], ["Billeh", "Yazan N.", ""], ["Barahona", "Mauricio", ""]]}, {"id": "1511.00964", "submitter": "Markus Goldhacker", "authors": "Markus Goldhacker, Ana Maria Tom\\'e, Mark W. Greenlee, Elmar W. Lang", "title": "Frequency-resolved dynamic functional connectivity and scale-invariant\n  connectivity-state behavior", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "q-bio.NC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Investigating temporal variability of functional connectivity is an emerging\nfield in connectomics. Entering dynamic functional connectivity by applying\nsliding window techniques on resting-state fMRI (rs-fMRI) time courses emerged\nfrom this topic. We introduce frequency-resolved dynamic functional\nconnectivity (frdFC) by means of multivariate empirical mode decomposition\n(MEMD) followed up by filter-bank investigations. We develop our method on the\nmost canonical form by applying a sliding window approach to the intrinsic mode\nfunctions (IMFs) resulting from MEMD. We explore two modifications:\nuniform-amplitude frequency scales by normalizing the IMFs by their\ninstantaneous amplitude and cumulative scales. By exploiting the well\nestablished concept of scale-invariance in resting-state parameters, we compare\nour frdFC approaches. In general, we find that MEMD is capable of generating\ntime courses to perform frdFC and we discover that the structure of\nconnectivity-states is robust over frequency scales and even becomes more\nevident with decreasing frequency. This scale-stability varies with the number\nof extracted clusters when applying k-means. We find a scale-stability drop-off\nfrom k = 4 to k = 5 extracted connectivity-states, which is corroborated by\nnull-models, simulations, theoretical considerations, filter-banks, and\nscale-adjusted windows. Our filter-bank studies show that filter design is more\ndelicate in the rs-fMRI than in the simulated case. Besides offering a baseline\nfor further frdFC research, we suggest and demonstrate the use of\nscale-stability as a quality criterion for connectivity-state and model\nselection. We present first evidence showing that scale-invariance plays an\nimportant role in connectivity-state considerations. A data repository of our\nfrequency-resolved time-series is provided.\n", "versions": [{"version": "v1", "created": "Tue, 3 Nov 2015 16:11:58 GMT"}], "update_date": "2015-11-04", "authors_parsed": [["Goldhacker", "Markus", ""], ["Tom\u00e9", "Ana Maria", ""], ["Greenlee", "Mark W.", ""], ["Lang", "Elmar W.", ""]]}, {"id": "1511.01681", "submitter": "Espen Hagen", "authors": "Espen Hagen, David Dahmen, Maria L. Stavrinou, Henrik Lind\\'en, Tom\n  Tetzlaff, Sacha J van Albada, Sonja Gr\\\"un, Markus Diesmann, Gaute T.\n  Einevoll", "title": "Hybrid scheme for modeling local field potentials from point-neuron\n  networks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "q-bio.NC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Due to rapid advances in multielectrode recording technology, the local field\npotential (LFP) has again become a popular measure of neuronal activity in both\nbasic research and clinical applications. Proper understanding of the LFP\nrequires detailed mathematical modeling incorporating the anatomical and\nelectrophysiological features of neurons near the recording electrode, as well\nas synaptic inputs from the entire network. Here we propose a hybrid modeling\nscheme combining the efficiency of commonly used simplified point-neuron\nnetwork models with the biophysical principles underlying LFP generation by\nreal neurons. The scheme can be used with an arbitrary number of point-neuron\nnetwork populations. The LFP predictions rely on populations of\nnetwork-equivalent, anatomically reconstructed multicompartment neuron models\nwith layer-specific synaptic connectivity. The present scheme allows for a full\nseparation of the network dynamics simulation and LFP generation. For\nillustration, we apply the scheme to a full-scale cortical network model for a\n$\\sim$1 mm$^2$ patch of primary visual cortex and predict laminar LFPs for\ndifferent network states, assess the relative LFP contribution from different\nlaminar populations, and investigate the role of synaptic input correlations\nand neuron density on the LFP. The generic nature of the hybrid scheme and its\npublicly available implementation in \\texttt{hybridLFPy} form the basis for LFP\npredictions from other point-neuron network models, as well as extensions of\nthe current application to larger circuitry and additional biological detail.\n", "versions": [{"version": "v1", "created": "Thu, 5 Nov 2015 10:10:12 GMT"}, {"version": "v2", "created": "Wed, 20 Jan 2016 22:18:06 GMT"}], "update_date": "2016-01-22", "authors_parsed": [["Hagen", "Espen", ""], ["Dahmen", "David", ""], ["Stavrinou", "Maria L.", ""], ["Lind\u00e9n", "Henrik", ""], ["Tetzlaff", "Tom", ""], ["van Albada", "Sacha J", ""], ["Gr\u00fcn", "Sonja", ""], ["Diesmann", "Markus", ""], ["Einevoll", "Gaute T.", ""]]}, {"id": "1511.02086", "submitter": "Gal Mishne", "authors": "Gal Mishne, Ronen Talmon, Ron Meir, Jackie Schiller, Uri Dubin and\n  Ronald R. Coifman", "title": "Hierarchical Coupled Geometry Analysis for Neuronal Structure and\n  Activity Pattern Discovery", "comments": "13 pages, 9 figures", "journal-ref": null, "doi": "10.1109/JSTSP.2016.2602061", "report-no": null, "categories": "q-bio.QM q-bio.NC stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In the wake of recent advances in experimental methods in neuroscience, the\nability to record in-vivo neuronal activity from awake animals has become\nfeasible. The availability of such rich and detailed physiological measurements\ncalls for the development of advanced data analysis tools, as commonly used\ntechniques do not suffice to capture the spatio-temporal network complexity. In\nthis paper, we propose a new hierarchical coupled geometry analysis, which\nexploits the hidden connectivity structures between neurons and the dynamic\npatterns at multiple time-scales. Our approach gives rise to the joint\norganization of neurons and dynamic patterns in data-driven hierarchical data\nstructures. These structures provide local to global data representations, from\nlocal partitioning of the data in flexible trees through a new multiscale\nmetric to a global manifold embedding. The application of our techniques to\nin-vivo neuronal recordings demonstrate the capability of extracting neuronal\nactivity patterns and identifying temporal trends, associated with particular\nbehavioral events and manipulations introduced in the experiments.\n", "versions": [{"version": "v1", "created": "Fri, 6 Nov 2015 14:14:29 GMT"}], "update_date": "2016-11-03", "authors_parsed": [["Mishne", "Gal", ""], ["Talmon", "Ronen", ""], ["Meir", "Ron", ""], ["Schiller", "Jackie", ""], ["Dubin", "Uri", ""], ["Coifman", "Ronald R.", ""]]}, {"id": "1511.02976", "submitter": "James Shine", "authors": "James M. Shine, Patrick G. Bissett, Peter T. Bell, Oluwasanmi Koyejo,\n  Joshua H. Balsters, Krzysztof J. Gorgolewski, Craig A. Moodie, Russell A.\n  Poldrack", "title": "The Dynamics of Functional Brain Networks: Integrated Network States\n  during Cognitive Function", "comments": "38 pages, 4 figures", "journal-ref": null, "doi": "10.1016/j.neuron.2016.09.018", "report-no": null, "categories": "q-bio.NC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Higher brain function relies upon the ability to flexibly integrate\ninformation across specialized communities of brain regions, however it is\nunclear how this mechanism manifests over time. In this study, we use\ntime-resolved network analysis of functional magnetic resonance imaging data to\ndemonstrate that the human brain traverses between two functional states that\nmaximize either segregation into tight-knit communities or integration across\notherwise disparate neural regions. The integrated state enables faster and\nmore accurate performance on a cognitive task, and is associated with dilations\nin pupil diameter, suggesting that ascending neuromodulatory systems may govern\nthe transition between these alternative modes of brain function. Our data\nconfirm a direct link between cognitive performance and the dynamic\nreorganization of the network structure of the brain.\n", "versions": [{"version": "v1", "created": "Tue, 10 Nov 2015 02:57:08 GMT"}, {"version": "v2", "created": "Wed, 3 Feb 2016 18:50:08 GMT"}, {"version": "v3", "created": "Mon, 1 Aug 2016 03:46:56 GMT"}], "update_date": "2017-05-30", "authors_parsed": [["Shine", "James M.", ""], ["Bissett", "Patrick G.", ""], ["Bell", "Peter T.", ""], ["Koyejo", "Oluwasanmi", ""], ["Balsters", "Joshua H.", ""], ["Gorgolewski", "Krzysztof J.", ""], ["Moodie", "Craig A.", ""], ["Poldrack", "Russell A.", ""]]}, {"id": "1511.03726", "submitter": "Camilo Lamus", "authors": "Camilo Lamus, Matti S. Hamalainen, Emery N. Brown, and Patrick L.\n  Purdon", "title": "An Analysis of How Spatiotemporal Dynamic Models of Brain Activity Could\n  Improve MEG/EEG Inverse Solutions", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP q-bio.NC q-bio.QM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  MEG and EEG are noninvasive functional neuroimaging techniques that provide\nrecordings of brain activity with high temporal resolution, and thus provide a\nunique window to study fast time-scale neural dynamics in humans. However, the\naccuracy of brain activity estimates resulting from these data is limited\nmainly because 1) the number of sensors is much smaller than the number of\nsources, and 2) the low sensitivity of the recording device to deep or radially\noriented sources. These factors limit the number of sources that can be\nrecovered and bias estimates to superficial cortical areas, resulting in the\nneed to include a priori information about the source activity. The question of\nhow to specify this information and how it might lead to improved solutions\nremains a critical open problem. In this paper we show that the incorporation\nof knowledge about the brain's underlying connectivity and spatiotemporal\ndynamics could dramatically improve inverse solutions. To do this, we develop\nthe concept of the \\textit{dynamic lead field mapping}, which expresses how\ninformation about source activity at a given time is mapped not only to the\nimmediate measurement, but to a time series of measurements. With this mapping\nwe show that the number of source parameters that can be recovered could\nincrease by up to a factor of ${\\sim20}$, and that such improvement is\nprimarily represented by deep cortical areas. Our result implies that future\ndevelopments in MEG/EEG analysis that model spatialtemporal dynamics have the\npotential to dramatically increase source resolution.\n", "versions": [{"version": "v1", "created": "Wed, 11 Nov 2015 23:12:06 GMT"}], "update_date": "2015-11-13", "authors_parsed": [["Lamus", "Camilo", ""], ["Hamalainen", "Matti S.", ""], ["Brown", "Emery N.", ""], ["Purdon", "Patrick L.", ""]]}, {"id": "1511.03895", "submitter": "Pau Closas", "authors": "Pau Closas and Antoni Guillamon", "title": "Sequential estimation of intrinsic activity and synaptic input in single\n  neurons by particle filtering with optimal importance density", "comments": "Submitted for publication in the Special Issue on Advanced Signal\n  Processing in Brain Networks of the IEEE Journal on Selected Topics in Signal\n  Processing", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.CO q-bio.NC stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper deals with the problem of inferring the signals and parameters\nthat cause neural activity to occur. The ultimate challenge being to unveil\nbrain's connectivity, here we focus on a microscopic vision of the problem,\nwhere single neurons (potentially connected to a network of peers) are at the\ncore of our study. The sole observation available are noisy, sampled voltage\ntraces obtained from intracellular recordings. We design algorithms and\ninference methods using the tools provided by stochastic filtering, that allow\na probabilistic interpretation and treatment of the problem. Using particle\nfiltering we are able to reconstruct traces of voltages and estimate the time\ncourse of auxiliary variables. By extending the algorithm, through PMCMC\nmethodology, we are able to estimate hidden physiological parameters as well,\nlike intrinsic conductances or reversal potentials. Last, but not least, the\nmethod is applied to estimate synaptic conductances arriving at a target cell,\nthus reconstructing the synaptic excitatory/inhibitory input traces. Notably,\nthese estimations have a bound-achieving performance even in spiking regimes.\n", "versions": [{"version": "v1", "created": "Thu, 12 Nov 2015 13:47:06 GMT"}], "update_date": "2015-11-13", "authors_parsed": [["Closas", "Pau", ""], ["Guillamon", "Antoni", ""]]}, {"id": "1511.04156", "submitter": "Josh Merel", "authors": "Josh Merel, David Carlson, Liam Paninski, John P. Cunningham", "title": "Neuroprosthetic decoder training as imitation learning", "comments": null, "journal-ref": null, "doi": "10.1371/journal.pcbi.1004948", "report-no": null, "categories": "stat.ML cs.LG q-bio.NC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Neuroprosthetic brain-computer interfaces function via an algorithm which\ndecodes neural activity of the user into movements of an end effector, such as\na cursor or robotic arm. In practice, the decoder is often learned by updating\nits parameters while the user performs a task. When the user's intention is not\ndirectly observable, recent methods have demonstrated value in training the\ndecoder against a surrogate for the user's intended movement. We describe how\ntraining a decoder in this way is a novel variant of an imitation learning\nproblem, where an oracle or expert is employed for supervised training in lieu\nof direct observations, which are not available. Specifically, we describe how\na generic imitation learning meta-algorithm, dataset aggregation (DAgger, [1]),\ncan be adapted to train a generic brain-computer interface. By deriving\nexisting learning algorithms for brain-computer interfaces in this framework,\nwe provide a novel analysis of regret (an important metric of learning\nefficacy) for brain-computer interfaces. This analysis allows us to\ncharacterize the space of algorithmic variants and bounds on their regret\nrates. Existing approaches for decoder learning have been performed in the\ncursor control setting, but the available design principles for these decoders\nare such that it has been impossible to scale them to naturalistic settings.\nLeveraging our findings, we then offer an algorithm that combines imitation\nlearning with optimal control, which should allow for training of arbitrary\neffectors for which optimal control can generate goal-oriented control. We\ndemonstrate this novel and general BCI algorithm with simulated neuroprosthetic\ncontrol of a 26 degree-of-freedom model of an arm, a sophisticated and\nrealistic end effector.\n", "versions": [{"version": "v1", "created": "Fri, 13 Nov 2015 04:21:33 GMT"}, {"version": "v2", "created": "Mon, 14 Mar 2016 16:39:03 GMT"}], "update_date": "2016-09-28", "authors_parsed": [["Merel", "Josh", ""], ["Carlson", "David", ""], ["Paninski", "Liam", ""], ["Cunningham", "John P.", ""]]}, {"id": "1511.04202", "submitter": "Jochen Laubrock", "authors": "Anke Cajar, Ralf Engbert, Jochen Laubrock", "title": "Spatial frequency processing in the central and peripheral visual field\n  during scene viewing", "comments": "4 Figures", "journal-ref": "Vision Research, 127 (2016), 186-197", "doi": "10.1016/j.visres.2016.05.008", "report-no": null, "categories": "q-bio.NC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Visuospatial attention and gaze control depend on the interaction of foveal\nand peripheral processing. The foveal and peripheral regions of the visual\nfield are differentially sensitive to parts of the spatial-frequency spectrum.\nIn two experiments, we investigated how the selective attenuation of spatial\nfrequencies in the central or the peripheral visual field affects eye-movement\nbehavior during real-world scene viewing. Gaze-contingent low-pass or high-pass\nfilters with varying filter levels (i.e., cutoff frequencies; Experiment 1) or\nfilter sizes (Experiment 2) were applied. Compared to unfiltered control\nconditions, mean fixation durations increased most with central high-pass and\nperipheral low-pass filtering. Increasing filter size prolonged fixation\ndurations with peripheral filtering, but not with central filtering. Increasing\nfilter level prolonged fixation durations with low-pass filtering, but not with\nhigh-pass filtering. These effects indicate that fixation durations are not\nalways longer under conditions of increased processing difficulty. Saccade\namplitudes largely adapted to processing difficulty: amplitudes increased with\ncentral filtering and decreased with peripheral filtering; the effects\nstrengthened with increasing filter size and filter level. In addition, we\nobserved a trade-off between saccade timing and saccadic selection, since\nsaccade amplitudes were modulated when fixation durations were unaffected by\nthe experimental manipulations. We conclude that interactions of perception and\ngaze control are highly sensitive to experimental manipulations of input images\nas long as the residual information can still be accessed for gaze control.\n", "versions": [{"version": "v1", "created": "Fri, 13 Nov 2015 08:50:14 GMT"}, {"version": "v2", "created": "Mon, 15 Feb 2016 09:19:12 GMT"}, {"version": "v3", "created": "Thu, 14 Apr 2016 13:51:23 GMT"}, {"version": "v4", "created": "Thu, 20 Oct 2016 10:31:15 GMT"}], "update_date": "2016-10-21", "authors_parsed": [["Cajar", "Anke", ""], ["Engbert", "Ralf", ""], ["Laubrock", "Jochen", ""]]}, {"id": "1511.04338", "submitter": "Bulcs\\'u S\\'andor", "authors": "Bulcs\\'u S\\'andor, Tim Jahn, Laura Martin and Claudius Gros", "title": "The sensorimotor loop as a dynamical system: How regular motion\n  primitives may emerge from self-organized limit cycles", "comments": null, "journal-ref": "Front. Robot. AI 2:31 (2015)", "doi": "10.3389/frobt.2015.00031", "report-no": null, "categories": "q-bio.NC cond-mat.dis-nn cs.RO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We investigate the sensorimotor loop of simple robots simulated within the\nLPZRobots environment from the point of view of dynamical systems theory. For a\nrobot with a cylindrical shaped body and an actuator controlled by a single\nproprioceptual neuron we find various types of periodic motions in terms of\nstable limit cycles. These are self-organized in the sense, that the dynamics\nof the actuator kicks in only, for a certain range of parameters, when the\nbarrel is already rolling, stopping otherwise. The stability of the resulting\nrolling motions terminates generally, as a function of the control parameters,\nat points where fold bifurcations of limit cycles occur. We find that several\nbranches of motion types exist for the same parameters, in terms of the\nrelative frequencies of the barrel and of the actuator, having each their\nrespective basins of attractions in terms of initial conditions. For low\ndrivings stable limit cycles describing periodic and drifting back-and-forth\nmotions are found additionally. These modes allow to generate symmetry breaking\nexplorative behavior purely by the timing of an otherwise neutral signal with\nrespect to the cyclic back-and-forth motion of the robot.\n", "versions": [{"version": "v1", "created": "Fri, 13 Nov 2015 16:20:38 GMT"}, {"version": "v2", "created": "Wed, 14 Mar 2018 12:44:16 GMT"}], "update_date": "2019-01-18", "authors_parsed": [["S\u00e1ndor", "Bulcs\u00fa", ""], ["Jahn", "Tim", ""], ["Martin", "Laura", ""], ["Gros", "Claudius", ""]]}, {"id": "1511.04345", "submitter": "Lilianne Mujica-Parodi", "authors": "D.J. DeDora, S. Nedic, P. Katti, S. Arnab, L.L. Wald, A. Takahashi,\n  K.R.A. Van Dijk, H.H. Strey, L.R. Mujica-Parodi", "title": "Signal Fluctuation Sensitivity: an improved metric for optimizing\n  detection of resting-state fMRI networks", "comments": "27 pages, 4 figures, 2 tables. Contact Information: Lilianne R.\n  Mujica-Parodi, Laboratory for Computational Neurodiagnostics, Department of\n  Biomedical Engineering, Stony Brook University, Stony Brook, NY,\n  Lilianne.Strey@stonybrook.edu (www.lcneuro.org)", "journal-ref": "https://www.frontiersin.org/article/10.3389/fnins.2016.00180", "doi": "10.3389/fnins.2016.00180", "report-no": null, "categories": "q-bio.NC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Task-free connectivity analyses have emerged as a powerful tool in functional\nneuroimaging. Because the cross-correlations that underlie connectivity\nmeasures are sensitive to distortion of time-series, here we used a novel\ndynamic phantom to provide a ground truth for dynamic fidelity between blood\noxygen level dependent (BOLD)-like inputs and fMRI outputs. We found that the\nde facto quality-metric for task-free fMRI, temporal signal to noise ratio\n(tSNR), correlated inversely with dynamic fidelity; thus, studies optimized for\ntSNR actually produced time-series that showed the greatest distortion of\nsignal dynamics. Instead, the phantom showed that dynamic fidelity is\nreasonably approximated by a measure that, unlike tSNR, dissociates signal\ndynamics from scanner artifact. We then tested this measure, signal fluctuation\nsensitivity (SFS), against human resting-state data. As predicted by the\nphantom, SFS--and not tSNR--is associated with enhanced sensitivity to both\nlocal and long-range connectivity within the brain's default mode network.\n", "versions": [{"version": "v1", "created": "Fri, 13 Nov 2015 16:34:14 GMT"}], "update_date": "2020-02-05", "authors_parsed": [["DeDora", "D. J.", ""], ["Nedic", "S.", ""], ["Katti", "P.", ""], ["Arnab", "S.", ""], ["Wald", "L. L.", ""], ["Takahashi", "A.", ""], ["Van Dijk", "K. R. A.", ""], ["Strey", "H. H.", ""], ["Mujica-Parodi", "L. R.", ""]]}, {"id": "1511.04467", "submitter": "Umberto Esposito", "authors": "Umberto Esposito and Eleni Vasilaki", "title": "Detection of multiple and overlapping bidirectional communities within\n  large, directed and weighted networks of neurons", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.SI cs.DS physics.soc-ph q-bio.NC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  With the recent explosion of publicly available biological data, the analysis\nof networks has gained significant interest. In particular, recent promising\nresults in Neuroscience show that the way neurons and areas of the brain are\nconnected to each other plays a fundamental role in cognitive functions and\nbehaviour. Revealing pattern and structures within such an intricate volume of\nconnections is a hard problem that has its roots in Graph and Network Theory.\nSince many real world situations can be modelled through networks, structures\ndetection algorithms find application in almost every field of Science. These\nare NP-complete problems; therefore the generally used approach is through\nheuristic algorithms. Here, we formulate the problem of finding structures in\nnetworks of neurons in terms of a community detection problem. We introduce a\ndefinition of community and we construct a statistics-based heuristic algorithm\nfor directed and weighted networks aiming at identifying overlapping\nbidirectional communities in large networks. We carry out a systematic analysis\nof the algorithm's performance, showing excellent results over a wide range of\nparameters (successful detection percentages almost $100\\%$ all the time).\nAlso, we show results on the computational time needed and we suggest future\ndirections on how to improve computational performance.\n", "versions": [{"version": "v1", "created": "Fri, 13 Nov 2015 21:36:43 GMT"}], "update_date": "2015-11-17", "authors_parsed": [["Esposito", "Umberto", ""], ["Vasilaki", "Eleni", ""]]}, {"id": "1511.04643", "submitter": "Satohiro Tajima", "authors": "Satohiro Tajima", "title": "Sensory Polymorphism and Behavior: When Machine Vision Meets Monkey Eyes", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "q-bio.NC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Polymorphism in the peripheral sensory system (e.g., congenital individual\ndifferences in photopigment configuration) is important in diverse research\nfields, ranging from evolutionary biology to engineering, because of its\npotential relationship to the cognitive and behavioral variability among\nindividuals. However, there is a gap between the current understanding of\nsensory polymorphism and the behavioral variability that is an outcome of\npotentially complex cognitive processes in natural environments. Linking\nperipheral sensor properties to behavior requires computational models of\nnervous processes transforming sensory representations into actions, based on\nquantitative data from physiological and behavioral studies. Recently, studies\nbased on machine vision approaches are shedding light on the quantitative\nrelationships between sensory polymorphisms and the resulting behavioral\nvariability. To reach a convergent understanding of the functional impacts of\nsensory polymorphisms in realistic environments, a close coordination among\nphysiological, behavioral, and computational approaches is required. Aiming at\nenhancing such integrative researches, this paper provides an overview for the\nrecent progresses in those interdisciplinary approaches, and suggests effective\nstrategies for such integrative paradigms.\n", "versions": [{"version": "v1", "created": "Sun, 15 Nov 2015 02:19:05 GMT"}, {"version": "v2", "created": "Fri, 6 Jan 2017 20:47:40 GMT"}], "update_date": "2017-01-10", "authors_parsed": [["Tajima", "Satohiro", ""]]}, {"id": "1511.04780", "submitter": "Sebastian Weichwald", "authors": "Sebastian Weichwald, Timm Meyer, Ozan \\\"Ozdenizci, Bernhard\n  Sch\\\"olkopf, Tonio Ball, Moritz Grosse-Wentrup", "title": "Causal interpretation rules for encoding and decoding models in\n  neuroimaging", "comments": "accepted manuscript", "journal-ref": "NeuroImage, 110:48-59, 2015", "doi": "10.1016/j.neuroimage.2015.01.036", "report-no": null, "categories": "stat.ML cs.LG q-bio.NC stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Causal terminology is often introduced in the interpretation of encoding and\ndecoding models trained on neuroimaging data. In this article, we investigate\nwhich causal statements are warranted and which ones are not supported by\nempirical evidence. We argue that the distinction between encoding and decoding\nmodels is not sufficient for this purpose: relevant features in encoding and\ndecoding models carry a different meaning in stimulus- and in response-based\nexperimental paradigms. We show that only encoding models in the stimulus-based\nsetting support unambiguous causal interpretations. By combining encoding and\ndecoding models trained on the same data, however, we obtain insights into\ncausal relations beyond those that are implied by each individual model type.\nWe illustrate the empirical relevance of our theoretical findings on EEG data\nrecorded during a visuo-motor learning task.\n", "versions": [{"version": "v1", "created": "Sun, 15 Nov 2015 23:16:10 GMT"}], "update_date": "2015-11-17", "authors_parsed": [["Weichwald", "Sebastian", ""], ["Meyer", "Timm", ""], ["\u00d6zdenizci", "Ozan", ""], ["Sch\u00f6lkopf", "Bernhard", ""], ["Ball", "Tonio", ""], ["Grosse-Wentrup", "Moritz", ""]]}, {"id": "1511.04892", "submitter": "Christian Engwer", "authors": "Christian Engwer and Johannes Vorwerk and Jakob Ludewig and Carsten H.\n  Wolters", "title": "A discontinuous Galerkin Method for the EEG Forward Problem using the\n  Subtraction Approach", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CE math.NA q-bio.NC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In order to perform electroencephalography (EEG) source reconstruction, i.e.,\nto localize the sources underlying a measured EEG, the electric potential\ndistribution at the electrodes generated by a dipolar current source in the\nbrain has to be simulated, which is the so-called EEG forward problem. To solve\nit accurately, it is necessary to apply numerical methods that are able to take\nthe individual geometry and conductivity distribution of the subject's head\ninto account. In this context, the finite element method (FEM) has shown high\nnumerical accuracy with the possibility to model complex geometries and\nconductive features, e.g., white matter conductivity anisotropy. In this\narticle, we introduce and analyze the application of a discontinuous Galerkin\n(DG) method, a finite element method that includes features of the finite\nvolume framework, to the EEG forward problem. The DG-FEM approach fulfills the\nconservation property of electric charge also in the discrete case, making it\nattractive for a variety of applications. Furthermore, as we show, this\napproach can alleviate modeling inaccuracies that might occur in head\ngeometries when using classical FE methods, e.g., so-called \"skull leakage\neffects\", which may occur in areas where the thickness of the skull is in the\nrange of the mesh resolution. Therefore, we derive a DG formulation of the FEM\nsubtraction approach for the EEG forward problem and present numerical results\nthat highlight the advantageous features and the potential benefits of the\nproposed approach.\n", "versions": [{"version": "v1", "created": "Mon, 16 Nov 2015 10:13:34 GMT"}, {"version": "v2", "created": "Tue, 15 Nov 2016 15:56:15 GMT"}], "update_date": "2016-11-16", "authors_parsed": [["Engwer", "Christian", ""], ["Vorwerk", "Johannes", ""], ["Ludewig", "Jakob", ""], ["Wolters", "Carsten H.", ""]]}, {"id": "1511.05056", "submitter": "Camilo Lamus", "authors": "Camilo Lamus, Matti S. H\\\"am\\\"al\\\"ainen, Simona Temereanca, Emery N.\n  Brown, and Patrick L. Purdon", "title": "A Spatiotemporal Dynamic Solution to the MEG Inverse Problem: An\n  Empirical Bayes Approach", "comments": null, "journal-ref": "NeuroImage, vol. 63, no. 2, pp. 894-909. 2012", "doi": null, "report-no": null, "categories": "stat.AP q-bio.NC q-bio.QM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  MEG/EEG are non-invasive imaging techniques that record brain activity with\nhigh temporal resolution. However, estimation of brain source currents from\nsurface recordings requires solving an ill-posed inverse problem. Converging\nlines of evidence in neuroscience, from neuronal network models to\nresting-state imaging and neurophysiology, suggest that cortical activation is\na distributed spatiotemporal dynamic process, supported by both local and\nlong-distance neuroanatomic connections. Because spatiotemporal dynamics of\nthis kind are central to brain physiology, inverse solutions could be improved\nby incorporating models of these dynamics. In this article, we present a model\nfor cortical activity based on nearest-neighbor autoregression that\nincorporates local spatiotemporal interactions between distributed sources in a\nmanner consistent with neurophysiology and neuroanatomy. We develop a dynamic\nMaximum a Posteriori Expectation-Maximization (dMAP-EM) source localization\nalgorithm for estimation of cortical sources and model parameters based on the\nKalman Filter, the Fixed Interval Smoother, and the EM algorithms. We apply the\ndMAP-EM algorithm to simulated experiments as well as to human experimental\ndata. Furthermore, we derive expressions to relate our dynamic estimation\nformulas to those of standard static models, and show how dynamic methods\noptimally assimilate past and future data. Our results establish the\nfeasibility of spatiotemporal dynamic estimation in large-scale distributed\nsource spaces with several thousand source locations and hundreds of sensors,\nwith resulting inverse solutions that provide substantial performance\nimprovements over static methods.\n", "versions": [{"version": "v1", "created": "Mon, 16 Nov 2015 17:45:05 GMT"}, {"version": "v2", "created": "Thu, 7 Apr 2016 00:59:37 GMT"}, {"version": "v3", "created": "Sun, 19 Jun 2016 00:02:53 GMT"}], "update_date": "2016-06-21", "authors_parsed": [["Lamus", "Camilo", ""], ["H\u00e4m\u00e4l\u00e4inen", "Matti S.", ""], ["Temereanca", "Simona", ""], ["Brown", "Emery N.", ""], ["Purdon", "Patrick L.", ""]]}, {"id": "1511.05222", "submitter": "Alexander Rivkind", "authors": "Alexander Rivkind, Omri Barak", "title": "Local Dynamics in Trained Recurrent Neural Networks", "comments": null, "journal-ref": "Phys. Rev. Lett. 118, 258101 (2017)", "doi": "10.1103/PhysRevLett.118.258101", "report-no": null, "categories": "q-bio.NC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Learning a task induces connectivity changes in neural circuits, thereby\nchanging their dynamics. To elucidate task related neural dynamics we study\ntrained Recurrent Neural Networks. We develop a Mean Field Theory for Reservoir\nComputing networks trained to have multiple fixed point attractors. Our main\nresult is that the dynamics of the network's output in the vicinity of\nattractors is governed by a low order linear Ordinary Differential Equation.\nStability of the resulting ODE can be assessed, predicting training success or\nfailure. As a consequence, networks of Rectified Linear (RLU) and of sigmoidal\nnonlinearities are shown to have diametrically different properties when it\ncomes to learning attractors. Furthermore, a characteristic time constant,\nwhich remains finite at the edge of chaos, offers an explanation of the\nnetwork's output robustness in the presence of variability of the internal\nneural dynamics. Finally, the proposed theory predicts state dependent\nfrequency selectivity in network response.\n", "versions": [{"version": "v1", "created": "Mon, 16 Nov 2015 23:38:55 GMT"}, {"version": "v2", "created": "Wed, 16 Dec 2015 12:08:48 GMT"}, {"version": "v3", "created": "Fri, 18 Dec 2015 08:41:06 GMT"}, {"version": "v4", "created": "Sat, 20 Feb 2016 20:32:18 GMT"}, {"version": "v5", "created": "Wed, 13 Jul 2016 06:27:41 GMT"}], "update_date": "2017-06-28", "authors_parsed": [["Rivkind", "Alexander", ""], ["Barak", "Omri", ""]]}, {"id": "1511.05227", "submitter": "Marisa Eisenberg", "authors": "Olivia J. Walch, Marisa C. Eisenberg", "title": "Parameter identifiability and identifiable combinations in generalized\n  Hodgkin-Huxley models", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "q-bio.NC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The use of Hodgkin-Huxley (HH) equations abounds in the literature, but the\nidentifiability of the HH model parameters has not been broadly considered.\nIdentifiability analysis addresses the question of whether it is possible to\nestimate the model parameters for a given choice of measurement data and\nexperimental inputs. Here we explore the structural identifiability properties\nof a generalized form of HH from voltage clamp data. Through a scaling\nargument, we conclude that the steady-state gating variables are not\nidentifiable from voltage clamp data, and then further show that their product\ntogether with the conductance term forms an identifiable combination. We\nadditionally show that these parameters become identifiable when the initial\nconditions for each of the gating variables are known. The time constants for\neach gating variable are shown to be identifiable, and a novel method for\nestimating them is presented. Finally, the exponents of the gating variables\nare shown to be identifiable in the two-gate case, and we conjecture these to\nbe identifiable in the general case. These results are broadly applicable to\nmodels using HH-like formalisms, and show in general which parameters and\ncombinations of parameters are possible to estimate from voltage clamp data.\n", "versions": [{"version": "v1", "created": "Mon, 16 Nov 2015 23:58:18 GMT"}], "update_date": "2015-11-18", "authors_parsed": [["Walch", "Olivia J.", ""], ["Eisenberg", "Marisa C.", ""]]}, {"id": "1511.05519", "submitter": "Alina Gavrilut", "authors": "Maricel Agop, Alina Gavrilut, Gabriel Crumpei, Mitica Craus, Vlad\n  Birlescu", "title": "Brain dynamics through spectral-structural neuronal networks", "comments": "arXiv admin note: text overlap with arXiv:0910.2741 by other authors", "journal-ref": null, "doi": null, "report-no": null, "categories": "q-bio.NC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Starting from the morphological-functional assumption of the fractal brain, a\nmathematical model is given by activating brain non-differentiable dynamics\nthrough the determinism-nondeterminism inference of the responsible mechanisms.\nThe postulation of a scale covariance principle in Schrodinger type\nrepresentation of the brain geodesics implies the spectral functionality of the\nbrain dynamics through mechanisms of tunelling, percolation etc., while in the\nhydrodynamical type representation, it implies their structural functionality\nthrough mechanisms of wave schock, solitons type etc. For external constraints\nproportional with the states density, the fluctuations of the brain stationary\ndynamics activate both the spectral neuronal networks and the structural ones\nthrough a mapping principle of two distinct classes of cnoidal oscillation\nmodes. The spectral-structural compatibility of the neuronal networks generates\nthe communication codes of algebraic type, while the same compatibility on the\nsolitonic component induces a strange topology (the direct product of the\nspectral topology and the structural one) that is responsible of the quadruple\nlaw(for instance, the nucleotide base from the human DNA structure).\nImplications in the elucidation of some neuropsychological mechanisms (memory\nlocation and functioning, dementia etc.) are also presented.\n", "versions": [{"version": "v1", "created": "Mon, 16 Nov 2015 08:51:47 GMT"}], "update_date": "2015-11-18", "authors_parsed": [["Agop", "Maricel", ""], ["Gavrilut", "Alina", ""], ["Crumpei", "Gabriel", ""], ["Craus", "Mitica", ""], ["Birlescu", "Vlad", ""]]}, {"id": "1511.06352", "submitter": "Richard Betzel", "authors": "Richard F. Betzel, Makoto Fukushima, Ye He, Xi-Nian Zuo, Olaf Sporns", "title": "Dynamic fluctuations coincide with periods of high and low modularity in\n  resting-state functional brain networks", "comments": "47 Pages, 8 Figures, 4 Supplementary Figures", "journal-ref": null, "doi": "10.1016/j.neuroimage.2015.12.001", "report-no": null, "categories": "q-bio.NC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We investigate the relationship of resting-state fMRI functional connectivity\nestimated over long periods of time with time-varying functional connectivity\nestimated over shorter time intervals. We show that using Pearson's correlation\nto estimate functional connectivity implies that the range of fluctuations of\nfunctional connections over short time scales is subject to statistical\nconstraints imposed by their connectivity strength over longer scales. We\npresent a method for estimating time-varying functional connectivity that is\ndesigned to mitigate this issue and allows us to identify episodes where\nfunctional connections are unexpectedly strong or weak. We apply this method to\ndata recorded from $N=80$ participants, and show that the number of\nunexpectedly strong/weak connections fluctuates over time, and that these\nvariations coincide with intermittent periods of high and low modularity in\ntime-varying functional connectivity. We also find that during periods of\nrelative quiescence regions associated with default mode network tend to join\ncommunities with attentional, control, and primary sensory systems. In\ncontrast, during periods where many connections are unexpectedly strong/weak,\ndefault mode regions dissociate and form distinct modules. Finally, we go on to\nshow that, while all functional connections can at times manifest stronger\n(more positively correlated) or weaker (more negatively correlated) than\nexpected, a small number of connections, mostly within the visual and\nsomatomotor networks, do so a disproportional number of times. Our statistical\napproach allows the detection of functional connections that fluctuate more or\nless than expected based on their long-time averages and may be of use in\nfuture studies characterizing the spatio-temporal patterns of time-varying\nfunctional connectivity\n", "versions": [{"version": "v1", "created": "Thu, 19 Nov 2015 20:46:05 GMT"}], "update_date": "2016-09-08", "authors_parsed": [["Betzel", "Richard F.", ""], ["Fukushima", "Makoto", ""], ["He", "Ye", ""], ["Zuo", "Xi-Nian", ""], ["Sporns", "Olaf", ""]]}, {"id": "1511.06380", "submitter": "William Lotter", "authors": "William Lotter, Gabriel Kreiman, David Cox", "title": "Unsupervised Learning of Visual Structure using Predictive Generative\n  Networks", "comments": "under review as conference paper at ICLR 2016", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI cs.CV q-bio.NC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The ability to predict future states of the environment is a central pillar\nof intelligence. At its core, effective prediction requires an internal model\nof the world and an understanding of the rules by which the world changes.\nHere, we explore the internal models developed by deep neural networks trained\nusing a loss based on predicting future frames in synthetic video sequences,\nusing a CNN-LSTM-deCNN framework. We first show that this architecture can\nachieve excellent performance in visual sequence prediction tasks, including\nstate-of-the-art performance in a standard 'bouncing balls' dataset (Sutskever\net al., 2009). Using a weighted mean-squared error and adversarial loss\n(Goodfellow et al., 2014), the same architecture successfully extrapolates\nout-of-the-plane rotations of computer-generated faces. Furthermore, despite\nbeing trained end-to-end to predict only pixel-level information, our\nPredictive Generative Networks learn a representation of the latent structure\nof the underlying three-dimensional objects themselves. Importantly, we find\nthat this representation is naturally tolerant to object transformations, and\ngeneralizes well to new tasks, such as classification of static images. Similar\nmodels trained solely with a reconstruction loss fail to generalize as\neffectively. We argue that prediction can serve as a powerful unsupervised loss\nfor learning rich internal representations of high-level object features.\n", "versions": [{"version": "v1", "created": "Thu, 19 Nov 2015 21:10:17 GMT"}, {"version": "v2", "created": "Wed, 20 Jan 2016 05:50:46 GMT"}], "update_date": "2016-01-21", "authors_parsed": [["Lotter", "William", ""], ["Kreiman", "Gabriel", ""], ["Cox", "David", ""]]}, {"id": "1511.06427", "submitter": "Makoto Fukushima", "authors": "Makoto Fukushima, Richard F. Betzel, Ye He, Marcel A. de Reus, Martijn\n  P. van den Heuvel, Xi-Nian Zuo, Olaf Sporns", "title": "Fluctuations between high- and low-modularity topology in time-resolved\n  functional connectivity", "comments": "Reorganized the paper; to appear in NeuroImage; arXiv abstract\n  shortened to fit within character limits", "journal-ref": "NeuroImage, vol. 180, pp. 406-416, 2018", "doi": "10.1016/j.neuroimage.2017.08.044", "report-no": null, "categories": "q-bio.NC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Modularity is an important topological attribute for functional brain\nnetworks. Recent studies have reported that modularity of functional networks\nvaries not only across individuals being related to demographics and cognitive\nperformance, but also within individuals co-occurring with fluctuations in\nnetwork properties of functional connectivity, estimated over short time\nintervals. However, characteristics of these time-resolved functional networks\nduring periods of high and low modularity have remained largely unexplored. In\nthis study we investigate spatiotemporal properties of time-resolved networks\nin the high and low modularity periods during rest, with a particular focus on\ntheir spatial connectivity patterns, temporal homogeneity and test-retest\nreliability. We show that spatial connectivity patterns of time-resolved\nnetworks in the high and low modularity periods are represented by increased\nand decreased dissociation of the default mode network module from\ntask-positive network modules, respectively. We also find that the instances of\ntime-resolved functional connectivity sampled from within the high (low)\nmodularity period are relatively homogeneous (heterogeneous) over time,\nindicating that during the low modularity period the default mode network\ninteracts with other networks in a variable manner. We confirmed that the\noccurrence of the high and low modularity periods varies across individuals\nwith moderate inter-session test-retest reliability and that it is correlated\nwith previously-reported individual differences in the modularity of functional\nconnectivity estimated over longer timescales. Our findings illustrate how\ntime-resolved functional networks are spatiotemporally organized during periods\nof high and low modularity, allowing one to trace individual differences in\nlong-timescale modularity to the variable occurrence of network configurations\nat shorter timescales.\n", "versions": [{"version": "v1", "created": "Thu, 19 Nov 2015 22:35:55 GMT"}, {"version": "v2", "created": "Wed, 24 Aug 2016 18:40:03 GMT"}, {"version": "v3", "created": "Tue, 22 Aug 2017 22:01:42 GMT"}], "update_date": "2018-10-05", "authors_parsed": [["Fukushima", "Makoto", ""], ["Betzel", "Richard F.", ""], ["He", "Ye", ""], ["de Reus", "Marcel A.", ""], ["Heuvel", "Martijn P. van den", ""], ["Zuo", "Xi-Nian", ""], ["Sporns", "Olaf", ""]]}, {"id": "1511.06920", "submitter": "David Angulo-Garcia", "authors": "David Angulo-Garcia, Joshua D. Berke, Alessandro Torcini", "title": "Cell assembly dynamics of sparsely-connected inhibitory networks: a\n  simple model for the collective activity of striatal projection neurons", "comments": "22 pages, 9 figures", "journal-ref": "PLOS Computational Biology 12(2): e1004778 (2016)", "doi": "10.1371/journal.pcbi.1004778", "report-no": null, "categories": "q-bio.NC nlin.CD", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Striatal projection neurons form a sparsely-connected inhibitory network, and\nthis arrangement may be essential for the appropriate temporal organization of\nbehavior. Here we show that a simplified, sparse inhibitory network of\nLeaky-Integrate-and-Fire neurons can reproduce some key features of striatal\npopulation activity, as observed in brain slices [Carrillo-Reid et al., J.\nNeurophysiology 99 (2008) 1435{1450]. In particular we develop a new metric to\ndetermine the conditions under which sparse inhibitory networks form\nanti-correlated cell assemblies with time-varying activity of individual cells.\nWe found that under these conditions the network displays an input-specific\nsequence of cell assembly switching, that effectively discriminates similar\ninputs. Our results support the proposal [Ponzi and Wickens, PLoS Comp Biol 9\n(2013) e1002954] that GABAergic connections between striatal projection neurons\nallow stimulus-selective, temporally-extended sequential activation of cell\nassemblies. Furthermore, we help to show how altered intrastriatal GABAergic\nsignaling may produce aberrant network-level information processing in\ndisorders such as Parkinson's and Huntington's diseases.\n", "versions": [{"version": "v1", "created": "Sat, 21 Nov 2015 20:23:44 GMT"}], "update_date": "2016-06-29", "authors_parsed": [["Angulo-Garcia", "David", ""], ["Berke", "Joshua D.", ""], ["Torcini", "Alessandro", ""]]}, {"id": "1511.07222", "submitter": "Sarah Beul", "authors": "Sarah F. Beul, Helen Barbas, Claus C. Hilgetag", "title": "A predictive structural model of the primate connectome", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "q-bio.NC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Anatomical connectivity imposes strong constraints on brain function, but\nthere is no general agreement about principles that govern its organization.\nBased on extensive quantitative data we tested the power of three models to\npredict connections of the primate cerebral cortex: architectonic similarity\n(structural model), spatial proximity (distance model) and thickness similarity\n(thickness model). Architectonic similarity showed the strongest and most\nconsistent influence on connection features. This parameter was strongly\nassociated with the presence or absence of inter-areal connections and when\nintegrated with spatial distance, the model allowed predicting the existence of\nprojections with very high accuracy. Moreover, architectonic similarity was\nstrongly related to the laminar pattern of projections origins, and the\nabsolute number of cortical connections of an area. By contrast, cortical\nthickness similarity and distance were not systematically related to connection\nfeatures. These findings suggest that cortical architecture provides a general\norganizing principle for connections in the primate brain.\n", "versions": [{"version": "v1", "created": "Mon, 23 Nov 2015 13:48:23 GMT"}, {"version": "v2", "created": "Mon, 30 May 2016 12:03:05 GMT"}], "update_date": "2016-05-31", "authors_parsed": [["Beul", "Sarah F.", ""], ["Barbas", "Helen", ""], ["Hilgetag", "Claus C.", ""]]}, {"id": "1511.07242", "submitter": "Ihor Lubashevsky", "authors": "Ihor Lubashevsky and Marie Watanabe", "title": "Statistical Properties of Gray Color Categorization: Asymptotics of\n  Psychometric Function", "comments": "Presented at the 47th ISCIE International Symposium on Stochastic\n  Systems Theory and Its Applications, December 5-8, 2015, Honolulu, Hawaii,\n  USA", "journal-ref": null, "doi": null, "report-no": null, "categories": "q-bio.NC physics.bio-ph", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The results of our experiments on categorical perception of different shades\nof gray are reported. A special color generator was created for conducting the\nexperiments on categorizing a random sequence of colors into two classes,\nlight-gray and dark-gray. The collected data are analyzed based on constructing\n(i) the asymptotics of the corresponding psychometric functions and (ii) the\nmean decision time in categorizing a given shade of gray depending on the shade\nbrightness (shade number). Conclusions about plausible mechanisms governing\ncategorical perception, at least for the analyzed system, are drawn.\n", "versions": [{"version": "v1", "created": "Sun, 15 Nov 2015 00:16:18 GMT"}], "update_date": "2015-11-24", "authors_parsed": [["Lubashevsky", "Ihor", ""], ["Watanabe", "Marie", ""]]}, {"id": "1511.07827", "submitter": "Robert Leech", "authors": "Romy Lorenz, Ricardo P Monti, Ines R Violante, Aldo A Faisal,\n  Christoforos Anagnostopoulos, Robert Leech and Giovanni Montana", "title": "Stopping criteria for boosting automatic experimental design using\n  real-time fMRI with Bayesian optimization", "comments": "Oral presentation at MLINI 2015 - 5th NIPS Workshop on Machine\n  Learning and Interpretation in Neuroimaging: Beyond the Scanner", "journal-ref": null, "doi": null, "report-no": "MLINI/2015/15", "categories": "q-bio.NC stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Bayesian optimization has been proposed as a practical and efficient tool\nthrough which to tune parameters in many difficult settings. Recently, such\ntechniques have been combined with real-time fMRI to propose a novel framework\nwhich turns on its head the conventional functional neuroimaging approach. This\nclosed-loop method automatically designs the optimal experiment to evoke a\ndesired target brain pattern. One of the challenges associated with extending\nsuch methods to real-time brain imaging is the need for adequate stopping\ncriteria, an aspect of Bayesian optimization which has received limited\nattention. In light of high scanning costs and limited attentional capacities\nof subjects an accurate and reliable stopping criteria is essential. In order\nto address this issue we propose and empirically study the performance of two\nstopping criteria.\n", "versions": [{"version": "v1", "created": "Tue, 24 Nov 2015 18:28:54 GMT"}, {"version": "v2", "created": "Tue, 22 Mar 2016 16:54:36 GMT"}], "update_date": "2016-06-10", "authors_parsed": [["Lorenz", "Romy", ""], ["Monti", "Ricardo P", ""], ["Violante", "Ines R", ""], ["Faisal", "Aldo A", ""], ["Anagnostopoulos", "Christoforos", ""], ["Leech", "Robert", ""], ["Montana", "Giovanni", ""]]}, {"id": "1511.07962", "submitter": "Christoph Adami", "authors": "Jory Schossau, Christoph Adami, and Arend Hintze", "title": "Information-theoretic neuro-correlates boost evolution of cognitive\n  systems", "comments": "26 pages, 6 figures plus 3 Suppl. figures (included). To appear in\n  special issue \"Information Theoretic Incentives for Cognitive Systems\" of\n  journal \"Entropy\"", "journal-ref": "Entropy 18 (2016) 6", "doi": "10.3390/e18010006", "report-no": null, "categories": "q-bio.NC cs.IT math.IT nlin.AO q-bio.PE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Genetic Algorithms (GA) are a powerful set of tools for search and\noptimization that mimic the process of natural selection, and have been used\nsuccessfully in a wide variety of problems, including evolving neural networks\nto solve cognitive tasks. Despite their success, GAs sometimes fail to locate\nthe highest peaks of the fitness landscape, in particular if the landscape is\nrugged and contains multiple peaks. Reaching distant and higher peaks is\ndifficult because valleys need to be crossed, in a process that (at least\ntemporarily) runs against the fitness maximization objective. Here we propose\nand test a number of information-theoretic (as well as network-based) measures\nthat can be used in conjunction with a fitness maximization objective\n(so-called ``neuro-correlates\") to evolve neural controllers for two widely\ndifferent tasks: a behavioral task that requires information integration, and a\ncognitive task that requires memory and logic. We find that judiciously chosen\nneuro-correlates can significantly aid GAs to find the highest peaks.\n", "versions": [{"version": "v1", "created": "Wed, 25 Nov 2015 06:07:10 GMT"}], "update_date": "2016-01-05", "authors_parsed": [["Schossau", "Jory", ""], ["Adami", "Christoph", ""], ["Hintze", "Arend", ""]]}, {"id": "1511.08260", "submitter": "Nancy (Xin Ru) Wang", "authors": "Nancy X. R. Wang, Jared D. Olson, Jeffrey G. Ojemann, Rajesh P.N. Rao,\n  Bingni W. Brunton", "title": "Unsupervised decoding of long-term, naturalistic human neural recordings\n  with automated video and audio annotations", "comments": null, "journal-ref": "Frontiers in human neuroscience 2016", "doi": "10.3389/fnhum.2016.00165", "report-no": null, "categories": "q-bio.NC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Fully automated decoding of human activities and intentions from direct\nneural recordings is a tantalizing challenge in brain-computer interfacing.\nMost ongoing efforts have focused on training decoders on specific, stereotyped\ntasks in laboratory settings. Implementing brain-computer interfaces (BCIs) in\nnatural settings requires adaptive strategies and scalable algorithms that\nrequire minimal supervision. Here we propose an unsupervised approach to\ndecoding neural states from human brain recordings acquired in a naturalistic\ncontext. We demonstrate our approach on continuous long-term\nelectrocorticographic (ECoG) data recorded over many days from the brain\nsurface of subjects in a hospital room, with simultaneous audio and video\nrecordings. We first discovered clusters in high-dimensional ECoG recordings\nand then annotated coherent clusters using speech and movement labels extracted\nautomatically from audio and video recordings. To our knowledge, this\nrepresents the first time techniques from computer vision and speech processing\nhave been used for natural ECoG decoding. Our results show that our\nunsupervised approach can discover distinct behaviors from ECoG data, including\nmoving, speaking and resting. We verify the accuracy of our approach by\ncomparing to manual annotations. Projecting the discovered cluster centers back\nonto the brain, this technique opens the door to automated functional brain\nmapping in natural settings.\n", "versions": [{"version": "v1", "created": "Thu, 26 Nov 2015 01:02:03 GMT"}, {"version": "v2", "created": "Tue, 8 Dec 2015 06:44:07 GMT"}], "update_date": "2018-01-22", "authors_parsed": [["Wang", "Nancy X. R.", ""], ["Olson", "Jared D.", ""], ["Ojemann", "Jeffrey G.", ""], ["Rao", "Rajesh P. N.", ""], ["Brunton", "Bingni W.", ""]]}, {"id": "1511.08779", "submitter": "Jaan Aru", "authors": "Ardi Tampuu, Tambet Matiisen, Dorian Kodelja, Ilya Kuzovkin, Kristjan\n  Korjus, Juhan Aru, Jaan Aru and Raul Vicente", "title": "Multiagent Cooperation and Competition with Deep Reinforcement Learning", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.LG q-bio.NC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Multiagent systems appear in most social, economical, and political\nsituations. In the present work we extend the Deep Q-Learning Network\narchitecture proposed by Google DeepMind to multiagent environments and\ninvestigate how two agents controlled by independent Deep Q-Networks interact\nin the classic videogame Pong. By manipulating the classical rewarding scheme\nof Pong we demonstrate how competitive and collaborative behaviors emerge.\nCompetitive agents learn to play and score efficiently. Agents trained under\ncollaborative rewarding schemes find an optimal strategy to keep the ball in\nthe game as long as possible. We also describe the progression from competitive\nto collaborative behavior. The present work demonstrates that Deep Q-Networks\ncan become a practical tool for studying the decentralized learning of\nmultiagent systems living in highly complex environments.\n", "versions": [{"version": "v1", "created": "Fri, 27 Nov 2015 20:01:45 GMT"}], "update_date": "2015-11-30", "authors_parsed": [["Tampuu", "Ardi", ""], ["Matiisen", "Tambet", ""], ["Kodelja", "Dorian", ""], ["Kuzovkin", "Ilya", ""], ["Korjus", "Kristjan", ""], ["Aru", "Juhan", ""], ["Aru", "Jaan", ""], ["Vicente", "Raul", ""]]}, {"id": "1511.08855", "submitter": "Francisco De Sousa Webber", "authors": "Francisco De Sousa Webber", "title": "Semantic Folding Theory And its Application in Semantic Fingerprinting", "comments": "59 pages, white paper", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.CL q-bio.NC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Human language is recognized as a very complex domain since decades. No\ncomputer system has been able to reach human levels of performance so far. The\nonly known computational system capable of proper language processing is the\nhuman brain. While we gather more and more data about the brain, its\nfundamental computational processes still remain obscure. The lack of a sound\ncomputational brain theory also prevents the fundamental understanding of\nNatural Language Processing. As always when science lacks a theoretical\nfoundation, statistical modeling is applied to accommodate as many sampled\nreal-world data as possible. An unsolved fundamental issue is the actual\nrepresentation of language (data) within the brain, denoted as the\nRepresentational Problem. Starting with Jeff Hawkins' Hierarchical Temporal\nMemory (HTM) theory, a consistent computational theory of the human cortex, we\nhave developed a corresponding theory of language data representation: The\nSemantic Folding Theory. The process of encoding words, by using a topographic\nsemantic space as distributional reference frame into a sparse binary\nrepresentational vector is called Semantic Folding and is the central topic of\nthis document. Semantic Folding describes a method of converting language from\nits symbolic representation (text) into an explicit, semantically grounded\nrepresentation that can be generically processed by Hawkins' HTM networks. As\nit turned out, this change in representation, by itself, can solve many complex\nNLP problems by applying Boolean operators and a generic similarity function\nlike the Euclidian Distance. Many practical problems of statistical NLP\nsystems, like the high cost of computation, the fundamental incongruity of\nprecision and recall , the complex tuning procedures etc., can be elegantly\novercome by applying Semantic Folding.\n", "versions": [{"version": "v1", "created": "Sat, 28 Nov 2015 00:13:09 GMT"}, {"version": "v2", "created": "Wed, 16 Mar 2016 22:04:51 GMT"}], "update_date": "2016-03-18", "authors_parsed": [["Webber", "Francisco De Sousa", ""]]}, {"id": "1511.09325", "submitter": "Pier Stanislao Paolucci", "authors": "Elena Pastorelli, Pier Stanislao Paolucci, Roberto Ammendola, Andrea\n  Biagioni, Ottorino Frezza, Francesca Lo Cicero, Alessandro Lonardo, Michele\n  Martinelli, Francesco Simula, Piero Vicini", "title": "Scaling to 1024 software processes and hardware cores of the distributed\n  simulation of a spiking neural network including up to 20G synapses", "comments": "6 pages, 4 figures, 1 table", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC q-bio.NC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This short report describes the scaling, up to 1024 software processes and\nhardware cores, of a distributed simulator of plastic spiking neural networks.\nA previous report demonstrated good scalability of the simulator up to 128\nprocesses. Herein we extend the speed-up measurements and strong and weak\nscaling analysis of the simulator to the range between 1 and 1024 software\nprocesses and hardware cores. We simulated two-dimensional grids of cortical\ncolumns including up to ~20G synapses connecting ~11M neurons. The neural\nnetwork was distributed over a set of MPI processes and the simulations were\nrun on a server platform composed of up to 64 dual-socket nodes, each socket\nequipped with Intel Haswell E5-2630 v3 processors (8 cores @ 2.4 GHz clock).\nAll nodes are interconned through an InfiniBand network. The DPSNN simulator\nhas been developed by INFN in the framework of EURETILE and CORTICONIC European\nFET Project and will be used by the WaveScalEW tem in the framework of the\nHuman Brain Project (HBP), SubProject 2 - Cognitive and Systems Neuroscience.\nThis report lays the groundwork for a more thorough comparison with the neural\nsimulation tool NEST.\n", "versions": [{"version": "v1", "created": "Mon, 30 Nov 2015 14:30:42 GMT"}], "update_date": "2015-12-01", "authors_parsed": [["Pastorelli", "Elena", ""], ["Paolucci", "Pier Stanislao", ""], ["Ammendola", "Roberto", ""], ["Biagioni", "Andrea", ""], ["Frezza", "Ottorino", ""], ["Cicero", "Francesca Lo", ""], ["Lonardo", "Alessandro", ""], ["Martinelli", "Michele", ""], ["Simula", "Francesco", ""], ["Vicini", "Piero", ""]]}, {"id": "1511.09364", "submitter": "Maximilian Schmidt", "authors": "Maximilian Schmidt, Rembrandt Bakker, Kelly Shen, Gleb Bezgin,\n  Claus-Christian Hilgetag, Markus Diesmann and Sacha J. van Albada", "title": "Full-density multi-scale account of structure and dynamics of macaque\n  visual cortex", "comments": null, "journal-ref": null, "doi": "10.1371/journal.pcbi.1006359", "report-no": null, "categories": "q-bio.NC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a multi-scale spiking network model of all vision-related areas of\nmacaque cortex that represents each area by a full-scale microcircuit with\narea-specific architecture. The layer- and population-resolved network\nconnectivity integrates axonal tracing data from the CoCoMac database with\nrecent quantitative tracing data, and is systematically refined using dynamical\nconstraints. Simulations reveal a stable asynchronous irregular ground state\nwith heterogeneous activity across areas, layers, and populations. Elicited by\nlarge-scale interactions, the model reproduces longer intrinsic time scales in\nhigher compared to early visual areas. Activity propagates down the visual\nhierarchy, similar to experimental results associated with visual imagery.\nCortico-cortical interaction patterns agree well with fMRI resting-state\nfunctional connectivity. The model bridges the gap between local and\nlarge-scale accounts of cortex, and clarifies how the detailed connectivity of\ncortex shapes its dynamics on multiple scales.\n", "versions": [{"version": "v1", "created": "Mon, 30 Nov 2015 16:06:40 GMT"}, {"version": "v2", "created": "Tue, 1 Dec 2015 15:25:06 GMT"}, {"version": "v3", "created": "Mon, 8 Feb 2016 19:20:50 GMT"}, {"version": "v4", "created": "Fri, 15 Apr 2016 08:05:14 GMT"}], "update_date": "2018-10-23", "authors_parsed": [["Schmidt", "Maximilian", ""], ["Bakker", "Rembrandt", ""], ["Shen", "Kelly", ""], ["Bezgin", "Gleb", ""], ["Hilgetag", "Claus-Christian", ""], ["Diesmann", "Markus", ""], ["van Albada", "Sacha J.", ""]]}, {"id": "1511.09426", "submitter": "Cengiz Pehlevan", "authors": "Cengiz Pehlevan, Dmitri B. Chklovskii", "title": "A Normative Theory of Adaptive Dimensionality Reduction in Neural\n  Networks", "comments": "Advances in Neural Information Processing Systems (NIPS), 2015", "journal-ref": null, "doi": null, "report-no": null, "categories": "q-bio.NC cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  To make sense of the world our brains must analyze high-dimensional datasets\nstreamed by our sensory organs. Because such analysis begins with\ndimensionality reduction, modelling early sensory processing requires\nbiologically plausible online dimensionality reduction algorithms. Recently, we\nderived such an algorithm, termed similarity matching, from a Multidimensional\nScaling (MDS) objective function. However, in the existing algorithm, the\nnumber of output dimensions is set a priori by the number of output neurons and\ncannot be changed. Because the number of informative dimensions in sensory\ninputs is variable there is a need for adaptive dimensionality reduction. Here,\nwe derive biologically plausible dimensionality reduction algorithms which\nadapt the number of output dimensions to the eigenspectrum of the input\ncovariance matrix. We formulate three objective functions which, in the offline\nsetting, are optimized by the projections of the input dataset onto its\nprincipal subspace scaled by the eigenvalues of the output covariance matrix.\nIn turn, the output eigenvalues are computed as i) soft-thresholded, ii)\nhard-thresholded, iii) equalized thresholded eigenvalues of the input\ncovariance matrix. In the online setting, we derive the three corresponding\nadaptive algorithms and map them onto the dynamics of neuronal activity in\nnetworks with biologically plausible local learning rules. Remarkably, in the\nlast two networks, neurons are divided into two classes which we identify with\nprincipal neurons and interneurons in biological circuits.\n", "versions": [{"version": "v1", "created": "Mon, 30 Nov 2015 18:45:30 GMT"}, {"version": "v2", "created": "Tue, 26 Jan 2016 18:44:23 GMT"}], "update_date": "2016-01-27", "authors_parsed": [["Pehlevan", "Cengiz", ""], ["Chklovskii", "Dmitri B.", ""]]}, {"id": "1511.09468", "submitter": "Cengiz Pehlevan", "authors": "Cengiz Pehlevan, Dmitri B. Chklovskii", "title": "Optimization theory of Hebbian/anti-Hebbian networks for PCA and\n  whitening", "comments": "Annual Allerton Conference on Communication, Control, and Computing\n  (Allerton) 2015", "journal-ref": null, "doi": "10.1109/ALLERTON.2015.7447180", "report-no": null, "categories": "q-bio.NC cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In analyzing information streamed by sensory organs, our brains face\nchallenges similar to those solved in statistical signal processing. This\nsuggests that biologically plausible implementations of online signal\nprocessing algorithms may model neural computation. Here, we focus on such\nworkhorses of signal processing as Principal Component Analysis (PCA) and\nwhitening which maximize information transmission in the presence of noise. We\nadopt the similarity matching framework, recently developed for principal\nsubspace extraction, but modify the existing objective functions by adding a\ndecorrelating term. From the modified objective functions, we derive online PCA\nand whitening algorithms which are implementable by neural networks with local\nlearning rules, i.e. synaptic weight updates that depend on the activity of\nonly pre- and postsynaptic neurons. Our theory offers a principled model of\nneural computations and makes testable predictions such as the dropout of\nunderutilized neurons.\n", "versions": [{"version": "v1", "created": "Mon, 30 Nov 2015 20:52:39 GMT"}], "update_date": "2016-04-27", "authors_parsed": [["Pehlevan", "Cengiz", ""], ["Chklovskii", "Dmitri B.", ""]]}]