[{"id": "1411.0054", "submitter": "Emanuel Diamant", "authors": "Emanuel Diamant", "title": "Cognitive image processing: the time is right to recognize that the\n  world does not rest more on turtles and elephants", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CY q-bio.NC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Traditional image processing is a field of science and technology developed\nto facilitate human-centered image management. But today, when huge volumes of\nvisual data inundate our surroundings (due to the explosive growth of\nimage-capturing devices, proliferation of Internet communication means and\nvideo sharing services over the World Wide Web), human-centered handling of\nBig-data flows is impossible anymore. Therefore, it has to be replaced with a\nmachine (computer) supported counterpart. Of course, such an artificial\ncounterpart must be equipped with some cognitive abilities, usually\ncharacteristic for a human being. Indeed, in the past decade, a new computer\ndesign trend - Cognitive Computer development - is become visible. Cognitive\nimage processing definitely will be one of its main duties. It must be\nspecially mentioned that this trend is a particular case of a much more general\nmovement - the transition from a \"computational data-processing paradigm\" to a\n\"cognitive information-processing paradigm\", which affects today many fields of\nscience, technology, and engineering. This transition is a blessed novelty, but\nits success is hampered by the lack of a clear delimitation between the notion\nof data and the notion of information. Elaborating the case of cognitive image\nprocessing, the paper intends to clarify these important research issues.\n", "versions": [{"version": "v1", "created": "Sat, 1 Nov 2014 01:50:01 GMT"}], "update_date": "2014-11-05", "authors_parsed": [["Diamant", "Emanuel", ""]]}, {"id": "1411.0159", "submitter": "Viktoras Veitas Mr.", "authors": "David Weinbaum (Weaver) and Viktoras Veitas", "title": "Synthetic Cognitive Development: where intelligence comes from", "comments": "Preprint. 28 pages LaTeX, 5 figures, 1 table; en-US proofreading;\n  section 4.2 rewritten; bibliography corrected", "journal-ref": null, "doi": null, "report-no": null, "categories": "q-bio.NC nlin.AO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The human cognitive system is a remarkable exemplar of a general intelligent\nsystem whose competence is not confined to a specific problem domain.\nEvidently, general cognitive competences are a product of a prolonged and\ncomplex process of cognitive development. Therefore, the process of cognitive\ndevelopment is a primary key to understanding the emergence of intelligent\nbehavior. This paper develops the theoretical foundations for a model that\ngeneralizes the process of cognitive development. The model aims to provide a\nrealistic scheme for the synthesis of scalable cognitive systems with an\nopen-ended range of capabilities. Major concepts and theories of human\ncognitive development are introduced and briefly explored focusing on the\nenactive approach to cognition and the concept of sense-making. The initial\nscheme of human cognitive development is then generalized by introducing the\nphilosophy of individuation and the abstract mechanism of transduction. The\ntheory of individuation provides the ground for the necessary paradigmatic\nshift from cognitive systems as given products to cognitive development as a\nformative process of self-organization. Next, the conceptual model is specified\nas a scalable scheme of networks of agents. The mechanisms of individuation are\nformulated in context independent information theoretical terms. Finally, the\npaper discusses two concrete aspects of the generative model -- mechanisms of\ntransduction and value modulating systems. These are topics of further research\ntowards a computationally realizable model.\n", "versions": [{"version": "v1", "created": "Sat, 1 Nov 2014 19:28:51 GMT"}, {"version": "v2", "created": "Sat, 13 Dec 2014 18:57:14 GMT"}], "update_date": "2014-12-16", "authors_parsed": [["Weinbaum", "David", "", "Weaver"], ["Veitas", "Viktoras", ""]]}, {"id": "1411.0165", "submitter": "Markus Meister", "authors": "Markus Meister", "title": "Can Humans Really Discriminate 1 Trillion Odors?", "comments": "14 pages, 4 figures. Revised version has same technical content, more\n  introduction for non-experts, more thoughts in the discussion", "journal-ref": null, "doi": "10.7554/eLife.07865", "report-no": null, "categories": "q-bio.NC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A recent paper in a prominent science magazine claims to show that humans can\ndiscriminate at least 1 trillion odors. The authors reached that conclusion\nafter performing just 260 comparisons of two smells, of which about half could\nbe discriminated. Furthermore the paper claims that the human ability to\ndiscriminate smells vastly exceeds our abilities to discriminate colors or\nmusical tones. Here I show that all these statements are wrong by astronomical\nfactors. A reanalysis of the authors' experiments shows they are also\nconsistent with humans discriminating just 10 odors. The paper's extravagant\nclaims are based on errors of mathematical logic. Further analysis highlights\nthe importance of establishing how many dimensions the perceptual odor space\nhas. I review some arguments on the topic and propose experimental avenues\ntowards an answer.\n", "versions": [{"version": "v1", "created": "Sat, 1 Nov 2014 20:32:04 GMT"}, {"version": "v2", "created": "Thu, 20 Nov 2014 04:42:26 GMT"}], "update_date": "2016-09-09", "authors_parsed": [["Meister", "Markus", ""]]}, {"id": "1411.0247", "submitter": "Timothy Lillicrap", "authors": "Timothy P. Lillicrap, Daniel Cownden, Douglas B. Tweed, Colin J.\n  Akerman", "title": "Random feedback weights support learning in deep neural networks", "comments": "14 pages, 5 figures in main text; 13 pages appendix", "journal-ref": null, "doi": null, "report-no": null, "categories": "q-bio.NC cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The brain processes information through many layers of neurons. This deep\narchitecture is representationally powerful, but it complicates learning by\nmaking it hard to identify the responsible neurons when a mistake is made. In\nmachine learning, the backpropagation algorithm assigns blame to a neuron by\ncomputing exactly how it contributed to an error. To do this, it multiplies\nerror signals by matrices consisting of all the synaptic weights on the\nneuron's axon and farther downstream. This operation requires a precisely\nchoreographed transport of synaptic weight information, which is thought to be\nimpossible in the brain. Here we present a surprisingly simple algorithm for\ndeep learning, which assigns blame by multiplying error signals by random\nsynaptic weights. We show that a network can learn to extract useful\ninformation from signals sent through these random feedback connections. In\nessence, the network learns to learn. We demonstrate that this new mechanism\nperforms as quickly and accurately as backpropagation on a variety of problems\nand describe the principles which underlie its function. Our demonstration\nprovides a plausible basis for how a neuron can be adapted using error signals\ngenerated at distal locations in the brain, and thus dispels long-held\nassumptions about the algorithmic constraints on learning in neural circuits.\n", "versions": [{"version": "v1", "created": "Sun, 2 Nov 2014 12:31:15 GMT"}], "update_date": "2014-11-04", "authors_parsed": [["Lillicrap", "Timothy P.", ""], ["Cownden", "Daniel", ""], ["Tweed", "Douglas B.", ""], ["Akerman", "Colin J.", ""]]}, {"id": "1411.0291", "submitter": "Li Zhaoping", "authors": "Li Zhaoping and Li Zhe", "title": "Primary visual cortex as a saliency map: parameter-free prediction of\n  behavior from V1 physiology", "comments": "11 figures, 66 pages", "journal-ref": "PLoS Comput Biol 11(10): e1004375 (2015)", "doi": "10.1371/journal.pcbi.1004375", "report-no": null, "categories": "q-bio.NC physics.bio-ph", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  It has been hypothesized that neural activities in the primary visual cortex\n(V1) represent a saliency map of the visual field to exogenously guide\nattention. This hypothesis has so far provided only qualitative predictions and\ntheir confirmations. We report this hypothesis' first quantitative prediction,\nderived without free parameters, and its confirmation by human behavioral data.\nThe hypothesis provides a direct link between V1 neural responses to a visual\nlocation and the saliency of that location to guide attention exogenously. In a\nvisual input containing many bars, one of them saliently different from all the\nother bars which are identical to each other, saliency at the singleton's\nlocation can be measured by the shortness of the reaction time in a visual\nsearch task to find the singleton. The hypothesis predicts quantitatively the\nwhole distribution of the reaction times to find a singleton unique in color,\norientation, and motion direction from the reaction times to find other types\nof singletons. The predicted distribution matches the experimentally observed\ndistribution in all six human observers. A requirement for this successful\nprediction is a data-motivated assumption that V1 lacks neurons tuned\nsimultaneously to color, orientation, and motion direction of visual inputs.\nSince evidence suggests that extrastriate cortices do have such neurons, we\ndiscuss the possibility that the extrastriate cortices play no role in guiding\nexogenous attention so that they can be devoted to other functional roles like\nvisual decoding or endogenous attention.\n", "versions": [{"version": "v1", "created": "Sun, 2 Nov 2014 18:21:39 GMT"}], "update_date": "2015-10-08", "authors_parsed": [["Zhaoping", "Li", ""], ["Zhe", "Li", ""]]}, {"id": "1411.0431", "submitter": "Valmir Barbosa", "authors": "Luciano Dyballa, Valmir C. Barbosa", "title": "Further insights into the interareal connectivity of a cortical network", "comments": null, "journal-ref": "Network Science 3 (2015), 526-550", "doi": "10.1017/nws.2015.19", "report-no": null, "categories": "q-bio.NC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Over the past years, network science has proven invaluable as a means to\nbetter understand many of the processes taking place in the brain. Recently,\ninterareal connectivity data of the macaque cortex was made available with\ngreat richness of detail. We explore new aspects of this dataset, such as a\ncorrelation between connection weights and cortical hierarchy. We also look at\nthe link-community structure that emerges from the data to uncover the major\ncommunication pathways in the network, and moreover investigate its reciprocal\nconnections, showing that they share similar properties.\n", "versions": [{"version": "v1", "created": "Mon, 3 Nov 2014 11:22:27 GMT"}], "update_date": "2015-12-24", "authors_parsed": [["Dyballa", "Luciano", ""], ["Barbosa", "Valmir C.", ""]]}, {"id": "1411.0432", "submitter": "Jannis Schuecker", "authors": "Jannis Schuecker and Markus Diesmann and Moritz Helias", "title": "Modulated escape from a metastable state driven by colored noise", "comments": null, "journal-ref": "Phys. Rev. E 92, 052119 (2015)", "doi": "10.1103/PhysRevE.92.052119", "report-no": null, "categories": "cond-mat.stat-mech q-bio.NC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Many phenomena in nature are described by excitable systems driven by colored\nnoise. The temporal correlations in the fluctuations hinder an analytical\ntreatment. We here present a general method of reduction to a white-noise\nsystem, capturing the color of the noise by effective and time-dependent\nboundary conditions. We apply the formalism to a model of the excitability of\nneuronal membranes, the leaky integrate-and-fire neuron model, revealing an\nanalytical expression for the linear response of the system valid up to\nmoderate frequencies. The closed form analytical expression enables the\ncharacterization of the response properties of such excitable units and the\nassessment of oscillations emerging in networks thereof.\n", "versions": [{"version": "v1", "created": "Mon, 3 Nov 2014 11:24:05 GMT"}, {"version": "v2", "created": "Tue, 17 Feb 2015 18:06:37 GMT"}, {"version": "v3", "created": "Fri, 8 May 2015 15:27:38 GMT"}, {"version": "v4", "created": "Fri, 24 Jul 2015 13:51:17 GMT"}, {"version": "v5", "created": "Mon, 27 Jul 2015 08:34:40 GMT"}, {"version": "v6", "created": "Mon, 24 Aug 2015 15:31:40 GMT"}, {"version": "v7", "created": "Tue, 22 Sep 2015 11:42:47 GMT"}, {"version": "v8", "created": "Fri, 20 Nov 2015 16:15:50 GMT"}], "update_date": "2015-11-25", "authors_parsed": [["Schuecker", "Jannis", ""], ["Diesmann", "Markus", ""], ["Helias", "Moritz", ""]]}, {"id": "1411.1045", "submitter": "Matthias K\\\"ummerer", "authors": "Matthias K\\\"ummerer and Lucas Theis and Matthias Bethge", "title": "Deep Gaze I: Boosting Saliency Prediction with Feature Maps Trained on\n  ImageNet", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV q-bio.NC stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recent results suggest that state-of-the-art saliency models perform far from\noptimal in predicting fixations. This lack in performance has been attributed\nto an inability to model the influence of high-level image features such as\nobjects. Recent seminal advances in applying deep neural networks to tasks like\nobject recognition suggests that they are able to capture this kind of\nstructure. However, the enormous amount of training data necessary to train\nthese networks makes them difficult to apply directly to saliency prediction.\nWe present a novel way of reusing existing neural networks that have been\npretrained on the task of object recognition in models of fixation prediction.\nUsing the well-known network of Krizhevsky et al. (2012), we come up with a new\nsaliency model that significantly outperforms all state-of-the-art models on\nthe MIT Saliency Benchmark. We show that the structure of this network allows\nnew insights in the psychophysics of fixation selection and potentially their\nneural implementation. To train our network, we build on recent work on the\nmodeling of saliency as point processes.\n", "versions": [{"version": "v1", "created": "Tue, 4 Nov 2014 20:56:51 GMT"}, {"version": "v2", "created": "Thu, 18 Dec 2014 17:22:49 GMT"}, {"version": "v3", "created": "Thu, 26 Feb 2015 19:05:11 GMT"}, {"version": "v4", "created": "Thu, 9 Apr 2015 09:48:11 GMT"}], "update_date": "2015-04-10", "authors_parsed": [["K\u00fcmmerer", "Matthias", ""], ["Theis", "Lucas", ""], ["Bethge", "Matthias", ""]]}, {"id": "1411.1190", "submitter": "Bo Chen", "authors": "Bo Chen and Pietro Perona", "title": "Towards an optimal decision strategy of visual search", "comments": "19 pages, 6 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "q-bio.NC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Searching for objects amongst clutter is a key ability of visual systems.\nSpeed and accuracy are often crucial: how can the visual system trade off these\ncompeting quantities for optimal performance in different tasks? How does the\ntrade-off depend on target appearance and scene complexity? We show that the\noptimal tradeoff strategy may be cast as the solution to a partially observable\nMarkov decision process (POMDP) and computed by a dynamic programming\nprocedure. However, this procedure is computationally intensive when the visual\nscene becomes too cluttered. Therefore, we also conjecture an optimal strategy\nthat scales to large number of clutters. Our conjecture applies to homogeneous\nvisual search and for a special case of heterogenous search where the\nsignal-to-noise ratio differs across location. Using the conjecture we show\nthat two existing decision mechanisms for analyzing human data, namely\ndiffusion-to-bound and maximum-of-output, are sub-optimal; the optimal strategy\ninstead employs two scaled diffusions.\n", "versions": [{"version": "v1", "created": "Wed, 5 Nov 2014 08:53:19 GMT"}], "update_date": "2014-11-06", "authors_parsed": [["Chen", "Bo", ""], ["Perona", "Pietro", ""]]}, {"id": "1411.1257", "submitter": "Kourosh Maboudi M.Sc", "authors": "Kourosh Maboudi, Moein Esghaei, and Mohammad Reza Daliri", "title": "Low Frequency LFP in Macaque MT Predicts Reaction Time in an Attentive\n  Task", "comments": "20 pages, 4 figures The paper has been withdrawn by the author due to\n  misinterpretation of output results of the classification algorithm and so\n  the produced figures have some major problems and need more investigation", "journal-ref": null, "doi": null, "report-no": null, "categories": "q-bio.NC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Neural oscillations are related to a wide variety of cognitive functions,\nincluding attention. However, there is still a controversy over the frequency\nbands that have functional roles in attention. In this study, using a spatial\nattention task we found that phase of low frequency oscillations could predict\nthe reaction time of the monkey, when the monkey is attending to the target\nstimulus as opposed to attending a distractor. This finding provides strong\nevidence for the functional role of low frequency bands in attentional\nmodulation of neural activities.\n", "versions": [{"version": "v1", "created": "Wed, 5 Nov 2014 12:49:08 GMT"}, {"version": "v2", "created": "Mon, 10 Nov 2014 04:56:40 GMT"}], "update_date": "2014-11-11", "authors_parsed": [["Maboudi", "Kourosh", ""], ["Esghaei", "Moein", ""], ["Daliri", "Mohammad Reza", ""]]}, {"id": "1411.1400", "submitter": "Si-Wei Qiu", "authors": "Siwei Qiu and Carson Chow", "title": "Field theory for biophysical neural networks", "comments": "7 pages, 3 figures, The 32nd International Symposium on Lattice Field\n  Theory 23-28 June, 2014 Columbia University New York, NY", "journal-ref": null, "doi": null, "report-no": null, "categories": "q-bio.NC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The human brain is a complex system composed of a network of hundreds of\nbillions of discrete neurons that are coupled through time dependent synapses.\nSimulating the entire brain is a daunting challenge. Here, we show how ideas\nfrom quantum field theory can be used to construct an effective reduced theory,\nwhich may be analyzed with lattice computations. We give some examples of how\nthe formalism can be applied to biophysically plausible neural network models.\n", "versions": [{"version": "v1", "created": "Wed, 5 Nov 2014 04:52:06 GMT"}], "update_date": "2014-11-07", "authors_parsed": [["Qiu", "Siwei", ""], ["Chow", "Carson", ""]]}, {"id": "1411.1612", "submitter": "Andrey Demichev", "authors": "Andrey Demichev", "title": "Effect of Activity and Inter-Cluster Correlations on\n  Information-Theoretic Properties of Neural Networks", "comments": "16 pages, 9 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "q-bio.NC cond-mat.dis-nn", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  On the basis of solutions of the master equation for networks with a small\nnumber of neurons it is shown that the conditional entropy and integrated\ninformation of neural networks depend on their average activity and\ninter-cluster correlations.\n", "versions": [{"version": "v1", "created": "Thu, 6 Nov 2014 13:48:41 GMT"}], "update_date": "2014-11-07", "authors_parsed": [["Demichev", "Andrey", ""]]}, {"id": "1411.1658", "submitter": "Peter Thomas PhD", "authors": "Peter J. Thomas and Benjamin Lindner", "title": "Asymptotic Phase for Stochastic Oscillators", "comments": "5 pages, 3 figures", "journal-ref": "Phys. Rev. Lett. 113(25):254101, Dec. 2014", "doi": "10.1103/PhysRevLett.113.254101", "report-no": null, "categories": "q-bio.NC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Oscillations and noise are ubiquitous in physical and biological systems.\nWhen oscillations arise from a deterministic limit cycle, entrainment and\nsynchronization may be analyzed in terms of the asymptotic phase function. In\nthe presence of noise, the asymptotic phase is no longer well defined. We\nintroduce a new definition of asymptotic phase in terms of the slowest decaying\nmodes of the Kolmogorov backward operator. Our stochastic asymptotic phase is\nwell defined for noisy oscillators, even when the oscillations are noise\ndependent. It reduces to the classical asymptotic phase in the limit of\nvanishing noise. The phase can be obtained either by solving an eigenvalue\nproblem, or by empirical observation of an oscillating density's approach to\nits steady state.\n", "versions": [{"version": "v1", "created": "Thu, 6 Nov 2014 16:41:46 GMT"}, {"version": "v2", "created": "Sun, 18 Jan 2015 21:17:02 GMT"}], "update_date": "2015-01-20", "authors_parsed": [["Thomas", "Peter J.", ""], ["Lindner", "Benjamin", ""]]}, {"id": "1411.1949", "submitter": "Roberto D. Pascual-Marqui", "authors": "Roberto D Pascual-Marqui, Dietrich Lehmann, Pascal Faber, Patricia\n  Milz, Kieko Kochi, Masafumi Yoshimura, Keiichiro Nishida, Toshiaki Isotani,\n  Toshihiko Kinoshita", "title": "The resting microstate networks (RMN): cortical distributions, dynamics,\n  and frequency specific information flow", "comments": "pre-print, technical report, The KEY Institute for Brain-Mind\n  Research (Zurich), Kansai Medical University (Osaka)", "journal-ref": null, "doi": null, "report-no": null, "categories": "q-bio.NC q-bio.QM", "license": "http://creativecommons.org/licenses/by-nc-sa/3.0/", "abstract": "  A brain microstate is characterized by a unique, fixed spatial distribution\nof electrically active neurons with time varying amplitude. It is hypothesized\nthat a microstate implements a functional/physiological state of the brain\nduring which specific neural computations are performed. Based on this\nhypothesis, brain electrical activity is modeled as a time sequence of\nnon-overlapping microstates with variable, finite durations (Lehmann and\nSkrandies 1980, 1984; Lehmann et al 1987). In this study, EEG recordings from\n109 participants during eyes closed resting condition are modeled with four\nmicrostates. In a first part, a new confirmatory statistics method is\nintroduced for the determination of the cortical distributions of electric\nneuronal activity that generate each microstate. All microstates have common\nposterior cingulate generators, while three microstates additionally include\nactivity in the left occipital/parietal, right occipital/parietal, and anterior\ncingulate cortices. This appears to be a fragmented version of the\nmetabolically (PET/fMRI) computed default mode network (DMN), supporting the\nnotion that these four regions activate sequentially at high time resolution,\nand that slow metabolic imaging corresponds to a low-pass filtered version. In\nthe second part of this study, the microstate amplitude time series are used as\nthe basis for estimating the strength, directionality, and spectral\ncharacteristics (i.e., which oscillations are preferentially transmitted) of\nthe connections that are mediated by the microstate transitions. The results\nshow that the posterior cingulate is an important hub, sending alpha and beta\noscillatory information to all other microstate generator regions.\nInterestingly, beyond alpha, beta oscillations are essential in the maintenance\nof the brain during resting state.\n", "versions": [{"version": "v1", "created": "Fri, 7 Nov 2014 15:35:34 GMT"}, {"version": "v2", "created": "Sat, 15 Nov 2014 01:52:15 GMT"}], "update_date": "2014-11-18", "authors_parsed": [["Pascual-Marqui", "Roberto D", ""], ["Lehmann", "Dietrich", ""], ["Faber", "Pascal", ""], ["Milz", "Patricia", ""], ["Kochi", "Kieko", ""], ["Yoshimura", "Masafumi", ""], ["Nishida", "Keiichiro", ""], ["Isotani", "Toshiaki", ""], ["Kinoshita", "Toshihiko", ""]]}, {"id": "1411.2103", "submitter": "Michael Schaub", "authors": "Yazan N. Billeh, Michael T. Schaub, Costas A. Anastassiou, Mauricio\n  Barahona, Christof Koch", "title": "Revealing cell assemblies at multiple levels of granularity", "comments": "18 pages; 13 Figures; published as open access in J Neuro Methods", "journal-ref": "Journal of Neuroscience Methods, Volume 236, 30 October 2014,\n  Pages 92-106, ISSN 0165-0270", "doi": "10.1016/j.jneumeth.2014.08.011", "report-no": null, "categories": "q-bio.NC physics.data-an", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Background: Current neuronal monitoring techniques, such as calcium imaging\nand multi-electrode arrays, enable recordings of spiking activity from hundreds\nof neurons simultaneously. Of primary importance in systems neuroscience is the\nidentification of cell assemblies: groups of neurons that cooperate in some\nform within the recorded population.\n  New Method: We introduce a simple, integrated framework for the detection of\ncell-assemblies from spiking data without a priori assumptions about the size\nor number of groups present. We define a biophysically-inspired measure to\nextract a directed functional connectivity matrix between both excitatory and\ninhibitory neurons based on their spiking history. The resulting network\nrepresentation is analyzed using the Markov Stability framework, a graph\ntheoretical method for community detection across scales, to reveal groups of\nneurons that are significantly related in the recorded time-series at different\nlevels of granularity.\n  Results and comparison with existing methods: Using synthetic spike-trains,\nincluding simulated data from leaky-integrate-and-fire networks, our method is\nable to identify important patterns in the data such as hierarchical structure\nthat are missed by other standard methods. We further apply the method to\nexperimental data from retinal ganglion cells of mouse and salamander, in which\nwe identify cell-groups that correspond to known functional types, and to\nhippocampal recordings from rats exploring a linear track, where we detect\nplace cells with high fidelity.\n  Conclusions: We present a versatile method to detect neural assemblies in\nspiking data applicable across a spectrum of relevant scales that contributes\nto understanding spatio-temporal information gathered from systems neuroscience\nexperiments.\n", "versions": [{"version": "v1", "created": "Sat, 8 Nov 2014 10:02:33 GMT"}], "update_date": "2014-11-11", "authors_parsed": [["Billeh", "Yazan N.", ""], ["Schaub", "Michael T.", ""], ["Anastassiou", "Costas A.", ""], ["Barahona", "Mauricio", ""], ["Koch", "Christof", ""]]}, {"id": "1411.2136", "submitter": "Alexander  Mathis", "authors": "Alexander Mathis and Martin B. Stemmler and Andreas V.M. Herz", "title": "Probable nature of higher-dimensional symmetries underlying mammalian\n  grid-cell activity patterns", "comments": "12 pages, 6 figures", "journal-ref": null, "doi": "10.7554/eLife.05979", "report-no": null, "categories": "q-bio.NC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Lattices abound in nature - from the crystal structure of minerals to the\nhoney-comb organization of ommatidia in the compound eye of insects. Such\nregular arrangements provide solutions for optimally dense packings, efficient\nresource distribution and cryptographic schemes, highlighting the importance of\nlattice theory in mathematics and physics, biology and economics, and computer\nscience and coding theory. Do lattices also play a role in how the brain\nrepresents information? To answer this question, we focus on higher-dimensional\nstimulus domains, with particular emphasis on neural representations of the\nphysical space explored by an animal. Using information theory, we ask how to\noptimize the spatial resolution of neuronal lattice codes.\n  We show that the hexagonal activity patterns of grid cells found in the\nhippocampal formation of mammals navigating on a flat surface lead to the\nhighest spatial resolution in a two-dimensional world. For species that move\nfreely in a three-dimensional environment, the firing fields should be arranged\nalong a face-centered cubic (FCC) lattice or a equally dense non-lattice\nvariant thereof known as a hexagonal close packing (HCP). This quantitative\nprediction could be tested experimentally in flying bats, arboreal monkeys, or\ncetaceans. More generally, our theoretical results suggest that the brain\nencodes higher-dimensional sensory or cognitive variables with populations of\ngrid-cell-like neurons whose activity patterns exhibit lattice structures at\nmultiple, nested scales.\n", "versions": [{"version": "v1", "created": "Sat, 8 Nov 2014 16:37:00 GMT"}], "update_date": "2015-05-12", "authors_parsed": [["Mathis", "Alexander", ""], ["Stemmler", "Martin B.", ""], ["Herz", "Andreas V. M.", ""]]}, {"id": "1411.2273", "submitter": "Wilten Nicola", "authors": "Wilten Nicola, Cheng Ly, Sue Ann Campbell", "title": "One-Dimensional Population Density Approaches to Recurrently Coupled\n  Networks of Neurons with Noise", "comments": "26 Pages, 6 Figures", "journal-ref": null, "doi": "10.1137/140995738", "report-no": null, "categories": "q-bio.NC math.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Mean-field systems have been previously derived for networks of coupled,\ntwo-dimensional, integrate-and-fire neurons such as the Izhikevich, adapting\nexponential (AdEx) and quartic integrate and fire (QIF), among others.\nUnfortunately, the mean-field systems have a degree of frequency error and the\nnetworks analyzed often do not include noise when there is adaptation. Here, we\nderive a one-dimensional partial differential equation (PDE) approximation for\nthe marginal voltage density under a first order moment closure for coupled\nnetworks of integrate-and-fire neurons with white noise inputs. The PDE has\nsubstantially less frequency error than the mean-field system, and provides a\ngreat deal more information, at the cost of analytical tractability. The\nconvergence properties of the mean-field system in the low noise limit are\nelucidated. A novel method for the analysis of the stability of the\nasynchronous tonic firing solution is also presented and implemented. Unlike\nprevious attempts at stability analysis with these network types, information\nabout the marginal densities of the adaptation variables is used. This method\ncan in principle be applied to other systems with nonlinear partial\ndifferential equations.\n", "versions": [{"version": "v1", "created": "Sun, 9 Nov 2014 19:30:36 GMT"}], "update_date": "2016-05-19", "authors_parsed": [["Nicola", "Wilten", ""], ["Ly", "Cheng", ""], ["Campbell", "Sue Ann", ""]]}, {"id": "1411.2821", "submitter": "Saeed Afshar", "authors": "Saeed Afshar, Libin George, Jonathan Tapson, Andre van Schaik, Philip\n  de Chazal, Tara Julia Hamilton", "title": "Turn Down that Noise: Synaptic Encoding of Afferent SNR in a Single\n  Spiking Neuron", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NE q-bio.NC", "license": "http://creativecommons.org/licenses/by/3.0/", "abstract": "  We have added a simplified neuromorphic model of Spike Time Dependent\nPlasticity (STDP) to the Synapto-dendritic Kernel Adapting Neuron (SKAN). The\nresulting neuron model is the first to show synaptic encoding of afferent\nsignal to noise ratio in addition to the unsupervised learning of spatio\ntemporal spike patterns. The neuron model is particularly suitable for\nimplementation in digital neuromorphic hardware as it does not use any complex\nmathematical operations and uses a novel approach to achieve synaptic\nhomeostasis. The neurons noise compensation properties are characterized and\ntested on noise corrupted zeros digits of the MNIST handwritten dataset.\nResults show the simultaneously learning common patterns in its input data\nwhile dynamically weighing individual afferent channels based on their signal\nto noise ratio. Despite its simplicity the interesting behaviors of the neuron\nmodel and the resulting computational power may offer insights into biological\nsystems.\n", "versions": [{"version": "v1", "created": "Tue, 11 Nov 2014 14:22:37 GMT"}], "update_date": "2014-11-12", "authors_parsed": [["Afshar", "Saeed", ""], ["George", "Libin", ""], ["Tapson", "Jonathan", ""], ["van Schaik", "Andre", ""], ["de Chazal", "Philip", ""], ["Hamilton", "Tara Julia", ""]]}, {"id": "1411.2832", "submitter": "Adam Barrett DPhil", "authors": "Adam B. Barrett", "title": "Exploration of synergistic and redundant information sharing in static\n  and dynamical Gaussian systems", "comments": "29 pages, 4 figures, in press Physical Review E, minor revisions to\n  original version in response to peer review", "journal-ref": "Phys. Rev. E 91, 052802 (2015)", "doi": "10.1103/PhysRevE.91.052802", "report-no": null, "categories": "cs.IT math.IT q-bio.NC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  To fully characterize the information that two `source' variables carry about\na third `target' variable, one must decompose the total information into\nredundant, unique and synergistic components, i.e. obtain a partial information\ndecomposition (PID). However Shannon's theory of information does not provide\nformulae to fully determine these quantities. Several recent studies have begun\naddressing this. Some possible definitions for PID quantities have been\nproposed, and some analyses have been carried out on systems composed of\ndiscrete variables. Here we present the first in-depth analysis of PIDs on\nGaussian systems, both static and dynamical. We show that, for a broad class of\nGaussian systems, previously proposed PID formulae imply that: (i) redundancy\nreduces to the minimum information provided by either source variable, and\nhence is independent of correlation between sources; (ii) synergy is the extra\ninformation contributed by the weaker source when the stronger source is known,\nand can either increase or decrease with correlation between sources. We find\nthat Gaussian systems frequently exhibit net synergy, i.e. the information\ncarried jointly by both sources is greater than the sum of informations carried\nby each source individually. Drawing from several explicit examples, we discuss\nthe implications of these findings for measures of information transfer and\ninformation-based measures of complexity, both generally and within a\nneuroscience setting. Importantly, by providing independent formulae for\nsynergy and redundancy applicable to continuous time-series data, we open up a\nnew approach to characterizing and quantifying information sharing amongst\ncomplex system variables.\n", "versions": [{"version": "v1", "created": "Tue, 11 Nov 2014 14:35:17 GMT"}, {"version": "v2", "created": "Thu, 30 Apr 2015 13:19:16 GMT"}], "update_date": "2015-05-13", "authors_parsed": [["Barrett", "Adam B.", ""]]}, {"id": "1411.3191", "submitter": "Michael Schwemmer", "authors": "Michael A. Schwemmer, Adrienne L. Fairhall, Sophie Den\\'eve, and Eric\n  T. Shea-Brown", "title": "Constructing precisely computing networks with biophysical spiking\n  neurons", "comments": "46 pages, 14 figures, Updated to final version", "journal-ref": null, "doi": null, "report-no": null, "categories": "q-bio.NC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  While spike timing has been shown to carry detailed stimulus information at\nthe sensory periphery, its possible role in network computation is less clear.\nMost models of computation by neural networks are based on population firing\nrates. In equivalent spiking implementations, firing is assumed to be random\nsuch that averaging across populations of neurons recovers the rate-based\napproach. Recently, however, Den\\'eve and colleagues have suggested that the\nspiking behavior of neurons may be fundamental to how neuronal networks\ncompute, with precise spike timing determined by each neuron's contribution to\nproducing the desired output. By postulating that each neuron fires in order to\nreduce the error in the network's output, it was demonstrated that linear\ncomputations can be carried out by networks of integrate-and-fire neurons that\ncommunicate through instantaneous synapses. This left open, however, the\npossibility that realistic networks, with conductance-based neurons with\nsubthreshold nonlinearity and the slower timescales of biophysical synapses,\nmay not fit into this framework. Here, we show how the spike-based approach can\nbe extended to biophysically plausible networks. We then show that our network\nreproduces a number of key features of cortical networks including irregular\nand Poisson-like spike times and a tight balance between excitation and\ninhibition. Lastly, we discuss how the behavior of our model scales with\nnetwork size, or with the number of neurons \"recorded\" from a larger computing\nnetwork. These results significantly increase the biological plausibility of\nthe spike-based approach to network computation.\n", "versions": [{"version": "v1", "created": "Wed, 12 Nov 2014 15:47:56 GMT"}, {"version": "v2", "created": "Thu, 16 Jul 2015 16:49:02 GMT"}], "update_date": "2015-07-17", "authors_parsed": [["Schwemmer", "Michael A.", ""], ["Fairhall", "Adrienne L.", ""], ["Den\u00e9ve", "Sophie", ""], ["Shea-Brown", "Eric T.", ""]]}, {"id": "1411.3489", "submitter": "Lu\\'is F.  Seoane MSc", "authors": "Lu\\'is F. Seoane, Stephan Gabler, Benjamin Blankertz", "title": "Images from the Mind: BCI image evolution based on Rapid Serial Visual\n  Presentation of polygon primitives", "comments": "22 pages, 8 figures", "journal-ref": null, "doi": "10.1080/2326263X.2015.1060819", "report-no": null, "categories": "q-bio.NC cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper provides a proof of concept for an EEG-based reconstruction of a\nvisual image which is on a user's mind. Our approach is based on the Rapid\nSerial Visual Presentation (RSVP) of polygon primitives and Brain-Computer\nInterface (BCI) technology. The presentation of polygons that contribute to\nbuild a target image (because they match the shape and/or color of the target)\ntrigger attention-related EEG patterns. Accordingly, these target primitives\ncan be determined using BCI classification of Event-Related Potentials (ERPs).\nThey are then accumulated in the display until a satisfactory reconstruction is\nreached. Selection steps have an average classification accuracy of $75\\%$.\n$25\\%$ of the images could be reconstructed completely, while more than $65\\%$\nof the available visual details could be captured on average. Most of the\nmisclassifications were not misinterpretations of the BCI concerning users'\nintent; rather, users tried to select polygons that were different than what\nwas intended by the experimenters. Open problems and alternatives to develop a\npractical BCI-based image reconstruction application are discussed.\n", "versions": [{"version": "v1", "created": "Thu, 13 Nov 2014 10:19:14 GMT"}, {"version": "v2", "created": "Mon, 13 Jul 2015 13:44:50 GMT"}], "update_date": "2015-07-14", "authors_parsed": [["Seoane", "Lu\u00eds F.", ""], ["Gabler", "Stephan", ""], ["Blankertz", "Benjamin", ""]]}, {"id": "1411.3917", "submitter": "Andrew Magyar", "authors": "Andrew Magyar, John Collins", "title": "Two-population model for MTL neurons: The vast majority are almost\n  silent", "comments": null, "journal-ref": "Phys. Rev. E 92, 012712 (2015)", "doi": "10.1103/PhysRevE.92.012712", "report-no": null, "categories": "q-bio.NC q-bio.QM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recordings in the human medial temporal lobe have found many neurons that\nrespond to pictures (and related stimuli) of just one particular person out of\nthose presented. It has been proposed that these are concept cells, responding\nto just a single concept. However, a direct experimental test of the concept\ncell idea appears impossible, because it would need the measurement of the\nresponse of each cell to enormous numbers of other stimuli. Here we propose a\nnew statistical method for analysis of the data, that gives a more powerful way\nto analyze how close data are to the concept-cell idea. It exploits the large\nnumber of sampled neurons, to give sensitivity to situations where the average\nresponse sparsity is to much less than one response for the number of presented\nstimuli. We show that a conventional model where a single sparsity is\npostulated for all neurons gives an extremely poor fit to the data. In contrast\na model with two dramatically different populations give an excellent fit to\ndata from the hippocampus and entorhinal cortex. In the hippocampus, one\npopulation has 7% of the cells with a 2.6% sparsity. But a much larger fraction\n93% respond to only 0.1% of the stimuli. This results in an extreme bias in the\nreported responsive of neurons compared with a typical neuron. Finally, we show\nhow to allow for the fact that some of reported identified units correspond to\nmultiple neurons, and find that our conclusions at the neural level are\nquantitatively changed but strengthened, with an even stronger difference\nbetween the two populations.\n", "versions": [{"version": "v1", "created": "Fri, 14 Nov 2014 14:21:39 GMT"}], "update_date": "2015-07-22", "authors_parsed": [["Magyar", "Andrew", ""], ["Collins", "John", ""]]}, {"id": "1411.3956", "submitter": "Gabriel Ocker", "authors": "Gabriel Koch Ocker and Ashok Litwin-Kumar and Brent Doiron", "title": "Self-organization of microcircuits in networks of neurons with plastic\n  synapses", "comments": "first edit: typo in arxiv title, rearranged article to put methods\n  after discussion. second edit: results section significantly rewritten for\n  style and presentation, figure 9 revised", "journal-ref": null, "doi": null, "report-no": null, "categories": "q-bio.NC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The synaptic connectivity of cortical networks features an overrepresentation\nof certain wiring motifs compared to simple random-network models. This\nstructure is shaped, in part, by synaptic plasticity that promotes or\nsuppresses connections between neurons depending on their spiking activity.\nFrequently, theoretical studies focus on how feedforward inputs drive\nplasticity to create this network structure. We study the complementary\nscenario of self-organized structure in a recurrent network, with spike\ntiming-dependent plasticity driven by spontaneous dynamics. We develop a\nself-consistent theory that describes the evolution of network structure by\ncombining fast spiking covariance with a fast-slow theory for synaptic weight\ndynamics. Through a finite-size expansion of network dynamics, we obtain a\nlow-dimensional set of nonlinear differential equations for the evolution of\ntwo-synapse connectivity motifs. With this theory in hand, we explore how the\nform of the plasticity rule drives the evolution of microcircuits in cortical\nnetworks. When potentiation and depression are in approximate balance, synaptic\ndynamics depend on the frequency of weighted divergent, convergent, and chain\nmotifs. For additive, Hebbian STDP, these motif interactions create\ninstabilities in synaptic dynamics that either promote or suppress the initial\nnetwork structure. Our work provides a consistent theoretical framework for\nstudying how spiking activity in recurrent networks interacts with synaptic\nplasticity to determine network structure.\n", "versions": [{"version": "v1", "created": "Fri, 14 Nov 2014 16:24:42 GMT"}, {"version": "v2", "created": "Sat, 22 Nov 2014 20:41:51 GMT"}, {"version": "v3", "created": "Sat, 20 Dec 2014 17:45:43 GMT"}], "update_date": "2014-12-23", "authors_parsed": [["Ocker", "Gabriel Koch", ""], ["Litwin-Kumar", "Ashok", ""], ["Doiron", "Brent", ""]]}, {"id": "1411.4077", "submitter": "Scott Linderman", "authors": "Scott W. Linderman, Christopher H. Stock, and Ryan P. Adams", "title": "A framework for studying synaptic plasticity with neural spike train\n  data", "comments": "Presented at Neural Information Processing Systems (NIPS) 2014", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML q-bio.NC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Learning and memory in the brain are implemented by complex, time-varying\nchanges in neural circuitry. The computational rules according to which\nsynaptic weights change over time are the subject of much research, and are not\nprecisely understood. Until recently, limitations in experimental methods have\nmade it challenging to test hypotheses about synaptic plasticity on a large\nscale. However, as such data become available and these barriers are lifted, it\nbecomes necessary to develop analysis techniques to validate plasticity models.\nHere, we present a highly extensible framework for modeling arbitrary synaptic\nplasticity rules on spike train data in populations of interconnected neurons.\nWe treat synaptic weights as a (potentially nonlinear) dynamical system\nembedded in a fully-Bayesian generalized linear model (GLM). In addition, we\nprovide an algorithm for inferring synaptic weight trajectories alongside the\nparameters of the GLM and of the learning rules. Using this method, we perform\nmodel comparison of two proposed variants of the well-known\nspike-timing-dependent plasticity (STDP) rule, where nonlinear effects play a\nsubstantial role. On synthetic data generated from the biophysical simulator\nNEURON, we show that we can recover the weight trajectories, the pattern of\nconnectivity, and the underlying learning rules.\n", "versions": [{"version": "v1", "created": "Fri, 14 Nov 2014 23:01:38 GMT"}], "update_date": "2014-11-18", "authors_parsed": [["Linderman", "Scott W.", ""], ["Stock", "Christopher H.", ""], ["Adams", "Ryan P.", ""]]}, {"id": "1411.4179", "submitter": "Sieu Ha", "authors": "Sieu D. Ha, Jian Shi, Yasmine Meroz, L. Mahadevan, and Shriram\n  Ramanathan", "title": "Neuromimetic Circuits with Synaptic Devices based on Strongly Correlated\n  Electron Systems", "comments": null, "journal-ref": "Phys. Rev. Applied 2, 064003 (2014)", "doi": "10.1103/PhysRevApplied.2.064003", "report-no": null, "categories": "cond-mat.str-el cs.ET q-bio.NC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Strongly correlated electron systems such as the rare-earth nickelates\n(RNiO3, R = rare-earth element) can exhibit synapse-like continuous long term\npotentiation and depression when gated with ionic liquids; exploiting the\nextreme sensitivity of coupled charge, spin, orbital, and lattice degrees of\nfreedom to stoichiometry. We present experimental real-time, device-level\nclassical conditioning and unlearning using nickelate-based synaptic devices in\nan electronic circuit compatible with both excitatory and inhibitory neurons.\nWe establish a physical model for the device behavior based on electric-field\ndriven coupled ionic-electronic diffusion that can be utilized for design of\nmore complex systems. We use the model to simulate a variety of associate and\nnon-associative learning mechanisms, as well as a feedforward recurrent network\nfor storing memory. Our circuit intuitively parallels biological neural\narchitectures, and it can be readily generalized to other forms of cellular\nlearning and extinction. The simulation of neural function with electronic\ndevice analogues may provide insight into biological processes such as decision\nmaking, learning and adaptation, while facilitating advanced parallel\ninformation processing in hardware.\n", "versions": [{"version": "v1", "created": "Sat, 15 Nov 2014 20:05:10 GMT"}], "update_date": "2014-12-09", "authors_parsed": [["Ha", "Sieu D.", ""], ["Shi", "Jian", ""], ["Meroz", "Yasmine", ""], ["Mahadevan", "L.", ""], ["Ramanathan", "Shriram", ""]]}, {"id": "1411.4334", "submitter": "Adri\\`a Tauste Campo", "authors": "Adri\\`a Tauste Campo, Marina Martinez-Garcia, Ver\\'onica N\\'acher,\n  Ranulfo Romo and Gustavo Deco", "title": "Task-driven intra- and interarea communications in primate cerebral\n  cortex", "comments": "Article and Supplementary Information", "journal-ref": null, "doi": "10.1073/pnas.1503937112", "report-no": null, "categories": "q-bio.NC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Neural correlations during a cognitive task are central to study brain\ninformation processing and computation. However, they have been poorly analyzed\ndue to the difficulty of recording simultaneous single neurons during task\nperformance. In the present work, we quantified neural directional correlations\nusing spike trains that were simultaneously recorded in sensory, premotor, and\nmotor cortical areas of two monkeys during a somatosensory discrimination task.\nUpon modeling spike trains as binary time series, we used a nonparametric\nBayesian method to estimate pairwise directional correlations between many\npairs of neurons throughout different stages of the task, namely, perception,\nworking memory, decision making, and motor report. We find that solving the\ntask involves feedforward and feedback correlation paths linking sensory and\nmotor areas during certain task intervals. Specifically, information is\ncommunicated by task-driven neural correlations that are significantly delayed\nacross secondary somatosensory cortex, premotor, and motor areas when decision\nmaking takes place. Crucially, when sensory comparison is no longer requested\nfor task performance, a major proportion of directional correlations\nconsistently vanish across all cortical areas.\n", "versions": [{"version": "v1", "created": "Mon, 17 Nov 2014 00:12:30 GMT"}, {"version": "v2", "created": "Tue, 5 May 2015 19:41:31 GMT"}], "update_date": "2016-02-17", "authors_parsed": [["Campo", "Adri\u00e0 Tauste", ""], ["Martinez-Garcia", "Marina", ""], ["N\u00e1cher", "Ver\u00f3nica", ""], ["Romo", "Ranulfo", ""], ["Deco", "Gustavo", ""]]}, {"id": "1411.4554", "submitter": "Markus Dahlem", "authors": "Bas-Jan Zandt and Bennie ten Haken and Michel J.A.M. van Putten and\n  Markus A. Dahlem", "title": "How does Spreading Depression Spread? - Physiology and Modeling", "comments": "14 pages, 6 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "q-bio.NC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Spreading depression (SD) is a wave phenomenon in gray matter tissue.\nLocally, it is characterized by massive re-distribution of ions across cell\nmembranes. As a consequence, there is a sustained membrane depolarization and\ntissue polarization that depresses any normal electrical activity. Despite\nthese dramatic cortical events, SD remains difficult to observe in humans\nnoninvasively, which for long has slowed advances in this field. The growing\nappreciation of its clinical importance in migraine and stroke is therefore\nconsistent with an increasing need for computational methods that tackle the\ncomplexity of the problem at multiple levels. In this review, we focus on\nmathematical tools to investigate the question of spread and its two\ncomplementary aspects: What are the physiological mechanisms and what is the\nspatial extent of SD in the cortex? This review discusses two types of models\nused to study these two questions, namely Hodgkin-Huxley type and generic\nactivator-inhibitor models, and the recent advances in techniques to link them.\n", "versions": [{"version": "v1", "created": "Mon, 17 Nov 2014 17:11:23 GMT"}], "update_date": "2014-11-18", "authors_parsed": [["Zandt", "Bas-Jan", ""], ["Haken", "Bennie ten", ""], ["van Putten", "Michel J. A. M.", ""], ["Dahlem", "Markus A.", ""]]}, {"id": "1411.4625", "submitter": "Christopher Hillar", "authors": "Christopher Hillar, Ngoc M. Tran", "title": "Robust exponential memory in Hopfield networks", "comments": "23 pages, 6 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "nlin.AO math-ph math.MP q-bio.NC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The Hopfield recurrent neural network is a classical auto-associative model\nof memory, in which collections of symmetrically-coupled McCulloch-Pitts\nneurons interact to perform emergent computation. Although previous researchers\nhave explored the potential of this network to solve combinatorial optimization\nproblems and store memories as attractors of its deterministic dynamics, a\nbasic open problem is to design a family of Hopfield networks with a number of\nnoise-tolerant memories that grows exponentially with neural population size.\nHere, we discover such networks by minimizing probability flow, a recently\nproposed objective for estimating parameters in discrete maximum entropy\nmodels. By descending the gradient of the convex probability flow, our networks\nadapt synaptic weights to achieve robust exponential storage, even when\npresented with vanishingly small numbers of training patterns. In addition to\nproviding a new set of error-correcting codes that achieve Shannon's channel\ncapacity bound, these networks also efficiently solve a variant of the hidden\nclique problem in computer science, opening new avenues for real-world\napplications of computational models originating from biology.\n", "versions": [{"version": "v1", "created": "Mon, 17 Nov 2014 20:25:07 GMT"}, {"version": "v2", "created": "Fri, 5 Jun 2015 23:14:55 GMT"}], "update_date": "2015-06-09", "authors_parsed": [["Hillar", "Christopher", ""], ["Tran", "Ngoc M.", ""]]}, {"id": "1411.4702", "submitter": "Michael Ferrier", "authors": "Michael R. Ferrier", "title": "Toward a Universal Cortical Algorithm: Examining Hierarchical Temporal\n  Memory in Light of Frontal Cortical Function", "comments": "105 pages, 6 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "q-bio.NC cs.AI cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A wide range of evidence points toward the existence of a common algorithm\nunderlying the processing of information throughout the cerebral cortex.\nSeveral hypothesized features of this cortical algorithm are reviewed,\nincluding sparse distributed representation, Bayesian inference, hierarchical\norganization composed of alternating template matching and pooling layers,\ntemporal slowness and predictive coding. Hierarchical Temporal Memory (HTM) is\na family of learning algorithms and corresponding theories of cortical function\nthat embodies these principles. HTM has previously been applied mainly to\nperceptual tasks typical of posterior cortex. In order to evaluate HTM as a\ncandidate model of cortical function, it is necessary also to investigate its\ncompatibility with the requirements of frontal cortical function. To this end,\na variety of models of frontal cortical function are reviewed and integrated,\nto arrive at the hypothesis that frontal functions including attention, working\nmemory and action selection depend largely upon the same basic algorithms as do\nposterior functions, with the notable additions of a mechanism for the active\nmaintenance of representations and of multiple cortico-striato-thalamo-cortical\nloops that allow communication between regions of frontal cortex to be gated in\nan adaptive manner. Computational models of this system are reviewed. Finally,\nthere is a discussion of how HTM can contribute to the understanding of frontal\ncortical function, and of what the requirements of frontal cortical function\nmean for the future development of HTM.\n", "versions": [{"version": "v1", "created": "Tue, 18 Nov 2014 00:38:30 GMT"}], "update_date": "2014-11-19", "authors_parsed": [["Ferrier", "Michael R.", ""]]}, {"id": "1411.4704", "submitter": "Neil Page", "authors": "N.W. Page, C. Hall and S.D. Page", "title": "Deep Brain Stimulation for Parkinson's Disease: a survey of experiences\n  perceived by recipients and carers", "comments": "45 pages, 32 figures and 23 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "q-bio.NC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Perceived outcomes from DBS for PD were sampled for 52 cases by surveying 46\nDBS recipients and 45 carers. Post-DBS experience ranged from 10-129 months.\nThere were significant variations in perceived outcomes over time. In some\ncases extreme variations were experienced as a consequence of hardware and\nother problems requiring additional surgery. Nevertheless most in this group\nwent on to ultimately report good outcomes. Holistic assessments of experiences\nwere largely positive, but in some cases there were significant differences in\nthe assessments by recipients and carers. For assessments valid at the time of\ninterview 26 recipients and 17 carers commented that the outcome was good. A\nsecond group of 11 recipients and 12 carers reported mixed results but overall\na positive experience. A third group of 6 recipients and 8 carers reported\nnegatively about the outcomes. Many considered overall quality of life much\nbetter following DBS, more so recipients than carers. Post-DBS experiences of\nboth motor and non-motor symptoms varied greatly between cases. When\nconsidering the average of participant responses, tremor and dyskinesias were\nconsidered better or much better following DBS, with benefits sustained with\ntime. 12 months after DBS many symptoms were on average considered the same or\nbetter after DBS, but for many, some decline in benefits was apparent over this\nperiod. Some symptoms were reported to show no improvement, or be worse\nfollowing DBS. 12 months after the procedure the average of participant\nresponses indicated that symptoms including speech, postural stability,\nswallowing, handwriting, cognitive function and incontinence were worse.\n", "versions": [{"version": "v1", "created": "Tue, 18 Nov 2014 00:45:44 GMT"}, {"version": "v2", "created": "Mon, 10 Aug 2015 00:00:51 GMT"}], "update_date": "2015-08-11", "authors_parsed": [["Page", "N. W.", ""], ["Hall", "C.", ""], ["Page", "S. D.", ""]]}, {"id": "1411.4770", "submitter": "Sacha van Albada", "authors": "Sacha Jennifer van Albada, Moritz Helias, Markus Diesmann", "title": "Scalability of asynchronous networks is limited by one-to-one mapping\n  between effective connectivity and correlations", "comments": null, "journal-ref": null, "doi": "10.1371/journal.pcbi.1004490", "report-no": null, "categories": "q-bio.NC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Network models are routinely downscaled compared to nature in terms of\nnumbers of nodes or edges because of a lack of computational resources, often\nwithout explicit mention of the limitations this entails. While reliable\nmethods have long existed to adjust parameters such that the first-order\nstatistics of network dynamics are conserved, here we show that limitations\nalready arise if also second-order statistics are to be maintained. The\ntemporal structure of pairwise averaged correlations in the activity of\nrecurrent networks is determined by the effective population-level\nconnectivity. We first show that in general the converse is also true and\nexplicitly mention degenerate cases when this one-to-one relationship does not\nhold. The one-to-one correspondence between effective connectivity and the\ntemporal structure of pairwise averaged correlations implies that network\nscalings should preserve the effective connectivity if pairwise averaged\ncorrelations are to be held constant. Changes in effective connectivity can\neven push a network from a linearly stable to an unstable, oscillatory regime\nand vice versa. On this basis, we derive conditions for the preservation of\nboth mean population-averaged activities and pairwise averaged correlations\nunder a change in numbers of neurons or synapses in the asynchronous regime\ntypical of cortical networks. We find that mean activities and correlation\nstructure can be maintained by an appropriate scaling of the synaptic weights,\nbut only over a range of numbers of synapses that is limited by the variance of\nexternal inputs to the network. Our results therefore show that the\nreducibility of asynchronous networks is fundamentally limited.\n", "versions": [{"version": "v1", "created": "Tue, 18 Nov 2014 09:16:23 GMT"}, {"version": "v2", "created": "Wed, 15 Apr 2015 08:39:31 GMT"}, {"version": "v3", "created": "Sat, 4 Jul 2015 16:57:42 GMT"}], "update_date": "2016-02-17", "authors_parsed": [["van Albada", "Sacha Jennifer", ""], ["Helias", "Moritz", ""], ["Diesmann", "Markus", ""]]}, {"id": "1411.4980", "submitter": "David Sterratt", "authors": "David C. Sterratt and Oksana Sorokina and J. Douglas Armstrong", "title": "Integration of rule-based models and compartmental models of neurons", "comments": "Presented to the Third International Workshop on Hybrid Systems\n  Biology Vienna, Austria, July 23-24, 2014 at the International Conference on\n  Computer-Aided Verification 2014", "journal-ref": null, "doi": null, "report-no": null, "categories": "q-bio.NC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Synaptic plasticity depends on the interaction between electrical activity in\nneurons and the synaptic proteome, the collection of over 1000 proteins in the\npost-synaptic density (PSD) of synapses. To construct models of synaptic\nplasticity with realistic numbers of proteins, we aim to combine rule-based\nmodels of molecular interactions in the synaptic proteome with compartmental\nmodels of the electrical activity of neurons. Rule-based models allow\ninteractions between the combinatorially large number of protein complexes in\nthe postsynaptic proteome to be expressed straightforwardly. Simulations of\nrule-based models are stochastic and thus can deal with the small copy numbers\nof proteins and complexes in the PSD. Compartmental models of neurons are\nexpressed as systems of coupled ordinary differential equations and solved\ndeterministically. We present an algorithm which incorporates stochastic\nrule-based models into deterministic compartmental models and demonstrate an\nimplementation (\"KappaNEURON\") of this hybrid system using the SpatialKappa and\nNEURON simulators.\n", "versions": [{"version": "v1", "created": "Tue, 18 Nov 2014 19:43:09 GMT"}], "update_date": "2014-11-19", "authors_parsed": [["Sterratt", "David C.", ""], ["Sorokina", "Oksana", ""], ["Armstrong", "J. Douglas", ""]]}, {"id": "1411.5258", "submitter": "Ido Kanter", "authors": "Roni Vardi, Amir Goldental, Hagar Marmari, Haya Brama, Edward Stern,\n  Shira Sardi, Pinhas Sabo and Ido Kanter", "title": "Neuronal Response Impedance Mechanism Implementing Cooperative Networks\n  with Low Firing Rates and Microseconds Precision", "comments": "35 pages, 13 figures", "journal-ref": "Front. Neural Circuits 9:29 (2015)", "doi": "10.3389/fncir.2015.00029", "report-no": null, "categories": "q-bio.NC cond-mat.stat-mech physics.bio-ph", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Realizations of low firing rates in neural networks usually require globally\nbalanced distributions among excitatory and inhibitory links, while feasibility\nof temporal coding is limited by neuronal millisecond precision. We show that\ncooperation, governing global network features, emerges through nodal\nproperties, as opposed to link distributions. Using in vitro and in vivo\nexperiments we demonstrate microsecond precision of neuronal response timings\nunder low stimulation frequencies, whereas moderate frequencies result in a\nchaotic neuronal phase characterized by degraded precision. Above a critical\nstimulation frequency, which varies among neurons, response failures were found\nto emerge stochastically such that the neuron functions as a low pass filter,\nsaturating the average inter-spike-interval. This intrinsic neuronal response\nimpedance mechanism leads to cooperation on a network level, such that firing\nrates are suppressed towards the lowest neuronal critical frequency\nsimultaneously with neuronal microsecond precision. Our findings open up\nopportunities of controlling global features of network dynamics through few\nnodes with extreme properties.\n", "versions": [{"version": "v1", "created": "Wed, 19 Nov 2014 15:45:05 GMT"}, {"version": "v2", "created": "Mon, 1 Jun 2015 08:34:15 GMT"}], "update_date": "2015-06-16", "authors_parsed": [["Vardi", "Roni", ""], ["Goldental", "Amir", ""], ["Marmari", "Hagar", ""], ["Brama", "Haya", ""], ["Stern", "Edward", ""], ["Sardi", "Shira", ""], ["Sabo", "Pinhas", ""], ["Kanter", "Ido", ""]]}, {"id": "1411.5340", "submitter": "Michelle Greene", "authors": "Michelle R. Greene, Christopher Baldassano, Andre Esteva, Diane M.\n  Beck and Li Fei-Fei", "title": "Affordances Provide a Fundamental Categorization Principle for Visual\n  Scenes", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "q-bio.NC cs.CV cs.HC", "license": "http://creativecommons.org/licenses/by-nc-sa/3.0/", "abstract": "  How do we know that a kitchen is a kitchen by looking? Relatively little is\nknown about how we conceptualize and categorize different visual environments.\nTraditional models of visual perception posit that scene categorization is\nachieved through the recognition of a scene's objects, yet these models cannot\naccount for the mounting evidence that human observers are relatively\ninsensitive to the local details in an image. Psychologists have long theorized\nthat the affordances, or actionable possibilities of a stimulus are pivotal to\nits perception. To what extent are scene categories created from similar\naffordances? Using a large-scale experiment using hundreds of scene categories,\nwe show that the activities afforded by a visual scene provide a fundamental\ncategorization principle. Affordance-based similarity explained the majority of\nthe structure in the human scene categorization patterns, outperforming\nalternative similarities based on objects or visual features. We all models\nwere combined, affordances provided the majority of the predictive power in the\ncombined model, and nearly half of the total explained variance is captured\nonly by affordances. These results challenge many existing models of high-level\nvisual perception, and provide immediately testable hypotheses for the\nfunctional organization of the human perceptual system.\n", "versions": [{"version": "v1", "created": "Wed, 19 Nov 2014 19:58:59 GMT"}], "update_date": "2014-11-20", "authors_parsed": [["Greene", "Michelle R.", ""], ["Baldassano", "Christopher", ""], ["Esteva", "Andre", ""], ["Beck", "Diane M.", ""], ["Fei-Fei", "Li", ""]]}, {"id": "1411.5383", "submitter": "Zeyuan Allen-Zhu", "authors": "Zeyuan Allen-Zhu, Rati Gelashvili, Silvio Micali, Nir Shavit", "title": "Johnson-Lindenstrauss Compression with Neuroscience-Based Constraints", "comments": "A shorter version of this paper has appeared in the Proceedings of\n  the National Academy of Sciences", "journal-ref": null, "doi": "10.1073/pnas.1419100111", "report-no": null, "categories": "q-bio.NC cs.DS math.PR math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Johnson-Lindenstrauss (JL) matrices implemented by sparse random synaptic\nconnections are thought to be a prime candidate for how convergent pathways in\nthe brain compress information. However, to date, there is no complete\nmathematical support for such implementations given the constraints of real\nneural tissue. The fact that neurons are either excitatory or inhibitory\nimplies that every so implementable JL matrix must be sign-consistent (i.e.,\nall entries in a single column must be either all non-negative or all\nnon-positive), and the fact that any given neuron connects to a relatively\nsmall subset of other neurons implies that the JL matrix had better be sparse.\n  We construct sparse JL matrices that are sign-consistent, and prove that our\nconstruction is essentially optimal. Our work answers a mathematical question\nthat was triggered by earlier work and is necessary to justify the existence of\nJL compression in the brain, and emphasizes that inhibition is crucial if\nneurons are to perform efficient, correlation-preserving compression.\n", "versions": [{"version": "v1", "created": "Wed, 19 Nov 2014 21:12:12 GMT"}], "update_date": "2014-11-21", "authors_parsed": [["Allen-Zhu", "Zeyuan", ""], ["Gelashvili", "Rati", ""], ["Micali", "Silvio", ""], ["Shavit", "Nir", ""]]}, {"id": "1411.5878", "submitter": "Ming-Ming Cheng Prof.", "authors": "Ali Borji, Ming-Ming Cheng, Qibin Hou, Huaizu Jiang, Jia Li", "title": "Salient Object Detection: A Survey", "comments": null, "journal-ref": "Computational Visual Media, 5(2):117-150, 2019", "doi": "10.1007/s41095-019-0149-9", "report-no": null, "categories": "cs.CV cs.AI q-bio.NC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Detecting and segmenting salient objects from natural scenes, often referred\nto as salient object detection, has attracted great interest in computer\nvision. While many models have been proposed and several applications have\nemerged, a deep understanding of achievements and issues remains lacking. We\naim to provide a comprehensive review of recent progress in salient object\ndetection and situate this field among other closely related areas such as\ngeneric scene segmentation, object proposal generation, and saliency for\nfixation prediction. Covering 228 publications, we survey i) roots, key\nconcepts, and tasks, ii) core techniques and main modeling trends, and iii)\ndatasets and evaluation metrics for salient object detection. We also discuss\nopen problems such as evaluation metrics and dataset bias in model performance,\nand suggest future research directions.\n", "versions": [{"version": "v1", "created": "Tue, 18 Nov 2014 22:41:24 GMT"}, {"version": "v2", "created": "Mon, 7 Aug 2017 17:17:03 GMT"}, {"version": "v3", "created": "Wed, 16 Aug 2017 21:45:20 GMT"}, {"version": "v4", "created": "Mon, 4 Sep 2017 02:51:33 GMT"}, {"version": "v5", "created": "Thu, 6 Sep 2018 19:31:29 GMT"}, {"version": "v6", "created": "Mon, 1 Jul 2019 11:13:07 GMT"}], "update_date": "2019-07-02", "authors_parsed": [["Borji", "Ali", ""], ["Cheng", "Ming-Ming", ""], ["Hou", "Qibin", ""], ["Jiang", "Huaizu", ""], ["Li", "Jia", ""]]}, {"id": "1411.5881", "submitter": "Shaista Hussain", "authors": "Shaista Hussain, Shih-Chii Liu and Arindam Basu", "title": "Hardware-Amenable Structural Learning for Spike-based Pattern\n  Classification using a Simple Model of Active Dendrites", "comments": "Accepted for publication in Neural Computation", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NE q-bio.NC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper presents a spike-based model which employs neurons with\nfunctionally distinct dendritic compartments for classifying high dimensional\nbinary patterns. The synaptic inputs arriving on each dendritic subunit are\nnonlinearly processed before being linearly integrated at the soma, giving the\nneuron a capacity to perform a large number of input-output mappings. The model\nutilizes sparse synaptic connectivity; where each synapse takes a binary value.\nThe optimal connection pattern of a neuron is learned by using a simple\nhardware-friendly, margin enhancing learning algorithm inspired by the\nmechanism of structural plasticity in biological neurons. The learning\nalgorithm groups correlated synaptic inputs on the same dendritic branch. Since\nthe learning results in modified connection patterns, it can be incorporated\ninto current event-based neuromorphic systems with little overhead. This work\nalso presents a branch-specific spike-based version of this structural\nplasticity rule. The proposed model is evaluated on benchmark binary\nclassification problems and its performance is compared against that achieved\nusing Support Vector Machine (SVM) and Extreme Learning Machine (ELM)\ntechniques. Our proposed method attains comparable performance while utilizing\n10 to 50% less computational resources than the other reported techniques.\n", "versions": [{"version": "v1", "created": "Thu, 20 Nov 2014 07:50:06 GMT"}, {"version": "v2", "created": "Tue, 25 Nov 2014 08:03:27 GMT"}], "update_date": "2014-11-26", "authors_parsed": [["Hussain", "Shaista", ""], ["Liu", "Shih-Chii", ""], ["Basu", "Arindam", ""]]}, {"id": "1411.6106", "submitter": "David Holcman", "authors": "K. Dao Duc, Z. Schuss, D. Holcman", "title": "Oscillatory Survival Probability: Analytical, Numerical Study for\n  oscillatory narrow escape and applications to neural network dynamics", "comments": "25 p", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.AP math-ph math.MP physics.data-an q-bio.NC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study the escape of Brownian motion from the domain of attraction $\\Omega$\nof a stable focus with a strong drift. The boundary $\\partial \\Omega$ of\n$\\Omega$ is an unstable limit cycle of the drift and the focus is very close to\nthe limit cycle. We find a new phenomenon of oscillatory decay of the peaks of\nthe survival probability of the Brownian motion in $\\Omega$. We compute\nexplicitly the complex-valued second eigenvalue $\\lambda_2(\\Omega$) of the\nFokker-Planck operator with Dirichlet boundary conditions and show that it is\nresponsible for the peaks. Specifically, we demonstrate that the dominant\noscillation frequency equals $1/{\\mathfrak{I}}m\\{\\lambda_2(\\Omega)\\}$ and is\nindependent of the relative noise strength. We apply the analysis to a\ncanonical system and compare the density of exit points on $\\partial \\Omega$ to\nthat obtained from stochastic simulations. We find that this density is\nconcentrated in a small portion of the boundary, thus rendering the exit a\nnarrow escape problem. Unlike the case in the classical activated escape\nproblem, the principal eigenvalue does not necessarily decay exponentially as\nthe relative noise strength decays. The oscillatory narrow escape problem\narises in a mathematical model of neural networks with synaptic depression. We\nidentify oscillation peaks in the density of the time the network spends in a\nspecific state. This observation explains the oscillations of stochastic\ntrajectories around a focus prior to escape and also the non-Poissonian escape\ntimes. This phenomenon has been observed and reported in the neural network\nliterature.\n", "versions": [{"version": "v1", "created": "Sat, 22 Nov 2014 09:24:54 GMT"}], "update_date": "2014-11-25", "authors_parsed": [["Duc", "K. Dao", ""], ["Schuss", "Z.", ""], ["Holcman", "D.", ""]]}, {"id": "1411.6191", "submitter": "David Balduzzi", "authors": "David Balduzzi, Hastagiri Vanchinathan, Joachim Buhmann", "title": "Kickback cuts Backprop's red-tape: Biologically plausible credit\n  assignment in neural networks", "comments": "7 pages. To appear, AAAI-15", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.NE q-bio.NC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Error backpropagation is an extremely effective algorithm for assigning\ncredit in artificial neural networks. However, weight updates under Backprop\ndepend on lengthy recursive computations and require separate output and error\nmessages -- features not shared by biological neurons, that are perhaps\nunnecessary. In this paper, we revisit Backprop and the credit assignment\nproblem. We first decompose Backprop into a collection of interacting learning\nalgorithms; provide regret bounds on the performance of these sub-algorithms;\nand factorize Backprop's error signals. Using these results, we derive a new\ncredit assignment algorithm for nonparametric regression, Kickback, that is\nsignificantly simpler than Backprop. Finally, we provide a sufficient condition\nfor Kickback to follow error gradients, and show that Kickback matches\nBackprop's performance on real-world regression benchmarks.\n", "versions": [{"version": "v1", "created": "Sun, 23 Nov 2014 04:58:22 GMT"}], "update_date": "2014-11-25", "authors_parsed": [["Balduzzi", "David", ""], ["Vanchinathan", "Hastagiri", ""], ["Buhmann", "Joachim", ""]]}, {"id": "1411.6422", "submitter": "Umut G\\\"u\\c{c}l\\\"u", "authors": "Umut G\\\"u\\c{c}l\\\"u, Marcel A. J. van Gerven", "title": "Deep Neural Networks Reveal a Gradient in the Complexity of Neural\n  Representations across the Brain's Ventral Visual Pathway", "comments": null, "journal-ref": null, "doi": "10.1523/JNEUROSCI.5023-14.2015", "report-no": null, "categories": "q-bio.NC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Converging evidence suggests that the mammalian ventral visual pathway\nencodes increasingly complex stimulus features in downstream areas. Using deep\nconvolutional neural networks, we can now quantitatively demonstrate that there\nis indeed an explicit gradient for feature complexity in the ventral pathway of\nthe human brain. Our approach also allows stimulus features of increasing\ncomplexity to be mapped across the human brain, providing an automated approach\nto probing how representations are mapped across the cortical sheet. Finally,\nit is shown that deep convolutional neural networks allow decoding of\nrepresentations in the human brain at a previously unattainable degree of\naccuracy, providing a more sensitive window into the human brain.\n", "versions": [{"version": "v1", "created": "Mon, 24 Nov 2014 12:02:26 GMT"}], "update_date": "2017-03-13", "authors_parsed": [["G\u00fc\u00e7l\u00fc", "Umut", ""], ["van Gerven", "Marcel A. J.", ""]]}, {"id": "1411.6470", "submitter": "Robert Rockenfeller", "authors": "Robert Rockenfeller, Michael G\u007fuenther, Syn Schmitt, Thomas G\u007foetz", "title": "Comparing different muscle activation dynamics using sensitivity\n  analysis", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.DS q-bio.NC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we mathematically compared two models of mammalian striated\nmuscle activation dynamics proposed by Hatze and Zajac. Both models are\nrepresentative of a broad variety of biomechanical models formulated as\nordinary differential equations (ODEs). The models incorporate some parameters\nthat directly represent known physiological properties. Other parameters have\nbeen introduced to reproduce empirical observations. We used sensitivity\nanalysis as a mathematical tool for investigating the influence of model\nparameters on the solution of the ODEs. That is, we adopted a former approach\nfor calculating such (first order) sensitivities. Additionally, we expanded it\nto treating initial conditions as parameters and to calculating second order\nsensitivities. The latter quantify the non-linearly coupled effect of any\ncombination of two parameters. As a completion we used a global sensitivity\nanalysis approach to take the variability of parameters into account. The\nmethod we suggest has numerous uses. A theoretician striving for model\nreduction may use it for identifying particularly low sensitivities to detect\nsuperfluous parameters. An experimenter may use it for identifying particularly\nhigh sensitivities to facilitate determining the parameter value with maximised\nprecision.\n", "versions": [{"version": "v1", "created": "Mon, 24 Nov 2014 14:45:34 GMT"}], "update_date": "2014-11-25", "authors_parsed": [["Rockenfeller", "Robert", ""], ["G\u007fuenther", "Michael", ""], ["Schmitt", "Syn", ""], ["G\u007foetz", "Thomas", ""]]}, {"id": "1411.6768", "submitter": "Yuriy Parzhin", "authors": "Yuri Parzhin", "title": "Hypotheses of neural code and the information model of the\n  neuron-detector", "comments": "15 pages, 5 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NE cs.AI q-bio.NC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper deals with the problem of neural code solving. On the basis of the\nformulated hypotheses the information model of a neuron-detector is suggested,\nthe detector being one of the basic elements of an artificial neural network\n(ANN). The paper subjects the connectionist paradigm of ANN building to\ncriticism and suggests a new presentation paradigm for ANN building and\nneuroelements (NE) learning. The adequacy of the suggested model is proved by\nthe fact that is does not contradict the modern propositions of neuropsychology\nand neurophysiology.\n", "versions": [{"version": "v1", "created": "Tue, 25 Nov 2014 08:52:14 GMT"}], "update_date": "2014-11-26", "authors_parsed": [["Parzhin", "Yuri", ""]]}, {"id": "1411.6912", "submitter": "Julien Hubert", "authors": "Julien Hubert and Takashi Ikegami", "title": "Short-Term Memory Through Persistent Activity: Evolution of\n  Self-Stopping and Self-Sustaining Activity in Spiking Neural Networks", "comments": "28 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NE q-bio.NC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Memories in the brain are separated in two categories: short-term and\nlong-term memories. Long-term memories remain for a lifetime, while short-term\nones exist from a few milliseconds to a few minutes. Within short-term memory\nstudies, there is debate about what neural structure could implement it.\nIndeed, mechanisms responsible for long-term memories appear inadequate for the\ntask. Instead, it has been proposed that short-term memories could be sustained\nby the persistent activity of a group of neurons. In this work, we explore what\ntopology could sustain short-term memories, not by designing a model from\nspecific hypotheses, but through Darwinian evolution in order to obtain new\ninsights into its implementation. We evolved 10 networks capable of retaining\ninformation for a fixed duration between 2 and 11s. Our main finding has been\nthat the evolution naturally created two functional modules in the network: one\nwhich sustains the information containing primarily excitatory neurons, while\nthe other, which is responsible for forgetting, was composed mainly of\ninhibitory neurons. This demonstrates how the balance between inhibition and\nexcitation plays an important role in cognition.\n", "versions": [{"version": "v1", "created": "Tue, 25 Nov 2014 16:32:14 GMT"}], "update_date": "2014-11-26", "authors_parsed": [["Hubert", "Julien", ""], ["Ikegami", "Takashi", ""]]}, {"id": "1411.7165", "submitter": "Simona Mancini", "authors": "S Mancini (MAPMO)", "title": "Decision-making and interacting neuron populations", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.AP q-bio.NC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this article we present the modeling of bi-stability view problems\ndescribed by the activity or firing rates of two interacting population of\nneurons. Starting from the study of a complex system, the sys-tem of stochastic\ndifferential equations describing the time evolution of the activity of the two\npopulations of neurons, we point out the strength and weakness of this model\nand consider its associated par-tial differential equation, which resolution\ngives statistical information on the firing rates distributions. The slow-fast\ncharacterization of the solutions finally leads us to a complexity reduction of\nthe model by the definition of a one-dimensional stochastic differential\nequation and its associated one-dimensional partial differential equation. This\nlast model turns out to be well adapted to the resolution of the prob-lem\ngiving access, in particular, to reaction times and performance, two\nmacroscopic variables describing the decision-making in the view problem.\n", "versions": [{"version": "v1", "created": "Wed, 26 Nov 2014 10:27:02 GMT"}], "update_date": "2014-11-27", "authors_parsed": [["Mancini", "S", "", "MAPMO"]]}, {"id": "1411.7635", "submitter": "Simone Carlo Surace", "authors": "Simone Carlo Surace, Jean-Pascal Pfister", "title": "A statistical model for in vivo neuronal dynamics", "comments": "31 pages, 10 figures", "journal-ref": null, "doi": "10.1371/journal.pone.0142435", "report-no": null, "categories": "q-bio.NC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Single neuron models have a long tradition in computational neuroscience.\nDetailed biophysical models such as the Hodgkin-Huxley model as well as\nsimplified neuron models such as the class of integrate-and-fire models relate\nthe input current to the membrane potential of the neuron. Those types of\nmodels have been extensively fitted to in vitro data where the input current is\ncontrolled. Those models are however of little use when it comes to\ncharacterize intracellular in vivo recordings since the input to the neuron is\nnot known. Here we propose a novel single neuron model that characterizes the\nstatistical properties of in vivo recordings. More specifically, we propose a\nstochastic process where the subthreshold membrane potential follows a Gaussian\nprocess and the spike emission intensity depends nonlinearly on the membrane\npotential as well as the spiking history. We first show that the model has a\nrich dynamical repertoire since it can capture arbitrary subthreshold\nautocovariance functions, firing-rate adaptations as well as arbitrary shapes\nof the action potential. We then show that this model can be efficiently fitted\nto data without overfitting. Finally, we show that this model can be used to\ncharacterize and therefore precisely compare various intracellular in vivo\nrecordings from different animals and experimental conditions.\n", "versions": [{"version": "v1", "created": "Thu, 27 Nov 2014 16:10:19 GMT"}, {"version": "v2", "created": "Fri, 17 Apr 2015 15:21:04 GMT"}, {"version": "v3", "created": "Fri, 23 Oct 2015 13:13:20 GMT"}], "update_date": "2016-11-02", "authors_parsed": [["Surace", "Simone Carlo", ""], ["Pfister", "Jean-Pascal", ""]]}, {"id": "1411.7706", "submitter": "Scott Linderman", "authors": "Scott W. Linderman, Matthew J. Johnson, Matthew A. Wilson, and Zhe\n  Chen", "title": "A Nonparametric Bayesian Approach to Uncovering Rat Hippocampal\n  Population Codes During Spatial Navigation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML q-bio.NC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Rodent hippocampal population codes represent important spatial information\nabout the environment during navigation. Several computational methods have\nbeen developed to uncover the neural representation of spatial topology\nembedded in rodent hippocampal ensemble spike activity. Here we extend our\nprevious work and propose a nonparametric Bayesian approach to infer rat\nhippocampal population codes during spatial navigation. To tackle the model\nselection problem, we leverage a nonparametric Bayesian model. Specifically, to\nanalyze rat hippocampal ensemble spiking activity, we apply a hierarchical\nDirichlet process-hidden Markov model (HDP-HMM) using two Bayesian inference\nmethods, one based on Markov chain Monte Carlo (MCMC) and the other based on\nvariational Bayes (VB). We demonstrate the effectiveness of our Bayesian\napproaches on recordings from a freely-behaving rat navigating in an open field\nenvironment. We find that MCMC-based inference with Hamiltonian Monte Carlo\n(HMC) hyperparameter sampling is flexible and efficient, and outperforms VB and\nMCMC approaches with hyperparameters set by empirical Bayes.\n", "versions": [{"version": "v1", "created": "Thu, 27 Nov 2014 21:17:47 GMT"}], "update_date": "2014-12-01", "authors_parsed": [["Linderman", "Scott W.", ""], ["Johnson", "Matthew J.", ""], ["Wilson", "Matthew A.", ""], ["Chen", "Zhe", ""]]}, {"id": "1411.7916", "submitter": "Thomas Pfeil", "authors": "Thomas Pfeil, Jakob Jordan, Tom Tetzlaff, Andreas Gr\\\"ubl, Johannes\n  Schemmel, Markus Diesmann, Karlheinz Meier", "title": "The effect of heterogeneity on decorrelation mechanisms in spiking\n  neural networks: a neuromorphic-hardware study", "comments": "20 pages, 10 figures, supplements", "journal-ref": "Phys. Rev. X 6, 021023 (2016)", "doi": "10.1103/PhysRevX.6.021023", "report-no": null, "categories": "q-bio.NC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  High-level brain function such as memory, classification or reasoning can be\nrealized by means of recurrent networks of simplified model neurons. Analog\nneuromorphic hardware constitutes a fast and energy efficient substrate for the\nimplementation of such neural computing architectures in technical applications\nand neuroscientific research. The functional performance of neural networks is\noften critically dependent on the level of correlations in the neural activity.\nIn finite networks, correlations are typically inevitable due to shared\npresynaptic input. Recent theoretical studies have shown that inhibitory\nfeedback, abundant in biological neural networks, can actively suppress these\nshared-input correlations and thereby enable neurons to fire nearly\nindependently. For networks of spiking neurons, the decorrelating effect of\ninhibitory feedback has so far been explicitly demonstrated only for\nhomogeneous networks of neurons with linear sub-threshold dynamics. Theory,\nhowever, suggests that the effect is a general phenomenon, present in any\nsystem with sufficient inhibitory feedback, irrespective of the details of the\nnetwork structure or the neuronal and synaptic properties. Here, we investigate\nthe effect of network heterogeneity on correlations in sparse, random networks\nof inhibitory neurons with non-linear, conductance-based synapses. Emulations\nof these networks on the analog neuromorphic hardware system Spikey allow us to\ntest the efficiency of decorrelation by inhibitory feedback in the presence of\nhardware-specific heterogeneities. The configurability of the hardware\nsubstrate enables us to modulate the extent of heterogeneity in a systematic\nmanner. We selectively study the effects of shared input and recurrent\nconnections on correlations in membrane potentials and spike trains. Our\nresults confirm ...\n", "versions": [{"version": "v1", "created": "Fri, 28 Nov 2014 15:51:59 GMT"}, {"version": "v2", "created": "Fri, 3 Jul 2015 12:38:05 GMT"}, {"version": "v3", "created": "Fri, 19 Feb 2016 20:30:02 GMT"}, {"version": "v4", "created": "Thu, 9 Jun 2016 14:23:52 GMT"}], "update_date": "2016-06-10", "authors_parsed": [["Pfeil", "Thomas", ""], ["Jordan", "Jakob", ""], ["Tetzlaff", "Tom", ""], ["Gr\u00fcbl", "Andreas", ""], ["Schemmel", "Johannes", ""], ["Diesmann", "Markus", ""], ["Meier", "Karlheinz", ""]]}]