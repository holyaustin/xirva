[{"id": "2002.00094", "submitter": "Lia Papadopoulos", "authors": "Lia Papadopoulos, Christopher W. Lynn, Demian Battaglia, Danielle S.\n  Bassett", "title": "Relations between large scale brain connectivity and effects of regional\n  stimulation depend on collective dynamical state", "comments": "Main Text: 32 pages, 10 figures; Supplement: 14 pages, 10 figures", "journal-ref": "PLoS Computational Biology, 16-9 (2020)", "doi": "10.1371/journal.pcbi.1008144", "report-no": null, "categories": "q-bio.NC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  At the macroscale, the brain operates as a network of interconnected neuronal\npopulations, which display rhythmic dynamics that support interareal\ncommunication. Understanding how stimulation of a particular brain area impacts\nsuch concerted activity is important for gaining basic insights into brain\nfunction and for developing neuromodulation as a therapeutic tool. However, it\nremains difficult to predict the downstream effects of focal stimulation.\nSpecifically, little is known about how the collective oscillatory regime of\nnetwork activity may affect the outcomes of regional perturbations on\ncooperative dynamics. Here, we combine connectome data and biophysical modeling\nto begin filling these gaps. By tuning parameters that control the collective\ndynamics of the network, we identify distinct states of simulated brain\nactivity, and investigate how the distributed effects of stimulation manifest\nin different states. When baseline oscillations are weak, the stimulated area\nexhibits enhanced power and frequency, and due to network interactions, nearby\nregions develop phase locked activity in the excited frequency band.\nImportantly, we find that focal stimulation also causes more distributed\nmodifications to network coherence at regions' baseline oscillation\nfrequencies, and that these effects are better predicted by functional rather\nthan structural connectivity. In contrast, when the network operates in a\nregime of stronger endogenous oscillations, stimulation causes only slight\nshifts in power and frequency, and network averaged changes in coherence are\nmore homogenous across the choice of the stimulated area. In sum, this work\nbuilds upon and extends previous computational studies investigating the\nimpacts of stimulation, and highlights that both the stimulation site, and,\ncrucially, the regime of brain network dynamics, can influence the network wide\nresponses to local perturbations.\n", "versions": [{"version": "v1", "created": "Fri, 31 Jan 2020 23:11:40 GMT"}, {"version": "v2", "created": "Wed, 12 Feb 2020 22:39:12 GMT"}], "update_date": "2020-11-12", "authors_parsed": [["Papadopoulos", "Lia", ""], ["Lynn", "Christopher W.", ""], ["Battaglia", "Demian", ""], ["Bassett", "Danielle S.", ""]]}, {"id": "2002.00448", "submitter": "Ekkehard Ullner", "authors": "Afifurrahman and Ekkehard Ullner and Antonio Politi", "title": "Stability of synchronous states in sparse neuronal networks", "comments": "9 pages, 7 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "q-bio.NC math.DS nlin.AO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The stability of synchronous states is analysed in the context of two\npopulations of inhibitory and excitatory neurons, characterized by different\npulse-widths. The problem is reduced to that of determining the eigenvalues of\na suitable class of sparse random matrices, randomness being a consequence of\nthe network structure. A detailed analysis, which includes also the study of\nfinite-amplitude perturbations, is performed in the limit of narrow pulses,\nfinding that the stability depends crucially on the relative pulse-width. This\nhas implications for the overall property of the asynchronous (balanced)\nregime.\n", "versions": [{"version": "v1", "created": "Sun, 2 Feb 2020 18:29:36 GMT"}], "update_date": "2020-02-04", "authors_parsed": [["Afifurrahman", "", ""], ["Ullner", "Ekkehard", ""], ["Politi", "Antonio", ""]]}, {"id": "2002.00895", "submitter": "Carlo Fulvi Mari Ph.D.", "authors": "Carlo Fulvi Mari", "title": "Inferring population statistics of receptor neurons sensitivities and\n  firing-rates from general functional requirements", "comments": "9 pages, 2+1 figures (PDF). Minor modifications", "journal-ref": "BioSystems (2020) 104153", "doi": "10.1016/j.biosystems.2020.104153", "report-no": null, "categories": "q-bio.NC cond-mat.dis-nn physics.bio-ph", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  On the basis of the evident ability of neuronal olfactory systems to evaluate\nthe intensity of an odorous stimulus and at the same time also recognise the\nidentity of the odorant over a large range of concentrations, a few\nbiologically-realistic hypotheses on some of the underlying neural processes\nare made. In particular, it is assumed that the receptor neurons mean\nfiring-rate scale monotonically with odorant intensity, and that the receptor\nsensitivities range widely across odorants and receptor neurons hence leading\nto highly distributed representations of the stimuli. The mathematical\nimplementation of the phenomenological postulates allows for inferring explicit\nfunctional relationships between some measurable quantities. It results that\nboth the dependence of the mean firing-rate on odorant concentration and the\nstatistical distribution of receptor sensitivity across the neuronal population\nare power-laws, whose respective exponents are in an arithmetic, testable\nrelationship.\n  In order to test quantitatively the prediction of power-law dependence of\npopulation mean firing-rate on odorant concentration, a probabilistic model is\ncreated to extract information from data available in the experimental\nliterature. The values of the free parameters of the model are estimated by an\ninfo-geometric Bayesian maximum-likelihood inference which keeps into account\nthe prior distribution of the parameters. The eventual goodness of fit is\nquantified by means of a distribution-independent test.\n  [CONTINUES]\n", "versions": [{"version": "v1", "created": "Mon, 3 Feb 2020 17:15:59 GMT"}, {"version": "v2", "created": "Sat, 2 May 2020 19:21:26 GMT"}], "update_date": "2020-06-29", "authors_parsed": [["Mari", "Carlo Fulvi", ""]]}, {"id": "2002.00920", "submitter": "Vincent Adam", "authors": "Vincent Adam, Alexandre Hyafil", "title": "Non-linear regression models for behavioral and neural data analysis", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP q-bio.NC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Regression models are popular tools in empirical sciences to infer the\ninfluence of a set of variables onto a dependent variable given an experimental\ndataset. In neuroscience and cognitive psychology, Generalized Linear Models\n(GLMs) -including linear regression, logistic regression, and Poisson GLM- is\nthe regression model of choice to study the factors that drive participant's\nchoices, reaction times and neural activations. These methods are however\nlimited as they only capture linear contributions of each regressors. Here, we\nintroduce an extension of GLMs called Generalized Unrestricted Models (GUMs),\nwhich allows to infer a much richer set of contributions of the regressors to\nthe dependent variable, including possible interactions between the regressors.\nIn a GUM, each regressor is passed through a linear or nonlinear function, and\nthe contribution of the different resulting transformed regressors can be\nsummed or multiplied to generate a predictor for the dependent variable. We\npropose a Bayesian treatment of these models in which we endow functions with\nGaussian Process priors, and we present two methods to compute a posterior over\nthe functions given a dataset: the Laplace method and a sparse variational\napproach, which scales better for large dataset. For each method, we assess the\nquality of the model estimation and we detail how the hyperparameters (defining\nfor example the expected smoothness of the function) can be fitted. Finally, we\nillustrate the power of the method on a behavioral dataset where subjects\nreported the average perceived orientation of a series of gratings. The method\nallows to recover the mapping of the grating angle onto perceptual evidence for\neach subject, as well as the impact of the grating based on its position.\nOverall, GUMs provides a very rich and flexible framework to run nonlinear\nregression analysis in neuroscience, psychology, and beyond.\n", "versions": [{"version": "v1", "created": "Mon, 3 Feb 2020 18:00:18 GMT"}], "update_date": "2020-02-04", "authors_parsed": [["Adam", "Vincent", ""], ["Hyafil", "Alexandre", ""]]}, {"id": "2002.01117", "submitter": "Seo-Hyun Lee", "authors": "Seo-Hyun Lee, Minji Lee, Seong-Whan Lee", "title": "Spatio-Temporal Dynamics of Visual Imagery for Intuitive Brain-Computer\n  Interface", "comments": "5 pages, 4 figures, 3 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC q-bio.NC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Visual imagery is an intuitive brain-computer interface paradigm, referring\nto the emergence of the visual scene. Despite its convenience, analysis of its\nintrinsic characteristics is limited. In this study, we demonstrate the effect\nof time interval and channel selection that affects the decoding performance of\nthe multi-class visual imagery. We divided the epoch into time intervals of 0-1\ns and 1-2 s and performed six-class classification in three different brain\nregions: whole brain, visual cortex, and prefrontal cortex. In the time\ninterval, 0-1 s group showed 24.2 % of average classification accuracy, which\nwas significantly higher than the 1-2 s group in the prefrontal cortex. In the\nthree different regions, the classification accuracy of the prefrontal cortex\nshowed significantly higher performance than the visual cortex in 0-1 s\ninterval group, implying the cognitive arousal during the visual imagery. This\nfinding would provide crucial information in improving the decoding\nperformance.\n", "versions": [{"version": "v1", "created": "Tue, 4 Feb 2020 04:25:53 GMT"}, {"version": "v2", "created": "Thu, 27 Feb 2020 05:05:48 GMT"}], "update_date": "2020-02-28", "authors_parsed": [["Lee", "Seo-Hyun", ""], ["Lee", "Minji", ""], ["Lee", "Seong-Whan", ""]]}, {"id": "2002.01120", "submitter": "Byoung-Hee Kwon", "authors": "Byoung-Hee Kwon, Ji-Hoon Jeong, Dong-Joo Kim", "title": "A Novel Framework for Visual Motion Imagery Classification Using 3D\n  Virtual BCI Platform", "comments": "5 pages, 4 figures, 2 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC q-bio.NC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this study, 3D brain-computer interface (BCI) training platforms were used\nto stimulate the subjects for visual motion imagery and visual perception. We\nmeasured the activation brain region and alpha-band power activity when the\nsubjects perceived and imagined the stimuli. Based on this, 4-class were\nclassified in visual stimuli session and visual motion imagery session\nrespectively. The results showed that the occipital region is involved in\nvisual perception and visual motion imagery, and alpha-band power is increased\nin visual motion imagery session and decreased in visual motion stimuli\nsession. Compared with the performance of visual motion imagery and motor\nimagery, visual motion imagery has higher performance than motor imagery. The\nbinary class was classified using one versus rest approach as well as analysis\nof brain activation to prove that visual-related brain wave signals are\nmeaningful, and the results were significant.\n", "versions": [{"version": "v1", "created": "Tue, 4 Feb 2020 04:30:38 GMT"}], "update_date": "2020-02-05", "authors_parsed": [["Kwon", "Byoung-Hee", ""], ["Jeong", "Ji-Hoon", ""], ["Kim", "Dong-Joo", ""]]}, {"id": "2002.01126", "submitter": "Jenifer Kalafatovich", "authors": "Jenifer Kalafatovich, Minji Lee", "title": "Neural Oscillations for Encoding and Decoding Declarative Memory using\n  EEG Signals", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NE eess.SP q-bio.NC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Declarative memory has been studied for its relationship with remembering\ndaily life experiences. Previous studies reported changes in power spectra\nduring encoding phase related to behavioral performance, however decoding phase\nstill needs to be explored. This study investigates neural oscillations changes\nrelated to memory process. Participants were asked to perform a memory task for\nencoding and decoding phase while EEG signals were recorded. Results showed\nthat for encoding phase, there was a significant decrease of power in low beta,\nhigh beta bands over fronto-central area and a decrease in low beta, high beta\nand gamma bands over left temporal area related to successful subsequent memory\neffects. For decoding phase, only significant decreases of alpha power were\nobserved over fronto-central area. This finding showed relevance of beta and\nalpha band for encoding and decoding phase of a memory task respectively.\n", "versions": [{"version": "v1", "created": "Tue, 4 Feb 2020 04:53:30 GMT"}], "update_date": "2020-02-05", "authors_parsed": [["Kalafatovich", "Jenifer", ""], ["Lee", "Minji", ""]]}, {"id": "2002.01467", "submitter": "Lilianne Mujica-Parodi", "authors": "LR Mujica-Parodi and HH Strey", "title": "Making Sense of Computational Psychiatry", "comments": "16 pages, 5 figures. Int J Neuropsychopharmacol. 2020 Mar 27", "journal-ref": null, "doi": "10.1093/ijnp/pyaa013", "report-no": null, "categories": "q-bio.NC stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In psychiatry, we often speak of constructing \"models.\" Here we try to make\nsense of what such a claim might mean, starting with the most fundamental\nquestion: \"What is (and isn't) a model?\". We then discuss, in a concrete\nmeasurable sense, what it means for a model to be useful. In so doing, we first\nidentify the added value that a computational model can provide, in the context\nof accuracy and power. We then present the limitations of standard statistical\nmethods and provide suggestions for how we can expand the explanatory power of\nour analyses by reconceptualizing statistical models as dynamical systems.\nFinally, we address the problem of model building, suggesting ways in which\ncomputational psychiatry can escape the potential for cognitive biases imposed\nby classical hypothesis-driven research, exploiting deep systems-level\ninformation contained within neuroimaging data to advance our understanding of\npsychiatric neuroscience.\n", "versions": [{"version": "v1", "created": "Tue, 4 Feb 2020 18:46:58 GMT"}, {"version": "v2", "created": "Tue, 14 Apr 2020 15:30:17 GMT"}], "update_date": "2020-04-15", "authors_parsed": [["Mujica-Parodi", "LR", ""], ["Strey", "HH", ""]]}, {"id": "2002.01925", "submitter": "Khansa Rasheed", "authors": "Khansa Rasheed, Adnan Qayyum, Junaid Qadir, Shobi Sivathamboo, Patrick\n  Kwan, Levin Kuhlmann, Terence O'Brien, and Adeel Razi", "title": "Machine Learning for Predicting Epileptic Seizures Using EEG Signals: A\n  Review", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG eess.SP q-bio.NC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  With the advancement in artificial intelligence (AI) and machine learning\n(ML) techniques, researchers are striving towards employing these techniques\nfor advancing clinical practice. One of the key objectives in healthcare is the\nearly detection and prediction of disease to timely provide preventive\ninterventions. This is especially the case for epilepsy, which is characterized\nby recurrent and unpredictable seizures. Patients can be relieved from the\nadverse consequences of epileptic seizures if it could somehow be predicted in\nadvance. Despite decades of research, seizure prediction remains an unsolved\nproblem. This is likely to remain at least partly because of the inadequate\namount of data to resolve the problem. There have been exciting new\ndevelopments in ML-based algorithms that have the potential to deliver a\nparadigm shift in the early and accurate prediction of epileptic seizures. Here\nwe provide a comprehensive review of state-of-the-art ML techniques in early\nprediction of seizures using EEG signals. We will identify the gaps,\nchallenges, and pitfalls in the current research and recommend future\ndirections.\n", "versions": [{"version": "v1", "created": "Tue, 4 Feb 2020 06:22:24 GMT"}], "update_date": "2020-02-06", "authors_parsed": [["Rasheed", "Khansa", ""], ["Qayyum", "Adnan", ""], ["Qadir", "Junaid", ""], ["Sivathamboo", "Shobi", ""], ["Kwan", "Patrick", ""], ["Kuhlmann", "Levin", ""], ["O'Brien", "Terence", ""], ["Razi", "Adeel", ""]]}, {"id": "2002.02381", "submitter": "Cecilia Romaro Miss", "authors": "Cecilia Romaro, Antonio Carlos Roque and Jose Roberto Castilho\n  Piqueira", "title": "Boundary solution based on rescaling method: recoup the first and\n  second-order statistics of neuron network dynamics", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "q-bio.NC cs.NA math.DS math.NA", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  There is a strong nexus between the network size and the computational\nresources available, which may impede a neuroscience study. In the meantime,\nrescaling the network while maintaining its behavior is not a trivial mission.\nAdditionally, modeling patterns of connections under topographic organization\npresents an extra challenge: to solve the network boundaries or mingled with an\nunwished behavior. This behavior, for example, could be an inset oscillation\ndue to the torus solution; or a blend with/of unbalanced neurons due to a lack\n(or overdose) of connections. We detail the network rescaling method able to\nsustain behavior statistical utilized in Romaro et al. (2018) and present a\nboundary solution method based on the previous statistics recoup idea.\n", "versions": [{"version": "v1", "created": "Fri, 31 Jan 2020 21:10:08 GMT"}], "update_date": "2020-02-07", "authors_parsed": [["Romaro", "Cecilia", ""], ["Roque", "Antonio Carlos", ""], ["Piqueira", "Jose Roberto Castilho", ""]]}, {"id": "2002.02425", "submitter": "Marco Alberto Javarone", "authors": "Marco Alberto Javarone, Olivia Gosseries, Daniele Marinazzo, Quentin\n  Noirhomme, Vincent Bonhomme, Steven Laureys, Srivas Chennu", "title": "A mean field approach to model levels of consciousness from EEG\n  recordings", "comments": "23 pages, 6 figures. Accepted for publication in Journal of\n  Statistical Mechanics: Theory and Experiment", "journal-ref": null, "doi": null, "report-no": null, "categories": "q-bio.NC cond-mat.dis-nn cond-mat.stat-mech", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We introduce a mean-field model for analysing the dynamics of human\nconsciousness. In particular, inspired by the Giulio Tononi's Integrated\nInformation Theory and by the Max Tegmark's representation of consciousness, we\nstudy order-disorder phase transitions on Curie-Weiss models generated by\nprocessing EEG signals. The latter have been recorded on healthy individuals\nundergoing deep sedation. Then, we implement a machine learning tool for\nclassifying mental states using, as input, the critical temperatures computed\nin the Curie-Weiss models. Results show that, by the proposed method, it is\npossible to discriminate between states of awareness and states of deep\nsedation. Besides, we identify a state space for representing the path between\nmental states, whose dimensions correspond to critical temperatures computed\nover different frequency bands of the EEG signal. Beyond possible theoretical\nimplications in the study of human consciousness, resulting from our model, we\ndeem relevant to emphasise that the proposed method could be exploited for\nclinical applications.\n", "versions": [{"version": "v1", "created": "Thu, 6 Feb 2020 18:09:37 GMT"}, {"version": "v2", "created": "Tue, 4 Aug 2020 16:03:08 GMT"}], "update_date": "2020-08-05", "authors_parsed": [["Javarone", "Marco Alberto", ""], ["Gosseries", "Olivia", ""], ["Marinazzo", "Daniele", ""], ["Noirhomme", "Quentin", ""], ["Bonhomme", "Vincent", ""], ["Laureys", "Steven", ""], ["Chennu", "Srivas", ""]]}, {"id": "2002.02496", "submitter": "Omri Barak", "authors": "Tie Xu and Omri Barak", "title": "Implementing Inductive bias for different navigation tasks through\n  diverse RNN attractors", "comments": "Main text: 11 pages, 5 figures. Supplementary: 16 pages, 14 figures,\n  3 tables. Accepted as a conference paper for ICLR 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "q-bio.NC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Navigation is crucial for animal behavior and is assumed to require an\ninternal representation of the external environment, termed a cognitive map.\nThe precise form of this representation is often considered to be a metric\nrepresentation of space. An internal representation, however, is judged by its\ncontribution to performance on a given task, and may thus vary between\ndifferent types of navigation tasks. Here we train a recurrent neural network\nthat controls an agent performing several navigation tasks in a simple\nenvironment. To focus on internal representations, we split learning into a\ntask-agnostic pre-training stage that modifies internal connectivity and a\ntask-specific Q learning stage that controls the network's output. We show that\npre-training shapes the attractor landscape of the networks, leading to either\na continuous attractor, discrete attractors or a disordered state. These\nstructures induce bias onto the Q-Learning phase, leading to a performance\npattern across the tasks corresponding to metric and topological regularities.\nBy combining two types of networks in a modular structure, we could get better\nperformance for both regularities. Our results show that, in recurrent\nnetworks, inductive bias takes the form of attractor landscapes -- which can be\nshaped by pre-training and analyzed using dynamical systems methods.\nFurthermore, we demonstrate that non-metric representations are useful for\nnavigation tasks, and their combination with metric representation leads to\nflexibile multiple-task learning.\n", "versions": [{"version": "v1", "created": "Thu, 6 Feb 2020 20:05:13 GMT"}], "update_date": "2020-02-10", "authors_parsed": [["Xu", "Tie", ""], ["Barak", "Omri", ""]]}, {"id": "2002.02807", "submitter": "Thomas Passer Jensen", "authors": "T. P. Jensen, S. Tata, A. J. Ijspeert, S. Tolu", "title": "Adaptive control for hindlimb locomotion in a simulated mouse through\n  temporal cerebellar learning", "comments": "To be published in NICE '20: Proceedings of the 8th Annual\n  Neuro-inspired Computational Elements Workshop. 8 pages, 13 figures", "journal-ref": null, "doi": "10.1145/3381755", "report-no": null, "categories": "q-bio.NC cs.NE", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Human beings and other vertebrates show remarkable performance and efficiency\nin locomotion, but the functioning of their biological control systems for\nlocomotion is still only partially understood. The basic patterns and timing\nfor locomotion are provided by a central pattern generator (CPG) in the spinal\ncord. The cerebellum is known to play an important role in adaptive locomotion.\nRecent studies have given insights into the error signals responsible for\ndriving the cerebellar adaptation in locomotion. However, the question of how\nthe cerebellar output influences the gait remains unanswered. We hypothesize\nthat the cerebellar correction is applied to the pattern formation part of the\nCPG. Here, a bio-inspired control system for adaptive locomotion of the\nmusculoskeletal system of the mouse is presented, where a cerebellar-like\nmodule adapts the step time by using the double support interlimb asymmetry as\na temporal teaching signal. The control system is tested on a simulated mouse\nin a split-belt treadmill setup similar to those used in experiments with real\nmice. The results show adaptive locomotion behavior in the interlimb parameters\nsimilar to that seen in humans and mice. The control system adaptively\ndecreases the double support asymmetry that occurs due to environmental\nperturbations in the split-belt protocol.\n", "versions": [{"version": "v1", "created": "Fri, 7 Feb 2020 14:25:21 GMT"}, {"version": "v2", "created": "Mon, 17 Feb 2020 09:00:27 GMT"}], "update_date": "2020-07-14", "authors_parsed": [["Jensen", "T. P.", ""], ["Tata", "S.", ""], ["Ijspeert", "A. J.", ""], ["Tolu", "S.", ""]]}, {"id": "2002.02936", "submitter": "Vincent Huin", "authors": "Vincent Huin (JPArc - U1172 Inserm), Claire-Marie Dhaenens (JPArc -\n  U837 Inserm), M\\'egane Homa (JPArc - U1172 Inserm), K\\'evin Carvalho, Luc\n  Bu\\'ee, Bernard Sablonni\\`ere", "title": "Neurogenetics of the Human Adenosine Receptor Genes: Genetic Structures\n  and Involvement in Brain Diseases", "comments": null, "journal-ref": "Journal of Caffeine and Adenosine Research, New Rochelle, N.Y. :\n  Mary Ann Liebert, Inc., [2018]-, 2019, 9 (3), pp.73-88", "doi": "10.1089/caff.2019.0011", "report-no": null, "categories": "q-bio.QM q-bio.NC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Adenosine receptors are G-protein-coupled receptors involved in a wide range\nof physiological and pathological phenomena in most mammalian systems. All four\nreceptors are widely expressed in the central nervous system, where they\nmodulate neurotransmitter release and neuronal plasticity. A large number of\ngene association studies have shown that common genetic variants of the\nadenosine receptors (encoded by the ADORA1, ADORA2A, ADORA2B and ADORA3 genes)\nhave a neuroprotective or neurodegenerative role in neurologic/psychiatric\ndiseases. New genetic studies of rare variants and few novel associations with\ndepression or epilepsy subtypes have recently been reported. Here, we review\nthe literature on the genetics of adenosine receptors in neurologic and/or\npsychiatric diseases in humans, and discuss perspectives for further genetic\nresearch. We also provide an update on the genetic structures of the four human\nadenosine receptor genes and their regulation - a topic that has not been\nextensively addressed. Our review emphasizes the importance of (i) better\ncharacterizing the genetics of adenosine receptor genes and (ii) understanding\nhow these genes are regulated.\n", "versions": [{"version": "v1", "created": "Fri, 31 Jan 2020 14:04:41 GMT"}], "update_date": "2020-02-10", "authors_parsed": [["Huin", "Vincent", "", "JPArc - U1172 Inserm"], ["Dhaenens", "Claire-Marie", "", "JPArc -\n  U837 Inserm"], ["Homa", "M\u00e9gane", "", "JPArc - U1172 Inserm"], ["Carvalho", "K\u00e9vin", ""], ["Bu\u00e9e", "Luc", ""], ["Sablonni\u00e8re", "Bernard", ""]]}, {"id": "2002.03211", "submitter": "Kendrick Kay", "authors": "Konrad P. Kording, Gunnar Blohm, Paul Schrater, Kendrick Kay", "title": "Appreciating the variety of goals in computational neuroscience", "comments": "Accepted for publication in Neurons, Behavior, Data Analysis, and\n  Theory", "journal-ref": null, "doi": null, "report-no": null, "categories": "q-bio.NC", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Within computational neuroscience, informal interactions with modelers often\nreveal wildly divergent goals. In this opinion piece, we explicitly address the\ndiversity of goals that motivate and ultimately influence modeling efforts. We\nargue that a wide range of goals can be meaningfully taken to be of highest\nimportance. A simple informal survey conducted on the Internet confirmed the\ndiversity of goals in the community. However, different priorities or\npreferences of individual researchers can lead to divergent model evaluation\ncriteria. We propose that many disagreements in evaluating the merit of\ncomputational research stem from differences in goals and not from the\nmechanics of constructing, describing, and validating models. We suggest that\nauthors state explicitly their goals when proposing models so that others can\njudge the quality of the research with respect to its stated goals.\n", "versions": [{"version": "v1", "created": "Sat, 8 Feb 2020 18:01:17 GMT"}], "update_date": "2020-02-11", "authors_parsed": [["Kording", "Konrad P.", ""], ["Blohm", "Gunnar", ""], ["Schrater", "Paul", ""], ["Kay", "Kendrick", ""]]}, {"id": "2002.03420", "submitter": "Omri Barak", "authors": "Omri Barak and Sandro Romani", "title": "Mapping low-dimensional dynamics to high-dimensional neural activity: A\n  derivation of the ring model from the neural engineering framework", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "q-bio.NC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Empirical estimates of the dimensionality of neural population activity are\noften much lower than the population size. Similar phenomena are also observed\nin trained and designed neural network models. These experimental and\ncomputational results suggest that mapping low-dimensional dynamics to\nhigh-dimensional neural space is a common feature of cortical computation.\nDespite the ubiquity of this observation, the constraints arising from such\nmapping are poorly understood. Here we consider a specific example of mapping\nlow-dimensional dynamics to high-dimensional neural activity -- the neural\nengineering framework. We analytically solve the framework for the classic ring\nmodel -- a neural network encoding a static or dynamic angular variable. Our\nresults provide a complete characterization of the success and failure modes\nfor this model. Based on similarities between this and other frameworks, we\nspeculate that these results could apply to more general scenarios.\n", "versions": [{"version": "v1", "created": "Sun, 9 Feb 2020 18:34:08 GMT"}], "update_date": "2020-02-11", "authors_parsed": [["Barak", "Omri", ""], ["Romani", "Sandro", ""]]}, {"id": "2002.03472", "submitter": "Dan Malowany PhD", "authors": "Dan Malowany, Hugo Guterman", "title": "Biologically Inspired Visual System Architecture for Object Recognition\n  in Autonomous Systems", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV q-bio.NC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Findings in recent years on the sensitivity of convolutional neural networks\nto additive noise, light conditions and to the wholeness of the training\ndataset, indicate that this technology still lacks the robustness needed for\nthe autonomous robotic industry. In an attempt to bring computer vision\nalgorithms closer to the capabilities of a human operator, the mechanisms of\nthe human visual system was analyzed in this work. Recent studies show that the\nmechanisms behind the recognition process in the human brain include continuous\ngeneration of predictions based on prior knowledge of the world. These\npredictions enable rapid generation of contextual hypotheses that bias the\noutcome of the recognition process. This mechanism is especially advantageous\nin situations of uncertainty, when visual input is ambiguous. In addition, the\nhuman visual system continuously updates its knowledge about the world based on\nthe gaps between its prediction and the visual feedback. Convolutional neural\nnetworks are feed forward in nature and lack such top-down contextual\nattenuation mechanisms. As a result, although they process massive amounts of\nvisual information during their operation, the information is not transformed\ninto knowledge that can be used to generate contextual predictions and improve\ntheir performance. In this work, an architecture was designed that aims to\nintegrate the concepts behind the top-down prediction and learning processes of\nthe human visual system with the state of the art bottom-up object recognition\nmodels, e.g., deep convolutional neural networks. The work focuses on two\nmechanisms of the human visual system: anticipation-driven perception and\nreinforcement-driven learning. Imitating these top-down mechanisms, together\nwith the state of the art bottom-up feed-forward algorithms, resulted in an\naccurate, robust, and continuously improving target recognition model.\n", "versions": [{"version": "v1", "created": "Sun, 9 Feb 2020 23:51:45 GMT"}, {"version": "v2", "created": "Wed, 22 Jul 2020 13:45:47 GMT"}], "update_date": "2020-07-23", "authors_parsed": [["Malowany", "Dan", ""], ["Guterman", "Hugo", ""]]}, {"id": "2002.03540", "submitter": "Wenzhuo Zhang", "authors": "Wen-Zhuo Zhang", "title": "A wave-pulse neural network for quasi-quantum coding", "comments": "5 pages, 2 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "q-bio.NC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We design a physical wave-pulse neural network (WPNN) for both wave and pulse\npropagation, which gives more degrees of freedom for neural coding than spike\nneural networks (SNN). We define the rules and the information entropy of this\nkind of neural network, where the signal speed, arrival time, and the length of\nconnections between neurons all become crucial parameters for signal coding. We\ncall it quasi-quantum coding (QQC) since the combination of wave and pulse\nsignals here behaves like a classical mimic of quantum wave-particle duality,\nand can be studied by borrowing some concepts form quantum mechanics. We\npresent that the quasi-quantum coding can give efficient methods for both sound\nand image recognitions. We also discuss the possibility of the wave-pulse\nneural network and the quasi-quantum coding methods running on it in biological\nbrains where both neural oscillations and action potentials are important to\ncognition.\n", "versions": [{"version": "v1", "created": "Thu, 6 Feb 2020 08:20:18 GMT"}], "update_date": "2020-02-11", "authors_parsed": [["Zhang", "Wen-Zhuo", ""]]}, {"id": "2002.03542", "submitter": "Caitlin Lienkaemper", "authors": "Alexander Kunin, Caitlin Lienkaemper, Zvi Rosen", "title": "Oriented Matroids and Combinatorial Neural Codes", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.CO math.AC q-bio.NC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A combinatorial neural code $\\mathscr C\\subseteq 2^{[n]}$ is convex if it\narises as the intersection pattern of convex open subsets of $\\mathbb R^d$. We\nrelate the emerging theory of convex neural codes to the established theory of\noriented matroids, both categorically and with respect to geometry and\ncomputational complexity. On the categorical side, we show that the map taking\nan acyclic oriented matroid to the code of positive parts of its topes is a\nfaithful functor. We adapt the oriented matroid ideal introduced by Novik,\nPostnikov, and Sturmfels into a functor from the category of oriented matroids\nto the category of rings; then, we show that the resulting ring maps naturally\nto the neural ring of the matroid's neural code.\n  For geometry and computational complexity, we show that a code has a\nrealization with convex polytopes if and only if it lies below the code of a\nrepresentable oriented matroid in the partial order of codes introduced by\nJeffs. We show that previously published examples of non-convex codes do not\nlie below any oriented matroids, and we construct examples of non-convex codes\nlying below non-representable oriented matroids. By way of this construction,\nwe can apply Mn\\\"{e}v-Sturmfels universality to show that deciding whether a\ncombinatorial code is convex is NP-hard.\n", "versions": [{"version": "v1", "created": "Mon, 10 Feb 2020 04:47:50 GMT"}], "update_date": "2020-02-11", "authors_parsed": [["Kunin", "Alexander", ""], ["Lienkaemper", "Caitlin", ""], ["Rosen", "Zvi", ""]]}, {"id": "2002.03553", "submitter": "Aaron Voelker", "authors": "Aaron R. Voelker and Daniel Rasmussen and Chris Eliasmith", "title": "A Spike in Performance: Training Hybrid-Spiking Neural Networks with\n  Quantized Activation Functions", "comments": "8 pages, 7 page supplementary", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG q-bio.NC stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The machine learning community has become increasingly interested in the\nenergy efficiency of neural networks. The Spiking Neural Network (SNN) is a\npromising approach to energy-efficient computing, since its activation levels\nare quantized into temporally sparse, one-bit values (i.e., \"spike\" events),\nwhich additionally converts the sum over weight-activity products into a simple\naddition of weights (one weight for each spike). However, the goal of\nmaintaining state-of-the-art (SotA) accuracy when converting a non-spiking\nnetwork into an SNN has remained an elusive challenge, primarily due to spikes\nhaving only a single bit of precision. Adopting tools from signal processing,\nwe cast neural activation functions as quantizers with temporally-diffused\nerror, and then train networks while smoothly interpolating between the\nnon-spiking and spiking regimes. We apply this technique to the Legendre Memory\nUnit (LMU) to obtain the first known example of a hybrid SNN outperforming SotA\nrecurrent architectures -- including the LSTM, GRU, and NRU -- in accuracy,\nwhile reducing activities to at most 3.74 bits on average with 1.26 significant\nbits multiplying each weight. We discuss how these methods can significantly\nimprove the energy efficiency of neural networks.\n", "versions": [{"version": "v1", "created": "Mon, 10 Feb 2020 05:24:27 GMT"}, {"version": "v2", "created": "Mon, 1 Mar 2021 22:02:28 GMT"}], "update_date": "2021-03-03", "authors_parsed": [["Voelker", "Aaron R.", ""], ["Rasmussen", "Daniel", ""], ["Eliasmith", "Chris", ""]]}, {"id": "2002.03738", "submitter": "Tim Palmer", "authors": "T.N.Palmer", "title": "Human Creativity and Consciousness: Unintended Consequences of the\n  Brain's Extraordinary Energy Efficiency?", "comments": "Based on invited talk at Models of Consciousness Conference,\n  Mathematical Institute, University of Oxford, September 2019", "journal-ref": null, "doi": "10.3390/e22030281", "report-no": null, "categories": "q-bio.NC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  It is proposed that both human creativity and human consciousness are\n(unintended) consequences of the human brain's extraordinary energy efficiency.\nThe topics of creativity and consciousness are treated separately, though have\na common sub-structure. It is argued that creativity arises from a synergy\nbetween two cognitive modes of the human brain (which broadly coincide with\nKahneman's Systems 1 and 2). In the first, available energy is spread across a\nrelatively large network of neurons. As such, the amount of energy per active\nneuron is so small that the operation of such neurons is susceptible to thermal\n(ultimately quantum decoherent) noise. In the second, available energy is\nfocussed on a small enough subset of neurons to guarantee a deterministic\noperation. An illustration of how this synergy can lead to creativity with\nimplications for computing in silicon are discussed. Starting with a discussion\nof the concept of free will, the notion of consciousness is defined in terms of\nan awareness of what are perceived to be nearby counterfactual worlds in state\nspace. It is argued that such awareness arises from an interplay between our\nmemories on the one hand, and quantum physical mechanisms (where, unlike in\nclassical physics, nearby counterfactual worlds play an indispensable dynamical\nrole) in the ion channels of neural networks. As with the brain's\nsusceptibility to noise, it is argued that in situations where quantum physics\nplays a role in the brain, it does so for reasons of energy efficiency. As an\nillustration of this definition of consciousness, a novel proposal is outlined\nas to why quantum entanglement appears so counter-intuitive.\n", "versions": [{"version": "v1", "created": "Fri, 31 Jan 2020 09:12:00 GMT"}], "update_date": "2020-04-22", "authors_parsed": [["Palmer", "T. N.", ""]]}, {"id": "2002.04176", "submitter": "Callum Stewart", "authors": "Callum L. Stewart, Amos Folarin, Richard Dobson", "title": "Personalized acute stress classification from physiological signals with\n  neural processes", "comments": "16 pages (inc. references), 5 figures, 3 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP q-bio.NC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Objective: A person's affective state has known relationships to\nphysiological processes which can be measured by wearable sensors. However,\nwhile there are general trends those relationships can be person-specific. This\nwork proposes using neural processes as a way to address individual\ndifferences.\n  Methods: Stress classifiers built from classic machine learning models and\nfrom neural processes are compared on two datasets using\nleave-one-participant-out cross-validation. The neural processes models are\ncontextualized on data from a brief period of a particular person's recording.\n  Results: The neural processes models outperformed the standard machine\nlearning models, and had the best performance when using periods of stress and\nbaseline as context. Contextual points chosen from other participants led to\nlower performance.\n  Conclusion: Neural processes can learn to adapt to person-specific\nphysiological sensor data. There are a wide range of affective and medical\napplications for which this model could prove useful.\n", "versions": [{"version": "v1", "created": "Tue, 11 Feb 2020 02:38:39 GMT"}], "update_date": "2020-02-12", "authors_parsed": [["Stewart", "Callum L.", ""], ["Folarin", "Amos", ""], ["Dobson", "Richard", ""]]}, {"id": "2002.04309", "submitter": "Miguel Aguilera", "authors": "Miguel Aguilera, S. Amin Moosavi, Hideaki Shimazaki", "title": "A unifying framework for mean-field theories of asymmetric kinetic Ising\n  systems", "comments": null, "journal-ref": "Nature Communications 12, 1197 (2021)", "doi": "10.1038/s41467-021-20890-5", "report-no": null, "categories": "cond-mat.dis-nn cond-mat.stat-mech nlin.AO physics.bio-ph q-bio.NC", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Kinetic Ising models are powerful tools for studying the non-equilibrium\ndynamics of complex systems. As their behavior is not tractable for large\nnetworks, many mean-field methods have been proposed for their analysis, each\nbased on unique assumptions about the system's temporal evolution. This\ndisparity of approaches makes it challenging to systematically advance\nmean-field methods beyond previous contributions. Here, we propose a unifying\nframework for mean-field theories of asymmetric kinetic Ising systems from an\ninformation geometry perspective. The framework is built on Plefka expansions\nof a system around a simplified model obtained by an orthogonal projection to a\nsub-manifold of tractable probability distributions. This view not only unifies\nprevious methods but also allows us to develop novel methods that, in contrast\nwith traditional approaches, preserve the system's correlations. We show that\nthese new methods can outperform previous ones in predicting and assessing\nnetwork properties near maximally fluctuating regimes.\n", "versions": [{"version": "v1", "created": "Tue, 11 Feb 2020 11:02:56 GMT"}, {"version": "v2", "created": "Thu, 30 Apr 2020 16:41:17 GMT"}, {"version": "v3", "created": "Mon, 25 Jan 2021 19:06:03 GMT"}, {"version": "v4", "created": "Thu, 11 Mar 2021 11:39:49 GMT"}, {"version": "v5", "created": "Wed, 12 May 2021 13:56:04 GMT"}], "update_date": "2021-05-13", "authors_parsed": [["Aguilera", "Miguel", ""], ["Moosavi", "S. Amin", ""], ["Shimazaki", "Hideaki", ""]]}, {"id": "2002.04501", "submitter": "Karl Friston", "authors": "Karl Friston, Lancelot Da Costa and Thomas Parr", "title": "Some interesting observations on the free energy principle", "comments": "A response to a technical critique [arXiv:2001.06408] of the free\n  energy principle as presented in \"Life as we know it\"", "journal-ref": null, "doi": null, "report-no": null, "categories": "q-bio.NC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Biehl et al (2020) present some interesting observations on an early\nformulation of the free energy principle in (Friston, 2013). We use these\nobservations to scaffold a discussion of the technical arguments that\nunderwrite the free energy principle. This discussion focuses on solenoidal\ncoupling between various (subsets of) states in sparsely coupled systems that\npossess a Markov blanket - and the distinction between exact and approximate\nBayesian inference, implied by the ensuing Bayesian mechanics.\n", "versions": [{"version": "v1", "created": "Wed, 5 Feb 2020 13:40:48 GMT"}], "update_date": "2020-02-12", "authors_parsed": [["Friston", "Karl", ""], ["Da Costa", "Lancelot", ""], ["Parr", "Thomas", ""]]}, {"id": "2002.04527", "submitter": "Sebastiano Stramaglia", "authors": "Davide Nuzzi, Mario Pellicoro, Leonardo Angelini, Daniele Marinazzo,\n  Sebastiano Stramaglia", "title": "Synergistic information in a dynamical model implemented on the human\n  structural connectome reveals spatially distinct associations with age", "comments": "18 pages, 8 figures", "journal-ref": null, "doi": "10.1162/netn_a_00146", "report-no": null, "categories": "q-bio.NC cond-mat.dis-nn cond-mat.stat-mech", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We implement the dynamical Ising model on the large scale architecture of\nwhite matter connections of healthy subjects in the age range 4-85 years, and\nanalyze the dynamics in terms of the synergy, a quantity measuring the extent\nto which the joint state of pairs of variables is projected onto the dynamics\nof a target one. We find that the amount of synergy in explaining the dynamics\nof the hubs of the structural connectivity (in terms of degree strength) peaks\nbefore the critical temperature, and can thus be considered as a precursor of a\ncritical transition. Conversely the greatest amount of synergy goes into\nexplaining the dynamics of more central nodes. We also find that the aging of\nthe structural connectivity is associated to significant changes in the\nsimulated dynamics: there are brain regions whose synergy decreases with age,\nin particular the frontal pole, the Subcallosal area and the Supplementary\nMotor area; these areas could then be more likely to show a decline in terms of\nthe capability to perform higher order computation (if structural connectivity\nwas the sole variable). On the other hand, several regions in the temporal\ncortex show a positive correlation with age in the first 30 years of life, i.e.\nduring brain maturation.\n", "versions": [{"version": "v1", "created": "Tue, 11 Feb 2020 16:30:51 GMT"}], "update_date": "2020-05-15", "authors_parsed": [["Nuzzi", "Davide", ""], ["Pellicoro", "Mario", ""], ["Angelini", "Leonardo", ""], ["Marinazzo", "Daniele", ""], ["Stramaglia", "Sebastiano", ""]]}, {"id": "2002.04806", "submitter": "Terrence Sejnowski", "authors": "Terrence J. Sejnowski", "title": "The Unreasonable Effectiveness of Deep Learning in Artificial\n  Intelligence", "comments": null, "journal-ref": "Proceedings of the National Academy of Sciences U.S.A. (2020)\n  https://www.pnas.org/content/early/2020/01/23/1907373117", "doi": "10.1073/pnas.1907373117", "report-no": null, "categories": "q-bio.NC cs.AI cs.LG cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Deep learning networks have been trained to recognize speech, caption\nphotographs and translate text between languages at high levels of performance.\nAlthough applications of deep learning networks to real world problems have\nbecome ubiquitous, our understanding of why they are so effective is lacking.\nThese empirical results should not be possible according to sample complexity\nin statistics and non-convex optimization theory. However, paradoxes in the\ntraining and effectiveness of deep learning networks are being investigated and\ninsights are being found in the geometry of high-dimensional spaces. A\nmathematical theory of deep learning would illuminate how they function, allow\nus to assess the strengths and weaknesses of different network architectures\nand lead to major improvements. Deep learning has provided natural ways for\nhumans to communicate with digital devices and is foundational for building\nartificial general intelligence. Deep learning was inspired by the architecture\nof the cerebral cortex and insights into autonomy and general intelligence may\nbe found in other brain regions that are essential for planning and survival,\nbut major breakthroughs will be needed to achieve these goals.\n", "versions": [{"version": "v1", "created": "Wed, 12 Feb 2020 05:25:15 GMT"}], "update_date": "2020-02-13", "authors_parsed": [["Sejnowski", "Terrence J.", ""]]}, {"id": "2002.04924", "submitter": "Mattias Nilsson", "authors": "Mattias Nilsson, Foteini Liwicki and Fredrik Sandin", "title": "Synaptic Integration of Spatiotemporal Features with a Dynamic\n  Neuromorphic Processor", "comments": "Copyright 2020 IEEE. Personal use of this material is permitted.\n  Permission from IEEE must be obtained for all other uses, in any current or\n  future media, including reprinting/republishing this material for advertising\n  or promotional purposes, creating new collective works, for resale or\n  redistribution to servers or lists, or reuse of any copyrighted component of\n  this work in other works", "journal-ref": "2020 International Joint Conference on Neural Networks (IJCNN),\n  2020, pp. 1-7", "doi": "10.1109/IJCNN48605.2020.9207210", "report-no": null, "categories": "cs.NE cs.CV q-bio.NC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Spiking neurons can perform spatiotemporal feature detection by nonlinear\nsynaptic and dendritic integration of presynaptic spike patterns.\nMulticompartment models of non-linear dendrites and related neuromorphic\ncircuit designs enable faithful imitation of such dynamic integration\nprocesses, but these approaches are also associated with a relatively high\ncomputing cost or circuit size. Here, we investigate synaptic integration of\nspatiotemporal spike patterns with multiple dynamic synapses on point-neurons\nin the DYNAP-SE neuromorphic processor, which offers a complementary\nresource-efficient, albeit less flexible, approach to feature detection. We\ninvestigate how previously proposed excitatory--inhibitory pairs of dynamic\nsynapses can be combined to integrate multiple inputs, and we generalize that\nconcept to a case in which one inhibitory synapse is combined with multiple\nexcitatory synapses. We characterize the resulting delayed excitatory\npostsynaptic potentials (EPSPs) by measuring and analyzing the membrane\npotentials of the neuromorphic neuronal circuits. We find that biologically\nrelevant EPSP delays, with variability of order 10 milliseconds per neuron, can\nbe realized in the proposed manner by selecting different synapse combinations,\nthanks to device mismatch. Based on these results, we demonstrate that a single\npoint-neuron with dynamic synapses in the DYNAP-SE can respond selectively to\npresynaptic spikes with a particular spatiotemporal structure, which enables,\nfor instance, visual feature tuning of single neurons.\n", "versions": [{"version": "v1", "created": "Wed, 12 Feb 2020 11:26:35 GMT"}, {"version": "v2", "created": "Tue, 1 Jun 2021 14:05:32 GMT"}], "update_date": "2021-06-02", "authors_parsed": [["Nilsson", "Mattias", ""], ["Liwicki", "Foteini", ""], ["Sandin", "Fredrik", ""]]}, {"id": "2002.05487", "submitter": "Essam Rashed", "authors": "Essam A. Rashed, Jose Gomez-Tames, Akimasa Hirata", "title": "End-to-end semantic segmentation of personalized deep brain structures\n  for non-invasive brain stimulation", "comments": "To appear in Neural Networks", "journal-ref": "Neural Networks 125, pp. 233-244, 2020", "doi": "10.1016/j.neunet.2020.02.006", "report-no": null, "categories": "eess.IV cs.LG q-bio.NC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Electro-stimulation or modulation of deep brain regions is commonly used in\nclinical procedures for the treatment of several nervous system disorders. In\nparticular, transcranial direct current stimulation (tDCS) is widely used as an\naffordable clinical application that is applied through electrodes attached to\nthe scalp. However, it is difficult to determine the amount and distribution of\nthe electric field (EF) in the different brain regions due to anatomical\ncomplexity and high inter-subject variability. Personalized tDCS is an emerging\nclinical procedure that is used to tolerate electrode montage for accurate\ntargeting. This procedure is guided by computational head models generated from\nanatomical images such as MRI. Distribution of the EF in segmented head models\ncan be calculated through simulation studies. Therefore, fast, accurate, and\nfeasible segmentation of different brain structures would lead to a better\nadjustment for customized tDCS studies. In this study, a single-encoder\nmulti-decoders convolutional neural network is proposed for deep brain\nsegmentation. The proposed architecture is trained to segment seven deep brain\nstructures using T1-weighted MRI. Network generated models are compared with a\nreference model constructed using a semi-automatic method, and it presents a\nhigh matching especially in Thalamus (Dice Coefficient (DC) = 94.70%), Caudate\n(DC = 91.98%) and Putamen (DC = 90.31%) structures. Electric field distribution\nduring tDCS in generated and reference models matched well each other,\nsuggesting its potential usefulness in clinical practice.\n", "versions": [{"version": "v1", "created": "Thu, 13 Feb 2020 13:17:25 GMT"}], "update_date": "2020-04-29", "authors_parsed": [["Rashed", "Essam A.", ""], ["Gomez-Tames", "Jose", ""], ["Hirata", "Akimasa", ""]]}, {"id": "2002.05619", "submitter": "Cristiano Capone", "authors": "Paolo Muratore, Cristiano Capone, Pier Stanislao Paolucci", "title": "Target spiking patterns enable efficient and biologically plausible\n  learning for complex temporal tasks", "comments": "22 pages, 5 figures. original research", "journal-ref": "PLOS ONE 16(2): e0247014 (2021)", "doi": "10.1371/journal.pone.0247014", "report-no": null, "categories": "q-bio.NC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recurrent spiking neural networks (RSNN) in the human brain learn to perform\na wide range of perceptual, cognitive and motor tasks very efficiently in terms\nof energy consumption and requires very few examples. This motivates the search\nfor biologically inspired learning rules for RSNNs to improve our understanding\nof brain computation and the efficiency of artificial intelligence. Several\nspiking models and learning rules have been proposed, but it remains a\nchallenge to design RSNNs whose learning relies on biologically plausible\nmechanisms and are capable of solving complex temporal tasks. In this paper, we\nderive a learning rule, local to the synapse, from a simple mathematical\nprinciple, the maximization of the likelihood for the network to solve a\nspecific task. We propose a novel target-based learning scheme in which the\nlearning rule derived from likelihood maximization is used to mimic a specific\nspiking pattern that encodes the solution to complex temporal tasks. This\nmethod makes the learning extremely rapid and precise, outperforming state of\nthe art algorithms for RSNNs. We demonstrate the capacity of our model to\ntackle several problems like learning multidimensional trajectories and solving\nthe classical temporal XOR benchmark. Finally, we show that an online\napproximation of the gradient ascent, in addition to guaranteeing complete\nlocality in time and space, allows learning after very few presentations of the\ntarget output. Our model can be applied to different types of biological\nneurons. The analytically derived plasticity learning rule is specific to each\nneuron model and can produce a theoretical prediction for experimental\nvalidation.\n", "versions": [{"version": "v1", "created": "Thu, 13 Feb 2020 16:44:52 GMT"}, {"version": "v2", "created": "Thu, 27 Feb 2020 20:45:08 GMT"}, {"version": "v3", "created": "Mon, 25 Jan 2021 21:52:56 GMT"}, {"version": "v4", "created": "Fri, 19 Mar 2021 16:56:09 GMT"}], "update_date": "2021-03-22", "authors_parsed": [["Muratore", "Paolo", ""], ["Capone", "Cristiano", ""], ["Paolucci", "Pier Stanislao", ""]]}, {"id": "2002.05774", "submitter": "Richard Carson", "authors": "Richard G. Carson", "title": "What is the function of inter-hemispheric inhibition?", "comments": "45 pages (including references and figure legends), 4 figures", "journal-ref": "Journal of Physiology. 2020 Aug 8. Online ahead of print", "doi": "10.1113/JP279793", "report-no": null, "categories": "q-bio.NC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  It is widely supposed that following unilateral brain injury, there arises an\nasymmetry in inter-hemispheric inhibition which has an adverse influence upon\nmotor control. I argue that this 'inter-hemispheric imbalance' model arises\nfrom a fundamental misunderstanding of the roles played by inter-hemispheric\n(callosal) projections in mammalian brains. Drawing upon a large body of\nempirical data, derived largely from animal models, and associated theoretical\nmodeling, it is demonstrated that inter-hemispheric projections perform\ncontrast enhancing and integrative functions via mechanisms such as\nsurround/lateral inhibition. The principal functional unit of callosal\ninfluence comprises a facilitatory centre and a depressing peripheral zone,\nthat together shape the influence of converging inputs to pyramidal neurons.\nInter-hemispheric inhibition is an instance of a more general feature of\nmammalian neural systems, whereby inhibitory interneurons act not simply to\nprevent over-excitation but to sculpt the output of specific circuits. The\nnarrowing of the excitatory focus that occurs through crossed surround\ninhibition is a highly conserved motif of transcallosal interactions in\nmammalian sensory and motor cortices. A case is presented that the notion of\n'inter-hemispheric imbalance' has been sustained, and clinical interventions\nderived from this model promoted, by erroneous assumptions concerning that\nrevealed by investigative techniques such as transcranial magnetic stimulation\n(TMS). The alternative perspective promoted by the present analysis, also\npermits the basis of positive (e.g. post stroke) associations between the\nstructural integrity of transcallosal projections and motor capability to be\nbetter understood.\n", "versions": [{"version": "v1", "created": "Thu, 13 Feb 2020 20:52:47 GMT"}], "update_date": "2020-08-17", "authors_parsed": [["Carson", "Richard G.", ""]]}, {"id": "2002.06060", "submitter": "Sebastian Weichwald", "authors": "Sebastian Weichwald and Jonas Peters", "title": "Causality in cognitive neuroscience: concepts, challenges, and\n  distributional robustness", "comments": null, "journal-ref": "Journal of Cognitive Neuroscience, 33(2):226-247, 2021", "doi": "10.1162/jocn_a_01623", "report-no": null, "categories": "q-bio.NC stat.AP stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  While probabilistic models describe the dependence structure between observed\nvariables, causal models go one step further: they predict, for example, how\ncognitive functions are affected by external interventions that perturb\nneuronal activity. In this review and perspective article, we introduce the\nconcept of causality in the context of cognitive neuroscience and review\nexisting methods for inferring causal relationships from data. Causal inference\nis an ambitious task that is particularly challenging in cognitive\nneuroscience. We discuss two difficulties in more detail: the scarcity of\ninterventional data and the challenge of finding the right variables. We argue\nfor distributional robustness as a guiding principle to tackle these problems.\nRobustness (or invariance) is a fundamental principle underlying causal\nmethodology. A causal model of a target variable generalises across\nenvironments or subjects as long as these environments leave the causal\nmechanisms intact. Consequently, if a candidate model does not generalise, then\neither it does not consist of the target variable's causes or the underlying\nvariables do not represent the correct granularity of the problem. In this\nsense, assessing generalisability may be useful when defining relevant\nvariables and can be used to partially compensate for the lack of\ninterventional data.\n", "versions": [{"version": "v1", "created": "Fri, 14 Feb 2020 14:49:34 GMT"}, {"version": "v2", "created": "Fri, 3 Jul 2020 07:39:52 GMT"}], "update_date": "2021-04-12", "authors_parsed": [["Weichwald", "Sebastian", ""], ["Peters", "Jonas", ""]]}, {"id": "2002.06514", "submitter": "Shubhankar Patankar", "authors": "Shubhankar P. Patankar, Jason Z. Kim, Fabio Pasqualetti, and Danielle\n  S. Bassett", "title": "Path-dependent connectivity, not modularity, consistently predicts\n  controllability of structural brain networks", "comments": "32 pages, 7 figures in main text, and 22 pages, 11 figures in\n  supplement", "journal-ref": null, "doi": "10.1162/netn_a_00157", "report-no": null, "categories": "q-bio.NC", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  The human brain displays rich communication dynamics that are thought to be\nparticularly well-reflected in its marked community structure. Yet, the precise\nrelationship between community structure in structural brain networks and the\ncommunication dynamics that can emerge therefrom is not well-understood. In\naddition to offering insight into the structure-function relationship of\nnetworked systems, such an understanding is a critical step towards the ability\nto manipulate the brain's large-scale dynamical activity in a targeted manner.\nWe investigate the role of community structure in the controllability of\nstructural brain networks. At the region level, we find that certain network\nmeasures of community structure are sometimes statistically correlated with\nmeasures of linear controllability. However, we then demonstrate that this\nrelationship depends on the distribution of network edge weights. We highlight\nthe complexity of the relationship between community structure and\ncontrollability by performing numerical simulations using canonical graph\nmodels with varying mesoscale architectures and edge weight distributions.\nFinally, we demonstrate that weighted subgraph centrality, a measure rooted in\nthe graph spectrum, and which captures higher-order graph architecture, is a\nstronger and more consistent predictor of controllability. Our study\ncontributes to an understanding of how the brain's diverse mesoscale structure\nsupports transient communication dynamics.\n", "versions": [{"version": "v1", "created": "Sun, 16 Feb 2020 06:07:17 GMT"}, {"version": "v2", "created": "Tue, 14 Jul 2020 00:55:45 GMT"}, {"version": "v3", "created": "Fri, 18 Dec 2020 08:49:17 GMT"}], "update_date": "2020-12-23", "authors_parsed": [["Patankar", "Shubhankar P.", ""], ["Kim", "Jason Z.", ""], ["Pasqualetti", "Fabio", ""], ["Bassett", "Danielle S.", ""]]}, {"id": "2002.06816", "submitter": "Pamela K. Douglas", "authors": "Pamela K. Douglas, Farzad Vasheghani Farahani", "title": "On the Similarity of Deep Learning Representations Across Didactic and\n  Adversarial Examples", "comments": "2 figures", "journal-ref": "Med NeurIPS 2019", "doi": null, "report-no": null, "categories": "cs.CV cs.LG eess.IV q-bio.NC", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  The increasing use of deep neural networks (DNNs) has motivated a parallel\nendeavor: the design of adversaries that profit from successful\nmisclassifications. However, not all adversarial examples are crafted for\nmalicious purposes. For example, real world systems often contain physical,\ntemporal, and sampling variability across instrumentation. Adversarial examples\nin the wild may inadvertently prove deleterious for accurate predictive\nmodeling. Conversely, naturally occurring covariance of image features may\nserve didactic purposes. Here, we studied the stability of deep learning\nrepresentations for neuroimaging classification across didactic and adversarial\nconditions characteristic of MRI acquisition variability. We show that\nrepresentational similarity and performance vary according to the frequency of\nadversarial examples in the input space.\n", "versions": [{"version": "v1", "created": "Mon, 17 Feb 2020 07:49:20 GMT"}], "update_date": "2020-02-18", "authors_parsed": [["Douglas", "Pamela K.", ""], ["Farahani", "Farzad Vasheghani", ""]]}, {"id": "2002.07029", "submitter": "Pragya Srivastava", "authors": "Pragya Srivastava, Erfan Nozari, Jason Z. Kim, Harang Ju, Dale Zhou,\n  Cassiano Becker, Fabio Pasqualetti and Danielle S. Bassett", "title": "Models of communication and control for brain networks: distinctions,\n  convergence, and future outlook", "comments": "23 pages, 4 figures", "journal-ref": "Network Neuroscience, 2020", "doi": "10.1162/netn_a_00158", "report-no": null, "categories": "q-bio.NC physics.bio-ph", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recent advances in computational models of signal propagation and routing in\nthe human brain have underscored the critical role of white matter structure. A\ncomplementary approach has utilized the framework of network control theory to\nbetter understand how white matter constrains the manner in which a region or\nset of regions can direct or control the activity of other regions. Despite the\npotential for both of these approaches to enhance our understanding of the role\nof network structure in brain function, little work has sought to understand\nthe relations between them. Here, we seek to explicitly bridge computational\nmodels of communication and principles of network control in a conceptual\nreview of the current literature. By drawing comparisons between communication\nand control models in terms of the level of abstraction, the dynamical\ncomplexity, the dependence on network attributes, and the interplay of multiple\nspatiotemporal scales, we highlight the convergence of and distinctions between\nthe two frameworks. Based on the understanding of the intertwined nature of\ncommunication and control in human brain networks, this work provides an\nintegrative perspective for the field and outlines exciting directions for\nfuture work.\n", "versions": [{"version": "v1", "created": "Mon, 17 Feb 2020 16:24:11 GMT"}], "update_date": "2021-03-15", "authors_parsed": [["Srivastava", "Pragya", ""], ["Nozari", "Erfan", ""], ["Kim", "Jason Z.", ""], ["Ju", "Harang", ""], ["Zhou", "Dale", ""], ["Becker", "Cassiano", ""], ["Pasqualetti", "Fabio", ""], ["Bassett", "Danielle S.", ""]]}, {"id": "2002.07534", "submitter": "Konstantinos Michmizos", "authors": "Praveenram Balachandar and Konstantinos P. Michmizos", "title": "A Spiking Neural Network Emulating the Structure of the Oculomotor\n  System Requires No Learning to Control a Biomimetic Robotic Head", "comments": "6 pages, 3 figures, IEEE International Conference on Biomedical\n  Robotics and Biomechatronics (BioRob) 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.RO cs.NE q-bio.NC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Robotic vision introduces requirements for real-time processing of\nfast-varying, noisy information in a continuously changing environment. In a\nreal-world environment, convenient assumptions, such as static camera systems\nand deep learning algorithms devouring high volumes of ideally slightly-varying\ndata are hard to survive. Leveraging on recent studies on the neural connectome\nassociated with eye movements, we designed a neuromorphic oculomotor controller\nand placed it at the heart of our in-house biomimetic robotic head prototype.\nThe controller is unique in the sense that (1) all data are encoded and\nprocessed by a spiking neural network (SNN), and (2) by mimicking the\nassociated brain areas' topology, the SNN is biologically interpretable and\nrequires no training to operate. Here, we report the robot's target tracking\nability, demonstrate that its eye kinematics are similar to those reported in\nhuman eye studies and show that a biologically-constrained learning, although\nnot required for the SNN's function, can be used to further refine its\nperformance. This work aligns with our ongoing effort to develop\nenergy-efficient neuromorphic SNNs and harness their emerging intelligence to\ncontrol biomimetic robots with versatility and robustness.\n", "versions": [{"version": "v1", "created": "Tue, 18 Feb 2020 13:03:06 GMT"}, {"version": "v2", "created": "Mon, 8 Jun 2020 17:48:53 GMT"}], "update_date": "2020-06-09", "authors_parsed": [["Balachandar", "Praveenram", ""], ["Michmizos", "Konstantinos P.", ""]]}, {"id": "2002.07541", "submitter": "Neelesh Kumar", "authors": "Neelesh Kumar and Konstantinos P. Michmizos", "title": "Machine Learning for Motor Learning: EEG-based Continuous Assessment of\n  Cognitive Engagement for Adaptive Rehabilitation Robots", "comments": "6 pages, 6 figures, 1 table", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.RO cs.HC cs.NE q-bio.NC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Although cognitive engagement (CE) is crucial for motor learning, it remains\nunderutilized in rehabilitation robots, partly because its assessment currently\nrelies on subjective and gross measurements taken intermittently. Here, we\npropose an end-to-end computational framework that assesses CE in real-time,\nusing electroencephalography (EEG) signals as objective measurements. The\nframework consists of i) a deep convolutional neural network (CNN) that\nextracts task-discriminative spatiotemporal EEG to predict the level of CE for\ntwo classes -- cognitively engaged vs. disengaged; and ii) a novel sliding\nwindow method that predicts continuous levels of CE in real-time. We evaluated\nour framework on 8 subjects using an in-house Go/No-Go experiment that adapted\nits gameplay parameters to induce cognitive fatigue. The proposed CNN had an\naverage leave-one-out accuracy of 88.13\\%. The CE prediction correlated well\nwith a commonly used behavioral metric based on self-reports taken every 5\nminutes ($\\rho$=0.93). Our results objectify CE in real-time and pave the way\nfor using CE as a rehabilitation parameter for tailoring robotic therapy to\neach patient's needs and skills.\n", "versions": [{"version": "v1", "created": "Tue, 18 Feb 2020 13:13:39 GMT"}, {"version": "v2", "created": "Wed, 19 Feb 2020 16:59:22 GMT"}], "update_date": "2020-02-20", "authors_parsed": [["Kumar", "Neelesh", ""], ["Michmizos", "Konstantinos P.", ""]]}, {"id": "2002.07655", "submitter": "Johannes Kleiner", "authors": "Johannes Kleiner and Sean Tull", "title": "The Mathematical Structure of Integrated Information Theory", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "q-bio.NC cs.AI cs.IT math.IT quant-ph", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Integrated Information Theory is one of the leading models of consciousness.\nIt aims to describe both the quality and quantity of the conscious experience\nof a physical system, such as the brain, in a particular state. In this\ncontribution, we propound the mathematical structure of the theory, separating\nthe essentials from auxiliary formal tools. We provide a definition of a\ngeneralized IIT which has IIT 3.0 of Tononi et. al., as well as the Quantum IIT\nintroduced by Zanardi et. al. as special cases. This provides an axiomatic\ndefinition of the theory which may serve as the starting point for future\nformal investigations and as an introduction suitable for researchers with a\nformal background.\n", "versions": [{"version": "v1", "created": "Tue, 18 Feb 2020 15:44:02 GMT"}], "update_date": "2020-02-19", "authors_parsed": [["Kleiner", "Johannes", ""], ["Tull", "Sean", ""]]}, {"id": "2002.07699", "submitter": "Bo Peng", "authors": "Bo Peng, Xiaohui Yao, Shannon L. Risacher, Andrew J. Saykin, Li Shen,\n  Xia Ning (for the Alzheimer's Disease Neuroimaging Initiative)", "title": "Cognitive Biomarker Prioritization in Alzheimer's Disease using Brain\n  Morphometric Data", "comments": "This paper has been accepted by BMC MIDM", "journal-ref": null, "doi": null, "report-no": null, "categories": "q-bio.QM cs.LG eess.IV q-bio.NC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Background:Cognitive assessments represent the most common clinical routine\nfor the diagnosis of Alzheimer's Disease (AD). Given a large number of\ncognitive assessment tools and time-limited office visits, it is important to\ndetermine a proper set of cognitive tests for different subjects. Most current\nstudies create guidelines of cognitive test selection for a targeted\npopulation, but they are not customized for each individual subject. In this\nmanuscript, we develop a machine learning paradigm enabling personalized\ncognitive assessments prioritization. Method: We adapt a newly developed\nlearning-to-rank approach PLTR to implement our paradigm. This method learns\nthe latent scoring function that pushes the most effective cognitive\nassessments onto the top of the prioritization list. We also extend PLTR to\nbetter separate the most effective cognitive assessments and the less effective\nones. Results: Our empirical study on the ADNI data shows that the proposed\nparadigm outperforms the state-of-the-art baselines on identifying and\nprioritizing individual-specific cognitive biomarkers. We conduct experiments\nin cross validation and level-out validation settings. In the two settings, our\nparadigm significantly outperforms the best baselines with improvement as much\nas 22.1% and 19.7%, respectively, on prioritizing cognitive features.\nConclusions: The proposed paradigm achieves superior performance on\nprioritizing cognitive biomarkers. The cognitive biomarkers prioritized on top\nhave great potentials to facilitate personalized diagnosis, disease subtyping,\nand ultimately precision medicine in AD.\n", "versions": [{"version": "v1", "created": "Tue, 18 Feb 2020 16:25:11 GMT"}, {"version": "v2", "created": "Tue, 3 Nov 2020 15:26:49 GMT"}, {"version": "v3", "created": "Mon, 9 Nov 2020 13:26:14 GMT"}, {"version": "v4", "created": "Thu, 12 Nov 2020 15:26:14 GMT"}, {"version": "v5", "created": "Fri, 13 Nov 2020 01:49:34 GMT"}], "update_date": "2020-11-16", "authors_parsed": [["Peng", "Bo", "", "for the Alzheimer's Disease Neuroimaging Initiative"], ["Yao", "Xiaohui", "", "for the Alzheimer's Disease Neuroimaging Initiative"], ["Risacher", "Shannon L.", "", "for the Alzheimer's Disease Neuroimaging Initiative"], ["Saykin", "Andrew J.", "", "for the Alzheimer's Disease Neuroimaging Initiative"], ["Shen", "Li", "", "for the Alzheimer's Disease Neuroimaging Initiative"], ["Ning", "Xia", "", "for the Alzheimer's Disease Neuroimaging Initiative"]]}, {"id": "2002.07716", "submitter": "Bartosz Jura", "authors": "Bartosz Jura", "title": "Synaptic clock as a neural substrate of consciousness", "comments": "15 pages, 4 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "q-bio.NC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this study the temporal aspect of consciousness is analyzed. We start from\nthe notion that while conscious experience seems to change constantly, yet for\nany contents of experience to be consciously perceived they must last for some\nnon-zero duration of time, which appears to constitute a certain conflict. We\nposit that, in terms of phenomenological analysis of consciousness, the\ntemporal aspect, and this apparent conflict in particular, is the most basic\nproperty, likely inherent to any conceivable form of consciousness. We then put\nforward arguments for the synaptic clock to be a content-specific neural\nsubstrate of consciousness, showing how it would correspond to this temporal\naspect. This proposal is considered in light of the information integration\ntheory of consciousness, and other theories that relate consciousness to\nprocesses of learning and memory. It is outlined how this can offer a concrete\nway of relating the properties of consciousness directly to the neural\nplasticity mechanisms of learning and memory. In this regard, we propose a\nviewpoint, in which an association between different contents of conscious\nexperience can be created in a form of relational memory only if they occur at\nthe very same moment of subjective time, with moments of subjective time having\ndifferent durations depending on the type of information processed,\nproportional to the time units of corresponding synaptic clocks, and being in\nprinciple different for different brain regions and nervous systems in\ndifferent animal species. Finally, using this viewpoint, we consider the two\nalternative views on the structure of consciousness, namely a static and a\ndynamic one, and argue in favor of the latter, proposing that consciousness can\nbe best understood if change is considered its only dimension.\n", "versions": [{"version": "v1", "created": "Tue, 18 Feb 2020 16:43:58 GMT"}], "update_date": "2020-02-19", "authors_parsed": [["Jura", "Bartosz", ""]]}, {"id": "2002.07874", "submitter": "Matthew Leming", "authors": "Matthew Leming, Juan Manuel G\\'orriz, John Suckling", "title": "Ensemble Deep Learning on Large, Mixed-Site fMRI Datasets in Autism and\n  Other Tasks", "comments": null, "journal-ref": null, "doi": "10.1142/S0129065720500124", "report-no": null, "categories": "q-bio.QM cs.LG eess.IV q-bio.NC stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Deep learning models for MRI classification face two recurring problems: they\nare typically limited by low sample size, and are abstracted by their own\ncomplexity (the \"black box problem\"). In this paper, we train a convolutional\nneural network (CNN) with the largest multi-source, functional MRI (fMRI)\nconnectomic dataset ever compiled, consisting of 43,858 datapoints. We apply\nthis model to a cross-sectional comparison of autism (ASD) vs typically\ndeveloping (TD) controls that has proved difficult to characterise with\ninferential statistics. To contextualise these findings, we additionally\nperform classifications of gender and task vs rest. Employing class-balancing\nto build a training set, we trained 3$\\times$300 modified CNNs in an ensemble\nmodel to classify fMRI connectivity matrices with overall AUROCs of 0.6774,\n0.7680, and 0.9222 for ASD vs TD, gender, and task vs rest, respectively.\nAdditionally, we aim to address the black box problem in this context using two\nvisualization methods. First, class activation maps show which functional\nconnections of the brain our models focus on when performing classification.\nSecond, by analyzing maximal activations of the hidden layers, we were also\nable to explore how the model organizes a large and mixed-centre dataset,\nfinding that it dedicates specific areas of its hidden layers to processing\ndifferent covariates of data (depending on the independent variable analyzed),\nand other areas to mix data from different sources. Our study finds that deep\nlearning models that distinguish ASD from TD controls focus broadly on temporal\nand cerebellar connections, with a particularly high focus on the right caudate\nnucleus and paracentral sulcus.\n", "versions": [{"version": "v1", "created": "Fri, 14 Feb 2020 17:28:16 GMT"}, {"version": "v2", "created": "Wed, 27 May 2020 16:31:37 GMT"}], "update_date": "2020-05-28", "authors_parsed": [["Leming", "Matthew", ""], ["G\u00f3rriz", "Juan Manuel", ""], ["Suckling", "John", ""]]}, {"id": "2002.08063", "submitter": "Benedetta Franceschiello Dr.", "authors": "Benedetta Franceschiello and Alexey Mashtakov and Giovanna Citti and\n  Alessandro Sarti", "title": "Geometrical Optical Illusion via Sub-Riemannian Geodesics in the\n  Roto-Translation Group", "comments": "26 pages, 10 figures", "journal-ref": "Differential Geometry and its Applications, 2019", "doi": "10.1016/j.difgeo.2019.03.007", "report-no": null, "categories": "q-bio.NC math.DG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a neuro-mathematical model for geometrical optical illusions\n(GOIs), a class of illusory phenomena that consists in a mismatch of\ngeometrical properties of the visual stimulus and its associated percept. They\ntake place in the visual areas V1/V2 whose functional architecture have been\nmodelled in previous works by Citti and Sarti as a Lie group equipped with a\nsub-Riemannian (SR) metric. Here we extend their model proposing that the\nmetric responsible for the cortical connectivity is modulated by the modelled\nneuro-physiological response of simple cells to the visual stimulus, hence\nproviding a more biologically plausible model that takes into account a\npresence of visual stimulus. Illusory contours in our model are described as\ngeodesics in the new metric. The model is confirmed by numerical simulations,\nwhere we compute the geodesics via SR-Fast Marching.\n", "versions": [{"version": "v1", "created": "Wed, 19 Feb 2020 08:55:06 GMT"}], "update_date": "2020-02-20", "authors_parsed": [["Franceschiello", "Benedetta", ""], ["Mashtakov", "Alexey", ""], ["Citti", "Giovanna", ""], ["Sarti", "Alessandro", ""]]}, {"id": "2002.08104", "submitter": "Aleksandra Nowak", "authors": "Romuald A. Janik and Aleksandra Nowak", "title": "Analyzing Neural Networks Based on Random Graphs", "comments": "Added new results and discussion", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV q-bio.NC stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We perform a massive evaluation of neural networks with architectures\ncorresponding to random graphs of various types. We investigate various\nstructural and numerical properties of the graphs in relation to neural network\ntest accuracy. We find that none of the classical numerical graph invariants by\nitself allows to single out the best networks. Consequently, we introduce a new\nnumerical graph characteristic that selects a set of quasi-1-dimensional\ngraphs, which are a majority among the best performing networks. We also find\nthat networks with primarily short-range connections perform better than\nnetworks which allow for many long-range connections. Moreover, many resolution\nreducing pathways are beneficial. We provide a dataset of 1020 graphs and the\ntest accuracies of their corresponding neural networks at\nhttps://github.com/rmldj/random-graph-nn-paper\n", "versions": [{"version": "v1", "created": "Wed, 19 Feb 2020 11:04:49 GMT"}, {"version": "v2", "created": "Tue, 14 Jul 2020 17:13:59 GMT"}, {"version": "v3", "created": "Wed, 2 Dec 2020 11:29:36 GMT"}], "update_date": "2020-12-03", "authors_parsed": [["Janik", "Romuald A.", ""], ["Nowak", "Aleksandra", ""]]}, {"id": "2002.08354", "submitter": "Konstantinos Michmizos", "authors": "Neelesh Kumar and Konstantinos P. Michmizos", "title": "Deep Learning of Movement Intent and Reaction Time for EEG-informed\n  Adaptation of Rehabilitation Robots", "comments": "6 pages, 3 figures, 3 tables. arXiv admin note: text overlap with\n  arXiv:2002.07541", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.RO cs.HC cs.NE q-bio.NC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Mounting evidence suggests that adaptation is a crucial mechanism for\nrehabilitation robots in promoting motor learning. Yet, it is commonly based on\nrobot-derived movement kinematics, which is a rather subjective measurement of\nperformance, especially in the presence of a sensorimotor impairment. Here, we\npropose a deep convolutional neural network (CNN) that uses\nelectroencephalography (EEG) as an objective measurement of two kinematics\ncomponents that are typically used to assess motor learning and thereby\nadaptation: i) the intent to initiate a goal-directed movement, and ii) the\nreaction time (RT) of that movement. We evaluated our CNN on data acquired from\nan in-house experiment where 13 subjects moved a rehabilitation robotic arm in\nfour directions on a plane, in response to visual stimuli. Our CNN achieved\naverage test accuracies of 80.08% and 79.82% in a binary classification of the\nintent (intent vs. no intent) and RT (slow vs. fast), respectively. Our results\ndemonstrate how individual movement components implicated in distinct types of\nmotor learning can be predicted from synchronized EEG data acquired before the\nstart of the movement. Our approach can, therefore, inform robotic adaptation\nin real-time and has the potential to further improve one's ability to perform\nthe rehabilitation task.\n", "versions": [{"version": "v1", "created": "Tue, 18 Feb 2020 13:20:46 GMT"}], "update_date": "2020-02-20", "authors_parsed": [["Kumar", "Neelesh", ""], ["Michmizos", "Konstantinos P.", ""]]}, {"id": "2002.08468", "submitter": "Jae-Geun Yoon", "authors": "Jae-Geun Yoon and Minji Lee", "title": "Effective Correlates of Motor Imagery Performance based on Default Mode\n  Network in Resting-State", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "q-bio.NC cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Motor imagery based brain-computer interfaces (MI-BCIs) allow the control of\ndevices and communication by imagining different muscle movements. However,\nmost studies have reported a problem of \"BCI-illiteracy\" that does not have\nenough performance to use MI-BCI. Therefore, understanding subjects with poor\nperformance and finding the cause of performance variation is still an\nimportant challenge. In this study, we proposed predictors of MI performance\nusing effective connectivity in resting-state EEG. As a result, the high and\nlow MI performance groups had a significant difference as 23% MI performance\ndifference. We also found that connection from right lateral parietal to left\nlateral parietal in resting-state EEG was correlated significantly with MI\nperformance (r = -0.37). These findings could help to understand BCI-illiteracy\nand to consider alternatives that are appropriate for the subject.\n", "versions": [{"version": "v1", "created": "Tue, 11 Feb 2020 04:22:05 GMT"}], "update_date": "2020-02-21", "authors_parsed": [["Yoon", "Jae-Geun", ""], ["Lee", "Minji", ""]]}, {"id": "2002.08469", "submitter": "Young-Seok Kweon", "authors": "Young-Seok Kweon, Minji Lee, Dong-Ok Won, Kwang-Suk Seo", "title": "Prediction of Individual Propofol Requirements based on Preoperative EEG\n  Signals", "comments": "5 pages, 1 figure, 1 table", "journal-ref": null, "doi": null, "report-no": null, "categories": "q-bio.NC eess.SP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The patient must be given an adequate amount of propofol for safe surgery\nsince overcapacity and low capacity cause accidents. However, the sensitivity\nof propofol varies from patient to patient, making it very difficult to\ndetermine the propofol requirements for anesthesia. This paper aims to propose\na neurophysiological predictor of propofol requirements based on the\npreoperative electroencephalogram (EEG). We exploited the canonical correlation\nanalysis that infers the amount of information on the propofol requirements.\nThe results showed that the preoperative EEG included the factor that could\nexplain the propofol requirements. Specifically, the frontal and posterior\nregions had crucial information on the propofol requirements. Moreover, there\nwas a significantly different power in the frontal and posterior regions\nbetween baseline and unconsciousness periods, unlike the alpha power in the\ncentral region. These findings showed the potential that preoperative EEG could\npredict the propofol requirements.\n", "versions": [{"version": "v1", "created": "Tue, 11 Feb 2020 04:05:59 GMT"}], "update_date": "2020-02-24", "authors_parsed": [["Kweon", "Young-Seok", ""], ["Lee", "Minji", ""], ["Won", "Dong-Ok", ""], ["Seo", "Kwang-Suk", ""]]}, {"id": "2002.08470", "submitter": "Kevin Scharp", "authors": "Alison Duncan Kerr and Kevin Scharp", "title": "The Information in Emotion Communication", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "q-bio.NC cs.LG q-bio.PE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  How much information is transmitted when animals use emotions to communicate?\nIt is clear that emotions are used as communication systems in humans and other\nspecies. The quantitative theory of emotion information presented here is based\non Shannon's mathematical theory of information in communication systems. The\ntheory explains myriad aspects of emotion communication and offers dozens of\nnew directions for research. It is superior to the \"contagion\" theory of\nemotion spreading, which is currently dominant. One important application of\nthe information theory of emotion communication is that it permits the\ndevelopment of emotion security systems for social networks to guard against\nthe widespread emotion manipulation we see online today.\n", "versions": [{"version": "v1", "created": "Fri, 14 Feb 2020 22:42:26 GMT"}], "update_date": "2020-02-21", "authors_parsed": [["Kerr", "Alison Duncan", ""], ["Scharp", "Kevin", ""]]}, {"id": "2002.08813", "submitter": "Alain Destexhe", "authors": "Alain Destexhe and Jonathan D. Touboul", "title": "Is there sufficient evidence for criticality in cortical systems?", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "q-bio.NC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Many studies have found evidence that the brain operates at a critical point,\na processus known as self-organized criticality. A recent paper found\nremarkable scalings suggestive of criticality in systems as different as neural\ncultures, anesthetized or awake brains. We point out here that the diversity of\nthese states would question any claimed role of criticality in information\nprocessing. Furthermore, we show that two non-critical systems pass all the\ntests for criticality, a control that was not provided in the original article.\nWe conclude that such false positives demonstrate that the presence of\ncriticality in the brain is still not proven and that we need better methods\nthat scaling analyses.\n", "versions": [{"version": "v1", "created": "Thu, 20 Feb 2020 15:47:41 GMT"}, {"version": "v2", "created": "Fri, 10 Jul 2020 19:26:44 GMT"}, {"version": "v3", "created": "Mon, 28 Dec 2020 16:05:58 GMT"}], "update_date": "2020-12-29", "authors_parsed": [["Destexhe", "Alain", ""], ["Touboul", "Jonathan D.", ""]]}, {"id": "2002.08891", "submitter": "Maxwell Bertolero Dr", "authors": "Maxwell A Bertolero and Danielle S Bassett", "title": "Deep Neural Networks Carve the Brain at its Joints", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "q-bio.NC physics.data-an q-bio.QM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  How an individual's unique brain connectivity determines that individual's\ncognition, behavior, and risk for pathology is a fundamental question in basic\nand clinical neuroscience. In seeking answers, many have turned to machine\nlearning, with some noting the particular promise of deep neural networks in\nmodelling complex non-linear functions. However, it is not clear that complex\nfunctions actually exist between brain connectivity and behavior, and thus if\ndeep neural networks necessarily outperform simpler linear models, or if their\nresults would be interpretable. Here we show that, across 52 subject measures\nof cognition and behavior, deep neural networks fit to each brain region's\nconnectivity outperform linear regression, particularly for the brain's\nconnector hubs--regions with diverse brain connectivity--whereas the two\napproaches perform similarly when fit to brain systems. Critically, averaging\ndeep neural network predictions across brain regions results in the most\naccurate predictions, demonstrating the ability of deep neural networks to\neasily model the various functions that exists between regional brain\nconnectivity and behavior, carving the brain at its joints. Finally, we shine\nlight into the black box of deep neural networks using multislice network\nmodels. We determined that the relationship between connector hubs and behavior\nis best captured by modular deep neural networks. Our results demonstrate that\nboth simple and complex relationships exist between brain connectivity and\nbehavior, and that deep neural networks can fit both. Moreover, deep neural\nnetworks are particularly powerful when they are first fit to the various\nfunctions of a system independently and then combined. Finally, deep neural\nnetworks are interpretable when their architectures are structurally\ncharacterized using multislice network models.\n", "versions": [{"version": "v1", "created": "Thu, 20 Feb 2020 17:37:53 GMT"}, {"version": "v2", "created": "Wed, 9 Sep 2020 15:09:51 GMT"}], "update_date": "2020-09-10", "authors_parsed": [["Bertolero", "Maxwell A", ""], ["Bassett", "Danielle S", ""]]}, {"id": "2002.08975", "submitter": "Aria Wang", "authors": "Aria Yuan Wang and Michael J. Tarr", "title": "Learning Intermediate Features of Object Affordances with a\n  Convolutional Neural Network", "comments": "Published on 2018 Conference on Cognitive Computational Neuroscience.\n  See <https://ccneuro.org/2018/Papers/ViewPapers.asp?PaperNum=1134>", "journal-ref": null, "doi": "10.32470/CCN.2018.1134-0", "report-no": null, "categories": "q-bio.NC cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Our ability to interact with the world around us relies on being able to\ninfer what actions objects afford -- often referred to as affordances. The\nneural mechanisms of object-action associations are realized in the visuomotor\npathway where information about both visual properties and actions is\nintegrated into common representations. However, explicating these mechanisms\nis particularly challenging in the case of affordances because there is hardly\nany one-to-one mapping between visual features and inferred actions. To better\nunderstand the nature of affordances, we trained a deep convolutional neural\nnetwork (CNN) to recognize affordances from images and to learn the underlying\nfeatures or the dimensionality of affordances. Such features form an underlying\ncompositional structure for the general representation of affordances which can\nthen be tested against human neural data. We view this representational\nanalysis as the first step towards a more formal account of how humans perceive\nand interact with the environment.\n", "versions": [{"version": "v1", "created": "Thu, 20 Feb 2020 19:04:40 GMT"}], "update_date": "2020-02-24", "authors_parsed": [["Wang", "Aria Yuan", ""], ["Tarr", "Michael J.", ""]]}, {"id": "2002.09034", "submitter": "Narciso L\\'opez-L\\'opez", "authors": "Narciso L\\'opez-L\\'opez, Andrea V\\'azquez, Cyril Poupon,\n  Jean-Fran\\c{c}ois Mangin, Pamela Guevara", "title": "Cortical surface parcellation based on intra-subject white matter fiber\n  clustering", "comments": "This research has received funding from the European Union's Horizon\n  2020 research and innovation programme under the Marie Sklodowska-Curie\n  Actions H2020-MSCA-RISE-2015 BIRDS GA No. 690941, CONICYT PFCHA/ DOCTORADO\n  NACIONAL/2016-21160342, CONICYT FONDECYT 1190701, CONICYT PIA/Anillo de\n  Investigaci\\'on en Ciencia y Tecnolog\\'ia ACT172121 and CONICYT Basal Center\n  FB0008", "journal-ref": null, "doi": "10.1109/CHILECON47746.2019.8988066", "report-no": null, "categories": "q-bio.NC cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a hybrid method that performs the complete parcellation of the\ncerebral cortex of an individual, based on the connectivity information of the\nwhite matter fibers from a whole-brain tractography dataset. The method\nconsists of five steps, first intra-subject clustering is performed on the\nbrain tractography. The fibers that make up each cluster are then intersected\nwith the cortical mesh and then filtered to discard outliers. In addition, the\nmethod resolves the overlapping between the different intersection regions\n(sub-parcels) throughout the cortex efficiently. Finally, a post-processing is\ndone to achieve more uniform sub-parcels. The output is the complete labeling\nof cortical mesh vertices, representing the different cortex sub-parcels, with\nstrong connections to other sub-parcels. We evaluated our method with measures\nof brain connectivity such as functional segregation (clustering coefficient),\nfunctional integration (characteristic path length) and small-world. Results in\nfive subjects from ARCHI database show a good individual cortical parcellation\nfor each one, composed of about 200 subparcels per hemisphere and complying\nwith these connectivity measures.\n", "versions": [{"version": "v1", "created": "Sun, 16 Feb 2020 19:14:39 GMT"}], "update_date": "2020-02-24", "authors_parsed": [["L\u00f3pez-L\u00f3pez", "Narciso", ""], ["V\u00e1zquez", "Andrea", ""], ["Poupon", "Cyril", ""], ["Mangin", "Jean-Fran\u00e7ois", ""], ["Guevara", "Pamela", ""]]}, {"id": "2002.09035", "submitter": "Abdelrahman Zayed", "authors": "Abdelrahman Zayed, Yasser Iturria-Medina, Arno Villringer, Bernhard\n  Sehm and Christopher J. Steele", "title": "Rapid Quantification of White Matter Disconnection in the Human Brain", "comments": "2020 42nd Annual International Conference of the IEEE Engineering in\n  Medicine and Biology Society (EMBC)", "journal-ref": null, "doi": null, "report-no": null, "categories": "q-bio.NC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  With an estimated five million new stroke survivors every year and a rapidly\naging population suffering from hyperintensities and diseases of presumed\nvascular origin that affect white matter and contribute to cognitive decline,\nit is critical that we understand the impact of white matter damage on brain\nstructure and behavior. Current techniques for assessing the impact of lesions\nconsider only location, type, and extent, while ignoring how the affected\nregion was connected to the rest of the brain. Regional brain function is a\nproduct of both local structure and its connectivity. Therefore, obtaining a\nmap of white matter disconnection is a crucial step that could help us predict\nthe behavioral deficits that patients exhibit. In the present work, we\nintroduce a new practical method for computing lesion-based white matter\ndisconnection maps that require only moderate computational resources. We\nachieve this by creating diffusion tractography models of the brains of healthy\nadults and assessing the connectivity between small regions. We then interrupt\nthese connectivity models by projecting patients' lesions into them to compute\npredicted white matter disconnection. A quantified disconnection map can be\ncomputed for an individual patient in approximately 35 seconds using a single\ncore CPU-based computation. In comparison, a similar quantification performed\nwith other tools provided by MRtrix3 takes 5.47 minutes.\n", "versions": [{"version": "v1", "created": "Mon, 17 Feb 2020 01:57:21 GMT"}, {"version": "v2", "created": "Wed, 20 May 2020 02:19:47 GMT"}], "update_date": "2020-05-21", "authors_parsed": [["Zayed", "Abdelrahman", ""], ["Iturria-Medina", "Yasser", ""], ["Villringer", "Arno", ""], ["Sehm", "Bernhard", ""], ["Steele", "Christopher J.", ""]]}, {"id": "2002.09117", "submitter": "Mauricio Girardi-Schappo", "authors": "Mauricio Girardi-Schappo, Ludmila Brochini, Ariadne A. Costa, Tawan T.\n  A. Carvalho, Osame Kinouchi", "title": "Synaptic balance due to homeostatically self-organized quasicritical\n  dynamics", "comments": "18 pages, 4 figures", "journal-ref": "Phys. Rev. Research 2, 012042(R) (2020)", "doi": "10.1103/PhysRevResearch.2.012042", "report-no": null, "categories": "nlin.AO cond-mat.dis-nn cond-mat.stat-mech q-bio.NC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recent experiments suggested that homeostatic regulation of synaptic balance\nleads the visual system to recover and maintain a regime of power-law\navalanches. Here we study an excitatory/inhibitory (E/I) mean-field neuronal\nnetwork that has a critical point with power-law avalanches and synaptic\nbalance. When short term depression in inhibitory synapses and firing threshold\nadaptation are added, the system hovers around the critical point. This\nhomeostatically self-organized quasi-critical (SOqC) dynamics generates E/I\nsynaptic current cancellation in fast time scales, causing fluctuation-driven\nasynchronous-irregular (AI) firing. We present the full phase diagram of the\nmodel without adaptation varying external input versus synaptic coupling. This\nsystem has a rich dynamical repertoire of spiking patterns: synchronous regular\n(SR), asynchronous regular (AR), synchronous irregular (SI), slow oscillations\n(SO) and AI. It also presents dynamic balance of synaptic currents, since\ninhibitory currents try and compensate excitatory currents over time, resulting\nin both of them scaling linearly with external input. Our model thus unifies\ntwo different perspectives on cortical spontaneous activity: both critical\navalanches and fluctuation-driven AI firing arise from SOqC homeostatic\nadaptation, and are indeed two sides of the same coin.\n", "versions": [{"version": "v1", "created": "Fri, 21 Feb 2020 03:58:44 GMT"}], "update_date": "2020-02-24", "authors_parsed": [["Girardi-Schappo", "Mauricio", ""], ["Brochini", "Ludmila", ""], ["Costa", "Ariadne A.", ""], ["Carvalho", "Tawan T. A.", ""], ["Kinouchi", "Osame", ""]]}, {"id": "2002.09283", "submitter": "Bin Hu", "authors": "Hanshu Cai, Yiwen Gao, Shuting Sun, Na Li, Fuze Tian, Han Xiao,\n  Jianxiu Li, Zhengwu Yang, Xiaowei Li, Qinglin Zhao, Zhenyu Liu, Zhijun Yao,\n  Minqiang Yang, Hong Peng, Jing Zhu, Xiaowei Zhang, Guoping Gao, Fang Zheng,\n  Rui Li, Zhihua Guo, Rong Ma, Jing Yang, Lan Zhang, Xiping Hu, Yumin Li, Bin\n  Hu", "title": "MODMA dataset: a Multi-modal Open Dataset for Mental-disorder Analysis", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DL cs.LG q-bio.NC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  According to the World Health Organization, the number of mental disorder\npatients, especially depression patients, has grown rapidly and become a\nleading contributor to the global burden of disease. However, the present\ncommon practice of depression diagnosis is based on interviews and clinical\nscales carried out by doctors, which is not only labor-consuming but also\ntime-consuming. One important reason is due to the lack of physiological\nindicators for mental disorders. With the rising of tools such as data mining\nand artificial intelligence, using physiological data to explore new possible\nphysiological indicators of mental disorder and creating new applications for\nmental disorder diagnosis has become a new research hot topic. However, good\nquality physiological data for mental disorder patients are hard to acquire. We\npresent a multi-modal open dataset for mental-disorder analysis. The dataset\nincludes EEG and audio data from clinically depressed patients and matching\nnormal controls. All our patients were carefully diagnosed and selected by\nprofessional psychiatrists in hospitals. The EEG dataset includes not only data\ncollected using traditional 128-electrodes mounted elastic cap, but also a\nnovel wearable 3-electrode EEG collector for pervasive applications. The\n128-electrodes EEG signals of 53 subjects were recorded as both in resting\nstate and under stimulation; the 3-electrode EEG signals of 55 subjects were\nrecorded in resting state; the audio data of 52 subjects were recorded during\ninterviewing, reading, and picture description. We encourage other researchers\nin the field to use it for testing their methods of mental-disorder analysis.\n", "versions": [{"version": "v1", "created": "Thu, 20 Feb 2020 09:40:39 GMT"}, {"version": "v2", "created": "Wed, 4 Mar 2020 02:27:08 GMT"}, {"version": "v3", "created": "Thu, 5 Mar 2020 03:43:31 GMT"}], "update_date": "2020-03-06", "authors_parsed": [["Cai", "Hanshu", ""], ["Gao", "Yiwen", ""], ["Sun", "Shuting", ""], ["Li", "Na", ""], ["Tian", "Fuze", ""], ["Xiao", "Han", ""], ["Li", "Jianxiu", ""], ["Yang", "Zhengwu", ""], ["Li", "Xiaowei", ""], ["Zhao", "Qinglin", ""], ["Liu", "Zhenyu", ""], ["Yao", "Zhijun", ""], ["Yang", "Minqiang", ""], ["Peng", "Hong", ""], ["Zhu", "Jing", ""], ["Zhang", "Xiaowei", ""], ["Gao", "Guoping", ""], ["Zheng", "Fang", ""], ["Li", "Rui", ""], ["Guo", "Zhihua", ""], ["Ma", "Rong", ""], ["Yang", "Jing", ""], ["Zhang", "Lan", ""], ["Hu", "Xiping", ""], ["Li", "Yumin", ""], ["Hu", "Bin", ""]]}, {"id": "2002.09626", "submitter": "Thiago B. Burghi", "authors": "Thiago B. Burghi, Maarten Schoukens, Rodolphe Sepulchre", "title": "Feedback Identification of conductance-based models", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.SY cs.SY q-bio.NC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper applies the classical prediction error method (PEM) to the\nestimation of nonlinear discrete-time models of neuronal systems subject to\ninput-additive noise. While the nonlinear system exhibits excitability,\nbifurcations, and limit-cycle oscillations, we prove consistency of the\nparameter estimation procedure under output feedback. Hence, this paper\nprovides a rigorous framework for the application of conventional nonlinear\nsystem identification methods to discrete-time stochastic neuronal systems. The\nmain result exploits the elementary property that conductance-based models of\nneurons have an exponentially contracting inverse dynamics. This property is\nimplied by the voltage-clamp experiment, which has been the fundamental\nmodeling experiment of neurons ever since the pioneering work of Hodgkin and\nHuxley.\n", "versions": [{"version": "v1", "created": "Sat, 22 Feb 2020 05:00:18 GMT"}, {"version": "v2", "created": "Wed, 29 Jul 2020 08:40:41 GMT"}, {"version": "v3", "created": "Wed, 16 Dec 2020 13:37:46 GMT"}], "update_date": "2020-12-18", "authors_parsed": [["Burghi", "Thiago B.", ""], ["Schoukens", "Maarten", ""], ["Sepulchre", "Rodolphe", ""]]}, {"id": "2002.10034", "submitter": "Sema Candemir", "authors": "Sema Candemir, Xuan V. Nguyen, Luciano M. Prevedello, Matthew T.\n  Bigelow, Richard D.White, Barbaros S. Erdal (for the Alzheimer's Disease\n  Neuroimaging Initiative)", "title": "Predicting Rate of Cognitive Decline at Baseline Using a Deep Neural\n  Network with Multidata Analysis", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "q-bio.QM cs.LG eess.IV q-bio.NC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Purpose: This study investigates whether a machine-learning-based system can\npredict the rate of cognitive decline in mildly cognitively impaired patients\nby processing only the clinical and imaging data collected at the initial\nvisit.\n  Approach: We built a predictive model based on a supervised hybrid neural\nnetwork utilizing a 3-Dimensional Convolutional Neural Network to perform\nvolume analysis of Magnetic Resonance Imaging and integration of non-imaging\nclinical data at the fully connected layer of the architecture. The experiments\nare conducted on the Alzheimers Disease Neuroimaging Initiative dataset.\n  Results: Experimental results confirm that there is a correlation between\ncognitive decline and the data obtained at the first visit. The system achieved\nan area under the receiver operator curve (AUC) of 0.70 for cognitive decline\nclass prediction.\n  Conclusion: To our knowledge, this is the first study that predicts slowly\ndeteriorating/stable or rapidly deteriorating classes by processing routinely\ncollected baseline clinical and demographic data (Baseline MRI, Baseline MMSE,\nScalar Volumetric data, Age, Gender, Education, Ethnicity, and Race). The\ntraining data is built based on MMSE-rate values. Unlike the studies in the\nliterature that focus on predicting Mild Cognitive Impairment-to-Alzheimer`s\ndisease conversion and disease classification, we approach the problem as an\nearly prediction of cognitive decline rate in MCI patients.\n", "versions": [{"version": "v1", "created": "Mon, 24 Feb 2020 01:39:17 GMT"}, {"version": "v2", "created": "Mon, 8 Jun 2020 05:40:42 GMT"}, {"version": "v3", "created": "Mon, 5 Oct 2020 23:14:23 GMT"}], "update_date": "2020-10-07", "authors_parsed": [["Candemir", "Sema", "", "for the Alzheimer's Disease\n  Neuroimaging Initiative"], ["Nguyen", "Xuan V.", "", "for the Alzheimer's Disease\n  Neuroimaging Initiative"], ["Prevedello", "Luciano M.", "", "for the Alzheimer's Disease\n  Neuroimaging Initiative"], ["Bigelow", "Matthew T.", "", "for the Alzheimer's Disease\n  Neuroimaging Initiative"], ["White", "Richard D.", "", "for the Alzheimer's Disease\n  Neuroimaging Initiative"], ["Erdal", "Barbaros S.", "", "for the Alzheimer's Disease\n  Neuroimaging Initiative"]]}, {"id": "2002.10284", "submitter": "Victor Swift", "authors": "Victor Swift", "title": "Word Embeddings Inherently Recover the Conceptual Organization of the\n  Human Mind", "comments": "12 pages, 4 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL q-bio.NC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Machine learning is a means to uncover deep patterns from rich sources of\ndata. Here, we find that machine learning can recover the conceptual\norganization of the human mind when applied to the natural language use of\nmillions of people. Utilizing text from billions of webpages, we recover most\nof the concepts contained in English, Dutch, and Japanese, as represented in\nlarge scale Word Association networks. Our results justify machine learning as\na means to probe the human mind, at a depth and scale that has been\nunattainable using self-report and observational methods. Beyond direct\npsychological applications, our methods may prove useful for projects concerned\nwith defining, assessing, relating, or uncovering concepts in any scientific\nfield.\n", "versions": [{"version": "v1", "created": "Thu, 6 Feb 2020 23:45:50 GMT"}], "update_date": "2020-02-25", "authors_parsed": [["Swift", "Victor", ""]]}, {"id": "2002.10378", "submitter": "Shanshan Qin", "authors": "Shanshan Qin, Nayantara Mudur and Cengiz Pehlevan", "title": "Contrastive Similarity Matching for Supervised Learning", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG q-bio.NC stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a novel biologically-plausible solution to the credit assignment\nproblem motivated by observations in the ventral visual pathway and trained\ndeep neural networks. In both, representations of objects in the same category\nbecome progressively more similar, while objects belonging to different\ncategories become less similar. We use this observation to motivate a\nlayer-specific learning goal in a deep network: each layer aims to learn a\nrepresentational similarity matrix that interpolates between previous and later\nlayers. We formulate this idea using a contrastive similarity matching\nobjective function and derive from it deep neural networks with feedforward,\nlateral, and feedback connections, and neurons that exhibit\nbiologically-plausible Hebbian and anti-Hebbian plasticity. Contrastive\nsimilarity matching can be interpreted as an energy-based learning algorithm,\nbut with significant differences from others in how a contrastive function is\nconstructed.\n", "versions": [{"version": "v1", "created": "Mon, 24 Feb 2020 17:10:21 GMT"}, {"version": "v2", "created": "Tue, 10 Mar 2020 02:48:47 GMT"}, {"version": "v3", "created": "Mon, 3 Aug 2020 16:56:36 GMT"}, {"version": "v4", "created": "Mon, 17 Aug 2020 14:21:49 GMT"}, {"version": "v5", "created": "Sun, 6 Dec 2020 02:09:28 GMT"}], "update_date": "2020-12-08", "authors_parsed": [["Qin", "Shanshan", ""], ["Mudur", "Nayantara", ""], ["Pehlevan", "Cengiz", ""]]}, {"id": "2002.10568", "submitter": "Lilianne Mujica-Parodi", "authors": "Anar Amgalan, Patrick Taylor, Lilianne R. Mujica-Parodi, and Hava T.\n  Siegelmann", "title": "Unique Scales Preserve Self-Similar Integrate-and-Fire Functionality of\n  Neuronal Clusters", "comments": "20 pages, 3 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "q-bio.NC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Identifying the brain's neuronal cluster size to be presented as nodes in a\nnetwork computation is critical to both neuroscience and artificial\nintelligence, as these define the cognitive blocks required for building\nintelligent computation. Experiments support many forms and sizes of neural\nclustering, while neural mass models (NMM) assume scale-invariant\nfunctionality. Here, we use computational simulations with brain-derived fMRI\nnetwork to show that not only brain network stays structurally self-similar\ncontinuously across scales, but also neuron-like signal integration\nfunctionality is preserved at particular scales. As such, we propose a\ncoarse-graining of network of neurons to ensemble-nodes, with multiple spikes\nmaking up its ensemble-spike, and time re-scaling factor defining its\nensemble-time step. The fractal-like spatiotemporal structure and function that\nemerge permit strategic choice in bridging across experimental scales for\ncomputational modeling, while also suggesting regulatory constraints on\ndevelopmental and/or evolutionary \"growth spurts\" in brain size, as per\npunctuated equilibrium theories in evolutionary biology.\n", "versions": [{"version": "v1", "created": "Mon, 24 Feb 2020 22:11:34 GMT"}], "update_date": "2020-02-26", "authors_parsed": [["Amgalan", "Anar", ""], ["Taylor", "Patrick", ""], ["Mujica-Parodi", "Lilianne R.", ""], ["Siegelmann", "Hava T.", ""]]}, {"id": "2002.10804", "submitter": "Yangsong Zhang", "authors": "Cunbo Li, Peiyang Li, Yangsong Zhang, Ning Li, Yajing Si, Fali Li,\n  Dezhong Yao and Peng Xu", "title": "Hierarchical emotion-recognition framework based on discriminative brain\n  neural network topology and ensemble co-decision strategy", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "q-bio.NC eess.SP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Brain neural networks characterize various information propagation patterns\nfor different emotional states. However, the statistical features based on\ntraditional graph theory may ignore the spacial network difference. To reveal\nthese inherent spatial features and increase the stability of emotional\nrecognition, we proposed a hierarchical framework that can perform the multiple\nemotion recognitions with the multiple emotion-related spatial network topology\npatterns (MESNP) by combining a supervised learning with ensemble co-decision\nstrategy. To evaluate the performance of our proposed MESNP approach, we\nconduct both off-line and simulated on-line experiments with two public\ndatasets i.e., MAHNOB and DEAP. The experiment results demonstrated that MESNP\ncan significantly enhance the classification performance for the multiple\nemotions. The highest accuracies of off-line experiments for MAHNOB-HCI and\nDEAP achieved 99.93% (3 classes) and 83.66% (4 classes), respectively. For\nsimulated on-line experiments, we also obtained the best classification\naccuracies with 100% (3 classes) for MAHNOB and 99.22% (4 classes) for DEAP by\nproposed MESNP. These results further proved the efficiency of MESNP for\nstructured feature extraction in mult-classification emotional task.\n", "versions": [{"version": "v1", "created": "Tue, 25 Feb 2020 11:47:53 GMT"}], "update_date": "2020-02-26", "authors_parsed": [["Li", "Cunbo", ""], ["Li", "Peiyang", ""], ["Zhang", "Yangsong", ""], ["Li", "Ning", ""], ["Si", "Yajing", ""], ["Li", "Fali", ""], ["Yao", "Dezhong", ""], ["Xu", "Peng", ""]]}, {"id": "2002.10882", "submitter": "James Tee", "authors": "James Tee and Desmond P. Taylor", "title": "A Quantized Representation of Intertemporal Choice in the Brain", "comments": "9 pages, 19 figures. arXiv admin note: substantial text overlap with\n  arXiv:1805.01631", "journal-ref": null, "doi": null, "report-no": null, "categories": "q-bio.NC stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Value [4][5] is typically modeled using a continuous representation (i.e., a\nReal number). A discrete representation of value has recently been postulated\n[6]. A quantized representation of probability in the brain was also posited\nand supported by experimental data [7]. Value and probability are inter-related\nvia Prospect Theory [4][5]. In this paper, we hypothesize that intertemporal\nchoices may also be quantized. For example, people may treat (or discount) 16\ndays indifferently to 17 days. To test this, we analyzed an intertemporal task\nby using 2 novel models: quantized hyperbolic discounting, and quantized\nexponential discounting. Our work here is a re-examination of the behavioral\ndata previously collected for an fMRI study [8]. Both quantized hyperbolic and\nquantized exponential models were compared using AIC and BIC tests. We found\nthat 13/20 participants were best fit to the quantized exponential model, while\nthe remaining 7/20 were best fit to the quantized hyperbolic model. Overall,\n15/20 participants were best fit to models with a 5-bit precision (i.e., 2^5 =\n32 steps). In conclusion, regardless of hyperbolic or exponential, quantized\nversions of these models are better fit to the experimental data than their\ncontinuous forms. We finally outline some potential applications of our\nfindings.\n", "versions": [{"version": "v1", "created": "Mon, 24 Feb 2020 03:24:08 GMT"}, {"version": "v2", "created": "Mon, 10 Aug 2020 00:40:49 GMT"}, {"version": "v3", "created": "Tue, 15 Sep 2020 22:54:03 GMT"}], "update_date": "2020-09-17", "authors_parsed": [["Tee", "James", ""], ["Taylor", "Desmond P.", ""]]}, {"id": "2002.10936", "submitter": "Matthew Leming", "authors": "Matthew Leming, John Suckling", "title": "Stochastic encoding of graphs in deep learning allows for complex\n  analysis of gender classification in resting-state and task functional brain\n  networks from the UK Biobank", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "q-bio.NC cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Classification of whole-brain functional connectivity MRI data with\nconvolutional neural networks (CNNs) has shown promise, but the complexity of\nthese models impedes understanding of which aspects of brain activity\ncontribute to classification. While visualization techniques have been\ndeveloped to interpret CNNs, bias inherent in the method of encoding abstract\ninput data, as well as the natural variance of deep learning models, detract\nfrom the accuracy of these techniques. We introduce a stochastic encoding\nmethod in an ensemble of CNNs to classify functional connectomes by gender. We\napplied our method to resting-state and task data from the UK BioBank, using\ntwo visualization techniques to measure the salience of three brain networks\ninvolved in task- and resting-states, and their interaction. To regress\nconfounding factors such as head motion, age, and intracranial volume, we\nintroduced a multivariate balancing algorithm to ensure equal distributions of\nsuch covariates between classes in our data. We achieved a final AUROC of\n0.8459. We found that resting-state data classifies more accurately than task\ndata, with the inner salience network playing the most important role of the\nthree networks overall in classification of resting-state data and connections\nto the central executive network in task data.\n", "versions": [{"version": "v1", "created": "Tue, 25 Feb 2020 15:10:51 GMT"}, {"version": "v2", "created": "Wed, 27 May 2020 16:20:28 GMT"}], "update_date": "2020-05-28", "authors_parsed": [["Leming", "Matthew", ""], ["Suckling", "John", ""]]}, {"id": "2002.10948", "submitter": "Dmitry V. Dylov", "authors": "Dmitrii Krylov, Remi Tachet, Romain Laroche, Michael Rosenblum, Dmitry\n  V. Dylov", "title": "Reinforcement Learning Framework for Deep Brain Stimulation Study", "comments": "7 pages + 1 references, 7 figures. arXiv admin note: text overlap\n  with arXiv:1909.12154", "journal-ref": null, "doi": "10.24963/ijcai.2020/394", "report-no": null, "categories": "q-bio.NC cs.AI cs.LG cs.SY eess.SY", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Malfunctioning neurons in the brain sometimes operate synchronously,\nreportedly causing many neurological diseases, e.g. Parkinson's. Suppression\nand control of this collective synchronous activity are therefore of great\nimportance for neuroscience, and can only rely on limited engineering trials\ndue to the need to experiment with live human brains. We present the first\nReinforcement Learning gym framework that emulates this collective behavior of\nneurons and allows us to find suppression parameters for the environment of\nsynthetic degenerate models of neurons. We successfully suppress synchrony via\nRL for three pathological signaling regimes, characterize the framework's\nstability to noise, and further remove the unwanted oscillations by engaging\nmultiple PPO agents.\n", "versions": [{"version": "v1", "created": "Sat, 22 Feb 2020 16:48:43 GMT"}], "update_date": "2020-12-07", "authors_parsed": [["Krylov", "Dmitrii", ""], ["Tachet", "Remi", ""], ["Laroche", "Romain", ""], ["Rosenblum", "Michael", ""], ["Dylov", "Dmitry V.", ""]]}, {"id": "2002.11006", "submitter": "Christian Keup", "authors": "Christian Keup, Tobias K\\\"uhn, David Dahmen, Moritz Helias", "title": "Transient chaotic dimensionality expansion by recurrent networks", "comments": "Final, shortened post-print version. (see v3 for extended material on\n  microscopic and macroscopic chaos)", "journal-ref": "Phys. Rev. X 11, 021064 (2021)", "doi": "10.1103/PhysRevX.11.021064", "report-no": null, "categories": "cond-mat.dis-nn q-bio.NC", "license": "http://creativecommons.org/licenses/by-sa/4.0/", "abstract": "  Neurons in the brain communicate with spikes, which are discrete events in\ntime and value. Functional network models often employ rate units that are\ncontinuously coupled by analog signals. Is there a qualitative difference\nimplied by these two forms of signaling? We develop a unified mean-field theory\nfor large random networks to show that first- and second-order statistics in\nrate and binary networks are in fact identical if rate neurons receive the\nright amount of noise. Their response to presented stimuli, however, can be\nradically different. We quantify these differences by studying how nearby state\ntrajectories evolve over time, asking to what extent the dynamics is chaotic.\nChaos in the two models is found to be qualitatively different. In binary\nnetworks we find a network-size-dependent transition to chaos and a chaotic\nsubmanifold whose dimensionality expands stereotypically with time, while rate\nnetworks with matched statistics are nonchaotic. Dimensionality expansion in\nchaotic binary networks aids classification in reservoir computing and optimal\nperformance is reached within about a single activation per neuron; a fast\nmechanism for computation that we demonstrate also in spiking networks. A\ngeneralization of this mechanism extends to rate networks in their respective\nchaotic regimes.\n", "versions": [{"version": "v1", "created": "Tue, 25 Feb 2020 16:26:29 GMT"}, {"version": "v2", "created": "Wed, 6 May 2020 15:04:30 GMT"}, {"version": "v3", "created": "Tue, 24 Nov 2020 12:44:23 GMT"}, {"version": "v4", "created": "Mon, 19 Jul 2021 17:38:52 GMT"}], "update_date": "2021-07-20", "authors_parsed": [["Keup", "Christian", ""], ["K\u00fchn", "Tobias", ""], ["Dahmen", "David", ""], ["Helias", "Moritz", ""]]}, {"id": "2002.11181", "submitter": "Timothy Oleskiw", "authors": "Timothy D. Oleskiw, Wyeth Bair, Eric Shea-Brown, Nicolas Brunel", "title": "Firing rate of the leaky integrate-and-fire neuron with stochastic\n  conductance-based synaptic inputs with short decay times", "comments": "5 pages, 2 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "q-bio.NC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We compute the firing rate of a leaky integrate-and-fire (LIF) neuron with\nstochastic conductance-based inputs in the limit when synaptic decay times are\nmuch shorter than the membrane time constant. A comparison of our analytical\nresults to numeric simulations is presented for a range of\nbiophysically-realistic parameters.\n", "versions": [{"version": "v1", "created": "Tue, 25 Feb 2020 21:26:40 GMT"}], "update_date": "2020-02-27", "authors_parsed": [["Oleskiw", "Timothy D.", ""], ["Bair", "Wyeth", ""], ["Shea-Brown", "Eric", ""], ["Brunel", "Nicolas", ""]]}, {"id": "2002.11192", "submitter": "Alessandro Rossi", "authors": "Alessandro Rossi, Sara Ermini, Dario Bernabini, Dario Zanca, Marino\n  Todisco, Alessandro Genovese, and Antonio Rizzo", "title": "End-to-End Models for the Analysis of System 1 and System 2 Interactions\n  based on Eye-Tracking Data", "comments": "11 pages, 2 figures, 1 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "q-bio.NC cs.CV cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  While theories postulating a dual cognitive system take hold, quantitative\nconfirmations are still needed to understand and identify interactions between\nthe two systems or conflict events. Eye movements are among the most direct\nmarkers of the individual attentive load and may serve as an important proxy of\ninformation. In this work we propose a computational method, within a modified\nvisual version of the well-known Stroop test, for the identification of\ndifferent tasks and potential conflicts events between the two systems through\nthe collection and processing of data related to eye movements. A statistical\nanalysis shows that the selected variables can characterize the variation of\nattentive load within different scenarios. Moreover, we show that Machine\nLearning techniques allow to distinguish between different tasks with a good\nclassification accuracy and to investigate more in depth the gaze dynamics.\n", "versions": [{"version": "v1", "created": "Mon, 3 Feb 2020 17:46:13 GMT"}], "update_date": "2020-02-27", "authors_parsed": [["Rossi", "Alessandro", ""], ["Ermini", "Sara", ""], ["Bernabini", "Dario", ""], ["Zanca", "Dario", ""], ["Todisco", "Marino", ""], ["Genovese", "Alessandro", ""], ["Rizzo", "Antonio", ""]]}, {"id": "2002.11195", "submitter": "Ilya A. Surov Mr.", "authors": "Ilya A. Surov", "title": "Quantum Cognitive Triad. Semantic geometry of context representation", "comments": "44 pages, 6 figures", "journal-ref": null, "doi": "10.1007/s10699-020-09712-x", "report-no": null, "categories": "q-bio.NC cs.AI quant-ph", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The paper describes an algorithm for semantic representation of behavioral\ncontexts relative to a dichotomic decision alternative. The contexts are\nrepresented as quantum qubit states in two-dimensional Hilbert space visualized\nas points on the Bloch sphere. The azimuthal coordinate of this sphere\nfunctions as a one-dimensional semantic space in which the contexts are\naccommodated according to their subjective relevance to the considered\nuncertainty. The contexts are processed in triples defined by knowledge of a\nsubject about a binary situational factor. The obtained triads of context\nrepresentations function as stable cognitive structure at the same time\nallowing a subject to model probabilistically-variative behavior. The developed\nalgorithm illustrates an approach for quantitative subjectively-semantic\nmodeling of behavior based on conceptual and mathematical apparatus of quantum\ntheory.\n", "versions": [{"version": "v1", "created": "Sat, 22 Feb 2020 17:29:10 GMT"}, {"version": "v2", "created": "Tue, 22 Dec 2020 14:43:28 GMT"}], "update_date": "2020-12-23", "authors_parsed": [["Surov", "Ilya A.", ""]]}, {"id": "2002.11319", "submitter": "Milo M. Lin", "authors": "Paul J. Blazek, Milo M. Lin", "title": "A neural network model of perception and reasoning", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.NE q-bio.NC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  How perception and reasoning arise from neuronal network activity is poorly\nunderstood. This is reflected in the fundamental limitations of connectionist\nartificial intelligence, typified by deep neural networks trained via\ngradient-based optimization. Despite success on many tasks, such networks\nremain unexplainable black boxes incapable of symbolic reasoning and concept\ngeneralization. Here we show that a simple set of biologically consistent\norganizing principles confer these capabilities to neuronal networks. To\ndemonstrate, we implement these principles in a novel machine learning\nalgorithm, based on concept construction instead of optimization, to design\ndeep neural networks that reason with explainable neuron activity. On a range\nof tasks including NP-hard problems, their reasoning capabilities grant\nadditional cognitive functions, like deliberating through self-analysis,\ntolerating adversarial attacks, and learning transferable rules from simple\nexamples to solve problems of unencountered complexity. The networks also\nnaturally display properties of biological nervous systems inherently absent in\ncurrent deep neural networks, including sparsity, modularity, and both\ndistributed and localized firing patterns. Because they do not sacrifice\nperformance, compactness, or training time on standard learning tasks, these\nnetworks provide a new black-box-free approach to artificial intelligence. They\nlikewise serve as a quantitative framework to understand the emergence of\ncognition from neuronal networks.\n", "versions": [{"version": "v1", "created": "Wed, 26 Feb 2020 06:26:04 GMT"}], "update_date": "2020-02-27", "authors_parsed": [["Blazek", "Paul J.", ""], ["Lin", "Milo M.", ""]]}, {"id": "2002.11754", "submitter": "Matthias Hohmann", "authors": "Matthias R. Hohmann, Lisa Konieczny, Michelle Hackl, Brian Wirth,\n  Talha Zaman, Raffi Enficiaud, Moritz Grosse-Wentrup, Bernhard Sch\\\"olkopf", "title": "MYND: Unsupervised Evaluation of Novel BCI Control Strategies on\n  Consumer Hardware", "comments": "9 pages, 5 figures. Submitted to PNAS. Minor revision", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC q-bio.NC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Neurophysiological studies are typically conducted in laboratories with\nlimited ecological validity, scalability, and generalizability of findings.\nThis is a significant challenge for the development of brain-computer\ninterfaces (BCIs), which ultimately need to function in unsupervised settings\non consumer-grade hardware. We introduce MYND: A framework that couples\nconsumer-grade recording hardware with an easy-to-use application for the\nunsupervised evaluation of BCI control strategies. Subjects are guided through\nexperiment selection, hardware fitting, recording, and data upload in order to\nself-administer multi-day studies that include neurophysiological recordings\nand questionnaires. As a use case, we evaluate two BCI control strategies\n(\"Positive memories\" and \"Music imagery\") in a realistic scenario by combining\nMYND with a four-channel electroencephalogram (EEG). Thirty subjects recorded\n70.4 hours of EEG data with the system at home. The median headset fitting time\nwas 25.9 seconds, and a median signal quality of 90.2% was retained during\nrecordings.Neural activity in both control strategies could be decoded with an\naverage offline accuracy of 68.5% and 64.0% across all days. The repeated\nunsupervised execution of the same strategy affected performance, which could\nbe tackled by implementing feedback to let subjects switch between strategies\nor devise new strategies with the platform.\n", "versions": [{"version": "v1", "created": "Wed, 26 Feb 2020 19:24:24 GMT"}, {"version": "v2", "created": "Sun, 15 Mar 2020 16:48:25 GMT"}], "update_date": "2020-03-17", "authors_parsed": [["Hohmann", "Matthias R.", ""], ["Konieczny", "Lisa", ""], ["Hackl", "Michelle", ""], ["Wirth", "Brian", ""], ["Zaman", "Talha", ""], ["Enficiaud", "Raffi", ""], ["Grosse-Wentrup", "Moritz", ""], ["Sch\u00f6lkopf", "Bernhard", ""]]}, {"id": "2002.11843", "submitter": "Ruthvik Vaila", "authors": "Ruthvik Vaila, John Chiasson, Vishal Saxena", "title": "A Deep Unsupervised Feature Learning Spiking Neural Network with\n  Binarized Classification Layers for EMNIST Classification using SpykeFlow", "comments": "A section of of this work is Submitted to IEEE TETCI 2020 Journal", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NE cs.LG eess.IV q-bio.NC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  End user AI is trained on large server farms with data collected from the\nusers. With ever increasing demand for IOT devices, there is a need for deep\nlearning approaches that can be implemented (at the edge) in an energy\nefficient manner. In this work we approach this using spiking neural networks.\nThe unsupervised learning technique of spike timing dependent plasticity (STDP)\nusing binary activations are used to extract features from spiking input data.\nGradient descent (backpropagation) is used only on the output layer to perform\nthe training for classification. The accuracies obtained for the balanced\nEMNIST data set compare favorably with other approaches. The effect of\nstochastic gradient descent (SGD) approximations on learning capabilities of\nour network are also explored.\n", "versions": [{"version": "v1", "created": "Wed, 26 Feb 2020 23:47:35 GMT"}, {"version": "v2", "created": "Mon, 11 May 2020 18:35:35 GMT"}, {"version": "v3", "created": "Thu, 21 May 2020 13:59:09 GMT"}, {"version": "v4", "created": "Wed, 28 Oct 2020 12:54:03 GMT"}], "update_date": "2020-10-29", "authors_parsed": [["Vaila", "Ruthvik", ""], ["Chiasson", "John", ""], ["Saxena", "Vishal", ""]]}, {"id": "2002.11870", "submitter": "Nai Ding", "authors": "Nai Ding", "title": "A Structure-based Memory Maintenance Model for Neural Tracking of\n  Linguistic Structures", "comments": "13 pages, 3 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "q-bio.NC", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  It is recently demonstrated that cortical activity can track the time courses\nof phrases and sentences during speech listening. Here, we propose a plausible\nneural processing framework to explain this phenomenon. It is argued that the\nbrain maintains the neural representation of a linguistic unit, i.e., a word or\na phrase, in a processing buffer until the unit is integrated into a\nhigher-level structure. After being integrated, the unit is removed from the\nbuffer and becomes activated long-term memory. In this model, the duration each\nunit is maintained in the processing buffer depends on the linguistic structure\nof the speech input. It is shown that the number of items retained in the\nprocessing buffer follows the time courses of phrases and sentences, in line\nwith neurophysiological data, whether the syntactic structure of a sentence is\nmentally parsed using a bottom-up or top-down predictive model. This model\ngenerates a range of testable predictions about the link between linguistic\nstructures, their dynamic psychological representations and their neural\nunderpinnings.\n", "versions": [{"version": "v1", "created": "Thu, 27 Feb 2020 01:51:03 GMT"}], "update_date": "2020-02-28", "authors_parsed": [["Ding", "Nai", ""]]}, {"id": "2002.12443", "submitter": "Marius Emar Yamakou", "authors": "Marius E. Yamakou, Poul G. Hjorth, Erik A. Martens", "title": "Optimal self-induced stochastic resonance in multiplex neural networks:\n  electrical versus chemical synapses", "comments": "24 pages, 7 figures", "journal-ref": null, "doi": "10.3389/fncom.2020.00062", "report-no": null, "categories": "q-bio.NC nlin.AO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Electrical and chemical synapses shape the dynamics of neural networks and\ntheir functional roles in information processing have been a longstanding\nquestion in neurobiology. In this paper, we investigate the role of synapses on\nthe optimization of the phenomenon of self-induced stochastic resonance in a\ndelayed multiplex neural network by using analytical and numerical methods. We\nconsider a two-layer multiplex network, in which at the intra-layer level\nneurons are coupled either by electrical synapses or by inhibitory chemical\nsynapses. For each isolated layer, computations indicate that weaker electrical\nand chemical synaptic couplings are better optimizers of self-induced\nstochastic resonance. In addition, regardless of the synaptic strengths,\nshorter electrical synaptic delays are found to be better optimizers of the\nphenomenon than shorter chemical synaptic delays, while longer chemical\nsynaptic delays are better optimizers than longer electrical synaptic delays --\nin both cases, the poorer optimizers are in fact worst. It is found that\nelectrical, inhibitory, or excitatory chemical multiplexing of the two layers\nhaving only electrical synapses at the intra-layer levels can each optimize the\nphenomenon. And only excitatory chemical multiplexing of the two layers having\nonly inhibitory chemical synapses at the intra-layer levels can optimize the\nphenomenon. These results may guide experiments aimed at establishing or\nconfirming the mechanism of self-induced stochastic resonance in networks of\nartificial neural circuits, as well as in real biological neural networks.\n", "versions": [{"version": "v1", "created": "Thu, 27 Feb 2020 21:13:14 GMT"}, {"version": "v2", "created": "Sun, 8 Mar 2020 20:43:58 GMT"}, {"version": "v3", "created": "Sat, 13 Jun 2020 10:26:48 GMT"}], "update_date": "2020-06-16", "authors_parsed": [["Yamakou", "Marius E.", ""], ["Hjorth", "Poul G.", ""], ["Martens", "Erik A.", ""]]}, {"id": "2002.12637", "submitter": "Johann Summhammer", "authors": "Johann Summhammer, Georg Sulyok, Gustav Bernroider and Massimo Cocchi", "title": "Forces from Lipids and Ionic Diffusion: Probing lateral membrane effects\n  by an optimized filter region of voltage dependent K+ channels", "comments": "18 pages, 5 Figures, Conference contribution at \"The Human Brain Seen\n  From Multiple Perspectives\", Turin, Italy, 14 Sep 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "physics.bio-ph q-bio.NC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We investigate the possible influence of poly unsaturated fatty acids (PUFAs)\nin the lipid bilayer of cell membranes on the conduction properties of the\nselectivity filter of voltage gated ion channels. Any change of this\nconductivity can have major consequences on electrical signalling in brain and\nother cells. At the microscopic level a change in the concentration of PUFAs\ncan cause a change in the pressure distribution within the lipid bilayer, and\nthis in turn can lead to a change in the length of the selectivity filter. In\norder to estimate the consequences this might entail for the ion channel's\nconductivity we performed high resolution molecular dynamics (MD) simulations\nof the passage of K+ ions and H2O molecules through the selectivity filter of\nthe KcsA potassium ion channel. Our results show that a change in length of the\nselectivity filter of as little as 4%, independent of whether the filter is\nmade longer or shorter, will reduce the K+ ion current by around 50%. And\nfurther squeezing or stretching by about 10% can effectively stop the current.\nFor instructive purposes and to facilitate further interpretation of the\nresults, we give details of the force models employed in the MD simulations. We\nfinally discuss the present results in the context of possible effects of\nmembrane lipid compositions on Kv ion permeation.\n", "versions": [{"version": "v1", "created": "Fri, 28 Feb 2020 10:29:12 GMT"}], "update_date": "2020-03-02", "authors_parsed": [["Summhammer", "Johann", ""], ["Sulyok", "Georg", ""], ["Bernroider", "Gustav", ""], ["Cocchi", "Massimo", ""]]}, {"id": "2002.12776", "submitter": "Radu Grosu", "authors": "Radu Grosu", "title": "ResNets, NeuralODEs and CT-RNNs are Particular Neural Regulatory\n  Networks", "comments": "9 pages, 4 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "q-bio.NC cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper shows that ResNets, NeuralODEs, and CT-RNNs, are particular neural\nregulatory networks (NRNs), a biophysical model for the nonspiking neurons\nencountered in small species, such as the C.elegans nematode, and in the retina\nof large species. Compared to ResNets, NeuralODEs and CT-RNNs, NRNs have an\nadditional multiplicative term in their synaptic computation, allowing them to\nadapt to each particular input. This additional flexibility makes NRNs $M$\ntimes more succinct than NeuralODEs and CT-RNNs, where $M$ is proportional to\nthe size of the training set. Moreover, as NeuralODEs and CT-RNNs are $N$ times\nmore succinct than ResNets, where $N$ is the number of integration steps\nrequired to compute the output $F(x)$ for a given input $x$, NRNs are in total\n$M\\,{\\cdot}\\,N$ more succinct than ResNets. For a given approximation task,\nthis considerable succinctness allows to learn a very small and therefore\nunderstandable NRN, whose behavior can be explained in terms of well\nestablished architectural motifs, that NRNs share with gene regulatory\nnetworks, such as, activation, inhibition, sequentialization, mutual exclusion,\nand synchronization. To the best of our knowledge, this paper unifies for the\nfirst time the mainstream work on deep neural networks with the one in biology\nand neuroscience in a quantitative fashion.\n", "versions": [{"version": "v1", "created": "Wed, 26 Feb 2020 18:33:20 GMT"}, {"version": "v2", "created": "Tue, 3 Mar 2020 13:05:48 GMT"}, {"version": "v3", "created": "Thu, 19 Mar 2020 07:20:12 GMT"}], "update_date": "2020-03-20", "authors_parsed": [["Grosu", "Radu", ""]]}]