[{"id": "1508.00073", "submitter": "Liane Gabora", "authors": "Liane Gabora and Nicole Carbert", "title": "A Study and Preliminary Model of Cross-Domain Influences on Creativity", "comments": "6 pages", "journal-ref": "(2015). In R. Dale, C. Jennings, P. Maglio, T. Matlock, D. Noelle,\n  A. Warlaumont & J. Yashimi (Eds.), Proceedings of the 37th annual meeting of\n  Cognitive Science Society (pp. 758-763). Austin TX: Cognitive Science Society", "doi": null, "report-no": null, "categories": "q-bio.NC q-bio.PE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper takes a two-pronged approach to investigate the phenomenon of\ncross-domain influence on creativity. We present a study in which creative\nindividuals were asked to list influences on their creative work. More than\nhalf the listed influences were unrelated to their creative domain, thus\ndemonstrating empirically that cross-domain influence is widespread. We then\npresent a preliminary model of exaptation, a form of cross-domain influence on\ncreativity in which a different context suggests a new use for an existing\nitem, using an example from the study.\n", "versions": [{"version": "v1", "created": "Sat, 1 Aug 2015 04:39:16 GMT"}, {"version": "v2", "created": "Mon, 15 Jul 2019 21:23:48 GMT"}], "update_date": "2019-07-17", "authors_parsed": [["Gabora", "Liane", ""], ["Carbert", "Nicole", ""]]}, {"id": "1508.00150", "submitter": "Carina Curto", "authors": "Carina Curto, Elizabeth Gross, Jack Jeffries, Katherine Morrison,\n  Mohamed Omar, Zvi Rosen, Anne Shiu, and Nora Youngs", "title": "What makes a neural code convex?", "comments": "25 pages, 9 figures, and 2 tables. Supplementary Text begins on page\n  17. Accepted to SIAM Journal on Applied Algebra and Geometry (SIAGA)", "journal-ref": null, "doi": null, "report-no": null, "categories": "q-bio.NC math.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Neural codes allow the brain to represent, process, and store information\nabout the world. Combinatorial codes, comprised of binary patterns of neural\nactivity, encode information via the collective behavior of populations of\nneurons. A code is called convex if its codewords correspond to regions defined\nby an arrangement of convex open sets in Euclidean space. Convex codes have\nbeen observed experimentally in many brain areas, including sensory cortices\nand the hippocampus, where neurons exhibit convex receptive fields. What makes\na neural code convex? That is, how can we tell from the intrinsic structure of\na code if there exists a corresponding arrangement of convex open sets? In this\nwork, we provide a complete characterization of local obstructions to\nconvexity. This motivates us to define max intersection-complete codes, a\nfamily guaranteed to have no local obstructions. We then show how our\ncharacterization enables one to use free resolutions of Stanley-Reisner ideals\nin order to detect violations of convexity. Taken together, these results\nprovide a significant advance in understanding the intrinsic combinatorial\nproperties of convex codes.\n", "versions": [{"version": "v1", "created": "Sat, 1 Aug 2015 17:20:44 GMT"}, {"version": "v2", "created": "Fri, 20 Nov 2015 17:31:09 GMT"}, {"version": "v3", "created": "Tue, 3 May 2016 18:53:18 GMT"}, {"version": "v4", "created": "Wed, 21 Dec 2016 18:53:42 GMT"}], "update_date": "2016-12-22", "authors_parsed": [["Curto", "Carina", ""], ["Gross", "Elizabeth", ""], ["Jeffries", "Jack", ""], ["Morrison", "Katherine", ""], ["Omar", "Mohamed", ""], ["Rosen", "Zvi", ""], ["Shiu", "Anne", ""], ["Youngs", "Nora", ""]]}, {"id": "1508.00165", "submitter": "Luca Mazzucato", "authors": "Luca Mazzucato, Alfredo Fontanini, and Giancarlo La Camera", "title": "Dynamics of multi-stable states during ongoing and evoked cortical\n  activity", "comments": "34 pages, 11 figures; v2: typos in Methods section corrected; v3:\n  typos corrected", "journal-ref": "J Neurosci. 2015 May 27;35(21):8214-31", "doi": "10.1523/JNEUROSCI.4819-14.2015", "report-no": null, "categories": "q-bio.NC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Single trial analyses of ensemble activity in alert animals demonstrate that\ncortical circuits dynamics evolve through temporal sequences of metastable\nstates. Metastability has been studied for its potential role in sensory\ncoding, memory and decision-making. Yet, very little is known about the network\nmechanisms responsible for its genesis. It is often assumed that the onset of\nstate sequences is triggered by an external stimulus. Here we show that state\nsequences can be observed also in the absence of overt sensory stimulation.\nAnalysis of multielectrode recordings from the gustatory cortex of alert rats\nrevealed ongoing sequences of states, where single neurons spontaneously attain\nseveral firing rates across different states. This single neuron\nmulti-stability represents a challenge to existing spiking network models,\nwhere typically each neuron is at most bi-stable. We present a recurrent\nspiking network model that accounts for both the spontaneous generation of\nstate sequences and the multi-stability in single neuron firing rates. Each\nstate results from the activation of neural clusters with potentiated\nintra-cluster connections, with the firing rate in each cluster depending on\nthe number of active clusters. Simulations show that the models ensemble\nactivity hops among the different states, reproducing the ongoing dynamics\nobserved in the data. When probed with external stimuli, the model predicts the\nquenching of single neuron multi-stability into bi-stability and the reduction\nof trial-by-trial variability. Both predictions were confirmed in the data.\nAltogether, these results provide a theoretical framework that captures both\nongoing and evoked network dynamics in a single mechanistic model.\n", "versions": [{"version": "v1", "created": "Sat, 1 Aug 2015 20:23:12 GMT"}, {"version": "v2", "created": "Thu, 4 Feb 2016 04:48:05 GMT"}, {"version": "v3", "created": "Tue, 22 Mar 2016 14:41:18 GMT"}], "update_date": "2016-03-23", "authors_parsed": [["Mazzucato", "Luca", ""], ["Fontanini", "Alfredo", ""], ["La Camera", "Giancarlo", ""]]}, {"id": "1508.00429", "submitter": "Alireza Alemi", "authors": "Alireza Alemi, Carlo Baldassi, Nicolas Brunel, Riccardo Zecchina", "title": "A three-threshold learning rule approaches the maximal capacity of\n  recurrent neural networks", "comments": "24 pages, 10 figures, to be published in PLOS Computational Biology", "journal-ref": null, "doi": "10.1371/journal.pcbi.1004439", "report-no": null, "categories": "q-bio.NC cond-mat.dis-nn", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Understanding the theoretical foundations of how memories are encoded and\nretrieved in neural populations is a central challenge in neuroscience. A\npopular theoretical scenario for modeling memory function is the attractor\nneural network scenario, whose prototype is the Hopfield model. The model has a\npoor storage capacity, compared with the capacity achieved with perceptron\nlearning algorithms. Here, by transforming the perceptron learning rule, we\npresent an online learning rule for a recurrent neural network that achieves\nnear-maximal storage capacity without an explicit supervisory error signal,\nrelying only upon locally accessible information. The fully-connected network\nconsists of excitatory binary neurons with plastic recurrent connections and\nnon-plastic inhibitory feedback stabilizing the network dynamics; the memory\npatterns are presented online as strong afferent currents, producing a bimodal\ndistribution for the neuron synaptic inputs. Synapses corresponding to active\ninputs are modified as a function of the value of the local fields with respect\nto three thresholds. Above the highest threshold, and below the lowest\nthreshold, no plasticity occurs. In between these two thresholds,\npotentiation/depression occurs when the local field is above/below an\nintermediate threshold. We simulated and analyzed a network of binary neurons\nimplementing this rule and measured its storage capacity for different sizes of\nthe basins of attraction. The storage capacity obtained through numerical\nsimulations is shown to be close to the value predicted by analytical\ncalculations. We also measured the dependence of capacity on the strength of\nexternal inputs. Finally, we quantified the statistics of the resulting\nsynaptic connectivity matrix, and found that both the fraction of zero weight\nsynapses and the degree of symmetry of the weight matrix increase with the\nnumber of stored patterns.\n", "versions": [{"version": "v1", "created": "Mon, 3 Aug 2015 14:22:52 GMT"}], "update_date": "2016-02-17", "authors_parsed": [["Alemi", "Alireza", ""], ["Baldassi", "Carlo", ""], ["Brunel", "Nicolas", ""], ["Zecchina", "Riccardo", ""]]}, {"id": "1508.00434", "submitter": "Diederik Aerts", "authors": "Diederik Aerts, Jonito Aerts Argu\\\"elles, Lester Beltran, Suzette\n  Geriente, Massimiliano Sassoli de Bianchi, Sandro Sozzo and Tomas Veloz", "title": "Spin and Wind Directions I: Identifying Entanglement in Nature and\n  Cognition", "comments": "The content of the previous article's versions has now been expanded\n  and reorganized in a two-part article of which this is the first half, the\n  second half being entitled 'Spin and Wind Directions II: A Bell State Quantum\n  Model' and to be found at arXiv:1706.01188", "journal-ref": "Foundations of Science, 23, pp. 323-335 (2018)", "doi": "10.1007/s10699-017-9528-9", "report-no": null, "categories": "q-bio.NC quant-ph", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a cognitive psychology experiment where participants were asked to\nselect pairs of spatial directions that they considered to be the best example\nof 'Two Different Wind Directions'. Data are shown to violate the CHSH version\nof Bell's inequality with the same magnitude as in typical Bell-test\nexperiments with entangled spins. Wind directions thus appear to be conceptual\nentities connected through meaning, in human cognition, in a similar way as\nspins appear to be entangled in experiments conducted in physics laboratories.\nThis is the first part of a two-part article. In the second part we present a\nsymmetrized version of the same experiment for which we provide a quantum\nmodeling of the collected data in Hilbert space.\n", "versions": [{"version": "v1", "created": "Fri, 31 Jul 2015 00:45:18 GMT"}, {"version": "v2", "created": "Thu, 20 Aug 2015 11:56:36 GMT"}, {"version": "v3", "created": "Tue, 6 Jun 2017 02:32:11 GMT"}], "update_date": "2019-02-12", "authors_parsed": [["Aerts", "Diederik", ""], ["Argu\u00eblles", "Jonito Aerts", ""], ["Beltran", "Lester", ""], ["Geriente", "Suzette", ""], ["de Bianchi", "Massimiliano Sassoli", ""], ["Sozzo", "Sandro", ""], ["Veloz", "Tomas", ""]]}, {"id": "1508.00919", "submitter": "Eva Lang", "authors": "Eva Lang", "title": "A Multiscale Analysis of Traveling Waves in Stochastic Neural Fields", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.PR q-bio.NC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We analyze the effects of noise on the traveling wave dynamics in neural\nfields. The noise influences the dynamics on two scales: first, it causes\nfluctuations in the wave profile, and second, it causes a random shift in the\nphase of the wave. We formulate the problem in a weighted $L^2$-space, allowing\nus to separate the two spatial scales. By tracking the stochastic solution with\na reference wave we obtain an expression for the stochastic phase. We derive an\nexpansion of the stochastic wave, describing the influence of the noise to\ndifferent orders of the noise strength. To first order of the noise strength,\nthe phase shift is roughly diffusive and the fluctuations are given by a\nstationary Ornstein-Uhlenbeck process orthogonal to the direction of movement.\nThis also expresses the stability of the wave under noise.\n", "versions": [{"version": "v1", "created": "Tue, 4 Aug 2015 20:48:38 GMT"}], "update_date": "2015-08-06", "authors_parsed": [["Lang", "Eva", ""]]}, {"id": "1508.01023", "submitter": "Bokai Cao", "authors": "Bokai Cao, Xiangnan Kong, Philip S. Yu", "title": "A review of heterogeneous data mining for brain disorders", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CE cs.DB q-bio.NC stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  With rapid advances in neuroimaging techniques, the research on brain\ndisorder identification has become an emerging area in the data mining\ncommunity. Brain disorder data poses many unique challenges for data mining\nresearch. For example, the raw data generated by neuroimaging experiments is in\ntensor representations, with typical characteristics of high dimensionality,\nstructural complexity and nonlinear separability. Furthermore, brain\nconnectivity networks can be constructed from the tensor data, embedding subtle\ninteractions between brain regions. Other clinical measures are usually\navailable reflecting the disease status from different perspectives. It is\nexpected that integrating complementary information in the tensor data and the\nbrain network data, and incorporating other clinical parameters will be\npotentially transformative for investigating disease mechanisms and for\ninforming therapeutic interventions. Many research efforts have been devoted to\nthis area. They have achieved great success in various applications, such as\ntensor-based modeling, subgraph pattern mining, multi-view feature analysis. In\nthis paper, we review some recent data mining methods that are used for\nanalyzing brain disorders.\n", "versions": [{"version": "v1", "created": "Wed, 5 Aug 2015 09:57:32 GMT"}], "update_date": "2015-08-06", "authors_parsed": [["Cao", "Bokai", ""], ["Kong", "Xiangnan", ""], ["Yu", "Philip S.", ""]]}, {"id": "1508.01060", "submitter": "Witali Dunin-Barkowski L", "authors": "Ksenia P. Solovyeva, Iakov M. Karandashev, Alex Zhavoronkov and Witali\n  L. Dunin-Barkowski", "title": "Models of Innate Neural Attractors and Their Applications for Neural\n  Information Processing", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "q-bio.NC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this work we reveal and explore a new class of attractor neural networks,\nbased on inborn connections provided by model molecular markers, the molecular\nmarker based attractor neural networks (MMBANN). We have explored conditions\nfor the existence of attractor states, critical relations between their\nparameters and the spectrum of single neuron models, which can implement the\nMMBANN. Besides, we describe functional models (perceptron and SOM) which\nobtain significant advantages, while using MMBANN. In particular, the\nperceptron based on MMBANN, gets specificity gain in orders of error\nprobabilities values, MMBANN SOM obtains real neurophysiological meaning, the\nnumber of possible grandma cells increases 1000- fold with MMBANN. Each set of\nmarkers has a metric, which is used to make connections between neurons\ncontaining the markers. The resulting neural networks have sets of attractor\nstates, which can serve as finite grids for representation of variables in\ncomputations. These grids may show dimensions of d = 0, 1, 2,... We work with\nstatic and dynamic attractor neural networks of dimensions d = 0 and d = 1. We\nalso argue that the number of dimensions which can be represented by attractors\nof activities of neural networks with the number of elements N=104 does not\nexceed 8.\n", "versions": [{"version": "v1", "created": "Wed, 5 Aug 2015 13:09:43 GMT"}], "update_date": "2015-08-06", "authors_parsed": [["Solovyeva", "Ksenia P.", ""], ["Karandashev", "Iakov M.", ""], ["Zhavoronkov", "Alex", ""], ["Dunin-Barkowski", "Witali L.", ""]]}, {"id": "1508.01537", "submitter": "Rui Ponte Costa", "authors": "Rui Ponte Costa", "title": "Computational model of axon guidance", "comments": "Master research thesis", "journal-ref": null, "doi": null, "report-no": null, "categories": "q-bio.NC", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Axon guidance (AG) towards their target during embryogenesis or after injury\nis an important issue in the development of neuronal networks. During their\ngrowth, axons often face complex decisions that are difficult to understand\nwhen observing just a small part of the problem. In this work we propose a\ncomputational model of AG based on activity-independent mechanisms that takes\ninto account the most important aspects of AG.\n  The model includes the main elements (neurons, with soma, axon and growth\ncone; glial cells acting as guideposts) and mechanisms (attraction/repulsion\nguidance cues, growth cone adaptation, tissue-gradient intersections, axonal\ntransport, changes in the growth cone complexity and a range of responses for\neach receptor). The growth cone guidance is defined as a function that maps the\nreceptor activation by ligands into a repulsive or attractive force. This force\nis then converted into a turn- ing angle using spherical coordinates. A\nregulatory network between the receptors and the intracellular proteins is\nconsidered, leading to more complex and realistic behaviors. The ligand\ndiffusion through the extracellular environment is modeled with linear or\nexponential functions. Concerning experimentation, it was developed the first\ncomputational model and a new theoretical model of the midline crossing of\nDrosophila axons that focus all the decision points. The computational model\ncreated allows describing to a great extent the behaviors that have been\nreported in the literature, for three different pathfinding scenarios: (i)\nnormal, (ii) comm mutant and (iii) robo mutant. Moreover, this model suggests\nnew hypotheses, being the most relevant the existence of an inhibitory link\nbetween the DCC receptor and the Comm protein that is Netrin-mediated or\nmediated by a third unknown signal.\n", "versions": [{"version": "v1", "created": "Thu, 6 Aug 2015 20:40:34 GMT"}], "update_date": "2015-08-10", "authors_parsed": [["Costa", "Rui Ponte", ""]]}, {"id": "1508.01741", "submitter": "Joshua Goldwyn", "authors": "Joshua H. Goldwyn and John Rinzel", "title": "Neuronal coupling by endogenous electric fields: Cable theory and\n  applications to coincidence detector neurons in the auditory brainstem", "comments": "40 pages, 11 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "q-bio.NC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The ongoing activity of neurons generates a spatially- and time-varying field\nof extracellular voltage ($V_e$). This $V_e$ field reflects population-level\nneural activity, but does it modulate neural dynamics and the function of\nneural circuits? We provide a cable theory framework to study how a bundle of\nmodel neurons generates $V_e$ and how this $V_e$ feeds back and influences\nmembrane potential ($V_m$). We find that these \"ephaptic interactions\" are\nsmall but not negligible. The model neural population can generate $V_e$ with\nmillivolt-scale amplitude and this $V_e$ perturbs the $V_m$ of \"nearby\" cables\nand effectively increases their electrotonic length. After using passive cable\ntheory to systematically study ephaptic coupling, we explore a test case: the\nmedial superior olive (MSO) in the auditory brainstem. The MSO is a possible\nlocus of ephaptic interactions: sounds evoke large $V_e$ in vivo in this\nnucleus (millivolt-scale). The $V_e$ response is thought to be generated by MSO\nneurons that perform a known neuronal computation with submillisecond temporal\nprecision (coincidence detection to encode sound source location). Using a\nbiophysically-based model of MSO neurons, we find millivolt-scale ephaptic\ninteractions consistent with the passive cable theory results. These subtle\nmembrane potential perturbations induce changes in spike initiation threshold,\nspike time synchrony, and time difference sensitivity. These results suggest\nthat ephaptic coupling may influence MSO function.\n", "versions": [{"version": "v1", "created": "Fri, 7 Aug 2015 16:09:25 GMT"}], "update_date": "2015-08-10", "authors_parsed": [["Goldwyn", "Joshua H.", ""], ["Rinzel", "John", ""]]}, {"id": "1508.02257", "submitter": "Rakesh Sengupta", "authors": "Rakesh Sengupta", "title": "Pre-stimulus oscillatory brain states and cognition: a theoretical\n  approach", "comments": "5 pages, no figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "q-bio.NC q-bio.QM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Spontaneous oscillations measured by local field potentials,\nelectroencephalograms and magnetoencephalograms exhibits variety of\noscillations spanning frrequency band (1Hz-100Hz) in animals and humans. Both\ninstantaneous power and phase of these ongoing oscillations have commonly been\nobserved to correlate with peristimulus processing in animals and humans.\nHowever, despite of numerous attempts it is not clear whether the same\nmechanisms can give rise to a range of oscillations as observed in vivo during\nresting state spontaneous oscillatory activity of the brain. In this paper I\ninvetigate the spontaneous activity in the cortex. The paper attempts to\nestablish analytically the conjecture that under certain conditions, a neural\nassembly can give rise to outputs that can be characterized by generalized\noscillatory functions. It is possible to validate the analytical predictions\nwith a neural mass model to show what the characteristic frequencies for such a\nbrain state should be if they have to respond to external stimuli though it is\nnot explored directly in the paper. In this paper we have attempted to show how\nan oscillatory dynamics might arise from a combination of a feed-forward and\nrecurrent neural assembly. Following that we have shown how naturally some of\nthe EEG and MEG band activities in the pre-stimulus $\\alpha$ ($\\sim$ 10 Hz) can\nbe explained from the resulting neural dynamics operating on a limited capacity\ncognitive systems. This provides a very important clue regarding how\npre-stimulus brain oscillatory dynamics generates a window to consciousness.\n", "versions": [{"version": "v1", "created": "Mon, 10 Aug 2015 14:13:14 GMT"}], "update_date": "2015-08-11", "authors_parsed": [["Sengupta", "Rakesh", ""]]}, {"id": "1508.02414", "submitter": "David A. Kessler", "authors": "David A. Kessler and Herbert Levine", "title": "Generic Criticality in Ecological and Neuronal Networks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cond-mat.dis-nn q-bio.NC q-bio.PE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We investigate the dynamics of two models of biological networks with purely\nsuppressive interactions between the units; species interacting via niche\ncompetition and neurons via inhibitory synaptic coupling. In both of these\ncases, power-law scaling of the density of states with probability arises\nwithout any fine-tuning of the model parameters. These results argue against\nthe increasingly popular notion that non-equilibrium living systems operate at\nspecial critical points, driven by there by evolution so as to enable adaptive\nprocessing of input data.\n", "versions": [{"version": "v1", "created": "Mon, 10 Aug 2015 20:24:01 GMT"}], "update_date": "2015-08-12", "authors_parsed": [["Kessler", "David A.", ""], ["Levine", "Herbert", ""]]}, {"id": "1508.02505", "submitter": "Maryam Keyvanara", "authors": "Maryam Keyvanara, Seyed Amirhassan Monadjemi", "title": "Simulating Brain Reaction to Methamphetamine Regarding Consumer\n  Personality", "comments": "10 Pages, 4 Figures, Journal Paper", "journal-ref": "International Journal of Artificial Intelligence & Applications\n  (IJAIA) Vol. 6, No. 4, July 2015, pp. 63-72", "doi": "10.5121/ijaia.2015.6406", "report-no": null, "categories": "cs.NE q-bio.NC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Addiction, as a nervous disease, can be analysed using mathematical modelling\nand computer simulations. In this paper, we use an existing mathematical model\nto predict and simulate human brain response to the consumption of a single\ndose of methamphetamine. The model is implemented and coded in Matlab. Three\ntypes of personalities including introverts, ambiverts and extroverts are\nstudied. The parameters of the mathematical model are calibrated and optimized,\naccording to psychological theories, using a real coded genetic algorithm. The\nsimulations show significant correlation between people response to\nmethamphetamine abuse and their personality. They also show that one of the\ncauses of tendency to stimulants roots in consumers personality traits. The\nresults can be used as a tool for reducing attitude towards addiction.\n", "versions": [{"version": "v1", "created": "Tue, 11 Aug 2015 07:44:18 GMT"}], "update_date": "2015-08-17", "authors_parsed": [["Keyvanara", "Maryam", ""], ["Monadjemi", "Seyed Amirhassan", ""]]}, {"id": "1508.02739", "submitter": "Kwang Hyun Ko", "authors": "Kwang Hyun Ko", "title": "Origins of Bipedalism", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "q-bio.PE q-bio.NC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The following manuscript reviews various theories of bipedalism and provides\na holistic answer to human evolution. There are two questions regarding\nbipedalism: i) why were the earliest hominins partially bipedal? and ii) why\ndid hominins become increasingly bipedal over time and replace their less\nbipedal ancestors? To answer these questions, the prominent theories in the\nfield, such as the savanna-based theory, the postural feeding hypotheses, and\nthe provisioning model, are collectively examined. Because biological evolution\nis an example of trial and error and not a simple causation, there may be\nmultiple answers to the evolution of bipedalism. The postural feeding\nhypothesis (reaching for food/balancing) provides an explanation for the\npartial bipedalism of the earliest hominins. The savannah-based theory\ndescribes how the largely bipedal hominins that started to settle on the ground\nbecame increasingly bipedal. The provisioning model (food-gathering/monogamy)\nexplains questions arising after the postural feeding hypothesis and before the\nsavannah theory in an evolutionary timeline. Indeed, there are no straight\nlines between the theories, and multiple forces could have pushed the evolution\nof bipedalism at different points. Finally, this manuscript states that the\narboreal hominins that possessed ambiguous traits of bipedalism were eliminated\nthrough choice and selection. Using the biological analogy of the okapi and\ngiraffe, I explain how one of the branches (Homo) became increasingly bipedal\nwhile the other (Pan) adapted to locomotion for forest life by narrowing the\nanatomical/biological focus in evolution.\n", "versions": [{"version": "v1", "created": "Sun, 9 Aug 2015 08:43:49 GMT"}, {"version": "v2", "created": "Sat, 17 Oct 2015 16:48:55 GMT"}], "update_date": "2015-10-20", "authors_parsed": [["Ko", "Kwang Hyun", ""]]}, {"id": "1508.02792", "submitter": "Thomas M. Breuel", "authors": "Thomas M. Breuel", "title": "Possible Mechanisms for Neural Reconfigurability and their Implications", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NE q-bio.NC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The paper introduces a biologically and evolutionarily plausible neural\narchitecture that allows a single group of neurons, or an entire cortical\npathway, to be dynamically reconfigured to perform multiple, potentially very\ndifferent computations. The paper shows that reconfigurability can account for\nthe observed stochastic and distributed coding behavior of neurons and provides\na parsimonious explanation for timing phenomena in psychophysical experiments.\nIt also shows that reconfigurable pathways correspond to classes of statistical\nclassifiers that include decision lists, decision trees, and hierarchical\nBayesian methods. Implications for the interpretation of neurophysiological and\npsychophysical results are discussed, and future experiments for testing the\nreconfigurability hypothesis are explored.\n", "versions": [{"version": "v1", "created": "Wed, 12 Aug 2015 01:23:35 GMT"}], "update_date": "2015-08-13", "authors_parsed": [["Breuel", "Thomas M.", ""]]}, {"id": "1508.03373", "submitter": "Vaibhav Srivastava", "authors": "Vaibhav Srivastava and Samuel F. Feng and Jonathan D. Cohen and Naomi\n  Ehrich Leonard and Amitai Shenhav", "title": "A martingale analysis of first passage times of time-dependent Wiener\n  diffusion models", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.PR math.OC q-bio.NC q-fin.MF", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Research in psychology and neuroscience has successfully modeled decision\nmaking as a process of noisy evidence accumulation to a decision bound. While\nthere are several variants and implementations of this idea, the majority of\nthese models make use of a noisy accumulation between two absorbing boundaries.\nA common assumption of these models is that decision parameters, e.g., the rate\nof accumulation (drift rate), remain fixed over the course of a decision,\nallowing the derivation of analytic formulas for the probabilities of hitting\nthe upper or lower decision threshold, and the mean decision time. There is\nreason to believe, however, that many types of behavior would be better\ndescribed by a model in which the parameters were allowed to vary over the\ncourse of the decision process.\n  In this paper, we use martingale theory to derive formulas for the mean\ndecision time, hitting probabilities, and first passage time (FPT) densities of\na Wiener process with time-varying drift between two time-varying absorbing\nboundaries. This model was first studied by Ratcliff (1980) in the two-stage\nform, and here we consider the same model for an arbitrary number of stages\n(i.e. intervals of time during which parameters are constant). Our calculations\nenable direct computation of mean decision times and hitting probabilities for\nthe associated multistage process. We also provide a review of how martingale\ntheory may be used to analyze similar models employing Wiener processes by\nre-deriving some classical results. In concert with a variety of numerical\ntools already available, the current derivations should encourage mathematical\nanalysis of more complex models of decision making with time-varying evidence.\n", "versions": [{"version": "v1", "created": "Thu, 13 Aug 2015 22:05:36 GMT"}, {"version": "v2", "created": "Sun, 27 Sep 2015 01:01:13 GMT"}, {"version": "v3", "created": "Fri, 30 Sep 2016 20:04:13 GMT"}], "update_date": "2016-10-04", "authors_parsed": [["Srivastava", "Vaibhav", ""], ["Feng", "Samuel F.", ""], ["Cohen", "Jonathan D.", ""], ["Leonard", "Naomi Ehrich", ""], ["Shenhav", "Amitai", ""]]}, {"id": "1508.03527", "submitter": "Chris Antonopoulos Dr.", "authors": "Chris G. Antonopoulos, Shambhavi Srivastava, Sandro E. de S. Pinto,\n  Murilo S. Baptista", "title": "Do Brain Networks Evolve by Maximizing their Information Flow Capacity?", "comments": "27 pages, 8 figures, 2 tables, supporting_information included,\n  published in PLOS Computational Biology", "journal-ref": null, "doi": "10.1371/journal.pcbi.1004372", "report-no": null, "categories": "q-bio.NC q-bio.MN", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a working hypothesis supported by numerical simulations that brain\nnetworks evolve based on the principle of the maximization of their internal\ninformation flow capacity. We find that synchronous behavior and capacity of\ninformation flow of the evolved networks reproduce well the same behaviors\nobserved in the brain dynamical networks of Caenorhabditis elegans and humans,\nnetworks of Hindmarsh-Rose neurons with graphs given by these brain networks.\nWe make a strong case to verify our hypothesis by showing that the neural\nnetworks with the closest graph distance to the brain networks of\nCaenorhabditis elegans and humans are the Hindmarsh-Rose neural networks\nevolved with coupling strengths that maximize information flow capacity.\nSurprisingly, we find that global neural synchronization levels decrease during\nbrain evolution, reflecting on an underlying global no Hebbian-like evolution\nprocess, which is driven by no Hebbian-like learning behaviors for some of the\nclusters during evolution, and Hebbian-like learning rules for clusters where\nneurons increase their synchronization.\n", "versions": [{"version": "v1", "created": "Tue, 28 Jul 2015 19:03:29 GMT"}], "update_date": "2015-08-17", "authors_parsed": [["Antonopoulos", "Chris G.", ""], ["Srivastava", "Shambhavi", ""], ["Pinto", "Sandro E. de S.", ""], ["Baptista", "Murilo S.", ""]]}, {"id": "1508.03788", "submitter": "Kamal Shadi", "authors": "Kamal Shadi, Saideh Bakhshi, David A. Gutman, Helen S. Mayberg,\n  Constantine Dovrolis", "title": "A symmetry-based method to infer structural brain networks from\n  tractography data", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "q-bio.NC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recent progress in diffusion MRI and tractography algorithms as well as the\nlaunch of the Human Connectome Project (HCP) have provided brain research with\nan abundance of structural connectivity data. In this work, we describe and\nevaluate a method that can infer the structural brain network that\ninterconnects a given set of Regions of Interest (ROIs) from tractography data.\nThe proposed method, referred to as Minimum Asymmetry Network Inference\nAlgorithm (MANIA), differs from prior work because it does not determine the\nconnectivity between two ROIs based on an arbitrary connectivity threshold.\nInstead, we exploit a basic limitation of the tractography process: the\nobserved streamlines from a source to a target do not provide any information\nabout the polarity of the underlying white matter, and so if there are some\nfibers connecting two voxels (or two ROIs) X and Y tractography should be able\nin principle to follow this connection in both directions, from X to Y and from\nY to X. We leverage this limitation to formulate the network inference process\nas an optimization problem that minimizes the (appropriately normalized)\nasymmetry of the observed network. We evaluate the proposed method on a noise\nmodel that randomly corrupts the observed connectivity of synthetic networks.\nAs a case-study, we apply MANIA on diffusion MRI data from 28 healthy subjects\nto infer the structural network between 18 corticolimbic ROIs that are\nassociated with various neuropsychiatric conditions including depression,\nanxiety and addiction.\n", "versions": [{"version": "v1", "created": "Sun, 16 Aug 2015 03:43:53 GMT"}, {"version": "v2", "created": "Wed, 1 Jun 2016 02:02:59 GMT"}], "update_date": "2016-06-02", "authors_parsed": [["Shadi", "Kamal", ""], ["Bakhshi", "Saideh", ""], ["Gutman", "David A.", ""], ["Mayberg", "Helen S.", ""], ["Dovrolis", "Constantine", ""]]}, {"id": "1508.03929", "submitter": "Saeed Reza Kheradpisheh", "authors": "Saeed Reza Kheradpisheh, Masoud Ghodrati, Mohammad Ganjtabesh,\n  Timoth\\'ee Masquelier", "title": "Deep Networks Can Resemble Human Feed-forward Vision in Invariant Object\n  Recognition", "comments": null, "journal-ref": "Scientific Reports (2016) 6: 32672", "doi": "10.1038/srep32672", "report-no": null, "categories": "cs.CV q-bio.NC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Deep convolutional neural networks (DCNNs) have attracted much attention\nrecently, and have shown to be able to recognize thousands of object categories\nin natural image databases. Their architecture is somewhat similar to that of\nthe human visual system: both use restricted receptive fields, and a hierarchy\nof layers which progressively extract more and more abstracted features. Yet it\nis unknown whether DCNNs match human performance at the task of view-invariant\nobject recognition, whether they make similar errors and use similar\nrepresentations for this task, and whether the answers depend on the magnitude\nof the viewpoint variations. To investigate these issues, we benchmarked eight\nstate-of-the-art DCNNs, the HMAX model, and a baseline shallow model and\ncompared their results to those of humans with backward masking. Unlike in all\nprevious DCNN studies, we carefully controlled the magnitude of the viewpoint\nvariations to demonstrate that shallow nets can outperform deep nets and humans\nwhen variations are weak. When facing larger variations, however, more layers\nwere needed to match human performance and error distributions, and to have\nrepresentations that are consistent with human behavior. A very deep net with\n18 layers even outperformed humans at the highest variation level, using the\nmost human-like representations.\n", "versions": [{"version": "v1", "created": "Mon, 17 Aug 2015 05:46:24 GMT"}, {"version": "v2", "created": "Sun, 21 Feb 2016 14:34:44 GMT"}, {"version": "v3", "created": "Fri, 4 Mar 2016 15:54:25 GMT"}, {"version": "v4", "created": "Tue, 28 Jun 2016 10:55:18 GMT"}], "update_date": "2016-09-13", "authors_parsed": [["Kheradpisheh", "Saeed Reza", ""], ["Ghodrati", "Masoud", ""], ["Ganjtabesh", "Mohammad", ""], ["Masquelier", "Timoth\u00e9e", ""]]}, {"id": "1508.04241", "submitter": "Andrew Magyar", "authors": "Andrew Magyar", "title": "Beta Distribution of Human MTL Neuron Sparsity: A Sparse and Skewed Code", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "q-bio.NC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Single unit recordings in the human medial temporal lobe (MTL) have revealed\na population of cells with conceptually based, highly selective activity,\nindicating the presence of a sparse neural code. Building off previous work by\nthe author and J.C. Collins, this paper develops a statistical model for\nanalyzing this data, based on maximum likelihood analysis. The goal is to infer\nthe underlying distribution of neural response probabilities across the\npopulation of MTL cells. The response probability, or neuronal sparsity, is\ndefined as the total probability that the neuron produces an above-threshold\nfiring rate during the presentation of a randomly selected stimulus. Applying\nthe method, it is shown that a beta-distributed neuronal sparsity across the\ncells of the MTL is consistent with the data. The resulting fits reveal a\nsparse and highly skewed code, with a huge majority of neurons exhibiting\nextremely low response probabilities, and a smaller minority possessing\nconsiderably higher response probabilities. The distributions are closely\napproximated by a power law at low sparsity values. Strikingly similar skewed\ndistributions have been found in the statistics of place cell activity in rats,\nsuggesting similar underlying coding dynamics between the human MTL and the rat\nhippocampus.\n", "versions": [{"version": "v1", "created": "Tue, 18 Aug 2015 08:38:24 GMT"}], "update_date": "2015-08-19", "authors_parsed": [["Magyar", "Andrew", ""]]}, {"id": "1508.04561", "submitter": "Wlodzislaw Duch", "authors": "W{\\l}odzis{\\l}aw Duch", "title": "Memetics and Neural Models of Conspiracy Theories", "comments": "14 pages, 7 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "q-bio.NC cs.AI cs.NE", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Conspiracy theories, or in general seriously distorted beliefs, are\nwidespread. How and why are they formed in the brain is still more a matter of\nspeculation rather than science. In this paper one plausible mechanisms is\ninvestigated: rapid freezing of high neuroplasticity (RFHN). Emotional arousal\nincreases neuroplasticity and leads to creation of new pathways spreading\nneural activation. Using the language of neurodynamics a meme is defined as\nquasi-stable associative memory attractor state. Depending on the temporal\ncharacteristics of the incoming information and the plasticity of the network,\nmemory may self-organize creating memes with large attractor basins, linking\nmany unrelated input patterns. Memes with fake rich associations distort\nrelations between memory states. Simulations of various neural network models\ntrained with competitive Hebbian learning (CHL) on stationary and\nnon-stationary data lead to the same conclusion: short learning with high\nplasticity followed by rapid decrease of plasticity leads to memes with large\nattraction basins, distorting input pattern representations in associative\nmemory. Such system-level models may be used to understand creation of\ndistorted beliefs and formation of conspiracy memes, understood as strong\nattractor states of the neurodynamics.\n", "versions": [{"version": "v1", "created": "Wed, 19 Aug 2015 08:20:17 GMT"}, {"version": "v2", "created": "Sun, 17 Jan 2021 17:38:40 GMT"}], "update_date": "2021-01-19", "authors_parsed": [["Duch", "W\u0142odzis\u0142aw", ""]]}, {"id": "1508.04624", "submitter": "Alexander K. Vidybida", "authors": "Alexander K. Vidybida", "title": "Fast {\\large\\it Cl-}type inhibitory neuron with delayed feedback has\n  non-markov output statistics", "comments": "22 pages, 2 figures, 43 Refs. arXiv admin note: text overlap with\n  arXiv:1503.03312", "journal-ref": null, "doi": null, "report-no": null, "categories": "q-bio.NC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  For a class of fast {\\it Cl-}type inhibitory spiking neuron models with\ndelayed feedback fed with a Poisson stochastic process of excitatory impulses,\nit is proven that the stream of output interspike intervals cannot be presented\nas a Markov process of any order.\n", "versions": [{"version": "v1", "created": "Wed, 19 Aug 2015 12:51:59 GMT"}, {"version": "v2", "created": "Tue, 19 Dec 2017 11:19:46 GMT"}], "update_date": "2017-12-20", "authors_parsed": [["Vidybida", "Alexander K.", ""]]}, {"id": "1508.04751", "submitter": "Ehtibar Dzhafarov", "authors": "Ehtibar N. Dzhafarov, Janne V. Kujala, Victor H. Cervantes, Ru Zhang,\n  and Matt Jones", "title": "On Contextuality in Behavioral Data", "comments": "to be published in April 2016 in vol. 374 issue 2066 of Philosophical\n  Transactions of the Royal Society A", "journal-ref": null, "doi": "10.1098/rsta.2015.0234", "report-no": null, "categories": "q-bio.NC quant-ph", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Dzhafarov, Zhang, and Kujala (Phil. Trans. Roy. Soc. A 374, 20150099)\nreviewed several behavioral data sets imitating the formal design of the\nquantum-mechanical contextuality experiments. The conclusion was that none of\nthese data sets exhibited contextuality if understood in the generalized sense\nproposed in Dzhafarov, Kujala, and Larsson (Found. Phys. 7, 762-782, 2015),\nwhile the traditional definition of contextuality does not apply to these data\nbecause they violate the condition of consistent connectedness (also known as\nmarginal selectivity, no-signaling condition, no-disturbance principle, etc.).\nIn this paper we clarify the relationship between (in)consistent connectedness\nand (non)contextuality, as well as between the traditional and extended\ndefinitions of (non)contextuality, using as an example the\nClauser-Horn-Shimony-Holt (CHSH) inequalities originally designed for detecting\ncontextuality in entangled particles.\n", "versions": [{"version": "v1", "created": "Tue, 18 Aug 2015 19:01:06 GMT"}, {"version": "v2", "created": "Thu, 22 Oct 2015 17:06:13 GMT"}, {"version": "v3", "created": "Wed, 25 Nov 2015 07:35:12 GMT"}, {"version": "v4", "created": "Thu, 17 Dec 2015 03:20:25 GMT"}, {"version": "v5", "created": "Thu, 24 Mar 2016 15:10:45 GMT"}], "update_date": "2016-09-28", "authors_parsed": [["Dzhafarov", "Ehtibar N.", ""], ["Kujala", "Janne V.", ""], ["Cervantes", "Victor H.", ""], ["Zhang", "Ru", ""], ["Jones", "Matt", ""]]}, {"id": "1508.05414", "submitter": "Joshua Vogelstein", "authors": "Raag D. Airan, Joshua T. Vogelstein, Jay J. Pillai, Brian Caffo, James\n  J. Pekar, and Haris I. Sair", "title": "Stability and Localization of inter-individual differences in functional\n  connectivity", "comments": "14 pages, 5 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP q-bio.NC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Much recent attention has been paid to quantifying anatomic and functional\nneuroimaging on the individual subject level. For optimal individual subject\ncharacterization, specific acquisition and analysis features need to be\nidentified that maximize inter-individual variability while concomitantly\nminimizing intra-subject variability. Here we develop a non-parametric\nstatistical metric that quantifies the degree to which a parameter set allows\nthis individual subject differentiation. We apply this metric to analyzing\npublicly available test-retest resting-state fMRI (rs-fMRI) data sets. We find\nthat for the question of maximizing individual differentiation, there is a\nrelative tradeoff between increasing sampling through increased sampling\nfrequency or increased acquisition time; that for the sizes of the interrogated\ndata sets, only 4-5 min of acquisition time is necessary to perfectly\ndifferentiate each subject; and that brain regions that most contribute to\nindividuals unique characterization lie in association cortices thought to\ncontribute to higher cognitive function. These findings may guide optimal\nrs-fMRI experiment design and may aid elucidation of the neural bases for\nsubject-to-subject differences.\n", "versions": [{"version": "v1", "created": "Fri, 21 Aug 2015 21:15:25 GMT"}, {"version": "v2", "created": "Wed, 11 May 2016 21:52:21 GMT"}], "update_date": "2016-05-13", "authors_parsed": [["Airan", "Raag D.", ""], ["Vogelstein", "Joshua T.", ""], ["Pillai", "Jay J.", ""], ["Caffo", "Brian", ""], ["Pekar", "James J.", ""], ["Sair", "Haris I.", ""]]}, {"id": "1508.05468", "submitter": "Henry Tuckwell", "authors": "Henry C. Tuckwell, Ying Zhou and Nicholas J. Penington", "title": "Simplified models of pacemaker spiking in raphe and locus coeruleus\n  neurons", "comments": "21 pages, 11 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "q-bio.NC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Many central neurons, and in particular certain brainstem aminergic neurons\nexhibit spontaneous and fairly regular spiking with frequencies of order a few\nHz. A large number of ion channel types contribute to such spiking so that\naccurate modeling of spike generation leads to the requirement of solving very\nlarge systems of differential equations, ordinary in the first instance. Since\nanalysis of spiking behavior when many synaptic inputs are active adds further\nto the number of components, it is useful to have simplified mathematical\nmodels of spiking in such neurons so that, for example, stochastic features of\ninputs and output spike trains can be incorporated. In this article we\ninvestigate two simple two-component models which mimic features of spiking in\nserotonergic neurons of the dorsal raphe nucleus and noradrenergic neurons of\nthe locus coeruleus. The first model is of the Fitzhugh-Nagumo type and the\nsecond is a reduced Hodgkin-Huxley model. For each model solutions are computed\nwith two representative sets of parameters. Frequency versus input currents are\nfound and reveal Hodgkin type 2 behavior. For the first model a bifurcation and\nphase plane analysis supports these findings. The spike trajectories in the\nsecond model are very similar to those in DRN SE pacemaker activity but there\nare more parameters than in the Fitzhugh-Nagumo type model. The article\nconcludes with a brief review of previous modeling of these types of neurons\nand its relevance to studies of serotonergic involvement in spatial working\nmemory and obsessive-compulsive disorder.\n", "versions": [{"version": "v1", "created": "Sat, 22 Aug 2015 04:39:03 GMT"}], "update_date": "2015-08-25", "authors_parsed": [["Tuckwell", "Henry C.", ""], ["Zhou", "Ying", ""], ["Penington", "Nicholas J.", ""]]}, {"id": "1508.05707", "submitter": "Lucilla de Arcangelis", "authors": "Vittorio Capano, Hans J. Herrmann and Lucilla de Arcangelis", "title": "Optimal percentage of inhibitory synapses in multi-task learning", "comments": "5 pages, 5 figures", "journal-ref": "SCIENTIFIC REPORTS vol 5 page 9895 (2015)", "doi": "10.1038/srep09895", "report-no": null, "categories": "q-bio.NC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Performing more tasks in parallel is a typical feature of complex brains.\nThese are characterized by the coexistence of excitatory and inhibitory\nsynapses, whose percentage in mammals is measured to have a typical value of\n20-30\\%. Here we investigate parallel learning of more Boolean rules in\nneuronal networks. We find that multi-task learning results from the\nalternation of learning and forgetting of the individual rules. Interestingly,\na fraction of 30\\% inhibitory synapses optimizes the overall performance,\ncarving a complex backbone supporting information transmission with a minimal\nshortest path length. We show that 30\\% inhibitory synapses is the percentage\nmaximizing the learning performance since it guarantees, at the same time, the\nnetwork excitability necessary to express the response and the variability\nrequired to confine the employment of resources.\n", "versions": [{"version": "v1", "created": "Mon, 24 Aug 2015 07:27:05 GMT"}], "update_date": "2015-08-25", "authors_parsed": [["Capano", "Vittorio", ""], ["Herrmann", "Hans J.", ""], ["de Arcangelis", "Lucilla", ""]]}, {"id": "1508.05782", "submitter": "Jaan Aru", "authors": "Madis Vasser, Markus K\\\"angsepp, Jaan Aru", "title": "Change Blindness in 3D Virtual Reality", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "q-bio.NC cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In the present change blindness study subjects explored stereoscopic three\ndimensional (3D) environments through a virtual reality (VR) headset. A novel\nmethod that tracked the subjects' head movements was used for inducing changes\nin the scene whenever the changing object was out of the field of view. The\neffect of change location (foreground or background in 3D depth) on change\nblindness was investigated. Two experiments were conducted, one in the lab (n =\n50) and the other online (n = 25). Up to 25% of the changes were undetected and\nthe mean overall search time was 27 seconds in the lab study. Results indicated\nsignificantly lower change detection success and more change cycles if the\nchanges occurred in the background, with no differences in overall search\ntimes. The results confirm findings from previous studies and extend them to 3D\nenvironments. The study also demonstrates the feasibility of online VR\nexperiments.\n", "versions": [{"version": "v1", "created": "Mon, 24 Aug 2015 12:33:10 GMT"}], "update_date": "2015-08-25", "authors_parsed": [["Vasser", "Madis", ""], ["K\u00e4ngsepp", "Markus", ""], ["Aru", "Jaan", ""]]}, {"id": "1508.05929", "submitter": "Matthew Fisher", "authors": "Matthew P. A. Fisher", "title": "Quantum Cognition: The possibility of processing with nuclear spins in\n  the brain", "comments": "8 pages, 3 figures", "journal-ref": "Annals of Physics 362, 593-602 (2015)", "doi": null, "report-no": null, "categories": "q-bio.NC physics.bio-ph quant-ph", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The possibility that quantum processing with nuclear spins might be operative\nin the brain is proposed and then explored. Phosphorus is identified as the\nunique biological element with a nuclear spin that can serve as a qubit for\nsuch putative quantum processing - a neural qubit - while the phosphate ion is\nthe only possible qubit-transporter. We identify the \"Posner molecule\",\n$\\text{Ca}_9 (\\text{PO}_4)_6$, as the unique molecule that can protect the\nneural qubits on very long times and thereby serve as a (working)\nquantum-memory. A central requirement for quantum-processing is quantum\nentanglement. It is argued that the enzyme catalyzed chemical reaction which\nbreaks a pyrophosphate ion into two phosphate ions can quantum entangle pairs\nof qubits. Posner molecules, formed by binding such phosphate pairs with\nextracellular calcium ions, will inherit the nuclear spin entanglement. A\nmechanism for transporting Posner molecules into presynaptic neurons during a\n\"kiss and run\" exocytosis, which releases neurotransmitters into the synaptic\ncleft, is proposed. Quantum measurements can occur when a pair of Posner\nmolecules chemically bind and subsequently melt, releasing a shower of\nintra-cellular calcium ions that can trigger further neurotransmitter release\nand enhance the probability of post-synaptic neuron firing. Multiple entangled\nPosner molecules, triggering non-local quantum correlations of neuron firing\nrates, would provide the key mechanism for neural quantum processing.\nImplications, both in vitro and in vivo, are briefly mentioned.\n", "versions": [{"version": "v1", "created": "Wed, 19 Aug 2015 18:23:38 GMT"}, {"version": "v2", "created": "Sat, 29 Aug 2015 00:00:42 GMT"}], "update_date": "2015-11-10", "authors_parsed": [["Fisher", "Matthew P. A.", ""]]}, {"id": "1508.06226", "submitter": "Edgardo Bonzi", "authors": "E. V. Bonzi, G. B. Grad, A. M. Maggi, M. R. Mu\\~n\\'oz", "title": "Study of the characteristic parameters of the normal voices of\n  Argentinian speakers", "comments": "5 pages, 6 figures", "journal-ref": "Papers in Physics 6, 060002 (2014)", "doi": "10.4279/PIP.060002", "report-no": null, "categories": "q-bio.NC cs.SD", "license": "http://creativecommons.org/licenses/by/3.0/", "abstract": "  The voice laboratory permits to study the human voices using a method that is\nobjective and noninvasive. In this work, we have studied the parameters of the\nhuman voice such as pitch, formant, jitter, shimmer and harmonic-noise ratio of\na group of young people. This statistical information of parameters is obtained\nfrom Argentinian speakers.\n", "versions": [{"version": "v1", "created": "Thu, 18 Dec 2014 17:11:56 GMT"}], "update_date": "2015-08-26", "authors_parsed": [["Bonzi", "E. V.", ""], ["Grad", "G. B.", ""], ["Maggi", "A. M.", ""], ["Mu\u00f1\u00f3z", "M. R.", ""]]}, {"id": "1508.06486", "submitter": "Jonathan Kadmon", "authors": "Jonathan Kadmon and Haim Sompolinsky", "title": "Transition to chaos in random neuronal networks", "comments": "28 Pages, 12 Figures, 5 Appendices", "journal-ref": "Phys. Rev. X 5, 041030 (2015)", "doi": "10.1103/PhysRevX.5.041030", "report-no": null, "categories": "cond-mat.dis-nn nlin.CD q-bio.NC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Firing patterns in the central nervous system often exhibit strong temporal\nirregularity and heterogeneity in their time averaged response properties.\nPrevious studies suggested that these properties are outcome of an intrinsic\nchaotic dynamics. Indeed, simplified rate-based large neuronal networks with\nrandom synaptic connections are known to exhibit sharp transition from fixed\npoint to chaotic dynamics when the synaptic gain is increased. However, the\nexistence of a similar transition in neuronal circuit models with more\nrealistic architectures and firing dynamics has not been established.\n  In this work we investigate rate based dynamics of neuronal circuits composed\nof several subpopulations and random connectivity. Nonzero connections are\neither positive-for excitatory neurons, or negative for inhibitory ones, while\nsingle neuron output is strictly positive; in line with known constraints in\nmany biological systems. Using Dynamic Mean Field Theory, we find the phase\ndiagram depicting the regimes of stable fixed point, unstable dynamic and\nchaotic rate fluctuations. We characterize the properties of systems near the\nchaotic transition and show that dilute excitatory-inhibitory architectures\nexhibit the same onset to chaos as a network with Gaussian connectivity.\nInterestingly, the critical properties near transition depend on the shape of\nthe single- neuron input-output transfer function near firing threshold.\nFinally, we investigate network models with spiking dynamics. When synaptic\ntime constants are slow relative to the mean inverse firing rates, the network\nundergoes a sharp transition from fast spiking fluctuations and static firing\nrates to a state with slow chaotic rate fluctuations. When the synaptic time\nconstants are finite, the transition becomes smooth and obeys scaling\nproperties, similar to crossover phenomena in statistical mechanics\n", "versions": [{"version": "v1", "created": "Wed, 26 Aug 2015 13:32:45 GMT"}], "update_date": "2015-11-25", "authors_parsed": [["Kadmon", "Jonathan", ""], ["Sompolinsky", "Haim", ""]]}, {"id": "1508.06576", "submitter": "Leon Gatys", "authors": "Leon A. Gatys, Alexander S. Ecker and Matthias Bethge", "title": "A Neural Algorithm of Artistic Style", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.NE q-bio.NC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In fine art, especially painting, humans have mastered the skill to create\nunique visual experiences through composing a complex interplay between the\ncontent and style of an image. Thus far the algorithmic basis of this process\nis unknown and there exists no artificial system with similar capabilities.\nHowever, in other key areas of visual perception such as object and face\nrecognition near-human performance was recently demonstrated by a class of\nbiologically inspired vision models called Deep Neural Networks. Here we\nintroduce an artificial system based on a Deep Neural Network that creates\nartistic images of high perceptual quality. The system uses neural\nrepresentations to separate and recombine content and style of arbitrary\nimages, providing a neural algorithm for the creation of artistic images.\nMoreover, in light of the striking similarities between performance-optimised\nartificial neural networks and biological vision, our work offers a path\nforward to an algorithmic understanding of how humans create and perceive\nartistic imagery.\n", "versions": [{"version": "v1", "created": "Wed, 26 Aug 2015 17:14:42 GMT"}, {"version": "v2", "created": "Wed, 2 Sep 2015 08:24:59 GMT"}], "update_date": "2015-09-03", "authors_parsed": [["Gatys", "Leon A.", ""], ["Ecker", "Alexander S.", ""], ["Bethge", "Matthias", ""]]}, {"id": "1508.06579", "submitter": "Yuri A. Dabaghian", "authors": "Yuri Dabaghian", "title": "Geometry of Spatial Memory Replay", "comments": "15 pages, 5 figures, Neural Computation, 2016", "journal-ref": null, "doi": null, "report-no": null, "categories": "q-bio.NC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Place cells in the rat hippocampus play a key role in creating the animal's\ninternal representation of the world. During active navigation, these cells\nspike only in discrete locations, together encoding a map of the environment.\nElectrophysiological recordings have shown that the animal can revisit this map\nmentally, during both sleep and awake states, reactivating the place cells that\nfired during its exploration in the same sequence they were originally\nactivated. Although consistency of place cell activity during active navigation\nis arguably enforced by sensory and proprioceptive inputs, it remains unclear\nhow a consistent representation of space can be maintained during spontaneous\nreplay. We propose a model that can account for this phenomenon and suggests\nthat a spatially consistent replay requires a number of constraints on the\nhippocampal network that affect its synaptic architecture and the statistics of\nsynaptic connection strengths.\n", "versions": [{"version": "v1", "created": "Wed, 26 Aug 2015 17:28:48 GMT"}, {"version": "v2", "created": "Sun, 20 Mar 2016 18:43:16 GMT"}], "update_date": "2016-03-22", "authors_parsed": [["Dabaghian", "Yuri", ""]]}, {"id": "1508.06818", "submitter": "Anna Kutschireiter", "authors": "Anna Kutschireiter, Simone Carlo Surace, Henning Sprekeler,\n  Jean-Pascal Pfister", "title": "The Neural Particle Filter", "comments": null, "journal-ref": null, "doi": "10.1038/s41598-017-17246-9.", "report-no": null, "categories": "q-bio.NC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The robust estimation of dynamically changing features, such as the position\nof prey, is one of the hallmarks of perception. On an abstract, algorithmic\nlevel, nonlinear Bayesian filtering, i.e. the estimation of temporally changing\nsignals based on the history of observations, provides a mathematical framework\nfor dynamic perception in real time. Since the general, nonlinear filtering\nproblem is analytically intractable, particle filters are considered among the\nmost powerful approaches to approximating the solution numerically. Yet, these\nalgorithms prevalently rely on importance weights, and thus it remains an\nunresolved question how the brain could implement such an inference strategy\nwith a neuronal population. Here, we propose the Neural Particle Filter (NPF),\na weight-less particle filter that can be interpreted as the neuronal dynamics\nof a recurrently connected neural network that receives feed-forward input from\nsensory neurons and represents the posterior probability distribution in terms\nof samples. Specifically, this algorithm bridges the gap between the\ncomputational task of online state estimation and an implementation that allows\nnetworks of neurons in the brain to perform nonlinear Bayesian filtering. The\nmodel captures not only the properties of temporal and multisensory integration\naccording to Bayesian statistics, but also allows online learning with a\nmaximum likelihood approach. With an example from multisensory integration, we\ndemonstrate that the numerical performance of the model is adequate to account\nfor both filtering and identification problems. Due to the weightless approach,\nour algorithm alleviates the 'curse of dimensionality' and thus outperforms\nconventional, weighted particle filters in higher dimensions for a limited\nnumber of particles.\n", "versions": [{"version": "v1", "created": "Thu, 27 Aug 2015 11:58:55 GMT"}, {"version": "v2", "created": "Wed, 30 Nov 2016 21:00:09 GMT"}], "update_date": "2019-11-19", "authors_parsed": [["Kutschireiter", "Anna", ""], ["Surace", "Simone Carlo", ""], ["Sprekeler", "Henning", ""], ["Pfister", "Jean-Pascal", ""]]}, {"id": "1508.06936", "submitter": "Alex Barnett", "authors": "Alex H. Barnett, Jeremy F. Magland, and Leslie F. Greengard", "title": "Validation of neural spike sorting algorithms without ground-truth\n  information", "comments": "22 pages, 7 figures; submitted to J. Neurosci. Meth", "journal-ref": null, "doi": null, "report-no": null, "categories": "q-bio.NC cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We describe a suite of validation metrics that assess the credibility of a\ngiven automatic spike sorting algorithm applied to a given electrophysiological\nrecording, when ground-truth is unavailable. By rerunning the spike sorter two\nor more times, the metrics measure stability under various perturbations\nconsistent with variations in the data itself, making no assumptions about the\nnoise model, nor about the internal workings of the sorting algorithm. Such\nstability is a prerequisite for reproducibility of results. We illustrate the\nmetrics on standard sorting algorithms for both in vivo and ex vivo recordings.\nWe believe that such metrics could reduce the significant human labor currently\nspent on validation, and should form an essential part of large-scale automated\nspike sorting and systematic benchmarking of algorithms.\n", "versions": [{"version": "v1", "created": "Thu, 27 Aug 2015 16:53:20 GMT"}], "update_date": "2015-08-28", "authors_parsed": [["Barnett", "Alex H.", ""], ["Magland", "Jeremy F.", ""], ["Greengard", "Leslie F.", ""]]}, {"id": "1508.06944", "submitter": "Yoram Burak", "authors": "Nimrod Shaham, Yoram Burak", "title": "Continuous parameter working memory in a balanced chaotic neural network", "comments": "Expanded and revised version of the manuscript. Accepted to PLoS\n  Computational Biology (2017). 29 pages, 8 figures and 4 supplementary figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cond-mat.dis-nn cs.NE q-bio.NC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  It has been proposed that neural noise in the cortex arises from chaotic\ndynamics in the balanced state: in this model of cortical dynamics, the\nexcitatory and inhibitory inputs to each neuron approximately cancel, and\nactivity is driven by fluctuations of the synaptic inputs around their mean. It\nremains unclear whether neural networks in the balanced state can perform tasks\nthat are highly sensitive to noise, such as storage of continuous parameters in\nworking memory, while also accounting for the irregular behavior of single\nneurons. Here we show that continuous parameter working memory can be\nmaintained in the balanced state, in a neural circuit with a simple network\narchitecture. We show analytically that in the limit of an infinite network,\nthe dynamics generated by this architecture are characterized by a continuous\nset of steady balanced states, allowing for the indefinite storage of a\ncontinuous parameter. In finite networks, we show that the chaotic noise drives\ndiffusive motion along the approximate attractor, which gradually degrades the\nstored memory. We analyze the dynamics and show that the slow diffusive motion\ninduces slowly decaying temporal cross correlations in the activity, which\ndiffer substantially from those previously described in the balanced state. We\ncalculate the diffusivity, and show that it is inversely proportional to the\nsystem size. For large enough (but realistic) neural population sizes, and with\nsuitable tuning of the network connections, the proposed balanced network can\nsustain continuous parameter values in memory over time scales larger by\nseveral orders of magnitude than the single neuron time scale.\n", "versions": [{"version": "v1", "created": "Thu, 27 Aug 2015 17:24:13 GMT"}, {"version": "v2", "created": "Tue, 1 Sep 2015 14:25:08 GMT"}, {"version": "v3", "created": "Thu, 7 Jan 2016 09:15:14 GMT"}, {"version": "v4", "created": "Thu, 27 Apr 2017 15:39:27 GMT"}], "update_date": "2017-04-28", "authors_parsed": [["Shaham", "Nimrod", ""], ["Burak", "Yoram", ""]]}, {"id": "1508.07857", "submitter": "Dmytro Grytskyy", "authors": "Dmytro Grytskyy, Markus Diesmann, Moritz Helias", "title": "A reaction diffusion-like formalism for plastic neural networks reveals\n  dissipative solitons at criticality", "comments": null, "journal-ref": "Phys. Rev. E 93, 062303 (2016)", "doi": "10.1103/PhysRevE.93.062303", "report-no": null, "categories": "cond-mat.dis-nn q-bio.NC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Self-organized structures in networks with spike-timing dependent plasticity\n(STDP) are likely to play a central role for information processing in the\nbrain. In the present study we derive a reaction-diffusion-like formalism for\nplastic feed-forward networks of nonlinear rate neurons with a correlation\nsensitive learning rule inspired by and being qualitatively similar to STDP.\nAfter obtaining equations that describe the change of the spatial shape of the\nsignal from layer to layer, we derive a criterion for the non-linearity\nnecessary to obtain stable dynamics for arbitrary input. We classify the\npossible scenarios of signal evolution and find that close to the transition to\nthe unstable regime meta-stable solutions appear. The form of these dissipative\nsolitons is determined analytically and the evolution and interaction of\nseveral such coexistent objects is investigated.\n", "versions": [{"version": "v1", "created": "Mon, 31 Aug 2015 14:48:24 GMT"}, {"version": "v2", "created": "Fri, 4 Sep 2015 12:36:57 GMT"}, {"version": "v3", "created": "Mon, 7 Dec 2015 23:58:14 GMT"}], "update_date": "2016-06-15", "authors_parsed": [["Grytskyy", "Dmytro", ""], ["Diesmann", "Markus", ""], ["Helias", "Moritz", ""]]}]