[{"id": "1307.0058", "submitter": "Kristina D Simmons", "authors": "K. D. Simmons, J. S. Prentice, G. Tkacik, J. Homann, H. K. Yee, S. E.\n  Palmer, P. C. Nelson, V. Balasubramanian", "title": "Transformation of stimulus correlations by the retina", "comments": "author list corrected in metadata", "journal-ref": null, "doi": "10.1371/journal.pcbi.1003344", "report-no": null, "categories": "q-bio.NC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Redundancies and correlations in the responses of sensory neurons seem to\nwaste neural resources but can carry cues about structured stimuli and may help\nthe brain to correct for response errors. To assess how the retina negotiates\nthis tradeoff, we measured simultaneous responses from populations of ganglion\ncells presented with natural and artificial stimuli that varied greatly in\ncorrelation structure. We found that pairwise correlations in the retinal\noutput remained similar across stimuli with widely different spatio-temporal\ncorrelations including white noise and natural movies. Meanwhile, purely\nspatial correlations tended to increase correlations in the retinal response.\nResponding to more correlated stimuli, ganglion cells had faster temporal\nkernels and tended to have stronger surrounds. These properties of individual\ncells, along with gain changes that opposed changes in effective contrast at\nthe ganglion cell input, largely explained the similarity of pairwise\ncorrelations across stimuli where receptive field measurements were possible.\n", "versions": [{"version": "v1", "created": "Sat, 29 Jun 2013 01:22:42 GMT"}, {"version": "v2", "created": "Tue, 2 Jul 2013 13:17:12 GMT"}], "update_date": "2014-03-05", "authors_parsed": [["Simmons", "K. D.", ""], ["Prentice", "J. S.", ""], ["Tkacik", "G.", ""], ["Homann", "J.", ""], ["Yee", "H. K.", ""], ["Palmer", "S. E.", ""], ["Nelson", "P. C.", ""], ["Balasubramanian", "V.", ""]]}, {"id": "1307.0131", "submitter": "Anita Mehta", "authors": "Pranay Goel and Anita Mehta", "title": "Learning theories reveal loss of pancreatic electrical connectivity in\n  diabetes as an adaptive response", "comments": "15 pages, 5 figures. To appear in PLoS One (2013)", "journal-ref": "PLoS ONE 8(8): e70366 (2013)", "doi": "10.1371/journal.pone.0070366", "report-no": null, "categories": "physics.bio-ph q-bio.CB q-bio.NC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Cells of almost all solid tissues are connected with gap junctions which\npermit the direct transfer of ions and small molecules, integral to regulating\ncoordinated function in the tissue. The pancreatic islets of Langerhans are\nresponsible for secreting the hormone insulin in response to glucose\nstimulation. Gap junctions are the only electrical contacts between the\nbeta-cells in the tissue of these excitable islets. It is generally believed\nthat they are responsible for synchrony of the membrane voltage oscillations\namong beta-cells, and thereby pulsatility of insulin secretion. Most attempts\nto understand connectivity in islets are often interpreted, bottom-up, in terms\nof measurements of gap junctional conductance. This does not, however explain\nsystematic changes, such as a diminished junctional conductance in type 2\ndiabetes. We attempt to address this deficit via the model presented here,\nwhich is a learning theory of gap junctional adaptation derived with analogy to\nneural systems. Here, gap junctions are modelled as bonds in a beta-cell\nnetwork, that are altered according to homeostatic rules of plasticity. Our\nanalysis reveals that it is nearly impossible to view gap junctions as\nhomogeneous across a tissue. A modified view that accommodates heterogeneity of\njunction strengths in the islet can explain why, for example, a loss of gap\njunction conductance in diabetes is necessary for an increase in plasma insulin\nlevels following hyperglycemia.\n", "versions": [{"version": "v1", "created": "Sat, 29 Jun 2013 17:05:41 GMT"}], "update_date": "2013-08-06", "authors_parsed": [["Goel", "Pranay", ""], ["Mehta", "Anita", ""]]}, {"id": "1307.0225", "submitter": "William Bialek", "authors": "Stephanie E. Palmer, Olivier Marre, Michael J. Berry II, and William\n  Bialek", "title": "Predictive information in a sensory population", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "q-bio.NC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Guiding behavior requires the brain to make predictions about future sensory\ninputs. Here we show that efficient predictive computation starts at the\nearliest stages of the visual system. We estimate how much information groups\nof retinal ganglion cells carry about the future state of their visual inputs,\nand show that every cell we can observe participates in a group of cells for\nwhich this predictive information is close to the physical limit set by the\nstatistical structure of the inputs themselves. Groups of cells in the retina\nalso carry information about the future state of their own activity, and we\nshow that this information can be compressed further and encoded by downstream\npredictor neurons, which then exhibit interesting feature selectivity.\nEfficient representation of predictive information is a candidate principle\nthat can be applied at each stage of neural computation.\n", "versions": [{"version": "v1", "created": "Sun, 30 Jun 2013 17:36:28 GMT"}], "update_date": "2013-07-02", "authors_parsed": [["Palmer", "Stephanie E.", ""], ["Marre", "Olivier", ""], ["Berry", "Michael J.", "II"], ["Bialek", "William", ""]]}, {"id": "1307.0597", "submitter": "Eleonora Catsigeras", "authors": "Eleonora Catsigeras", "title": "Dale's Principle is necessary for an optimal neuronal network's dynamics", "comments": "25 pages, 5 figures. The final revised version of this preprint will\n  appear in Applied Mathematics (Irvine), ISSN 2152-7385, Special Issue on\n  Biomathematics at http://www.scirp.org/journal/am/", "journal-ref": "Applied Mathematics (Irvine) ISSN: 21527385, Vol. 4 Issue 10B,\n  pp.: 15 - 29; 2013", "doi": "10.4236/am.2013.410A2002", "report-no": "Premat 2013/159", "categories": "physics.bio-ph q-bio.NC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study a mathematical model of biological neuronal networks composed by any\nfinite number $N \\geq 2$ of non necessarily identical cells. The model is a\ndeterministic dynamical system governed by finite-dimensional impulsive\ndifferential equations. The statical structure of the network is described by a\ndirected and weighted graph whose nodes are certain subsets of neurons, and\nwhose edges are the groups of synaptical connections among those subsets.\nFirst, we prove that among all the possible networks such that their respective\ngraphs are mutually isomorphic, there exists a dynamical optimum. This optimal\nnetwork exhibits the richest dynamics: namely, it is capable to show the most\ndiverse set of responses (i.e. orbits in the future) under external stimulus or\nsignals. Second, we prove that all the neurons of a dynamically optimal\nneuronal network necessarily satisfy Dale's Principle, i.e. each neuron must be\neither excitatory or inhibitory, but not mixed. So, Dale's Principle is a\nmathematical necessary consequence of a theoretic optimization process of the\ndynamics of the network. Finally, we prove that Dale's Principle is not\nsufficient for the dynamical optimization of the network.\n", "versions": [{"version": "v1", "created": "Tue, 2 Jul 2013 06:26:37 GMT"}], "update_date": "2013-11-19", "authors_parsed": [["Catsigeras", "Eleonora", ""]]}, {"id": "1307.0798", "submitter": "Georgi Medvedev S.", "authors": "Georgi S. Medvedev", "title": "Small-world networks of Kuramoto oscillators", "comments": null, "journal-ref": "Physica D 266, 13-22, 2014", "doi": "10.1016/j.physd.2013.09.008", "report-no": null, "categories": "nlin.AO math.OC q-bio.NC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The Kuramoto model of coupled phase oscillators on small-world (SW) graphs is\nanalyzed in this work. When the number of oscillators in the network goes to\ninfinity, the model acquires a family of steady state solutions of degree q,\ncalled q-twisted states. We show that this class of solutions plays an\nimportant role in the formation of spatial patterns in the Kuramoto model on SW\ngraphs. In particular, the analysis of q-twisted elucidates the role of\nlong-range random connections in shaping the attractors in this model.\n  We develop two complementary approaches for studying q-twisted states in the\ncoupled oscillator model on SW graphs: the linear stability analysis and the\nnumerical continuation. The former approach shows that long-range random\nconnections in the SW graphs promote synchronization and yields the estimate of\nthe synchronization rate as a function of the SW randomization parameter. The\ncontinuation shows that the increase of the long-range connections results in\npatterns consisting of one or several plateaus separated by sharp interfaces.\n  These results elucidate the pattern formation mechanisms in nonlocally\ncoupled dynamical systems on random graphs.\n", "versions": [{"version": "v1", "created": "Tue, 2 Jul 2013 19:22:52 GMT"}, {"version": "v2", "created": "Fri, 22 Nov 2013 20:31:32 GMT"}], "update_date": "2015-06-16", "authors_parsed": [["Medvedev", "Georgi S.", ""]]}, {"id": "1307.0969", "submitter": "Adri\\'an  Navas MSC", "authors": "Adri\\'an Navas, David Papo, Stefano Boccaletti, F. del-Pozo, Ricardo\n  Bajo, Fernando Maest\\'u, Pedro Gil, Irene Sendi\\~na-Nadal and Javier M.\n  Buld\\'u", "title": "Functional Hubs in Mild Cognitive Impairment", "comments": "12 pages, 5 figures, to appear in International Journal of\n  Bifurcations and Chaos, 2013", "journal-ref": null, "doi": null, "report-no": null, "categories": "q-bio.NC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We investigate how hubs of functional brain networks are modified as a result\nof mild cognitive impairment (MCI), a condition causing a slight but noticeable\ndecline in cognitive abilities, which sometimes precedes the onset of\nAlzheimer's disease. We used magnetoencephalography (MEG) to investigate the\nfunctional brain networks of a group of patients suffering from MCI and a\ncontrol group of healthy subjects, during the execution of a short-term memory\ntask. Couplings between brain sites were evaluated using synchronization\nlikelihood, from which a network of functional interdependencies was\nconstructed and the centrality, i.e. importance, of their nodes quantified. The\nresults showed that, with respect to healthy controls, MCI patients were\nassociated with decreases and increases in hub centrality respectively in\noccipital and central scalp regions, supporting the hypothesis that MCI\nmodifies functional brain network topology, leading to more random structures.\n", "versions": [{"version": "v1", "created": "Wed, 3 Jul 2013 11:16:56 GMT"}, {"version": "v2", "created": "Wed, 10 Jul 2013 13:08:49 GMT"}], "update_date": "2013-07-11", "authors_parsed": [["Navas", "Adri\u00e1n", ""], ["Papo", "David", ""], ["Boccaletti", "Stefano", ""], ["del-Pozo", "F.", ""], ["Bajo", "Ricardo", ""], ["Maest\u00fa", "Fernando", ""], ["Gil", "Pedro", ""], ["Sendi\u00f1a-Nadal", "Irene", ""], ["Buld\u00fa", "Javier M.", ""]]}, {"id": "1307.2129", "submitter": "Diego Fasoli", "authors": "D. Fasoli, O. Faugeras", "title": "Finite size effects in the correlation structure of stochastic neural\n  networks: analysis of different connectivity matrices and failure of the\n  mean-field theory", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.DS q-bio.NC stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We quantify the finite size effects in a stochastic network made up of rate\nneurons, for several kinds of recurrent connectivity matrices. This analysis is\nperformed by means of a perturbative expansion of the neural equations, where\nthe perturbative parameters are the intensities of the sources of randomness in\nthe system. In detail, these parameters are the variances of the background or\ninput noise, of the initial conditions and of the distribution of the synaptic\nweights. The technique developed in this article can be used to study systems\nwhich are invariant under the exchange of the neural indices and it allows us\nto quantify the correlation structure of the network, in terms of pairwise and\nhigher order correlations between the neurons. We also determine the relation\nbetween the correlation and the external input of the network, showing that\nstrong signals coming from the environment reduce significantly the amount of\ncorrelation between the neurons. Moreover we prove that in general the\nphenomenon of propagation of chaos does not occur, even in the thermodynamic\nlimit, due to the correlation structure of the 3 sources of randomness\nconsidered in the model. Furthermore, we show that the propagation of chaos\ndoes not depend only on the number of neurons in the network, but also and\nmainly on the number of incoming connections per neuron. To conclude, we prove\nthat for special values of the parameters of the system the neurons become\nperfectly correlated, a phenomenon that we have called stochastic\nsynchronization. These discoveries clearly prevent the use of the mean-field\ntheory in the description of the neural network.\n", "versions": [{"version": "v1", "created": "Mon, 8 Jul 2013 15:33:13 GMT"}], "update_date": "2013-07-09", "authors_parsed": [["Fasoli", "D.", ""], ["Faugeras", "O.", ""]]}, {"id": "1307.2150", "submitter": "Yaroslav Halchenko", "authors": "Yaroslav O. Halchenko, Michael Hanke, James V. Haxby, Stephen Jose\n  Hanson, Christoph S. Herrmann", "title": "Transmodal Analysis of Neural Signals", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "q-bio.NC cs.LG q-bio.QM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Localizing neuronal activity in the brain, both in time and in space, is a\ncentral challenge to advance the understanding of brain function. Because of\nthe inability of any single neuroimaging techniques to cover all aspects at\nonce, there is a growing interest to combine signals from multiple modalities\nin order to benefit from the advantages of each acquisition method. Due to the\ncomplexity and unknown parameterization of any suggested complete model of BOLD\nresponse in functional magnetic resonance imaging (fMRI), the development of a\nreliable ultimate fusion approach remains difficult. But besides the primary\ngoal of superior temporal and spatial resolution, conjoint analysis of data\nfrom multiple imaging modalities can alternatively be used to segregate neural\ninformation from physiological and acquisition noise. In this paper we suggest\na novel methodology which relies on constructing a quantifiable mapping of data\nfrom one modality (electroencephalography; EEG) into another (fMRI), called\ntransmodal analysis of neural signals (TRANSfusion). TRANSfusion attempts to\nmap neural data embedded within the EEG signal into its reflection in fMRI\ndata. Assessing the mapping performance on unseen data allows to localize brain\nareas where a significant portion of the signal could be reliably\nreconstructed, hence the areas neural activity of which is reflected in both\nEEG and fMRI data. Consecutive analysis of the learnt model allows to localize\nareas associated with specific frequency bands of EEG, or areas functionally\nrelated (connected or coherent) to any given EEG sensor. We demonstrate the\nperformance of TRANSfusion on artificial and real data from an auditory\nexperiment. We further speculate on possible alternative uses: cross-modal data\nfiltering and EEG-driven interpolation of fMRI signals to obtain arbitrarily\nhigh temporal sampling of BOLD.\n", "versions": [{"version": "v1", "created": "Mon, 8 Jul 2013 16:30:29 GMT"}], "update_date": "2013-07-09", "authors_parsed": [["Halchenko", "Yaroslav O.", ""], ["Hanke", "Michael", ""], ["Haxby", "James V.", ""], ["Hanson", "Stephen Jose", ""], ["Herrmann", "Christoph S.", ""]]}, {"id": "1307.2196", "submitter": "Dongjin Seo", "authors": "Dongjin Seo, Jose M. Carmena, Jan M. Rabaey, Elad Alon, and Michel M.\n  Maharbiz", "title": "Neural Dust: An Ultrasonic, Low Power Solution for Chronic Brain-Machine\n  Interfaces", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "q-bio.NC physics.ins-det", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A major hurdle in brain-machine interfaces (BMI) is the lack of an\nimplantable neural interface system that remains viable for a lifetime. This\npaper explores the fundamental system design trade-offs and ultimate size,\npower, and bandwidth scaling limits of neural recording systems built from\nlow-power CMOS circuitry coupled with ultrasonic power delivery and backscatter\ncommunication. In particular, we propose an ultra-miniature as well as\nextremely compliant system that enables massive scaling in the number of neural\nrecordings from the brain while providing a path towards truly chronic BMI.\nThese goals are achieved via two fundamental technology innovations: 1)\nthousands of 10 - 100 \\mu m scale, free-floating, independent sensor nodes, or\nneural dust, that detect and report local extracellular electrophysiological\ndata, and 2) a sub-cranial interrogator that establishes power and\ncommunication links with the neural dust.\n", "versions": [{"version": "v1", "created": "Mon, 8 Jul 2013 18:19:33 GMT"}], "update_date": "2013-07-09", "authors_parsed": [["Seo", "Dongjin", ""], ["Carmena", "Jose M.", ""], ["Rabaey", "Jan M.", ""], ["Alon", "Elad", ""], ["Maharbiz", "Michel M.", ""]]}, {"id": "1307.2544", "submitter": "Simona Mancini", "authors": "Jos\\'e A. Carrillo, St\\'ephane Cordier (MAPMO), Gustavo Deco, Simona\n  Mancini (MAPMO)", "title": "Complexity Reduction of Rate-Equations Models for Two-Choice\n  Decision-Making", "comments": null, "journal-ref": null, "doi": "10.1371/journal.pone.0080820", "report-no": null, "categories": "math.AP q-bio.NC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We are concerned with the complexity reduction of a stochastic system of\ndifferential equations governing the dynamics of a neuronal circuit describing\na decision-making task. This reduction is based on the slow-fast behavior of\nthe problem and holds on the whole phase space and not only locally around the\nspontaneous state. Macroscopic quantities, such as performance and reaction\ntimes, computed applying this reduction are in agreement with previous works in\nwhich the complexity reduction is locally performed at the spontaneous point by\nmeans of a Taylor expansion.\n", "versions": [{"version": "v1", "created": "Tue, 9 Jul 2013 19:21:08 GMT"}], "update_date": "2014-03-05", "authors_parsed": [["Carrillo", "Jos\u00e9 A.", "", "MAPMO"], ["Cordier", "St\u00e9phane", "", "MAPMO"], ["Deco", "Gustavo", "", "MAPMO"], ["Mancini", "Simona", "", "MAPMO"]]}, {"id": "1307.2730", "submitter": "Anna Cattani", "authors": "Claudio Canuto, Anna Cattani", "title": "The derivation of continuum limits of neuronal networks with\n  gap-junction couplings", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "q-bio.NC math.DS math.NA", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider an idealized network, formed by N neurons individually described\nby the FitzHugh-Nagumo equations and connected by electrical synapses. The\nlimit for N to infinity of the resulting discrete model is thoroughly\ninvestigated, with the aim of identifying a model for a continuum of neurons\nhaving an equivalent behaviour. Two strategies for passing to the limit are\nanalysed: i) a more conventional approach, based on a fixed nearest-neighbour\nconnection topology accompanied by a suitable scaling of the diffusion\ncoefficients; ii) a new approach, in which the number of connections to any\ngiven neuron varies with N according to a precise law, which simultaneously\nguarantees the non-triviality of the limit and the locality of neuronal\ninteractions. Both approaches yield in the limit a pde-based model, in which\nthe distribution of action potential obeys a nonlinear\nreaction-convection-diffusion equation; convection accounts for the possible\nlack of symmetry in the connection topology. Several convergence issues are\ndiscussed, both theoretically and numerically.\n", "versions": [{"version": "v1", "created": "Wed, 10 Jul 2013 09:41:27 GMT"}], "update_date": "2013-07-11", "authors_parsed": [["Canuto", "Claudio", ""], ["Cattani", "Anna", ""]]}, {"id": "1307.2798", "submitter": "Diego Fasoli", "authors": "D. Fasoli, O. Faugeras", "title": "Correlation structure of stochastic neural networks with generic\n  connectivity matrices", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "q-bio.NC math.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Using a perturbative expansion for weak synaptic weights and weak sources of\nrandomness, we calculate the correlation structure of neural networks with\ngeneric connectivity matrices. In detail, the perturbative parameters are the\nmean and the standard deviation of the synaptic weights, together with the\nstandard deviations of the background noise of the membrane potentials and of\ntheir initial conditions. We also show how to determine the correlation\nstructure of the system when the synaptic connections have a random topology.\nThis analysis is performed on rate neurons described by Wilson and Cowan\nequations, since this allows us to find analytic results. Moreover, the\nperturbative expansion can be developed at any order and for a generic\nconnectivity matrix. We finally show an example of application of this\ntechnique for a particular case of biologically relevant topology of the\nsynaptic connections.\n", "versions": [{"version": "v1", "created": "Wed, 10 Jul 2013 14:05:42 GMT"}], "update_date": "2013-07-11", "authors_parsed": [["Fasoli", "D.", ""], ["Faugeras", "O.", ""]]}, {"id": "1307.3235", "submitter": "Yu Hu", "authors": "Yu Hu, Joel Zylberberg, Eric Shea-Brown", "title": "The sign rule and beyond: Boundary effects, flexibility, and noise\n  correlations in neural population codes", "comments": "41 pages, 5 figures", "journal-ref": null, "doi": "10.1371/journal.pcbi.1003469", "report-no": null, "categories": "q-bio.NC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Over repeat presentations of the same stimulus, sensory neurons show variable\nresponses. This \"noise\" is typically correlated between pairs of cells, and a\nquestion with rich history in neuroscience is how these noise correlations\nimpact the population's ability to encode the stimulus. Here, we consider a\nvery general setting for population coding, investigating how information\nvaries as a function of noise correlations, with all other aspects of the\nproblem - neural tuning curves, etc. - held fixed. This work yields unifying\ninsights into the role of noise correlations. These are summarized in the form\nof theorems, and illustrated with numerical examples involving neurons with\ndiverse tuning curves. Our main contributions are as follows.\n  (1) We generalize previous results to prove a sign rule (SR) - if noise\ncorrelations between pairs of neurons have opposite signs vs. their signal\ncorrelations, then coding performance will improve compared to the independent\ncase. This holds for three different metrics of coding performance, and for\narbitrary tuning curves and levels of heterogeneity. This generality is true\nfor our other results as well.\n  (2) As also pointed out in the literature, the SR does not provide a\nnecessary condition for good coding. We show that a diverse set of correlation\nstructures can improve coding. Many of these violate the SR, as do\nexperimentally observed correlations. There is structure to this diversity: we\nprove that the optimal correlation structures must lie on boundaries of the\npossible set of noise correlations.\n  (3) We provide a novel set of necessary and sufficient conditions, under\nwhich the coding performance (in the presence of noise) will be as good as it\nwould be if there were no noise present at all.\n", "versions": [{"version": "v1", "created": "Thu, 11 Jul 2013 19:52:20 GMT"}, {"version": "v2", "created": "Fri, 12 Jul 2013 19:49:11 GMT"}, {"version": "v3", "created": "Thu, 22 Aug 2013 18:52:35 GMT"}, {"version": "v4", "created": "Wed, 15 Jan 2014 23:46:25 GMT"}], "update_date": "2015-06-16", "authors_parsed": [["Hu", "Yu", ""], ["Zylberberg", "Joel", ""], ["Shea-Brown", "Eric", ""]]}, {"id": "1307.3591", "submitter": "Rava A. da Silveira", "authors": "Rava Azeredo da Silveira and Michael J. Berry II", "title": "High-Fidelity Coding with Correlated Neurons", "comments": "Includes the Supplementary Material, as well as 7 figures and 3\n  supplementary figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "q-bio.NC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Positive correlations in the activity of neurons are widely observed in the\nbrain. Previous studies have shown these correlations to be detrimental to the\nfidelity of population codes or at best marginally favorable compared to\nindependent codes. Here, we show that positive correlations can enhance coding\nperformance by astronomical factors. Specifically, the probability of\ndiscrimination error can be suppressed by many orders of magnitude. Likewise,\nthe number of stimuli encoded--the capacity--can be enhanced by similarly large\nfactors. These effects do not necessitate unrealistic correlation values and\ncan occur for populations with a few tens of neurons. We further show that both\neffects benefit from heterogeneity commonly seen in population activity. Error\nsuppression and capacity enhancement rest upon a pattern of correlation. In the\nlimit of perfect coding, this pattern leads to a `lock-in' of response\nprobabilities that eliminates variability in the subspace relevant for stimulus\ndiscrimination. We discuss the nature of this pattern and suggest experimental\ntests to identify it.\n", "versions": [{"version": "v1", "created": "Fri, 12 Jul 2013 22:58:24 GMT"}, {"version": "v2", "created": "Wed, 17 Jul 2013 17:29:14 GMT"}, {"version": "v3", "created": "Fri, 19 Jul 2013 18:11:14 GMT"}], "update_date": "2013-07-22", "authors_parsed": [["da Silveira", "Rava Azeredo", ""], ["Berry", "Michael J.", "II"]]}, {"id": "1307.3941", "submitter": "Jose Fontanari", "authors": "Paulo F. C. Tilles and Jose F. Fontanari", "title": "Reinforcement and inference in cross-situational word learning", "comments": null, "journal-ref": "Front. Behav. Neurosci. 7:163 (2013)", "doi": "10.3389/fnbeh.2013.00163", "report-no": null, "categories": "q-bio.NC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Cross-situational word learning is based on the notion that a learner can\ndetermine the referent of a word by finding something in common across many\nobserved uses of that word. Here we propose an adaptive learning algorithm that\ncontains a parameter that controls the strength of the reinforcement applied to\nassociations between concurrent words and referents, and a parameter that\nregulates inference, which includes built-in biases, such as mutual\nexclusivity, and information of past learning events. By adjusting these\nparameters so that the model predictions agree with data from representative\nexperiments on cross-situational word learning, we were able to explain the\nlearning strategies adopted by the participants of those experiments in terms\nof a trade-off between reinforcement and inference. These strategies can vary\nwildly depending on the conditions of the experiments. For instance, for fast\nmapping experiments (i.e., the correct referent could, in principle, be\ninferred in a single observation) inference is prevalent, whereas for\nsegregated contextual diversity experiments (i.e., the referents are separated\nin groups and are exhibited with members of their groups only) reinforcement is\npredominant. Other experiments are explained with more balanced doses of\nreinforcement and inference.\n", "versions": [{"version": "v1", "created": "Mon, 15 Jul 2013 13:55:11 GMT"}], "update_date": "2013-11-20", "authors_parsed": [["Tilles", "Paulo F. C.", ""], ["Fontanari", "Jose F.", ""]]}, {"id": "1307.4171", "submitter": "Revati Shriram", "authors": "Revati Shriram, M. Sundhararajan, Nivedita Daimiwal", "title": "Human Brain Mapping based on COLD Signal Hemodynamic Response and\n  Electrical Neuroimaging", "comments": "5 pages. arXiv admin note: substantial text overlap with\n  arXiv:1212.3786", "journal-ref": null, "doi": "10.5120/10464-5175", "report-no": null, "categories": "cs.ET q-bio.NC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  To understand Working of Human Brain measurements related to the brain\nfunction are required. These measurements should be possibly non-invasive.\nBrain should be disturbed as less as possible during the measurement.\nIntegration of various modalities plays a vital role in understanding the\ncognitive and the behavioral changes in the human brain. It is an important\nsource of converging evidence about specific aspects of neural functions and\ndysfunctions under certain pathological conditions. Focal changes in cortical\nblood flow are tightly coupled with the changes in neuronal activity. This\nconstitutes the option to map the hemodynamic response and infer principles of\nthe cortical processing, even of complex tasks. The very high temporal\nresolution of EEG and good spatial resolution by NIRS make this concurrent\nmeasurement unique to study the spatio-temporal dynamics of large scale\nneuronal networks in the human brain. Such integration of two techniques will\nhelp to overcome the limitations of a specific method. Such as insensitivity of\nelectroencephalogram (EEG) to unsynchronized neural events or lack of near\ninfrared spectroscopy (NIRS) to low metabolic demand. A combination of EEG and\nNIRS will be more informative than the two separate analyses in both\nmodalities.\n", "versions": [{"version": "v1", "created": "Tue, 16 Jul 2013 06:06:03 GMT"}], "update_date": "2015-06-16", "authors_parsed": [["Shriram", "Revati", ""], ["Sundhararajan", "M.", ""], ["Daimiwal", "Nivedita", ""]]}, {"id": "1307.4685", "submitter": "Virginia Dominguez", "authors": "Samuel Johnson, Virginia Dominguez-Garcia, and Miguel A. Munoz", "title": "Factors determining nestedness in complex networks", "comments": "7 pages, 4 figures", "journal-ref": "PLoS ONE 8(9): e74025. 2013", "doi": "10.1371/journal.pone.0074025", "report-no": null, "categories": "physics.soc-ph cs.SI q-bio.MN q-bio.NC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Understanding the causes and effects of network structural features is a key\ntask in deciphering complex systems. In this context, the property of network\nnestedness has aroused a fair amount of interest as regards ecological\nnetworks. Indeed, Bastolla et al. introduced a simple measure of network\nnestedness which opened the door to analytical understanding, allowing them to\nconclude that biodiversity is strongly enhanced in highly nested mutualistic\nnetworks. Here, we suggest a slightly refined version of such a measure and go\non to study how it is influenced by the most basic structural properties of\nnetworks, such as degree distribution and degree-degree correlations (i.e.\nassortativity). We find that heterogeneity in the degree has a very strong\ninfluence on nestedness. Once such an influence has been discounted, we find\nthat nestedness is strongly correlated with disassortativity and hence, as\nrandom (neutral) networks have been recently found to be naturally\ndisassortative, they tend to be naturally nested just as the result of chance.\n", "versions": [{"version": "v1", "created": "Wed, 17 Jul 2013 16:18:23 GMT"}, {"version": "v2", "created": "Fri, 20 Sep 2013 11:32:03 GMT"}], "update_date": "2013-09-23", "authors_parsed": [["Johnson", "Samuel", ""], ["Dominguez-Garcia", "Virginia", ""], ["Munoz", "Miguel A.", ""]]}, {"id": "1307.5250", "submitter": "Kristian Weegink Mr", "authors": "Kristian J. Weegink, Paul A. Bellette, John J. Varghese, Peter A.\n  Silburn, Paul A. Meehan, Andrew P. Bradley", "title": "Efficient Micro-electrode Recording Modeling using a Filtered Point\n  Process", "comments": null, "journal-ref": "TNSRE-2014-00276.R1", "doi": "10.1109/TNSRE.2016.2573318", "report-no": null, "categories": "q-bio.NC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper we present an efficient model of the neuronal potentials\nrecorded by a deep brain stimulation microelectrode (DBS MER) in the\nsubthalamic nucleus. It is shown that a computationally efficient filtered\npoint process consisting of 10,000 neurons, including extracellular filtering\nclosely matches recordings from 13 Parkinson's disease patients. The recordings\nwere compared using their voltage amplitude distributions, power spectral\ndensity estimates and phase synchrony. It was found that interspike interval\ntimes modeled using a Weibull distribution with a shape parameter of 0.8,\nslightly non-Poisosnian, gave the best fit of the simulations to patient\nrecordings. These results indicate that part of the `background activity'\npresent in an DBS MER can be considered to be a very local field potential due\nto the surrounding neuronal activity.Therefore, the statistics of the\ninterspike interval times modify the structure of the background activity.\n", "versions": [{"version": "v1", "created": "Fri, 19 Jul 2013 15:03:05 GMT"}], "update_date": "2016-07-05", "authors_parsed": [["Weegink", "Kristian J.", ""], ["Bellette", "Paul A.", ""], ["Varghese", "John J.", ""], ["Silburn", "Peter A.", ""], ["Meehan", "Paul A.", ""], ["Bradley", "Andrew P.", ""]]}, {"id": "1307.5452", "submitter": "Johanne Hizanidis", "authors": "Johanne Hizanidis, Vasilis Kanas, Anastasios Bezerianos, Tassos\n  Bountis", "title": "Chimera states in networks of nonlocally coupled Hindmarsh-Rose neuron\n  models", "comments": null, "journal-ref": null, "doi": "10.1142/S0218127414500308", "report-no": null, "categories": "nlin.CD q-bio.NC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We have identified the occurrence of chimera states for various coupling\nschemes in networks of two-dimensional and three-dimensional Hindmarsh-Rose\noscillators, which represent realistic models of neuronal ensembles. This\nresult, together with recent studies on multiple chimera states in nonlocally\ncoupled FitzHugh-Nagumo oscillators, provide strong evidence that the\nphenomenon of chimeras may indeed be relevant in neuroscience applications.\nMoreover, our work verifies the existence of chimera states in coupled bistable\nelements, whereas to date chimeras were known to arise in models possessing a\nsingle stable limit cycle. Finally, we have identified an interesting class of\nmixed oscillatory states, in which desynchronized neurons are uniformly\ninterspersed among the remaining ones that are either stationary or oscillate\nin synchronized motion.\n", "versions": [{"version": "v1", "created": "Sat, 20 Jul 2013 19:11:36 GMT"}], "update_date": "2015-06-16", "authors_parsed": [["Hizanidis", "Johanne", ""], ["Kanas", "Vasilis", ""], ["Bezerianos", "Anastasios", ""], ["Bountis", "Tassos", ""]]}, {"id": "1307.5478", "submitter": "Andrew Whalen", "authors": "Andrew J. Whalen, Sean N. Brennan, Timothy D. Sauer and Steven J.\n  Schiff", "title": "Observability and Controllability of Nonlinear Networks: The Role of\n  Symmetry", "comments": "19 pages, 9 figures", "journal-ref": null, "doi": "10.1103/PhysRevX.5.011005", "report-no": null, "categories": "q-bio.NC nlin.CD q-bio.QM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Observability and controllability are essential concepts to the design of\npredictive observer models and feedback controllers of networked systems. For\nexample, noncontrollable mathematical models of real systems have subspaces\nthat influence model behavior, but cannot be controlled by an input. Such\nsubspaces can be difficult to determine in complex nonlinear networks. Since\nalmost all of the present theory was developed for linear networks without\nsymmetries, here we present a numerical and group representational framework,\nto quantify the observability and controllability of nonlinear networks with\nexplicit symmetries that shows the connection between symmetries and nonlinear\nmeasures of observability and controllability. We numerically observe and\ntheoretically predict that not all symmetries have the same effect on network\nobservation and control. Our analysis shows that the presence of symmetry in a\nnetwork may decrease observability and controllability, although networks\ncontaining only rotational symmetries remain controllable and observable. These\nresults alter our view of the nature of observability and controllability in\ncomplex networks, change our understanding of structural controllability, and\naffect the design of mathematical models to observe and control such networks.\n", "versions": [{"version": "v1", "created": "Sat, 20 Jul 2013 23:23:37 GMT"}, {"version": "v2", "created": "Mon, 5 Aug 2013 15:06:46 GMT"}, {"version": "v3", "created": "Mon, 6 Oct 2014 20:44:34 GMT"}], "update_date": "2018-04-18", "authors_parsed": [["Whalen", "Andrew J.", ""], ["Brennan", "Sean N.", ""], ["Sauer", "Timothy D.", ""], ["Schiff", "Steven J.", ""]]}, {"id": "1307.5684", "submitter": "Lucas Paletta", "authors": "Jason Satel, Ross Story, Matthew D. Hilchey, Zhiguo Wang and Raymond\n  M. Klein", "title": "Using a Dynamic Neural Field Model to Explore a Direct Collicular\n  Inhibition Account of Inhibition of Return", "comments": null, "journal-ref": null, "doi": null, "report-no": "ISACS/2013/01", "categories": "q-bio.NC cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  When the interval between a transient ash of light (a \"cue\") and a second\nvisual response signal (a \"target\") exceeds at least 200ms, responding is\nslowest in the direction indicated by the first signal. This phenomenon is\ncommonly referred to as inhibition of return (IOR). The dynamic neural field\nmodel (DNF) has proven to have broad explanatory power for IOR, effectively\ncapturing many empirical results. Previous work has used a short-term\ndepression (STD) implementation of IOR, but this approach fails to explain many\nbehavioral phenomena observed in the literature. Here, we explore a variant\nmodel of IOR involving a combination of STD and delayed direct collicular\ninhibition. We demonstrate that this hybrid model can better reproduce\nestablished behavioural results. We use the results of this model to propose\nseveral experiments that would yield particularly valuable insight into the\nnature of the neurophysiological mechanisms underlying IOR.\n", "versions": [{"version": "v1", "created": "Mon, 22 Jul 2013 12:47:18 GMT"}], "update_date": "2013-07-23", "authors_parsed": [["Satel", "Jason", ""], ["Story", "Ross", ""], ["Hilchey", "Matthew D.", ""], ["Wang", "Zhiguo", ""], ["Klein", "Raymond M.", ""]]}, {"id": "1307.5713", "submitter": "Lucas Paletta", "authors": "Min Zhao, Andre G. Marquez", "title": "Understanding Humans' Strategies in Maze Solving", "comments": null, "journal-ref": null, "doi": null, "report-no": "ISACS/2013/06", "categories": "cs.CV cs.AI q-bio.NC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Navigating through a visual maze relies on the strategic use of eye movements\nto select and identify the route. When navigating the maze, there are\ntrade-offs between exploring to the environment and relying on memory. This\nstudy examined strategies used to navigating through novel and familiar mazes\nthat were viewed from above and traversed by a mouse cursor. Eye and mouse\nmovements revealed two modes that almost never occurred concurrently:\nexploration and guidance. Analyses showed that people learned mazes and were\nable to devise and carry out complex, multi-faceted strategies that traded-off\nvisual exploration against active motor performance. These strategies took into\naccount available visual information, memory, confidence, the estimated cost in\ntime for exploration, and idiosyncratic tolerance for error. Understanding the\nstrategies humans used for maze solving is valuable for applications in\ncognitive neuroscience as well as in AI, robotics and human-robot interactions.\n", "versions": [{"version": "v1", "created": "Mon, 22 Jul 2013 13:57:10 GMT"}], "update_date": "2013-07-23", "authors_parsed": [["Zhao", "Min", ""], ["Marquez", "Andre G.", ""]]}, {"id": "1307.5728", "submitter": "Josef Ladenbauer", "authors": "Josef Ladenbauer, Moritz Augustin and Klaus Obermayer", "title": "How adaptation currents change threshold, gain and variability of\n  neuronal spiking", "comments": "20 pages, 8 figures; Journal of Neurophysiology (in press)", "journal-ref": null, "doi": "10.1152/jn.00586.2013", "report-no": null, "categories": "q-bio.NC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Many types of neurons exhibit spike rate adaptation, mediated by intrinsic\nslow $\\mathrm{K}^+$-currents, which effectively inhibit neuronal responses. How\nthese adaptation currents change the relationship between in-vivo like\nfluctuating synaptic input, spike rate output and the spike train statistics,\nhowever, is not well understood. In this computational study we show that an\nadaptation current which primarily depends on the subthreshold membrane voltage\nchanges the neuronal input-output relationship (I-O curve) subtractively,\nthereby increasing the response threshold. A spike-dependent adaptation current\nalters the I-O curve divisively, thus reducing the response gain. Both types of\nadaptation currents naturally increase the mean inter-spike interval (ISI), but\nthey can affect ISI variability in opposite ways. A subthreshold current always\ncauses an increase of variability while a spike-triggered current decreases\nhigh variability caused by fluctuation-dominated inputs and increases low\nvariability when the average input is large. The effects on I-O curves match\nthose caused by synaptic inhibition in networks with asynchronous irregular\nactivity, for which we find subtractive and divisive changes caused by external\nand recurrent inhibition, respectively. Synaptic inhibition, however, always\nincreases the ISI variability. We analytically derive expressions for the I-O\ncurve and ISI variability, which demonstrate the robustness of our results.\nFurthermore, we show how the biophysical parameters of slow\n$\\mathrm{K}^+$-conductances contribute to the two different types of adaptation\ncurrents and find that $\\mathrm{Ca}^{2+}$-activated $\\mathrm{K}^+$-currents are\neffectively captured by a simple spike-dependent description, while\nmuscarine-sensitive or $\\mathrm{Na}^+$-activated $\\mathrm{K}^+$-currents show a\ndominant subthreshold component.\n", "versions": [{"version": "v1", "created": "Mon, 22 Jul 2013 14:24:43 GMT"}, {"version": "v2", "created": "Thu, 7 Nov 2013 10:39:01 GMT"}], "update_date": "2013-11-08", "authors_parsed": [["Ladenbauer", "Josef", ""], ["Augustin", "Moritz", ""], ["Obermayer", "Klaus", ""]]}, {"id": "1307.6028", "submitter": "Daniela Andres Dr", "authors": "Daniela Sabrina Andres, Florian Gomez, Daniel Cerquetti, Marcelo\n  Merello and Ruedi Stoop", "title": "A hierarchical coding-window model of Parkinson's disease", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "q-bio.NC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Parkinson's disease is an ongoing challenge to theoretical neuroscience and\nto medical treatment. During the evolution of the disease, neurodegeneration\nleads to physiological and anatomical changes that affect the neuronal\ndischarge of the Basal Ganglia to an extent that impairs normal behavioral\npatterns. To investigate this problem, single Globus Pallidus pars interna\n(GPi) neurons of the 6-OHDA rat model of Parkinson's disease were\nextracellularly recorded at different degrees of alertness and compared to\nnon-Parkinson control neurons. A structure function analysis of these data\nrevealed that the temporal range of rate-coded information in GPi was\nsubstantially reduced in the Parkinson animal-model, suggesting that a\ndominance of small neighborhood dynamics could be the hallmark of Parkinson's\ndisease. A mathematical-model of the GPi circuit, where the small neighborhood\ncoupling is expressed in terms of a diffusion constant, corroborates this\ninterpretation.\n", "versions": [{"version": "v1", "created": "Tue, 23 Jul 2013 11:45:49 GMT"}, {"version": "v2", "created": "Fri, 6 Dec 2013 10:45:15 GMT"}, {"version": "v3", "created": "Thu, 6 Mar 2014 14:03:37 GMT"}], "update_date": "2014-03-07", "authors_parsed": [["Andres", "Daniela Sabrina", ""], ["Gomez", "Florian", ""], ["Cerquetti", "Daniel", ""], ["Merello", "Marcelo", ""], ["Stoop", "Ruedi", ""]]}, {"id": "1307.6445", "submitter": "Willy Wong", "authors": "Willy Wong", "title": "On the Rate Coding Response of Peripheral Sensory Neurons", "comments": null, "journal-ref": null, "doi": "10.1007/s00422-020-00848-4", "report-no": null, "categories": "q-bio.NC cond-mat.stat-mech physics.bio-ph", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The rate coding response of a single peripheral sensory neuron in the\nasymptotic, near-equilibrium limit can be derived using information theory,\nasymptotic Bayesian statistics and a theory of complex systems. Almost no\nbiological knowledge is required. The theoretical expression shows good\nagreement with spike-frequency adaptation data across different sensory\nmodalities and animal species. The approach permits the discovery of a new\nneurophysiological equation and shares similarities with statistical physics.\n", "versions": [{"version": "v1", "created": "Wed, 24 Jul 2013 14:58:55 GMT"}, {"version": "v2", "created": "Fri, 22 Nov 2013 13:59:42 GMT"}, {"version": "v3", "created": "Tue, 29 Mar 2016 08:04:16 GMT"}, {"version": "v4", "created": "Sun, 6 Oct 2019 00:36:41 GMT"}, {"version": "v5", "created": "Fri, 29 Nov 2019 13:41:03 GMT"}, {"version": "v6", "created": "Fri, 20 Dec 2019 17:02:01 GMT"}, {"version": "v7", "created": "Thu, 10 Dec 2020 20:06:35 GMT"}], "update_date": "2020-12-14", "authors_parsed": [["Wong", "Willy", ""]]}, {"id": "1307.6894", "submitter": "David Spivak", "authors": "Dylan Rupel and David I. Spivak", "title": "The operad of temporal wiring diagrams: formalizing a graphical language\n  for discrete-time processes", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.CT cs.PL q-bio.NC", "license": "http://creativecommons.org/licenses/by/3.0/", "abstract": "  We investigate the hierarchical structure of processes using the mathematical\ntheory of operads. Information or material enters a given process as a stream\nof inputs, and the process converts it to a stream of outputs. Output streams\ncan then be supplied to other processes in an organized manner, and the\nresulting system of interconnected processes can itself be considered a macro\nprocess. To model the inherent structure in this kind of system, we define an\noperad $\\mathcal{W}$ of black boxes and directed wiring diagrams, and we define\na $\\mathcal{W}$-algebra $\\mathcal{P}$ of processes (which we call propagators,\nafter Radul and Sussman). Previous operadic models of wiring diagrams use\nundirected wires without length, useful for modeling static systems of\nconstraints, whereas we use directed wires with length, useful for modeling\ndynamic flows of information. We give multiple examples throughout to ground\nthe ideas.\n", "versions": [{"version": "v1", "created": "Thu, 25 Jul 2013 23:33:24 GMT"}], "update_date": "2013-07-29", "authors_parsed": [["Rupel", "Dylan", ""], ["Spivak", "David I.", ""]]}, {"id": "1307.6921", "submitter": "Yuriy Pershin", "authors": "Y. V. Pershin and M. Di Ventra", "title": "Memcapacitive neural networks", "comments": null, "journal-ref": "Electronics Letters 50, 141 (2014)", "doi": "10.1049/el.2013.2463", "report-no": null, "categories": "cond-mat.dis-nn cs.ET cs.NE q-bio.NC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We show that memcapacitive (memory capacitive) systems can be used as\nsynapses in artificial neural networks. As an example of our approach, we\ndiscuss the architecture of an integrate-and-fire neural network based on\nmemcapacitive synapses. Moreover, we demonstrate that the\nspike-timing-dependent plasticity can be simply realized with some of these\ndevices. Memcapacitive synapses are a low-energy alternative to memristive\nsynapses for neuromorphic computation.\n", "versions": [{"version": "v1", "created": "Fri, 26 Jul 2013 04:42:49 GMT"}], "update_date": "2016-06-24", "authors_parsed": [["Pershin", "Y. V.", ""], ["Di Ventra", "M.", ""]]}, {"id": "1307.7302", "submitter": "Thomas Dean", "authors": "Thomas Dean, Biafra Ahanonu, Mainak Chowdhury, Anjali Datta, Andre\n  Esteva, Daniel Eth, Nobie Redmon, Oleg Rumyantsev, Ysis Tarter", "title": "On the Technology Prospects and Investment Opportunities for Scalable\n  Neuroscience", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "q-bio.NC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Two major initiatives to accelerate research in the brain sciences have\nfocused attention on developing a new generation of scientific instruments for\nneuroscience. These instruments will be used to record static (structural) and\ndynamic (behavioral) information at unprecedented spatial and temporal\nresolution and report out that information in a form suitable for computational\nanalysis. We distinguish between recording - taking measurements of individual\ncells and the extracellular matrix - and reporting - transcoding, packaging and\ntransmitting the resulting information for subsequent analysis - as these\nrepresent very different challenges as we scale the relevant technologies to\nsupport simultaneously tracking the many neurons that comprise neural circuits\nof interest. We investigate a diverse set of technologies with the purpose of\nanticipating their development over the span of the next 10 years and\ncategorizing their impact in terms of short-term [1-2 years], medium-term [2-5\nyears] and longer-term [5-10 years] deliverables.\n", "versions": [{"version": "v1", "created": "Sat, 27 Jul 2013 20:25:00 GMT"}], "update_date": "2013-07-30", "authors_parsed": [["Dean", "Thomas", ""], ["Ahanonu", "Biafra", ""], ["Chowdhury", "Mainak", ""], ["Datta", "Anjali", ""], ["Esteva", "Andre", ""], ["Eth", "Daniel", ""], ["Redmon", "Nobie", ""], ["Rumyantsev", "Oleg", ""], ["Tarter", "Ysis", ""]]}, {"id": "1307.7342", "submitter": "Tomasz Rutkowski", "authors": "Hiromu Mori, Shoji Makino, and Tomasz M. Rutkowski", "title": "Multi-command Chest Tactile Brain Computer Interface for Small Vehicle\n  Robot Navigation", "comments": "accepted as a full paper for The 2013 International Conference on\n  Brain and Health Informatics; to appear in Lecture Notes in Computer Science\n  (LNCS), Springer Verlag Berlin Heidelberg, 2013; http://link.springer.com/", "journal-ref": null, "doi": null, "report-no": null, "categories": "q-bio.NC cs.HC cs.RO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The presented study explores the extent to which tactile stimuli delivered to\nfive chest positions of a healthy user can serve as a platform for a brain\ncomputer interface (BCI) that could be used in an interactive application such\nas robotic vehicle operation. The five chest locations are used to evoke\ntactile brain potential responses, thus defining a tactile brain computer\ninterface (tBCI). Experimental results with five subjects performing online\ntBCI provide a validation of the chest location tBCI paradigm, while the\nfeasibility of the concept is illuminated through information-transfer rates.\nAdditionally an offline classification improvement with a linear SVM classifier\nis presented through the case study.\n", "versions": [{"version": "v1", "created": "Sun, 28 Jul 2013 08:24:20 GMT"}], "update_date": "2013-07-30", "authors_parsed": [["Mori", "Hiromu", ""], ["Makino", "Shoji", ""], ["Rutkowski", "Tomasz M.", ""]]}, {"id": "1307.7658", "submitter": "Daniel Larremore", "authors": "Daniel B. Larremore, Woodrow L. Shew, Edward Ott, Francesco\n  Sorrentino, Juan G. Restrepo", "title": "Inhibition causes ceaseless dynamics in networks of excitable nodes", "comments": "11 pages, 6 figures", "journal-ref": "Phys. Rev. Lett. 112, 138103 (2014)", "doi": "10.1103/PhysRevLett.112.138103", "report-no": null, "categories": "q-bio.NC cond-mat.dis-nn", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The collective dynamics of a network of excitable nodes changes dramatically\nwhen inhibitory nodes are introduced. We consider inhibitory nodes which may be\nactivated just like excitatory nodes but, upon activating, decrease the\nprobability of activation of network neighbors. We show that, although the\ndirect effect of inhibitory nodes is to decrease activity, the collective\ndynamics becomes self-sustaining. We explain this counterintuitive result by\ndefining and analyzing a \"branching function\" which may be thought of as an\nactivity-dependent branching ratio. The shape of the branching function implies\nthat for a range of global coupling parameters dynamics are self-sustaining.\nWithin the self-sustaining region of parameter space lies a critical line along\nwhich dynamics take the form of avalanches with universal scaling of size and\nduration, embedded in ceaseless timeseries of activity. Our analyses, confirmed\nby numerical simulation, suggest that inhibition may play a counterintuitive\nrole in excitable networks.\n", "versions": [{"version": "v1", "created": "Mon, 29 Jul 2013 17:36:18 GMT"}, {"version": "v2", "created": "Fri, 20 Dec 2013 20:49:45 GMT"}, {"version": "v3", "created": "Fri, 21 Feb 2014 17:20:05 GMT"}], "update_date": "2014-04-03", "authors_parsed": [["Larremore", "Daniel B.", ""], ["Shew", "Woodrow L.", ""], ["Ott", "Edward", ""], ["Sorrentino", "Francesco", ""], ["Restrepo", "Juan G.", ""]]}, {"id": "1307.7701", "submitter": "Michael Plaksin", "authors": "Michael Plaksin, Shy Shoham and Eitan Kimmel", "title": "Intramembrane Cavitation as a Predictive Bio-Piezoelectric Mechanism for\n  Ultrasonic Brain Stimulation", "comments": "One can find the supplemental material at the end of the PDF file", "journal-ref": "Phys. Rev. X 4, 011004 (2014)", "doi": "10.1103/PhysRevX.4.011004", "report-no": null, "categories": "q-bio.NC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Low-intensity ultrasonic waves can remotely and nondestructively excite\ncentral nervous system (CNS) neurons. While diverse applications for this\neffect are already emerging, the biophysical transduction mechanism underlying\nthis excitation remains unclear. Recently, we suggested that ultrasound-induced\nintramembrane cavitation within the bilayer membrane could underlie the\nbiomechanics of a range of observed acoustic bioeffects. In this paper, we show\nthat, in CNS neurons, ultrasound-induced cavitation of these nanometric bilayer\nsonophores can induce a complex mechanoelectrical interplay leading to\nexcitation, primarily through the effect of currents induced by membrane\ncapacitance changes. Our model explains the basic features of CNS\nacoustostimulation and predicts how the experimentally observed efficacy of\nmouse motor cortical ultrasonic stimulation depends on stimulation parameters.\nThese results support the hypothesis that neuronal intramembrane\npiezoelectricity underlies ultrasound-induced neurostimulation, and suggest\nthat other interactions between the nervous system and pressure waves or\nperturbations could be explained by this new mode of biological piezoelectric\ntransduction.\n", "versions": [{"version": "v1", "created": "Mon, 29 Jul 2013 19:45:35 GMT"}, {"version": "v2", "created": "Mon, 27 Jan 2014 20:35:23 GMT"}], "update_date": "2014-01-29", "authors_parsed": [["Plaksin", "Michael", ""], ["Shoham", "Shy", ""], ["Kimmel", "Eitan", ""]]}, {"id": "1307.7785", "submitter": "Dezhe Jin", "authors": "Aaron Miller and Dezhe Z. Jin", "title": "Potentiation Decay of Synapses and the Length Distributions of Synfire\n  Chains Self-organized in Recurrent Neural Networks", "comments": null, "journal-ref": null, "doi": "10.1103/PhysRevE.88.062716", "report-no": null, "categories": "q-bio.NC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Synfire chains are thought to underlie precisely-timed sequences of spikes\nobserved in various brain regions and across species. How they are formed is\nnot understood. Here we analyze self-organization of synfire chains through the\nspike-timing dependent plasticity (STDP) of the synapses, axon remodeling, and\npotentiation decay of synaptic weights in networks of neurons driven by noisy\nexternal inputs and subject to dominant feedback inhibition. Potentiation decay\nis the gradual, activity-independent reduction of synaptic weights over time.\nWe show that potentiation decay enables a dynamic and statistically stable\nnetwork connectivity when neurons spike spontaneously. Periodic stimulation of\na subset of neurons leads to formation of synfire chains through a random\nrecruitment process, which terminates when the chain connects to itself and\nforms a loop. We demonstrate that chain length distributions depend on the\npotentiation decay. Fast potentiation decay leads to long chains with wide\ndistributions, while slow potentiation decay leads to short chains with narrow\ndistributions. We suggest that the potentiation decay, which corresponds to the\ndecay of early long-term potentiation of synapses (E-LTP), is an important\nsynaptic plasticity rule in regulating formation of neural circuity through\nSTDP.\n", "versions": [{"version": "v1", "created": "Tue, 30 Jul 2013 02:17:14 GMT"}, {"version": "v2", "created": "Wed, 21 Aug 2013 00:45:47 GMT"}, {"version": "v3", "created": "Tue, 26 Nov 2013 21:16:52 GMT"}], "update_date": "2015-06-16", "authors_parsed": [["Miller", "Aaron", ""], ["Jin", "Dezhe Z.", ""]]}, {"id": "1307.7872", "submitter": "Claudius Gros", "authors": "Claudius Gros", "title": "Generating functionals for guided self-organization", "comments": "To be published in \"Guided Self-Organization: Inception\", Springer\n  Series on Emergence, Complexity and Computation, M. Prokopenko (ed),\n  Proceedings of Fifth International Workshop on Guided Self-Organization,\n  Sydney 2012", "journal-ref": "M. Prokopenko (ed.), Guided Self-Organization: Inception, 53-66,\n  Springer (2014)", "doi": null, "report-no": null, "categories": "q-bio.NC nlin.AO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Time evolution equations for dynamical systems can often be derived from\ngenerating functionals. Examples are Newton's equations of motion in classical\ndynamics which can be generated within the Lagrange or the Hamiltonian\nformalism. We propose that generating functionals for self-organizing complex\nsystems offer several advantages. Generating functionals allow to formulate\ncomplex dynamical systems systematically and the results obtained are typically\nvalid for classes of complex systems, as defined by the type of their\nrespective generating functionals. The generated dynamical systems tend, in\naddition, to be minimal, containing only few free and undetermined parameters.\nWe point out that two or more generating functionals may be used to define a\ncomplex system and that multiple generating function may not, and should not,\nbe combined into a single overall objective function. We provide and discuss\nexamples in terms of adapting neural networks.\n", "versions": [{"version": "v1", "created": "Tue, 30 Jul 2013 08:53:04 GMT"}], "update_date": "2014-04-23", "authors_parsed": [["Gros", "Claudius", ""]]}, {"id": "1307.7897", "submitter": "Samir  Avdakovic", "authors": "Ibrahim Omerhodzic, Samir Avdakovic, Amir Nuhanovic, Kemal Dizdarevic", "title": "Energy Distribution of EEG Signals: EEG Signal Wavelet-Neural Network\n  Classifier", "comments": null, "journal-ref": "World Academy of Science, Engineering and Technology, 61,\n  1190-1195, 2010", "doi": null, "report-no": null, "categories": "cs.NE q-bio.NC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, a wavelet-based neural network (WNN) classifier for\nrecognizing EEG signals is implemented and tested under three sets EEG signals\n(healthy subjects, patients with epilepsy and patients with epileptic syndrome\nduring the seizure). First, the Discrete Wavelet Transform (DWT) with the\nMulti-Resolution Analysis (MRA) is applied to decompose EEG signal at\nresolution levels of the components of the EEG signal (delta, theta, alpha,\nbeta and gamma) and the Parsevals theorem are employed to extract the\npercentage distribution of energy features of the EEG signal at different\nresolution levels. Second, the neural network (NN) classifies these extracted\nfeatures to identify the EEGs type according to the percentage distribution of\nenergy features. The performance of the proposed algorithm has been evaluated\nusing in total 300 EEG signals. The results showed that the proposed classifier\nhas the ability of recognizing and classifying EEG signals efficiently.\n", "versions": [{"version": "v1", "created": "Tue, 30 Jul 2013 10:30:21 GMT"}], "update_date": "2013-07-31", "authors_parsed": [["Omerhodzic", "Ibrahim", ""], ["Avdakovic", "Samir", ""], ["Nuhanovic", "Amir", ""], ["Dizdarevic", "Kemal", ""]]}, {"id": "1307.8398", "submitter": "Lars English", "authors": "Liam Timms and Lars Q. English", "title": "Synchronization in Phase-Coupled Kuramoto Oscillator Networks with\n  Axonal Delay and Synaptic Plasticity", "comments": "9 pages, 8 figures", "journal-ref": null, "doi": "10.1103/PhysRevE.89.032906", "report-no": null, "categories": "nlin.AO q-bio.NC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We explore both analytically and numerically an ensemble of coupled\nphase-oscillators governed by a Kuramoto-type system of differential equations.\nHowever, we have included the effects of time-delay (due to finite\nsignal-propagation speeds) and network plasticity (via dynamic coupling\nconstants) inspired by the Hebbian learning rule in neuroscience. When\ntime-delay and learning effects combine, novel synchronization phenomena are\nobserved. We investigate the formation of spatio-temporal patterns in both one-\nand two-dimensional oscillator lattices with periodic boundary conditions and\ncomment on the role of dimensionality.\n", "versions": [{"version": "v1", "created": "Wed, 31 Jul 2013 17:40:03 GMT"}], "update_date": "2015-06-16", "authors_parsed": [["Timms", "Liam", ""], ["English", "Lars Q.", ""]]}]