[{"id": "1701.00082", "submitter": "Stefano Cavallari", "authors": "Stefano Cavallari", "title": "A computational investigation of the relationships between single-neuron\n  and network dynamics in the cerebral cortex", "comments": "PhD thesis (integrate-and-fire neurons, recurrent neural networks,\n  current based neurons, conductance based neurons, LFP, EEG, information\n  encoding, GLM, Wiener filtering, PV-pos interneuron) 170 pages, 57 figures, 8\n  tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "q-bio.NC cs.IT math.IT q-bio.QM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Functions of brain areas in complex animals are believed to rely on the\ndynamics of networks of neurons rather than on single neurons. On the other\nhand, the network dynamics reflect and arise from the integration and\ncoordination of the activity of populations of single neurons. Understanding\nhow single-neurons and neural-circuits dynamics complement each other to\nproduce brain functions is thus of paramount importance. LFPs and EEGs are good\nindicators of the dynamics of mesoscopic and macroscopic populations of\nneurons, while microscopic-level activities can be documented by measuring the\nmembrane potential, the synaptic currents or the spiking activity of individual\nneurons. In this thesis we develop mathematical modelling and mathematical\nanalysis tools that can help the interpretation of joint measures of neural\nactivity at microscopic and mesoscopic or macroscopic scales. In particular, we\ndevelop network models of recurrent cortical circuits that can clarify the\nimpact of several aspects of single-neuron (i.e., microscopic-level) dynamics\non the activity of the whole neural population (as measured by LFP). We then\ndevelop statistical tools to characterize the relationship between the action\npotential firing of single neurons and mass signals. We apply these latter\nanalysis techniques to joint recordings of the firing activity of individual\ncell-type identified neurons and mesoscopic (i.e., LFP) and macroscopic (i.e.,\nEEG) signals in the mouse neocortex. We identified several general aspects of\nthe relationship between cell-specific neural firing and mass circuit activity,\nproviding for example general and robust mathematical rules which infer\nsingle-neuron firing activity from mass measures such as the LFP and the EEG.\n", "versions": [{"version": "v1", "created": "Sat, 31 Dec 2016 09:57:03 GMT"}], "update_date": "2017-01-03", "authors_parsed": [["Cavallari", "Stefano", ""]]}, {"id": "1701.00096", "submitter": "J\\'er\\'emy Guillon", "authors": "Jeremy Guillon, Yohan Attal, Olivier Colliot, Valentina La Corte,\n  Bruno Dubois, Denis Schwartz, Mario Chavez, Fabrizio De Vico Fallani", "title": "Loss of brain inter-frequency hubs in Alzheimer's disease", "comments": "27 pages, 6 figures, 3 tables, 3 supplementary figures", "journal-ref": "Scientific Reports, 7:10879, 2017", "doi": "10.1038/s41598-017-07846-w", "report-no": null, "categories": "q-bio.NC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Alzheimer's disease (AD) causes alterations of brain network structure and\nfunction. The latter consists of connectivity changes between oscillatory\nprocesses at different frequency channels. We proposed a multi-layer network\napproach to analyze multiple-frequency brain networks inferred from\nmagnetoencephalographic recordings during resting-states in AD subjects and\nage-matched controls. Main results showed that brain networks tend to\nfacilitate information propagation across different frequencies, as measured by\nthe multi-participation coefficient (MPC). However, regional connectivity in AD\nsubjects was abnormally distributed across frequency bands as compared to\ncontrols, causing significant decreases of MPC. This effect was mainly\nlocalized in association areas and in the cingulate cortex, which acted, in the\nhealthy group, as a true inter-frequency hub. MPC values significantly\ncorrelated with memory impairment of AD subjects, as measured by the total\nrecall score. Most predictive regions belonged to components of the\ndefault-mode network that are typically affected by atrophy, metabolism\ndisruption and amyloid-beta deposition. We evaluated the diagnostic power of\nthe MPC and we showed that it led to increased classification accuracy (78.39%)\nand sensitivity (91.11%). These findings shed new light on the brain functional\nalterations underlying AD and provide analytical tools for identifying\nmulti-frequency neural mechanisms of brain diseases.\n", "versions": [{"version": "v1", "created": "Sat, 31 Dec 2016 12:29:14 GMT"}, {"version": "v2", "created": "Tue, 3 Jan 2017 15:36:51 GMT"}, {"version": "v3", "created": "Wed, 8 Mar 2017 17:31:01 GMT"}, {"version": "v4", "created": "Fri, 10 Mar 2017 17:06:53 GMT"}], "update_date": "2017-10-17", "authors_parsed": [["Guillon", "Jeremy", ""], ["Attal", "Yohan", ""], ["Colliot", "Olivier", ""], ["La Corte", "Valentina", ""], ["Dubois", "Bruno", ""], ["Schwartz", "Denis", ""], ["Chavez", "Mario", ""], ["Fallani", "Fabrizio De Vico", ""]]}, {"id": "1701.00390", "submitter": "Paolo Del Giudice", "authors": "Gabriel Baglietto, Guido Gigante and Paolo Del Giudice", "title": "Density-based clustering: A 'landscape view' of multi-channel neural\n  data for inference and dynamic complexity analysis", "comments": null, "journal-ref": null, "doi": "10.1371/journal.pone.0174918", "report-no": null, "categories": "q-bio.NC cond-mat.dis-nn", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Simultaneous recordings from N electrodes generate N-dimensional time series\nthat call for efficient representations to expose relevant aspects of the\nunderlying dynamics. Binning the time series defines neural activity vectors\nthat populate the N-dimensional space as a density distribution, especially\ninformative when the neural dynamics performs a noisy path through metastable\nstates (often a case of interest in neuroscience); this makes clustering in the\nN-dimensional space a natural choice. We apply a variant of the 'mean-shift'\nalgorithm to perform such clustering, and validate it on an Hopfield network in\nthe glassy phase, in which metastable states are uncorrelated from memory\nattractors. The neural states identified as clusters' centroids are then used\nto define a parsimonious parametrization of the synaptic matrix, which allows a\nsignificant improvement in inferring the synaptic couplings from neural\nactivities. We next consider the more realistic case of a multi-modular spiking\nnetwork, with spike-frequency adaptation (SFA) inducing history-dependent\neffects; we develop a procedure, inspired by Boltzmann learning but extending\nits domain of application, to learn inter-module synaptic couplings so that the\nspiking network reproduces a prescribed pattern of spatial correlations. After\nclustering the activity generated by multi-modular spiking networks, we\nrepresent their multi-dimensional dynamics as the symbolic sequence of the\nclusters' centroids, which naturally lends itself to complexity estimates that\nprovide information on memory effects like those induced by SFA. To obtain a\nrelative complexity measure we compare the Lempel-Ziv complexity of the actual\ncentroid sequence to the one of Markov processes sharing the same transition\nprobabilities between centroids; as an illustration, we show that the\ndependence of such relative complexity on the time scale of SFA.\n", "versions": [{"version": "v1", "created": "Mon, 2 Jan 2017 14:02:24 GMT"}], "update_date": "2017-07-05", "authors_parsed": [["Baglietto", "Gabriel", ""], ["Gigante", "Guido", ""], ["Del Giudice", "Paolo", ""]]}, {"id": "1701.00648", "submitter": "Wilhelm Braun", "authors": "Wilhelm Braun and R\\\"udiger Thul", "title": "Sign-changes as a universal concept in first-passage time calculations", "comments": "7 pages, 6 figures. Accepted for publication in Phys. Rev. E", "journal-ref": null, "doi": "10.1103/PhysRevE.95.012114", "report-no": null, "categories": "q-bio.NC physics.data-an", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  First-passage time problems are ubiquitous across many fields of study\nincluding transport processes in semiconductors and biological synapses,\nevolutionary game theory and percolation. Despite their prominence,\nfirst-passage time calculations have proven to be particularly challenging.\nAnalytical results to date have often been obtained under strong conditions,\nleaving most of the exploration of first-passage time problems to direct\nnumerical computations. Here we present an analytical approach that allows the\nderivation of first-passage time distributions for the wide class of\nnon-differentiable Gaussian processes. We demonstrate that the concept of sign\nchanges naturally generalises the common practice of counting crossings to\ndetermine first-passage events. Our method works across a wide range of\ntime-dependent boundaries and noise strengths thus alleviating common hurdles\nin first-passage time calculations.\n", "versions": [{"version": "v1", "created": "Tue, 3 Jan 2017 11:06:40 GMT"}], "update_date": "2017-02-01", "authors_parsed": [["Braun", "Wilhelm", ""], ["Thul", "R\u00fcdiger", ""]]}, {"id": "1701.00765", "submitter": "Jordi Garcia-Ojalvo", "authors": "Belen Sancristobal, Beatriz Rebollo, Pol Boada, Maria V.\n  Sanchez-Vives, Jordi Garcia-Ojalvo", "title": "Collective Stochastic Coherence in Recurrent Neuronal Networks", "comments": "30 pages (including supplementary material), 6 figures", "journal-ref": "Nature Physics, vol. 12, 881-887 (2016)", "doi": "10.1038/nphys3739", "report-no": null, "categories": "nlin.AO cond-mat.stat-mech physics.bio-ph q-bio.NC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recurrent networks of dynamic elements frequently exhibit emergent collective\noscillations, which can display substantial regularity even when the individual\nelements are considerably noisy. How noise-induced dynamics at the local level\ncoexists with regular oscillations at the global level is still unclear. Here\nwe show that a combination of stochastic recurrence-based initiation with\ndeterministic refractoriness in an excitable network can reconcile these two\nfeatures, leading to maximum collective coherence for an intermediate noise\nlevel. We report this behavior in the slow oscillation regime exhibited by a\ncerebral cortex network under dynamical conditions resembling slow-wave sleep\nand anaesthesia. Computational analysis of a biologically realistic network\nmodel reveals that an intermediate level of background noise leads to\nquasi-regular dynamics. We verify this prediction experimentally in cortical\nslices subject to varying amounts of extracellular potassium, which modulates\nneuronal excitability and thus synaptic noise. The model also predicts that\nthis effectively regular state should exhibit noise-induced memory of the\nspatial propagation profile of the collective oscillations, which is also\nverified experimentally. Taken together, these results allow us to construe the\nenhanced regularity observed experimentally in the brain as an instance of\ncollective stochastic coherence.\n", "versions": [{"version": "v1", "created": "Tue, 3 Jan 2017 18:11:23 GMT"}], "update_date": "2017-01-04", "authors_parsed": [["Sancristobal", "Belen", ""], ["Rebollo", "Beatriz", ""], ["Boada", "Pol", ""], ["Sanchez-Vives", "Maria V.", ""], ["Garcia-Ojalvo", "Jordi", ""]]}, {"id": "1701.00838", "submitter": "Vishwa Goudar", "authors": "Vishwa Goudar and Dean Buonomano", "title": "Encoding Sensory and Motor Patterns as Time-Invariant Trajectories in\n  Recurrent Neural Networks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "q-bio.NC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Much of the information the brain processes and stores is temporal in nature\n- a spoken word or a handwritten signature, for example, is defined by how it\nunfolds in time. However, it remains unclear how neural circuits encode complex\ntime-varying patterns. We show that by tuning the weights of a recurrent neural\nnetwork (RNN), it can recognize and then transcribe spoken digits. The model\nelucidates how neural dynamics in cortical networks may resolve three\nfundamental challenges: first, encode multiple time-varying sensory and motor\npatterns as stable neural trajectories; second, generalize across relevant\nspatial features; third, identify the same stimuli played at different speeds -\nwe show that this temporal invariance emerges because the recurrent dynamics\ngenerate neural trajectories with appropriately modulated angular velocities.\nTogether our results generate testable predictions as to how recurrent networks\nmay use different mechanisms to generalize across the relevant spatial and\ntemporal features of complex time-varying stimuli.\n", "versions": [{"version": "v1", "created": "Tue, 3 Jan 2017 21:22:42 GMT"}, {"version": "v2", "created": "Sun, 13 Aug 2017 19:35:03 GMT"}], "update_date": "2017-08-15", "authors_parsed": [["Goudar", "Vishwa", ""], ["Buonomano", "Dean", ""]]}, {"id": "1701.00939", "submitter": "Dmitry Krotov", "authors": "Dmitry Krotov, John J Hopfield", "title": "Dense Associative Memory is Robust to Adversarial Inputs", "comments": null, "journal-ref": "Neural Computation Volume 30, Issue 12, December 2018 p.3151-3167", "doi": "10.1162/neco_a_01143", "report-no": null, "categories": "cs.LG cs.CR cs.CV q-bio.NC stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Deep neural networks (DNN) trained in a supervised way suffer from two known\nproblems. First, the minima of the objective function used in learning\ncorrespond to data points (also known as rubbish examples or fooling images)\nthat lack semantic similarity with the training data. Second, a clean input can\nbe changed by a small, and often imperceptible for human vision, perturbation,\nso that the resulting deformed input is misclassified by the network. These\nfindings emphasize the differences between the ways DNN and humans classify\npatterns, and raise a question of designing learning algorithms that more\naccurately mimic human perception compared to the existing methods.\n  Our paper examines these questions within the framework of Dense Associative\nMemory (DAM) models. These models are defined by the energy function, with\nhigher order (higher than quadratic) interactions between the neurons. We show\nthat in the limit when the power of the interaction vertex in the energy\nfunction is sufficiently large, these models have the following three\nproperties. First, the minima of the objective function are free from rubbish\nimages, so that each minimum is a semantically meaningful pattern. Second,\nartificial patterns poised precisely at the decision boundary look ambiguous to\nhuman subjects and share aspects of both classes that are separated by that\ndecision boundary. Third, adversarial images constructed by models with small\npower of the interaction vertex, which are equivalent to DNN with rectified\nlinear units (ReLU), fail to transfer to and fool the models with higher order\ninteractions. This opens up a possibility to use higher order models for\ndetecting and stopping malicious adversarial attacks. The presented results\nsuggest that DAM with higher order energy functions are closer to human visual\nperception than DNN with ReLUs.\n", "versions": [{"version": "v1", "created": "Wed, 4 Jan 2017 09:40:09 GMT"}], "update_date": "2018-11-27", "authors_parsed": [["Krotov", "Dmitry", ""], ["Hopfield", "John J", ""]]}, {"id": "1701.01101", "submitter": "John Medaglia", "authors": "John D. Medaglia, Danielle S. Bassett", "title": "Network Analyses and Nervous System Disorders", "comments": "38 pages, 9 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "q-bio.NC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Network analyses in nervous system disorders involves constructing and\nanalyzing anatomical and functional brain networks from neuroimaging data to\ndescribe and predict the clinical syndromes that result from neuropathology. A\nnetwork view of neurological disease and clinical syndromes facilitates\naccurate quantitative characterizations and mathematical models of complex\nnervous system disorders with relatively simple tools drawn from the field of\ngraph theory. Networks are predominantly constructed from in vivo data acquired\nusing physiological and neuroimaging techniques at the macroscale of nervous\nsystem organization. Studies support the emerging view that neuropsychiatric\nand neurological disorders result from pathological processes that disrupt the\nbrain's economically wired small-world organization. The lens of network\nscience offers theoretical insight into progressive neurodegeneration,\nneuropsychological dysfunction, and potential anatomical targets for\nintervensions ranging form pharmacological agents to brain stimulation.\n", "versions": [{"version": "v1", "created": "Wed, 4 Jan 2017 18:27:30 GMT"}], "update_date": "2017-01-05", "authors_parsed": [["Medaglia", "John D.", ""], ["Bassett", "Danielle S.", ""]]}, {"id": "1701.01150", "submitter": "Maxwell Bertolero", "authors": "M.A. Bertolero, B.T.T. Yeo, M. D'Esposito", "title": "The Diverse Club: The Integrative Core of Complex Networks", "comments": null, "journal-ref": null, "doi": "10.1038/s41467-017-01189-w", "report-no": null, "categories": "q-bio.NC physics.soc-ph", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A complex system can be represented and analyzed as a network, where nodes\nrepresent the units of the network and edges represent connections between\nthose units. For example, a brain network represents neurons as nodes and axons\nbetween neurons as edges. In many networks, some nodes have a\ndisproportionately high number of edges. These nodes also have many edges\nbetween each other, and are referred to as the rich club. In many different\nnetworks, the nodes of this club are assumed to support global network\nintegration. However, another set of nodes potentially exhibits a connectivity\nstructure that is more advantageous to global network integration. Here, in a\nmyriad of different biological and man-made networks, we discover the diverse\nclub--a set of nodes that have edges diversely distributed across the network.\nThe diverse club exhibits, to a greater extent than the rich club, properties\nconsistent with an integrative network function--these nodes are more highly\ninterconnected and their edges are more critical for efficient global\nintegration. Moreover, we present a generative evolutionary network model that\nproduces networks with a diverse club but not a rich club, thus demonstrating\nthat these two clubs potentially evolved via distinct selection pressures.\nGiven the variety of different networks that we analyzed--the c. elegans, the\nmacaque brain, the human brain, the United States power grid, and global air\ntraffic--the diverse club appears to be ubiquitous in complex networks. These\nresults warrant the distinction and analysis of two critical clubs of nodes in\nall complex systems.\n", "versions": [{"version": "v1", "created": "Wed, 4 Jan 2017 21:16:50 GMT"}, {"version": "v2", "created": "Tue, 27 Jun 2017 16:39:23 GMT"}], "update_date": "2017-11-03", "authors_parsed": [["Bertolero", "M. A.", ""], ["Yeo", "B. T. T.", ""], ["D'Esposito", "M.", ""]]}, {"id": "1701.01219", "submitter": "Geoffrey Goodhill", "authors": "Geoffrey J Goodhill", "title": "Is neuroscience facing up to statistical power?", "comments": "5 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "q-bio.NC stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  It has been demonstrated that the statistical power of many neuroscience\nstudies is very low, so that the results are unlikely to be robustly\nreproducible. How are neuroscientists and the journals in which they publish\nresponding to this problem? Here I review the sample size justifications\nprovided for all 15 papers published in one recent issue of the leading journal\nNature Neuroscience. Of these, only one claimed it was adequately powered. The\nothers mostly appealed to the sample sizes used in earlier studies, despite a\nlack of evidence that these earlier studies were adequately powered. Thus,\nconcerns regarding statistical power in neuroscience have mostly not yet been\naddressed.\n", "versions": [{"version": "v1", "created": "Thu, 5 Jan 2017 06:07:48 GMT"}], "update_date": "2017-01-06", "authors_parsed": [["Goodhill", "Geoffrey J", ""]]}, {"id": "1701.01311", "submitter": "Demian Wassermann", "authors": "Nahuel Lascano (ATHENA), Guillermo Gallardo (ATHENA), Rachid Deriche\n  (ATHENA), Dorian Mazauric (ABS), Demian Wassermann (ATHENA)", "title": "Extracting the Groupwise Core Structural Connectivity Network: Bridging\n  Statistical and Graph-Theoretical Approaches", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "q-bio.NC cs.DM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Finding the common structural brain connectivity network for a given\npopulation is an open problem, crucial for current neuro-science. Recent\nevidence suggests there's a tightly connected network shared between humans.\nObtaining this network will, among many advantages , allow us to focus\ncognitive and clinical analyses on common connections, thus increasing their\nstatistical power. In turn, knowledge about the common network will facilitate\nnovel analyses to understand the structure-function relationship in the brain.\nIn this work, we present a new algorithm for computing the core structural\nconnectivity network of a subject sample combining graph theory and statistics.\nOur algorithm works in accordance with novel evidence on brain topology. We\nanalyze the problem theoretically and prove its complexity. Using 309 subjects,\nwe show its advantages when used as a feature selection for connectivity\nanalysis on populations, outperforming the current approaches.\n", "versions": [{"version": "v1", "created": "Thu, 5 Jan 2017 13:44:59 GMT"}, {"version": "v2", "created": "Fri, 6 Jan 2017 15:41:49 GMT"}, {"version": "v3", "created": "Mon, 9 Jan 2017 14:21:38 GMT"}], "update_date": "2017-01-10", "authors_parsed": [["Lascano", "Nahuel", "", "ATHENA"], ["Gallardo", "Guillermo", "", "ATHENA"], ["Deriche", "Rachid", "", "ATHENA"], ["Mazauric", "Dorian", "", "ABS"], ["Wassermann", "Demian", "", "ATHENA"]]}, {"id": "1701.01315", "submitter": "Guillermo Gallardo", "authors": "Guillermo Gallardo (ATHENA), William Wells Iii (HMS), Rachid Deriche\n  (ATHENA), Demian Wassermann (ATHENA)", "title": "Groupwise Structural Parcellation of the Cortex: A Sound Approach Based\n  on Logistic Models", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP q-bio.NC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Current theories hold that brain function is highly related to long-range\nphysical connections through axonal bundles, namely extrinsic connectiv-ity.\nHowever, obtaining a groupwise cortical parcellation based on extrinsic\nconnectivity remains challenging. Current parcellation methods are\ncompu-tationally expensive; need tuning of several parameters or rely on ad-hoc\nconstraints. Furthermore, none of these methods present a model for the\ncortical extrinsic connectivity of the cortex. To tackle these problems, we\npropose a parsimonious model for the extrinsic connectivity and an efficient\nparceling technique based on clustering of tractograms. Our technique allows\nthe creation of single subject and groupwise parcellations of the whole cortex.\nThe parcellations obtained with our technique are in agreement with structural\nand functional parcellations in the literature. In particular, the motor and\nsensory cortex are subdivided in agreement with the human ho-munculus of\nPenfield. We illustrate this by comparing our resulting parcels with the motor\nstrip mapping included in the Human Connectome Project data.\n", "versions": [{"version": "v1", "created": "Thu, 5 Jan 2017 13:52:57 GMT"}], "update_date": "2017-01-06", "authors_parsed": [["Gallardo", "Guillermo", "", "ATHENA"], ["Wells", "William", "Iii", "HMS"], ["Deriche", "Rachid", "", "ATHENA"], ["Wassermann", "Demian", "", "ATHENA"]]}, {"id": "1701.01531", "submitter": "Evelyn Tang", "authors": "Evelyn Tang and Danielle S. Bassett", "title": "Control of Dynamics in Brain Networks", "comments": "Intended for a Colloquium in Rev. Mod. Phys", "journal-ref": null, "doi": "10.1103/RevModPhys.90.031003", "report-no": null, "categories": "q-bio.QM cond-mat.dis-nn cond-mat.soft physics.bio-ph q-bio.NC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The ability to effectively control brain dynamics holds great promise for the\nenhancement of cognitive function in humans, and the betterment of their\nquality of life. Yet, successfully controlling dynamics in neural systems is\nchallenging, in part due to the immense complexity of the brain and the large\nset of interactions that can drive any single change. While we have gained some\nunderstanding of the control of single neurons, the control of large-scale\nneural systems -- networks of multiply interacting components -- remains poorly\nunderstood. Efforts to address this gap include the construction of tools for\nthe control of brain networks, mostly adapted from control and dynamical\nsystems theory. Informed by current opportunities for practical intervention,\nthese theoretical contributions provide models that draw from a wide array of\nmathematical approaches. We present intriguing recent developments for\neffective strategies of control in dynamic brain networks, and we also describe\npotential mechanisms that underlie such processes. We review efforts in the\ncontrol of general neurophysiological processes with implications for brain\ndevelopment and cognitive function, as well as the control of altered\nneurophysiological processes in medical contexts such as anesthesia\nadministration, seizure suppression, and deep-brain stimulation for Parkinson's\ndisease. We conclude with a forward-looking discussion regarding how emerging\nresults from network control -- especially approaches that deal with nonlinear\ndynamics or more realistic trajectories for control transitions -- could be\nused to directly address pressing questions in neuroscience.\n", "versions": [{"version": "v1", "created": "Fri, 6 Jan 2017 03:14:45 GMT"}, {"version": "v2", "created": "Thu, 19 Jan 2017 15:17:18 GMT"}, {"version": "v3", "created": "Mon, 14 May 2018 22:52:03 GMT"}], "update_date": "2018-08-29", "authors_parsed": [["Tang", "Evelyn", ""], ["Bassett", "Danielle S.", ""]]}, {"id": "1701.01693", "submitter": "Justus Kromer", "authors": "Justus Kromer, Ali Khaledi-Nasab, Lutz Schimansky-Geier, Alexander B.\n  Neiman", "title": "Emergent stochastic oscillations and signal detection in tree networks\n  of excitable elements", "comments": "15 pages 8 figures", "journal-ref": "Scientific Reports 7, 3956 (2017)", "doi": "10.1038/s41598-017-04193-8", "report-no": null, "categories": "physics.bio-ph cond-mat.dis-nn nlin.AO q-bio.NC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study the stochastic dynamics of strongly-coupled excitable elements on a\ntree network. The peripheral nodes receive independent random inputs which may\ninduce large spiking events propagating through the branches of the tree and\nleading to global coherent oscillations in the network. This scenario may be\nrelevant to action potential generation in certain sensory neurons, which\npossess myelinated distal dendritic tree-like arbors with excitable nodes of\nRanvier at peripheral and branching nodes and exhibit noisy periodic sequences\nof action potentials.\n  We focus on the spiking statistics of the central node, which fires in\nresponse to a noisy input at peripheral nodes. We show that, in the strong\ncoupling regime, relevant to myelinated dendritic trees, the spike train\nstatistics can be predicted from an isolated excitable element with rescaled\nparameters according to the network topology. Furthermore, we show that by\nvarying the network topology the spike train statistics of the central node can\nbe tuned to have a certain firing rate and variability, or to allow for an\noptimal discrimination of inputs applied at the peripheral nodes.\n", "versions": [{"version": "v1", "created": "Fri, 6 Jan 2017 17:02:21 GMT"}], "update_date": "2017-07-14", "authors_parsed": [["Kromer", "Justus", ""], ["Khaledi-Nasab", "Ali", ""], ["Schimansky-Geier", "Lutz", ""], ["Neiman", "Alexander B.", ""]]}, {"id": "1701.01769", "submitter": "Weishun Zhong", "authors": "Weishun Zhong, David J. Schwab, Arvind Murugan", "title": "Associative pattern recognition through macro-molecular self-assembly", "comments": "13 pages, 7 figures; reference added", "journal-ref": null, "doi": "10.1007/s10955-017-1774-2", "report-no": null, "categories": "cond-mat.dis-nn cond-mat.stat-mech physics.bio-ph q-bio.NC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We show that macro-molecular self-assembly can recognize and classify\nhigh-dimensional patterns in the concentrations of $N$ distinct molecular\nspecies. Similar to associative neural networks, the recognition here leverages\ndynamical attractors to recognize and reconstruct partially corrupted patterns.\nTraditional parameters of pattern recognition theory, such as sparsity,\nfidelity, and capacity are related to physical parameters, such as nucleation\nbarriers, interaction range, and non-equilibrium assembly forces. Notably, we\nfind that self-assembly bears greater similarity to continuous attractor neural\nnetworks, such as place cell networks that store spatial memories, rather than\ndiscrete memory networks. This relationship suggests that features and\ntrade-offs seen here are not tied to details of self-assembly or neural network\nmodels but are instead intrinsic to associative pattern recognition carried out\nthrough short-ranged interactions.\n", "versions": [{"version": "v1", "created": "Fri, 6 Jan 2017 22:16:07 GMT"}, {"version": "v2", "created": "Fri, 24 Feb 2017 18:31:18 GMT"}], "update_date": "2017-04-26", "authors_parsed": [["Zhong", "Weishun", ""], ["Schwab", "David J.", ""], ["Murugan", "Arvind", ""]]}, {"id": "1701.02072", "submitter": "Hau-tieng Wu", "authors": "Antonio Cicone and Hau-Tieng Wu", "title": "How nonlinear-type time-frequency analysis can help in sensing\n  instantaneous heart rate and instantaneous respiratory rate from\n  photoplethysmography in a reliable way", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "q-bio.NC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Despite the population of the noninvasive, economic, comfortable, and\neasy-to-install photoplethysmography (PPG), it is still lacking a\nmathematically rigorous and stable algorithm which is able to simultaneously\nextract from a single-channel PPG signal the instantaneous heart rate (IHR) and\nthe instantaneous respiratory rate (IRR). In this paper, a novel algorithm\ncalled deppG is provided to tackle this challenge. deppG is composed of two\ntheoretically solid nonlinear-type time-frequency analyses techniques, the\nde-shape short time Fourier transform and the synchrosqueezing transform, which\nallows us to extract the instantaneous physiological information from the PPG\nsignal in a reliable way. To test its performance, in addition to validating\nthe algorithm by a simulated signal and discussing the meaning of\n\"instantaneous\", the algorithm is applied to two publicly available batch\ndatabases, the Capnobase and the ICASSP 2015 signal processing cup. The former\ncontains PPG signals relative to spontaneous or controlled breathing in static\npatients, and the latter is made up of PPG signals collected from subjects\ndoing intense physical activities. The accuracies of the estimated IHR and IRR\nare compared with the ones obtained by other methods, and represent the\nstate-of-the-art in this field of research. The results suggest the potential\nof deppG to extract instantaneous physiological information from a signal\nacquired from widely available wearable devices, even when a subject carries\nout intense physical activities.\n", "versions": [{"version": "v1", "created": "Mon, 9 Jan 2017 06:39:31 GMT"}, {"version": "v2", "created": "Fri, 1 Sep 2017 23:50:46 GMT"}], "update_date": "2017-09-05", "authors_parsed": [["Cicone", "Antonio", ""], ["Wu", "Hau-Tieng", ""]]}, {"id": "1701.02133", "submitter": "Michele Svanera", "authors": "Michele Svanera, Sergio Benini, Gal Raz, Talma Hendler, Rainer Goebel,\n  and Giancarlo Valente", "title": "Deep driven fMRI decoding of visual categories", "comments": "Presented at MLINI-2016 workshop, 2016 (arXiv:1701.01437)", "journal-ref": null, "doi": null, "report-no": "MLINI/2016/01", "categories": "stat.ML cs.LG q-bio.NC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Deep neural networks have been developed drawing inspiration from the brain\nvisual pathway, implementing an end-to-end approach: from image data to video\nobject classes. However building an fMRI decoder with the typical structure of\nConvolutional Neural Network (CNN), i.e. learning multiple level of\nrepresentations, seems impractical due to lack of brain data. As a possible\nsolution, this work presents the first hybrid fMRI and deep features decoding\napproach: collected fMRI and deep learnt representations of video object\nclasses are linked together by means of Kernel Canonical Correlation Analysis.\nIn decoding, this allows exploiting the discriminatory power of CNN by relating\nthe fMRI representation to the last layer of CNN (fc7). We show the\neffectiveness of embedding fMRI data onto a subspace related to deep features\nin distinguishing semantic visual categories based solely on brain imaging\ndata.\n", "versions": [{"version": "v1", "created": "Mon, 9 Jan 2017 11:06:39 GMT"}], "update_date": "2017-01-10", "authors_parsed": [["Svanera", "Michele", ""], ["Benini", "Sergio", ""], ["Raz", "Gal", ""], ["Hendler", "Talma", ""], ["Goebel", "Rainer", ""], ["Valente", "Giancarlo", ""]]}, {"id": "1701.02272", "submitter": "Tom Portegys", "authors": "Thomas E. Portegys", "title": "Morphognosis: the shape of knowledge in space and time", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "q-bio.NC cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Artificial intelligence research to a great degree focuses on the brain and\nbehaviors that the brain generates. But the brain, an extremely complex\nstructure resulting from millions of years of evolution, can be viewed as a\nsolution to problems posed by an environment existing in space and time. The\nenvironment generates signals that produce sensory events within an organism.\nBuilding an internal spatial and temporal model of the environment allows an\norganism to navigate and manipulate the environment. Higher intelligence might\nbe the ability to process information coming from a larger extent of\nspace-time. In keeping with nature's penchant for extending rather than\nreplacing, the purpose of the mammalian neocortex might then be to record\nevents from distant reaches of space and time and render them, as though yet\nnear and present, to the older, deeper brain whose instinctual roles have\nchanged little over eons. Here this notion is embodied in a model called\nmorphognosis (morpho = shape and gnosis = knowledge). Its basic structure is a\npyramid of event recordings called a morphognostic. At the apex of the pyramid\nare the most recent and nearby events. Receding from the apex are less recent\nand possibly more distant events. A morphognostic can thus be viewed as a\nstructure of progressively larger chunks of space-time knowledge. A set of\nmorphognostics forms long-term memories that are learned by exposure to the\nenvironment. A cellular automaton is used as the platform to investigate the\nmorphognosis model, using a simulated organism that learns to forage in its\nworld for food, build a nest, and play the game of Pong.\n", "versions": [{"version": "v1", "created": "Thu, 5 Jan 2017 23:10:54 GMT"}, {"version": "v2", "created": "Sat, 4 Mar 2017 18:20:10 GMT"}], "update_date": "2017-03-07", "authors_parsed": [["Portegys", "Thomas E.", ""]]}, {"id": "1701.02599", "submitter": "Vaibhav Wasnik", "authors": "Vaibhav Wasnik", "title": "Issues in data expansion in understanding criticality in biological\n  systems", "comments": "6 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "q-bio.NC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  At the point of a second order phase transition also termed as a critical\npoint, systems display long range order and their macroscopic behaviors are\nindependent of the microscopic details making up the system. Due to these\nproperties, it has long been speculated that biological systems that show\nsimilar behavior despite having very different microscopics, may be operating\nnear a critical point. Recent methods in neuroscience are making it possible to\nexplore whether criticality exists in neural networks. Despite being large in\nsize, many data sets are still only a minute sample of the neural system and\nmethods towards expanding these data sets have to be considered in order to\nstudy the existence of criticality. In this work we develop an analytical\nmethod of expanding a dataset to the large N limit so that statements about the\ncritical nature of the data set could be made. We also show using a particular\ndataset analyzed computationally in literature that expanding data sets keeping\nthe moments of the original data set need not lead to unique values of the\ncritical temperature when the large N limit is considered analytically, despite\nthe mirage of them appearing to do so when analyzed computationally. This\nsuggests that not all available data sets from experiments are amenable for\nunderstanding the critically of the underlying system.\n", "versions": [{"version": "v1", "created": "Tue, 10 Jan 2017 14:06:21 GMT"}, {"version": "v2", "created": "Mon, 17 Jul 2017 08:55:17 GMT"}], "update_date": "2017-07-18", "authors_parsed": [["Wasnik", "Vaibhav", ""]]}, {"id": "1701.02898", "submitter": "Riccardo Volpi", "authors": "Matteo Zanotto, Riccardo Volpi, Alessandro Maccione, Luca Berdondini,\n  Diego Sona, Vittorio Murino", "title": "Modeling Retinal Ganglion Cell Population Activity with Restricted\n  Boltzmann Machines", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV q-bio.NC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The retina is a complex nervous system which encodes visual stimuli before\nhigher order processing occurs in the visual cortex. In this study we evaluated\nwhether information about the stimuli received by the retina can be retrieved\nfrom the firing rate distribution of Retinal Ganglion Cells (RGCs), exploiting\nHigh-Density 64x64 MEA technology. To this end, we modeled the RGC population\nactivity using mean-covariance Restricted Boltzmann Machines, latent variable\nmodels capable of learning the joint distribution of a set of continuous\nobserved random variables and a set of binary unobserved random units. The idea\nwas to figure out if binary latent states encode the regularities associated to\ndifferent visual stimuli, as modes in the joint distribution. We measured the\ngoodness of mcRBM encoding by calculating the Mutual Information between the\nlatent states and the stimuli shown to the retina. Results show that binary\nstates can encode the regularities associated to different stimuli, using both\ngratings and natural scenes as stimuli. We also discovered that hidden\nvariables encode interesting properties of retinal activity, interpreted as\npopulation receptive fields. We further investigated the ability of the model\nto learn different modes in population activity by comparing results associated\nto a retina in normal conditions and after pharmacologically blocking GABA\nreceptors (GABAC at first, and then also GABAA and GABAB). As expected, Mutual\nInformation tends to decrease if we pharmacologically block receptors. We\nfinally stress that the computational method described in this work could\npotentially be applied to any kind of neural data obtained through MEA\ntechnology, though different techniques should be applied to interpret the\nresults.\n", "versions": [{"version": "v1", "created": "Wed, 11 Jan 2017 09:27:29 GMT"}, {"version": "v2", "created": "Tue, 17 Jan 2017 10:01:46 GMT"}], "update_date": "2017-01-18", "authors_parsed": [["Zanotto", "Matteo", ""], ["Volpi", "Riccardo", ""], ["Maccione", "Alessandro", ""], ["Berdondini", "Luca", ""], ["Sona", "Diego", ""], ["Murino", "Vittorio", ""]]}, {"id": "1701.02942", "submitter": "Thomas Nichols", "authors": "Thomas E. Nichols, Anders Eklund, Hans Knutsson", "title": "A defense of using resting state fMRI as null data for estimating false\n  positive rates", "comments": "Update: Title changed to be more informative, abstract expanded. Body\n  text unchanged", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP q-bio.NC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A recent Editorial by Slotnick (2017) reconsiders the findings of our paper\non the accuracy of false positive rate control with cluster inference in fMRI\n(Eklund et al, 2016), in particular criticising our use of resting state fMRI\ndata as a source for null data in the evaluation of task fMRI methods. We\ndefend this use of resting fMRI data, as while there is much structure in this\ndata, we argue it is representative of task data noise and as such analysis\nsoftware should be able to accommodate this noise. We also discuss a potential\nproblem with Slotnick's own method.\n", "versions": [{"version": "v1", "created": "Wed, 11 Jan 2017 12:12:33 GMT"}, {"version": "v2", "created": "Thu, 12 Jan 2017 13:36:18 GMT"}, {"version": "v3", "created": "Wed, 18 Jan 2017 13:33:16 GMT"}], "update_date": "2017-01-19", "authors_parsed": [["Nichols", "Thomas E.", ""], ["Eklund", "Anders", ""], ["Knutsson", "Hans", ""]]}, {"id": "1701.03164", "submitter": "Danielle Oliveira Costa Santos Dr.", "authors": "A.J. da Silva, S. Floquet, D.O.C. Santos", "title": "Statistical crossover and nonextensive behavior of the neuronal\n  short-term depression", "comments": "13 pages, 4 figures, J Biol Phys (2017)", "journal-ref": null, "doi": "10.1007/s10867-017-9474-3", "report-no": null, "categories": "q-bio.NC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The theoretical basis of neuronal coding, associated with short term\ndegradation in synaptic transmission, is a matter of debate in the literature.\nIn fact, electrophysiological signals are commonly characterized as inversely\nproportional to stimulus intensity. Among theoretical descriptions of this\nphenomenon, models based on $1/f$-dependency are employed to investigate the\nbiophysical properties of the short term synaptic depression. In this work we\nformulated a model based on a paradigmatic \\textit{q}-differential equation to\nobtain a generalized formalism useful for investigation of nonextensivity in\nthis specific type of synaptic plasticity. Our analysis reveals nonextensivity\nin data from electrophysiological recordings and also a statistical crossover\nin neurotransmission. In particular, statistical transitions providesadditional\nsupport to the hypothesis of heterogeneous release probability of\nneurotransmitters. On the other hand, the simple vesicle model agrees with data\nonly at low frequency stimulations. Thus, the present work presents a method to\ndemonstrate that short-term depression is not only governed by random\nmechanisms but also by a nonextensive behavior. Our findings also conciliate\nmorphological and electrophysiological investigations into a coherent\nbiophysical scenario.\n", "versions": [{"version": "v1", "created": "Tue, 27 Dec 2016 13:16:06 GMT"}, {"version": "v2", "created": "Sun, 5 Feb 2017 12:09:22 GMT"}, {"version": "v3", "created": "Tue, 14 Nov 2017 21:36:15 GMT"}, {"version": "v4", "created": "Fri, 17 Nov 2017 21:48:46 GMT"}], "update_date": "2017-11-21", "authors_parsed": [["da Silva", "A. J.", ""], ["Floquet", "S.", ""], ["Santos", "D. O. C.", ""]]}, {"id": "1701.04277", "submitter": "Anna Levina", "authors": "Anna Levina and Viola Priesemann", "title": "Subsampling scaling: a theory about inference from partly observed\n  systems", "comments": "33 pages, including 15 pages of supplementary", "journal-ref": "Nature Communications 8, 15140, 2017", "doi": "10.1038/ncomms15140", "report-no": null, "categories": "physics.data-an q-bio.NC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In real-world applications, observations are often constrained to a small\nfraction of a system. Such spatial subsampling can be caused by the\ninaccessibility or the sheer size of the system, and cannot be overcome by\nlonger sampling. Spatial subsampling can strongly bias inferences about a\nsystem's aggregated properties. To overcome the bias, we derive analytically a\nsubsampling scaling framework that is applicable to different observables,\nincluding distributions of neuronal avalanches, of number of people infected\nduring an epidemic outbreak, and of node degrees. We demonstrate how to infer\nthe correct distributions of the underlying full system, how to apply it to\ndistinguish critical from subcritical systems, and how to disentangle\nsubsampling and finite size effects. Lastly, we apply subsampling scaling to\nneuronal avalanche models and to recordings from developing neural networks. We\nshow that only mature, but not young networks follow power-law scaling,\nindicating self-organization to criticality during development.\n", "versions": [{"version": "v1", "created": "Mon, 16 Jan 2017 13:13:52 GMT"}], "update_date": "2017-06-02", "authors_parsed": [["Levina", "Anna", ""], ["Priesemann", "Viola", ""]]}, {"id": "1701.04404", "submitter": "Kevin Hannay", "authors": "Kevin M. Hannay, Daniel B. Forger, Victoria Booth", "title": "Macroscopic Models for Networks of Coupled Biological Oscillators", "comments": "9 pages, 5 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "q-bio.QM nlin.CD q-bio.NC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The study of synchronization in populations of coupled biological oscillators\nis fundamental to many areas of biology to include neuroscience, cardiac\ndynamics and circadian rhythms. Studying these systems may involve tracking the\nconcentration of hundreds of variables in thousands of individual cells\nresulting in an extremely high-dimensional description of the system. However,\nfor many of these systems the behaviors of interest occur on a collective or\nmacroscopic scale. We define a new macroscopic reduction for networks of\ncoupled oscillators motivated by an elegant structure we find in experimental\nmeasurements of circadian gene expression and several mathematical models for\ncoupled biological oscillators. We characterize the emergence of this structure\nthrough a simple argument and demonstrate its applicability to stochastic and\nheterogeneous systems of coupled oscillators. Finally, we perform the\nmacroscopic reduction for the heterogeneous stochastic Kuramoto equation and\ncompare the low-dimensional macroscopic model with numerical results from the\nhigh-dimensional microscopic model.\n", "versions": [{"version": "v1", "created": "Mon, 16 Jan 2017 13:55:09 GMT"}], "update_date": "2017-01-18", "authors_parsed": [["Hannay", "Kevin M.", ""], ["Forger", "Daniel B.", ""], ["Booth", "Victoria", ""]]}, {"id": "1701.04674", "submitter": "Ron Dekel", "authors": "Ron Dekel", "title": "Human perception in computer vision", "comments": "Under review as a conference paper at ICLR 2017", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV q-bio.NC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Computer vision has made remarkable progress in recent years. Deep neural\nnetwork (DNN) models optimized to identify objects in images exhibit\nunprecedented task-trained accuracy and, remarkably, some generalization\nability: new visual problems can now be solved more easily based on previous\nlearning. Biological vision (learned in life and through evolution) is also\naccurate and general-purpose. Is it possible that these different learning\nregimes converge to similar problem-dependent optimal computations? We\ntherefore asked whether the human system-level computation of visual perception\nhas DNN correlates and considered several anecdotal test cases. We found that\nperceptual sensitivity to image changes has DNN mid-computation correlates,\nwhile sensitivity to segmentation, crowding and shape has DNN end-computation\ncorrelates. Our results quantify the applicability of using DNN computation to\nestimate perceptual loss, and are consistent with the fascinating theoretical\nview that properties of human perception are a consequence of\narchitecture-independent visual learning.\n", "versions": [{"version": "v1", "created": "Tue, 17 Jan 2017 14:00:30 GMT"}], "update_date": "2017-01-18", "authors_parsed": [["Dekel", "Ron", ""]]}, {"id": "1701.04675", "submitter": "Alessandro Fontana", "authors": "Alessandro Fontana", "title": "VOCSMAT: a connectionist-inspired treatment proposal for relational\n  traumas", "comments": "23 pages, 12 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "q-bio.NC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Psychological traumas are thought to be present in a wide range of\nconditions, including post-traumatic stress disorder, disorganised attachment,\npersonality disorders, dissociative identity disorder and psychosis. This work\npresents a new psychotherapy for psychological traumas, based on a functional\nmodel of the mind, built with elements borrowed from the fields of computer\nscience, artificial intelligence and neural networks. The model revolves around\nthe concept of hierarchical value and explains the emergence of dissociation\nand splitting in response to emotional pain. The key intuition is that traumas\nare caused by too strong negative emotions, which are in turn made possible by\na low-value self, which is in turn determined by low-value self-associated\nideas. The therapeutic method compiles a list of patient's traumas, identifies\nfor each trauma a list of low-value self-associated ideas, and provides for\neach idea a list of counterexamples, to raise the self value and solve the\ntrauma. Since the psychotherapy proposed has not been clinically tested,\nstatements on its effectiveness are premature. However, since the conceptual\nbasis is solid and traumas are hypothesised to be present in many psychological\ndisorders, the potential gain may be substantial.\n", "versions": [{"version": "v1", "created": "Tue, 17 Jan 2017 14:01:49 GMT"}, {"version": "v2", "created": "Wed, 7 Mar 2018 13:20:49 GMT"}], "update_date": "2018-03-08", "authors_parsed": [["Fontana", "Alessandro", ""]]}, {"id": "1701.04782", "submitter": "Rossana Mastrandrea", "authors": "Rossana Mastrandrea, Andrea Gabrielli, Fabrizio Piras, Gianfranco\n  Spalletta, Guido Caldarelli and Tommaso Gili", "title": "Organization and hierarchy of the human functional brain network lead to\n  a chain-like core", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "q-bio.NC physics.bio-ph physics.soc-ph stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The brain is a paradigmatic example of a complex system as its functionality\nemerges as a global property of local mesoscopic and microscopic interactions.\nComplex network theory allows to elicit the functional architecture of the\nbrain in terms of links (correlations) between nodes (grey matter regions) and\nto extract information out of the noise. Here we present the analysis of\nfunctional magnetic resonance imaging data from forty healthy humans during the\nresting condition for the investigation of the basal scaffold of the functional\nbrain network organization. We show how brain regions tend to coordinate by\nforming a highly hierarchical chain-like structure of homogeneously clustered\nanatomical areas. A maximum spanning tree approach revealed the centrality of\nthe occipital cortex and the peculiar aggregation of cerebellar regions to form\na closed core. We also report the hierarchy of network segregation and the\nlevel of clusters integration as a function of the connectivity strength\nbetween brain regions.\n", "versions": [{"version": "v1", "created": "Tue, 17 Jan 2017 17:44:16 GMT"}], "update_date": "2017-01-18", "authors_parsed": [["Mastrandrea", "Rossana", ""], ["Gabrielli", "Andrea", ""], ["Piras", "Fabrizio", ""], ["Spalletta", "Gianfranco", ""], ["Caldarelli", "Guido", ""], ["Gili", "Tommaso", ""]]}, {"id": "1701.04893", "submitter": "Benjamin Dunn", "authors": "Benjamin Dunn, Daniel Wennberg, Ziwei Huang, Yasser Roudi", "title": "Grid cells show field-to-field variability and this explains the\n  aperiodic response of inhibitory interneurons", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "q-bio.NC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Research on network mechanisms and coding properties of grid cells assume\nthat the firing rate of a grid cell in each of its fields is the same.\nFurthermore, proposed network models predict spatial regularities in the firing\nof inhibitory interneurons that are inconsistent with experimental data. In\nthis paper, by analyzing the response of grid cells recorded from rats during\nfree navigation, we first show that there are strong variations in the mean\nfiring rate of the fields of individual grid cells and thus show that the data\nis inconsistent with the theoretical models that predict similar peak\nmagnitudes. We then build a two population excitatory-inhibitory network model\nin which sparse spatially selective input to the excitatory cells, presumed to\narise from e.g. salient external stimuli, hippocampus or a combination of both,\nleads to the variability in the firing field amplitudes of grid cells. We show\nthat, when combined with appropriate connectivity between the excitatory and\ninhibitory neurons, the variability in the firing field amplitudes of grid\ncells results in inhibitory neurons that do not exhibit regular spatial firing,\nconsistent with experimental data. Finally, we show that even if the spatial\npositions of the fields are maintained, variations in the firing rates of the\nfields of grid cells are enough to cause remapping of hippocampal cells.\n", "versions": [{"version": "v1", "created": "Tue, 17 Jan 2017 22:40:59 GMT"}], "update_date": "2017-01-19", "authors_parsed": [["Dunn", "Benjamin", ""], ["Wennberg", "Daniel", ""], ["Huang", "Ziwei", ""], ["Roudi", "Yasser", ""]]}, {"id": "1701.04905", "submitter": "Michael Schaub", "authors": "Yazan N. Billeh and Michael T. Schaub", "title": "Feedforward Architectures Driven by Inhibitory Interactions", "comments": "13 pages, 6 figures, J Comput Neurosci (2017)", "journal-ref": null, "doi": "10.1007/s10827-017-0669-1", "report-no": null, "categories": "q-bio.NC cond-mat.dis-nn nlin.PS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Directed information transmission is paramount for many social, physical, and\nbiological systems. For neural systems, scientists have studied this problem\nunder the paradigm of feedforward networks for decades. In most models of\nfeedforward networks, activity is exclusively driven by excitatory neurons and\nthe wiring patterns between them, while inhibitory neurons play only a\nstabilizing role for the network dynamics. Motivated by recent experimental\ndiscoveries of hippocampal circuitry, cortical circuitry, and the diversity of\ninhibitory neurons throughout the brain, here we illustrate that one can\nconstruct such networks even if the connectivity between the excitatory units\nin the system remains random. This is achieved by endowing inhibitory nodes\nwith a more active role in the network. Our findings demonstrate that apparent\nfeedforward activity can be caused by a much broader network-architectural\nbasis than often assumed.\n", "versions": [{"version": "v1", "created": "Wed, 18 Jan 2017 00:13:43 GMT"}, {"version": "v2", "created": "Fri, 20 Oct 2017 21:50:21 GMT"}, {"version": "v3", "created": "Fri, 17 Nov 2017 22:34:51 GMT"}], "update_date": "2017-11-21", "authors_parsed": [["Billeh", "Yazan N.", ""], ["Schaub", "Michael T.", ""]]}, {"id": "1701.05004", "submitter": "Ardavan Salehi Nobandegani", "authors": "Ardavan Salehi Nobandegani, Thomas R. Shultz", "title": "Converting Cascade-Correlation Neural Nets into Probabilistic Generative\n  Models", "comments": null, "journal-ref": "Proceedings of the 39th Annual Conference of the Cognitive Science\n  Society (2017) (pp. 1029-1034). Austin, TX: Cognitive Science Society", "doi": null, "report-no": null, "categories": "q-bio.NC cs.AI cs.NE stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Humans are not only adept in recognizing what class an input instance belongs\nto (i.e., classification task), but perhaps more remarkably, they can imagine\n(i.e., generate) plausible instances of a desired class with ease, when\nprompted. Inspired by this, we propose a framework which allows transforming\nCascade-Correlation Neural Networks (CCNNs) into probabilistic generative\nmodels, thereby enabling CCNNs to generate samples from a category of interest.\nCCNNs are a well-known class of deterministic, discriminative NNs, which\nautonomously construct their topology, and have been successful in giving\naccounts for a variety of psychological phenomena. Our proposed framework is\nbased on a Markov Chain Monte Carlo (MCMC) method, called the\nMetropolis-adjusted Langevin algorithm, which capitalizes on the gradient\ninformation of the target distribution to direct its explorations towards\nregions of high probability, thereby achieving good mixing properties. Through\nextensive simulations, we demonstrate the efficacy of our proposed framework.\n", "versions": [{"version": "v1", "created": "Wed, 18 Jan 2017 10:51:58 GMT"}], "update_date": "2021-07-02", "authors_parsed": [["Nobandegani", "Ardavan Salehi", ""], ["Shultz", "Thomas R.", ""]]}, {"id": "1701.05056", "submitter": "Elena Bertolotti", "authors": "Elena Bertolotti, Raffaella Burioni, Matteo di Volo, Alessandro\n  Vezzani", "title": "Synchronization and long-time memory in neural networks with inhibitory\n  hubs and synaptic plasticity", "comments": "11 pages, 11 figures", "journal-ref": "Phys. Rev. E 95, 012308 (2017)", "doi": "10.1103/PhysRevE.95.012308", "report-no": null, "categories": "cond-mat.dis-nn q-bio.NC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We investigate the dynamical role of inhibitory and highly connected nodes\n(hub) in synchronization and input processing of leaky-integrate-and-fire\nneural networks with short term synaptic plasticity. We take advantage of a\nheterogeneous mean-field approximation to encode the role of network structure\nand we tune the fraction of inhibitory neurons $f_I$ and their connectivity\nlevel to investigate the cooperation between hub features and inhibition. We\nshow that, depending on $f_I$, highly connected inhibitory nodes strongly drive\nthe synchronization properties of the overall network through dynamical\ntransitions from synchronous to asynchronous regimes. Furthermore, a metastable\nregime with long memory of external inputs emerges for a specific fraction of\nhub inhibitory neurons, underlining the role of inhibition and connectivity\nalso for input processing in neural networks.\n", "versions": [{"version": "v1", "created": "Wed, 18 Jan 2017 13:29:43 GMT"}], "update_date": "2017-01-25", "authors_parsed": [["Bertolotti", "Elena", ""], ["Burioni", "Raffaella", ""], ["di Volo", "Matteo", ""], ["Vezzani", "Alessandro", ""]]}, {"id": "1701.05080", "submitter": "Amos Korman", "authors": "Ofer Feinerman, Amos Korman\\\"e (CNRS, IRIF, GANG)", "title": "Individual versus collective cognition in social insects", "comments": null, "journal-ref": "The Journal of Experimental Biology, The Company of Biologists\n  2017, 220, pp.73 - 82", "doi": "10.1242/jeb.143891", "report-no": null, "categories": "q-bio.NC cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The concerted responses of eusocial insects to environmental stimuli are\noften referred to as collective cognition on the level of the colony.To achieve\ncollective cognitiona group can draw on two different sources: individual\ncognitionand the connectivity between individuals.Computation in\nneural-networks, for example,is attributedmore tosophisticated communication\nschemes than to the complexity of individual neurons. The case of social\ninsects, however, can be expected to differ. This is since individual insects\nare cognitively capable units that are often able to process information that\nis directly relevant at the level of the colony.Furthermore, involved\ncommunication patterns seem difficult to implement in a group of insects since\nthese lack clear network structure.This review discusses links between the\ncognition of an individual insect and that of the colony. We provide examples\nfor collective cognition whose sources span the full spectrum between\namplification of individual insect cognition and emergent group-level\nprocesses.\n", "versions": [{"version": "v1", "created": "Tue, 17 Jan 2017 12:43:17 GMT"}], "update_date": "2017-01-20", "authors_parsed": [["Feinerman", "Ofer", "", "CNRS, IRIF, GANG"], ["Korman\u00eb", "Amos", "", "CNRS, IRIF, GANG"]]}, {"id": "1701.05157", "submitter": "Satohiro Tajima", "authors": "Satohiro Tajima and Ryota Kanai", "title": "Integrated information and dimensionality in continuous attractor\n  dynamics", "comments": null, "journal-ref": "Neurosci Conscious 2017, 3 (1): nix011", "doi": "10.1093/nc/nix011", "report-no": null, "categories": "q-bio.NC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  There has been increasing interest in the integrated information theory (IIT)\nofconsciousness, which hypothesizes that consciousness is integrated\ninformation withinneuronal dynamics. However, the current formulation of IIT\nposes both practical andtheoretical problems when we aim to empirically test\nthe theory by computingintegrated information from neuronal signals. For\nexample, measuring integratedinformation requires observing all the elements in\nthe considered system at the sametime, but this is practically rather\ndifficult. In addition, the interpretation of the spatialpartition needed to\ncompute integrated information becomes vague in continuous time-series\nvariables due to a general property of nonlinear dynamical systems known\nas\"embedding.\" Here, we propose that some aspects of such problems are resolved\nbyconsidering the topological dimensionality of shared attractor dynamics as an\nindicatorof integrated information in continuous attractor dynamics. In this\nformulation, theeffects of unobserved nodes on the attractor dynamics can be\nreconstructed using atechnique called delay embedding, which allows us to\nidentify the dimensionality of anembedded attractor from partial observations.\nWe propose that the topologicaldimensionality represents a critical property of\nintegrated information, as it is invariantto general coordinate\ntransformations. We illustrate this new framework with simpleexamples and\ndiscuss how it fits together with recent findings based on neuralrecordings\nfrom awake and anesthetized animals. This topological approach extendsthe\nexisting notions of IIT to continuous dynamical systems and offers a\nmuch-neededframework for testing the theory with experimental data by\nsubstantially relaxing theconditions required for evaluating integrated\ninformation in real neural systems.\n", "versions": [{"version": "v1", "created": "Wed, 18 Jan 2017 17:34:36 GMT"}, {"version": "v2", "created": "Fri, 20 Jan 2017 18:03:30 GMT"}], "update_date": "2017-07-04", "authors_parsed": [["Tajima", "Satohiro", ""], ["Kanai", "Ryota", ""]]}, {"id": "1701.05363", "submitter": "Arthur Mensch", "authors": "Arthur Mensch (PARIETAL, NEUROSPIN), Julien Mairal (Thoth), Bertrand\n  Thirion (PARIETAL, NEUROSPIN), Gael Varoquaux (NEUROSPIN, PARIETAL)", "title": "Stochastic Subsampling for Factorizing Huge Matrices", "comments": "IEEE Transactions on Signal Processing, Institute of Electrical and\n  Electronics Engineers, A Para\\^itre", "journal-ref": "IEEE Transactions on Signal Processing, 2018, 66 (1), pp 113-128", "doi": "10.1109/TSP.2017.2752697", "report-no": null, "categories": "stat.ML cs.LG math.OC q-bio.NC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a matrix-factorization algorithm that scales to input matrices\nwith both huge number of rows and columns. Learned factors may be sparse or\ndense and/or non-negative, which makes our algorithm suitable for dictionary\nlearning, sparse component analysis, and non-negative matrix factorization. Our\nalgorithm streams matrix columns while subsampling them to iteratively learn\nthe matrix factors. At each iteration, the row dimension of a new sample is\nreduced by subsampling, resulting in lower time complexity compared to a simple\nstreaming algorithm. Our method comes with convergence guarantees to reach a\nstationary point of the matrix-factorization problem. We demonstrate its\nefficiency on massive functional Magnetic Resonance Imaging data (2 TB), and on\npatches extracted from hyperspectral images (103 GB). For both problems, which\ninvolve different penalties on rows and columns, we obtain significant\nspeed-ups compared to state-of-the-art algorithms.\n", "versions": [{"version": "v1", "created": "Thu, 19 Jan 2017 10:35:01 GMT"}, {"version": "v2", "created": "Wed, 26 Jul 2017 14:29:34 GMT"}, {"version": "v3", "created": "Mon, 30 Oct 2017 09:24:27 GMT"}], "update_date": "2017-11-15", "authors_parsed": [["Mensch", "Arthur", "", "PARIETAL, NEUROSPIN"], ["Mairal", "Julien", "", "Thoth"], ["Thirion", "Bertrand", "", "PARIETAL, NEUROSPIN"], ["Varoquaux", "Gael", "", "NEUROSPIN, PARIETAL"]]}, {"id": "1701.05685", "submitter": "Jonathan Rubin", "authors": "J.E. Rubin, B. Krauskopf, H.M. Osinga", "title": "Quantitative modeling and analysis of bifurcation-induced bursting", "comments": null, "journal-ref": "Phys. Rev. E 97, 012215 (2018)", "doi": "10.1103/PhysRevE.97.012215", "report-no": null, "categories": "math.DS nlin.CD q-bio.NC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Modeling and parameter estimation for neuronal dynamics are often challenging\nbecause many parameters can range over orders of magnitude and are difficult to\nmeasure experimentally. Moreover, selecting a suitable model complexity\nrequires a sufficient understanding of the model's potential use, such as\nhighlighting essential mechanisms underlying qualitative behavior or precisely\nquantifying realistic dynamics. We present a novel approach that can guide\nmodel development and tuning to achieve desired qualitative and quantitative\nsolution properties. Our approach relies on the presence of disparate time\nscales and employs techniques of separating the dynamics of fast and slow\nvariables, which are well known in the analysis of qualitative solution\nfeatures. We build on these methods to show how it is also possible to obtain\nquantitative solution features by imposing designed dynamics for the slow\nvariables in the form of specified two-dimensional paths in a\nbifurcation-parameter landscape.\n", "versions": [{"version": "v1", "created": "Fri, 20 Jan 2017 04:30:47 GMT"}], "update_date": "2018-01-31", "authors_parsed": [["Rubin", "J. E.", ""], ["Krauskopf", "B.", ""], ["Osinga", "H. M.", ""]]}, {"id": "1701.06333", "submitter": "Tony Lindeberg", "authors": "Tony Lindeberg", "title": "Normative theory of visual receptive fields", "comments": "15 pages, 17 figures. arXiv admin note: text overlap with\n  arXiv:1210.0754", "journal-ref": "Substantially revised version in Heliyon 7(1): e05897: 1-20, 2021", "doi": "10.1016/j.heliyon.2021.e05897", "report-no": null, "categories": "q-bio.NC cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This article gives an overview of a normative computational theory of visual\nreceptive fields, by which idealized functional models of early spatial,\nspatio-chromatic and spatio-temporal receptive fields can be derived in an\naxiomatic way based on structural properties of the environment in combination\nwith assumptions about the internal structure of a vision system to guarantee\nconsistent handling of image representations over multiple spatial and temporal\nscales. Interestingly, this theory leads to predictions about visual receptive\nfield shapes with qualitatively very good similarity to biological receptive\nfields measured in the retina, the LGN and the primary visual cortex (V1) of\nmammals.\n", "versions": [{"version": "v1", "created": "Mon, 23 Jan 2017 11:13:07 GMT"}, {"version": "v2", "created": "Mon, 30 Jan 2017 13:55:58 GMT"}, {"version": "v3", "created": "Tue, 7 Feb 2017 07:46:27 GMT"}, {"version": "v4", "created": "Thu, 16 Feb 2017 13:52:12 GMT"}], "update_date": "2021-01-25", "authors_parsed": [["Lindeberg", "Tony", ""]]}, {"id": "1701.06859", "submitter": "Laurent Perrinet", "authors": "Laurent Perrinet (INT)", "title": "Sparse models for Computer Vision", "comments": null, "journal-ref": "Biologically inspired computer vision, 2015, 9783527680863", "doi": "10.1002/9783527680863.ch14", "report-no": null, "categories": "cs.CV q-bio.NC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The representation of images in the brain is known to be sparse. That is, as\nneural activity is recorded in a visual area ---for instance the primary visual\ncortex of primates--- only a few neurons are active at a given time with\nrespect to the whole population. It is believed that such a property reflects\nthe efficient match of the representation with the statistics of natural\nscenes. Applying such a paradigm to computer vision therefore seems a promising\napproach towards more biomimetic algorithms. Herein, we will describe a\nbiologically-inspired approach to this problem. First, we will describe an\nunsupervised learning paradigm which is particularly adapted to the efficient\ncoding of image patches. Then, we will outline a complete multi-scale framework\n---SparseLets--- implementing a biologically inspired sparse representation of\nnatural images. Finally, we will propose novel methods for integrating prior\ninformation into these algorithms and provide some preliminary experimental\nresults. We will conclude by giving some perspective on applying such\nalgorithms to computer vision. More specifically, we will propose that\nbio-inspired approaches may be applied to computer vision using predictive\ncoding schemes, sparse models being one simple and efficient instance of such\nschemes.\n", "versions": [{"version": "v1", "created": "Tue, 24 Jan 2017 13:20:11 GMT"}], "update_date": "2017-01-25", "authors_parsed": [["Perrinet", "Laurent", "", "INT"]]}, {"id": "1701.07061", "submitter": "Diego Mateos", "authors": "D. M. Mateos, R. Guevara Erra, R. Wennberg, J.L. Perez Velazquez", "title": "Measures of Entropy and Complexity in altered states of consciousness", "comments": "2 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "q-bio.NC cond-mat.stat-mech", "license": "http://creativecommons.org/publicdomain/zero/1.0/", "abstract": "  Quantification of complexity in neurophysiological signals has been studied\nusing different methods, especially those from information or dynamical system\ntheory. These studies revealed the dependence on different states of\nconsciousness, particularly that wakefulness is characterized by larger\ncomplexity of brain signals perhaps due to the necessity of the brain to handle\nvaried sensorimotor information. Thus these frameworks are very useful in\nattempts at quantifying cognitive states. We set out to analyze different types\nof signals including scalp and intracerebral electroencephalography (EEG), and\nmagnetoencephalography (MEG) in subjects during different states of\nconsciousness: awake, sleep stages and epileptic seizures. The signals were\nanalyzed using a statistical (Permutation Entropy) and a deterministic\n(Permutation Lempel Ziv Complexity) analytical method. The results are\npresented in a complexity vs entropy graph, showing that the values of entropy\nand complexity of the signals tend to be greatest when the subjects are in\nfully alert states, falling in states with loss of awareness or consciousness.\nThese results are robust for all three types of recordings. We propose that the\ninvestigation of the structure of cognition using the frameworks of complexity\nwill reveal mechanistic aspects of brain dynamics associated not only with\naltered states of consciousness but also with normal and pathological\nconditions.\n", "versions": [{"version": "v1", "created": "Mon, 9 Jan 2017 20:10:15 GMT"}], "update_date": "2017-01-26", "authors_parsed": [["Mateos", "D. M.", ""], ["Erra", "R. Guevara", ""], ["Wennberg", "R.", ""], ["Velazquez", "J. L. Perez", ""]]}, {"id": "1701.07083", "submitter": "Tim Althoff", "authors": "Tim Althoff and Eric Horvitz and Ryen W. White and Jamie Zeitzer", "title": "Harnessing the Web for Population-Scale Physiological Sensing: A Case\n  Study of Sleep and Performance", "comments": "Published in Proceedings of WWW 2017", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC cs.CY cs.IR q-bio.NC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Human cognitive performance is critical to productivity, learning, and\naccident avoidance. Cognitive performance varies throughout each day and is in\npart driven by intrinsic, near 24-hour circadian rhythms. Prior research on the\nimpact of sleep and circadian rhythms on cognitive performance has typically\nbeen restricted to small-scale laboratory-based studies that do not capture the\nvariability of real-world conditions, such as environmental factors,\nmotivation, and sleep patterns in real-world settings. Given these limitations,\nleading sleep researchers have called for larger in situ monitoring of sleep\nand performance. We present the largest study to date on the impact of\nobjectively measured real-world sleep on performance enabled through a\nreframing of everyday interactions with a web search engine as a series of\nperformance tasks. Our analysis includes 3 million nights of sleep and 75\nmillion interaction tasks. We measure cognitive performance through the speed\nof keystroke and click interactions on a web search engine and correlate them\nto wearable device-defined sleep measures over time. We demonstrate that\nreal-world performance varies throughout the day and is influenced by both\ncircadian rhythms, chronotype (morning/evening preference), and prior sleep\nduration and timing. We develop a statistical model that operationalizes a\nlarge body of work on sleep and performance and demonstrates that our estimates\nof circadian rhythms, homeostatic sleep drive, and sleep inertia align with\nexpectations from laboratory-based sleep studies. Further, we quantify the\nimpact of insufficient sleep on real-world performance and show that two\nconsecutive nights with less than six hours of sleep are associated with\ndecreases in performance which last for a period of six days. This work\ndemonstrates the feasibility of using online interactions for large-scale\nphysiological sensing.\n", "versions": [{"version": "v1", "created": "Sat, 21 Jan 2017 19:49:43 GMT"}, {"version": "v2", "created": "Sat, 25 Feb 2017 00:07:28 GMT"}], "update_date": "2017-02-28", "authors_parsed": [["Althoff", "Tim", ""], ["Horvitz", "Eric", ""], ["White", "Ryen W.", ""], ["Zeitzer", "Jamie", ""]]}, {"id": "1701.07128", "submitter": "Farzad Farkhooi", "authors": "Farzad Farkhooi and Wilhelm Stannat", "title": "A complete mean-field theory for dynamics of binary recurrent neural\n  networks", "comments": "5 pages, 2 figures, Accepted (Physical Review Letters)", "journal-ref": "Phys. Rev. Lett. 119, 208301 (2017)", "doi": "10.1103/PhysRevLett.119.208301", "report-no": null, "categories": "physics.bio-ph cond-mat.dis-nn q-bio.NC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We develop a unified theory that encompasses the macroscopic dynamics of\nrecurrent interactions of binary units within arbitrary network architectures.\nUsing the martingale theory, our mathematical analysis provides a complete\ndescription of nonequilibrium fluctuations in networks with finite size and\nfinite degree of interactions. Our approach allows the investigation of systems\nfor which a deterministic mean-field theory breaks down. To demonstrate this,\nwe uncover a novel dynamic state in which a recurrent network of binary units\nwith statistically inhomogeneous interactions, along with an asynchronous\nbehavior, also exhibits collective nontrivial stochastic fluctuations in the\nthermodynamical limit.\n", "versions": [{"version": "v1", "created": "Wed, 25 Jan 2017 01:23:39 GMT"}, {"version": "v2", "created": "Tue, 17 Oct 2017 08:28:45 GMT"}], "update_date": "2017-11-22", "authors_parsed": [["Farkhooi", "Farzad", ""], ["Stannat", "Wilhelm", ""]]}, {"id": "1701.07138", "submitter": "Wiktor Mlynarski", "authors": "Wiktor M{\\l}ynarski, Josh H. McDermott", "title": "Learning Mid-Level Auditory Codes from Natural Sound Statistics", "comments": "38 pages, 12 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "q-bio.NC cs.SD", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Interaction with the world requires an organism to transform sensory signals\ninto representations in which behaviorally meaningful properties of the\nenvironment are made explicit. These representations are derived through\ncascades of neuronal processing stages in which neurons at each stage recode\nthe output of preceding stages. Explanations of sensory coding may thus involve\nunderstanding how low-level patterns are combined into more complex structures.\nAlthough models exist in the visual domain to explain how mid-level features\nsuch as junctions and curves might be derived from oriented filters in early\nvisual cortex, little is known about analogous grouping principles for\nmid-level auditory representations. We propose a hierarchical generative model\nof natural sounds that learns combinations of spectrotemporal features from\nnatural stimulus statistics. In the first layer the model forms a sparse\nconvolutional code of spectrograms using a dictionary of learned\nspectrotemporal kernels. To generalize from specific kernel activation\npatterns, the second layer encodes patterns of time-varying magnitude of\nmultiple first layer coefficients. Because second-layer features are sensitive\nto combinations of spectrotemporal features, the representation they support\nencodes more complex acoustic patterns than the first layer. When trained on\ncorpora of speech and environmental sounds, some second-layer units learned to\ngroup spectrotemporal features that occur together in natural sounds. Others\ninstantiate opponency between dissimilar sets of spectrotemporal features. Such\ngroupings might be instantiated by neurons in the auditory cortex, providing a\nhypothesis for mid-level neuronal computation.\n", "versions": [{"version": "v1", "created": "Wed, 25 Jan 2017 02:00:50 GMT"}, {"version": "v2", "created": "Thu, 26 Jan 2017 16:34:43 GMT"}, {"version": "v3", "created": "Fri, 27 Jan 2017 19:21:10 GMT"}, {"version": "v4", "created": "Mon, 15 May 2017 21:39:01 GMT"}, {"version": "v5", "created": "Sun, 15 Oct 2017 02:18:20 GMT"}], "update_date": "2017-10-17", "authors_parsed": [["M\u0142ynarski", "Wiktor", ""], ["McDermott", "Josh H.", ""]]}, {"id": "1701.07213", "submitter": "David H\\\"ubner", "authors": "D H\\\"ubner, T Verhoeven, K Schmid, K-R M\\\"uller, M Tangermann, P-J\n  Kindermans", "title": "Learning from Label Proportions in Brain-Computer Interfaces: Online\n  Unsupervised Learning with Guarantees", "comments": "The EEG data of 13 subjects is freely available online at:\n  http://doi.org/10.5281/zenodo.192684", "journal-ref": null, "doi": "10.1371/journal.pone.0175856", "report-no": null, "categories": "stat.ML cs.HC q-bio.NC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Objective: Using traditional approaches, a Brain-Computer Interface (BCI)\nrequires the collection of calibration data for new subjects prior to online\nuse. Calibration time can be reduced or eliminated e.g.~by transfer of a\npre-trained classifier or unsupervised adaptive classification methods which\nlearn from scratch and adapt over time. While such heuristics work well in\npractice, none of them can provide theoretical guarantees. Our objective is to\nmodify an event-related potential (ERP) paradigm to work in unison with the\nmachine learning decoder to achieve a reliable calibration-less decoding with a\nguarantee to recover the true class means.\n  Method: We introduce learning from label proportions (LLP) to the BCI\ncommunity as a new unsupervised, and easy-to-implement classification approach\nfor ERP-based BCIs. The LLP estimates the mean target and non-target responses\nbased on known proportions of these two classes in different groups of the\ndata. We modified a visual ERP speller to meet the requirements of the LLP. For\nevaluation, we ran simulations on artificially created data sets and conducted\nan online BCI study with N=13 subjects performing a copy-spelling task.\n  Results: Theoretical considerations show that LLP is guaranteed to minimize\nthe loss function similarly to a corresponding supervised classifier. It\nperformed well in simulations and in the online application, where 84.5% of\ncharacters were spelled correctly on average without prior calibration.\n  Significance: The continuously adapting LLP classifier is the first\nunsupervised decoder for ERP BCIs guaranteed to find the true class means. This\nmakes it an ideal solution to avoid a tedious calibration and to tackle\nnon-stationarities in the data. Additionally, LLP works on complementary\nprinciples compared to existing unsupervised methods, allowing for their\nfurther enhancement when combined with LLP.\n", "versions": [{"version": "v1", "created": "Wed, 25 Jan 2017 09:09:08 GMT"}], "update_date": "2017-07-05", "authors_parsed": [["H\u00fcbner", "D", ""], ["Verhoeven", "T", ""], ["Schmid", "K", ""], ["M\u00fcller", "K-R", ""], ["Tangermann", "M", ""], ["Kindermans", "P-J", ""]]}, {"id": "1701.07243", "submitter": "Francois Meyer", "authors": "Fran\\c{c}ois G. Meyer, Alexander M. Benison, Zachariah Smith, and\n  Daniel S. Barth", "title": "Decoding Epileptogenesis in a Reduced State Space", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "q-bio.NC cs.LG q-bio.QM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We describe here the recent results of a multidisciplinary effort to design a\nbiomarker that can actively and continuously decode the progressive changes in\nneuronal organization leading to epilepsy, a process known as epileptogenesis.\nUsing an animal model of acquired epilepsy, wechronically record hippocampal\nevoked potentials elicited by an auditory stimulus. Using a set of reduced\ncoordinates, our algorithm can identify universal smooth low-dimensional\nconfigurations of the auditory evoked potentials that correspond to distinct\nstages of epileptogenesis. We use a hidden Markov model to learn the dynamics\nof the evoked potential, as it evolves along these smooth low-dimensional\nsubsets. We provide experimental evidence that the biomarker is able to exploit\nsubtle changes in the evoked potential to reliably decode the stage of\nepileptogenesis and predict whether an animal will eventually recover from the\ninjury, or develop spontaneous seizures.\n", "versions": [{"version": "v1", "created": "Wed, 25 Jan 2017 10:25:59 GMT"}], "update_date": "2017-01-26", "authors_parsed": [["Meyer", "Fran\u00e7ois G.", ""], ["Benison", "Alexander M.", ""], ["Smith", "Zachariah", ""], ["Barth", "Daniel S.", ""]]}, {"id": "1701.07646", "submitter": "Danielle Bassett", "authors": "Pranav G. Reddy, Marcelo G. Mattar, Andrew C. Murphy, Nicholas F.\n  Wymbs, Scott T. Grafton, Theodore D. Satterthwaite, Danielle S. Bassett", "title": "Brain State Flexibility Accompanies Motor-Skill Acquisition", "comments": "36 pages, 7 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "q-bio.NC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Learning requires the traversal of inherently distinct cognitive states to\nproduce behavioral adaptation. Yet, tools to explicitly measure these states\nwith non-invasive imaging -- and to assess their dynamics during learning --\nremain limited. Here, we describe an approach based on a novel application of\ngraph theory in which points in time are represented by network nodes, and\nsimilarities in brain states between two different time points are represented\nas network edges. We use a graph-based clustering technique to identify\nclusters of time points representing canonical brain states, and to assess the\nmanner in which the brain moves from one state to another as learning\nprogresses. We observe the presence of two primary states characterized by\neither high activation in sensorimotor cortex or high activation in a\nfrontal-subcortical system. Flexible switching among these primary states and\nother less common states becomes more frequent as learning progresses, and is\ninversely correlated with individual differences in learning rate. These\nresults are consistent with the notion that the development of automaticity is\nassociated with a greater freedom to use cognitive resources for other\nprocesses. Taken together, our work offers new insights into the constrained,\nlow dimensional nature of brain dynamics characteristic of early learning,\nwhich give way to less constrained, high-dimensional dynamics in later\nlearning.\n", "versions": [{"version": "v1", "created": "Thu, 26 Jan 2017 10:44:29 GMT"}], "update_date": "2017-01-27", "authors_parsed": [["Reddy", "Pranav G.", ""], ["Mattar", "Marcelo G.", ""], ["Murphy", "Andrew C.", ""], ["Wymbs", "Nicholas F.", ""], ["Grafton", "Scott T.", ""], ["Satterthwaite", "Theodore D.", ""], ["Bassett", "Danielle S.", ""]]}, {"id": "1701.07775", "submitter": "Xerxes D. Arsiwalla", "authors": "Ivan Herreros-Alonso, Xerxes D. Arsiwalla, Paul F.M.J. Verschure", "title": "A Forward Model at Purkinje Cell Synapses Facilitates Cerebellar\n  Anticipatory Control", "comments": "NIPS 2016", "journal-ref": "Advances in Neural Information Processing Systems, 29: 3828-3836,\n  (2016)", "doi": null, "report-no": null, "categories": "q-bio.NC cs.SY math.OC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  How does our motor system solve the problem of anticipatory control in spite\nof a wide spectrum of response dynamics from different musculo-skeletal\nsystems, transport delays as well as response latencies throughout the central\nnervous system? To a great extent, our highly-skilled motor responses are a\nresult of a reactive feedback system, originating in the brain-stem and spinal\ncord, combined with a feed-forward anticipatory system, that is adaptively\nfine-tuned by sensory experience and originates in the cerebellum. Based on\nthat interaction we design the counterfactual predictive control (CFPC)\narchitecture, an anticipatory adaptive motor control scheme in which a\nfeed-forward module, based on the cerebellum, steers an error feedback\ncontroller with counterfactual error signals. Those are signals that trigger\nreactions as actual errors would, but that do not code for any current or\nforthcoming errors. In order to determine the optimal learning strategy, we\nderive a novel learning rule for the feed-forward module that involves an\neligibility trace and operates at the synaptic level. In particular, our\neligibility trace provides a mechanism beyond co-incidence detection in that it\nconvolves a history of prior synaptic inputs with error signals. In the context\nof cerebellar physiology, this solution implies that Purkinje cell synapses\nshould generate eligibility traces using a forward model of the system being\ncontrolled. From an engineering perspective, CFPC provides a general-purpose\nanticipatory control architecture equipped with a learning rule that exploits\nthe full dynamics of the closed-loop system.\n", "versions": [{"version": "v1", "created": "Thu, 26 Jan 2017 16:59:20 GMT"}], "update_date": "2017-01-27", "authors_parsed": [["Herreros-Alonso", "Ivan", ""], ["Arsiwalla", "Xerxes D.", ""], ["Verschure", "Paul F. M. J.", ""]]}, {"id": "1701.07847", "submitter": "Dmitry Petrov", "authors": "Dmitry Petrov, Boris Gutman, Alexander Ivanov, Joshua Faskowitz, Neda\n  Jahanshad, Mikhail Belyaev, Paul Thompson", "title": "Structural Connectome Validation Using Pairwise Classification", "comments": "Accepted for IEEE International Symposium on Biomedical Imaging 2017", "journal-ref": null, "doi": null, "report-no": null, "categories": "q-bio.NC cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this work, we study the extent to which structural connectomes and\ntopological derivative measures are unique to individual changes within human\nbrains. To do so, we classify structural connectome pairs from two large\nlongitudinal datasets as either belonging to the same individual or not. Our\ndata is comprised of 227 individuals from the Alzheimer's Disease Neuroimaging\nInitiative (ADNI) and 226 from the Parkinson's Progression Markers Initiative\n(PPMI). We achieve 0.99 area under the ROC curve score for features which\nrepresent either weights or network structure of the connectomes (node degrees,\nPageRank and local efficiency). Our approach may be useful for eliminating\nnoisy features as a preprocessing step in brain aging studies and early\ndiagnosis classification problems.\n", "versions": [{"version": "v1", "created": "Thu, 26 Jan 2017 19:13:36 GMT"}, {"version": "v2", "created": "Mon, 30 Jan 2017 19:55:15 GMT"}], "update_date": "2017-02-01", "authors_parsed": [["Petrov", "Dmitry", ""], ["Gutman", "Boris", ""], ["Ivanov", "Alexander", ""], ["Faskowitz", "Joshua", ""], ["Jahanshad", "Neda", ""], ["Belyaev", "Mikhail", ""], ["Thompson", "Paul", ""]]}, {"id": "1701.07879", "submitter": "Gerard Rinkus", "authors": "Gerard Rinkus", "title": "A Radically New Theory of how the Brain Represents and Computes with\n  Probabilities", "comments": "33 pages, 10 figures - Sec. explaining single cell tuning fns as\n  artifacts of embedding SDRs in superposition removed (for future paper) -\n  Clarified that a given SDR code represents the whole likelihood distribution\n  over stored hypotheses at a coarsely-ranked level of fidelity (Submitted for\n  review)", "journal-ref": null, "doi": null, "report-no": null, "categories": "q-bio.NC cs.CV cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The brain is believed to implement probabilistic reasoning and to represent\ninformation via population, or distributed, coding. Most previous\npopulation-based probabilistic (PPC) theories share several basic properties:\n1) continuous-valued neurons; 2) fully(densely)-distributed codes, i.e.,\nall(most) units participate in every code; 3) graded synapses; 4) rate coding;\n5) units have innate unimodal tuning functions (TFs); 6) intrinsically noisy\nunits; and 7) noise/correlation is considered harmful. We present a radically\ndifferent theory that assumes: 1) binary units; 2) only a small subset of\nunits, i.e., a sparse distributed representation (SDR) (cell assembly),\ncomprises any individual code; 3) binary synapses; 4) signaling formally\nrequires only single (i.e., first) spikes; 5) units initially have completely\nflat TFs (all weights zero); 6) units are far less intrinsically noisy than\ntraditionally thought; rather 7) noise is a resource generated/used to cause\nsimilar inputs to map to similar codes, controlling a tradeoff between storage\ncapacity and embedding the input space statistics in the pattern of\nintersections over stored codes, epiphenomenally determining correlation\npatterns across neurons. The theory, Sparsey, was introduced 20+ years ago as a\ncanonical cortical circuit/algorithm model achieving efficient sequence\nlearning/recognition, but not elaborated as an alternative to PPC theories.\nHere, we show that: a) the active SDR simultaneously represents both the most\nsimilar/likely input and the entire (coarsely-ranked) similarity\nlikelihood/distribution over all stored inputs (hypotheses); and b) given an\ninput, the SDR code selection algorithm, which underlies both learning and\ninference, updates both the most likely hypothesis and the entire likelihood\ndistribution (cf. belief update) with a number of steps that remains constant\nas the number of stored items increases.\n", "versions": [{"version": "v1", "created": "Thu, 26 Jan 2017 21:16:32 GMT"}, {"version": "v2", "created": "Mon, 30 Jan 2017 03:37:42 GMT"}, {"version": "v3", "created": "Thu, 23 Feb 2017 19:16:39 GMT"}, {"version": "v4", "created": "Wed, 21 Feb 2018 23:00:01 GMT"}], "update_date": "2018-02-23", "authors_parsed": [["Rinkus", "Gerard", ""]]}, {"id": "1701.08100", "submitter": "Ardavan Salehi Nobandegani", "authors": "Ardavan Salehi Nobandegani, Ioannis N. Psaromiligkos", "title": "The Causal Frame Problem: An Algorithmic Perspective", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI q-bio.NC stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The Frame Problem (FP) is a puzzle in philosophy of mind and epistemology,\narticulated by the Stanford Encyclopedia of Philosophy as follows: \"How do we\naccount for our apparent ability to make decisions on the basis only of what is\nrelevant to an ongoing situation without having explicitly to consider all that\nis not relevant?\" In this work, we focus on the causal variant of the FP, the\nCausal Frame Problem (CFP). Assuming that a reasoner's mental causal model can\nbe (implicitly) represented by a causal Bayes net, we first introduce a notion\ncalled Potential Level (PL). PL, in essence, encodes the relative position of a\nnode with respect to its neighbors in a causal Bayes net. Drawing on the\npsychological literature on causal judgment, we substantiate the claim that PL\nmay bear on how time is encoded in the mind. Using PL, we propose an inference\nframework, called the PL-based Inference Framework (PLIF), which permits a\nboundedly-rational approach to the CFP to be formally articulated at Marr's\nalgorithmic level of analysis. We show that our proposed framework, PLIF, is\nconsistent with a wide range of findings in causal judgment literature, and\nthat PL and PLIF make a number of predictions, some of which are already\nsupported by existing findings.\n", "versions": [{"version": "v1", "created": "Thu, 26 Jan 2017 16:42:29 GMT"}], "update_date": "2017-01-30", "authors_parsed": [["Nobandegani", "Ardavan Salehi", ""], ["Psaromiligkos", "Ioannis N.", ""]]}, {"id": "1701.08663", "submitter": "Franziska Oschmann", "authors": "Konstantin Mergenthaler, Franziska Oschmann, Jeremy Petravicz,\n  Dipanjan Roy, Mriganka Sur, Klaus Obermayer", "title": "A computational study on synaptic and extrasynaptic effects of astrocyte\n  glutamate uptake on orientation tuning in V1", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "q-bio.NC q-bio.MN", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Astrocytes affect neural transmission by a tight control via glutamate\ntransporters on glutamate concentrations in direct vicinity to the synaptic\ncleft and by extracellular glutamate. Their relevance for information\nrepresentation has been supported by in-vivo studies in ferret and mouse\nprimary visual cortex. In ferret blocking glutamate transport pharmacologically\nbroadened tuning curves and enhanced the response at preferred orientation. In\nknock-out mice with reduced expression of glutamate transporters sharpened\ntuning was observed. It is however unclear how focal and ambient changes in\nglutamate concentration affect stimulus representation. Here we develop a\ncomputational framework, which allows the investigation of synaptic and\nextrasynaptic effects of glutamate uptake on orientation tuning in recurrently\nconnected network models with pinwheel-domain (ferret) or salt-and-pepper\n(mouse) organization. This model proposed that glutamate uptake shapes\ninformation representation when it affects the contribution of excitatory and\ninhibitory neurons to the network activity. Namely, strengthening the\ncontribution of excitatory neurons generally broadens tuning and elevates the\nresponse. In contrast, strengthening the contribution of inhibitory neurons can\nhave a sharpening effect on tuning. In addition local representational topology\nalso plays a role: In the pinwheel-domain model effects were strongest within\ndomains - regions where neighboring neurons share preferred orientations.\nAround pinwheels but also within salt-and-pepper networks the effects were less\nstrong. Our model proposes that the pharmacological intervention in ferret\nincreases the contribution of excitatory cells, while the reduced expression in\nmouse increases the contribution of inhibitory cells to network activity.\n", "versions": [{"version": "v1", "created": "Mon, 30 Jan 2017 15:52:45 GMT"}], "update_date": "2017-01-31", "authors_parsed": [["Mergenthaler", "Konstantin", ""], ["Oschmann", "Franziska", ""], ["Petravicz", "Jeremy", ""], ["Roy", "Dipanjan", ""], ["Sur", "Mriganka", ""], ["Obermayer", "Klaus", ""]]}]