[{"id": "1710.00559", "submitter": "Christoph Bandt", "authors": "Christoph Bandt", "title": "Crude EEG parameter provides sleep medicine with well-defined continuous\n  hypnograms", "comments": "23 pages, 15 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP q-bio.NC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  To evaluate EEG data, one can count local maxima and minima on a fine scale,\nin a sliding window analysis. This straightforward calculation, which\nsimplifies and improves previous work on permutation entropy, directly defines\na good proxy for brain activity in an EEG channel during an epoch of 30\nseconds. Different channels and persons can be compared when they are measured\nwith the same device and prefiltering options. This could lead to a rigorously\ndefined and suitably standardized biomarker of cortex activity, like blood\npressure or laboratory values. Applied to sleep EEG, the algorithm yields\nhypnograms with continuous scale which show amazing coincidence with sleep\nstage annotation by trained experts. Although produced by a crude method,\ncontinuous hypnograms provide a lot of details. For example, sleep depth\nusually decreases from evening to morning even within the same annotated sleep\nstage, except for REM phases where mean sleep depth is rather constant, but\ndifferent in frontal and parietal channels. The diagnostic potential of the\nmethod is demonstrated with two hypnograms of narcoleptic patients. In all 10\nsubjects, infra-slow oscillations of activity with a wavelength between 30s and\ntwo minutes were clearly seen, particularly strong at the onset of sleep and in\nS2 phases. The suggested method needs to be checked and improved. In its\npresent form it seems already an appropriate tool for screening long-term EEG\ndata.\n", "versions": [{"version": "v1", "created": "Mon, 2 Oct 2017 09:55:45 GMT"}], "update_date": "2017-10-03", "authors_parsed": [["Bandt", "Christoph", ""]]}, {"id": "1710.00869", "submitter": "Nick Wasylyshyn", "authors": "Nick Wasylyshyn (1, 2), Brett Hemenway (2), Javier O. Garcia (1, 2),\n  Christopher N. Cascio (2), Matthew Brook O'Donnell (2), C. Raymond Bingham\n  (3), Bruce Simons-Morton (4), Jean M. Vettel (1, 2, 5), Emily B. Falk (2)\n  ((1) US Army Research Laboratory, (2) University of Pennsylvania, (3)\n  University of Michigan Transportation Research Institute, (4) Eunice Kennedy\n  Shriver National Institute on Child Health and Human Development, (5)\n  University of California Santa Barbara)", "title": "Global Brain Dynamics During Social Exclusion Predict Subsequent\n  Behavioral Conformity", "comments": "32 pages, 5 figures, 1 table", "journal-ref": null, "doi": null, "report-no": null, "categories": "q-bio.NC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Individuals react differently to social experiences; for example, people who\nare more sensitive to negative social experiences, such as being excluded, may\nbe more likely to adapt their behavior to fit in with others. We examined\nwhether functional brain connectivity during social exclusion in the fMRI\nscanner can be used to predict subsequent conformity to peer norms. Adolescent\nmales (N = 57) completed a two-part study on teen driving risk: a social\nexclusion task (Cyberball) during an fMRI session and a subsequent driving\nsimulator session in which they drove alone and in the presence of a peer who\nexpressed risk-averse or risk-accepting driving norms. We computed the\ndifference in functional connectivity between social exclusion and social\ninclusion from each node in the brain to nodes in two brain networks, one\npreviously associated with mentalizing (medial prefrontal cortex,\ntemporoparietal junction, precuneus, temporal poles) and another with social\npain (anterior cingulate cortex, anterior insula). Using cross-validated\nmachine learning, this measure of global network connectivity during exclusion\npredicts the extent of conformity to peer pressure during driving in the\nsubsequent experimental session. These findings extend our understanding of how\nglobal neural dynamics guide social behavior, revealing functional network\nactivity that captures individual differences.\n", "versions": [{"version": "v1", "created": "Mon, 2 Oct 2017 19:08:41 GMT"}], "update_date": "2017-10-04", "authors_parsed": [["Wasylyshyn", "Nick", ""], ["Hemenway", "Brett", ""], ["Garcia", "Javier O.", ""], ["Cascio", "Christopher N.", ""], ["O'Donnell", "Matthew Brook", ""], ["Bingham", "C. Raymond", ""], ["Simons-Morton", "Bruce", ""], ["Vettel", "Jean M.", ""], ["Falk", "Emily B.", ""]]}, {"id": "1710.01117", "submitter": "Alexandra Diem PhD", "authors": "Alexandra K. Diem, Roxana O. Carare, Neil. W. Bressloff", "title": "A control mechanism for intramural periarterial drainage via astrocytes:\n  How neuronal activity could improve waste clearance from the brain", "comments": "Article in progress", "journal-ref": null, "doi": "10.1371/journal.pone.0205276", "report-no": null, "categories": "q-bio.TO cs.CE q-bio.NC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The mechanisms behind waste clearance from deep within the parenchyma of the\nbrain remain unclear to this date. Experimental evidence has shown that one\npathway for waste clearance, termed intramural periarterial drainage (IPAD), is\nthe rapid drainage of interstitial fluid (ISF) via basement membranes (BM) of\nthe smooth muscle cells (SMC) of cerebral arteries and its failure is closely\nassociated with the pathology of Alzheimer's disease (AD). We have previously\nshown that arterial pulsations from the heart beat are not strong enough to\ndrive waste clearance. Here we demonstrate computational evidence for a\nmechanism for cerebral waste clearance that is driven by functional hyperaemia,\nthat is, the dilation of cerebral arteries as a consequence of increased\nneuronal demand. This mechanism is based on our model for fluid flow through\nthe vascular basement membrane. It accounts for waste clearance rates observed\nin mouse experiments and aligns with pathological observations as well as\nrecommendations to lower the individual risk of AD, such as keeping mentally\nand physically active.\n", "versions": [{"version": "v1", "created": "Fri, 29 Sep 2017 11:43:48 GMT"}, {"version": "v2", "created": "Fri, 6 Oct 2017 10:44:10 GMT"}], "update_date": "2018-11-21", "authors_parsed": [["Diem", "Alexandra K.", ""], ["Carare", "Roxana O.", ""], ["Bressloff", "Neil. W.", ""]]}, {"id": "1710.01951", "submitter": "Sara Zannone", "authors": "Sara Zannone, Zuzanna Brzosko, Ole Paulsen, Claudia Clopath", "title": "Acetylcholine-modulated plasticity in reward-driven navigation: a\n  computational study", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "q-bio.NC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Neuromodulation plays a fundamental role in the acquisition of new\nbehaviours. Our experimental findings show that, whereas acetylcholine biases\nhippocampal synaptic plasticity towards depression, the subsequent application\nof dopamine can retroactively convert depression into potentiation. We\npreviously demonstrated that incorporating this sequentially neuromodulated\nSpike-Timing-Dependent Plasticity (STDP) rule in a network model of navigation\nyields effective learning of changing reward locations. Here, we further\ncharacterize the effects of cholinergic depression on behaviour. We find that\nacetylcholine, by allowing learning from negative outcomes, influences\nexploration in a non-trivial manner that highly depends on the specifics of the\nmodel, the environment and the task. Interestingly, sequentially neuromodulated\nSTDP also yields flexible learning, surpassing the performance of other\nreward-modulated plasticity rules.\n", "versions": [{"version": "v1", "created": "Thu, 5 Oct 2017 10:27:10 GMT"}], "update_date": "2017-10-06", "authors_parsed": [["Zannone", "Sara", ""], ["Brzosko", "Zuzanna", ""], ["Paulsen", "Ole", ""], ["Clopath", "Claudia", ""]]}, {"id": "1710.02113", "submitter": "Muhammad Yousefnezhad", "authors": "Muhammad Yousefnezhad and Daoqiang Zhang", "title": "Anatomical Pattern Analysis for decoding visual stimuli in human brains", "comments": "Published in Cognitive Computation", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.CV q-bio.NC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Background: A universal unanswered question in neuroscience and machine\nlearning is whether computers can decode the patterns of the human brain.\nMulti-Voxels Pattern Analysis (MVPA) is a critical tool for addressing this\nquestion. However, there are two challenges in the previous MVPA methods, which\ninclude decreasing sparsity and noise in the extracted features and increasing\nthe performance of prediction.\n  Methods: In overcoming mentioned challenges, this paper proposes Anatomical\nPattern Analysis (APA) for decoding visual stimuli in the human brain. This\nframework develops a novel anatomical feature extraction method and a new\nimbalance AdaBoost algorithm for binary classification. Further, it utilizes an\nError-Correcting Output Codes (ECOC) method for multiclass prediction. APA can\nautomatically detect active regions for each category of the visual stimuli.\nMoreover, it enables us to combine homogeneous datasets for applying advanced\nclassification.\n  Results and Conclusions: Experimental studies on 4 visual categories (words,\nconsonants, objects and scrambled photos) demonstrate that the proposed\napproach achieves superior performance to state-of-the-art methods.\n", "versions": [{"version": "v1", "created": "Thu, 5 Oct 2017 16:57:21 GMT"}], "update_date": "2017-10-06", "authors_parsed": [["Yousefnezhad", "Muhammad", ""], ["Zhang", "Daoqiang", ""]]}, {"id": "1710.02165", "submitter": "Matthew Cieslak", "authors": "Matthew Cieslak, Tegan Brennan, Wendy Meiring, Lukas J. Volz, Clint\n  Greene, Alexander Asturias, Subhash Suri, Scott T. Grafton", "title": "Analytic tractography: A closed-form solution for estimating local white\n  matter connectivity with diffusion MRI", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "q-bio.NC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  White matter structures composed of myelinated axons in the living human\nbrain are primarily studied by diffusion-weighted MRI (dMRI). These long-range\nprojections are typically characterized in a two-step process: dMRI is used to\nestimate the orientation of axons within each voxel, then these local\norientations are linked together to estimate the spatial extent of putative\nwhite matter bundles. Tractography, the process of tracing bundles across\nvoxels, either requires computationally expensive (probabilistic) simulations\nto model uncertainty in fiber orientation or ignores it completely\n(deterministic). Probabilistic simulation necessarily generates a finite number\nof trajectories, introducing \"simulation error\" to trajectory estimates. Here\nwe introduce a method to analytically (via a closed-form solution) take an\norientation distribution function (ODF) from each voxel and calculate the\nprobabilities that a trajectory projects from a voxel into each directly\nadjacent voxel. We validate our method by demonstrating that probabilistic\nsimulations converge to our analytically computed probabilities at the voxel\nlevel as the number of simulated seeds increases. We show that our method\naccurately calculates the ground-truth transition probabilities from a phantom\ndataset. As a demonstration, we incoroporate our analytic method for voxel\ntransition probabilities into the Voxel Graph framework, creating a\nquantitative framework for assessing white matter structure that we call\n\"analytic tractography\". The long-range connectivity problem is reduced to\nfinding paths in a graph whose adjacency structure reflects voxel-to-voxel\nanalytic transition probabilities. We demonstrate this approach performs\ncomparably to many current probabilistic and deterministic approaches at a\nfraction of the computational cost. Open source software software is provided.\n", "versions": [{"version": "v1", "created": "Thu, 5 Oct 2017 18:04:18 GMT"}], "update_date": "2017-10-09", "authors_parsed": [["Cieslak", "Matthew", ""], ["Brennan", "Tegan", ""], ["Meiring", "Wendy", ""], ["Volz", "Lukas J.", ""], ["Greene", "Clint", ""], ["Asturias", "Alexander", ""], ["Suri", "Subhash", ""], ["Grafton", "Scott T.", ""]]}, {"id": "1710.02199", "submitter": "Joaquin Goni", "authors": "Enrico Amico, Joaqu\\'in Go\\~ni", "title": "Mapping hybrid functional-structural connectivity traits in the human\n  connectome", "comments": "article: 34 pages, 4 figures; supplementary material: 5 pages, 5\n  figures", "journal-ref": "Network Neuroscience; 2018", "doi": null, "report-no": null, "categories": "q-bio.NC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  One of the crucial questions in neuroscience is how a rich functional\nrepertoire of brain states relates to its underlying structural organization.\nHow to study the associations between these structural and functional layers is\nan open problem that involves novel conceptual ways of tackling this question.\nWe here propose an extension of the Connectivity Independent Component Analysis\n(connICA) framework, to identify joint structural-functional connectivity\ntraits. Here, we extend connICA to integrate structural and functional\nconnectomes by merging them into common hybrid connectivity patterns that\nrepresent the connectivity fingerprint of a subject. We test this extended\napproach on the 100 unrelated subjects from the Human Connectome Project. The\nmethod is able to extract main independent structural-functional connectivity\npatterns from the entire cohort that are sensitive to the realization of\ndifferent tasks. The hybrid connICA extracted two main task-sensitive hybrid\ntraits. The first, encompassing the within and between connections of dorsal\nattentional and visual areas, as well as fronto-parietal circuits. The second,\nmainly encompassing the connectivity between visual, attentional, DMN and\nsubcortical networks. Overall, these findings confirms the potential ofthe\nhybrid connICA for the compression of structural/functional connectomes into\nintegrated patterns from a set of individual brain networks.\n", "versions": [{"version": "v1", "created": "Thu, 5 Oct 2017 20:13:05 GMT"}, {"version": "v2", "created": "Wed, 31 Jan 2018 15:06:04 GMT"}, {"version": "v3", "created": "Wed, 21 Feb 2018 15:30:39 GMT"}], "update_date": "2018-02-22", "authors_parsed": [["Amico", "Enrico", ""], ["Go\u00f1i", "Joaqu\u00edn", ""]]}, {"id": "1710.02316", "submitter": "Jean Stawiaski", "authors": "Jean Stawiaski", "title": "A Multiscale Patch Based Convolutional Network for Brain Tumor\n  Segmentation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV q-bio.NC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This article presents a multiscale patch based convolutional neural network\nfor the automatic segmentation of brain tumors in multi-modality 3D MR images.\nWe use multiscale deep supervision and inputs to train a convolutional network.\nWe evaluate the effectiveness of the proposed approach on the BRATS 2017\nsegmentation challenge where we obtained dice scores of 0.755, 0.900, 0.782 and\n95% Hausdorff distance of 3.63mm, 4.10mm, and 6.81mm for enhanced tumor core,\nwhole tumor and tumor core respectively.\n", "versions": [{"version": "v1", "created": "Fri, 6 Oct 2017 09:04:28 GMT"}], "update_date": "2017-10-09", "authors_parsed": [["Stawiaski", "Jean", ""]]}, {"id": "1710.02387", "submitter": "Chrystopher L. Nehaniv", "authors": "Chrystopher L. Nehaniv and Elena Antonova", "title": "Simulating and Reconstructing Neurodynamics with Epsilon-Automata\n  Applied to Electroencephalography (EEG) Microstate Sequences", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "q-bio.NC cs.FL nlin.AO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We introduce new techniques to the analysis of neural spatiotemporal dynamics\nvia applying $\\epsilon$-machine reconstruction to electroencephalography (EEG)\nmicrostate sequences. Microstates are short duration quasi-stable states of the\ndynamically changing electrical field topographies recorded via an array of\nelectrodes from the human scalp, and cluster into four canonical classes. The\nsequence of microstates observed under particular conditions can be considered\nan information source with unknown underlying structure. $\\epsilon$-machines\nare discrete dynamical system automata with state-dependent probabilities on\ndifferent future observations (in this case the next measured EEG microstate).\nThey artificially reproduce underlying structure in an optimally predictive\nmanner as generative models exhibiting dynamics emulating the behaviour of the\nsource. Here we present experiments using both simulations and empirical data\nsupporting the value of associating these discrete dynamical systems with\nmental states (e.g. mind-wandering, focused attention, etc.) and with clinical\npopulations. The neurodynamics of mental states and clinical populations can\nthen be further characterized by properties of these dynamical systems,\nincluding: i) statistical complexity (determined by the number of states of the\ncorresponding $\\epsilon$-automaton); ii) entropy rate; iii) characteristic\nsequence patterning (syntax, probabilistic grammars); iv) duration, persistence\nand stability of dynamical patterns; and v) algebraic measures such as\nKrohn-Rhodes complexity or holonomy length of the decompositions of these. The\npotential applications include the characterization of mental states in\nneurodynamic terms for mental health diagnostics, well-being interventions,\nhuman-machine interface, and others on both subject-specific and\ngroup/population-level.\n", "versions": [{"version": "v1", "created": "Fri, 29 Sep 2017 09:19:14 GMT"}], "update_date": "2017-10-09", "authors_parsed": [["Nehaniv", "Chrystopher L.", ""], ["Antonova", "Elena", ""]]}, {"id": "1710.02423", "submitter": "David Holcman", "authors": "J. Cartailler and D. Holcman", "title": "Voltage laws for three-dimensional microdomains with cusp-shaped funnels\n  derived from Poisson-Nernst-Planck equations", "comments": "28 pages, 6 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "q-bio.NC math.AP physics.bio-ph q-bio.SC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study the electro-diffusion properties of a domain containing a\ncusp-shaped structure in three dimensions when one ionic specie is dominant.\nThe mathematical problem consists in solving the steady-state\nPoisson-Nernst-Planck (PNP) equation with an integral constraint for the number\nof charges. A non-homogeneous Neumann boundary condition is imposed on the\nboundary. We construct an asymptotic approximation for certain singular limits\nthat agree with numerical simulations. Finally, we analyse the consequences of\nnon-homogeneous surface charge density. We conclude that the geometry of\ncusp-shaped domains influences the voltage profile, specifically inside the\ncusp structure. The main results are summarized in the form of new\nthree-dimensional electrostatic laws for non-electroneutral electrolytes. We\ndiscuss applications to dendritic spines in neuroscience.\n", "versions": [{"version": "v1", "created": "Fri, 6 Oct 2017 14:30:59 GMT"}], "update_date": "2017-10-09", "authors_parsed": [["Cartailler", "J.", ""], ["Holcman", "D.", ""]]}, {"id": "1710.02563", "submitter": "Luca Mazzucato", "authors": "Luca Mazzucato, Giancarlo La Camera, Alfredo Fontanini", "title": "Expectation-induced modulation of metastable activity underlies faster\n  coding of sensory stimuli", "comments": "37 pages, 4+3 figures; v2: improved results, 7 new supplementary\n  figures; refs added", "journal-ref": null, "doi": null, "report-no": null, "categories": "q-bio.NC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Sensory stimuli can be recognized more rapidly when they are expected. This\nphenomenon depends on expectation affecting the cortical processing of sensory\ninformation. However, virtually nothing is known on the mechanisms responsible\nfor the effects of expectation on sensory networks. Here, we report a novel\ncomputational mechanism underlying the expectation-dependent acceleration of\ncoding observed in the gustatory cortex (GC) of alert rats. We use a recurrent\nspiking network model with a clustered architecture capturing essential\nfeatures of cortical activity, including the metastable activity observed in GC\nbefore and after gustatory stimulation. Relying both on network theory and\ncomputer simulations, we propose that expectation exerts its function by\nmodulating the intrinsically generated dynamics preceding taste delivery. Our\nmodel, whose predictions are confirmed in the experimental data, demonstrates\nhow the modulation of intrinsic metastable activity can shape sensory coding\nand mediate cognitive processes such as the expectation of relevant events.\nAltogether, these results provide a biologically plausible theory of\nexpectation and ascribe a new functional role to intrinsically generated,\nmetastable activity.\n", "versions": [{"version": "v1", "created": "Fri, 6 Oct 2017 19:30:20 GMT"}, {"version": "v2", "created": "Fri, 2 Nov 2018 17:53:21 GMT"}], "update_date": "2018-11-05", "authors_parsed": [["Mazzucato", "Luca", ""], ["La Camera", "Giancarlo", ""], ["Fontanini", "Alfredo", ""]]}, {"id": "1710.02623", "submitter": "Yuri A. Dabaghian", "authors": "Andrey Babichev, Dmitriy Morozov and Yuri Dabaghian", "title": "Robust spatial memory maps encoded in networks with transient\n  connections", "comments": "24 pages, 10 figures, 4 supplementary figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "q-bio.NC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The spiking activity of principal cells in mammalian hippocampus encodes an\ninternalized neuronal representation of the ambient space---a cognitive map.\nOnce learned, such a map enables the animal to navigate a given environment for\na long period. However, the neuronal substrate that produces this map remains\ntransient: the synaptic connections in the hippocampus and in the downstream\nneuronal networks never cease to form and to deteriorate at a rapid rate. How\ncan the brain maintain a robust, reliable representation of space using a\nnetwork that constantly changes its architecture? Here, we demonstrate, using\nnovel Algebraic Topology techniques, that cognitive map's stability is a\ngeneric, emergent phenomenon. The model allows evaluating the effect produced\nby specific physiological parameters, e.g., the distribution of connections'\ndecay times, on the properties of the cognitive map as a whole. It also points\nout that spatial memory deterioration caused by weakening or excessive loss of\nthe synaptic connections may be compensated by simulating the neuronal\nactivity. Lastly, the model explicates functional importance of the\ncomplementary learning systems for processing spatial information at different\nlevels of spatiotemporal granularity, by establishing three complementary\ntimescales at which spatial information unfolds. Thus, the model provides a\nprincipal insight into how can the brain develop a reliable representation of\nthe world, learn and retain memories despite complex plasticity of the\nunderlying networks and allows studying how instabilities and memory\ndeterioration mechanisms may affect learning process.\n", "versions": [{"version": "v1", "created": "Sat, 7 Oct 2017 02:50:37 GMT"}], "update_date": "2017-10-10", "authors_parsed": [["Babichev", "Andrey", ""], ["Morozov", "Dmitriy", ""], ["Dabaghian", "Yuri", ""]]}, {"id": "1710.03070", "submitter": "Brian DePasquale", "authors": "Brian DePasquale, Christopher J. Cueva, Kanaka Rajan, G. Sean Escola,\n  L.F. Abbott", "title": "full-FORCE: A Target-Based Method for Training Recurrent Networks", "comments": "20 pages, 8 figures", "journal-ref": null, "doi": "10.1371/journal.pone.0191527", "report-no": null, "categories": "cs.NE cs.LG q-bio.NC stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Trained recurrent networks are powerful tools for modeling dynamic neural\ncomputations. We present a target-based method for modifying the full\nconnectivity matrix of a recurrent network to train it to perform tasks\ninvolving temporally complex input/output transformations. The method\nintroduces a second network during training to provide suitable \"target\"\ndynamics useful for performing the task. Because it exploits the full recurrent\nconnectivity, the method produces networks that perform tasks with fewer\nneurons and greater noise robustness than traditional least-squares (FORCE)\napproaches. In addition, we show how introducing additional input signals into\nthe target-generating network, which act as task hints, greatly extends the\nrange of tasks that can be learned and provides control over the complexity and\nnature of the dynamics of the trained, task-performing network.\n", "versions": [{"version": "v1", "created": "Mon, 9 Oct 2017 13:00:08 GMT"}], "update_date": "2018-07-04", "authors_parsed": [["DePasquale", "Brian", ""], ["Cueva", "Christopher J.", ""], ["Rajan", "Kanaka", ""], ["Escola", "G. Sean", ""], ["Abbott", "L. F.", ""]]}, {"id": "1710.03071", "submitter": "R. Ozgur Doruk", "authors": "Ozgur Doruk, Kechen Zhang", "title": "Building a Dynamical Network Model from Neural Spiking Data: Application\n  of Poisson Likelihood", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "q-bio.NC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Research showed that, the information transmitted in biological neurons is\nencoded in the instants of successive action potentials or their firing rate.\nIn addition to that, in-vivo operation of the neuron makes measurement\ndifficult and thus continuous data collection is restricted. Due to those\nreasons, classical mean square estimation techniques that are frequently used\nin neural network training is very difficult to apply. In such situations,\npoint processes and related likelihood methods may be beneficial. In this\nstudy, we will present how one can apply certain methods to use the\nstimulus-response data obtained from a neural process in the mathematical\nmodeling of a neuron. The study is theoretical in nature and it will be\nsupported by simulations. In addition it will be compared to a similar study\nperformed on the same network model.\n", "versions": [{"version": "v1", "created": "Mon, 9 Oct 2017 13:05:20 GMT"}, {"version": "v2", "created": "Mon, 8 Jan 2018 19:14:34 GMT"}], "update_date": "2018-01-10", "authors_parsed": [["Doruk", "Ozgur", ""], ["Zhang", "Kechen", ""]]}, {"id": "1710.03613", "submitter": "Oren Miron", "authors": "Oren Miron, Andrew L. Beam, Isaac S. Kohane", "title": "Auditory Brainstem Response in Infants and Children with Autism: A\n  Meta-Analysis", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "q-bio.NC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Infants with autism were recently found to have prolonged Auditory Brainstem\nResponse (ABR); however, at older ages, findings are contradictory. We compared\nABR differences between participants with autism and controls with respect to\nage using a meta-analysis. Data sources included MEDLINE, EMBASE, Web of\nScience, Google Scholar, HOLLIS and ScienceDirect from their inception to June\n2016. The 25 studies that were included had a total of 1349 participants (727\nparticipants with autism and 622 controls) and an age range of 0-40 years.\nProlongation of wave V in autism had a significant negative correlation with\nage (R2=0.23; P=.01). The 22 studies below age 18 years showed a significantly\nprolonged wave V in autism (Standard Mean Difference=0.6 [95% CI, 0.5 to 0.8];\nP<.001). The 3 studies above 18 years of age showed a significantly shorter\nwave V in autism (SMD=-0.6 [95% CI, -1.0 to -0.2]; P=.004). Prolonged ABR was\nconsistent in infants and children with autism, suggesting it can serve as an\nautism biomarker at infancy. As the ABR is routinely used to screen infants for\nhearing impairment, the opportunity for replication studies is extensive.\n", "versions": [{"version": "v1", "created": "Tue, 10 Oct 2017 13:59:04 GMT"}], "update_date": "2017-10-11", "authors_parsed": [["Miron", "Oren", ""], ["Beam", "Andrew L.", ""], ["Kohane", "Isaac S.", ""]]}, {"id": "1710.03667", "submitter": "Madhu Advani", "authors": "Madhu S. Advani, Andrew M. Saxe", "title": "High-dimensional dynamics of generalization error in neural networks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG physics.data-an q-bio.NC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We perform an average case analysis of the generalization dynamics of large\nneural networks trained using gradient descent. We study the\npractically-relevant \"high-dimensional\" regime where the number of free\nparameters in the network is on the order of or even larger than the number of\nexamples in the dataset. Using random matrix theory and exact solutions in\nlinear models, we derive the generalization error and training error dynamics\nof learning and analyze how they depend on the dimensionality of data and\nsignal to noise ratio of the learning problem. We find that the dynamics of\ngradient descent learning naturally protect against overtraining and\noverfitting in large networks. Overtraining is worst at intermediate network\nsizes, when the effective number of free parameters equals the number of\nsamples, and thus can be reduced by making a network smaller or larger.\nAdditionally, in the high-dimensional regime, low generalization error requires\nstarting with small initial weights. We then turn to non-linear neural\nnetworks, and show that making networks very large does not harm their\ngeneralization performance. On the contrary, it can in fact reduce\novertraining, even without early stopping or regularization of any sort. We\nidentify two novel phenomena underlying this behavior in overcomplete models:\nfirst, there is a frozen subspace of the weights in which no learning occurs\nunder gradient descent; and second, the statistical properties of the\nhigh-dimensional regime yield better-conditioned input correlations which\nprotect against overtraining. We demonstrate that naive application of\nworst-case theories such as Rademacher complexity are inaccurate in predicting\nthe generalization performance of deep neural networks, and derive an\nalternative bound which incorporates the frozen subspace and conditioning\neffects and qualitatively matches the behavior observed in simulation.\n", "versions": [{"version": "v1", "created": "Tue, 10 Oct 2017 15:48:12 GMT"}], "update_date": "2017-10-11", "authors_parsed": [["Advani", "Madhu S.", ""], ["Saxe", "Andrew M.", ""]]}, {"id": "1710.03923", "submitter": "Muhammad Yousefnezhad", "authors": "Muhammad Yousefnezhad and Daoqiang Zhang", "title": "Deep Hyperalignment", "comments": "31st Conference on Neural Information Processing Systems (NIPS 2017),\n  Long Beach, CA, USA", "journal-ref": null, "doi": null, "report-no": null, "categories": "q-bio.NC cs.CV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper proposes Deep Hyperalignment (DHA) as a regularized, deep\nextension, scalable Hyperalignment (HA) method, which is well-suited for\napplying functional alignment to fMRI datasets with nonlinearity,\nhigh-dimensionality (broad ROI), and a large number of subjects. Unlink\nprevious methods, DHA is not limited by a restricted fixed kernel function.\nFurther, it uses a parametric approach, rank-$m$ Singular Value Decomposition\n(SVD), and stochastic gradient descent for optimization. Therefore, DHA has a\nsuitable time complexity for large datasets, and DHA does not require the\ntraining data when it computes the functional alignment for a new subject.\nExperimental studies on multi-subject fMRI analysis confirm that the DHA method\nachieves superior performance to other state-of-the-art HA algorithms.\n", "versions": [{"version": "v1", "created": "Wed, 11 Oct 2017 06:21:45 GMT"}], "update_date": "2017-10-12", "authors_parsed": [["Yousefnezhad", "Muhammad", ""], ["Zhang", "Daoqiang", ""]]}, {"id": "1710.03999", "submitter": "Guillaume Pernelle", "authors": "Guillaume Pernelle, Wilten Nicola, Claudia Clopath", "title": "Gap junction plasticity can lead to spindle oscillations", "comments": "arXiv admin note: text overlap with arXiv:1707.02324", "journal-ref": null, "doi": null, "report-no": null, "categories": "q-bio.NC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Patterns of waxing and waning oscillations, called spindles, are observed in\nmultiple brain regions during sleep. Spindle are thought to be involved in\nmemory consolidation. The origin of spindle oscillations is ongoing work but\nexperimental results point towards the thalamic reticular nucleus (TRN) as a\nlikely candidate. The TRN is rich in electrical synapses, also called gap\njunctions, which promote synchrony in neural activity. Moreover, gap junctions\nundergo activity-dependent long-term plasticity. We hypothesized that gap\njunction plasticity can modulate spindle oscillations. We developed a\ncomputational model of gap junction plasticity in recurrent networks of TRN and\nthalamocortical neurons (TC). We showed that gap junction coupling can modulate\nthe TRN-TC network synchrony and that gap junction plasticity is a plausible\nmechanism for the generation of sleep-spindles. Finally, our results are robust\nto the simulation of pharmacological manipulation of spindles, such as the\nadministration of propofol, an anesthetics known to generate spindles in\nhumans.\n", "versions": [{"version": "v1", "created": "Wed, 11 Oct 2017 10:57:46 GMT"}], "update_date": "2017-10-12", "authors_parsed": [["Pernelle", "Guillaume", ""], ["Nicola", "Wilten", ""], ["Clopath", "Claudia", ""]]}, {"id": "1710.04056", "submitter": "Onerva Korhonen", "authors": "Elisa Ryypp\\\"o, Enrico Glerean, Elvira Brattico, Jari Saram\\\"aki,\n  Onerva Korhonen", "title": "Regions of Interest as nodes of dynamic functional brain networks", "comments": "23 pages, 7 figures. Accepted for publication in Network\n  Neuroscience. Supplementary information available at\n  https://github.com/onerva-korhonen/ROI_consistency. This is a major revision\n  of the earlier manuscript: two additional parcellation schemes have been\n  added and discussion of the results extended; results unchanged", "journal-ref": "Network Neuroscience 2018; 2(4), 513-535", "doi": "10.1162/netn_a_00047", "report-no": null, "categories": "q-bio.NC", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  The properties of functional brain networks strongly depend on how their\nnodes are chosen. Commonly, nodes are defined by Regions of Interest (ROIs),\npre-determined groupings of fMRI measurement voxels. Earlier, we have\ndemonstrated that the functional homogeneity of ROIs, captured by their spatial\nconsistency, varies widely across ROIs in commonly-used brain atlases. Here, we\nask how ROIs behave as nodes of dynamic brain networks. To this end, we use two\nmeasures: spatiotemporal consistency measures changes in spatial consistency\nacross time and network turnover quantifies the changes in the local network\nstructure around a ROI. We find that spatial consistency varies non-uniformly\nin space and time, which is reflected in the variation of spatiotemporal\nconsistency across ROIs. Further, we see time-dependent changes in the network\nneighborhoods of the ROIs, reflected in high network turnover. Network turnover\nis nonuniformly distributed across ROIs: ROIs with high spatiotemporal\nconsistency have low network turnover. Finally, we reveal that there is rich\nvoxel-level correlation structure inside ROIs. Because the internal structure\nand the connectivity of ROIs vary in time, the common approach of using static\nnode definitions may be surprisingly inaccurate. Therefore, network\nneuroscience would greatly benefit from node definition strategies tailored for\ndynamical networks.\n", "versions": [{"version": "v1", "created": "Wed, 11 Oct 2017 13:26:23 GMT"}, {"version": "v2", "created": "Tue, 6 Feb 2018 11:29:41 GMT"}], "update_date": "2019-11-25", "authors_parsed": [["Ryypp\u00f6", "Elisa", ""], ["Glerean", "Enrico", ""], ["Brattico", "Elvira", ""], ["Saram\u00e4ki", "Jari", ""], ["Korhonen", "Onerva", ""]]}, {"id": "1710.04173", "submitter": "Sabine Ploux Dr.", "authors": "Sabine Ploux, Rui Wang, ZhengFeng Zhong, Hai Zhao, Yang Xin and\n  Bao-Liang Lu", "title": "Structural Stability of Lexical Semantic Spaces: Nouns in Chinese and\n  French", "comments": "17 pages, 4 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "q-bio.NC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Many studies in the neurosciences have dealt with the semantic processing of\nwords or categories, but few have looked into the semantic organization of the\nlexicon thought as a system. The present study was designed to try to move\ntowards this goal, using both electrophysiological and corpus-based data, and\nto compare two languages from different families: French and Mandarin Chinese.\n  We conducted an EEG-based semantic-decision experiment using 240 words from\neight categories (clothing, parts of a house, tools, vehicles,\nfruits/vegetables, animals, body parts, and people) as the material. A\ndata-analysis method (correspondence analysis) commonly used in computational\nlinguistics was applied to the electrophysiological signals.\n  The present cross-language comparison indicated stability for the following\naspects of the languages' lexical semantic organizations: (1) the\nliving/nonliving distinction, which showed up as a main factor for both\nlanguages; (2) greater dispersion of the living categories as compared to the\nnonliving ones; (3) prototypicality of the \\emph{animals} category within the\nliving categories, and with respect to the living/nonliving distinction; and\n(4) the existence of a person-centered reference gradient. Our\nelectrophysiological analysis indicated stability of the networks at play in\neach of these processes. Stability was also observed in the data taken from\nword usage in the languages (synonyms and associated words obtained from\ntextual corpora).\n", "versions": [{"version": "v1", "created": "Wed, 11 Oct 2017 16:59:12 GMT"}], "update_date": "2017-10-12", "authors_parsed": [["Ploux", "Sabine", ""], ["Wang", "Rui", ""], ["Zhong", "ZhengFeng", ""], ["Zhao", "Hai", ""], ["Xin", "Yang", ""], ["Lu", "Bao-Liang", ""]]}, {"id": "1710.04400", "submitter": "Olha Shchur", "authors": "A. Vidybida, O. Shchur", "title": "Information reduction in a reverberatory neuronal network through\n  convergence to complex oscillatory firing patterns", "comments": "15 pages, 8 figures, 4 tables, manuscript accepted by Biosystems.\n  Content of this work was presented at the Twelvth International Neural Coding\n  Workshop in Cologne, Germany", "journal-ref": null, "doi": "10.1016/j.biosystems.2017.07.008", "report-no": null, "categories": "q-bio.NC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study dynamics of a reverberating neural net by means of computer\nsimulation. The net, which is composed of 9 leaky integrate-and-fire (LIF)\nneurons arranged in a square lattice, is fully connected with interneuronal\ncommunication delay proportional to the corresponding distance. The network is\ninitially stimulated with different stimuli and then goes freely. For each\nstimulus, in the course of free evolution, activity either dies out completely\nor the network converges to a periodic trajectory, which may be different for\ndifferent stimuli. The latter is observed for a set of 285290 initial stimuli\nwhich constitutes 83% of all stimuli applied. By applying each stimulus from\nthe set, we found 102 different periodic end-states. By analyzing the\ntrajectories, we conclude that neuronal firing is the necessary prerequisite\nfor merging different trajectories into a single one, which eventually\ntransforms into a periodic regime. Observed phenomena of self-organization in\nthe time domain are discussed as a possible model for processes taking place\nduring perception. The repetitive firing in the periodic regimes could underpin\nmemory formation.\n", "versions": [{"version": "v1", "created": "Thu, 12 Oct 2017 08:07:14 GMT"}], "update_date": "2017-10-13", "authors_parsed": [["Vidybida", "A.", ""], ["Shchur", "O.", ""]]}, {"id": "1710.04462", "submitter": "Ehsan Arbabi", "authors": "Ali Saeedi and Ehsan Arbabi", "title": "Effects of Images with Different Levels of Familiarity on EEG", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML q-bio.NC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Evaluating human brain potentials during watching different images can be\nused for memory evaluation, information retrieving, guilty-innocent\nidentification and examining the brain response. In this study, the effects of\nwatching images, with different levels of familiarity, on subjects'\nElectroencephalogram (EEG) have been studied. Three different groups of images\nwith three familiarity levels of \"unfamiliar\", \"familiar\" and \"very familiar\"\nhave been considered for this study. EEG signals of 21 subjects (14 men) were\nrecorded. After signal acquisition, pre-processing, including noise and\nartifact removal, were performed on epochs of data. Features, including\nspatial-statistical, wavelet, frequency and harmonic parameters, and also\ncorrelation between recording channels, were extracted from the data. Then, we\nevaluated the efficiency of the extracted features by using p-value and also an\northogonal feature selection method (combination of Gram-Schmitt method and\nFisher discriminant ratio) for feature dimensional reduction. As the final step\nof feature selection, we used 'add-r take-away l' method for choosing the most\ndiscriminative features. For data classification, including all two-class and\nthree-class cases, we applied Support Vector Machine (SVM) on the extracted\nfeatures. The correct classification rates (CCR) for \"unfamiliar-familiar\",\n\"unfamiliar-very familiar\" and \"familiar-very familiar\" cases were 85.6%,\n92.6%, and 70.6%, respectively. The best results of classifications were\nobtained in pre-frontal and frontal regions of brain. Also, wavelet, frequency\nand harmonic features were among the most discriminative features. Finally, in\nthree-class case, the best CCR was 86.8%.\n", "versions": [{"version": "v1", "created": "Thu, 12 Oct 2017 11:39:48 GMT"}], "update_date": "2017-10-13", "authors_parsed": [["Saeedi", "Ali", ""], ["Arbabi", "Ehsan", ""]]}, {"id": "1710.04538", "submitter": "Aditi Kathpalia", "authors": "Aditi Kathpalia and Nithin Nagaraj", "title": "Causality Testing: A Data Compression Framework", "comments": "6 pages, 4 figures in main article and 7 pages, 8 figures in\n  supplemental material", "journal-ref": null, "doi": null, "report-no": null, "categories": "physics.data-an physics.bio-ph q-bio.NC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Causality testing, the act of determining cause and effect from measurements,\nis widely used in physics, climatology, neuroscience, econometrics and other\ndisciplines. As a result, a large number of causality testing methods based on\nvarious principles have been developed. Causal relationships in complex systems\nare typically accompanied by entropic exchanges which are encoded in patterns\nof dynamical measurements. A data compression algorithm which can extract these\nencoded patterns could be used for inferring these relations. This motivates us\nto propose, for the first time, a generic causality testing framework based on\ndata compression. The framework unifies existing causality testing methods and\nenables us to innovate a novel Compression-Complexity Causality measure. This\nmeasure is rigorously tested on simulated and real-world time series and is\nfound to overcome the limitations of Granger Causality and Transfer Entropy,\nespecially for noisy and non-synchronous measurements. Additionally, it gives\ninsight on the `kind' of causal influence between input time series by the\nnotions of positive and negative causality.\n", "versions": [{"version": "v1", "created": "Wed, 11 Oct 2017 13:22:45 GMT"}, {"version": "v2", "created": "Sun, 18 Feb 2018 07:19:43 GMT"}], "update_date": "2018-02-20", "authors_parsed": [["Kathpalia", "Aditi", ""], ["Nagaraj", "Nithin", ""]]}, {"id": "1710.04897", "submitter": "Alessandro Treves", "authors": "Michelangelo Naim, Vezha Boboeva, Chol Jun Kang, Alessandro Treves", "title": "Reducing a cortical network to a Potts model yields storage capacity\n  estimates", "comments": "51 pages, 7 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "q-bio.NC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  An autoassociative network of Potts units, coupled via tensor connections,\nhas been proposed and analysed as an effective model of an extensive cortical\nnetwork with distinct short- and long-range synaptic connections, but it has\nnot been clarified in what sense it can be regarded as an effective model. We\ndraw here the correspondence between the two, which indicates the need to\nintroduce a local feedback term in the reduced model, i.e., in the Potts\nnetwork. An effective model allows the study of phase transitions. As an\nexample, we study the storage capacity of the Potts network with this\nadditional term, the local feedback $w$, which contributes to drive the\nactivity of the network towards one of the stored patterns. The storage\ncapacity calculation, performed using replica tools, is limited to fully\nconnected networks, for which a Hamiltonian can be defined. To extend the\nresults to the case of intermediate partial connectivity, we also derive the\nself-consistent signal-to-noise analysis for the Potts network; and finally we\ndiscuss implications for semantic memory in humans.\n", "versions": [{"version": "v1", "created": "Fri, 13 Oct 2017 12:54:38 GMT"}, {"version": "v2", "created": "Thu, 26 Oct 2017 14:46:24 GMT"}, {"version": "v3", "created": "Fri, 2 Feb 2018 14:25:56 GMT"}], "update_date": "2018-02-05", "authors_parsed": [["Naim", "Michelangelo", ""], ["Boboeva", "Vezha", ""], ["Kang", "Chol Jun", ""], ["Treves", "Alessandro", ""]]}, {"id": "1710.04931", "submitter": "Jakob Jordan", "authors": "Jakob Jordan, Mihai A. Petrovici, Oliver Breitwieser, Johannes\n  Schemmel, Karlheinz Meier, Markus Diesmann, Tom Tetzlaff", "title": "Deterministic networks for probabilistic computing", "comments": "22 pages, 11 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "q-bio.NC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Neural-network models of high-level brain functions such as memory recall and\nreasoning often rely on the presence of stochasticity. The majority of these\nmodels assumes that each neuron in the functional network is equipped with its\nown private source of randomness, often in the form of uncorrelated external\nnoise. However, both in vivo and in silico, the number of noise sources is\nlimited due to space and bandwidth constraints. Hence, neurons in large\nnetworks usually need to share noise sources. Here, we show that the resulting\nshared-noise correlations can significantly impair the performance of\nstochastic network models. We demonstrate that this problem can be overcome by\nusing deterministic recurrent neural networks as sources of uncorrelated noise,\nexploiting the decorrelating effect of inhibitory feedback. Consequently, even\na single recurrent network of a few hundred neurons can serve as a natural\nnoise source for large ensembles of functional networks, each comprising\nthousands of units. We successfully apply the proposed framework to a diverse\nset of binary-unit networks with different dimensionalities and entropies, as\nwell as to a network reproducing handwritten digits with distinct predefined\nfrequencies. Finally, we show that the same design transfers to functional\nnetworks of spiking neurons.\n", "versions": [{"version": "v1", "created": "Fri, 13 Oct 2017 14:10:43 GMT"}, {"version": "v2", "created": "Tue, 7 Nov 2017 13:29:19 GMT"}], "update_date": "2017-11-08", "authors_parsed": [["Jordan", "Jakob", ""], ["Petrovici", "Mihai A.", ""], ["Breitwieser", "Oliver", ""], ["Schemmel", "Johannes", ""], ["Meier", "Karlheinz", ""], ["Diesmann", "Markus", ""], ["Tetzlaff", "Tom", ""]]}, {"id": "1710.04947", "submitter": "Lucas Lacasa", "authors": "Uri Hasson, Jacopo Iacovacci, Ben Davis, Ryan Flanagan, Enzo\n  Tagliazucchi, Helmut Laufs, Lucas Lacasa", "title": "A combinatorial framework to quantify peak/pit asymmetries in complex\n  dynamics", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "physics.data-an q-bio.NC q-bio.QM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We explore a combinatorial framework which efficiently quantifies the\nasymmetries between minima and maxima in local fluctuations of time series. We\nfirstly showcase its performance by applying it to a battery of synthetic\ncases. We find rigorous results on some canonical dynamical models (stochastic\nprocesses with and without correlations, chaotic processes) complemented by\nextensive numerical simulations for a range of processes which indicate that\nthe methodology correctly distinguishes different complex dynamics and\noutperforms state of the art metrics in several cases. Subsequently, we apply\nthis methodology to real-world problems emerging across several disciplines\nincluding cases in neurobiology, finance and climate science. We conclude that\ndifferences between the statistics of local maxima and local minima in time\nseries are highly informative of the complex underlying dynamics and a\ngraph-theoretic extraction procedure allows to use these features for\nstatistical learning purposes.\n", "versions": [{"version": "v1", "created": "Fri, 13 Oct 2017 14:50:00 GMT"}], "update_date": "2017-10-16", "authors_parsed": [["Hasson", "Uri", ""], ["Iacovacci", "Jacopo", ""], ["Davis", "Ben", ""], ["Flanagan", "Ryan", ""], ["Tagliazucchi", "Enzo", ""], ["Laufs", "Helmut", ""], ["Lacasa", "Lucas", ""]]}, {"id": "1710.05067", "submitter": "Thierry Mora", "authors": "Christophe Gardella, Olivier Marre, Thierry Mora", "title": "Blindfold learning of an accurate neural metric", "comments": null, "journal-ref": "Proc Natl Acad Sci USA (2018)", "doi": "10.1073/pnas.1718710115", "report-no": null, "categories": "q-bio.NC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The brain has no direct access to physical stimuli, but only to the spiking\nactivity evoked in sensory organs. It is unclear how the brain can structure\nits representation of the world based on differences between those noisy,\ncorrelated responses alone. Here we show how to build a distance map of\nresponses from the structure of the population activity of retinal ganglion\ncells, allowing for the accurate discrimination of distinct visual stimuli from\nthe retinal response. We introduce the Temporal Restricted Boltzmann Machine to\nlearn the spatiotemporal structure of the population activity, and use this\nmodel to define a distance between spike trains. We show that this metric\noutperforms existing neural distances at discriminating pairs of stimuli that\nare barely distinguishable. The proposed method provides a generic and\nbiologically plausible way to learn to associate similar stimuli based on their\nspiking responses, without any other knowledge of these stimuli.\n", "versions": [{"version": "v1", "created": "Fri, 13 Oct 2017 20:08:43 GMT"}], "update_date": "2018-04-16", "authors_parsed": [["Gardella", "Christophe", ""], ["Marre", "Olivier", ""], ["Mora", "Thierry", ""]]}, {"id": "1710.05098", "submitter": "Songting Li", "authors": "Songting Li, Nan Liu, Xiaohui Zhang, Douglas Zhou, and David Cai", "title": "Determination of Effective Synaptic Conductances Using Somatic Voltage\n  Clamp", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "q-bio.NC q-bio.QM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The interplay between excitatory and inhibitory neurons imparts rich\nfunctions of the brain. To understand the underlying synaptic mechanisms, a\nfundamental approach is to study the dynamics of excitatory and inhibitory\nconductances of each neuron. The traditional method of determining conductance\nemploys the synaptic current-voltage (I-V) relation obtained via voltage clamp.\nUsing theoretical analysis, electrophysiological experiments, and realistic\nsimulations, here we demonstrate that the traditional method conceptually fails\nto measure the conductance due to the neglect of a nonlinear interaction\nbetween the clamp current and the synaptic current. Consequently, it incurs\nsubstantial measurement error, even giving rise to unphysically negative\nconductance as observed in experiments. To elucidate synaptic impact on\nneuronal information processing, we introduce the concept of effective\nconductance and propose a framework to determine it accurately. Our work\nsuggests re-examination of previous studies involving conductance measurement\nand provides a reliable approach to assess synaptic influence on neuronal\ncomputation.\n", "versions": [{"version": "v1", "created": "Fri, 13 Oct 2017 23:10:20 GMT"}], "update_date": "2017-10-17", "authors_parsed": [["Li", "Songting", ""], ["Liu", "Nan", ""], ["Zhang", "Xiaohui", ""], ["Zhou", "Douglas", ""], ["Cai", "David", ""]]}, {"id": "1710.05113", "submitter": "Songting Li", "authors": "Songting Li, Xiaohui Zhang, Douglas Zhou, and David Cai", "title": "A New Framework for Determination of Excitatory and Inhibitory\n  Conductances Using Somatic Clamp", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "q-bio.NC q-bio.QM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The interaction between excitation and inhibition is crucial for brain\ncomputation. To understand synaptic mechanisms underlying brain function, it is\nimportant to separate excitatory and inhibitory inputs to a target neuron. In\nthe traditional method, after applying somatic current or voltage clamp, the\nexcitatory and inhibitory conductances are determined from the synaptic\ncurrent-voltage (I-V) relation --- the slope corresponds to the total\nconductance and the intercept corresponds to the reversal current. Because of\nthe space clamp effect, the measured conductance in general deviates\nsubstantially from the local conductance on the dendrite. Therefore, the\ninterpretation of the conductance measured by the traditional method remains to\nbe clarified. In this work, based on the investigation of an idealized\nball-and-stick neuron model and a biologically realistic pyramidal neuron\nmodel, we first demonstrate both analytically and numerically that the\nconductance determined by the traditional method has no clear biological\ninterpretation due to the neglect of a nonlinear interaction between the clamp\ncurrent and the synaptic current across the spatial dendrites. As a\nconsequence, the traditional method can induce an arbitrarily large error of\nconductance measurement, sometimes even leads to unphysically negative\nconductance. To circumvent the difficulty of elucidating synaptic impact on\nneuronal computation using the traditional method, we then propose a framework\nto determine the effective conductance that reflects directly the functional\nimpact of synaptic inputs on action potential initiation and thereby neuronal\ninformation processing. Our framework has been further verified in realistic\nneuron simulations, thus greatly improves upon the traditional approach by\nproviding a reliable and accurate assessment of the role of synaptic activity\nin neuronal computation.\n", "versions": [{"version": "v1", "created": "Sat, 14 Oct 2017 00:45:38 GMT"}], "update_date": "2017-10-17", "authors_parsed": [["Li", "Songting", ""], ["Zhang", "Xiaohui", ""], ["Zhou", "Douglas", ""], ["Cai", "David", ""]]}, {"id": "1710.05183", "submitter": "Thomas Dean", "authors": "Thomas Dean", "title": "Inferring Mesoscale Models of Neural Computation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "q-bio.NC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recent years have seen dramatic progress in the development of techniques for\nmeasuring the activity and connectivity of large populations of neurons in the\nbrain. However, as these techniques grow ever more powerful---allowing us to\neven contemplate measuring every neuron in entire brain---a new problem arises:\nhow do we make sense of the mountains of data that these techniques produce?\nHere, we argue that the time is ripe for building an intermediate or\n\"mesoscale\" computational theory that can bridge between single-cell\n(microscale) accounts of neural function and behavioral (macroscale) accounts\nof animal cognition and environmental complexity. Just as digital accounts of\ncomputation in conventional computers abstract away the non-essential dynamics\nof the analog circuits that implementing gates and registers, so too a\ncomputational account of animal cognition can afford to abstract from the\nnon-essential dynamics of neurons. We argue that the geometry of neural\ncircuits is essential in explaining the computational limitations and\ntechnological innovations inherent in biological information processing. We\npropose a blueprint for how to employ tools from modern machine learning to\nautomatically infer a satisfying mesoscale account of neural computation that\ncombines functional and structural data, with an emphasis on learning and\nexploiting regularity and repeating motifs in neuronal circuits. Rather than\nsuggest a specific theory, we present a new class of scientific instruments\nthat can enable neuroscientists to design, propose, implement and test\nmesoscale theories of neural computation.\n", "versions": [{"version": "v1", "created": "Sat, 14 Oct 2017 13:26:42 GMT"}, {"version": "v2", "created": "Thu, 19 Oct 2017 12:19:22 GMT"}], "update_date": "2017-10-20", "authors_parsed": [["Dean", "Thomas", ""]]}, {"id": "1710.05189", "submitter": "Nicolas Rougier", "authors": "Nicolas P. Rougier", "title": "A graphical, scalable and intuitive method for the placement and the\n  connection of biological cells", "comments": "Corresponding code at https://github.com/rougier/spatial-computation", "journal-ref": null, "doi": "10.3389/fninf.2018.00012", "report-no": null, "categories": "cs.NE cs.GR q-bio.NC", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  We introduce a graphical method originating from the computer graphics domain\nthat is used for the arbitrary and intuitive placement of cells over a\ntwo-dimensional manifold. Using a bitmap image as input, where the color\nindicates the identity of the different structures and the alpha channel\nindicates the local cell density, this method guarantees a discrete\ndistribution of cell position respecting the local density function. This\nmethod scales to any number of cells, allows to specify several different\nstructures at once with arbitrary shapes and provides a scalable and versatile\nalternative to the more classical assumption of a uniform non-spatial\ndistribution. Furthermore, several connection schemes can be derived from the\npaired distances between cells using either an automatic mapping or a\nuser-defined local reference frame, providing new computational properties for\nthe underlying model. The method is illustrated on a discrete homogeneous\nneural field, on the distribution of cones and rods in the retina and on a\ncoronal view of the basal ganglia.\n", "versions": [{"version": "v1", "created": "Sat, 14 Oct 2017 14:10:39 GMT"}], "update_date": "2018-03-23", "authors_parsed": [["Rougier", "Nicolas P.", ""]]}, {"id": "1710.05201", "submitter": "Songting Li", "authors": "Qing-long L. Gu, Songting Li, Wei P. Dai, Douglas Zhou, and David Cai", "title": "Emergence of a Balanced Core through Dynamical Competition in\n  Heterogeneous Neuronal Networks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "q-bio.NC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The balance between excitation and inhibition is crucial for neuronal\ncomputation. It is observed that the balanced state of neuronal networks exists\nin many experiments, yet its underlying mechanism remains to be fully\nclarified. Theoretical studies of the balanced state mainly focus on the\nanalysis of the homogeneous Erd$\\ddot{\\text{o}}$s-R\\'enyi network. However,\nneuronal networks have been found to be inhomogeneous in many cortical areas.\nIn particular, the connectivity of neuronal networks can be of the type of\nscale-free, small-world, or even with specific motifs. In this work, we examine\nthe questions of whether the balanced state is universal with respect to\nnetwork topology and what characteristics the balanced state possesses in\ninhomogeneous networks such as scale-free and small-world networks. We discover\nthat, for a sparsely but strongly connected inhomogeneous network, despite that\nthe whole network receives external inputs, there is a small active subnetwork\n(active core) inherently embedded within it. The neurons in this active core\nhave relatively high firing rates while the neurons in the rest of the network\nare quiescent. Surprisingly, the active core possesses a balanced state and\nthis state is independent of the model of single-neuron dynamics. The dynamics\nof the active core can be well predicted using the Fokker-Planck equation with\nthe mean-field assumption. Our results suggest that, in the presence of\ninhomogeneous network connectivity, the balanced state may be ubiquitous in the\nbrain, and the network connectivity in the active core is essentially close to\nthe Erd$\\ddot{\\text{o}}$s-R\\'enyi structure. The existence of the small active\ncore embedded in a large network may provide a potential dynamical scenario\nunderlying sparse coding in neuronal networks.\n", "versions": [{"version": "v1", "created": "Sat, 14 Oct 2017 15:47:49 GMT"}], "update_date": "2017-10-17", "authors_parsed": [["Gu", "Qing-long L.", ""], ["Li", "Songting", ""], ["Dai", "Wei P.", ""], ["Zhou", "Douglas", ""], ["Cai", "David", ""]]}, {"id": "1710.05246", "submitter": "Don Krieger", "authors": "Don Krieger, Paul Shepard, Ben Zusman, Anirban Jana, David O. Okonkwo", "title": "Shared High Value Research Resources: The CamCAN Human Lifespan\n  Neuroimaging Dataset Processed on the Open Science Grid", "comments": "8 pages, 7 figures; Proceedings of the 2017 IEEE International\n  Conference on Bioinformatics and Biomedicine; Keynote to The International\n  Workshop on High Throughput Computing in Bioinformatics and Biomedicine using\n  the Open Science Grid", "journal-ref": null, "doi": null, "report-no": null, "categories": "q-bio.NC cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The CamCAN Lifespan Neuroimaging Dataset, Cambridge (UK) Centre for Ageing\nand Neuroscience, was acquired and processed beginning in December, 2016. The\nreferee consensus solver deployed to the Open Science Grid was used for this\ntask. The dataset includes demographic and screening measures, a\nhigh-resolution MRI scan of the brain, and whole-head magnetoencephalographic\n(MEG) recordings during eyes closed rest (560 sec), a simple task (540 sec),\nand passive listening/viewing (140 sec). The data were collected from 619\nneurologically normal individuals, ages 18-87. The processed results from the\nresting recordings are completed and available online. These constitute 1.7\nTBytes of data including the location within the brain (1 mm resolution), time\nstamp (1 msec resolution), and 80 msec time course for each of 3.7 billion\nvalidated neuroelectric events, i.e. mean 6.1 million events for each of the\n619 participants.\n  The referee consensus solver provides high yield (mean 11,000 neuroelectric\ncurrents/sec; standard deviation (sd): 3500/sec) high confidence (p < 10-12 for\neach identified current) measures of the neuroelectric currents whose magnetic\nfields are detected in the MEG recordings. We describe the solver, the\nimplementation of the solver deployed on the Open Science Grid, the workflow\nmanagement system, the opportunistic use of high performance computing (HPC)\nresources to add computing capacity to the Open Science Grid reserved for this\nproject, and our initial findings from the recently completed processing of the\nresting recordings. This required 14 million core hours, i.e. 40 core hours per\nsecond of data.\n", "versions": [{"version": "v1", "created": "Sat, 14 Oct 2017 23:16:17 GMT"}, {"version": "v2", "created": "Tue, 17 Oct 2017 00:38:06 GMT"}, {"version": "v3", "created": "Fri, 8 Dec 2017 14:11:10 GMT"}], "update_date": "2017-12-11", "authors_parsed": [["Krieger", "Don", ""], ["Shepard", "Paul", ""], ["Zusman", "Ben", ""], ["Jana", "Anirban", ""], ["Okonkwo", "David O.", ""]]}, {"id": "1710.05292", "submitter": "Farnaz Zamani Esfahlani", "authors": "Farnaz Zamani Esfahlani and Hiroki Sayama", "title": "A Percolation-based Thresholding Method with Applications in Functional\n  Connectivity Analysis", "comments": "12 pages, 6 figures; to appear in the Proceedings of CompleNet 2018,\n  in press", "journal-ref": null, "doi": null, "report-no": null, "categories": "q-bio.QM q-bio.NC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Despite the recent advances in developing more effective thresholding methods\nto convert weighted networks to unweighted counterparts, there are still\nseveral limitations that need to be addressed. One such limitation is the\ninability of the most existing thresholding methods to take into account the\ntopological properties of the original weighted networks during the\nbinarization process, which could ultimately result in unweighted networks that\nhave drastically different topological properties than the original weighted\nnetworks. In this study, we propose a new thresholding method based on the\npercolation theory to address this limitation. The performance of the proposed\nmethod was validated and compared to the existing thresholding methods using\nsimulated and real-world functional connectivity networks in the brain.\nComparison of macroscopic and microscopic properties of the resulted unweighted\nnetworks to the original weighted networks suggest that the proposed\nthresholding method can successfully maintain the topological properties of the\noriginal weighted networks.\n", "versions": [{"version": "v1", "created": "Sun, 15 Oct 2017 07:15:39 GMT"}, {"version": "v2", "created": "Fri, 1 Dec 2017 05:34:42 GMT"}], "update_date": "2017-12-04", "authors_parsed": [["Esfahlani", "Farnaz Zamani", ""], ["Sayama", "Hiroki", ""]]}, {"id": "1710.05494", "submitter": "Cecilia Diniz Behn", "authors": "K. Kalmbach, V. Booth, and C. G. Diniz Behn", "title": "REM sleep complicates period adding bifurcations from monophasic to\n  polyphasic sleep behavior in a sleep-wake regulatory network model for human\n  sleep", "comments": "20 pages, 7 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "q-bio.NC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The structure of human sleep changes across development as it consolidates\nfrom the polyphasic sleep of infants to the single nighttime sleep period\ntypical in adults. Across this same developmental period, time scales of the\nhomeostatic sleep drive, the physiological drive to sleep that increases with\ntime spent awake, also change and presumably govern the transition from\npolyphasic to monophasic sleep behavior. Using a physiologically-based,\nsleep-wake regulatory network model for human sleep, we investigated the\ndynamics of wake, rapid eye movement (REM) sleep, and non-REM (NREM) sleep\nduring this transition by varying the homeostatic sleep drive time constants.\nPreviously, we introduced an algorithm for constructing a one-dimensional\ncircle map that represents the dynamics of the full sleep-wake network model.\nBy tracking bifurcations in the piecewise continuous circle map as the\nhomeostatic sleep drive time constants are varied, we establish evidence for a\nborder collision bifurcation that results in period-adding-like behavior in the\nnumber of sleep cycles per day. Interestingly, this bifurcation is preceded by\nbifurcations in the number of REM bouts per sleep cycle that exhibit truncated\nperiod-adding-like behavior. The interaction of these bifurcations in numbers\nof sleep episodes and numbers of REM bouts per sleep episode generate\nnon-monotonic variation in sleep cycle patterns as well as quasi-periodic\npatterns during the transition from polyphasic to monophasic sleep behavior.\nThis analysis may have implications for understanding changes in sleep in early\nchildhood when preschoolers transition from napping to non-napping behavior,\nand the wide interindividual variation observed during this transition.\n", "versions": [{"version": "v1", "created": "Mon, 16 Oct 2017 04:02:09 GMT"}], "update_date": "2017-10-17", "authors_parsed": [["Kalmbach", "K.", ""], ["Booth", "V.", ""], ["Behn", "C. G. Diniz", ""]]}, {"id": "1710.05596", "submitter": "Pierre Gabriel", "authors": "Gr\\'egory Dumont, Pierre Gabriel (LMV)", "title": "The mean-field equation of a leaky integrate-and-fire neural network:\n  measure solutions and steady states", "comments": null, "journal-ref": "Nonlinearity, 2020, 33 (12), pp.6381-6420", "doi": "10.1088/1361-6544/aba6d8", "report-no": null, "categories": "math.AP q-bio.NC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Neural network dynamics emerge from the interaction of spiking cells. One way\nto formulate the problem is through a theoretical framework inspired by ideas\ncoming from statistical physics, the so-called mean-field theory. In this\ndocument, we investigate different issues related to the mean-field description\nof an excitatory network made up of leaky integrate-and-fire neurons. The\ndescription is written in the form a nonlinear partial differential equation\nwhich is known to blow up in finite time when the network is strongly\nconnected. We prove that in a moderate coupling regime the equation is globally\nwell-posed in the space of measures, and that there exist stationary solutions.\nIn the case of weak connectivity we also demonstrate the uniqueness of the\nsteady state and its global exponential stability. The method to show those\nmathematical results relies on a contraction argument of Doeblin's type in the\nlinear case, which corresponds to a population of non-interacting units.\n", "versions": [{"version": "v1", "created": "Mon, 16 Oct 2017 09:49:51 GMT"}, {"version": "v2", "created": "Fri, 24 Nov 2017 09:30:07 GMT"}, {"version": "v3", "created": "Wed, 5 Dec 2018 14:50:54 GMT"}, {"version": "v4", "created": "Wed, 4 Nov 2020 15:31:10 GMT"}], "update_date": "2020-11-11", "authors_parsed": [["Dumont", "Gr\u00e9gory", "", "LMV"], ["Gabriel", "Pierre", "", "LMV"]]}, {"id": "1710.05608", "submitter": "Viola Priesemann", "authors": "Mathias Sogorski and Theo Geisel and Viola Priesemann", "title": "Correlated microtiming deviations in jazz and rock music", "comments": null, "journal-ref": null, "doi": "10.1371/journal.pone.0186361", "report-no": null, "categories": "q-bio.NC nlin.CD", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Musical rhythms performed by humans typically show temporal fluctuations.\nWhile they have been characterized in simple rhythmic tasks, it is an open\nquestion what is the nature of temporal fluctuations, when several musicians\nperform music jointly in all its natural complexity. To study such fluctuations\nin over 100 original jazz and rock/pop recordings played with and without\nmetronome we developed a semi-automated workflow allowing the extraction of\ncymbal beat onsets with millisecond precision. Analyzing the inter-beat\ninterval (IBI) time series revealed evidence for two long-range correlated\nprocesses characterized by power laws in the IBI power spectral densities. One\nprocess dominates on short timescales ($t < 8$ beats) and reflects microtiming\nvariability in the generation of single beats. The other dominates on longer\ntimescales and reflects slow tempo variations. Whereas the latter did not show\ndifferences between musical genres (jazz vs. rock/pop), the process on short\ntimescales showed higher variability for jazz recordings, indicating that jazz\nmakes stronger use of microtiming fluctuations within a measure than rock/pop.\nOur results elucidate principles of rhythmic performance and can inspire\nalgorithms for artificial music generation. By studying microtiming\nfluctuations in original music recordings, we bridge the gap between\nminimalistic tapping paradigms and expressive rhythmic performances.\n", "versions": [{"version": "v1", "created": "Mon, 16 Oct 2017 10:34:11 GMT"}], "update_date": "2018-02-07", "authors_parsed": [["Sogorski", "Mathias", ""], ["Geisel", "Theo", ""], ["Priesemann", "Viola", ""]]}, {"id": "1710.05945", "submitter": "Alex Piet", "authors": "Alex Piet, Ahmed El Hady, and Carlos D Brody", "title": "Rats optimally accumulate and discount evidence in a dynamic environment", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "q-bio.NC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  How choices are made within noisy environments is a central question in the\nneuroscience of decision making. Previous work has characterized temporal\naccumulation of evidence for decision-making in static environments. However,\nreal-world decision-making involves environments with statistics that change\nover time. This requires discounting old evidence that may no longer inform the\ncurrent state of the world. Here we designed a rat behavioral task with a\ndynamic environment, to probe whether rodents can optimally discount evidence\nby adapting the timescale over which they accumulate it. Extending existing\nresults about optimal inference in a dynamic environment, we show that the\noptimal timescale for evidence discounting depends on both the stimulus\nstatistics and noise in sensory processing. We found that when both of these\ncomponents were taken into account, rats accumulated and temporally discounted\nevidence almost optimally. Furthermore, we found that by changing the dynamics\nof the environment, experimenters could control the rats' accumulation\ntimescale, switching them from accumulating over short timescales to\naccumulating over long timescales and back. The theoretical framework also\nmakes quantitative predictions regarding the timing of changes of mind in the\ndynamic environment. This study establishes a quantitative behavioral framework\nto control and investigate neural mechanisms underlying the adaptive nature of\nevidence accumulation timescales and changes of mind.\n", "versions": [{"version": "v1", "created": "Mon, 16 Oct 2017 18:10:44 GMT"}], "update_date": "2017-10-18", "authors_parsed": [["Piet", "Alex", ""], ["Hady", "Ahmed El", ""], ["Brody", "Carlos D", ""]]}, {"id": "1710.05967", "submitter": "Yuri A. Dabaghian", "authors": "Andrey Babichev and Yuri Dabaghian", "title": "Topological Schemas of Memory Spaces", "comments": "24 pages, 8 Figures, 1 Suppl. Figure", "journal-ref": null, "doi": null, "report-no": null, "categories": "q-bio.NC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Hippocampal cognitive map---a neuronal representation of the spatial\nenvironment---is broadly discussed in the computational neuroscience literature\nfor decades. More recent studies point out that hippocampus plays a major role\nin producing yet another cognitive framework that incorporates not only\nspatial, but also nonspatial memories---the memory space. However, unlike\ncognitive maps, memory spaces have been barely studied from a theoretical\nperspective. Here we propose an approach for modeling hippocampal memory spaces\nas an epiphenomenon of neuronal spiking activity. First, we suggest that the\nmemory space may be viewed as a finite topological space---a hypothesis that\nallows treating both spatial and nonspatial aspects of hippocampal function on\nequal footing. We then model the topological properties of the memory space to\ndemonstrate that this concept naturally incorporates the notion of a cognitive\nmap. Lastly, we suggest a formal description of the memory consolidation\nprocess and point out a connection between the proposed model of the memory\nspaces to the so-called Morris' schemas, which emerge as the most compact\nrepresentation of the memory structure.\n", "versions": [{"version": "v1", "created": "Mon, 16 Oct 2017 19:32:04 GMT"}], "update_date": "2017-10-18", "authors_parsed": [["Babichev", "Andrey", ""], ["Dabaghian", "Yuri", ""]]}, {"id": "1710.06487", "submitter": "SueYeon Chung", "authors": "SueYeon Chung, Daniel D. Lee, Haim Sompolinsky", "title": "Classification and Geometry of General Perceptual Manifolds", "comments": "24 pages, 12 figures, Supplementary Materials", "journal-ref": "Phys. Rev. X 8, 031003 (2018)", "doi": "10.1103/PhysRevX.8.031003", "report-no": null, "categories": "cond-mat.dis-nn cond-mat.stat-mech cs.NE q-bio.NC stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Perceptual manifolds arise when a neural population responds to an ensemble\nof sensory signals associated with different physical features (e.g.,\norientation, pose, scale, location, and intensity) of the same perceptual\nobject. Object recognition and discrimination requires classifying the\nmanifolds in a manner that is insensitive to variability within a manifold. How\nneuronal systems give rise to invariant object classification and recognition\nis a fundamental problem in brain theory as well as in machine learning. Here\nwe study the ability of a readout network to classify objects from their\nperceptual manifold representations. We develop a statistical mechanical theory\nfor the linear classification of manifolds with arbitrary geometry revealing a\nremarkable relation to the mathematics of conic decomposition. Novel\ngeometrical measures of manifold radius and manifold dimension are introduced\nwhich can explain the classification capacity for manifolds of various\ngeometries. The general theory is demonstrated on a number of representative\nmanifolds, including L2 ellipsoids prototypical of strictly convex manifolds,\nL1 balls representing polytopes consisting of finite sample points, and\norientation manifolds which arise from neurons tuned to respond to a continuous\nangle variable, such as object orientation. The effects of label sparsity on\nthe classification capacity of manifolds are elucidated, revealing a scaling\nrelation between label sparsity and manifold radius. Theoretical predictions\nare corroborated by numerical simulations using recently developed algorithms\nto compute maximum margin solutions for manifold dichotomies. Our theory and\nits extensions provide a powerful and rich framework for applying statistical\nmechanics of linear classification to data arising from neuronal responses to\nobject stimuli, as well as to artificial deep networks trained for object\nrecognition tasks.\n", "versions": [{"version": "v1", "created": "Tue, 17 Oct 2017 20:06:25 GMT"}, {"version": "v2", "created": "Mon, 26 Feb 2018 03:27:25 GMT"}, {"version": "v3", "created": "Sun, 24 Jun 2018 15:46:57 GMT"}], "update_date": "2018-07-11", "authors_parsed": [["Chung", "SueYeon", ""], ["Lee", "Daniel D.", ""], ["Sompolinsky", "Haim", ""]]}, {"id": "1710.06867", "submitter": "Simon DeDeo", "authors": "Alexander T. J. Barron, Jenny Huang, Rebecca L. Spang, Simon DeDeo", "title": "Individuals, Institutions, and Innovation in the Debates of the French\n  Revolution", "comments": "8 pages, 3 figures, 1 table. Comments solicited", "journal-ref": "Proceedings of the National Academy of Sciences, 201717729 (2018)", "doi": "10.1073/pnas.1717729115", "report-no": null, "categories": "physics.soc-ph cs.IT math.IT nlin.AO q-bio.NC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The French Revolution brought principles of \"liberty, equality, and\nbrotherhood\" to bear on the day-to-day challenges of governing what was then\nthe largest country in Europe. Its experiments provided a model for future\nrevolutions and democracies across the globe, but this first modern revolution\nhad no model to follow. Using reconstructed transcripts of debates held in the\nRevolution's first parliament, we present a quantitative analysis of how this\nsystem managed innovation. We use information theory to track the creation,\ntransmission, and destruction of patterns of word-use across over 40,000\nspeeches and more than one thousand speakers. The parliament as a whole was\nbiased toward the adoption of new patterns, but speakers' individual qualities\ncould break these overall trends. Speakers on the left innovated at higher\nrates while speakers on the right acted, often successfully, to preserve prior\npatterns. Key players such as Robespierre (on the left) and Abb\\'e Maury (on\nthe right) played information-processing roles emblematic of their politics.\nNewly-created organizational functions---such as the Assembly's President and\ncommittee chairs---had significant effects on debate outcomes, and a distinct\ntransition appears mid-way through the parliament when committees, external to\nthe debate process, gain new powers to \"propose and dispose\" to the body as a\nwhole. Taken together, these quantitative results align with existing\nqualitative interpretations but also reveal crucial information-processing\ndynamics that have hitherto been overlooked. Great orators had the public's\nattention, but deputies (mostly on the political left) who mastered the\ncommittee system gained new powers to shape revolutionary legislation.\n", "versions": [{"version": "v1", "created": "Wed, 18 Oct 2017 18:00:47 GMT"}], "update_date": "2018-09-25", "authors_parsed": [["Barron", "Alexander T. J.", ""], ["Huang", "Jenny", ""], ["Spang", "Rebecca L.", ""], ["DeDeo", "Simon", ""]]}, {"id": "1710.07551", "submitter": "Tuka Alhanai", "authors": "Tuka Alhanai, Rhoda Au, and James Glass", "title": "Spoken Language Biomarkers for Detecting Cognitive Impairment", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.CL q-bio.NC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this study we developed an automated system that evaluates speech and\nlanguage features from audio recordings of neuropsychological examinations of\n92 subjects in the Framingham Heart Study. A total of 265 features were used in\nan elastic-net regularized binomial logistic regression model to classify the\npresence of cognitive impairment, and to select the most predictive features.\nWe compared performance with a demographic model from 6,258 subjects in the\ngreater study cohort (0.79 AUC), and found that a system that incorporated both\naudio and text features performed the best (0.92 AUC), with a True Positive\nRate of 29% (at 0% False Positive Rate) and a good model fit (Hosmer-Lemeshow\ntest > 0.05). We also found that decreasing pitch and jitter, shorter segments\nof speech, and responses phrased as questions were positively associated with\ncognitive impairment.\n", "versions": [{"version": "v1", "created": "Fri, 20 Oct 2017 14:41:43 GMT"}], "update_date": "2017-10-23", "authors_parsed": [["Alhanai", "Tuka", ""], ["Au", "Rhoda", ""], ["Glass", "James", ""]]}, {"id": "1710.07613", "submitter": "Fabio Bagarello Dr.", "authors": "F. Bagarello, I. Basieva, A. Khrennikov", "title": "Quantum field inspired model of decision making: Asymptotic\n  stabilization of belief state via interaction with surrounding mental\n  environment", "comments": "In press in Journal of Mathematical Psychology", "journal-ref": null, "doi": null, "report-no": null, "categories": "q-bio.NC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper is devoted to justification of quantum-like models of the process\nof decision making based on the theory of open quantum systems, i.e. decision\nmaking is considered as decoherence. This process is modeled as interaction of\na decision maker, Alice, with a mental (information) environment ${\\cal R}$\nsurrounding her. Such an interaction generates \"dissipation of uncertainty\"\nfrom Alice's belief-state $\\rho(t)$ into ${\\cal R}$ and asymptotic\nstabilization of $\\rho(t)$ to a steady belief-state. The latter is treated as\nthe decision state. Mathematically the problem under study is about finding\nconstraints on ${\\cal R}$ guaranteeing such stabilization. We found a partial\nsolution of this problem (in the form of sufficient conditions). We present the\ncorresponding decision making analysis for one class of mental environments,\nthe so-called \"almost homogeneous environments\", with the illustrative\nexamples: a) behavior of electorate interacting with the mass-media\n\"reservoir\", b) consumers' persuasion. We also comment on other classes of\nmental environments.\n", "versions": [{"version": "v1", "created": "Sun, 15 Oct 2017 07:24:12 GMT"}], "update_date": "2017-10-23", "authors_parsed": [["Bagarello", "F.", ""], ["Basieva", "I.", ""], ["Khrennikov", "A.", ""]]}, {"id": "1710.07659", "submitter": "Andreas St\\\"ockel", "authors": "Andreas St\\\"ockel, Aaron R. Voelker, Chris Eliasmith", "title": "Point Neurons with Conductance-Based Synapses in the Neural Engineering\n  Framework", "comments": "24 pages, 12 figures, 1 table", "journal-ref": null, "doi": null, "report-no": null, "categories": "q-bio.NC cs.AI cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The mathematical model underlying the Neural Engineering Framework (NEF)\nexpresses neuronal input as a linear combination of synaptic currents. However,\nin biology, synapses are not perfect current sources and are thus nonlinear.\nDetailed synapse models are based on channel conductances instead of currents,\nwhich require independent handling of excitatory and inhibitory synapses. This,\nin particular, significantly affects the influence of inhibitory signals on the\nneuronal dynamics. In this technical report we first summarize the relevant\nportions of the NEF and conductance-based synapse models. We then discuss a\nna\\\"ive translation between populations of LIF neurons with current- and\nconductance-based synapses based on an estimation of an average membrane\npotential. Experiments show that this simple approach works relatively well for\nfeed-forward communication channels, yet performance degrades for NEF networks\ndescribing more complex dynamics, such as integration.\n", "versions": [{"version": "v1", "created": "Fri, 20 Oct 2017 18:35:23 GMT"}], "update_date": "2017-10-24", "authors_parsed": [["St\u00f6ckel", "Andreas", ""], ["Voelker", "Aaron R.", ""], ["Eliasmith", "Chris", ""]]}, {"id": "1710.07705", "submitter": "Miroslaw Latka", "authors": "Miroslaw Latka, Slawomir Budrewicz, Klaudia Kozlowska, Magdalena\n  Koszewicz and Bruce J. West", "title": "Signatures of Parkinson's disease in complexity of low-frequency\n  fluctuations of postural sway velocity", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "q-bio.NC physics.bio-ph", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Posturography is routinely used to qualitatively assess one of the cardinal\nsymptoms of Parkinson's disease -- postural instability. While most measures of\nbalance control are derived from displacement of the center of pressure there\nis evidence that such control is more likely to be velocity-based. We performed\nstatic posturographic tests (eyes open and eyes closed) during quiet standing\nin narrow stance for n=30 patients with Parkinson's disease (PDP) in the ON\nstate and compared the results with those of n=30 age-matched senior controls\n(HSC) and n=60 young controls (HYC). We used differentiator filters to generate\ntime series of low-frequency fluctuations of sway velocity and calculated their\nLempel-Ziv complexity (LZC). With eyes closed, the mediolateral LZC of HSC 0.21\n(0.02) was significantly higher than those of HYC 0.19 (0.02) and PDP 0.18\n(002). Thus aging and PD have opposite effects on mediolateral LZC which\nstrongly differentiates between HSC and PDP (92% sensitivity and 87%\nspecificity).\n", "versions": [{"version": "v1", "created": "Fri, 20 Oct 2017 21:11:00 GMT"}], "update_date": "2017-10-24", "authors_parsed": [["Latka", "Miroslaw", ""], ["Budrewicz", "Slawomir", ""], ["Kozlowska", "Klaudia", ""], ["Koszewicz", "Magdalena", ""], ["West", "Bruce J.", ""]]}, {"id": "1710.07829", "submitter": "Gerard Rinkus", "authors": "Rod Rinkus, Jasmin Leveille", "title": "Superposed Episodic and Semantic Memory via Sparse Distributed\n  Representation", "comments": "8 pages, 4 figures. Submitted to NIPS CIAI 2017 wkshp", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NE q-bio.NC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The abilities to perceive, learn, and use generalities, similarities,\nclasses, i.e., semantic memory (SM), is central to cognition. Machine learning\n(ML), neural network, and AI research has been primarily driven by tasks\nrequiring such abilities. However, another central facet of cognition,\nsingle-trial formation of permanent memories of experiences, i.e., episodic\nmemory (EM), has had relatively little focus. Only recently has EM-like\nfunctionality been added to Deep Learning (DL) models, e.g., Neural Turing\nMachine, Memory Networks. However, in these cases: a) EM is implemented as a\nseparate module, which entails substantial data movement (and so, time and\npower) between the DL net itself and EM; and b) individual items are stored\nlocalistically within the EM, precluding realizing the exponential\nrepresentational efficiency of distributed over localist coding. We describe\nSparsey, an unsupervised, hierarchical, spatial/spatiotemporal associative\nmemory model differing fundamentally from mainstream ML models, most crucially,\nin its use of sparse distributed representations (SDRs), or, cell assemblies,\nwhich admits an extremely efficient, single-trial learning algorithm that maps\ninput similarity into code space similarity (measured as intersection). SDRs of\nindividual inputs are stored in superposition and because similarity is\npreserved, the patterns of intersections over the assigned codes reflect the\nsimilarity, i.e., statistical, structure, of all orders, not simply pairwise,\nover the inputs. Thus, SM, i.e., a generative model, is built as a\ncomputationally free side effect of the act of storing episodic memory traces\nof individual inputs, either spatial patterns or sequences. We report initial\nresults on MNIST and on the Weizmann video event recognition benchmarks. While\nwe have not yet attained SOTA class accuracy, learning takes only minutes on a\nsingle CPU.\n", "versions": [{"version": "v1", "created": "Sat, 21 Oct 2017 17:07:59 GMT"}], "update_date": "2017-10-24", "authors_parsed": [["Rinkus", "Rod", ""], ["Leveille", "Jasmin", ""]]}, {"id": "1710.08037", "submitter": "Ricardo Bru\\~na", "authors": "Ricardo Bru\\~na, Fernando Maest\\'u, Ernesto Pereda", "title": "Phase Locking Value revisited: teaching new tricks to an old dog", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.SP nlin.CD physics.data-an q-bio.NC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Despite the increase in calculation power in the last decades, the estimation\nof brain connectivity is still a tedious task. The high computational cost of\nthe algorithms escalates with the square of the number of signals evaluated,\nusually in the range of thousands. In this work we propose a re-formulation of\na widely used algorithm that allows the estimation of whole brain connectivity\nin much smaller times. We start from the original implementation of Phase\nLocking Value (PLV) and re-formulated it in a highly computational efficient\nway. Besides, this formulation stresses its strong similarity with coherence,\nwhich we used to introduce two new metrics insensitive to zero lag\nsynchronization, the imaginary part of PLV (iPLV) and its corrected counterpart\n(ciPLV). The new implementation of PLV avoids some highly CPU-expensive\noperations, and achieved a 100-fold speedup over the original algorithm. The\nnew derived metrics were highly robust in the presence of volume conduction.\nciPLV, in particular, proved capable of ignoring zero-lag connectivity, while\ncorrectly estimating nonzero-lag connectivity. Our implementation of PLV makes\nit possible to calculate whole-brain connectivity in much shorter times. The\nresults of the simulations using ciPLV suggest that this metric is ideal to\nmeasure synchronization in the presence of volume conduction or source leakage\neffects.\n", "versions": [{"version": "v1", "created": "Sun, 22 Oct 2017 23:40:29 GMT"}, {"version": "v2", "created": "Wed, 21 Feb 2018 18:01:58 GMT"}, {"version": "v3", "created": "Thu, 28 Jun 2018 12:12:46 GMT"}], "update_date": "2018-06-29", "authors_parsed": [["Bru\u00f1a", "Ricardo", ""], ["Maest\u00fa", "Fernando", ""], ["Pereda", "Ernesto", ""]]}, {"id": "1710.08384", "submitter": "Diego Mateos", "authors": "D.M. Mateos, R.Wennberg, R. Guevara, and J.L. Perez Velazquez", "title": "Consciousness as a global property of brain dynamic activity", "comments": "13 pages, 5 figures", "journal-ref": "Phys. Rev. E 96, 062410 (2017)", "doi": "10.1103/PhysRevE.96.062410", "report-no": null, "categories": "q-bio.NC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We seek general principles of the structure of the cellular collective\nactivity associated with conscious awareness. Can we obtain evidence for\nfeatures of the optimal brain organization that allows for adequate processing\nof stimuli and that may guide the emergence of cognition and consciousness?\nAnalysing brain recordings in conscious and unconscious states, we followed\ninitially the classic approach in physics when it comes to understanding\ncollective behaviours of systems composed of a myriad of units: the assessment\nof the number of possible configurations (microstates) that the system can\nadopt, for which we use a global entropic measure associated with the number of\nconnected brain regions. Having found maximal entropy in conscious states, we\nthen inspected the microscopic nature of the configurations of connections\nusing an adequate complexity measure, and found higher complexity in states\ncharacterised not only by conscious awareness but also by subconscious\ncognitive processing, such as sleep stages. Our observations indicate that\nconscious awareness is associated with maximal global (macroscopic) entropy and\nwith the short time scale (microscopic) complexity of the configurations of\nconnected brain networks in pathological unconscious states (seizures and\ncoma), but the microscopic view captures the high complexity in physiological\nunconscious states (sleep) where there is information processing. As such, our\nresults support the global nature of conscious awareness, as advocated by\nseveral theories of cognition. We thus hope that our studies represent\npreliminary steps to reveal aspects of the structure of cognition that leads to\nconscious awareness.\n", "versions": [{"version": "v1", "created": "Fri, 13 Oct 2017 14:07:50 GMT"}], "update_date": "2017-12-27", "authors_parsed": [["Mateos", "D. M.", ""], ["Wennberg", "R.", ""], ["Guevara", "R.", ""], ["Velazquez", "J. L. Perez", ""]]}, {"id": "1710.08916", "submitter": "William Softky Ph.D.", "authors": "William Softky, Criscillia Benford", "title": "Sensory Metrics of Neuromechanical Trust", "comments": "59 pages, 14 figures", "journal-ref": "Softky, W. & Benford C. (2017). \"Sensory Metrics of\n  Neuromechanical Trust.\" Neural Computation 29, 2293-2351", "doi": "10.1162/NECO_a_00988", "report-no": null, "categories": "q-bio.NC nlin.AO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Today digital sources supply an unprecedented component of human sensorimotor\ndata, the consumption of which is correlated with poorly understood maladies\nsuch as Internet Addiction Disorder and Internet Gaming Disorder. This paper\noffers a mathematical understanding of human sensorimotor processing as\nmultiscale, continuous-time vibratory interaction. We quantify human\ninformational needs using the signal processing metrics of entropy, noise,\ndimensionality, continuity, latency, and bandwidth. Using these metrics, we\ndefine the trust humans experience as a primitive statistical algorithm\nprocessing finely grained sensorimotor data from neuromechanical interaction.\nThis definition of neuromechanical trust implies that artificial sensorimotor\ninputs and interactions that attract low-level attention through frequent\ndiscontinuities and enhanced coherence will decalibrate a brain's\nrepresentation of its world over the long term by violating the implicit\nstatistical contract for which self-calibration evolved. This approach allows\nus to model addiction in general as the result of homeostatic regulation gone\nawry in novel environments and digital dependency as a sub-case in which the\ndecalibration caused by digital sensorimotor data spurs yet more consumption of\nthem. We predict that institutions can use these sensorimotor metrics to\nquantify media richness to improve employee well-being; that dyads and\nfamily-size groups will bond and heal best through low-latency, high-resolution\nmultisensory interaction such as shared meals and reciprocated touch; and that\nindividuals can improve sensory and sociosensory resolution through deliberate\nsensory reintegration practices. We conclude that we humans are the victims of\nour own success, our hands so skilled they fill the world with captivating\nthings, our eyes so innocent they follow eagerly.\n", "versions": [{"version": "v1", "created": "Thu, 19 Oct 2017 17:13:42 GMT"}], "update_date": "2017-10-26", "authors_parsed": [["Softky", "William", ""], ["Benford", "Criscillia", ""]]}, {"id": "1710.08961", "submitter": "Milad Makkie", "authors": "Milad Makkie, Heng Huang, Yu Zhao, Athanasios V. Vasilakos, Tianming\n  Liu", "title": "Fast and Scalable Distributed Deep Convolutional Autoencoder for fMRI\n  Big Data Analytics", "comments": "This work is submitted to SIGKDD 2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.LG cs.NE q-bio.NC stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In recent years, analyzing task-based fMRI (tfMRI) data has become an\nessential tool for understanding brain function and networks. However, due to\nthe sheer size of tfMRI data, its intrinsic complex structure, and lack of\nground truth of underlying neural activities, modeling tfMRI data is hard and\nchallenging. Previously proposed data-modeling methods including Independent\nComponent Analysis (ICA) and Sparse Dictionary Learning only provided a weakly\nestablished model based on blind source separation under the strong assumption\nthat original fMRI signals could be linearly decomposed into time series\ncomponents with corresponding spatial maps. Meanwhile, analyzing and learning a\nlarge amount of tfMRI data from a variety of subjects has been shown to be very\ndemanding but yet challenging even with technological advances in computational\nhardware. Given the Convolutional Neural Network (CNN), a robust method for\nlearning high-level abstractions from low-level data such as tfMRI time series,\nin this work we propose a fast and scalable novel framework for distributed\ndeep Convolutional Autoencoder model. This model aims to both learn the complex\nhierarchical structure of the tfMRI data and to leverage the processing power\nof multiple GPUs in a distributed fashion. To implement such a model, we have\ncreated an enhanced processing pipeline on the top of Apache Spark and\nTensorflow library, leveraging from a very large cluster of GPU machines.\nExperimental data from applying the model on the Human Connectome Project (HCP)\nshow that the proposed model is efficient and scalable toward tfMRI big data\nanalytics, thus enabling data-driven extraction of hierarchical neuroscientific\ninformation from massive fMRI big data in the future.\n", "versions": [{"version": "v1", "created": "Tue, 24 Oct 2017 19:35:51 GMT"}, {"version": "v2", "created": "Wed, 13 Dec 2017 07:46:58 GMT"}, {"version": "v3", "created": "Sun, 4 Mar 2018 21:31:55 GMT"}], "update_date": "2018-03-06", "authors_parsed": [["Makkie", "Milad", ""], ["Huang", "Heng", ""], ["Zhao", "Yu", ""], ["Vasilakos", "Athanasios V.", ""], ["Liu", "Tianming", ""]]}, {"id": "1710.09118", "submitter": "Chang Sub Kim", "authors": "Chang Sub Kim", "title": "Recognition Dynamics in the Brain under the Free Energy Principle", "comments": "34 pages, 5 figures; Revised; Figure added", "journal-ref": "Neural Computation 30, 2616-2659 (2018);\n  https://doi.org/10.1162/neco_a_01115", "doi": null, "report-no": null, "categories": "q-bio.NC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We formulate the computational processes of perception in the framework of\nthe principle of least action by postulating the theoretical action as a time\nintegral of the free energy in the brain sciences. The free energy principle is\naccordingly rephrased as that for autopoietic grounds all viable organisms\nattempt to minimize the sensory uncertainty about the unpredictable environment\nover a temporal horizon. By varying the informational action, we derive the\nbrain's recognition dynamics (RD) which conducts Bayesian filtering of the\nexternal causes from noisy sensory inputs. Consequently, we effectively cast\nthe gradient-descent scheme of minimizing the free energy into Hamiltonian\nmechanics by addressing only positions and momenta of the organisms'\nrepresentations of the causal environment. To manifest the utility of our\ntheory, we show how the RD may be implemented in a neuronally based biophysical\nmodel at a single-cell level and subsequently in a coarse-grained, hierarchical\narchitecture of the brain. We also present formal solutions to the RD for a\nmodel brain in linear regime and analyze the perceptual trajectories around\nattractors in neural state space.\n", "versions": [{"version": "v1", "created": "Wed, 25 Oct 2017 08:34:02 GMT"}, {"version": "v2", "created": "Thu, 2 Nov 2017 06:46:02 GMT"}, {"version": "v3", "created": "Sat, 20 Jan 2018 06:21:29 GMT"}], "update_date": "2019-02-21", "authors_parsed": [["Kim", "Chang Sub", ""]]}, {"id": "1710.09139", "submitter": "Martin V\\\"olker", "authors": "Martin V\\\"olker, Robin T. Schirrmeister, Lukas D. J. Fiederer, Wolfram\n  Burgard, Tonio Ball", "title": "Deep Transfer Learning for Error Decoding from Non-Invasive EEG", "comments": "6 pages, 9 figures, The 6th International Winter Conference on\n  Brain-Computer Interface 2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.HC q-bio.NC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We recorded high-density EEG in a flanker task experiment (31 subjects) and\nan online BCI control paradigm (4 subjects). On these datasets, we evaluated\nthe use of transfer learning for error decoding with deep convolutional neural\nnetworks (deep ConvNets). In comparison with a regularized linear discriminant\nanalysis (rLDA) classifier, ConvNets were significantly better in both intra-\nand inter-subject decoding, achieving an average accuracy of 84.1 % within\nsubject and 81.7 % on unknown subjects (flanker task). Neither method was,\nhowever, able to generalize reliably between paradigms. Visualization of\nfeatures the ConvNets learned from the data showed plausible patterns of brain\nactivity, revealing both similarities and differences between the different\nkinds of errors. Our findings indicate that deep learning techniques are useful\nto infer information about the correctness of action in BCI applications,\nparticularly for the transfer of pre-trained classifiers to new recording\nsessions or subjects.\n", "versions": [{"version": "v1", "created": "Wed, 25 Oct 2017 09:47:04 GMT"}, {"version": "v2", "created": "Thu, 14 Dec 2017 16:46:48 GMT"}, {"version": "v3", "created": "Wed, 10 Jan 2018 09:13:24 GMT"}], "update_date": "2018-01-11", "authors_parsed": [["V\u00f6lker", "Martin", ""], ["Schirrmeister", "Robin T.", ""], ["Fiederer", "Lukas D. J.", ""], ["Burgard", "Wolfram", ""], ["Ball", "Tonio", ""]]}, {"id": "1710.09929", "submitter": "Trang-Anh Estelle Nghiem", "authors": "Trang-Anh Nghiem, Olivier Marre, Alain Destexhe and Ulisse Ferrari", "title": "Pairwise Ising model analysis of human cortical neuron recordings", "comments": "8 pages, 3 figures, Geometric Science of Information 2017 conference", "journal-ref": null, "doi": "10.1007/978-3-319-68445-1_30", "report-no": null, "categories": "q-bio.NC cond-mat.dis-nn", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  During wakefulness and deep sleep brain states, cortical neural networks show\na different behavior, with the second characterized by transients of high\nnetwork activity. To investigate their impact on neuronal behavior, we apply a\npairwise Ising model analysis by inferring the maximum entropy model that\nreproduces single and pairwise moments of the neuron's spiking activity. In\nthis work we first review the inference algorithm introduced in Ferrari,Phys.\nRev. E (2016). We then succeed in applying the algorithm to infer the model\nfrom a large ensemble of neurons recorded by multi-electrode array in human\ntemporal cortex. We compare the Ising model performance in capturing the\nstatistical properties of the network activity during wakefulness and deep\nsleep. For the latter, the pairwise model misses relevant transients of high\nnetwork activity, suggesting that additional constraints are necessary to\naccurately model the data.\n", "versions": [{"version": "v1", "created": "Thu, 26 Oct 2017 22:04:53 GMT"}], "update_date": "2017-10-30", "authors_parsed": [["Nghiem", "Trang-Anh", ""], ["Marre", "Olivier", ""], ["Destexhe", "Alain", ""], ["Ferrari", "Ulisse", ""]]}, {"id": "1710.10153", "submitter": "Kelly Iarosz", "authors": "E.L. Lameu, E.E.N. Macau, F.S. Borges, K.C. Iarosz, I.L. Caldas, R.R.\n  Borges, P.R. Protachevicz, R.L. Viana, A.M. Batista", "title": "Alterations in brain connectivity due to plasticity and synaptic delay", "comments": null, "journal-ref": null, "doi": "10.1140/epjst/e2018-00090-6", "report-no": null, "categories": "q-bio.NC physics.bio-ph", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Brain plasticity refers to brain's ability to change neuronal connections, as\na result of environmental stimuli, new experiences, or damage. In this work, we\nstudy the effects of the synaptic delay on both the coupling strengths and\nsynchronisation in a neuronal network with synaptic plasticity. We build a\nnetwork of Hodgkin-Huxley neurons, where the plasticity is given by the Hebbian\nrules. We verify that without time delay the excitatory synapses became\nstronger from the high frequency to low frequency neurons and the inhibitory\nsynapses increases in the opposite way, when the delay is increased the network\npresents a non-trivial topology. Regarding the synchronisation, only for small\nvalues of the synaptic delay this phenomenon is observed.\n", "versions": [{"version": "v1", "created": "Fri, 27 Oct 2017 14:21:21 GMT"}], "update_date": "2018-11-14", "authors_parsed": [["Lameu", "E. L.", ""], ["Macau", "E. E. N.", ""], ["Borges", "F. S.", ""], ["Iarosz", "K. C.", ""], ["Caldas", "I. L.", ""], ["Borges", "R. R.", ""], ["Protachevicz", "P. R.", ""], ["Viana", "R. L.", ""], ["Batista", "A. M.", ""]]}, {"id": "1710.10428", "submitter": "Dongsung Huh", "authors": "Dongsung Huh", "title": "Generalized Phase Representation of Integrate-and-Fire Models", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.DS q-bio.NC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The quadratic integrate-and-fire (QIF) model captures the normal form\nbifurcation dynamics of Type-I neurons found in cortex. Remarkably, this model\nis known to have a dual equivalent representation in terms of phase, called the\n$\\theta$-model, which has advantages for numerical simulations and analysis\nover the QIF model. Here, I investigate the nature of the dual representation\nand derive the general phase model expression for all integrate-and-fire\nmodels. Moreover, I show the condition for which the phase models exhibit\ndiscontinuous onset firing rate, the hallmark of Type-II spiking neurons.\n", "versions": [{"version": "v1", "created": "Sat, 28 Oct 2017 09:03:43 GMT"}, {"version": "v2", "created": "Wed, 22 Nov 2017 07:31:24 GMT"}, {"version": "v3", "created": "Thu, 23 Nov 2017 08:34:46 GMT"}], "update_date": "2017-11-27", "authors_parsed": [["Huh", "Dongsung", ""]]}, {"id": "1710.10548", "submitter": "Stephanie Elizabeth Palmer", "authors": "Joseph A. Lombardo, Matthew V. Macellaio, Bing Liu, Stephanie E.\n  Palmer, and Leslie C. Osborne", "title": "State Dependence of Stimulus-Induced Variability Tuning in Macaque MT", "comments": "36 pages, 18 figures", "journal-ref": null, "doi": "10.1371/journal.pcbi.1006527", "report-no": null, "categories": "q-bio.NC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Behavioral states marked by varying levels of arousal and attention modulate\nsome properties of cortical responses (e.g. average firing rates or pairwise\ncorrelations), yet it is not fully understood what drives these response\nchanges and how they might affect downstream stimulus decoding. Here we show\nthat changes in state modulate the tuning of response variance-to-mean ratios\n(Fano factors) in a fashion that is neither predicted by a Poisson spiking\nmodel nor changes in the mean firing rate, with a substantial effect on\nstimulus discriminability. We recorded motion-sensitive neurons in middle\ntemporal cortex (MT) in two states: alert fixation and light, opioid\nanesthesia. Anesthesia tended to lower average spike counts, without decreasing\ntrial-to-trial variability compared to the alert state. Under anesthesia,\nwithin-trial fluctuations in excitability were correlated over longer time\nscales compared to the alert state, creating supra-Poisson Fano factors. In\ncontrast, alert-state MT neurons have higher mean firing rates and largely\nsub-Poisson variability that is stimulus-dependent and cannot be explained by\nfiring rate differences alone. The absence of such stimulus-induced variability\ntuning in the anesthetized state suggests different sources of variability\nbetween states. A simple model explains state-dependent shifts in the\ndistribution of observed Fano factors via a suppression in the variance of gain\nfluctuations in the alert state. A population model with stimulus-induced\nvariability tuning and behaviorally constrained information-limiting\ncorrelations explores the potential enhancement in stimulus discriminability by\nthe cortical population in the alert state.\n", "versions": [{"version": "v1", "created": "Sun, 29 Oct 2017 01:12:17 GMT"}, {"version": "v2", "created": "Wed, 3 Oct 2018 23:13:18 GMT"}], "update_date": "2018-11-21", "authors_parsed": [["Lombardo", "Joseph A.", ""], ["Macellaio", "Matthew V.", ""], ["Liu", "Bing", ""], ["Palmer", "Stephanie E.", ""], ["Osborne", "Leslie C.", ""]]}, {"id": "1710.11227", "submitter": "Ivan Yu. Tyukin", "authors": "Ivan Y. Tyukin, Alexander N. Gorban, Carlos Calvo, Julia Makarova,\n  Valeri A. Makarov", "title": "High-dimensional brain. A tool for encoding and rapid learning of\n  memories by single neurons", "comments": null, "journal-ref": null, "doi": "10.1007/s11538-018-0415-5", "report-no": null, "categories": "q-bio.NC", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Codifying memories is one of the fundamental problems of modern Neuroscience.\nThe functional mechanisms behind this phenomenon remain largely unknown.\nExperimental evidence suggests that some of the memory functions are performed\nby stratified brain structures such as, e.g., the hippocampus. In this\nparticular case, single neurons in the CA1 region receive a highly\nmultidimensional input from the CA3 area, which is a hub for information\nprocessing. We thus assess the implication of the abundance of neuronal\nsignalling routes converging onto single cells on the information processing.\nWe show that single neurons can selectively detect and learn arbitrary\ninformation items, given that they operate in high dimensions. The argument is\nbased on Stochastic Separation Theorems and the concentration of measure\nphenomena. We demonstrate that a simple enough functional neuronal model is\ncapable of explaining: i) the extreme selectivity of single neurons to the\ninformation content, ii) simultaneous separation of several uncorrelated\nstimuli or informational items from a large set, and iii) dynamic learning of\nnew items by associating them with already \"known\" ones. These results\nconstitute a basis for organization of complex memories in ensembles of single\nneurons. Moreover, they show that no a priori assumptions on the structural\norganization of neuronal ensembles are necessary for explaining basic concepts\nof static and dynamic memories.\n", "versions": [{"version": "v1", "created": "Mon, 30 Oct 2017 20:30:00 GMT"}, {"version": "v2", "created": "Sat, 27 Jan 2018 11:28:38 GMT"}], "update_date": "2018-03-28", "authors_parsed": [["Tyukin", "Ivan Y.", ""], ["Gorban", "Alexander N.", ""], ["Calvo", "Carlos", ""], ["Makarova", "Julia", ""], ["Makarov", "Valeri A.", ""]]}, {"id": "1710.11279", "submitter": "Samir Chowdhury", "authors": "Samir Chowdhury, Bowen Dai, and Facundo M\\'emoli", "title": "The Importance of Forgetting: Limiting Memory Improves Recovery of\n  Topological Characteristics from Neural Data", "comments": null, "journal-ref": null, "doi": "10.1371/journal.pone.0202561", "report-no": null, "categories": "q-bio.NC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We develop of a line of work initiated by Curto and Itskov towards\nunderstanding the amount of information contained in the spike trains of\nhippocampal place cells via topology considerations. Previously, it was\nestablished that simply knowing which groups of place cells fire together in an\nanimal's hippocampus is sufficient to extract the global topology of the\nanimal's physical environment. We model a system where collections of place\ncells group and ungroup according to short-term plasticity rules. In\nparticular, we obtain the surprising result that in experiments with spurious\nfiring, the accuracy of the extracted topological information decreases with\nthe persistence (beyond a certain regime) of the cell groups. This suggests\nthat synaptic transience, or forgetting, is a mechanism by which the brain\ncounteracts the effects of spurious place cell activity.\n", "versions": [{"version": "v1", "created": "Tue, 31 Oct 2017 00:28:11 GMT"}], "update_date": "2018-11-21", "authors_parsed": [["Chowdhury", "Samir", ""], ["Dai", "Bowen", ""], ["M\u00e9moli", "Facundo", ""]]}, {"id": "1710.11413", "submitter": "Johann H. Mart\\'inez", "authors": "J. H. Mart\\'inez, J. M. Buld\\'u, D. Papo, F. De Vico Fallani and M.\n  Chavez", "title": "Role of inter-hemispheric connections in functional brain networks", "comments": "12 pages 5 figures (main), 9 pages and 8 figures (Supp Info)", "journal-ref": null, "doi": null, "report-no": null, "categories": "q-bio.NC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Today the human brain can be modeled as a graph where nodes represent\ndifferent regions and links stand for statistical interactions between their\nactivities as recorded by different neuroimaging techniques. Empirical studies\nhave lead to the hypothesis that brain functions rely on the coordination of a\nscattered mosaic of functionally specialized brain regions (modules or\nsub-networks), forming a web-like structure of coordinated assemblies (a\nnetwork of networks). The study of brain dynamics would therefore benefit from\nan inspection of how functional sub-networks interact between them. In this\npaper, we model the brain as an interconnected system composed of two specific\nsub-networks, the left (L) and right (R) hemispheres, which compete with each\nother for centrality, a topological measure of importance in a networked\nsystem. Specifically, we consideredfunctional brain networks derived from\nhigh-density electroencephalographic (EEG) recordings and investigated how node\ncentrality is shaped by interhemispheric connections. Our results show that the\ndistribution of centrality strongly depends on the number of functional\nconnections between hemispheres and the way these connections are distributed.\nAdditionally, we investigated the consequences of node failure on hemispherical\ncentrality, and showed how the abundance of inter-hemispheric links favors the\nfunctional balance of centrality distribution between the hemispheres.\n", "versions": [{"version": "v1", "created": "Tue, 31 Oct 2017 11:37:55 GMT"}], "update_date": "2017-11-01", "authors_parsed": [["Mart\u00ednez", "J. H.", ""], ["Buld\u00fa", "J. M.", ""], ["Papo", "D.", ""], ["Fallani", "F. De Vico", ""], ["Chavez", "M.", ""]]}, {"id": "1710.11438", "submitter": "Arthur Mensch", "authors": "Arthur Mensch (PARIETAL, NEUROSPIN), Julien Mairal (Thoth, LJK),\n  Danilo Bzdok, Bertrand Thirion (PARIETAL, NEUROSPIN), Ga\\\"el Varoquaux\n  (PARIETAL, NEUROSPIN)", "title": "Learning Neural Representations of Human Cognition across Many fMRI\n  Studies", "comments": "Advances in Neural Information Processing Systems, Dec 2017, Long\n  Beach, United States. 2017", "journal-ref": "Advances in Neural Information Processing Systems, 2017", "doi": null, "report-no": null, "categories": "stat.ML cs.LG q-bio.NC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Cognitive neuroscience is enjoying rapid increase in extensive public\nbrain-imaging datasets. It opens the door to large-scale statistical models.\nFinding a unified perspective for all available data calls for scalable and\nautomated solutions to an old challenge: how to aggregate heterogeneous\ninformation on brain function into a universal cognitive system that relates\nmental operations/cognitive processes/psychological tasks to brain networks? We\ncast this challenge in a machine-learning approach to predict conditions from\nstatistical brain maps across different studies. For this, we leverage\nmulti-task learning and multi-scale dimension reduction to learn\nlow-dimensional representations of brain images that carry cognitive\ninformation and can be robustly associated with psychological stimuli. Our\nmulti-dataset classification model achieves the best prediction performance on\nseveral large reference datasets, compared to models without cognitive-aware\nlow-dimension representations, it brings a substantial performance boost to the\nanalysis of small datasets, and can be introspected to identify universal\ntemplate cognitive concepts.\n", "versions": [{"version": "v1", "created": "Tue, 31 Oct 2017 12:51:36 GMT"}, {"version": "v2", "created": "Sat, 11 Nov 2017 03:16:14 GMT"}], "update_date": "2019-05-16", "authors_parsed": [["Mensch", "Arthur", "", "PARIETAL, NEUROSPIN"], ["Mairal", "Julien", "", "Thoth, LJK"], ["Bzdok", "Danilo", "", "PARIETAL, NEUROSPIN"], ["Thirion", "Bertrand", "", "PARIETAL, NEUROSPIN"], ["Varoquaux", "Ga\u00ebl", "", "PARIETAL, NEUROSPIN"]]}, {"id": "1710.11569", "submitter": "Adam Noel", "authors": "Adam Noel, Shayan Monabbati, Dimitrios Makrakis, Andrew W. Eckford", "title": "Timing Control of Single Neuron Spikes with Optogenetic Stimulation", "comments": "6 pages, 8 figures, 3 tables. To be presented at the 2018 IEEE\n  International Conference on Communications (IEEE ICC 2018) in May 2018", "journal-ref": null, "doi": "10.1109/ICC.2018.8422667", "report-no": null, "categories": "q-bio.NC physics.bio-ph", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper predicts the ability to externally control the firing times of a\ncortical neuron whose behavior follows the Izhikevich neuron model. The\nIzhikevich neuron model provides an efficient and biologically plausible method\nto track a cortical neuron's membrane potential and its firing times. The\nexternal control is a simple optogenetic model represented by a constant\ncurrent source that can be turned on or off. This paper considers a firing\nfrequency that is sufficiently low for the membrane potential to return to its\nresting potential after it fires. The time required for the neuron to charge\nand for the neuron to recover to the resting potential are fitted to functions\nof the Izhikevich neuron model parameters. Results show that linear functions\nof the model parameters can be used to predict the charging times with some\naccuracy and are sufficient to estimate the highest firing frequency achievable\nwithout interspike interference.\n", "versions": [{"version": "v1", "created": "Tue, 31 Oct 2017 16:33:45 GMT"}, {"version": "v2", "created": "Tue, 13 Feb 2018 08:29:49 GMT"}], "update_date": "2018-09-06", "authors_parsed": [["Noel", "Adam", ""], ["Monabbati", "Shayan", ""], ["Makrakis", "Dimitrios", ""], ["Eckford", "Andrew W.", ""]]}, {"id": "1710.11612", "submitter": "Zachary Kilpatrick PhD", "authors": "Nikhil Krishnan, Daniel B Poll, and Zachary P Kilpatrick", "title": "Synaptic efficacy shapes resource limitations in working memory", "comments": "26 pages, 12 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "q-bio.NC nlin.PS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Working memory (WM) is limited in its temporal length and capacity. Classic\nconceptions of WM capacity assume the system possesses a finite number of\nslots, but recent evidence suggests WM may be a continuous resource. Resource\nmodels typically assume there is no hard upper bound on the number of items\nthat can be stored, but WM fidelity decreases with the number of items. We\nanalyze a neural field model of multi-item WM that associates each item with\nthe location of a bump in a finite spatial domain, considering items that span\na one-dimensional continuous feature space. Our analysis relates the neural\narchitecture of the network to accumulated errors and capacity limitations\narising during the delay period of a multi-item WM task. Networks with stronger\nsynapses support wider bumps that interact more, whereas networks with weaker\nsynapses support narrower bumps that are more susceptible to noise\nperturbations. There is an optimal synaptic strength that both limits bump\ninteraction events and the effects of noise perturbations. This optimum shifts\nto weaker synapses as the number of items stored in the network is increased.\nOur model not only provides a neural circuit explanation for WM capacity, but\nalso speaks to how capacity relates to the arrangement of stored items in a\nfeature space.\n", "versions": [{"version": "v1", "created": "Tue, 31 Oct 2017 17:43:10 GMT"}, {"version": "v2", "created": "Sun, 11 Feb 2018 04:50:55 GMT"}], "update_date": "2018-02-13", "authors_parsed": [["Krishnan", "Nikhil", ""], ["Poll", "Daniel B", ""], ["Kilpatrick", "Zachary P", ""]]}]