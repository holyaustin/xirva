[{"id": "2010.00308", "submitter": "Takuya Hayashi", "authors": "Takuya Hayashi, Yujie Hou, Matthew F Glasser, Joonas A Autio, Kenneth\n  Knoblauch, Miho Inoue-Murayama, Tim Coalson, Essa Yacoub, Stephen Smith,\n  Henry Kennedy, and David C Van Essen", "title": "The NonHuman Primate Neuroimaging & Neuroanatomy Project", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "q-bio.NC", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Multi-modal neuroimaging projects are advancing our understanding of human\nbrain architecture, function, connectivity using high-quality non-invasive data\nfrom many subjects. However, ground truth validation of connectivity using\ninvasive tracers is not feasible in humans. Our NonHuman Primate Neuroimaging &\nNeuroanatomy Project (NHP_NNP) is an international effort (6 laboratories in 5\ncountries) to: (i) acquire and analyze high-quality multi-modal brain imaging\ndata of macaque and marmoset monkeys using protocols and methods adapted from\nthe HCP; (ii) acquire quantitative invasive tract-tracing data for cortical and\nsubcortical projections to cortical areas; and (iii) map the distributions of\ndifferent brain cell types with immunocytochemical stains to better define\nbrain areal boundaries. We are acquiring high-resolution structural,\nfunctional, and diffusion MRI data together with behavioral measures from over\n100 individual macaques and marmosets in order to generate non-invasive\nmeasures of brain architecture such as myelin and cortical thickness maps, as\nwell as functional and diffusion tractography-based connectomes. We are using\nclassical and next-generation anatomical tracers to generate quantitative\nconnectivity maps based on brain-wide counting of labeled cortical and\nsubcortical neurons, providing ground truth measures of connectivity. Advanced\nstatistical modeling techniques address the consistency of both kinds of data\nacross individuals, allowing comparison of tracer-based and non-invasive\nMRI-based connectivity measures. We aim to develop improved cortical and\nsubcortical areal atlases by combining histological and imaging methods.\nFinally, we are collecting genetic and sociality-associated behavioral data in\nall animals in an effort to understand how genetic variation shapes the\nconnectome and behavior.\n", "versions": [{"version": "v1", "created": "Thu, 1 Oct 2020 11:38:14 GMT"}, {"version": "v2", "created": "Mon, 14 Dec 2020 04:42:11 GMT"}, {"version": "v3", "created": "Mon, 11 Jan 2021 02:24:33 GMT"}], "update_date": "2021-01-12", "authors_parsed": [["Hayashi", "Takuya", ""], ["Hou", "Yujie", ""], ["Glasser", "Matthew F", ""], ["Autio", "Joonas A", ""], ["Knoblauch", "Kenneth", ""], ["Inoue-Murayama", "Miho", ""], ["Coalson", "Tim", ""], ["Yacoub", "Essa", ""], ["Smith", "Stephen", ""], ["Kennedy", "Henry", ""], ["Van Essen", "David C", ""]]}, {"id": "2010.00504", "submitter": "Sarah Marzen", "authors": "Alexander Hsu and Sarah Marzen", "title": "Time cells might be optimized for predictive capacity, not redundancy\n  reduction or memory capacity", "comments": null, "journal-ref": null, "doi": "10.1103/PhysRevE.102.062404", "report-no": null, "categories": "q-bio.NC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recently, researchers have found time cells in the hippocampus that appear to\ncontain information about the timing of past events. Some researchers have\nargued that time cells are taking a Laplace transform of their input in order\nto reconstruct the past stimulus. We argue that stimulus prediction, not\nstimulus reconstruction or redundancy reduction, is in better agreement with\nobserved responses of time cells. In the process, we introduce new analyses of\nnonlinear, continuous-time reservoirs that model these time cells.\n", "versions": [{"version": "v1", "created": "Thu, 1 Oct 2020 15:53:43 GMT"}], "update_date": "2020-12-30", "authors_parsed": [["Hsu", "Alexander", ""], ["Marzen", "Sarah", ""]]}, {"id": "2010.00516", "submitter": "Meenakshi Khosla", "authors": "Meenakshi Khosla, Gia H. Ngo, Keith Jamison, Amy Kuceyeski and Mert R.\n  Sabuncu", "title": "Neural encoding with visual attention", "comments": "NeurIPS 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG q-bio.NC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Visual perception is critically influenced by the focus of attention. Due to\nlimited resources, it is well known that neural representations are biased in\nfavor of attended locations. Using concurrent eye-tracking and functional\nMagnetic Resonance Imaging (fMRI) recordings from a large cohort of human\nsubjects watching movies, we first demonstrate that leveraging gaze\ninformation, in the form of attentional masking, can significantly improve\nbrain response prediction accuracy in a neural encoding model. Next, we propose\na novel approach to neural encoding by including a trainable soft-attention\nmodule. Using our new approach, we demonstrate that it is possible to learn\nvisual attention policies by end-to-end learning merely on fMRI response data,\nand without relying on any eye-tracking. Interestingly, we find that attention\nlocations estimated by the model on independent data agree well with the\ncorresponding eye fixation patterns, despite no explicit supervision to do so.\nTogether, these findings suggest that attention modules can be instrumental in\nneural encoding models of visual stimuli.\n", "versions": [{"version": "v1", "created": "Thu, 1 Oct 2020 16:04:21 GMT"}], "update_date": "2020-10-02", "authors_parsed": [["Khosla", "Meenakshi", ""], ["Ngo", "Gia H.", ""], ["Jamison", "Keith", ""], ["Kuceyeski", "Amy", ""], ["Sabuncu", "Mert R.", ""]]}, {"id": "2010.00525", "submitter": "David Lipshutz", "authors": "David Lipshutz, Yanis Bahroun, Siavash Golkar, Anirvan M. Sengupta,\n  Dmitri B. Chklovskii", "title": "A biologically plausible neural network for multi-channel Canonical\n  Correlation Analysis", "comments": "46 pages, 14 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "q-bio.NC cs.NE stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Cortical pyramidal neurons receive inputs from multiple distinct neural\npopulations and integrate these inputs in separate dendritic compartments. We\nexplore the possibility that cortical microcircuits implement Canonical\nCorrelation Analysis (CCA), an unsupervised learning method that projects the\ninputs onto a common subspace so as to maximize the correlations between the\nprojections. To this end, we seek a multi-channel CCA algorithm that can be\nimplemented in a biologically plausible neural network. For biological\nplausibility, we require that the network operates in the online setting and\nits synaptic update rules are local. Starting from a novel CCA objective\nfunction, we derive an online optimization algorithm whose optimization steps\ncan be implemented in a single-layer neural network with multi-compartmental\nneurons and local non-Hebbian learning rules. We also derive an extension of\nour online CCA algorithm with adaptive output rank and output whitening.\nInterestingly, the extension maps onto a neural network whose neural\narchitecture and synaptic updates resemble neural circuitry and synaptic\nplasticity observed experimentally in cortical pyramidal neurons.\n", "versions": [{"version": "v1", "created": "Thu, 1 Oct 2020 16:17:53 GMT"}, {"version": "v2", "created": "Fri, 13 Nov 2020 22:27:18 GMT"}, {"version": "v3", "created": "Thu, 11 Mar 2021 16:52:07 GMT"}, {"version": "v4", "created": "Fri, 26 Mar 2021 16:18:09 GMT"}], "update_date": "2021-03-29", "authors_parsed": [["Lipshutz", "David", ""], ["Bahroun", "Yanis", ""], ["Golkar", "Siavash", ""], ["Sengupta", "Anirvan M.", ""], ["Chklovskii", "Dmitri B.", ""]]}, {"id": "2010.00541", "submitter": "Ines Samengo Dr.", "authors": "Nicol\\'as Vattuone (1,2), Thomas Wachtler (1), In\\'es Samengo (1) ((1)\n  Department of Biology II, Ludwig-Maximilians-Universit\\\"at M\\\"unchen and\n  Bernstein Center for Computational Neuroscience, Munich, Germany. (2)\n  Department of Medical Physics and Instituto Balseiro, Centro At\\'omico\n  Bariloche, Argentina)", "title": "Perceptual spaces and their symmetries: The geometry of color space", "comments": "42 pages, 9 figures, 1 appendix. (v2) 47 pages, 10 figures, 1\n  appendix. (v3) Text modified after peer-review process. (v4) 34 pages, 1\n  appendix, 10 figures. Article accepted to be published at Mathematical\n  Neuroscience and Applications", "journal-ref": null, "doi": "10.46298/mna.7108", "report-no": null, "categories": "q-bio.NC", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Our sensory systems transform external signals into neural activity, thereby\nproducing percepts. We are endowed with an intuitive notion of similarity\nbetween percepts, that need not reflect the proximity of the physical\nproperties of the corresponding external stimuli. The quantitative\ncharacterization of the geometry of percepts is therefore an endeavour that\nmust be accomplished behaviorally. Here we characterized the geometry of color\nspace using discrimination and matching experiments. We proposed an\nindividually tailored metric defined in terms of the minimal chromatic\ndifference required for each observer to differentiate a stimulus from its\nsurround. Next, we showed that this perceptual metric was particularly adequate\nto describe two additional experiments, since it revealed the natural symmetry\nof perceptual computations. In one of the experiments, observers were required\nto discriminate two stimuli surrounded by a chromaticity that differed from\nthat of the tested stimuli. In the perceptual coordinates, the change in\ndiscrimination thresholds induced by the surround followed a simple law that\nonly depended on the perceptual distance between the surround and each of the\ntwo compared stimuli. In the other experiment, subjects were asked to match the\ncolor of two stimuli surrounded by two different chromaticities. Again, in the\nperceptual coordinates the induction effect produced by surrounds followed a\nsimple, symmetric law. We conclude that the individually-tailored notion of\nperceptual distance reveals the symmetry of the laws governing perceptual\ncomputations.\n", "versions": [{"version": "v1", "created": "Thu, 1 Oct 2020 16:52:29 GMT"}, {"version": "v2", "created": "Sat, 16 Jan 2021 22:11:26 GMT"}, {"version": "v3", "created": "Wed, 19 May 2021 14:26:16 GMT"}, {"version": "v4", "created": "Wed, 14 Jul 2021 14:38:57 GMT"}], "update_date": "2021-07-15", "authors_parsed": [["Vattuone", "Nicol\u00e1s", ""], ["Wachtler", "Thomas", ""], ["Samengo", "In\u00e9s", ""]]}, {"id": "2010.00659", "submitter": "Linden Parkes", "authors": "Linden Parkes, Tyler M. Moore, Monica E. Calkins, Matthew Cieslak,\n  David R. Roalf, Daniel H. Wolf, Ruben C. Gur, Raquel E. Gur, Theodore D.\n  Satterthwaite, Danielle S. Bassett", "title": "Network controllability in transmodal cortex predicts psychosis spectrum\n  symptoms", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "q-bio.NC", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  The psychosis spectrum is associated with structural dysconnectivity\nconcentrated in transmodal association cortex. However, understanding of this\npathophysiology has been limited by an exclusive focus on the direct\nconnections to a region. Using Network Control Theory, we measured variation in\nboth direct and indirect structural connections to a region to gain new\ninsights into the pathophysiology of the psychosis spectrum.\n  We used psychosis symptom data and structural connectivity in 1,068 youths\naged 8 to 22 years from the Philadelphia Neurodevelopmental Cohort. Applying a\nNetwork Control Theory metric called average controllability, we estimated each\nbrain region's capacity to leverage its direct and indirect structural\nconnections to control linear brain dynamics. Next, using non-linear\nregression, we determined the accuracy with which average controllability could\npredict negative and positive psychosis spectrum symptoms in out-of-sample\ntesting. We also compared prediction performance for average controllability\nversus strength, which indexes only direct connections to a region. Finally, we\nassessed how the prediction performance for psychosis spectrum symptoms varied\nover the functional hierarchy spanning unimodal to transmodal cortex.\n  Average controllability outperformed strength at predicting positive\npsychosis spectrum symptoms, demonstrating that indexing indirect structural\nconnections to a region improved prediction performance. Critically, improved\nprediction was concentrated in association cortex for average controllability,\nwhereas prediction performance for strength was uniform across the cortex,\nsuggesting that indexing indirect connections is crucial in association cortex.\n  Examining inter-individual variation in direct and indirect structural\nconnections to association cortex is crucial for accurate prediction of\npositive psychosis spectrum symptoms.\n", "versions": [{"version": "v1", "created": "Thu, 1 Oct 2020 19:40:42 GMT"}], "update_date": "2020-10-05", "authors_parsed": [["Parkes", "Linden", ""], ["Moore", "Tyler M.", ""], ["Calkins", "Monica E.", ""], ["Cieslak", "Matthew", ""], ["Roalf", "David R.", ""], ["Wolf", "Daniel H.", ""], ["Gur", "Ruben C.", ""], ["Gur", "Raquel E.", ""], ["Satterthwaite", "Theodore D.", ""], ["Bassett", "Danielle S.", ""]]}, {"id": "2010.00948", "submitter": "Narayan Puthanmadam Subramaniyam", "authors": "Narayan Puthanmadam Subramaniyam, Reik V. Donner, Davide Caron,\n  Gabriella Panuccio, Jari Hyttinen", "title": "Causal coupling inference from multivariate time series based on ordinal\n  partition transition networks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME physics.data-an q-bio.NC", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Identifying causal relationships is a challenging yet crucial problem in many\nfields of science like epidemiology, climatology, ecology, genomics, economics\nand neuroscience, to mention only a few. Recent studies have demonstrated that\nordinal partition transition networks (OPTNs) allow inferring the coupling\ndirection between two dynamical systems. In this work, we generalize this\nconcept to the study of the interactions among multiple dynamical systems and\nwe propose a new method to detect causality in multivariate observational data.\nBy applying this method to numerical simulations of coupled linear stochastic\nprocesses as well as two examples of interacting nonlinear dynamical systems\n(coupled Lorenz systems and a network of neural mass models), we demonstrate\nthat our approach can reliably identify the direction of interactions and the\nassociated coupling delays. Finally, we study real-world observational\nmicroelectrode array electrophysiology data from rodent brain slices to\nidentify the causal coupling structures underlying epileptiform activity. Our\nresults, both from simulations and real-world data, suggest that OPTNs can\nprovide a complementary and robust approach to infer causal effect networks\nfrom multivariate observational data.\n", "versions": [{"version": "v1", "created": "Fri, 2 Oct 2020 12:25:02 GMT"}, {"version": "v2", "created": "Tue, 6 Oct 2020 06:19:15 GMT"}, {"version": "v3", "created": "Wed, 7 Oct 2020 06:28:21 GMT"}, {"version": "v4", "created": "Tue, 1 Jun 2021 21:36:12 GMT"}], "update_date": "2021-06-03", "authors_parsed": [["Subramaniyam", "Narayan Puthanmadam", ""], ["Donner", "Reik V.", ""], ["Caron", "Davide", ""], ["Panuccio", "Gabriella", ""], ["Hyttinen", "Jari", ""]]}, {"id": "2010.01047", "submitter": "Beren Millidge Mr", "authors": "Beren Millidge, Alexander Tschantz, Anil Seth, Christopher L Buckley", "title": "Relaxing the Constraints on Predictive Coding Models", "comments": "02/10/20 initial upload; 10/10/20 minor fixes", "journal-ref": null, "doi": null, "report-no": null, "categories": "q-bio.NC cs.AI stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Predictive coding is an influential theory of cortical function which posits\nthat the principal computation the brain performs, which underlies both\nperception and learning, is the minimization of prediction errors. While\nmotivated by high-level notions of variational inference, detailed\nneurophysiological models of cortical microcircuits which can implements its\ncomputations have been developed. Moreover, under certain conditions,\npredictive coding has been shown to approximate the backpropagation of error\nalgorithm, and thus provides a relatively biologically plausible\ncredit-assignment mechanism for training deep networks. However, standard\nimplementations of the algorithm still involve potentially neurally implausible\nfeatures such as identical forward and backward weights, backward nonlinear\nderivatives, and 1-1 error unit connectivity. In this paper, we show that these\nfeatures are not integral to the algorithm and can be removed either directly\nor through learning additional sets of parameters with Hebbian update rules\nwithout noticeable harm to learning performance. Our work thus relaxes current\nconstraints on potential microcircuit designs and hopefully opens up new\nregions of the design-space for neuromorphic implementations of predictive\ncoding.\n", "versions": [{"version": "v1", "created": "Fri, 2 Oct 2020 15:21:37 GMT"}, {"version": "v2", "created": "Sat, 10 Oct 2020 14:09:12 GMT"}], "update_date": "2020-10-13", "authors_parsed": [["Millidge", "Beren", ""], ["Tschantz", "Alexander", ""], ["Seth", "Anil", ""], ["Buckley", "Christopher L", ""]]}, {"id": "2010.01231", "submitter": "Arun Das", "authors": "Arun Das, Jeffrey Mock, Henry Chacon, Farzan Irani, Edward Golob,\n  Peyman Najafirad", "title": "Stuttering Speech Disfluency Prediction using Explainable Attribution\n  Vectors of Facial Muscle Movements", "comments": "Submitting to IEEE Trans. 10 pages, 7 figures. Final Manuscript", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV q-bio.NC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Speech disorders such as stuttering disrupt the normal fluency of speech by\ninvoluntary repetitions, prolongations and blocking of sounds and syllables. In\naddition to these disruptions to speech fluency, most adults who stutter (AWS)\nalso experience numerous observable secondary behaviors before, during, and\nafter a stuttering moment, often involving the facial muscles. Recent studies\nhave explored automatic detection of stuttering using Artificial Intelligence\n(AI) based algorithm from respiratory rate, audio, etc. during speech\nutterance. However, most methods require controlled environments and/or\ninvasive wearable sensors, and are unable explain why a decision (fluent vs\nstuttered) was made. We hypothesize that pre-speech facial activity in AWS,\nwhich can be captured non-invasively, contains enough information to accurately\nclassify the upcoming utterance as either fluent or stuttered. Towards this\nend, this paper proposes a novel explainable AI (XAI) assisted convolutional\nneural network (CNN) classifier to predict near future stuttering by learning\ntemporal facial muscle movement patterns of AWS and explains the important\nfacial muscles and actions involved. Statistical analyses reveal significantly\nhigh prevalence of cheek muscles (p<0.005) and lip muscles (p<0.005) to predict\nstuttering and shows a behavior conducive of arousal and anticipation to speak.\nThe temporal study of these upper and lower facial muscles may facilitate early\ndetection of stuttering, promote automated assessment of stuttering and have\napplication in behavioral therapies by providing automatic non-invasive\nfeedback in realtime.\n", "versions": [{"version": "v1", "created": "Fri, 2 Oct 2020 23:45:41 GMT"}], "update_date": "2020-10-06", "authors_parsed": [["Das", "Arun", ""], ["Mock", "Jeffrey", ""], ["Chacon", "Henry", ""], ["Irani", "Farzan", ""], ["Golob", "Edward", ""], ["Najafirad", "Peyman", ""]]}, {"id": "2010.01430", "submitter": "Alexey Zakharov", "authors": "Alexey Zakharov, Matthew Crosby, Zafeirios Fountas", "title": "Episodic Memory for Learning Subjective-Timescale Models", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI q-bio.NC stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In model-based learning, an agent's model is commonly defined over\ntransitions between consecutive states of an environment even though planning\noften requires reasoning over multi-step timescales, with intermediate states\neither unnecessary, or worse, accumulating prediction error. In contrast,\nintelligent behaviour in biological organisms is characterised by the ability\nto plan over varying temporal scales depending on the context. Inspired by the\nrecent works on human time perception, we devise a novel approach to learning a\ntransition dynamics model, based on the sequences of episodic memories that\ndefine the agent's subjective timescale - over which it learns world dynamics\nand over which future planning is performed. We implement this in the framework\nof active inference and demonstrate that the resulting subjective-timescale\nmodel (STM) can systematically vary the temporal extent of its predictions\nwhile preserving the same computational efficiency. Additionally, we show that\nSTM predictions are more likely to introduce future salient events (for example\nnew objects coming into view), incentivising exploration of new areas of the\nenvironment. As a result, STM produces more informative action-conditioned\nroll-outs that assist the agent in making better decisions. We validate\nsignificant improvement in our STM agent's performance in the Animal-AI\nenvironment against a baseline system, trained using the environment's\nobjective-timescale dynamics.\n", "versions": [{"version": "v1", "created": "Sat, 3 Oct 2020 21:55:40 GMT"}], "update_date": "2020-10-06", "authors_parsed": [["Zakharov", "Alexey", ""], ["Crosby", "Matthew", ""], ["Fountas", "Zafeirios", ""]]}, {"id": "2010.01472", "submitter": "Pavel Tolmachev", "authors": "Pavel Tolmachev and Jonathan H. Manton", "title": "New Insights on Learning Rules for Hopfield Networks: Memory and\n  Objective Function Minimisation", "comments": "8 pages, IEEE-Xplore, 2020 International Joint Conference on Neural\n  Networks (IJCNN), Glasgow", "journal-ref": "2020 International Joint Conference on Neural Networks (IJCNN),\n  Glasgow, United Kingdom, 2020, pp. 1-8", "doi": "10.1109/IJCNN48605.2020.9207405", "report-no": null, "categories": "cs.NE q-bio.NC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Hopfield neural networks are a possible basis for modelling associative\nmemory in living organisms. After summarising previous studies in the field, we\ntake a new look at learning rules, exhibiting them as descent-type algorithms\nfor various cost functions. We also propose several new cost functions suitable\nfor learning. We discuss the role of biases (the external inputs) in the\nlearning process in Hopfield networks. Furthermore, we apply Newtons method for\nlearning memories, and experimentally compare the performances of various\nlearning rules. Finally, to add to the debate whether allowing connections of a\nneuron to itself enhances memory capacity, we numerically investigate the\neffects of self coupling.\n  Keywords: Hopfield Networks, associative memory, content addressable memory,\nlearning rules, gradient descent, attractor networks\n", "versions": [{"version": "v1", "created": "Sun, 4 Oct 2020 03:02:40 GMT"}], "update_date": "2020-10-06", "authors_parsed": [["Tolmachev", "Pavel", ""], ["Manton", "Jonathan H.", ""]]}, {"id": "2010.01591", "submitter": "Richard Betzel", "authors": "Richard Betzel", "title": "Network neuroscience and the connectomics revolution", "comments": "24 pages, 5 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "q-bio.NC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Connectomics and network neuroscience offer quantitative scientific\nframeworks for modeling and analyzing networks of structurally and functionally\ninteracting neurons, neuronal populations, and macroscopic brain areas. This\nshift in perspective and emphasis on distributed brain function has provided\nfundamental insight into the role played by the brain's network architecture in\ncognition, disease, development, and aging. In this chapter, we review the core\nconcepts of human connectomics at the macroscale. From the construction of\nnetworks using functional and diffusion MRI data, to their subsequent analysis\nusing methods from network neuroscience, this review highlights key findings,\ncommonly-used methodologies, and discusses several emerging frontiers in\nconnectomics.\n", "versions": [{"version": "v1", "created": "Sun, 4 Oct 2020 14:36:03 GMT"}], "update_date": "2020-10-06", "authors_parsed": [["Betzel", "Richard", ""]]}, {"id": "2010.01914", "submitter": "Patrick Krauss", "authors": "Patrick Krauss and Achim Schilling", "title": "Towards a Cognitive Computational Neuroscience of Auditory Phantom\n  Perceptions", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "q-bio.NC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In order to gain a mechanistic understanding of how tinnitus emerges in the\nbrain, we must build biologically plausible computational models that mimic\nboth tinnitus development and perception, and test the tentative models with\nbrain and behavioral experiments. With a special focus on tinnitus research, we\nreview recent work at the intersection of artificial intelligence, psychology\nand neuroscience, indicating a new research agenda that follows the idea that\nexperiments will yield theoretical insight only when employed to test\nbrain-computational models. This view challenges the popular belief, that\ntinnitus research is primarily data limited, and that producing large,\nmulti-modal, and complex datasets, analyzed with advanced data analysis\nalgorithms, will finally lead to fundamental insights into how tinnitus\nemerges. However, there is converging evidence that although modern\ntechnologies allow assessing neural activity in unprecedentedly rich ways in\nboth, animals and humans, empirical testing one verbally defined hypothesis\nabout tinnitus after another, will never lead to a mechanistic understanding.\nInstead, hypothesis testing needs to be complemented with the construction of\ncomputational models that generate verifiable predictions. We argue, that even\nthough, contemporary artificial intelligence and machine learning approaches\nlargely lack biological plausibility, the models to be constructed will have to\ndraw on concepts from these fields, since they have already proven to do well\nin modeling brain function. Nevertheless, biological fidelity will have to be\nincreased successively, leading to ever better and fine-grained models,\nallowing at the end for even testing possible treatment strategies in silico,\nbefore application in animal or patient studies.\n", "versions": [{"version": "v1", "created": "Mon, 5 Oct 2020 10:55:03 GMT"}], "update_date": "2020-10-06", "authors_parsed": [["Krauss", "Patrick", ""], ["Schilling", "Achim", ""]]}, {"id": "2010.02012", "submitter": "Muhammad Yousefnezhad", "authors": "Muhammad Yousefnezhad, Jeffrey Sawalha, Alessandro Selvitella,\n  Daoqiang Zhang", "title": "Deep Representational Similarity Learning for analyzing neural\n  signatures in task-based fMRI dataset", "comments": "Neuroinformatics", "journal-ref": null, "doi": "10.1007/s12021-020-09494-4", "report-no": null, "categories": "eess.IV cs.AI cs.LG q-bio.NC", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Similarity analysis is one of the crucial steps in most fMRI studies.\nRepresentational Similarity Analysis (RSA) can measure similarities of neural\nsignatures generated by different cognitive states. This paper develops Deep\nRepresentational Similarity Learning (DRSL), a deep extension of RSA that is\nappropriate for analyzing similarities between various cognitive tasks in fMRI\ndatasets with a large number of subjects, and high-dimensionality -- such as\nwhole-brain images. Unlike the previous methods, DRSL is not limited by a\nlinear transformation or a restricted fixed nonlinear kernel function -- such\nas Gaussian kernel. DRSL utilizes a multi-layer neural network for mapping\nneural responses to linear space, where this network can implement a customized\nnonlinear transformation for each subject separately. Furthermore, utilizing a\ngradient-based optimization in DRSL can significantly reduce runtime of\nanalysis on large datasets because it uses a batch of samples in each iteration\nrather than all neural responses to find an optimal solution. Empirical studies\non multi-subject fMRI datasets with various tasks -- including visual stimuli,\ndecision making, flavor, and working memory -- confirm that the proposed method\nachieves superior performance to other state-of-the-art RSA algorithms.\n", "versions": [{"version": "v1", "created": "Mon, 28 Sep 2020 18:30:14 GMT"}], "update_date": "2020-10-06", "authors_parsed": [["Yousefnezhad", "Muhammad", ""], ["Sawalha", "Jeffrey", ""], ["Selvitella", "Alessandro", ""], ["Zhang", "Daoqiang", ""]]}, {"id": "2010.02167", "submitter": "Xueqing Liu", "authors": "Xueqing Liu, Linbi Hong, and Paul Sajda", "title": "Latent neural source recovery via transcoding of simultaneous EEG-fMRI", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG eess.SP q-bio.NC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Simultaneous EEG-fMRI is a multi-modal neuroimaging technique that provides\ncomplementary spatial and temporal resolution for inferring a latent source\nspace of neural activity. In this paper we address this inference problem\nwithin the framework of transcoding -- mapping from a specific encoding\n(modality) to a decoding (the latent source space) and then encoding the latent\nsource space to the other modality. Specifically, we develop a symmetric method\nconsisting of a cyclic convolutional transcoder that transcodes EEG to fMRI and\nvice versa. Without any prior knowledge of either the hemodynamic response\nfunction or lead field matrix, the method exploits the temporal and spatial\nrelationships between the modalities and latent source spaces to learn these\nmappings. We show, for real EEG-fMRI data, how well the modalities can be\ntranscoded from one to another as well as the source spaces that are recovered,\nall on unseen data. In addition to enabling a new way to symmetrically infer a\nlatent source space, the method can also be seen as low-cost computational\nneuroimaging -- i.e. generating an 'expensive' fMRI BOLD image from 'low cost'\nEEG data.\n", "versions": [{"version": "v1", "created": "Mon, 5 Oct 2020 17:17:29 GMT"}], "update_date": "2020-10-06", "authors_parsed": [["Liu", "Xueqing", ""], ["Hong", "Linbi", ""], ["Sajda", "Paul", ""]]}, {"id": "2010.02634", "submitter": "Ethan Harris", "authors": "Ethan Harris, Daniela Mihai, Jonathon Hare", "title": "How Convolutional Neural Network Architecture Biases Learned Opponency\n  and Colour Tuning", "comments": "Final version; Accepted for publication in Neural Computation", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG cs.NE q-bio.NC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recent work suggests that changing Convolutional Neural Network (CNN)\narchitecture by introducing a bottleneck in the second layer can yield changes\nin learned function. To understand this relationship fully requires a way of\nquantitatively comparing trained networks. The fields of electrophysiology and\npsychophysics have developed a wealth of methods for characterising visual\nsystems which permit such comparisons. Inspired by these methods, we propose an\napproach to obtaining spatial and colour tuning curves for convolutional\nneurons, which can be used to classify cells in terms of their spatial and\ncolour opponency. We perform these classifications for a range of CNNs with\ndifferent depths and bottleneck widths. Our key finding is that networks with a\nbottleneck show a strong functional organisation: almost all cells in the\nbottleneck layer become both spatially and colour opponent, cells in the layer\nfollowing the bottleneck become non-opponent. The colour tuning data can\nfurther be used to form a rich understanding of how colour is encoded by a\nnetwork. As a concrete demonstration, we show that shallower networks without a\nbottleneck learn a complex non-linear colour system, whereas deeper networks\nwith tight bottlenecks learn a simple channel opponent code in the bottleneck\nlayer. We further develop a method of obtaining a hue sensitivity curve for a\ntrained CNN which enables high level insights that complement the low level\nfindings from the colour tuning data. We go on to train a series of networks\nunder different conditions to ascertain the robustness of the discussed\nresults. Ultimately, our methods and findings coalesce with prior art,\nstrengthening our ability to interpret trained CNNs and furthering our\nunderstanding of the connection between architecture and learned\nrepresentation. Code for all experiments is available at\nhttps://github.com/ecs-vlc/opponency.\n", "versions": [{"version": "v1", "created": "Tue, 6 Oct 2020 11:33:48 GMT"}], "update_date": "2020-10-07", "authors_parsed": [["Harris", "Ethan", ""], ["Mihai", "Daniela", ""], ["Hare", "Jonathon", ""]]}, {"id": "2010.02993", "submitter": "Erik Fagerholm", "authors": "Erik D. Fagerholm, Karl J. Friston, Rosalyn J. Moran, Robert Leech", "title": "The principle of stationary action in neural systems", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "q-bio.NC physics.class-ph", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The principle of stationary action is a cornerstone of modern physics,\nproviding a powerful framework for investigating dynamical systems found in\nclassical mechanics through to quantum field theory. However, computational\nneuroscience, despite its heavy reliance on concepts in physics, is anomalous\nin this regard as its main equations of motion are not compatible with a\nLagrangian formulation and hence with the principle of stationary action.\nTaking the Dynamic Causal Modelling neuronal state equation, Hodgkin-Huxley\nmodel, and the Leaky Integrate-and-Fire model as examples, we show that it is\npossible to write complex oscillatory forms of these equations in terms of a\nsingle Lagrangian. We therefore bring mathematical descriptions in\ncomputational neuroscience under the remit of the principle of stationary\naction and use this reformulation to express symmetries and associated\nconservation laws arising in neural systems.\n", "versions": [{"version": "v1", "created": "Tue, 6 Oct 2020 19:43:55 GMT"}], "update_date": "2020-10-08", "authors_parsed": [["Fagerholm", "Erik D.", ""], ["Friston", "Karl J.", ""], ["Moran", "Rosalyn J.", ""], ["Leech", "Robert", ""]]}, {"id": "2010.04007", "submitter": "Jon Haitz Legarreta", "authors": "Jon Haitz Legarreta, Laurent Petit, Fran\\c{c}ois Rheault, Guillaume\n  Theaud, Carl Lemaire, Maxime Descoteaux and Pierre-Marc Jodoin", "title": "Tractography filtering using autoencoders", "comments": "Preprint. Paper under review", "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.AI cs.CV cs.LG q-bio.NC q-bio.QM", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Current brain white matter fiber tracking techniques show a number of\nproblems, including: generating large proportions of streamlines that do not\naccurately describe the underlying anatomy; extracting streamlines that are not\nsupported by the underlying diffusion signal; and under-representing some fiber\npopulations, among others. In this paper, we describe a novel unsupervised\nlearning method to filter streamlines from diffusion MRI tractography, and\nhence, to obtain more reliable tractograms.\n  We show that a convolutional neural network autoencoder provides a\nstraightforward and elegant way to learn a robust representation of brain\nstreamlines, which can be used to filter undesired samples with a nearest\nneighbor algorithm. Our method, dubbed FINTA (Filtering in Tractography using\nAutoencoders) comes with several key advantages: training does not need labeled\ndata, as it uses raw tractograms, it is fast and easily reproducible, it does\nnot rely on the input diffusion MRI data, and thus, does not suffer from domain\nadaptation issues. We demonstrate the ability of FINTA to discriminate between\n\"plausible\" and \"implausible\" streamlines as well as to recover individual\nstreamline group instances from a raw tractogram, from both synthetic and real\nhuman brain diffusion MRI tractography data, including partial tractograms.\nResults reveal that FINTA has a superior filtering performance compared to\nstate-of-the-art methods.\n  Together, this work brings forward a new deep learning framework in\ntractography based on autoencoders, and shows how it can be applied for\nfiltering purposes. It sets the foundations for opening up new prospects\ntowards more accurate and robust tractometry and connectivity diffusion MRI\nanalyses, which may ultimately lead to improve the imaging of the white matter\nanatomy.\n", "versions": [{"version": "v1", "created": "Wed, 7 Oct 2020 16:45:55 GMT"}], "update_date": "2020-10-09", "authors_parsed": [["Legarreta", "Jon Haitz", ""], ["Petit", "Laurent", ""], ["Rheault", "Fran\u00e7ois", ""], ["Theaud", "Guillaume", ""], ["Lemaire", "Carl", ""], ["Descoteaux", "Maxime", ""], ["Jodoin", "Pierre-Marc", ""]]}, {"id": "2010.04123", "submitter": "Pedro Carelli", "authors": "Nastaran Lotfi, Tha\\'is Feliciano, Leandro A. A. Aguiar, Thais\n  Priscila Lima Silva, Tawan T. A. Carvalho, Osvaldo A. Rosso, Mauro Copelli,\n  Fernanda S. Matias, and Pedro V. Carelli", "title": "Statistical complexity is maximized close to criticality in cortical\n  dynamics", "comments": "8 pages, 6 figures", "journal-ref": "Phys. Rev. E 103, 012415 (2021)", "doi": "10.1103/PhysRevE.103.012415", "report-no": null, "categories": "q-bio.NC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Complex systems are typically characterized as an intermediate situation\nbetween a complete regular structure and a random system. Brain signals can be\nstudied as a striking example of such systems: cortical states can range from\nhighly synchronous and ordered neuronal activity (with higher spiking\nvariability) to desynchronized and disordered regimes (with lower spiking\nvariability). It has been recently shown, by testing independent signatures of\ncriticality, that a phase transition occurs in a cortical state of intermediate\nspiking variability. Here, we use a symbolic information approach to show that,\ndespite the monotonical increase of the Shannon entropy between ordered and\ndisordered regimes, we can determine an intermediate state of maximum\ncomplexity based on the Jensen disequilibrium measure. More specifically, we\nshow that statistical complexity is maximized close to criticality for cortical\nspiking data of urethane-anesthetized rats, as well as for a network model of\nexcitable elements that presents a critical point of a non-equilibrium phase\ntransition.\n", "versions": [{"version": "v1", "created": "Thu, 8 Oct 2020 17:02:23 GMT"}], "update_date": "2021-02-03", "authors_parsed": [["Lotfi", "Nastaran", ""], ["Feliciano", "Tha\u00eds", ""], ["Aguiar", "Leandro A. A.", ""], ["Silva", "Thais Priscila Lima", ""], ["Carvalho", "Tawan T. A.", ""], ["Rosso", "Osvaldo A.", ""], ["Copelli", "Mauro", ""], ["Matias", "Fernanda S.", ""], ["Carelli", "Pedro V.", ""]]}, {"id": "2010.04247", "submitter": "Jeremie Fish", "authors": "Jeremie Fish, Alexander DeWitt, Abd AlRahman R. AlMomani, Paul J.\n  Laurienti and Erik Bollt", "title": "Entropic Causal Inference for Neurological Applications", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "q-bio.NC math.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The ultimate goal of cognitive neuroscience is to understand the mechanistic\nneural processes underlying the functional organization of the brain. Key to\nthis study is understanding structure of both the structural and functional\nconnectivity between anatomical regions. In this paper we follow previous work\nin developing a simple dynamical model of the brain by simulating its various\nregions as Kuramoto oscillators whose coupling structure is described by a\ncomplex network. However in our simulations rather than generating synthetic\nnetworks, we simulate our synthetic model but coupled by a real network of the\nanatomical brain regions which has been reconstructed from diffusion tensor\nimaging (DTI) data. By using an information theoretic approach that defines\ndirect information flow in terms of causation entropy (CSE), we show that we\ncan more accurately recover the true structural network than either of the\npopular correlation or LASSO regression techniques. We demonstrate the\neffectiveness of our method when applied to data simulated on the realistic DTI\nnetwork, as well as on randomly generated small-world and Erd\\\"os-R\\'enyi (ER)\nnetworks.\n", "versions": [{"version": "v1", "created": "Thu, 8 Oct 2020 20:17:37 GMT"}, {"version": "v2", "created": "Wed, 3 Feb 2021 20:52:34 GMT"}], "update_date": "2021-02-05", "authors_parsed": [["Fish", "Jeremie", ""], ["DeWitt", "Alexander", ""], ["AlMomani", "Abd AlRahman R.", ""], ["Laurienti", "Paul J.", ""], ["Bollt", "Erik", ""]]}, {"id": "2010.04269", "submitter": "Melia Bonomo", "authors": "Anthony K. Brandt, Melia E. Bonomo, J. Todd Frazier, Christof Karmonik", "title": "Music Cognition is Shaped by Exposure", "comments": "18 pages, 4 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "q-bio.NC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Music is a universal feature of human experience: humans all over the world\nsing and build musical instruments. However, scientists have long speculated\nthe degree to which music cognition is innate or acquired through exposure and\nlearning. Here, we report evidence for the significance of the latter based on\ndifferences in neural processing of culturally familiar and unfamiliar music.\nIn a music and brain connectivity study that is part of a larger clinical\ninvestigation into music listening and stroke recovery at Houston Methodist\nHospital's Center for Performing Arts Medicine, functional magnetic resonance\nimaging was performed on healthy subjects while they listened to self-selected\nmusic to which they felt a positive emotional attachment, as well as culturally\nfamiliar music, culturally unfamiliar music, and several excerpts of speech. J.\nS. Bach's 2-part Invention in C-Major BWV 772 was chosen as the culturally\nfamiliar example, and Gagaku, the court music of medieval Japan, served as the\nculturally unfamiliar example because of its conspicuous differences from the\ntraditional Western musical experience of the subjects. There was a marked\ncontrast among the responses to the different types of music, whereby the brain\ntreated the unfamiliar music as an outlier. During the self-selected and Bach\ntracks, a subject's whole-brain network exhibited modular organization that was\nsignificantly coordinated with the network's flexibility. Meanwhile, when the\nGagaku music was played, this relationship between brain network modularity and\nflexibility largely disappeared. In addition, while the auditory cortex's\nflexibility during the self-selected piece was equivalent to that during Bach,\nit was more flexible during Gagaku. Given the strikingly different whole-brain\nresponses, the results suggest that music cognition is strongly impacted by\nenculturation and exposure.\n", "versions": [{"version": "v1", "created": "Thu, 8 Oct 2020 21:32:17 GMT"}], "update_date": "2020-10-12", "authors_parsed": [["Brandt", "Anthony K.", ""], ["Bonomo", "Melia E.", ""], ["Frazier", "J. Todd", ""], ["Karmonik", "Christof", ""]]}, {"id": "2010.04325", "submitter": "Joonas Autio", "authors": "Joonas A. Autio, Qi Zhu, Xiaolian Li, Matthew F. Glasser, Caspar M.\n  Schwiedrzik, Damien A. Fair, Jan Zimmermann, Essa Yacoub, Ravi S. Menon,\n  David C. Van Essen, Takuya Hayashi, Brian Russ, Wim Vanduffel", "title": "Minimal Specifications for Non-Human Primate MRI: Challenges in\n  Standardizing and Harmonizing Data Collection", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "q-bio.NC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recent methodological advances in MRI have enabled substantial growth in\nneuroimaging studies of non-human primates (NHPs), while open data-sharing\nthrough the PRIME-DE initiative has increased the availability of NHP MRI data\nand the need for robust multi-subject multi-center analyses. Streamlined\nacquisition and analysis protocols would accelerate and improve these efforts.\nHowever, consensus on minimal standards for data acquisition protocols and\nanalysis pipelines for NHP imaging remains to be established, particularly for\nmulti-center studies. Here, we draw parallels between NHP and human\nneuroimaging and provide minimal guidelines for harmonizing and standardizing\ndata acquisition. We advocate robust translation of widely used open-access\ntoolkits that are well established for analyzing human data. We also encourage\nthe use of validated, automated pre-processing tools for analyzing NHP data\nsets. These guidelines aim to refine methodological and analytical strategies\nfor small and large-scale NHP neuroimaging data. This will improve\nreproducibility of results, and accelerate the convergence between NHP and\nhuman neuroimaging strategies which will ultimately benefit fundamental and\ntranslational brain science.\n", "versions": [{"version": "v1", "created": "Fri, 9 Oct 2020 01:53:59 GMT"}], "update_date": "2020-10-12", "authors_parsed": [["Autio", "Joonas A.", ""], ["Zhu", "Qi", ""], ["Li", "Xiaolian", ""], ["Glasser", "Matthew F.", ""], ["Schwiedrzik", "Caspar M.", ""], ["Fair", "Damien A.", ""], ["Zimmermann", "Jan", ""], ["Yacoub", "Essa", ""], ["Menon", "Ravi S.", ""], ["Van Essen", "David C.", ""], ["Hayashi", "Takuya", ""], ["Russ", "Brian", ""], ["Vanduffel", "Wim", ""]]}, {"id": "2010.04466", "submitter": "Robert Tjarko Lange", "authors": "Robert Tjarko Lange and Henning Sprekeler", "title": "Learning not to learn: Nature versus nurture in silico", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI cs.NE q-bio.NC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Animals are equipped with a rich innate repertoire of sensory, behavioral and\nmotor skills, which allows them to interact with the world immediately after\nbirth. At the same time, many behaviors are highly adaptive and can be tailored\nto specific environments by means of learning. In this work, we use\nmathematical analysis and the framework of meta-learning (or 'learning to\nlearn') to answer when it is beneficial to learn such an adaptive strategy and\nwhen to hard-code a heuristic behavior. We find that the interplay of\necological uncertainty, task complexity and the agents' lifetime has crucial\neffects on the meta-learned amortized Bayesian inference performed by an agent.\nThere exist two regimes: One in which meta-learning yields a learning algorithm\nthat implements task-dependent information-integration and a second regime in\nwhich meta-learning imprints a heuristic or 'hard-coded' behavior. Further\nanalysis reveals that non-adaptive behaviors are not only optimal for aspects\nof the environment that are stable across individuals, but also in situations\nwhere an adaptation to the environment would in fact be highly beneficial, but\ncould not be done quickly enough to be exploited within the remaining lifetime.\nHard-coded behaviors should hence not only be those that always work, but also\nthose that are too complex to be learned within a reasonable time frame.\n", "versions": [{"version": "v1", "created": "Fri, 9 Oct 2020 09:47:40 GMT"}, {"version": "v2", "created": "Thu, 4 Mar 2021 11:27:16 GMT"}], "update_date": "2021-03-05", "authors_parsed": [["Lange", "Robert Tjarko", ""], ["Sprekeler", "Henning", ""]]}, {"id": "2010.04844", "submitter": "James Michaelov", "authors": "James A. Michaelov and Benjamin K. Bergen", "title": "How well does surprisal explain N400 amplitude under different\n  experimental conditions?", "comments": "To be presented at CoNLL 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI cs.IT cs.LG math.IT q-bio.NC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We investigate the extent to which word surprisal can be used to predict a\nneural measure of human language processing difficulty - the N400. To do this,\nwe use recurrent neural networks to calculate the surprisal of stimuli from\npreviously published neurolinguistic studies of the N400. We find that\nsurprisal can predict N400 amplitude in a wide range of cases, and the cases\nwhere it cannot do so provide valuable insight into the neurocognitive\nprocesses underlying the response.\n", "versions": [{"version": "v1", "created": "Fri, 9 Oct 2020 23:18:23 GMT"}], "update_date": "2020-10-13", "authors_parsed": [["Michaelov", "James A.", ""], ["Bergen", "Benjamin K.", ""]]}, {"id": "2010.04875", "submitter": "Alex Williams", "authors": "Alex H. Williams, Anthony Degleris, Yixin Wang, Scott W. Linderman", "title": "Point process models for sequence detection in high-dimensional neural\n  spike trains", "comments": "24 pages, 5 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG q-bio.NC", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Sparse sequences of neural spikes are posited to underlie aspects of working\nmemory, motor production, and learning. Discovering these sequences in an\nunsupervised manner is a longstanding problem in statistical neuroscience.\nPromising recent work utilized a convolutive nonnegative matrix factorization\nmodel to tackle this challenge. However, this model requires spike times to be\ndiscretized, utilizes a sub-optimal least-squares criterion, and does not\nprovide uncertainty estimates for model predictions or estimated parameters. We\naddress each of these shortcomings by developing a point process model that\ncharacterizes fine-scale sequences at the level of individual spikes and\nrepresents sequence occurrences as a small number of marked events in\ncontinuous time. This ultra-sparse representation of sequence events opens new\npossibilities for spike train modeling. For example, we introduce learnable\ntime warping parameters to model sequences of varying duration, which have been\nexperimentally observed in neural circuits. We demonstrate these advantages on\nexperimental recordings from songbird higher vocal center and rodent\nhippocampus.\n", "versions": [{"version": "v1", "created": "Sat, 10 Oct 2020 02:21:44 GMT"}], "update_date": "2020-10-13", "authors_parsed": [["Williams", "Alex H.", ""], ["Degleris", "Anthony", ""], ["Wang", "Yixin", ""], ["Linderman", "Scott W.", ""]]}, {"id": "2010.05497", "submitter": "Rini Sharon A", "authors": "Rini A Sharon, Hema A Murthy", "title": "The \"Sound of Silence\" in EEG -- Cognitive voice activity detection", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG eess.SP q-bio.NC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Speech cognition bears potential application as a brain computer interface\nthat can improve the quality of life for the otherwise communication impaired\npeople. While speech and resting state EEG are popularly studied, here we\nattempt to explore a \"non-speech\"(NS) state of brain activity corresponding to\nthe silence regions of speech audio. Firstly, speech perception is studied to\ninspect the existence of such a state, followed by its identification in speech\nimagination. Analogous to how voice activity detection is employed to enhance\nthe performance of speech recognition, the EEG state activity detection\nprotocol implemented here is applied to boost the confidence of imagined speech\nEEG decoding. Classification of speech and NS state is done using two datasets\ncollected from laboratory-based and commercial-based devices. The state\nsequential information thus obtained is further utilized to reduce the search\nspace of imagined EEG unit recognition. Temporal signal structures and\ntopographic maps of NS states are visualized across subjects and sessions. The\nrecognition performance and the visual distinction observed demonstrates the\nexistence of silence signatures in EEG.\n", "versions": [{"version": "v1", "created": "Mon, 12 Oct 2020 07:47:36 GMT"}], "update_date": "2020-10-13", "authors_parsed": [["Sharon", "Rini A", ""], ["Murthy", "Hema A", ""]]}, {"id": "2010.06078", "submitter": "Seung Suk Kang", "authors": "Seung Suk Kang, Ph.D., Scott R. Sponheim, Ph.D., Kelvin O. Lim, M.D", "title": "Interoception Underlies The Therapeutic Effects of Mindfulness\n  Meditation for Post-Traumatic Stress Disorder: A Randomized Clinical Trial", "comments": "17 pages, 5 figures, 1 table", "journal-ref": null, "doi": null, "report-no": null, "categories": "q-bio.NC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Mindfulness-based interventions have proven its efficacy in treating\npost-traumatic stress disorder (PTSD), but the underlying neurobiological\nmechanism is unknown. To determine the neurobiological mechanism of action of\nmindfulness-based stress reduction (MBSR) treating PTSD, we conducted a\nrandomized clinical trial (RCT) in which 98 veterans with PTSD were randomly\nassigned to receive MBSR therapy (n = 47) or present-centered group therapy\n(PCGT; n = 51; an active-control condition). Pre- and post-intervention\nmeasures of PTSD symptom severity (PTSD Checklist) and brain activity measures\nof electroencephalography (EEG) were assessed, including spectral power of\nspontaneous neural oscillatory activities during resting and meditation\nperiods, time-frequency (TF) power of cognitive task-related brain responses,\nand TF power of heartbeat-evoked brain responses (HEBR) that reflect cardiac\ninteroceptive brain responses during resting and meditation. Compared to\ncontrols, the MBSR group had greater improvements in PTSD symptoms, spontaneous\nEEG alpha (8-13 Hz) power in posterior sites, task-related frontal theta power\n(4-7 Hz in 140-220 ms post-stimulus), and frontal theta HEBR (3-5 Hz and\n265-328 ms post-R-peak). Latent difference score modeling found that only the\nchanges in the frontal theta HEBR mediated the MBSR treatment effect. Brain\nsource-level analysis found that the theta HEBR changes in the anterior\ncingulate cortex, anterior insular cortex, and the lateral prefrontal cortex\npredicted PTSD symptom improvements. These results indicated that mindfulness\nmeditation improves spontaneous brain activities reflecting internally oriented\nrelaxation and brain functions of attentional control. However, interoceptive\nbrain capacity enhanced by MBSR appears to be the primary cerebral mechanism\nthat regulates emotional disturbances and improves anxiety symptoms of PTSD.\n", "versions": [{"version": "v1", "created": "Mon, 12 Oct 2020 23:41:40 GMT"}], "update_date": "2020-10-14", "authors_parsed": [["Kang", "Seung Suk", ""], ["D.", "Ph.", ""], ["Sponheim", "Scott R.", ""], ["D.", "Ph.", ""], ["Lim", "Kelvin O.", ""], ["D", "M.", ""]]}, {"id": "2010.06225", "submitter": "Didier Pinault", "authors": "Caroline Lahogue (NCPS, FMTS, UNISTRA), Didier Pinault (NCPS, FMTS,\n  UNISTRA)", "title": "Transcranial Bipolar Direct Current Stimulation of the Frontoparietal\n  Cortex Reduces Ketamine-Induced Oscillopathies: A Pilot Study in the Sedated\n  Rat", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "q-bio.NC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Running title: Frontoparietal anodal tDCS reduces ketamine-induced\noscillopathies.Abstract: During the prodromal phase of schizophrenia with its\ncomplex and insidious clinical picture, electroencephalographic recordings\ndetect widespread oscillation disturbances (or oscillopathies). Neural\noscillations are electro-biomarkers of the connectivity state within systems. A\nsingle systemic administration of ketamine, a non-competitive NMDA glutamate\nreceptor antagonist, transiently reproduces the oscillopathies with a clinical\npicture reminiscent of the psychosis prodrome. This acute pharmacological model\nmay help the research and development of innovative treatments against the\npsychotic transition. Transcranial electrical stimulation is recognized as an\nappropriate non-invasive therapeutic modality since it can increase cognitive\nperformance and modulate neural oscillations with little or no side effects.\nTherefore, our objective was to set up, in the sedated adult rat, a stimulation\nmethod able to normalize the ketamine-induced oscillopathies. Unilateral\ntranscranial frontoparietal anodal stimulation by direct current (<+1 mA) was\napplied in ketamine-treated rats. A concomitant electroencephalographic\nrecording of the parietal cortex measured the stimulation effects on its\nspontaneously-occurring oscillations. A 5-min bipolar anodal tDCS immediately\nand quickly reduced, significantly with an intensity-effect relationship, the\nketamine-induced oscillopathies at least in the bilateral parietal cortex. A\nduration effect was also recorded. These preliminary neurophysiological\nfindings are promising for developing a therapeutic proof-of-concept against\nneuropsychiatric disorders.\n", "versions": [{"version": "v1", "created": "Tue, 13 Oct 2020 08:14:53 GMT"}], "update_date": "2020-10-14", "authors_parsed": [["Lahogue", "Caroline", "", "NCPS, FMTS, UNISTRA"], ["Pinault", "Didier", "", "NCPS, FMTS,\n  UNISTRA"]]}, {"id": "2010.07071", "submitter": "Simona Olmi", "authors": "Halgurd Taher, Alessandro Torcini, Simona Olmi", "title": "Exact neural mass model for synaptic-based working memory", "comments": "47 pages, 14 figures", "journal-ref": null, "doi": "10.1371/journal.pcbi.1008533", "report-no": null, "categories": "q-bio.NC cond-mat.dis-nn math.DS nlin.AO physics.bio-ph", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A synaptic theory of Working Memory (WM) has been developed in the last\ndecade as a possible alternative to the persistent spiking paradigm. In this\ncontext, we have developed a neural mass model able to reproduce exactly the\ndynamics of heterogeneous spiking neural networks encompassing realistic\ncellular mechanisms for short-term synaptic plasticity. This population model\nreproduces the macroscopic dynamics of the network in terms of the firing rate\nand the mean membrane potential. The latter quantity allows us to get insight\non Local Field Potential and electroencephalographic signals measured during WM\ntasks to characterize the brain activity. More specifically synaptic\nfacilitation and depression integrate each other to efficiently mimic WM\noperations via either synaptic reactivation or persistent activity. Memory\naccess and loading are associated to stimulus-locked transient oscillations\nfollowed by a steady-state activity in the $\\beta-\\gamma$ band, thus resembling\nwhat observed in the cortex during vibrotactile stimuli in humans and object\nrecognition in monkeys. Memory juggling and competition emerge already by\nloading only two items. However more items can be stored in WM by considering\nneural architectures composed of multiple excitatory populations and a common\ninhibitory pool. Memory capacity depends strongly on the presentation rate of\nthe items and it maximizes for an optimal frequency range. In particular we\nprovide an analytic expression for the maximal memory capacity. Furthermore,\nthe mean membrane potential turns out to be a suitable proxy to measure the\nmemory load, analogously to event driven potentials in experiments on humans.\nFinally we show that the $\\gamma$ power increases with the number of loaded\nitems, as reported in many experiments, while $\\theta$ and $\\beta$ power reveal\nnon monotonic behaviours.\n", "versions": [{"version": "v1", "created": "Wed, 14 Oct 2020 13:17:26 GMT"}], "update_date": "2021-01-27", "authors_parsed": [["Taher", "Halgurd", ""], ["Torcini", "Alessandro", ""], ["Olmi", "Simona", ""]]}, {"id": "2010.07143", "submitter": "Simon Wein", "authors": "Simon Wein, Wilhelm Malloni, Ana Maria Tom\\'e, Sebastian M. Frank,\n  Gina-Isabelle Henze, Stefan W\\\"ust, Mark W. Greenlee, Elmar W. Lang", "title": "A Graph Neural Network Framework for Causal Inference in Brain Networks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "q-bio.NC cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A central question in neuroscience is how self-organizing dynamic\ninteractions in the brain emerge on their relatively static structural\nbackbone. Due to the complexity of spatial and temporal dependencies between\ndifferent brain areas, fully comprehending the interplay between structure and\nfunction is still challenging and an area of intense research. In this paper we\npresent a graph neural network (GNN) framework, to describe functional\ninteractions based on the structural anatomical layout. A GNN allows us to\nprocess graph-structured spatio-temporal signals, providing a possibility to\ncombine structural information derived from diffusion tensor imaging (DTI) with\ntemporal neural activity profiles, like observed in functional magnetic\nresonance imaging (fMRI). Moreover, dynamic interactions between different\nbrain regions learned by this data-driven approach can provide a multi-modal\nmeasure of causal connectivity strength. We assess the proposed model's\naccuracy by evaluating its capabilities to replicate empirically observed\nneural activation profiles, and compare the performance to those of a vector\nauto regression (VAR), like typically used in Granger causality. We show that\nGNNs are able to capture long-term dependencies in data and also\ncomputationally scale up to the analysis of large-scale networks. Finally we\nconfirm that features learned by a GNN can generalize across MRI scanner types\nand acquisition protocols, by demonstrating that the performance on small\ndatasets can be improved by pre-training the GNN on data from an earlier and\ndifferent study. We conclude that the proposed multi-modal GNN framework can\nprovide a novel perspective on the structure-function relationship in the\nbrain. Therewith this approach can be promising for the characterization of the\ninformation flow in brain networks.\n", "versions": [{"version": "v1", "created": "Wed, 14 Oct 2020 15:01:21 GMT"}], "update_date": "2020-10-15", "authors_parsed": [["Wein", "Simon", ""], ["Malloni", "Wilhelm", ""], ["Tom\u00e9", "Ana Maria", ""], ["Frank", "Sebastian M.", ""], ["Henze", "Gina-Isabelle", ""], ["W\u00fcst", "Stefan", ""], ["Greenlee", "Mark W.", ""], ["Lang", "Elmar W.", ""]]}, {"id": "2010.07162", "submitter": "Konstantinos Spiliotis", "authors": "Konstantinos Spiliotis and Jens Starke and Denise Franz and Angelika\n  Richter and R\\\"udiger K\\\"ohling", "title": "Deep brain stimulation for movement disorder treatment: Exploring\n  frequency-dependent efficacy in a computational network model", "comments": "40 pages, 16 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "q-bio.NC math.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A large scale computational model of the basal ganglia (BG) network is\nproposed to describes movement disorder including deep brain stimulation (DBS).\nThe model of this complex network considers four areas of the basal ganglia\nnetwork: the subthalamic nucleus (STN) as target area of DBS, globus pallidus,\nboth pars externa and pars interna (GPe-GPi), and the thalamus (THA).\nParkinsonian conditions are simulated by assuming reduced dopaminergic input\nand corresponding pronounced inhibitory or disinhibited projections to GPe and\nGPi. Macroscopic quantities can be derived which correlate closely to thalamic\nresponses and hence motor programme fidelity. It can be demonstrated that\ndepending on different levels of striatal projections to the GPe and GPi, the\ndynamics of these macroscopic quantities switch from normal conditions to\nparkinsonian. Simulating DBS on the STN affects the dynamics of the entire\nnetwork, increasing the thalamic activity to levels close to normal, while\ndiffering from both normal and parkinsonian dynamics. Using the mentioned\nmacroscopic quantities, the model proposes optimal DBS frequency ranges above\n130 Hz.\n", "versions": [{"version": "v1", "created": "Wed, 14 Oct 2020 15:28:27 GMT"}, {"version": "v2", "created": "Fri, 12 Mar 2021 14:57:39 GMT"}], "update_date": "2021-03-15", "authors_parsed": [["Spiliotis", "Konstantinos", ""], ["Starke", "Jens", ""], ["Franz", "Denise", ""], ["Richter", "Angelika", ""], ["K\u00f6hling", "R\u00fcdiger", ""]]}, {"id": "2010.07208", "submitter": "Cl\\'ement Moulin-Frier", "authors": "Cl\\'ement Moulin-Frier, Jules Brochard, Freek Stulp, Pierre-Yves\n  Oudeyer", "title": "Emergent Jaw Predominance in Vocal Development through Stochastic\n  Optimization", "comments": null, "journal-ref": "IEEE Transactions on Cognitive and Developmental Systems (Volume:\n  12 , Issue: 3 , Sept. 2020)", "doi": "10.1109/TCDS.2017.2704912", "report-no": null, "categories": "cs.SD eess.AS q-bio.NC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Infant vocal babbling strongly relies on jaw oscillations, especially at the\nstage of canonical babbling, which underlies the syllabic structure of world\nlanguages. In this paper, we propose, model and analyze an hypothesis to\nexplain this predominance of the jaw in early babbling. This hypothesis states\nthat general stochastic optimization principles, when applied to learning\nsensorimotor control, automatically generate ordered babbling stages with a\npredominant exploration of jaw movements in early stages. The reason is that\nthose movements impact the auditory effects more than other articulators. In\nprevious computational models, such general principles were shown to\nselectively freeze and free degrees of freedom in a model reproducing the\nproximo-distal development observed in infant arm reaching. The contribution of\nthis paper is to show how, using the same methods, we are able to explain such\npatterns in vocal development. We present three experiments. The two first ones\nshow that the recruitment order of articulators emerging from stochastic\noptimization depends on the target sound to be achieved but that on average the\njaw is largely chosen as the first recruited articulator. The third experiment\nanalyses in more detail how the emerging recruitment order is shaped by the\ndynamics of the optimization process.\n", "versions": [{"version": "v1", "created": "Thu, 8 Oct 2020 15:25:06 GMT"}], "update_date": "2020-10-15", "authors_parsed": [["Moulin-Frier", "Cl\u00e9ment", ""], ["Brochard", "Jules", ""], ["Stulp", "Freek", ""], ["Oudeyer", "Pierre-Yves", ""]]}, {"id": "2010.07801", "submitter": "Shengling Shi", "authors": "Rik J. C. van Esch, Shengling Shi, Antoine Bernas, Svitlana Zinger,\n  Albert P. Aldenkamp, Paul M. J. Van den Hof", "title": "A Bayesian method for inference of effective connectivity in brain\n  networks for detecting the Mozart effect", "comments": null, "journal-ref": null, "doi": "10.1016/j.compbiomed.2020.104055", "report-no": null, "categories": "eess.SY cs.SY q-bio.NC q-bio.QM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Several studies claim that listening to Mozart music affects cognition and\ncan be used to treat neurological conditions like epilepsy. Research into this\nMozart effect has not addressed how dynamic interactions between brain\nnetworks, i.e. effective connectivity, are affected. The Granger-causality\nanalysis is often used to infer effective connectivity. First, we investigate\nif a new method, Bayesian topology identification, can be used as an\nalternative. Both methods are evaluated on simulation data, where the Bayesian\nmethod outperforms the Granger-causality analysis in the inference of\nconnectivity graphs of dynamic networks, especially for short data lengths. In\nthe second part, the Bayesian method is extended to enable the inference of\nchanges in effective connectivity between groups of subjects. Next, we apply\nboth methods to fMRI scans of 16 healthy subjects, who were scanned before and\nafter exposure to Mozart's sonata K448 at least 2 hours a day for 7 days. Here,\nwe investigate if the effective connectivity of the subjects significantly\nchanged after listening to Mozart music. The Bayesian method detected changes\nin effective connectivity between networks related to cognitive processing and\ncontrol: First, in the connection from the central executive to the superior\nsensori-motor network. Second, in the connection from the posterior default\nmode to the fronto-parietal right network. Finally, in the connection from the\nanterior default mode to the dorsal attention network, but only in a subgroup\nof subjects with a longer listening duration. Only in this last connection an\neffect was found by the Granger-causality analysis.\n", "versions": [{"version": "v1", "created": "Thu, 15 Oct 2020 14:44:44 GMT"}], "update_date": "2020-10-16", "authors_parsed": [["van Esch", "Rik J. C.", ""], ["Shi", "Shengling", ""], ["Bernas", "Antoine", ""], ["Zinger", "Svitlana", ""], ["Aldenkamp", "Albert P.", ""], ["Hof", "Paul M. J. Van den", ""]]}, {"id": "2010.07858", "submitter": "Cecilia Jarne", "authors": "Cecilia Jarne", "title": "What you need to know to train recurrent neural networks to make Flip\n  Flops memories and more", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG q-bio.NC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Training neural networks to perform different tasks is relevant across\nvarious disciplines that go beyond Machine Learning. In particular, Recurrent\nNeural Networks (RNN) are of great interest to different scientific\ncommunities. Open-source frameworks dedicated to Machine Learning such as\nTensorflow \\cite{chollet2015keras} and Keras \\cite{tensorflow2015-whitepaper}\nhas produced significative changes in the development of technologies that we\ncurrently use. One relevant problem that can be approach is how to build the\nmodels for the study of dynamical systems, and how to extract the relevant\ninformation to be able to answer the scientific questions of interest. The\npurpose of the present work is to contribute to this aim by using a temporal\nprocessing task, in this case, a 3-bit Flip Flop memory, to show the modeling\nprocedure in every step: from equations to the software code, using Tensorflow\nand Keras. The obtained networks are analyzed to describe the dynamics and to\nshow different visualization and analysis tools. The code developed in this\npremier is provided to be used as a base for model other systems.\n", "versions": [{"version": "v1", "created": "Thu, 15 Oct 2020 16:25:29 GMT"}], "update_date": "2020-10-16", "authors_parsed": [["Jarne", "Cecilia", ""]]}, {"id": "2010.07888", "submitter": "Anna Levina", "authors": "Roxana Zeraati, Viola Priesemann, Anna Levina", "title": "Self-organization toward criticality by synaptic plasticity", "comments": "34 pages, 7 figures", "journal-ref": null, "doi": "10.3389/fphy.2021.619661", "report-no": null, "categories": "q-bio.NC cond-mat.dis-nn physics.bio-ph", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Self-organized criticality has been proposed to be a universal mechanism for\nthe emergence of scale-free dynamics in many complex systems, and possibly in\nthe brain. While such scale-free patterns were identified experimentally in\nmany different types of neural recordings, the biological principles behind\ntheir emergence remained unknown. Utilizing different network models and\nmotivated by experimental observations, synaptic plasticity was proposed as a\npossible mechanism to self-organize brain dynamics towards a critical point. In\nthis review, we discuss how various biologically plausible plasticity rules\noperating across multiple timescales are implemented in the models and how they\nalter the network's dynamical state through modification of number and strength\nof the connections between the neurons. Some of these rules help to stabilize\ncriticality, some need additional mechanisms to prevent divergence from the\ncritical state. We propose that rules that are capable of bringing the network\nto criticality can be classified by how long the near-critical dynamics\npersists after their disabling. Finally, we discuss the role of\nself-organization and criticality in computation. Overall, the concept of\ncriticality helps to shed light on brain function and self-organization, yet\nthe overall dynamics of living neural networks seem to harnesses not only\ncriticality for computation, but also deviations thereof.\n", "versions": [{"version": "v1", "created": "Thu, 15 Oct 2020 17:08:05 GMT"}], "update_date": "2021-05-11", "authors_parsed": [["Zeraati", "Roxana", ""], ["Priesemann", "Viola", ""], ["Levina", "Anna", ""]]}, {"id": "2010.08195", "submitter": "Philippe Robert S.", "authors": "Philippe Robert and Gaetan Vignoud", "title": "Stochastic Models of Neural Synaptic Plasticity", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.PR q-bio.NC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In neuroscience, learning and memory are usually associated to long-term\nchanges of neuronal connectivity. In this context, synaptic plasticity refers\nto the set of mechanisms driving the dynamics of neuronal connections, called\n{\\em synapses} and represented by a scalar value, the synaptic weight.\nSpike-Timing Dependent Plasticity (STDP) is a biologically-based model\nrepresenting the time evolution of the synaptic weight as a functional of the\npast spiking activity of adjacent neurons.\n  If numerous models of neuronal cells have been proposed in the mathematical\nliterature, few of them include a variable for the time-varying strength of the\nconnection. A new, general, mathematical framework is introduced to study\nsynaptic plasticity associated to different STDP rules. The system composed of\ntwo neurons connected by a single synapse is investigated and a stochastic\nprocess describing its dynamical behavior is presented and analyzed. The notion\nof plasticity kernel is introduced as a key component of plastic neural\nnetworks models, generalizing a notion used for pair-based models. We show that\na large number of STDP rules from neuroscience and physics can be represented\nby this formalism. Several aspects of these models are discussed and compared\nto canonical models of computational neuroscience. An important sub-class of\nplasticity kernels with a Markovian formulation is also defined and\ninvestigated. In these models, the time evolution of cellular processes such as\nthe neuronal membrane potential and the concentrations of chemical components\ncreated/suppressed by spiking activity has the Markov property.\n", "versions": [{"version": "v1", "created": "Fri, 16 Oct 2020 06:59:22 GMT"}, {"version": "v2", "created": "Wed, 9 Jun 2021 06:41:36 GMT"}], "update_date": "2021-06-10", "authors_parsed": [["Robert", "Philippe", ""], ["Vignoud", "Gaetan", ""]]}, {"id": "2010.08377", "submitter": "Robert Geirhos", "authors": "Robert Geirhos, Kantharaju Narayanappa, Benjamin Mitzkus, Matthias\n  Bethge, Felix A. Wichmann, Wieland Brendel", "title": "On the surprising similarities between supervised and self-supervised\n  models", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI cs.LG q-bio.NC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  How do humans learn to acquire a powerful, flexible and robust representation\nof objects? While much of this process remains unknown, it is clear that humans\ndo not require millions of object labels. Excitingly, recent algorithmic\nadvancements in self-supervised learning now enable convolutional neural\nnetworks (CNNs) to learn useful visual object representations without\nsupervised labels, too. In the light of this recent breakthrough, we here\ncompare self-supervised networks to supervised models and human behaviour. We\ntested models on 15 generalisation datasets for which large-scale human\nbehavioural data is available (130K highly controlled psychophysical trials).\nSurprisingly, current self-supervised CNNs share four key characteristics of\ntheir supervised counterparts: (1.) relatively poor noise robustness (with the\nnotable exception of SimCLR), (2.) non-human category-level error patterns,\n(3.) non-human image-level error patterns (yet high similarity to supervised\nmodel errors) and (4.) a bias towards texture. Taken together, these results\nsuggest that the strategies learned through today's supervised and\nself-supervised training objectives end up being surprisingly similar, but\ndistant from human-like behaviour. That being said, we are clearly just at the\nbeginning of what could be called a self-supervised revolution of machine\nvision, and we are hopeful that future self-supervised models behave\ndifferently from supervised ones, and---perhaps---more similar to robust human\nobject recognition.\n", "versions": [{"version": "v1", "created": "Fri, 16 Oct 2020 13:28:13 GMT"}], "update_date": "2020-10-19", "authors_parsed": [["Geirhos", "Robert", ""], ["Narayanappa", "Kantharaju", ""], ["Mitzkus", "Benjamin", ""], ["Bethge", "Matthias", ""], ["Wichmann", "Felix A.", ""], ["Brendel", "Wieland", ""]]}, {"id": "2010.08569", "submitter": "Gabriel Silva", "authors": "Paul Y. Wang, Sandalika Sapra, Vivek Kurien George, Gabriel A. Silva", "title": "Generalizable Machine Learning in Neuroscience using Graph Neural\n  Networks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG q-bio.NC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Although a number of studies have explored deep learning in neuroscience, the\napplication of these algorithms to neural systems on a microscopic scale, i.e.\nparameters relevant to lower scales of organization, remains relatively novel.\nMotivated by advances in whole-brain imaging, we examined the performance of\ndeep learning models on microscopic neural dynamics and resulting emergent\nbehaviors using calcium imaging data from the nematode C. elegans. We show that\nneural networks perform remarkably well on both neuron-level dynamics\nprediction, and behavioral state classification. In addition, we compared the\nperformance of structure agnostic neural networks and graph neural networks to\ninvestigate if graph structure can be exploited as a favorable inductive bias.\nTo perform this experiment, we designed a graph neural network which explicitly\ninfers relations between neurons from neural activity and leverages the\ninferred graph structure during computations. In our experiments, we found that\ngraph neural networks generally outperformed structure agnostic models and\nexcel in generalization on unseen organisms, implying a potential path to\ngeneralizable machine learning in neuroscience.\n", "versions": [{"version": "v1", "created": "Fri, 16 Oct 2020 18:09:46 GMT"}], "update_date": "2020-10-20", "authors_parsed": [["Wang", "Paul Y.", ""], ["Sapra", "Sandalika", ""], ["George", "Vivek Kurien", ""], ["Silva", "Gabriel A.", ""]]}, {"id": "2010.08690", "submitter": "Jeffrey Shainline", "authors": "Jeffrey M. Shainline", "title": "Optoelectronic Intelligence", "comments": "10 pages, five figures, perspective article", "journal-ref": null, "doi": "10.1063/5.0040567", "report-no": null, "categories": "cs.ET cs.AI cs.NE q-bio.NC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  To design and construct hardware for general intelligence, we must consider\nprinciples of both neuroscience and very-large-scale integration. For large\nneural systems capable of general intelligence, the attributes of photonics for\ncommunication and electronics for computation are complementary and\ninterdependent. Using light for communication enables high fan-out as well as\nlow-latency signaling across large systems with no traffic-dependent\nbottlenecks. For computation, the inherent nonlinearities, high speed, and low\npower consumption of Josephson circuits are conducive to complex neural\nfunctions. Operation at 4\\,K enables the use of single-photon detectors and\nsilicon light sources, two features that lead to efficiency and economical\nscalability. Here I sketch a concept for optoelectronic hardware, beginning\nwith synaptic circuits, continuing through wafer-scale integration, and\nextending to systems interconnected with fiber-optic white matter, potentially\nat the scale of the human brain and beyond.\n", "versions": [{"version": "v1", "created": "Sat, 17 Oct 2020 01:26:29 GMT"}], "update_date": "2021-05-05", "authors_parsed": [["Shainline", "Jeffrey M.", ""]]}, {"id": "2010.08715", "submitter": "Ilya Kuzovkin", "authors": "Ilya Kuzovkin", "title": "Understanding Information Processing in Human Brain by Interpreting\n  Machine Learning Models", "comments": "Defended on September 22, 2020 (video recording at\n  https://www.uttv.ee/naita?id=30480). Supervisor: Dr. Raul Vicente Zafra\n  (Computational Neuroscience Lab, University of Tarty, Estonia). Opponents:\n  Dr. Fabian Sinz (IRG Neuronal Intelligence, University of T\\\"ubingen,\n  Germany), Dr. Tim C Kietzmann (Donders Institute for Brain, Cognition and\n  Behaviour, Radboud University, Netherlands)", "journal-ref": null, "doi": null, "report-no": "Dissertationes Informaticae Universitatis Tartuensis 19", "categories": "q-bio.NC cs.AI cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The thesis explores the role machine learning methods play in creating\nintuitive computational models of neural processing. Combined with\ninterpretability techniques, machine learning could replace human modeler and\nshift the focus of human effort to extracting the knowledge from the ready-made\nmodels and articulating that knowledge into intuitive descroptions of reality.\nThis perspective makes the case in favor of the larger role that exploratory\nand data-driven approach to computational neuroscience could play while\ncoexisting alongside the traditional hypothesis-driven approach.\n  We exemplify the proposed approach in the context of the knowledge\nrepresentation taxonomy with three research projects that employ\ninterpretability techniques on top of machine learning methods at three\ndifferent levels of neural organization. The first study (Chapter 3) explores\nfeature importance analysis of a random forest decoder trained on intracerebral\nrecordings from 100 human subjects to identify spectrotemporal signatures that\ncharacterize local neural activity during the task of visual categorization.\nThe second study (Chapter 4) employs representation similarity analysis to\ncompare the neural responses of the areas along the ventral stream with the\nactivations of the layers of a deep convolutional neural network. The third\nstudy (Chapter 5) proposes a method that allows test subjects to visually\nexplore the state representation of their neural signal in real time. This is\nachieved by using a topology-preserving dimensionality reduction technique that\nallows to transform the neural data from the multidimensional representation\nused by the computer into a two-dimensional representation a human can grasp.\n  The approach, the taxonomy, and the examples, present a strong case for the\napplicability of machine learning methods to automatic knowledge discovery in\nneuroscience.\n", "versions": [{"version": "v1", "created": "Sat, 17 Oct 2020 04:37:26 GMT"}], "update_date": "2020-10-20", "authors_parsed": [["Kuzovkin", "Ilya", ""]]}, {"id": "2010.09325", "submitter": "Matej Hoffmann", "authors": "Matej Hoffmann", "title": "Body models in humans, animals, and robots", "comments": "27 pages, 8 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "q-bio.NC cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Humans and animals excel in combining information from multiple sensory\nmodalities, controlling their complex bodies, adapting to growth, failures, or\nusing tools. These capabilities are also highly desirable in robots. They are\ndisplayed by machines to some extent - yet, as is so often the case, the\nartificial creatures are lagging behind. The key foundation is an internal\nrepresentation of the body that the agent - human, animal, or robot - has\ndeveloped. In the biological realm, evidence has been accumulated by diverse\ndisciplines giving rise to the concepts of body image, body schema, and others.\nIn robotics, a model of the robot is an indispensable component that enables to\ncontrol the machine. In this article I compare the character of body\nrepresentations in biology with their robotic counterparts and relate that to\nthe differences in performance that we observe. I put forth a number of axes\nregarding the nature of such body models: fixed vs. plastic, amodal vs. modal,\nexplicit vs. implicit, serial vs. parallel, modular vs. holistic, and\ncentralized vs. distributed. An interesting trend emerges: on many of the axes,\nthere is a sequence from robot body models, over body image, body schema, to\nthe body representation in lower animals like the octopus. In some sense,\nrobots have a lot in common with Ian Waterman - \"the man who lost his body\" -\nin that they rely on an explicit, veridical body model (body image taken to the\nextreme) and lack any implicit, multimodal representation (like the body\nschema) of their bodies. I will then detail how robots can inform the\nbiological sciences dealing with body representations and finally, I will study\nwhich of the features of the \"body in the brain\" should be transferred to\nrobots, giving rise to more adaptive and resilient, self-calibrating machines.\n", "versions": [{"version": "v1", "created": "Mon, 19 Oct 2020 09:07:11 GMT"}], "update_date": "2020-10-20", "authors_parsed": [["Hoffmann", "Matej", ""]]}, {"id": "2010.09441", "submitter": "Gholamreza Jafari", "authors": "Z. Moradimanesh (1), R. Khosrowabadi (1), M. Eshaghi Gordji (2), G. R.\n  Jafari (3, 1 and 4) ((1) Institute for Cognitive and Brain Sciences, Shahid\n  Beheshti University, (2) Department of Mathematics, Semnan University, (3)\n  Department of Physics, Shahid Beheshti University, (4) Department of Network\n  and Data Science, Central European University)", "title": "Altered structural balance of resting-state networks in autism", "comments": "19 pages, 6 figures, 3 tables, 2 supplementary figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "q-bio.NC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  What makes a network complex, in addition to its size, is the interconnected\ninteractions between elements, disruption of which inevitably results in\ndysfunction. Likewise, the brain networks' complexity arises from interactions\nbeyond pair connections, as it is simplistic to assume that in complex networks\nstate of a link is independently determined only according to its two\nconstituting nodes. This is particularly of note in genetically complex brain\nimpairments, such as the autism spectrum disorder (ASD). Accordingly,\nstructural balance theory (SBT) affirms that in the real-world signed networks,\na link is remarkably influenced by each of its two nodes' interactions with the\nthird node within a triadic interrelationship. Thus, it is plausible to ask\nwhether ASD is associated with altered structural balance resulting from\natypical triadic interactions. In other words, it is the abnormal interplay of\npositive and negative interactions that matter in ASD, besides and beyond hypo\n(hyper) pair connectivity. To address this, we explore triadic interactions in\nthe rs-fMRI network of participants with ASD relative to healthy controls\n(CON). We demonstrate that balanced triads are overrepresented in the ASD and\nCON networks while unbalanced triads are underrepresented, providing first-time\nempirical evidence for the strong notion of structural balance on the brain\nnetworks. We further analyze the frequency and energy distribution of triads\nand suggest an alternative description for the reduced functional integration\nand segregation in the ASD brain networks. Last but not least, we observe that\nenergy of the salient and the default mode networks are lower in autism, which\nmay be a reflection of the difficulty in flexible behaviors. Altogether, these\nresults highlight the potential value of SBT as a new perspective in functional\nconnectivity studies, especially in neurodevelopmental disorders.\n", "versions": [{"version": "v1", "created": "Tue, 6 Oct 2020 16:28:35 GMT"}, {"version": "v2", "created": "Wed, 21 Oct 2020 13:14:42 GMT"}], "update_date": "2020-10-22", "authors_parsed": [["Moradimanesh", "Z.", "", "3, 1 and 4"], ["Khosrowabadi", "R.", "", "3, 1 and 4"], ["Gordji", "M. Eshaghi", "", "3, 1 and 4"], ["Jafari", "G. R.", "", "3, 1 and 4"]]}, {"id": "2010.09568", "submitter": "Vince Grolmusz", "authors": "Laszlo Keresztes and Evelin Szogi and Balint Varga and Vince Grolmusz", "title": "Introducing and Applying Newtonian Blurring: An Augmented Dataset of\n  126,000 Human Connectomes at braingraph.org", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "q-bio.NC cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Gaussian blurring is a well-established method for image data augmentation:\nit may generate a large set of images from a small set of pictures for training\nand testing purposes for Artificial Intelligence (AI) applications. When we\napply AI for non-imagelike biological data, hardly any related method exists.\nHere we introduce the \"Newtonian blurring\" in human braingraph (or connectome)\naugmentation: Started from a dataset of 1053 subjects, we first repeat a\nprobabilistic weighted braingraph construction algorithm 10 times for\ndescribing the connections of distinct cerebral areas, then take 7 repetitions\nin every possible way, delete the lower and upper extremes, and average the\nremaining 7-2=5 edge-weights for the data of each subject. This way we augment\nthe 1053 graph-set to 120 x 1053 = 126,360 graphs. In augmentation techniques,\nit is an important requirement that no artificial additions should be\nintroduced into the dataset. Gaussian blurring and also this Newtonian blurring\nsatisfy this goal. The resulting dataset of 126,360 graphs, each in 5\nresolutions (i.e., 631,800 graphs in total), is freely available at the site\nhttps://braingraph.org/cms/download-pit-group-connectomes/. Augmenting with\nNewtonian blurring may also be applicable in other non-image related fields,\nwhere probabilistic processing and data averaging are implemented.\n", "versions": [{"version": "v1", "created": "Mon, 19 Oct 2020 14:51:59 GMT"}, {"version": "v2", "created": "Tue, 20 Oct 2020 07:36:01 GMT"}, {"version": "v3", "created": "Wed, 21 Oct 2020 16:31:26 GMT"}], "update_date": "2020-10-22", "authors_parsed": [["Keresztes", "Laszlo", ""], ["Szogi", "Evelin", ""], ["Varga", "Balint", ""], ["Grolmusz", "Vince", ""]]}, {"id": "2010.09588", "submitter": "Neta Maimon", "authors": "Maxim Bez, Neta B. Maimon, Denis Ddobot, Lior Molcho, Nathan Intrator,\n  Eli Kakiashvilli, and Amitai Bickel", "title": "Continuous monitoring of cognitive load using advanced computerized\n  analysis of brain signals during virtual simulator training for laparoscopic\n  surgery, reflects laparoscopic dexterity. A comparative study using a novel\n  wireless device", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "q-bio.NC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Simulation-based training is an effective tool for acquiring practical\nskills, specifically to train new surgeons in a controlled and hazard-free\nenvironment, it is however important to measure participants cognitive load to\ndecide whether they are ready to go into a real surgery. In the present study\nwe measured performance on a surgery simulator of medical students and interns,\nwhile their brain activity was monitored by a mobile EEG device. 38 medical\nstudentswere underwent 3 experiments undergoing a task with Simbionix\nsimulator, while their brain activity was measured using a single-channel EEG\ndevice (Aurora by Neurosteer). On each experiment, participants performed 3\nrepeats of a simulator task using laparoscopic hands. The retention between\ntasks was different on each experiment, to examine changes in performance and\ncognitive load biomarkers that occur during the task or as a results of night\nsleep consolidation. The participants behavioral performance improved with\ntrial repetition in all 3 experiments. In Exps. 1 & 2, the theta band activity\nsignificantly decreased with better individual performance, as exhibited by\nsome of the behavioral measurements of the simulator. The novel VC9 biomarker\n(previously shown to correlate with cognitive load), exhibited a significant\ndecrease with better individual performance shown by all behavioral\nmeasurements. In correspondence with previous research, theta decreased with\nlower cognitive load and higher performance and the novel biomarker, VC9,\nshowed higher sensitivity to load changes. Together, these measurements might\nbe for neuroimaging assessment of cognitive load while performing simulator\nlaparoscopic tasks. This could potentially be expanded to evaluate efficacy of\ndifferent medical simulations to provide more efficient training to medical\nstaff and to measure cognitive and mental load in real laparoscopic surgeries.\n", "versions": [{"version": "v1", "created": "Mon, 19 Oct 2020 15:11:48 GMT"}, {"version": "v2", "created": "Mon, 12 Apr 2021 17:55:43 GMT"}], "update_date": "2021-04-13", "authors_parsed": [["Bez", "Maxim", ""], ["Maimon", "Neta B.", ""], ["Ddobot", "Denis", ""], ["Molcho", "Lior", ""], ["Intrator", "Nathan", ""], ["Kakiashvilli", "Eli", ""], ["Bickel", "Amitai", ""]]}, {"id": "2010.09601", "submitter": "Carl Nelson", "authors": "Carl J. Nelson and Stephen Bonner", "title": "Neuronal graphs: a graph theory primer for microscopic, functional\n  networks of neurons recorded by Calcium imaging", "comments": "28 pages, 11 figures, 1 table, 2 boxes", "journal-ref": null, "doi": null, "report-no": null, "categories": "q-bio.NC q-bio.QM", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Connected networks are a fundamental structure of neurobiology. Understanding\nthese networks will help us elucidate the neural mechanisms of computation.\nMathematically speaking these networks are `graphs' - structures containing\nobjects that are connected. In neuroscience, the objects could be regions of\nthe brain, e.g. fMRI data, or be individual neurons, e.g. calcium imaging with\nfluorescence microscopy. The formal study of graphs, graph theory, can provide\nneuroscientists with a large bank of algorithms for exploring networks. Graph\ntheory has already been applied in a variety of ways to fMRI data but, more\nrecently, has begun to be applied at the scales of neurons, e.g. from\nfunctional calcium imaging. In this primer we explain the basics of graph\ntheory and relate them to features of microscopic functional networks of\nneurons from calcium imaging - neuronal graphs. We explore recent examples of\ngraph theory applied to calcium imaging and we highlight some areas where\nresearchers new to the field could go awry.\n", "versions": [{"version": "v1", "created": "Mon, 19 Oct 2020 15:33:34 GMT"}], "update_date": "2020-10-20", "authors_parsed": [["Nelson", "Carl J.", ""], ["Bonner", "Stephen", ""]]}, {"id": "2010.09660", "submitter": "James Fitzgerald", "authors": "Tirthabir Biswas, James E. Fitzgerald", "title": "A geometric framework to predict structure from function in neural\n  networks", "comments": "32 pages, 10 figures, reorganized sections, corrected typos, minor\n  style changes to graphics, updated abstract", "journal-ref": null, "doi": null, "report-no": null, "categories": "q-bio.NC cond-mat.dis-nn physics.bio-ph", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Neural computation in biological and artificial networks relies on nonlinear\nsynaptic integration. The structural connectivity matrix of synaptic weights\nbetween neurons is a critical determinant of overall network function, but\nquantitative links between neural network structure and function are complex\nand subtle. For example, many networks can give rise to similar functional\nresponses, and the same network can function differently depending on context.\nWhether certain patterns of synaptic connectivity are required to generate\nspecific network-level computations is largely unknown. Here we introduce a\ngeometric framework for identifying synaptic connections required by\nsteady-state responses in recurrent networks of rectified-linear neurons.\nAssuming that the number of specified response patterns does not exceed the\nnumber of input synapses, we analytically calculate the solution space of all\nfeedforward and recurrent connectivity matrices that can generate the specified\nresponses from the network inputs. A generalization accounting for noise\nfurther reveals that the solution space geometry can undergo topological\ntransitions as the allowed error increases, which could provide insight into\nboth neuroscience and machine learning. We ultimately use this geometric\ncharacterization to derive certainty conditions guaranteeing a non-zero synapse\nbetween neurons. Our theoretical framework could thus be applied to neural\nactivity data to make rigorous anatomical predictions that follow generally\nfrom the model architecture.\n", "versions": [{"version": "v1", "created": "Mon, 19 Oct 2020 16:56:55 GMT"}, {"version": "v2", "created": "Tue, 22 Dec 2020 13:33:56 GMT"}], "update_date": "2020-12-23", "authors_parsed": [["Biswas", "Tirthabir", ""], ["Fitzgerald", "James E.", ""]]}, {"id": "2010.09690", "submitter": "Mingyuan Meng", "authors": "Xingyu Yang, Mingyuan Meng, Shanlin Xiao, and Zhiyi Yu", "title": "SPA: Stochastic Probability Adjustment for System Balance of\n  Unsupervised SNNs", "comments": "Published at the 25th International Conference on Pattern Recognition\n  (ICPR2020)", "journal-ref": "2020 25th International Conference on Pattern Recognition (ICPR),\n  2021, pp. 6417-6424", "doi": "10.1109/ICPR48806.2021.9412266", "report-no": null, "categories": "cs.NE cs.LG q-bio.NC", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Spiking neural networks (SNNs) receive widespread attention because of their\nlow-power hardware characteristic and brain-like signal response mechanism, but\ncurrently, the performance of SNNs is still behind Artificial Neural Networks\n(ANNs). We build an information theory-inspired system called Stochastic\nProbability Adjustment (SPA) system to reduce this gap. The SPA maps the\nsynapses and neurons of SNNs into a probability space where a neuron and all\nconnected pre-synapses are represented by a cluster. The movement of synaptic\ntransmitter between different clusters is modeled as a Brownian-like stochastic\nprocess in which the transmitter distribution is adaptive at different firing\nphases. We experimented with a wide range of existing unsupervised SNN\narchitectures and achieved consistent performance improvements. The\nimprovements in classification accuracy have reached 1.99% and 6.29% on the\nMNIST and EMNIST datasets respectively.\n", "versions": [{"version": "v1", "created": "Mon, 19 Oct 2020 17:28:38 GMT"}, {"version": "v2", "created": "Thu, 6 May 2021 07:53:42 GMT"}], "update_date": "2021-05-07", "authors_parsed": [["Yang", "Xingyu", ""], ["Meng", "Mingyuan", ""], ["Xiao", "Shanlin", ""], ["Yu", "Zhiyi", ""]]}, {"id": "2010.09932", "submitter": "Sophia Shatek", "authors": "Amanda K. Robinson, Tijl Grootswagers, Sophia M. Shatek, Jack Gerboni,\n  Alex Holcombe, Thomas A. Carlson", "title": "Overlapping neural representations for the position of visible and\n  imagined objects", "comments": "All data and analysis code for this study are available at\n  https://osf.io/8v47t/", "journal-ref": null, "doi": null, "report-no": null, "categories": "q-bio.NC", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Humans can covertly track the position of an object, even if the object is\ntemporarily occluded. What are the neural mechanisms underlying our capacity to\ntrack moving objects when there is no physical stimulus for the brain to track?\nOne possibility is that the brain 'fills-in' information about imagined objects\nusing internally generated representations similar to those generated by\nfeed-forward perceptual mechanisms. Alternatively, the brain might deploy a\nhigher order mechanism, for example using an object tracking model that\nintegrates visual signals and motion dynamics. In the present study, we used\nEEG and time-resolved multivariate pattern analyses to investigate the spatial\nprocessing of visible and imagined objects. Participants tracked an object that\nmoved in discrete steps around fixation, occupying six consecutive locations.\nThey were asked to imagine that the object continued on the same trajectory\nafter it disappeared and move their attention to the corresponding positions.\nTime-resolved decoding of EEG data revealed that the location of the visible\nstimuli could be decoded shortly after image onset, consistent with early\nretinotopic visual processes. For processing of unseen/imagined positions, the\npatterns of neural activity resembled stimulus-driven mid-level visual\nprocesses, but were detected earlier than perceptual mechanisms, implicating an\nanticipatory and more variable tracking mechanism. Encoding models revealed\nthat spatial representations were much weaker for imagined than visible\nstimuli. Monitoring the position of imagined objects thus utilises similar\nperceptual and attentional processes as monitoring objects that are actually\npresent, but with different temporal dynamics. These results indicate that\ninternally generated representations rely on top-down processes, and their\ntiming is influenced by the predictability of the stimulus.\n", "versions": [{"version": "v1", "created": "Tue, 20 Oct 2020 00:09:06 GMT"}, {"version": "v2", "created": "Wed, 11 Nov 2020 23:42:35 GMT"}], "update_date": "2020-11-13", "authors_parsed": [["Robinson", "Amanda K.", ""], ["Grootswagers", "Tijl", ""], ["Shatek", "Sophia M.", ""], ["Gerboni", "Jack", ""], ["Holcombe", "Alex", ""], ["Carlson", "Thomas A.", ""]]}, {"id": "2010.10353", "submitter": "Alexandre Moly", "authors": "Alexandre Moly, Alexandre Aksenov, Alim Louis Benabid, Tetiana\n  Aksenova", "title": "Online adaptive group-wise sparse NPLS for ECoG neural signal decoding", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.SP q-bio.NC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Objective. Brain-computer interfaces (BCIs) create a new communication\npathway between the brain and an effector without neuromuscular activation. BCI\nexperiments highlighted high intra and inter-subjects variability in the BCI\ndecoders. Although BCI model is generally relying on neurological markers\ngeneralizable on the majority of subjects, it requires to generate a wide range\nof neural features to include possible neurophysiological patterns. However,\nthe processing of noisy and high dimensional features, such as brain signals,\nbrings several challenges to overcome such as model calibration issues, model\ngeneralization and interpretation problems and hardware related obstacles.\nApproach. An online adaptive group-wise sparse decoder named Lp-Penalized\nREW-NPLS algorithm (PREW-NPLS) is presented to reduce the feature space\ndimension employed for BCI decoding. The proposed decoder was designed to\ncreate BCI systems with low computational cost suited for portable applications\nand tested during offline pseudo-online study based on online closed-loop BCI\ncontrol of the left and right 3D arm movements of a virtual avatar from the\nECoG recordings of a tetraplegic patient.\n  Main results. PREW-NPLS algorithm highlight at least as good decoding\nperformance as REW-NPLS algorithm. However, the decoding performance obtained\nwith PREW-NPLS were achieved thanks to sparse models with up to 64% and 75% of\nthe electrodes set to 0 for the left and right hand models respectively using\nL1-PREW-NPLS.\n  Significance. The designed solution proposed an online incremental adaptive\nalgorithm suitable for online adaptive decoder calibration which estimate\nsparse decoding solutions. The PREW-NPLS models are suited for portable\napplications with low computational power using only small number of electrodes\nwith degrading the decoding performance.\n", "versions": [{"version": "v1", "created": "Wed, 14 Oct 2020 13:51:33 GMT"}], "update_date": "2020-10-21", "authors_parsed": [["Moly", "Alexandre", ""], ["Aksenov", "Alexandre", ""], ["Benabid", "Alim Louis", ""], ["Aksenova", "Tetiana", ""]]}, {"id": "2010.10444", "submitter": "Masanao Ozawa", "authors": "Masanao Ozawa and Andrei Khrennikov", "title": "Modeling combination of question order effect, response replicability\n  effect, and QQ-equality with quantum instruments", "comments": "39 pages, title changed, accepted for publication in J. Math. Psychol", "journal-ref": "Journal of Mathematical Psychology 100, 102491, 2021", "doi": "10.1016/j.jmp.2020.102491", "report-no": null, "categories": "q-bio.NC", "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/", "abstract": "  We continue to analyze basic constraints on the human decision making from\nthe viewpoint of quantum measurement theory (QMT). As it has been found, the\nconventional QMT based on the projection postulate cannot account for the\ncombination of the question order effect (QOE) and the response replicability\neffect (RRE). This was alarming finding for quantum-like modeling of decision\nmaking. Recently, it was shown that this difficulty can be resolved by using of\nthe general QMT based on quantum instruments. In the present paper we analyse\nthe problem of the combination of QOE, RRE, and the well-known QQ-equality\n(QQE). This equality was derived by Busemeyer and Wang and it was shown (in a\njoint paper with Solloway and Shiffrin) that statistical data from many social\nopinion polls satisfy it. Here we construct quantum instruments satisfying QOE,\nRRE and QQE. The general features of our approach are formalized with\npostulates that generalize (the Wang-Busemeyer) postulates for quantum-like\nmodeling of decision making. Moreover, we show that our model closely\nreproduces the statistics of the well-known Clinton-Gore Poll data with a prior\nbelief state independent of the question order. This model successfully\ncorrects for the order effect in the data to determine the \"genuine\"\ndistribution of the opinions in the Poll. The paper also provides an accessible\nintroduction to the theory of quantum instruments - the most general\nmathematical framework for quantum measurements.\n", "versions": [{"version": "v1", "created": "Sun, 11 Oct 2020 13:41:04 GMT"}, {"version": "v2", "created": "Fri, 4 Dec 2020 17:06:41 GMT"}], "update_date": "2021-02-19", "authors_parsed": [["Ozawa", "Masanao", ""], ["Khrennikov", "Andrei", ""]]}, {"id": "2010.10926", "submitter": "Gerard Rinkus", "authors": "Rod Rinkus", "title": "Efficient Similarity-Preserving Unsupervised Learning using Modular\n  Sparse Distributed Codes and Novelty-Contingent Noise", "comments": "11 pages, 6 figures. arXiv admin note: text overlap with\n  arXiv:1701.07879", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.NE q-bio.NC", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  There is increasing realization in neuroscience that information is\nrepresented in the brain, e.g., neocortex, hippocampus, in the form sparse\ndistributed codes (SDCs), a kind of cell assembly. Two essential questions are:\na) how are such codes formed on the basis of single trials, and how is\nsimilarity preserved during learning, i.e., how do more similar inputs get\nmapped to more similar SDCs. I describe a novel Modular Sparse Distributed Code\n(MSDC) that provides simple, neurally plausible answers to both questions. An\nMSDC coding field (CF) consists of Q WTA competitive modules (CMs), each\ncomprised of K binary units (analogs of principal cells). The modular nature of\nthe CF makes possible a single-trial, unsupervised learning algorithm that\napproximately preserves similarity and crucially, runs in fixed time, i.e., the\nnumber of steps needed to store an item remains constant as the number of\nstored items grows. Further, once items are stored as MSDCs in superposition\nand such that their intersection structure reflects input similarity, both\nfixed time best-match retrieval and fixed time belief update (updating the\nprobabilities of all stored items) also become possible. The algorithm's core\nprinciple is simply to add noise into the process of choosing a code, i.e.,\nchoosing a winner in each CM, which is proportional to the novelty of the\ninput. This causes the expected intersection of the code for an input, X, with\nthe code of each previously stored input, Y, to be proportional to the\nsimilarity of X and Y. Results demonstrating these capabilities for spatial\npatterns are given in the appendix.\n", "versions": [{"version": "v1", "created": "Mon, 19 Oct 2020 17:36:04 GMT"}], "update_date": "2020-10-22", "authors_parsed": [["Rinkus", "Rod", ""]]}, {"id": "2010.10995", "submitter": "Harikrishnan Nellippallil Balakrishnan", "authors": "Harikrishnan NB and Pranay SY and Nithin Nagaraj", "title": "A Neurochaos Learning Architecture for Genome Classification", "comments": "20 pages, 20 images", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NE cs.LG q-bio.NC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  There has been empirical evidence of presence of non-linearity and chaos at\nthe level of single neurons in biological neural networks. The properties of\nchaotic neurons inspires us to employ them in artificial learning systems.\nHere, we propose a Neurochaos Learning (NL) architecture, where the neurons\nused to extract features from data are 1D chaotic maps. ChaosFEX+SVM, an\ninstance of this NL architecture, is proposed as a hybrid combination of chaos\nand classical machine learning algorithm. We formally prove that a single layer\nof NL with a finite number of 1D chaotic neurons satisfies the Universal\nApproximation Theorem with an exact value for the number of chaotic neurons\nneeded to approximate a discrete real valued function with finite support. This\nis made possible due to the topological transitivity property of chaos and the\nexistence of uncountably infinite number of dense orbits for the chosen 1D\nchaotic map. The chaotic neurons in NL get activated under the presence of an\ninput stimulus (data) and output a chaotic firing trajectory. From such chaotic\nfiring trajectories of individual neurons of NL, we extract Firing Time, Firing\nRate, Energy and Entropy that constitute ChaosFEX features. These ChaosFEX\nfeatures are then fed to a Support Vector Machine with linear kernel for\nclassification. The effectiveness of chaotic feature engineering performed by\nNL (ChaosFEX+SVM) is demonstrated for synthetic and real world datasets in the\nlow and high training sample regimes. Specifically, we consider the problem of\nclassification of genome sequences of SARS-CoV-2 from other coronaviruses\n(SARS-CoV-1, MERS-CoV and others). With just one training sample per class for\n1000 random trials of training, we report an average macro F1-score > 0.99 for\nthe classification of SARS-CoV-2 from SARS-CoV-1 genome sequences. Robustness\nof ChaosFEX features to additive noise is also demonstrated.\n", "versions": [{"version": "v1", "created": "Mon, 12 Oct 2020 19:07:02 GMT"}], "update_date": "2020-10-22", "authors_parsed": [["NB", "Harikrishnan", ""], ["SY", "Pranay", ""], ["Nagaraj", "Nithin", ""]]}, {"id": "2010.11124", "submitter": "Paul Smolen", "authors": "Paul Smolen, Douglas A Baxter, John H Byrne", "title": "Modeling Suggests Combined-Drug Treatments for Disorders Impairing\n  Synaptic Plasticity via Shared Signaling Pathways", "comments": "Accepted to Journal of Computational Neuroscience", "journal-ref": null, "doi": null, "report-no": null, "categories": "q-bio.NC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Genetic disorders such as Rubinstein-Taybi syndrome (RTS) and Coffin-Lowry\nsyndrome (CLS) cause lifelong cognitive disability, including deficits in\nlearning and memory. Can pharmacological therapies be suggested to improve\nlearning and memory in these disorders? To address this question, we simulated\ndrug effects within a computational model describing induction of late\nlong-term potentiation (L-LTP). Biochemical pathways impaired in these and\nother disorders converge on a common target, histone acetylation by\nacetyltransferases such as CREB binding protein (CBP), which facilitates gene\ninduction necessary for L-LTP. We focused on four drug classes: tropomyosin\nreceptor kinase B (TrkB) agonists, cAMP phosphodiesterase inhibitors, histone\ndeacetylase inhibitors, and ampakines. Simulations suggested each drug type\nalone may rescue deficits in L-LTP. A potential disadvantage, however, was the\nnecessity of simulating strong drug effects (high doses), which could produce\nadverse side effects. Thus, we investigated the effects of six drug pairs among\nthe four classes described above. These combination treatments normalized\nimpaired L-LTP with substantially smaller drug doses. In addition three of\nthese combinations, a TrkB agonist paired with an ampakine and a cAMP\nphosphodiesterase inhibitor paired with a TrkB agonist or an ampakine,\nexhibited strong synergism in L-LTP rescue. Therefore, we suggest these drug\ncombinations are promising candidates for further empirical studies in animal\nmodels of genetic disorders that impair acetylation, L-LTP, and learning.\n", "versions": [{"version": "v1", "created": "Wed, 21 Oct 2020 16:33:31 GMT"}], "update_date": "2020-10-22", "authors_parsed": [["Smolen", "Paul", ""], ["Baxter", "Douglas A", ""], ["Byrne", "John H", ""]]}, {"id": "2010.11237", "submitter": "Jahan Schad", "authors": "Jahan N. Schad", "title": "Embodied Consciousness Theory", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "q-bio.NC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The mysterious phenomenon of consciousness, after having been the subject of\nphilosophic attention for few millennia, has drawn much scientific curiosity in\nrecent decades; and many brilliant minds of various areas of sciences are\ntrying to throw some light on it. Present neuroscience knowledge allows\nenvisioning the neural processes behind the physical aspects of consciousness,\nhowever what may be behind the experiences of it, which has no physical\ninsignia, has remained a mystery despite the grounds gained in recent well\nreceived efforts: monumental endeavors concluding in the known theories such as\nglobal workspace theory (GWT) [1] and integrated information theory (IIT) [2],\nfall short of providing a solid theory for consciousness: the former proposes a\nsimple hypothesis concerning the neural basis of 'making a conscious mental\neffort,' and the latter, assuming 'experience to be an intrinsic property of\nthe brain,' formulates how it is transitioned, through certain information\nbased neural activity, to its physical substrate. Present work puts forward a\ntheory of consciousness, rooted in the simple and straightforward implications\nof the computational operation of the brain which is consistent with well known\nfacts and recent findings, which indicates presence of motor cortex activity in\nrelaying conscious experiences. Despite simplicity, the theory provides\nfundamental basis for physicalism [3], as well answers for some Meta level\nproblems of consciousness.\n", "versions": [{"version": "v1", "created": "Wed, 21 Oct 2020 18:38:43 GMT"}], "update_date": "2020-10-23", "authors_parsed": [["Schad", "Jahan N.", ""]]}, {"id": "2010.11370", "submitter": "Gelareh Mohammadi", "authors": "Gelareh Mohammadi and Patrik Vuilleumier", "title": "A Multi-Componential Approach to Emotion Recognition and the Effect of\n  Personality", "comments": "13 pages", "journal-ref": "IEEE Transactions on Affective Computing, 2020", "doi": "10.1109/TAFFC.2020.3028109", "report-no": null, "categories": "q-bio.NC cs.AI cs.LG cs.RO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Emotions are an inseparable part of human nature affecting our behavior in\nresponse to the outside world. Although most empirical studies have been\ndominated by two theoretical models including discrete categories of emotion\nand dichotomous dimensions, results from neuroscience approaches suggest a\nmulti-processes mechanism underpinning emotional experience with a large\noverlap across different emotions. While these findings are consistent with the\ninfluential theories of emotion in psychology that emphasize a role for\nmultiple component processes to generate emotion episodes, few studies have\nsystematically investigated the relationship between discrete emotions and a\nfull componential view. This paper applies a componential framework with a\ndata-driven approach to characterize emotional experiences evoked during movie\nwatching. The results suggest that differences between various emotions can be\ncaptured by a few (at least 6) latent dimensions, each defined by features\nassociated with component processes, including appraisal, expression,\nphysiology, motivation, and feeling. In addition, the link between discrete\nemotions and component model is explored and results show that a componential\nmodel with a limited number of descriptors is still able to predict the level\nof experienced discrete emotion(s) to a satisfactory level. Finally, as\nappraisals may vary according to individual dispositions and biases, we also\nstudy the relationship between personality traits and emotions in our\ncomputational framework and show that the role of personality on discrete\nemotion differences can be better justified using the component model.\n", "versions": [{"version": "v1", "created": "Thu, 22 Oct 2020 01:27:23 GMT"}], "update_date": "2020-10-23", "authors_parsed": [["Mohammadi", "Gelareh", ""], ["Vuilleumier", "Patrik", ""]]}, {"id": "2010.11413", "submitter": "Baihan Lin", "authors": "Baihan Lin, Djallel Bouneffouf, Guillermo Cecchi", "title": "Predicting Human Decision Making in Psychological Tasks with Recurrent\n  Neural Networks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI q-bio.NC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Unlike traditional time series, the action sequences of human decision making\nusually involve many cognitive processes such as beliefs, desires, intentions\nand theory of mind, i.e. what others are thinking. This makes predicting human\ndecision making challenging to be treated agnostically to the underlying\npsychological mechanisms. We propose to use a recurrent neural network\narchitecture based on long short-term memory networks (LSTM) to predict the\ntime series of the actions taken by the human subjects at each step of their\ndecision making, the first application of such methods in this research domain.\nWe trained our prediction networks on the behavioral data from several\npublished psychological experiments of human decision making, and demonstrated\na clear advantage over the state-of-the-art methods in predicting human\ndecision making trajectories in both single-agent scenarios such as Iowa\nGambling Task and multi-agent scenarios such as Iterated Prisoner's Dilemma.\n", "versions": [{"version": "v1", "created": "Thu, 22 Oct 2020 03:36:03 GMT"}], "update_date": "2020-10-23", "authors_parsed": [["Lin", "Baihan", ""], ["Bouneffouf", "Djallel", ""], ["Cecchi", "Guillermo", ""]]}, {"id": "2010.11765", "submitter": "Aran Nayebi", "authors": "Aran Nayebi, Sanjana Srivastava, Surya Ganguli, Daniel L.K. Yamins", "title": "Identifying Learning Rules From Neural Network Observables", "comments": "NeurIPS 2020 Camera Ready Version, 21 pages including supplementary\n  information, 13 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "q-bio.NC cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The brain modifies its synaptic strengths during learning in order to better\nadapt to its environment. However, the underlying plasticity rules that govern\nlearning are unknown. Many proposals have been suggested, including Hebbian\nmechanisms, explicit error backpropagation, and a variety of alternatives. It\nis an open question as to what specific experimental measurements would need to\nbe made to determine whether any given learning rule is operative in a real\nbiological system. In this work, we take a \"virtual experimental\" approach to\nthis problem. Simulating idealized neuroscience experiments with artificial\nneural networks, we generate a large-scale dataset of learning trajectories of\naggregate statistics measured in a variety of neural network architectures,\nloss functions, learning rule hyperparameters, and parameter initializations.\nWe then take a discriminative approach, training linear and simple non-linear\nclassifiers to identify learning rules from features based on these\nobservables. We show that different classes of learning rules can be separated\nsolely on the basis of aggregate statistics of the weights, activations, or\ninstantaneous layer-wise activity changes, and that these results generalize to\nlimited access to the trajectory and held-out architectures and learning\ncurricula. We identify the statistics of each observable that are most relevant\nfor rule identification, finding that statistics from network activities across\ntraining are more robust to unit undersampling and measurement noise than those\nobtained from the synaptic strengths. Our results suggest that activation\npatterns, available from electrophysiological recordings of post-synaptic\nactivities on the order of several hundred units, frequently measured at wider\nintervals over the course of learning, may provide a good basis on which to\nidentify learning rules.\n", "versions": [{"version": "v1", "created": "Thu, 22 Oct 2020 14:36:54 GMT"}, {"version": "v2", "created": "Tue, 8 Dec 2020 18:48:02 GMT"}], "update_date": "2020-12-09", "authors_parsed": [["Nayebi", "Aran", ""], ["Srivastava", "Sanjana", ""], ["Ganguli", "Surya", ""], ["Yamins", "Daniel L. K.", ""]]}, {"id": "2010.11810", "submitter": "R. James Cotton", "authors": "R. James Cotton, Fabian H. Sinz, Andreas S. Tolias", "title": "Factorized Neural Processes for Neural Processes: $K$-Shot Prediction of\n  Neural Responses", "comments": "14 pages, 5 figures, NeurIPS 2020 conference paper", "journal-ref": null, "doi": null, "report-no": null, "categories": "q-bio.NC cs.LG cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In recent years, artificial neural networks have achieved state-of-the-art\nperformance for predicting the responses of neurons in the visual cortex to\nnatural stimuli. However, they require a time consuming parameter optimization\nprocess for accurately modeling the tuning function of newly observed neurons,\nwhich prohibits many applications including real-time, closed-loop experiments.\nWe overcome this limitation by formulating the problem as $K$-shot prediction\nto directly infer a neuron's tuning function from a small set of\nstimulus-response pairs using a Neural Process. This required us to developed a\nFactorized Neural Process, which embeds the observed set into a latent space\npartitioned into the receptive field location and the tuning function\nproperties. We show on simulated responses that the predictions and\nreconstructed receptive fields from the Factorized Neural Process approach\nground truth with increasing number of trials. Critically, the latent\nrepresentation that summarizes the tuning function of a neuron is inferred in a\nquick, single forward pass through the network. Finally, we validate this\napproach on real neural data from visual cortex and find that the predictive\naccuracy is comparable to -- and for small $K$ even greater than --\noptimization based approaches, while being substantially faster. We believe\nthis novel deep learning systems identification framework will facilitate\nbetter real-time integration of artificial neural network modeling into\nneuroscience experiments.\n", "versions": [{"version": "v1", "created": "Thu, 22 Oct 2020 15:43:59 GMT"}], "update_date": "2020-10-24", "authors_parsed": [["Cotton", "R. James", ""], ["Sinz", "Fabian H.", ""], ["Tolias", "Andreas S.", ""]]}, {"id": "2010.12127", "submitter": "Dongqi Han", "authors": "Dongqi Han, Erik De Schutter, Sungho Hong", "title": "Lamina-specific neuronal properties promote robust, stable signal\n  propagation in feedforward networks", "comments": "NeurIPS 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "q-bio.NC cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Feedforward networks (FFN) are ubiquitous structures in neural systems and\nhave been studied to understand mechanisms of reliable signal and information\ntransmission. In many FFNs, neurons in one layer have intrinsic properties that\nare distinct from those in their pre-/postsynaptic layers, but how this affects\nnetwork-level information processing remains unexplored. Here we show that\nlayer-to-layer heterogeneity arising from lamina-specific cellular properties\nfacilitates signal and information transmission in FFNs. Specifically, we found\nthat signal transformations, made by each layer of neurons on an input-driven\nspike signal, demodulate signal distortions introduced by preceding layers.\nThis mechanism boosts information transfer carried by a propagating spike\nsignal and thereby supports reliable spike signal and information transmission\nin a deep FFN. Our study suggests that distinct cell types in neural circuits,\nperforming different computational functions, facilitate information processing\non the whole.\n", "versions": [{"version": "v1", "created": "Fri, 23 Oct 2020 01:57:46 GMT"}], "update_date": "2020-10-26", "authors_parsed": [["Han", "Dongqi", ""], ["De Schutter", "Erik", ""], ["Hong", "Sungho", ""]]}, {"id": "2010.12395", "submitter": "Fabian Mikulasch", "authors": "Fabian Alexander Mikulasch, Lucas Rudelt, Viola Priesemann", "title": "Local dendritic balance enables learning of efficient representations in\n  networks of spiking neurons", "comments": "34 Pages, 14 Figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "q-bio.NC", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  How can neural networks learn to efficiently represent complex and\nhigh-dimensional inputs via local plasticity mechanisms? Classical models of\nrepresentation learning assume that input weights are learned via pairwise\nHebbian-like plasticity. Here, we show that pairwise Hebbian-like plasticity\nonly works under unrealistic requirements on neural dynamics and input\nstatistics. To overcome these limitations, we derive from first principles a\nlearning scheme based on voltage-dependent synaptic plasticity rules. Here,\ninhibition learns to locally balance excitatory input in individual dendritic\ncompartments, and thereby can modulate excitatory synaptic plasticity to learn\nefficient representations. We demonstrate in simulations that this learning\nscheme works robustly even for complex, high-dimensional and correlated inputs,\nand with inhibitory transmission delays, where Hebbian-like plasticity fails.\nOur results draw a direct connection between dendritic excitatory-inhibitory\nbalance and voltage-dependent synaptic plasticity as observed in vivo, and\nsuggest that both are crucial for representation learning.\n", "versions": [{"version": "v1", "created": "Fri, 23 Oct 2020 13:29:41 GMT"}], "update_date": "2020-10-26", "authors_parsed": [["Mikulasch", "Fabian Alexander", ""], ["Rudelt", "Lucas", ""], ["Priesemann", "Viola", ""]]}, {"id": "2010.12600", "submitter": "William Lemaire", "authors": "William Lemaire (1), Maher Benhouria (1), Konin Koua (1), Wei Tong\n  (2), Gabriel Martin-Hardy (1), Melanie Stamp (3), Kumaravelu Ganesan (3),\n  Louis-Philippe Gauthier (1), Marwan Besrour (1), Arman Ahnood (4), David John\n  Garrett (4), S\\'ebastien Roy (1), Michael Ibbotson (2,5), Steven Prawer (3),\n  R\\'ejean Fontaine (1) ((1) Interdisciplinary Institute for Technological\n  Innovation (3IT), Universit\\'e de Sherbrooke, Sherbrooke, Quebec, Canada, (2)\n  National Vision Research Institute, Australian College of Optometry, Carlton,\n  Victoria, Australia, (3) School of Physics, The University of Melbourne,\n  Parkville, Victoria, Australia, (4) School of Engineering, RMIT University,\n  Melbourne, Victoria, Australia, (5) Department of Optometry and Vision\n  Sciences, The University of Melbourne, Parkville, Victoria, Australia)", "title": "Retinal Ganglion Cell Stimulation with an Optically Powered Retinal\n  Prosthesis", "comments": "17 pages, 13 figures, to be submitted to IEEE Transactions on\n  Biomedical Circuits and Systems", "journal-ref": null, "doi": null, "report-no": null, "categories": "q-bio.NC cs.SY eess.SY", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Objective. Clinical trials previously demonstrated the spectacular capacity\nto elicit visual percepts in blind patients affected with retinal diseases by\nelectrically stimulating the remaining neurons on the retina. However, these\nimplants restored very limited visual acuity and required transcutaneous cables\ntraversing the eyeball, leading to reduced reliability and complex surgery with\nhigh postoperative infection risks. Approach. To overcome the limitations\nimposed by cables, a retinal implant architecture in which near-infrared\nillumination carries both power and data through the pupil is presented. A high\nefficiency multi-junction photovoltaic cell transduces the optical power to a\nCMOS stimulator capable of delivering flexible interleaved sequential\nstimulation through a diamond microelectrode array. To demonstrate the capacity\nto elicit a neural response with this approach while complying with the optical\nirradiance safety limit at the pupil, fluorescence imaging with a calcium\nindicator is used on a degenerate rat retina. Main results. The power delivered\nby the laser at safe irradiance of 4 mW/mm2 is shown to be sufficient to both\npower the stimulator ASIC and elicit a response in retinal ganglion cells\n(RGCs), with the ability to generate of up to 35 000 pulses per second at the\naverage stimulation threshold. Significance. This confirms the feasibility of\nwirelessly generating a response in RGCs with a digital stimulation controller\nthat can deliver complex multipolar stimulation patterns at high repetition\nrates.\n", "versions": [{"version": "v1", "created": "Fri, 23 Oct 2020 18:11:56 GMT"}], "update_date": "2020-10-27", "authors_parsed": [["Lemaire", "William", ""], ["Benhouria", "Maher", ""], ["Koua", "Konin", ""], ["Tong", "Wei", ""], ["Martin-Hardy", "Gabriel", ""], ["Stamp", "Melanie", ""], ["Ganesan", "Kumaravelu", ""], ["Gauthier", "Louis-Philippe", ""], ["Besrour", "Marwan", ""], ["Ahnood", "Arman", ""], ["Garrett", "David John", ""], ["Roy", "S\u00e9bastien", ""], ["Ibbotson", "Michael", ""], ["Prawer", "Steven", ""], ["Fontaine", "R\u00e9jean", ""]]}, {"id": "2010.12632", "submitter": "David Lipshutz", "authors": "David Lipshutz, Dmitri B. Chklovskii", "title": "Bio-NICA: A biologically inspired single-layer network for Nonnegative\n  Independent Component Analysis", "comments": "5 pages, 4 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.SP cs.NE q-bio.NC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Blind source separation, the problem of separating mixtures of unknown\nsignals into their distinct sources, is an important problem for both\nbiological and engineered signal processing systems. Nonnegative Independent\nComponent Analysis (NICA) is a special case of blind source separation that\nassumes the mixture is a linear combination of independent, nonnegative\nsources. In this work, we derive a single-layer neural network implementation\nof NICA satisfying the following 3 constraints, which are relevant for\nbiological systems and the design of neuromorphic hardware: (i) the network\noperates in the online setting, (ii) the synaptic learning rules are local, and\n(iii) the neural outputs are nonnegative.\n", "versions": [{"version": "v1", "created": "Fri, 23 Oct 2020 19:31:49 GMT"}], "update_date": "2020-10-27", "authors_parsed": [["Lipshutz", "David", ""], ["Chklovskii", "Dmitri B.", ""]]}, {"id": "2010.12644", "submitter": "David Lipshutz", "authors": "David Lipshutz, Charlie Windolf, Siavash Golkar, Dmitri B. Chklovskii", "title": "A biologically plausible neural network for Slow Feature Analysis", "comments": "17 pages, 7 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "q-bio.NC cs.LG cs.NE stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Learning latent features from time series data is an important problem in\nboth machine learning and brain function. One approach, called Slow Feature\nAnalysis (SFA), leverages the slowness of many salient features relative to the\nrapidly varying input signals. Furthermore, when trained on naturalistic\nstimuli, SFA reproduces interesting properties of cells in the primary visual\ncortex and hippocampus, suggesting that the brain uses temporal slowness as a\ncomputational principle for learning latent features. However, despite the\npotential relevance of SFA for modeling brain function, there is currently no\nSFA algorithm with a biologically plausible neural network implementation, by\nwhich we mean an algorithm operates in the online setting and can be mapped\nonto a neural network with local synaptic updates. In this work, starting from\nan SFA objective, we derive an SFA algorithm, called Bio-SFA, with a\nbiologically plausible neural network implementation. We validate Bio-SFA on\nnaturalistic stimuli.\n", "versions": [{"version": "v1", "created": "Fri, 23 Oct 2020 20:09:03 GMT"}], "update_date": "2020-10-27", "authors_parsed": [["Lipshutz", "David", ""], ["Windolf", "Charlie", ""], ["Golkar", "Siavash", ""], ["Chklovskii", "Dmitri B.", ""]]}, {"id": "2010.12660", "submitter": "Siavash Golkar", "authors": "Siavash Golkar, David Lipshutz, Yanis Bahroun, Anirvan M. Sengupta,\n  Dmitri B. Chklovskii", "title": "A simple normative network approximates local non-Hebbian learning in\n  the cortex", "comments": "Body and supplementary materials of NeurIPS 2020 paper. 19 pages, 7\n  figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "q-bio.NC cs.LG cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  To guide behavior, the brain extracts relevant features from high-dimensional\ndata streamed by sensory organs. Neuroscience experiments demonstrate that the\nprocessing of sensory inputs by cortical neurons is modulated by instructive\nsignals which provide context and task-relevant information. Here, adopting a\nnormative approach, we model these instructive signals as supervisory inputs\nguiding the projection of the feedforward data. Mathematically, we start with a\nfamily of Reduced-Rank Regression (RRR) objective functions which include\nReduced Rank (minimum) Mean Square Error (RRMSE) and Canonical Correlation\nAnalysis (CCA), and derive novel offline and online optimization algorithms,\nwhich we call Bio-RRR. The online algorithms can be implemented by neural\nnetworks whose synaptic learning rules resemble calcium plateau potential\ndependent plasticity observed in the cortex. We detail how, in our model, the\ncalcium plateau potential can be interpreted as a backpropagating error signal.\nWe demonstrate that, despite relying exclusively on biologically plausible\nlocal learning rules, our algorithms perform competitively with existing\nimplementations of RRMSE and CCA.\n", "versions": [{"version": "v1", "created": "Fri, 23 Oct 2020 20:49:44 GMT"}], "update_date": "2020-10-27", "authors_parsed": [["Golkar", "Siavash", ""], ["Lipshutz", "David", ""], ["Bahroun", "Yanis", ""], ["Sengupta", "Anirvan M.", ""], ["Chklovskii", "Dmitri B.", ""]]}, {"id": "2010.12948", "submitter": "Mengjin Dong", "authors": "Mengjin Dong, Long Xie, Sandhitsu R. Das, Jiancong Wang, Laura E.M.\n  Wisse, Robin deFlores, David A. Wolk, Paul Yushkevich (for the Alzheimer's\n  Disease Neuroimaging Initiative)", "title": "DeepAtrophy: Teaching a Neural Network to Differentiate Progressive\n  Changes from Noise on Longitudinal MRI in Alzheimer's Disease", "comments": "Submitted to a journal, IF ~ 6", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG eess.IV q-bio.NC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Volume change measures derived from longitudinal MRI (e.g. hippocampal\natrophy) are a well-studied biomarker of disease progression in Alzheimer's\nDisease (AD) and are used in clinical trials to track the therapeutic efficacy\nof disease-modifying treatments. However, longitudinal MRI change measures can\nbe confounded by non-biological factors, such as different degrees of head\nmotion and susceptibility artifact between pairs of MRI scans. We hypothesize\nthat deep learning methods applied directly to pairs of longitudinal MRI scans\ncan be trained to differentiate between biological changes and non-biological\nfactors better than conventional approaches based on deformable image\nregistration. To achieve this, we make a simplifying assumption that biological\nfactors are associated with time (i.e. the hippocampus shrinks overtime in the\naging population) whereas non-biological factors are independent of time. We\nthen formulate deep learning networks to infer the temporal order of\nsame-subject MRI scans input to the network in arbitrary order; as well as to\ninfer ratios between interscan intervals for two pairs of same-subject MRI\nscans. In the test dataset, these networks perform better in tasks of temporal\nordering (89.3%) and interscan interval inference (86.1%) than a\nstate-of-the-art deformation-based morphometry method ALOHA (76.6% and 76.1%\nrespectively) (Das et al., 2012). Furthermore, we derive a disease progression\nscore from the network that is able to detect a group difference between 58\npreclinical AD and 75 beta-amyloid-negative cognitively normal individuals\nwithin one year, compared to two years for ALOHA. This suggests that deep\nlearning can be trained to differentiate MRI changes due to biological factors\n(tissue loss) from changes due to non-biological factors, leading to novel\nbiomarkers that are more sensitive to longitudinal changes at the earliest\nstages of AD.\n", "versions": [{"version": "v1", "created": "Sat, 24 Oct 2020 18:23:02 GMT"}], "update_date": "2020-10-27", "authors_parsed": [["Dong", "Mengjin", "", "for the Alzheimer's\n  Disease Neuroimaging Initiative"], ["Xie", "Long", "", "for the Alzheimer's\n  Disease Neuroimaging Initiative"], ["Das", "Sandhitsu R.", "", "for the Alzheimer's\n  Disease Neuroimaging Initiative"], ["Wang", "Jiancong", "", "for the Alzheimer's\n  Disease Neuroimaging Initiative"], ["Wisse", "Laura E. M.", "", "for the Alzheimer's\n  Disease Neuroimaging Initiative"], ["deFlores", "Robin", "", "for the Alzheimer's\n  Disease Neuroimaging Initiative"], ["Wolk", "David A.", "", "for the Alzheimer's\n  Disease Neuroimaging Initiative"], ["Yushkevich", "Paul", "", "for the Alzheimer's\n  Disease Neuroimaging Initiative"]]}, {"id": "2010.13458", "submitter": "Cintya Dutta", "authors": "Cintya Nirvana Dutta, Pamela K. Douglas, Hernando Ombao", "title": "Structural Brain Asymmetries in Youths with Combined and Inattentive\n  Presentations of Attention Deficit Hyperactivity Disorder", "comments": "5 pages, 3 figures, 1 table, submitted to ISBI conference", "journal-ref": null, "doi": null, "report-no": null, "categories": "q-bio.NC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Alterations in structural brain laterality are reported in\nattention-deficit/hyperactivity disorder (ADHD). However, few studies examined\ndifferences within presentations of ADHD. We investigate asymmetry index (AI)\nacross 13 subcortical and 33 cortical regions from anatomical metrics of\nvolume, surface area, and thickness. Structural T1-weighted MRI data were\nobtained from youths with inattentive (n = 64) and combined (n = 51)\npresentations, and aged-matched controls (n = 298). We used a linear mixed\neffect model that accounts for data site heterogeneity, while studying\nassociations between AI and covariates of presentation and age. Our paper\ncontributes to the functional results seen among ADHD presentations evidencing\ndisrupted connectivity in motor networks from ADHD-C and cingulo-frontal\nnetworks from ADHD-I, as well as new findings in the temporal cortex and\ndefault mode subnetworks. Age patterns of structural asymmetries vary with\npresentation type. Linear mixed effects model is a practical tool for\ncharacterizing associations between brain asymmetries, diagnosis, and\nneurodevelopment.\n", "versions": [{"version": "v1", "created": "Mon, 26 Oct 2020 09:54:18 GMT"}], "update_date": "2020-10-27", "authors_parsed": [["Dutta", "Cintya Nirvana", ""], ["Douglas", "Pamela K.", ""], ["Ombao", "Hernando", ""]]}, {"id": "2010.13459", "submitter": "Marie-Constance Corsi", "authors": "Marie-Constance Corsi, Mario Chavez, Denis Schwartz, Nathalie George,\n  Laurent Hugueville, Ari E. Kahn, Sophie Dupont, Danielle S. Bassett and\n  Fabrizio De Vico Fallani", "title": "BCI learning induces core-periphery reorganization in M/EEG multiplex\n  brain networks", "comments": "This is the version of the article before editing, as submitted by an\n  author to the Journal of Neural Engineering. IOP Publishing Ltd is not\n  responsible for any errors or omissions in this version of the manuscript or\n  any version derived from it. The Version of Record is available online\n  athttp://iopscience.iop.org/article/10.1088/1741-2552/abef39", "journal-ref": null, "doi": "10.1088/1741-2552/abef39", "report-no": null, "categories": "q-bio.NC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Brain-computer interfaces (BCIs) constitute a promising tool for\ncommunication and control. However, mastering non-invasive closed-loop systems\nremains a learned skill that is difficult to develop for a non-negligible\nproportion of users. The involved learning process induces neural changes\nassociated with a brain network reorganization that remains poorly understood.\nTo address this inter-subject variability, we adopted a multilayer approach to\nintegrate brain network properties from electroencephalographic (EEG) and\nmagnetoencephalographic (MEG) data resulting from a four-session BCI training\nprogram followed by a group of healthy subjects. Our method gives access to the\ncontribution of each layer to multilayer network that tends to be equal with\ntime. We show that regardless the chosen modality, a progressive increase in\nthe integration of somatosensory areas in the alpha band was paralleled by a\ndecrease of the integration of visual processing and working memory areas in\nthe beta band. Notably, only brain network properties in multilayer network\ncorrelated with future BCI scores in the alpha2 band: positively in\nsomatosensory and decision-making related areas and negatively in associative\nareas. Our findings cast new light on neural processes underlying BCI training.\nIntegrating multimodal brain network properties provides new information that\ncorrelates with behavioral performance and could be considered as a potential\nmarker of BCI learning.\n", "versions": [{"version": "v1", "created": "Mon, 26 Oct 2020 09:56:14 GMT"}, {"version": "v2", "created": "Wed, 17 Mar 2021 09:08:55 GMT"}], "update_date": "2021-03-18", "authors_parsed": [["Corsi", "Marie-Constance", ""], ["Chavez", "Mario", ""], ["Schwartz", "Denis", ""], ["George", "Nathalie", ""], ["Hugueville", "Laurent", ""], ["Kahn", "Ari E.", ""], ["Dupont", "Sophie", ""], ["Bassett", "Danielle S.", ""], ["Fallani", "Fabrizio De Vico", ""]]}, {"id": "2010.14699", "submitter": "Luke Rast", "authors": "Luke Rast and Jan Drugowitsch", "title": "Adaptation Properties Allow Identification of Optimized Neural Codes", "comments": "v2: Corrected typos and issues displaying the figures. Edits to\n  supplementary material", "journal-ref": "Advances in Neural Information Processing Systems 33 (2020)", "doi": null, "report-no": null, "categories": "q-bio.NC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The adaptation of neural codes to the statistics of their environment is well\ncaptured by efficient coding approaches. Here we solve an inverse problem:\ncharacterizing the objective and constraint functions that efficient codes\nappear to be optimal for, on the basis of how they adapt to different stimulus\ndistributions. We formulate a general efficient coding problem, with flexible\nobjective and constraint functions and minimal parametric assumptions. Solving\nspecial cases of this model, we provide solutions to broad classes of Fisher\ninformation-based efficient coding problems, generalizing a wide range of\nprevious results. We show that different objective function types impose\nqualitatively different adaptation behaviors, while constraints enforce\ncharacteristic deviations from classic efficient coding signatures. Despite\ninteraction between these effects, clear signatures emerge for both\nunconstrained optimization problems and information-maximizing objective\nfunctions. Asking for a fixed-point of the neural code adaptation, we find an\nobjective-independent characterization of constraints on the neural code. We\nuse this result to propose an experimental paradigm that can characterize both\nthe objective and constraint functions that an observed code appears to be\noptimized for.\n", "versions": [{"version": "v1", "created": "Wed, 28 Oct 2020 02:03:05 GMT"}, {"version": "v2", "created": "Mon, 22 Feb 2021 21:57:14 GMT"}], "update_date": "2021-02-25", "authors_parsed": [["Rast", "Luke", ""], ["Drugowitsch", "Jan", ""]]}, {"id": "2010.14828", "submitter": "Sebastian Lotter", "authors": "Sebastian Lotter and Maximilian Sch\\\"afer and Johannes Zeitler and\n  Robert Schober", "title": "Receptor Saturation Modeling for Synaptic DMC", "comments": "6 pages, 1 table, 4 figures. Submitted to the 2021 IEEE International\n  Conference on Communications (ICC) on October 28, 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.ET cs.IT math.IT q-bio.NC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Synaptic communication is a natural Molecular Communication (MC) system which\nmay serve as a blueprint for the design of synthetic MC systems. In particular,\nit features highly specialized mechanisms to enable inter-symbol interference\n(ISI)-free and energy efficient communication. The understanding of synaptic MC\nis furthermore critical for disruptive innovations in the context of\nbrain-machine interfaces. However, the physical modeling of synaptic MC is\ncomplicated by the possible saturation of the molecular receiver arising from\nthe competition of postsynaptic receptors for neurotransmitters. Saturation\nrenders the system behavior nonlinear and is commonly neglected in existing\nanalytical models. In this work, we propose a novel model for receptor\nsaturation in terms of a nonlinear, state-dependent boundary condition for\nFick's diffusion equation. We solve the resulting boundary-value problem using\nan eigenfunction expansion of the Laplace operator and the incorporation of the\nreceiver memory as feedback system into the corresponding state-space\ndescription. The presented solution is numerically stable and computationally\nefficient. Furthermore, the proposed model is validated with particle-based\nstochastic computer simulations.\n", "versions": [{"version": "v1", "created": "Wed, 28 Oct 2020 09:01:33 GMT"}], "update_date": "2020-10-29", "authors_parsed": [["Lotter", "Sebastian", ""], ["Sch\u00e4fer", "Maximilian", ""], ["Zeitler", "Johannes", ""], ["Schober", "Robert", ""]]}, {"id": "2010.15081", "submitter": "Xilin Liu", "authors": "Xilin Liu, Hongjie Zhu, Tian Qiu, Srihari Y. Sritharan, Dengteng Ge,\n  Shu Yang, Milin Zhang, Andrew G. Richardson, Timothy H. Lucas, Nader Engheta,\n  and Jan Van der Spiegel", "title": "A Fully Integrated Sensor-Brain-Machine Interface System for Restoring\n  Somatosensation", "comments": "12 pages, 17 figures", "journal-ref": "IEEE Sensors Journal, 2020", "doi": "10.1109/JSEN.2020.3030899", "report-no": null, "categories": "q-bio.NC cs.SY eess.SY", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Sensory feedback is critical to the performance of neural prostheses that\nrestore movement control after neurological injury. Recent advances in direct\nneural control of paralyzed arms present new requirements for miniaturized,\nlow-power sensor systems. To address this challenge, we developed a\nfully-integrated wireless sensor-brain-machine interface (SBMI) system for\ncommunicating key somatosensory signals, fingertip forces and limb joint\nangles, to the brain. The system consists of a tactile force sensor, an\nelectrogoniometer, and a neural interface. The tactile force sensor features a\nnovel optical waveguide on CMOS design for sensing. The electrogoniometer\nintegrates an ultra low-power digital signal processor (DSP) for real-time\njoint angle measurement. The neural interface enables bidirectional neural\nstimulation and recording. Innovative designs of sensors and sensing\ninterfaces, analog-to-digital converters (ADC) and ultra wide-band (UWB)\nwireless transceivers have been developed. The prototypes have been fabricated\nin 180nm standard CMOS technology and tested on the bench and in vivo. The\ndeveloped system provides a novel solution for providing somatosensory feedback\nto next-generation neural prostheses.\n", "versions": [{"version": "v1", "created": "Sat, 17 Oct 2020 04:58:45 GMT"}], "update_date": "2020-10-29", "authors_parsed": [["Liu", "Xilin", ""], ["Zhu", "Hongjie", ""], ["Qiu", "Tian", ""], ["Sritharan", "Srihari Y.", ""], ["Ge", "Dengteng", ""], ["Yang", "Shu", ""], ["Zhang", "Milin", ""], ["Richardson", "Andrew G.", ""], ["Lucas", "Timothy H.", ""], ["Engheta", "Nader", ""], ["Van der Spiegel", "Jan", ""]]}, {"id": "2010.15191", "submitter": "Anne Churchland", "authors": "Joao Couto, Simon Musall, Xiaonan R Sun, Anup Khanal, Steven Gluf,\n  Shreya Saxena, Ian Kinsella, Taiga Abe, John P. Cunningham, Liam Paninski,\n  Anne K Churchland", "title": "Chronic, cortex-wide imaging of specific cell populations during\n  behavior", "comments": "36 pages, 7 figures, 2 supplementary figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "q-bio.NC", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Measurements of neuronal activity across brain areas are important for\nunderstanding the neural correlates of cognitive and motor processes like\nattention, decision-making, and action selection. However, techniques that\nallow cellular resolution measurements are expensive and require a high degree\nof technical expertise, which limits their broad use. Widefield imaging of\ngenetically encoded indicators is a high throughput, cost effective, and\nflexible approach to measure activity of specific cell populations with high\ntemporal resolution and a cortex-wide field of view. Here we outline our\nprotocol for assembling a widefield setup, a surgical preparation to image\nthrough the intact skull, and imaging neural activity chronically in behaving,\ntransgenic mice that express a calcium indicator in specific subpopulations of\ncortical neurons. Further, we highlight a processing pipeline that leverages\nnovel, cloud-based methods to analyze large-scale imaging datasets. The\nprotocol targets labs that are seeking to build macroscopes, optimize surgical\nprocedures for long-term chronic imaging, and/or analyze cortex-wide neuronal\nrecordings.\n", "versions": [{"version": "v1", "created": "Wed, 28 Oct 2020 19:21:52 GMT"}], "update_date": "2020-10-30", "authors_parsed": [["Couto", "Joao", ""], ["Musall", "Simon", ""], ["Sun", "Xiaonan R", ""], ["Khanal", "Anup", ""], ["Gluf", "Steven", ""], ["Saxena", "Shreya", ""], ["Kinsella", "Ian", ""], ["Abe", "Taiga", ""], ["Cunningham", "John P.", ""], ["Paninski", "Liam", ""], ["Churchland", "Anne K", ""]]}, {"id": "2010.15272", "submitter": "Alejandro Ehecatl Morales Huitron", "authors": "Alejandro Morales and Tom Froese", "title": "The distribution of inhibitory neurons in the C. elegans connectome\n  facilitates self-optimization of coordinated neural activity", "comments": "to be published in the 2020 IEEE Symposium Series on Computational\n  Intelligence", "journal-ref": null, "doi": null, "report-no": null, "categories": "q-bio.NC cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The nervous system of the nematode soil worm Caenorhabditis elegans exhibits\nremarkable complexity despite the worm's small size. A general challenge is to\nbetter understand the relationship between neural organization and neural\nactivity at the system level, including the functional roles of inhibitory\nconnections. Here we implemented an abstract simulation model of the C. elegans\nconnectome that approximates the neurotransmitter identity of each neuron, and\nwe explored the functional role of these physiological differences for neural\nactivity. In particular, we created a Hopfield neural network in which all of\nthe worm's neurons characterized by inhibitory neurotransmitters are assigned\ninhibitory outgoing connections. Then, we created a control condition in which\nthe same number of inhibitory connections are arbitrarily distributed across\nthe network. A comparison of these two conditions revealed that the biological\ndistribution of inhibitory connections facilitates the self-optimization of\ncoordinated neural activity compared with an arbitrary distribution of\ninhibitory connections.\n", "versions": [{"version": "v1", "created": "Wed, 28 Oct 2020 23:11:37 GMT"}], "update_date": "2020-10-30", "authors_parsed": [["Morales", "Alejandro", ""], ["Froese", "Tom", ""]]}, {"id": "2010.15308", "submitter": "Kohei Ichikawa", "authors": "Kohei Ichikawa and Kunihiko Kaneko", "title": "Short term memory by transient oscillatory dynamics in recurrent neural\n  networks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "q-bio.NC nlin.AO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Despite the importance of short-term memory in cognitive function, how the\ninput information is encoded and sustained in neural activity dynamics remains\nelusive. Here, by training recurrent neural networks to short-term memory tasks\nand analyzing the dynamics, the characteristic of the short-term memory\nmechanism was obtained in which the input information was encoded in the\namplitude of transient oscillation, rather than the stationary neural\nactivities. This transient orbit was attracted to a slow manifold, which\nallowed for the discarding of irrelevant information. Strong contraction to the\nmanifold results in the noise robustness of the transient orbit, accordingly to\nthe memory. The generality of the result and its relevance to neural\ninformation processing were discussed.\n", "versions": [{"version": "v1", "created": "Thu, 29 Oct 2020 01:50:34 GMT"}], "update_date": "2020-10-30", "authors_parsed": [["Ichikawa", "Kohei", ""], ["Kaneko", "Kunihiko", ""]]}, {"id": "2010.15334", "submitter": "Hayato Chiba", "authors": "Kiyoshi Kotani, Akihiko Akao, Hayato Chiba", "title": "Bifurcation of the neuronal population dynamics of the modified theta\n  model: transition to macroscopic gamma oscillation", "comments": null, "journal-ref": null, "doi": "10.1016/j.physd.2020.132789", "report-no": null, "categories": "q-bio.NC math.DS nlin.AO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Interactions of inhibitory neurons produce gamma oscillations (30--80 Hz) in\nthe local field potential, which is known to be involved in functions such as\ncognition and attention. In this study, the modified theta model is considered\nto investigate the theoretical relationship between the microscopic structure\nof inhibitory neurons and their gamma oscillations under a wide class of\ndistribution functions of tonic currents on individual neurons. The stability\nand bifurcation of gamma oscillations for the Vlasov equation of the model is\ninvestigated by the generalized spectral theory. It is shown that as a\nconnection probability of neurons increases, a pair of generalized eigenvalues\ncrosses the imaginary axis twice, which implies that a stable gamma oscillation\nexists only when the connection probability has a value within a suitable\nrange. On the other hand, when the distribution of tonic currents on individual\nneurons is the Lorentzian distribution, the Vlasov equation is reduced to a\nfinite dimensional dynamical system. The bifurcation analyses of the reduced\nequation exhibit equivalent results with the generalized spectral theory. It is\nalso demonstrated that the numerical computations of neuronal population follow\nthe analyses of the generalized spectral theory as well as the bifurcation\nanalysis of the reduced equation.\n", "versions": [{"version": "v1", "created": "Thu, 29 Oct 2020 03:18:20 GMT"}], "update_date": "2021-02-03", "authors_parsed": [["Kotani", "Kiyoshi", ""], ["Akao", "Akihiko", ""], ["Chiba", "Hayato", ""]]}, {"id": "2010.15573", "submitter": "Andrei Khrennikov Yu", "authors": "Irina Basieva, Andrei Khrennikov, and Masanao Ozawa", "title": "Quantum-like modeling in biology with open quantum systems and\n  instruments", "comments": null, "journal-ref": "Biosystems 201, 104328, 2021", "doi": "10.1016/j.biosystems.2020.104328", "report-no": null, "categories": "physics.bio-ph q-bio.NC quant-ph", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present the novel approach to mathematical modeling of information\nprocesses in biosystems. It explores the mathematical formalism and methodology\nof quantum theory, especially quantum measurement theory. This approach is\nknown as {\\it quantum-like} and it should be distinguished from study of\ngenuine quantum physical processes in biosystems (quantum biophysics, quantum\ncognition). It is based on quantum information representation of biosystem's\nstate and modeling its dynamics in the framework of theory of open quantum\nsystems. This paper starts with the non-physicist friendly presentation of\nquantum measurement theory, from the original von Neumann formulation to modern\ntheory of quantum instruments. Then, latter is applied to model combinations of\ncognitive effects and gene regulation of glucose/lactose metabolism in\nEscherichia coli bacterium. The most general construction of quantum\ninstruments is based on the scheme of indirect measurement, in that measurement\napparatus plays the role of the environment for a biosystem. The biological\nessence of this scheme is illustrated by quantum formalization of Helmholtz\nsensation-perception theory. Then we move to open systems dynamics and consider\nquantum master equation, with concentrating on quantum Markov processes. In\nthis framework, we model functioning of biological functions such as\npsychological functions and epigenetic mutation.\n", "versions": [{"version": "v1", "created": "Tue, 27 Oct 2020 18:38:16 GMT"}], "update_date": "2021-06-15", "authors_parsed": [["Basieva", "Irina", ""], ["Khrennikov", "Andrei", ""], ["Ozawa", "Masanao", ""]]}, {"id": "2010.15594", "submitter": "Muhammad Yousefnezhad", "authors": "Muhammad Yousefnezhad, Alessandro Selvitella, Daoqiang Zhang, Andrew\n  J. Greenshaw, Russell Greiner", "title": "Shared Space Transfer Learning for analyzing multi-site fMRI data", "comments": "34th Conference on Neural Information Processing Systems (NeurIPS\n  2020), Vancouver, Canada. The Supplementary Material:\n  https://www.yousefnezhad.com/publications/NeurIPS2020_Paper4157_SuppMat.zip", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI eess.IV math.FA q-bio.NC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Multi-voxel pattern analysis (MVPA) learns predictive models from task-based\nfunctional magnetic resonance imaging (fMRI) data, for distinguishing when\nsubjects are performing different cognitive tasks -- e.g., watching movies or\nmaking decisions. MVPA works best with a well-designed feature set and an\nadequate sample size. However, most fMRI datasets are noisy, high-dimensional,\nexpensive to collect, and with small sample sizes. Further, training a robust,\ngeneralized predictive model that can analyze homogeneous cognitive tasks\nprovided by multi-site fMRI datasets has additional challenges. This paper\nproposes the Shared Space Transfer Learning (SSTL) as a novel transfer learning\n(TL) approach that can functionally align homogeneous multi-site fMRI datasets,\nand so improve the prediction performance in every site. SSTL first extracts a\nset of common features for all subjects in each site. It then uses TL to map\nthese site-specific features to a site-independent shared space in order to\nimprove the performance of the MVPA. SSTL uses a scalable optimization\nprocedure that works effectively for high-dimensional fMRI datasets. The\noptimization procedure extracts the common features for each site by using a\nsingle-iteration algorithm and maps these site-specific common features to the\nsite-independent shared space. We evaluate the effectiveness of the proposed\nmethod for transferring between various cognitive tasks. Our comprehensive\nexperiments validate that SSTL achieves superior performance to other\nstate-of-the-art analysis techniques.\n", "versions": [{"version": "v1", "created": "Sat, 24 Oct 2020 08:50:26 GMT"}], "update_date": "2020-10-30", "authors_parsed": [["Yousefnezhad", "Muhammad", ""], ["Selvitella", "Alessandro", ""], ["Zhang", "Daoqiang", ""], ["Greenshaw", "Andrew J.", ""], ["Greiner", "Russell", ""]]}, {"id": "2010.16124", "submitter": "Javier Orlandi G", "authors": "Javier G. Orlandi and Jaume Casademunt", "title": "Stochastic quorum percolation and noise focusing in neuronal networks", "comments": null, "journal-ref": null, "doi": "10.1209/0295-5075/133/48002", "report-no": null, "categories": "q-bio.NC cond-mat.stat-mech", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recent experiments have shown that the spontaneous activity of young\ndissociated neuronal cultures can be described as a process of highly\ninhomogeneous nucleation and front propagation due to the localization of noise\nactivity, i.e., noise focusing. However, the basic understanding of the\nmechanisms of noise build-up leading to the nucleation remain an open\nfundamental problem. Here we present a minimal dynamical model called\nstochastic quorum percolation that can account for the observed phenomena,\nwhile providing a robust theoretical framework. The model reproduces the first\nand second order phase--transitions of bursting dynamics and neuronal\navalanches respectively, and captures the profound effect metric correlations\nin the network topology can have on the dynamics. The application of our\nresults to other systems such as in the propagation of infectious diseases and\nof rumors is discussed.\n", "versions": [{"version": "v1", "created": "Fri, 30 Oct 2020 08:49:45 GMT"}], "update_date": "2021-05-26", "authors_parsed": [["Orlandi", "Javier G.", ""], ["Casademunt", "Jaume", ""]]}]